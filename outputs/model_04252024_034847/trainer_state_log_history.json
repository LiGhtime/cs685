[
    {
        "loss": 3.4823,
        "grad_norm": 1.9617481231689453,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.0001288992008249549,
        "step": 1
    },
    {
        "loss": 4.0313,
        "grad_norm": 3.2041451930999756,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.0002577984016499098,
        "step": 2
    },
    {
        "loss": 3.8904,
        "grad_norm": 2.555835485458374,
        "learning_rate": 2.4e-05,
        "epoch": 0.0003866976024748647,
        "step": 3
    },
    {
        "loss": 4.3423,
        "grad_norm": 3.7417595386505127,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0005155968032998196,
        "step": 4
    },
    {
        "loss": 4.0307,
        "grad_norm": 2.430260181427002,
        "learning_rate": 4e-05,
        "epoch": 0.0006444960041247745,
        "step": 5
    },
    {
        "loss": 3.6189,
        "grad_norm": 2.1935107707977295,
        "learning_rate": 4.8e-05,
        "epoch": 0.0007733952049497294,
        "step": 6
    },
    {
        "loss": 4.1653,
        "grad_norm": 3.293205976486206,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0009022944057746842,
        "step": 7
    },
    {
        "loss": 3.8317,
        "grad_norm": 3.604905605316162,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0010311936065996391,
        "step": 8
    },
    {
        "loss": 3.3661,
        "grad_norm": 2.2627551555633545,
        "learning_rate": 7.2e-05,
        "epoch": 0.001160092807424594,
        "step": 9
    },
    {
        "loss": 3.7454,
        "grad_norm": 2.7168374061584473,
        "learning_rate": 8e-05,
        "epoch": 0.001288992008249549,
        "step": 10
    },
    {
        "loss": 3.0931,
        "grad_norm": 1.82854425907135,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0014178912090745037,
        "step": 11
    },
    {
        "loss": 3.5132,
        "grad_norm": 2.2246146202087402,
        "learning_rate": 9.6e-05,
        "epoch": 0.0015467904098994587,
        "step": 12
    },
    {
        "loss": 3.5199,
        "grad_norm": 1.4308444261550903,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0016756896107244135,
        "step": 13
    },
    {
        "loss": 3.1579,
        "grad_norm": 1.953324317932129,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0018045888115493685,
        "step": 14
    },
    {
        "loss": 3.3321,
        "grad_norm": 2.390659809112549,
        "learning_rate": 0.00012,
        "epoch": 0.0019334880123743233,
        "step": 15
    },
    {
        "loss": 3.3498,
        "grad_norm": 3.04976487159729,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0020623872131992783,
        "step": 16
    },
    {
        "loss": 2.899,
        "grad_norm": 2.3018391132354736,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.002191286414024233,
        "step": 17
    },
    {
        "loss": 3.1079,
        "grad_norm": Infinity,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.002320185614849188,
        "step": 18
    },
    {
        "loss": 3.5845,
        "grad_norm": 3.0829296112060547,
        "learning_rate": 0.000144,
        "epoch": 0.0024490848156741426,
        "step": 19
    },
    {
        "loss": 2.7712,
        "grad_norm": 2.1789073944091797,
        "learning_rate": 0.000152,
        "epoch": 0.002577984016499098,
        "step": 20
    },
    {
        "loss": 3.0225,
        "grad_norm": 1.9993314743041992,
        "learning_rate": 0.00016,
        "epoch": 0.0027068832173240526,
        "step": 21
    },
    {
        "loss": 3.2622,
        "grad_norm": 1.371408462524414,
        "learning_rate": 0.000168,
        "epoch": 0.0028357824181490074,
        "step": 22
    },
    {
        "loss": 2.7104,
        "grad_norm": 1.8133646249771118,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.002964681618973962,
        "step": 23
    },
    {
        "loss": 2.8595,
        "grad_norm": 1.5179691314697266,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.0030935808197989174,
        "step": 24
    },
    {
        "loss": 2.4486,
        "grad_norm": 1.8071047067642212,
        "learning_rate": 0.000192,
        "epoch": 0.003222480020623872,
        "step": 25
    },
    {
        "loss": 2.3554,
        "grad_norm": 1.551374077796936,
        "learning_rate": 0.0002,
        "epoch": 0.003351379221448827,
        "step": 26
    },
    {
        "loss": 2.9968,
        "grad_norm": 1.4646908044815063,
        "learning_rate": 0.00019999998149226221,
        "epoch": 0.0034802784222737818,
        "step": 27
    },
    {
        "loss": 2.3564,
        "grad_norm": 2.2040038108825684,
        "learning_rate": 0.0001999999259690557,
        "epoch": 0.003609177623098737,
        "step": 28
    },
    {
        "loss": 2.6052,
        "grad_norm": 1.070785403251648,
        "learning_rate": 0.00019999983343040105,
        "epoch": 0.0037380768239236918,
        "step": 29
    },
    {
        "loss": 2.7306,
        "grad_norm": 1.025121808052063,
        "learning_rate": 0.00019999970387633246,
        "epoch": 0.0038669760247486465,
        "step": 30
    },
    {
        "loss": 2.5883,
        "grad_norm": 1.29859459400177,
        "learning_rate": 0.0001999995373068979,
        "epoch": 0.003995875225573602,
        "step": 31
    },
    {
        "loss": 2.4854,
        "grad_norm": 1.805351972579956,
        "learning_rate": 0.00019999933372215903,
        "epoch": 0.0041247744263985565,
        "step": 32
    },
    {
        "loss": 2.853,
        "grad_norm": 1.2657036781311035,
        "learning_rate": 0.0001999990931221912,
        "epoch": 0.004253673627223511,
        "step": 33
    },
    {
        "loss": 2.924,
        "grad_norm": 1.1883023977279663,
        "learning_rate": 0.0001999988155070835,
        "epoch": 0.004382572828048466,
        "step": 34
    },
    {
        "loss": 2.86,
        "grad_norm": 1.4281904697418213,
        "learning_rate": 0.00019999850087693867,
        "epoch": 0.004511472028873421,
        "step": 35
    },
    {
        "loss": 1.5798,
        "grad_norm": 3.0672414302825928,
        "learning_rate": 0.0001999981492318732,
        "epoch": 0.004640371229698376,
        "step": 36
    },
    {
        "loss": 2.2772,
        "grad_norm": 1.2907944917678833,
        "learning_rate": 0.00019999776057201722,
        "epoch": 0.0047692704305233305,
        "step": 37
    },
    {
        "loss": 2.8831,
        "grad_norm": 1.421180009841919,
        "learning_rate": 0.00019999733489751458,
        "epoch": 0.004898169631348285,
        "step": 38
    },
    {
        "loss": 2.433,
        "grad_norm": 1.2569447755813599,
        "learning_rate": 0.00019999687220852288,
        "epoch": 0.005027068832173241,
        "step": 39
    },
    {
        "loss": 2.346,
        "grad_norm": 2.0183839797973633,
        "learning_rate": 0.00019999637250521336,
        "epoch": 0.005155968032998196,
        "step": 40
    },
    {
        "loss": 2.5575,
        "grad_norm": 1.3994771242141724,
        "learning_rate": 0.000199995835787771,
        "epoch": 0.0052848672338231505,
        "step": 41
    },
    {
        "loss": 2.7738,
        "grad_norm": 1.4030845165252686,
        "learning_rate": 0.00019999526205639453,
        "epoch": 0.005413766434648105,
        "step": 42
    },
    {
        "loss": 2.6329,
        "grad_norm": 1.3750892877578735,
        "learning_rate": 0.0001999946513112962,
        "epoch": 0.00554266563547306,
        "step": 43
    },
    {
        "loss": 2.3796,
        "grad_norm": 2.473461151123047,
        "learning_rate": 0.00019999400355270216,
        "epoch": 0.005671564836298015,
        "step": 44
    },
    {
        "loss": 2.7498,
        "grad_norm": 1.3688890933990479,
        "learning_rate": 0.00019999331878085212,
        "epoch": 0.00580046403712297,
        "step": 45
    },
    {
        "loss": 2.4833,
        "grad_norm": 1.710984706878662,
        "learning_rate": 0.00019999259699599964,
        "epoch": 0.005929363237947924,
        "step": 46
    },
    {
        "loss": 2.8265,
        "grad_norm": 1.2580186128616333,
        "learning_rate": 0.00019999183819841183,
        "epoch": 0.00605826243877288,
        "step": 47
    },
    {
        "loss": 2.2729,
        "grad_norm": 2.277966022491455,
        "learning_rate": 0.00019999104238836956,
        "epoch": 0.006187161639597835,
        "step": 48
    },
    {
        "loss": 2.6164,
        "grad_norm": 1.2022193670272827,
        "learning_rate": 0.00019999020956616744,
        "epoch": 0.00631606084042279,
        "step": 49
    },
    {
        "loss": 1.8517,
        "grad_norm": 1.756743311882019,
        "learning_rate": 0.00019998933973211372,
        "epoch": 0.006444960041247744,
        "step": 50
    },
    {
        "loss": 1.757,
        "grad_norm": 1.929313063621521,
        "learning_rate": 0.00019998843288653037,
        "epoch": 0.006573859242072699,
        "step": 51
    },
    {
        "loss": 2.6978,
        "grad_norm": 1.617903709411621,
        "learning_rate": 0.00019998748902975305,
        "epoch": 0.006702758442897654,
        "step": 52
    },
    {
        "loss": 2.5593,
        "grad_norm": 1.7850507497787476,
        "learning_rate": 0.00019998650816213117,
        "epoch": 0.006831657643722609,
        "step": 53
    },
    {
        "loss": 2.2025,
        "grad_norm": 2.2771553993225098,
        "learning_rate": 0.00019998549028402783,
        "epoch": 0.0069605568445475635,
        "step": 54
    },
    {
        "loss": 2.6243,
        "grad_norm": 1.5262370109558105,
        "learning_rate": 0.00019998443539581971,
        "epoch": 0.007089456045372518,
        "step": 55
    },
    {
        "loss": 2.7596,
        "grad_norm": 1.2342735528945923,
        "learning_rate": 0.00019998334349789732,
        "epoch": 0.007218355246197474,
        "step": 56
    },
    {
        "loss": 1.9323,
        "grad_norm": 2.350027084350586,
        "learning_rate": 0.00019998221459066487,
        "epoch": 0.007347254447022429,
        "step": 57
    },
    {
        "loss": 1.6456,
        "grad_norm": 2.5186736583709717,
        "learning_rate": 0.00019998104867454018,
        "epoch": 0.0074761536478473835,
        "step": 58
    },
    {
        "loss": 1.8919,
        "grad_norm": 2.2604732513427734,
        "learning_rate": 0.00019997984574995483,
        "epoch": 0.007605052848672338,
        "step": 59
    },
    {
        "loss": 2.3253,
        "grad_norm": 1.5076009035110474,
        "learning_rate": 0.00019997860581735414,
        "epoch": 0.007733952049497293,
        "step": 60
    },
    {
        "loss": 2.7788,
        "grad_norm": 1.3799164295196533,
        "learning_rate": 0.00019997732887719701,
        "epoch": 0.007862851250322248,
        "step": 61
    },
    {
        "loss": 2.5492,
        "grad_norm": 1.199083924293518,
        "learning_rate": 0.00019997601492995613,
        "epoch": 0.007991750451147204,
        "step": 62
    },
    {
        "loss": 2.2393,
        "grad_norm": 2.174144744873047,
        "learning_rate": 0.00019997466397611785,
        "epoch": 0.008120649651972157,
        "step": 63
    },
    {
        "loss": 2.0408,
        "grad_norm": 1.082923173904419,
        "learning_rate": 0.00019997327601618229,
        "epoch": 0.008249548852797113,
        "step": 64
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.351813316345215,
        "learning_rate": 0.00019997185105066314,
        "epoch": 0.008378448053622067,
        "step": 65
    },
    {
        "loss": 2.3763,
        "grad_norm": 1.397922158241272,
        "learning_rate": 0.00019997038908008786,
        "epoch": 0.008507347254447023,
        "step": 66
    },
    {
        "loss": 2.84,
        "grad_norm": 1.1796507835388184,
        "learning_rate": 0.00019996889010499768,
        "epoch": 0.008636246455271977,
        "step": 67
    },
    {
        "loss": 2.8129,
        "grad_norm": 1.2723654508590698,
        "learning_rate": 0.00019996735412594735,
        "epoch": 0.008765145656096932,
        "step": 68
    },
    {
        "loss": 2.133,
        "grad_norm": 1.5296800136566162,
        "learning_rate": 0.00019996578114350554,
        "epoch": 0.008894044856921888,
        "step": 69
    },
    {
        "loss": 2.5627,
        "grad_norm": 1.7397527694702148,
        "learning_rate": 0.00019996417115825436,
        "epoch": 0.009022944057746842,
        "step": 70
    },
    {
        "loss": 2.6422,
        "grad_norm": 1.3966046571731567,
        "learning_rate": 0.00019996252417078987,
        "epoch": 0.009151843258571797,
        "step": 71
    },
    {
        "loss": 2.6101,
        "grad_norm": 1.072081208229065,
        "learning_rate": 0.00019996084018172164,
        "epoch": 0.009280742459396751,
        "step": 72
    },
    {
        "loss": 2.8653,
        "grad_norm": 1.1596896648406982,
        "learning_rate": 0.00019995911919167304,
        "epoch": 0.009409641660221707,
        "step": 73
    },
    {
        "loss": 1.6086,
        "grad_norm": 2.1770832538604736,
        "learning_rate": 0.00019995736120128105,
        "epoch": 0.009538540861046661,
        "step": 74
    },
    {
        "loss": 2.203,
        "grad_norm": 1.6158967018127441,
        "learning_rate": 0.00019995556621119648,
        "epoch": 0.009667440061871617,
        "step": 75
    },
    {
        "loss": 1.9302,
        "grad_norm": 2.3115108013153076,
        "learning_rate": 0.0001999537342220837,
        "epoch": 0.00979633926269657,
        "step": 76
    },
    {
        "loss": 2.1511,
        "grad_norm": 2.2797818183898926,
        "learning_rate": 0.00019995186523462087,
        "epoch": 0.009925238463521526,
        "step": 77
    },
    {
        "loss": 2.451,
        "grad_norm": 1.991248607635498,
        "learning_rate": 0.00019994995924949978,
        "epoch": 0.010054137664346482,
        "step": 78
    },
    {
        "loss": 2.377,
        "grad_norm": 1.7440024614334106,
        "learning_rate": 0.00019994801626742594,
        "epoch": 0.010183036865171436,
        "step": 79
    },
    {
        "loss": 2.5698,
        "grad_norm": 1.1014962196350098,
        "learning_rate": 0.00019994603628911855,
        "epoch": 0.010311936065996391,
        "step": 80
    },
    {
        "loss": 2.4588,
        "grad_norm": 1.6812992095947266,
        "learning_rate": 0.0001999440193153105,
        "epoch": 0.010440835266821345,
        "step": 81
    },
    {
        "loss": 2.3916,
        "grad_norm": 1.1382588148117065,
        "learning_rate": 0.0001999419653467484,
        "epoch": 0.010569734467646301,
        "step": 82
    },
    {
        "loss": 1.8948,
        "grad_norm": 2.2840399742126465,
        "learning_rate": 0.00019993987438419252,
        "epoch": 0.010698633668471255,
        "step": 83
    },
    {
        "loss": 2.5869,
        "grad_norm": 1.4664630889892578,
        "learning_rate": 0.00019993774642841684,
        "epoch": 0.01082753286929621,
        "step": 84
    },
    {
        "loss": 2.4764,
        "grad_norm": 1.5741503238677979,
        "learning_rate": 0.00019993558148020905,
        "epoch": 0.010956432070121164,
        "step": 85
    },
    {
        "loss": 2.5919,
        "grad_norm": 1.1892610788345337,
        "learning_rate": 0.00019993337954037053,
        "epoch": 0.01108533127094612,
        "step": 86
    },
    {
        "loss": 2.4164,
        "grad_norm": 1.1814818382263184,
        "learning_rate": 0.0001999311406097163,
        "epoch": 0.011214230471771076,
        "step": 87
    },
    {
        "loss": 2.3055,
        "grad_norm": 1.806555151939392,
        "learning_rate": 0.00019992886468907513,
        "epoch": 0.01134312967259603,
        "step": 88
    },
    {
        "loss": 2.0173,
        "grad_norm": 2.2413010597229004,
        "learning_rate": 0.00019992655177928948,
        "epoch": 0.011472028873420985,
        "step": 89
    },
    {
        "loss": 2.3911,
        "grad_norm": 1.1876940727233887,
        "learning_rate": 0.00019992420188121544,
        "epoch": 0.01160092807424594,
        "step": 90
    },
    {
        "loss": 2.2782,
        "grad_norm": 1.7293914556503296,
        "learning_rate": 0.00019992181499572286,
        "epoch": 0.011729827275070895,
        "step": 91
    },
    {
        "loss": 2.3926,
        "grad_norm": 1.3362562656402588,
        "learning_rate": 0.00019991939112369526,
        "epoch": 0.011858726475895849,
        "step": 92
    },
    {
        "loss": 2.1801,
        "grad_norm": 1.6889069080352783,
        "learning_rate": 0.00019991693026602983,
        "epoch": 0.011987625676720804,
        "step": 93
    },
    {
        "loss": 2.492,
        "grad_norm": 1.067997932434082,
        "learning_rate": 0.00019991443242363752,
        "epoch": 0.01211652487754576,
        "step": 94
    },
    {
        "loss": 2.2853,
        "grad_norm": 1.8583544492721558,
        "learning_rate": 0.00019991189759744287,
        "epoch": 0.012245424078370714,
        "step": 95
    },
    {
        "loss": 2.5012,
        "grad_norm": 1.3248474597930908,
        "learning_rate": 0.00019990932578838418,
        "epoch": 0.01237432327919567,
        "step": 96
    },
    {
        "loss": 2.4901,
        "grad_norm": 0.9751110076904297,
        "learning_rate": 0.00019990671699741337,
        "epoch": 0.012503222480020624,
        "step": 97
    },
    {
        "loss": 2.6319,
        "grad_norm": 1.1833000183105469,
        "learning_rate": 0.00019990407122549617,
        "epoch": 0.01263212168084558,
        "step": 98
    },
    {
        "loss": 2.0165,
        "grad_norm": 2.0646636486053467,
        "learning_rate": 0.0001999013884736119,
        "epoch": 0.012761020881670533,
        "step": 99
    },
    {
        "loss": 1.8704,
        "grad_norm": 2.3384315967559814,
        "learning_rate": 0.00019989866874275353,
        "epoch": 0.012889920082495489,
        "step": 100
    },
    {
        "loss": 2.8626,
        "grad_norm": 1.480448842048645,
        "learning_rate": 0.00019989591203392788,
        "epoch": 0.013018819283320443,
        "step": 101
    },
    {
        "loss": 2.3076,
        "grad_norm": 1.8082352876663208,
        "learning_rate": 0.00019989311834815528,
        "epoch": 0.013147718484145398,
        "step": 102
    },
    {
        "loss": 2.3271,
        "grad_norm": 1.6523479223251343,
        "learning_rate": 0.0001998902876864699,
        "epoch": 0.013276617684970354,
        "step": 103
    },
    {
        "loss": 2.1395,
        "grad_norm": 1.3925837278366089,
        "learning_rate": 0.00019988742004991945,
        "epoch": 0.013405516885795308,
        "step": 104
    },
    {
        "loss": 2.2883,
        "grad_norm": 1.0642257928848267,
        "learning_rate": 0.00019988451543956543,
        "epoch": 0.013534416086620264,
        "step": 105
    },
    {
        "loss": 1.4123,
        "grad_norm": 2.301563262939453,
        "learning_rate": 0.00019988157385648304,
        "epoch": 0.013663315287445217,
        "step": 106
    },
    {
        "loss": 2.5365,
        "grad_norm": 1.1147810220718384,
        "learning_rate": 0.000199878595301761,
        "epoch": 0.013792214488270173,
        "step": 107
    },
    {
        "loss": 1.7626,
        "grad_norm": 2.0391829013824463,
        "learning_rate": 0.00019987557977650197,
        "epoch": 0.013921113689095127,
        "step": 108
    },
    {
        "loss": 2.2352,
        "grad_norm": 1.4785678386688232,
        "learning_rate": 0.00019987252728182207,
        "epoch": 0.014050012889920083,
        "step": 109
    },
    {
        "loss": 2.6735,
        "grad_norm": 1.1023503541946411,
        "learning_rate": 0.00019986943781885125,
        "epoch": 0.014178912090745037,
        "step": 110
    },
    {
        "loss": 2.5417,
        "grad_norm": 1.1287472248077393,
        "learning_rate": 0.0001998663113887331,
        "epoch": 0.014307811291569992,
        "step": 111
    },
    {
        "loss": 2.5019,
        "grad_norm": 1.2665480375289917,
        "learning_rate": 0.00019986314799262482,
        "epoch": 0.014436710492394948,
        "step": 112
    },
    {
        "loss": 2.5619,
        "grad_norm": 1.50442373752594,
        "learning_rate": 0.00019985994763169738,
        "epoch": 0.014565609693219902,
        "step": 113
    },
    {
        "loss": 2.0941,
        "grad_norm": 2.2783994674682617,
        "learning_rate": 0.00019985671030713542,
        "epoch": 0.014694508894044857,
        "step": 114
    },
    {
        "loss": 1.6299,
        "grad_norm": 2.183007001876831,
        "learning_rate": 0.00019985343602013726,
        "epoch": 0.014823408094869811,
        "step": 115
    },
    {
        "loss": 2.5174,
        "grad_norm": 1.565119981765747,
        "learning_rate": 0.0001998501247719149,
        "epoch": 0.014952307295694767,
        "step": 116
    },
    {
        "loss": 1.8666,
        "grad_norm": 2.4257986545562744,
        "learning_rate": 0.00019984677656369396,
        "epoch": 0.015081206496519721,
        "step": 117
    },
    {
        "loss": 1.9854,
        "grad_norm": 1.4317271709442139,
        "learning_rate": 0.00019984339139671384,
        "epoch": 0.015210105697344677,
        "step": 118
    },
    {
        "loss": 2.2105,
        "grad_norm": 1.1275098323822021,
        "learning_rate": 0.00019983996927222758,
        "epoch": 0.01533900489816963,
        "step": 119
    },
    {
        "loss": 2.5297,
        "grad_norm": 1.6894649267196655,
        "learning_rate": 0.00019983651019150184,
        "epoch": 0.015467904098994586,
        "step": 120
    },
    {
        "loss": 2.4904,
        "grad_norm": 1.032475233078003,
        "learning_rate": 0.0001998330141558171,
        "epoch": 0.015596803299819542,
        "step": 121
    },
    {
        "loss": 2.1446,
        "grad_norm": 1.7624047994613647,
        "learning_rate": 0.00019982948116646737,
        "epoch": 0.015725702500644496,
        "step": 122
    },
    {
        "loss": 2.3823,
        "grad_norm": 1.5595550537109375,
        "learning_rate": 0.00019982591122476043,
        "epoch": 0.01585460170146945,
        "step": 123
    },
    {
        "loss": 2.3556,
        "grad_norm": 1.9948993921279907,
        "learning_rate": 0.00019982230433201773,
        "epoch": 0.015983500902294407,
        "step": 124
    },
    {
        "loss": 1.7961,
        "grad_norm": 1.9182394742965698,
        "learning_rate": 0.00019981866048957432,
        "epoch": 0.01611240010311936,
        "step": 125
    },
    {
        "loss": 1.8198,
        "grad_norm": 1.5454742908477783,
        "learning_rate": 0.00019981497969877903,
        "epoch": 0.016241299303944315,
        "step": 126
    },
    {
        "loss": 2.6863,
        "grad_norm": 1.6786932945251465,
        "learning_rate": 0.0001998112619609943,
        "epoch": 0.01637019850476927,
        "step": 127
    },
    {
        "loss": 2.1825,
        "grad_norm": 1.6449482440948486,
        "learning_rate": 0.00019980750727759632,
        "epoch": 0.016499097705594226,
        "step": 128
    },
    {
        "loss": 1.7129,
        "grad_norm": 1.2623412609100342,
        "learning_rate": 0.00019980371564997483,
        "epoch": 0.016627996906419182,
        "step": 129
    },
    {
        "loss": 2.532,
        "grad_norm": 1.9992948770523071,
        "learning_rate": 0.00019979988707953335,
        "epoch": 0.016756896107244134,
        "step": 130
    },
    {
        "loss": 1.9907,
        "grad_norm": 1.8946894407272339,
        "learning_rate": 0.00019979602156768906,
        "epoch": 0.01688579530806909,
        "step": 131
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.9653538465499878,
        "learning_rate": 0.00019979211911587278,
        "epoch": 0.017014694508894045,
        "step": 132
    },
    {
        "loss": 2.0175,
        "grad_norm": 2.0584723949432373,
        "learning_rate": 0.000199788179725529,
        "epoch": 0.017143593709719,
        "step": 133
    },
    {
        "loss": 1.6613,
        "grad_norm": 2.1572887897491455,
        "learning_rate": 0.00019978420339811597,
        "epoch": 0.017272492910543953,
        "step": 134
    },
    {
        "loss": 2.1383,
        "grad_norm": 1.141776204109192,
        "learning_rate": 0.00019978019013510548,
        "epoch": 0.01740139211136891,
        "step": 135
    },
    {
        "loss": 2.3777,
        "grad_norm": 1.2638866901397705,
        "learning_rate": 0.0001997761399379831,
        "epoch": 0.017530291312193864,
        "step": 136
    },
    {
        "loss": 2.8261,
        "grad_norm": 0.9968140125274658,
        "learning_rate": 0.000199772052808248,
        "epoch": 0.01765919051301882,
        "step": 137
    },
    {
        "loss": 2.2045,
        "grad_norm": 1.7014210224151611,
        "learning_rate": 0.00019976792874741307,
        "epoch": 0.017788089713843776,
        "step": 138
    },
    {
        "loss": 2.5681,
        "grad_norm": 1.176948070526123,
        "learning_rate": 0.00019976376775700485,
        "epoch": 0.017916988914668728,
        "step": 139
    },
    {
        "loss": 2.5704,
        "grad_norm": 1.28900945186615,
        "learning_rate": 0.00019975956983856353,
        "epoch": 0.018045888115493684,
        "step": 140
    },
    {
        "loss": 1.6574,
        "grad_norm": 2.1010117530822754,
        "learning_rate": 0.00019975533499364301,
        "epoch": 0.01817478731631864,
        "step": 141
    },
    {
        "loss": 2.3357,
        "grad_norm": 1.5349692106246948,
        "learning_rate": 0.00019975106322381084,
        "epoch": 0.018303686517143595,
        "step": 142
    },
    {
        "loss": 2.3406,
        "grad_norm": 1.8054413795471191,
        "learning_rate": 0.00019974675453064822,
        "epoch": 0.018432585717968547,
        "step": 143
    },
    {
        "loss": 2.3653,
        "grad_norm": 1.302085280418396,
        "learning_rate": 0.00019974240891575007,
        "epoch": 0.018561484918793503,
        "step": 144
    },
    {
        "loss": 2.2406,
        "grad_norm": 1.5780848264694214,
        "learning_rate": 0.00019973802638072487,
        "epoch": 0.01869038411961846,
        "step": 145
    },
    {
        "loss": 2.016,
        "grad_norm": 1.7995679378509521,
        "learning_rate": 0.0001997336069271949,
        "epoch": 0.018819283320443414,
        "step": 146
    },
    {
        "loss": 2.9049,
        "grad_norm": 1.2341418266296387,
        "learning_rate": 0.000199729150556796,
        "epoch": 0.01894818252126837,
        "step": 147
    },
    {
        "loss": 2.4162,
        "grad_norm": 1.3531460762023926,
        "learning_rate": 0.00019972465727117778,
        "epoch": 0.019077081722093322,
        "step": 148
    },
    {
        "loss": 2.5032,
        "grad_norm": 1.0595695972442627,
        "learning_rate": 0.00019972012707200336,
        "epoch": 0.019205980922918277,
        "step": 149
    },
    {
        "loss": 2.6838,
        "grad_norm": 1.3496577739715576,
        "learning_rate": 0.0001997155599609497,
        "epoch": 0.019334880123743233,
        "step": 150
    },
    {
        "loss": 2.3211,
        "grad_norm": 1.3623169660568237,
        "learning_rate": 0.00019971095593970727,
        "epoch": 0.01946377932456819,
        "step": 151
    },
    {
        "loss": 2.3482,
        "grad_norm": 1.8842754364013672,
        "learning_rate": 0.00019970631500998033,
        "epoch": 0.01959267852539314,
        "step": 152
    },
    {
        "loss": 2.6871,
        "grad_norm": 1.9569430351257324,
        "learning_rate": 0.00019970163717348668,
        "epoch": 0.019721577726218097,
        "step": 153
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.3478878736495972,
        "learning_rate": 0.0001996969224319579,
        "epoch": 0.019850476927043052,
        "step": 154
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.2970412969589233,
        "learning_rate": 0.00019969217078713914,
        "epoch": 0.019979376127868008,
        "step": 155
    },
    {
        "loss": 2.4749,
        "grad_norm": 0.8990387916564941,
        "learning_rate": 0.00019968738224078925,
        "epoch": 0.020108275328692964,
        "step": 156
    },
    {
        "loss": 2.5236,
        "grad_norm": 1.2339892387390137,
        "learning_rate": 0.00019968255679468075,
        "epoch": 0.020237174529517916,
        "step": 157
    },
    {
        "loss": 2.7602,
        "grad_norm": 1.2601279020309448,
        "learning_rate": 0.00019967769445059977,
        "epoch": 0.02036607373034287,
        "step": 158
    },
    {
        "loss": 2.5346,
        "grad_norm": 1.707589030265808,
        "learning_rate": 0.0001996727952103462,
        "epoch": 0.020494972931167827,
        "step": 159
    },
    {
        "loss": 2.3562,
        "grad_norm": 1.5939605236053467,
        "learning_rate": 0.0001996678590757334,
        "epoch": 0.020623872131992783,
        "step": 160
    },
    {
        "loss": 2.6086,
        "grad_norm": 0.9617232084274292,
        "learning_rate": 0.0001996628860485886,
        "epoch": 0.020752771332817735,
        "step": 161
    },
    {
        "loss": 2.113,
        "grad_norm": 2.0242502689361572,
        "learning_rate": 0.00019965787613075256,
        "epoch": 0.02088167053364269,
        "step": 162
    },
    {
        "loss": 1.287,
        "grad_norm": 2.5052037239074707,
        "learning_rate": 0.0001996528293240797,
        "epoch": 0.021010569734467646,
        "step": 163
    },
    {
        "loss": 2.0978,
        "grad_norm": 1.7315821647644043,
        "learning_rate": 0.0001996477456304382,
        "epoch": 0.021139468935292602,
        "step": 164
    },
    {
        "loss": 1.8996,
        "grad_norm": 2.113215208053589,
        "learning_rate": 0.0001996426250517097,
        "epoch": 0.021268368136117557,
        "step": 165
    },
    {
        "loss": 2.421,
        "grad_norm": 1.8218735456466675,
        "learning_rate": 0.00019963746758978968,
        "epoch": 0.02139726733694251,
        "step": 166
    },
    {
        "loss": 2.2484,
        "grad_norm": 1.6942636966705322,
        "learning_rate": 0.0001996322732465872,
        "epoch": 0.021526166537767465,
        "step": 167
    },
    {
        "loss": 1.9287,
        "grad_norm": 1.7932119369506836,
        "learning_rate": 0.00019962704202402493,
        "epoch": 0.02165506573859242,
        "step": 168
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.8478789329528809,
        "learning_rate": 0.00019962177392403928,
        "epoch": 0.021783964939417377,
        "step": 169
    },
    {
        "loss": 2.5014,
        "grad_norm": 1.962821364402771,
        "learning_rate": 0.00019961646894858023,
        "epoch": 0.02191286414024233,
        "step": 170
    },
    {
        "loss": 2.1358,
        "grad_norm": 1.8207896947860718,
        "learning_rate": 0.00019961112709961144,
        "epoch": 0.022041763341067284,
        "step": 171
    },
    {
        "loss": 2.5374,
        "grad_norm": 1.4506059885025024,
        "learning_rate": 0.00019960574837911023,
        "epoch": 0.02217066254189224,
        "step": 172
    },
    {
        "loss": 2.0902,
        "grad_norm": 1.674075722694397,
        "learning_rate": 0.00019960033278906757,
        "epoch": 0.022299561742717196,
        "step": 173
    },
    {
        "loss": 2.5542,
        "grad_norm": 1.595948338508606,
        "learning_rate": 0.00019959488033148803,
        "epoch": 0.02242846094354215,
        "step": 174
    },
    {
        "loss": 2.4104,
        "grad_norm": 1.3720048666000366,
        "learning_rate": 0.0001995893910083899,
        "epoch": 0.022557360144367104,
        "step": 175
    },
    {
        "loss": 2.5148,
        "grad_norm": 1.4405661821365356,
        "learning_rate": 0.0001995838648218051,
        "epoch": 0.02268625934519206,
        "step": 176
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.1584389209747314,
        "learning_rate": 0.00019957830177377912,
        "epoch": 0.022815158546017015,
        "step": 177
    },
    {
        "loss": 2.9052,
        "grad_norm": 0.9885630011558533,
        "learning_rate": 0.00019957270186637117,
        "epoch": 0.02294405774684197,
        "step": 178
    },
    {
        "loss": 2.3392,
        "grad_norm": 1.5882962942123413,
        "learning_rate": 0.00019956706510165407,
        "epoch": 0.023072956947666923,
        "step": 179
    },
    {
        "loss": 2.4542,
        "grad_norm": 1.468361496925354,
        "learning_rate": 0.00019956139148171435,
        "epoch": 0.02320185614849188,
        "step": 180
    },
    {
        "loss": 2.1278,
        "grad_norm": 1.4449280500411987,
        "learning_rate": 0.00019955568100865205,
        "epoch": 0.023330755349316834,
        "step": 181
    },
    {
        "loss": 1.9168,
        "grad_norm": 1.2337957620620728,
        "learning_rate": 0.00019954993368458097,
        "epoch": 0.02345965455014179,
        "step": 182
    },
    {
        "loss": 1.3263,
        "grad_norm": 1.863480567932129,
        "learning_rate": 0.00019954414951162849,
        "epoch": 0.023588553750966745,
        "step": 183
    },
    {
        "loss": 1.6611,
        "grad_norm": 1.9282567501068115,
        "learning_rate": 0.0001995383284919357,
        "epoch": 0.023717452951791698,
        "step": 184
    },
    {
        "loss": 2.3439,
        "grad_norm": 1.6146130561828613,
        "learning_rate": 0.00019953247062765722,
        "epoch": 0.023846352152616653,
        "step": 185
    },
    {
        "loss": 2.3369,
        "grad_norm": 1.8996927738189697,
        "learning_rate": 0.00019952657592096137,
        "epoch": 0.02397525135344161,
        "step": 186
    },
    {
        "loss": 2.1419,
        "grad_norm": 2.0624983310699463,
        "learning_rate": 0.00019952064437403014,
        "epoch": 0.024104150554266564,
        "step": 187
    },
    {
        "loss": 1.8014,
        "grad_norm": 1.1987297534942627,
        "learning_rate": 0.0001995146759890591,
        "epoch": 0.02423304975509152,
        "step": 188
    },
    {
        "loss": 1.2719,
        "grad_norm": 2.615151882171631,
        "learning_rate": 0.0001995086707682575,
        "epoch": 0.024361948955916472,
        "step": 189
    },
    {
        "loss": 2.4347,
        "grad_norm": 1.357682704925537,
        "learning_rate": 0.00019950262871384818,
        "epoch": 0.024490848156741428,
        "step": 190
    },
    {
        "loss": 2.4271,
        "grad_norm": 1.0232032537460327,
        "learning_rate": 0.0001994965498280676,
        "epoch": 0.024619747357566384,
        "step": 191
    },
    {
        "loss": 2.6332,
        "grad_norm": 1.2707229852676392,
        "learning_rate": 0.00019949043411316594,
        "epoch": 0.02474864655839134,
        "step": 192
    },
    {
        "loss": 2.3332,
        "grad_norm": 1.6000667810440063,
        "learning_rate": 0.00019948428157140694,
        "epoch": 0.02487754575921629,
        "step": 193
    },
    {
        "loss": 2.4861,
        "grad_norm": 1.4798802137374878,
        "learning_rate": 0.000199478092205068,
        "epoch": 0.025006444960041247,
        "step": 194
    },
    {
        "loss": 2.0426,
        "grad_norm": 1.2771352529525757,
        "learning_rate": 0.00019947186601644012,
        "epoch": 0.025135344160866203,
        "step": 195
    },
    {
        "loss": 2.3689,
        "grad_norm": 1.4542311429977417,
        "learning_rate": 0.00019946560300782799,
        "epoch": 0.02526424336169116,
        "step": 196
    },
    {
        "loss": 1.5032,
        "grad_norm": 2.313541889190674,
        "learning_rate": 0.00019945930318154985,
        "epoch": 0.025393142562516114,
        "step": 197
    },
    {
        "loss": 2.2306,
        "grad_norm": 1.5952069759368896,
        "learning_rate": 0.00019945296653993766,
        "epoch": 0.025522041763341066,
        "step": 198
    },
    {
        "loss": 2.1368,
        "grad_norm": 1.6344289779663086,
        "learning_rate": 0.00019944659308533696,
        "epoch": 0.025650940964166022,
        "step": 199
    },
    {
        "loss": 2.2288,
        "grad_norm": 0.9539589881896973,
        "learning_rate": 0.00019944018282010685,
        "epoch": 0.025779840164990978,
        "step": 200
    },
    {
        "loss": 2.3546,
        "grad_norm": 1.5541762113571167,
        "learning_rate": 0.00019943373574662015,
        "epoch": 0.025908739365815933,
        "step": 201
    },
    {
        "loss": 2.6602,
        "grad_norm": 1.1671931743621826,
        "learning_rate": 0.0001994272518672633,
        "epoch": 0.026037638566640885,
        "step": 202
    },
    {
        "loss": 2.2148,
        "grad_norm": 1.6629812717437744,
        "learning_rate": 0.0001994207311844363,
        "epoch": 0.02616653776746584,
        "step": 203
    },
    {
        "loss": 2.2802,
        "grad_norm": 1.8025760650634766,
        "learning_rate": 0.00019941417370055288,
        "epoch": 0.026295436968290797,
        "step": 204
    },
    {
        "loss": 1.9109,
        "grad_norm": 2.0911221504211426,
        "learning_rate": 0.00019940757941804024,
        "epoch": 0.026424336169115752,
        "step": 205
    },
    {
        "loss": 2.112,
        "grad_norm": 2.342432737350464,
        "learning_rate": 0.00019940094833933934,
        "epoch": 0.026553235369940708,
        "step": 206
    },
    {
        "loss": 2.535,
        "grad_norm": 1.3015261888504028,
        "learning_rate": 0.00019939428046690473,
        "epoch": 0.02668213457076566,
        "step": 207
    },
    {
        "loss": 2.6539,
        "grad_norm": 1.1949341297149658,
        "learning_rate": 0.00019938757580320444,
        "epoch": 0.026811033771590616,
        "step": 208
    },
    {
        "loss": 2.3314,
        "grad_norm": 1.419958472251892,
        "learning_rate": 0.00019938083435072033,
        "epoch": 0.02693993297241557,
        "step": 209
    },
    {
        "loss": 2.4311,
        "grad_norm": 1.3782254457473755,
        "learning_rate": 0.00019937405611194774,
        "epoch": 0.027068832173240527,
        "step": 210
    },
    {
        "loss": 1.5346,
        "grad_norm": 1.6325433254241943,
        "learning_rate": 0.00019936724108939572,
        "epoch": 0.02719773137406548,
        "step": 211
    },
    {
        "loss": 2.2286,
        "grad_norm": 1.3214480876922607,
        "learning_rate": 0.00019936038928558685,
        "epoch": 0.027326630574890435,
        "step": 212
    },
    {
        "loss": 2.357,
        "grad_norm": 1.1555001735687256,
        "learning_rate": 0.00019935350070305732,
        "epoch": 0.02745552977571539,
        "step": 213
    },
    {
        "loss": 1.6705,
        "grad_norm": 1.5244446992874146,
        "learning_rate": 0.00019934657534435702,
        "epoch": 0.027584428976540346,
        "step": 214
    },
    {
        "loss": 2.3627,
        "grad_norm": 2.0798912048339844,
        "learning_rate": 0.0001993396132120494,
        "epoch": 0.027713328177365302,
        "step": 215
    },
    {
        "loss": 2.7396,
        "grad_norm": 0.9908525943756104,
        "learning_rate": 0.00019933261430871152,
        "epoch": 0.027842227378190254,
        "step": 216
    },
    {
        "loss": 1.8075,
        "grad_norm": 2.4406466484069824,
        "learning_rate": 0.00019932557863693403,
        "epoch": 0.02797112657901521,
        "step": 217
    },
    {
        "loss": 2.4881,
        "grad_norm": 1.5080604553222656,
        "learning_rate": 0.00019931850619932126,
        "epoch": 0.028100025779840165,
        "step": 218
    },
    {
        "loss": 2.4555,
        "grad_norm": 1.1581788063049316,
        "learning_rate": 0.00019931139699849108,
        "epoch": 0.02822892498066512,
        "step": 219
    },
    {
        "loss": 2.4489,
        "grad_norm": 1.429112195968628,
        "learning_rate": 0.00019930425103707503,
        "epoch": 0.028357824181490073,
        "step": 220
    },
    {
        "loss": 2.5026,
        "grad_norm": 1.7180317640304565,
        "learning_rate": 0.00019929706831771818,
        "epoch": 0.02848672338231503,
        "step": 221
    },
    {
        "loss": 1.8645,
        "grad_norm": 1.6876752376556396,
        "learning_rate": 0.00019928984884307925,
        "epoch": 0.028615622583139984,
        "step": 222
    },
    {
        "loss": 2.392,
        "grad_norm": 1.47605299949646,
        "learning_rate": 0.00019928259261583055,
        "epoch": 0.02874452178396494,
        "step": 223
    },
    {
        "loss": 2.06,
        "grad_norm": 1.1950092315673828,
        "learning_rate": 0.00019927529963865808,
        "epoch": 0.028873420984789896,
        "step": 224
    },
    {
        "loss": 1.8313,
        "grad_norm": 1.8246595859527588,
        "learning_rate": 0.0001992679699142613,
        "epoch": 0.029002320185614848,
        "step": 225
    },
    {
        "loss": 2.4664,
        "grad_norm": 1.714709758758545,
        "learning_rate": 0.00019926060344535338,
        "epoch": 0.029131219386439804,
        "step": 226
    },
    {
        "loss": 1.912,
        "grad_norm": 1.6736921072006226,
        "learning_rate": 0.00019925320023466103,
        "epoch": 0.02926011858726476,
        "step": 227
    },
    {
        "loss": 2.0424,
        "grad_norm": 2.156862258911133,
        "learning_rate": 0.00019924576028492456,
        "epoch": 0.029389017788089715,
        "step": 228
    },
    {
        "loss": 2.4764,
        "grad_norm": 1.5913139581680298,
        "learning_rate": 0.000199238283598898,
        "epoch": 0.029517916988914667,
        "step": 229
    },
    {
        "loss": 2.6517,
        "grad_norm": 1.0526384115219116,
        "learning_rate": 0.00019923077017934877,
        "epoch": 0.029646816189739623,
        "step": 230
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.7754521369934082,
        "learning_rate": 0.00019922322002905804,
        "epoch": 0.02977571539056458,
        "step": 231
    },
    {
        "loss": 2.676,
        "grad_norm": 1.3305186033248901,
        "learning_rate": 0.00019921563315082055,
        "epoch": 0.029904614591389534,
        "step": 232
    },
    {
        "loss": 2.3585,
        "grad_norm": 1.3819090127944946,
        "learning_rate": 0.0001992080095474446,
        "epoch": 0.03003351379221449,
        "step": 233
    },
    {
        "loss": 2.4109,
        "grad_norm": 1.9982283115386963,
        "learning_rate": 0.00019920034922175212,
        "epoch": 0.030162412993039442,
        "step": 234
    },
    {
        "loss": 2.9682,
        "grad_norm": 1.0063109397888184,
        "learning_rate": 0.0001991926521765786,
        "epoch": 0.030291312193864398,
        "step": 235
    },
    {
        "loss": 2.6336,
        "grad_norm": 0.9906517267227173,
        "learning_rate": 0.00019918491841477317,
        "epoch": 0.030420211394689353,
        "step": 236
    },
    {
        "loss": 2.4962,
        "grad_norm": 1.858814001083374,
        "learning_rate": 0.00019917714793919844,
        "epoch": 0.03054911059551431,
        "step": 237
    },
    {
        "loss": 2.7581,
        "grad_norm": 0.9981606602668762,
        "learning_rate": 0.00019916934075273076,
        "epoch": 0.03067800979633926,
        "step": 238
    },
    {
        "loss": 2.3874,
        "grad_norm": 1.3100308179855347,
        "learning_rate": 0.00019916149685826,
        "epoch": 0.030806908997164217,
        "step": 239
    },
    {
        "loss": 2.5356,
        "grad_norm": 1.3749772310256958,
        "learning_rate": 0.00019915361625868957,
        "epoch": 0.030935808197989172,
        "step": 240
    },
    {
        "loss": 2.3397,
        "grad_norm": 1.0822556018829346,
        "learning_rate": 0.00019914569895693656,
        "epoch": 0.031064707398814128,
        "step": 241
    },
    {
        "loss": 2.2653,
        "grad_norm": 1.7014520168304443,
        "learning_rate": 0.00019913774495593152,
        "epoch": 0.031193606599639084,
        "step": 242
    },
    {
        "loss": 2.4305,
        "grad_norm": 0.739794135093689,
        "learning_rate": 0.00019912975425861876,
        "epoch": 0.031322505800464036,
        "step": 243
    },
    {
        "loss": 1.8738,
        "grad_norm": 2.0758564472198486,
        "learning_rate": 0.00019912172686795604,
        "epoch": 0.03145140500128899,
        "step": 244
    },
    {
        "loss": 2.3466,
        "grad_norm": 1.1568371057510376,
        "learning_rate": 0.00019911366278691466,
        "epoch": 0.03158030420211395,
        "step": 245
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.3742321729660034,
        "learning_rate": 0.00019910556201847967,
        "epoch": 0.0317092034029389,
        "step": 246
    },
    {
        "loss": 2.0377,
        "grad_norm": 1.2797496318817139,
        "learning_rate": 0.0001990974245656496,
        "epoch": 0.03183810260376386,
        "step": 247
    },
    {
        "loss": 2.5688,
        "grad_norm": 1.24356210231781,
        "learning_rate": 0.00019908925043143652,
        "epoch": 0.031967001804588814,
        "step": 248
    },
    {
        "loss": 2.3173,
        "grad_norm": 1.287519931793213,
        "learning_rate": 0.00019908103961886617,
        "epoch": 0.03209590100541377,
        "step": 249
    },
    {
        "loss": 1.5209,
        "grad_norm": 2.2579586505889893,
        "learning_rate": 0.00019907279213097782,
        "epoch": 0.03222480020623872,
        "step": 250
    },
    {
        "loss": 1.6936,
        "grad_norm": 1.8920856714248657,
        "learning_rate": 0.00019906450797082425,
        "epoch": 0.032353699407063674,
        "step": 251
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.4467276334762573,
        "learning_rate": 0.00019905618714147197,
        "epoch": 0.03248259860788863,
        "step": 252
    },
    {
        "loss": 2.0233,
        "grad_norm": 1.660426378250122,
        "learning_rate": 0.0001990478296460009,
        "epoch": 0.032611497808713585,
        "step": 253
    },
    {
        "loss": 1.2871,
        "grad_norm": 2.0722057819366455,
        "learning_rate": 0.00019903943548750468,
        "epoch": 0.03274039700953854,
        "step": 254
    },
    {
        "loss": 1.8016,
        "grad_norm": 2.22735595703125,
        "learning_rate": 0.00019903100466909038,
        "epoch": 0.0328692962103635,
        "step": 255
    },
    {
        "loss": 1.8911,
        "grad_norm": 1.630867838859558,
        "learning_rate": 0.00019902253719387874,
        "epoch": 0.03299819541118845,
        "step": 256
    },
    {
        "loss": 2.0786,
        "grad_norm": 1.8217999935150146,
        "learning_rate": 0.00019901403306500407,
        "epoch": 0.03312709461201341,
        "step": 257
    },
    {
        "loss": 1.793,
        "grad_norm": 2.306617259979248,
        "learning_rate": 0.00019900549228561414,
        "epoch": 0.033255993812838364,
        "step": 258
    },
    {
        "loss": 2.5396,
        "grad_norm": 1.6504151821136475,
        "learning_rate": 0.00019899691485887039,
        "epoch": 0.03338489301366331,
        "step": 259
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.5683931112289429,
        "learning_rate": 0.00019898830078794784,
        "epoch": 0.03351379221448827,
        "step": 260
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.6260305643081665,
        "learning_rate": 0.00019897965007603497,
        "epoch": 0.033642691415313224,
        "step": 261
    },
    {
        "loss": 2.6851,
        "grad_norm": 1.0140442848205566,
        "learning_rate": 0.0001989709627263339,
        "epoch": 0.03377159061613818,
        "step": 262
    },
    {
        "loss": 2.7079,
        "grad_norm": 1.0938141345977783,
        "learning_rate": 0.0001989622387420603,
        "epoch": 0.033900489816963135,
        "step": 263
    },
    {
        "loss": 2.2425,
        "grad_norm": 1.6023023128509521,
        "learning_rate": 0.00019895347812644342,
        "epoch": 0.03402938901778809,
        "step": 264
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.571128010749817,
        "learning_rate": 0.00019894468088272596,
        "epoch": 0.034158288218613046,
        "step": 265
    },
    {
        "loss": 1.9165,
        "grad_norm": 1.5356824398040771,
        "learning_rate": 0.00019893584701416434,
        "epoch": 0.034287187419438,
        "step": 266
    },
    {
        "loss": 2.565,
        "grad_norm": 1.0675290822982788,
        "learning_rate": 0.00019892697652402843,
        "epoch": 0.03441608662026296,
        "step": 267
    },
    {
        "loss": 2.5648,
        "grad_norm": 1.2940528392791748,
        "learning_rate": 0.0001989180694156017,
        "epoch": 0.034544985821087906,
        "step": 268
    },
    {
        "loss": 1.9389,
        "grad_norm": 2.8703904151916504,
        "learning_rate": 0.00019890912569218115,
        "epoch": 0.03467388502191286,
        "step": 269
    },
    {
        "loss": 2.5347,
        "grad_norm": 1.3613773584365845,
        "learning_rate": 0.00019890014535707735,
        "epoch": 0.03480278422273782,
        "step": 270
    },
    {
        "loss": 1.1436,
        "grad_norm": 2.055140733718872,
        "learning_rate": 0.00019889112841361438,
        "epoch": 0.03493168342356277,
        "step": 271
    },
    {
        "loss": 2.1882,
        "grad_norm": 1.0205368995666504,
        "learning_rate": 0.00019888207486512996,
        "epoch": 0.03506058262438773,
        "step": 272
    },
    {
        "loss": 1.9381,
        "grad_norm": 1.987376093864441,
        "learning_rate": 0.00019887298471497525,
        "epoch": 0.035189481825212685,
        "step": 273
    },
    {
        "loss": 2.5058,
        "grad_norm": 1.3274941444396973,
        "learning_rate": 0.00019886385796651505,
        "epoch": 0.03531838102603764,
        "step": 274
    },
    {
        "loss": 1.8309,
        "grad_norm": 2.151310443878174,
        "learning_rate": 0.00019885469462312764,
        "epoch": 0.035447280226862596,
        "step": 275
    },
    {
        "loss": 2.3876,
        "grad_norm": 1.3927792310714722,
        "learning_rate": 0.00019884549468820488,
        "epoch": 0.03557617942768755,
        "step": 276
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.6888583898544312,
        "learning_rate": 0.00019883625816515218,
        "epoch": 0.0357050786285125,
        "step": 277
    },
    {
        "loss": 2.1095,
        "grad_norm": 1.5509374141693115,
        "learning_rate": 0.0001988269850573885,
        "epoch": 0.035833977829337456,
        "step": 278
    },
    {
        "loss": 2.0491,
        "grad_norm": 1.7706854343414307,
        "learning_rate": 0.0001988176753683463,
        "epoch": 0.03596287703016241,
        "step": 279
    },
    {
        "loss": 2.2283,
        "grad_norm": 2.233473300933838,
        "learning_rate": 0.0001988083291014716,
        "epoch": 0.03609177623098737,
        "step": 280
    },
    {
        "loss": 2.55,
        "grad_norm": 1.3284850120544434,
        "learning_rate": 0.000198798946260224,
        "epoch": 0.03622067543181232,
        "step": 281
    },
    {
        "loss": 2.6792,
        "grad_norm": 1.1546796560287476,
        "learning_rate": 0.00019878952684807656,
        "epoch": 0.03634957463263728,
        "step": 282
    },
    {
        "loss": 1.8082,
        "grad_norm": 1.2677404880523682,
        "learning_rate": 0.00019878007086851596,
        "epoch": 0.036478473833462234,
        "step": 283
    },
    {
        "loss": 2.4883,
        "grad_norm": 1.0808184146881104,
        "learning_rate": 0.00019877057832504234,
        "epoch": 0.03660737303428719,
        "step": 284
    },
    {
        "loss": 2.5071,
        "grad_norm": 1.4548382759094238,
        "learning_rate": 0.00019876104922116942,
        "epoch": 0.036736272235112145,
        "step": 285
    },
    {
        "loss": 2.4092,
        "grad_norm": 1.243038535118103,
        "learning_rate": 0.00019875148356042447,
        "epoch": 0.036865171435937094,
        "step": 286
    },
    {
        "loss": 1.8657,
        "grad_norm": 2.352285146713257,
        "learning_rate": 0.00019874188134634822,
        "epoch": 0.03699407063676205,
        "step": 287
    },
    {
        "loss": 2.4335,
        "grad_norm": 1.156527042388916,
        "learning_rate": 0.00019873224258249502,
        "epoch": 0.037122969837587005,
        "step": 288
    },
    {
        "loss": 2.4943,
        "grad_norm": 1.1277639865875244,
        "learning_rate": 0.0001987225672724327,
        "epoch": 0.03725186903841196,
        "step": 289
    },
    {
        "loss": 2.6905,
        "grad_norm": 1.4334666728973389,
        "learning_rate": 0.00019871285541974259,
        "epoch": 0.03738076823923692,
        "step": 290
    },
    {
        "loss": 2.0151,
        "grad_norm": 1.329315185546875,
        "learning_rate": 0.0001987031070280196,
        "epoch": 0.03750966744006187,
        "step": 291
    },
    {
        "loss": 2.4306,
        "grad_norm": 1.5186537504196167,
        "learning_rate": 0.00019869332210087213,
        "epoch": 0.03763856664088683,
        "step": 292
    },
    {
        "loss": 2.1323,
        "grad_norm": 0.9683755040168762,
        "learning_rate": 0.0001986835006419221,
        "epoch": 0.037767465841711784,
        "step": 293
    },
    {
        "loss": 2.3926,
        "grad_norm": 0.9585103392601013,
        "learning_rate": 0.00019867364265480504,
        "epoch": 0.03789636504253674,
        "step": 294
    },
    {
        "loss": 1.6931,
        "grad_norm": 1.6820855140686035,
        "learning_rate": 0.00019866374814316986,
        "epoch": 0.03802526424336169,
        "step": 295
    },
    {
        "loss": 1.6786,
        "grad_norm": 1.9143003225326538,
        "learning_rate": 0.0001986538171106791,
        "epoch": 0.038154163444186644,
        "step": 296
    },
    {
        "loss": 2.2949,
        "grad_norm": 1.463536024093628,
        "learning_rate": 0.00019864384956100876,
        "epoch": 0.0382830626450116,
        "step": 297
    },
    {
        "loss": 2.2746,
        "grad_norm": 1.625756859779358,
        "learning_rate": 0.0001986338454978484,
        "epoch": 0.038411961845836555,
        "step": 298
    },
    {
        "loss": 2.6429,
        "grad_norm": 1.3677207231521606,
        "learning_rate": 0.000198623804924901,
        "epoch": 0.03854086104666151,
        "step": 299
    },
    {
        "loss": 2.3616,
        "grad_norm": 1.4138015508651733,
        "learning_rate": 0.00019861372784588325,
        "epoch": 0.038669760247486466,
        "step": 300
    },
    {
        "loss": 2.5516,
        "grad_norm": 1.2232533693313599,
        "learning_rate": 0.0001986036142645251,
        "epoch": 0.03879865944831142,
        "step": 301
    },
    {
        "loss": 2.5417,
        "grad_norm": 1.145107626914978,
        "learning_rate": 0.0001985934641845702,
        "epoch": 0.03892755864913638,
        "step": 302
    },
    {
        "loss": 2.1178,
        "grad_norm": 1.0282766819000244,
        "learning_rate": 0.00019858327760977567,
        "epoch": 0.03905645784996133,
        "step": 303
    },
    {
        "loss": 2.4877,
        "grad_norm": 1.388466477394104,
        "learning_rate": 0.0001985730545439121,
        "epoch": 0.03918535705078628,
        "step": 304
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.3131903409957886,
        "learning_rate": 0.0001985627949907636,
        "epoch": 0.03931425625161124,
        "step": 305
    },
    {
        "loss": 1.6357,
        "grad_norm": 1.9088292121887207,
        "learning_rate": 0.00019855249895412778,
        "epoch": 0.03944315545243619,
        "step": 306
    },
    {
        "loss": 2.4454,
        "grad_norm": 1.293786883354187,
        "learning_rate": 0.0001985421664378158,
        "epoch": 0.03957205465326115,
        "step": 307
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.087962031364441,
        "learning_rate": 0.0001985317974456523,
        "epoch": 0.039700953854086105,
        "step": 308
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.317360520362854,
        "learning_rate": 0.00019852139198147533,
        "epoch": 0.03982985305491106,
        "step": 309
    },
    {
        "loss": 1.9406,
        "grad_norm": 1.513134241104126,
        "learning_rate": 0.0001985109500491366,
        "epoch": 0.039958752255736016,
        "step": 310
    },
    {
        "loss": 2.5055,
        "grad_norm": 2.003178834915161,
        "learning_rate": 0.00019850047165250123,
        "epoch": 0.04008765145656097,
        "step": 311
    },
    {
        "loss": 2.3843,
        "grad_norm": 1.5862287282943726,
        "learning_rate": 0.00019848995679544782,
        "epoch": 0.04021655065738593,
        "step": 312
    },
    {
        "loss": 2.3311,
        "grad_norm": 1.7953646183013916,
        "learning_rate": 0.0001984794054818685,
        "epoch": 0.040345449858210876,
        "step": 313
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.6839007139205933,
        "learning_rate": 0.00019846881771566892,
        "epoch": 0.04047434905903583,
        "step": 314
    },
    {
        "loss": 1.7982,
        "grad_norm": 3.074915885925293,
        "learning_rate": 0.0001984581935007682,
        "epoch": 0.04060324825986079,
        "step": 315
    },
    {
        "loss": 2.7051,
        "grad_norm": 1.359904170036316,
        "learning_rate": 0.00019844753284109886,
        "epoch": 0.04073214746068574,
        "step": 316
    },
    {
        "loss": 2.1896,
        "grad_norm": 1.1068660020828247,
        "learning_rate": 0.00019843683574060706,
        "epoch": 0.0408610466615107,
        "step": 317
    },
    {
        "loss": 2.0296,
        "grad_norm": 2.197749376296997,
        "learning_rate": 0.0001984261022032524,
        "epoch": 0.040989945862335654,
        "step": 318
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.2790937423706055,
        "learning_rate": 0.00019841533223300789,
        "epoch": 0.04111884506316061,
        "step": 319
    },
    {
        "loss": 2.1974,
        "grad_norm": 1.7425570487976074,
        "learning_rate": 0.00019840452583386013,
        "epoch": 0.041247744263985565,
        "step": 320
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.9880672693252563,
        "learning_rate": 0.00019839368300980916,
        "epoch": 0.04137664346481052,
        "step": 321
    },
    {
        "loss": 2.108,
        "grad_norm": 1.9762732982635498,
        "learning_rate": 0.00019838280376486848,
        "epoch": 0.04150554266563547,
        "step": 322
    },
    {
        "loss": 1.986,
        "grad_norm": 1.873885154724121,
        "learning_rate": 0.00019837188810306508,
        "epoch": 0.041634441866460425,
        "step": 323
    },
    {
        "loss": 2.4449,
        "grad_norm": 1.3683404922485352,
        "learning_rate": 0.0001983609360284395,
        "epoch": 0.04176334106728538,
        "step": 324
    },
    {
        "loss": 2.281,
        "grad_norm": 1.622100591659546,
        "learning_rate": 0.00019834994754504566,
        "epoch": 0.04189224026811034,
        "step": 325
    },
    {
        "loss": 2.4594,
        "grad_norm": 1.2478020191192627,
        "learning_rate": 0.00019833892265695103,
        "epoch": 0.04202113946893529,
        "step": 326
    },
    {
        "loss": 2.1747,
        "grad_norm": 1.9770326614379883,
        "learning_rate": 0.00019832786136823646,
        "epoch": 0.04215003866976025,
        "step": 327
    },
    {
        "loss": 2.47,
        "grad_norm": 1.446980595588684,
        "learning_rate": 0.00019831676368299643,
        "epoch": 0.042278937870585204,
        "step": 328
    },
    {
        "loss": 2.3088,
        "grad_norm": 1.831225872039795,
        "learning_rate": 0.00019830562960533872,
        "epoch": 0.04240783707141016,
        "step": 329
    },
    {
        "loss": 1.5828,
        "grad_norm": 2.3514699935913086,
        "learning_rate": 0.00019829445913938472,
        "epoch": 0.042536736272235115,
        "step": 330
    },
    {
        "loss": 2.5247,
        "grad_norm": 1.0424765348434448,
        "learning_rate": 0.0001982832522892692,
        "epoch": 0.042665635473060064,
        "step": 331
    },
    {
        "loss": 1.7988,
        "grad_norm": 1.6680272817611694,
        "learning_rate": 0.00019827200905914042,
        "epoch": 0.04279453467388502,
        "step": 332
    },
    {
        "loss": 2.4867,
        "grad_norm": 1.1343499422073364,
        "learning_rate": 0.00019826072945316016,
        "epoch": 0.042923433874709975,
        "step": 333
    },
    {
        "loss": 0.9994,
        "grad_norm": 2.2695538997650146,
        "learning_rate": 0.0001982494134755036,
        "epoch": 0.04305233307553493,
        "step": 334
    },
    {
        "loss": 2.2193,
        "grad_norm": 1.5127770900726318,
        "learning_rate": 0.00019823806113035937,
        "epoch": 0.043181232276359886,
        "step": 335
    },
    {
        "loss": 2.3823,
        "grad_norm": 1.512560486793518,
        "learning_rate": 0.00019822667242192963,
        "epoch": 0.04331013147718484,
        "step": 336
    },
    {
        "loss": 2.137,
        "grad_norm": 1.2559893131256104,
        "learning_rate": 0.00019821524735442995,
        "epoch": 0.0434390306780098,
        "step": 337
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.2017215490341187,
        "learning_rate": 0.00019820378593208936,
        "epoch": 0.04356792987883475,
        "step": 338
    },
    {
        "loss": 2.3163,
        "grad_norm": 1.2057616710662842,
        "learning_rate": 0.00019819228815915038,
        "epoch": 0.04369682907965971,
        "step": 339
    },
    {
        "loss": 2.698,
        "grad_norm": 0.8766151070594788,
        "learning_rate": 0.00019818075403986899,
        "epoch": 0.04382572828048466,
        "step": 340
    },
    {
        "loss": 2.0571,
        "grad_norm": 1.8553298711776733,
        "learning_rate": 0.00019816918357851452,
        "epoch": 0.04395462748130961,
        "step": 341
    },
    {
        "loss": 2.3027,
        "grad_norm": 1.2305567264556885,
        "learning_rate": 0.00019815757677936992,
        "epoch": 0.04408352668213457,
        "step": 342
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.4602855443954468,
        "learning_rate": 0.00019814593364673145,
        "epoch": 0.044212425882959525,
        "step": 343
    },
    {
        "loss": 2.0456,
        "grad_norm": 1.222413182258606,
        "learning_rate": 0.00019813425418490892,
        "epoch": 0.04434132508378448,
        "step": 344
    },
    {
        "loss": 1.9966,
        "grad_norm": 1.698820948600769,
        "learning_rate": 0.00019812253839822548,
        "epoch": 0.044470224284609436,
        "step": 345
    },
    {
        "loss": 2.4381,
        "grad_norm": 1.6487364768981934,
        "learning_rate": 0.0001981107862910178,
        "epoch": 0.04459912348543439,
        "step": 346
    },
    {
        "loss": 2.3611,
        "grad_norm": 1.092170000076294,
        "learning_rate": 0.000198098997867636,
        "epoch": 0.04472802268625935,
        "step": 347
    },
    {
        "loss": 2.2867,
        "grad_norm": 1.8065071105957031,
        "learning_rate": 0.00019808717313244358,
        "epoch": 0.0448569218870843,
        "step": 348
    },
    {
        "loss": 2.3827,
        "grad_norm": 1.5137230157852173,
        "learning_rate": 0.0001980753120898176,
        "epoch": 0.04498582108790925,
        "step": 349
    },
    {
        "loss": 2.3739,
        "grad_norm": 1.8064098358154297,
        "learning_rate": 0.00019806341474414837,
        "epoch": 0.04511472028873421,
        "step": 350
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.9159783124923706,
        "learning_rate": 0.00019805148109983985,
        "epoch": 0.04524361948955916,
        "step": 351
    },
    {
        "loss": 2.0129,
        "grad_norm": 1.9849231243133545,
        "learning_rate": 0.00019803951116130928,
        "epoch": 0.04537251869038412,
        "step": 352
    },
    {
        "loss": 2.2785,
        "grad_norm": 1.5190520286560059,
        "learning_rate": 0.00019802750493298738,
        "epoch": 0.045501417891209074,
        "step": 353
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.4024584293365479,
        "learning_rate": 0.00019801546241931837,
        "epoch": 0.04563031709203403,
        "step": 354
    },
    {
        "loss": 2.1556,
        "grad_norm": 0.8789021372795105,
        "learning_rate": 0.00019800338362475982,
        "epoch": 0.045759216292858985,
        "step": 355
    },
    {
        "loss": 2.2383,
        "grad_norm": 0.9271734356880188,
        "learning_rate": 0.0001979912685537827,
        "epoch": 0.04588811549368394,
        "step": 356
    },
    {
        "loss": 2.3385,
        "grad_norm": 0.8926571607589722,
        "learning_rate": 0.00019797911721087155,
        "epoch": 0.0460170146945089,
        "step": 357
    },
    {
        "loss": 2.4429,
        "grad_norm": 1.501628041267395,
        "learning_rate": 0.00019796692960052416,
        "epoch": 0.046145913895333845,
        "step": 358
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.6879630088806152,
        "learning_rate": 0.0001979547057272519,
        "epoch": 0.0462748130961588,
        "step": 359
    },
    {
        "loss": 2.4118,
        "grad_norm": 0.9568758010864258,
        "learning_rate": 0.00019794244559557947,
        "epoch": 0.04640371229698376,
        "step": 360
    },
    {
        "loss": 2.2127,
        "grad_norm": 1.4225720167160034,
        "learning_rate": 0.00019793014921004503,
        "epoch": 0.04653261149780871,
        "step": 361
    },
    {
        "loss": 1.8051,
        "grad_norm": 2.0459864139556885,
        "learning_rate": 0.0001979178165752001,
        "epoch": 0.04666151069863367,
        "step": 362
    },
    {
        "loss": 2.6456,
        "grad_norm": 1.0350780487060547,
        "learning_rate": 0.0001979054476956097,
        "epoch": 0.046790409899458624,
        "step": 363
    },
    {
        "loss": 2.2115,
        "grad_norm": 1.4184989929199219,
        "learning_rate": 0.00019789304257585225,
        "epoch": 0.04691930910028358,
        "step": 364
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.2849618196487427,
        "learning_rate": 0.0001978806012205195,
        "epoch": 0.047048208301108535,
        "step": 365
    },
    {
        "loss": 2.5262,
        "grad_norm": 1.0445730686187744,
        "learning_rate": 0.0001978681236342167,
        "epoch": 0.04717710750193349,
        "step": 366
    },
    {
        "loss": 2.8734,
        "grad_norm": 1.0844870805740356,
        "learning_rate": 0.00019785560982156256,
        "epoch": 0.047306006702758446,
        "step": 367
    },
    {
        "loss": 2.4041,
        "grad_norm": 1.349277138710022,
        "learning_rate": 0.00019784305978718904,
        "epoch": 0.047434905903583395,
        "step": 368
    },
    {
        "loss": 2.6175,
        "grad_norm": 1.3109948635101318,
        "learning_rate": 0.00019783047353574165,
        "epoch": 0.04756380510440835,
        "step": 369
    },
    {
        "loss": 2.0724,
        "grad_norm": 1.4795875549316406,
        "learning_rate": 0.0001978178510718792,
        "epoch": 0.047692704305233306,
        "step": 370
    },
    {
        "loss": 2.4536,
        "grad_norm": 1.2712799310684204,
        "learning_rate": 0.00019780519240027397,
        "epoch": 0.04782160350605826,
        "step": 371
    },
    {
        "loss": 2.0926,
        "grad_norm": 2.103769063949585,
        "learning_rate": 0.00019779249752561166,
        "epoch": 0.04795050270688322,
        "step": 372
    },
    {
        "loss": 1.5733,
        "grad_norm": 1.346026062965393,
        "learning_rate": 0.00019777976645259129,
        "epoch": 0.04807940190770817,
        "step": 373
    },
    {
        "loss": 1.7968,
        "grad_norm": 2.19169545173645,
        "learning_rate": 0.0001977669991859254,
        "epoch": 0.04820830110853313,
        "step": 374
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.0749523639678955,
        "learning_rate": 0.00019775419573033975,
        "epoch": 0.048337200309358085,
        "step": 375
    },
    {
        "loss": 2.4544,
        "grad_norm": 1.3638403415679932,
        "learning_rate": 0.0001977413560905737,
        "epoch": 0.04846609951018304,
        "step": 376
    },
    {
        "loss": 2.2258,
        "grad_norm": 1.4483708143234253,
        "learning_rate": 0.00019772848027137984,
        "epoch": 0.04859499871100799,
        "step": 377
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.110964059829712,
        "learning_rate": 0.00019771556827752424,
        "epoch": 0.048723897911832945,
        "step": 378
    },
    {
        "loss": 2.5048,
        "grad_norm": 1.198879361152649,
        "learning_rate": 0.00019770262011378634,
        "epoch": 0.0488527971126579,
        "step": 379
    },
    {
        "loss": 1.9874,
        "grad_norm": 1.9317846298217773,
        "learning_rate": 0.00019768963578495897,
        "epoch": 0.048981696313482856,
        "step": 380
    },
    {
        "loss": 2.105,
        "grad_norm": 1.6892709732055664,
        "learning_rate": 0.0001976766152958483,
        "epoch": 0.04911059551430781,
        "step": 381
    },
    {
        "loss": 1.7572,
        "grad_norm": 2.036365509033203,
        "learning_rate": 0.00019766355865127397,
        "epoch": 0.04923949471513277,
        "step": 382
    },
    {
        "loss": 2.7153,
        "grad_norm": 1.1320629119873047,
        "learning_rate": 0.00019765046585606895,
        "epoch": 0.04936839391595772,
        "step": 383
    },
    {
        "loss": 2.0744,
        "grad_norm": 0.9909888505935669,
        "learning_rate": 0.0001976373369150796,
        "epoch": 0.04949729311678268,
        "step": 384
    },
    {
        "loss": 1.6535,
        "grad_norm": 1.9945003986358643,
        "learning_rate": 0.00019762417183316565,
        "epoch": 0.049626192317607634,
        "step": 385
    },
    {
        "loss": 2.4856,
        "grad_norm": 1.2759279012680054,
        "learning_rate": 0.0001976109706152002,
        "epoch": 0.04975509151843258,
        "step": 386
    },
    {
        "loss": 2.3929,
        "grad_norm": 1.4498919248580933,
        "learning_rate": 0.00019759773326606976,
        "epoch": 0.04988399071925754,
        "step": 387
    },
    {
        "loss": 2.5944,
        "grad_norm": 1.6285040378570557,
        "learning_rate": 0.00019758445979067423,
        "epoch": 0.050012889920082494,
        "step": 388
    },
    {
        "loss": 2.4455,
        "grad_norm": 1.1453205347061157,
        "learning_rate": 0.00019757115019392683,
        "epoch": 0.05014178912090745,
        "step": 389
    },
    {
        "loss": 2.2954,
        "grad_norm": 1.4470382928848267,
        "learning_rate": 0.00019755780448075413,
        "epoch": 0.050270688321732405,
        "step": 390
    },
    {
        "loss": 2.4537,
        "grad_norm": 1.2398877143859863,
        "learning_rate": 0.00019754442265609618,
        "epoch": 0.05039958752255736,
        "step": 391
    },
    {
        "loss": 1.9895,
        "grad_norm": 2.857710838317871,
        "learning_rate": 0.00019753100472490626,
        "epoch": 0.05052848672338232,
        "step": 392
    },
    {
        "loss": 2.1283,
        "grad_norm": 1.317814826965332,
        "learning_rate": 0.00019751755069215111,
        "epoch": 0.05065738592420727,
        "step": 393
    },
    {
        "loss": 2.3836,
        "grad_norm": 1.5282684564590454,
        "learning_rate": 0.00019750406056281083,
        "epoch": 0.05078628512503223,
        "step": 394
    },
    {
        "loss": 2.4312,
        "grad_norm": 1.3994477987289429,
        "learning_rate": 0.0001974905343418788,
        "epoch": 0.05091518432585718,
        "step": 395
    },
    {
        "loss": 1.8404,
        "grad_norm": 1.3564428091049194,
        "learning_rate": 0.0001974769720343619,
        "epoch": 0.05104408352668213,
        "step": 396
    },
    {
        "loss": 2.3922,
        "grad_norm": 1.703709602355957,
        "learning_rate": 0.00019746337364528015,
        "epoch": 0.05117298272750709,
        "step": 397
    },
    {
        "loss": 2.1127,
        "grad_norm": 1.595308542251587,
        "learning_rate": 0.00019744973917966717,
        "epoch": 0.051301881928332044,
        "step": 398
    },
    {
        "loss": 1.8318,
        "grad_norm": 1.6395282745361328,
        "learning_rate": 0.00019743606864256982,
        "epoch": 0.051430781129157,
        "step": 399
    },
    {
        "loss": 2.1419,
        "grad_norm": 1.5210481882095337,
        "learning_rate": 0.00019742236203904826,
        "epoch": 0.051559680329981955,
        "step": 400
    },
    {
        "loss": 2.1278,
        "grad_norm": 1.8839420080184937,
        "learning_rate": 0.00019740861937417602,
        "epoch": 0.05168857953080691,
        "step": 401
    },
    {
        "loss": 2.3291,
        "grad_norm": 1.444154143333435,
        "learning_rate": 0.00019739484065304014,
        "epoch": 0.051817478731631866,
        "step": 402
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.1816086769104004,
        "learning_rate": 0.0001973810258807408,
        "epoch": 0.05194637793245682,
        "step": 403
    },
    {
        "loss": 2.1692,
        "grad_norm": 1.1991013288497925,
        "learning_rate": 0.0001973671750623916,
        "epoch": 0.05207527713328177,
        "step": 404
    },
    {
        "loss": 1.5316,
        "grad_norm": 2.1617610454559326,
        "learning_rate": 0.00019735328820311948,
        "epoch": 0.052204176334106726,
        "step": 405
    },
    {
        "loss": 2.3937,
        "grad_norm": 1.0007151365280151,
        "learning_rate": 0.00019733936530806477,
        "epoch": 0.05233307553493168,
        "step": 406
    },
    {
        "loss": 2.4804,
        "grad_norm": 2.0556294918060303,
        "learning_rate": 0.00019732540638238107,
        "epoch": 0.05246197473575664,
        "step": 407
    },
    {
        "loss": 2.0823,
        "grad_norm": 1.731122612953186,
        "learning_rate": 0.00019731141143123534,
        "epoch": 0.05259087393658159,
        "step": 408
    },
    {
        "loss": 1.9645,
        "grad_norm": 1.0605534315109253,
        "learning_rate": 0.00019729738045980787,
        "epoch": 0.05271977313740655,
        "step": 409
    },
    {
        "loss": 2.4789,
        "grad_norm": 0.8629014492034912,
        "learning_rate": 0.00019728331347329233,
        "epoch": 0.052848672338231505,
        "step": 410
    },
    {
        "loss": 1.9986,
        "grad_norm": 1.489296793937683,
        "learning_rate": 0.00019726921047689562,
        "epoch": 0.05297757153905646,
        "step": 411
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.289499044418335,
        "learning_rate": 0.0001972550714758381,
        "epoch": 0.053106470739881416,
        "step": 412
    },
    {
        "loss": 1.2979,
        "grad_norm": 2.01684832572937,
        "learning_rate": 0.00019724089647535334,
        "epoch": 0.053235369940706365,
        "step": 413
    },
    {
        "loss": 1.6402,
        "grad_norm": 2.0030314922332764,
        "learning_rate": 0.0001972266854806883,
        "epoch": 0.05336426914153132,
        "step": 414
    },
    {
        "loss": 2.6688,
        "grad_norm": 1.327958345413208,
        "learning_rate": 0.00019721243849710323,
        "epoch": 0.053493168342356276,
        "step": 415
    },
    {
        "loss": 2.5105,
        "grad_norm": 1.422705054283142,
        "learning_rate": 0.00019719815552987174,
        "epoch": 0.05362206754318123,
        "step": 416
    },
    {
        "loss": 2.4235,
        "grad_norm": 1.4913331270217896,
        "learning_rate": 0.00019718383658428074,
        "epoch": 0.05375096674400619,
        "step": 417
    },
    {
        "loss": 1.5178,
        "grad_norm": 1.8942275047302246,
        "learning_rate": 0.00019716948166563044,
        "epoch": 0.05387986594483114,
        "step": 418
    },
    {
        "loss": 2.6654,
        "grad_norm": 1.0725116729736328,
        "learning_rate": 0.0001971550907792344,
        "epoch": 0.0540087651456561,
        "step": 419
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.0712531805038452,
        "learning_rate": 0.00019714066393041946,
        "epoch": 0.054137664346481054,
        "step": 420
    },
    {
        "loss": 1.9915,
        "grad_norm": 0.965260922908783,
        "learning_rate": 0.0001971262011245258,
        "epoch": 0.05426656354730601,
        "step": 421
    },
    {
        "loss": 1.9227,
        "grad_norm": 1.7669017314910889,
        "learning_rate": 0.00019711170236690688,
        "epoch": 0.05439546274813096,
        "step": 422
    },
    {
        "loss": 1.6554,
        "grad_norm": 1.9781525135040283,
        "learning_rate": 0.0001970971676629295,
        "epoch": 0.054524361948955914,
        "step": 423
    },
    {
        "loss": 1.4007,
        "grad_norm": 2.0320072174072266,
        "learning_rate": 0.00019708259701797375,
        "epoch": 0.05465326114978087,
        "step": 424
    },
    {
        "loss": 1.8193,
        "grad_norm": 1.8151055574417114,
        "learning_rate": 0.00019706799043743298,
        "epoch": 0.054782160350605826,
        "step": 425
    },
    {
        "loss": 2.4973,
        "grad_norm": 1.8347480297088623,
        "learning_rate": 0.00019705334792671395,
        "epoch": 0.05491105955143078,
        "step": 426
    },
    {
        "loss": 2.6489,
        "grad_norm": 2.025247097015381,
        "learning_rate": 0.00019703866949123658,
        "epoch": 0.05503995875225574,
        "step": 427
    },
    {
        "loss": 2.0931,
        "grad_norm": 1.246363878250122,
        "learning_rate": 0.00019702395513643425,
        "epoch": 0.05516885795308069,
        "step": 428
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.5203567743301392,
        "learning_rate": 0.0001970092048677535,
        "epoch": 0.05529775715390565,
        "step": 429
    },
    {
        "loss": 1.815,
        "grad_norm": 2.557898998260498,
        "learning_rate": 0.00019699441869065423,
        "epoch": 0.055426656354730604,
        "step": 430
    },
    {
        "loss": 2.7329,
        "grad_norm": 1.2724882364273071,
        "learning_rate": 0.00019697959661060962,
        "epoch": 0.05555555555555555,
        "step": 431
    },
    {
        "loss": 2.1472,
        "grad_norm": 2.062910795211792,
        "learning_rate": 0.00019696473863310606,
        "epoch": 0.05568445475638051,
        "step": 432
    },
    {
        "loss": 2.1811,
        "grad_norm": 1.127959966659546,
        "learning_rate": 0.0001969498447636434,
        "epoch": 0.055813353957205464,
        "step": 433
    },
    {
        "loss": 2.2717,
        "grad_norm": 2.2638070583343506,
        "learning_rate": 0.0001969349150077346,
        "epoch": 0.05594225315803042,
        "step": 434
    },
    {
        "loss": 2.4878,
        "grad_norm": 1.1929205656051636,
        "learning_rate": 0.00019691994937090604,
        "epoch": 0.056071152358855375,
        "step": 435
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.4731231927871704,
        "learning_rate": 0.0001969049478586973,
        "epoch": 0.05620005155968033,
        "step": 436
    },
    {
        "loss": 1.7375,
        "grad_norm": 1.9454302787780762,
        "learning_rate": 0.00019688991047666122,
        "epoch": 0.056328950760505286,
        "step": 437
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.9877452850341797,
        "learning_rate": 0.00019687483723036402,
        "epoch": 0.05645784996133024,
        "step": 438
    },
    {
        "loss": 1.7133,
        "grad_norm": 2.267709970474243,
        "learning_rate": 0.00019685972812538512,
        "epoch": 0.0565867491621552,
        "step": 439
    },
    {
        "loss": 1.9848,
        "grad_norm": 1.5842061042785645,
        "learning_rate": 0.00019684458316731717,
        "epoch": 0.056715648362980146,
        "step": 440
    },
    {
        "loss": 2.4214,
        "grad_norm": 1.6109015941619873,
        "learning_rate": 0.00019682940236176623,
        "epoch": 0.0568445475638051,
        "step": 441
    },
    {
        "loss": 2.2603,
        "grad_norm": 1.6799978017807007,
        "learning_rate": 0.0001968141857143515,
        "epoch": 0.05697344676463006,
        "step": 442
    },
    {
        "loss": 2.3543,
        "grad_norm": 1.3301293849945068,
        "learning_rate": 0.00019679893323070553,
        "epoch": 0.05710234596545501,
        "step": 443
    },
    {
        "loss": 2.602,
        "grad_norm": 1.1449064016342163,
        "learning_rate": 0.00019678364491647408,
        "epoch": 0.05723124516627997,
        "step": 444
    },
    {
        "loss": 1.1749,
        "grad_norm": 2.016695261001587,
        "learning_rate": 0.00019676832077731614,
        "epoch": 0.057360144367104925,
        "step": 445
    },
    {
        "loss": 2.7027,
        "grad_norm": 1.17135488986969,
        "learning_rate": 0.00019675296081890408,
        "epoch": 0.05748904356792988,
        "step": 446
    },
    {
        "loss": 2.8188,
        "grad_norm": 1.489059567451477,
        "learning_rate": 0.00019673756504692347,
        "epoch": 0.057617942768754836,
        "step": 447
    },
    {
        "loss": 2.4886,
        "grad_norm": 1.5443905591964722,
        "learning_rate": 0.00019672213346707306,
        "epoch": 0.05774684196957979,
        "step": 448
    },
    {
        "loss": 1.5753,
        "grad_norm": 2.328690528869629,
        "learning_rate": 0.00019670666608506498,
        "epoch": 0.05787574117040474,
        "step": 449
    },
    {
        "loss": 2.1353,
        "grad_norm": 1.5572878122329712,
        "learning_rate": 0.00019669116290662456,
        "epoch": 0.058004640371229696,
        "step": 450
    },
    {
        "loss": 2.5029,
        "grad_norm": 1.0571329593658447,
        "learning_rate": 0.00019667562393749032,
        "epoch": 0.05813353957205465,
        "step": 451
    },
    {
        "loss": 2.2061,
        "grad_norm": 1.420796275138855,
        "learning_rate": 0.00019666004918341413,
        "epoch": 0.05826243877287961,
        "step": 452
    },
    {
        "loss": 2.261,
        "grad_norm": 1.803571105003357,
        "learning_rate": 0.00019664443865016106,
        "epoch": 0.05839133797370456,
        "step": 453
    },
    {
        "loss": 2.4911,
        "grad_norm": 1.9618902206420898,
        "learning_rate": 0.00019662879234350937,
        "epoch": 0.05852023717452952,
        "step": 454
    },
    {
        "loss": 2.3424,
        "grad_norm": 1.2305783033370972,
        "learning_rate": 0.00019661311026925068,
        "epoch": 0.058649136375354474,
        "step": 455
    },
    {
        "loss": 1.7756,
        "grad_norm": 1.494949460029602,
        "learning_rate": 0.00019659739243318972,
        "epoch": 0.05877803557617943,
        "step": 456
    },
    {
        "loss": 2.1741,
        "grad_norm": 1.4045419692993164,
        "learning_rate": 0.0001965816388411446,
        "epoch": 0.058906934777004386,
        "step": 457
    },
    {
        "loss": 2.545,
        "grad_norm": 1.117095947265625,
        "learning_rate": 0.00019656584949894656,
        "epoch": 0.059035833977829334,
        "step": 458
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.4043986797332764,
        "learning_rate": 0.00019655002441244007,
        "epoch": 0.05916473317865429,
        "step": 459
    },
    {
        "loss": 2.0541,
        "grad_norm": 1.6476861238479614,
        "learning_rate": 0.00019653416358748283,
        "epoch": 0.059293632379479246,
        "step": 460
    },
    {
        "loss": 2.3461,
        "grad_norm": 1.6542073488235474,
        "learning_rate": 0.00019651826702994592,
        "epoch": 0.0594225315803042,
        "step": 461
    },
    {
        "loss": 1.6374,
        "grad_norm": 1.8909571170806885,
        "learning_rate": 0.00019650233474571342,
        "epoch": 0.05955143078112916,
        "step": 462
    },
    {
        "loss": 1.9469,
        "grad_norm": 1.676120638847351,
        "learning_rate": 0.0001964863667406828,
        "epoch": 0.05968032998195411,
        "step": 463
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.6908257007598877,
        "learning_rate": 0.00019647036302076462,
        "epoch": 0.05980922918277907,
        "step": 464
    },
    {
        "loss": 1.6112,
        "grad_norm": 2.4480643272399902,
        "learning_rate": 0.00019645432359188282,
        "epoch": 0.059938128383604024,
        "step": 465
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.6559991836547852,
        "learning_rate": 0.00019643824845997442,
        "epoch": 0.06006702758442898,
        "step": 466
    },
    {
        "loss": 2.4887,
        "grad_norm": 1.475799798965454,
        "learning_rate": 0.00019642213763098975,
        "epoch": 0.06019592678525393,
        "step": 467
    },
    {
        "loss": 2.1333,
        "grad_norm": 1.7622437477111816,
        "learning_rate": 0.00019640599111089222,
        "epoch": 0.060324825986078884,
        "step": 468
    },
    {
        "loss": 1.718,
        "grad_norm": 2.107989549636841,
        "learning_rate": 0.00019638980890565862,
        "epoch": 0.06045372518690384,
        "step": 469
    },
    {
        "loss": 1.9881,
        "grad_norm": 1.2667948007583618,
        "learning_rate": 0.00019637359102127886,
        "epoch": 0.060582624387728795,
        "step": 470
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.552122712135315,
        "learning_rate": 0.00019635733746375605,
        "epoch": 0.06071152358855375,
        "step": 471
    },
    {
        "loss": 2.5977,
        "grad_norm": 1.3432697057724,
        "learning_rate": 0.00019634104823910655,
        "epoch": 0.060840422789378706,
        "step": 472
    },
    {
        "loss": 1.7831,
        "grad_norm": 1.9747925996780396,
        "learning_rate": 0.00019632472335335982,
        "epoch": 0.06096932199020366,
        "step": 473
    },
    {
        "loss": 1.7287,
        "grad_norm": 1.7640269994735718,
        "learning_rate": 0.0001963083628125587,
        "epoch": 0.06109822119102862,
        "step": 474
    },
    {
        "loss": 1.4822,
        "grad_norm": 2.371720314025879,
        "learning_rate": 0.00019629196662275904,
        "epoch": 0.06122712039185357,
        "step": 475
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.7531154155731201,
        "learning_rate": 0.00019627553479003004,
        "epoch": 0.06135601959267852,
        "step": 476
    },
    {
        "loss": 2.4074,
        "grad_norm": 1.0812970399856567,
        "learning_rate": 0.0001962590673204539,
        "epoch": 0.06148491879350348,
        "step": 477
    },
    {
        "loss": 1.8484,
        "grad_norm": 1.67794668674469,
        "learning_rate": 0.00019624256422012625,
        "epoch": 0.06161381799432843,
        "step": 478
    },
    {
        "loss": 1.6662,
        "grad_norm": 1.9061346054077148,
        "learning_rate": 0.0001962260254951558,
        "epoch": 0.06174271719515339,
        "step": 479
    },
    {
        "loss": 2.4379,
        "grad_norm": 1.3725391626358032,
        "learning_rate": 0.00019620945115166432,
        "epoch": 0.061871616395978345,
        "step": 480
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.0234642028808594,
        "learning_rate": 0.00019619284119578697,
        "epoch": 0.0620005155968033,
        "step": 481
    },
    {
        "loss": 2.1824,
        "grad_norm": 2.0517358779907227,
        "learning_rate": 0.00019617619563367197,
        "epoch": 0.062129414797628256,
        "step": 482
    },
    {
        "loss": 1.8052,
        "grad_norm": 2.165276050567627,
        "learning_rate": 0.00019615951447148077,
        "epoch": 0.06225831399845321,
        "step": 483
    },
    {
        "loss": 2.2012,
        "grad_norm": 1.0706475973129272,
        "learning_rate": 0.00019614279771538803,
        "epoch": 0.06238721319927817,
        "step": 484
    },
    {
        "loss": 2.2108,
        "grad_norm": 1.5962789058685303,
        "learning_rate": 0.0001961260453715814,
        "epoch": 0.06251611240010312,
        "step": 485
    },
    {
        "loss": 1.4761,
        "grad_norm": 1.9563848972320557,
        "learning_rate": 0.000196109257446262,
        "epoch": 0.06264501160092807,
        "step": 486
    },
    {
        "loss": 2.3284,
        "grad_norm": 1.4908891916275024,
        "learning_rate": 0.00019609243394564386,
        "epoch": 0.06277391080175303,
        "step": 487
    },
    {
        "loss": 2.4469,
        "grad_norm": 1.566209316253662,
        "learning_rate": 0.0001960755748759543,
        "epoch": 0.06290281000257798,
        "step": 488
    },
    {
        "loss": 1.9737,
        "grad_norm": 2.0605859756469727,
        "learning_rate": 0.00019605868024343378,
        "epoch": 0.06303170920340294,
        "step": 489
    },
    {
        "loss": 2.4454,
        "grad_norm": 1.3231804370880127,
        "learning_rate": 0.00019604175005433599,
        "epoch": 0.0631606084042279,
        "step": 490
    },
    {
        "loss": 2.5895,
        "grad_norm": 0.9973350167274475,
        "learning_rate": 0.0001960247843149276,
        "epoch": 0.06328950760505285,
        "step": 491
    },
    {
        "loss": 2.3854,
        "grad_norm": 1.093674898147583,
        "learning_rate": 0.00019600778303148868,
        "epoch": 0.0634184068058778,
        "step": 492
    },
    {
        "loss": 2.5897,
        "grad_norm": 1.5928646326065063,
        "learning_rate": 0.00019599074621031226,
        "epoch": 0.06354730600670276,
        "step": 493
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.6600912809371948,
        "learning_rate": 0.0001959736738577046,
        "epoch": 0.06367620520752772,
        "step": 494
    },
    {
        "loss": 2.5524,
        "grad_norm": 1.7020214796066284,
        "learning_rate": 0.0001959565659799852,
        "epoch": 0.06380510440835267,
        "step": 495
    },
    {
        "loss": 2.2094,
        "grad_norm": 1.4505797624588013,
        "learning_rate": 0.00019593942258348652,
        "epoch": 0.06393400360917763,
        "step": 496
    },
    {
        "loss": 2.5181,
        "grad_norm": 1.8970749378204346,
        "learning_rate": 0.0001959222436745543,
        "epoch": 0.06406290281000258,
        "step": 497
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.6690248250961304,
        "learning_rate": 0.0001959050292595474,
        "epoch": 0.06419180201082754,
        "step": 498
    },
    {
        "loss": 2.3083,
        "grad_norm": 1.394665241241455,
        "learning_rate": 0.00019588777934483785,
        "epoch": 0.06432070121165248,
        "step": 499
    },
    {
        "loss": 2.589,
        "grad_norm": 1.4191604852676392,
        "learning_rate": 0.00019587049393681076,
        "epoch": 0.06444960041247744,
        "step": 500
    },
    {
        "loss": 2.2464,
        "grad_norm": 1.1193312406539917,
        "learning_rate": 0.0001958531730418644,
        "epoch": 0.06457849961330239,
        "step": 501
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.8962130546569824,
        "learning_rate": 0.00019583581666641017,
        "epoch": 0.06470739881412735,
        "step": 502
    },
    {
        "loss": 2.4766,
        "grad_norm": 1.0108208656311035,
        "learning_rate": 0.00019581842481687265,
        "epoch": 0.0648362980149523,
        "step": 503
    },
    {
        "loss": 2.5157,
        "grad_norm": 0.9319729804992676,
        "learning_rate": 0.0001958009974996895,
        "epoch": 0.06496519721577726,
        "step": 504
    },
    {
        "loss": 1.8316,
        "grad_norm": 1.564918875694275,
        "learning_rate": 0.0001957835347213115,
        "epoch": 0.06509409641660222,
        "step": 505
    },
    {
        "loss": 2.163,
        "grad_norm": 1.716963768005371,
        "learning_rate": 0.00019576603648820262,
        "epoch": 0.06522299561742717,
        "step": 506
    },
    {
        "loss": 2.3599,
        "grad_norm": 1.1810905933380127,
        "learning_rate": 0.00019574850280683988,
        "epoch": 0.06535189481825213,
        "step": 507
    },
    {
        "loss": 2.1421,
        "grad_norm": 1.4659286737442017,
        "learning_rate": 0.00019573093368371346,
        "epoch": 0.06548079401907708,
        "step": 508
    },
    {
        "loss": 1.8371,
        "grad_norm": 1.5980180501937866,
        "learning_rate": 0.0001957133291253267,
        "epoch": 0.06560969321990204,
        "step": 509
    },
    {
        "loss": 1.7988,
        "grad_norm": 1.4193497896194458,
        "learning_rate": 0.000195695689138196,
        "epoch": 0.065738592420727,
        "step": 510
    },
    {
        "loss": 2.021,
        "grad_norm": 1.8238197565078735,
        "learning_rate": 0.00019567801372885079,
        "epoch": 0.06586749162155195,
        "step": 511
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.5063652992248535,
        "learning_rate": 0.00019566030290383383,
        "epoch": 0.0659963908223769,
        "step": 512
    },
    {
        "loss": 2.3037,
        "grad_norm": 1.5133299827575684,
        "learning_rate": 0.00019564255666970078,
        "epoch": 0.06612529002320186,
        "step": 513
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.7409605979919434,
        "learning_rate": 0.00019562477503302055,
        "epoch": 0.06625418922402682,
        "step": 514
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.5776050090789795,
        "learning_rate": 0.00019560695800037507,
        "epoch": 0.06638308842485177,
        "step": 515
    },
    {
        "loss": 2.3359,
        "grad_norm": 1.2457109689712524,
        "learning_rate": 0.00019558910557835942,
        "epoch": 0.06651198762567673,
        "step": 516
    },
    {
        "loss": 1.0364,
        "grad_norm": 1.8778928518295288,
        "learning_rate": 0.00019557121777358174,
        "epoch": 0.06664088682650167,
        "step": 517
    },
    {
        "loss": 2.6167,
        "grad_norm": 1.821790099143982,
        "learning_rate": 0.00019555329459266328,
        "epoch": 0.06676978602732662,
        "step": 518
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.5724899768829346,
        "learning_rate": 0.0001955353360422384,
        "epoch": 0.06689868522815158,
        "step": 519
    },
    {
        "loss": 2.3535,
        "grad_norm": 1.5988341569900513,
        "learning_rate": 0.0001955173421289546,
        "epoch": 0.06702758442897654,
        "step": 520
    },
    {
        "loss": 2.4048,
        "grad_norm": 1.320529580116272,
        "learning_rate": 0.0001954993128594723,
        "epoch": 0.06715648362980149,
        "step": 521
    },
    {
        "loss": 1.9063,
        "grad_norm": 1.9027208089828491,
        "learning_rate": 0.00019548124824046517,
        "epoch": 0.06728538283062645,
        "step": 522
    },
    {
        "loss": 2.2353,
        "grad_norm": 1.7997605800628662,
        "learning_rate": 0.0001954631482786199,
        "epoch": 0.0674142820314514,
        "step": 523
    },
    {
        "loss": 2.6256,
        "grad_norm": 1.4714016914367676,
        "learning_rate": 0.00019544501298063635,
        "epoch": 0.06754318123227636,
        "step": 524
    },
    {
        "loss": 2.3649,
        "grad_norm": 1.6669979095458984,
        "learning_rate": 0.00019542684235322733,
        "epoch": 0.06767208043310131,
        "step": 525
    },
    {
        "loss": 2.4317,
        "grad_norm": 1.6225471496582031,
        "learning_rate": 0.00019540863640311879,
        "epoch": 0.06780097963392627,
        "step": 526
    },
    {
        "loss": 2.0971,
        "grad_norm": 1.658956527709961,
        "learning_rate": 0.00019539039513704972,
        "epoch": 0.06792987883475123,
        "step": 527
    },
    {
        "loss": 1.8322,
        "grad_norm": 1.9461090564727783,
        "learning_rate": 0.00019537211856177224,
        "epoch": 0.06805877803557618,
        "step": 528
    },
    {
        "loss": 2.3418,
        "grad_norm": 0.9819530844688416,
        "learning_rate": 0.00019535380668405153,
        "epoch": 0.06818767723640114,
        "step": 529
    },
    {
        "loss": 1.8814,
        "grad_norm": 1.8702855110168457,
        "learning_rate": 0.00019533545951066578,
        "epoch": 0.06831657643722609,
        "step": 530
    },
    {
        "loss": 2.2757,
        "grad_norm": 1.670125126838684,
        "learning_rate": 0.0001953170770484063,
        "epoch": 0.06844547563805105,
        "step": 531
    },
    {
        "loss": 1.9231,
        "grad_norm": 1.2871309518814087,
        "learning_rate": 0.00019529865930407748,
        "epoch": 0.068574374838876,
        "step": 532
    },
    {
        "loss": 2.2355,
        "grad_norm": 0.8897700905799866,
        "learning_rate": 0.0001952802062844967,
        "epoch": 0.06870327403970096,
        "step": 533
    },
    {
        "loss": 1.9132,
        "grad_norm": 1.5373462438583374,
        "learning_rate": 0.00019526171799649439,
        "epoch": 0.06883217324052592,
        "step": 534
    },
    {
        "loss": 1.8417,
        "grad_norm": 1.3966307640075684,
        "learning_rate": 0.0001952431944469142,
        "epoch": 0.06896107244135086,
        "step": 535
    },
    {
        "loss": 2.424,
        "grad_norm": 1.261224627494812,
        "learning_rate": 0.00019522463564261257,
        "epoch": 0.06908997164217581,
        "step": 536
    },
    {
        "loss": 1.9591,
        "grad_norm": 1.9282737970352173,
        "learning_rate": 0.0001952060415904592,
        "epoch": 0.06921887084300077,
        "step": 537
    },
    {
        "loss": 2.5696,
        "grad_norm": 1.164300560951233,
        "learning_rate": 0.0001951874122973368,
        "epoch": 0.06934777004382572,
        "step": 538
    },
    {
        "loss": 2.0455,
        "grad_norm": 1.291171669960022,
        "learning_rate": 0.000195168747770141,
        "epoch": 0.06947666924465068,
        "step": 539
    },
    {
        "loss": 2.0639,
        "grad_norm": 1.437549114227295,
        "learning_rate": 0.00019515004801578067,
        "epoch": 0.06960556844547564,
        "step": 540
    },
    {
        "loss": 2.4326,
        "grad_norm": 1.531693696975708,
        "learning_rate": 0.00019513131304117753,
        "epoch": 0.06973446764630059,
        "step": 541
    },
    {
        "loss": 2.6498,
        "grad_norm": 1.363265037536621,
        "learning_rate": 0.00019511254285326646,
        "epoch": 0.06986336684712555,
        "step": 542
    },
    {
        "loss": 1.6169,
        "grad_norm": 2.0287764072418213,
        "learning_rate": 0.00019509373745899532,
        "epoch": 0.0699922660479505,
        "step": 543
    },
    {
        "loss": 2.4373,
        "grad_norm": 0.9168703556060791,
        "learning_rate": 0.000195074896865325,
        "epoch": 0.07012116524877546,
        "step": 544
    },
    {
        "loss": 1.9727,
        "grad_norm": 2.0333898067474365,
        "learning_rate": 0.00019505602107922946,
        "epoch": 0.07025006444960041,
        "step": 545
    },
    {
        "loss": 1.9057,
        "grad_norm": 1.2770649194717407,
        "learning_rate": 0.00019503711010769568,
        "epoch": 0.07037896365042537,
        "step": 546
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.4948090314865112,
        "learning_rate": 0.0001950181639577236,
        "epoch": 0.07050786285125032,
        "step": 547
    },
    {
        "loss": 2.249,
        "grad_norm": 1.8656717538833618,
        "learning_rate": 0.00019499918263632626,
        "epoch": 0.07063676205207528,
        "step": 548
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.4944249391555786,
        "learning_rate": 0.00019498016615052967,
        "epoch": 0.07076566125290024,
        "step": 549
    },
    {
        "loss": 2.0246,
        "grad_norm": 1.7992035150527954,
        "learning_rate": 0.0001949611145073729,
        "epoch": 0.07089456045372519,
        "step": 550
    },
    {
        "loss": 1.8434,
        "grad_norm": 1.7062904834747314,
        "learning_rate": 0.00019494202771390797,
        "epoch": 0.07102345965455015,
        "step": 551
    },
    {
        "loss": 1.2882,
        "grad_norm": 2.209129571914673,
        "learning_rate": 0.00019492290577719995,
        "epoch": 0.0711523588553751,
        "step": 552
    },
    {
        "loss": 1.8,
        "grad_norm": 1.835942029953003,
        "learning_rate": 0.00019490374870432696,
        "epoch": 0.07128125805620004,
        "step": 553
    },
    {
        "loss": 2.1693,
        "grad_norm": 1.51225745677948,
        "learning_rate": 0.00019488455650238003,
        "epoch": 0.071410157257025,
        "step": 554
    },
    {
        "loss": 2.2765,
        "grad_norm": 1.3229472637176514,
        "learning_rate": 0.00019486532917846328,
        "epoch": 0.07153905645784996,
        "step": 555
    },
    {
        "loss": 2.4952,
        "grad_norm": 1.3433070182800293,
        "learning_rate": 0.00019484606673969378,
        "epoch": 0.07166795565867491,
        "step": 556
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.5974090099334717,
        "learning_rate": 0.00019482676919320159,
        "epoch": 0.07179685485949987,
        "step": 557
    },
    {
        "loss": 2.6405,
        "grad_norm": 1.5498617887496948,
        "learning_rate": 0.00019480743654612983,
        "epoch": 0.07192575406032482,
        "step": 558
    },
    {
        "loss": 2.3193,
        "grad_norm": 1.5463714599609375,
        "learning_rate": 0.00019478806880563455,
        "epoch": 0.07205465326114978,
        "step": 559
    },
    {
        "loss": 2.4247,
        "grad_norm": 1.0251137018203735,
        "learning_rate": 0.0001947686659788848,
        "epoch": 0.07218355246197473,
        "step": 560
    },
    {
        "loss": 2.2161,
        "grad_norm": 1.4798166751861572,
        "learning_rate": 0.0001947492280730627,
        "epoch": 0.07231245166279969,
        "step": 561
    },
    {
        "loss": 2.1841,
        "grad_norm": 1.4536802768707275,
        "learning_rate": 0.00019472975509536315,
        "epoch": 0.07244135086362465,
        "step": 562
    },
    {
        "loss": 2.6164,
        "grad_norm": 1.1485992670059204,
        "learning_rate": 0.00019471024705299428,
        "epoch": 0.0725702500644496,
        "step": 563
    },
    {
        "loss": 2.3437,
        "grad_norm": 1.1578460931777954,
        "learning_rate": 0.00019469070395317705,
        "epoch": 0.07269914926527456,
        "step": 564
    },
    {
        "loss": 2.442,
        "grad_norm": 1.3990424871444702,
        "learning_rate": 0.00019467112580314543,
        "epoch": 0.07282804846609951,
        "step": 565
    },
    {
        "loss": 2.2899,
        "grad_norm": 1.8512537479400635,
        "learning_rate": 0.00019465151261014636,
        "epoch": 0.07295694766692447,
        "step": 566
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.5428417921066284,
        "learning_rate": 0.00019463186438143976,
        "epoch": 0.07308584686774942,
        "step": 567
    },
    {
        "loss": 2.154,
        "grad_norm": 1.1543400287628174,
        "learning_rate": 0.00019461218112429852,
        "epoch": 0.07321474606857438,
        "step": 568
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.564091205596924,
        "learning_rate": 0.00019459246284600848,
        "epoch": 0.07334364526939934,
        "step": 569
    },
    {
        "loss": 1.4774,
        "grad_norm": 2.2035083770751953,
        "learning_rate": 0.00019457270955386847,
        "epoch": 0.07347254447022429,
        "step": 570
    },
    {
        "loss": 2.3709,
        "grad_norm": 1.1214295625686646,
        "learning_rate": 0.00019455292125519023,
        "epoch": 0.07360144367104923,
        "step": 571
    },
    {
        "loss": 1.9795,
        "grad_norm": 1.9393187761306763,
        "learning_rate": 0.00019453309795729854,
        "epoch": 0.07373034287187419,
        "step": 572
    },
    {
        "loss": 2.0444,
        "grad_norm": 1.7595072984695435,
        "learning_rate": 0.00019451323966753105,
        "epoch": 0.07385924207269914,
        "step": 573
    },
    {
        "loss": 2.0951,
        "grad_norm": 0.9978696703910828,
        "learning_rate": 0.00019449334639323843,
        "epoch": 0.0739881412735241,
        "step": 574
    },
    {
        "loss": 2.2059,
        "grad_norm": 1.8117989301681519,
        "learning_rate": 0.0001944734181417843,
        "epoch": 0.07411704047434906,
        "step": 575
    },
    {
        "loss": 2.3445,
        "grad_norm": 0.9616260528564453,
        "learning_rate": 0.00019445345492054509,
        "epoch": 0.07424593967517401,
        "step": 576
    },
    {
        "loss": 2.101,
        "grad_norm": 0.9215405583381653,
        "learning_rate": 0.00019443345673691038,
        "epoch": 0.07437483887599897,
        "step": 577
    },
    {
        "loss": 2.1705,
        "grad_norm": 1.3610330820083618,
        "learning_rate": 0.00019441342359828253,
        "epoch": 0.07450373807682392,
        "step": 578
    },
    {
        "loss": 2.2175,
        "grad_norm": 2.043870210647583,
        "learning_rate": 0.00019439335551207698,
        "epoch": 0.07463263727764888,
        "step": 579
    },
    {
        "loss": 2.0528,
        "grad_norm": 1.2513043880462646,
        "learning_rate": 0.00019437325248572194,
        "epoch": 0.07476153647847383,
        "step": 580
    },
    {
        "loss": 2.2937,
        "grad_norm": 0.9622134566307068,
        "learning_rate": 0.00019435311452665867,
        "epoch": 0.07489043567929879,
        "step": 581
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.4502308368682861,
        "learning_rate": 0.00019433294164234138,
        "epoch": 0.07501933488012374,
        "step": 582
    },
    {
        "loss": 1.4907,
        "grad_norm": 1.8496403694152832,
        "learning_rate": 0.00019431273384023706,
        "epoch": 0.0751482340809487,
        "step": 583
    },
    {
        "loss": 2.1287,
        "grad_norm": 1.4312645196914673,
        "learning_rate": 0.00019429249112782584,
        "epoch": 0.07527713328177366,
        "step": 584
    },
    {
        "loss": 2.2095,
        "grad_norm": 1.0098752975463867,
        "learning_rate": 0.00019427221351260058,
        "epoch": 0.07540603248259861,
        "step": 585
    },
    {
        "loss": 1.9673,
        "grad_norm": 1.5201947689056396,
        "learning_rate": 0.00019425190100206714,
        "epoch": 0.07553493168342357,
        "step": 586
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.6208447217941284,
        "learning_rate": 0.0001942315536037443,
        "epoch": 0.07566383088424852,
        "step": 587
    },
    {
        "loss": 2.2785,
        "grad_norm": 1.7621517181396484,
        "learning_rate": 0.0001942111713251638,
        "epoch": 0.07579273008507348,
        "step": 588
    },
    {
        "loss": 1.7134,
        "grad_norm": 1.5209112167358398,
        "learning_rate": 0.00019419075417387015,
        "epoch": 0.07592162928589843,
        "step": 589
    },
    {
        "loss": 2.4301,
        "grad_norm": 1.412805199623108,
        "learning_rate": 0.00019417030215742093,
        "epoch": 0.07605052848672338,
        "step": 590
    },
    {
        "loss": 2.4823,
        "grad_norm": 1.3746299743652344,
        "learning_rate": 0.0001941498152833865,
        "epoch": 0.07617942768754833,
        "step": 591
    },
    {
        "loss": 2.2921,
        "grad_norm": 1.419003963470459,
        "learning_rate": 0.00019412929355935022,
        "epoch": 0.07630832688837329,
        "step": 592
    },
    {
        "loss": 2.1814,
        "grad_norm": 2.1464500427246094,
        "learning_rate": 0.0001941087369929082,
        "epoch": 0.07643722608919824,
        "step": 593
    },
    {
        "loss": 1.8808,
        "grad_norm": 1.7482550144195557,
        "learning_rate": 0.0001940881455916697,
        "epoch": 0.0765661252900232,
        "step": 594
    },
    {
        "loss": 2.0101,
        "grad_norm": 1.3823621273040771,
        "learning_rate": 0.00019406751936325664,
        "epoch": 0.07669502449084815,
        "step": 595
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.1174330711364746,
        "learning_rate": 0.0001940468583153039,
        "epoch": 0.07682392369167311,
        "step": 596
    },
    {
        "loss": 1.6476,
        "grad_norm": 1.615407109260559,
        "learning_rate": 0.0001940261624554593,
        "epoch": 0.07695282289249807,
        "step": 597
    },
    {
        "loss": 2.6063,
        "grad_norm": 1.042967677116394,
        "learning_rate": 0.00019400543179138354,
        "epoch": 0.07708172209332302,
        "step": 598
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.2971299886703491,
        "learning_rate": 0.00019398466633075006,
        "epoch": 0.07721062129414798,
        "step": 599
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.0827997922897339,
        "learning_rate": 0.00019396386608124544,
        "epoch": 0.07733952049497293,
        "step": 600
    },
    {
        "loss": 2.1703,
        "grad_norm": 1.8564257621765137,
        "learning_rate": 0.0001939430310505689,
        "epoch": 0.07746841969579789,
        "step": 601
    },
    {
        "loss": 1.6626,
        "grad_norm": 1.9426686763763428,
        "learning_rate": 0.00019392216124643265,
        "epoch": 0.07759731889662284,
        "step": 602
    },
    {
        "loss": 2.3701,
        "grad_norm": 1.1379141807556152,
        "learning_rate": 0.00019390125667656177,
        "epoch": 0.0777262180974478,
        "step": 603
    },
    {
        "loss": 1.9122,
        "grad_norm": 1.8095943927764893,
        "learning_rate": 0.00019388031734869415,
        "epoch": 0.07785511729827276,
        "step": 604
    },
    {
        "loss": 2.0529,
        "grad_norm": 1.5041671991348267,
        "learning_rate": 0.00019385934327058056,
        "epoch": 0.07798401649909771,
        "step": 605
    },
    {
        "loss": 1.7875,
        "grad_norm": 1.6872187852859497,
        "learning_rate": 0.0001938383344499847,
        "epoch": 0.07811291569992267,
        "step": 606
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.7240983247756958,
        "learning_rate": 0.0001938172908946831,
        "epoch": 0.07824181490074762,
        "step": 607
    },
    {
        "loss": 1.4356,
        "grad_norm": 1.8144164085388184,
        "learning_rate": 0.0001937962126124651,
        "epoch": 0.07837071410157256,
        "step": 608
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.3837465047836304,
        "learning_rate": 0.0001937750996111329,
        "epoch": 0.07849961330239752,
        "step": 609
    },
    {
        "loss": 2.0335,
        "grad_norm": 1.5960201025009155,
        "learning_rate": 0.00019375395189850166,
        "epoch": 0.07862851250322248,
        "step": 610
    },
    {
        "loss": 1.923,
        "grad_norm": 1.7836226224899292,
        "learning_rate": 0.00019373276948239922,
        "epoch": 0.07875741170404743,
        "step": 611
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.6400721073150635,
        "learning_rate": 0.0001937115523706664,
        "epoch": 0.07888631090487239,
        "step": 612
    },
    {
        "loss": 1.9844,
        "grad_norm": 1.2616002559661865,
        "learning_rate": 0.0001936903005711568,
        "epoch": 0.07901521010569734,
        "step": 613
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.4509309530258179,
        "learning_rate": 0.0001936690140917369,
        "epoch": 0.0791441093065223,
        "step": 614
    },
    {
        "loss": 2.4599,
        "grad_norm": 1.4769707918167114,
        "learning_rate": 0.00019364769294028596,
        "epoch": 0.07927300850734725,
        "step": 615
    },
    {
        "loss": 1.2034,
        "grad_norm": 1.8392342329025269,
        "learning_rate": 0.00019362633712469607,
        "epoch": 0.07940190770817221,
        "step": 616
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.638683795928955,
        "learning_rate": 0.00019360494665287227,
        "epoch": 0.07953080690899716,
        "step": 617
    },
    {
        "loss": 1.6816,
        "grad_norm": 2.4226319789886475,
        "learning_rate": 0.0001935835215327323,
        "epoch": 0.07965970610982212,
        "step": 618
    },
    {
        "loss": 1.7287,
        "grad_norm": 1.7332377433776855,
        "learning_rate": 0.0001935620617722068,
        "epoch": 0.07978860531064708,
        "step": 619
    },
    {
        "loss": 1.9967,
        "grad_norm": 1.197197437286377,
        "learning_rate": 0.00019354056737923915,
        "epoch": 0.07991750451147203,
        "step": 620
    },
    {
        "loss": 2.364,
        "grad_norm": 1.0835081338882446,
        "learning_rate": 0.00019351903836178563,
        "epoch": 0.08004640371229699,
        "step": 621
    },
    {
        "loss": 1.8824,
        "grad_norm": 1.6005812883377075,
        "learning_rate": 0.00019349747472781532,
        "epoch": 0.08017530291312194,
        "step": 622
    },
    {
        "loss": 2.4122,
        "grad_norm": 1.007738471031189,
        "learning_rate": 0.00019347587648531008,
        "epoch": 0.0803042021139469,
        "step": 623
    },
    {
        "loss": 2.0254,
        "grad_norm": 2.283381938934326,
        "learning_rate": 0.00019345424364226464,
        "epoch": 0.08043310131477185,
        "step": 624
    },
    {
        "loss": 2.2671,
        "grad_norm": 0.7296956777572632,
        "learning_rate": 0.00019343257620668648,
        "epoch": 0.08056200051559681,
        "step": 625
    },
    {
        "loss": 2.5058,
        "grad_norm": 1.270681619644165,
        "learning_rate": 0.0001934108741865958,
        "epoch": 0.08069089971642175,
        "step": 626
    },
    {
        "loss": 1.9193,
        "grad_norm": 1.8089544773101807,
        "learning_rate": 0.0001933891375900259,
        "epoch": 0.08081979891724671,
        "step": 627
    },
    {
        "loss": 1.2284,
        "grad_norm": 2.0124857425689697,
        "learning_rate": 0.00019336736642502257,
        "epoch": 0.08094869811807166,
        "step": 628
    },
    {
        "loss": 2.7121,
        "grad_norm": 1.2349846363067627,
        "learning_rate": 0.00019334556069964452,
        "epoch": 0.08107759731889662,
        "step": 629
    },
    {
        "loss": 2.37,
        "grad_norm": 1.2163559198379517,
        "learning_rate": 0.00019332372042196326,
        "epoch": 0.08120649651972157,
        "step": 630
    },
    {
        "loss": 1.1368,
        "grad_norm": 1.871875286102295,
        "learning_rate": 0.00019330184560006306,
        "epoch": 0.08133539572054653,
        "step": 631
    },
    {
        "loss": 2.3135,
        "grad_norm": 2.1482508182525635,
        "learning_rate": 0.00019327993624204098,
        "epoch": 0.08146429492137149,
        "step": 632
    },
    {
        "loss": 2.4439,
        "grad_norm": 1.0817400217056274,
        "learning_rate": 0.0001932579923560069,
        "epoch": 0.08159319412219644,
        "step": 633
    },
    {
        "loss": 1.4016,
        "grad_norm": 2.6570887565612793,
        "learning_rate": 0.00019323601395008342,
        "epoch": 0.0817220933230214,
        "step": 634
    },
    {
        "loss": 2.082,
        "grad_norm": 1.5694588422775269,
        "learning_rate": 0.000193214001032406,
        "epoch": 0.08185099252384635,
        "step": 635
    },
    {
        "loss": 1.9657,
        "grad_norm": 2.3171911239624023,
        "learning_rate": 0.00019319195361112277,
        "epoch": 0.08197989172467131,
        "step": 636
    },
    {
        "loss": 2.1977,
        "grad_norm": 1.4644501209259033,
        "learning_rate": 0.00019316987169439473,
        "epoch": 0.08210879092549626,
        "step": 637
    },
    {
        "loss": 2.0291,
        "grad_norm": 1.3253425359725952,
        "learning_rate": 0.00019314775529039556,
        "epoch": 0.08223769012632122,
        "step": 638
    },
    {
        "loss": 2.6929,
        "grad_norm": 1.0116592645645142,
        "learning_rate": 0.00019312560440731184,
        "epoch": 0.08236658932714618,
        "step": 639
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.7829774618148804,
        "learning_rate": 0.00019310341905334272,
        "epoch": 0.08249548852797113,
        "step": 640
    },
    {
        "loss": 2.7209,
        "grad_norm": 1.1682353019714355,
        "learning_rate": 0.0001930811992367003,
        "epoch": 0.08262438772879609,
        "step": 641
    },
    {
        "loss": 2.4103,
        "grad_norm": 1.0038996934890747,
        "learning_rate": 0.00019305894496560928,
        "epoch": 0.08275328692962104,
        "step": 642
    },
    {
        "loss": 2.2284,
        "grad_norm": 1.061350703239441,
        "learning_rate": 0.00019303665624830724,
        "epoch": 0.082882186130446,
        "step": 643
    },
    {
        "loss": 2.5578,
        "grad_norm": 0.9599969387054443,
        "learning_rate": 0.00019301433309304442,
        "epoch": 0.08301108533127094,
        "step": 644
    },
    {
        "loss": 2.3296,
        "grad_norm": 2.311750888824463,
        "learning_rate": 0.00019299197550808387,
        "epoch": 0.0831399845320959,
        "step": 645
    },
    {
        "loss": 1.9796,
        "grad_norm": 1.6463885307312012,
        "learning_rate": 0.0001929695835017013,
        "epoch": 0.08326888373292085,
        "step": 646
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.536744475364685,
        "learning_rate": 0.00019294715708218532,
        "epoch": 0.0833977829337458,
        "step": 647
    },
    {
        "loss": 1.8222,
        "grad_norm": 1.5214451551437378,
        "learning_rate": 0.00019292469625783708,
        "epoch": 0.08352668213457076,
        "step": 648
    },
    {
        "loss": 2.5701,
        "grad_norm": 0.9294540882110596,
        "learning_rate": 0.00019290220103697058,
        "epoch": 0.08365558133539572,
        "step": 649
    },
    {
        "loss": 2.2467,
        "grad_norm": 1.4822453260421753,
        "learning_rate": 0.00019287967142791258,
        "epoch": 0.08378448053622067,
        "step": 650
    },
    {
        "loss": 2.5842,
        "grad_norm": 1.6406410932540894,
        "learning_rate": 0.0001928571074390025,
        "epoch": 0.08391337973704563,
        "step": 651
    },
    {
        "loss": 2.6763,
        "grad_norm": 1.2317180633544922,
        "learning_rate": 0.00019283450907859245,
        "epoch": 0.08404227893787058,
        "step": 652
    },
    {
        "loss": 2.0228,
        "grad_norm": 1.4981001615524292,
        "learning_rate": 0.0001928118763550474,
        "epoch": 0.08417117813869554,
        "step": 653
    },
    {
        "loss": 1.5341,
        "grad_norm": 1.9941905736923218,
        "learning_rate": 0.00019278920927674493,
        "epoch": 0.0843000773395205,
        "step": 654
    },
    {
        "loss": 2.4467,
        "grad_norm": 1.2247854471206665,
        "learning_rate": 0.00019276650785207537,
        "epoch": 0.08442897654034545,
        "step": 655
    },
    {
        "loss": 1.6106,
        "grad_norm": 1.5149091482162476,
        "learning_rate": 0.00019274377208944175,
        "epoch": 0.08455787574117041,
        "step": 656
    },
    {
        "loss": 2.482,
        "grad_norm": 1.2992051839828491,
        "learning_rate": 0.0001927210019972598,
        "epoch": 0.08468677494199536,
        "step": 657
    },
    {
        "loss": 2.5966,
        "grad_norm": 1.8026679754257202,
        "learning_rate": 0.00019269819758395803,
        "epoch": 0.08481567414282032,
        "step": 658
    },
    {
        "loss": 2.2707,
        "grad_norm": 1.326528787612915,
        "learning_rate": 0.00019267535885797758,
        "epoch": 0.08494457334364527,
        "step": 659
    },
    {
        "loss": 2.2393,
        "grad_norm": 1.1374431848526,
        "learning_rate": 0.0001926524858277723,
        "epoch": 0.08507347254447023,
        "step": 660
    },
    {
        "loss": 1.7144,
        "grad_norm": 1.9154237508773804,
        "learning_rate": 0.00019262957850180876,
        "epoch": 0.08520237174529519,
        "step": 661
    },
    {
        "loss": 2.4278,
        "grad_norm": 1.770923137664795,
        "learning_rate": 0.00019260663688856622,
        "epoch": 0.08533127094612013,
        "step": 662
    },
    {
        "loss": 2.772,
        "grad_norm": 1.377348780632019,
        "learning_rate": 0.0001925836609965366,
        "epoch": 0.08546017014694508,
        "step": 663
    },
    {
        "loss": 1.6985,
        "grad_norm": 2.481935739517212,
        "learning_rate": 0.00019256065083422463,
        "epoch": 0.08558906934777004,
        "step": 664
    },
    {
        "loss": 1.6454,
        "grad_norm": 1.7388941049575806,
        "learning_rate": 0.0001925376064101475,
        "epoch": 0.085717968548595,
        "step": 665
    },
    {
        "loss": 2.1439,
        "grad_norm": 1.386670708656311,
        "learning_rate": 0.0001925145277328353,
        "epoch": 0.08584686774941995,
        "step": 666
    },
    {
        "loss": 2.6036,
        "grad_norm": 1.1130268573760986,
        "learning_rate": 0.00019249141481083066,
        "epoch": 0.0859757669502449,
        "step": 667
    },
    {
        "loss": 2.8989,
        "grad_norm": 0.9838290214538574,
        "learning_rate": 0.00019246826765268897,
        "epoch": 0.08610466615106986,
        "step": 668
    },
    {
        "loss": 2.2143,
        "grad_norm": 1.50546133518219,
        "learning_rate": 0.00019244508626697825,
        "epoch": 0.08623356535189482,
        "step": 669
    },
    {
        "loss": 2.4468,
        "grad_norm": 1.0159257650375366,
        "learning_rate": 0.00019242187066227922,
        "epoch": 0.08636246455271977,
        "step": 670
    },
    {
        "loss": 2.0622,
        "grad_norm": 1.9571722745895386,
        "learning_rate": 0.00019239862084718525,
        "epoch": 0.08649136375354473,
        "step": 671
    },
    {
        "loss": 2.1497,
        "grad_norm": 1.5367145538330078,
        "learning_rate": 0.0001923753368303023,
        "epoch": 0.08662026295436968,
        "step": 672
    },
    {
        "loss": 2.1944,
        "grad_norm": 1.5006091594696045,
        "learning_rate": 0.00019235201862024916,
        "epoch": 0.08674916215519464,
        "step": 673
    },
    {
        "loss": 1.8953,
        "grad_norm": 1.4166181087493896,
        "learning_rate": 0.0001923286662256571,
        "epoch": 0.0868780613560196,
        "step": 674
    },
    {
        "loss": 2.2452,
        "grad_norm": 1.3075892925262451,
        "learning_rate": 0.00019230527965517012,
        "epoch": 0.08700696055684455,
        "step": 675
    },
    {
        "loss": 1.0062,
        "grad_norm": 1.9942877292633057,
        "learning_rate": 0.00019228185891744494,
        "epoch": 0.0871358597576695,
        "step": 676
    },
    {
        "loss": 2.2276,
        "grad_norm": 1.1934640407562256,
        "learning_rate": 0.00019225840402115082,
        "epoch": 0.08726475895849446,
        "step": 677
    },
    {
        "loss": 2.3955,
        "grad_norm": 1.4460957050323486,
        "learning_rate": 0.00019223491497496965,
        "epoch": 0.08739365815931942,
        "step": 678
    },
    {
        "loss": 2.2768,
        "grad_norm": 1.7516396045684814,
        "learning_rate": 0.00019221139178759612,
        "epoch": 0.08752255736014437,
        "step": 679
    },
    {
        "loss": 2.3869,
        "grad_norm": 1.4081720113754272,
        "learning_rate": 0.00019218783446773734,
        "epoch": 0.08765145656096932,
        "step": 680
    },
    {
        "loss": 1.8037,
        "grad_norm": 2.078176259994507,
        "learning_rate": 0.0001921642430241132,
        "epoch": 0.08778035576179427,
        "step": 681
    },
    {
        "loss": 2.1625,
        "grad_norm": 1.8576260805130005,
        "learning_rate": 0.00019214061746545622,
        "epoch": 0.08790925496261923,
        "step": 682
    },
    {
        "loss": 1.9068,
        "grad_norm": 1.6677416563034058,
        "learning_rate": 0.0001921169578005115,
        "epoch": 0.08803815416344418,
        "step": 683
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.1374504566192627,
        "learning_rate": 0.00019209326403803675,
        "epoch": 0.08816705336426914,
        "step": 684
    },
    {
        "loss": 1.5361,
        "grad_norm": 1.5877015590667725,
        "learning_rate": 0.00019206953618680235,
        "epoch": 0.0882959525650941,
        "step": 685
    },
    {
        "loss": 2.0452,
        "grad_norm": 1.7348482608795166,
        "learning_rate": 0.00019204577425559126,
        "epoch": 0.08842485176591905,
        "step": 686
    },
    {
        "loss": 1.5918,
        "grad_norm": 1.2434412240982056,
        "learning_rate": 0.0001920219782531991,
        "epoch": 0.088553750966744,
        "step": 687
    },
    {
        "loss": 2.3857,
        "grad_norm": 1.0427987575531006,
        "learning_rate": 0.00019199814818843404,
        "epoch": 0.08868265016756896,
        "step": 688
    },
    {
        "loss": 2.1597,
        "grad_norm": 1.8148335218429565,
        "learning_rate": 0.00019197428407011696,
        "epoch": 0.08881154936839392,
        "step": 689
    },
    {
        "loss": 2.6487,
        "grad_norm": 1.2285354137420654,
        "learning_rate": 0.00019195038590708117,
        "epoch": 0.08894044856921887,
        "step": 690
    },
    {
        "loss": 2.3353,
        "grad_norm": 1.8334355354309082,
        "learning_rate": 0.00019192645370817276,
        "epoch": 0.08906934777004383,
        "step": 691
    },
    {
        "loss": 2.329,
        "grad_norm": 1.5925800800323486,
        "learning_rate": 0.00019190248748225035,
        "epoch": 0.08919824697086878,
        "step": 692
    },
    {
        "loss": 1.9702,
        "grad_norm": 1.3829046487808228,
        "learning_rate": 0.00019187848723818514,
        "epoch": 0.08932714617169374,
        "step": 693
    },
    {
        "loss": 2.3437,
        "grad_norm": 1.6537681818008423,
        "learning_rate": 0.0001918544529848609,
        "epoch": 0.0894560453725187,
        "step": 694
    },
    {
        "loss": 2.2493,
        "grad_norm": 1.2543350458145142,
        "learning_rate": 0.00019183038473117404,
        "epoch": 0.08958494457334365,
        "step": 695
    },
    {
        "loss": 1.9643,
        "grad_norm": 1.779503345489502,
        "learning_rate": 0.0001918062824860336,
        "epoch": 0.0897138437741686,
        "step": 696
    },
    {
        "loss": 1.8592,
        "grad_norm": 1.6272083520889282,
        "learning_rate": 0.00019178214625836107,
        "epoch": 0.08984274297499356,
        "step": 697
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.7216657400131226,
        "learning_rate": 0.0001917579760570906,
        "epoch": 0.0899716421758185,
        "step": 698
    },
    {
        "loss": 2.0788,
        "grad_norm": 1.432969570159912,
        "learning_rate": 0.00019173377189116893,
        "epoch": 0.09010054137664346,
        "step": 699
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.1403026580810547,
        "learning_rate": 0.0001917095337695553,
        "epoch": 0.09022944057746841,
        "step": 700
    },
    {
        "loss": 1.7076,
        "grad_norm": 1.506911277770996,
        "learning_rate": 0.00019168526170122165,
        "epoch": 0.09035833977829337,
        "step": 701
    },
    {
        "loss": 1.2691,
        "grad_norm": 2.2604777812957764,
        "learning_rate": 0.00019166095569515235,
        "epoch": 0.09048723897911833,
        "step": 702
    },
    {
        "loss": 2.3456,
        "grad_norm": 1.5675201416015625,
        "learning_rate": 0.00019163661576034434,
        "epoch": 0.09061613817994328,
        "step": 703
    },
    {
        "loss": 2.2297,
        "grad_norm": 1.5071711540222168,
        "learning_rate": 0.00019161224190580726,
        "epoch": 0.09074503738076824,
        "step": 704
    },
    {
        "loss": 1.8333,
        "grad_norm": 1.2599908113479614,
        "learning_rate": 0.0001915878341405631,
        "epoch": 0.09087393658159319,
        "step": 705
    },
    {
        "loss": 1.6683,
        "grad_norm": 1.7378376722335815,
        "learning_rate": 0.0001915633924736466,
        "epoch": 0.09100283578241815,
        "step": 706
    },
    {
        "loss": 2.3679,
        "grad_norm": 1.454472541809082,
        "learning_rate": 0.00019153891691410494,
        "epoch": 0.0911317349832431,
        "step": 707
    },
    {
        "loss": 1.8954,
        "grad_norm": 2.153749465942383,
        "learning_rate": 0.0001915144074709978,
        "epoch": 0.09126063418406806,
        "step": 708
    },
    {
        "loss": 2.5157,
        "grad_norm": 1.3950546979904175,
        "learning_rate": 0.00019148986415339756,
        "epoch": 0.09138953338489302,
        "step": 709
    },
    {
        "loss": 1.4029,
        "grad_norm": 1.6277018785476685,
        "learning_rate": 0.00019146528697038898,
        "epoch": 0.09151843258571797,
        "step": 710
    },
    {
        "loss": 2.4481,
        "grad_norm": 1.5135934352874756,
        "learning_rate": 0.00019144067593106944,
        "epoch": 0.09164733178654293,
        "step": 711
    },
    {
        "loss": 2.1918,
        "grad_norm": 1.2569091320037842,
        "learning_rate": 0.00019141603104454887,
        "epoch": 0.09177623098736788,
        "step": 712
    },
    {
        "loss": 1.8794,
        "grad_norm": 1.7775980234146118,
        "learning_rate": 0.00019139135231994962,
        "epoch": 0.09190513018819284,
        "step": 713
    },
    {
        "loss": 2.3187,
        "grad_norm": 1.7149326801300049,
        "learning_rate": 0.00019136663976640673,
        "epoch": 0.0920340293890178,
        "step": 714
    },
    {
        "loss": 2.281,
        "grad_norm": 1.636501669883728,
        "learning_rate": 0.00019134189339306757,
        "epoch": 0.09216292858984275,
        "step": 715
    },
    {
        "loss": 1.3109,
        "grad_norm": 2.0715675354003906,
        "learning_rate": 0.00019131711320909215,
        "epoch": 0.09229182779066769,
        "step": 716
    },
    {
        "loss": 2.5737,
        "grad_norm": 1.3306137323379517,
        "learning_rate": 0.00019129229922365304,
        "epoch": 0.09242072699149265,
        "step": 717
    },
    {
        "loss": 2.5426,
        "grad_norm": 1.0885841846466064,
        "learning_rate": 0.0001912674514459352,
        "epoch": 0.0925496261923176,
        "step": 718
    },
    {
        "loss": 1.86,
        "grad_norm": 2.254734516143799,
        "learning_rate": 0.00019124256988513615,
        "epoch": 0.09267852539314256,
        "step": 719
    },
    {
        "loss": 2.1103,
        "grad_norm": 1.7107254266738892,
        "learning_rate": 0.00019121765455046598,
        "epoch": 0.09280742459396751,
        "step": 720
    },
    {
        "loss": 1.7664,
        "grad_norm": 2.3936715126037598,
        "learning_rate": 0.00019119270545114712,
        "epoch": 0.09293632379479247,
        "step": 721
    },
    {
        "loss": 2.7088,
        "grad_norm": 1.339862585067749,
        "learning_rate": 0.00019116772259641468,
        "epoch": 0.09306522299561742,
        "step": 722
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.388115406036377,
        "learning_rate": 0.0001911427059955161,
        "epoch": 0.09319412219644238,
        "step": 723
    },
    {
        "loss": 2.3755,
        "grad_norm": 1.399182915687561,
        "learning_rate": 0.00019111765565771148,
        "epoch": 0.09332302139726734,
        "step": 724
    },
    {
        "loss": 2.131,
        "grad_norm": 1.4018256664276123,
        "learning_rate": 0.00019109257159227332,
        "epoch": 0.09345192059809229,
        "step": 725
    },
    {
        "loss": 2.6997,
        "grad_norm": 1.4438507556915283,
        "learning_rate": 0.00019106745380848654,
        "epoch": 0.09358081979891725,
        "step": 726
    },
    {
        "loss": 1.9041,
        "grad_norm": 1.8656796216964722,
        "learning_rate": 0.00019104230231564863,
        "epoch": 0.0937097189997422,
        "step": 727
    },
    {
        "loss": 1.4545,
        "grad_norm": 1.9285598993301392,
        "learning_rate": 0.00019101711712306955,
        "epoch": 0.09383861820056716,
        "step": 728
    },
    {
        "loss": 2.4361,
        "grad_norm": 1.4520832300186157,
        "learning_rate": 0.00019099189824007176,
        "epoch": 0.09396751740139211,
        "step": 729
    },
    {
        "loss": 2.8145,
        "grad_norm": 1.350701928138733,
        "learning_rate": 0.00019096664567599006,
        "epoch": 0.09409641660221707,
        "step": 730
    },
    {
        "loss": 2.5171,
        "grad_norm": 1.0785046815872192,
        "learning_rate": 0.00019094135944017185,
        "epoch": 0.09422531580304203,
        "step": 731
    },
    {
        "loss": 2.1801,
        "grad_norm": 1.6112533807754517,
        "learning_rate": 0.00019091603954197694,
        "epoch": 0.09435421500386698,
        "step": 732
    },
    {
        "loss": 2.2392,
        "grad_norm": 1.1268562078475952,
        "learning_rate": 0.00019089068599077767,
        "epoch": 0.09448311420469194,
        "step": 733
    },
    {
        "loss": 2.0352,
        "grad_norm": 1.5473294258117676,
        "learning_rate": 0.0001908652987959587,
        "epoch": 0.09461201340551689,
        "step": 734
    },
    {
        "loss": 2.1535,
        "grad_norm": 1.3479599952697754,
        "learning_rate": 0.00019083987796691726,
        "epoch": 0.09474091260634183,
        "step": 735
    },
    {
        "loss": 1.7044,
        "grad_norm": 2.039080858230591,
        "learning_rate": 0.00019081442351306292,
        "epoch": 0.09486981180716679,
        "step": 736
    },
    {
        "loss": 1.6179,
        "grad_norm": 2.1215381622314453,
        "learning_rate": 0.0001907889354438179,
        "epoch": 0.09499871100799175,
        "step": 737
    },
    {
        "loss": 2.4594,
        "grad_norm": 1.1012276411056519,
        "learning_rate": 0.00019076341376861664,
        "epoch": 0.0951276102088167,
        "step": 738
    },
    {
        "loss": 2.477,
        "grad_norm": 1.0204123258590698,
        "learning_rate": 0.00019073785849690612,
        "epoch": 0.09525650940964166,
        "step": 739
    },
    {
        "loss": 2.286,
        "grad_norm": 1.0446233749389648,
        "learning_rate": 0.00019071226963814574,
        "epoch": 0.09538540861046661,
        "step": 740
    },
    {
        "loss": 2.0247,
        "grad_norm": 1.4155409336090088,
        "learning_rate": 0.0001906866472018074,
        "epoch": 0.09551430781129157,
        "step": 741
    },
    {
        "loss": 2.3688,
        "grad_norm": 1.2023636102676392,
        "learning_rate": 0.00019066099119737526,
        "epoch": 0.09564320701211652,
        "step": 742
    },
    {
        "loss": 2.0106,
        "grad_norm": 1.8247650861740112,
        "learning_rate": 0.0001906353016343461,
        "epoch": 0.09577210621294148,
        "step": 743
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.3222953081130981,
        "learning_rate": 0.000190609578522229,
        "epoch": 0.09590100541376644,
        "step": 744
    },
    {
        "loss": 2.5334,
        "grad_norm": 0.9565017223358154,
        "learning_rate": 0.0001905838218705455,
        "epoch": 0.09602990461459139,
        "step": 745
    },
    {
        "loss": 2.4148,
        "grad_norm": 1.7455297708511353,
        "learning_rate": 0.0001905580316888295,
        "epoch": 0.09615880381541635,
        "step": 746
    },
    {
        "loss": 1.7731,
        "grad_norm": 1.52367103099823,
        "learning_rate": 0.00019053220798662744,
        "epoch": 0.0962877030162413,
        "step": 747
    },
    {
        "loss": 2.4403,
        "grad_norm": 1.4255205392837524,
        "learning_rate": 0.00019050635077349804,
        "epoch": 0.09641660221706626,
        "step": 748
    },
    {
        "loss": 1.9215,
        "grad_norm": 1.6540427207946777,
        "learning_rate": 0.00019048046005901248,
        "epoch": 0.09654550141789121,
        "step": 749
    },
    {
        "loss": 1.8541,
        "grad_norm": 1.3496981859207153,
        "learning_rate": 0.0001904545358527543,
        "epoch": 0.09667440061871617,
        "step": 750
    },
    {
        "loss": 1.5726,
        "grad_norm": 1.433943510055542,
        "learning_rate": 0.00019042857816431953,
        "epoch": 0.09680329981954112,
        "step": 751
    },
    {
        "loss": 2.3233,
        "grad_norm": 1.4432119131088257,
        "learning_rate": 0.0001904025870033165,
        "epoch": 0.09693219902036608,
        "step": 752
    },
    {
        "loss": 2.562,
        "grad_norm": 1.6426454782485962,
        "learning_rate": 0.00019037656237936594,
        "epoch": 0.09706109822119102,
        "step": 753
    },
    {
        "loss": 1.7888,
        "grad_norm": 1.6773271560668945,
        "learning_rate": 0.00019035050430210105,
        "epoch": 0.09718999742201598,
        "step": 754
    },
    {
        "loss": 2.0613,
        "grad_norm": 1.9698340892791748,
        "learning_rate": 0.00019032441278116725,
        "epoch": 0.09731889662284093,
        "step": 755
    },
    {
        "loss": 2.3493,
        "grad_norm": 1.2295281887054443,
        "learning_rate": 0.00019029828782622253,
        "epoch": 0.09744779582366589,
        "step": 756
    },
    {
        "loss": 2.5403,
        "grad_norm": 1.664595127105713,
        "learning_rate": 0.00019027212944693715,
        "epoch": 0.09757669502449084,
        "step": 757
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.7669613361358643,
        "learning_rate": 0.00019024593765299373,
        "epoch": 0.0977055942253158,
        "step": 758
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.4198987483978271,
        "learning_rate": 0.0001902197124540873,
        "epoch": 0.09783449342614076,
        "step": 759
    },
    {
        "loss": 2.0675,
        "grad_norm": 2.041727304458618,
        "learning_rate": 0.00019019345385992527,
        "epoch": 0.09796339262696571,
        "step": 760
    },
    {
        "loss": 2.1304,
        "grad_norm": 1.9612195491790771,
        "learning_rate": 0.00019016716188022733,
        "epoch": 0.09809229182779067,
        "step": 761
    },
    {
        "loss": 1.8342,
        "grad_norm": 2.175168514251709,
        "learning_rate": 0.00019014083652472562,
        "epoch": 0.09822119102861562,
        "step": 762
    },
    {
        "loss": 2.2575,
        "grad_norm": 1.1298147439956665,
        "learning_rate": 0.0001901144778031646,
        "epoch": 0.09835009022944058,
        "step": 763
    },
    {
        "loss": 2.0116,
        "grad_norm": 1.5606986284255981,
        "learning_rate": 0.000190088085725301,
        "epoch": 0.09847898943026553,
        "step": 764
    },
    {
        "loss": 2.2267,
        "grad_norm": 1.1062747240066528,
        "learning_rate": 0.0001900616603009041,
        "epoch": 0.09860788863109049,
        "step": 765
    },
    {
        "loss": 2.4055,
        "grad_norm": 1.4539119005203247,
        "learning_rate": 0.00019003520153975528,
        "epoch": 0.09873678783191545,
        "step": 766
    },
    {
        "loss": 2.1501,
        "grad_norm": 2.359786033630371,
        "learning_rate": 0.00019000870945164847,
        "epoch": 0.0988656870327404,
        "step": 767
    },
    {
        "loss": 2.1459,
        "grad_norm": 1.3402719497680664,
        "learning_rate": 0.00018998218404638978,
        "epoch": 0.09899458623356536,
        "step": 768
    },
    {
        "loss": 2.3911,
        "grad_norm": 1.3618165254592896,
        "learning_rate": 0.00018995562533379774,
        "epoch": 0.09912348543439031,
        "step": 769
    },
    {
        "loss": 2.3256,
        "grad_norm": 1.3307217359542847,
        "learning_rate": 0.00018992903332370318,
        "epoch": 0.09925238463521527,
        "step": 770
    },
    {
        "loss": 2.1798,
        "grad_norm": 1.0024299621582031,
        "learning_rate": 0.00018990240802594925,
        "epoch": 0.09938128383604021,
        "step": 771
    },
    {
        "loss": 1.9849,
        "grad_norm": 2.0846505165100098,
        "learning_rate": 0.00018987574945039147,
        "epoch": 0.09951018303686517,
        "step": 772
    },
    {
        "loss": 2.0101,
        "grad_norm": 1.8344018459320068,
        "learning_rate": 0.00018984905760689754,
        "epoch": 0.09963908223769012,
        "step": 773
    },
    {
        "loss": 2.2322,
        "grad_norm": 1.8796647787094116,
        "learning_rate": 0.0001898223325053477,
        "epoch": 0.09976798143851508,
        "step": 774
    },
    {
        "loss": 1.9445,
        "grad_norm": 1.8754557371139526,
        "learning_rate": 0.00018979557415563426,
        "epoch": 0.09989688063934003,
        "step": 775
    },
    {
        "loss": 2.516,
        "grad_norm": 1.2137062549591064,
        "learning_rate": 0.00018976878256766203,
        "epoch": 0.10002577984016499,
        "step": 776
    },
    {
        "loss": 2.4798,
        "grad_norm": 1.0708094835281372,
        "learning_rate": 0.00018974195775134804,
        "epoch": 0.10015467904098994,
        "step": 777
    },
    {
        "loss": 1.6687,
        "grad_norm": 1.7993673086166382,
        "learning_rate": 0.0001897150997166216,
        "epoch": 0.1002835782418149,
        "step": 778
    },
    {
        "loss": 2.3537,
        "grad_norm": 1.2447874546051025,
        "learning_rate": 0.0001896882084734243,
        "epoch": 0.10041247744263986,
        "step": 779
    },
    {
        "loss": 1.689,
        "grad_norm": 2.167490005493164,
        "learning_rate": 0.0001896612840317101,
        "epoch": 0.10054137664346481,
        "step": 780
    },
    {
        "loss": 2.484,
        "grad_norm": 1.3271116018295288,
        "learning_rate": 0.00018963432640144525,
        "epoch": 0.10067027584428977,
        "step": 781
    },
    {
        "loss": 1.9237,
        "grad_norm": 1.8010611534118652,
        "learning_rate": 0.00018960733559260818,
        "epoch": 0.10079917504511472,
        "step": 782
    },
    {
        "loss": 1.519,
        "grad_norm": 2.7159175872802734,
        "learning_rate": 0.0001895803116151897,
        "epoch": 0.10092807424593968,
        "step": 783
    },
    {
        "loss": 2.1802,
        "grad_norm": 1.3236314058303833,
        "learning_rate": 0.00018955325447919284,
        "epoch": 0.10105697344676463,
        "step": 784
    },
    {
        "loss": 1.5406,
        "grad_norm": 1.5348381996154785,
        "learning_rate": 0.00018952616419463296,
        "epoch": 0.10118587264758959,
        "step": 785
    },
    {
        "loss": 2.0006,
        "grad_norm": 1.2097729444503784,
        "learning_rate": 0.0001894990407715376,
        "epoch": 0.10131477184841454,
        "step": 786
    },
    {
        "loss": 1.8201,
        "grad_norm": 1.3801500797271729,
        "learning_rate": 0.00018947188421994672,
        "epoch": 0.1014436710492395,
        "step": 787
    },
    {
        "loss": 2.2414,
        "grad_norm": 1.077873706817627,
        "learning_rate": 0.00018944469454991239,
        "epoch": 0.10157257025006446,
        "step": 788
    },
    {
        "loss": 2.0468,
        "grad_norm": 1.901863694190979,
        "learning_rate": 0.00018941747177149897,
        "epoch": 0.1017014694508894,
        "step": 789
    },
    {
        "loss": 2.2345,
        "grad_norm": 1.458769679069519,
        "learning_rate": 0.00018939021589478314,
        "epoch": 0.10183036865171435,
        "step": 790
    },
    {
        "loss": 2.4127,
        "grad_norm": 0.918205976486206,
        "learning_rate": 0.00018936292692985378,
        "epoch": 0.10195926785253931,
        "step": 791
    },
    {
        "loss": 1.7546,
        "grad_norm": 1.6121803522109985,
        "learning_rate": 0.000189335604886812,
        "epoch": 0.10208816705336426,
        "step": 792
    },
    {
        "loss": 2.281,
        "grad_norm": 1.2045624256134033,
        "learning_rate": 0.00018930824977577126,
        "epoch": 0.10221706625418922,
        "step": 793
    },
    {
        "loss": 1.7008,
        "grad_norm": 2.08933162689209,
        "learning_rate": 0.00018928086160685715,
        "epoch": 0.10234596545501418,
        "step": 794
    },
    {
        "loss": 2.2248,
        "grad_norm": 1.7349766492843628,
        "learning_rate": 0.0001892534403902075,
        "epoch": 0.10247486465583913,
        "step": 795
    },
    {
        "loss": 2.0521,
        "grad_norm": 1.7999709844589233,
        "learning_rate": 0.0001892259861359724,
        "epoch": 0.10260376385666409,
        "step": 796
    },
    {
        "loss": 2.6723,
        "grad_norm": 1.1013500690460205,
        "learning_rate": 0.00018919849885431423,
        "epoch": 0.10273266305748904,
        "step": 797
    },
    {
        "loss": 2.3063,
        "grad_norm": 1.2951045036315918,
        "learning_rate": 0.00018917097855540747,
        "epoch": 0.102861562258314,
        "step": 798
    },
    {
        "loss": 1.6461,
        "grad_norm": 1.6335984468460083,
        "learning_rate": 0.00018914342524943898,
        "epoch": 0.10299046145913895,
        "step": 799
    },
    {
        "loss": 1.8126,
        "grad_norm": 2.0906970500946045,
        "learning_rate": 0.0001891158389466076,
        "epoch": 0.10311936065996391,
        "step": 800
    },
    {
        "loss": 2.3902,
        "grad_norm": 1.522708535194397,
        "learning_rate": 0.00018908821965712473,
        "epoch": 0.10324825986078887,
        "step": 801
    },
    {
        "loss": 2.0607,
        "grad_norm": 2.0968236923217773,
        "learning_rate": 0.00018906056739121364,
        "epoch": 0.10337715906161382,
        "step": 802
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.3086647987365723,
        "learning_rate": 0.00018903288215910995,
        "epoch": 0.10350605826243878,
        "step": 803
    },
    {
        "loss": 2.0527,
        "grad_norm": 1.5777777433395386,
        "learning_rate": 0.00018900516397106154,
        "epoch": 0.10363495746326373,
        "step": 804
    },
    {
        "loss": 2.8103,
        "grad_norm": 2.5348761081695557,
        "learning_rate": 0.00018897741283732837,
        "epoch": 0.10376385666408869,
        "step": 805
    },
    {
        "loss": 2.3822,
        "grad_norm": 1.6390269994735718,
        "learning_rate": 0.00018894962876818273,
        "epoch": 0.10389275586491364,
        "step": 806
    },
    {
        "loss": 2.0459,
        "grad_norm": 1.5209875106811523,
        "learning_rate": 0.00018892181177390896,
        "epoch": 0.10402165506573859,
        "step": 807
    },
    {
        "loss": 2.2599,
        "grad_norm": 1.0081207752227783,
        "learning_rate": 0.00018889396186480366,
        "epoch": 0.10415055426656354,
        "step": 808
    },
    {
        "loss": 1.9837,
        "grad_norm": 1.9236581325531006,
        "learning_rate": 0.00018886607905117564,
        "epoch": 0.1042794534673885,
        "step": 809
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.252453088760376,
        "learning_rate": 0.0001888381633433458,
        "epoch": 0.10440835266821345,
        "step": 810
    },
    {
        "loss": 2.2781,
        "grad_norm": 0.9821133613586426,
        "learning_rate": 0.00018881021475164738,
        "epoch": 0.10453725186903841,
        "step": 811
    },
    {
        "loss": 2.3641,
        "grad_norm": 1.2690776586532593,
        "learning_rate": 0.00018878223328642555,
        "epoch": 0.10466615106986336,
        "step": 812
    },
    {
        "loss": 1.6964,
        "grad_norm": 2.0522711277008057,
        "learning_rate": 0.00018875421895803786,
        "epoch": 0.10479505027068832,
        "step": 813
    },
    {
        "loss": 2.2525,
        "grad_norm": 1.847594976425171,
        "learning_rate": 0.00018872617177685395,
        "epoch": 0.10492394947151328,
        "step": 814
    },
    {
        "loss": 2.3356,
        "grad_norm": 1.7021299600601196,
        "learning_rate": 0.00018869809175325558,
        "epoch": 0.10505284867233823,
        "step": 815
    },
    {
        "loss": 1.513,
        "grad_norm": 1.9385292530059814,
        "learning_rate": 0.0001886699788976367,
        "epoch": 0.10518174787316319,
        "step": 816
    },
    {
        "loss": 1.269,
        "grad_norm": 2.864654064178467,
        "learning_rate": 0.0001886418332204035,
        "epoch": 0.10531064707398814,
        "step": 817
    },
    {
        "loss": 1.8553,
        "grad_norm": 2.066101551055908,
        "learning_rate": 0.00018861365473197414,
        "epoch": 0.1054395462748131,
        "step": 818
    },
    {
        "loss": 2.2595,
        "grad_norm": 1.3611509799957275,
        "learning_rate": 0.00018858544344277906,
        "epoch": 0.10556844547563805,
        "step": 819
    },
    {
        "loss": 2.3223,
        "grad_norm": 1.7037792205810547,
        "learning_rate": 0.0001885571993632608,
        "epoch": 0.10569734467646301,
        "step": 820
    },
    {
        "loss": 1.7798,
        "grad_norm": 2.126655340194702,
        "learning_rate": 0.00018852892250387405,
        "epoch": 0.10582624387728796,
        "step": 821
    },
    {
        "loss": 2.1212,
        "grad_norm": 1.8832671642303467,
        "learning_rate": 0.0001885006128750856,
        "epoch": 0.10595514307811292,
        "step": 822
    },
    {
        "loss": 1.9444,
        "grad_norm": 1.5023002624511719,
        "learning_rate": 0.00018847227048737444,
        "epoch": 0.10608404227893788,
        "step": 823
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.2088887691497803,
        "learning_rate": 0.0001884438953512316,
        "epoch": 0.10621294147976283,
        "step": 824
    },
    {
        "loss": 2.33,
        "grad_norm": 2.237046957015991,
        "learning_rate": 0.00018841548747716025,
        "epoch": 0.10634184068058777,
        "step": 825
    },
    {
        "loss": 2.2935,
        "grad_norm": 1.7455748319625854,
        "learning_rate": 0.00018838704687567574,
        "epoch": 0.10647073988141273,
        "step": 826
    },
    {
        "loss": 2.1922,
        "grad_norm": 1.7048084735870361,
        "learning_rate": 0.0001883585735573055,
        "epoch": 0.10659963908223768,
        "step": 827
    },
    {
        "loss": 1.8297,
        "grad_norm": 1.670231580734253,
        "learning_rate": 0.00018833006753258907,
        "epoch": 0.10672853828306264,
        "step": 828
    },
    {
        "loss": 1.1277,
        "grad_norm": 1.7482050657272339,
        "learning_rate": 0.00018830152881207804,
        "epoch": 0.1068574374838876,
        "step": 829
    },
    {
        "loss": 2.1804,
        "grad_norm": 1.5567095279693604,
        "learning_rate": 0.00018827295740633617,
        "epoch": 0.10698633668471255,
        "step": 830
    },
    {
        "loss": 2.2175,
        "grad_norm": 1.7984726428985596,
        "learning_rate": 0.00018824435332593932,
        "epoch": 0.10711523588553751,
        "step": 831
    },
    {
        "loss": 2.2364,
        "grad_norm": 1.2736878395080566,
        "learning_rate": 0.00018821571658147543,
        "epoch": 0.10724413508636246,
        "step": 832
    },
    {
        "loss": 2.2191,
        "grad_norm": 1.7558397054672241,
        "learning_rate": 0.0001881870471835445,
        "epoch": 0.10737303428718742,
        "step": 833
    },
    {
        "loss": 2.5809,
        "grad_norm": 1.2960516214370728,
        "learning_rate": 0.0001881583451427587,
        "epoch": 0.10750193348801237,
        "step": 834
    },
    {
        "loss": 2.5252,
        "grad_norm": 0.9645295143127441,
        "learning_rate": 0.00018812961046974218,
        "epoch": 0.10763083268883733,
        "step": 835
    },
    {
        "loss": 1.7496,
        "grad_norm": 1.6489295959472656,
        "learning_rate": 0.0001881008431751312,
        "epoch": 0.10775973188966229,
        "step": 836
    },
    {
        "loss": 2.1513,
        "grad_norm": 0.9425705671310425,
        "learning_rate": 0.00018807204326957413,
        "epoch": 0.10788863109048724,
        "step": 837
    },
    {
        "loss": 2.445,
        "grad_norm": 1.2855808734893799,
        "learning_rate": 0.0001880432107637314,
        "epoch": 0.1080175302913122,
        "step": 838
    },
    {
        "loss": 1.5203,
        "grad_norm": 1.712690830230713,
        "learning_rate": 0.00018801434566827552,
        "epoch": 0.10814642949213715,
        "step": 839
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.4777781963348389,
        "learning_rate": 0.000187985447993891,
        "epoch": 0.10827532869296211,
        "step": 840
    },
    {
        "loss": 1.8446,
        "grad_norm": 1.7176313400268555,
        "learning_rate": 0.0001879565177512745,
        "epoch": 0.10840422789378706,
        "step": 841
    },
    {
        "loss": 2.6367,
        "grad_norm": 1.2503200769424438,
        "learning_rate": 0.0001879275549511346,
        "epoch": 0.10853312709461202,
        "step": 842
    },
    {
        "loss": 1.079,
        "grad_norm": 3.011012554168701,
        "learning_rate": 0.00018789855960419212,
        "epoch": 0.10866202629543696,
        "step": 843
    },
    {
        "loss": 2.2009,
        "grad_norm": 1.866780400276184,
        "learning_rate": 0.00018786953172117977,
        "epoch": 0.10879092549626192,
        "step": 844
    },
    {
        "loss": 2.1982,
        "grad_norm": 1.1598846912384033,
        "learning_rate": 0.00018784047131284237,
        "epoch": 0.10891982469708687,
        "step": 845
    },
    {
        "loss": 2.4293,
        "grad_norm": 1.480351209640503,
        "learning_rate": 0.00018781137838993676,
        "epoch": 0.10904872389791183,
        "step": 846
    },
    {
        "loss": 2.395,
        "grad_norm": 1.1826889514923096,
        "learning_rate": 0.00018778225296323182,
        "epoch": 0.10917762309873678,
        "step": 847
    },
    {
        "loss": 2.0669,
        "grad_norm": 1.9608274698257446,
        "learning_rate": 0.00018775309504350852,
        "epoch": 0.10930652229956174,
        "step": 848
    },
    {
        "loss": 2.1324,
        "grad_norm": 1.118019461631775,
        "learning_rate": 0.0001877239046415597,
        "epoch": 0.1094354215003867,
        "step": 849
    },
    {
        "loss": 2.6285,
        "grad_norm": 1.5549970865249634,
        "learning_rate": 0.00018769468176819043,
        "epoch": 0.10956432070121165,
        "step": 850
    },
    {
        "loss": 2.6618,
        "grad_norm": 1.132634162902832,
        "learning_rate": 0.00018766542643421766,
        "epoch": 0.1096932199020366,
        "step": 851
    },
    {
        "loss": 1.8872,
        "grad_norm": 1.5781021118164062,
        "learning_rate": 0.00018763613865047037,
        "epoch": 0.10982211910286156,
        "step": 852
    },
    {
        "loss": 2.4174,
        "grad_norm": 1.4483394622802734,
        "learning_rate": 0.00018760681842778956,
        "epoch": 0.10995101830368652,
        "step": 853
    },
    {
        "loss": 2.7058,
        "grad_norm": 1.2381079196929932,
        "learning_rate": 0.0001875774657770283,
        "epoch": 0.11007991750451147,
        "step": 854
    },
    {
        "loss": 2.1717,
        "grad_norm": 1.550173044204712,
        "learning_rate": 0.00018754808070905158,
        "epoch": 0.11020881670533643,
        "step": 855
    },
    {
        "loss": 2.2308,
        "grad_norm": 1.216902256011963,
        "learning_rate": 0.0001875186632347364,
        "epoch": 0.11033771590616138,
        "step": 856
    },
    {
        "loss": 2.0055,
        "grad_norm": 1.3257904052734375,
        "learning_rate": 0.00018748921336497184,
        "epoch": 0.11046661510698634,
        "step": 857
    },
    {
        "loss": 2.2868,
        "grad_norm": 1.7627495527267456,
        "learning_rate": 0.00018745973111065892,
        "epoch": 0.1105955143078113,
        "step": 858
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.229926347732544,
        "learning_rate": 0.00018743021648271054,
        "epoch": 0.11072441350863625,
        "step": 859
    },
    {
        "loss": 2.2095,
        "grad_norm": 2.0289406776428223,
        "learning_rate": 0.00018740066949205172,
        "epoch": 0.11085331270946121,
        "step": 860
    },
    {
        "loss": 2.212,
        "grad_norm": 1.3207932710647583,
        "learning_rate": 0.00018737109014961946,
        "epoch": 0.11098221191028615,
        "step": 861
    },
    {
        "loss": 0.9426,
        "grad_norm": 1.7137948274612427,
        "learning_rate": 0.00018734147846636266,
        "epoch": 0.1111111111111111,
        "step": 862
    },
    {
        "loss": 2.2351,
        "grad_norm": 1.3021185398101807,
        "learning_rate": 0.00018731183445324225,
        "epoch": 0.11124001031193606,
        "step": 863
    },
    {
        "loss": 2.423,
        "grad_norm": 1.0423280000686646,
        "learning_rate": 0.0001872821581212311,
        "epoch": 0.11136890951276102,
        "step": 864
    },
    {
        "loss": 1.3692,
        "grad_norm": 1.878812551498413,
        "learning_rate": 0.000187252449481314,
        "epoch": 0.11149780871358597,
        "step": 865
    },
    {
        "loss": 2.6074,
        "grad_norm": 1.15459144115448,
        "learning_rate": 0.00018722270854448775,
        "epoch": 0.11162670791441093,
        "step": 866
    },
    {
        "loss": 1.5414,
        "grad_norm": 2.4258880615234375,
        "learning_rate": 0.00018719293532176118,
        "epoch": 0.11175560711523588,
        "step": 867
    },
    {
        "loss": 2.7137,
        "grad_norm": 1.543477177619934,
        "learning_rate": 0.0001871631298241549,
        "epoch": 0.11188450631606084,
        "step": 868
    },
    {
        "loss": 1.7103,
        "grad_norm": 1.677139401435852,
        "learning_rate": 0.0001871332920627016,
        "epoch": 0.1120134055168858,
        "step": 869
    },
    {
        "loss": 2.2005,
        "grad_norm": 1.6202476024627686,
        "learning_rate": 0.00018710342204844588,
        "epoch": 0.11214230471771075,
        "step": 870
    },
    {
        "loss": 2.4839,
        "grad_norm": 1.026806116104126,
        "learning_rate": 0.00018707351979244422,
        "epoch": 0.1122712039185357,
        "step": 871
    },
    {
        "loss": 2.3717,
        "grad_norm": 1.2997890710830688,
        "learning_rate": 0.00018704358530576514,
        "epoch": 0.11240010311936066,
        "step": 872
    },
    {
        "loss": 2.7391,
        "grad_norm": 1.3036808967590332,
        "learning_rate": 0.000187013618599489,
        "epoch": 0.11252900232018562,
        "step": 873
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.3783725500106812,
        "learning_rate": 0.00018698361968470808,
        "epoch": 0.11265790152101057,
        "step": 874
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.6332825422286987,
        "learning_rate": 0.0001869535885725267,
        "epoch": 0.11278680072183553,
        "step": 875
    },
    {
        "loss": 1.7829,
        "grad_norm": 1.5746008157730103,
        "learning_rate": 0.00018692352527406095,
        "epoch": 0.11291569992266048,
        "step": 876
    },
    {
        "loss": 1.9104,
        "grad_norm": 1.7112352848052979,
        "learning_rate": 0.00018689342980043896,
        "epoch": 0.11304459912348544,
        "step": 877
    },
    {
        "loss": 2.1818,
        "grad_norm": 1.080814242362976,
        "learning_rate": 0.0001868633021628007,
        "epoch": 0.1131734983243104,
        "step": 878
    },
    {
        "loss": 1.4692,
        "grad_norm": 2.1115598678588867,
        "learning_rate": 0.00018683314237229802,
        "epoch": 0.11330239752513535,
        "step": 879
    },
    {
        "loss": 1.7851,
        "grad_norm": 2.068859577178955,
        "learning_rate": 0.00018680295044009478,
        "epoch": 0.11343129672596029,
        "step": 880
    },
    {
        "loss": 1.2302,
        "grad_norm": 2.196168899536133,
        "learning_rate": 0.00018677272637736655,
        "epoch": 0.11356019592678525,
        "step": 881
    },
    {
        "loss": 1.846,
        "grad_norm": 1.6494909524917603,
        "learning_rate": 0.00018674247019530104,
        "epoch": 0.1136890951276102,
        "step": 882
    },
    {
        "loss": 2.1821,
        "grad_norm": 1.1548240184783936,
        "learning_rate": 0.00018671218190509766,
        "epoch": 0.11381799432843516,
        "step": 883
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.2821931838989258,
        "learning_rate": 0.00018668186151796773,
        "epoch": 0.11394689352926012,
        "step": 884
    },
    {
        "loss": 2.1119,
        "grad_norm": 1.8386551141738892,
        "learning_rate": 0.00018665150904513455,
        "epoch": 0.11407579273008507,
        "step": 885
    },
    {
        "loss": 2.1699,
        "grad_norm": 2.046168565750122,
        "learning_rate": 0.00018662112449783323,
        "epoch": 0.11420469193091003,
        "step": 886
    },
    {
        "loss": 2.3372,
        "grad_norm": 1.4699076414108276,
        "learning_rate": 0.0001865907078873107,
        "epoch": 0.11433359113173498,
        "step": 887
    },
    {
        "loss": 2.8676,
        "grad_norm": 1.2422726154327393,
        "learning_rate": 0.00018656025922482585,
        "epoch": 0.11446249033255994,
        "step": 888
    },
    {
        "loss": 1.3832,
        "grad_norm": 1.9517689943313599,
        "learning_rate": 0.00018652977852164936,
        "epoch": 0.1145913895333849,
        "step": 889
    },
    {
        "loss": 1.9704,
        "grad_norm": 1.2083027362823486,
        "learning_rate": 0.0001864992657890639,
        "epoch": 0.11472028873420985,
        "step": 890
    },
    {
        "loss": 2.0717,
        "grad_norm": 1.9008868932724,
        "learning_rate": 0.00018646872103836383,
        "epoch": 0.1148491879350348,
        "step": 891
    },
    {
        "loss": 1.8599,
        "grad_norm": 1.3249729871749878,
        "learning_rate": 0.00018643814428085536,
        "epoch": 0.11497808713585976,
        "step": 892
    },
    {
        "loss": 2.4393,
        "grad_norm": 1.6162067651748657,
        "learning_rate": 0.0001864075355278568,
        "epoch": 0.11510698633668472,
        "step": 893
    },
    {
        "loss": 2.1491,
        "grad_norm": 1.790610671043396,
        "learning_rate": 0.000186376894790698,
        "epoch": 0.11523588553750967,
        "step": 894
    },
    {
        "loss": 2.2183,
        "grad_norm": 1.6414499282836914,
        "learning_rate": 0.00018634622208072083,
        "epoch": 0.11536478473833463,
        "step": 895
    },
    {
        "loss": 2.2693,
        "grad_norm": 1.186452865600586,
        "learning_rate": 0.0001863155174092789,
        "epoch": 0.11549368393915958,
        "step": 896
    },
    {
        "loss": 2.6368,
        "grad_norm": 1.4347124099731445,
        "learning_rate": 0.0001862847807877377,
        "epoch": 0.11562258313998454,
        "step": 897
    },
    {
        "loss": 2.2462,
        "grad_norm": 1.5879549980163574,
        "learning_rate": 0.00018625401222747455,
        "epoch": 0.11575148234080948,
        "step": 898
    },
    {
        "loss": 2.1939,
        "grad_norm": 1.7384942770004272,
        "learning_rate": 0.00018622321173987857,
        "epoch": 0.11588038154163444,
        "step": 899
    },
    {
        "loss": 2.3034,
        "grad_norm": 1.4031213521957397,
        "learning_rate": 0.00018619237933635075,
        "epoch": 0.11600928074245939,
        "step": 900
    },
    {
        "loss": 2.5037,
        "grad_norm": 1.6718438863754272,
        "learning_rate": 0.00018616151502830376,
        "epoch": 0.11613817994328435,
        "step": 901
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.9826686382293701,
        "learning_rate": 0.00018613061882716224,
        "epoch": 0.1162670791441093,
        "step": 902
    },
    {
        "loss": 1.7099,
        "grad_norm": 1.7405383586883545,
        "learning_rate": 0.00018609969074436254,
        "epoch": 0.11639597834493426,
        "step": 903
    },
    {
        "loss": 1.6057,
        "grad_norm": 1.727156639099121,
        "learning_rate": 0.00018606873079135284,
        "epoch": 0.11652487754575921,
        "step": 904
    },
    {
        "loss": 1.7534,
        "grad_norm": 2.1199326515197754,
        "learning_rate": 0.00018603773897959313,
        "epoch": 0.11665377674658417,
        "step": 905
    },
    {
        "loss": 2.3296,
        "grad_norm": 1.2426323890686035,
        "learning_rate": 0.00018600671532055518,
        "epoch": 0.11678267594740913,
        "step": 906
    },
    {
        "loss": 2.3127,
        "grad_norm": 1.6602059602737427,
        "learning_rate": 0.00018597565982572252,
        "epoch": 0.11691157514823408,
        "step": 907
    },
    {
        "loss": 2.3563,
        "grad_norm": 1.2180641889572144,
        "learning_rate": 0.00018594457250659048,
        "epoch": 0.11704047434905904,
        "step": 908
    },
    {
        "loss": 2.4278,
        "grad_norm": 1.516946792602539,
        "learning_rate": 0.00018591345337466619,
        "epoch": 0.11716937354988399,
        "step": 909
    },
    {
        "loss": 2.4726,
        "grad_norm": 1.1117990016937256,
        "learning_rate": 0.00018588230244146858,
        "epoch": 0.11729827275070895,
        "step": 910
    },
    {
        "loss": 1.6788,
        "grad_norm": 1.6531795263290405,
        "learning_rate": 0.00018585111971852828,
        "epoch": 0.1174271719515339,
        "step": 911
    },
    {
        "loss": 2.0641,
        "grad_norm": 1.7547725439071655,
        "learning_rate": 0.00018581990521738772,
        "epoch": 0.11755607115235886,
        "step": 912
    },
    {
        "loss": 2.5956,
        "grad_norm": 1.372177004814148,
        "learning_rate": 0.00018578865894960112,
        "epoch": 0.11768497035318382,
        "step": 913
    },
    {
        "loss": 1.8492,
        "grad_norm": 2.030794143676758,
        "learning_rate": 0.00018575738092673443,
        "epoch": 0.11781386955400877,
        "step": 914
    },
    {
        "loss": 2.5433,
        "grad_norm": 1.0887649059295654,
        "learning_rate": 0.00018572607116036532,
        "epoch": 0.11794276875483373,
        "step": 915
    },
    {
        "loss": 1.3599,
        "grad_norm": 1.9472554922103882,
        "learning_rate": 0.00018569472966208333,
        "epoch": 0.11807166795565867,
        "step": 916
    },
    {
        "loss": 2.4583,
        "grad_norm": 1.7484431266784668,
        "learning_rate": 0.00018566335644348958,
        "epoch": 0.11820056715648362,
        "step": 917
    },
    {
        "loss": 1.5184,
        "grad_norm": 1.3486924171447754,
        "learning_rate": 0.00018563195151619706,
        "epoch": 0.11832946635730858,
        "step": 918
    },
    {
        "loss": 2.2871,
        "grad_norm": 1.1052981615066528,
        "learning_rate": 0.0001856005148918304,
        "epoch": 0.11845836555813354,
        "step": 919
    },
    {
        "loss": 2.1533,
        "grad_norm": 1.0056084394454956,
        "learning_rate": 0.00018556904658202612,
        "epoch": 0.11858726475895849,
        "step": 920
    },
    {
        "loss": 2.3045,
        "grad_norm": 1.0247154235839844,
        "learning_rate": 0.00018553754659843228,
        "epoch": 0.11871616395978345,
        "step": 921
    },
    {
        "loss": 2.4787,
        "grad_norm": 1.4614313840866089,
        "learning_rate": 0.00018550601495270877,
        "epoch": 0.1188450631606084,
        "step": 922
    },
    {
        "loss": 1.9218,
        "grad_norm": 0.970182478427887,
        "learning_rate": 0.0001854744516565272,
        "epoch": 0.11897396236143336,
        "step": 923
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.0378589630126953,
        "learning_rate": 0.00018544285672157085,
        "epoch": 0.11910286156225831,
        "step": 924
    },
    {
        "loss": 1.7005,
        "grad_norm": 1.6034340858459473,
        "learning_rate": 0.00018541123015953473,
        "epoch": 0.11923176076308327,
        "step": 925
    },
    {
        "loss": 1.871,
        "grad_norm": 2.182856559753418,
        "learning_rate": 0.0001853795719821256,
        "epoch": 0.11936065996390822,
        "step": 926
    },
    {
        "loss": 2.5601,
        "grad_norm": 1.0501093864440918,
        "learning_rate": 0.00018534788220106184,
        "epoch": 0.11948955916473318,
        "step": 927
    },
    {
        "loss": 2.5255,
        "grad_norm": 1.4446460008621216,
        "learning_rate": 0.00018531616082807358,
        "epoch": 0.11961845836555814,
        "step": 928
    },
    {
        "loss": 1.8981,
        "grad_norm": 1.7652466297149658,
        "learning_rate": 0.00018528440787490263,
        "epoch": 0.11974735756638309,
        "step": 929
    },
    {
        "loss": 2.6415,
        "grad_norm": 1.4788740873336792,
        "learning_rate": 0.00018525262335330255,
        "epoch": 0.11987625676720805,
        "step": 930
    },
    {
        "loss": 2.085,
        "grad_norm": 1.5302984714508057,
        "learning_rate": 0.0001852208072750385,
        "epoch": 0.120005155968033,
        "step": 931
    },
    {
        "loss": 2.4491,
        "grad_norm": 1.2265568971633911,
        "learning_rate": 0.00018518895965188734,
        "epoch": 0.12013405516885796,
        "step": 932
    },
    {
        "loss": 2.1954,
        "grad_norm": 1.851613998413086,
        "learning_rate": 0.00018515708049563764,
        "epoch": 0.12026295436968291,
        "step": 933
    },
    {
        "loss": 2.3643,
        "grad_norm": 1.7073664665222168,
        "learning_rate": 0.0001851251698180896,
        "epoch": 0.12039185357050786,
        "step": 934
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.0791348218917847,
        "learning_rate": 0.00018509322763105512,
        "epoch": 0.12052075277133281,
        "step": 935
    },
    {
        "loss": 1.7311,
        "grad_norm": 1.415213942527771,
        "learning_rate": 0.00018506125394635774,
        "epoch": 0.12064965197215777,
        "step": 936
    },
    {
        "loss": 1.8711,
        "grad_norm": 1.826219916343689,
        "learning_rate": 0.0001850292487758327,
        "epoch": 0.12077855117298272,
        "step": 937
    },
    {
        "loss": 1.8163,
        "grad_norm": 2.3938281536102295,
        "learning_rate": 0.00018499721213132686,
        "epoch": 0.12090745037380768,
        "step": 938
    },
    {
        "loss": 2.4351,
        "grad_norm": 1.6369887590408325,
        "learning_rate": 0.0001849651440246987,
        "epoch": 0.12103634957463263,
        "step": 939
    },
    {
        "loss": 2.3784,
        "grad_norm": 1.5679410696029663,
        "learning_rate": 0.00018493304446781842,
        "epoch": 0.12116524877545759,
        "step": 940
    },
    {
        "loss": 2.5202,
        "grad_norm": 1.4250625371932983,
        "learning_rate": 0.0001849009134725678,
        "epoch": 0.12129414797628255,
        "step": 941
    },
    {
        "loss": 2.3462,
        "grad_norm": 1.1206305027008057,
        "learning_rate": 0.00018486875105084031,
        "epoch": 0.1214230471771075,
        "step": 942
    },
    {
        "loss": 2.2065,
        "grad_norm": 1.2806241512298584,
        "learning_rate": 0.00018483655721454102,
        "epoch": 0.12155194637793246,
        "step": 943
    },
    {
        "loss": 1.4878,
        "grad_norm": 1.7939258813858032,
        "learning_rate": 0.00018480433197558656,
        "epoch": 0.12168084557875741,
        "step": 944
    },
    {
        "loss": 2.4175,
        "grad_norm": 1.1255028247833252,
        "learning_rate": 0.00018477207534590536,
        "epoch": 0.12180974477958237,
        "step": 945
    },
    {
        "loss": 2.3635,
        "grad_norm": 1.445117712020874,
        "learning_rate": 0.0001847397873374373,
        "epoch": 0.12193864398040732,
        "step": 946
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.016289472579956,
        "learning_rate": 0.000184707467962134,
        "epoch": 0.12206754318123228,
        "step": 947
    },
    {
        "loss": 2.2226,
        "grad_norm": 1.4800747632980347,
        "learning_rate": 0.00018467511723195852,
        "epoch": 0.12219644238205724,
        "step": 948
    },
    {
        "loss": 2.5075,
        "grad_norm": 1.2858799695968628,
        "learning_rate": 0.00018464273515888575,
        "epoch": 0.12232534158288219,
        "step": 949
    },
    {
        "loss": 2.429,
        "grad_norm": 0.9309933185577393,
        "learning_rate": 0.000184610321754902,
        "epoch": 0.12245424078370715,
        "step": 950
    },
    {
        "loss": 1.9329,
        "grad_norm": 1.7987295389175415,
        "learning_rate": 0.0001845778770320053,
        "epoch": 0.1225831399845321,
        "step": 951
    },
    {
        "loss": 1.9839,
        "grad_norm": 2.203993558883667,
        "learning_rate": 0.00018454540100220514,
        "epoch": 0.12271203918535704,
        "step": 952
    },
    {
        "loss": 1.7863,
        "grad_norm": 1.7434420585632324,
        "learning_rate": 0.00018451289367752273,
        "epoch": 0.122840938386182,
        "step": 953
    },
    {
        "loss": 2.1179,
        "grad_norm": 1.7181116342544556,
        "learning_rate": 0.00018448035506999085,
        "epoch": 0.12296983758700696,
        "step": 954
    },
    {
        "loss": 2.1083,
        "grad_norm": 1.6698733568191528,
        "learning_rate": 0.00018444778519165374,
        "epoch": 0.12309873678783191,
        "step": 955
    },
    {
        "loss": 2.4886,
        "grad_norm": 1.5270826816558838,
        "learning_rate": 0.00018441518405456732,
        "epoch": 0.12322763598865687,
        "step": 956
    },
    {
        "loss": 1.7931,
        "grad_norm": 1.8478760719299316,
        "learning_rate": 0.0001843825516707991,
        "epoch": 0.12335653518948182,
        "step": 957
    },
    {
        "loss": 2.4215,
        "grad_norm": 2.148209810256958,
        "learning_rate": 0.00018434988805242806,
        "epoch": 0.12348543439030678,
        "step": 958
    },
    {
        "loss": 2.441,
        "grad_norm": 1.5236643552780151,
        "learning_rate": 0.00018431719321154483,
        "epoch": 0.12361433359113173,
        "step": 959
    },
    {
        "loss": 1.4215,
        "grad_norm": 2.067143440246582,
        "learning_rate": 0.0001842844671602515,
        "epoch": 0.12374323279195669,
        "step": 960
    },
    {
        "loss": 2.5607,
        "grad_norm": 1.4820570945739746,
        "learning_rate": 0.00018425170991066185,
        "epoch": 0.12387213199278164,
        "step": 961
    },
    {
        "loss": 2.1823,
        "grad_norm": 2.0997138023376465,
        "learning_rate": 0.0001842189214749011,
        "epoch": 0.1240010311936066,
        "step": 962
    },
    {
        "loss": 2.6617,
        "grad_norm": 2.7524943351745605,
        "learning_rate": 0.00018418610186510606,
        "epoch": 0.12412993039443156,
        "step": 963
    },
    {
        "loss": 2.3742,
        "grad_norm": 1.6102665662765503,
        "learning_rate": 0.000184153251093425,
        "epoch": 0.12425882959525651,
        "step": 964
    },
    {
        "loss": 2.6246,
        "grad_norm": 1.2509324550628662,
        "learning_rate": 0.00018412036917201787,
        "epoch": 0.12438772879608147,
        "step": 965
    },
    {
        "loss": 2.1475,
        "grad_norm": 1.2645728588104248,
        "learning_rate": 0.00018408745611305603,
        "epoch": 0.12451662799690642,
        "step": 966
    },
    {
        "loss": 2.3864,
        "grad_norm": 1.9844640493392944,
        "learning_rate": 0.00018405451192872243,
        "epoch": 0.12464552719773138,
        "step": 967
    },
    {
        "loss": 1.7462,
        "grad_norm": 1.7986444234848022,
        "learning_rate": 0.00018402153663121148,
        "epoch": 0.12477442639855633,
        "step": 968
    },
    {
        "loss": 2.0992,
        "grad_norm": 1.9960527420043945,
        "learning_rate": 0.00018398853023272914,
        "epoch": 0.12490332559938129,
        "step": 969
    },
    {
        "loss": 2.3504,
        "grad_norm": 1.0351659059524536,
        "learning_rate": 0.00018395549274549295,
        "epoch": 0.12503222480020623,
        "step": 970
    },
    {
        "loss": 1.858,
        "grad_norm": 1.927776575088501,
        "learning_rate": 0.00018392242418173183,
        "epoch": 0.1251611240010312,
        "step": 971
    },
    {
        "loss": 1.852,
        "grad_norm": 1.5772799253463745,
        "learning_rate": 0.0001838893245536863,
        "epoch": 0.12529002320185614,
        "step": 972
    },
    {
        "loss": 2.0169,
        "grad_norm": 1.1067649126052856,
        "learning_rate": 0.0001838561938736083,
        "epoch": 0.1254189224026811,
        "step": 973
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.382525086402893,
        "learning_rate": 0.00018382303215376138,
        "epoch": 0.12554782160350605,
        "step": 974
    },
    {
        "loss": 2.4702,
        "grad_norm": 1.8678061962127686,
        "learning_rate": 0.00018378983940642041,
        "epoch": 0.125676720804331,
        "step": 975
    },
    {
        "loss": 2.2553,
        "grad_norm": 1.2194383144378662,
        "learning_rate": 0.00018375661564387196,
        "epoch": 0.12580562000515597,
        "step": 976
    },
    {
        "loss": 2.2455,
        "grad_norm": 1.4954779148101807,
        "learning_rate": 0.00018372336087841388,
        "epoch": 0.12593451920598092,
        "step": 977
    },
    {
        "loss": 2.239,
        "grad_norm": 0.9719751477241516,
        "learning_rate": 0.00018369007512235563,
        "epoch": 0.12606341840680588,
        "step": 978
    },
    {
        "loss": 1.4328,
        "grad_norm": 1.9181143045425415,
        "learning_rate": 0.00018365675838801803,
        "epoch": 0.12619231760763083,
        "step": 979
    },
    {
        "loss": 1.9551,
        "grad_norm": 1.4186362028121948,
        "learning_rate": 0.00018362341068773347,
        "epoch": 0.1263212168084558,
        "step": 980
    },
    {
        "loss": 1.0308,
        "grad_norm": 2.402846574783325,
        "learning_rate": 0.00018359003203384576,
        "epoch": 0.12645011600928074,
        "step": 981
    },
    {
        "loss": 1.7939,
        "grad_norm": 1.6202038526535034,
        "learning_rate": 0.0001835566224387102,
        "epoch": 0.1265790152101057,
        "step": 982
    },
    {
        "loss": 2.1383,
        "grad_norm": 1.608603835105896,
        "learning_rate": 0.00018352318191469343,
        "epoch": 0.12670791441093066,
        "step": 983
    },
    {
        "loss": 2.2058,
        "grad_norm": 1.581388235092163,
        "learning_rate": 0.00018348971047417364,
        "epoch": 0.1268368136117556,
        "step": 984
    },
    {
        "loss": 2.1485,
        "grad_norm": 1.38566255569458,
        "learning_rate": 0.0001834562081295405,
        "epoch": 0.12696571281258057,
        "step": 985
    },
    {
        "loss": 2.0093,
        "grad_norm": 1.8440155982971191,
        "learning_rate": 0.00018342267489319505,
        "epoch": 0.12709461201340552,
        "step": 986
    },
    {
        "loss": 2.1596,
        "grad_norm": 1.3342045545578003,
        "learning_rate": 0.0001833891107775497,
        "epoch": 0.12722351121423048,
        "step": 987
    },
    {
        "loss": 2.2928,
        "grad_norm": 1.2676804065704346,
        "learning_rate": 0.00018335551579502844,
        "epoch": 0.12735241041505543,
        "step": 988
    },
    {
        "loss": 1.3847,
        "grad_norm": 1.9510878324508667,
        "learning_rate": 0.0001833218899580666,
        "epoch": 0.1274813096158804,
        "step": 989
    },
    {
        "loss": 1.4585,
        "grad_norm": 1.196858286857605,
        "learning_rate": 0.0001832882332791109,
        "epoch": 0.12761020881670534,
        "step": 990
    },
    {
        "loss": 1.6767,
        "grad_norm": 1.939017415046692,
        "learning_rate": 0.0001832545457706196,
        "epoch": 0.1277391080175303,
        "step": 991
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.382413387298584,
        "learning_rate": 0.00018322082744506223,
        "epoch": 0.12786800721835526,
        "step": 992
    },
    {
        "loss": 2.0552,
        "grad_norm": 1.5236138105392456,
        "learning_rate": 0.00018318707831491978,
        "epoch": 0.1279969064191802,
        "step": 993
    },
    {
        "loss": 2.4953,
        "grad_norm": 1.3867614269256592,
        "learning_rate": 0.00018315329839268472,
        "epoch": 0.12812580562000517,
        "step": 994
    },
    {
        "loss": 1.779,
        "grad_norm": 1.9109954833984375,
        "learning_rate": 0.0001831194876908608,
        "epoch": 0.12825470482083012,
        "step": 995
    },
    {
        "loss": 2.4287,
        "grad_norm": 1.0554704666137695,
        "learning_rate": 0.0001830856462219632,
        "epoch": 0.12838360402165508,
        "step": 996
    },
    {
        "loss": 2.253,
        "grad_norm": 1.5347741842269897,
        "learning_rate": 0.0001830517739985185,
        "epoch": 0.12851250322248,
        "step": 997
    },
    {
        "loss": 1.7063,
        "grad_norm": 1.6033354997634888,
        "learning_rate": 0.0001830178710330647,
        "epoch": 0.12864140242330496,
        "step": 998
    },
    {
        "loss": 2.0676,
        "grad_norm": 1.6298978328704834,
        "learning_rate": 0.00018298393733815114,
        "epoch": 0.12877030162412992,
        "step": 999
    },
    {
        "loss": 2.4283,
        "grad_norm": 1.4445502758026123,
        "learning_rate": 0.00018294997292633852,
        "epoch": 0.12889920082495487,
        "step": 1000
    },
    {
        "loss": 2.2018,
        "grad_norm": 1.6588243246078491,
        "learning_rate": 0.00018291597781019893,
        "epoch": 0.12902810002577983,
        "step": 1001
    },
    {
        "loss": 1.5147,
        "grad_norm": 2.4400975704193115,
        "learning_rate": 0.00018288195200231582,
        "epoch": 0.12915699922660479,
        "step": 1002
    },
    {
        "loss": 1.6997,
        "grad_norm": 1.4411228895187378,
        "learning_rate": 0.00018284789551528406,
        "epoch": 0.12928589842742974,
        "step": 1003
    },
    {
        "loss": 1.6677,
        "grad_norm": 1.901996374130249,
        "learning_rate": 0.0001828138083617097,
        "epoch": 0.1294147976282547,
        "step": 1004
    },
    {
        "loss": 2.0017,
        "grad_norm": 1.697517991065979,
        "learning_rate": 0.00018277969055421038,
        "epoch": 0.12954369682907965,
        "step": 1005
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.9328707456588745,
        "learning_rate": 0.0001827455421054149,
        "epoch": 0.1296725960299046,
        "step": 1006
    },
    {
        "loss": 2.3891,
        "grad_norm": 1.144773006439209,
        "learning_rate": 0.0001827113630279635,
        "epoch": 0.12980149523072956,
        "step": 1007
    },
    {
        "loss": 1.9931,
        "grad_norm": 2.1706061363220215,
        "learning_rate": 0.00018267715333450773,
        "epoch": 0.12993039443155452,
        "step": 1008
    },
    {
        "loss": 2.0605,
        "grad_norm": 1.059907078742981,
        "learning_rate": 0.00018264291303771046,
        "epoch": 0.13005929363237947,
        "step": 1009
    },
    {
        "loss": 2.2972,
        "grad_norm": 1.2887035608291626,
        "learning_rate": 0.00018260864215024588,
        "epoch": 0.13018819283320443,
        "step": 1010
    },
    {
        "loss": 1.9893,
        "grad_norm": 1.376720666885376,
        "learning_rate": 0.0001825743406847996,
        "epoch": 0.13031709203402939,
        "step": 1011
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.5352954864501953,
        "learning_rate": 0.00018254000865406837,
        "epoch": 0.13044599123485434,
        "step": 1012
    },
    {
        "loss": 2.1108,
        "grad_norm": 1.66523015499115,
        "learning_rate": 0.00018250564607076038,
        "epoch": 0.1305748904356793,
        "step": 1013
    },
    {
        "loss": 2.1748,
        "grad_norm": 1.7692549228668213,
        "learning_rate": 0.00018247125294759518,
        "epoch": 0.13070378963650425,
        "step": 1014
    },
    {
        "loss": 2.4035,
        "grad_norm": 1.4077181816101074,
        "learning_rate": 0.00018243682929730345,
        "epoch": 0.1308326888373292,
        "step": 1015
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.4155223369598389,
        "learning_rate": 0.00018240237513262732,
        "epoch": 0.13096158803815416,
        "step": 1016
    },
    {
        "loss": 2.0855,
        "grad_norm": 1.8049781322479248,
        "learning_rate": 0.00018236789046632016,
        "epoch": 0.13109048723897912,
        "step": 1017
    },
    {
        "loss": 2.7762,
        "grad_norm": 1.2699472904205322,
        "learning_rate": 0.0001823333753111466,
        "epoch": 0.13121938643980408,
        "step": 1018
    },
    {
        "loss": 2.3433,
        "grad_norm": 1.7342778444290161,
        "learning_rate": 0.00018229882967988264,
        "epoch": 0.13134828564062903,
        "step": 1019
    },
    {
        "loss": 2.1798,
        "grad_norm": 1.0852981805801392,
        "learning_rate": 0.00018226425358531547,
        "epoch": 0.131477184841454,
        "step": 1020
    },
    {
        "loss": 2.5184,
        "grad_norm": 1.2609540224075317,
        "learning_rate": 0.0001822296470402436,
        "epoch": 0.13160608404227894,
        "step": 1021
    },
    {
        "loss": 1.7708,
        "grad_norm": 1.3394283056259155,
        "learning_rate": 0.00018219501005747684,
        "epoch": 0.1317349832431039,
        "step": 1022
    },
    {
        "loss": 2.6981,
        "grad_norm": 1.8252612352371216,
        "learning_rate": 0.0001821603426498362,
        "epoch": 0.13186388244392885,
        "step": 1023
    },
    {
        "loss": 2.0491,
        "grad_norm": 0.9839637279510498,
        "learning_rate": 0.000182125644830154,
        "epoch": 0.1319927816447538,
        "step": 1024
    },
    {
        "loss": 2.3823,
        "grad_norm": 1.0623317956924438,
        "learning_rate": 0.00018209091661127377,
        "epoch": 0.13212168084557877,
        "step": 1025
    },
    {
        "loss": 2.3416,
        "grad_norm": 1.5081853866577148,
        "learning_rate": 0.00018205615800605038,
        "epoch": 0.13225058004640372,
        "step": 1026
    },
    {
        "loss": 2.3849,
        "grad_norm": 1.212125301361084,
        "learning_rate": 0.00018202136902734984,
        "epoch": 0.13237947924722868,
        "step": 1027
    },
    {
        "loss": 2.5502,
        "grad_norm": 1.5217797756195068,
        "learning_rate": 0.0001819865496880495,
        "epoch": 0.13250837844805363,
        "step": 1028
    },
    {
        "loss": 2.3753,
        "grad_norm": 2.0177175998687744,
        "learning_rate": 0.00018195170000103792,
        "epoch": 0.1326372776488786,
        "step": 1029
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.637001872062683,
        "learning_rate": 0.0001819168199792148,
        "epoch": 0.13276617684970354,
        "step": 1030
    },
    {
        "loss": 2.8382,
        "grad_norm": 1.7662012577056885,
        "learning_rate": 0.0001818819096354912,
        "epoch": 0.1328950760505285,
        "step": 1031
    },
    {
        "loss": 2.3221,
        "grad_norm": 1.4972686767578125,
        "learning_rate": 0.00018184696898278937,
        "epoch": 0.13302397525135345,
        "step": 1032
    },
    {
        "loss": 1.7453,
        "grad_norm": 1.5528335571289062,
        "learning_rate": 0.0001818119980340427,
        "epoch": 0.1331528744521784,
        "step": 1033
    },
    {
        "loss": 2.114,
        "grad_norm": 1.5952001810073853,
        "learning_rate": 0.00018177699680219588,
        "epoch": 0.13328177365300334,
        "step": 1034
    },
    {
        "loss": 2.5136,
        "grad_norm": 1.0444669723510742,
        "learning_rate": 0.0001817419653002048,
        "epoch": 0.1334106728538283,
        "step": 1035
    },
    {
        "loss": 1.4516,
        "grad_norm": 2.0819976329803467,
        "learning_rate": 0.0001817069035410365,
        "epoch": 0.13353957205465325,
        "step": 1036
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.703221321105957,
        "learning_rate": 0.0001816718115376693,
        "epoch": 0.1336684712554782,
        "step": 1037
    },
    {
        "loss": 1.8707,
        "grad_norm": 1.6606175899505615,
        "learning_rate": 0.00018163668930309265,
        "epoch": 0.13379737045630316,
        "step": 1038
    },
    {
        "loss": 2.543,
        "grad_norm": 1.456874966621399,
        "learning_rate": 0.0001816015368503072,
        "epoch": 0.13392626965712812,
        "step": 1039
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.4737733602523804,
        "learning_rate": 0.0001815663541923248,
        "epoch": 0.13405516885795307,
        "step": 1040
    },
    {
        "loss": 2.54,
        "grad_norm": 1.0412479639053345,
        "learning_rate": 0.0001815311413421685,
        "epoch": 0.13418406805877803,
        "step": 1041
    },
    {
        "loss": 1.8033,
        "grad_norm": 1.2623295783996582,
        "learning_rate": 0.00018149589831287252,
        "epoch": 0.13431296725960298,
        "step": 1042
    },
    {
        "loss": 1.4802,
        "grad_norm": 2.3863766193389893,
        "learning_rate": 0.00018146062511748218,
        "epoch": 0.13444186646042794,
        "step": 1043
    },
    {
        "loss": 2.0383,
        "grad_norm": 1.3503056764602661,
        "learning_rate": 0.00018142532176905403,
        "epoch": 0.1345707656612529,
        "step": 1044
    },
    {
        "loss": 2.1238,
        "grad_norm": 1.4926128387451172,
        "learning_rate": 0.0001813899882806558,
        "epoch": 0.13469966486207785,
        "step": 1045
    },
    {
        "loss": 2.6567,
        "grad_norm": 1.0995558500289917,
        "learning_rate": 0.00018135462466536636,
        "epoch": 0.1348285640629028,
        "step": 1046
    },
    {
        "loss": 2.4744,
        "grad_norm": 1.3195446729660034,
        "learning_rate": 0.0001813192309362757,
        "epoch": 0.13495746326372776,
        "step": 1047
    },
    {
        "loss": 1.716,
        "grad_norm": 1.4346981048583984,
        "learning_rate": 0.00018128380710648496,
        "epoch": 0.13508636246455272,
        "step": 1048
    },
    {
        "loss": 2.5303,
        "grad_norm": 1.2935996055603027,
        "learning_rate": 0.00018124835318910644,
        "epoch": 0.13521526166537767,
        "step": 1049
    },
    {
        "loss": 1.6933,
        "grad_norm": 1.9840660095214844,
        "learning_rate": 0.0001812128691972636,
        "epoch": 0.13534416086620263,
        "step": 1050
    },
    {
        "loss": 1.6938,
        "grad_norm": 1.562814474105835,
        "learning_rate": 0.00018117735514409103,
        "epoch": 0.13547306006702758,
        "step": 1051
    },
    {
        "loss": 2.3801,
        "grad_norm": 1.2337212562561035,
        "learning_rate": 0.0001811418110427344,
        "epoch": 0.13560195926785254,
        "step": 1052
    },
    {
        "loss": 1.8666,
        "grad_norm": 1.6106562614440918,
        "learning_rate": 0.0001811062369063505,
        "epoch": 0.1357308584686775,
        "step": 1053
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.3171310424804688,
        "learning_rate": 0.00018107063274810727,
        "epoch": 0.13585975766950245,
        "step": 1054
    },
    {
        "loss": 1.6178,
        "grad_norm": 1.8319683074951172,
        "learning_rate": 0.0001810349985811838,
        "epoch": 0.1359886568703274,
        "step": 1055
    },
    {
        "loss": 2.0499,
        "grad_norm": 1.733730435371399,
        "learning_rate": 0.00018099933441877023,
        "epoch": 0.13611755607115236,
        "step": 1056
    },
    {
        "loss": 2.738,
        "grad_norm": 1.5185606479644775,
        "learning_rate": 0.0001809636402740678,
        "epoch": 0.13624645527197732,
        "step": 1057
    },
    {
        "loss": 1.9907,
        "grad_norm": 2.171630859375,
        "learning_rate": 0.0001809279161602889,
        "epoch": 0.13637535447280227,
        "step": 1058
    },
    {
        "loss": 2.7101,
        "grad_norm": 1.4820674657821655,
        "learning_rate": 0.00018089216209065697,
        "epoch": 0.13650425367362723,
        "step": 1059
    },
    {
        "loss": 1.1397,
        "grad_norm": 2.196770668029785,
        "learning_rate": 0.00018085637807840652,
        "epoch": 0.13663315287445219,
        "step": 1060
    },
    {
        "loss": 1.844,
        "grad_norm": 1.9842275381088257,
        "learning_rate": 0.00018082056413678318,
        "epoch": 0.13676205207527714,
        "step": 1061
    },
    {
        "loss": 2.6754,
        "grad_norm": 1.6862581968307495,
        "learning_rate": 0.00018078472027904368,
        "epoch": 0.1368909512761021,
        "step": 1062
    },
    {
        "loss": 2.5767,
        "grad_norm": 1.66872239112854,
        "learning_rate": 0.00018074884651845578,
        "epoch": 0.13701985047692705,
        "step": 1063
    },
    {
        "loss": 2.3779,
        "grad_norm": 1.6579028367996216,
        "learning_rate": 0.00018071294286829833,
        "epoch": 0.137148749677752,
        "step": 1064
    },
    {
        "loss": 1.9235,
        "grad_norm": 1.6341885328292847,
        "learning_rate": 0.00018067700934186123,
        "epoch": 0.13727764887857696,
        "step": 1065
    },
    {
        "loss": 2.5026,
        "grad_norm": 1.4029653072357178,
        "learning_rate": 0.00018064104595244543,
        "epoch": 0.13740654807940192,
        "step": 1066
    },
    {
        "loss": 1.5322,
        "grad_norm": 1.801939845085144,
        "learning_rate": 0.00018060505271336297,
        "epoch": 0.13753544728022687,
        "step": 1067
    },
    {
        "loss": 2.3612,
        "grad_norm": 1.4296190738677979,
        "learning_rate": 0.0001805690296379369,
        "epoch": 0.13766434648105183,
        "step": 1068
    },
    {
        "loss": 1.8521,
        "grad_norm": 1.6313962936401367,
        "learning_rate": 0.00018053297673950133,
        "epoch": 0.13779324568187679,
        "step": 1069
    },
    {
        "loss": 1.992,
        "grad_norm": 2.02168345451355,
        "learning_rate": 0.00018049689403140147,
        "epoch": 0.1379221448827017,
        "step": 1070
    },
    {
        "loss": 2.0073,
        "grad_norm": 1.7497649192810059,
        "learning_rate": 0.00018046078152699345,
        "epoch": 0.13805104408352667,
        "step": 1071
    },
    {
        "loss": 2.2572,
        "grad_norm": 1.0763531923294067,
        "learning_rate": 0.00018042463923964447,
        "epoch": 0.13817994328435163,
        "step": 1072
    },
    {
        "loss": 2.0708,
        "grad_norm": 2.19830060005188,
        "learning_rate": 0.00018038846718273282,
        "epoch": 0.13830884248517658,
        "step": 1073
    },
    {
        "loss": 2.0938,
        "grad_norm": 1.5622931718826294,
        "learning_rate": 0.00018035226536964775,
        "epoch": 0.13843774168600154,
        "step": 1074
    },
    {
        "loss": 2.483,
        "grad_norm": 1.4883992671966553,
        "learning_rate": 0.0001803160338137895,
        "epoch": 0.1385666408868265,
        "step": 1075
    },
    {
        "loss": 2.2706,
        "grad_norm": 1.1675550937652588,
        "learning_rate": 0.00018027977252856937,
        "epoch": 0.13869554008765145,
        "step": 1076
    },
    {
        "loss": 1.7057,
        "grad_norm": 1.8211650848388672,
        "learning_rate": 0.00018024348152740965,
        "epoch": 0.1388244392884764,
        "step": 1077
    },
    {
        "loss": 2.0475,
        "grad_norm": 1.3337209224700928,
        "learning_rate": 0.00018020716082374364,
        "epoch": 0.13895333848930136,
        "step": 1078
    },
    {
        "loss": 1.3151,
        "grad_norm": 1.9694849252700806,
        "learning_rate": 0.0001801708104310156,
        "epoch": 0.13908223769012631,
        "step": 1079
    },
    {
        "loss": 2.1934,
        "grad_norm": 1.4019523859024048,
        "learning_rate": 0.0001801344303626808,
        "epoch": 0.13921113689095127,
        "step": 1080
    },
    {
        "loss": 2.3444,
        "grad_norm": 1.5271772146224976,
        "learning_rate": 0.0001800980206322055,
        "epoch": 0.13934003609177623,
        "step": 1081
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.438373327255249,
        "learning_rate": 0.00018006158125306697,
        "epoch": 0.13946893529260118,
        "step": 1082
    },
    {
        "loss": 1.5913,
        "grad_norm": 2.0609841346740723,
        "learning_rate": 0.0001800251122387534,
        "epoch": 0.13959783449342614,
        "step": 1083
    },
    {
        "loss": 2.4334,
        "grad_norm": 2.0371592044830322,
        "learning_rate": 0.00017998861360276388,
        "epoch": 0.1397267336942511,
        "step": 1084
    },
    {
        "loss": 2.2516,
        "grad_norm": 1.1854404211044312,
        "learning_rate": 0.0001799520853586087,
        "epoch": 0.13985563289507605,
        "step": 1085
    },
    {
        "loss": 2.1161,
        "grad_norm": 1.9134585857391357,
        "learning_rate": 0.00017991552751980887,
        "epoch": 0.139984532095901,
        "step": 1086
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.2936716079711914,
        "learning_rate": 0.00017987894009989646,
        "epoch": 0.14011343129672596,
        "step": 1087
    },
    {
        "loss": 2.2842,
        "grad_norm": 1.809607744216919,
        "learning_rate": 0.0001798423231124145,
        "epoch": 0.14024233049755092,
        "step": 1088
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.0071064233779907,
        "learning_rate": 0.00017980567657091694,
        "epoch": 0.14037122969837587,
        "step": 1089
    },
    {
        "loss": 1.8456,
        "grad_norm": 2.2325053215026855,
        "learning_rate": 0.00017976900048896865,
        "epoch": 0.14050012889920083,
        "step": 1090
    },
    {
        "loss": 1.7185,
        "grad_norm": 1.8749011754989624,
        "learning_rate": 0.0001797322948801455,
        "epoch": 0.14062902810002578,
        "step": 1091
    },
    {
        "loss": 1.8329,
        "grad_norm": 1.0556365251541138,
        "learning_rate": 0.00017969555975803417,
        "epoch": 0.14075792730085074,
        "step": 1092
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.0900938510894775,
        "learning_rate": 0.0001796587951362324,
        "epoch": 0.1408868265016757,
        "step": 1093
    },
    {
        "loss": 2.1303,
        "grad_norm": 1.38897705078125,
        "learning_rate": 0.0001796220010283488,
        "epoch": 0.14101572570250065,
        "step": 1094
    },
    {
        "loss": 2.1034,
        "grad_norm": 1.2523545026779175,
        "learning_rate": 0.00017958517744800283,
        "epoch": 0.1411446249033256,
        "step": 1095
    },
    {
        "loss": 1.5019,
        "grad_norm": 1.9799716472625732,
        "learning_rate": 0.00017954832440882497,
        "epoch": 0.14127352410415056,
        "step": 1096
    },
    {
        "loss": 2.2498,
        "grad_norm": 1.0010833740234375,
        "learning_rate": 0.00017951144192445648,
        "epoch": 0.14140242330497552,
        "step": 1097
    },
    {
        "loss": 1.6569,
        "grad_norm": 2.258453607559204,
        "learning_rate": 0.00017947453000854963,
        "epoch": 0.14153132250580047,
        "step": 1098
    },
    {
        "loss": 1.6116,
        "grad_norm": 2.223398208618164,
        "learning_rate": 0.00017943758867476755,
        "epoch": 0.14166022170662543,
        "step": 1099
    },
    {
        "loss": 2.1933,
        "grad_norm": 1.2804033756256104,
        "learning_rate": 0.00017940061793678423,
        "epoch": 0.14178912090745038,
        "step": 1100
    },
    {
        "loss": 1.9562,
        "grad_norm": 1.8224172592163086,
        "learning_rate": 0.0001793636178082846,
        "epoch": 0.14191802010827534,
        "step": 1101
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.9973663091659546,
        "learning_rate": 0.0001793265883029644,
        "epoch": 0.1420469193091003,
        "step": 1102
    },
    {
        "loss": 2.5659,
        "grad_norm": 1.8041744232177734,
        "learning_rate": 0.0001792895294345303,
        "epoch": 0.14217581850992525,
        "step": 1103
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.3745096921920776,
        "learning_rate": 0.00017925244121669978,
        "epoch": 0.1423047177107502,
        "step": 1104
    },
    {
        "loss": 2.5663,
        "grad_norm": 1.4952130317687988,
        "learning_rate": 0.0001792153236632012,
        "epoch": 0.14243361691157516,
        "step": 1105
    },
    {
        "loss": 2.1313,
        "grad_norm": 1.6786904335021973,
        "learning_rate": 0.0001791781767877739,
        "epoch": 0.1425625161124001,
        "step": 1106
    },
    {
        "loss": 2.1646,
        "grad_norm": 1.9189255237579346,
        "learning_rate": 0.00017914100060416788,
        "epoch": 0.14269141531322505,
        "step": 1107
    },
    {
        "loss": 2.326,
        "grad_norm": 1.410061001777649,
        "learning_rate": 0.0001791037951261441,
        "epoch": 0.14282031451405,
        "step": 1108
    },
    {
        "loss": 1.9364,
        "grad_norm": 1.9581732749938965,
        "learning_rate": 0.0001790665603674744,
        "epoch": 0.14294921371487496,
        "step": 1109
    },
    {
        "loss": 2.18,
        "grad_norm": 1.0128036737442017,
        "learning_rate": 0.0001790292963419413,
        "epoch": 0.1430781129156999,
        "step": 1110
    },
    {
        "loss": 2.1676,
        "grad_norm": 1.626032829284668,
        "learning_rate": 0.0001789920030633384,
        "epoch": 0.14320701211652487,
        "step": 1111
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.551090955734253,
        "learning_rate": 0.00017895468054546985,
        "epoch": 0.14333591131734982,
        "step": 1112
    },
    {
        "loss": 2.6092,
        "grad_norm": 1.0677306652069092,
        "learning_rate": 0.0001789173288021508,
        "epoch": 0.14346481051817478,
        "step": 1113
    },
    {
        "loss": 2.1133,
        "grad_norm": 1.067539095878601,
        "learning_rate": 0.00017887994784720722,
        "epoch": 0.14359370971899973,
        "step": 1114
    },
    {
        "loss": 2.3056,
        "grad_norm": 1.6963988542556763,
        "learning_rate": 0.0001788425376944758,
        "epoch": 0.1437226089198247,
        "step": 1115
    },
    {
        "loss": 2.4755,
        "grad_norm": 1.4239519834518433,
        "learning_rate": 0.00017880509835780409,
        "epoch": 0.14385150812064965,
        "step": 1116
    },
    {
        "loss": 2.6758,
        "grad_norm": 1.8076083660125732,
        "learning_rate": 0.00017876762985105042,
        "epoch": 0.1439804073214746,
        "step": 1117
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.142707347869873,
        "learning_rate": 0.000178730132188084,
        "epoch": 0.14410930652229956,
        "step": 1118
    },
    {
        "loss": 1.9155,
        "grad_norm": 1.3277173042297363,
        "learning_rate": 0.00017869260538278474,
        "epoch": 0.1442382057231245,
        "step": 1119
    },
    {
        "loss": 1.3369,
        "grad_norm": 2.4610280990600586,
        "learning_rate": 0.00017865504944904333,
        "epoch": 0.14436710492394947,
        "step": 1120
    },
    {
        "loss": 1.7851,
        "grad_norm": 1.5617880821228027,
        "learning_rate": 0.00017861746440076134,
        "epoch": 0.14449600412477442,
        "step": 1121
    },
    {
        "loss": 2.3927,
        "grad_norm": 1.3775227069854736,
        "learning_rate": 0.00017857985025185097,
        "epoch": 0.14462490332559938,
        "step": 1122
    },
    {
        "loss": 2.4785,
        "grad_norm": 1.1734858751296997,
        "learning_rate": 0.00017854220701623533,
        "epoch": 0.14475380252642434,
        "step": 1123
    },
    {
        "loss": 2.2427,
        "grad_norm": 1.602971076965332,
        "learning_rate": 0.00017850453470784827,
        "epoch": 0.1448827017272493,
        "step": 1124
    },
    {
        "loss": 2.2352,
        "grad_norm": 1.5219167470932007,
        "learning_rate": 0.00017846683334063433,
        "epoch": 0.14501160092807425,
        "step": 1125
    },
    {
        "loss": 2.0141,
        "grad_norm": 1.8475631475448608,
        "learning_rate": 0.00017842910292854887,
        "epoch": 0.1451405001288992,
        "step": 1126
    },
    {
        "loss": 2.2952,
        "grad_norm": 1.1808545589447021,
        "learning_rate": 0.00017839134348555794,
        "epoch": 0.14526939932972416,
        "step": 1127
    },
    {
        "loss": 1.6613,
        "grad_norm": 1.4831043481826782,
        "learning_rate": 0.00017835355502563845,
        "epoch": 0.1453982985305491,
        "step": 1128
    },
    {
        "loss": 1.5554,
        "grad_norm": 1.3408976793289185,
        "learning_rate": 0.00017831573756277793,
        "epoch": 0.14552719773137407,
        "step": 1129
    },
    {
        "loss": 2.3218,
        "grad_norm": 1.2905789613723755,
        "learning_rate": 0.00017827789111097468,
        "epoch": 0.14565609693219903,
        "step": 1130
    },
    {
        "loss": 2.1821,
        "grad_norm": 1.790634036064148,
        "learning_rate": 0.0001782400156842378,
        "epoch": 0.14578499613302398,
        "step": 1131
    },
    {
        "loss": 2.2071,
        "grad_norm": 1.5274096727371216,
        "learning_rate": 0.000178202111296587,
        "epoch": 0.14591389533384894,
        "step": 1132
    },
    {
        "loss": 2.175,
        "grad_norm": 0.9576509594917297,
        "learning_rate": 0.00017816417796205278,
        "epoch": 0.1460427945346739,
        "step": 1133
    },
    {
        "loss": 1.4381,
        "grad_norm": 2.5080671310424805,
        "learning_rate": 0.0001781262156946764,
        "epoch": 0.14617169373549885,
        "step": 1134
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.7831703424453735,
        "learning_rate": 0.00017808822450850973,
        "epoch": 0.1463005929363238,
        "step": 1135
    },
    {
        "loss": 2.0303,
        "grad_norm": 2.094980001449585,
        "learning_rate": 0.00017805020441761537,
        "epoch": 0.14642949213714876,
        "step": 1136
    },
    {
        "loss": 2.1208,
        "grad_norm": 1.2019339799880981,
        "learning_rate": 0.00017801215543606668,
        "epoch": 0.14655839133797371,
        "step": 1137
    },
    {
        "loss": 2.0666,
        "grad_norm": 1.805692434310913,
        "learning_rate": 0.0001779740775779476,
        "epoch": 0.14668729053879867,
        "step": 1138
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.4151101112365723,
        "learning_rate": 0.00017793597085735292,
        "epoch": 0.14681618973962363,
        "step": 1139
    },
    {
        "loss": 2.0079,
        "grad_norm": 1.523793339729309,
        "learning_rate": 0.00017789783528838798,
        "epoch": 0.14694508894044858,
        "step": 1140
    },
    {
        "loss": 1.8957,
        "grad_norm": 1.670946478843689,
        "learning_rate": 0.00017785967088516886,
        "epoch": 0.14707398814127354,
        "step": 1141
    },
    {
        "loss": 2.3536,
        "grad_norm": 1.2690045833587646,
        "learning_rate": 0.00017782147766182228,
        "epoch": 0.14720288734209847,
        "step": 1142
    },
    {
        "loss": 2.087,
        "grad_norm": 1.0868338346481323,
        "learning_rate": 0.0001777832556324856,
        "epoch": 0.14733178654292342,
        "step": 1143
    },
    {
        "loss": 2.116,
        "grad_norm": 1.5304863452911377,
        "learning_rate": 0.000177745004811307,
        "epoch": 0.14746068574374838,
        "step": 1144
    },
    {
        "loss": 1.0891,
        "grad_norm": 1.9962568283081055,
        "learning_rate": 0.00017770672521244508,
        "epoch": 0.14758958494457333,
        "step": 1145
    },
    {
        "loss": 1.8644,
        "grad_norm": 2.25104022026062,
        "learning_rate": 0.00017766841685006932,
        "epoch": 0.1477184841453983,
        "step": 1146
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.7556918859481812,
        "learning_rate": 0.00017763007973835965,
        "epoch": 0.14784738334622324,
        "step": 1147
    },
    {
        "loss": 2.1221,
        "grad_norm": 2.1386210918426514,
        "learning_rate": 0.00017759171389150678,
        "epoch": 0.1479762825470482,
        "step": 1148
    },
    {
        "loss": 2.5011,
        "grad_norm": 0.9479283094406128,
        "learning_rate": 0.00017755331932371203,
        "epoch": 0.14810518174787315,
        "step": 1149
    },
    {
        "loss": 1.783,
        "grad_norm": 1.4417389631271362,
        "learning_rate": 0.00017751489604918733,
        "epoch": 0.1482340809486981,
        "step": 1150
    },
    {
        "loss": 2.5022,
        "grad_norm": 1.5796300172805786,
        "learning_rate": 0.00017747644408215517,
        "epoch": 0.14836298014952307,
        "step": 1151
    },
    {
        "loss": 1.7273,
        "grad_norm": 1.781626582145691,
        "learning_rate": 0.0001774379634368488,
        "epoch": 0.14849187935034802,
        "step": 1152
    },
    {
        "loss": 1.8364,
        "grad_norm": 1.52727210521698,
        "learning_rate": 0.00017739945412751198,
        "epoch": 0.14862077855117298,
        "step": 1153
    },
    {
        "loss": 1.8384,
        "grad_norm": 1.343114972114563,
        "learning_rate": 0.00017736091616839914,
        "epoch": 0.14874967775199793,
        "step": 1154
    },
    {
        "loss": 2.288,
        "grad_norm": 1.4434385299682617,
        "learning_rate": 0.00017732234957377525,
        "epoch": 0.1488785769528229,
        "step": 1155
    },
    {
        "loss": 1.8748,
        "grad_norm": 2.1345956325531006,
        "learning_rate": 0.00017728375435791595,
        "epoch": 0.14900747615364784,
        "step": 1156
    },
    {
        "loss": 2.3993,
        "grad_norm": 1.0867512226104736,
        "learning_rate": 0.00017724513053510746,
        "epoch": 0.1491363753544728,
        "step": 1157
    },
    {
        "loss": 2.4294,
        "grad_norm": 1.4928075075149536,
        "learning_rate": 0.0001772064781196465,
        "epoch": 0.14926527455529776,
        "step": 1158
    },
    {
        "loss": 1.9169,
        "grad_norm": 1.530980110168457,
        "learning_rate": 0.00017716779712584053,
        "epoch": 0.1493941737561227,
        "step": 1159
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.4115777015686035,
        "learning_rate": 0.00017712908756800744,
        "epoch": 0.14952307295694767,
        "step": 1160
    },
    {
        "loss": 1.5587,
        "grad_norm": 2.376403331756592,
        "learning_rate": 0.00017709034946047578,
        "epoch": 0.14965197215777262,
        "step": 1161
    },
    {
        "loss": 1.724,
        "grad_norm": 1.8974601030349731,
        "learning_rate": 0.00017705158281758465,
        "epoch": 0.14978087135859758,
        "step": 1162
    },
    {
        "loss": 1.7599,
        "grad_norm": 2.1792545318603516,
        "learning_rate": 0.0001770127876536837,
        "epoch": 0.14990977055942253,
        "step": 1163
    },
    {
        "loss": 1.4872,
        "grad_norm": 1.7149221897125244,
        "learning_rate": 0.00017697396398313317,
        "epoch": 0.1500386697602475,
        "step": 1164
    },
    {
        "loss": 1.643,
        "grad_norm": 1.9891464710235596,
        "learning_rate": 0.0001769351118203038,
        "epoch": 0.15016756896107245,
        "step": 1165
    },
    {
        "loss": 2.3348,
        "grad_norm": 1.3737925291061401,
        "learning_rate": 0.0001768962311795769,
        "epoch": 0.1502964681618974,
        "step": 1166
    },
    {
        "loss": 2.329,
        "grad_norm": 1.5180009603500366,
        "learning_rate": 0.0001768573220753443,
        "epoch": 0.15042536736272236,
        "step": 1167
    },
    {
        "loss": 2.567,
        "grad_norm": 1.0819951295852661,
        "learning_rate": 0.00017681838452200847,
        "epoch": 0.1505542665635473,
        "step": 1168
    },
    {
        "loss": 1.3799,
        "grad_norm": 2.035231113433838,
        "learning_rate": 0.00017677941853398225,
        "epoch": 0.15068316576437227,
        "step": 1169
    },
    {
        "loss": 2.3181,
        "grad_norm": 2.1156680583953857,
        "learning_rate": 0.00017674042412568915,
        "epoch": 0.15081206496519722,
        "step": 1170
    },
    {
        "loss": 1.5136,
        "grad_norm": 1.9352219104766846,
        "learning_rate": 0.00017670140131156306,
        "epoch": 0.15094096416602218,
        "step": 1171
    },
    {
        "loss": 2.4098,
        "grad_norm": 1.3466863632202148,
        "learning_rate": 0.00017666235010604852,
        "epoch": 0.15106986336684713,
        "step": 1172
    },
    {
        "loss": 1.1128,
        "grad_norm": 2.532475471496582,
        "learning_rate": 0.00017662327052360053,
        "epoch": 0.1511987625676721,
        "step": 1173
    },
    {
        "loss": 1.9665,
        "grad_norm": 2.299656391143799,
        "learning_rate": 0.00017658416257868454,
        "epoch": 0.15132766176849705,
        "step": 1174
    },
    {
        "loss": 0.7818,
        "grad_norm": 1.741969108581543,
        "learning_rate": 0.00017654502628577655,
        "epoch": 0.151456560969322,
        "step": 1175
    },
    {
        "loss": 2.3554,
        "grad_norm": 1.8135676383972168,
        "learning_rate": 0.00017650586165936306,
        "epoch": 0.15158546017014696,
        "step": 1176
    },
    {
        "loss": 1.2095,
        "grad_norm": 2.5602612495422363,
        "learning_rate": 0.000176466668713941,
        "epoch": 0.1517143593709719,
        "step": 1177
    },
    {
        "loss": 2.3318,
        "grad_norm": 1.0990904569625854,
        "learning_rate": 0.0001764274474640179,
        "epoch": 0.15184325857179687,
        "step": 1178
    },
    {
        "loss": 2.2599,
        "grad_norm": 1.2919206619262695,
        "learning_rate": 0.00017638819792411165,
        "epoch": 0.1519721577726218,
        "step": 1179
    },
    {
        "loss": 2.3878,
        "grad_norm": 1.7067251205444336,
        "learning_rate": 0.00017634892010875066,
        "epoch": 0.15210105697344675,
        "step": 1180
    },
    {
        "loss": 2.266,
        "grad_norm": 1.456554651260376,
        "learning_rate": 0.00017630961403247377,
        "epoch": 0.1522299561742717,
        "step": 1181
    },
    {
        "loss": 2.1383,
        "grad_norm": 1.6772518157958984,
        "learning_rate": 0.00017627027970983038,
        "epoch": 0.15235885537509666,
        "step": 1182
    },
    {
        "loss": 1.7097,
        "grad_norm": 1.5813730955123901,
        "learning_rate": 0.0001762309171553802,
        "epoch": 0.15248775457592162,
        "step": 1183
    },
    {
        "loss": 1.5586,
        "grad_norm": 1.971490740776062,
        "learning_rate": 0.00017619152638369352,
        "epoch": 0.15261665377674657,
        "step": 1184
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.9054142236709595,
        "learning_rate": 0.00017615210740935098,
        "epoch": 0.15274555297757153,
        "step": 1185
    },
    {
        "loss": 1.7538,
        "grad_norm": 1.654052734375,
        "learning_rate": 0.00017611266024694373,
        "epoch": 0.15287445217839649,
        "step": 1186
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.5602248907089233,
        "learning_rate": 0.00017607318491107333,
        "epoch": 0.15300335137922144,
        "step": 1187
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.5648494958877563,
        "learning_rate": 0.00017603368141635176,
        "epoch": 0.1531322505800464,
        "step": 1188
    },
    {
        "loss": 1.1339,
        "grad_norm": 2.180781364440918,
        "learning_rate": 0.0001759941497774014,
        "epoch": 0.15326114978087135,
        "step": 1189
    },
    {
        "loss": 2.337,
        "grad_norm": 1.2218507528305054,
        "learning_rate": 0.0001759545900088551,
        "epoch": 0.1533900489816963,
        "step": 1190
    },
    {
        "loss": 0.9785,
        "grad_norm": 2.1982767581939697,
        "learning_rate": 0.00017591500212535608,
        "epoch": 0.15351894818252126,
        "step": 1191
    },
    {
        "loss": 2.2755,
        "grad_norm": 1.509616732597351,
        "learning_rate": 0.000175875386141558,
        "epoch": 0.15364784738334622,
        "step": 1192
    },
    {
        "loss": 2.4245,
        "grad_norm": 1.7462959289550781,
        "learning_rate": 0.00017583574207212489,
        "epoch": 0.15377674658417118,
        "step": 1193
    },
    {
        "loss": 1.8028,
        "grad_norm": 2.1101460456848145,
        "learning_rate": 0.0001757960699317312,
        "epoch": 0.15390564578499613,
        "step": 1194
    },
    {
        "loss": 2.1485,
        "grad_norm": 1.3593175411224365,
        "learning_rate": 0.00017575636973506177,
        "epoch": 0.1540345449858211,
        "step": 1195
    },
    {
        "loss": 2.4714,
        "grad_norm": 1.040320634841919,
        "learning_rate": 0.0001757166414968118,
        "epoch": 0.15416344418664604,
        "step": 1196
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.4174312353134155,
        "learning_rate": 0.00017567688523168688,
        "epoch": 0.154292343387471,
        "step": 1197
    },
    {
        "loss": 1.6149,
        "grad_norm": 1.8623239994049072,
        "learning_rate": 0.000175637100954403,
        "epoch": 0.15442124258829595,
        "step": 1198
    },
    {
        "loss": 2.0215,
        "grad_norm": 2.209054470062256,
        "learning_rate": 0.0001755972886796865,
        "epoch": 0.1545501417891209,
        "step": 1199
    },
    {
        "loss": 1.658,
        "grad_norm": 1.9277313947677612,
        "learning_rate": 0.00017555744842227407,
        "epoch": 0.15467904098994587,
        "step": 1200
    },
    {
        "loss": 1.9461,
        "grad_norm": 1.8385610580444336,
        "learning_rate": 0.0001755175801969128,
        "epoch": 0.15480794019077082,
        "step": 1201
    },
    {
        "loss": 1.8821,
        "grad_norm": 2.150672197341919,
        "learning_rate": 0.00017547768401836002,
        "epoch": 0.15493683939159578,
        "step": 1202
    },
    {
        "loss": 1.6799,
        "grad_norm": 2.0198323726654053,
        "learning_rate": 0.0001754377599013836,
        "epoch": 0.15506573859242073,
        "step": 1203
    },
    {
        "loss": 2.5544,
        "grad_norm": 1.081157922744751,
        "learning_rate": 0.00017539780786076156,
        "epoch": 0.1551946377932457,
        "step": 1204
    },
    {
        "loss": 2.1366,
        "grad_norm": 1.3613852262496948,
        "learning_rate": 0.0001753578279112824,
        "epoch": 0.15532353699407064,
        "step": 1205
    },
    {
        "loss": 1.4252,
        "grad_norm": 2.183082342147827,
        "learning_rate": 0.00017531782006774483,
        "epoch": 0.1554524361948956,
        "step": 1206
    },
    {
        "loss": 2.1167,
        "grad_norm": 1.4228843450546265,
        "learning_rate": 0.000175277784344958,
        "epoch": 0.15558133539572055,
        "step": 1207
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.0011813640594482,
        "learning_rate": 0.00017523772075774127,
        "epoch": 0.1557102345965455,
        "step": 1208
    },
    {
        "loss": 1.8542,
        "grad_norm": 2.274054765701294,
        "learning_rate": 0.0001751976293209244,
        "epoch": 0.15583913379737047,
        "step": 1209
    },
    {
        "loss": 1.914,
        "grad_norm": 1.2245619297027588,
        "learning_rate": 0.0001751575100493474,
        "epoch": 0.15596803299819542,
        "step": 1210
    },
    {
        "loss": 2.0431,
        "grad_norm": 1.2743395566940308,
        "learning_rate": 0.00017511736295786062,
        "epoch": 0.15609693219902038,
        "step": 1211
    },
    {
        "loss": 2.3695,
        "grad_norm": 1.2586231231689453,
        "learning_rate": 0.00017507718806132475,
        "epoch": 0.15622583139984533,
        "step": 1212
    },
    {
        "loss": 1.7876,
        "grad_norm": 2.539046287536621,
        "learning_rate": 0.00017503698537461065,
        "epoch": 0.1563547306006703,
        "step": 1213
    },
    {
        "loss": 1.8627,
        "grad_norm": 2.2544658184051514,
        "learning_rate": 0.00017499675491259954,
        "epoch": 0.15648362980149524,
        "step": 1214
    },
    {
        "loss": 1.4689,
        "grad_norm": 2.755937337875366,
        "learning_rate": 0.00017495649669018297,
        "epoch": 0.15661252900232017,
        "step": 1215
    },
    {
        "loss": 2.0919,
        "grad_norm": 1.842028260231018,
        "learning_rate": 0.00017491621072226264,
        "epoch": 0.15674142820314513,
        "step": 1216
    },
    {
        "loss": 2.2907,
        "grad_norm": 1.7552831172943115,
        "learning_rate": 0.00017487589702375064,
        "epoch": 0.15687032740397008,
        "step": 1217
    },
    {
        "loss": 1.8491,
        "grad_norm": 1.3839454650878906,
        "learning_rate": 0.0001748355556095693,
        "epoch": 0.15699922660479504,
        "step": 1218
    },
    {
        "loss": 1.5119,
        "grad_norm": 2.2140910625457764,
        "learning_rate": 0.0001747951864946511,
        "epoch": 0.15712812580562,
        "step": 1219
    },
    {
        "loss": 1.4428,
        "grad_norm": 1.8237887620925903,
        "learning_rate": 0.00017475478969393896,
        "epoch": 0.15725702500644495,
        "step": 1220
    },
    {
        "loss": 2.4735,
        "grad_norm": 1.519373893737793,
        "learning_rate": 0.00017471436522238586,
        "epoch": 0.1573859242072699,
        "step": 1221
    },
    {
        "loss": 2.3101,
        "grad_norm": 2.1708950996398926,
        "learning_rate": 0.00017467391309495517,
        "epoch": 0.15751482340809486,
        "step": 1222
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.1671620607376099,
        "learning_rate": 0.0001746334333266204,
        "epoch": 0.15764372260891982,
        "step": 1223
    },
    {
        "loss": 2.5497,
        "grad_norm": 1.755165696144104,
        "learning_rate": 0.0001745929259323654,
        "epoch": 0.15777262180974477,
        "step": 1224
    },
    {
        "loss": 2.3741,
        "grad_norm": 1.6118866205215454,
        "learning_rate": 0.00017455239092718406,
        "epoch": 0.15790152101056973,
        "step": 1225
    },
    {
        "loss": 1.58,
        "grad_norm": 2.1072909832000732,
        "learning_rate": 0.0001745118283260807,
        "epoch": 0.15803042021139468,
        "step": 1226
    },
    {
        "loss": 2.3073,
        "grad_norm": 1.7018848657608032,
        "learning_rate": 0.00017447123814406975,
        "epoch": 0.15815931941221964,
        "step": 1227
    },
    {
        "loss": 1.2638,
        "grad_norm": 2.493636131286621,
        "learning_rate": 0.0001744306203961758,
        "epoch": 0.1582882186130446,
        "step": 1228
    },
    {
        "loss": 1.9806,
        "grad_norm": 1.4435654878616333,
        "learning_rate": 0.0001743899750974338,
        "epoch": 0.15841711781386955,
        "step": 1229
    },
    {
        "loss": 1.498,
        "grad_norm": 2.027834177017212,
        "learning_rate": 0.00017434930226288865,
        "epoch": 0.1585460170146945,
        "step": 1230
    },
    {
        "loss": 1.4171,
        "grad_norm": 2.1255300045013428,
        "learning_rate": 0.0001743086019075957,
        "epoch": 0.15867491621551946,
        "step": 1231
    },
    {
        "loss": 1.5111,
        "grad_norm": 1.848471999168396,
        "learning_rate": 0.0001742678740466204,
        "epoch": 0.15880381541634442,
        "step": 1232
    },
    {
        "loss": 2.2382,
        "grad_norm": 1.638433814048767,
        "learning_rate": 0.0001742271186950383,
        "epoch": 0.15893271461716937,
        "step": 1233
    },
    {
        "loss": 2.1901,
        "grad_norm": 1.0265395641326904,
        "learning_rate": 0.00017418633586793522,
        "epoch": 0.15906161381799433,
        "step": 1234
    },
    {
        "loss": 2.0281,
        "grad_norm": 1.4163039922714233,
        "learning_rate": 0.0001741455255804071,
        "epoch": 0.15919051301881929,
        "step": 1235
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.8956249952316284,
        "learning_rate": 0.00017410468784756006,
        "epoch": 0.15931941221964424,
        "step": 1236
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.088036060333252,
        "learning_rate": 0.00017406382268451043,
        "epoch": 0.1594483114204692,
        "step": 1237
    },
    {
        "loss": 1.868,
        "grad_norm": 1.2264310121536255,
        "learning_rate": 0.00017402293010638457,
        "epoch": 0.15957721062129415,
        "step": 1238
    },
    {
        "loss": 2.3843,
        "grad_norm": 1.003242015838623,
        "learning_rate": 0.00017398201012831912,
        "epoch": 0.1597061098221191,
        "step": 1239
    },
    {
        "loss": 2.37,
        "grad_norm": 1.778502345085144,
        "learning_rate": 0.0001739410627654608,
        "epoch": 0.15983500902294406,
        "step": 1240
    },
    {
        "loss": 1.7482,
        "grad_norm": 1.3908718824386597,
        "learning_rate": 0.0001739000880329664,
        "epoch": 0.15996390822376902,
        "step": 1241
    },
    {
        "loss": 1.9776,
        "grad_norm": 1.9001479148864746,
        "learning_rate": 0.000173859085946003,
        "epoch": 0.16009280742459397,
        "step": 1242
    },
    {
        "loss": 1.9554,
        "grad_norm": 1.7403103113174438,
        "learning_rate": 0.00017381805651974769,
        "epoch": 0.16022170662541893,
        "step": 1243
    },
    {
        "loss": 2.2599,
        "grad_norm": 2.3489174842834473,
        "learning_rate": 0.00017377699976938766,
        "epoch": 0.16035060582624389,
        "step": 1244
    },
    {
        "loss": 2.4865,
        "grad_norm": 1.467071533203125,
        "learning_rate": 0.00017373591571012034,
        "epoch": 0.16047950502706884,
        "step": 1245
    },
    {
        "loss": 2.0043,
        "grad_norm": 1.557769775390625,
        "learning_rate": 0.00017369480435715313,
        "epoch": 0.1606084042278938,
        "step": 1246
    },
    {
        "loss": 0.8964,
        "grad_norm": 2.501274585723877,
        "learning_rate": 0.00017365366572570366,
        "epoch": 0.16073730342871875,
        "step": 1247
    },
    {
        "loss": 1.5758,
        "grad_norm": 2.1486895084381104,
        "learning_rate": 0.00017361249983099947,
        "epoch": 0.1608662026295437,
        "step": 1248
    },
    {
        "loss": 1.2739,
        "grad_norm": 1.7091734409332275,
        "learning_rate": 0.00017357130668827845,
        "epoch": 0.16099510183036866,
        "step": 1249
    },
    {
        "loss": 2.6547,
        "grad_norm": 1.008318305015564,
        "learning_rate": 0.00017353008631278832,
        "epoch": 0.16112400103119362,
        "step": 1250
    },
    {
        "loss": 2.4791,
        "grad_norm": 1.2557482719421387,
        "learning_rate": 0.0001734888387197871,
        "epoch": 0.16125290023201855,
        "step": 1251
    },
    {
        "loss": 2.5059,
        "grad_norm": 1.0432045459747314,
        "learning_rate": 0.00017344756392454272,
        "epoch": 0.1613817994328435,
        "step": 1252
    },
    {
        "loss": 1.8548,
        "grad_norm": 1.7934297323226929,
        "learning_rate": 0.00017340626194233325,
        "epoch": 0.16151069863366846,
        "step": 1253
    },
    {
        "loss": 1.8134,
        "grad_norm": 1.3261798620224,
        "learning_rate": 0.00017336493278844681,
        "epoch": 0.16163959783449341,
        "step": 1254
    },
    {
        "loss": 2.2723,
        "grad_norm": 1.7962148189544678,
        "learning_rate": 0.00017332357647818162,
        "epoch": 0.16176849703531837,
        "step": 1255
    },
    {
        "loss": 1.1024,
        "grad_norm": 2.7557835578918457,
        "learning_rate": 0.00017328219302684586,
        "epoch": 0.16189739623614333,
        "step": 1256
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.2801798582077026,
        "learning_rate": 0.0001732407824497579,
        "epoch": 0.16202629543696828,
        "step": 1257
    },
    {
        "loss": 2.477,
        "grad_norm": 0.876685380935669,
        "learning_rate": 0.00017319934476224593,
        "epoch": 0.16215519463779324,
        "step": 1258
    },
    {
        "loss": 1.8641,
        "grad_norm": 1.5669000148773193,
        "learning_rate": 0.0001731578799796484,
        "epoch": 0.1622840938386182,
        "step": 1259
    },
    {
        "loss": 2.8672,
        "grad_norm": 1.3828248977661133,
        "learning_rate": 0.00017311638811731372,
        "epoch": 0.16241299303944315,
        "step": 1260
    },
    {
        "loss": 1.7601,
        "grad_norm": 2.1957790851593018,
        "learning_rate": 0.00017307486919060022,
        "epoch": 0.1625418922402681,
        "step": 1261
    },
    {
        "loss": 2.0222,
        "grad_norm": 1.9607843160629272,
        "learning_rate": 0.00017303332321487637,
        "epoch": 0.16267079144109306,
        "step": 1262
    },
    {
        "loss": 1.6472,
        "grad_norm": 1.4889353513717651,
        "learning_rate": 0.00017299175020552061,
        "epoch": 0.16279969064191802,
        "step": 1263
    },
    {
        "loss": 2.0558,
        "grad_norm": 1.0668082237243652,
        "learning_rate": 0.00017295015017792137,
        "epoch": 0.16292858984274297,
        "step": 1264
    },
    {
        "loss": 2.3852,
        "grad_norm": 1.3287886381149292,
        "learning_rate": 0.00017290852314747713,
        "epoch": 0.16305748904356793,
        "step": 1265
    },
    {
        "loss": 2.1929,
        "grad_norm": 1.1469286680221558,
        "learning_rate": 0.0001728668691295963,
        "epoch": 0.16318638824439288,
        "step": 1266
    },
    {
        "loss": 1.5361,
        "grad_norm": 2.0890233516693115,
        "learning_rate": 0.00017282518813969733,
        "epoch": 0.16331528744521784,
        "step": 1267
    },
    {
        "loss": 2.1138,
        "grad_norm": 2.0116021633148193,
        "learning_rate": 0.00017278348019320863,
        "epoch": 0.1634441866460428,
        "step": 1268
    },
    {
        "loss": 2.0137,
        "grad_norm": 1.6661367416381836,
        "learning_rate": 0.00017274174530556858,
        "epoch": 0.16357308584686775,
        "step": 1269
    },
    {
        "loss": 1.5815,
        "grad_norm": 2.509024143218994,
        "learning_rate": 0.0001726999834922256,
        "epoch": 0.1637019850476927,
        "step": 1270
    },
    {
        "loss": 1.6796,
        "grad_norm": 2.222867012023926,
        "learning_rate": 0.00017265819476863797,
        "epoch": 0.16383088424851766,
        "step": 1271
    },
    {
        "loss": 2.1565,
        "grad_norm": 1.5398733615875244,
        "learning_rate": 0.000172616379150274,
        "epoch": 0.16395978344934262,
        "step": 1272
    },
    {
        "loss": 2.3722,
        "grad_norm": 1.293315052986145,
        "learning_rate": 0.00017257453665261196,
        "epoch": 0.16408868265016757,
        "step": 1273
    },
    {
        "loss": 2.248,
        "grad_norm": 1.1875079870224,
        "learning_rate": 0.00017253266729114,
        "epoch": 0.16421758185099253,
        "step": 1274
    },
    {
        "loss": 1.5979,
        "grad_norm": 1.947176456451416,
        "learning_rate": 0.00017249077108135637,
        "epoch": 0.16434648105181748,
        "step": 1275
    },
    {
        "loss": 2.2643,
        "grad_norm": 1.7836081981658936,
        "learning_rate": 0.000172448848038769,
        "epoch": 0.16447538025264244,
        "step": 1276
    },
    {
        "loss": 1.7368,
        "grad_norm": 2.016246795654297,
        "learning_rate": 0.00017240689817889598,
        "epoch": 0.1646042794534674,
        "step": 1277
    },
    {
        "loss": 2.4607,
        "grad_norm": 1.4464261531829834,
        "learning_rate": 0.00017236492151726532,
        "epoch": 0.16473317865429235,
        "step": 1278
    },
    {
        "loss": 2.3603,
        "grad_norm": 1.0560226440429688,
        "learning_rate": 0.00017232291806941477,
        "epoch": 0.1648620778551173,
        "step": 1279
    },
    {
        "loss": 2.3535,
        "grad_norm": 1.3392207622528076,
        "learning_rate": 0.00017228088785089214,
        "epoch": 0.16499097705594226,
        "step": 1280
    },
    {
        "loss": 1.7452,
        "grad_norm": 1.6530143022537231,
        "learning_rate": 0.00017223883087725511,
        "epoch": 0.16511987625676722,
        "step": 1281
    },
    {
        "loss": 1.4282,
        "grad_norm": 2.086554527282715,
        "learning_rate": 0.00017219674716407134,
        "epoch": 0.16524877545759217,
        "step": 1282
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.0716235637664795,
        "learning_rate": 0.00017215463672691823,
        "epoch": 0.16537767465841713,
        "step": 1283
    },
    {
        "loss": 1.1946,
        "grad_norm": 2.174330472946167,
        "learning_rate": 0.00017211249958138316,
        "epoch": 0.16550657385924208,
        "step": 1284
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.687429666519165,
        "learning_rate": 0.00017207033574306345,
        "epoch": 0.16563547306006704,
        "step": 1285
    },
    {
        "loss": 2.2628,
        "grad_norm": 1.5344483852386475,
        "learning_rate": 0.00017202814522756623,
        "epoch": 0.165764372260892,
        "step": 1286
    },
    {
        "loss": 2.1005,
        "grad_norm": 1.9170008897781372,
        "learning_rate": 0.0001719859280505085,
        "epoch": 0.16589327146171692,
        "step": 1287
    },
    {
        "loss": 1.9368,
        "grad_norm": 1.5838109254837036,
        "learning_rate": 0.00017194368422751715,
        "epoch": 0.16602217066254188,
        "step": 1288
    },
    {
        "loss": 2.2371,
        "grad_norm": 1.557114601135254,
        "learning_rate": 0.00017190141377422893,
        "epoch": 0.16615106986336683,
        "step": 1289
    },
    {
        "loss": 1.2438,
        "grad_norm": 2.029588222503662,
        "learning_rate": 0.00017185911670629045,
        "epoch": 0.1662799690641918,
        "step": 1290
    },
    {
        "loss": 2.3876,
        "grad_norm": 1.7188141345977783,
        "learning_rate": 0.0001718167930393582,
        "epoch": 0.16640886826501675,
        "step": 1291
    },
    {
        "loss": 1.9552,
        "grad_norm": 2.2967705726623535,
        "learning_rate": 0.00017177444278909846,
        "epoch": 0.1665377674658417,
        "step": 1292
    },
    {
        "loss": 2.2649,
        "grad_norm": 1.4753094911575317,
        "learning_rate": 0.00017173206597118735,
        "epoch": 0.16666666666666666,
        "step": 1293
    },
    {
        "loss": 1.7471,
        "grad_norm": 1.4873911142349243,
        "learning_rate": 0.0001716896626013109,
        "epoch": 0.1667955658674916,
        "step": 1294
    },
    {
        "loss": 0.5399,
        "grad_norm": 1.7628251314163208,
        "learning_rate": 0.00017164723269516493,
        "epoch": 0.16692446506831657,
        "step": 1295
    },
    {
        "loss": 2.8633,
        "grad_norm": 1.636610984802246,
        "learning_rate": 0.00017160477626845504,
        "epoch": 0.16705336426914152,
        "step": 1296
    },
    {
        "loss": 2.1624,
        "grad_norm": 1.7016352415084839,
        "learning_rate": 0.00017156229333689668,
        "epoch": 0.16718226346996648,
        "step": 1297
    },
    {
        "loss": 1.7239,
        "grad_norm": 1.8505631685256958,
        "learning_rate": 0.00017151978391621509,
        "epoch": 0.16731116267079144,
        "step": 1298
    },
    {
        "loss": 1.7074,
        "grad_norm": 1.8816547393798828,
        "learning_rate": 0.00017147724802214534,
        "epoch": 0.1674400618716164,
        "step": 1299
    },
    {
        "loss": 2.1696,
        "grad_norm": 1.7229312658309937,
        "learning_rate": 0.00017143468567043234,
        "epoch": 0.16756896107244135,
        "step": 1300
    },
    {
        "loss": 2.4376,
        "grad_norm": 1.7807047367095947,
        "learning_rate": 0.00017139209687683072,
        "epoch": 0.1676978602732663,
        "step": 1301
    },
    {
        "loss": 2.1684,
        "grad_norm": 1.0473719835281372,
        "learning_rate": 0.00017134948165710488,
        "epoch": 0.16782675947409126,
        "step": 1302
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.318858027458191,
        "learning_rate": 0.00017130684002702914,
        "epoch": 0.16795565867491621,
        "step": 1303
    },
    {
        "loss": 1.6393,
        "grad_norm": 1.7803970575332642,
        "learning_rate": 0.0001712641720023874,
        "epoch": 0.16808455787574117,
        "step": 1304
    },
    {
        "loss": 1.5335,
        "grad_norm": 2.144369602203369,
        "learning_rate": 0.00017122147759897352,
        "epoch": 0.16821345707656613,
        "step": 1305
    },
    {
        "loss": 2.4647,
        "grad_norm": 1.1353198289871216,
        "learning_rate": 0.00017117875683259098,
        "epoch": 0.16834235627739108,
        "step": 1306
    },
    {
        "loss": 2.0145,
        "grad_norm": 1.576714038848877,
        "learning_rate": 0.00017113600971905307,
        "epoch": 0.16847125547821604,
        "step": 1307
    },
    {
        "loss": 1.8756,
        "grad_norm": 1.3110193014144897,
        "learning_rate": 0.00017109323627418287,
        "epoch": 0.168600154679041,
        "step": 1308
    },
    {
        "loss": 2.0795,
        "grad_norm": 1.7687513828277588,
        "learning_rate": 0.00017105043651381317,
        "epoch": 0.16872905387986595,
        "step": 1309
    },
    {
        "loss": 1.9614,
        "grad_norm": 1.8400421142578125,
        "learning_rate": 0.00017100761045378652,
        "epoch": 0.1688579530806909,
        "step": 1310
    },
    {
        "loss": 2.3062,
        "grad_norm": 2.618861675262451,
        "learning_rate": 0.00017096475810995511,
        "epoch": 0.16898685228151586,
        "step": 1311
    },
    {
        "loss": 2.2985,
        "grad_norm": 1.5553686618804932,
        "learning_rate": 0.00017092187949818106,
        "epoch": 0.16911575148234081,
        "step": 1312
    },
    {
        "loss": 1.6678,
        "grad_norm": 2.153749942779541,
        "learning_rate": 0.000170878974634336,
        "epoch": 0.16924465068316577,
        "step": 1313
    },
    {
        "loss": 2.5457,
        "grad_norm": 1.221803069114685,
        "learning_rate": 0.00017083604353430136,
        "epoch": 0.16937354988399073,
        "step": 1314
    },
    {
        "loss": 1.8818,
        "grad_norm": 1.7010823488235474,
        "learning_rate": 0.0001707930862139684,
        "epoch": 0.16950244908481568,
        "step": 1315
    },
    {
        "loss": 2.5106,
        "grad_norm": 1.4087860584259033,
        "learning_rate": 0.00017075010268923787,
        "epoch": 0.16963134828564064,
        "step": 1316
    },
    {
        "loss": 1.7875,
        "grad_norm": 2.0470023155212402,
        "learning_rate": 0.00017070709297602033,
        "epoch": 0.1697602474864656,
        "step": 1317
    },
    {
        "loss": 2.7865,
        "grad_norm": 1.1494882106781006,
        "learning_rate": 0.0001706640570902361,
        "epoch": 0.16988914668729055,
        "step": 1318
    },
    {
        "loss": 2.3203,
        "grad_norm": 1.2090853452682495,
        "learning_rate": 0.00017062099504781508,
        "epoch": 0.1700180458881155,
        "step": 1319
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.5277228355407715,
        "learning_rate": 0.00017057790686469688,
        "epoch": 0.17014694508894046,
        "step": 1320
    },
    {
        "loss": 1.2469,
        "grad_norm": 2.3433737754821777,
        "learning_rate": 0.0001705347925568308,
        "epoch": 0.17027584428976542,
        "step": 1321
    },
    {
        "loss": 2.4359,
        "grad_norm": 1.390722393989563,
        "learning_rate": 0.0001704916521401758,
        "epoch": 0.17040474349059037,
        "step": 1322
    },
    {
        "loss": 2.4396,
        "grad_norm": 1.0168484449386597,
        "learning_rate": 0.00017044848563070058,
        "epoch": 0.17053364269141533,
        "step": 1323
    },
    {
        "loss": 2.1385,
        "grad_norm": 0.9244910478591919,
        "learning_rate": 0.00017040529304438332,
        "epoch": 0.17066254189224025,
        "step": 1324
    },
    {
        "loss": 2.3363,
        "grad_norm": 1.0347789525985718,
        "learning_rate": 0.000170362074397212,
        "epoch": 0.1707914410930652,
        "step": 1325
    },
    {
        "loss": 2.4728,
        "grad_norm": 0.8788376450538635,
        "learning_rate": 0.00017031882970518423,
        "epoch": 0.17092034029389017,
        "step": 1326
    },
    {
        "loss": 2.0629,
        "grad_norm": 1.6803743839263916,
        "learning_rate": 0.00017027555898430725,
        "epoch": 0.17104923949471512,
        "step": 1327
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.5214992761611938,
        "learning_rate": 0.00017023226225059787,
        "epoch": 0.17117813869554008,
        "step": 1328
    },
    {
        "loss": 2.3673,
        "grad_norm": 1.5560767650604248,
        "learning_rate": 0.00017018893952008263,
        "epoch": 0.17130703789636503,
        "step": 1329
    },
    {
        "loss": 1.8264,
        "grad_norm": 1.9884843826293945,
        "learning_rate": 0.00017014559080879763,
        "epoch": 0.17143593709719,
        "step": 1330
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.161466121673584,
        "learning_rate": 0.00017010221613278858,
        "epoch": 0.17156483629801494,
        "step": 1331
    },
    {
        "loss": 2.6785,
        "grad_norm": 1.0520156621932983,
        "learning_rate": 0.00017005881550811086,
        "epoch": 0.1716937354988399,
        "step": 1332
    },
    {
        "loss": 1.9068,
        "grad_norm": 1.1200915575027466,
        "learning_rate": 0.0001700153889508294,
        "epoch": 0.17182263469966486,
        "step": 1333
    },
    {
        "loss": 2.0206,
        "grad_norm": 1.5163545608520508,
        "learning_rate": 0.00016997193647701872,
        "epoch": 0.1719515339004898,
        "step": 1334
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.0311269760131836,
        "learning_rate": 0.00016992845810276302,
        "epoch": 0.17208043310131477,
        "step": 1335
    },
    {
        "loss": 1.8238,
        "grad_norm": 1.2102906703948975,
        "learning_rate": 0.00016988495384415595,
        "epoch": 0.17220933230213972,
        "step": 1336
    },
    {
        "loss": 1.9443,
        "grad_norm": 1.8123928308486938,
        "learning_rate": 0.00016984142371730089,
        "epoch": 0.17233823150296468,
        "step": 1337
    },
    {
        "loss": 1.6104,
        "grad_norm": 1.9366722106933594,
        "learning_rate": 0.0001697978677383107,
        "epoch": 0.17246713070378963,
        "step": 1338
    },
    {
        "loss": 1.5042,
        "grad_norm": 1.8567880392074585,
        "learning_rate": 0.00016975428592330783,
        "epoch": 0.1725960299046146,
        "step": 1339
    },
    {
        "loss": 1.8865,
        "grad_norm": 1.4696381092071533,
        "learning_rate": 0.00016971067828842426,
        "epoch": 0.17272492910543955,
        "step": 1340
    },
    {
        "loss": 1.787,
        "grad_norm": 1.5478134155273438,
        "learning_rate": 0.00016966704484980163,
        "epoch": 0.1728538283062645,
        "step": 1341
    },
    {
        "loss": 1.9016,
        "grad_norm": 1.5543808937072754,
        "learning_rate": 0.000169623385623591,
        "epoch": 0.17298272750708946,
        "step": 1342
    },
    {
        "loss": 1.1978,
        "grad_norm": 0.9326057434082031,
        "learning_rate": 0.00016957970062595308,
        "epoch": 0.1731116267079144,
        "step": 1343
    },
    {
        "loss": 2.1905,
        "grad_norm": 1.6041816473007202,
        "learning_rate": 0.0001695359898730581,
        "epoch": 0.17324052590873937,
        "step": 1344
    },
    {
        "loss": 2.4914,
        "grad_norm": 1.5210464000701904,
        "learning_rate": 0.00016949225338108574,
        "epoch": 0.17336942510956432,
        "step": 1345
    },
    {
        "loss": 2.359,
        "grad_norm": 1.503485918045044,
        "learning_rate": 0.00016944849116622533,
        "epoch": 0.17349832431038928,
        "step": 1346
    },
    {
        "loss": 2.1864,
        "grad_norm": 1.0776768922805786,
        "learning_rate": 0.00016940470324467563,
        "epoch": 0.17362722351121423,
        "step": 1347
    },
    {
        "loss": 2.0453,
        "grad_norm": 1.7396495342254639,
        "learning_rate": 0.00016936088963264494,
        "epoch": 0.1737561227120392,
        "step": 1348
    },
    {
        "loss": 2.0919,
        "grad_norm": 1.6113649606704712,
        "learning_rate": 0.00016931705034635108,
        "epoch": 0.17388502191286415,
        "step": 1349
    },
    {
        "loss": 2.2071,
        "grad_norm": 1.094049334526062,
        "learning_rate": 0.00016927318540202143,
        "epoch": 0.1740139211136891,
        "step": 1350
    },
    {
        "loss": 2.2784,
        "grad_norm": 1.0684716701507568,
        "learning_rate": 0.00016922929481589272,
        "epoch": 0.17414282031451406,
        "step": 1351
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.2255034446716309,
        "learning_rate": 0.0001691853786042113,
        "epoch": 0.174271719515339,
        "step": 1352
    },
    {
        "loss": 1.9881,
        "grad_norm": 1.6878234148025513,
        "learning_rate": 0.00016914143678323297,
        "epoch": 0.17440061871616397,
        "step": 1353
    },
    {
        "loss": 1.9352,
        "grad_norm": 1.7776063680648804,
        "learning_rate": 0.00016909746936922298,
        "epoch": 0.17452951791698892,
        "step": 1354
    },
    {
        "loss": 2.1536,
        "grad_norm": 1.2307926416397095,
        "learning_rate": 0.00016905347637845614,
        "epoch": 0.17465841711781388,
        "step": 1355
    },
    {
        "loss": 2.0491,
        "grad_norm": 1.8115986585617065,
        "learning_rate": 0.0001690094578272166,
        "epoch": 0.17478731631863884,
        "step": 1356
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.131258249282837,
        "learning_rate": 0.000168965413731798,
        "epoch": 0.1749162155194638,
        "step": 1357
    },
    {
        "loss": 2.1416,
        "grad_norm": 1.9630732536315918,
        "learning_rate": 0.00016892134410850362,
        "epoch": 0.17504511472028875,
        "step": 1358
    },
    {
        "loss": 2.5888,
        "grad_norm": 1.14718496799469,
        "learning_rate": 0.00016887724897364587,
        "epoch": 0.1751740139211137,
        "step": 1359
    },
    {
        "loss": 2.2438,
        "grad_norm": 1.7053529024124146,
        "learning_rate": 0.00016883312834354688,
        "epoch": 0.17530291312193863,
        "step": 1360
    },
    {
        "loss": 1.8287,
        "grad_norm": 1.7598669528961182,
        "learning_rate": 0.0001687889822345381,
        "epoch": 0.1754318123227636,
        "step": 1361
    },
    {
        "loss": 2.2638,
        "grad_norm": 2.1056435108184814,
        "learning_rate": 0.00016874481066296037,
        "epoch": 0.17556071152358854,
        "step": 1362
    },
    {
        "loss": 1.7124,
        "grad_norm": 1.892121434211731,
        "learning_rate": 0.00016870061364516404,
        "epoch": 0.1756896107244135,
        "step": 1363
    },
    {
        "loss": 1.3788,
        "grad_norm": 1.6344873905181885,
        "learning_rate": 0.0001686563911975089,
        "epoch": 0.17581850992523845,
        "step": 1364
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.1766504049301147,
        "learning_rate": 0.000168612143336364,
        "epoch": 0.1759474091260634,
        "step": 1365
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.6741197109222412,
        "learning_rate": 0.00016856787007810795,
        "epoch": 0.17607630832688836,
        "step": 1366
    },
    {
        "loss": 2.0271,
        "grad_norm": 1.5540891885757446,
        "learning_rate": 0.00016852357143912872,
        "epoch": 0.17620520752771332,
        "step": 1367
    },
    {
        "loss": 2.6125,
        "grad_norm": 1.0485830307006836,
        "learning_rate": 0.00016847924743582362,
        "epoch": 0.17633410672853828,
        "step": 1368
    },
    {
        "loss": 2.0502,
        "grad_norm": 1.1671969890594482,
        "learning_rate": 0.0001684348980845994,
        "epoch": 0.17646300592936323,
        "step": 1369
    },
    {
        "loss": 1.9184,
        "grad_norm": 1.8949143886566162,
        "learning_rate": 0.0001683905234018722,
        "epoch": 0.1765919051301882,
        "step": 1370
    },
    {
        "loss": 1.6468,
        "grad_norm": 2.1385114192962646,
        "learning_rate": 0.00016834612340406755,
        "epoch": 0.17672080433101314,
        "step": 1371
    },
    {
        "loss": 1.9799,
        "grad_norm": 2.6380932331085205,
        "learning_rate": 0.00016830169810762026,
        "epoch": 0.1768497035318381,
        "step": 1372
    },
    {
        "loss": 1.8813,
        "grad_norm": 1.3783140182495117,
        "learning_rate": 0.00016825724752897464,
        "epoch": 0.17697860273266305,
        "step": 1373
    },
    {
        "loss": 2.1296,
        "grad_norm": 1.9566631317138672,
        "learning_rate": 0.00016821277168458415,
        "epoch": 0.177107501933488,
        "step": 1374
    },
    {
        "loss": 1.5095,
        "grad_norm": 1.9502853155136108,
        "learning_rate": 0.00016816827059091188,
        "epoch": 0.17723640113431297,
        "step": 1375
    },
    {
        "loss": 2.4136,
        "grad_norm": 1.0409737825393677,
        "learning_rate": 0.00016812374426443004,
        "epoch": 0.17736530033513792,
        "step": 1376
    },
    {
        "loss": 2.0568,
        "grad_norm": 1.7759543657302856,
        "learning_rate": 0.00016807919272162028,
        "epoch": 0.17749419953596288,
        "step": 1377
    },
    {
        "loss": 2.2689,
        "grad_norm": 1.4482336044311523,
        "learning_rate": 0.0001680346159789736,
        "epoch": 0.17762309873678783,
        "step": 1378
    },
    {
        "loss": 1.8615,
        "grad_norm": 1.7591787576675415,
        "learning_rate": 0.00016799001405299019,
        "epoch": 0.1777519979376128,
        "step": 1379
    },
    {
        "loss": 1.9634,
        "grad_norm": 1.5313119888305664,
        "learning_rate": 0.0001679453869601798,
        "epoch": 0.17788089713843774,
        "step": 1380
    },
    {
        "loss": 1.4615,
        "grad_norm": 1.9988459348678589,
        "learning_rate": 0.00016790073471706127,
        "epoch": 0.1780097963392627,
        "step": 1381
    },
    {
        "loss": 1.8333,
        "grad_norm": 1.4953522682189941,
        "learning_rate": 0.00016785605734016286,
        "epoch": 0.17813869554008765,
        "step": 1382
    },
    {
        "loss": 2.359,
        "grad_norm": 2.1597485542297363,
        "learning_rate": 0.00016781135484602216,
        "epoch": 0.1782675947409126,
        "step": 1383
    },
    {
        "loss": 1.4364,
        "grad_norm": 2.1997249126434326,
        "learning_rate": 0.00016776662725118593,
        "epoch": 0.17839649394173757,
        "step": 1384
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.9590823650360107,
        "learning_rate": 0.00016772187457221035,
        "epoch": 0.17852539314256252,
        "step": 1385
    },
    {
        "loss": 1.7064,
        "grad_norm": 1.90891432762146,
        "learning_rate": 0.00016767709682566085,
        "epoch": 0.17865429234338748,
        "step": 1386
    },
    {
        "loss": 1.5234,
        "grad_norm": 1.60318124294281,
        "learning_rate": 0.0001676322940281121,
        "epoch": 0.17878319154421243,
        "step": 1387
    },
    {
        "loss": 1.8035,
        "grad_norm": 2.4916553497314453,
        "learning_rate": 0.00016758746619614806,
        "epoch": 0.1789120907450374,
        "step": 1388
    },
    {
        "loss": 1.5486,
        "grad_norm": 2.0279717445373535,
        "learning_rate": 0.000167542613346362,
        "epoch": 0.17904098994586234,
        "step": 1389
    },
    {
        "loss": 1.5963,
        "grad_norm": 1.6766566038131714,
        "learning_rate": 0.00016749773549535636,
        "epoch": 0.1791698891466873,
        "step": 1390
    },
    {
        "loss": 1.8049,
        "grad_norm": 1.4781341552734375,
        "learning_rate": 0.00016745283265974298,
        "epoch": 0.17929878834751226,
        "step": 1391
    },
    {
        "loss": 1.4784,
        "grad_norm": 2.235517978668213,
        "learning_rate": 0.00016740790485614276,
        "epoch": 0.1794276875483372,
        "step": 1392
    },
    {
        "loss": 1.1894,
        "grad_norm": 2.80145001411438,
        "learning_rate": 0.000167362952101186,
        "epoch": 0.17955658674916217,
        "step": 1393
    },
    {
        "loss": 1.9675,
        "grad_norm": 1.5609545707702637,
        "learning_rate": 0.00016731797441151214,
        "epoch": 0.17968548594998712,
        "step": 1394
    },
    {
        "loss": 2.1995,
        "grad_norm": 1.709318995475769,
        "learning_rate": 0.00016727297180376994,
        "epoch": 0.17981438515081208,
        "step": 1395
    },
    {
        "loss": 2.7438,
        "grad_norm": 1.436269760131836,
        "learning_rate": 0.0001672279442946173,
        "epoch": 0.179943284351637,
        "step": 1396
    },
    {
        "loss": 2.0685,
        "grad_norm": 1.4277598857879639,
        "learning_rate": 0.00016718289190072131,
        "epoch": 0.18007218355246196,
        "step": 1397
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.098889946937561,
        "learning_rate": 0.00016713781463875842,
        "epoch": 0.18020108275328692,
        "step": 1398
    },
    {
        "loss": 2.7154,
        "grad_norm": 1.5467345714569092,
        "learning_rate": 0.0001670927125254141,
        "epoch": 0.18032998195411187,
        "step": 1399
    },
    {
        "loss": 2.3473,
        "grad_norm": 1.4217838048934937,
        "learning_rate": 0.00016704758557738322,
        "epoch": 0.18045888115493683,
        "step": 1400
    },
    {
        "loss": 1.5156,
        "grad_norm": 1.35018789768219,
        "learning_rate": 0.0001670024338113697,
        "epoch": 0.18058778035576178,
        "step": 1401
    },
    {
        "loss": 2.1277,
        "grad_norm": 1.7495851516723633,
        "learning_rate": 0.0001669572572440866,
        "epoch": 0.18071667955658674,
        "step": 1402
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.273223638534546,
        "learning_rate": 0.00016691205589225634,
        "epoch": 0.1808455787574117,
        "step": 1403
    },
    {
        "loss": 1.86,
        "grad_norm": 1.2577816247940063,
        "learning_rate": 0.00016686682977261032,
        "epoch": 0.18097447795823665,
        "step": 1404
    },
    {
        "loss": 1.8569,
        "grad_norm": 1.8577306270599365,
        "learning_rate": 0.0001668215789018893,
        "epoch": 0.1811033771590616,
        "step": 1405
    },
    {
        "loss": 1.735,
        "grad_norm": 1.481501579284668,
        "learning_rate": 0.00016677630329684306,
        "epoch": 0.18123227635988656,
        "step": 1406
    },
    {
        "loss": 2.2051,
        "grad_norm": 1.737194538116455,
        "learning_rate": 0.0001667310029742306,
        "epoch": 0.18136117556071152,
        "step": 1407
    },
    {
        "loss": 1.2256,
        "grad_norm": 1.5010178089141846,
        "learning_rate": 0.00016668567795082002,
        "epoch": 0.18149007476153647,
        "step": 1408
    },
    {
        "loss": 2.1623,
        "grad_norm": 1.892148494720459,
        "learning_rate": 0.0001666403282433886,
        "epoch": 0.18161897396236143,
        "step": 1409
    },
    {
        "loss": 2.0453,
        "grad_norm": 1.5971131324768066,
        "learning_rate": 0.00016659495386872278,
        "epoch": 0.18174787316318639,
        "step": 1410
    },
    {
        "loss": 1.6953,
        "grad_norm": 1.656553030014038,
        "learning_rate": 0.00016654955484361804,
        "epoch": 0.18187677236401134,
        "step": 1411
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.1982814073562622,
        "learning_rate": 0.00016650413118487914,
        "epoch": 0.1820056715648363,
        "step": 1412
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.9620341062545776,
        "learning_rate": 0.00016645868290931978,
        "epoch": 0.18213457076566125,
        "step": 1413
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.0578646659851074,
        "learning_rate": 0.00016641321003376284,
        "epoch": 0.1822634699664862,
        "step": 1414
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.168602705001831,
        "learning_rate": 0.00016636771257504037,
        "epoch": 0.18239236916731116,
        "step": 1415
    },
    {
        "loss": 1.7139,
        "grad_norm": 1.4784274101257324,
        "learning_rate": 0.0001663221905499935,
        "epoch": 0.18252126836813612,
        "step": 1416
    },
    {
        "loss": 2.5054,
        "grad_norm": 1.2891215085983276,
        "learning_rate": 0.00016627664397547237,
        "epoch": 0.18265016756896107,
        "step": 1417
    },
    {
        "loss": 2.2351,
        "grad_norm": 1.5504204034805298,
        "learning_rate": 0.0001662310728683363,
        "epoch": 0.18277906676978603,
        "step": 1418
    },
    {
        "loss": 2.1385,
        "grad_norm": 1.3198662996292114,
        "learning_rate": 0.00016618547724545364,
        "epoch": 0.182907965970611,
        "step": 1419
    },
    {
        "loss": 1.4211,
        "grad_norm": 1.699845790863037,
        "learning_rate": 0.00016613985712370178,
        "epoch": 0.18303686517143594,
        "step": 1420
    },
    {
        "loss": 2.4402,
        "grad_norm": 1.3175861835479736,
        "learning_rate": 0.0001660942125199673,
        "epoch": 0.1831657643722609,
        "step": 1421
    },
    {
        "loss": 2.514,
        "grad_norm": 1.2183325290679932,
        "learning_rate": 0.00016604854345114568,
        "epoch": 0.18329466357308585,
        "step": 1422
    },
    {
        "loss": 2.497,
        "grad_norm": 1.04647696018219,
        "learning_rate": 0.00016600284993414162,
        "epoch": 0.1834235627739108,
        "step": 1423
    },
    {
        "loss": 1.7896,
        "grad_norm": 1.8376997709274292,
        "learning_rate": 0.00016595713198586876,
        "epoch": 0.18355246197473576,
        "step": 1424
    },
    {
        "loss": 2.3483,
        "grad_norm": 1.4853183031082153,
        "learning_rate": 0.00016591138962324982,
        "epoch": 0.18368136117556072,
        "step": 1425
    },
    {
        "loss": 2.2766,
        "grad_norm": 1.6008634567260742,
        "learning_rate": 0.00016586562286321656,
        "epoch": 0.18381026037638568,
        "step": 1426
    },
    {
        "loss": 1.5598,
        "grad_norm": 2.1604697704315186,
        "learning_rate": 0.00016581983172270977,
        "epoch": 0.18393915957721063,
        "step": 1427
    },
    {
        "loss": 1.7364,
        "grad_norm": 2.208428382873535,
        "learning_rate": 0.0001657740162186792,
        "epoch": 0.1840680587780356,
        "step": 1428
    },
    {
        "loss": 2.0532,
        "grad_norm": 1.4617648124694824,
        "learning_rate": 0.00016572817636808376,
        "epoch": 0.18419695797886054,
        "step": 1429
    },
    {
        "loss": 2.4291,
        "grad_norm": 2.003657817840576,
        "learning_rate": 0.00016568231218789126,
        "epoch": 0.1843258571796855,
        "step": 1430
    },
    {
        "loss": 2.2765,
        "grad_norm": 1.6828281879425049,
        "learning_rate": 0.00016563642369507852,
        "epoch": 0.18445475638051045,
        "step": 1431
    },
    {
        "loss": 2.0759,
        "grad_norm": 1.3633227348327637,
        "learning_rate": 0.0001655905109066314,
        "epoch": 0.18458365558133538,
        "step": 1432
    },
    {
        "loss": 2.2629,
        "grad_norm": 1.4492321014404297,
        "learning_rate": 0.00016554457383954473,
        "epoch": 0.18471255478216034,
        "step": 1433
    },
    {
        "loss": 1.7818,
        "grad_norm": 1.9187324047088623,
        "learning_rate": 0.00016549861251082237,
        "epoch": 0.1848414539829853,
        "step": 1434
    },
    {
        "loss": 1.8329,
        "grad_norm": 1.7334117889404297,
        "learning_rate": 0.00016545262693747707,
        "epoch": 0.18497035318381025,
        "step": 1435
    },
    {
        "loss": 1.905,
        "grad_norm": 2.0210375785827637,
        "learning_rate": 0.00016540661713653068,
        "epoch": 0.1850992523846352,
        "step": 1436
    },
    {
        "loss": 2.3439,
        "grad_norm": 1.0328574180603027,
        "learning_rate": 0.00016536058312501383,
        "epoch": 0.18522815158546016,
        "step": 1437
    },
    {
        "loss": 2.5942,
        "grad_norm": 1.039058804512024,
        "learning_rate": 0.00016531452491996634,
        "epoch": 0.18535705078628512,
        "step": 1438
    },
    {
        "loss": 1.4792,
        "grad_norm": 1.8993946313858032,
        "learning_rate": 0.0001652684425384368,
        "epoch": 0.18548594998711007,
        "step": 1439
    },
    {
        "loss": 1.8703,
        "grad_norm": 2.1264281272888184,
        "learning_rate": 0.0001652223359974829,
        "epoch": 0.18561484918793503,
        "step": 1440
    },
    {
        "loss": 2.0627,
        "grad_norm": 2.0300686359405518,
        "learning_rate": 0.00016517620531417113,
        "epoch": 0.18574374838875998,
        "step": 1441
    },
    {
        "loss": 1.6715,
        "grad_norm": 1.7095000743865967,
        "learning_rate": 0.00016513005050557703,
        "epoch": 0.18587264758958494,
        "step": 1442
    },
    {
        "loss": 1.575,
        "grad_norm": 2.006838083267212,
        "learning_rate": 0.00016508387158878493,
        "epoch": 0.1860015467904099,
        "step": 1443
    },
    {
        "loss": 1.4267,
        "grad_norm": 2.139267683029175,
        "learning_rate": 0.00016503766858088828,
        "epoch": 0.18613044599123485,
        "step": 1444
    },
    {
        "loss": 1.69,
        "grad_norm": 2.20628023147583,
        "learning_rate": 0.0001649914414989893,
        "epoch": 0.1862593451920598,
        "step": 1445
    },
    {
        "loss": 2.6878,
        "grad_norm": 1.9576823711395264,
        "learning_rate": 0.00016494519036019918,
        "epoch": 0.18638824439288476,
        "step": 1446
    },
    {
        "loss": 2.4987,
        "grad_norm": 1.1919022798538208,
        "learning_rate": 0.00016489891518163798,
        "epoch": 0.18651714359370972,
        "step": 1447
    },
    {
        "loss": 2.1529,
        "grad_norm": 1.7914994955062866,
        "learning_rate": 0.00016485261598043466,
        "epoch": 0.18664604279453467,
        "step": 1448
    },
    {
        "loss": 1.955,
        "grad_norm": 1.6878108978271484,
        "learning_rate": 0.00016480629277372714,
        "epoch": 0.18677494199535963,
        "step": 1449
    },
    {
        "loss": 2.2394,
        "grad_norm": 1.8874801397323608,
        "learning_rate": 0.00016475994557866214,
        "epoch": 0.18690384119618458,
        "step": 1450
    },
    {
        "loss": 2.8153,
        "grad_norm": 1.094563603401184,
        "learning_rate": 0.0001647135744123953,
        "epoch": 0.18703274039700954,
        "step": 1451
    },
    {
        "loss": 1.2283,
        "grad_norm": 1.7388386726379395,
        "learning_rate": 0.00016466717929209112,
        "epoch": 0.1871616395978345,
        "step": 1452
    },
    {
        "loss": 1.9996,
        "grad_norm": 1.1906243562698364,
        "learning_rate": 0.000164620760234923,
        "epoch": 0.18729053879865945,
        "step": 1453
    },
    {
        "loss": 1.3918,
        "grad_norm": 1.9147989749908447,
        "learning_rate": 0.00016457431725807315,
        "epoch": 0.1874194379994844,
        "step": 1454
    },
    {
        "loss": 2.4791,
        "grad_norm": 1.5073776245117188,
        "learning_rate": 0.0001645278503787327,
        "epoch": 0.18754833720030936,
        "step": 1455
    },
    {
        "loss": 2.1673,
        "grad_norm": 1.9838207960128784,
        "learning_rate": 0.00016448135961410153,
        "epoch": 0.18767723640113432,
        "step": 1456
    },
    {
        "loss": 2.1127,
        "grad_norm": 0.9374592304229736,
        "learning_rate": 0.00016443484498138844,
        "epoch": 0.18780613560195927,
        "step": 1457
    },
    {
        "loss": 1.9686,
        "grad_norm": 1.402747631072998,
        "learning_rate": 0.00016438830649781106,
        "epoch": 0.18793503480278423,
        "step": 1458
    },
    {
        "loss": 2.5448,
        "grad_norm": 1.164881944656372,
        "learning_rate": 0.0001643417441805958,
        "epoch": 0.18806393400360918,
        "step": 1459
    },
    {
        "loss": 2.0966,
        "grad_norm": 1.2511680126190186,
        "learning_rate": 0.00016429515804697794,
        "epoch": 0.18819283320443414,
        "step": 1460
    },
    {
        "loss": 1.3881,
        "grad_norm": 2.240149974822998,
        "learning_rate": 0.00016424854811420157,
        "epoch": 0.1883217324052591,
        "step": 1461
    },
    {
        "loss": 2.6662,
        "grad_norm": 1.3772780895233154,
        "learning_rate": 0.00016420191439951959,
        "epoch": 0.18845063160608405,
        "step": 1462
    },
    {
        "loss": 1.8363,
        "grad_norm": 1.7946419715881348,
        "learning_rate": 0.00016415525692019362,
        "epoch": 0.188579530806909,
        "step": 1463
    },
    {
        "loss": 2.3296,
        "grad_norm": 1.4962724447250366,
        "learning_rate": 0.00016410857569349423,
        "epoch": 0.18870843000773396,
        "step": 1464
    },
    {
        "loss": 1.6169,
        "grad_norm": 1.7524234056472778,
        "learning_rate": 0.00016406187073670064,
        "epoch": 0.18883732920855892,
        "step": 1465
    },
    {
        "loss": 2.2779,
        "grad_norm": 1.8349817991256714,
        "learning_rate": 0.00016401514206710097,
        "epoch": 0.18896622840938387,
        "step": 1466
    },
    {
        "loss": 2.3205,
        "grad_norm": 1.4691100120544434,
        "learning_rate": 0.000163968389701992,
        "epoch": 0.18909512761020883,
        "step": 1467
    },
    {
        "loss": 2.0504,
        "grad_norm": 1.8409756422042847,
        "learning_rate": 0.00016392161365867932,
        "epoch": 0.18922402681103379,
        "step": 1468
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.2614500522613525,
        "learning_rate": 0.0001638748139544774,
        "epoch": 0.1893529260118587,
        "step": 1469
    },
    {
        "loss": 1.1649,
        "grad_norm": 4.28313684463501,
        "learning_rate": 0.00016382799060670933,
        "epoch": 0.18948182521268367,
        "step": 1470
    },
    {
        "loss": 2.2286,
        "grad_norm": 1.2354737520217896,
        "learning_rate": 0.000163781143632707,
        "epoch": 0.18961072441350862,
        "step": 1471
    },
    {
        "loss": 2.2806,
        "grad_norm": 1.6720868349075317,
        "learning_rate": 0.000163734273049811,
        "epoch": 0.18973962361433358,
        "step": 1472
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.137874722480774,
        "learning_rate": 0.00016368737887537073,
        "epoch": 0.18986852281515854,
        "step": 1473
    },
    {
        "loss": 1.7156,
        "grad_norm": 1.676185131072998,
        "learning_rate": 0.00016364046112674432,
        "epoch": 0.1899974220159835,
        "step": 1474
    },
    {
        "loss": 2.389,
        "grad_norm": 1.6597143411636353,
        "learning_rate": 0.00016359351982129852,
        "epoch": 0.19012632121680845,
        "step": 1475
    },
    {
        "loss": 2.0191,
        "grad_norm": 1.671796202659607,
        "learning_rate": 0.00016354655497640897,
        "epoch": 0.1902552204176334,
        "step": 1476
    },
    {
        "loss": 2.4369,
        "grad_norm": 0.8638933300971985,
        "learning_rate": 0.00016349956660945982,
        "epoch": 0.19038411961845836,
        "step": 1477
    },
    {
        "loss": 2.004,
        "grad_norm": 2.016416549682617,
        "learning_rate": 0.00016345255473784417,
        "epoch": 0.19051301881928331,
        "step": 1478
    },
    {
        "loss": 1.8372,
        "grad_norm": 1.8444072008132935,
        "learning_rate": 0.0001634055193789636,
        "epoch": 0.19064191802010827,
        "step": 1479
    },
    {
        "loss": 1.8397,
        "grad_norm": 1.4139323234558105,
        "learning_rate": 0.00016335846055022846,
        "epoch": 0.19077081722093323,
        "step": 1480
    },
    {
        "loss": 2.0971,
        "grad_norm": 2.591599464416504,
        "learning_rate": 0.0001633113782690579,
        "epoch": 0.19089971642175818,
        "step": 1481
    },
    {
        "loss": 2.7372,
        "grad_norm": 1.1667909622192383,
        "learning_rate": 0.00016326427255287953,
        "epoch": 0.19102861562258314,
        "step": 1482
    },
    {
        "loss": 1.4649,
        "grad_norm": 1.9397813081741333,
        "learning_rate": 0.0001632171434191298,
        "epoch": 0.1911575148234081,
        "step": 1483
    },
    {
        "loss": 1.897,
        "grad_norm": 1.776694893836975,
        "learning_rate": 0.0001631699908852538,
        "epoch": 0.19128641402423305,
        "step": 1484
    },
    {
        "loss": 2.1905,
        "grad_norm": 1.8167901039123535,
        "learning_rate": 0.00016312281496870526,
        "epoch": 0.191415313225058,
        "step": 1485
    },
    {
        "loss": 2.2833,
        "grad_norm": 2.372035503387451,
        "learning_rate": 0.00016307561568694657,
        "epoch": 0.19154421242588296,
        "step": 1486
    },
    {
        "loss": 1.8479,
        "grad_norm": 2.2304084300994873,
        "learning_rate": 0.00016302839305744877,
        "epoch": 0.19167311162670791,
        "step": 1487
    },
    {
        "loss": 2.2942,
        "grad_norm": 2.048625946044922,
        "learning_rate": 0.00016298114709769153,
        "epoch": 0.19180201082753287,
        "step": 1488
    },
    {
        "loss": 1.4587,
        "grad_norm": 2.138200283050537,
        "learning_rate": 0.00016293387782516315,
        "epoch": 0.19193091002835783,
        "step": 1489
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.6800581216812134,
        "learning_rate": 0.0001628865852573606,
        "epoch": 0.19205980922918278,
        "step": 1490
    },
    {
        "loss": 2.0068,
        "grad_norm": 2.5733766555786133,
        "learning_rate": 0.00016283926941178947,
        "epoch": 0.19218870843000774,
        "step": 1491
    },
    {
        "loss": 2.5348,
        "grad_norm": 1.3986965417861938,
        "learning_rate": 0.0001627919303059639,
        "epoch": 0.1923176076308327,
        "step": 1492
    },
    {
        "loss": 2.1641,
        "grad_norm": 1.9828938245773315,
        "learning_rate": 0.0001627445679574067,
        "epoch": 0.19244650683165765,
        "step": 1493
    },
    {
        "loss": 2.7478,
        "grad_norm": 1.0654780864715576,
        "learning_rate": 0.0001626971823836493,
        "epoch": 0.1925754060324826,
        "step": 1494
    },
    {
        "loss": 2.3955,
        "grad_norm": 1.4796488285064697,
        "learning_rate": 0.00016264977360223165,
        "epoch": 0.19270430523330756,
        "step": 1495
    },
    {
        "loss": 1.8744,
        "grad_norm": 2.212827444076538,
        "learning_rate": 0.00016260234163070235,
        "epoch": 0.19283320443413252,
        "step": 1496
    },
    {
        "loss": 2.4434,
        "grad_norm": 0.9446887373924255,
        "learning_rate": 0.0001625548864866186,
        "epoch": 0.19296210363495747,
        "step": 1497
    },
    {
        "loss": 2.4528,
        "grad_norm": 1.401140570640564,
        "learning_rate": 0.0001625074081875461,
        "epoch": 0.19309100283578243,
        "step": 1498
    },
    {
        "loss": 1.9052,
        "grad_norm": 2.767526865005493,
        "learning_rate": 0.00016245990675105922,
        "epoch": 0.19321990203660738,
        "step": 1499
    },
    {
        "loss": 2.2986,
        "grad_norm": 2.088714599609375,
        "learning_rate": 0.00016241238219474077,
        "epoch": 0.19334880123743234,
        "step": 1500
    },
    {
        "loss": 2.2489,
        "grad_norm": 1.9398679733276367,
        "learning_rate": 0.00016236483453618226,
        "epoch": 0.1934777004382573,
        "step": 1501
    },
    {
        "loss": 2.3082,
        "grad_norm": 1.7548810243606567,
        "learning_rate": 0.00016231726379298364,
        "epoch": 0.19360659963908225,
        "step": 1502
    },
    {
        "loss": 1.9955,
        "grad_norm": 1.2663969993591309,
        "learning_rate": 0.0001622696699827535,
        "epoch": 0.1937354988399072,
        "step": 1503
    },
    {
        "loss": 2.1714,
        "grad_norm": 1.0851436853408813,
        "learning_rate": 0.00016222205312310886,
        "epoch": 0.19386439804073216,
        "step": 1504
    },
    {
        "loss": 1.5082,
        "grad_norm": 1.3603882789611816,
        "learning_rate": 0.00016217441323167531,
        "epoch": 0.1939932972415571,
        "step": 1505
    },
    {
        "loss": 2.2947,
        "grad_norm": 1.3342504501342773,
        "learning_rate": 0.00016212675032608706,
        "epoch": 0.19412219644238204,
        "step": 1506
    },
    {
        "loss": 2.25,
        "grad_norm": 1.7845368385314941,
        "learning_rate": 0.00016207906442398674,
        "epoch": 0.194251095643207,
        "step": 1507
    },
    {
        "loss": 2.0162,
        "grad_norm": 1.9800119400024414,
        "learning_rate": 0.00016203135554302542,
        "epoch": 0.19437999484403196,
        "step": 1508
    },
    {
        "loss": 2.3493,
        "grad_norm": 1.1433377265930176,
        "learning_rate": 0.0001619836237008629,
        "epoch": 0.1945088940448569,
        "step": 1509
    },
    {
        "loss": 2.1338,
        "grad_norm": 1.3676915168762207,
        "learning_rate": 0.0001619358689151673,
        "epoch": 0.19463779324568187,
        "step": 1510
    },
    {
        "loss": 1.9732,
        "grad_norm": 1.0782036781311035,
        "learning_rate": 0.00016188809120361522,
        "epoch": 0.19476669244650682,
        "step": 1511
    },
    {
        "loss": 1.6081,
        "grad_norm": 2.463075876235962,
        "learning_rate": 0.00016184029058389186,
        "epoch": 0.19489559164733178,
        "step": 1512
    },
    {
        "loss": 1.6818,
        "grad_norm": 2.9448704719543457,
        "learning_rate": 0.00016179246707369088,
        "epoch": 0.19502449084815673,
        "step": 1513
    },
    {
        "loss": 1.4404,
        "grad_norm": 2.238335371017456,
        "learning_rate": 0.0001617446206907143,
        "epoch": 0.1951533900489817,
        "step": 1514
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.4435508251190186,
        "learning_rate": 0.00016169675145267278,
        "epoch": 0.19528228924980665,
        "step": 1515
    },
    {
        "loss": 1.5005,
        "grad_norm": 2.0875656604766846,
        "learning_rate": 0.00016164885937728526,
        "epoch": 0.1954111884506316,
        "step": 1516
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.750290036201477,
        "learning_rate": 0.0001616009444822793,
        "epoch": 0.19554008765145656,
        "step": 1517
    },
    {
        "loss": 2.177,
        "grad_norm": 1.386307716369629,
        "learning_rate": 0.00016155300678539073,
        "epoch": 0.1956689868522815,
        "step": 1518
    },
    {
        "loss": 2.236,
        "grad_norm": 1.3589270114898682,
        "learning_rate": 0.000161505046304364,
        "epoch": 0.19579788605310647,
        "step": 1519
    },
    {
        "loss": 2.416,
        "grad_norm": 1.7907648086547852,
        "learning_rate": 0.00016145706305695188,
        "epoch": 0.19592678525393142,
        "step": 1520
    },
    {
        "loss": 1.8068,
        "grad_norm": 1.8707283735275269,
        "learning_rate": 0.00016140905706091556,
        "epoch": 0.19605568445475638,
        "step": 1521
    },
    {
        "loss": 1.4823,
        "grad_norm": 1.2852150201797485,
        "learning_rate": 0.00016136102833402477,
        "epoch": 0.19618458365558133,
        "step": 1522
    },
    {
        "loss": 2.1757,
        "grad_norm": 1.395835518836975,
        "learning_rate": 0.0001613129768940575,
        "epoch": 0.1963134828564063,
        "step": 1523
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.600172758102417,
        "learning_rate": 0.00016126490275880027,
        "epoch": 0.19644238205723125,
        "step": 1524
    },
    {
        "loss": 2.1294,
        "grad_norm": 1.123948097229004,
        "learning_rate": 0.00016121680594604788,
        "epoch": 0.1965712812580562,
        "step": 1525
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.4382060766220093,
        "learning_rate": 0.00016116868647360368,
        "epoch": 0.19670018045888116,
        "step": 1526
    },
    {
        "loss": 2.2699,
        "grad_norm": 1.2204235792160034,
        "learning_rate": 0.00016112054435927928,
        "epoch": 0.1968290796597061,
        "step": 1527
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.1303119659423828,
        "learning_rate": 0.0001610723796208947,
        "epoch": 0.19695797886053107,
        "step": 1528
    },
    {
        "loss": 2.369,
        "grad_norm": 1.6577497720718384,
        "learning_rate": 0.00016102419227627835,
        "epoch": 0.19708687806135602,
        "step": 1529
    },
    {
        "loss": 1.6209,
        "grad_norm": 1.9144697189331055,
        "learning_rate": 0.000160975982343267,
        "epoch": 0.19721577726218098,
        "step": 1530
    },
    {
        "loss": 2.3105,
        "grad_norm": 1.2604109048843384,
        "learning_rate": 0.00016092774983970582,
        "epoch": 0.19734467646300594,
        "step": 1531
    },
    {
        "loss": 1.0723,
        "grad_norm": 2.0483558177948,
        "learning_rate": 0.0001608794947834483,
        "epoch": 0.1974735756638309,
        "step": 1532
    },
    {
        "loss": 2.1943,
        "grad_norm": 1.449884057044983,
        "learning_rate": 0.00016083121719235623,
        "epoch": 0.19760247486465585,
        "step": 1533
    },
    {
        "loss": 2.4479,
        "grad_norm": 1.1152280569076538,
        "learning_rate": 0.00016078291708429985,
        "epoch": 0.1977313740654808,
        "step": 1534
    },
    {
        "loss": 2.3344,
        "grad_norm": 1.6007202863693237,
        "learning_rate": 0.00016073459447715763,
        "epoch": 0.19786027326630576,
        "step": 1535
    },
    {
        "loss": 1.7367,
        "grad_norm": 1.7332749366760254,
        "learning_rate": 0.00016068624938881643,
        "epoch": 0.19798917246713071,
        "step": 1536
    },
    {
        "loss": 1.9732,
        "grad_norm": 1.5423026084899902,
        "learning_rate": 0.0001606378818371714,
        "epoch": 0.19811807166795567,
        "step": 1537
    },
    {
        "loss": 2.0362,
        "grad_norm": 1.3230043649673462,
        "learning_rate": 0.00016058949184012602,
        "epoch": 0.19824697086878063,
        "step": 1538
    },
    {
        "loss": 1.8972,
        "grad_norm": 1.6802263259887695,
        "learning_rate": 0.00016054107941559212,
        "epoch": 0.19837587006960558,
        "step": 1539
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.208367109298706,
        "learning_rate": 0.00016049264458148973,
        "epoch": 0.19850476927043054,
        "step": 1540
    },
    {
        "loss": 1.05,
        "grad_norm": 2.520622968673706,
        "learning_rate": 0.00016044418735574727,
        "epoch": 0.19863366847125546,
        "step": 1541
    },
    {
        "loss": 2.4516,
        "grad_norm": 1.1891921758651733,
        "learning_rate": 0.0001603957077563014,
        "epoch": 0.19876256767208042,
        "step": 1542
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.0939106941223145,
        "learning_rate": 0.00016034720580109709,
        "epoch": 0.19889146687290538,
        "step": 1543
    },
    {
        "loss": 2.1525,
        "grad_norm": 1.7035472393035889,
        "learning_rate": 0.00016029868150808756,
        "epoch": 0.19902036607373033,
        "step": 1544
    },
    {
        "loss": 2.5246,
        "grad_norm": 1.1994824409484863,
        "learning_rate": 0.00016025013489523427,
        "epoch": 0.1991492652745553,
        "step": 1545
    },
    {
        "loss": 2.1439,
        "grad_norm": 2.6720919609069824,
        "learning_rate": 0.00016020156598050704,
        "epoch": 0.19927816447538024,
        "step": 1546
    },
    {
        "loss": 2.3673,
        "grad_norm": 2.1009910106658936,
        "learning_rate": 0.0001601529747818839,
        "epoch": 0.1994070636762052,
        "step": 1547
    },
    {
        "loss": 1.836,
        "grad_norm": 1.5628305673599243,
        "learning_rate": 0.000160104361317351,
        "epoch": 0.19953596287703015,
        "step": 1548
    },
    {
        "loss": 2.2732,
        "grad_norm": 1.6935640573501587,
        "learning_rate": 0.000160055725604903,
        "epoch": 0.1996648620778551,
        "step": 1549
    },
    {
        "loss": 2.2997,
        "grad_norm": 1.9353300333023071,
        "learning_rate": 0.00016000706766254247,
        "epoch": 0.19979376127868007,
        "step": 1550
    },
    {
        "loss": 2.0539,
        "grad_norm": 1.670081615447998,
        "learning_rate": 0.00015995838750828054,
        "epoch": 0.19992266047950502,
        "step": 1551
    },
    {
        "loss": 2.0163,
        "grad_norm": 1.5485949516296387,
        "learning_rate": 0.00015990968516013627,
        "epoch": 0.20005155968032998,
        "step": 1552
    },
    {
        "loss": 2.641,
        "grad_norm": 1.1287075281143188,
        "learning_rate": 0.00015986096063613716,
        "epoch": 0.20018045888115493,
        "step": 1553
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.049336314201355,
        "learning_rate": 0.0001598122139543188,
        "epoch": 0.2003093580819799,
        "step": 1554
    },
    {
        "loss": 1.8696,
        "grad_norm": 1.6220184564590454,
        "learning_rate": 0.00015976344513272497,
        "epoch": 0.20043825728280484,
        "step": 1555
    },
    {
        "loss": 2.1172,
        "grad_norm": 1.7990931272506714,
        "learning_rate": 0.0001597146541894077,
        "epoch": 0.2005671564836298,
        "step": 1556
    },
    {
        "loss": 1.5561,
        "grad_norm": 1.3934154510498047,
        "learning_rate": 0.00015966584114242716,
        "epoch": 0.20069605568445475,
        "step": 1557
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.9156568050384521,
        "learning_rate": 0.00015961700600985185,
        "epoch": 0.2008249548852797,
        "step": 1558
    },
    {
        "loss": 2.4899,
        "grad_norm": 1.3087058067321777,
        "learning_rate": 0.00015956814880975818,
        "epoch": 0.20095385408610467,
        "step": 1559
    },
    {
        "loss": 2.3021,
        "grad_norm": 1.984826683998108,
        "learning_rate": 0.000159519269560231,
        "epoch": 0.20108275328692962,
        "step": 1560
    },
    {
        "loss": 1.9432,
        "grad_norm": 2.0459182262420654,
        "learning_rate": 0.00015947036827936305,
        "epoch": 0.20121165248775458,
        "step": 1561
    },
    {
        "loss": 2.0444,
        "grad_norm": 2.016784429550171,
        "learning_rate": 0.00015942144498525554,
        "epoch": 0.20134055168857953,
        "step": 1562
    },
    {
        "loss": 2.5627,
        "grad_norm": 1.4647727012634277,
        "learning_rate": 0.00015937249969601752,
        "epoch": 0.2014694508894045,
        "step": 1563
    },
    {
        "loss": 1.9698,
        "grad_norm": 1.8053123950958252,
        "learning_rate": 0.00015932353242976638,
        "epoch": 0.20159835009022944,
        "step": 1564
    },
    {
        "loss": 1.8462,
        "grad_norm": 1.6453297138214111,
        "learning_rate": 0.0001592745432046276,
        "epoch": 0.2017272492910544,
        "step": 1565
    },
    {
        "loss": 2.4213,
        "grad_norm": 1.89290452003479,
        "learning_rate": 0.00015922553203873477,
        "epoch": 0.20185614849187936,
        "step": 1566
    },
    {
        "loss": 2.1107,
        "grad_norm": 0.9034083485603333,
        "learning_rate": 0.00015917649895022963,
        "epoch": 0.2019850476927043,
        "step": 1567
    },
    {
        "loss": 1.0737,
        "grad_norm": 2.4056808948516846,
        "learning_rate": 0.0001591274439572619,
        "epoch": 0.20211394689352927,
        "step": 1568
    },
    {
        "loss": 1.6511,
        "grad_norm": 1.8132708072662354,
        "learning_rate": 0.0001590783670779896,
        "epoch": 0.20224284609435422,
        "step": 1569
    },
    {
        "loss": 1.6022,
        "grad_norm": 1.2559340000152588,
        "learning_rate": 0.00015902926833057882,
        "epoch": 0.20237174529517918,
        "step": 1570
    },
    {
        "loss": 1.5793,
        "grad_norm": 2.7365527153015137,
        "learning_rate": 0.00015898014773320357,
        "epoch": 0.20250064449600413,
        "step": 1571
    },
    {
        "loss": 1.5496,
        "grad_norm": 2.1244802474975586,
        "learning_rate": 0.0001589310053040462,
        "epoch": 0.2026295436968291,
        "step": 1572
    },
    {
        "loss": 2.179,
        "grad_norm": 1.5270954370498657,
        "learning_rate": 0.00015888184106129695,
        "epoch": 0.20275844289765405,
        "step": 1573
    },
    {
        "loss": 1.9515,
        "grad_norm": 1.6746437549591064,
        "learning_rate": 0.0001588326550231542,
        "epoch": 0.202887342098479,
        "step": 1574
    },
    {
        "loss": 2.2842,
        "grad_norm": 1.4410704374313354,
        "learning_rate": 0.00015878344720782442,
        "epoch": 0.20301624129930396,
        "step": 1575
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.4157726764678955,
        "learning_rate": 0.00015873421763352207,
        "epoch": 0.2031451405001289,
        "step": 1576
    },
    {
        "loss": 2.0312,
        "grad_norm": 1.3494030237197876,
        "learning_rate": 0.00015868496631846974,
        "epoch": 0.20327403970095384,
        "step": 1577
    },
    {
        "loss": 2.4725,
        "grad_norm": 2.5047895908355713,
        "learning_rate": 0.000158635693280898,
        "epoch": 0.2034029389017788,
        "step": 1578
    },
    {
        "loss": 2.0544,
        "grad_norm": 1.2291264533996582,
        "learning_rate": 0.00015858639853904563,
        "epoch": 0.20353183810260375,
        "step": 1579
    },
    {
        "loss": 2.4951,
        "grad_norm": 1.4672369956970215,
        "learning_rate": 0.00015853708211115914,
        "epoch": 0.2036607373034287,
        "step": 1580
    },
    {
        "loss": 1.2889,
        "grad_norm": 3.054384231567383,
        "learning_rate": 0.00015848774401549335,
        "epoch": 0.20378963650425366,
        "step": 1581
    },
    {
        "loss": 1.994,
        "grad_norm": 2.2464263439178467,
        "learning_rate": 0.00015843838427031094,
        "epoch": 0.20391853570507862,
        "step": 1582
    },
    {
        "loss": 1.7053,
        "grad_norm": 2.3324437141418457,
        "learning_rate": 0.00015838900289388272,
        "epoch": 0.20404743490590357,
        "step": 1583
    },
    {
        "loss": 1.9834,
        "grad_norm": 1.5773663520812988,
        "learning_rate": 0.00015833959990448738,
        "epoch": 0.20417633410672853,
        "step": 1584
    },
    {
        "loss": 1.6863,
        "grad_norm": 1.4861605167388916,
        "learning_rate": 0.00015829017532041165,
        "epoch": 0.20430523330755349,
        "step": 1585
    },
    {
        "loss": 2.1528,
        "grad_norm": 1.2794722318649292,
        "learning_rate": 0.00015824072915995036,
        "epoch": 0.20443413250837844,
        "step": 1586
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.11554217338562,
        "learning_rate": 0.0001581912614414062,
        "epoch": 0.2045630317092034,
        "step": 1587
    },
    {
        "loss": 1.473,
        "grad_norm": 2.200265407562256,
        "learning_rate": 0.00015814177218308984,
        "epoch": 0.20469193091002835,
        "step": 1588
    },
    {
        "loss": 2.5307,
        "grad_norm": 1.2946083545684814,
        "learning_rate": 0.00015809226140331997,
        "epoch": 0.2048208301108533,
        "step": 1589
    },
    {
        "loss": 2.2367,
        "grad_norm": 1.417913794517517,
        "learning_rate": 0.00015804272912042335,
        "epoch": 0.20494972931167826,
        "step": 1590
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.7005946636199951,
        "learning_rate": 0.00015799317535273447,
        "epoch": 0.20507862851250322,
        "step": 1591
    },
    {
        "loss": 2.5857,
        "grad_norm": 1.433954119682312,
        "learning_rate": 0.00015794360011859592,
        "epoch": 0.20520752771332817,
        "step": 1592
    },
    {
        "loss": 2.3823,
        "grad_norm": 1.0121177434921265,
        "learning_rate": 0.00015789400343635824,
        "epoch": 0.20533642691415313,
        "step": 1593
    },
    {
        "loss": 1.1106,
        "grad_norm": 2.4462499618530273,
        "learning_rate": 0.00015784438532437983,
        "epoch": 0.2054653261149781,
        "step": 1594
    },
    {
        "loss": 2.0995,
        "grad_norm": 2.1713638305664062,
        "learning_rate": 0.00015779474580102716,
        "epoch": 0.20559422531580304,
        "step": 1595
    },
    {
        "loss": 2.3513,
        "grad_norm": 1.3013638257980347,
        "learning_rate": 0.00015774508488467445,
        "epoch": 0.205723124516628,
        "step": 1596
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.3746005296707153,
        "learning_rate": 0.0001576954025937039,
        "epoch": 0.20585202371745295,
        "step": 1597
    },
    {
        "loss": 1.2704,
        "grad_norm": 2.7388103008270264,
        "learning_rate": 0.00015764569894650575,
        "epoch": 0.2059809229182779,
        "step": 1598
    },
    {
        "loss": 1.9451,
        "grad_norm": 1.5014169216156006,
        "learning_rate": 0.000157595973961478,
        "epoch": 0.20610982211910286,
        "step": 1599
    },
    {
        "loss": 1.9952,
        "grad_norm": 1.7666490077972412,
        "learning_rate": 0.00015754622765702653,
        "epoch": 0.20623872131992782,
        "step": 1600
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.115277051925659,
        "learning_rate": 0.00015749646005156525,
        "epoch": 0.20636762052075278,
        "step": 1601
    },
    {
        "loss": 1.0565,
        "grad_norm": 2.104905843734741,
        "learning_rate": 0.00015744667116351585,
        "epoch": 0.20649651972157773,
        "step": 1602
    },
    {
        "loss": 2.0224,
        "grad_norm": 3.5305352210998535,
        "learning_rate": 0.00015739686101130787,
        "epoch": 0.2066254189224027,
        "step": 1603
    },
    {
        "loss": 1.885,
        "grad_norm": 1.9773738384246826,
        "learning_rate": 0.00015734702961337887,
        "epoch": 0.20675431812322764,
        "step": 1604
    },
    {
        "loss": 2.3652,
        "grad_norm": 1.819791555404663,
        "learning_rate": 0.0001572971769881741,
        "epoch": 0.2068832173240526,
        "step": 1605
    },
    {
        "loss": 2.4505,
        "grad_norm": 1.536857008934021,
        "learning_rate": 0.0001572473031541468,
        "epoch": 0.20701211652487755,
        "step": 1606
    },
    {
        "loss": 1.8986,
        "grad_norm": 1.7984219789505005,
        "learning_rate": 0.000157197408129758,
        "epoch": 0.2071410157257025,
        "step": 1607
    },
    {
        "loss": 1.9436,
        "grad_norm": 1.0972412824630737,
        "learning_rate": 0.00015714749193347654,
        "epoch": 0.20726991492652747,
        "step": 1608
    },
    {
        "loss": 2.5787,
        "grad_norm": 1.1178969144821167,
        "learning_rate": 0.00015709755458377916,
        "epoch": 0.20739881412735242,
        "step": 1609
    },
    {
        "loss": 2.0734,
        "grad_norm": 2.4041237831115723,
        "learning_rate": 0.00015704759609915043,
        "epoch": 0.20752771332817738,
        "step": 1610
    },
    {
        "loss": 2.1498,
        "grad_norm": 1.5252279043197632,
        "learning_rate": 0.0001569976164980827,
        "epoch": 0.20765661252900233,
        "step": 1611
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.4839553833007812,
        "learning_rate": 0.00015694761579907612,
        "epoch": 0.2077855117298273,
        "step": 1612
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.935802936553955,
        "learning_rate": 0.00015689759402063877,
        "epoch": 0.20791441093065224,
        "step": 1613
    },
    {
        "loss": 1.7574,
        "grad_norm": 1.7605723142623901,
        "learning_rate": 0.0001568475511812864,
        "epoch": 0.20804331013147717,
        "step": 1614
    },
    {
        "loss": 1.919,
        "grad_norm": 1.730649471282959,
        "learning_rate": 0.00015679748729954257,
        "epoch": 0.20817220933230213,
        "step": 1615
    },
    {
        "loss": 1.9486,
        "grad_norm": 2.007145881652832,
        "learning_rate": 0.00015674740239393872,
        "epoch": 0.20830110853312708,
        "step": 1616
    },
    {
        "loss": 1.9679,
        "grad_norm": 1.7515790462493896,
        "learning_rate": 0.00015669729648301402,
        "epoch": 0.20843000773395204,
        "step": 1617
    },
    {
        "loss": 2.2552,
        "grad_norm": 1.2811959981918335,
        "learning_rate": 0.00015664716958531537,
        "epoch": 0.208558906934777,
        "step": 1618
    },
    {
        "loss": 2.6251,
        "grad_norm": 1.276871681213379,
        "learning_rate": 0.00015659702171939749,
        "epoch": 0.20868780613560195,
        "step": 1619
    },
    {
        "loss": 1.359,
        "grad_norm": 2.9028704166412354,
        "learning_rate": 0.0001565468529038229,
        "epoch": 0.2088167053364269,
        "step": 1620
    },
    {
        "loss": 1.9823,
        "grad_norm": 1.8541319370269775,
        "learning_rate": 0.00015649666315716173,
        "epoch": 0.20894560453725186,
        "step": 1621
    },
    {
        "loss": 1.847,
        "grad_norm": 1.1908270120620728,
        "learning_rate": 0.00015644645249799206,
        "epoch": 0.20907450373807682,
        "step": 1622
    },
    {
        "loss": 2.4563,
        "grad_norm": 1.763060212135315,
        "learning_rate": 0.00015639622094489955,
        "epoch": 0.20920340293890177,
        "step": 1623
    },
    {
        "loss": 1.7264,
        "grad_norm": 2.3331243991851807,
        "learning_rate": 0.00015634596851647764,
        "epoch": 0.20933230213972673,
        "step": 1624
    },
    {
        "loss": 2.3583,
        "grad_norm": 1.0559608936309814,
        "learning_rate": 0.0001562956952313275,
        "epoch": 0.20946120134055168,
        "step": 1625
    },
    {
        "loss": 2.01,
        "grad_norm": 1.9794946908950806,
        "learning_rate": 0.00015624540110805804,
        "epoch": 0.20959010054137664,
        "step": 1626
    },
    {
        "loss": 1.6988,
        "grad_norm": 1.4986008405685425,
        "learning_rate": 0.00015619508616528593,
        "epoch": 0.2097189997422016,
        "step": 1627
    },
    {
        "loss": 0.9695,
        "grad_norm": 2.026848077774048,
        "learning_rate": 0.00015614475042163536,
        "epoch": 0.20984789894302655,
        "step": 1628
    },
    {
        "loss": 2.4558,
        "grad_norm": 1.4358919858932495,
        "learning_rate": 0.00015609439389573844,
        "epoch": 0.2099767981438515,
        "step": 1629
    },
    {
        "loss": 2.2916,
        "grad_norm": 1.3806511163711548,
        "learning_rate": 0.00015604401660623487,
        "epoch": 0.21010569734467646,
        "step": 1630
    },
    {
        "loss": 2.2427,
        "grad_norm": 1.8958977460861206,
        "learning_rate": 0.000155993618571772,
        "epoch": 0.21023459654550142,
        "step": 1631
    },
    {
        "loss": 2.1478,
        "grad_norm": 2.189250946044922,
        "learning_rate": 0.0001559431998110049,
        "epoch": 0.21036349574632637,
        "step": 1632
    },
    {
        "loss": 2.3017,
        "grad_norm": 1.0059747695922852,
        "learning_rate": 0.00015589276034259637,
        "epoch": 0.21049239494715133,
        "step": 1633
    },
    {
        "loss": 1.2185,
        "grad_norm": 2.0465455055236816,
        "learning_rate": 0.00015584230018521677,
        "epoch": 0.21062129414797628,
        "step": 1634
    },
    {
        "loss": 2.3064,
        "grad_norm": 1.3671560287475586,
        "learning_rate": 0.00015579181935754423,
        "epoch": 0.21075019334880124,
        "step": 1635
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.773110032081604,
        "learning_rate": 0.00015574131787826439,
        "epoch": 0.2108790925496262,
        "step": 1636
    },
    {
        "loss": 2.4075,
        "grad_norm": 1.9366754293441772,
        "learning_rate": 0.00015569079576607063,
        "epoch": 0.21100799175045115,
        "step": 1637
    },
    {
        "loss": 2.1572,
        "grad_norm": 1.8612060546875,
        "learning_rate": 0.00015564025303966398,
        "epoch": 0.2111368909512761,
        "step": 1638
    },
    {
        "loss": 2.4528,
        "grad_norm": 1.8339121341705322,
        "learning_rate": 0.00015558968971775308,
        "epoch": 0.21126579015210106,
        "step": 1639
    },
    {
        "loss": 2.1252,
        "grad_norm": 1.818794846534729,
        "learning_rate": 0.00015553910581905415,
        "epoch": 0.21139468935292602,
        "step": 1640
    },
    {
        "loss": 1.0003,
        "grad_norm": 1.9183255434036255,
        "learning_rate": 0.00015548850136229105,
        "epoch": 0.21152358855375097,
        "step": 1641
    },
    {
        "loss": 2.1233,
        "grad_norm": 2.8499374389648438,
        "learning_rate": 0.00015543787636619532,
        "epoch": 0.21165248775457593,
        "step": 1642
    },
    {
        "loss": 2.4753,
        "grad_norm": 1.034775972366333,
        "learning_rate": 0.00015538723084950598,
        "epoch": 0.21178138695540089,
        "step": 1643
    },
    {
        "loss": 2.5717,
        "grad_norm": 2.5241849422454834,
        "learning_rate": 0.00015533656483096973,
        "epoch": 0.21191028615622584,
        "step": 1644
    },
    {
        "loss": 2.4618,
        "grad_norm": 2.1993014812469482,
        "learning_rate": 0.00015528587832934086,
        "epoch": 0.2120391853570508,
        "step": 1645
    },
    {
        "loss": 1.8345,
        "grad_norm": 2.2103686332702637,
        "learning_rate": 0.00015523517136338118,
        "epoch": 0.21216808455787575,
        "step": 1646
    },
    {
        "loss": 2.5585,
        "grad_norm": 1.6902573108673096,
        "learning_rate": 0.00015518444395186016,
        "epoch": 0.2122969837587007,
        "step": 1647
    },
    {
        "loss": 2.238,
        "grad_norm": 1.6582509279251099,
        "learning_rate": 0.00015513369611355477,
        "epoch": 0.21242588295952566,
        "step": 1648
    },
    {
        "loss": 2.0087,
        "grad_norm": 1.500235915184021,
        "learning_rate": 0.00015508292786724952,
        "epoch": 0.21255478216035062,
        "step": 1649
    },
    {
        "loss": 2.3497,
        "grad_norm": 1.223923683166504,
        "learning_rate": 0.00015503213923173662,
        "epoch": 0.21268368136117555,
        "step": 1650
    },
    {
        "loss": 2.1112,
        "grad_norm": 2.0623366832733154,
        "learning_rate": 0.00015498133022581563,
        "epoch": 0.2128125805620005,
        "step": 1651
    },
    {
        "loss": 2.3897,
        "grad_norm": 0.9362038969993591,
        "learning_rate": 0.00015493050086829376,
        "epoch": 0.21294147976282546,
        "step": 1652
    },
    {
        "loss": 2.0199,
        "grad_norm": 1.441096544265747,
        "learning_rate": 0.0001548796511779858,
        "epoch": 0.21307037896365041,
        "step": 1653
    },
    {
        "loss": 2.3767,
        "grad_norm": 1.4438188076019287,
        "learning_rate": 0.00015482878117371396,
        "epoch": 0.21319927816447537,
        "step": 1654
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.0238183736801147,
        "learning_rate": 0.000154777890874308,
        "epoch": 0.21332817736530033,
        "step": 1655
    },
    {
        "loss": 1.5716,
        "grad_norm": 1.8330553770065308,
        "learning_rate": 0.00015472698029860525,
        "epoch": 0.21345707656612528,
        "step": 1656
    },
    {
        "loss": 1.4069,
        "grad_norm": 1.5371001958847046,
        "learning_rate": 0.00015467604946545045,
        "epoch": 0.21358597576695024,
        "step": 1657
    },
    {
        "loss": 2.191,
        "grad_norm": 1.3404580354690552,
        "learning_rate": 0.00015462509839369592,
        "epoch": 0.2137148749677752,
        "step": 1658
    },
    {
        "loss": 2.087,
        "grad_norm": 1.7751189470291138,
        "learning_rate": 0.00015457412710220142,
        "epoch": 0.21384377416860015,
        "step": 1659
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.5492950677871704,
        "learning_rate": 0.00015452313560983426,
        "epoch": 0.2139726733694251,
        "step": 1660
    },
    {
        "loss": 1.8202,
        "grad_norm": 1.3634281158447266,
        "learning_rate": 0.00015447212393546918,
        "epoch": 0.21410157257025006,
        "step": 1661
    },
    {
        "loss": 2.1304,
        "grad_norm": 1.588539481163025,
        "learning_rate": 0.00015442109209798832,
        "epoch": 0.21423047177107502,
        "step": 1662
    },
    {
        "loss": 2.3145,
        "grad_norm": 1.3831201791763306,
        "learning_rate": 0.00015437004011628142,
        "epoch": 0.21435937097189997,
        "step": 1663
    },
    {
        "loss": 2.0632,
        "grad_norm": 1.89664888381958,
        "learning_rate": 0.00015431896800924562,
        "epoch": 0.21448827017272493,
        "step": 1664
    },
    {
        "loss": 2.2032,
        "grad_norm": 2.2372443675994873,
        "learning_rate": 0.00015426787579578548,
        "epoch": 0.21461716937354988,
        "step": 1665
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.9413018226623535,
        "learning_rate": 0.00015421676349481301,
        "epoch": 0.21474606857437484,
        "step": 1666
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.2673841714859009,
        "learning_rate": 0.0001541656311252477,
        "epoch": 0.2148749677751998,
        "step": 1667
    },
    {
        "loss": 2.1006,
        "grad_norm": 1.4989957809448242,
        "learning_rate": 0.00015411447870601646,
        "epoch": 0.21500386697602475,
        "step": 1668
    },
    {
        "loss": 2.3701,
        "grad_norm": 2.060551643371582,
        "learning_rate": 0.00015406330625605355,
        "epoch": 0.2151327661768497,
        "step": 1669
    },
    {
        "loss": 1.8849,
        "grad_norm": 2.067845344543457,
        "learning_rate": 0.00015401211379430073,
        "epoch": 0.21526166537767466,
        "step": 1670
    },
    {
        "loss": 1.1307,
        "grad_norm": 2.13501238822937,
        "learning_rate": 0.0001539609013397071,
        "epoch": 0.21539056457849962,
        "step": 1671
    },
    {
        "loss": 2.038,
        "grad_norm": 1.7352970838546753,
        "learning_rate": 0.00015390966891122923,
        "epoch": 0.21551946377932457,
        "step": 1672
    },
    {
        "loss": 2.1228,
        "grad_norm": 1.6645065546035767,
        "learning_rate": 0.00015385841652783104,
        "epoch": 0.21564836298014953,
        "step": 1673
    },
    {
        "loss": 1.5976,
        "grad_norm": 2.81955623626709,
        "learning_rate": 0.0001538071442084838,
        "epoch": 0.21577726218097448,
        "step": 1674
    },
    {
        "loss": 2.5564,
        "grad_norm": 1.2875480651855469,
        "learning_rate": 0.0001537558519721663,
        "epoch": 0.21590616138179944,
        "step": 1675
    },
    {
        "loss": 1.8002,
        "grad_norm": 1.1356937885284424,
        "learning_rate": 0.00015370453983786452,
        "epoch": 0.2160350605826244,
        "step": 1676
    },
    {
        "loss": 2.3546,
        "grad_norm": 1.274544596672058,
        "learning_rate": 0.0001536532078245719,
        "epoch": 0.21616395978344935,
        "step": 1677
    },
    {
        "loss": 1.8487,
        "grad_norm": 2.078340530395508,
        "learning_rate": 0.00015360185595128924,
        "epoch": 0.2162928589842743,
        "step": 1678
    },
    {
        "loss": 0.9885,
        "grad_norm": 1.9661448001861572,
        "learning_rate": 0.00015355048423702468,
        "epoch": 0.21642175818509926,
        "step": 1679
    },
    {
        "loss": 2.373,
        "grad_norm": 1.7698649168014526,
        "learning_rate": 0.00015349909270079374,
        "epoch": 0.21655065738592422,
        "step": 1680
    },
    {
        "loss": 1.8422,
        "grad_norm": 1.7239629030227661,
        "learning_rate": 0.0001534476813616192,
        "epoch": 0.21667955658674917,
        "step": 1681
    },
    {
        "loss": 1.9579,
        "grad_norm": 2.301344871520996,
        "learning_rate": 0.00015339625023853118,
        "epoch": 0.21680845578757413,
        "step": 1682
    },
    {
        "loss": 2.039,
        "grad_norm": 1.5368304252624512,
        "learning_rate": 0.00015334479935056727,
        "epoch": 0.21693735498839908,
        "step": 1683
    },
    {
        "loss": 0.9982,
        "grad_norm": 1.3043501377105713,
        "learning_rate": 0.00015329332871677213,
        "epoch": 0.21706625418922404,
        "step": 1684
    },
    {
        "loss": 1.8484,
        "grad_norm": 1.8789879083633423,
        "learning_rate": 0.00015324183835619795,
        "epoch": 0.217195153390049,
        "step": 1685
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.9358917474746704,
        "learning_rate": 0.00015319032828790407,
        "epoch": 0.21732405259087392,
        "step": 1686
    },
    {
        "loss": 2.3103,
        "grad_norm": 1.6176831722259521,
        "learning_rate": 0.00015313879853095722,
        "epoch": 0.21745295179169888,
        "step": 1687
    },
    {
        "loss": 1.9498,
        "grad_norm": 1.323272466659546,
        "learning_rate": 0.00015308724910443138,
        "epoch": 0.21758185099252383,
        "step": 1688
    },
    {
        "loss": 1.8179,
        "grad_norm": 0.8455782532691956,
        "learning_rate": 0.00015303568002740783,
        "epoch": 0.2177107501933488,
        "step": 1689
    },
    {
        "loss": 1.6254,
        "grad_norm": 1.7804291248321533,
        "learning_rate": 0.0001529840913189751,
        "epoch": 0.21783964939417375,
        "step": 1690
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.112800121307373,
        "learning_rate": 0.00015293248299822894,
        "epoch": 0.2179685485949987,
        "step": 1691
    },
    {
        "loss": 2.4148,
        "grad_norm": 1.3606317043304443,
        "learning_rate": 0.00015288085508427253,
        "epoch": 0.21809744779582366,
        "step": 1692
    },
    {
        "loss": 1.4597,
        "grad_norm": 2.2803125381469727,
        "learning_rate": 0.0001528292075962161,
        "epoch": 0.2182263469966486,
        "step": 1693
    },
    {
        "loss": 1.9277,
        "grad_norm": 1.291700005531311,
        "learning_rate": 0.00015277754055317724,
        "epoch": 0.21835524619747357,
        "step": 1694
    },
    {
        "loss": 2.0445,
        "grad_norm": 2.292945623397827,
        "learning_rate": 0.0001527258539742807,
        "epoch": 0.21848414539829852,
        "step": 1695
    },
    {
        "loss": 2.1104,
        "grad_norm": 2.3945157527923584,
        "learning_rate": 0.00015267414787865864,
        "epoch": 0.21861304459912348,
        "step": 1696
    },
    {
        "loss": 2.1968,
        "grad_norm": 2.375922679901123,
        "learning_rate": 0.00015262242228545017,
        "epoch": 0.21874194379994844,
        "step": 1697
    },
    {
        "loss": 2.4034,
        "grad_norm": 1.3486391305923462,
        "learning_rate": 0.0001525706772138019,
        "epoch": 0.2188708430007734,
        "step": 1698
    },
    {
        "loss": 2.2539,
        "grad_norm": 1.503170132637024,
        "learning_rate": 0.0001525189126828674,
        "epoch": 0.21899974220159835,
        "step": 1699
    },
    {
        "loss": 2.8195,
        "grad_norm": 1.3622575998306274,
        "learning_rate": 0.0001524671287118076,
        "epoch": 0.2191286414024233,
        "step": 1700
    },
    {
        "loss": 2.055,
        "grad_norm": 1.628346562385559,
        "learning_rate": 0.00015241532531979065,
        "epoch": 0.21925754060324826,
        "step": 1701
    },
    {
        "loss": 1.8151,
        "grad_norm": 2.0024502277374268,
        "learning_rate": 0.0001523635025259917,
        "epoch": 0.2193864398040732,
        "step": 1702
    },
    {
        "loss": 2.608,
        "grad_norm": 1.6142297983169556,
        "learning_rate": 0.00015231166034959323,
        "epoch": 0.21951533900489817,
        "step": 1703
    },
    {
        "loss": 2.8337,
        "grad_norm": 0.9821321964263916,
        "learning_rate": 0.00015225979880978497,
        "epoch": 0.21964423820572312,
        "step": 1704
    },
    {
        "loss": 2.0722,
        "grad_norm": 2.1051948070526123,
        "learning_rate": 0.0001522079179257636,
        "epoch": 0.21977313740654808,
        "step": 1705
    },
    {
        "loss": 2.5424,
        "grad_norm": 1.2533425092697144,
        "learning_rate": 0.0001521560177167332,
        "epoch": 0.21990203660737304,
        "step": 1706
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.6364924907684326,
        "learning_rate": 0.00015210409820190472,
        "epoch": 0.220030935808198,
        "step": 1707
    },
    {
        "loss": 2.326,
        "grad_norm": 1.7486884593963623,
        "learning_rate": 0.00015205215940049653,
        "epoch": 0.22015983500902295,
        "step": 1708
    },
    {
        "loss": 2.3508,
        "grad_norm": 2.175684928894043,
        "learning_rate": 0.000152000201331734,
        "epoch": 0.2202887342098479,
        "step": 1709
    },
    {
        "loss": 2.1324,
        "grad_norm": 1.8436201810836792,
        "learning_rate": 0.00015194822401484961,
        "epoch": 0.22041763341067286,
        "step": 1710
    },
    {
        "loss": 1.2053,
        "grad_norm": 2.083333730697632,
        "learning_rate": 0.00015189622746908312,
        "epoch": 0.22054653261149781,
        "step": 1711
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.1112751960754395,
        "learning_rate": 0.00015184421171368114,
        "epoch": 0.22067543181232277,
        "step": 1712
    },
    {
        "loss": 1.8959,
        "grad_norm": 1.1782505512237549,
        "learning_rate": 0.00015179217676789772,
        "epoch": 0.22080433101314773,
        "step": 1713
    },
    {
        "loss": 2.2936,
        "grad_norm": 1.732287883758545,
        "learning_rate": 0.0001517401226509937,
        "epoch": 0.22093323021397268,
        "step": 1714
    },
    {
        "loss": 1.4141,
        "grad_norm": 2.101001739501953,
        "learning_rate": 0.00015168804938223723,
        "epoch": 0.22106212941479764,
        "step": 1715
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.1716811656951904,
        "learning_rate": 0.00015163595698090352,
        "epoch": 0.2211910286156226,
        "step": 1716
    },
    {
        "loss": 2.4204,
        "grad_norm": 1.2062216997146606,
        "learning_rate": 0.0001515838454662747,
        "epoch": 0.22131992781644755,
        "step": 1717
    },
    {
        "loss": 1.7373,
        "grad_norm": 1.6781444549560547,
        "learning_rate": 0.0001515317148576402,
        "epoch": 0.2214488270172725,
        "step": 1718
    },
    {
        "loss": 1.5063,
        "grad_norm": 1.9655930995941162,
        "learning_rate": 0.00015147956517429635,
        "epoch": 0.22157772621809746,
        "step": 1719
    },
    {
        "loss": 2.4456,
        "grad_norm": 1.5695537328720093,
        "learning_rate": 0.00015142739643554662,
        "epoch": 0.22170662541892242,
        "step": 1720
    },
    {
        "loss": 1.767,
        "grad_norm": 1.5725306272506714,
        "learning_rate": 0.00015137520866070157,
        "epoch": 0.22183552461974737,
        "step": 1721
    },
    {
        "loss": 2.4141,
        "grad_norm": 1.6811057329177856,
        "learning_rate": 0.00015132300186907868,
        "epoch": 0.2219644238205723,
        "step": 1722
    },
    {
        "loss": 1.6218,
        "grad_norm": 1.5830894708633423,
        "learning_rate": 0.00015127077608000257,
        "epoch": 0.22209332302139725,
        "step": 1723
    },
    {
        "loss": 1.8537,
        "grad_norm": 1.952102780342102,
        "learning_rate": 0.00015121853131280484,
        "epoch": 0.2222222222222222,
        "step": 1724
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.293837308883667,
        "learning_rate": 0.00015116626758682422,
        "epoch": 0.22235112142304717,
        "step": 1725
    },
    {
        "loss": 2.2683,
        "grad_norm": 1.1520220041275024,
        "learning_rate": 0.00015111398492140626,
        "epoch": 0.22248002062387212,
        "step": 1726
    },
    {
        "loss": 1.9117,
        "grad_norm": 2.2439613342285156,
        "learning_rate": 0.00015106168333590373,
        "epoch": 0.22260891982469708,
        "step": 1727
    },
    {
        "loss": 1.7184,
        "grad_norm": 2.1716904640197754,
        "learning_rate": 0.0001510093628496763,
        "epoch": 0.22273781902552203,
        "step": 1728
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.7645766735076904,
        "learning_rate": 0.00015095702348209058,
        "epoch": 0.222866718226347,
        "step": 1729
    },
    {
        "loss": 1.6861,
        "grad_norm": 1.9864718914031982,
        "learning_rate": 0.0001509046652525203,
        "epoch": 0.22299561742717194,
        "step": 1730
    },
    {
        "loss": 2.0929,
        "grad_norm": 1.339857578277588,
        "learning_rate": 0.00015085228818034608,
        "epoch": 0.2231245166279969,
        "step": 1731
    },
    {
        "loss": 2.3695,
        "grad_norm": 1.4810134172439575,
        "learning_rate": 0.00015079989228495558,
        "epoch": 0.22325341582882186,
        "step": 1732
    },
    {
        "loss": 2.2392,
        "grad_norm": 2.010420560836792,
        "learning_rate": 0.00015074747758574332,
        "epoch": 0.2233823150296468,
        "step": 1733
    },
    {
        "loss": 1.7755,
        "grad_norm": 1.2690448760986328,
        "learning_rate": 0.00015069504410211089,
        "epoch": 0.22351121423047177,
        "step": 1734
    },
    {
        "loss": 1.6778,
        "grad_norm": 2.5223405361175537,
        "learning_rate": 0.0001506425918534668,
        "epoch": 0.22364011343129672,
        "step": 1735
    },
    {
        "loss": 2.3394,
        "grad_norm": 1.093072533607483,
        "learning_rate": 0.00015059012085922652,
        "epoch": 0.22376901263212168,
        "step": 1736
    },
    {
        "loss": 2.2539,
        "grad_norm": 1.2656835317611694,
        "learning_rate": 0.0001505376311388124,
        "epoch": 0.22389791183294663,
        "step": 1737
    },
    {
        "loss": 2.3398,
        "grad_norm": 0.970930814743042,
        "learning_rate": 0.00015048512271165375,
        "epoch": 0.2240268110337716,
        "step": 1738
    },
    {
        "loss": 1.5897,
        "grad_norm": 1.6177507638931274,
        "learning_rate": 0.00015043259559718684,
        "epoch": 0.22415571023459654,
        "step": 1739
    },
    {
        "loss": 2.3195,
        "grad_norm": 1.1841448545455933,
        "learning_rate": 0.00015038004981485488,
        "epoch": 0.2242846094354215,
        "step": 1740
    },
    {
        "loss": 2.4699,
        "grad_norm": 1.190209984779358,
        "learning_rate": 0.0001503274853841078,
        "epoch": 0.22441350863624646,
        "step": 1741
    },
    {
        "loss": 2.1679,
        "grad_norm": 1.5843919515609741,
        "learning_rate": 0.00015027490232440274,
        "epoch": 0.2245424078370714,
        "step": 1742
    },
    {
        "loss": 2.649,
        "grad_norm": 1.4427227973937988,
        "learning_rate": 0.0001502223006552034,
        "epoch": 0.22467130703789637,
        "step": 1743
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.6562108993530273,
        "learning_rate": 0.0001501696803959807,
        "epoch": 0.22480020623872132,
        "step": 1744
    },
    {
        "loss": 2.2292,
        "grad_norm": 1.1050771474838257,
        "learning_rate": 0.00015011704156621218,
        "epoch": 0.22492910543954628,
        "step": 1745
    },
    {
        "loss": 1.2173,
        "grad_norm": 1.863756775856018,
        "learning_rate": 0.00015006438418538242,
        "epoch": 0.22505800464037123,
        "step": 1746
    },
    {
        "loss": 2.1541,
        "grad_norm": 1.5953348875045776,
        "learning_rate": 0.00015001170827298271,
        "epoch": 0.2251869038411962,
        "step": 1747
    },
    {
        "loss": 1.3014,
        "grad_norm": 1.8152705430984497,
        "learning_rate": 0.00014995901384851136,
        "epoch": 0.22531580304202115,
        "step": 1748
    },
    {
        "loss": 1.6427,
        "grad_norm": 1.2959182262420654,
        "learning_rate": 0.00014990630093147348,
        "epoch": 0.2254447022428461,
        "step": 1749
    },
    {
        "loss": 2.3419,
        "grad_norm": 1.2170791625976562,
        "learning_rate": 0.0001498535695413809,
        "epoch": 0.22557360144367106,
        "step": 1750
    },
    {
        "loss": 1.8565,
        "grad_norm": 2.223968505859375,
        "learning_rate": 0.00014980081969775253,
        "epoch": 0.225702500644496,
        "step": 1751
    },
    {
        "loss": 1.9919,
        "grad_norm": 1.6158931255340576,
        "learning_rate": 0.00014974805142011386,
        "epoch": 0.22583139984532097,
        "step": 1752
    },
    {
        "loss": 2.033,
        "grad_norm": 1.5142385959625244,
        "learning_rate": 0.0001496952647279974,
        "epoch": 0.22596029904614592,
        "step": 1753
    },
    {
        "loss": 2.1385,
        "grad_norm": 1.3419893980026245,
        "learning_rate": 0.00014964245964094232,
        "epoch": 0.22608919824697088,
        "step": 1754
    },
    {
        "loss": 2.062,
        "grad_norm": 1.5616530179977417,
        "learning_rate": 0.00014958963617849475,
        "epoch": 0.22621809744779584,
        "step": 1755
    },
    {
        "loss": 2.4098,
        "grad_norm": 1.5115619897842407,
        "learning_rate": 0.00014953679436020755,
        "epoch": 0.2263469966486208,
        "step": 1756
    },
    {
        "loss": 1.5464,
        "grad_norm": 1.8659157752990723,
        "learning_rate": 0.0001494839342056403,
        "epoch": 0.22647589584944575,
        "step": 1757
    },
    {
        "loss": 2.3583,
        "grad_norm": 1.7079871892929077,
        "learning_rate": 0.00014943105573435946,
        "epoch": 0.2266047950502707,
        "step": 1758
    },
    {
        "loss": 1.3538,
        "grad_norm": 2.8979508876800537,
        "learning_rate": 0.00014937815896593826,
        "epoch": 0.22673369425109563,
        "step": 1759
    },
    {
        "loss": 2.2985,
        "grad_norm": 2.0348567962646484,
        "learning_rate": 0.00014932524391995667,
        "epoch": 0.22686259345192059,
        "step": 1760
    },
    {
        "loss": 1.74,
        "grad_norm": 2.075723171234131,
        "learning_rate": 0.0001492723106160015,
        "epoch": 0.22699149265274554,
        "step": 1761
    },
    {
        "loss": 2.243,
        "grad_norm": 1.4036129713058472,
        "learning_rate": 0.00014921935907366622,
        "epoch": 0.2271203918535705,
        "step": 1762
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.187164306640625,
        "learning_rate": 0.0001491663893125511,
        "epoch": 0.22724929105439545,
        "step": 1763
    },
    {
        "loss": 1.846,
        "grad_norm": 1.5614216327667236,
        "learning_rate": 0.00014911340135226317,
        "epoch": 0.2273781902552204,
        "step": 1764
    },
    {
        "loss": 1.7556,
        "grad_norm": 2.3894457817077637,
        "learning_rate": 0.00014906039521241614,
        "epoch": 0.22750708945604536,
        "step": 1765
    },
    {
        "loss": 1.6413,
        "grad_norm": 2.2343080043792725,
        "learning_rate": 0.00014900737091263052,
        "epoch": 0.22763598865687032,
        "step": 1766
    },
    {
        "loss": 1.7338,
        "grad_norm": 2.408304214477539,
        "learning_rate": 0.00014895432847253349,
        "epoch": 0.22776488785769528,
        "step": 1767
    },
    {
        "loss": 1.2334,
        "grad_norm": 2.302037000656128,
        "learning_rate": 0.00014890126791175897,
        "epoch": 0.22789378705852023,
        "step": 1768
    },
    {
        "loss": 2.1076,
        "grad_norm": 1.3926218748092651,
        "learning_rate": 0.00014884818924994754,
        "epoch": 0.2280226862593452,
        "step": 1769
    },
    {
        "loss": 2.639,
        "grad_norm": 1.446873426437378,
        "learning_rate": 0.00014879509250674658,
        "epoch": 0.22815158546017014,
        "step": 1770
    },
    {
        "loss": 2.2236,
        "grad_norm": 1.1184908151626587,
        "learning_rate": 0.00014874197770181006,
        "epoch": 0.2282804846609951,
        "step": 1771
    },
    {
        "loss": 1.9913,
        "grad_norm": 2.5317702293395996,
        "learning_rate": 0.00014868884485479868,
        "epoch": 0.22840938386182005,
        "step": 1772
    },
    {
        "loss": 1.9996,
        "grad_norm": 1.4446810483932495,
        "learning_rate": 0.00014863569398537984,
        "epoch": 0.228538283062645,
        "step": 1773
    },
    {
        "loss": 2.5395,
        "grad_norm": 1.659189224243164,
        "learning_rate": 0.00014858252511322755,
        "epoch": 0.22866718226346996,
        "step": 1774
    },
    {
        "loss": 2.1604,
        "grad_norm": 1.1411818265914917,
        "learning_rate": 0.00014852933825802259,
        "epoch": 0.22879608146429492,
        "step": 1775
    },
    {
        "loss": 2.0499,
        "grad_norm": 1.384560465812683,
        "learning_rate": 0.00014847613343945226,
        "epoch": 0.22892498066511988,
        "step": 1776
    },
    {
        "loss": 2.3483,
        "grad_norm": 1.2758394479751587,
        "learning_rate": 0.00014842291067721056,
        "epoch": 0.22905387986594483,
        "step": 1777
    },
    {
        "loss": 1.8444,
        "grad_norm": 1.8054193258285522,
        "learning_rate": 0.00014836966999099823,
        "epoch": 0.2291827790667698,
        "step": 1778
    },
    {
        "loss": 1.0148,
        "grad_norm": 1.4079060554504395,
        "learning_rate": 0.00014831641140052252,
        "epoch": 0.22931167826759474,
        "step": 1779
    },
    {
        "loss": 1.7893,
        "grad_norm": 1.964208722114563,
        "learning_rate": 0.00014826313492549736,
        "epoch": 0.2294405774684197,
        "step": 1780
    },
    {
        "loss": 1.7613,
        "grad_norm": 2.312190532684326,
        "learning_rate": 0.00014820984058564328,
        "epoch": 0.22956947666924465,
        "step": 1781
    },
    {
        "loss": 2.0778,
        "grad_norm": 1.7866652011871338,
        "learning_rate": 0.00014815652840068736,
        "epoch": 0.2296983758700696,
        "step": 1782
    },
    {
        "loss": 0.8771,
        "grad_norm": 2.6904029846191406,
        "learning_rate": 0.0001481031983903635,
        "epoch": 0.22982727507089457,
        "step": 1783
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.097791075706482,
        "learning_rate": 0.000148049850574412,
        "epoch": 0.22995617427171952,
        "step": 1784
    },
    {
        "loss": 1.3824,
        "grad_norm": 2.331017017364502,
        "learning_rate": 0.00014799648497257977,
        "epoch": 0.23008507347254448,
        "step": 1785
    },
    {
        "loss": 2.2314,
        "grad_norm": 1.3398394584655762,
        "learning_rate": 0.00014794310160462035,
        "epoch": 0.23021397267336943,
        "step": 1786
    },
    {
        "loss": 2.6246,
        "grad_norm": 1.160925269126892,
        "learning_rate": 0.0001478897004902939,
        "epoch": 0.2303428718741944,
        "step": 1787
    },
    {
        "loss": 2.5795,
        "grad_norm": 1.3109469413757324,
        "learning_rate": 0.00014783628164936706,
        "epoch": 0.23047177107501934,
        "step": 1788
    },
    {
        "loss": 1.3462,
        "grad_norm": 2.28945255279541,
        "learning_rate": 0.00014778284510161307,
        "epoch": 0.2306006702758443,
        "step": 1789
    },
    {
        "loss": 1.0647,
        "grad_norm": 2.0933103561401367,
        "learning_rate": 0.00014772939086681172,
        "epoch": 0.23072956947666926,
        "step": 1790
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.5231740474700928,
        "learning_rate": 0.00014767591896474935,
        "epoch": 0.2308584686774942,
        "step": 1791
    },
    {
        "loss": 2.081,
        "grad_norm": 1.7135589122772217,
        "learning_rate": 0.00014762242941521883,
        "epoch": 0.23098736787831917,
        "step": 1792
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.183737277984619,
        "learning_rate": 0.0001475689222380196,
        "epoch": 0.23111626707914412,
        "step": 1793
    },
    {
        "loss": 2.419,
        "grad_norm": 1.035454273223877,
        "learning_rate": 0.00014751539745295756,
        "epoch": 0.23124516627996908,
        "step": 1794
    },
    {
        "loss": 1.4466,
        "grad_norm": 1.6003273725509644,
        "learning_rate": 0.0001474618550798452,
        "epoch": 0.231374065480794,
        "step": 1795
    },
    {
        "loss": 1.3553,
        "grad_norm": 1.8410454988479614,
        "learning_rate": 0.0001474082951385015,
        "epoch": 0.23150296468161896,
        "step": 1796
    },
    {
        "loss": 1.7478,
        "grad_norm": 2.06891131401062,
        "learning_rate": 0.00014735471764875185,
        "epoch": 0.23163186388244392,
        "step": 1797
    },
    {
        "loss": 1.9943,
        "grad_norm": 1.3014694452285767,
        "learning_rate": 0.00014730112263042824,
        "epoch": 0.23176076308326887,
        "step": 1798
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.8939740657806396,
        "learning_rate": 0.00014724751010336914,
        "epoch": 0.23188966228409383,
        "step": 1799
    },
    {
        "loss": 1.8149,
        "grad_norm": 2.03244686126709,
        "learning_rate": 0.0001471938800874195,
        "epoch": 0.23201856148491878,
        "step": 1800
    },
    {
        "loss": 2.1507,
        "grad_norm": 2.2604820728302,
        "learning_rate": 0.00014714023260243073,
        "epoch": 0.23214746068574374,
        "step": 1801
    },
    {
        "loss": 2.4511,
        "grad_norm": 1.6113091707229614,
        "learning_rate": 0.00014708656766826065,
        "epoch": 0.2322763598865687,
        "step": 1802
    },
    {
        "loss": 2.0579,
        "grad_norm": 1.2810704708099365,
        "learning_rate": 0.0001470328853047736,
        "epoch": 0.23240525908739365,
        "step": 1803
    },
    {
        "loss": 1.984,
        "grad_norm": 2.2804722785949707,
        "learning_rate": 0.0001469791855318404,
        "epoch": 0.2325341582882186,
        "step": 1804
    },
    {
        "loss": 1.8323,
        "grad_norm": 1.6989518404006958,
        "learning_rate": 0.00014692546836933825,
        "epoch": 0.23266305748904356,
        "step": 1805
    },
    {
        "loss": 1.5848,
        "grad_norm": 1.933577299118042,
        "learning_rate": 0.00014687173383715082,
        "epoch": 0.23279195668986852,
        "step": 1806
    },
    {
        "loss": 2.3658,
        "grad_norm": 1.1157011985778809,
        "learning_rate": 0.00014681798195516818,
        "epoch": 0.23292085589069347,
        "step": 1807
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.1535288095474243,
        "learning_rate": 0.0001467642127432869,
        "epoch": 0.23304975509151843,
        "step": 1808
    },
    {
        "loss": 0.7278,
        "grad_norm": 1.69559645652771,
        "learning_rate": 0.00014671042622140985,
        "epoch": 0.23317865429234338,
        "step": 1809
    },
    {
        "loss": 2.3784,
        "grad_norm": 1.0190263986587524,
        "learning_rate": 0.0001466566224094464,
        "epoch": 0.23330755349316834,
        "step": 1810
    },
    {
        "loss": 2.2851,
        "grad_norm": 1.873205304145813,
        "learning_rate": 0.00014660280132731228,
        "epoch": 0.2334364526939933,
        "step": 1811
    },
    {
        "loss": 2.4481,
        "grad_norm": 1.0937159061431885,
        "learning_rate": 0.00014654896299492963,
        "epoch": 0.23356535189481825,
        "step": 1812
    },
    {
        "loss": 2.2052,
        "grad_norm": 1.696184515953064,
        "learning_rate": 0.00014649510743222692,
        "epoch": 0.2336942510956432,
        "step": 1813
    },
    {
        "loss": 2.4637,
        "grad_norm": 1.2548922300338745,
        "learning_rate": 0.0001464412346591391,
        "epoch": 0.23382315029646816,
        "step": 1814
    },
    {
        "loss": 1.6345,
        "grad_norm": 1.4743685722351074,
        "learning_rate": 0.0001463873446956074,
        "epoch": 0.23395204949729312,
        "step": 1815
    },
    {
        "loss": 2.4078,
        "grad_norm": 1.4977177381515503,
        "learning_rate": 0.00014633343756157945,
        "epoch": 0.23408094869811807,
        "step": 1816
    },
    {
        "loss": 1.576,
        "grad_norm": 1.5943937301635742,
        "learning_rate": 0.00014627951327700927,
        "epoch": 0.23420984789894303,
        "step": 1817
    },
    {
        "loss": 1.317,
        "grad_norm": 1.8604792356491089,
        "learning_rate": 0.00014622557186185712,
        "epoch": 0.23433874709976799,
        "step": 1818
    },
    {
        "loss": 2.3012,
        "grad_norm": 1.1301984786987305,
        "learning_rate": 0.00014617161333608974,
        "epoch": 0.23446764630059294,
        "step": 1819
    },
    {
        "loss": 1.8917,
        "grad_norm": 2.356694221496582,
        "learning_rate": 0.0001461176377196801,
        "epoch": 0.2345965455014179,
        "step": 1820
    },
    {
        "loss": 1.2238,
        "grad_norm": 2.3695476055145264,
        "learning_rate": 0.00014606364503260753,
        "epoch": 0.23472544470224285,
        "step": 1821
    },
    {
        "loss": 1.8192,
        "grad_norm": 2.277785539627075,
        "learning_rate": 0.00014600963529485766,
        "epoch": 0.2348543439030678,
        "step": 1822
    },
    {
        "loss": 2.4354,
        "grad_norm": 1.1871764659881592,
        "learning_rate": 0.00014595560852642247,
        "epoch": 0.23498324310389276,
        "step": 1823
    },
    {
        "loss": 2.1734,
        "grad_norm": 1.8832911252975464,
        "learning_rate": 0.0001459015647473003,
        "epoch": 0.23511214230471772,
        "step": 1824
    },
    {
        "loss": 2.0704,
        "grad_norm": 1.5254133939743042,
        "learning_rate": 0.0001458475039774956,
        "epoch": 0.23524104150554268,
        "step": 1825
    },
    {
        "loss": 1.815,
        "grad_norm": 2.2886555194854736,
        "learning_rate": 0.00014579342623701925,
        "epoch": 0.23536994070636763,
        "step": 1826
    },
    {
        "loss": 1.7824,
        "grad_norm": 1.6907719373703003,
        "learning_rate": 0.00014573933154588838,
        "epoch": 0.2354988399071926,
        "step": 1827
    },
    {
        "loss": 2.0468,
        "grad_norm": 1.5615702867507935,
        "learning_rate": 0.00014568521992412646,
        "epoch": 0.23562773910801754,
        "step": 1828
    },
    {
        "loss": 2.2629,
        "grad_norm": 1.8324522972106934,
        "learning_rate": 0.00014563109139176312,
        "epoch": 0.2357566383088425,
        "step": 1829
    },
    {
        "loss": 2.4186,
        "grad_norm": 1.6465201377868652,
        "learning_rate": 0.00014557694596883426,
        "epoch": 0.23588553750966745,
        "step": 1830
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.7634930610656738,
        "learning_rate": 0.0001455227836753821,
        "epoch": 0.23601443671049238,
        "step": 1831
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.3141798973083496,
        "learning_rate": 0.0001454686045314551,
        "epoch": 0.23614333591131734,
        "step": 1832
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.5001718997955322,
        "learning_rate": 0.00014541440855710786,
        "epoch": 0.2362722351121423,
        "step": 1833
    },
    {
        "loss": 2.0326,
        "grad_norm": 1.6901564598083496,
        "learning_rate": 0.00014536019577240133,
        "epoch": 0.23640113431296725,
        "step": 1834
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.334234356880188,
        "learning_rate": 0.0001453059661974026,
        "epoch": 0.2365300335137922,
        "step": 1835
    },
    {
        "loss": 1.9184,
        "grad_norm": 2.7352542877197266,
        "learning_rate": 0.00014525171985218507,
        "epoch": 0.23665893271461716,
        "step": 1836
    },
    {
        "loss": 1.9566,
        "grad_norm": 1.1618531942367554,
        "learning_rate": 0.0001451974567568282,
        "epoch": 0.23678783191544212,
        "step": 1837
    },
    {
        "loss": 1.8083,
        "grad_norm": 1.7589839696884155,
        "learning_rate": 0.00014514317693141776,
        "epoch": 0.23691673111626707,
        "step": 1838
    },
    {
        "loss": 1.6483,
        "grad_norm": 2.1268632411956787,
        "learning_rate": 0.0001450888803960457,
        "epoch": 0.23704563031709203,
        "step": 1839
    },
    {
        "loss": 2.0195,
        "grad_norm": 1.126098394393921,
        "learning_rate": 0.0001450345671708101,
        "epoch": 0.23717452951791698,
        "step": 1840
    },
    {
        "loss": 2.0913,
        "grad_norm": 2.0554187297821045,
        "learning_rate": 0.0001449802372758153,
        "epoch": 0.23730342871874194,
        "step": 1841
    },
    {
        "loss": 1.9873,
        "grad_norm": 1.4296680688858032,
        "learning_rate": 0.0001449258907311718,
        "epoch": 0.2374323279195669,
        "step": 1842
    },
    {
        "loss": 2.1569,
        "grad_norm": 1.5175120830535889,
        "learning_rate": 0.00014487152755699616,
        "epoch": 0.23756122712039185,
        "step": 1843
    },
    {
        "loss": 2.0674,
        "grad_norm": 1.394822120666504,
        "learning_rate": 0.0001448171477734112,
        "epoch": 0.2376901263212168,
        "step": 1844
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.3806989192962646,
        "learning_rate": 0.00014476275140054584,
        "epoch": 0.23781902552204176,
        "step": 1845
    },
    {
        "loss": 1.7095,
        "grad_norm": 2.3958778381347656,
        "learning_rate": 0.00014470833845853516,
        "epoch": 0.23794792472286672,
        "step": 1846
    },
    {
        "loss": 2.5813,
        "grad_norm": 1.5092875957489014,
        "learning_rate": 0.00014465390896752043,
        "epoch": 0.23807682392369167,
        "step": 1847
    },
    {
        "loss": 2.2355,
        "grad_norm": 1.7425469160079956,
        "learning_rate": 0.0001445994629476489,
        "epoch": 0.23820572312451663,
        "step": 1848
    },
    {
        "loss": 2.4803,
        "grad_norm": 1.2442772388458252,
        "learning_rate": 0.00014454500041907406,
        "epoch": 0.23833462232534158,
        "step": 1849
    },
    {
        "loss": 2.1524,
        "grad_norm": 1.9496045112609863,
        "learning_rate": 0.00014449052140195546,
        "epoch": 0.23846352152616654,
        "step": 1850
    },
    {
        "loss": 2.6494,
        "grad_norm": 1.9118595123291016,
        "learning_rate": 0.00014443602591645882,
        "epoch": 0.2385924207269915,
        "step": 1851
    },
    {
        "loss": 2.1555,
        "grad_norm": 1.3335636854171753,
        "learning_rate": 0.00014438151398275584,
        "epoch": 0.23872131992781645,
        "step": 1852
    },
    {
        "loss": 2.1587,
        "grad_norm": 1.8904350996017456,
        "learning_rate": 0.00014432698562102434,
        "epoch": 0.2388502191286414,
        "step": 1853
    },
    {
        "loss": 1.2698,
        "grad_norm": 1.257673978805542,
        "learning_rate": 0.00014427244085144837,
        "epoch": 0.23897911832946636,
        "step": 1854
    },
    {
        "loss": 1.9195,
        "grad_norm": 1.7688690423965454,
        "learning_rate": 0.00014421787969421784,
        "epoch": 0.23910801753029132,
        "step": 1855
    },
    {
        "loss": 2.5494,
        "grad_norm": 1.110274076461792,
        "learning_rate": 0.00014416330216952887,
        "epoch": 0.23923691673111627,
        "step": 1856
    },
    {
        "loss": 2.5427,
        "grad_norm": 1.2226990461349487,
        "learning_rate": 0.00014410870829758356,
        "epoch": 0.23936581593194123,
        "step": 1857
    },
    {
        "loss": 1.8723,
        "grad_norm": 1.9686871767044067,
        "learning_rate": 0.0001440540980985901,
        "epoch": 0.23949471513276618,
        "step": 1858
    },
    {
        "loss": 2.7155,
        "grad_norm": 1.6888799667358398,
        "learning_rate": 0.00014399947159276277,
        "epoch": 0.23962361433359114,
        "step": 1859
    },
    {
        "loss": 2.1413,
        "grad_norm": 1.728596568107605,
        "learning_rate": 0.00014394482880032174,
        "epoch": 0.2397525135344161,
        "step": 1860
    },
    {
        "loss": 2.139,
        "grad_norm": 1.0647234916687012,
        "learning_rate": 0.00014389016974149333,
        "epoch": 0.23988141273524105,
        "step": 1861
    },
    {
        "loss": 2.4147,
        "grad_norm": 1.2578794956207275,
        "learning_rate": 0.00014383549443650986,
        "epoch": 0.240010311936066,
        "step": 1862
    },
    {
        "loss": 2.1128,
        "grad_norm": 1.7470475435256958,
        "learning_rate": 0.00014378080290560966,
        "epoch": 0.24013921113689096,
        "step": 1863
    },
    {
        "loss": 0.8596,
        "grad_norm": 2.714446544647217,
        "learning_rate": 0.00014372609516903707,
        "epoch": 0.24026811033771592,
        "step": 1864
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.7072508335113525,
        "learning_rate": 0.0001436713712470424,
        "epoch": 0.24039700953854087,
        "step": 1865
    },
    {
        "loss": 2.0796,
        "grad_norm": 1.6496213674545288,
        "learning_rate": 0.00014361663115988198,
        "epoch": 0.24052590873936583,
        "step": 1866
    },
    {
        "loss": 1.899,
        "grad_norm": 1.6446447372436523,
        "learning_rate": 0.0001435618749278181,
        "epoch": 0.24065480794019076,
        "step": 1867
    },
    {
        "loss": 1.8821,
        "grad_norm": 2.017946720123291,
        "learning_rate": 0.00014350710257111904,
        "epoch": 0.2407837071410157,
        "step": 1868
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.3115432262420654,
        "learning_rate": 0.00014345231411005906,
        "epoch": 0.24091260634184067,
        "step": 1869
    },
    {
        "loss": 1.77,
        "grad_norm": 1.321629524230957,
        "learning_rate": 0.00014339750956491836,
        "epoch": 0.24104150554266562,
        "step": 1870
    },
    {
        "loss": 1.739,
        "grad_norm": 2.5224204063415527,
        "learning_rate": 0.00014334268895598308,
        "epoch": 0.24117040474349058,
        "step": 1871
    },
    {
        "loss": 2.0337,
        "grad_norm": 1.699542760848999,
        "learning_rate": 0.0001432878523035454,
        "epoch": 0.24129930394431554,
        "step": 1872
    },
    {
        "loss": 1.5385,
        "grad_norm": 1.4906202554702759,
        "learning_rate": 0.00014323299962790332,
        "epoch": 0.2414282031451405,
        "step": 1873
    },
    {
        "loss": 2.0465,
        "grad_norm": 1.7838929891586304,
        "learning_rate": 0.0001431781309493608,
        "epoch": 0.24155710234596545,
        "step": 1874
    },
    {
        "loss": 2.2873,
        "grad_norm": 2.0280065536499023,
        "learning_rate": 0.00014312324628822779,
        "epoch": 0.2416860015467904,
        "step": 1875
    },
    {
        "loss": 1.5767,
        "grad_norm": 3.736104726791382,
        "learning_rate": 0.00014306834566482008,
        "epoch": 0.24181490074761536,
        "step": 1876
    },
    {
        "loss": 1.9748,
        "grad_norm": 2.7172279357910156,
        "learning_rate": 0.0001430134290994594,
        "epoch": 0.2419437999484403,
        "step": 1877
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.7981793880462646,
        "learning_rate": 0.00014295849661247336,
        "epoch": 0.24207269914926527,
        "step": 1878
    },
    {
        "loss": 1.6835,
        "grad_norm": 2.016214370727539,
        "learning_rate": 0.00014290354822419553,
        "epoch": 0.24220159835009022,
        "step": 1879
    },
    {
        "loss": 2.1086,
        "grad_norm": 1.4916704893112183,
        "learning_rate": 0.00014284858395496522,
        "epoch": 0.24233049755091518,
        "step": 1880
    },
    {
        "loss": 1.9912,
        "grad_norm": 1.0551341772079468,
        "learning_rate": 0.00014279360382512786,
        "epoch": 0.24245939675174014,
        "step": 1881
    },
    {
        "loss": 2.2586,
        "grad_norm": 1.6955697536468506,
        "learning_rate": 0.0001427386078550345,
        "epoch": 0.2425882959525651,
        "step": 1882
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.5798450708389282,
        "learning_rate": 0.0001426835960650422,
        "epoch": 0.24271719515339005,
        "step": 1883
    },
    {
        "loss": 1.8587,
        "grad_norm": 1.5245453119277954,
        "learning_rate": 0.00014262856847551382,
        "epoch": 0.242846094354215,
        "step": 1884
    },
    {
        "loss": 1.9338,
        "grad_norm": 2.31301212310791,
        "learning_rate": 0.00014257352510681809,
        "epoch": 0.24297499355503996,
        "step": 1885
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.300258755683899,
        "learning_rate": 0.00014251846597932956,
        "epoch": 0.24310389275586491,
        "step": 1886
    },
    {
        "loss": 2.4834,
        "grad_norm": 1.6874243021011353,
        "learning_rate": 0.00014246339111342863,
        "epoch": 0.24323279195668987,
        "step": 1887
    },
    {
        "loss": 2.2572,
        "grad_norm": 1.399822473526001,
        "learning_rate": 0.00014240830052950158,
        "epoch": 0.24336169115751483,
        "step": 1888
    },
    {
        "loss": 2.1256,
        "grad_norm": 1.262208342552185,
        "learning_rate": 0.0001423531942479404,
        "epoch": 0.24349059035833978,
        "step": 1889
    },
    {
        "loss": 1.9712,
        "grad_norm": 2.1700596809387207,
        "learning_rate": 0.00014229807228914292,
        "epoch": 0.24361948955916474,
        "step": 1890
    },
    {
        "loss": 1.895,
        "grad_norm": 1.572052240371704,
        "learning_rate": 0.00014224293467351282,
        "epoch": 0.2437483887599897,
        "step": 1891
    },
    {
        "loss": 1.6451,
        "grad_norm": 1.5850633382797241,
        "learning_rate": 0.00014218778142145957,
        "epoch": 0.24387728796081465,
        "step": 1892
    },
    {
        "loss": 2.1206,
        "grad_norm": 1.1984604597091675,
        "learning_rate": 0.0001421326125533984,
        "epoch": 0.2440061871616396,
        "step": 1893
    },
    {
        "loss": 2.4535,
        "grad_norm": 1.7066928148269653,
        "learning_rate": 0.0001420774280897503,
        "epoch": 0.24413508636246456,
        "step": 1894
    },
    {
        "loss": 2.0471,
        "grad_norm": 1.4374808073043823,
        "learning_rate": 0.0001420222280509421,
        "epoch": 0.24426398556328952,
        "step": 1895
    },
    {
        "loss": 1.9508,
        "grad_norm": 1.6513057947158813,
        "learning_rate": 0.00014196701245740634,
        "epoch": 0.24439288476411447,
        "step": 1896
    },
    {
        "loss": 2.0849,
        "grad_norm": 2.239058494567871,
        "learning_rate": 0.00014191178132958135,
        "epoch": 0.24452178396493943,
        "step": 1897
    },
    {
        "loss": 1.6077,
        "grad_norm": 1.5108166933059692,
        "learning_rate": 0.00014185653468791113,
        "epoch": 0.24465068316576438,
        "step": 1898
    },
    {
        "loss": 2.4576,
        "grad_norm": 1.7018041610717773,
        "learning_rate": 0.00014180127255284557,
        "epoch": 0.24477958236658934,
        "step": 1899
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.5943020582199097,
        "learning_rate": 0.00014174599494484023,
        "epoch": 0.2449084815674143,
        "step": 1900
    },
    {
        "loss": 1.8244,
        "grad_norm": 1.97645902633667,
        "learning_rate": 0.00014169070188435625,
        "epoch": 0.24503738076823925,
        "step": 1901
    },
    {
        "loss": 2.2541,
        "grad_norm": 1.5942329168319702,
        "learning_rate": 0.0001416353933918607,
        "epoch": 0.2451662799690642,
        "step": 1902
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.1588494777679443,
        "learning_rate": 0.00014158006948782634,
        "epoch": 0.24529517916988916,
        "step": 1903
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.2621219158172607,
        "learning_rate": 0.00014152473019273144,
        "epoch": 0.2454240783707141,
        "step": 1904
    },
    {
        "loss": 2.2183,
        "grad_norm": 1.5399972200393677,
        "learning_rate": 0.00014146937552706026,
        "epoch": 0.24555297757153904,
        "step": 1905
    },
    {
        "loss": 2.0795,
        "grad_norm": 1.8770365715026855,
        "learning_rate": 0.00014141400551130246,
        "epoch": 0.245681876772364,
        "step": 1906
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.8331588506698608,
        "learning_rate": 0.00014135862016595357,
        "epoch": 0.24581077597318896,
        "step": 1907
    },
    {
        "loss": 2.1006,
        "grad_norm": 1.0450361967086792,
        "learning_rate": 0.00014130321951151474,
        "epoch": 0.2459396751740139,
        "step": 1908
    },
    {
        "loss": 2.2215,
        "grad_norm": 1.3641366958618164,
        "learning_rate": 0.00014124780356849278,
        "epoch": 0.24606857437483887,
        "step": 1909
    },
    {
        "loss": 2.35,
        "grad_norm": 1.7443357706069946,
        "learning_rate": 0.0001411923723574002,
        "epoch": 0.24619747357566382,
        "step": 1910
    },
    {
        "loss": 2.5456,
        "grad_norm": 1.6153594255447388,
        "learning_rate": 0.00014113692589875503,
        "epoch": 0.24632637277648878,
        "step": 1911
    },
    {
        "loss": 1.6213,
        "grad_norm": 2.0499467849731445,
        "learning_rate": 0.00014108146421308113,
        "epoch": 0.24645527197731373,
        "step": 1912
    },
    {
        "loss": 0.964,
        "grad_norm": 2.0949409008026123,
        "learning_rate": 0.0001410259873209079,
        "epoch": 0.2465841711781387,
        "step": 1913
    },
    {
        "loss": 2.3471,
        "grad_norm": 1.4582346677780151,
        "learning_rate": 0.00014097049524277032,
        "epoch": 0.24671307037896364,
        "step": 1914
    },
    {
        "loss": 2.4815,
        "grad_norm": 1.029500961303711,
        "learning_rate": 0.0001409149879992091,
        "epoch": 0.2468419695797886,
        "step": 1915
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.3220869302749634,
        "learning_rate": 0.0001408594656107705,
        "epoch": 0.24697086878061356,
        "step": 1916
    },
    {
        "loss": 2.0401,
        "grad_norm": 1.2238225936889648,
        "learning_rate": 0.0001408039280980064,
        "epoch": 0.2470997679814385,
        "step": 1917
    },
    {
        "loss": 1.6269,
        "grad_norm": 1.5335230827331543,
        "learning_rate": 0.0001407483754814742,
        "epoch": 0.24722866718226347,
        "step": 1918
    },
    {
        "loss": 2.2056,
        "grad_norm": 1.2759124040603638,
        "learning_rate": 0.00014069280778173705,
        "epoch": 0.24735756638308842,
        "step": 1919
    },
    {
        "loss": 1.6033,
        "grad_norm": 1.5503791570663452,
        "learning_rate": 0.00014063722501936362,
        "epoch": 0.24748646558391338,
        "step": 1920
    },
    {
        "loss": 2.2245,
        "grad_norm": 1.5240042209625244,
        "learning_rate": 0.00014058162721492805,
        "epoch": 0.24761536478473833,
        "step": 1921
    },
    {
        "loss": 1.5277,
        "grad_norm": 1.8424513339996338,
        "learning_rate": 0.0001405260143890102,
        "epoch": 0.2477442639855633,
        "step": 1922
    },
    {
        "loss": 2.0871,
        "grad_norm": 0.7918904423713684,
        "learning_rate": 0.00014047038656219538,
        "epoch": 0.24787316318638825,
        "step": 1923
    },
    {
        "loss": 2.2501,
        "grad_norm": 1.16099214553833,
        "learning_rate": 0.00014041474375507448,
        "epoch": 0.2480020623872132,
        "step": 1924
    },
    {
        "loss": 2.2905,
        "grad_norm": 2.0620274543762207,
        "learning_rate": 0.00014035908598824403,
        "epoch": 0.24813096158803816,
        "step": 1925
    },
    {
        "loss": 2.4406,
        "grad_norm": 1.3559223413467407,
        "learning_rate": 0.0001403034132823059,
        "epoch": 0.2482598607888631,
        "step": 1926
    },
    {
        "loss": 2.2206,
        "grad_norm": 1.3640788793563843,
        "learning_rate": 0.00014024772565786773,
        "epoch": 0.24838875998968807,
        "step": 1927
    },
    {
        "loss": 1.8721,
        "grad_norm": 1.4048136472702026,
        "learning_rate": 0.00014019202313554247,
        "epoch": 0.24851765919051302,
        "step": 1928
    },
    {
        "loss": 2.5145,
        "grad_norm": 1.3261078596115112,
        "learning_rate": 0.00014013630573594873,
        "epoch": 0.24864655839133798,
        "step": 1929
    },
    {
        "loss": 2.078,
        "grad_norm": 1.6520771980285645,
        "learning_rate": 0.0001400805734797105,
        "epoch": 0.24877545759216294,
        "step": 1930
    },
    {
        "loss": 2.0841,
        "grad_norm": 1.7597986459732056,
        "learning_rate": 0.00014002482638745742,
        "epoch": 0.2489043567929879,
        "step": 1931
    },
    {
        "loss": 1.6882,
        "grad_norm": 1.873809576034546,
        "learning_rate": 0.0001399690644798245,
        "epoch": 0.24903325599381285,
        "step": 1932
    },
    {
        "loss": 1.7121,
        "grad_norm": 1.8607831001281738,
        "learning_rate": 0.00013991328777745229,
        "epoch": 0.2491621551946378,
        "step": 1933
    },
    {
        "loss": 1.9362,
        "grad_norm": 1.726663589477539,
        "learning_rate": 0.00013985749630098676,
        "epoch": 0.24929105439546276,
        "step": 1934
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.3983793258666992,
        "learning_rate": 0.0001398016900710794,
        "epoch": 0.2494199535962877,
        "step": 1935
    },
    {
        "loss": 2.383,
        "grad_norm": 1.1893579959869385,
        "learning_rate": 0.00013974586910838724,
        "epoch": 0.24954885279711267,
        "step": 1936
    },
    {
        "loss": 1.7606,
        "grad_norm": 1.3628515005111694,
        "learning_rate": 0.00013969003343357257,
        "epoch": 0.24967775199793762,
        "step": 1937
    },
    {
        "loss": 2.421,
        "grad_norm": 1.5302411317825317,
        "learning_rate": 0.00013963418306730329,
        "epoch": 0.24980665119876258,
        "step": 1938
    },
    {
        "loss": 1.9215,
        "grad_norm": 1.8318742513656616,
        "learning_rate": 0.00013957831803025264,
        "epoch": 0.24993555039958754,
        "step": 1939
    },
    {
        "loss": 1.2957,
        "grad_norm": 2.6693005561828613,
        "learning_rate": 0.0001395224383430994,
        "epoch": 0.25006444960041246,
        "step": 1940
    },
    {
        "loss": 1.6994,
        "grad_norm": 1.7199389934539795,
        "learning_rate": 0.00013946654402652759,
        "epoch": 0.25019334880123745,
        "step": 1941
    },
    {
        "loss": 1.8421,
        "grad_norm": 1.632591962814331,
        "learning_rate": 0.00013941063510122684,
        "epoch": 0.2503222480020624,
        "step": 1942
    },
    {
        "loss": 1.4632,
        "grad_norm": 1.6233352422714233,
        "learning_rate": 0.00013935471158789205,
        "epoch": 0.25045114720288736,
        "step": 1943
    },
    {
        "loss": 1.2617,
        "grad_norm": 2.2343084812164307,
        "learning_rate": 0.00013929877350722363,
        "epoch": 0.2505800464037123,
        "step": 1944
    },
    {
        "loss": 1.8822,
        "grad_norm": 1.1156642436981201,
        "learning_rate": 0.00013924282087992733,
        "epoch": 0.25070894560453727,
        "step": 1945
    },
    {
        "loss": 1.6958,
        "grad_norm": 1.3086591958999634,
        "learning_rate": 0.00013918685372671423,
        "epoch": 0.2508378448053622,
        "step": 1946
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.7800432443618774,
        "learning_rate": 0.00013913087206830085,
        "epoch": 0.2509667440061872,
        "step": 1947
    },
    {
        "loss": 1.8226,
        "grad_norm": 2.3682477474212646,
        "learning_rate": 0.0001390748759254091,
        "epoch": 0.2510956432070121,
        "step": 1948
    },
    {
        "loss": 2.203,
        "grad_norm": 1.553088903427124,
        "learning_rate": 0.00013901886531876616,
        "epoch": 0.2512245424078371,
        "step": 1949
    },
    {
        "loss": 2.114,
        "grad_norm": 1.4531075954437256,
        "learning_rate": 0.0001389628402691047,
        "epoch": 0.251353441608662,
        "step": 1950
    },
    {
        "loss": 2.4958,
        "grad_norm": 1.4244585037231445,
        "learning_rate": 0.00013890680079716255,
        "epoch": 0.251482340809487,
        "step": 1951
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.9623382091522217,
        "learning_rate": 0.00013885074692368313,
        "epoch": 0.25161124001031193,
        "step": 1952
    },
    {
        "loss": 2.0981,
        "grad_norm": 1.6786746978759766,
        "learning_rate": 0.00013879467866941494,
        "epoch": 0.2517401392111369,
        "step": 1953
    },
    {
        "loss": 1.8757,
        "grad_norm": 2.6321418285369873,
        "learning_rate": 0.00013873859605511194,
        "epoch": 0.25186903841196184,
        "step": 1954
    },
    {
        "loss": 1.7251,
        "grad_norm": 1.9542194604873657,
        "learning_rate": 0.0001386824991015334,
        "epoch": 0.2519979376127868,
        "step": 1955
    },
    {
        "loss": 1.9975,
        "grad_norm": 1.2465287446975708,
        "learning_rate": 0.00013862638782944386,
        "epoch": 0.25212683681361175,
        "step": 1956
    },
    {
        "loss": 1.1289,
        "grad_norm": 2.3242857456207275,
        "learning_rate": 0.00013857026225961314,
        "epoch": 0.25225573601443674,
        "step": 1957
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.76651132106781,
        "learning_rate": 0.00013851412241281644,
        "epoch": 0.25238463521526167,
        "step": 1958
    },
    {
        "loss": 2.093,
        "grad_norm": 1.8737618923187256,
        "learning_rate": 0.0001384579683098341,
        "epoch": 0.2525135344160866,
        "step": 1959
    },
    {
        "loss": 2.0358,
        "grad_norm": 2.078794479370117,
        "learning_rate": 0.00013840179997145197,
        "epoch": 0.2526424336169116,
        "step": 1960
    },
    {
        "loss": 2.0483,
        "grad_norm": 1.421742558479309,
        "learning_rate": 0.00013834561741846093,
        "epoch": 0.2527713328177365,
        "step": 1961
    },
    {
        "loss": 2.1681,
        "grad_norm": 1.4555104970932007,
        "learning_rate": 0.00013828942067165725,
        "epoch": 0.2529002320185615,
        "step": 1962
    },
    {
        "loss": 1.6975,
        "grad_norm": 1.8491908311843872,
        "learning_rate": 0.00013823320975184242,
        "epoch": 0.2530291312193864,
        "step": 1963
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.5642504692077637,
        "learning_rate": 0.00013817698467982314,
        "epoch": 0.2531580304202114,
        "step": 1964
    },
    {
        "loss": 2.3637,
        "grad_norm": 1.182937741279602,
        "learning_rate": 0.00013812074547641144,
        "epoch": 0.2532869296210363,
        "step": 1965
    },
    {
        "loss": 1.8161,
        "grad_norm": 1.7894755601882935,
        "learning_rate": 0.0001380644921624245,
        "epoch": 0.2534158288218613,
        "step": 1966
    },
    {
        "loss": 2.6738,
        "grad_norm": 0.9438596367835999,
        "learning_rate": 0.0001380082247586848,
        "epoch": 0.25354472802268624,
        "step": 1967
    },
    {
        "loss": 2.3727,
        "grad_norm": 1.802240252494812,
        "learning_rate": 0.00013795194328601986,
        "epoch": 0.2536736272235112,
        "step": 1968
    },
    {
        "loss": 2.6758,
        "grad_norm": 1.3537777662277222,
        "learning_rate": 0.00013789564776526273,
        "epoch": 0.25380252642433615,
        "step": 1969
    },
    {
        "loss": 2.3231,
        "grad_norm": 2.037517786026001,
        "learning_rate": 0.0001378393382172513,
        "epoch": 0.25393142562516113,
        "step": 1970
    },
    {
        "loss": 2.0531,
        "grad_norm": 1.976638674736023,
        "learning_rate": 0.00013778301466282889,
        "epoch": 0.25406032482598606,
        "step": 1971
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.6356537342071533,
        "learning_rate": 0.0001377266771228439,
        "epoch": 0.25418922402681104,
        "step": 1972
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.6343432664871216,
        "learning_rate": 0.00013767032561814996,
        "epoch": 0.254318123227636,
        "step": 1973
    },
    {
        "loss": 2.2097,
        "grad_norm": 2.0163846015930176,
        "learning_rate": 0.00013761396016960585,
        "epoch": 0.25444702242846096,
        "step": 1974
    },
    {
        "loss": 1.9985,
        "grad_norm": 1.4648246765136719,
        "learning_rate": 0.0001375575807980755,
        "epoch": 0.2545759216292859,
        "step": 1975
    },
    {
        "loss": 1.3033,
        "grad_norm": 2.223015308380127,
        "learning_rate": 0.000137501187524428,
        "epoch": 0.25470482083011087,
        "step": 1976
    },
    {
        "loss": 2.1783,
        "grad_norm": 1.1021523475646973,
        "learning_rate": 0.00013744478036953762,
        "epoch": 0.2548337200309358,
        "step": 1977
    },
    {
        "loss": 1.6795,
        "grad_norm": 1.9779692888259888,
        "learning_rate": 0.00013738835935428364,
        "epoch": 0.2549626192317608,
        "step": 1978
    },
    {
        "loss": 2.0714,
        "grad_norm": 1.668472409248352,
        "learning_rate": 0.00013733192449955073,
        "epoch": 0.2550915184325857,
        "step": 1979
    },
    {
        "loss": 2.35,
        "grad_norm": 1.0145699977874756,
        "learning_rate": 0.0001372754758262284,
        "epoch": 0.2552204176334107,
        "step": 1980
    },
    {
        "loss": 2.4956,
        "grad_norm": 1.1418778896331787,
        "learning_rate": 0.0001372190133552114,
        "epoch": 0.2553493168342356,
        "step": 1981
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.6713342666625977,
        "learning_rate": 0.00013716253710739957,
        "epoch": 0.2554782160350606,
        "step": 1982
    },
    {
        "loss": 1.4314,
        "grad_norm": 2.30415415763855,
        "learning_rate": 0.00013710604710369793,
        "epoch": 0.25560711523588553,
        "step": 1983
    },
    {
        "loss": 2.5489,
        "grad_norm": 1.2980185747146606,
        "learning_rate": 0.00013704954336501648,
        "epoch": 0.2557360144367105,
        "step": 1984
    },
    {
        "loss": 2.2037,
        "grad_norm": 1.47411048412323,
        "learning_rate": 0.00013699302591227034,
        "epoch": 0.25586491363753544,
        "step": 1985
    },
    {
        "loss": 2.2049,
        "grad_norm": 1.5581384897232056,
        "learning_rate": 0.00013693649476637977,
        "epoch": 0.2559938128383604,
        "step": 1986
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.104547142982483,
        "learning_rate": 0.00013687994994827,
        "epoch": 0.25612271203918535,
        "step": 1987
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.2437020540237427,
        "learning_rate": 0.00013682339147887132,
        "epoch": 0.25625161124001034,
        "step": 1988
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.8529659509658813,
        "learning_rate": 0.00013676681937911919,
        "epoch": 0.25638051044083526,
        "step": 1989
    },
    {
        "loss": 2.1672,
        "grad_norm": 2.2219350337982178,
        "learning_rate": 0.00013671023366995403,
        "epoch": 0.25650940964166025,
        "step": 1990
    },
    {
        "loss": 1.7477,
        "grad_norm": 2.1543281078338623,
        "learning_rate": 0.00013665363437232126,
        "epoch": 0.2566383088424852,
        "step": 1991
    },
    {
        "loss": 1.9444,
        "grad_norm": 1.661224365234375,
        "learning_rate": 0.00013659702150717144,
        "epoch": 0.25676720804331016,
        "step": 1992
    },
    {
        "loss": 2.6481,
        "grad_norm": 1.074302077293396,
        "learning_rate": 0.00013654039509546006,
        "epoch": 0.2568961072441351,
        "step": 1993
    },
    {
        "loss": 2.2757,
        "grad_norm": 1.2148867845535278,
        "learning_rate": 0.00013648375515814767,
        "epoch": 0.25702500644496,
        "step": 1994
    },
    {
        "loss": 1.9133,
        "grad_norm": 1.4120192527770996,
        "learning_rate": 0.00013642710171619975,
        "epoch": 0.257153905645785,
        "step": 1995
    },
    {
        "loss": 1.4437,
        "grad_norm": 1.9923803806304932,
        "learning_rate": 0.00013637043479058694,
        "epoch": 0.2572828048466099,
        "step": 1996
    },
    {
        "loss": 1.1815,
        "grad_norm": 2.9745330810546875,
        "learning_rate": 0.00013631375440228472,
        "epoch": 0.2574117040474349,
        "step": 1997
    },
    {
        "loss": 1.5929,
        "grad_norm": 1.9317479133605957,
        "learning_rate": 0.00013625706057227358,
        "epoch": 0.25754060324825984,
        "step": 1998
    },
    {
        "loss": 1.8554,
        "grad_norm": 1.7898626327514648,
        "learning_rate": 0.00013620035332153908,
        "epoch": 0.2576695024490848,
        "step": 1999
    },
    {
        "loss": 2.3422,
        "grad_norm": 2.3864006996154785,
        "learning_rate": 0.0001361436326710716,
        "epoch": 0.25779840164990975,
        "step": 2000
    },
    {
        "loss": 1.6851,
        "grad_norm": 1.5203932523727417,
        "learning_rate": 0.0001360868986418666,
        "epoch": 0.25792730085073473,
        "step": 2001
    },
    {
        "loss": 1.7249,
        "grad_norm": 2.168760061264038,
        "learning_rate": 0.00013603015125492448,
        "epoch": 0.25805620005155966,
        "step": 2002
    },
    {
        "loss": 1.9272,
        "grad_norm": 1.363623023033142,
        "learning_rate": 0.00013597339053125052,
        "epoch": 0.25818509925238464,
        "step": 2003
    },
    {
        "loss": 2.2858,
        "grad_norm": 1.3084912300109863,
        "learning_rate": 0.00013591661649185497,
        "epoch": 0.25831399845320957,
        "step": 2004
    },
    {
        "loss": 1.442,
        "grad_norm": 2.092209577560425,
        "learning_rate": 0.00013585982915775298,
        "epoch": 0.25844289765403455,
        "step": 2005
    },
    {
        "loss": 2.0652,
        "grad_norm": 2.569587230682373,
        "learning_rate": 0.00013580302854996473,
        "epoch": 0.2585717968548595,
        "step": 2006
    },
    {
        "loss": 2.561,
        "grad_norm": 3.927720785140991,
        "learning_rate": 0.00013574621468951517,
        "epoch": 0.25870069605568446,
        "step": 2007
    },
    {
        "loss": 1.6542,
        "grad_norm": 2.515909194946289,
        "learning_rate": 0.00013568938759743425,
        "epoch": 0.2588295952565094,
        "step": 2008
    },
    {
        "loss": 1.7092,
        "grad_norm": 2.085136651992798,
        "learning_rate": 0.0001356325472947568,
        "epoch": 0.2589584944573344,
        "step": 2009
    },
    {
        "loss": 1.8563,
        "grad_norm": 2.3919384479522705,
        "learning_rate": 0.00013557569380252252,
        "epoch": 0.2590873936581593,
        "step": 2010
    },
    {
        "loss": 2.2038,
        "grad_norm": 1.3433440923690796,
        "learning_rate": 0.00013551882714177592,
        "epoch": 0.2592162928589843,
        "step": 2011
    },
    {
        "loss": 1.5608,
        "grad_norm": 2.2417099475860596,
        "learning_rate": 0.0001354619473335666,
        "epoch": 0.2593451920598092,
        "step": 2012
    },
    {
        "loss": 1.9636,
        "grad_norm": 2.007007122039795,
        "learning_rate": 0.00013540505439894881,
        "epoch": 0.2594740912606342,
        "step": 2013
    },
    {
        "loss": 1.8213,
        "grad_norm": 2.4407975673675537,
        "learning_rate": 0.00013534814835898176,
        "epoch": 0.2596029904614591,
        "step": 2014
    },
    {
        "loss": 2.1372,
        "grad_norm": 2.174527406692505,
        "learning_rate": 0.00013529122923472948,
        "epoch": 0.2597318896622841,
        "step": 2015
    },
    {
        "loss": 1.9061,
        "grad_norm": 1.347317099571228,
        "learning_rate": 0.00013523429704726084,
        "epoch": 0.25986078886310904,
        "step": 2016
    },
    {
        "loss": 2.2779,
        "grad_norm": 1.390629529953003,
        "learning_rate": 0.00013517735181764964,
        "epoch": 0.259989688063934,
        "step": 2017
    },
    {
        "loss": 1.9092,
        "grad_norm": 1.2207223176956177,
        "learning_rate": 0.0001351203935669743,
        "epoch": 0.26011858726475895,
        "step": 2018
    },
    {
        "loss": 2.2406,
        "grad_norm": 1.3687634468078613,
        "learning_rate": 0.00013506342231631833,
        "epoch": 0.26024748646558393,
        "step": 2019
    },
    {
        "loss": 1.7565,
        "grad_norm": 2.125232458114624,
        "learning_rate": 0.00013500643808676984,
        "epoch": 0.26037638566640886,
        "step": 2020
    },
    {
        "loss": 2.166,
        "grad_norm": 1.3517215251922607,
        "learning_rate": 0.00013494944089942175,
        "epoch": 0.26050528486723384,
        "step": 2021
    },
    {
        "loss": 1.823,
        "grad_norm": 2.595921754837036,
        "learning_rate": 0.00013489243077537193,
        "epoch": 0.26063418406805877,
        "step": 2022
    },
    {
        "loss": 2.2181,
        "grad_norm": 1.6065133810043335,
        "learning_rate": 0.0001348354077357229,
        "epoch": 0.26076308326888376,
        "step": 2023
    },
    {
        "loss": 2.6401,
        "grad_norm": 1.2258541584014893,
        "learning_rate": 0.00013477837180158206,
        "epoch": 0.2608919824697087,
        "step": 2024
    },
    {
        "loss": 2.235,
        "grad_norm": 1.3527129888534546,
        "learning_rate": 0.00013472132299406146,
        "epoch": 0.26102088167053367,
        "step": 2025
    },
    {
        "loss": 2.1082,
        "grad_norm": 1.1156619787216187,
        "learning_rate": 0.0001346642613342781,
        "epoch": 0.2611497808713586,
        "step": 2026
    },
    {
        "loss": 2.2764,
        "grad_norm": 1.1794143915176392,
        "learning_rate": 0.0001346071868433535,
        "epoch": 0.2612786800721836,
        "step": 2027
    },
    {
        "loss": 1.6517,
        "grad_norm": 1.8754509687423706,
        "learning_rate": 0.0001345500995424141,
        "epoch": 0.2614075792730085,
        "step": 2028
    },
    {
        "loss": 2.2976,
        "grad_norm": 1.8365020751953125,
        "learning_rate": 0.00013449299945259105,
        "epoch": 0.2615364784738335,
        "step": 2029
    },
    {
        "loss": 2.4001,
        "grad_norm": 1.8691571950912476,
        "learning_rate": 0.00013443588659502024,
        "epoch": 0.2616653776746584,
        "step": 2030
    },
    {
        "loss": 2.2989,
        "grad_norm": 1.5639313459396362,
        "learning_rate": 0.00013437876099084222,
        "epoch": 0.26179427687548334,
        "step": 2031
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.2581027746200562,
        "learning_rate": 0.00013432162266120232,
        "epoch": 0.26192317607630833,
        "step": 2032
    },
    {
        "loss": 2.0048,
        "grad_norm": 1.2355424165725708,
        "learning_rate": 0.0001342644716272506,
        "epoch": 0.26205207527713326,
        "step": 2033
    },
    {
        "loss": 1.4638,
        "grad_norm": 1.1957417726516724,
        "learning_rate": 0.0001342073079101417,
        "epoch": 0.26218097447795824,
        "step": 2034
    },
    {
        "loss": 1.8405,
        "grad_norm": 1.9224274158477783,
        "learning_rate": 0.0001341501315310351,
        "epoch": 0.26230987367878317,
        "step": 2035
    },
    {
        "loss": 1.5261,
        "grad_norm": 2.130448579788208,
        "learning_rate": 0.00013409294251109496,
        "epoch": 0.26243877287960815,
        "step": 2036
    },
    {
        "loss": 0.9348,
        "grad_norm": 1.9580296277999878,
        "learning_rate": 0.00013403574087148998,
        "epoch": 0.2625676720804331,
        "step": 2037
    },
    {
        "loss": 2.1165,
        "grad_norm": 1.917446494102478,
        "learning_rate": 0.00013397852663339367,
        "epoch": 0.26269657128125806,
        "step": 2038
    },
    {
        "loss": 2.6367,
        "grad_norm": 1.6816118955612183,
        "learning_rate": 0.00013392129981798409,
        "epoch": 0.262825470482083,
        "step": 2039
    },
    {
        "loss": 2.6002,
        "grad_norm": 1.2957217693328857,
        "learning_rate": 0.00013386406044644408,
        "epoch": 0.262954369682908,
        "step": 2040
    },
    {
        "loss": 2.2538,
        "grad_norm": 1.6428263187408447,
        "learning_rate": 0.00013380680853996104,
        "epoch": 0.2630832688837329,
        "step": 2041
    },
    {
        "loss": 1.5832,
        "grad_norm": 1.8500977754592896,
        "learning_rate": 0.0001337495441197271,
        "epoch": 0.2632121680845579,
        "step": 2042
    },
    {
        "loss": 2.083,
        "grad_norm": 1.0584163665771484,
        "learning_rate": 0.00013369226720693885,
        "epoch": 0.2633410672853828,
        "step": 2043
    },
    {
        "loss": 2.6019,
        "grad_norm": 1.6537013053894043,
        "learning_rate": 0.0001336349778227977,
        "epoch": 0.2634699664862078,
        "step": 2044
    },
    {
        "loss": 2.1486,
        "grad_norm": 1.4366652965545654,
        "learning_rate": 0.0001335776759885095,
        "epoch": 0.2635988656870327,
        "step": 2045
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.2378103733062744,
        "learning_rate": 0.00013352036172528485,
        "epoch": 0.2637277648878577,
        "step": 2046
    },
    {
        "loss": 1.5985,
        "grad_norm": 1.9005812406539917,
        "learning_rate": 0.00013346303505433891,
        "epoch": 0.26385666408868264,
        "step": 2047
    },
    {
        "loss": 1.9941,
        "grad_norm": 1.4774551391601562,
        "learning_rate": 0.0001334056959968914,
        "epoch": 0.2639855632895076,
        "step": 2048
    },
    {
        "loss": 2.4312,
        "grad_norm": 1.1844468116760254,
        "learning_rate": 0.00013334834457416668,
        "epoch": 0.26411446249033255,
        "step": 2049
    },
    {
        "loss": 2.029,
        "grad_norm": 1.0944465398788452,
        "learning_rate": 0.0001332909808073936,
        "epoch": 0.26424336169115753,
        "step": 2050
    },
    {
        "loss": 2.078,
        "grad_norm": 2.3229382038116455,
        "learning_rate": 0.0001332336047178056,
        "epoch": 0.26437226089198246,
        "step": 2051
    },
    {
        "loss": 1.9008,
        "grad_norm": 1.2071754932403564,
        "learning_rate": 0.00013317621632664083,
        "epoch": 0.26450116009280744,
        "step": 2052
    },
    {
        "loss": 1.8418,
        "grad_norm": 1.803663730621338,
        "learning_rate": 0.0001331188156551418,
        "epoch": 0.26463005929363237,
        "step": 2053
    },
    {
        "loss": 2.186,
        "grad_norm": 2.049021005630493,
        "learning_rate": 0.00013306140272455563,
        "epoch": 0.26475895849445735,
        "step": 2054
    },
    {
        "loss": 1.445,
        "grad_norm": 2.453105926513672,
        "learning_rate": 0.00013300397755613402,
        "epoch": 0.2648878576952823,
        "step": 2055
    },
    {
        "loss": 2.3154,
        "grad_norm": 1.241559386253357,
        "learning_rate": 0.00013294654017113316,
        "epoch": 0.26501675689610726,
        "step": 2056
    },
    {
        "loss": 2.3048,
        "grad_norm": 1.214052438735962,
        "learning_rate": 0.0001328890905908138,
        "epoch": 0.2651456560969322,
        "step": 2057
    },
    {
        "loss": 2.7081,
        "grad_norm": 1.4605164527893066,
        "learning_rate": 0.00013283162883644107,
        "epoch": 0.2652745552977572,
        "step": 2058
    },
    {
        "loss": 1.6958,
        "grad_norm": 1.6044948101043701,
        "learning_rate": 0.00013277415492928487,
        "epoch": 0.2654034544985821,
        "step": 2059
    },
    {
        "loss": 2.1791,
        "grad_norm": 1.8607195615768433,
        "learning_rate": 0.00013271666889061933,
        "epoch": 0.2655323536994071,
        "step": 2060
    },
    {
        "loss": 1.6987,
        "grad_norm": 1.549646258354187,
        "learning_rate": 0.00013265917074172317,
        "epoch": 0.265661252900232,
        "step": 2061
    },
    {
        "loss": 2.0037,
        "grad_norm": 1.7471457719802856,
        "learning_rate": 0.0001326016605038797,
        "epoch": 0.265790152101057,
        "step": 2062
    },
    {
        "loss": 2.1872,
        "grad_norm": 1.770861268043518,
        "learning_rate": 0.0001325441381983765,
        "epoch": 0.2659190513018819,
        "step": 2063
    },
    {
        "loss": 1.4038,
        "grad_norm": 1.403943657875061,
        "learning_rate": 0.0001324866038465058,
        "epoch": 0.2660479505027069,
        "step": 2064
    },
    {
        "loss": 2.4253,
        "grad_norm": 1.3778403997421265,
        "learning_rate": 0.00013242905746956417,
        "epoch": 0.26617684970353184,
        "step": 2065
    },
    {
        "loss": 2.0829,
        "grad_norm": 2.2021942138671875,
        "learning_rate": 0.00013237149908885273,
        "epoch": 0.2663057489043568,
        "step": 2066
    },
    {
        "loss": 1.9258,
        "grad_norm": 1.7782557010650635,
        "learning_rate": 0.00013231392872567693,
        "epoch": 0.26643464810518175,
        "step": 2067
    },
    {
        "loss": 2.2388,
        "grad_norm": 1.1374328136444092,
        "learning_rate": 0.00013225634640134674,
        "epoch": 0.2665635473060067,
        "step": 2068
    },
    {
        "loss": 2.023,
        "grad_norm": 1.5678128004074097,
        "learning_rate": 0.00013219875213717653,
        "epoch": 0.26669244650683166,
        "step": 2069
    },
    {
        "loss": 1.7668,
        "grad_norm": 1.97677743434906,
        "learning_rate": 0.00013214114595448505,
        "epoch": 0.2668213457076566,
        "step": 2070
    },
    {
        "loss": 1.5566,
        "grad_norm": 1.8299349546432495,
        "learning_rate": 0.00013208352787459559,
        "epoch": 0.26695024490848157,
        "step": 2071
    },
    {
        "loss": 2.2122,
        "grad_norm": 2.066676616668701,
        "learning_rate": 0.00013202589791883572,
        "epoch": 0.2670791441093065,
        "step": 2072
    },
    {
        "loss": 1.4488,
        "grad_norm": 1.9567307233810425,
        "learning_rate": 0.0001319682561085374,
        "epoch": 0.2672080433101315,
        "step": 2073
    },
    {
        "loss": 2.1625,
        "grad_norm": 1.5649226903915405,
        "learning_rate": 0.00013191060246503698,
        "epoch": 0.2673369425109564,
        "step": 2074
    },
    {
        "loss": 2.1452,
        "grad_norm": 1.363675594329834,
        "learning_rate": 0.0001318529370096754,
        "epoch": 0.2674658417117814,
        "step": 2075
    },
    {
        "loss": 2.3049,
        "grad_norm": 1.391086459159851,
        "learning_rate": 0.00013179525976379767,
        "epoch": 0.2675947409126063,
        "step": 2076
    },
    {
        "loss": 2.0963,
        "grad_norm": 1.3579999208450317,
        "learning_rate": 0.00013173757074875334,
        "epoch": 0.2677236401134313,
        "step": 2077
    },
    {
        "loss": 1.4506,
        "grad_norm": 1.7155803442001343,
        "learning_rate": 0.00013167986998589625,
        "epoch": 0.26785253931425623,
        "step": 2078
    },
    {
        "loss": 1.9001,
        "grad_norm": 2.199951171875,
        "learning_rate": 0.0001316221574965846,
        "epoch": 0.2679814385150812,
        "step": 2079
    },
    {
        "loss": 1.7224,
        "grad_norm": 2.8098995685577393,
        "learning_rate": 0.000131564433302181,
        "epoch": 0.26811033771590614,
        "step": 2080
    },
    {
        "loss": 2.171,
        "grad_norm": 1.7487585544586182,
        "learning_rate": 0.00013150669742405225,
        "epoch": 0.2682392369167311,
        "step": 2081
    },
    {
        "loss": 1.9806,
        "grad_norm": 2.030306577682495,
        "learning_rate": 0.0001314489498835697,
        "epoch": 0.26836813611755606,
        "step": 2082
    },
    {
        "loss": 2.0242,
        "grad_norm": 1.811731219291687,
        "learning_rate": 0.00013139119070210874,
        "epoch": 0.26849703531838104,
        "step": 2083
    },
    {
        "loss": 1.8616,
        "grad_norm": 1.432644009590149,
        "learning_rate": 0.00013133341990104923,
        "epoch": 0.26862593451920597,
        "step": 2084
    },
    {
        "loss": 2.6934,
        "grad_norm": 1.8053929805755615,
        "learning_rate": 0.00013127563750177536,
        "epoch": 0.26875483372003095,
        "step": 2085
    },
    {
        "loss": 2.4853,
        "grad_norm": 1.2525148391723633,
        "learning_rate": 0.00013121784352567554,
        "epoch": 0.2688837329208559,
        "step": 2086
    },
    {
        "loss": 2.2298,
        "grad_norm": 1.1134718656539917,
        "learning_rate": 0.00013116003799414246,
        "epoch": 0.26901263212168086,
        "step": 2087
    },
    {
        "loss": 1.7042,
        "grad_norm": 1.8159708976745605,
        "learning_rate": 0.0001311022209285731,
        "epoch": 0.2691415313225058,
        "step": 2088
    },
    {
        "loss": 1.9558,
        "grad_norm": 1.1561933755874634,
        "learning_rate": 0.0001310443923503688,
        "epoch": 0.2692704305233308,
        "step": 2089
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.1030914783477783,
        "learning_rate": 0.00013098655228093502,
        "epoch": 0.2693993297241557,
        "step": 2090
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.077101469039917,
        "learning_rate": 0.00013092870074168152,
        "epoch": 0.2695282289249807,
        "step": 2091
    },
    {
        "loss": 2.2594,
        "grad_norm": 1.427773118019104,
        "learning_rate": 0.00013087083775402235,
        "epoch": 0.2696571281258056,
        "step": 2092
    },
    {
        "loss": 2.0489,
        "grad_norm": 1.903097152709961,
        "learning_rate": 0.00013081296333937583,
        "epoch": 0.2697860273266306,
        "step": 2093
    },
    {
        "loss": 2.5877,
        "grad_norm": 1.4740484952926636,
        "learning_rate": 0.00013075507751916435,
        "epoch": 0.2699149265274555,
        "step": 2094
    },
    {
        "loss": 1.953,
        "grad_norm": 2.001530647277832,
        "learning_rate": 0.00013069718031481466,
        "epoch": 0.2700438257282805,
        "step": 2095
    },
    {
        "loss": 2.1067,
        "grad_norm": 1.1887998580932617,
        "learning_rate": 0.00013063927174775767,
        "epoch": 0.27017272492910543,
        "step": 2096
    },
    {
        "loss": 2.154,
        "grad_norm": 1.701767921447754,
        "learning_rate": 0.00013058135183942858,
        "epoch": 0.2703016241299304,
        "step": 2097
    },
    {
        "loss": 2.3413,
        "grad_norm": 1.8585618734359741,
        "learning_rate": 0.0001305234206112666,
        "epoch": 0.27043052333075535,
        "step": 2098
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.3569912910461426,
        "learning_rate": 0.00013046547808471542,
        "epoch": 0.27055942253158033,
        "step": 2099
    },
    {
        "loss": 2.3656,
        "grad_norm": 1.8828084468841553,
        "learning_rate": 0.0001304075242812226,
        "epoch": 0.27068832173240526,
        "step": 2100
    },
    {
        "loss": 1.2001,
        "grad_norm": 1.4886820316314697,
        "learning_rate": 0.00013034955922224003,
        "epoch": 0.27081722093323024,
        "step": 2101
    },
    {
        "loss": 1.8021,
        "grad_norm": 1.6085702180862427,
        "learning_rate": 0.0001302915829292238,
        "epoch": 0.27094612013405517,
        "step": 2102
    },
    {
        "loss": 1.2804,
        "grad_norm": 1.957128643989563,
        "learning_rate": 0.00013023359542363403,
        "epoch": 0.2710750193348801,
        "step": 2103
    },
    {
        "loss": 1.8613,
        "grad_norm": 1.2192940711975098,
        "learning_rate": 0.00013017559672693523,
        "epoch": 0.2712039185357051,
        "step": 2104
    },
    {
        "loss": 1.991,
        "grad_norm": 1.456695556640625,
        "learning_rate": 0.00013011758686059575,
        "epoch": 0.27133281773653,
        "step": 2105
    },
    {
        "loss": 1.554,
        "grad_norm": 2.175947904586792,
        "learning_rate": 0.00013005956584608827,
        "epoch": 0.271461716937355,
        "step": 2106
    },
    {
        "loss": 1.9723,
        "grad_norm": 1.4576464891433716,
        "learning_rate": 0.00013000153370488953,
        "epoch": 0.2715906161381799,
        "step": 2107
    },
    {
        "loss": 2.4174,
        "grad_norm": 1.345062017440796,
        "learning_rate": 0.0001299434904584804,
        "epoch": 0.2717195153390049,
        "step": 2108
    },
    {
        "loss": 1.01,
        "grad_norm": 1.7830687761306763,
        "learning_rate": 0.00012988543612834586,
        "epoch": 0.27184841453982983,
        "step": 2109
    },
    {
        "loss": 1.1901,
        "grad_norm": 1.425089716911316,
        "learning_rate": 0.00012982737073597507,
        "epoch": 0.2719773137406548,
        "step": 2110
    },
    {
        "loss": 1.9718,
        "grad_norm": 2.0817370414733887,
        "learning_rate": 0.0001297692943028611,
        "epoch": 0.27210621294147974,
        "step": 2111
    },
    {
        "loss": 2.0556,
        "grad_norm": 2.0550155639648438,
        "learning_rate": 0.0001297112068505013,
        "epoch": 0.2722351121423047,
        "step": 2112
    },
    {
        "loss": 1.9113,
        "grad_norm": 2.4088668823242188,
        "learning_rate": 0.00012965310840039695,
        "epoch": 0.27236401134312965,
        "step": 2113
    },
    {
        "loss": 2.0444,
        "grad_norm": 2.166506290435791,
        "learning_rate": 0.0001295949989740535,
        "epoch": 0.27249291054395464,
        "step": 2114
    },
    {
        "loss": 1.9596,
        "grad_norm": 2.115827798843384,
        "learning_rate": 0.00012953687859298046,
        "epoch": 0.27262180974477956,
        "step": 2115
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.7783561944961548,
        "learning_rate": 0.00012947874727869136,
        "epoch": 0.27275070894560455,
        "step": 2116
    },
    {
        "loss": 2.1659,
        "grad_norm": 1.3903194665908813,
        "learning_rate": 0.00012942060505270376,
        "epoch": 0.2728796081464295,
        "step": 2117
    },
    {
        "loss": 2.542,
        "grad_norm": 1.2082936763763428,
        "learning_rate": 0.00012936245193653924,
        "epoch": 0.27300850734725446,
        "step": 2118
    },
    {
        "loss": 1.5573,
        "grad_norm": 2.3261544704437256,
        "learning_rate": 0.0001293042879517235,
        "epoch": 0.2731374065480794,
        "step": 2119
    },
    {
        "loss": 2.0842,
        "grad_norm": 1.569630742073059,
        "learning_rate": 0.00012924611311978623,
        "epoch": 0.27326630574890437,
        "step": 2120
    },
    {
        "loss": 2.2415,
        "grad_norm": 1.5418981313705444,
        "learning_rate": 0.0001291879274622611,
        "epoch": 0.2733952049497293,
        "step": 2121
    },
    {
        "loss": 2.4461,
        "grad_norm": 1.535770297050476,
        "learning_rate": 0.0001291297310006858,
        "epoch": 0.2735241041505543,
        "step": 2122
    },
    {
        "loss": 1.6634,
        "grad_norm": 1.977470874786377,
        "learning_rate": 0.0001290715237566021,
        "epoch": 0.2736530033513792,
        "step": 2123
    },
    {
        "loss": 2.213,
        "grad_norm": 1.7823818922042847,
        "learning_rate": 0.00012901330575155554,
        "epoch": 0.2737819025522042,
        "step": 2124
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.5752636194229126,
        "learning_rate": 0.00012895507700709592,
        "epoch": 0.2739108017530291,
        "step": 2125
    },
    {
        "loss": 2.013,
        "grad_norm": 1.6282418966293335,
        "learning_rate": 0.0001288968375447768,
        "epoch": 0.2740397009538541,
        "step": 2126
    },
    {
        "loss": 1.0705,
        "grad_norm": 1.561842918395996,
        "learning_rate": 0.0001288385873861559,
        "epoch": 0.27416860015467903,
        "step": 2127
    },
    {
        "loss": 1.7307,
        "grad_norm": 1.2175023555755615,
        "learning_rate": 0.00012878032655279468,
        "epoch": 0.274297499355504,
        "step": 2128
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.5303049087524414,
        "learning_rate": 0.00012872205506625877,
        "epoch": 0.27442639855632894,
        "step": 2129
    },
    {
        "loss": 1.2288,
        "grad_norm": 2.1709513664245605,
        "learning_rate": 0.00012866377294811754,
        "epoch": 0.2745552977571539,
        "step": 2130
    },
    {
        "loss": 2.2497,
        "grad_norm": 1.268265962600708,
        "learning_rate": 0.00012860548021994444,
        "epoch": 0.27468419695797885,
        "step": 2131
    },
    {
        "loss": 1.5912,
        "grad_norm": 1.487152338027954,
        "learning_rate": 0.0001285471769033168,
        "epoch": 0.27481309615880384,
        "step": 2132
    },
    {
        "loss": 1.8684,
        "grad_norm": 1.5058770179748535,
        "learning_rate": 0.00012848886301981586,
        "epoch": 0.27494199535962877,
        "step": 2133
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.654096841812134,
        "learning_rate": 0.0001284305385910268,
        "epoch": 0.27507089456045375,
        "step": 2134
    },
    {
        "loss": 1.693,
        "grad_norm": 1.499356746673584,
        "learning_rate": 0.00012837220363853865,
        "epoch": 0.2751997937612787,
        "step": 2135
    },
    {
        "loss": 2.3344,
        "grad_norm": 1.4726285934448242,
        "learning_rate": 0.00012831385818394444,
        "epoch": 0.27532869296210366,
        "step": 2136
    },
    {
        "loss": 2.2169,
        "grad_norm": 1.4804250001907349,
        "learning_rate": 0.0001282555022488409,
        "epoch": 0.2754575921629286,
        "step": 2137
    },
    {
        "loss": 1.9238,
        "grad_norm": 2.070040702819824,
        "learning_rate": 0.00012819713585482888,
        "epoch": 0.27558649136375357,
        "step": 2138
    },
    {
        "loss": 2.2313,
        "grad_norm": 1.1468441486358643,
        "learning_rate": 0.00012813875902351293,
        "epoch": 0.2757153905645785,
        "step": 2139
    },
    {
        "loss": 1.7297,
        "grad_norm": 1.7851654291152954,
        "learning_rate": 0.00012808037177650152,
        "epoch": 0.2758442897654034,
        "step": 2140
    },
    {
        "loss": 2.0559,
        "grad_norm": 1.7221981287002563,
        "learning_rate": 0.00012802197413540691,
        "epoch": 0.2759731889662284,
        "step": 2141
    },
    {
        "loss": 2.0315,
        "grad_norm": 1.233107089996338,
        "learning_rate": 0.00012796356612184532,
        "epoch": 0.27610208816705334,
        "step": 2142
    },
    {
        "loss": 2.3991,
        "grad_norm": 1.4550145864486694,
        "learning_rate": 0.00012790514775743673,
        "epoch": 0.2762309873678783,
        "step": 2143
    },
    {
        "loss": 1.6999,
        "grad_norm": 1.3975296020507812,
        "learning_rate": 0.00012784671906380498,
        "epoch": 0.27635988656870325,
        "step": 2144
    },
    {
        "loss": 1.7379,
        "grad_norm": 1.7683192491531372,
        "learning_rate": 0.00012778828006257777,
        "epoch": 0.27648878576952823,
        "step": 2145
    },
    {
        "loss": 2.2142,
        "grad_norm": 1.762190341949463,
        "learning_rate": 0.00012772983077538656,
        "epoch": 0.27661768497035316,
        "step": 2146
    },
    {
        "loss": 2.0261,
        "grad_norm": 1.4968699216842651,
        "learning_rate": 0.00012767137122386655,
        "epoch": 0.27674658417117814,
        "step": 2147
    },
    {
        "loss": 2.2293,
        "grad_norm": 1.7707014083862305,
        "learning_rate": 0.00012761290142965695,
        "epoch": 0.2768754833720031,
        "step": 2148
    },
    {
        "loss": 1.4766,
        "grad_norm": 1.8613418340682983,
        "learning_rate": 0.00012755442141440052,
        "epoch": 0.27700438257282806,
        "step": 2149
    },
    {
        "loss": 1.7567,
        "grad_norm": 2.290433883666992,
        "learning_rate": 0.00012749593119974398,
        "epoch": 0.277133281773653,
        "step": 2150
    },
    {
        "loss": 1.6904,
        "grad_norm": 1.4443347454071045,
        "learning_rate": 0.00012743743080733774,
        "epoch": 0.27726218097447797,
        "step": 2151
    },
    {
        "loss": 2.1034,
        "grad_norm": 0.9389395713806152,
        "learning_rate": 0.00012737892025883603,
        "epoch": 0.2773910801753029,
        "step": 2152
    },
    {
        "loss": 1.7207,
        "grad_norm": 2.3373019695281982,
        "learning_rate": 0.00012732039957589675,
        "epoch": 0.2775199793761279,
        "step": 2153
    },
    {
        "loss": 1.6394,
        "grad_norm": 3.683908224105835,
        "learning_rate": 0.0001272618687801816,
        "epoch": 0.2776488785769528,
        "step": 2154
    },
    {
        "loss": 2.374,
        "grad_norm": 1.4885824918746948,
        "learning_rate": 0.00012720332789335612,
        "epoch": 0.2777777777777778,
        "step": 2155
    },
    {
        "loss": 1.8731,
        "grad_norm": 2.255558967590332,
        "learning_rate": 0.00012714477693708947,
        "epoch": 0.2779066769786027,
        "step": 2156
    },
    {
        "loss": 2.5291,
        "grad_norm": 1.5567795038223267,
        "learning_rate": 0.0001270862159330545,
        "epoch": 0.2780355761794277,
        "step": 2157
    },
    {
        "loss": 1.9343,
        "grad_norm": 1.8569012880325317,
        "learning_rate": 0.0001270276449029279,
        "epoch": 0.27816447538025263,
        "step": 2158
    },
    {
        "loss": 2.2982,
        "grad_norm": 1.6882561445236206,
        "learning_rate": 0.00012696906386839,
        "epoch": 0.2782933745810776,
        "step": 2159
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.9425768852233887,
        "learning_rate": 0.00012691047285112486,
        "epoch": 0.27842227378190254,
        "step": 2160
    },
    {
        "loss": 2.3476,
        "grad_norm": 1.0429173707962036,
        "learning_rate": 0.0001268518718728202,
        "epoch": 0.2785511729827275,
        "step": 2161
    },
    {
        "loss": 2.716,
        "grad_norm": 1.5872522592544556,
        "learning_rate": 0.00012679326095516749,
        "epoch": 0.27868007218355245,
        "step": 2162
    },
    {
        "loss": 1.848,
        "grad_norm": 2.0766236782073975,
        "learning_rate": 0.00012673464011986183,
        "epoch": 0.27880897138437744,
        "step": 2163
    },
    {
        "loss": 1.8406,
        "grad_norm": 1.566454529762268,
        "learning_rate": 0.00012667600938860195,
        "epoch": 0.27893787058520236,
        "step": 2164
    },
    {
        "loss": 2.3191,
        "grad_norm": 1.5526912212371826,
        "learning_rate": 0.0001266173687830903,
        "epoch": 0.27906676978602735,
        "step": 2165
    },
    {
        "loss": 1.7135,
        "grad_norm": 2.531109571456909,
        "learning_rate": 0.00012655871832503309,
        "epoch": 0.2791956689868523,
        "step": 2166
    },
    {
        "loss": 2.3617,
        "grad_norm": 1.2485157251358032,
        "learning_rate": 0.00012650005803613992,
        "epoch": 0.27932456818767726,
        "step": 2167
    },
    {
        "loss": 1.5751,
        "grad_norm": 1.7615350484848022,
        "learning_rate": 0.00012644138793812426,
        "epoch": 0.2794534673885022,
        "step": 2168
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.4049595594406128,
        "learning_rate": 0.00012638270805270314,
        "epoch": 0.27958236658932717,
        "step": 2169
    },
    {
        "loss": 2.1455,
        "grad_norm": 1.3244330883026123,
        "learning_rate": 0.00012632401840159715,
        "epoch": 0.2797112657901521,
        "step": 2170
    },
    {
        "loss": 2.0909,
        "grad_norm": 1.743038296699524,
        "learning_rate": 0.00012626531900653053,
        "epoch": 0.2798401649909771,
        "step": 2171
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.331004023551941,
        "learning_rate": 0.00012620660988923118,
        "epoch": 0.279969064191802,
        "step": 2172
    },
    {
        "loss": 2.2204,
        "grad_norm": 1.3772417306900024,
        "learning_rate": 0.00012614789107143057,
        "epoch": 0.280097963392627,
        "step": 2173
    },
    {
        "loss": 2.1102,
        "grad_norm": 1.7586463689804077,
        "learning_rate": 0.0001260891625748637,
        "epoch": 0.2802268625934519,
        "step": 2174
    },
    {
        "loss": 2.0665,
        "grad_norm": 1.7055376768112183,
        "learning_rate": 0.00012603042442126925,
        "epoch": 0.2803557617942769,
        "step": 2175
    },
    {
        "loss": 1.896,
        "grad_norm": 2.0223066806793213,
        "learning_rate": 0.0001259716766323894,
        "epoch": 0.28048466099510183,
        "step": 2176
    },
    {
        "loss": 2.2752,
        "grad_norm": 1.1890116930007935,
        "learning_rate": 0.00012591291922996992,
        "epoch": 0.28061356019592676,
        "step": 2177
    },
    {
        "loss": 2.0981,
        "grad_norm": 1.7677555084228516,
        "learning_rate": 0.00012585415223576014,
        "epoch": 0.28074245939675174,
        "step": 2178
    },
    {
        "loss": 1.3728,
        "grad_norm": 1.7923113107681274,
        "learning_rate": 0.000125795375671513,
        "epoch": 0.28087135859757667,
        "step": 2179
    },
    {
        "loss": 1.5984,
        "grad_norm": 1.483837604522705,
        "learning_rate": 0.00012573658955898487,
        "epoch": 0.28100025779840165,
        "step": 2180
    },
    {
        "loss": 1.4193,
        "grad_norm": 3.2577829360961914,
        "learning_rate": 0.00012567779391993572,
        "epoch": 0.2811291569992266,
        "step": 2181
    },
    {
        "loss": 2.3641,
        "grad_norm": 1.4604499340057373,
        "learning_rate": 0.00012561898877612898,
        "epoch": 0.28125805620005156,
        "step": 2182
    },
    {
        "loss": 2.19,
        "grad_norm": 1.6391044855117798,
        "learning_rate": 0.00012556017414933174,
        "epoch": 0.2813869554008765,
        "step": 2183
    },
    {
        "loss": 2.5963,
        "grad_norm": 1.7131881713867188,
        "learning_rate": 0.0001255013500613145,
        "epoch": 0.2815158546017015,
        "step": 2184
    },
    {
        "loss": 1.4417,
        "grad_norm": 2.0740320682525635,
        "learning_rate": 0.00012544251653385125,
        "epoch": 0.2816447538025264,
        "step": 2185
    },
    {
        "loss": 1.2817,
        "grad_norm": 2.128387928009033,
        "learning_rate": 0.00012538367358871949,
        "epoch": 0.2817736530033514,
        "step": 2186
    },
    {
        "loss": 1.4366,
        "grad_norm": 2.0136404037475586,
        "learning_rate": 0.00012532482124770025,
        "epoch": 0.2819025522041763,
        "step": 2187
    },
    {
        "loss": 1.7135,
        "grad_norm": 1.300275206565857,
        "learning_rate": 0.000125265959532578,
        "epoch": 0.2820314514050013,
        "step": 2188
    },
    {
        "loss": 1.6979,
        "grad_norm": 1.754847764968872,
        "learning_rate": 0.00012520708846514063,
        "epoch": 0.2821603506058262,
        "step": 2189
    },
    {
        "loss": 2.2541,
        "grad_norm": 1.8604557514190674,
        "learning_rate": 0.0001251482080671796,
        "epoch": 0.2822892498066512,
        "step": 2190
    },
    {
        "loss": 1.7938,
        "grad_norm": 1.538676142692566,
        "learning_rate": 0.00012508931836048977,
        "epoch": 0.28241814900747614,
        "step": 2191
    },
    {
        "loss": 1.1558,
        "grad_norm": 1.8382478952407837,
        "learning_rate": 0.00012503041936686942,
        "epoch": 0.2825470482083011,
        "step": 2192
    },
    {
        "loss": 1.5052,
        "grad_norm": 1.487518548965454,
        "learning_rate": 0.00012497151110812032,
        "epoch": 0.28267594740912605,
        "step": 2193
    },
    {
        "loss": 1.8453,
        "grad_norm": 1.8739888668060303,
        "learning_rate": 0.00012491259360604762,
        "epoch": 0.28280484660995103,
        "step": 2194
    },
    {
        "loss": 2.0031,
        "grad_norm": 1.500672698020935,
        "learning_rate": 0.0001248536668824599,
        "epoch": 0.28293374581077596,
        "step": 2195
    },
    {
        "loss": 1.6769,
        "grad_norm": 2.623722553253174,
        "learning_rate": 0.00012479473095916924,
        "epoch": 0.28306264501160094,
        "step": 2196
    },
    {
        "loss": 2.3547,
        "grad_norm": 1.361646056175232,
        "learning_rate": 0.00012473578585799094,
        "epoch": 0.28319154421242587,
        "step": 2197
    },
    {
        "loss": 2.2878,
        "grad_norm": 1.355973243713379,
        "learning_rate": 0.00012467683160074386,
        "epoch": 0.28332044341325086,
        "step": 2198
    },
    {
        "loss": 2.3711,
        "grad_norm": 1.7623029947280884,
        "learning_rate": 0.00012461786820925022,
        "epoch": 0.2834493426140758,
        "step": 2199
    },
    {
        "loss": 1.7826,
        "grad_norm": 2.8677995204925537,
        "learning_rate": 0.00012455889570533562,
        "epoch": 0.28357824181490077,
        "step": 2200
    },
    {
        "loss": 2.2447,
        "grad_norm": 2.143883466720581,
        "learning_rate": 0.00012449991411082894,
        "epoch": 0.2837071410157257,
        "step": 2201
    },
    {
        "loss": 1.6875,
        "grad_norm": 2.0130326747894287,
        "learning_rate": 0.00012444092344756258,
        "epoch": 0.2838360402165507,
        "step": 2202
    },
    {
        "loss": 1.055,
        "grad_norm": 2.728473424911499,
        "learning_rate": 0.00012438192373737217,
        "epoch": 0.2839649394173756,
        "step": 2203
    },
    {
        "loss": 2.2501,
        "grad_norm": 1.8734443187713623,
        "learning_rate": 0.0001243229150020967,
        "epoch": 0.2840938386182006,
        "step": 2204
    },
    {
        "loss": 2.1747,
        "grad_norm": 1.2896133661270142,
        "learning_rate": 0.00012426389726357857,
        "epoch": 0.2842227378190255,
        "step": 2205
    },
    {
        "loss": 2.1528,
        "grad_norm": 1.2188379764556885,
        "learning_rate": 0.0001242048705436635,
        "epoch": 0.2843516370198505,
        "step": 2206
    },
    {
        "loss": 1.8476,
        "grad_norm": 2.517577648162842,
        "learning_rate": 0.00012414583486420043,
        "epoch": 0.28448053622067543,
        "step": 2207
    },
    {
        "loss": 2.4724,
        "grad_norm": 2.3575189113616943,
        "learning_rate": 0.00012408679024704183,
        "epoch": 0.2846094354215004,
        "step": 2208
    },
    {
        "loss": 1.5924,
        "grad_norm": 2.5434930324554443,
        "learning_rate": 0.00012402773671404323,
        "epoch": 0.28473833462232534,
        "step": 2209
    },
    {
        "loss": 1.6402,
        "grad_norm": 2.3398284912109375,
        "learning_rate": 0.0001239686742870636,
        "epoch": 0.2848672338231503,
        "step": 2210
    },
    {
        "loss": 1.166,
        "grad_norm": 2.2083070278167725,
        "learning_rate": 0.00012390960298796523,
        "epoch": 0.28499613302397525,
        "step": 2211
    },
    {
        "loss": 2.1385,
        "grad_norm": 1.7395061254501343,
        "learning_rate": 0.0001238505228386136,
        "epoch": 0.2851250322248002,
        "step": 2212
    },
    {
        "loss": 2.2175,
        "grad_norm": 1.928809404373169,
        "learning_rate": 0.00012379143386087753,
        "epoch": 0.28525393142562516,
        "step": 2213
    },
    {
        "loss": 2.2746,
        "grad_norm": 1.712294101715088,
        "learning_rate": 0.00012373233607662902,
        "epoch": 0.2853828306264501,
        "step": 2214
    },
    {
        "loss": 1.9252,
        "grad_norm": 1.8928719758987427,
        "learning_rate": 0.00012367322950774346,
        "epoch": 0.2855117298272751,
        "step": 2215
    },
    {
        "loss": 1.5651,
        "grad_norm": 2.1095685958862305,
        "learning_rate": 0.00012361411417609946,
        "epoch": 0.2856406290281,
        "step": 2216
    },
    {
        "loss": 2.1973,
        "grad_norm": 1.493742823600769,
        "learning_rate": 0.00012355499010357874,
        "epoch": 0.285769528228925,
        "step": 2217
    },
    {
        "loss": 1.9362,
        "grad_norm": 1.4894100427627563,
        "learning_rate": 0.00012349585731206644,
        "epoch": 0.2858984274297499,
        "step": 2218
    },
    {
        "loss": 1.5422,
        "grad_norm": 1.5155813694000244,
        "learning_rate": 0.00012343671582345082,
        "epoch": 0.2860273266305749,
        "step": 2219
    },
    {
        "loss": 2.025,
        "grad_norm": 2.6327433586120605,
        "learning_rate": 0.00012337756565962334,
        "epoch": 0.2861562258313998,
        "step": 2220
    },
    {
        "loss": 0.7371,
        "grad_norm": 1.95970618724823,
        "learning_rate": 0.00012331840684247881,
        "epoch": 0.2862851250322248,
        "step": 2221
    },
    {
        "loss": 1.6936,
        "grad_norm": 2.527801990509033,
        "learning_rate": 0.00012325923939391509,
        "epoch": 0.28641402423304974,
        "step": 2222
    },
    {
        "loss": 1.8619,
        "grad_norm": 1.3930060863494873,
        "learning_rate": 0.00012320006333583325,
        "epoch": 0.2865429234338747,
        "step": 2223
    },
    {
        "loss": 1.4165,
        "grad_norm": 2.2430167198181152,
        "learning_rate": 0.00012314087869013762,
        "epoch": 0.28667182263469965,
        "step": 2224
    },
    {
        "loss": 1.8533,
        "grad_norm": 2.264570713043213,
        "learning_rate": 0.00012308168547873574,
        "epoch": 0.28680072183552463,
        "step": 2225
    },
    {
        "loss": 1.6499,
        "grad_norm": 2.5369303226470947,
        "learning_rate": 0.00012302248372353822,
        "epoch": 0.28692962103634956,
        "step": 2226
    },
    {
        "loss": 1.6088,
        "grad_norm": 2.297685146331787,
        "learning_rate": 0.0001229632734464588,
        "epoch": 0.28705852023717454,
        "step": 2227
    },
    {
        "loss": 1.7827,
        "grad_norm": 1.8907742500305176,
        "learning_rate": 0.00012290405466941457,
        "epoch": 0.28718741943799947,
        "step": 2228
    },
    {
        "loss": 1.978,
        "grad_norm": 1.5483695268630981,
        "learning_rate": 0.00012284482741432555,
        "epoch": 0.28731631863882445,
        "step": 2229
    },
    {
        "loss": 1.9582,
        "grad_norm": 1.9319653511047363,
        "learning_rate": 0.00012278559170311498,
        "epoch": 0.2874452178396494,
        "step": 2230
    },
    {
        "loss": 1.3026,
        "grad_norm": 2.2121212482452393,
        "learning_rate": 0.00012272634755770934,
        "epoch": 0.28757411704047436,
        "step": 2231
    },
    {
        "loss": 2.225,
        "grad_norm": 1.245364785194397,
        "learning_rate": 0.00012266709500003805,
        "epoch": 0.2877030162412993,
        "step": 2232
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.6118690967559814,
        "learning_rate": 0.00012260783405203376,
        "epoch": 0.2878319154421243,
        "step": 2233
    },
    {
        "loss": 2.1594,
        "grad_norm": 2.322720766067505,
        "learning_rate": 0.00012254856473563214,
        "epoch": 0.2879608146429492,
        "step": 2234
    },
    {
        "loss": 2.277,
        "grad_norm": 1.165278673171997,
        "learning_rate": 0.0001224892870727721,
        "epoch": 0.2880897138437742,
        "step": 2235
    },
    {
        "loss": 2.3202,
        "grad_norm": 1.9892899990081787,
        "learning_rate": 0.00012243000108539548,
        "epoch": 0.2882186130445991,
        "step": 2236
    },
    {
        "loss": 2.4753,
        "grad_norm": 1.466374397277832,
        "learning_rate": 0.00012237070679544724,
        "epoch": 0.2883475122454241,
        "step": 2237
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.988826870918274,
        "learning_rate": 0.00012231140422487555,
        "epoch": 0.288476411446249,
        "step": 2238
    },
    {
        "loss": 2.213,
        "grad_norm": 1.5374314785003662,
        "learning_rate": 0.00012225209339563145,
        "epoch": 0.288605310647074,
        "step": 2239
    },
    {
        "loss": 2.079,
        "grad_norm": 2.172861337661743,
        "learning_rate": 0.00012219277432966912,
        "epoch": 0.28873420984789894,
        "step": 2240
    },
    {
        "loss": 1.3892,
        "grad_norm": 1.523555040359497,
        "learning_rate": 0.0001221334470489459,
        "epoch": 0.2888631090487239,
        "step": 2241
    },
    {
        "loss": 1.6291,
        "grad_norm": 1.571834921836853,
        "learning_rate": 0.00012207411157542198,
        "epoch": 0.28899200824954885,
        "step": 2242
    },
    {
        "loss": 1.8715,
        "grad_norm": 1.53742253780365,
        "learning_rate": 0.0001220147679310607,
        "epoch": 0.28912090745037383,
        "step": 2243
    },
    {
        "loss": 1.987,
        "grad_norm": 2.060659885406494,
        "learning_rate": 0.0001219554161378283,
        "epoch": 0.28924980665119876,
        "step": 2244
    },
    {
        "loss": 2.3243,
        "grad_norm": 1.7994041442871094,
        "learning_rate": 0.00012189605621769426,
        "epoch": 0.28937870585202374,
        "step": 2245
    },
    {
        "loss": 1.8459,
        "grad_norm": 1.5855653285980225,
        "learning_rate": 0.00012183668819263086,
        "epoch": 0.28950760505284867,
        "step": 2246
    },
    {
        "loss": 2.261,
        "grad_norm": 1.5563209056854248,
        "learning_rate": 0.00012177731208461347,
        "epoch": 0.28963650425367365,
        "step": 2247
    },
    {
        "loss": 2.3404,
        "grad_norm": 1.9893434047698975,
        "learning_rate": 0.00012171792791562044,
        "epoch": 0.2897654034544986,
        "step": 2248
    },
    {
        "loss": 1.3643,
        "grad_norm": 2.378850221633911,
        "learning_rate": 0.00012165853570763315,
        "epoch": 0.2898943026553235,
        "step": 2249
    },
    {
        "loss": 1.9885,
        "grad_norm": 1.983151912689209,
        "learning_rate": 0.00012159913548263582,
        "epoch": 0.2900232018561485,
        "step": 2250
    },
    {
        "loss": 1.9922,
        "grad_norm": 2.054788827896118,
        "learning_rate": 0.0001215397272626158,
        "epoch": 0.2901521010569734,
        "step": 2251
    },
    {
        "loss": 1.7408,
        "grad_norm": 1.7845548391342163,
        "learning_rate": 0.00012148031106956328,
        "epoch": 0.2902810002577984,
        "step": 2252
    },
    {
        "loss": 1.5542,
        "grad_norm": 2.2510008811950684,
        "learning_rate": 0.00012142088692547145,
        "epoch": 0.29040989945862333,
        "step": 2253
    },
    {
        "loss": 2.4043,
        "grad_norm": 1.242072582244873,
        "learning_rate": 0.00012136145485233648,
        "epoch": 0.2905387986594483,
        "step": 2254
    },
    {
        "loss": 1.0622,
        "grad_norm": 2.217038631439209,
        "learning_rate": 0.0001213020148721574,
        "epoch": 0.29066769786027324,
        "step": 2255
    },
    {
        "loss": 2.4797,
        "grad_norm": 1.0121936798095703,
        "learning_rate": 0.00012124256700693621,
        "epoch": 0.2907965970610982,
        "step": 2256
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.5584521293640137,
        "learning_rate": 0.00012118311127867778,
        "epoch": 0.29092549626192316,
        "step": 2257
    },
    {
        "loss": 1.2619,
        "grad_norm": 1.8271914720535278,
        "learning_rate": 0.00012112364770939001,
        "epoch": 0.29105439546274814,
        "step": 2258
    },
    {
        "loss": 2.1248,
        "grad_norm": 2.0746915340423584,
        "learning_rate": 0.00012106417632108359,
        "epoch": 0.29118329466357307,
        "step": 2259
    },
    {
        "loss": 1.8297,
        "grad_norm": 2.09965181350708,
        "learning_rate": 0.00012100469713577212,
        "epoch": 0.29131219386439805,
        "step": 2260
    },
    {
        "loss": 1.749,
        "grad_norm": 2.0065133571624756,
        "learning_rate": 0.00012094521017547213,
        "epoch": 0.291441093065223,
        "step": 2261
    },
    {
        "loss": 2.4186,
        "grad_norm": 1.7093669176101685,
        "learning_rate": 0.00012088571546220293,
        "epoch": 0.29156999226604796,
        "step": 2262
    },
    {
        "loss": 1.8433,
        "grad_norm": 2.5225071907043457,
        "learning_rate": 0.00012082621301798686,
        "epoch": 0.2916988914668729,
        "step": 2263
    },
    {
        "loss": 2.3314,
        "grad_norm": 1.7572687864303589,
        "learning_rate": 0.00012076670286484897,
        "epoch": 0.2918277906676979,
        "step": 2264
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.9683492183685303,
        "learning_rate": 0.00012070718502481731,
        "epoch": 0.2919566898685228,
        "step": 2265
    },
    {
        "loss": 2.3888,
        "grad_norm": 1.0124151706695557,
        "learning_rate": 0.00012064765951992263,
        "epoch": 0.2920855890693478,
        "step": 2266
    },
    {
        "loss": 1.6925,
        "grad_norm": 1.468513011932373,
        "learning_rate": 0.00012058812637219852,
        "epoch": 0.2922144882701727,
        "step": 2267
    },
    {
        "loss": 1.4853,
        "grad_norm": 2.0012073516845703,
        "learning_rate": 0.00012052858560368156,
        "epoch": 0.2923433874709977,
        "step": 2268
    },
    {
        "loss": 2.5436,
        "grad_norm": 1.5576262474060059,
        "learning_rate": 0.000120469037236411,
        "epoch": 0.2924722866718226,
        "step": 2269
    },
    {
        "loss": 1.9797,
        "grad_norm": 1.4174565076828003,
        "learning_rate": 0.00012040948129242897,
        "epoch": 0.2926011858726476,
        "step": 2270
    },
    {
        "loss": 2.0608,
        "grad_norm": 1.2925829887390137,
        "learning_rate": 0.00012034991779378037,
        "epoch": 0.29273008507347253,
        "step": 2271
    },
    {
        "loss": 1.9623,
        "grad_norm": 1.7795778512954712,
        "learning_rate": 0.00012029034676251297,
        "epoch": 0.2928589842742975,
        "step": 2272
    },
    {
        "loss": 2.2276,
        "grad_norm": 1.4280807971954346,
        "learning_rate": 0.00012023076822067714,
        "epoch": 0.29298788347512245,
        "step": 2273
    },
    {
        "loss": 2.1054,
        "grad_norm": 1.4045913219451904,
        "learning_rate": 0.00012017118219032629,
        "epoch": 0.29311678267594743,
        "step": 2274
    },
    {
        "loss": 1.9255,
        "grad_norm": 1.5496504306793213,
        "learning_rate": 0.00012011158869351642,
        "epoch": 0.29324568187677236,
        "step": 2275
    },
    {
        "loss": 1.4165,
        "grad_norm": 1.4310039281845093,
        "learning_rate": 0.00012005198775230635,
        "epoch": 0.29337458107759734,
        "step": 2276
    },
    {
        "loss": 1.962,
        "grad_norm": 2.0954113006591797,
        "learning_rate": 0.00011999237938875762,
        "epoch": 0.29350348027842227,
        "step": 2277
    },
    {
        "loss": 2.1273,
        "grad_norm": 1.32851243019104,
        "learning_rate": 0.00011993276362493462,
        "epoch": 0.29363237947924725,
        "step": 2278
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.616755723953247,
        "learning_rate": 0.00011987314048290434,
        "epoch": 0.2937612786800722,
        "step": 2279
    },
    {
        "loss": 1.7561,
        "grad_norm": 2.046226978302002,
        "learning_rate": 0.00011981350998473661,
        "epoch": 0.29389017788089716,
        "step": 2280
    },
    {
        "loss": 1.7641,
        "grad_norm": 1.4986532926559448,
        "learning_rate": 0.00011975387215250393,
        "epoch": 0.2940190770817221,
        "step": 2281
    },
    {
        "loss": 2.5947,
        "grad_norm": 1.0610451698303223,
        "learning_rate": 0.00011969422700828157,
        "epoch": 0.2941479762825471,
        "step": 2282
    },
    {
        "loss": 1.7051,
        "grad_norm": 2.2816150188446045,
        "learning_rate": 0.00011963457457414735,
        "epoch": 0.294276875483372,
        "step": 2283
    },
    {
        "loss": 1.2371,
        "grad_norm": 2.3793671131134033,
        "learning_rate": 0.00011957491487218204,
        "epoch": 0.29440577468419693,
        "step": 2284
    },
    {
        "loss": 2.0478,
        "grad_norm": 1.8402401208877563,
        "learning_rate": 0.00011951524792446886,
        "epoch": 0.2945346738850219,
        "step": 2285
    },
    {
        "loss": 1.7049,
        "grad_norm": 2.2059969902038574,
        "learning_rate": 0.00011945557375309386,
        "epoch": 0.29466357308584684,
        "step": 2286
    },
    {
        "loss": 2.3742,
        "grad_norm": 1.9229656457901,
        "learning_rate": 0.00011939589238014571,
        "epoch": 0.2947924722866718,
        "step": 2287
    },
    {
        "loss": 2.037,
        "grad_norm": 1.370654821395874,
        "learning_rate": 0.00011933620382771576,
        "epoch": 0.29492137148749675,
        "step": 2288
    },
    {
        "loss": 2.8471,
        "grad_norm": 1.4025044441223145,
        "learning_rate": 0.000119276508117898,
        "epoch": 0.29505027068832174,
        "step": 2289
    },
    {
        "loss": 2.4117,
        "grad_norm": 1.6081557273864746,
        "learning_rate": 0.00011921680527278906,
        "epoch": 0.29517916988914666,
        "step": 2290
    },
    {
        "loss": 1.7423,
        "grad_norm": 2.8108389377593994,
        "learning_rate": 0.00011915709531448829,
        "epoch": 0.29530806908997165,
        "step": 2291
    },
    {
        "loss": 2.6794,
        "grad_norm": 1.1606048345565796,
        "learning_rate": 0.00011909737826509756,
        "epoch": 0.2954369682907966,
        "step": 2292
    },
    {
        "loss": 2.0345,
        "grad_norm": 2.286741256713867,
        "learning_rate": 0.00011903765414672146,
        "epoch": 0.29556586749162156,
        "step": 2293
    },
    {
        "loss": 1.9585,
        "grad_norm": 1.718138575553894,
        "learning_rate": 0.00011897792298146714,
        "epoch": 0.2956947666924465,
        "step": 2294
    },
    {
        "loss": 2.101,
        "grad_norm": 2.0075387954711914,
        "learning_rate": 0.00011891818479144438,
        "epoch": 0.29582366589327147,
        "step": 2295
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.8872069120407104,
        "learning_rate": 0.00011885843959876557,
        "epoch": 0.2959525650940964,
        "step": 2296
    },
    {
        "loss": 1.6358,
        "grad_norm": 2.9191267490386963,
        "learning_rate": 0.0001187986874255456,
        "epoch": 0.2960814642949214,
        "step": 2297
    },
    {
        "loss": 1.5316,
        "grad_norm": 1.7625097036361694,
        "learning_rate": 0.00011873892829390212,
        "epoch": 0.2962103634957463,
        "step": 2298
    },
    {
        "loss": 2.0199,
        "grad_norm": 1.5867338180541992,
        "learning_rate": 0.00011867916222595521,
        "epoch": 0.2963392626965713,
        "step": 2299
    },
    {
        "loss": 1.9673,
        "grad_norm": 2.1761765480041504,
        "learning_rate": 0.00011861938924382754,
        "epoch": 0.2964681618973962,
        "step": 2300
    },
    {
        "loss": 1.7518,
        "grad_norm": 1.7854889631271362,
        "learning_rate": 0.00011855960936964445,
        "epoch": 0.2965970610982212,
        "step": 2301
    },
    {
        "loss": 1.6781,
        "grad_norm": 1.6187520027160645,
        "learning_rate": 0.00011849982262553363,
        "epoch": 0.29672596029904613,
        "step": 2302
    },
    {
        "loss": 1.4994,
        "grad_norm": 1.3806140422821045,
        "learning_rate": 0.0001184400290336255,
        "epoch": 0.2968548594998711,
        "step": 2303
    },
    {
        "loss": 1.3258,
        "grad_norm": 1.9096628427505493,
        "learning_rate": 0.00011838022861605292,
        "epoch": 0.29698375870069604,
        "step": 2304
    },
    {
        "loss": 1.5894,
        "grad_norm": 2.2880077362060547,
        "learning_rate": 0.00011832042139495134,
        "epoch": 0.297112657901521,
        "step": 2305
    },
    {
        "loss": 2.1365,
        "grad_norm": 1.4401639699935913,
        "learning_rate": 0.00011826060739245866,
        "epoch": 0.29724155710234595,
        "step": 2306
    },
    {
        "loss": 1.8469,
        "grad_norm": 3.15385365486145,
        "learning_rate": 0.00011820078663071528,
        "epoch": 0.29737045630317094,
        "step": 2307
    },
    {
        "loss": 2.2343,
        "grad_norm": 1.7138065099716187,
        "learning_rate": 0.00011814095913186416,
        "epoch": 0.29749935550399587,
        "step": 2308
    },
    {
        "loss": 2.1585,
        "grad_norm": 1.6730929613113403,
        "learning_rate": 0.00011808112491805077,
        "epoch": 0.29762825470482085,
        "step": 2309
    },
    {
        "loss": 2.4755,
        "grad_norm": 1.6844007968902588,
        "learning_rate": 0.00011802128401142295,
        "epoch": 0.2977571539056458,
        "step": 2310
    },
    {
        "loss": 1.8451,
        "grad_norm": 1.6081045866012573,
        "learning_rate": 0.00011796143643413122,
        "epoch": 0.29788605310647076,
        "step": 2311
    },
    {
        "loss": 1.9976,
        "grad_norm": 1.5911363363265991,
        "learning_rate": 0.00011790158220832836,
        "epoch": 0.2980149523072957,
        "step": 2312
    },
    {
        "loss": 2.2441,
        "grad_norm": 1.0676015615463257,
        "learning_rate": 0.00011784172135616969,
        "epoch": 0.29814385150812067,
        "step": 2313
    },
    {
        "loss": 2.0607,
        "grad_norm": 2.034278392791748,
        "learning_rate": 0.000117781853899813,
        "epoch": 0.2982727507089456,
        "step": 2314
    },
    {
        "loss": 2.2189,
        "grad_norm": 1.4545245170593262,
        "learning_rate": 0.00011772197986141855,
        "epoch": 0.2984016499097706,
        "step": 2315
    },
    {
        "loss": 1.0025,
        "grad_norm": 2.1630618572235107,
        "learning_rate": 0.00011766209926314893,
        "epoch": 0.2985305491105955,
        "step": 2316
    },
    {
        "loss": 1.9597,
        "grad_norm": 2.169370412826538,
        "learning_rate": 0.00011760221212716934,
        "epoch": 0.2986594483114205,
        "step": 2317
    },
    {
        "loss": 1.0352,
        "grad_norm": 2.1085362434387207,
        "learning_rate": 0.00011754231847564719,
        "epoch": 0.2987883475122454,
        "step": 2318
    },
    {
        "loss": 1.9073,
        "grad_norm": 1.5456665754318237,
        "learning_rate": 0.00011748241833075241,
        "epoch": 0.2989172467130704,
        "step": 2319
    },
    {
        "loss": 1.7895,
        "grad_norm": 1.78773832321167,
        "learning_rate": 0.00011742251171465734,
        "epoch": 0.29904614591389533,
        "step": 2320
    },
    {
        "loss": 2.6314,
        "grad_norm": 1.5309805870056152,
        "learning_rate": 0.00011736259864953671,
        "epoch": 0.29917504511472026,
        "step": 2321
    },
    {
        "loss": 0.3447,
        "grad_norm": 0.9104504585266113,
        "learning_rate": 0.00011730267915756765,
        "epoch": 0.29930394431554525,
        "step": 2322
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.8723423480987549,
        "learning_rate": 0.00011724275326092957,
        "epoch": 0.2994328435163702,
        "step": 2323
    },
    {
        "loss": 1.5439,
        "grad_norm": 2.2789981365203857,
        "learning_rate": 0.00011718282098180442,
        "epoch": 0.29956174271719516,
        "step": 2324
    },
    {
        "loss": 1.6968,
        "grad_norm": 2.5663187503814697,
        "learning_rate": 0.00011712288234237634,
        "epoch": 0.2996906419180201,
        "step": 2325
    },
    {
        "loss": 2.019,
        "grad_norm": 1.7525050640106201,
        "learning_rate": 0.0001170629373648319,
        "epoch": 0.29981954111884507,
        "step": 2326
    },
    {
        "loss": 2.163,
        "grad_norm": 1.2634590864181519,
        "learning_rate": 0.00011700298607136007,
        "epoch": 0.29994844031967,
        "step": 2327
    },
    {
        "loss": 1.9533,
        "grad_norm": 1.950512170791626,
        "learning_rate": 0.00011694302848415208,
        "epoch": 0.300077339520495,
        "step": 2328
    },
    {
        "loss": 2.2415,
        "grad_norm": 2.183320999145508,
        "learning_rate": 0.00011688306462540155,
        "epoch": 0.3002062387213199,
        "step": 2329
    },
    {
        "loss": 1.8979,
        "grad_norm": 1.2333015203475952,
        "learning_rate": 0.00011682309451730433,
        "epoch": 0.3003351379221449,
        "step": 2330
    },
    {
        "loss": 1.7393,
        "grad_norm": 0.9955580830574036,
        "learning_rate": 0.00011676311818205867,
        "epoch": 0.3004640371229698,
        "step": 2331
    },
    {
        "loss": 1.5694,
        "grad_norm": 1.9015254974365234,
        "learning_rate": 0.0001167031356418651,
        "epoch": 0.3005929363237948,
        "step": 2332
    },
    {
        "loss": 1.6236,
        "grad_norm": 1.9136567115783691,
        "learning_rate": 0.00011664314691892642,
        "epoch": 0.30072183552461973,
        "step": 2333
    },
    {
        "loss": 2.2523,
        "grad_norm": 1.5070706605911255,
        "learning_rate": 0.00011658315203544778,
        "epoch": 0.3008507347254447,
        "step": 2334
    },
    {
        "loss": 2.4294,
        "grad_norm": 1.2422170639038086,
        "learning_rate": 0.00011652315101363653,
        "epoch": 0.30097963392626964,
        "step": 2335
    },
    {
        "loss": 1.9768,
        "grad_norm": 2.1968636512756348,
        "learning_rate": 0.00011646314387570234,
        "epoch": 0.3011085331270946,
        "step": 2336
    },
    {
        "loss": 2.1574,
        "grad_norm": 1.5248628854751587,
        "learning_rate": 0.0001164031306438572,
        "epoch": 0.30123743232791955,
        "step": 2337
    },
    {
        "loss": 2.3727,
        "grad_norm": 1.8613395690917969,
        "learning_rate": 0.00011634311134031519,
        "epoch": 0.30136633152874454,
        "step": 2338
    },
    {
        "loss": 1.9739,
        "grad_norm": 1.32151198387146,
        "learning_rate": 0.00011628308598729285,
        "epoch": 0.30149523072956946,
        "step": 2339
    },
    {
        "loss": 2.343,
        "grad_norm": 1.689182996749878,
        "learning_rate": 0.00011622305460700875,
        "epoch": 0.30162412993039445,
        "step": 2340
    },
    {
        "loss": 2.4726,
        "grad_norm": 1.4041938781738281,
        "learning_rate": 0.00011616301722168388,
        "epoch": 0.3017530291312194,
        "step": 2341
    },
    {
        "loss": 1.97,
        "grad_norm": 2.741795063018799,
        "learning_rate": 0.00011610297385354125,
        "epoch": 0.30188192833204436,
        "step": 2342
    },
    {
        "loss": 2.1578,
        "grad_norm": 1.7017978429794312,
        "learning_rate": 0.00011604292452480629,
        "epoch": 0.3020108275328693,
        "step": 2343
    },
    {
        "loss": 1.8222,
        "grad_norm": 1.043789029121399,
        "learning_rate": 0.00011598286925770653,
        "epoch": 0.30213972673369427,
        "step": 2344
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.9458487033843994,
        "learning_rate": 0.00011592280807447174,
        "epoch": 0.3022686259345192,
        "step": 2345
    },
    {
        "loss": 1.892,
        "grad_norm": 1.7540332078933716,
        "learning_rate": 0.00011586274099733376,
        "epoch": 0.3023975251353442,
        "step": 2346
    },
    {
        "loss": 0.9552,
        "grad_norm": 3.4475924968719482,
        "learning_rate": 0.00011580266804852682,
        "epoch": 0.3025264243361691,
        "step": 2347
    },
    {
        "loss": 1.9381,
        "grad_norm": 1.6758482456207275,
        "learning_rate": 0.00011574258925028712,
        "epoch": 0.3026553235369941,
        "step": 2348
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.2787587642669678,
        "learning_rate": 0.00011568250462485313,
        "epoch": 0.302784222737819,
        "step": 2349
    },
    {
        "loss": 0.4537,
        "grad_norm": 1.4655838012695312,
        "learning_rate": 0.00011562241419446544,
        "epoch": 0.302913121938644,
        "step": 2350
    },
    {
        "loss": 1.6375,
        "grad_norm": 1.956754207611084,
        "learning_rate": 0.00011556231798136688,
        "epoch": 0.30304202113946893,
        "step": 2351
    },
    {
        "loss": 1.9603,
        "grad_norm": 2.347266912460327,
        "learning_rate": 0.00011550221600780234,
        "epoch": 0.3031709203402939,
        "step": 2352
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.990067720413208,
        "learning_rate": 0.00011544210829601876,
        "epoch": 0.30329981954111884,
        "step": 2353
    },
    {
        "loss": 1.9531,
        "grad_norm": 1.4526888132095337,
        "learning_rate": 0.00011538199486826538,
        "epoch": 0.3034287187419438,
        "step": 2354
    },
    {
        "loss": 2.0484,
        "grad_norm": 1.9322788715362549,
        "learning_rate": 0.00011532187574679344,
        "epoch": 0.30355761794276875,
        "step": 2355
    },
    {
        "loss": 1.4015,
        "grad_norm": 2.495980978012085,
        "learning_rate": 0.00011526175095385634,
        "epoch": 0.30368651714359374,
        "step": 2356
    },
    {
        "loss": 2.1854,
        "grad_norm": 2.1636099815368652,
        "learning_rate": 0.00011520162051170956,
        "epoch": 0.30381541634441867,
        "step": 2357
    },
    {
        "loss": 1.5351,
        "grad_norm": 1.6164000034332275,
        "learning_rate": 0.00011514148444261063,
        "epoch": 0.3039443155452436,
        "step": 2358
    },
    {
        "loss": 1.5121,
        "grad_norm": 1.109283208847046,
        "learning_rate": 0.00011508134276881923,
        "epoch": 0.3040732147460686,
        "step": 2359
    },
    {
        "loss": 1.7258,
        "grad_norm": 2.604198455810547,
        "learning_rate": 0.00011502119551259705,
        "epoch": 0.3042021139468935,
        "step": 2360
    },
    {
        "loss": 1.4951,
        "grad_norm": 2.2444100379943848,
        "learning_rate": 0.00011496104269620795,
        "epoch": 0.3043310131477185,
        "step": 2361
    },
    {
        "loss": 2.1378,
        "grad_norm": 1.773393988609314,
        "learning_rate": 0.00011490088434191774,
        "epoch": 0.3044599123485434,
        "step": 2362
    },
    {
        "loss": 1.6342,
        "grad_norm": 2.069908857345581,
        "learning_rate": 0.00011484072047199433,
        "epoch": 0.3045888115493684,
        "step": 2363
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.23972749710083,
        "learning_rate": 0.00011478055110870767,
        "epoch": 0.3047177107501933,
        "step": 2364
    },
    {
        "loss": 1.6264,
        "grad_norm": 0.9187575578689575,
        "learning_rate": 0.0001147203762743297,
        "epoch": 0.3048466099510183,
        "step": 2365
    },
    {
        "loss": 1.7699,
        "grad_norm": 1.7980079650878906,
        "learning_rate": 0.00011466019599113442,
        "epoch": 0.30497550915184324,
        "step": 2366
    },
    {
        "loss": 1.8387,
        "grad_norm": 1.8961102962493896,
        "learning_rate": 0.00011460001028139793,
        "epoch": 0.3051044083526682,
        "step": 2367
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.2242436408996582,
        "learning_rate": 0.00011453981916739818,
        "epoch": 0.30523330755349315,
        "step": 2368
    },
    {
        "loss": 2.3335,
        "grad_norm": 2.5808093547821045,
        "learning_rate": 0.00011447962267141523,
        "epoch": 0.30536220675431813,
        "step": 2369
    },
    {
        "loss": 1.9231,
        "grad_norm": 1.5438883304595947,
        "learning_rate": 0.00011441942081573105,
        "epoch": 0.30549110595514306,
        "step": 2370
    },
    {
        "loss": 2.3731,
        "grad_norm": 1.4674460887908936,
        "learning_rate": 0.00011435921362262971,
        "epoch": 0.30562000515596804,
        "step": 2371
    },
    {
        "loss": 2.0184,
        "grad_norm": 1.8418684005737305,
        "learning_rate": 0.00011429900111439715,
        "epoch": 0.30574890435679297,
        "step": 2372
    },
    {
        "loss": 2.383,
        "grad_norm": 1.132645845413208,
        "learning_rate": 0.0001142387833133213,
        "epoch": 0.30587780355761796,
        "step": 2373
    },
    {
        "loss": 2.5791,
        "grad_norm": 1.0381089448928833,
        "learning_rate": 0.00011417856024169214,
        "epoch": 0.3060067027584429,
        "step": 2374
    },
    {
        "loss": 2.2533,
        "grad_norm": 1.8357439041137695,
        "learning_rate": 0.00011411833192180145,
        "epoch": 0.30613560195926787,
        "step": 2375
    },
    {
        "loss": 1.9506,
        "grad_norm": 1.4528396129608154,
        "learning_rate": 0.00011405809837594305,
        "epoch": 0.3062645011600928,
        "step": 2376
    },
    {
        "loss": 2.3062,
        "grad_norm": 2.231740951538086,
        "learning_rate": 0.0001139978596264127,
        "epoch": 0.3063934003609178,
        "step": 2377
    },
    {
        "loss": 1.9809,
        "grad_norm": 1.6916266679763794,
        "learning_rate": 0.00011393761569550802,
        "epoch": 0.3065222995617427,
        "step": 2378
    },
    {
        "loss": 2.1567,
        "grad_norm": 1.8668771982192993,
        "learning_rate": 0.00011387736660552866,
        "epoch": 0.3066511987625677,
        "step": 2379
    },
    {
        "loss": 1.5277,
        "grad_norm": 2.243391752243042,
        "learning_rate": 0.00011381711237877597,
        "epoch": 0.3067800979633926,
        "step": 2380
    },
    {
        "loss": 1.3784,
        "grad_norm": 2.095235824584961,
        "learning_rate": 0.0001137568530375535,
        "epoch": 0.3069089971642176,
        "step": 2381
    },
    {
        "loss": 1.3044,
        "grad_norm": 2.4599010944366455,
        "learning_rate": 0.00011369658860416639,
        "epoch": 0.30703789636504253,
        "step": 2382
    },
    {
        "loss": 1.0262,
        "grad_norm": 3.1924784183502197,
        "learning_rate": 0.0001136363191009219,
        "epoch": 0.3071667955658675,
        "step": 2383
    },
    {
        "loss": 1.9902,
        "grad_norm": 2.3620400428771973,
        "learning_rate": 0.00011357604455012902,
        "epoch": 0.30729569476669244,
        "step": 2384
    },
    {
        "loss": 1.91,
        "grad_norm": 1.1417193412780762,
        "learning_rate": 0.00011351576497409873,
        "epoch": 0.3074245939675174,
        "step": 2385
    },
    {
        "loss": 1.9851,
        "grad_norm": 1.4167721271514893,
        "learning_rate": 0.00011345548039514371,
        "epoch": 0.30755349316834235,
        "step": 2386
    },
    {
        "loss": 1.6472,
        "grad_norm": 2.375328540802002,
        "learning_rate": 0.00011339519083557868,
        "epoch": 0.30768239236916733,
        "step": 2387
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.5601369142532349,
        "learning_rate": 0.00011333489631772005,
        "epoch": 0.30781129156999226,
        "step": 2388
    },
    {
        "loss": 2.3146,
        "grad_norm": 1.4856983423233032,
        "learning_rate": 0.00011327459686388609,
        "epoch": 0.30794019077081725,
        "step": 2389
    },
    {
        "loss": 2.2617,
        "grad_norm": 1.4176251888275146,
        "learning_rate": 0.000113214292496397,
        "epoch": 0.3080690899716422,
        "step": 2390
    },
    {
        "loss": 1.5913,
        "grad_norm": 2.330183506011963,
        "learning_rate": 0.00011315398323757468,
        "epoch": 0.30819798917246716,
        "step": 2391
    },
    {
        "loss": 2.1576,
        "grad_norm": 1.7617301940917969,
        "learning_rate": 0.00011309366910974295,
        "epoch": 0.3083268883732921,
        "step": 2392
    },
    {
        "loss": 1.855,
        "grad_norm": 1.9865554571151733,
        "learning_rate": 0.00011303335013522729,
        "epoch": 0.308455787574117,
        "step": 2393
    },
    {
        "loss": 2.1759,
        "grad_norm": 2.8078842163085938,
        "learning_rate": 0.00011297302633635511,
        "epoch": 0.308584686774942,
        "step": 2394
    },
    {
        "loss": 1.986,
        "grad_norm": 1.985601544380188,
        "learning_rate": 0.00011291269773545555,
        "epoch": 0.3087135859757669,
        "step": 2395
    },
    {
        "loss": 1.6035,
        "grad_norm": 1.7606675624847412,
        "learning_rate": 0.00011285236435485946,
        "epoch": 0.3088424851765919,
        "step": 2396
    },
    {
        "loss": 1.3843,
        "grad_norm": 1.9754565954208374,
        "learning_rate": 0.00011279202621689964,
        "epoch": 0.30897138437741684,
        "step": 2397
    },
    {
        "loss": 2.1263,
        "grad_norm": 1.5728706121444702,
        "learning_rate": 0.00011273168334391044,
        "epoch": 0.3091002835782418,
        "step": 2398
    },
    {
        "loss": 2.2489,
        "grad_norm": 1.464044213294983,
        "learning_rate": 0.00011267133575822812,
        "epoch": 0.30922918277906675,
        "step": 2399
    },
    {
        "loss": 2.2242,
        "grad_norm": 1.9958678483963013,
        "learning_rate": 0.00011261098348219059,
        "epoch": 0.30935808197989173,
        "step": 2400
    },
    {
        "loss": 2.2144,
        "grad_norm": 1.7177685499191284,
        "learning_rate": 0.00011255062653813757,
        "epoch": 0.30948698118071666,
        "step": 2401
    },
    {
        "loss": 2.2456,
        "grad_norm": 1.3563077449798584,
        "learning_rate": 0.00011249026494841049,
        "epoch": 0.30961588038154164,
        "step": 2402
    },
    {
        "loss": 1.6867,
        "grad_norm": 2.5764734745025635,
        "learning_rate": 0.00011242989873535236,
        "epoch": 0.30974477958236657,
        "step": 2403
    },
    {
        "loss": 1.578,
        "grad_norm": 2.162869691848755,
        "learning_rate": 0.00011236952792130817,
        "epoch": 0.30987367878319155,
        "step": 2404
    },
    {
        "loss": 1.7135,
        "grad_norm": 2.2995409965515137,
        "learning_rate": 0.00011230915252862435,
        "epoch": 0.3100025779840165,
        "step": 2405
    },
    {
        "loss": 1.4749,
        "grad_norm": 2.222402334213257,
        "learning_rate": 0.00011224877257964918,
        "epoch": 0.31013147718484146,
        "step": 2406
    },
    {
        "loss": 1.5838,
        "grad_norm": 2.7021758556365967,
        "learning_rate": 0.0001121883880967326,
        "epoch": 0.3102603763856664,
        "step": 2407
    },
    {
        "loss": 2.2932,
        "grad_norm": 1.069441795349121,
        "learning_rate": 0.0001121279991022262,
        "epoch": 0.3103892755864914,
        "step": 2408
    },
    {
        "loss": 1.8787,
        "grad_norm": 1.6648228168487549,
        "learning_rate": 0.00011206760561848324,
        "epoch": 0.3105181747873163,
        "step": 2409
    },
    {
        "loss": 2.005,
        "grad_norm": 1.7064639329910278,
        "learning_rate": 0.0001120072076678587,
        "epoch": 0.3106470739881413,
        "step": 2410
    },
    {
        "loss": 1.607,
        "grad_norm": 2.51055645942688,
        "learning_rate": 0.00011194680527270913,
        "epoch": 0.3107759731889662,
        "step": 2411
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.2917439937591553,
        "learning_rate": 0.00011188639845539278,
        "epoch": 0.3109048723897912,
        "step": 2412
    },
    {
        "loss": 1.6144,
        "grad_norm": 1.1601425409317017,
        "learning_rate": 0.00011182598723826947,
        "epoch": 0.3110337715906161,
        "step": 2413
    },
    {
        "loss": 1.7617,
        "grad_norm": 2.1186888217926025,
        "learning_rate": 0.00011176557164370078,
        "epoch": 0.3111626707914411,
        "step": 2414
    },
    {
        "loss": 1.5748,
        "grad_norm": 2.143772602081299,
        "learning_rate": 0.0001117051516940498,
        "epoch": 0.31129156999226604,
        "step": 2415
    },
    {
        "loss": 0.9462,
        "grad_norm": 2.2094027996063232,
        "learning_rate": 0.00011164472741168121,
        "epoch": 0.311420469193091,
        "step": 2416
    },
    {
        "loss": 1.9534,
        "grad_norm": 1.8305491209030151,
        "learning_rate": 0.00011158429881896142,
        "epoch": 0.31154936839391595,
        "step": 2417
    },
    {
        "loss": 1.7866,
        "grad_norm": 1.2465406656265259,
        "learning_rate": 0.00011152386593825834,
        "epoch": 0.31167826759474093,
        "step": 2418
    },
    {
        "loss": 1.7452,
        "grad_norm": 2.1227498054504395,
        "learning_rate": 0.00011146342879194145,
        "epoch": 0.31180716679556586,
        "step": 2419
    },
    {
        "loss": 0.8442,
        "grad_norm": 2.2940428256988525,
        "learning_rate": 0.0001114029874023819,
        "epoch": 0.31193606599639084,
        "step": 2420
    },
    {
        "loss": 1.7012,
        "grad_norm": 2.4444119930267334,
        "learning_rate": 0.00011134254179195236,
        "epoch": 0.31206496519721577,
        "step": 2421
    },
    {
        "loss": 2.1068,
        "grad_norm": 2.3351850509643555,
        "learning_rate": 0.00011128209198302697,
        "epoch": 0.31219386439804075,
        "step": 2422
    },
    {
        "loss": 2.0937,
        "grad_norm": 2.2805368900299072,
        "learning_rate": 0.00011122163799798162,
        "epoch": 0.3123227635988657,
        "step": 2423
    },
    {
        "loss": 2.1927,
        "grad_norm": 2.105907917022705,
        "learning_rate": 0.00011116117985919357,
        "epoch": 0.31245166279969067,
        "step": 2424
    },
    {
        "loss": 2.0621,
        "grad_norm": 2.0941696166992188,
        "learning_rate": 0.00011110071758904175,
        "epoch": 0.3125805620005156,
        "step": 2425
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.0912549495697021,
        "learning_rate": 0.00011104025120990647,
        "epoch": 0.3127094612013406,
        "step": 2426
    },
    {
        "loss": 1.6458,
        "grad_norm": 2.113281488418579,
        "learning_rate": 0.00011097978074416976,
        "epoch": 0.3128383604021655,
        "step": 2427
    },
    {
        "loss": 2.4476,
        "grad_norm": 1.168968677520752,
        "learning_rate": 0.00011091930621421495,
        "epoch": 0.3129672596029905,
        "step": 2428
    },
    {
        "loss": 2.5587,
        "grad_norm": 1.3273203372955322,
        "learning_rate": 0.00011085882764242703,
        "epoch": 0.3130961588038154,
        "step": 2429
    },
    {
        "loss": 1.167,
        "grad_norm": 1.8323391675949097,
        "learning_rate": 0.00011079834505119242,
        "epoch": 0.31322505800464034,
        "step": 2430
    },
    {
        "loss": 2.3084,
        "grad_norm": 1.8769322633743286,
        "learning_rate": 0.00011073785846289904,
        "epoch": 0.3133539572054653,
        "step": 2431
    },
    {
        "loss": 2.2756,
        "grad_norm": 1.5818132162094116,
        "learning_rate": 0.00011067736789993628,
        "epoch": 0.31348285640629026,
        "step": 2432
    },
    {
        "loss": 1.5653,
        "grad_norm": 2.7541890144348145,
        "learning_rate": 0.000110616873384695,
        "epoch": 0.31361175560711524,
        "step": 2433
    },
    {
        "loss": 2.2031,
        "grad_norm": 1.4149739742279053,
        "learning_rate": 0.00011055637493956754,
        "epoch": 0.31374065480794017,
        "step": 2434
    },
    {
        "loss": 1.9173,
        "grad_norm": 1.8121503591537476,
        "learning_rate": 0.00011049587258694772,
        "epoch": 0.31386955400876515,
        "step": 2435
    },
    {
        "loss": 1.0542,
        "grad_norm": 2.3036680221557617,
        "learning_rate": 0.0001104353663492307,
        "epoch": 0.3139984532095901,
        "step": 2436
    },
    {
        "loss": 1.6294,
        "grad_norm": 1.6840769052505493,
        "learning_rate": 0.00011037485624881328,
        "epoch": 0.31412735241041506,
        "step": 2437
    },
    {
        "loss": 1.5449,
        "grad_norm": 1.9080679416656494,
        "learning_rate": 0.00011031434230809341,
        "epoch": 0.31425625161124,
        "step": 2438
    },
    {
        "loss": 1.6253,
        "grad_norm": 2.0765514373779297,
        "learning_rate": 0.00011025382454947068,
        "epoch": 0.314385150812065,
        "step": 2439
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.7760292291641235,
        "learning_rate": 0.00011019330299534606,
        "epoch": 0.3145140500128899,
        "step": 2440
    },
    {
        "loss": 2.1535,
        "grad_norm": 2.356290102005005,
        "learning_rate": 0.00011013277766812187,
        "epoch": 0.3146429492137149,
        "step": 2441
    },
    {
        "loss": 2.1274,
        "grad_norm": 1.4006705284118652,
        "learning_rate": 0.00011007224859020182,
        "epoch": 0.3147718484145398,
        "step": 2442
    },
    {
        "loss": 1.5464,
        "grad_norm": 2.212660312652588,
        "learning_rate": 0.00011001171578399103,
        "epoch": 0.3149007476153648,
        "step": 2443
    },
    {
        "loss": 1.8862,
        "grad_norm": 2.0444581508636475,
        "learning_rate": 0.00010995117927189609,
        "epoch": 0.3150296468161897,
        "step": 2444
    },
    {
        "loss": 2.2329,
        "grad_norm": 1.5550446510314941,
        "learning_rate": 0.00010989063907632477,
        "epoch": 0.3151585460170147,
        "step": 2445
    },
    {
        "loss": 2.5654,
        "grad_norm": 1.4989887475967407,
        "learning_rate": 0.00010983009521968634,
        "epoch": 0.31528744521783963,
        "step": 2446
    },
    {
        "loss": 1.7977,
        "grad_norm": 2.0383267402648926,
        "learning_rate": 0.00010976954772439145,
        "epoch": 0.3154163444186646,
        "step": 2447
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.089940309524536,
        "learning_rate": 0.000109708996612852,
        "epoch": 0.31554524361948955,
        "step": 2448
    },
    {
        "loss": 1.7249,
        "grad_norm": 2.3433659076690674,
        "learning_rate": 0.00010964844190748127,
        "epoch": 0.31567414282031453,
        "step": 2449
    },
    {
        "loss": 1.3854,
        "grad_norm": 2.4314746856689453,
        "learning_rate": 0.00010958788363069392,
        "epoch": 0.31580304202113946,
        "step": 2450
    },
    {
        "loss": 2.2338,
        "grad_norm": 1.2439653873443604,
        "learning_rate": 0.00010952732180490581,
        "epoch": 0.31593194122196444,
        "step": 2451
    },
    {
        "loss": 1.6449,
        "grad_norm": 3.0774483680725098,
        "learning_rate": 0.00010946675645253424,
        "epoch": 0.31606084042278937,
        "step": 2452
    },
    {
        "loss": 1.9621,
        "grad_norm": 2.365309715270996,
        "learning_rate": 0.00010940618759599774,
        "epoch": 0.31618973962361435,
        "step": 2453
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.9421011209487915,
        "learning_rate": 0.00010934561525771618,
        "epoch": 0.3163186388244393,
        "step": 2454
    },
    {
        "loss": 2.338,
        "grad_norm": 1.2217557430267334,
        "learning_rate": 0.00010928503946011068,
        "epoch": 0.31644753802526426,
        "step": 2455
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.2256237268447876,
        "learning_rate": 0.00010922446022560366,
        "epoch": 0.3165764372260892,
        "step": 2456
    },
    {
        "loss": 2.4208,
        "grad_norm": 1.2862616777420044,
        "learning_rate": 0.00010916387757661881,
        "epoch": 0.3167053364269142,
        "step": 2457
    },
    {
        "loss": 2.2151,
        "grad_norm": 1.7117674350738525,
        "learning_rate": 0.00010910329153558112,
        "epoch": 0.3168342356277391,
        "step": 2458
    },
    {
        "loss": 2.3443,
        "grad_norm": 1.0924383401870728,
        "learning_rate": 0.00010904270212491674,
        "epoch": 0.3169631348285641,
        "step": 2459
    },
    {
        "loss": 1.6058,
        "grad_norm": 1.9744584560394287,
        "learning_rate": 0.00010898210936705322,
        "epoch": 0.317092034029389,
        "step": 2460
    },
    {
        "loss": 2.2899,
        "grad_norm": 1.644361138343811,
        "learning_rate": 0.0001089215132844192,
        "epoch": 0.317220933230214,
        "step": 2461
    },
    {
        "loss": 2.1085,
        "grad_norm": 1.8737061023712158,
        "learning_rate": 0.00010886091389944455,
        "epoch": 0.3173498324310389,
        "step": 2462
    },
    {
        "loss": 2.0751,
        "grad_norm": 2.029545545578003,
        "learning_rate": 0.00010880031123456046,
        "epoch": 0.3174787316318639,
        "step": 2463
    },
    {
        "loss": 1.6123,
        "grad_norm": 1.9158973693847656,
        "learning_rate": 0.0001087397053121994,
        "epoch": 0.31760763083268884,
        "step": 2464
    },
    {
        "loss": 1.5185,
        "grad_norm": 1.4386967420578003,
        "learning_rate": 0.00010867909615479484,
        "epoch": 0.3177365300335138,
        "step": 2465
    },
    {
        "loss": 0.9191,
        "grad_norm": 2.600745916366577,
        "learning_rate": 0.0001086184837847815,
        "epoch": 0.31786542923433875,
        "step": 2466
    },
    {
        "loss": 1.9683,
        "grad_norm": 2.533342123031616,
        "learning_rate": 0.00010855786822459546,
        "epoch": 0.3179943284351637,
        "step": 2467
    },
    {
        "loss": 2.3284,
        "grad_norm": 1.8004904985427856,
        "learning_rate": 0.0001084972494966738,
        "epoch": 0.31812322763598866,
        "step": 2468
    },
    {
        "loss": 1.6201,
        "grad_norm": 2.828881025314331,
        "learning_rate": 0.00010843662762345477,
        "epoch": 0.3182521268368136,
        "step": 2469
    },
    {
        "loss": 1.6475,
        "grad_norm": 2.602466344833374,
        "learning_rate": 0.00010837600262737795,
        "epoch": 0.31838102603763857,
        "step": 2470
    },
    {
        "loss": 1.7062,
        "grad_norm": 1.2364946603775024,
        "learning_rate": 0.00010831537453088392,
        "epoch": 0.3185099252384635,
        "step": 2471
    },
    {
        "loss": 1.7851,
        "grad_norm": 1.794262170791626,
        "learning_rate": 0.00010825474335641441,
        "epoch": 0.3186388244392885,
        "step": 2472
    },
    {
        "loss": 1.5343,
        "grad_norm": 2.704472064971924,
        "learning_rate": 0.00010819410912641248,
        "epoch": 0.3187677236401134,
        "step": 2473
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.1865094900131226,
        "learning_rate": 0.00010813347186332203,
        "epoch": 0.3188966228409384,
        "step": 2474
    },
    {
        "loss": 2.3613,
        "grad_norm": 1.077077031135559,
        "learning_rate": 0.00010807283158958829,
        "epoch": 0.3190255220417633,
        "step": 2475
    },
    {
        "loss": 2.4303,
        "grad_norm": 1.0703948736190796,
        "learning_rate": 0.00010801218832765755,
        "epoch": 0.3191544212425883,
        "step": 2476
    },
    {
        "loss": 1.4348,
        "grad_norm": 2.024257183074951,
        "learning_rate": 0.00010795154209997722,
        "epoch": 0.31928332044341323,
        "step": 2477
    },
    {
        "loss": 1.4673,
        "grad_norm": 2.471853494644165,
        "learning_rate": 0.00010789089292899574,
        "epoch": 0.3194122196442382,
        "step": 2478
    },
    {
        "loss": 2.2316,
        "grad_norm": 1.401445984840393,
        "learning_rate": 0.00010783024083716272,
        "epoch": 0.31954111884506314,
        "step": 2479
    },
    {
        "loss": 2.3976,
        "grad_norm": 1.106872797012329,
        "learning_rate": 0.00010776958584692882,
        "epoch": 0.3196700180458881,
        "step": 2480
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.1879152059555054,
        "learning_rate": 0.00010770892798074576,
        "epoch": 0.31979891724671305,
        "step": 2481
    },
    {
        "loss": 1.73,
        "grad_norm": 1.7753171920776367,
        "learning_rate": 0.00010764826726106634,
        "epoch": 0.31992781644753804,
        "step": 2482
    },
    {
        "loss": 2.2769,
        "grad_norm": 1.5586034059524536,
        "learning_rate": 0.00010758760371034446,
        "epoch": 0.32005671564836297,
        "step": 2483
    },
    {
        "loss": 1.9514,
        "grad_norm": 1.8119075298309326,
        "learning_rate": 0.00010752693735103498,
        "epoch": 0.32018561484918795,
        "step": 2484
    },
    {
        "loss": 2.011,
        "grad_norm": 2.139702081680298,
        "learning_rate": 0.00010746626820559382,
        "epoch": 0.3203145140500129,
        "step": 2485
    },
    {
        "loss": 2.0128,
        "grad_norm": 1.7376652956008911,
        "learning_rate": 0.00010740559629647795,
        "epoch": 0.32044341325083786,
        "step": 2486
    },
    {
        "loss": 2.2144,
        "grad_norm": 1.5789241790771484,
        "learning_rate": 0.00010734492164614542,
        "epoch": 0.3205723124516628,
        "step": 2487
    },
    {
        "loss": 2.0354,
        "grad_norm": 1.5128737688064575,
        "learning_rate": 0.0001072842442770552,
        "epoch": 0.32070121165248777,
        "step": 2488
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.1140656471252441,
        "learning_rate": 0.00010722356421166732,
        "epoch": 0.3208301108533127,
        "step": 2489
    },
    {
        "loss": 1.8998,
        "grad_norm": 1.2464812994003296,
        "learning_rate": 0.00010716288147244285,
        "epoch": 0.3209590100541377,
        "step": 2490
    },
    {
        "loss": 1.6758,
        "grad_norm": 1.9891881942749023,
        "learning_rate": 0.00010710219608184369,
        "epoch": 0.3210879092549626,
        "step": 2491
    },
    {
        "loss": 1.6674,
        "grad_norm": 1.626238226890564,
        "learning_rate": 0.00010704150806233284,
        "epoch": 0.3212168084557876,
        "step": 2492
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.0487778186798096,
        "learning_rate": 0.00010698081743637434,
        "epoch": 0.3213457076566125,
        "step": 2493
    },
    {
        "loss": 1.4638,
        "grad_norm": 2.643096446990967,
        "learning_rate": 0.00010692012422643307,
        "epoch": 0.3214746068574375,
        "step": 2494
    },
    {
        "loss": 2.2158,
        "grad_norm": 2.6600873470306396,
        "learning_rate": 0.0001068594284549749,
        "epoch": 0.32160350605826243,
        "step": 2495
    },
    {
        "loss": 1.8273,
        "grad_norm": 1.9195258617401123,
        "learning_rate": 0.00010679873014446665,
        "epoch": 0.3217324052590874,
        "step": 2496
    },
    {
        "loss": 1.9473,
        "grad_norm": 1.9950392246246338,
        "learning_rate": 0.0001067380293173761,
        "epoch": 0.32186130445991235,
        "step": 2497
    },
    {
        "loss": 2.0515,
        "grad_norm": 1.566546082496643,
        "learning_rate": 0.00010667732599617197,
        "epoch": 0.32199020366073733,
        "step": 2498
    },
    {
        "loss": 1.7921,
        "grad_norm": 2.1478729248046875,
        "learning_rate": 0.00010661662020332383,
        "epoch": 0.32211910286156226,
        "step": 2499
    },
    {
        "loss": 1.8927,
        "grad_norm": 1.8139092922210693,
        "learning_rate": 0.00010655591196130232,
        "epoch": 0.32224800206238724,
        "step": 2500
    },
    {
        "loss": 1.7308,
        "grad_norm": 2.5421807765960693,
        "learning_rate": 0.0001064952012925788,
        "epoch": 0.32237690126321217,
        "step": 2501
    },
    {
        "loss": 0.7161,
        "grad_norm": 2.3022847175598145,
        "learning_rate": 0.00010643448821962563,
        "epoch": 0.3225058004640371,
        "step": 2502
    },
    {
        "loss": 1.4888,
        "grad_norm": 2.508385419845581,
        "learning_rate": 0.000106373772764916,
        "epoch": 0.3226346996648621,
        "step": 2503
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.2942841053009033,
        "learning_rate": 0.0001063130549509241,
        "epoch": 0.322763598865687,
        "step": 2504
    },
    {
        "loss": 2.2563,
        "grad_norm": 2.2044548988342285,
        "learning_rate": 0.00010625233480012493,
        "epoch": 0.322892498066512,
        "step": 2505
    },
    {
        "loss": 1.7304,
        "grad_norm": 1.858951210975647,
        "learning_rate": 0.00010619161233499423,
        "epoch": 0.3230213972673369,
        "step": 2506
    },
    {
        "loss": 1.3733,
        "grad_norm": 2.2781739234924316,
        "learning_rate": 0.00010613088757800884,
        "epoch": 0.3231502964681619,
        "step": 2507
    },
    {
        "loss": 2.3407,
        "grad_norm": 2.0867903232574463,
        "learning_rate": 0.00010607016055164623,
        "epoch": 0.32327919566898683,
        "step": 2508
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.6380623579025269,
        "learning_rate": 0.00010600943127838478,
        "epoch": 0.3234080948698118,
        "step": 2509
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.5348999500274658,
        "learning_rate": 0.00010594869978070381,
        "epoch": 0.32353699407063674,
        "step": 2510
    },
    {
        "loss": 1.887,
        "grad_norm": 1.860661506652832,
        "learning_rate": 0.0001058879660810833,
        "epoch": 0.3236658932714617,
        "step": 2511
    },
    {
        "loss": 2.0849,
        "grad_norm": 2.710855007171631,
        "learning_rate": 0.00010582723020200416,
        "epoch": 0.32379479247228665,
        "step": 2512
    },
    {
        "loss": 1.6386,
        "grad_norm": 2.5742392539978027,
        "learning_rate": 0.00010576649216594807,
        "epoch": 0.32392369167311164,
        "step": 2513
    },
    {
        "loss": 2.3218,
        "grad_norm": 1.3673194646835327,
        "learning_rate": 0.00010570575199539748,
        "epoch": 0.32405259087393656,
        "step": 2514
    },
    {
        "loss": 1.8368,
        "grad_norm": 2.058424472808838,
        "learning_rate": 0.0001056450097128356,
        "epoch": 0.32418149007476155,
        "step": 2515
    },
    {
        "loss": 2.428,
        "grad_norm": 1.804807186126709,
        "learning_rate": 0.00010558426534074656,
        "epoch": 0.3243103892755865,
        "step": 2516
    },
    {
        "loss": 1.8339,
        "grad_norm": 1.9748386144638062,
        "learning_rate": 0.00010552351890161518,
        "epoch": 0.32443928847641146,
        "step": 2517
    },
    {
        "loss": 2.3641,
        "grad_norm": 2.2594990730285645,
        "learning_rate": 0.00010546277041792698,
        "epoch": 0.3245681876772364,
        "step": 2518
    },
    {
        "loss": 1.8678,
        "grad_norm": 2.429917573928833,
        "learning_rate": 0.00010540201991216831,
        "epoch": 0.32469708687806137,
        "step": 2519
    },
    {
        "loss": 1.9129,
        "grad_norm": 1.78389573097229,
        "learning_rate": 0.0001053412674068263,
        "epoch": 0.3248259860788863,
        "step": 2520
    },
    {
        "loss": 2.0529,
        "grad_norm": 1.9100608825683594,
        "learning_rate": 0.00010528051292438876,
        "epoch": 0.3249548852797113,
        "step": 2521
    },
    {
        "loss": 2.2452,
        "grad_norm": 1.4861855506896973,
        "learning_rate": 0.00010521975648734421,
        "epoch": 0.3250837844805362,
        "step": 2522
    },
    {
        "loss": 1.8649,
        "grad_norm": 1.3393547534942627,
        "learning_rate": 0.00010515899811818204,
        "epoch": 0.3252126836813612,
        "step": 2523
    },
    {
        "loss": 1.967,
        "grad_norm": 1.3401820659637451,
        "learning_rate": 0.00010509823783939217,
        "epoch": 0.3253415828821861,
        "step": 2524
    },
    {
        "loss": 1.0759,
        "grad_norm": 2.0197255611419678,
        "learning_rate": 0.00010503747567346527,
        "epoch": 0.3254704820830111,
        "step": 2525
    },
    {
        "loss": 1.9032,
        "grad_norm": 1.7952971458435059,
        "learning_rate": 0.00010497671164289283,
        "epoch": 0.32559938128383603,
        "step": 2526
    },
    {
        "loss": 2.1092,
        "grad_norm": 2.124699115753174,
        "learning_rate": 0.0001049159457701669,
        "epoch": 0.325728280484661,
        "step": 2527
    },
    {
        "loss": 1.8562,
        "grad_norm": 1.9713523387908936,
        "learning_rate": 0.00010485517807778028,
        "epoch": 0.32585717968548594,
        "step": 2528
    },
    {
        "loss": 1.9983,
        "grad_norm": 2.4091718196868896,
        "learning_rate": 0.00010479440858822635,
        "epoch": 0.3259860788863109,
        "step": 2529
    },
    {
        "loss": 2.2298,
        "grad_norm": 1.099769949913025,
        "learning_rate": 0.00010473363732399935,
        "epoch": 0.32611497808713585,
        "step": 2530
    },
    {
        "loss": 2.422,
        "grad_norm": 1.672891616821289,
        "learning_rate": 0.00010467286430759394,
        "epoch": 0.32624387728796084,
        "step": 2531
    },
    {
        "loss": 2.0415,
        "grad_norm": 1.2108244895935059,
        "learning_rate": 0.00010461208956150557,
        "epoch": 0.32637277648878577,
        "step": 2532
    },
    {
        "loss": 1.9452,
        "grad_norm": 1.5599603652954102,
        "learning_rate": 0.00010455131310823034,
        "epoch": 0.32650167568961075,
        "step": 2533
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.0062321424484253,
        "learning_rate": 0.0001044905349702649,
        "epoch": 0.3266305748904357,
        "step": 2534
    },
    {
        "loss": 2.7301,
        "grad_norm": 1.4989176988601685,
        "learning_rate": 0.00010442975517010662,
        "epoch": 0.32675947409126066,
        "step": 2535
    },
    {
        "loss": 2.279,
        "grad_norm": 1.5627723932266235,
        "learning_rate": 0.00010436897373025338,
        "epoch": 0.3268883732920856,
        "step": 2536
    },
    {
        "loss": 1.7411,
        "grad_norm": 2.0390048027038574,
        "learning_rate": 0.00010430819067320373,
        "epoch": 0.32701727249291057,
        "step": 2537
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.4781478643417358,
        "learning_rate": 0.00010424740602145682,
        "epoch": 0.3271461716937355,
        "step": 2538
    },
    {
        "loss": 2.3639,
        "grad_norm": 1.7631415128707886,
        "learning_rate": 0.00010418661979751234,
        "epoch": 0.3272750708945604,
        "step": 2539
    },
    {
        "loss": 1.3075,
        "grad_norm": 1.97930908203125,
        "learning_rate": 0.0001041258320238707,
        "epoch": 0.3274039700953854,
        "step": 2540
    },
    {
        "loss": 1.5068,
        "grad_norm": 2.295657157897949,
        "learning_rate": 0.00010406504272303269,
        "epoch": 0.32753286929621034,
        "step": 2541
    },
    {
        "loss": 2.2579,
        "grad_norm": 1.1839394569396973,
        "learning_rate": 0.00010400425191749973,
        "epoch": 0.3276617684970353,
        "step": 2542
    },
    {
        "loss": 1.8618,
        "grad_norm": 2.661419630050659,
        "learning_rate": 0.00010394345962977392,
        "epoch": 0.32779066769786025,
        "step": 2543
    },
    {
        "loss": 2.3654,
        "grad_norm": 1.325714111328125,
        "learning_rate": 0.00010388266588235778,
        "epoch": 0.32791956689868523,
        "step": 2544
    },
    {
        "loss": 1.6062,
        "grad_norm": 1.7819582223892212,
        "learning_rate": 0.00010382187069775437,
        "epoch": 0.32804846609951016,
        "step": 2545
    },
    {
        "loss": 1.7311,
        "grad_norm": 1.5821784734725952,
        "learning_rate": 0.00010376107409846738,
        "epoch": 0.32817736530033514,
        "step": 2546
    },
    {
        "loss": 2.3532,
        "grad_norm": 1.6663641929626465,
        "learning_rate": 0.00010370027610700095,
        "epoch": 0.32830626450116007,
        "step": 2547
    },
    {
        "loss": 2.0208,
        "grad_norm": 2.052626609802246,
        "learning_rate": 0.00010363947674585967,
        "epoch": 0.32843516370198506,
        "step": 2548
    },
    {
        "loss": 2.5926,
        "grad_norm": 1.6062564849853516,
        "learning_rate": 0.00010357867603754877,
        "epoch": 0.32856406290281,
        "step": 2549
    },
    {
        "loss": 1.3419,
        "grad_norm": 2.4716432094573975,
        "learning_rate": 0.0001035178740045739,
        "epoch": 0.32869296210363497,
        "step": 2550
    },
    {
        "loss": 2.207,
        "grad_norm": 2.19590425491333,
        "learning_rate": 0.00010345707066944128,
        "epoch": 0.3288218613044599,
        "step": 2551
    },
    {
        "loss": 1.8589,
        "grad_norm": 2.0720059871673584,
        "learning_rate": 0.00010339626605465747,
        "epoch": 0.3289507605052849,
        "step": 2552
    },
    {
        "loss": 1.9638,
        "grad_norm": 2.104506731033325,
        "learning_rate": 0.00010333546018272968,
        "epoch": 0.3290796597061098,
        "step": 2553
    },
    {
        "loss": 2.1274,
        "grad_norm": 2.786609649658203,
        "learning_rate": 0.00010327465307616541,
        "epoch": 0.3292085589069348,
        "step": 2554
    },
    {
        "loss": 0.9957,
        "grad_norm": 2.375438690185547,
        "learning_rate": 0.00010321384475747272,
        "epoch": 0.3293374581077597,
        "step": 2555
    },
    {
        "loss": 1.831,
        "grad_norm": 1.8873522281646729,
        "learning_rate": 0.00010315303524916011,
        "epoch": 0.3294663573085847,
        "step": 2556
    },
    {
        "loss": 1.471,
        "grad_norm": 2.4616971015930176,
        "learning_rate": 0.00010309222457373655,
        "epoch": 0.32959525650940963,
        "step": 2557
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.2342582941055298,
        "learning_rate": 0.00010303141275371133,
        "epoch": 0.3297241557102346,
        "step": 2558
    },
    {
        "loss": 1.5526,
        "grad_norm": 1.4019626379013062,
        "learning_rate": 0.00010297059981159423,
        "epoch": 0.32985305491105954,
        "step": 2559
    },
    {
        "loss": 1.9168,
        "grad_norm": 1.6046152114868164,
        "learning_rate": 0.00010290978576989552,
        "epoch": 0.3299819541118845,
        "step": 2560
    },
    {
        "loss": 1.8977,
        "grad_norm": 2.758068561553955,
        "learning_rate": 0.00010284897065112575,
        "epoch": 0.33011085331270945,
        "step": 2561
    },
    {
        "loss": 2.3623,
        "grad_norm": 1.6693992614746094,
        "learning_rate": 0.00010278815447779594,
        "epoch": 0.33023975251353443,
        "step": 2562
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.4235528707504272,
        "learning_rate": 0.00010272733727241752,
        "epoch": 0.33036865171435936,
        "step": 2563
    },
    {
        "loss": 2.1595,
        "grad_norm": 1.0845078229904175,
        "learning_rate": 0.00010266651905750227,
        "epoch": 0.33049755091518435,
        "step": 2564
    },
    {
        "loss": 2.0617,
        "grad_norm": 1.9359749555587769,
        "learning_rate": 0.00010260569985556223,
        "epoch": 0.3306264501160093,
        "step": 2565
    },
    {
        "loss": 1.7572,
        "grad_norm": 1.6708385944366455,
        "learning_rate": 0.00010254487968911003,
        "epoch": 0.33075534931683426,
        "step": 2566
    },
    {
        "loss": 2.2515,
        "grad_norm": 1.9905285835266113,
        "learning_rate": 0.00010248405858065848,
        "epoch": 0.3308842485176592,
        "step": 2567
    },
    {
        "loss": 1.6389,
        "grad_norm": 2.307854175567627,
        "learning_rate": 0.0001024232365527209,
        "epoch": 0.33101314771848417,
        "step": 2568
    },
    {
        "loss": 1.6991,
        "grad_norm": 2.0592129230499268,
        "learning_rate": 0.00010236241362781072,
        "epoch": 0.3311420469193091,
        "step": 2569
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.62942373752594,
        "learning_rate": 0.00010230158982844196,
        "epoch": 0.3312709461201341,
        "step": 2570
    },
    {
        "loss": 2.205,
        "grad_norm": 1.295849323272705,
        "learning_rate": 0.00010224076517712875,
        "epoch": 0.331399845320959,
        "step": 2571
    },
    {
        "loss": 1.5334,
        "grad_norm": 1.7848576307296753,
        "learning_rate": 0.00010217993969638562,
        "epoch": 0.331528744521784,
        "step": 2572
    },
    {
        "loss": 2.222,
        "grad_norm": 1.675312876701355,
        "learning_rate": 0.00010211911340872748,
        "epoch": 0.3316576437226089,
        "step": 2573
    },
    {
        "loss": 1.5475,
        "grad_norm": 1.5679646730422974,
        "learning_rate": 0.00010205828633666943,
        "epoch": 0.33178654292343385,
        "step": 2574
    },
    {
        "loss": 2.187,
        "grad_norm": 1.3700973987579346,
        "learning_rate": 0.00010199745850272691,
        "epoch": 0.33191544212425883,
        "step": 2575
    },
    {
        "loss": 2.2562,
        "grad_norm": 1.5753540992736816,
        "learning_rate": 0.00010193662992941562,
        "epoch": 0.33204434132508376,
        "step": 2576
    },
    {
        "loss": 2.0979,
        "grad_norm": 1.9397023916244507,
        "learning_rate": 0.00010187580063925155,
        "epoch": 0.33217324052590874,
        "step": 2577
    },
    {
        "loss": 1.8051,
        "grad_norm": 1.0975894927978516,
        "learning_rate": 0.00010181497065475094,
        "epoch": 0.33230213972673367,
        "step": 2578
    },
    {
        "loss": 1.9428,
        "grad_norm": 1.3401511907577515,
        "learning_rate": 0.0001017541399984303,
        "epoch": 0.33243103892755865,
        "step": 2579
    },
    {
        "loss": 1.8974,
        "grad_norm": 1.394912600517273,
        "learning_rate": 0.00010169330869280647,
        "epoch": 0.3325599381283836,
        "step": 2580
    },
    {
        "loss": 1.9876,
        "grad_norm": 2.7028377056121826,
        "learning_rate": 0.00010163247676039632,
        "epoch": 0.33268883732920856,
        "step": 2581
    },
    {
        "loss": 1.8244,
        "grad_norm": 1.8413753509521484,
        "learning_rate": 0.00010157164422371714,
        "epoch": 0.3328177365300335,
        "step": 2582
    },
    {
        "loss": 2.0898,
        "grad_norm": 2.2506487369537354,
        "learning_rate": 0.00010151081110528635,
        "epoch": 0.3329466357308585,
        "step": 2583
    },
    {
        "loss": 2.154,
        "grad_norm": 0.9159085750579834,
        "learning_rate": 0.00010144997742762168,
        "epoch": 0.3330755349316834,
        "step": 2584
    },
    {
        "loss": 1.889,
        "grad_norm": 2.1169168949127197,
        "learning_rate": 0.00010138914321324094,
        "epoch": 0.3332044341325084,
        "step": 2585
    },
    {
        "loss": 2.063,
        "grad_norm": 1.8789198398590088,
        "learning_rate": 0.00010132830848466228,
        "epoch": 0.3333333333333333,
        "step": 2586
    },
    {
        "loss": 2.21,
        "grad_norm": 1.8573594093322754,
        "learning_rate": 0.00010126747326440391,
        "epoch": 0.3334622325341583,
        "step": 2587
    },
    {
        "loss": 2.2156,
        "grad_norm": 1.4412301778793335,
        "learning_rate": 0.00010120663757498423,
        "epoch": 0.3335911317349832,
        "step": 2588
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.7831764221191406,
        "learning_rate": 0.00010114580143892196,
        "epoch": 0.3337200309358082,
        "step": 2589
    },
    {
        "loss": 2.2462,
        "grad_norm": 1.0651291608810425,
        "learning_rate": 0.00010108496487873583,
        "epoch": 0.33384893013663314,
        "step": 2590
    },
    {
        "loss": 1.5628,
        "grad_norm": 2.1105823516845703,
        "learning_rate": 0.00010102412791694478,
        "epoch": 0.3339778293374581,
        "step": 2591
    },
    {
        "loss": 1.6005,
        "grad_norm": 1.7977021932601929,
        "learning_rate": 0.0001009632905760679,
        "epoch": 0.33410672853828305,
        "step": 2592
    },
    {
        "loss": 1.6455,
        "grad_norm": 5.016734600067139,
        "learning_rate": 0.00010090245287862447,
        "epoch": 0.33423562773910803,
        "step": 2593
    },
    {
        "loss": 2.5487,
        "grad_norm": 1.9825139045715332,
        "learning_rate": 0.0001008416148471338,
        "epoch": 0.33436452693993296,
        "step": 2594
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.4576939344406128,
        "learning_rate": 0.00010078077650411536,
        "epoch": 0.33449342614075794,
        "step": 2595
    },
    {
        "loss": 0.9288,
        "grad_norm": 2.934121608734131,
        "learning_rate": 0.0001007199378720888,
        "epoch": 0.33462232534158287,
        "step": 2596
    },
    {
        "loss": 1.194,
        "grad_norm": 1.041635513305664,
        "learning_rate": 0.00010065909897357386,
        "epoch": 0.33475122454240785,
        "step": 2597
    },
    {
        "loss": 1.408,
        "grad_norm": 2.467897891998291,
        "learning_rate": 0.00010059825983109024,
        "epoch": 0.3348801237432328,
        "step": 2598
    },
    {
        "loss": 1.7602,
        "grad_norm": 2.3061676025390625,
        "learning_rate": 0.00010053742046715793,
        "epoch": 0.33500902294405777,
        "step": 2599
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.7025433778762817,
        "learning_rate": 0.00010047658090429685,
        "epoch": 0.3351379221448827,
        "step": 2600
    },
    {
        "loss": 1.8576,
        "grad_norm": 1.7466901540756226,
        "learning_rate": 0.0001004157411650271,
        "epoch": 0.3352668213457077,
        "step": 2601
    },
    {
        "loss": 2.0087,
        "grad_norm": 2.1169703006744385,
        "learning_rate": 0.00010035490127186878,
        "epoch": 0.3353957205465326,
        "step": 2602
    },
    {
        "loss": 1.6525,
        "grad_norm": 3.724188804626465,
        "learning_rate": 0.00010029406124734209,
        "epoch": 0.3355246197473576,
        "step": 2603
    },
    {
        "loss": 2.2075,
        "grad_norm": 2.7393314838409424,
        "learning_rate": 0.00010023322111396722,
        "epoch": 0.3356535189481825,
        "step": 2604
    },
    {
        "loss": 2.2735,
        "grad_norm": 1.3501529693603516,
        "learning_rate": 0.00010017238089426441,
        "epoch": 0.3357824181490075,
        "step": 2605
    },
    {
        "loss": 2.281,
        "grad_norm": 1.6075992584228516,
        "learning_rate": 0.00010011154061075403,
        "epoch": 0.33591131734983243,
        "step": 2606
    },
    {
        "loss": 2.1459,
        "grad_norm": 1.6785218715667725,
        "learning_rate": 0.00010005070028595633,
        "epoch": 0.3360402165506574,
        "step": 2607
    },
    {
        "loss": 2.3584,
        "grad_norm": 1.7472935914993286,
        "learning_rate": 9.998985994239169e-05,
        "epoch": 0.33616911575148234,
        "step": 2608
    },
    {
        "loss": 1.8003,
        "grad_norm": 1.1571784019470215,
        "learning_rate": 9.992901960258045e-05,
        "epoch": 0.3362980149523073,
        "step": 2609
    },
    {
        "loss": 1.8331,
        "grad_norm": 2.0556037425994873,
        "learning_rate": 9.986817928904296e-05,
        "epoch": 0.33642691415313225,
        "step": 2610
    },
    {
        "loss": 1.2853,
        "grad_norm": 2.2540109157562256,
        "learning_rate": 9.980733902429953e-05,
        "epoch": 0.3365558133539572,
        "step": 2611
    },
    {
        "loss": 1.4525,
        "grad_norm": 2.0511341094970703,
        "learning_rate": 9.97464988308704e-05,
        "epoch": 0.33668471255478216,
        "step": 2612
    },
    {
        "loss": 2.3553,
        "grad_norm": 1.5287405252456665,
        "learning_rate": 9.968565873127602e-05,
        "epoch": 0.3368136117556071,
        "step": 2613
    },
    {
        "loss": 2.2987,
        "grad_norm": 1.234014630317688,
        "learning_rate": 9.962481874803653e-05,
        "epoch": 0.3369425109564321,
        "step": 2614
    },
    {
        "loss": 2.0769,
        "grad_norm": 1.9174647331237793,
        "learning_rate": 9.956397890367216e-05,
        "epoch": 0.337071410157257,
        "step": 2615
    },
    {
        "loss": 2.204,
        "grad_norm": 1.7340275049209595,
        "learning_rate": 9.950313922070308e-05,
        "epoch": 0.337200309358082,
        "step": 2616
    },
    {
        "loss": 2.4784,
        "grad_norm": 1.2894750833511353,
        "learning_rate": 9.944229972164934e-05,
        "epoch": 0.3373292085589069,
        "step": 2617
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.8504480123519897,
        "learning_rate": 9.938146042903109e-05,
        "epoch": 0.3374581077597319,
        "step": 2618
    },
    {
        "loss": 2.3842,
        "grad_norm": 1.7465945482254028,
        "learning_rate": 9.932062136536813e-05,
        "epoch": 0.3375870069605568,
        "step": 2619
    },
    {
        "loss": 2.1366,
        "grad_norm": 2.2650022506713867,
        "learning_rate": 9.925978255318045e-05,
        "epoch": 0.3377159061613818,
        "step": 2620
    },
    {
        "loss": 1.8381,
        "grad_norm": 1.275356650352478,
        "learning_rate": 9.919894401498776e-05,
        "epoch": 0.33784480536220673,
        "step": 2621
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.2698516845703125,
        "learning_rate": 9.913810577330975e-05,
        "epoch": 0.3379737045630317,
        "step": 2622
    },
    {
        "loss": 2.4832,
        "grad_norm": 1.3294771909713745,
        "learning_rate": 9.9077267850666e-05,
        "epoch": 0.33810260376385665,
        "step": 2623
    },
    {
        "loss": 1.8258,
        "grad_norm": 2.270982503890991,
        "learning_rate": 9.901643026957591e-05,
        "epoch": 0.33823150296468163,
        "step": 2624
    },
    {
        "loss": 2.6055,
        "grad_norm": 1.4583775997161865,
        "learning_rate": 9.895559305255883e-05,
        "epoch": 0.33836040216550656,
        "step": 2625
    },
    {
        "loss": 1.9359,
        "grad_norm": 2.513031005859375,
        "learning_rate": 9.8894756222134e-05,
        "epoch": 0.33848930136633154,
        "step": 2626
    },
    {
        "loss": 2.6585,
        "grad_norm": 1.1558725833892822,
        "learning_rate": 9.883391980082041e-05,
        "epoch": 0.33861820056715647,
        "step": 2627
    },
    {
        "loss": 2.2413,
        "grad_norm": 1.2366684675216675,
        "learning_rate": 9.87730838111369e-05,
        "epoch": 0.33874709976798145,
        "step": 2628
    },
    {
        "loss": 1.318,
        "grad_norm": 1.8550724983215332,
        "learning_rate": 9.871224827560227e-05,
        "epoch": 0.3388759989688064,
        "step": 2629
    },
    {
        "loss": 2.5577,
        "grad_norm": 1.109324336051941,
        "learning_rate": 9.86514132167351e-05,
        "epoch": 0.33900489816963136,
        "step": 2630
    },
    {
        "loss": 2.3429,
        "grad_norm": 1.7217847108840942,
        "learning_rate": 9.859057865705371e-05,
        "epoch": 0.3391337973704563,
        "step": 2631
    },
    {
        "loss": 1.7578,
        "grad_norm": 2.7453818321228027,
        "learning_rate": 9.852974461907631e-05,
        "epoch": 0.3392626965712813,
        "step": 2632
    },
    {
        "loss": 2.214,
        "grad_norm": 1.5855650901794434,
        "learning_rate": 9.846891112532099e-05,
        "epoch": 0.3393915957721062,
        "step": 2633
    },
    {
        "loss": 1.6662,
        "grad_norm": 1.6099640130996704,
        "learning_rate": 9.840807819830544e-05,
        "epoch": 0.3395204949729312,
        "step": 2634
    },
    {
        "loss": 1.6272,
        "grad_norm": 1.6321423053741455,
        "learning_rate": 9.834724586054736e-05,
        "epoch": 0.3396493941737561,
        "step": 2635
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.1537222862243652,
        "learning_rate": 9.828641413456406e-05,
        "epoch": 0.3397782933745811,
        "step": 2636
    },
    {
        "loss": 1.8577,
        "grad_norm": 1.942064642906189,
        "learning_rate": 9.822558304287275e-05,
        "epoch": 0.339907192575406,
        "step": 2637
    },
    {
        "loss": 1.7584,
        "grad_norm": 2.4577999114990234,
        "learning_rate": 9.816475260799026e-05,
        "epoch": 0.340036091776231,
        "step": 2638
    },
    {
        "loss": 2.0455,
        "grad_norm": 1.608718991279602,
        "learning_rate": 9.810392285243337e-05,
        "epoch": 0.34016499097705594,
        "step": 2639
    },
    {
        "loss": 1.4297,
        "grad_norm": 2.0540571212768555,
        "learning_rate": 9.804309379871845e-05,
        "epoch": 0.3402938901778809,
        "step": 2640
    },
    {
        "loss": 1.4703,
        "grad_norm": 2.034636974334717,
        "learning_rate": 9.79822654693616e-05,
        "epoch": 0.34042278937870585,
        "step": 2641
    },
    {
        "loss": 1.5805,
        "grad_norm": 2.378425359725952,
        "learning_rate": 9.792143788687884e-05,
        "epoch": 0.34055168857953083,
        "step": 2642
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.9070627689361572,
        "learning_rate": 9.786061107378574e-05,
        "epoch": 0.34068058778035576,
        "step": 2643
    },
    {
        "loss": 1.6753,
        "grad_norm": 1.9794069528579712,
        "learning_rate": 9.779978505259764e-05,
        "epoch": 0.34080948698118074,
        "step": 2644
    },
    {
        "loss": 1.8527,
        "grad_norm": 1.9988049268722534,
        "learning_rate": 9.773895984582952e-05,
        "epoch": 0.34093838618200567,
        "step": 2645
    },
    {
        "loss": 1.5595,
        "grad_norm": 1.5418308973312378,
        "learning_rate": 9.767813547599615e-05,
        "epoch": 0.34106728538283065,
        "step": 2646
    },
    {
        "loss": 2.2765,
        "grad_norm": 1.979864478111267,
        "learning_rate": 9.761731196561202e-05,
        "epoch": 0.3411961845836556,
        "step": 2647
    },
    {
        "loss": 2.2673,
        "grad_norm": 1.9697281122207642,
        "learning_rate": 9.755648933719118e-05,
        "epoch": 0.3413250837844805,
        "step": 2648
    },
    {
        "loss": 2.2957,
        "grad_norm": 1.7604665756225586,
        "learning_rate": 9.749566761324744e-05,
        "epoch": 0.3414539829853055,
        "step": 2649
    },
    {
        "loss": 2.0841,
        "grad_norm": 2.5745956897735596,
        "learning_rate": 9.743484681629427e-05,
        "epoch": 0.3415828821861304,
        "step": 2650
    },
    {
        "loss": 1.2923,
        "grad_norm": 2.156548023223877,
        "learning_rate": 9.73740269688447e-05,
        "epoch": 0.3417117813869554,
        "step": 2651
    },
    {
        "loss": 2.1273,
        "grad_norm": 1.2946412563323975,
        "learning_rate": 9.731320809341158e-05,
        "epoch": 0.34184068058778033,
        "step": 2652
    },
    {
        "loss": 2.2714,
        "grad_norm": 1.9442843198776245,
        "learning_rate": 9.725239021250721e-05,
        "epoch": 0.3419695797886053,
        "step": 2653
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.9855436086654663,
        "learning_rate": 9.719157334864371e-05,
        "epoch": 0.34209847898943024,
        "step": 2654
    },
    {
        "loss": 0.8907,
        "grad_norm": 2.133981466293335,
        "learning_rate": 9.713075752433268e-05,
        "epoch": 0.3422273781902552,
        "step": 2655
    },
    {
        "loss": 2.0185,
        "grad_norm": 2.104123115539551,
        "learning_rate": 9.70699427620854e-05,
        "epoch": 0.34235627739108015,
        "step": 2656
    },
    {
        "loss": 1.2357,
        "grad_norm": 2.4121556282043457,
        "learning_rate": 9.700912908441276e-05,
        "epoch": 0.34248517659190514,
        "step": 2657
    },
    {
        "loss": 1.9425,
        "grad_norm": 2.064694881439209,
        "learning_rate": 9.694831651382513e-05,
        "epoch": 0.34261407579273007,
        "step": 2658
    },
    {
        "loss": 2.034,
        "grad_norm": 2.137948989868164,
        "learning_rate": 9.688750507283274e-05,
        "epoch": 0.34274297499355505,
        "step": 2659
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.8846323490142822,
        "learning_rate": 9.682669478394515e-05,
        "epoch": 0.34287187419438,
        "step": 2660
    },
    {
        "loss": 1.7213,
        "grad_norm": 2.1723384857177734,
        "learning_rate": 9.676588566967153e-05,
        "epoch": 0.34300077339520496,
        "step": 2661
    },
    {
        "loss": 1.4272,
        "grad_norm": 2.7217209339141846,
        "learning_rate": 9.670507775252076e-05,
        "epoch": 0.3431296725960299,
        "step": 2662
    },
    {
        "loss": 2.1755,
        "grad_norm": 1.5251556634902954,
        "learning_rate": 9.664427105500108e-05,
        "epoch": 0.34325857179685487,
        "step": 2663
    },
    {
        "loss": 1.838,
        "grad_norm": 2.621798038482666,
        "learning_rate": 9.658346559962044e-05,
        "epoch": 0.3433874709976798,
        "step": 2664
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.73416268825531,
        "learning_rate": 9.652266140888626e-05,
        "epoch": 0.3435163701985048,
        "step": 2665
    },
    {
        "loss": 2.1193,
        "grad_norm": 1.768965721130371,
        "learning_rate": 9.646185850530553e-05,
        "epoch": 0.3436452693993297,
        "step": 2666
    },
    {
        "loss": 2.0889,
        "grad_norm": 1.267677664756775,
        "learning_rate": 9.64010569113847e-05,
        "epoch": 0.3437741686001547,
        "step": 2667
    },
    {
        "loss": 1.5245,
        "grad_norm": 1.9604191780090332,
        "learning_rate": 9.634025664962972e-05,
        "epoch": 0.3439030678009796,
        "step": 2668
    },
    {
        "loss": 2.1417,
        "grad_norm": 1.5877909660339355,
        "learning_rate": 9.627945774254617e-05,
        "epoch": 0.3440319670018046,
        "step": 2669
    },
    {
        "loss": 2.1525,
        "grad_norm": 1.324245572090149,
        "learning_rate": 9.621866021263903e-05,
        "epoch": 0.34416086620262953,
        "step": 2670
    },
    {
        "loss": 0.9018,
        "grad_norm": 2.321897506713867,
        "learning_rate": 9.615786408241278e-05,
        "epoch": 0.3442897654034545,
        "step": 2671
    },
    {
        "loss": 2.6862,
        "grad_norm": 1.6334141492843628,
        "learning_rate": 9.609706937437143e-05,
        "epoch": 0.34441866460427945,
        "step": 2672
    },
    {
        "loss": 2.0753,
        "grad_norm": 2.135671854019165,
        "learning_rate": 9.603627611101843e-05,
        "epoch": 0.34454756380510443,
        "step": 2673
    },
    {
        "loss": 2.1877,
        "grad_norm": 1.4963951110839844,
        "learning_rate": 9.59754843148567e-05,
        "epoch": 0.34467646300592936,
        "step": 2674
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.5735191106796265,
        "learning_rate": 9.591469400838847e-05,
        "epoch": 0.34480536220675434,
        "step": 2675
    },
    {
        "loss": 2.0831,
        "grad_norm": 1.85415780544281,
        "learning_rate": 9.585390521411579e-05,
        "epoch": 0.34493426140757927,
        "step": 2676
    },
    {
        "loss": 2.7675,
        "grad_norm": 1.5502238273620605,
        "learning_rate": 9.579311795453981e-05,
        "epoch": 0.34506316060840425,
        "step": 2677
    },
    {
        "loss": 1.6246,
        "grad_norm": 1.8806334733963013,
        "learning_rate": 9.573233225216117e-05,
        "epoch": 0.3451920598092292,
        "step": 2678
    },
    {
        "loss": 1.9869,
        "grad_norm": 1.3221431970596313,
        "learning_rate": 9.567154812948008e-05,
        "epoch": 0.34532095901005416,
        "step": 2679
    },
    {
        "loss": 1.3041,
        "grad_norm": 2.600285530090332,
        "learning_rate": 9.5610765608996e-05,
        "epoch": 0.3454498582108791,
        "step": 2680
    },
    {
        "loss": 2.4641,
        "grad_norm": 1.4323768615722656,
        "learning_rate": 9.554998471320795e-05,
        "epoch": 0.3455787574117041,
        "step": 2681
    },
    {
        "loss": 1.8547,
        "grad_norm": 2.6294384002685547,
        "learning_rate": 9.548920546461419e-05,
        "epoch": 0.345707656612529,
        "step": 2682
    },
    {
        "loss": 1.9067,
        "grad_norm": 1.5666236877441406,
        "learning_rate": 9.54284278857125e-05,
        "epoch": 0.34583655581335393,
        "step": 2683
    },
    {
        "loss": 1.8749,
        "grad_norm": 2.1898958683013916,
        "learning_rate": 9.5367651999e-05,
        "epoch": 0.3459654550141789,
        "step": 2684
    },
    {
        "loss": 1.1858,
        "grad_norm": 2.1407272815704346,
        "learning_rate": 9.530687782697308e-05,
        "epoch": 0.34609435421500384,
        "step": 2685
    },
    {
        "loss": 2.2999,
        "grad_norm": 1.8692518472671509,
        "learning_rate": 9.524610539212769e-05,
        "epoch": 0.3462232534158288,
        "step": 2686
    },
    {
        "loss": 1.6212,
        "grad_norm": 1.743411898612976,
        "learning_rate": 9.518533471695895e-05,
        "epoch": 0.34635215261665375,
        "step": 2687
    },
    {
        "loss": 2.2259,
        "grad_norm": 1.0563911199569702,
        "learning_rate": 9.51245658239615e-05,
        "epoch": 0.34648105181747874,
        "step": 2688
    },
    {
        "loss": 2.0894,
        "grad_norm": 2.0167853832244873,
        "learning_rate": 9.506379873562919e-05,
        "epoch": 0.34660995101830366,
        "step": 2689
    },
    {
        "loss": 1.8584,
        "grad_norm": 2.3149759769439697,
        "learning_rate": 9.500303347445528e-05,
        "epoch": 0.34673885021912865,
        "step": 2690
    },
    {
        "loss": 1.0759,
        "grad_norm": 1.7648961544036865,
        "learning_rate": 9.494227006293225e-05,
        "epoch": 0.3468677494199536,
        "step": 2691
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.8212320804595947,
        "learning_rate": 9.4881508523552e-05,
        "epoch": 0.34699664862077856,
        "step": 2692
    },
    {
        "loss": 1.7642,
        "grad_norm": 2.197795867919922,
        "learning_rate": 9.482074887880576e-05,
        "epoch": 0.3471255478216035,
        "step": 2693
    },
    {
        "loss": 2.001,
        "grad_norm": 2.0216548442840576,
        "learning_rate": 9.475999115118393e-05,
        "epoch": 0.34725444702242847,
        "step": 2694
    },
    {
        "loss": 1.3462,
        "grad_norm": 2.97481107711792,
        "learning_rate": 9.46992353631763e-05,
        "epoch": 0.3473833462232534,
        "step": 2695
    },
    {
        "loss": 2.4939,
        "grad_norm": 1.397674798965454,
        "learning_rate": 9.463848153727191e-05,
        "epoch": 0.3475122454240784,
        "step": 2696
    },
    {
        "loss": 1.8997,
        "grad_norm": 1.8382872343063354,
        "learning_rate": 9.457772969595906e-05,
        "epoch": 0.3476411446249033,
        "step": 2697
    },
    {
        "loss": 1.7579,
        "grad_norm": 2.4439854621887207,
        "learning_rate": 9.451697986172538e-05,
        "epoch": 0.3477700438257283,
        "step": 2698
    },
    {
        "loss": 1.3654,
        "grad_norm": 1.6987334489822388,
        "learning_rate": 9.445623205705764e-05,
        "epoch": 0.3478989430265532,
        "step": 2699
    },
    {
        "loss": 2.379,
        "grad_norm": 1.7879937887191772,
        "learning_rate": 9.439548630444202e-05,
        "epoch": 0.3480278422273782,
        "step": 2700
    },
    {
        "loss": 1.5539,
        "grad_norm": 1.830615758895874,
        "learning_rate": 9.433474262636376e-05,
        "epoch": 0.34815674142820313,
        "step": 2701
    },
    {
        "loss": 1.8739,
        "grad_norm": 1.8370188474655151,
        "learning_rate": 9.427400104530748e-05,
        "epoch": 0.3482856406290281,
        "step": 2702
    },
    {
        "loss": 1.6918,
        "grad_norm": 1.5518008470535278,
        "learning_rate": 9.421326158375695e-05,
        "epoch": 0.34841453982985304,
        "step": 2703
    },
    {
        "loss": 1.8399,
        "grad_norm": 2.5293166637420654,
        "learning_rate": 9.415252426419513e-05,
        "epoch": 0.348543439030678,
        "step": 2704
    },
    {
        "loss": 2.4276,
        "grad_norm": 1.3541964292526245,
        "learning_rate": 9.409178910910425e-05,
        "epoch": 0.34867233823150295,
        "step": 2705
    },
    {
        "loss": 2.264,
        "grad_norm": 1.4585260152816772,
        "learning_rate": 9.40310561409658e-05,
        "epoch": 0.34880123743232794,
        "step": 2706
    },
    {
        "loss": 1.7236,
        "grad_norm": 1.9761508703231812,
        "learning_rate": 9.397032538226029e-05,
        "epoch": 0.34893013663315287,
        "step": 2707
    },
    {
        "loss": 2.3462,
        "grad_norm": 1.897828459739685,
        "learning_rate": 9.390959685546747e-05,
        "epoch": 0.34905903583397785,
        "step": 2708
    },
    {
        "loss": 2.3602,
        "grad_norm": 2.304419755935669,
        "learning_rate": 9.384887058306634e-05,
        "epoch": 0.3491879350348028,
        "step": 2709
    },
    {
        "loss": 1.7071,
        "grad_norm": 1.63520348072052,
        "learning_rate": 9.378814658753507e-05,
        "epoch": 0.34931683423562776,
        "step": 2710
    },
    {
        "loss": 2.0036,
        "grad_norm": 1.6407979726791382,
        "learning_rate": 9.372742489135083e-05,
        "epoch": 0.3494457334364527,
        "step": 2711
    },
    {
        "loss": 2.2163,
        "grad_norm": 1.3076131343841553,
        "learning_rate": 9.36667055169901e-05,
        "epoch": 0.34957463263727767,
        "step": 2712
    },
    {
        "loss": 1.6421,
        "grad_norm": 2.386687994003296,
        "learning_rate": 9.36059884869285e-05,
        "epoch": 0.3497035318381026,
        "step": 2713
    },
    {
        "loss": 2.0842,
        "grad_norm": 2.1277356147766113,
        "learning_rate": 9.35452738236406e-05,
        "epoch": 0.3498324310389276,
        "step": 2714
    },
    {
        "loss": 1.6331,
        "grad_norm": 2.118190050125122,
        "learning_rate": 9.348456154960035e-05,
        "epoch": 0.3499613302397525,
        "step": 2715
    },
    {
        "loss": 1.4794,
        "grad_norm": 2.174572229385376,
        "learning_rate": 9.342385168728057e-05,
        "epoch": 0.3500902294405775,
        "step": 2716
    },
    {
        "loss": 2.3311,
        "grad_norm": 1.1141759157180786,
        "learning_rate": 9.336314425915342e-05,
        "epoch": 0.3502191286414024,
        "step": 2717
    },
    {
        "loss": 2.2025,
        "grad_norm": 2.484699249267578,
        "learning_rate": 9.330243928768993e-05,
        "epoch": 0.3503480278422274,
        "step": 2718
    },
    {
        "loss": 1.6297,
        "grad_norm": 1.3332840204238892,
        "learning_rate": 9.324173679536042e-05,
        "epoch": 0.35047692704305233,
        "step": 2719
    },
    {
        "loss": 2.1626,
        "grad_norm": 1.9337197542190552,
        "learning_rate": 9.31810368046342e-05,
        "epoch": 0.35060582624387726,
        "step": 2720
    },
    {
        "loss": 0.7482,
        "grad_norm": 1.4800511598587036,
        "learning_rate": 9.312033933797957e-05,
        "epoch": 0.35073472544470224,
        "step": 2721
    },
    {
        "loss": 1.9261,
        "grad_norm": 2.6590306758880615,
        "learning_rate": 9.305964441786409e-05,
        "epoch": 0.3508636246455272,
        "step": 2722
    },
    {
        "loss": 1.3172,
        "grad_norm": 2.398787021636963,
        "learning_rate": 9.299895206675426e-05,
        "epoch": 0.35099252384635216,
        "step": 2723
    },
    {
        "loss": 2.3627,
        "grad_norm": 1.8129782676696777,
        "learning_rate": 9.293826230711556e-05,
        "epoch": 0.3511214230471771,
        "step": 2724
    },
    {
        "loss": 1.3944,
        "grad_norm": 1.6945557594299316,
        "learning_rate": 9.287757516141272e-05,
        "epoch": 0.35125032224800207,
        "step": 2725
    },
    {
        "loss": 1.7605,
        "grad_norm": 1.651390552520752,
        "learning_rate": 9.281689065210925e-05,
        "epoch": 0.351379221448827,
        "step": 2726
    },
    {
        "loss": 1.8492,
        "grad_norm": 1.5357846021652222,
        "learning_rate": 9.275620880166788e-05,
        "epoch": 0.351508120649652,
        "step": 2727
    },
    {
        "loss": 2.2551,
        "grad_norm": 1.132411003112793,
        "learning_rate": 9.269552963255028e-05,
        "epoch": 0.3516370198504769,
        "step": 2728
    },
    {
        "loss": 2.4888,
        "grad_norm": 1.2179253101348877,
        "learning_rate": 9.263485316721711e-05,
        "epoch": 0.3517659190513019,
        "step": 2729
    },
    {
        "loss": 2.3223,
        "grad_norm": 1.320143699645996,
        "learning_rate": 9.257417942812812e-05,
        "epoch": 0.3518948182521268,
        "step": 2730
    },
    {
        "loss": 1.8926,
        "grad_norm": 1.5706969499588013,
        "learning_rate": 9.251350843774186e-05,
        "epoch": 0.3520237174529518,
        "step": 2731
    },
    {
        "loss": 1.5829,
        "grad_norm": 2.0918126106262207,
        "learning_rate": 9.245284021851612e-05,
        "epoch": 0.35215261665377673,
        "step": 2732
    },
    {
        "loss": 1.5098,
        "grad_norm": 2.1237661838531494,
        "learning_rate": 9.239217479290741e-05,
        "epoch": 0.3522815158546017,
        "step": 2733
    },
    {
        "loss": 2.2686,
        "grad_norm": 1.7541730403900146,
        "learning_rate": 9.233151218337137e-05,
        "epoch": 0.35241041505542664,
        "step": 2734
    },
    {
        "loss": 1.6338,
        "grad_norm": 1.631108283996582,
        "learning_rate": 9.22708524123626e-05,
        "epoch": 0.3525393142562516,
        "step": 2735
    },
    {
        "loss": 2.7294,
        "grad_norm": 1.5449659824371338,
        "learning_rate": 9.221019550233459e-05,
        "epoch": 0.35266821345707655,
        "step": 2736
    },
    {
        "loss": 2.4781,
        "grad_norm": 1.4919902086257935,
        "learning_rate": 9.214954147573975e-05,
        "epoch": 0.35279711265790153,
        "step": 2737
    },
    {
        "loss": 2.5537,
        "grad_norm": 1.6322108507156372,
        "learning_rate": 9.208889035502939e-05,
        "epoch": 0.35292601185872646,
        "step": 2738
    },
    {
        "loss": 2.225,
        "grad_norm": 1.302298903465271,
        "learning_rate": 9.202824216265396e-05,
        "epoch": 0.35305491105955145,
        "step": 2739
    },
    {
        "loss": 2.4558,
        "grad_norm": 1.6318261623382568,
        "learning_rate": 9.196759692106262e-05,
        "epoch": 0.3531838102603764,
        "step": 2740
    },
    {
        "loss": 2.1794,
        "grad_norm": 1.0333524942398071,
        "learning_rate": 9.190695465270343e-05,
        "epoch": 0.35331270946120136,
        "step": 2741
    },
    {
        "loss": 2.2773,
        "grad_norm": 1.5966347455978394,
        "learning_rate": 9.184631538002349e-05,
        "epoch": 0.3534416086620263,
        "step": 2742
    },
    {
        "loss": 2.2489,
        "grad_norm": 1.352309226989746,
        "learning_rate": 9.178567912546869e-05,
        "epoch": 0.35357050786285127,
        "step": 2743
    },
    {
        "loss": 1.9263,
        "grad_norm": 2.3834915161132812,
        "learning_rate": 9.172504591148384e-05,
        "epoch": 0.3536994070636762,
        "step": 2744
    },
    {
        "loss": 2.0367,
        "grad_norm": 1.4780155420303345,
        "learning_rate": 9.166441576051255e-05,
        "epoch": 0.3538283062645012,
        "step": 2745
    },
    {
        "loss": 1.869,
        "grad_norm": 2.046987295150757,
        "learning_rate": 9.160378869499746e-05,
        "epoch": 0.3539572054653261,
        "step": 2746
    },
    {
        "loss": 1.861,
        "grad_norm": 2.263266086578369,
        "learning_rate": 9.154316473737992e-05,
        "epoch": 0.3540861046661511,
        "step": 2747
    },
    {
        "loss": 1.5006,
        "grad_norm": 1.504370093345642,
        "learning_rate": 9.148254391010012e-05,
        "epoch": 0.354215003866976,
        "step": 2748
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.8511962890625,
        "learning_rate": 9.142192623559723e-05,
        "epoch": 0.354343903067801,
        "step": 2749
    },
    {
        "loss": 2.5652,
        "grad_norm": 1.8966368436813354,
        "learning_rate": 9.136131173630914e-05,
        "epoch": 0.35447280226862593,
        "step": 2750
    },
    {
        "loss": 0.8428,
        "grad_norm": 2.121591329574585,
        "learning_rate": 9.130070043467255e-05,
        "epoch": 0.3546017014694509,
        "step": 2751
    },
    {
        "loss": 2.2529,
        "grad_norm": 1.2899376153945923,
        "learning_rate": 9.124009235312312e-05,
        "epoch": 0.35473060067027584,
        "step": 2752
    },
    {
        "loss": 2.0611,
        "grad_norm": 2.3581418991088867,
        "learning_rate": 9.117948751409518e-05,
        "epoch": 0.3548594998711008,
        "step": 2753
    },
    {
        "loss": 1.8744,
        "grad_norm": 1.4288467168807983,
        "learning_rate": 9.111888594002183e-05,
        "epoch": 0.35498839907192575,
        "step": 2754
    },
    {
        "loss": 0.9449,
        "grad_norm": 2.033907413482666,
        "learning_rate": 9.105828765333508e-05,
        "epoch": 0.35511729827275074,
        "step": 2755
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.5225532054901123,
        "learning_rate": 9.099769267646573e-05,
        "epoch": 0.35524619747357566,
        "step": 2756
    },
    {
        "loss": 0.9715,
        "grad_norm": 2.750227689743042,
        "learning_rate": 9.093710103184325e-05,
        "epoch": 0.3553750966744006,
        "step": 2757
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.9381376504898071,
        "learning_rate": 9.087651274189589e-05,
        "epoch": 0.3555039958752256,
        "step": 2758
    },
    {
        "loss": 2.4195,
        "grad_norm": 2.088590383529663,
        "learning_rate": 9.081592782905074e-05,
        "epoch": 0.3556328950760505,
        "step": 2759
    },
    {
        "loss": 1.7249,
        "grad_norm": 1.982138752937317,
        "learning_rate": 9.075534631573354e-05,
        "epoch": 0.3557617942768755,
        "step": 2760
    },
    {
        "loss": 1.9588,
        "grad_norm": 3.027550220489502,
        "learning_rate": 9.069476822436893e-05,
        "epoch": 0.3558906934777004,
        "step": 2761
    },
    {
        "loss": 1.6763,
        "grad_norm": 2.616406202316284,
        "learning_rate": 9.063419357738008e-05,
        "epoch": 0.3560195926785254,
        "step": 2762
    },
    {
        "loss": 1.2881,
        "grad_norm": 1.6136023998260498,
        "learning_rate": 9.057362239718905e-05,
        "epoch": 0.3561484918793503,
        "step": 2763
    },
    {
        "loss": 2.1721,
        "grad_norm": 2.0693397521972656,
        "learning_rate": 9.051305470621648e-05,
        "epoch": 0.3562773910801753,
        "step": 2764
    },
    {
        "loss": 1.7051,
        "grad_norm": 2.681748390197754,
        "learning_rate": 9.045249052688186e-05,
        "epoch": 0.35640629028100024,
        "step": 2765
    },
    {
        "loss": 2.252,
        "grad_norm": 1.7440563440322876,
        "learning_rate": 9.039192988160326e-05,
        "epoch": 0.3565351894818252,
        "step": 2766
    },
    {
        "loss": 1.742,
        "grad_norm": 2.151193618774414,
        "learning_rate": 9.033137279279749e-05,
        "epoch": 0.35666408868265015,
        "step": 2767
    },
    {
        "loss": 1.5603,
        "grad_norm": 1.7857475280761719,
        "learning_rate": 9.027081928288006e-05,
        "epoch": 0.35679298788347513,
        "step": 2768
    },
    {
        "loss": 1.9194,
        "grad_norm": 2.132180690765381,
        "learning_rate": 9.021026937426518e-05,
        "epoch": 0.35692188708430006,
        "step": 2769
    },
    {
        "loss": 0.9796,
        "grad_norm": 1.7404284477233887,
        "learning_rate": 9.014972308936567e-05,
        "epoch": 0.35705078628512504,
        "step": 2770
    },
    {
        "loss": 1.9835,
        "grad_norm": 1.4316474199295044,
        "learning_rate": 9.008918045059294e-05,
        "epoch": 0.35717968548594997,
        "step": 2771
    },
    {
        "loss": 2.5012,
        "grad_norm": 1.661238193511963,
        "learning_rate": 9.00286414803572e-05,
        "epoch": 0.35730858468677495,
        "step": 2772
    },
    {
        "loss": 1.6444,
        "grad_norm": 2.059951066970825,
        "learning_rate": 8.99681062010673e-05,
        "epoch": 0.3574374838875999,
        "step": 2773
    },
    {
        "loss": 1.394,
        "grad_norm": 2.041738986968994,
        "learning_rate": 8.990757463513053e-05,
        "epoch": 0.35756638308842487,
        "step": 2774
    },
    {
        "loss": 2.0076,
        "grad_norm": 1.401571273803711,
        "learning_rate": 8.984704680495304e-05,
        "epoch": 0.3576952822892498,
        "step": 2775
    },
    {
        "loss": 1.8448,
        "grad_norm": 1.7125544548034668,
        "learning_rate": 8.978652273293948e-05,
        "epoch": 0.3578241814900748,
        "step": 2776
    },
    {
        "loss": 2.1347,
        "grad_norm": 1.9719934463500977,
        "learning_rate": 8.972600244149305e-05,
        "epoch": 0.3579530806908997,
        "step": 2777
    },
    {
        "loss": 1.8361,
        "grad_norm": 1.3232965469360352,
        "learning_rate": 8.966548595301573e-05,
        "epoch": 0.3580819798917247,
        "step": 2778
    },
    {
        "loss": 1.9895,
        "grad_norm": 1.2358882427215576,
        "learning_rate": 8.960497328990789e-05,
        "epoch": 0.3582108790925496,
        "step": 2779
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.0226194858551025,
        "learning_rate": 8.954446447456867e-05,
        "epoch": 0.3583397782933746,
        "step": 2780
    },
    {
        "loss": 2.2517,
        "grad_norm": 1.547706127166748,
        "learning_rate": 8.94839595293956e-05,
        "epoch": 0.35846867749419953,
        "step": 2781
    },
    {
        "loss": 1.9073,
        "grad_norm": 1.3640148639678955,
        "learning_rate": 8.942345847678494e-05,
        "epoch": 0.3585975766950245,
        "step": 2782
    },
    {
        "loss": 1.9061,
        "grad_norm": 1.2238484621047974,
        "learning_rate": 8.936296133913144e-05,
        "epoch": 0.35872647589584944,
        "step": 2783
    },
    {
        "loss": 1.7959,
        "grad_norm": 1.6926466226577759,
        "learning_rate": 8.930246813882834e-05,
        "epoch": 0.3588553750966744,
        "step": 2784
    },
    {
        "loss": 2.3006,
        "grad_norm": 1.1441961526870728,
        "learning_rate": 8.924197889826756e-05,
        "epoch": 0.35898427429749935,
        "step": 2785
    },
    {
        "loss": 1.6727,
        "grad_norm": 1.9048727750778198,
        "learning_rate": 8.918149363983948e-05,
        "epoch": 0.35911317349832433,
        "step": 2786
    },
    {
        "loss": 1.9873,
        "grad_norm": 1.6614919900894165,
        "learning_rate": 8.912101238593294e-05,
        "epoch": 0.35924207269914926,
        "step": 2787
    },
    {
        "loss": 1.3074,
        "grad_norm": 1.9009026288986206,
        "learning_rate": 8.906053515893544e-05,
        "epoch": 0.35937097189997425,
        "step": 2788
    },
    {
        "loss": 1.5826,
        "grad_norm": 2.235098361968994,
        "learning_rate": 8.900006198123284e-05,
        "epoch": 0.3594998711007992,
        "step": 2789
    },
    {
        "loss": 2.1931,
        "grad_norm": 1.5609042644500732,
        "learning_rate": 8.893959287520966e-05,
        "epoch": 0.35962877030162416,
        "step": 2790
    },
    {
        "loss": 1.902,
        "grad_norm": 2.509343147277832,
        "learning_rate": 8.887912786324872e-05,
        "epoch": 0.3597576695024491,
        "step": 2791
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.7237871885299683,
        "learning_rate": 8.881866696773151e-05,
        "epoch": 0.359886568703274,
        "step": 2792
    },
    {
        "loss": 1.6898,
        "grad_norm": 2.429558277130127,
        "learning_rate": 8.875821021103793e-05,
        "epoch": 0.360015467904099,
        "step": 2793
    },
    {
        "loss": 1.6977,
        "grad_norm": 1.5521016120910645,
        "learning_rate": 8.869775761554625e-05,
        "epoch": 0.3601443671049239,
        "step": 2794
    },
    {
        "loss": 1.3931,
        "grad_norm": 2.0941967964172363,
        "learning_rate": 8.863730920363339e-05,
        "epoch": 0.3602732663057489,
        "step": 2795
    },
    {
        "loss": 2.1364,
        "grad_norm": 1.6470433473587036,
        "learning_rate": 8.857686499767452e-05,
        "epoch": 0.36040216550657383,
        "step": 2796
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.2048838138580322,
        "learning_rate": 8.851642502004339e-05,
        "epoch": 0.3605310647073988,
        "step": 2797
    },
    {
        "loss": 2.0676,
        "grad_norm": 2.0613620281219482,
        "learning_rate": 8.845598929311221e-05,
        "epoch": 0.36065996390822375,
        "step": 2798
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.041074275970459,
        "learning_rate": 8.839555783925146e-05,
        "epoch": 0.36078886310904873,
        "step": 2799
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.1177499294281006,
        "learning_rate": 8.833513068083018e-05,
        "epoch": 0.36091776230987366,
        "step": 2800
    },
    {
        "loss": 2.4227,
        "grad_norm": 2.237863779067993,
        "learning_rate": 8.827470784021567e-05,
        "epoch": 0.36104666151069864,
        "step": 2801
    },
    {
        "loss": 2.5784,
        "grad_norm": 1.5933737754821777,
        "learning_rate": 8.821428933977392e-05,
        "epoch": 0.36117556071152357,
        "step": 2802
    },
    {
        "loss": 2.0235,
        "grad_norm": 1.1427580118179321,
        "learning_rate": 8.815387520186899e-05,
        "epoch": 0.36130445991234855,
        "step": 2803
    },
    {
        "loss": 0.9002,
        "grad_norm": 2.2366058826446533,
        "learning_rate": 8.809346544886348e-05,
        "epoch": 0.3614333591131735,
        "step": 2804
    },
    {
        "loss": 2.339,
        "grad_norm": 1.2539013624191284,
        "learning_rate": 8.803306010311838e-05,
        "epoch": 0.36156225831399846,
        "step": 2805
    },
    {
        "loss": 1.5308,
        "grad_norm": 2.323770761489868,
        "learning_rate": 8.797265918699296e-05,
        "epoch": 0.3616911575148234,
        "step": 2806
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.514064311981201,
        "learning_rate": 8.791226272284497e-05,
        "epoch": 0.3618200567156484,
        "step": 2807
    },
    {
        "loss": 1.4102,
        "grad_norm": 2.5651965141296387,
        "learning_rate": 8.785187073303038e-05,
        "epoch": 0.3619489559164733,
        "step": 2808
    },
    {
        "loss": 2.308,
        "grad_norm": 2.20770263671875,
        "learning_rate": 8.779148323990366e-05,
        "epoch": 0.3620778551172983,
        "step": 2809
    },
    {
        "loss": 2.1309,
        "grad_norm": 1.1870614290237427,
        "learning_rate": 8.773110026581748e-05,
        "epoch": 0.3622067543181232,
        "step": 2810
    },
    {
        "loss": 1.3261,
        "grad_norm": 2.173386335372925,
        "learning_rate": 8.767072183312283e-05,
        "epoch": 0.3623356535189482,
        "step": 2811
    },
    {
        "loss": 2.1017,
        "grad_norm": 1.2407807111740112,
        "learning_rate": 8.76103479641692e-05,
        "epoch": 0.3624645527197731,
        "step": 2812
    },
    {
        "loss": 2.1922,
        "grad_norm": 1.949550747871399,
        "learning_rate": 8.754997868130413e-05,
        "epoch": 0.3625934519205981,
        "step": 2813
    },
    {
        "loss": 2.5452,
        "grad_norm": 1.807958722114563,
        "learning_rate": 8.748961400687365e-05,
        "epoch": 0.36272235112142304,
        "step": 2814
    },
    {
        "loss": 2.5352,
        "grad_norm": 1.031150221824646,
        "learning_rate": 8.742925396322208e-05,
        "epoch": 0.362851250322248,
        "step": 2815
    },
    {
        "loss": 2.0437,
        "grad_norm": 1.0195116996765137,
        "learning_rate": 8.736889857269196e-05,
        "epoch": 0.36298014952307295,
        "step": 2816
    },
    {
        "loss": 2.3965,
        "grad_norm": 1.4592760801315308,
        "learning_rate": 8.730854785762405e-05,
        "epoch": 0.36310904872389793,
        "step": 2817
    },
    {
        "loss": 1.8592,
        "grad_norm": 2.0588130950927734,
        "learning_rate": 8.724820184035753e-05,
        "epoch": 0.36323794792472286,
        "step": 2818
    },
    {
        "loss": 2.1239,
        "grad_norm": 1.660918951034546,
        "learning_rate": 8.718786054322976e-05,
        "epoch": 0.36336684712554784,
        "step": 2819
    },
    {
        "loss": 1.5644,
        "grad_norm": 1.6506708860397339,
        "learning_rate": 8.712752398857636e-05,
        "epoch": 0.36349574632637277,
        "step": 2820
    },
    {
        "loss": 0.7649,
        "grad_norm": 1.7528773546218872,
        "learning_rate": 8.706719219873112e-05,
        "epoch": 0.36362464552719775,
        "step": 2821
    },
    {
        "loss": 2.3058,
        "grad_norm": 1.023863434791565,
        "learning_rate": 8.70068651960262e-05,
        "epoch": 0.3637535447280227,
        "step": 2822
    },
    {
        "loss": 1.5108,
        "grad_norm": 1.7284775972366333,
        "learning_rate": 8.694654300279195e-05,
        "epoch": 0.36388244392884767,
        "step": 2823
    },
    {
        "loss": 1.8971,
        "grad_norm": 2.363555908203125,
        "learning_rate": 8.688622564135687e-05,
        "epoch": 0.3640113431296726,
        "step": 2824
    },
    {
        "loss": 1.5061,
        "grad_norm": 2.3656420707702637,
        "learning_rate": 8.682591313404771e-05,
        "epoch": 0.3641402423304976,
        "step": 2825
    },
    {
        "loss": 1.3138,
        "grad_norm": 1.7263920307159424,
        "learning_rate": 8.676560550318951e-05,
        "epoch": 0.3642691415313225,
        "step": 2826
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.1786023378372192,
        "learning_rate": 8.670530277110531e-05,
        "epoch": 0.3643980407321475,
        "step": 2827
    },
    {
        "loss": 2.0031,
        "grad_norm": 1.604593276977539,
        "learning_rate": 8.664500496011658e-05,
        "epoch": 0.3645269399329724,
        "step": 2828
    },
    {
        "loss": 2.3147,
        "grad_norm": 1.8668889999389648,
        "learning_rate": 8.658471209254274e-05,
        "epoch": 0.36465583913379734,
        "step": 2829
    },
    {
        "loss": 1.8078,
        "grad_norm": 1.6219030618667603,
        "learning_rate": 8.652442419070153e-05,
        "epoch": 0.3647847383346223,
        "step": 2830
    },
    {
        "loss": 2.0575,
        "grad_norm": 1.3918042182922363,
        "learning_rate": 8.646414127690876e-05,
        "epoch": 0.36491363753544726,
        "step": 2831
    },
    {
        "loss": 0.8389,
        "grad_norm": 2.4720687866210938,
        "learning_rate": 8.64038633734785e-05,
        "epoch": 0.36504253673627224,
        "step": 2832
    },
    {
        "loss": 1.5086,
        "grad_norm": 1.8759948015213013,
        "learning_rate": 8.634359050272289e-05,
        "epoch": 0.36517143593709717,
        "step": 2833
    },
    {
        "loss": 2.2613,
        "grad_norm": 1.9037717580795288,
        "learning_rate": 8.628332268695216e-05,
        "epoch": 0.36530033513792215,
        "step": 2834
    },
    {
        "loss": 1.3337,
        "grad_norm": 2.550574779510498,
        "learning_rate": 8.622305994847476e-05,
        "epoch": 0.3654292343387471,
        "step": 2835
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.9510973691940308,
        "learning_rate": 8.616280230959729e-05,
        "epoch": 0.36555813353957206,
        "step": 2836
    },
    {
        "loss": 1.7357,
        "grad_norm": 1.7431739568710327,
        "learning_rate": 8.61025497926243e-05,
        "epoch": 0.365687032740397,
        "step": 2837
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.6596431732177734,
        "learning_rate": 8.604230241985867e-05,
        "epoch": 0.365815931941222,
        "step": 2838
    },
    {
        "loss": 1.2158,
        "grad_norm": 1.8545036315917969,
        "learning_rate": 8.598206021360112e-05,
        "epoch": 0.3659448311420469,
        "step": 2839
    },
    {
        "loss": 1.8681,
        "grad_norm": 1.29694402217865,
        "learning_rate": 8.592182319615066e-05,
        "epoch": 0.3660737303428719,
        "step": 2840
    },
    {
        "loss": 2.4277,
        "grad_norm": 1.3871029615402222,
        "learning_rate": 8.586159138980431e-05,
        "epoch": 0.3662026295436968,
        "step": 2841
    },
    {
        "loss": 2.2479,
        "grad_norm": 1.8407063484191895,
        "learning_rate": 8.580136481685712e-05,
        "epoch": 0.3663315287445218,
        "step": 2842
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.7569925785064697,
        "learning_rate": 8.574114349960229e-05,
        "epoch": 0.3664604279453467,
        "step": 2843
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.325242280960083,
        "learning_rate": 8.568092746033098e-05,
        "epoch": 0.3665893271461717,
        "step": 2844
    },
    {
        "loss": 2.7314,
        "grad_norm": 1.2146377563476562,
        "learning_rate": 8.562071672133251e-05,
        "epoch": 0.36671822634699663,
        "step": 2845
    },
    {
        "loss": 2.2775,
        "grad_norm": 1.751306176185608,
        "learning_rate": 8.55605113048941e-05,
        "epoch": 0.3668471255478216,
        "step": 2846
    },
    {
        "loss": 2.0611,
        "grad_norm": 2.1850244998931885,
        "learning_rate": 8.550031123330107e-05,
        "epoch": 0.36697602474864655,
        "step": 2847
    },
    {
        "loss": 2.2926,
        "grad_norm": 1.389212965965271,
        "learning_rate": 8.544011652883683e-05,
        "epoch": 0.36710492394947153,
        "step": 2848
    },
    {
        "loss": 1.7133,
        "grad_norm": 1.6925163269042969,
        "learning_rate": 8.537992721378272e-05,
        "epoch": 0.36723382315029646,
        "step": 2849
    },
    {
        "loss": 1.9574,
        "grad_norm": 1.2017199993133545,
        "learning_rate": 8.531974331041803e-05,
        "epoch": 0.36736272235112144,
        "step": 2850
    },
    {
        "loss": 1.6951,
        "grad_norm": 1.959676742553711,
        "learning_rate": 8.525956484102022e-05,
        "epoch": 0.36749162155194637,
        "step": 2851
    },
    {
        "loss": 2.2621,
        "grad_norm": 1.7283824682235718,
        "learning_rate": 8.519939182786454e-05,
        "epoch": 0.36762052075277135,
        "step": 2852
    },
    {
        "loss": 2.3438,
        "grad_norm": 3.0639708042144775,
        "learning_rate": 8.51392242932244e-05,
        "epoch": 0.3677494199535963,
        "step": 2853
    },
    {
        "loss": 2.0864,
        "grad_norm": 2.0427212715148926,
        "learning_rate": 8.507906225937103e-05,
        "epoch": 0.36787831915442126,
        "step": 2854
    },
    {
        "loss": 1.4112,
        "grad_norm": 2.0936005115509033,
        "learning_rate": 8.501890574857375e-05,
        "epoch": 0.3680072183552462,
        "step": 2855
    },
    {
        "loss": 2.1365,
        "grad_norm": 1.6133793592453003,
        "learning_rate": 8.495875478309977e-05,
        "epoch": 0.3681361175560712,
        "step": 2856
    },
    {
        "loss": 2.0049,
        "grad_norm": 1.9992408752441406,
        "learning_rate": 8.489860938521419e-05,
        "epoch": 0.3682650167568961,
        "step": 2857
    },
    {
        "loss": 1.2967,
        "grad_norm": 1.9321238994598389,
        "learning_rate": 8.483846957718022e-05,
        "epoch": 0.3683939159577211,
        "step": 2858
    },
    {
        "loss": 1.2694,
        "grad_norm": 2.472874402999878,
        "learning_rate": 8.477833538125878e-05,
        "epoch": 0.368522815158546,
        "step": 2859
    },
    {
        "loss": 1.59,
        "grad_norm": 2.328305244445801,
        "learning_rate": 8.47182068197089e-05,
        "epoch": 0.368651714359371,
        "step": 2860
    },
    {
        "loss": 1.4274,
        "grad_norm": 2.3674540519714355,
        "learning_rate": 8.465808391478747e-05,
        "epoch": 0.3687806135601959,
        "step": 2861
    },
    {
        "loss": 2.163,
        "grad_norm": 1.352615475654602,
        "learning_rate": 8.459796668874925e-05,
        "epoch": 0.3689095127610209,
        "step": 2862
    },
    {
        "loss": 1.9967,
        "grad_norm": 1.538925290107727,
        "learning_rate": 8.45378551638469e-05,
        "epoch": 0.36903841196184584,
        "step": 2863
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.7124358415603638,
        "learning_rate": 8.447774936233095e-05,
        "epoch": 0.36916731116267076,
        "step": 2864
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.4730273485183716,
        "learning_rate": 8.441764930644992e-05,
        "epoch": 0.36929621036349575,
        "step": 2865
    },
    {
        "loss": 1.3394,
        "grad_norm": 2.2381272315979004,
        "learning_rate": 8.435755501845014e-05,
        "epoch": 0.3694251095643207,
        "step": 2866
    },
    {
        "loss": 2.134,
        "grad_norm": 1.4673206806182861,
        "learning_rate": 8.42974665205757e-05,
        "epoch": 0.36955400876514566,
        "step": 2867
    },
    {
        "loss": 2.4137,
        "grad_norm": 1.2442253828048706,
        "learning_rate": 8.423738383506876e-05,
        "epoch": 0.3696829079659706,
        "step": 2868
    },
    {
        "loss": 1.4799,
        "grad_norm": 2.129817008972168,
        "learning_rate": 8.417730698416913e-05,
        "epoch": 0.36981180716679557,
        "step": 2869
    },
    {
        "loss": 2.0867,
        "grad_norm": 1.6915339231491089,
        "learning_rate": 8.411723599011456e-05,
        "epoch": 0.3699407063676205,
        "step": 2870
    },
    {
        "loss": 2.1475,
        "grad_norm": 1.3690208196640015,
        "learning_rate": 8.405717087514063e-05,
        "epoch": 0.3700696055684455,
        "step": 2871
    },
    {
        "loss": 1.3195,
        "grad_norm": 2.3969321250915527,
        "learning_rate": 8.399711166148074e-05,
        "epoch": 0.3701985047692704,
        "step": 2872
    },
    {
        "loss": 2.4209,
        "grad_norm": 1.6618045568466187,
        "learning_rate": 8.393705837136607e-05,
        "epoch": 0.3703274039700954,
        "step": 2873
    },
    {
        "loss": 2.3534,
        "grad_norm": 2.3140017986297607,
        "learning_rate": 8.38770110270256e-05,
        "epoch": 0.3704563031709203,
        "step": 2874
    },
    {
        "loss": 0.7742,
        "grad_norm": 2.534698963165283,
        "learning_rate": 8.381696965068623e-05,
        "epoch": 0.3705852023717453,
        "step": 2875
    },
    {
        "loss": 1.7534,
        "grad_norm": 2.035856008529663,
        "learning_rate": 8.375693426457245e-05,
        "epoch": 0.37071410157257023,
        "step": 2876
    },
    {
        "loss": 1.7328,
        "grad_norm": 1.3942699432373047,
        "learning_rate": 8.36969048909067e-05,
        "epoch": 0.3708430007733952,
        "step": 2877
    },
    {
        "loss": 2.5457,
        "grad_norm": 1.7261570692062378,
        "learning_rate": 8.363688155190919e-05,
        "epoch": 0.37097189997422014,
        "step": 2878
    },
    {
        "loss": 1.9178,
        "grad_norm": 1.5843284130096436,
        "learning_rate": 8.357686426979777e-05,
        "epoch": 0.3711007991750451,
        "step": 2879
    },
    {
        "loss": 1.7648,
        "grad_norm": 1.5429761409759521,
        "learning_rate": 8.35168530667881e-05,
        "epoch": 0.37122969837587005,
        "step": 2880
    },
    {
        "loss": 1.4851,
        "grad_norm": 1.5303210020065308,
        "learning_rate": 8.345684796509367e-05,
        "epoch": 0.37135859757669504,
        "step": 2881
    },
    {
        "loss": 2.5068,
        "grad_norm": 1.348190188407898,
        "learning_rate": 8.33968489869257e-05,
        "epoch": 0.37148749677751997,
        "step": 2882
    },
    {
        "loss": 1.8056,
        "grad_norm": 1.7972830533981323,
        "learning_rate": 8.333685615449302e-05,
        "epoch": 0.37161639597834495,
        "step": 2883
    },
    {
        "loss": 0.9846,
        "grad_norm": 2.619385004043579,
        "learning_rate": 8.327686949000224e-05,
        "epoch": 0.3717452951791699,
        "step": 2884
    },
    {
        "loss": 1.4932,
        "grad_norm": 1.5397487878799438,
        "learning_rate": 8.32168890156578e-05,
        "epoch": 0.37187419437999486,
        "step": 2885
    },
    {
        "loss": 2.3877,
        "grad_norm": 1.5572253465652466,
        "learning_rate": 8.315691475366165e-05,
        "epoch": 0.3720030935808198,
        "step": 2886
    },
    {
        "loss": 1.6663,
        "grad_norm": 1.955996036529541,
        "learning_rate": 8.309694672621364e-05,
        "epoch": 0.37213199278164477,
        "step": 2887
    },
    {
        "loss": 1.75,
        "grad_norm": 1.1672736406326294,
        "learning_rate": 8.303698495551115e-05,
        "epoch": 0.3722608919824697,
        "step": 2888
    },
    {
        "loss": 2.0336,
        "grad_norm": 1.6586112976074219,
        "learning_rate": 8.297702946374941e-05,
        "epoch": 0.3723897911832947,
        "step": 2889
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.5536882877349854,
        "learning_rate": 8.291708027312112e-05,
        "epoch": 0.3725186903841196,
        "step": 2890
    },
    {
        "loss": 1.8883,
        "grad_norm": 1.671506643295288,
        "learning_rate": 8.285713740581685e-05,
        "epoch": 0.3726475895849446,
        "step": 2891
    },
    {
        "loss": 1.9764,
        "grad_norm": 2.7578225135803223,
        "learning_rate": 8.279720088402469e-05,
        "epoch": 0.3727764887857695,
        "step": 2892
    },
    {
        "loss": 1.6832,
        "grad_norm": 1.7856113910675049,
        "learning_rate": 8.273727072993041e-05,
        "epoch": 0.3729053879865945,
        "step": 2893
    },
    {
        "loss": 1.5575,
        "grad_norm": 2.2465381622314453,
        "learning_rate": 8.267734696571745e-05,
        "epoch": 0.37303428718741943,
        "step": 2894
    },
    {
        "loss": 2.4993,
        "grad_norm": 1.3081412315368652,
        "learning_rate": 8.261742961356691e-05,
        "epoch": 0.3731631863882444,
        "step": 2895
    },
    {
        "loss": 1.7837,
        "grad_norm": 1.3324687480926514,
        "learning_rate": 8.255751869565751e-05,
        "epoch": 0.37329208558906934,
        "step": 2896
    },
    {
        "loss": 1.0836,
        "grad_norm": 2.080980062484741,
        "learning_rate": 8.249761423416545e-05,
        "epoch": 0.37342098478989433,
        "step": 2897
    },
    {
        "loss": 2.0307,
        "grad_norm": 1.897237777709961,
        "learning_rate": 8.243771625126473e-05,
        "epoch": 0.37354988399071926,
        "step": 2898
    },
    {
        "loss": 2.3836,
        "grad_norm": 1.068184733390808,
        "learning_rate": 8.23778247691269e-05,
        "epoch": 0.37367878319154424,
        "step": 2899
    },
    {
        "loss": 2.1232,
        "grad_norm": 1.6556429862976074,
        "learning_rate": 8.2317939809921e-05,
        "epoch": 0.37380768239236917,
        "step": 2900
    },
    {
        "loss": 2.2126,
        "grad_norm": 1.4650965929031372,
        "learning_rate": 8.225806139581382e-05,
        "epoch": 0.3739365815931941,
        "step": 2901
    },
    {
        "loss": 0.4318,
        "grad_norm": 1.7317529916763306,
        "learning_rate": 8.219818954896955e-05,
        "epoch": 0.3740654807940191,
        "step": 2902
    },
    {
        "loss": 2.4993,
        "grad_norm": 3.065934181213379,
        "learning_rate": 8.213832429155007e-05,
        "epoch": 0.374194379994844,
        "step": 2903
    },
    {
        "loss": 1.9454,
        "grad_norm": 1.685287594795227,
        "learning_rate": 8.207846564571483e-05,
        "epoch": 0.374323279195669,
        "step": 2904
    },
    {
        "loss": 1.6826,
        "grad_norm": 1.918642282485962,
        "learning_rate": 8.201861363362072e-05,
        "epoch": 0.3744521783964939,
        "step": 2905
    },
    {
        "loss": 2.1619,
        "grad_norm": 1.658982515335083,
        "learning_rate": 8.195876827742234e-05,
        "epoch": 0.3745810775973189,
        "step": 2906
    },
    {
        "loss": 2.2592,
        "grad_norm": 1.6926671266555786,
        "learning_rate": 8.189892959927163e-05,
        "epoch": 0.37470997679814383,
        "step": 2907
    },
    {
        "loss": 2.5795,
        "grad_norm": 1.6477783918380737,
        "learning_rate": 8.183909762131827e-05,
        "epoch": 0.3748388759989688,
        "step": 2908
    },
    {
        "loss": 1.5771,
        "grad_norm": 0.885058581829071,
        "learning_rate": 8.177927236570925e-05,
        "epoch": 0.37496777519979374,
        "step": 2909
    },
    {
        "loss": 1.9767,
        "grad_norm": 1.2222203016281128,
        "learning_rate": 8.17194538545892e-05,
        "epoch": 0.3750966744006187,
        "step": 2910
    },
    {
        "loss": 2.0844,
        "grad_norm": 3.2159922122955322,
        "learning_rate": 8.165964211010029e-05,
        "epoch": 0.37522557360144365,
        "step": 2911
    },
    {
        "loss": 1.4929,
        "grad_norm": 2.092160940170288,
        "learning_rate": 8.159983715438206e-05,
        "epoch": 0.37535447280226863,
        "step": 2912
    },
    {
        "loss": 2.005,
        "grad_norm": 1.7965176105499268,
        "learning_rate": 8.154003900957158e-05,
        "epoch": 0.37548337200309356,
        "step": 2913
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.6757961511611938,
        "learning_rate": 8.148024769780351e-05,
        "epoch": 0.37561227120391855,
        "step": 2914
    },
    {
        "loss": 1.4546,
        "grad_norm": 2.1363279819488525,
        "learning_rate": 8.142046324120979e-05,
        "epoch": 0.3757411704047435,
        "step": 2915
    },
    {
        "loss": 0.8425,
        "grad_norm": 2.021329641342163,
        "learning_rate": 8.136068566191999e-05,
        "epoch": 0.37587006960556846,
        "step": 2916
    },
    {
        "loss": 1.9977,
        "grad_norm": 2.31233549118042,
        "learning_rate": 8.1300914982061e-05,
        "epoch": 0.3759989688063934,
        "step": 2917
    },
    {
        "loss": 2.0697,
        "grad_norm": 1.8780245780944824,
        "learning_rate": 8.124115122375729e-05,
        "epoch": 0.37612786800721837,
        "step": 2918
    },
    {
        "loss": 1.8094,
        "grad_norm": 1.9254240989685059,
        "learning_rate": 8.118139440913066e-05,
        "epoch": 0.3762567672080433,
        "step": 2919
    },
    {
        "loss": 1.9632,
        "grad_norm": 1.7456904649734497,
        "learning_rate": 8.112164456030035e-05,
        "epoch": 0.3763856664088683,
        "step": 2920
    },
    {
        "loss": 1.6471,
        "grad_norm": 2.0028316974639893,
        "learning_rate": 8.106190169938313e-05,
        "epoch": 0.3765145656096932,
        "step": 2921
    },
    {
        "loss": 2.0063,
        "grad_norm": 1.4562714099884033,
        "learning_rate": 8.100216584849303e-05,
        "epoch": 0.3766434648105182,
        "step": 2922
    },
    {
        "loss": 2.2903,
        "grad_norm": 1.8825339078903198,
        "learning_rate": 8.094243702974158e-05,
        "epoch": 0.3767723640113431,
        "step": 2923
    },
    {
        "loss": 1.463,
        "grad_norm": 2.580425500869751,
        "learning_rate": 8.088271526523775e-05,
        "epoch": 0.3769012632121681,
        "step": 2924
    },
    {
        "loss": 2.1755,
        "grad_norm": 1.1119011640548706,
        "learning_rate": 8.082300057708777e-05,
        "epoch": 0.37703016241299303,
        "step": 2925
    },
    {
        "loss": 1.7718,
        "grad_norm": 2.0830812454223633,
        "learning_rate": 8.076329298739532e-05,
        "epoch": 0.377159061613818,
        "step": 2926
    },
    {
        "loss": 2.7033,
        "grad_norm": 1.8541982173919678,
        "learning_rate": 8.070359251826139e-05,
        "epoch": 0.37728796081464294,
        "step": 2927
    },
    {
        "loss": 2.0413,
        "grad_norm": 1.5021404027938843,
        "learning_rate": 8.064389919178451e-05,
        "epoch": 0.3774168600154679,
        "step": 2928
    },
    {
        "loss": 2.1895,
        "grad_norm": 1.2418336868286133,
        "learning_rate": 8.058421303006043e-05,
        "epoch": 0.37754575921629285,
        "step": 2929
    },
    {
        "loss": 2.2761,
        "grad_norm": 1.2484294176101685,
        "learning_rate": 8.052453405518216e-05,
        "epoch": 0.37767465841711784,
        "step": 2930
    },
    {
        "loss": 2.0626,
        "grad_norm": 1.1687363386154175,
        "learning_rate": 8.04648622892403e-05,
        "epoch": 0.37780355761794276,
        "step": 2931
    },
    {
        "loss": 2.4225,
        "grad_norm": 1.5366121530532837,
        "learning_rate": 8.040519775432252e-05,
        "epoch": 0.37793245681876775,
        "step": 2932
    },
    {
        "loss": 1.8755,
        "grad_norm": 1.5847828388214111,
        "learning_rate": 8.034554047251402e-05,
        "epoch": 0.3780613560195927,
        "step": 2933
    },
    {
        "loss": 1.8727,
        "grad_norm": 2.520446300506592,
        "learning_rate": 8.028589046589714e-05,
        "epoch": 0.37819025522041766,
        "step": 2934
    },
    {
        "loss": 1.4785,
        "grad_norm": 1.637675404548645,
        "learning_rate": 8.022624775655168e-05,
        "epoch": 0.3783191544212426,
        "step": 2935
    },
    {
        "loss": 1.4736,
        "grad_norm": 1.7150912284851074,
        "learning_rate": 8.016661236655469e-05,
        "epoch": 0.37844805362206757,
        "step": 2936
    },
    {
        "loss": 1.3708,
        "grad_norm": 2.501823902130127,
        "learning_rate": 8.010698431798038e-05,
        "epoch": 0.3785769528228925,
        "step": 2937
    },
    {
        "loss": 1.7934,
        "grad_norm": 1.5717412233352661,
        "learning_rate": 8.00473636329005e-05,
        "epoch": 0.3787058520237174,
        "step": 2938
    },
    {
        "loss": 2.2838,
        "grad_norm": 1.2701319456100464,
        "learning_rate": 7.998775033338381e-05,
        "epoch": 0.3788347512245424,
        "step": 2939
    },
    {
        "loss": 2.1815,
        "grad_norm": 1.1799019575119019,
        "learning_rate": 7.99281444414965e-05,
        "epoch": 0.37896365042536734,
        "step": 2940
    },
    {
        "loss": 1.9315,
        "grad_norm": 2.448227643966675,
        "learning_rate": 7.986854597930203e-05,
        "epoch": 0.3790925496261923,
        "step": 2941
    },
    {
        "loss": 1.5386,
        "grad_norm": 2.505941390991211,
        "learning_rate": 7.980895496886098e-05,
        "epoch": 0.37922144882701725,
        "step": 2942
    },
    {
        "loss": 1.7346,
        "grad_norm": 2.177328586578369,
        "learning_rate": 7.974937143223126e-05,
        "epoch": 0.37935034802784223,
        "step": 2943
    },
    {
        "loss": 1.1815,
        "grad_norm": 2.4484338760375977,
        "learning_rate": 7.968979539146801e-05,
        "epoch": 0.37947924722866716,
        "step": 2944
    },
    {
        "loss": 1.2161,
        "grad_norm": 2.2509536743164062,
        "learning_rate": 7.963022686862361e-05,
        "epoch": 0.37960814642949214,
        "step": 2945
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.566348910331726,
        "learning_rate": 7.957066588574762e-05,
        "epoch": 0.37973704563031707,
        "step": 2946
    },
    {
        "loss": 2.1868,
        "grad_norm": 1.8449420928955078,
        "learning_rate": 7.951111246488676e-05,
        "epoch": 0.37986594483114206,
        "step": 2947
    },
    {
        "loss": 1.8827,
        "grad_norm": 2.349168062210083,
        "learning_rate": 7.945156662808512e-05,
        "epoch": 0.379994844031967,
        "step": 2948
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.6551837921142578,
        "learning_rate": 7.939202839738376e-05,
        "epoch": 0.38012374323279197,
        "step": 2949
    },
    {
        "loss": 1.8126,
        "grad_norm": 2.000802993774414,
        "learning_rate": 7.933249779482112e-05,
        "epoch": 0.3802526424336169,
        "step": 2950
    },
    {
        "loss": 1.4464,
        "grad_norm": 2.323476791381836,
        "learning_rate": 7.92729748424327e-05,
        "epoch": 0.3803815416344419,
        "step": 2951
    },
    {
        "loss": 1.7968,
        "grad_norm": 2.070624589920044,
        "learning_rate": 7.921345956225124e-05,
        "epoch": 0.3805104408352668,
        "step": 2952
    },
    {
        "loss": 2.2366,
        "grad_norm": 1.638756275177002,
        "learning_rate": 7.915395197630653e-05,
        "epoch": 0.3806393400360918,
        "step": 2953
    },
    {
        "loss": 2.0628,
        "grad_norm": 2.3360817432403564,
        "learning_rate": 7.90944521066257e-05,
        "epoch": 0.3807682392369167,
        "step": 2954
    },
    {
        "loss": 2.4522,
        "grad_norm": 2.031796932220459,
        "learning_rate": 7.903495997523281e-05,
        "epoch": 0.3808971384377417,
        "step": 2955
    },
    {
        "loss": 1.9015,
        "grad_norm": 1.7878780364990234,
        "learning_rate": 7.897547560414917e-05,
        "epoch": 0.38102603763856663,
        "step": 2956
    },
    {
        "loss": 1.2794,
        "grad_norm": 1.573609471321106,
        "learning_rate": 7.891599901539322e-05,
        "epoch": 0.3811549368393916,
        "step": 2957
    },
    {
        "loss": 1.1931,
        "grad_norm": 1.9254709482192993,
        "learning_rate": 7.885653023098055e-05,
        "epoch": 0.38128383604021654,
        "step": 2958
    },
    {
        "loss": 1.2261,
        "grad_norm": 1.8767199516296387,
        "learning_rate": 7.879706927292374e-05,
        "epoch": 0.3814127352410415,
        "step": 2959
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.1093437671661377,
        "learning_rate": 7.873761616323257e-05,
        "epoch": 0.38154163444186645,
        "step": 2960
    },
    {
        "loss": 2.1283,
        "grad_norm": 2.191196918487549,
        "learning_rate": 7.867817092391388e-05,
        "epoch": 0.38167053364269143,
        "step": 2961
    },
    {
        "loss": 1.7157,
        "grad_norm": 1.8948636054992676,
        "learning_rate": 7.861873357697165e-05,
        "epoch": 0.38179943284351636,
        "step": 2962
    },
    {
        "loss": 2.1565,
        "grad_norm": 1.1923586130142212,
        "learning_rate": 7.855930414440685e-05,
        "epoch": 0.38192833204434135,
        "step": 2963
    },
    {
        "loss": 1.7663,
        "grad_norm": 2.4634320735931396,
        "learning_rate": 7.849988264821763e-05,
        "epoch": 0.3820572312451663,
        "step": 2964
    },
    {
        "loss": 2.0208,
        "grad_norm": 1.3779182434082031,
        "learning_rate": 7.844046911039909e-05,
        "epoch": 0.38218613044599126,
        "step": 2965
    },
    {
        "loss": 2.4774,
        "grad_norm": 1.6100598573684692,
        "learning_rate": 7.838106355294339e-05,
        "epoch": 0.3823150296468162,
        "step": 2966
    },
    {
        "loss": 1.691,
        "grad_norm": 2.8645732402801514,
        "learning_rate": 7.832166599783986e-05,
        "epoch": 0.38244392884764117,
        "step": 2967
    },
    {
        "loss": 1.7769,
        "grad_norm": 2.3160767555236816,
        "learning_rate": 7.826227646707474e-05,
        "epoch": 0.3825728280484661,
        "step": 2968
    },
    {
        "loss": 2.0529,
        "grad_norm": 1.6080946922302246,
        "learning_rate": 7.820289498263139e-05,
        "epoch": 0.3827017272492911,
        "step": 2969
    },
    {
        "loss": 2.2446,
        "grad_norm": 1.8300466537475586,
        "learning_rate": 7.81435215664901e-05,
        "epoch": 0.382830626450116,
        "step": 2970
    },
    {
        "loss": 1.2,
        "grad_norm": 2.019571542739868,
        "learning_rate": 7.808415624062828e-05,
        "epoch": 0.382959525650941,
        "step": 2971
    },
    {
        "loss": 2.1096,
        "grad_norm": 1.862653374671936,
        "learning_rate": 7.802479902702026e-05,
        "epoch": 0.3830884248517659,
        "step": 2972
    },
    {
        "loss": 1.6508,
        "grad_norm": 1.7366119623184204,
        "learning_rate": 7.79654499476373e-05,
        "epoch": 0.38321732405259085,
        "step": 2973
    },
    {
        "loss": 2.2522,
        "grad_norm": 2.0164124965667725,
        "learning_rate": 7.79061090244479e-05,
        "epoch": 0.38334622325341583,
        "step": 2974
    },
    {
        "loss": 1.6431,
        "grad_norm": 2.2198188304901123,
        "learning_rate": 7.78467762794173e-05,
        "epoch": 0.38347512245424076,
        "step": 2975
    },
    {
        "loss": 1.9277,
        "grad_norm": 1.1655670404434204,
        "learning_rate": 7.778745173450782e-05,
        "epoch": 0.38360402165506574,
        "step": 2976
    },
    {
        "loss": 1.2131,
        "grad_norm": 2.3603060245513916,
        "learning_rate": 7.772813541167874e-05,
        "epoch": 0.38373292085589067,
        "step": 2977
    },
    {
        "loss": 1.557,
        "grad_norm": 2.333510398864746,
        "learning_rate": 7.766882733288622e-05,
        "epoch": 0.38386182005671565,
        "step": 2978
    },
    {
        "loss": 2.5557,
        "grad_norm": 1.6457997560501099,
        "learning_rate": 7.76095275200835e-05,
        "epoch": 0.3839907192575406,
        "step": 2979
    },
    {
        "loss": 1.0263,
        "grad_norm": 2.1215672492980957,
        "learning_rate": 7.755023599522063e-05,
        "epoch": 0.38411961845836556,
        "step": 2980
    },
    {
        "loss": 1.981,
        "grad_norm": 1.575080394744873,
        "learning_rate": 7.749095278024468e-05,
        "epoch": 0.3842485176591905,
        "step": 2981
    },
    {
        "loss": 1.747,
        "grad_norm": 1.6139191389083862,
        "learning_rate": 7.74316778970996e-05,
        "epoch": 0.3843774168600155,
        "step": 2982
    },
    {
        "loss": 2.0317,
        "grad_norm": 2.3980021476745605,
        "learning_rate": 7.737241136772626e-05,
        "epoch": 0.3845063160608404,
        "step": 2983
    },
    {
        "loss": 1.7144,
        "grad_norm": 2.761049270629883,
        "learning_rate": 7.73131532140625e-05,
        "epoch": 0.3846352152616654,
        "step": 2984
    },
    {
        "loss": 2.1908,
        "grad_norm": 1.2813847064971924,
        "learning_rate": 7.725390345804293e-05,
        "epoch": 0.3847641144624903,
        "step": 2985
    },
    {
        "loss": 2.3099,
        "grad_norm": 1.1516119241714478,
        "learning_rate": 7.719466212159916e-05,
        "epoch": 0.3848930136633153,
        "step": 2986
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.557666301727295,
        "learning_rate": 7.713542922665972e-05,
        "epoch": 0.3850219128641402,
        "step": 2987
    },
    {
        "loss": 2.2107,
        "grad_norm": 2.153089761734009,
        "learning_rate": 7.707620479514988e-05,
        "epoch": 0.3851508120649652,
        "step": 2988
    },
    {
        "loss": 1.8809,
        "grad_norm": 1.3378028869628906,
        "learning_rate": 7.701698884899183e-05,
        "epoch": 0.38527971126579014,
        "step": 2989
    },
    {
        "loss": 1.5278,
        "grad_norm": 2.4953975677490234,
        "learning_rate": 7.695778141010462e-05,
        "epoch": 0.3854086104666151,
        "step": 2990
    },
    {
        "loss": 2.5698,
        "grad_norm": 1.6862587928771973,
        "learning_rate": 7.689858250040427e-05,
        "epoch": 0.38553750966744005,
        "step": 2991
    },
    {
        "loss": 1.8743,
        "grad_norm": 1.484138011932373,
        "learning_rate": 7.683939214180345e-05,
        "epoch": 0.38566640886826503,
        "step": 2992
    },
    {
        "loss": 1.7787,
        "grad_norm": 2.351655960083008,
        "learning_rate": 7.678021035621175e-05,
        "epoch": 0.38579530806908996,
        "step": 2993
    },
    {
        "loss": 1.6955,
        "grad_norm": 2.850515604019165,
        "learning_rate": 7.672103716553566e-05,
        "epoch": 0.38592420726991494,
        "step": 2994
    },
    {
        "loss": 1.953,
        "grad_norm": 1.7851918935775757,
        "learning_rate": 7.666187259167833e-05,
        "epoch": 0.38605310647073987,
        "step": 2995
    },
    {
        "loss": 2.2975,
        "grad_norm": 1.792371153831482,
        "learning_rate": 7.660271665653988e-05,
        "epoch": 0.38618200567156485,
        "step": 2996
    },
    {
        "loss": 1.5125,
        "grad_norm": 2.705620527267456,
        "learning_rate": 7.65435693820171e-05,
        "epoch": 0.3863109048723898,
        "step": 2997
    },
    {
        "loss": 1.8415,
        "grad_norm": 2.182556390762329,
        "learning_rate": 7.648443079000371e-05,
        "epoch": 0.38643980407321477,
        "step": 2998
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.6874337196350098,
        "learning_rate": 7.642530090239007e-05,
        "epoch": 0.3865687032740397,
        "step": 2999
    },
    {
        "loss": 2.4691,
        "grad_norm": 2.0017876625061035,
        "learning_rate": 7.636617974106339e-05,
        "epoch": 0.3866976024748647,
        "step": 3000
    },
    {
        "loss": 2.3724,
        "grad_norm": 1.377447247505188,
        "learning_rate": 7.630706732790773e-05,
        "epoch": 0.3868265016756896,
        "step": 3001
    },
    {
        "loss": 2.1176,
        "grad_norm": 1.4207756519317627,
        "learning_rate": 7.624796368480373e-05,
        "epoch": 0.3869554008765146,
        "step": 3002
    },
    {
        "loss": 2.2209,
        "grad_norm": 1.7727636098861694,
        "learning_rate": 7.618886883362893e-05,
        "epoch": 0.3870843000773395,
        "step": 3003
    },
    {
        "loss": 2.3926,
        "grad_norm": 1.308163046836853,
        "learning_rate": 7.61297827962576e-05,
        "epoch": 0.3872131992781645,
        "step": 3004
    },
    {
        "loss": 1.6852,
        "grad_norm": 2.152867078781128,
        "learning_rate": 7.607070559456069e-05,
        "epoch": 0.3873420984789894,
        "step": 3005
    },
    {
        "loss": 1.8997,
        "grad_norm": 2.6291749477386475,
        "learning_rate": 7.601163725040587e-05,
        "epoch": 0.3874709976798144,
        "step": 3006
    },
    {
        "loss": 2.0535,
        "grad_norm": 1.635648488998413,
        "learning_rate": 7.595257778565762e-05,
        "epoch": 0.38759989688063934,
        "step": 3007
    },
    {
        "loss": 1.073,
        "grad_norm": 1.7289096117019653,
        "learning_rate": 7.589352722217706e-05,
        "epoch": 0.3877287960814643,
        "step": 3008
    },
    {
        "loss": 2.128,
        "grad_norm": 2.550245523452759,
        "learning_rate": 7.583448558182209e-05,
        "epoch": 0.38785769528228925,
        "step": 3009
    },
    {
        "loss": 2.4356,
        "grad_norm": 1.7363826036453247,
        "learning_rate": 7.577545288644713e-05,
        "epoch": 0.3879865944831142,
        "step": 3010
    },
    {
        "loss": 1.6049,
        "grad_norm": 2.053071975708008,
        "learning_rate": 7.571642915790357e-05,
        "epoch": 0.38811549368393916,
        "step": 3011
    },
    {
        "loss": 1.6104,
        "grad_norm": 2.056060314178467,
        "learning_rate": 7.565741441803918e-05,
        "epoch": 0.3882443928847641,
        "step": 3012
    },
    {
        "loss": 1.2056,
        "grad_norm": 2.594219923019409,
        "learning_rate": 7.559840868869865e-05,
        "epoch": 0.3883732920855891,
        "step": 3013
    },
    {
        "loss": 1.0195,
        "grad_norm": 2.9086220264434814,
        "learning_rate": 7.553941199172316e-05,
        "epoch": 0.388502191286414,
        "step": 3014
    },
    {
        "loss": 2.0741,
        "grad_norm": 1.151134729385376,
        "learning_rate": 7.548042434895066e-05,
        "epoch": 0.388631090487239,
        "step": 3015
    },
    {
        "loss": 1.9343,
        "grad_norm": 1.9513238668441772,
        "learning_rate": 7.542144578221569e-05,
        "epoch": 0.3887599896880639,
        "step": 3016
    },
    {
        "loss": 2.3654,
        "grad_norm": 1.5605443716049194,
        "learning_rate": 7.536247631334947e-05,
        "epoch": 0.3888888888888889,
        "step": 3017
    },
    {
        "loss": 1.6874,
        "grad_norm": 1.7504993677139282,
        "learning_rate": 7.530351596417982e-05,
        "epoch": 0.3890177880897138,
        "step": 3018
    },
    {
        "loss": 2.5341,
        "grad_norm": 1.464332938194275,
        "learning_rate": 7.524456475653117e-05,
        "epoch": 0.3891466872905388,
        "step": 3019
    },
    {
        "loss": 1.7642,
        "grad_norm": 1.9613779783248901,
        "learning_rate": 7.518562271222458e-05,
        "epoch": 0.38927558649136373,
        "step": 3020
    },
    {
        "loss": 2.2632,
        "grad_norm": 1.4517205953598022,
        "learning_rate": 7.512668985307779e-05,
        "epoch": 0.3894044856921887,
        "step": 3021
    },
    {
        "loss": 1.7554,
        "grad_norm": 2.1035842895507812,
        "learning_rate": 7.506776620090503e-05,
        "epoch": 0.38953338489301365,
        "step": 3022
    },
    {
        "loss": 0.9542,
        "grad_norm": 2.903240442276001,
        "learning_rate": 7.500885177751717e-05,
        "epoch": 0.38966228409383863,
        "step": 3023
    },
    {
        "loss": 1.3938,
        "grad_norm": 2.629149913787842,
        "learning_rate": 7.49499466047217e-05,
        "epoch": 0.38979118329466356,
        "step": 3024
    },
    {
        "loss": 1.9101,
        "grad_norm": 2.094182252883911,
        "learning_rate": 7.48910507043226e-05,
        "epoch": 0.38992008249548854,
        "step": 3025
    },
    {
        "loss": 0.9158,
        "grad_norm": 2.5783915519714355,
        "learning_rate": 7.483216409812049e-05,
        "epoch": 0.39004898169631347,
        "step": 3026
    },
    {
        "loss": 1.7242,
        "grad_norm": 1.9776133298873901,
        "learning_rate": 7.477328680791255e-05,
        "epoch": 0.39017788089713845,
        "step": 3027
    },
    {
        "loss": 1.987,
        "grad_norm": 2.109637975692749,
        "learning_rate": 7.47144188554925e-05,
        "epoch": 0.3903067800979634,
        "step": 3028
    },
    {
        "loss": 2.0844,
        "grad_norm": 1.6502918004989624,
        "learning_rate": 7.46555602626505e-05,
        "epoch": 0.39043567929878836,
        "step": 3029
    },
    {
        "loss": 1.8762,
        "grad_norm": 1.8185420036315918,
        "learning_rate": 7.459671105117344e-05,
        "epoch": 0.3905645784996133,
        "step": 3030
    },
    {
        "loss": 1.0934,
        "grad_norm": 2.4648945331573486,
        "learning_rate": 7.453787124284459e-05,
        "epoch": 0.3906934777004383,
        "step": 3031
    },
    {
        "loss": 2.3412,
        "grad_norm": 1.391191840171814,
        "learning_rate": 7.447904085944379e-05,
        "epoch": 0.3908223769012632,
        "step": 3032
    },
    {
        "loss": 2.1646,
        "grad_norm": 1.099012017250061,
        "learning_rate": 7.442021992274735e-05,
        "epoch": 0.3909512761020882,
        "step": 3033
    },
    {
        "loss": 1.7058,
        "grad_norm": 2.2372193336486816,
        "learning_rate": 7.436140845452822e-05,
        "epoch": 0.3910801753029131,
        "step": 3034
    },
    {
        "loss": 1.7882,
        "grad_norm": 1.3576160669326782,
        "learning_rate": 7.430260647655567e-05,
        "epoch": 0.3912090745037381,
        "step": 3035
    },
    {
        "loss": 2.1276,
        "grad_norm": 1.4151270389556885,
        "learning_rate": 7.424381401059546e-05,
        "epoch": 0.391337973704563,
        "step": 3036
    },
    {
        "loss": 2.084,
        "grad_norm": 2.0864291191101074,
        "learning_rate": 7.418503107841006e-05,
        "epoch": 0.391466872905388,
        "step": 3037
    },
    {
        "loss": 2.4947,
        "grad_norm": 1.7657287120819092,
        "learning_rate": 7.412625770175818e-05,
        "epoch": 0.39159577210621294,
        "step": 3038
    },
    {
        "loss": 2.6595,
        "grad_norm": 1.521458387374878,
        "learning_rate": 7.406749390239502e-05,
        "epoch": 0.3917246713070379,
        "step": 3039
    },
    {
        "loss": 2.1455,
        "grad_norm": 2.11277437210083,
        "learning_rate": 7.400873970207234e-05,
        "epoch": 0.39185357050786285,
        "step": 3040
    },
    {
        "loss": 1.7125,
        "grad_norm": 2.456592321395874,
        "learning_rate": 7.394999512253824e-05,
        "epoch": 0.39198246970868783,
        "step": 3041
    },
    {
        "loss": 1.7597,
        "grad_norm": 2.253918409347534,
        "learning_rate": 7.389126018553738e-05,
        "epoch": 0.39211136890951276,
        "step": 3042
    },
    {
        "loss": 2.531,
        "grad_norm": 1.4481127262115479,
        "learning_rate": 7.38325349128107e-05,
        "epoch": 0.39224026811033774,
        "step": 3043
    },
    {
        "loss": 1.621,
        "grad_norm": 1.350784182548523,
        "learning_rate": 7.377381932609568e-05,
        "epoch": 0.39236916731116267,
        "step": 3044
    },
    {
        "loss": 2.017,
        "grad_norm": 2.1333818435668945,
        "learning_rate": 7.371511344712616e-05,
        "epoch": 0.39249806651198765,
        "step": 3045
    },
    {
        "loss": 2.3798,
        "grad_norm": 1.655921220779419,
        "learning_rate": 7.365641729763239e-05,
        "epoch": 0.3926269657128126,
        "step": 3046
    },
    {
        "loss": 1.8759,
        "grad_norm": 1.669854998588562,
        "learning_rate": 7.359773089934106e-05,
        "epoch": 0.3927558649136375,
        "step": 3047
    },
    {
        "loss": 1.7015,
        "grad_norm": 2.0133631229400635,
        "learning_rate": 7.353905427397515e-05,
        "epoch": 0.3928847641144625,
        "step": 3048
    },
    {
        "loss": 1.8027,
        "grad_norm": 2.1630096435546875,
        "learning_rate": 7.348038744325418e-05,
        "epoch": 0.3930136633152874,
        "step": 3049
    },
    {
        "loss": 2.1278,
        "grad_norm": 1.4498999118804932,
        "learning_rate": 7.342173042889394e-05,
        "epoch": 0.3931425625161124,
        "step": 3050
    },
    {
        "loss": 2.2968,
        "grad_norm": 1.5903126001358032,
        "learning_rate": 7.336308325260657e-05,
        "epoch": 0.39327146171693733,
        "step": 3051
    },
    {
        "loss": 0.4505,
        "grad_norm": 1.6836074590682983,
        "learning_rate": 7.33044459361006e-05,
        "epoch": 0.3934003609177623,
        "step": 3052
    },
    {
        "loss": 1.9202,
        "grad_norm": 1.4851710796356201,
        "learning_rate": 7.324581850108089e-05,
        "epoch": 0.39352926011858724,
        "step": 3053
    },
    {
        "loss": 1.6314,
        "grad_norm": 2.176997184753418,
        "learning_rate": 7.318720096924875e-05,
        "epoch": 0.3936581593194122,
        "step": 3054
    },
    {
        "loss": 2.1028,
        "grad_norm": 1.4411544799804688,
        "learning_rate": 7.312859336230167e-05,
        "epoch": 0.39378705852023715,
        "step": 3055
    },
    {
        "loss": 2.0547,
        "grad_norm": 2.1315114498138428,
        "learning_rate": 7.306999570193352e-05,
        "epoch": 0.39391595772106214,
        "step": 3056
    },
    {
        "loss": 1.5173,
        "grad_norm": 2.5393364429473877,
        "learning_rate": 7.301140800983459e-05,
        "epoch": 0.39404485692188707,
        "step": 3057
    },
    {
        "loss": 2.2647,
        "grad_norm": 1.4163763523101807,
        "learning_rate": 7.295283030769128e-05,
        "epoch": 0.39417375612271205,
        "step": 3058
    },
    {
        "loss": 2.3048,
        "grad_norm": 1.7364368438720703,
        "learning_rate": 7.28942626171865e-05,
        "epoch": 0.394302655323537,
        "step": 3059
    },
    {
        "loss": 1.8592,
        "grad_norm": 1.8777847290039062,
        "learning_rate": 7.28357049599993e-05,
        "epoch": 0.39443155452436196,
        "step": 3060
    },
    {
        "loss": 2.1567,
        "grad_norm": 1.8720312118530273,
        "learning_rate": 7.27771573578051e-05,
        "epoch": 0.3945604537251869,
        "step": 3061
    },
    {
        "loss": 2.0705,
        "grad_norm": 1.2379224300384521,
        "learning_rate": 7.271861983227558e-05,
        "epoch": 0.39468935292601187,
        "step": 3062
    },
    {
        "loss": 1.9019,
        "grad_norm": 2.338651418685913,
        "learning_rate": 7.266009240507864e-05,
        "epoch": 0.3948182521268368,
        "step": 3063
    },
    {
        "loss": 1.7588,
        "grad_norm": 2.1208250522613525,
        "learning_rate": 7.260157509787856e-05,
        "epoch": 0.3949471513276618,
        "step": 3064
    },
    {
        "loss": 1.9743,
        "grad_norm": 2.16286563873291,
        "learning_rate": 7.254306793233571e-05,
        "epoch": 0.3950760505284867,
        "step": 3065
    },
    {
        "loss": 2.2881,
        "grad_norm": 1.9431596994400024,
        "learning_rate": 7.248457093010685e-05,
        "epoch": 0.3952049497293117,
        "step": 3066
    },
    {
        "loss": 1.2057,
        "grad_norm": 2.906244993209839,
        "learning_rate": 7.242608411284496e-05,
        "epoch": 0.3953338489301366,
        "step": 3067
    },
    {
        "loss": 1.8492,
        "grad_norm": 2.041736602783203,
        "learning_rate": 7.236760750219916e-05,
        "epoch": 0.3954627481309616,
        "step": 3068
    },
    {
        "loss": 1.6865,
        "grad_norm": 1.6406487226486206,
        "learning_rate": 7.230914111981482e-05,
        "epoch": 0.39559164733178653,
        "step": 3069
    },
    {
        "loss": 1.8872,
        "grad_norm": 2.520989418029785,
        "learning_rate": 7.225068498733359e-05,
        "epoch": 0.3957205465326115,
        "step": 3070
    },
    {
        "loss": 0.9532,
        "grad_norm": 1.2334418296813965,
        "learning_rate": 7.219223912639332e-05,
        "epoch": 0.39584944573343644,
        "step": 3071
    },
    {
        "loss": 1.5122,
        "grad_norm": 2.2337965965270996,
        "learning_rate": 7.213380355862796e-05,
        "epoch": 0.39597834493426143,
        "step": 3072
    },
    {
        "loss": 1.9864,
        "grad_norm": 1.8752515316009521,
        "learning_rate": 7.20753783056677e-05,
        "epoch": 0.39610724413508636,
        "step": 3073
    },
    {
        "loss": 2.2045,
        "grad_norm": 1.6886502504348755,
        "learning_rate": 7.201696338913902e-05,
        "epoch": 0.39623614333591134,
        "step": 3074
    },
    {
        "loss": 1.7314,
        "grad_norm": 1.9445735216140747,
        "learning_rate": 7.195855883066438e-05,
        "epoch": 0.39636504253673627,
        "step": 3075
    },
    {
        "loss": 1.7769,
        "grad_norm": 2.1554689407348633,
        "learning_rate": 7.190016465186259e-05,
        "epoch": 0.39649394173756125,
        "step": 3076
    },
    {
        "loss": 1.7264,
        "grad_norm": 2.2282066345214844,
        "learning_rate": 7.184178087434844e-05,
        "epoch": 0.3966228409383862,
        "step": 3077
    },
    {
        "loss": 2.3529,
        "grad_norm": 1.6723713874816895,
        "learning_rate": 7.178340751973303e-05,
        "epoch": 0.39675174013921116,
        "step": 3078
    },
    {
        "loss": 2.1745,
        "grad_norm": 1.5321825742721558,
        "learning_rate": 7.172504460962353e-05,
        "epoch": 0.3968806393400361,
        "step": 3079
    },
    {
        "loss": 1.3824,
        "grad_norm": 2.6874585151672363,
        "learning_rate": 7.166669216562323e-05,
        "epoch": 0.3970095385408611,
        "step": 3080
    },
    {
        "loss": 1.7733,
        "grad_norm": 1.3616799116134644,
        "learning_rate": 7.160835020933158e-05,
        "epoch": 0.397138437741686,
        "step": 3081
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.7524164915084839,
        "learning_rate": 7.15500187623441e-05,
        "epoch": 0.39726733694251093,
        "step": 3082
    },
    {
        "loss": 2.0927,
        "grad_norm": 2.1719017028808594,
        "learning_rate": 7.149169784625247e-05,
        "epoch": 0.3973962361433359,
        "step": 3083
    },
    {
        "loss": 0.9959,
        "grad_norm": 2.8584580421447754,
        "learning_rate": 7.14333874826445e-05,
        "epoch": 0.39752513534416084,
        "step": 3084
    },
    {
        "loss": 2.1951,
        "grad_norm": 2.026226758956909,
        "learning_rate": 7.137508769310399e-05,
        "epoch": 0.3976540345449858,
        "step": 3085
    },
    {
        "loss": 2.1189,
        "grad_norm": 1.2076878547668457,
        "learning_rate": 7.131679849921089e-05,
        "epoch": 0.39778293374581075,
        "step": 3086
    },
    {
        "loss": 1.6328,
        "grad_norm": 1.9605882167816162,
        "learning_rate": 7.125851992254123e-05,
        "epoch": 0.39791183294663574,
        "step": 3087
    },
    {
        "loss": 2.1469,
        "grad_norm": 1.9688267707824707,
        "learning_rate": 7.120025198466712e-05,
        "epoch": 0.39804073214746066,
        "step": 3088
    },
    {
        "loss": 1.8444,
        "grad_norm": 1.5287365913391113,
        "learning_rate": 7.114199470715668e-05,
        "epoch": 0.39816963134828565,
        "step": 3089
    },
    {
        "loss": 1.4601,
        "grad_norm": 2.491940975189209,
        "learning_rate": 7.108374811157419e-05,
        "epoch": 0.3982985305491106,
        "step": 3090
    },
    {
        "loss": 1.8097,
        "grad_norm": 1.8367220163345337,
        "learning_rate": 7.102551221947985e-05,
        "epoch": 0.39842742974993556,
        "step": 3091
    },
    {
        "loss": 1.5052,
        "grad_norm": 2.0685055255889893,
        "learning_rate": 7.096728705242992e-05,
        "epoch": 0.3985563289507605,
        "step": 3092
    },
    {
        "loss": 2.4826,
        "grad_norm": 1.44426429271698,
        "learning_rate": 7.090907263197679e-05,
        "epoch": 0.39868522815158547,
        "step": 3093
    },
    {
        "loss": 1.561,
        "grad_norm": 2.2182414531707764,
        "learning_rate": 7.085086897966874e-05,
        "epoch": 0.3988141273524104,
        "step": 3094
    },
    {
        "loss": 2.0243,
        "grad_norm": 2.305811882019043,
        "learning_rate": 7.079267611705019e-05,
        "epoch": 0.3989430265532354,
        "step": 3095
    },
    {
        "loss": 1.6766,
        "grad_norm": 2.0653936862945557,
        "learning_rate": 7.073449406566143e-05,
        "epoch": 0.3990719257540603,
        "step": 3096
    },
    {
        "loss": 2.3308,
        "grad_norm": 2.184746265411377,
        "learning_rate": 7.067632284703894e-05,
        "epoch": 0.3992008249548853,
        "step": 3097
    },
    {
        "loss": 1.8642,
        "grad_norm": 1.3836474418640137,
        "learning_rate": 7.0618162482715e-05,
        "epoch": 0.3993297241557102,
        "step": 3098
    },
    {
        "loss": 2.179,
        "grad_norm": 1.078814148902893,
        "learning_rate": 7.056001299421785e-05,
        "epoch": 0.3994586233565352,
        "step": 3099
    },
    {
        "loss": 1.5839,
        "grad_norm": 1.5093718767166138,
        "learning_rate": 7.050187440307198e-05,
        "epoch": 0.39958752255736013,
        "step": 3100
    },
    {
        "loss": 2.1268,
        "grad_norm": 1.3122769594192505,
        "learning_rate": 7.044374673079761e-05,
        "epoch": 0.3997164217581851,
        "step": 3101
    },
    {
        "loss": 1.8467,
        "grad_norm": 1.7360066175460815,
        "learning_rate": 7.038562999891089e-05,
        "epoch": 0.39984532095901004,
        "step": 3102
    },
    {
        "loss": 1.7309,
        "grad_norm": 2.2898061275482178,
        "learning_rate": 7.032752422892408e-05,
        "epoch": 0.399974220159835,
        "step": 3103
    },
    {
        "loss": 2.3289,
        "grad_norm": 1.5847210884094238,
        "learning_rate": 7.02694294423453e-05,
        "epoch": 0.40010311936065995,
        "step": 3104
    },
    {
        "loss": 2.0652,
        "grad_norm": 1.3920537233352661,
        "learning_rate": 7.021134566067861e-05,
        "epoch": 0.40023201856148494,
        "step": 3105
    },
    {
        "loss": 2.2608,
        "grad_norm": 1.3525220155715942,
        "learning_rate": 7.015327290542394e-05,
        "epoch": 0.40036091776230986,
        "step": 3106
    },
    {
        "loss": 2.1872,
        "grad_norm": 1.246307134628296,
        "learning_rate": 7.009521119807732e-05,
        "epoch": 0.40048981696313485,
        "step": 3107
    },
    {
        "loss": 1.8491,
        "grad_norm": 2.0229341983795166,
        "learning_rate": 7.00371605601305e-05,
        "epoch": 0.4006187161639598,
        "step": 3108
    },
    {
        "loss": 1.7192,
        "grad_norm": 2.519050121307373,
        "learning_rate": 6.997912101307113e-05,
        "epoch": 0.40074761536478476,
        "step": 3109
    },
    {
        "loss": 2.4452,
        "grad_norm": 1.809628963470459,
        "learning_rate": 6.99210925783829e-05,
        "epoch": 0.4008765145656097,
        "step": 3110
    },
    {
        "loss": 2.5715,
        "grad_norm": 1.837363362312317,
        "learning_rate": 6.986307527754531e-05,
        "epoch": 0.40100541376643467,
        "step": 3111
    },
    {
        "loss": 1.7927,
        "grad_norm": 2.079136610031128,
        "learning_rate": 6.980506913203373e-05,
        "epoch": 0.4011343129672596,
        "step": 3112
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.4627410173416138,
        "learning_rate": 6.974707416331941e-05,
        "epoch": 0.4012632121680846,
        "step": 3113
    },
    {
        "loss": 1.8794,
        "grad_norm": 2.0259125232696533,
        "learning_rate": 6.968909039286949e-05,
        "epoch": 0.4013921113689095,
        "step": 3114
    },
    {
        "loss": 1.7256,
        "grad_norm": 1.9840854406356812,
        "learning_rate": 6.963111784214691e-05,
        "epoch": 0.4015210105697345,
        "step": 3115
    },
    {
        "loss": 2.4385,
        "grad_norm": 1.9039196968078613,
        "learning_rate": 6.957315653261041e-05,
        "epoch": 0.4016499097705594,
        "step": 3116
    },
    {
        "loss": 1.7944,
        "grad_norm": 1.4217616319656372,
        "learning_rate": 6.951520648571479e-05,
        "epoch": 0.4017788089713844,
        "step": 3117
    },
    {
        "loss": 1.6235,
        "grad_norm": 2.244506597518921,
        "learning_rate": 6.945726772291046e-05,
        "epoch": 0.40190770817220933,
        "step": 3118
    },
    {
        "loss": 2.1471,
        "grad_norm": 1.145664930343628,
        "learning_rate": 6.939934026564372e-05,
        "epoch": 0.40203660737303426,
        "step": 3119
    },
    {
        "loss": 1.8489,
        "grad_norm": 1.692886471748352,
        "learning_rate": 6.934142413535672e-05,
        "epoch": 0.40216550657385924,
        "step": 3120
    },
    {
        "loss": 1.7446,
        "grad_norm": 1.687633991241455,
        "learning_rate": 6.928351935348736e-05,
        "epoch": 0.40229440577468417,
        "step": 3121
    },
    {
        "loss": 2.2398,
        "grad_norm": 1.1989045143127441,
        "learning_rate": 6.922562594146942e-05,
        "epoch": 0.40242330497550916,
        "step": 3122
    },
    {
        "loss": 2.2485,
        "grad_norm": 1.6962260007858276,
        "learning_rate": 6.916774392073236e-05,
        "epoch": 0.4025522041763341,
        "step": 3123
    },
    {
        "loss": 1.8257,
        "grad_norm": 1.288002610206604,
        "learning_rate": 6.910987331270157e-05,
        "epoch": 0.40268110337715907,
        "step": 3124
    },
    {
        "loss": 2.6967,
        "grad_norm": 1.5882089138031006,
        "learning_rate": 6.905201413879804e-05,
        "epoch": 0.402810002577984,
        "step": 3125
    },
    {
        "loss": 1.892,
        "grad_norm": 1.213303804397583,
        "learning_rate": 6.899416642043867e-05,
        "epoch": 0.402938901778809,
        "step": 3126
    },
    {
        "loss": 1.1827,
        "grad_norm": 2.382446765899658,
        "learning_rate": 6.893633017903604e-05,
        "epoch": 0.4030678009796339,
        "step": 3127
    },
    {
        "loss": 2.0757,
        "grad_norm": 1.6749523878097534,
        "learning_rate": 6.887850543599852e-05,
        "epoch": 0.4031967001804589,
        "step": 3128
    },
    {
        "loss": 1.915,
        "grad_norm": 1.777150273323059,
        "learning_rate": 6.88206922127302e-05,
        "epoch": 0.4033255993812838,
        "step": 3129
    },
    {
        "loss": 2.1804,
        "grad_norm": 2.1811938285827637,
        "learning_rate": 6.8762890530631e-05,
        "epoch": 0.4034544985821088,
        "step": 3130
    },
    {
        "loss": 1.9362,
        "grad_norm": 1.6247642040252686,
        "learning_rate": 6.870510041109642e-05,
        "epoch": 0.40358339778293373,
        "step": 3131
    },
    {
        "loss": 1.4812,
        "grad_norm": 2.6420059204101562,
        "learning_rate": 6.86473218755177e-05,
        "epoch": 0.4037122969837587,
        "step": 3132
    },
    {
        "loss": 1.9839,
        "grad_norm": 1.086766242980957,
        "learning_rate": 6.85895549452819e-05,
        "epoch": 0.40384119618458364,
        "step": 3133
    },
    {
        "loss": 1.6632,
        "grad_norm": 1.959451675415039,
        "learning_rate": 6.853179964177176e-05,
        "epoch": 0.4039700953854086,
        "step": 3134
    },
    {
        "loss": 1.851,
        "grad_norm": 1.9420071840286255,
        "learning_rate": 6.84740559863656e-05,
        "epoch": 0.40409899458623355,
        "step": 3135
    },
    {
        "loss": 1.5961,
        "grad_norm": 1.7351518869400024,
        "learning_rate": 6.841632400043755e-05,
        "epoch": 0.40422789378705853,
        "step": 3136
    },
    {
        "loss": 2.0058,
        "grad_norm": 1.68656325340271,
        "learning_rate": 6.83586037053574e-05,
        "epoch": 0.40435679298788346,
        "step": 3137
    },
    {
        "loss": 2.1549,
        "grad_norm": 1.2793515920639038,
        "learning_rate": 6.830089512249054e-05,
        "epoch": 0.40448569218870845,
        "step": 3138
    },
    {
        "loss": 1.9714,
        "grad_norm": 2.9886562824249268,
        "learning_rate": 6.824319827319813e-05,
        "epoch": 0.4046145913895334,
        "step": 3139
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.5203542709350586,
        "learning_rate": 6.818551317883685e-05,
        "epoch": 0.40474349059035836,
        "step": 3140
    },
    {
        "loss": 1.8148,
        "grad_norm": 1.4705543518066406,
        "learning_rate": 6.812783986075923e-05,
        "epoch": 0.4048723897911833,
        "step": 3141
    },
    {
        "loss": 1.4858,
        "grad_norm": 1.6227880716323853,
        "learning_rate": 6.807017834031322e-05,
        "epoch": 0.40500128899200827,
        "step": 3142
    },
    {
        "loss": 2.3017,
        "grad_norm": 1.5726463794708252,
        "learning_rate": 6.801252863884258e-05,
        "epoch": 0.4051301881928332,
        "step": 3143
    },
    {
        "loss": 1.7474,
        "grad_norm": 1.9193108081817627,
        "learning_rate": 6.795489077768659e-05,
        "epoch": 0.4052590873936582,
        "step": 3144
    },
    {
        "loss": 1.4596,
        "grad_norm": 2.6535701751708984,
        "learning_rate": 6.789726477818014e-05,
        "epoch": 0.4053879865944831,
        "step": 3145
    },
    {
        "loss": 1.9401,
        "grad_norm": 2.5665977001190186,
        "learning_rate": 6.78396506616538e-05,
        "epoch": 0.4055168857953081,
        "step": 3146
    },
    {
        "loss": 2.5819,
        "grad_norm": 2.0079126358032227,
        "learning_rate": 6.778204844943376e-05,
        "epoch": 0.405645784996133,
        "step": 3147
    },
    {
        "loss": 1.9148,
        "grad_norm": 2.066943645477295,
        "learning_rate": 6.77244581628417e-05,
        "epoch": 0.405774684196958,
        "step": 3148
    },
    {
        "loss": 1.4455,
        "grad_norm": 4.184299945831299,
        "learning_rate": 6.76668798231949e-05,
        "epoch": 0.40590358339778293,
        "step": 3149
    },
    {
        "loss": 1.4114,
        "grad_norm": 1.8783259391784668,
        "learning_rate": 6.760931345180632e-05,
        "epoch": 0.4060324825986079,
        "step": 3150
    },
    {
        "loss": 2.0903,
        "grad_norm": 1.1996899843215942,
        "learning_rate": 6.755175906998442e-05,
        "epoch": 0.40616138179943284,
        "step": 3151
    },
    {
        "loss": 2.0579,
        "grad_norm": 1.794431209564209,
        "learning_rate": 6.74942166990332e-05,
        "epoch": 0.4062902810002578,
        "step": 3152
    },
    {
        "loss": 2.4437,
        "grad_norm": 1.8545981645584106,
        "learning_rate": 6.743668636025228e-05,
        "epoch": 0.40641918020108275,
        "step": 3153
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.4080805778503418,
        "learning_rate": 6.737916807493679e-05,
        "epoch": 0.4065480794019077,
        "step": 3154
    },
    {
        "loss": 2.1192,
        "grad_norm": 1.8929622173309326,
        "learning_rate": 6.73216618643773e-05,
        "epoch": 0.40667697860273266,
        "step": 3155
    },
    {
        "loss": 2.5585,
        "grad_norm": 1.4585826396942139,
        "learning_rate": 6.726416774986014e-05,
        "epoch": 0.4068058778035576,
        "step": 3156
    },
    {
        "loss": 2.5991,
        "grad_norm": 1.1239715814590454,
        "learning_rate": 6.720668575266692e-05,
        "epoch": 0.4069347770043826,
        "step": 3157
    },
    {
        "loss": 1.7864,
        "grad_norm": 2.111466407775879,
        "learning_rate": 6.714921589407497e-05,
        "epoch": 0.4070636762052075,
        "step": 3158
    },
    {
        "loss": 2.0685,
        "grad_norm": 1.8058891296386719,
        "learning_rate": 6.709175819535696e-05,
        "epoch": 0.4071925754060325,
        "step": 3159
    },
    {
        "loss": 2.6325,
        "grad_norm": 1.2481651306152344,
        "learning_rate": 6.703431267778118e-05,
        "epoch": 0.4073214746068574,
        "step": 3160
    },
    {
        "loss": 1.8048,
        "grad_norm": 1.5102691650390625,
        "learning_rate": 6.697687936261135e-05,
        "epoch": 0.4074503738076824,
        "step": 3161
    },
    {
        "loss": 2.6228,
        "grad_norm": 1.337283968925476,
        "learning_rate": 6.691945827110657e-05,
        "epoch": 0.4075792730085073,
        "step": 3162
    },
    {
        "loss": 1.2647,
        "grad_norm": 2.2866222858428955,
        "learning_rate": 6.686204942452174e-05,
        "epoch": 0.4077081722093323,
        "step": 3163
    },
    {
        "loss": 2.4236,
        "grad_norm": 1.2597497701644897,
        "learning_rate": 6.680465284410689e-05,
        "epoch": 0.40783707141015724,
        "step": 3164
    },
    {
        "loss": 1.8006,
        "grad_norm": 1.8148258924484253,
        "learning_rate": 6.674726855110764e-05,
        "epoch": 0.4079659706109822,
        "step": 3165
    },
    {
        "loss": 1.7739,
        "grad_norm": 2.1868584156036377,
        "learning_rate": 6.66898965667651e-05,
        "epoch": 0.40809486981180715,
        "step": 3166
    },
    {
        "loss": 2.1483,
        "grad_norm": 2.1840031147003174,
        "learning_rate": 6.663253691231572e-05,
        "epoch": 0.40822376901263213,
        "step": 3167
    },
    {
        "loss": 1.6684,
        "grad_norm": 1.5372191667556763,
        "learning_rate": 6.657518960899152e-05,
        "epoch": 0.40835266821345706,
        "step": 3168
    },
    {
        "loss": 2.0336,
        "grad_norm": 1.8956841230392456,
        "learning_rate": 6.651785467801982e-05,
        "epoch": 0.40848156741428204,
        "step": 3169
    },
    {
        "loss": 1.8312,
        "grad_norm": 2.3307626247406006,
        "learning_rate": 6.646053214062349e-05,
        "epoch": 0.40861046661510697,
        "step": 3170
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.4280242919921875,
        "learning_rate": 6.640322201802069e-05,
        "epoch": 0.40873936581593195,
        "step": 3171
    },
    {
        "loss": 1.322,
        "grad_norm": 2.1499428749084473,
        "learning_rate": 6.634592433142499e-05,
        "epoch": 0.4088682650167569,
        "step": 3172
    },
    {
        "loss": 1.1798,
        "grad_norm": 2.4792633056640625,
        "learning_rate": 6.62886391020455e-05,
        "epoch": 0.40899716421758187,
        "step": 3173
    },
    {
        "loss": 1.9313,
        "grad_norm": 2.168879270553589,
        "learning_rate": 6.623136635108651e-05,
        "epoch": 0.4091260634184068,
        "step": 3174
    },
    {
        "loss": 1.8241,
        "grad_norm": 2.1215548515319824,
        "learning_rate": 6.617410609974786e-05,
        "epoch": 0.4092549626192318,
        "step": 3175
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.1097321510314941,
        "learning_rate": 6.611685836922474e-05,
        "epoch": 0.4093838618200567,
        "step": 3176
    },
    {
        "loss": 1.9692,
        "grad_norm": 1.907279133796692,
        "learning_rate": 6.605962318070764e-05,
        "epoch": 0.4095127610208817,
        "step": 3177
    },
    {
        "loss": 2.2663,
        "grad_norm": 1.5012446641921997,
        "learning_rate": 6.600240055538245e-05,
        "epoch": 0.4096416602217066,
        "step": 3178
    },
    {
        "loss": 1.9537,
        "grad_norm": 1.9267964363098145,
        "learning_rate": 6.594519051443029e-05,
        "epoch": 0.4097705594225316,
        "step": 3179
    },
    {
        "loss": 1.6572,
        "grad_norm": 1.9971576929092407,
        "learning_rate": 6.58879930790279e-05,
        "epoch": 0.4098994586233565,
        "step": 3180
    },
    {
        "loss": 1.5668,
        "grad_norm": 2.1193528175354004,
        "learning_rate": 6.583080827034708e-05,
        "epoch": 0.4100283578241815,
        "step": 3181
    },
    {
        "loss": 0.4021,
        "grad_norm": 1.1622729301452637,
        "learning_rate": 6.577363610955505e-05,
        "epoch": 0.41015725702500644,
        "step": 3182
    },
    {
        "loss": 1.2404,
        "grad_norm": 2.7863824367523193,
        "learning_rate": 6.571647661781441e-05,
        "epoch": 0.4102861562258314,
        "step": 3183
    },
    {
        "loss": 1.7306,
        "grad_norm": 2.3949906826019287,
        "learning_rate": 6.565932981628296e-05,
        "epoch": 0.41041505542665635,
        "step": 3184
    },
    {
        "loss": 2.3286,
        "grad_norm": 1.9570105075836182,
        "learning_rate": 6.560219572611395e-05,
        "epoch": 0.41054395462748133,
        "step": 3185
    },
    {
        "loss": 1.6501,
        "grad_norm": 2.459744453430176,
        "learning_rate": 6.554507436845573e-05,
        "epoch": 0.41067285382830626,
        "step": 3186
    },
    {
        "loss": 1.5989,
        "grad_norm": 1.9022188186645508,
        "learning_rate": 6.548796576445213e-05,
        "epoch": 0.41080175302913124,
        "step": 3187
    },
    {
        "loss": 1.9467,
        "grad_norm": 3.0171217918395996,
        "learning_rate": 6.543086993524213e-05,
        "epoch": 0.4109306522299562,
        "step": 3188
    },
    {
        "loss": 1.278,
        "grad_norm": 1.9750313758850098,
        "learning_rate": 6.537378690195999e-05,
        "epoch": 0.41105955143078116,
        "step": 3189
    },
    {
        "loss": 1.7446,
        "grad_norm": 2.266986608505249,
        "learning_rate": 6.531671668573533e-05,
        "epoch": 0.4111884506316061,
        "step": 3190
    },
    {
        "loss": 2.3291,
        "grad_norm": 2.2507119178771973,
        "learning_rate": 6.525965930769289e-05,
        "epoch": 0.411317349832431,
        "step": 3191
    },
    {
        "loss": 2.011,
        "grad_norm": 1.5761306285858154,
        "learning_rate": 6.520261478895281e-05,
        "epoch": 0.411446249033256,
        "step": 3192
    },
    {
        "loss": 2.2091,
        "grad_norm": 2.8120172023773193,
        "learning_rate": 6.514558315063035e-05,
        "epoch": 0.4115751482340809,
        "step": 3193
    },
    {
        "loss": 2.2231,
        "grad_norm": 1.733971118927002,
        "learning_rate": 6.508856441383606e-05,
        "epoch": 0.4117040474349059,
        "step": 3194
    },
    {
        "loss": 2.4006,
        "grad_norm": 1.8863979578018188,
        "learning_rate": 6.503155859967565e-05,
        "epoch": 0.41183294663573083,
        "step": 3195
    },
    {
        "loss": 2.3834,
        "grad_norm": 1.866769552230835,
        "learning_rate": 6.497456572925011e-05,
        "epoch": 0.4119618458365558,
        "step": 3196
    },
    {
        "loss": 2.0242,
        "grad_norm": 2.2169711589813232,
        "learning_rate": 6.49175858236557e-05,
        "epoch": 0.41209074503738075,
        "step": 3197
    },
    {
        "loss": 1.605,
        "grad_norm": 2.166841506958008,
        "learning_rate": 6.486061890398371e-05,
        "epoch": 0.41221964423820573,
        "step": 3198
    },
    {
        "loss": 2.0058,
        "grad_norm": 1.2543556690216064,
        "learning_rate": 6.480366499132074e-05,
        "epoch": 0.41234854343903066,
        "step": 3199
    },
    {
        "loss": 2.0561,
        "grad_norm": 1.8458857536315918,
        "learning_rate": 6.474672410674855e-05,
        "epoch": 0.41247744263985564,
        "step": 3200
    },
    {
        "loss": 1.626,
        "grad_norm": 2.0889360904693604,
        "learning_rate": 6.468979627134409e-05,
        "epoch": 0.41260634184068057,
        "step": 3201
    },
    {
        "loss": 1.8357,
        "grad_norm": 1.6398241519927979,
        "learning_rate": 6.46328815061795e-05,
        "epoch": 0.41273524104150555,
        "step": 3202
    },
    {
        "loss": 1.9359,
        "grad_norm": 2.1235909461975098,
        "learning_rate": 6.457597983232195e-05,
        "epoch": 0.4128641402423305,
        "step": 3203
    },
    {
        "loss": 1.4514,
        "grad_norm": 1.9758766889572144,
        "learning_rate": 6.4519091270834e-05,
        "epoch": 0.41299303944315546,
        "step": 3204
    },
    {
        "loss": 2.0399,
        "grad_norm": 1.4619827270507812,
        "learning_rate": 6.44622158427731e-05,
        "epoch": 0.4131219386439804,
        "step": 3205
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.7413586378097534,
        "learning_rate": 6.440535356919208e-05,
        "epoch": 0.4132508378448054,
        "step": 3206
    },
    {
        "loss": 2.4726,
        "grad_norm": 1.4107403755187988,
        "learning_rate": 6.434850447113866e-05,
        "epoch": 0.4133797370456303,
        "step": 3207
    },
    {
        "loss": 2.0571,
        "grad_norm": 0.9710341095924377,
        "learning_rate": 6.429166856965587e-05,
        "epoch": 0.4135086362464553,
        "step": 3208
    },
    {
        "loss": 1.8521,
        "grad_norm": 2.027496337890625,
        "learning_rate": 6.423484588578174e-05,
        "epoch": 0.4136375354472802,
        "step": 3209
    },
    {
        "loss": 2.4379,
        "grad_norm": 1.5280500650405884,
        "learning_rate": 6.417803644054957e-05,
        "epoch": 0.4137664346481052,
        "step": 3210
    },
    {
        "loss": 2.3585,
        "grad_norm": 1.4444782733917236,
        "learning_rate": 6.412124025498755e-05,
        "epoch": 0.4138953338489301,
        "step": 3211
    },
    {
        "loss": 0.8696,
        "grad_norm": 2.8338069915771484,
        "learning_rate": 6.406445735011905e-05,
        "epoch": 0.4140242330497551,
        "step": 3212
    },
    {
        "loss": 0.8511,
        "grad_norm": 2.4426093101501465,
        "learning_rate": 6.400768774696257e-05,
        "epoch": 0.41415313225058004,
        "step": 3213
    },
    {
        "loss": 1.6915,
        "grad_norm": 3.163924217224121,
        "learning_rate": 6.395093146653166e-05,
        "epoch": 0.414282031451405,
        "step": 3214
    },
    {
        "loss": 2.238,
        "grad_norm": 1.6033190488815308,
        "learning_rate": 6.389418852983489e-05,
        "epoch": 0.41441093065222995,
        "step": 3215
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.612511157989502,
        "learning_rate": 6.383745895787594e-05,
        "epoch": 0.41453982985305493,
        "step": 3216
    },
    {
        "loss": 2.0946,
        "grad_norm": 1.8323801755905151,
        "learning_rate": 6.378074277165358e-05,
        "epoch": 0.41466872905387986,
        "step": 3217
    },
    {
        "loss": 2.159,
        "grad_norm": 1.300695776939392,
        "learning_rate": 6.372403999216148e-05,
        "epoch": 0.41479762825470484,
        "step": 3218
    },
    {
        "loss": 2.2761,
        "grad_norm": 1.700583815574646,
        "learning_rate": 6.366735064038854e-05,
        "epoch": 0.41492652745552977,
        "step": 3219
    },
    {
        "loss": 2.5444,
        "grad_norm": 1.6001473665237427,
        "learning_rate": 6.36106747373185e-05,
        "epoch": 0.41505542665635475,
        "step": 3220
    },
    {
        "loss": 2.1041,
        "grad_norm": 2.3456907272338867,
        "learning_rate": 6.35540123039303e-05,
        "epoch": 0.4151843258571797,
        "step": 3221
    },
    {
        "loss": 1.7049,
        "grad_norm": 2.8990867137908936,
        "learning_rate": 6.349736336119773e-05,
        "epoch": 0.41531322505800466,
        "step": 3222
    },
    {
        "loss": 2.1108,
        "grad_norm": 1.5897226333618164,
        "learning_rate": 6.344072793008974e-05,
        "epoch": 0.4154421242588296,
        "step": 3223
    },
    {
        "loss": 2.4832,
        "grad_norm": 2.054960012435913,
        "learning_rate": 6.338410603157022e-05,
        "epoch": 0.4155710234596546,
        "step": 3224
    },
    {
        "loss": 1.0795,
        "grad_norm": 2.2918224334716797,
        "learning_rate": 6.332749768659789e-05,
        "epoch": 0.4156999226604795,
        "step": 3225
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.896365761756897,
        "learning_rate": 6.327090291612674e-05,
        "epoch": 0.4158288218613045,
        "step": 3226
    },
    {
        "loss": 1.9839,
        "grad_norm": 1.7607494592666626,
        "learning_rate": 6.321432174110558e-05,
        "epoch": 0.4159577210621294,
        "step": 3227
    },
    {
        "loss": 1.7532,
        "grad_norm": 1.3613786697387695,
        "learning_rate": 6.315775418247815e-05,
        "epoch": 0.41608662026295434,
        "step": 3228
    },
    {
        "loss": 2.408,
        "grad_norm": 1.1801888942718506,
        "learning_rate": 6.31012002611832e-05,
        "epoch": 0.4162155194637793,
        "step": 3229
    },
    {
        "loss": 1.9049,
        "grad_norm": 2.388322591781616,
        "learning_rate": 6.304465999815445e-05,
        "epoch": 0.41634441866460425,
        "step": 3230
    },
    {
        "loss": 1.0039,
        "grad_norm": 2.0290234088897705,
        "learning_rate": 6.298813341432058e-05,
        "epoch": 0.41647331786542924,
        "step": 3231
    },
    {
        "loss": 2.0451,
        "grad_norm": 1.9249200820922852,
        "learning_rate": 6.293162053060514e-05,
        "epoch": 0.41660221706625417,
        "step": 3232
    },
    {
        "loss": 2.1817,
        "grad_norm": 1.6868155002593994,
        "learning_rate": 6.287512136792664e-05,
        "epoch": 0.41673111626707915,
        "step": 3233
    },
    {
        "loss": 2.2329,
        "grad_norm": 2.0080795288085938,
        "learning_rate": 6.281863594719854e-05,
        "epoch": 0.4168600154679041,
        "step": 3234
    },
    {
        "loss": 2.2604,
        "grad_norm": 1.9711817502975464,
        "learning_rate": 6.276216428932915e-05,
        "epoch": 0.41698891466872906,
        "step": 3235
    },
    {
        "loss": 1.6698,
        "grad_norm": 1.6325732469558716,
        "learning_rate": 6.270570641522176e-05,
        "epoch": 0.417117813869554,
        "step": 3236
    },
    {
        "loss": 1.5961,
        "grad_norm": 1.727769136428833,
        "learning_rate": 6.264926234577447e-05,
        "epoch": 0.41724671307037897,
        "step": 3237
    },
    {
        "loss": 1.7972,
        "grad_norm": 1.625434398651123,
        "learning_rate": 6.259283210188035e-05,
        "epoch": 0.4173756122712039,
        "step": 3238
    },
    {
        "loss": 2.0703,
        "grad_norm": 2.0013346672058105,
        "learning_rate": 6.253641570442737e-05,
        "epoch": 0.4175045114720289,
        "step": 3239
    },
    {
        "loss": 1.9374,
        "grad_norm": 2.9434261322021484,
        "learning_rate": 6.248001317429825e-05,
        "epoch": 0.4176334106728538,
        "step": 3240
    },
    {
        "loss": 2.339,
        "grad_norm": 1.301530122756958,
        "learning_rate": 6.242362453237073e-05,
        "epoch": 0.4177623098736788,
        "step": 3241
    },
    {
        "loss": 1.7006,
        "grad_norm": 1.6219322681427002,
        "learning_rate": 6.23672497995172e-05,
        "epoch": 0.4178912090745037,
        "step": 3242
    },
    {
        "loss": 1.561,
        "grad_norm": 1.8396486043930054,
        "learning_rate": 6.23108889966052e-05,
        "epoch": 0.4180201082753287,
        "step": 3243
    },
    {
        "loss": 1.9168,
        "grad_norm": 2.33647084236145,
        "learning_rate": 6.22545421444969e-05,
        "epoch": 0.41814900747615363,
        "step": 3244
    },
    {
        "loss": 1.7288,
        "grad_norm": 2.437873125076294,
        "learning_rate": 6.219820926404928e-05,
        "epoch": 0.4182779066769786,
        "step": 3245
    },
    {
        "loss": 1.6802,
        "grad_norm": 1.7620545625686646,
        "learning_rate": 6.214189037611428e-05,
        "epoch": 0.41840680587780354,
        "step": 3246
    },
    {
        "loss": 2.2049,
        "grad_norm": 2.4704370498657227,
        "learning_rate": 6.208558550153862e-05,
        "epoch": 0.41853570507862853,
        "step": 3247
    },
    {
        "loss": 1.5568,
        "grad_norm": 1.798460841178894,
        "learning_rate": 6.202929466116381e-05,
        "epoch": 0.41866460427945346,
        "step": 3248
    },
    {
        "loss": 1.9321,
        "grad_norm": 1.299906849861145,
        "learning_rate": 6.197301787582612e-05,
        "epoch": 0.41879350348027844,
        "step": 3249
    },
    {
        "loss": 1.9805,
        "grad_norm": 2.0309462547302246,
        "learning_rate": 6.191675516635674e-05,
        "epoch": 0.41892240268110337,
        "step": 3250
    },
    {
        "loss": 2.2791,
        "grad_norm": 2.0172321796417236,
        "learning_rate": 6.186050655358156e-05,
        "epoch": 0.41905130188192835,
        "step": 3251
    },
    {
        "loss": 2.4422,
        "grad_norm": 2.0747997760772705,
        "learning_rate": 6.180427205832122e-05,
        "epoch": 0.4191802010827533,
        "step": 3252
    },
    {
        "loss": 2.3088,
        "grad_norm": 1.1704258918762207,
        "learning_rate": 6.174805170139126e-05,
        "epoch": 0.41930910028357826,
        "step": 3253
    },
    {
        "loss": 2.5731,
        "grad_norm": 1.7929418087005615,
        "learning_rate": 6.169184550360185e-05,
        "epoch": 0.4194379994844032,
        "step": 3254
    },
    {
        "loss": 2.263,
        "grad_norm": 1.6980069875717163,
        "learning_rate": 6.163565348575802e-05,
        "epoch": 0.4195668986852282,
        "step": 3255
    },
    {
        "loss": 1.9871,
        "grad_norm": 1.2627649307250977,
        "learning_rate": 6.15794756686595e-05,
        "epoch": 0.4196957978860531,
        "step": 3256
    },
    {
        "loss": 1.5776,
        "grad_norm": 1.4845269918441772,
        "learning_rate": 6.152331207310081e-05,
        "epoch": 0.4198246970868781,
        "step": 3257
    },
    {
        "loss": 1.5557,
        "grad_norm": 2.920097827911377,
        "learning_rate": 6.146716271987109e-05,
        "epoch": 0.419953596287703,
        "step": 3258
    },
    {
        "loss": 1.3742,
        "grad_norm": 2.4302453994750977,
        "learning_rate": 6.141102762975436e-05,
        "epoch": 0.420082495488528,
        "step": 3259
    },
    {
        "loss": 1.7752,
        "grad_norm": 1.9009987115859985,
        "learning_rate": 6.13549068235293e-05,
        "epoch": 0.4202113946893529,
        "step": 3260
    },
    {
        "loss": 2.3662,
        "grad_norm": 1.6583846807479858,
        "learning_rate": 6.129880032196927e-05,
        "epoch": 0.4203402938901779,
        "step": 3261
    },
    {
        "loss": 1.948,
        "grad_norm": 2.8364925384521484,
        "learning_rate": 6.12427081458423e-05,
        "epoch": 0.42046919309100284,
        "step": 3262
    },
    {
        "loss": 2.1657,
        "grad_norm": 1.541022777557373,
        "learning_rate": 6.118663031591126e-05,
        "epoch": 0.42059809229182776,
        "step": 3263
    },
    {
        "loss": 1.5953,
        "grad_norm": 1.6968086957931519,
        "learning_rate": 6.113056685293358e-05,
        "epoch": 0.42072699149265275,
        "step": 3264
    },
    {
        "loss": 2.3895,
        "grad_norm": 1.3643110990524292,
        "learning_rate": 6.107451777766146e-05,
        "epoch": 0.4208558906934777,
        "step": 3265
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.5390437841415405,
        "learning_rate": 6.101848311084166e-05,
        "epoch": 0.42098478989430266,
        "step": 3266
    },
    {
        "loss": 1.5806,
        "grad_norm": 1.823140025138855,
        "learning_rate": 6.0962462873215787e-05,
        "epoch": 0.4211136890951276,
        "step": 3267
    },
    {
        "loss": 2.0182,
        "grad_norm": 1.8429992198944092,
        "learning_rate": 6.0906457085519875e-05,
        "epoch": 0.42124258829595257,
        "step": 3268
    },
    {
        "loss": 2.4813,
        "grad_norm": 1.0819488763809204,
        "learning_rate": 6.085046576848483e-05,
        "epoch": 0.4213714874967775,
        "step": 3269
    },
    {
        "loss": 2.1842,
        "grad_norm": 1.6679757833480835,
        "learning_rate": 6.079448894283605e-05,
        "epoch": 0.4215003866976025,
        "step": 3270
    },
    {
        "loss": 1.8267,
        "grad_norm": 2.2816336154937744,
        "learning_rate": 6.073852662929361e-05,
        "epoch": 0.4216292858984274,
        "step": 3271
    },
    {
        "loss": 1.7545,
        "grad_norm": 2.200801134109497,
        "learning_rate": 6.068257884857229e-05,
        "epoch": 0.4217581850992524,
        "step": 3272
    },
    {
        "loss": 2.0079,
        "grad_norm": 1.9719460010528564,
        "learning_rate": 6.062664562138138e-05,
        "epoch": 0.4218870843000773,
        "step": 3273
    },
    {
        "loss": 2.1977,
        "grad_norm": 1.480314016342163,
        "learning_rate": 6.0570726968424875e-05,
        "epoch": 0.4220159835009023,
        "step": 3274
    },
    {
        "loss": 2.0523,
        "grad_norm": 1.4034547805786133,
        "learning_rate": 6.051482291040125e-05,
        "epoch": 0.42214488270172723,
        "step": 3275
    },
    {
        "loss": 1.836,
        "grad_norm": 2.1179323196411133,
        "learning_rate": 6.0458933468003696e-05,
        "epoch": 0.4222737819025522,
        "step": 3276
    },
    {
        "loss": 1.4941,
        "grad_norm": 2.129389524459839,
        "learning_rate": 6.040305866192002e-05,
        "epoch": 0.42240268110337714,
        "step": 3277
    },
    {
        "loss": 1.8897,
        "grad_norm": 0.9662010669708252,
        "learning_rate": 6.034719851283244e-05,
        "epoch": 0.4225315803042021,
        "step": 3278
    },
    {
        "loss": 1.7423,
        "grad_norm": 2.2396392822265625,
        "learning_rate": 6.029135304141793e-05,
        "epoch": 0.42266047950502705,
        "step": 3279
    },
    {
        "loss": 1.3546,
        "grad_norm": 2.072115898132324,
        "learning_rate": 6.023552226834797e-05,
        "epoch": 0.42278937870585204,
        "step": 3280
    },
    {
        "loss": 1.5659,
        "grad_norm": 1.5493974685668945,
        "learning_rate": 6.01797062142885e-05,
        "epoch": 0.42291827790667696,
        "step": 3281
    },
    {
        "loss": 1.9892,
        "grad_norm": 2.095613956451416,
        "learning_rate": 6.012390489990018e-05,
        "epoch": 0.42304717710750195,
        "step": 3282
    },
    {
        "loss": 2.5144,
        "grad_norm": 1.3793528079986572,
        "learning_rate": 6.006811834583809e-05,
        "epoch": 0.4231760763083269,
        "step": 3283
    },
    {
        "loss": 1.7094,
        "grad_norm": 2.434893846511841,
        "learning_rate": 6.001234657275192e-05,
        "epoch": 0.42330497550915186,
        "step": 3284
    },
    {
        "loss": 2.1094,
        "grad_norm": 2.2364118099212646,
        "learning_rate": 5.99565896012858e-05,
        "epoch": 0.4234338747099768,
        "step": 3285
    },
    {
        "loss": 1.8341,
        "grad_norm": 1.4221771955490112,
        "learning_rate": 5.990084745207849e-05,
        "epoch": 0.42356277391080177,
        "step": 3286
    },
    {
        "loss": 1.8341,
        "grad_norm": 1.7191888093948364,
        "learning_rate": 5.9845120145763225e-05,
        "epoch": 0.4236916731116267,
        "step": 3287
    },
    {
        "loss": 1.5227,
        "grad_norm": 2.146402597427368,
        "learning_rate": 5.978940770296764e-05,
        "epoch": 0.4238205723124517,
        "step": 3288
    },
    {
        "loss": 1.5212,
        "grad_norm": 1.7680436372756958,
        "learning_rate": 5.973371014431406e-05,
        "epoch": 0.4239494715132766,
        "step": 3289
    },
    {
        "loss": 2.2248,
        "grad_norm": 1.6626157760620117,
        "learning_rate": 5.967802749041919e-05,
        "epoch": 0.4240783707141016,
        "step": 3290
    },
    {
        "loss": 2.2588,
        "grad_norm": 1.8858369588851929,
        "learning_rate": 5.962235976189421e-05,
        "epoch": 0.4242072699149265,
        "step": 3291
    },
    {
        "loss": 2.0664,
        "grad_norm": 1.7312941551208496,
        "learning_rate": 5.9566706979344735e-05,
        "epoch": 0.4243361691157515,
        "step": 3292
    },
    {
        "loss": 1.7162,
        "grad_norm": 3.3363165855407715,
        "learning_rate": 5.951106916337097e-05,
        "epoch": 0.42446506831657643,
        "step": 3293
    },
    {
        "loss": 2.1172,
        "grad_norm": 1.4873297214508057,
        "learning_rate": 5.9455446334567544e-05,
        "epoch": 0.4245939675174014,
        "step": 3294
    },
    {
        "loss": 1.5342,
        "grad_norm": 2.3227880001068115,
        "learning_rate": 5.9399838513523445e-05,
        "epoch": 0.42472286671822634,
        "step": 3295
    },
    {
        "loss": 2.0606,
        "grad_norm": 1.6166950464248657,
        "learning_rate": 5.934424572082222e-05,
        "epoch": 0.4248517659190513,
        "step": 3296
    },
    {
        "loss": 2.0535,
        "grad_norm": 1.316131353378296,
        "learning_rate": 5.92886679770418e-05,
        "epoch": 0.42498066511987626,
        "step": 3297
    },
    {
        "loss": 2.0645,
        "grad_norm": 1.5733693838119507,
        "learning_rate": 5.923310530275452e-05,
        "epoch": 0.42510956432070124,
        "step": 3298
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.621309757232666,
        "learning_rate": 5.9177557718527224e-05,
        "epoch": 0.42523846352152617,
        "step": 3299
    },
    {
        "loss": 1.7325,
        "grad_norm": 1.5525407791137695,
        "learning_rate": 5.912202524492102e-05,
        "epoch": 0.4253673627223511,
        "step": 3300
    },
    {
        "loss": 0.7984,
        "grad_norm": 2.063486099243164,
        "learning_rate": 5.9066507902491596e-05,
        "epoch": 0.4254962619231761,
        "step": 3301
    },
    {
        "loss": 1.6239,
        "grad_norm": 2.0237233638763428,
        "learning_rate": 5.901100571178897e-05,
        "epoch": 0.425625161124001,
        "step": 3302
    },
    {
        "loss": 1.5093,
        "grad_norm": 1.770348072052002,
        "learning_rate": 5.895551869335749e-05,
        "epoch": 0.425754060324826,
        "step": 3303
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.9591631889343262,
        "learning_rate": 5.890004686773599e-05,
        "epoch": 0.4258829595256509,
        "step": 3304
    },
    {
        "loss": 1.8611,
        "grad_norm": 2.075007438659668,
        "learning_rate": 5.884459025545754e-05,
        "epoch": 0.4260118587264759,
        "step": 3305
    },
    {
        "loss": 1.9148,
        "grad_norm": 2.9427478313446045,
        "learning_rate": 5.878914887704977e-05,
        "epoch": 0.42614075792730083,
        "step": 3306
    },
    {
        "loss": 1.8142,
        "grad_norm": 2.540891647338867,
        "learning_rate": 5.873372275303455e-05,
        "epoch": 0.4262696571281258,
        "step": 3307
    },
    {
        "loss": 2.2679,
        "grad_norm": 2.0155630111694336,
        "learning_rate": 5.8678311903928076e-05,
        "epoch": 0.42639855632895074,
        "step": 3308
    },
    {
        "loss": 1.1307,
        "grad_norm": 2.241813898086548,
        "learning_rate": 5.862291635024099e-05,
        "epoch": 0.4265274555297757,
        "step": 3309
    },
    {
        "loss": 2.0411,
        "grad_norm": 1.735382080078125,
        "learning_rate": 5.856753611247816e-05,
        "epoch": 0.42665635473060065,
        "step": 3310
    },
    {
        "loss": 1.8118,
        "grad_norm": 2.762078285217285,
        "learning_rate": 5.85121712111389e-05,
        "epoch": 0.42678525393142563,
        "step": 3311
    },
    {
        "loss": 2.0733,
        "grad_norm": 1.944135069847107,
        "learning_rate": 5.845682166671675e-05,
        "epoch": 0.42691415313225056,
        "step": 3312
    },
    {
        "loss": 2.3703,
        "grad_norm": 1.442533016204834,
        "learning_rate": 5.840148749969967e-05,
        "epoch": 0.42704305233307555,
        "step": 3313
    },
    {
        "loss": 1.6626,
        "grad_norm": 1.8937404155731201,
        "learning_rate": 5.834616873056979e-05,
        "epoch": 0.4271719515339005,
        "step": 3314
    },
    {
        "loss": 2.4449,
        "grad_norm": 1.3711967468261719,
        "learning_rate": 5.8290865379803663e-05,
        "epoch": 0.42730085073472546,
        "step": 3315
    },
    {
        "loss": 1.0039,
        "grad_norm": 2.488952398300171,
        "learning_rate": 5.8235577467872085e-05,
        "epoch": 0.4274297499355504,
        "step": 3316
    },
    {
        "loss": 2.075,
        "grad_norm": 1.4042689800262451,
        "learning_rate": 5.81803050152401e-05,
        "epoch": 0.42755864913637537,
        "step": 3317
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.3339557647705078,
        "learning_rate": 5.812504804236708e-05,
        "epoch": 0.4276875483372003,
        "step": 3318
    },
    {
        "loss": 1.9653,
        "grad_norm": 1.8719375133514404,
        "learning_rate": 5.806980656970673e-05,
        "epoch": 0.4278164475380253,
        "step": 3319
    },
    {
        "loss": 1.73,
        "grad_norm": 2.1650874614715576,
        "learning_rate": 5.8014580617706885e-05,
        "epoch": 0.4279453467388502,
        "step": 3320
    },
    {
        "loss": 2.5327,
        "grad_norm": 1.2364391088485718,
        "learning_rate": 5.795937020680964e-05,
        "epoch": 0.4280742459396752,
        "step": 3321
    },
    {
        "loss": 2.3102,
        "grad_norm": 1.9003052711486816,
        "learning_rate": 5.790417535745146e-05,
        "epoch": 0.4282031451405001,
        "step": 3322
    },
    {
        "loss": 1.4431,
        "grad_norm": 1.8265972137451172,
        "learning_rate": 5.7848996090063004e-05,
        "epoch": 0.4283320443413251,
        "step": 3323
    },
    {
        "loss": 1.7405,
        "grad_norm": 1.8234875202178955,
        "learning_rate": 5.779383242506911e-05,
        "epoch": 0.42846094354215003,
        "step": 3324
    },
    {
        "loss": 1.8842,
        "grad_norm": 1.556358814239502,
        "learning_rate": 5.773868438288883e-05,
        "epoch": 0.428589842742975,
        "step": 3325
    },
    {
        "loss": 2.2763,
        "grad_norm": 1.6221874952316284,
        "learning_rate": 5.768355198393553e-05,
        "epoch": 0.42871874194379994,
        "step": 3326
    },
    {
        "loss": 1.4115,
        "grad_norm": 2.5534396171569824,
        "learning_rate": 5.762843524861667e-05,
        "epoch": 0.4288476411446249,
        "step": 3327
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.506874918937683,
        "learning_rate": 5.757333419733407e-05,
        "epoch": 0.42897654034544985,
        "step": 3328
    },
    {
        "loss": 1.7803,
        "grad_norm": 1.6610722541809082,
        "learning_rate": 5.7518248850483535e-05,
        "epoch": 0.42910543954627484,
        "step": 3329
    },
    {
        "loss": 1.8905,
        "grad_norm": 2.1377482414245605,
        "learning_rate": 5.746317922845522e-05,
        "epoch": 0.42923433874709976,
        "step": 3330
    },
    {
        "loss": 0.5904,
        "grad_norm": 2.1197760105133057,
        "learning_rate": 5.740812535163345e-05,
        "epoch": 0.42936323794792475,
        "step": 3331
    },
    {
        "loss": 1.294,
        "grad_norm": 2.7350594997406006,
        "learning_rate": 5.73530872403966e-05,
        "epoch": 0.4294921371487497,
        "step": 3332
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.898483395576477,
        "learning_rate": 5.7298064915117355e-05,
        "epoch": 0.42962103634957466,
        "step": 3333
    },
    {
        "loss": 1.7028,
        "grad_norm": 1.9777356386184692,
        "learning_rate": 5.724305839616243e-05,
        "epoch": 0.4297499355503996,
        "step": 3334
    },
    {
        "loss": 1.115,
        "grad_norm": 2.430952310562134,
        "learning_rate": 5.718806770389278e-05,
        "epoch": 0.42987883475122457,
        "step": 3335
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.7932342290878296,
        "learning_rate": 5.7133092858663504e-05,
        "epoch": 0.4300077339520495,
        "step": 3336
    },
    {
        "loss": 1.8724,
        "grad_norm": 1.4190428256988525,
        "learning_rate": 5.707813388082377e-05,
        "epoch": 0.4301366331528744,
        "step": 3337
    },
    {
        "loss": 1.4883,
        "grad_norm": 5.368800163269043,
        "learning_rate": 5.702319079071688e-05,
        "epoch": 0.4302655323536994,
        "step": 3338
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.7110681533813477,
        "learning_rate": 5.696826360868028e-05,
        "epoch": 0.43039443155452434,
        "step": 3339
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.947413682937622,
        "learning_rate": 5.69133523550456e-05,
        "epoch": 0.4305233307553493,
        "step": 3340
    },
    {
        "loss": 0.6171,
        "grad_norm": 2.537602663040161,
        "learning_rate": 5.685845705013843e-05,
        "epoch": 0.43065222995617425,
        "step": 3341
    },
    {
        "loss": 1.5284,
        "grad_norm": 3.3207976818084717,
        "learning_rate": 5.6803577714278574e-05,
        "epoch": 0.43078112915699923,
        "step": 3342
    },
    {
        "loss": 1.6698,
        "grad_norm": 2.5310323238372803,
        "learning_rate": 5.674871436777986e-05,
        "epoch": 0.43091002835782416,
        "step": 3343
    },
    {
        "loss": 1.3343,
        "grad_norm": 2.5946853160858154,
        "learning_rate": 5.669386703095019e-05,
        "epoch": 0.43103892755864914,
        "step": 3344
    },
    {
        "loss": 0.972,
        "grad_norm": 3.0855438709259033,
        "learning_rate": 5.663903572409162e-05,
        "epoch": 0.43116782675947407,
        "step": 3345
    },
    {
        "loss": 1.944,
        "grad_norm": 1.4569679498672485,
        "learning_rate": 5.658422046750015e-05,
        "epoch": 0.43129672596029905,
        "step": 3346
    },
    {
        "loss": 2.1204,
        "grad_norm": 1.96361243724823,
        "learning_rate": 5.6529421281465943e-05,
        "epoch": 0.431425625161124,
        "step": 3347
    },
    {
        "loss": 1.7934,
        "grad_norm": 3.9380807876586914,
        "learning_rate": 5.647463818627319e-05,
        "epoch": 0.43155452436194897,
        "step": 3348
    },
    {
        "loss": 1.9042,
        "grad_norm": 1.6826961040496826,
        "learning_rate": 5.641987120220015e-05,
        "epoch": 0.4316834235627739,
        "step": 3349
    },
    {
        "loss": 1.9728,
        "grad_norm": 1.5376330614089966,
        "learning_rate": 5.6365120349519026e-05,
        "epoch": 0.4318123227635989,
        "step": 3350
    },
    {
        "loss": 1.3092,
        "grad_norm": 3.2393734455108643,
        "learning_rate": 5.6310385648496056e-05,
        "epoch": 0.4319412219644238,
        "step": 3351
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.2358802556991577,
        "learning_rate": 5.6255667119391686e-05,
        "epoch": 0.4320701211652488,
        "step": 3352
    },
    {
        "loss": 1.9615,
        "grad_norm": 1.9352298974990845,
        "learning_rate": 5.620096478246018e-05,
        "epoch": 0.4321990203660737,
        "step": 3353
    },
    {
        "loss": 1.6988,
        "grad_norm": 1.3379714488983154,
        "learning_rate": 5.614627865794984e-05,
        "epoch": 0.4323279195668987,
        "step": 3354
    },
    {
        "loss": 1.8367,
        "grad_norm": 1.999737024307251,
        "learning_rate": 5.609160876610298e-05,
        "epoch": 0.4324568187677236,
        "step": 3355
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.7994452714920044,
        "learning_rate": 5.6036955127155945e-05,
        "epoch": 0.4325857179685486,
        "step": 3356
    },
    {
        "loss": 1.9488,
        "grad_norm": 2.7636547088623047,
        "learning_rate": 5.5982317761339086e-05,
        "epoch": 0.43271461716937354,
        "step": 3357
    },
    {
        "loss": 1.6952,
        "grad_norm": 1.5043119192123413,
        "learning_rate": 5.592769668887661e-05,
        "epoch": 0.4328435163701985,
        "step": 3358
    },
    {
        "loss": 2.2897,
        "grad_norm": 2.1041901111602783,
        "learning_rate": 5.587309192998683e-05,
        "epoch": 0.43297241557102345,
        "step": 3359
    },
    {
        "loss": 1.5702,
        "grad_norm": 2.0948970317840576,
        "learning_rate": 5.581850350488194e-05,
        "epoch": 0.43310131477184843,
        "step": 3360
    },
    {
        "loss": 2.0111,
        "grad_norm": 1.5578192472457886,
        "learning_rate": 5.576393143376805e-05,
        "epoch": 0.43323021397267336,
        "step": 3361
    },
    {
        "loss": 1.5067,
        "grad_norm": 2.1720540523529053,
        "learning_rate": 5.5709375736845315e-05,
        "epoch": 0.43335911317349834,
        "step": 3362
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.6513267755508423,
        "learning_rate": 5.5654836434307775e-05,
        "epoch": 0.4334880123743233,
        "step": 3363
    },
    {
        "loss": 1.8919,
        "grad_norm": 1.9586527347564697,
        "learning_rate": 5.5600313546343474e-05,
        "epoch": 0.43361691157514826,
        "step": 3364
    },
    {
        "loss": 1.7191,
        "grad_norm": 1.8595772981643677,
        "learning_rate": 5.5545807093134225e-05,
        "epoch": 0.4337458107759732,
        "step": 3365
    },
    {
        "loss": 2.7892,
        "grad_norm": 1.8707561492919922,
        "learning_rate": 5.5491317094855935e-05,
        "epoch": 0.43387470997679817,
        "step": 3366
    },
    {
        "loss": 2.1662,
        "grad_norm": 1.7073677778244019,
        "learning_rate": 5.5436843571678286e-05,
        "epoch": 0.4340036091776231,
        "step": 3367
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.8464218378067017,
        "learning_rate": 5.5382386543764844e-05,
        "epoch": 0.4341325083784481,
        "step": 3368
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.072421073913574,
        "learning_rate": 5.532794603127329e-05,
        "epoch": 0.434261407579273,
        "step": 3369
    },
    {
        "loss": 1.2025,
        "grad_norm": 2.38547945022583,
        "learning_rate": 5.5273522054354964e-05,
        "epoch": 0.434390306780098,
        "step": 3370
    },
    {
        "loss": 1.8542,
        "grad_norm": 1.4623146057128906,
        "learning_rate": 5.521911463315512e-05,
        "epoch": 0.4345192059809229,
        "step": 3371
    },
    {
        "loss": 1.816,
        "grad_norm": 1.8082252740859985,
        "learning_rate": 5.516472378781299e-05,
        "epoch": 0.43464810518174785,
        "step": 3372
    },
    {
        "loss": 2.1037,
        "grad_norm": 1.702547311782837,
        "learning_rate": 5.511034953846155e-05,
        "epoch": 0.43477700438257283,
        "step": 3373
    },
    {
        "loss": 2.0089,
        "grad_norm": 2.0227396488189697,
        "learning_rate": 5.5055991905227734e-05,
        "epoch": 0.43490590358339776,
        "step": 3374
    },
    {
        "loss": 1.035,
        "grad_norm": 2.155284881591797,
        "learning_rate": 5.500165090823222e-05,
        "epoch": 0.43503480278422274,
        "step": 3375
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.698843002319336,
        "learning_rate": 5.4947326567589676e-05,
        "epoch": 0.43516370198504767,
        "step": 3376
    },
    {
        "loss": 2.2186,
        "grad_norm": 1.4835009574890137,
        "learning_rate": 5.48930189034084e-05,
        "epoch": 0.43529260118587265,
        "step": 3377
    },
    {
        "loss": 2.2007,
        "grad_norm": 1.8161581754684448,
        "learning_rate": 5.4838727935790746e-05,
        "epoch": 0.4354215003866976,
        "step": 3378
    },
    {
        "loss": 2.3487,
        "grad_norm": 1.3573416471481323,
        "learning_rate": 5.478445368483267e-05,
        "epoch": 0.43555039958752256,
        "step": 3379
    },
    {
        "loss": 1.4859,
        "grad_norm": 2.1987102031707764,
        "learning_rate": 5.473019617062412e-05,
        "epoch": 0.4356792987883475,
        "step": 3380
    },
    {
        "loss": 1.8598,
        "grad_norm": 1.4652563333511353,
        "learning_rate": 5.4675955413248727e-05,
        "epoch": 0.4358081979891725,
        "step": 3381
    },
    {
        "loss": 1.7962,
        "grad_norm": 3.4559412002563477,
        "learning_rate": 5.462173143278404e-05,
        "epoch": 0.4359370971899974,
        "step": 3382
    },
    {
        "loss": 2.0631,
        "grad_norm": 2.7116987705230713,
        "learning_rate": 5.456752424930125e-05,
        "epoch": 0.4360659963908224,
        "step": 3383
    },
    {
        "loss": 2.4742,
        "grad_norm": 1.4216774702072144,
        "learning_rate": 5.4513333882865404e-05,
        "epoch": 0.4361948955916473,
        "step": 3384
    },
    {
        "loss": 1.794,
        "grad_norm": 2.2523162364959717,
        "learning_rate": 5.445916035353533e-05,
        "epoch": 0.4363237947924723,
        "step": 3385
    },
    {
        "loss": 1.5035,
        "grad_norm": 2.312619686126709,
        "learning_rate": 5.440500368136368e-05,
        "epoch": 0.4364526939932972,
        "step": 3386
    },
    {
        "loss": 2.1903,
        "grad_norm": 2.1179404258728027,
        "learning_rate": 5.4350863886396764e-05,
        "epoch": 0.4365815931941222,
        "step": 3387
    },
    {
        "loss": 1.8164,
        "grad_norm": 1.445750117301941,
        "learning_rate": 5.429674098867461e-05,
        "epoch": 0.43671049239494714,
        "step": 3388
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.6678766012191772,
        "learning_rate": 5.424263500823119e-05,
        "epoch": 0.4368393915957721,
        "step": 3389
    },
    {
        "loss": 1.7116,
        "grad_norm": 1.6616075038909912,
        "learning_rate": 5.4188545965093986e-05,
        "epoch": 0.43696829079659705,
        "step": 3390
    },
    {
        "loss": 2.1941,
        "grad_norm": 1.7830674648284912,
        "learning_rate": 5.4134473879284405e-05,
        "epoch": 0.43709718999742203,
        "step": 3391
    },
    {
        "loss": 2.1447,
        "grad_norm": 1.7464627027511597,
        "learning_rate": 5.40804187708174e-05,
        "epoch": 0.43722608919824696,
        "step": 3392
    },
    {
        "loss": 1.4961,
        "grad_norm": 2.4002904891967773,
        "learning_rate": 5.4026380659701806e-05,
        "epoch": 0.43735498839907194,
        "step": 3393
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.8176720142364502,
        "learning_rate": 5.397235956594e-05,
        "epoch": 0.43748388759989687,
        "step": 3394
    },
    {
        "loss": 1.7648,
        "grad_norm": 1.9796563386917114,
        "learning_rate": 5.391835550952821e-05,
        "epoch": 0.43761278680072185,
        "step": 3395
    },
    {
        "loss": 1.2682,
        "grad_norm": 1.9579439163208008,
        "learning_rate": 5.3864368510456306e-05,
        "epoch": 0.4377416860015468,
        "step": 3396
    },
    {
        "loss": 1.7525,
        "grad_norm": 2.229830265045166,
        "learning_rate": 5.381039858870778e-05,
        "epoch": 0.43787058520237176,
        "step": 3397
    },
    {
        "loss": 1.6081,
        "grad_norm": 2.1691455841064453,
        "learning_rate": 5.375644576425987e-05,
        "epoch": 0.4379994844031967,
        "step": 3398
    },
    {
        "loss": 1.227,
        "grad_norm": 1.6679327487945557,
        "learning_rate": 5.370251005708353e-05,
        "epoch": 0.4381283836040217,
        "step": 3399
    },
    {
        "loss": 2.0152,
        "grad_norm": 1.9352244138717651,
        "learning_rate": 5.364859148714327e-05,
        "epoch": 0.4382572828048466,
        "step": 3400
    },
    {
        "loss": 2.0443,
        "grad_norm": 1.2202223539352417,
        "learning_rate": 5.359469007439726e-05,
        "epoch": 0.4383861820056716,
        "step": 3401
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.584731101989746,
        "learning_rate": 5.3540805838797416e-05,
        "epoch": 0.4385150812064965,
        "step": 3402
    },
    {
        "loss": 2.0911,
        "grad_norm": 1.311837911605835,
        "learning_rate": 5.348693880028928e-05,
        "epoch": 0.4386439804073215,
        "step": 3403
    },
    {
        "loss": 2.3239,
        "grad_norm": 1.351978063583374,
        "learning_rate": 5.343308897881191e-05,
        "epoch": 0.4387728796081464,
        "step": 3404
    },
    {
        "loss": 1.9941,
        "grad_norm": 1.7112210988998413,
        "learning_rate": 5.337925639429815e-05,
        "epoch": 0.4389017788089714,
        "step": 3405
    },
    {
        "loss": 2.233,
        "grad_norm": 1.241102933883667,
        "learning_rate": 5.3325441066674356e-05,
        "epoch": 0.43903067800979634,
        "step": 3406
    },
    {
        "loss": 2.4064,
        "grad_norm": 1.9169918298721313,
        "learning_rate": 5.327164301586047e-05,
        "epoch": 0.4391595772106213,
        "step": 3407
    },
    {
        "loss": 2.0612,
        "grad_norm": 1.9798606634140015,
        "learning_rate": 5.321786226177021e-05,
        "epoch": 0.43928847641144625,
        "step": 3408
    },
    {
        "loss": 1.9101,
        "grad_norm": 1.4940247535705566,
        "learning_rate": 5.316409882431066e-05,
        "epoch": 0.4394173756122712,
        "step": 3409
    },
    {
        "loss": 1.9457,
        "grad_norm": 1.6094111204147339,
        "learning_rate": 5.311035272338267e-05,
        "epoch": 0.43954627481309616,
        "step": 3410
    },
    {
        "loss": 2.2737,
        "grad_norm": 1.2187362909317017,
        "learning_rate": 5.305662397888067e-05,
        "epoch": 0.4396751740139211,
        "step": 3411
    },
    {
        "loss": 2.5206,
        "grad_norm": 1.9239717721939087,
        "learning_rate": 5.30029126106925e-05,
        "epoch": 0.43980407321474607,
        "step": 3412
    },
    {
        "loss": 1.6955,
        "grad_norm": 1.908023476600647,
        "learning_rate": 5.294921863869977e-05,
        "epoch": 0.439932972415571,
        "step": 3413
    },
    {
        "loss": 1.6833,
        "grad_norm": 2.392221212387085,
        "learning_rate": 5.289554208277745e-05,
        "epoch": 0.440061871616396,
        "step": 3414
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.2943185567855835,
        "learning_rate": 5.284188296279433e-05,
        "epoch": 0.4401907708172209,
        "step": 3415
    },
    {
        "loss": 2.0121,
        "grad_norm": 2.847036361694336,
        "learning_rate": 5.278824129861248e-05,
        "epoch": 0.4403196700180459,
        "step": 3416
    },
    {
        "loss": 1.9022,
        "grad_norm": 2.0318708419799805,
        "learning_rate": 5.273461711008767e-05,
        "epoch": 0.4404485692188708,
        "step": 3417
    },
    {
        "loss": 1.7551,
        "grad_norm": 1.903376817703247,
        "learning_rate": 5.2681010417069055e-05,
        "epoch": 0.4405774684196958,
        "step": 3418
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.4635534286499023,
        "learning_rate": 5.262742123939948e-05,
        "epoch": 0.44070636762052073,
        "step": 3419
    },
    {
        "loss": 1.5853,
        "grad_norm": 2.005319356918335,
        "learning_rate": 5.257384959691526e-05,
        "epoch": 0.4408352668213457,
        "step": 3420
    },
    {
        "loss": 1.9931,
        "grad_norm": 1.88055419921875,
        "learning_rate": 5.252029550944613e-05,
        "epoch": 0.44096416602217064,
        "step": 3421
    },
    {
        "loss": 2.0389,
        "grad_norm": 1.6244962215423584,
        "learning_rate": 5.246675899681543e-05,
        "epoch": 0.44109306522299563,
        "step": 3422
    },
    {
        "loss": 1.3463,
        "grad_norm": 1.4703394174575806,
        "learning_rate": 5.241324007883997e-05,
        "epoch": 0.44122196442382056,
        "step": 3423
    },
    {
        "loss": 2.0221,
        "grad_norm": 1.6103547811508179,
        "learning_rate": 5.235973877532996e-05,
        "epoch": 0.44135086362464554,
        "step": 3424
    },
    {
        "loss": 1.9901,
        "grad_norm": 1.3035472631454468,
        "learning_rate": 5.2306255106089245e-05,
        "epoch": 0.44147976282547047,
        "step": 3425
    },
    {
        "loss": 1.0783,
        "grad_norm": 2.851125717163086,
        "learning_rate": 5.2252789090914966e-05,
        "epoch": 0.44160866202629545,
        "step": 3426
    },
    {
        "loss": 2.2501,
        "grad_norm": 2.3844025135040283,
        "learning_rate": 5.219934074959789e-05,
        "epoch": 0.4417375612271204,
        "step": 3427
    },
    {
        "loss": 0.3743,
        "grad_norm": 1.3832707405090332,
        "learning_rate": 5.214591010192216e-05,
        "epoch": 0.44186646042794536,
        "step": 3428
    },
    {
        "loss": 0.8507,
        "grad_norm": 2.423353672027588,
        "learning_rate": 5.209249716766543e-05,
        "epoch": 0.4419953596287703,
        "step": 3429
    },
    {
        "loss": 1.8939,
        "grad_norm": 1.2962769269943237,
        "learning_rate": 5.203910196659871e-05,
        "epoch": 0.4421242588295953,
        "step": 3430
    },
    {
        "loss": 1.6187,
        "grad_norm": 1.7827783823013306,
        "learning_rate": 5.19857245184864e-05,
        "epoch": 0.4422531580304202,
        "step": 3431
    },
    {
        "loss": 2.5916,
        "grad_norm": 1.8345997333526611,
        "learning_rate": 5.1932364843086586e-05,
        "epoch": 0.4423820572312452,
        "step": 3432
    },
    {
        "loss": 1.9674,
        "grad_norm": 1.8999546766281128,
        "learning_rate": 5.187902296015053e-05,
        "epoch": 0.4425109564320701,
        "step": 3433
    },
    {
        "loss": 0.9469,
        "grad_norm": 2.7500274181365967,
        "learning_rate": 5.182569888942292e-05,
        "epoch": 0.4426398556328951,
        "step": 3434
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.0945522785186768,
        "learning_rate": 5.177239265064202e-05,
        "epoch": 0.44276875483372,
        "step": 3435
    },
    {
        "loss": 2.4136,
        "grad_norm": 1.4854668378829956,
        "learning_rate": 5.17191042635393e-05,
        "epoch": 0.442897654034545,
        "step": 3436
    },
    {
        "loss": 1.9881,
        "grad_norm": 1.7856192588806152,
        "learning_rate": 5.166583374783978e-05,
        "epoch": 0.44302655323536994,
        "step": 3437
    },
    {
        "loss": 1.8667,
        "grad_norm": 1.5473524332046509,
        "learning_rate": 5.161258112326173e-05,
        "epoch": 0.4431554524361949,
        "step": 3438
    },
    {
        "loss": 0.771,
        "grad_norm": 2.091996431350708,
        "learning_rate": 5.1559346409516915e-05,
        "epoch": 0.44328435163701985,
        "step": 3439
    },
    {
        "loss": 0.8453,
        "grad_norm": 1.9301928281784058,
        "learning_rate": 5.1506129626310416e-05,
        "epoch": 0.44341325083784483,
        "step": 3440
    },
    {
        "loss": 1.4344,
        "grad_norm": 1.4335176944732666,
        "learning_rate": 5.145293079334061e-05,
        "epoch": 0.44354215003866976,
        "step": 3441
    },
    {
        "loss": 2.1378,
        "grad_norm": 1.3176276683807373,
        "learning_rate": 5.1399749930299365e-05,
        "epoch": 0.44367104923949474,
        "step": 3442
    },
    {
        "loss": 1.4095,
        "grad_norm": 1.6264864206314087,
        "learning_rate": 5.134658705687181e-05,
        "epoch": 0.44379994844031967,
        "step": 3443
    },
    {
        "loss": 2.1766,
        "grad_norm": 1.8804364204406738,
        "learning_rate": 5.1293442192736466e-05,
        "epoch": 0.4439288476411446,
        "step": 3444
    },
    {
        "loss": 1.7094,
        "grad_norm": 1.818878173828125,
        "learning_rate": 5.1240315357565085e-05,
        "epoch": 0.4440577468419696,
        "step": 3445
    },
    {
        "loss": 0.9976,
        "grad_norm": 2.320995569229126,
        "learning_rate": 5.118720657102292e-05,
        "epoch": 0.4441866460427945,
        "step": 3446
    },
    {
        "loss": 1.5594,
        "grad_norm": 3.0914909839630127,
        "learning_rate": 5.1134115852768326e-05,
        "epoch": 0.4443155452436195,
        "step": 3447
    },
    {
        "loss": 1.343,
        "grad_norm": 2.162454128265381,
        "learning_rate": 5.108104322245315e-05,
        "epoch": 0.4444444444444444,
        "step": 3448
    },
    {
        "loss": 1.9397,
        "grad_norm": 1.0550785064697266,
        "learning_rate": 5.102798869972252e-05,
        "epoch": 0.4445733436452694,
        "step": 3449
    },
    {
        "loss": 2.148,
        "grad_norm": 1.6410083770751953,
        "learning_rate": 5.0974952304214764e-05,
        "epoch": 0.44470224284609433,
        "step": 3450
    },
    {
        "loss": 1.7466,
        "grad_norm": 1.9497315883636475,
        "learning_rate": 5.092193405556152e-05,
        "epoch": 0.4448311420469193,
        "step": 3451
    },
    {
        "loss": 2.211,
        "grad_norm": 1.8034248352050781,
        "learning_rate": 5.086893397338782e-05,
        "epoch": 0.44496004124774424,
        "step": 3452
    },
    {
        "loss": 2.3134,
        "grad_norm": 1.908706545829773,
        "learning_rate": 5.0815952077311826e-05,
        "epoch": 0.4450889404485692,
        "step": 3453
    },
    {
        "loss": 1.9101,
        "grad_norm": 1.6060919761657715,
        "learning_rate": 5.076298838694513e-05,
        "epoch": 0.44521783964939415,
        "step": 3454
    },
    {
        "loss": 2.3255,
        "grad_norm": 1.5407776832580566,
        "learning_rate": 5.0710042921892386e-05,
        "epoch": 0.44534673885021914,
        "step": 3455
    },
    {
        "loss": 2.328,
        "grad_norm": 2.442359685897827,
        "learning_rate": 5.0657115701751714e-05,
        "epoch": 0.44547563805104406,
        "step": 3456
    },
    {
        "loss": 1.9175,
        "grad_norm": 1.6199569702148438,
        "learning_rate": 5.060420674611428e-05,
        "epoch": 0.44560453725186905,
        "step": 3457
    },
    {
        "loss": 2.2253,
        "grad_norm": 2.0321521759033203,
        "learning_rate": 5.055131607456467e-05,
        "epoch": 0.445733436452694,
        "step": 3458
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.715587854385376,
        "learning_rate": 5.049844370668055e-05,
        "epoch": 0.44586233565351896,
        "step": 3459
    },
    {
        "loss": 2.2947,
        "grad_norm": 1.9801310300827026,
        "learning_rate": 5.04455896620329e-05,
        "epoch": 0.4459912348543439,
        "step": 3460
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.3482508659362793,
        "learning_rate": 5.039275396018591e-05,
        "epoch": 0.44612013405516887,
        "step": 3461
    },
    {
        "loss": 2.0743,
        "grad_norm": 2.0294246673583984,
        "learning_rate": 5.0339936620697e-05,
        "epoch": 0.4462490332559938,
        "step": 3462
    },
    {
        "loss": 1.8008,
        "grad_norm": 2.524491310119629,
        "learning_rate": 5.0287137663116713e-05,
        "epoch": 0.4463779324568188,
        "step": 3463
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.1901137828826904,
        "learning_rate": 5.02343571069888e-05,
        "epoch": 0.4465068316576437,
        "step": 3464
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.777198076248169,
        "learning_rate": 5.018159497185029e-05,
        "epoch": 0.4466357308584687,
        "step": 3465
    },
    {
        "loss": 1.7041,
        "grad_norm": 1.9270148277282715,
        "learning_rate": 5.012885127723135e-05,
        "epoch": 0.4467646300592936,
        "step": 3466
    },
    {
        "loss": 1.1276,
        "grad_norm": 1.3441327810287476,
        "learning_rate": 5.007612604265525e-05,
        "epoch": 0.4468935292601186,
        "step": 3467
    },
    {
        "loss": 2.0271,
        "grad_norm": 1.8038681745529175,
        "learning_rate": 5.0023419287638563e-05,
        "epoch": 0.44702242846094353,
        "step": 3468
    },
    {
        "loss": 2.153,
        "grad_norm": 1.6485790014266968,
        "learning_rate": 4.997073103169089e-05,
        "epoch": 0.4471513276617685,
        "step": 3469
    },
    {
        "loss": 1.4991,
        "grad_norm": 1.5213106870651245,
        "learning_rate": 4.991806129431502e-05,
        "epoch": 0.44728022686259344,
        "step": 3470
    },
    {
        "loss": 2.2269,
        "grad_norm": 2.2283575534820557,
        "learning_rate": 4.986541009500697e-05,
        "epoch": 0.4474091260634184,
        "step": 3471
    },
    {
        "loss": 0.9974,
        "grad_norm": 2.044036626815796,
        "learning_rate": 4.981277745325577e-05,
        "epoch": 0.44753802526424336,
        "step": 3472
    },
    {
        "loss": 1.2531,
        "grad_norm": 2.1754508018493652,
        "learning_rate": 4.97601633885437e-05,
        "epoch": 0.44766692446506834,
        "step": 3473
    },
    {
        "loss": 1.9876,
        "grad_norm": 1.826004981994629,
        "learning_rate": 4.9707567920346056e-05,
        "epoch": 0.44779582366589327,
        "step": 3474
    },
    {
        "loss": 1.9922,
        "grad_norm": 2.7740488052368164,
        "learning_rate": 4.9654991068131305e-05,
        "epoch": 0.44792472286671825,
        "step": 3475
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.0522243976593018,
        "learning_rate": 4.9602432851361095e-05,
        "epoch": 0.4480536220675432,
        "step": 3476
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.1613788604736328,
        "learning_rate": 4.954989328948999e-05,
        "epoch": 0.44818252126836816,
        "step": 3477
    },
    {
        "loss": 1.5738,
        "grad_norm": 2.0123302936553955,
        "learning_rate": 4.949737240196582e-05,
        "epoch": 0.4483114204691931,
        "step": 3478
    },
    {
        "loss": 2.3547,
        "grad_norm": 2.2437922954559326,
        "learning_rate": 4.9444870208229467e-05,
        "epoch": 0.4484403196700181,
        "step": 3479
    },
    {
        "loss": 1.8513,
        "grad_norm": 1.205074429512024,
        "learning_rate": 4.939238672771483e-05,
        "epoch": 0.448569218870843,
        "step": 3480
    },
    {
        "loss": 2.1925,
        "grad_norm": 1.8425610065460205,
        "learning_rate": 4.933992197984889e-05,
        "epoch": 0.44869811807166793,
        "step": 3481
    },
    {
        "loss": 2.1476,
        "grad_norm": 1.8936280012130737,
        "learning_rate": 4.9287475984051745e-05,
        "epoch": 0.4488270172724929,
        "step": 3482
    },
    {
        "loss": 1.4965,
        "grad_norm": 1.967875599861145,
        "learning_rate": 4.923504875973659e-05,
        "epoch": 0.44895591647331784,
        "step": 3483
    },
    {
        "loss": 2.024,
        "grad_norm": 1.4174864292144775,
        "learning_rate": 4.918264032630952e-05,
        "epoch": 0.4490848156741428,
        "step": 3484
    },
    {
        "loss": 1.4263,
        "grad_norm": 1.8619533777236938,
        "learning_rate": 4.913025070316983e-05,
        "epoch": 0.44921371487496775,
        "step": 3485
    },
    {
        "loss": 0.8632,
        "grad_norm": 2.042125940322876,
        "learning_rate": 4.9077879909709776e-05,
        "epoch": 0.44934261407579273,
        "step": 3486
    },
    {
        "loss": 2.1104,
        "grad_norm": 1.4840344190597534,
        "learning_rate": 4.902552796531461e-05,
        "epoch": 0.44947151327661766,
        "step": 3487
    },
    {
        "loss": 1.6445,
        "grad_norm": 2.2391703128814697,
        "learning_rate": 4.8973194889362725e-05,
        "epoch": 0.44960041247744265,
        "step": 3488
    },
    {
        "loss": 1.2602,
        "grad_norm": 2.8821756839752197,
        "learning_rate": 4.8920880701225393e-05,
        "epoch": 0.4497293116782676,
        "step": 3489
    },
    {
        "loss": 2.007,
        "grad_norm": 1.681909441947937,
        "learning_rate": 4.886858542026698e-05,
        "epoch": 0.44985821087909256,
        "step": 3490
    },
    {
        "loss": 1.6745,
        "grad_norm": 3.0526235103607178,
        "learning_rate": 4.8816309065844865e-05,
        "epoch": 0.4499871100799175,
        "step": 3491
    },
    {
        "loss": 1.9974,
        "grad_norm": 2.6640737056732178,
        "learning_rate": 4.876405165730934e-05,
        "epoch": 0.45011600928074247,
        "step": 3492
    },
    {
        "loss": 1.7894,
        "grad_norm": 1.3406176567077637,
        "learning_rate": 4.87118132140038e-05,
        "epoch": 0.4502449084815674,
        "step": 3493
    },
    {
        "loss": 1.8936,
        "grad_norm": 1.260245442390442,
        "learning_rate": 4.8659593755264434e-05,
        "epoch": 0.4503738076823924,
        "step": 3494
    },
    {
        "loss": 1.1059,
        "grad_norm": 2.0990679264068604,
        "learning_rate": 4.8607393300420664e-05,
        "epoch": 0.4505027068832173,
        "step": 3495
    },
    {
        "loss": 2.3797,
        "grad_norm": 1.120278239250183,
        "learning_rate": 4.855521186879468e-05,
        "epoch": 0.4506316060840423,
        "step": 3496
    },
    {
        "loss": 1.4285,
        "grad_norm": 2.39926815032959,
        "learning_rate": 4.8503049479701647e-05,
        "epoch": 0.4507605052848672,
        "step": 3497
    },
    {
        "loss": 1.9824,
        "grad_norm": 1.44842529296875,
        "learning_rate": 4.845090615244979e-05,
        "epoch": 0.4508894044856922,
        "step": 3498
    },
    {
        "loss": 1.1694,
        "grad_norm": 2.4099326133728027,
        "learning_rate": 4.839878190634014e-05,
        "epoch": 0.45101830368651713,
        "step": 3499
    },
    {
        "loss": 2.0099,
        "grad_norm": 2.0790910720825195,
        "learning_rate": 4.834667676066681e-05,
        "epoch": 0.4511472028873421,
        "step": 3500
    },
    {
        "loss": 2.1706,
        "grad_norm": 2.389376401901245,
        "learning_rate": 4.829459073471667e-05,
        "epoch": 0.45127610208816704,
        "step": 3501
    },
    {
        "loss": 1.9343,
        "grad_norm": 1.8230499029159546,
        "learning_rate": 4.8242523847769724e-05,
        "epoch": 0.451405001288992,
        "step": 3502
    },
    {
        "loss": 2.1901,
        "grad_norm": 1.3720148801803589,
        "learning_rate": 4.8190476119098714e-05,
        "epoch": 0.45153390048981695,
        "step": 3503
    },
    {
        "loss": 2.5779,
        "grad_norm": 1.2421847581863403,
        "learning_rate": 4.813844756796931e-05,
        "epoch": 0.45166279969064194,
        "step": 3504
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.1289933919906616,
        "learning_rate": 4.808643821364019e-05,
        "epoch": 0.45179169889146686,
        "step": 3505
    },
    {
        "loss": 2.2698,
        "grad_norm": 1.347109317779541,
        "learning_rate": 4.8034448075362907e-05,
        "epoch": 0.45192059809229185,
        "step": 3506
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.1575980186462402,
        "learning_rate": 4.7982477172381736e-05,
        "epoch": 0.4520494972931168,
        "step": 3507
    },
    {
        "loss": 2.5245,
        "grad_norm": 1.891671895980835,
        "learning_rate": 4.793052552393403e-05,
        "epoch": 0.45217839649394176,
        "step": 3508
    },
    {
        "loss": 2.4929,
        "grad_norm": 1.2318192720413208,
        "learning_rate": 4.787859314924997e-05,
        "epoch": 0.4523072956947667,
        "step": 3509
    },
    {
        "loss": 1.9103,
        "grad_norm": 1.5764760971069336,
        "learning_rate": 4.7826680067552496e-05,
        "epoch": 0.45243619489559167,
        "step": 3510
    },
    {
        "loss": 2.211,
        "grad_norm": 2.7087271213531494,
        "learning_rate": 4.7774786298057515e-05,
        "epoch": 0.4525650940964166,
        "step": 3511
    },
    {
        "loss": 1.6758,
        "grad_norm": 1.548010230064392,
        "learning_rate": 4.7722911859973795e-05,
        "epoch": 0.4526939932972416,
        "step": 3512
    },
    {
        "loss": 1.8853,
        "grad_norm": 1.7941815853118896,
        "learning_rate": 4.767105677250286e-05,
        "epoch": 0.4528228924980665,
        "step": 3513
    },
    {
        "loss": 1.9969,
        "grad_norm": 1.9763261079788208,
        "learning_rate": 4.761922105483909e-05,
        "epoch": 0.4529517916988915,
        "step": 3514
    },
    {
        "loss": 1.0187,
        "grad_norm": 2.3954851627349854,
        "learning_rate": 4.7567404726169795e-05,
        "epoch": 0.4530806908997164,
        "step": 3515
    },
    {
        "loss": 1.7448,
        "grad_norm": 1.7421170473098755,
        "learning_rate": 4.751560780567495e-05,
        "epoch": 0.4532095901005414,
        "step": 3516
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.6628731489181519,
        "learning_rate": 4.7463830312527524e-05,
        "epoch": 0.45333848930136633,
        "step": 3517
    },
    {
        "loss": 1.9628,
        "grad_norm": 2.6021876335144043,
        "learning_rate": 4.741207226589311e-05,
        "epoch": 0.45346738850219126,
        "step": 3518
    },
    {
        "loss": 2.2147,
        "grad_norm": 1.6740541458129883,
        "learning_rate": 4.736033368493028e-05,
        "epoch": 0.45359628770301624,
        "step": 3519
    },
    {
        "loss": 2.5981,
        "grad_norm": 1.5570509433746338,
        "learning_rate": 4.730861458879025e-05,
        "epoch": 0.45372518690384117,
        "step": 3520
    },
    {
        "loss": 0.8952,
        "grad_norm": 2.005974054336548,
        "learning_rate": 4.725691499661714e-05,
        "epoch": 0.45385408610466615,
        "step": 3521
    },
    {
        "loss": 1.9404,
        "grad_norm": 1.5259023904800415,
        "learning_rate": 4.7205234927547745e-05,
        "epoch": 0.4539829853054911,
        "step": 3522
    },
    {
        "loss": 1.9286,
        "grad_norm": 1.8156033754348755,
        "learning_rate": 4.715357440071172e-05,
        "epoch": 0.45411188450631607,
        "step": 3523
    },
    {
        "loss": 1.7281,
        "grad_norm": 1.8651704788208008,
        "learning_rate": 4.710193343523148e-05,
        "epoch": 0.454240783707141,
        "step": 3524
    },
    {
        "loss": 1.537,
        "grad_norm": 2.146942615509033,
        "learning_rate": 4.705031205022212e-05,
        "epoch": 0.454369682907966,
        "step": 3525
    },
    {
        "loss": 2.3428,
        "grad_norm": 1.5935914516448975,
        "learning_rate": 4.6998710264791615e-05,
        "epoch": 0.4544985821087909,
        "step": 3526
    },
    {
        "loss": 1.9442,
        "grad_norm": 1.5136513710021973,
        "learning_rate": 4.694712809804051e-05,
        "epoch": 0.4546274813096159,
        "step": 3527
    },
    {
        "loss": 2.0326,
        "grad_norm": 2.5535950660705566,
        "learning_rate": 4.689556556906226e-05,
        "epoch": 0.4547563805104408,
        "step": 3528
    },
    {
        "loss": 1.6183,
        "grad_norm": 2.2012901306152344,
        "learning_rate": 4.684402269694299e-05,
        "epoch": 0.4548852797112658,
        "step": 3529
    },
    {
        "loss": 1.3368,
        "grad_norm": 1.8562759160995483,
        "learning_rate": 4.679249950076149e-05,
        "epoch": 0.4550141789120907,
        "step": 3530
    },
    {
        "loss": 1.4566,
        "grad_norm": 1.5344905853271484,
        "learning_rate": 4.6740995999589366e-05,
        "epoch": 0.4551430781129157,
        "step": 3531
    },
    {
        "loss": 1.4517,
        "grad_norm": 2.424163579940796,
        "learning_rate": 4.668951221249086e-05,
        "epoch": 0.45527197731374064,
        "step": 3532
    },
    {
        "loss": 2.405,
        "grad_norm": 2.354323625564575,
        "learning_rate": 4.663804815852291e-05,
        "epoch": 0.4554008765145656,
        "step": 3533
    },
    {
        "loss": 1.6499,
        "grad_norm": 2.2682762145996094,
        "learning_rate": 4.6586603856735236e-05,
        "epoch": 0.45552977571539055,
        "step": 3534
    },
    {
        "loss": 2.3361,
        "grad_norm": 1.9769790172576904,
        "learning_rate": 4.653517932617013e-05,
        "epoch": 0.45565867491621553,
        "step": 3535
    },
    {
        "loss": 2.1532,
        "grad_norm": 2.5705952644348145,
        "learning_rate": 4.64837745858627e-05,
        "epoch": 0.45578757411704046,
        "step": 3536
    },
    {
        "loss": 1.754,
        "grad_norm": 1.1394520998001099,
        "learning_rate": 4.643238965484058e-05,
        "epoch": 0.45591647331786544,
        "step": 3537
    },
    {
        "loss": 1.3088,
        "grad_norm": 2.2871673107147217,
        "learning_rate": 4.638102455212418e-05,
        "epoch": 0.4560453725186904,
        "step": 3538
    },
    {
        "loss": 1.8938,
        "grad_norm": 1.7065544128417969,
        "learning_rate": 4.632967929672657e-05,
        "epoch": 0.45617427171951536,
        "step": 3539
    },
    {
        "loss": 1.9245,
        "grad_norm": 1.4164198637008667,
        "learning_rate": 4.627835390765338e-05,
        "epoch": 0.4563031709203403,
        "step": 3540
    },
    {
        "loss": 1.8074,
        "grad_norm": 2.2888238430023193,
        "learning_rate": 4.622704840390297e-05,
        "epoch": 0.45643207012116527,
        "step": 3541
    },
    {
        "loss": 1.9828,
        "grad_norm": 1.6425020694732666,
        "learning_rate": 4.617576280446637e-05,
        "epoch": 0.4565609693219902,
        "step": 3542
    },
    {
        "loss": 1.8871,
        "grad_norm": 1.936354398727417,
        "learning_rate": 4.612449712832714e-05,
        "epoch": 0.4566898685228152,
        "step": 3543
    },
    {
        "loss": 2.1866,
        "grad_norm": 1.8230130672454834,
        "learning_rate": 4.607325139446147e-05,
        "epoch": 0.4568187677236401,
        "step": 3544
    },
    {
        "loss": 1.4768,
        "grad_norm": 1.9733622074127197,
        "learning_rate": 4.602202562183827e-05,
        "epoch": 0.4569476669244651,
        "step": 3545
    },
    {
        "loss": 2.3737,
        "grad_norm": 2.447892189025879,
        "learning_rate": 4.597081982941902e-05,
        "epoch": 0.45707656612529,
        "step": 3546
    },
    {
        "loss": 1.715,
        "grad_norm": 2.182948589324951,
        "learning_rate": 4.591963403615772e-05,
        "epoch": 0.457205465326115,
        "step": 3547
    },
    {
        "loss": 1.4081,
        "grad_norm": 2.40122389793396,
        "learning_rate": 4.5868468261001116e-05,
        "epoch": 0.45733436452693993,
        "step": 3548
    },
    {
        "loss": 1.8573,
        "grad_norm": 1.454037070274353,
        "learning_rate": 4.581732252288842e-05,
        "epoch": 0.4574632637277649,
        "step": 3549
    },
    {
        "loss": 1.6342,
        "grad_norm": 3.868443012237549,
        "learning_rate": 4.576619684075142e-05,
        "epoch": 0.45759216292858984,
        "step": 3550
    },
    {
        "loss": 2.0456,
        "grad_norm": 1.17158043384552,
        "learning_rate": 4.571509123351463e-05,
        "epoch": 0.4577210621294148,
        "step": 3551
    },
    {
        "loss": 1.4669,
        "grad_norm": 1.5910804271697998,
        "learning_rate": 4.566400572009495e-05,
        "epoch": 0.45784996133023975,
        "step": 3552
    },
    {
        "loss": 1.844,
        "grad_norm": 2.3459575176239014,
        "learning_rate": 4.5612940319401983e-05,
        "epoch": 0.4579788605310647,
        "step": 3553
    },
    {
        "loss": 1.6167,
        "grad_norm": 2.019970655441284,
        "learning_rate": 4.556189505033779e-05,
        "epoch": 0.45810775973188966,
        "step": 3554
    },
    {
        "loss": 2.4136,
        "grad_norm": 1.7074521780014038,
        "learning_rate": 4.551086993179702e-05,
        "epoch": 0.4582366589327146,
        "step": 3555
    },
    {
        "loss": 2.0265,
        "grad_norm": 2.1668379306793213,
        "learning_rate": 4.545986498266692e-05,
        "epoch": 0.4583655581335396,
        "step": 3556
    },
    {
        "loss": 2.2786,
        "grad_norm": 1.8470550775527954,
        "learning_rate": 4.5408880221827145e-05,
        "epoch": 0.4584944573343645,
        "step": 3557
    },
    {
        "loss": 2.3803,
        "grad_norm": 1.2124446630477905,
        "learning_rate": 4.535791566814997e-05,
        "epoch": 0.4586233565351895,
        "step": 3558
    },
    {
        "loss": 1.4462,
        "grad_norm": 2.1372358798980713,
        "learning_rate": 4.530697134050022e-05,
        "epoch": 0.4587522557360144,
        "step": 3559
    },
    {
        "loss": 1.947,
        "grad_norm": 1.6163371801376343,
        "learning_rate": 4.525604725773508e-05,
        "epoch": 0.4588811549368394,
        "step": 3560
    },
    {
        "loss": 1.5431,
        "grad_norm": 2.0284101963043213,
        "learning_rate": 4.520514343870445e-05,
        "epoch": 0.4590100541376643,
        "step": 3561
    },
    {
        "loss": 1.437,
        "grad_norm": 1.9144121408462524,
        "learning_rate": 4.515425990225052e-05,
        "epoch": 0.4591389533384893,
        "step": 3562
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.8570189476013184,
        "learning_rate": 4.510339666720815e-05,
        "epoch": 0.45926785253931424,
        "step": 3563
    },
    {
        "loss": 1.5809,
        "grad_norm": 2.0813238620758057,
        "learning_rate": 4.505255375240455e-05,
        "epoch": 0.4593967517401392,
        "step": 3564
    },
    {
        "loss": 1.9026,
        "grad_norm": 1.5249794721603394,
        "learning_rate": 4.500173117665952e-05,
        "epoch": 0.45952565094096415,
        "step": 3565
    },
    {
        "loss": 1.0794,
        "grad_norm": 2.399059772491455,
        "learning_rate": 4.4950928958785263e-05,
        "epoch": 0.45965455014178913,
        "step": 3566
    },
    {
        "loss": 2.0649,
        "grad_norm": 1.591582179069519,
        "learning_rate": 4.490014711758641e-05,
        "epoch": 0.45978344934261406,
        "step": 3567
    },
    {
        "loss": 1.7449,
        "grad_norm": 1.5518513917922974,
        "learning_rate": 4.484938567186018e-05,
        "epoch": 0.45991234854343904,
        "step": 3568
    },
    {
        "loss": 2.0319,
        "grad_norm": 1.6976035833358765,
        "learning_rate": 4.4798644640396084e-05,
        "epoch": 0.46004124774426397,
        "step": 3569
    },
    {
        "loss": 2.5733,
        "grad_norm": 1.953662633895874,
        "learning_rate": 4.47479240419762e-05,
        "epoch": 0.46017014694508895,
        "step": 3570
    },
    {
        "loss": 1.4831,
        "grad_norm": 1.9528629779815674,
        "learning_rate": 4.4697223895374994e-05,
        "epoch": 0.4602990461459139,
        "step": 3571
    },
    {
        "loss": 2.4397,
        "grad_norm": 1.4531986713409424,
        "learning_rate": 4.4646544219359396e-05,
        "epoch": 0.46042794534673886,
        "step": 3572
    },
    {
        "loss": 2.0364,
        "grad_norm": 1.4407777786254883,
        "learning_rate": 4.459588503268868e-05,
        "epoch": 0.4605568445475638,
        "step": 3573
    },
    {
        "loss": 2.0128,
        "grad_norm": 1.9190378189086914,
        "learning_rate": 4.45452463541146e-05,
        "epoch": 0.4606857437483888,
        "step": 3574
    },
    {
        "loss": 1.7379,
        "grad_norm": 1.8155757188796997,
        "learning_rate": 4.449462820238135e-05,
        "epoch": 0.4608146429492137,
        "step": 3575
    },
    {
        "loss": 1.9605,
        "grad_norm": 2.2893221378326416,
        "learning_rate": 4.444403059622544e-05,
        "epoch": 0.4609435421500387,
        "step": 3576
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.718542456626892,
        "learning_rate": 4.4393453554375765e-05,
        "epoch": 0.4610724413508636,
        "step": 3577
    },
    {
        "loss": 2.2919,
        "grad_norm": 2.2577693462371826,
        "learning_rate": 4.434289709555376e-05,
        "epoch": 0.4612013405516886,
        "step": 3578
    },
    {
        "loss": 1.9447,
        "grad_norm": 2.640941619873047,
        "learning_rate": 4.4292361238473036e-05,
        "epoch": 0.4613302397525135,
        "step": 3579
    },
    {
        "loss": 2.1447,
        "grad_norm": 1.4637010097503662,
        "learning_rate": 4.424184600183977e-05,
        "epoch": 0.4614591389533385,
        "step": 3580
    },
    {
        "loss": 2.0848,
        "grad_norm": 1.6533931493759155,
        "learning_rate": 4.4191351404352344e-05,
        "epoch": 0.46158803815416344,
        "step": 3581
    },
    {
        "loss": 1.9217,
        "grad_norm": 2.1294515132904053,
        "learning_rate": 4.4140877464701635e-05,
        "epoch": 0.4617169373549884,
        "step": 3582
    },
    {
        "loss": 1.4647,
        "grad_norm": 2.299744129180908,
        "learning_rate": 4.4090424201570754e-05,
        "epoch": 0.46184583655581335,
        "step": 3583
    },
    {
        "loss": 1.9007,
        "grad_norm": 1.6219252347946167,
        "learning_rate": 4.403999163363528e-05,
        "epoch": 0.46197473575663833,
        "step": 3584
    },
    {
        "loss": 2.0079,
        "grad_norm": 1.4778833389282227,
        "learning_rate": 4.3989579779563e-05,
        "epoch": 0.46210363495746326,
        "step": 3585
    },
    {
        "loss": 1.3108,
        "grad_norm": 2.1021475791931152,
        "learning_rate": 4.393918865801417e-05,
        "epoch": 0.46223253415828824,
        "step": 3586
    },
    {
        "loss": 1.5429,
        "grad_norm": 2.1683554649353027,
        "learning_rate": 4.388881828764123e-05,
        "epoch": 0.46236143335911317,
        "step": 3587
    },
    {
        "loss": 2.0762,
        "grad_norm": 2.258500576019287,
        "learning_rate": 4.3838468687089054e-05,
        "epoch": 0.46249033255993816,
        "step": 3588
    },
    {
        "loss": 2.2354,
        "grad_norm": 2.322067975997925,
        "learning_rate": 4.3788139874994814e-05,
        "epoch": 0.4626192317607631,
        "step": 3589
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.7719366550445557,
        "learning_rate": 4.37378318699879e-05,
        "epoch": 0.462748130961588,
        "step": 3590
    },
    {
        "loss": 2.0601,
        "grad_norm": 2.4096105098724365,
        "learning_rate": 4.368754469069008e-05,
        "epoch": 0.462877030162413,
        "step": 3591
    },
    {
        "loss": 1.9717,
        "grad_norm": 1.235162615776062,
        "learning_rate": 4.3637278355715426e-05,
        "epoch": 0.4630059293632379,
        "step": 3592
    },
    {
        "loss": 1.4267,
        "grad_norm": 2.297987937927246,
        "learning_rate": 4.358703288367022e-05,
        "epoch": 0.4631348285640629,
        "step": 3593
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.6325088739395142,
        "learning_rate": 4.35368082931531e-05,
        "epoch": 0.46326372776488783,
        "step": 3594
    },
    {
        "loss": 1.449,
        "grad_norm": 2.3177621364593506,
        "learning_rate": 4.348660460275491e-05,
        "epoch": 0.4633926269657128,
        "step": 3595
    },
    {
        "loss": 1.6311,
        "grad_norm": 1.9632484912872314,
        "learning_rate": 4.343642183105876e-05,
        "epoch": 0.46352152616653775,
        "step": 3596
    },
    {
        "loss": 2.4806,
        "grad_norm": 1.4253016710281372,
        "learning_rate": 4.338625999664011e-05,
        "epoch": 0.46365042536736273,
        "step": 3597
    },
    {
        "loss": 2.0112,
        "grad_norm": 2.184619665145874,
        "learning_rate": 4.333611911806652e-05,
        "epoch": 0.46377932456818766,
        "step": 3598
    },
    {
        "loss": 1.8573,
        "grad_norm": 1.8036881685256958,
        "learning_rate": 4.328599921389795e-05,
        "epoch": 0.46390822376901264,
        "step": 3599
    },
    {
        "loss": 2.0561,
        "grad_norm": 1.4246174097061157,
        "learning_rate": 4.323590030268646e-05,
        "epoch": 0.46403712296983757,
        "step": 3600
    },
    {
        "loss": 2.0783,
        "grad_norm": 1.953467845916748,
        "learning_rate": 4.318582240297646e-05,
        "epoch": 0.46416602217066255,
        "step": 3601
    },
    {
        "loss": 1.595,
        "grad_norm": 1.830606460571289,
        "learning_rate": 4.313576553330445e-05,
        "epoch": 0.4642949213714875,
        "step": 3602
    },
    {
        "loss": 1.4873,
        "grad_norm": 2.344792604446411,
        "learning_rate": 4.308572971219926e-05,
        "epoch": 0.46442382057231246,
        "step": 3603
    },
    {
        "loss": 2.1055,
        "grad_norm": 2.015608072280884,
        "learning_rate": 4.3035714958181895e-05,
        "epoch": 0.4645527197731374,
        "step": 3604
    },
    {
        "loss": 1.9106,
        "grad_norm": 1.782077431678772,
        "learning_rate": 4.298572128976557e-05,
        "epoch": 0.4646816189739624,
        "step": 3605
    },
    {
        "loss": 2.0236,
        "grad_norm": 1.4917484521865845,
        "learning_rate": 4.293574872545566e-05,
        "epoch": 0.4648105181747873,
        "step": 3606
    },
    {
        "loss": 0.8653,
        "grad_norm": 2.0198779106140137,
        "learning_rate": 4.288579728374971e-05,
        "epoch": 0.4649394173756123,
        "step": 3607
    },
    {
        "loss": 2.1694,
        "grad_norm": 1.4239519834518433,
        "learning_rate": 4.2835866983137496e-05,
        "epoch": 0.4650683165764372,
        "step": 3608
    },
    {
        "loss": 1.9971,
        "grad_norm": 2.151181936264038,
        "learning_rate": 4.278595784210103e-05,
        "epoch": 0.4651972157772622,
        "step": 3609
    },
    {
        "loss": 2.0883,
        "grad_norm": 2.221008062362671,
        "learning_rate": 4.2736069879114314e-05,
        "epoch": 0.4653261149780871,
        "step": 3610
    },
    {
        "loss": 2.4051,
        "grad_norm": 2.008256435394287,
        "learning_rate": 4.268620311264372e-05,
        "epoch": 0.4654550141789121,
        "step": 3611
    },
    {
        "loss": 1.4745,
        "grad_norm": 2.0944724082946777,
        "learning_rate": 4.263635756114759e-05,
        "epoch": 0.46558391337973704,
        "step": 3612
    },
    {
        "loss": 1.6382,
        "grad_norm": 2.505068063735962,
        "learning_rate": 4.258653324307649e-05,
        "epoch": 0.465712812580562,
        "step": 3613
    },
    {
        "loss": 1.9832,
        "grad_norm": 1.3874982595443726,
        "learning_rate": 4.2536730176873196e-05,
        "epoch": 0.46584171178138695,
        "step": 3614
    },
    {
        "loss": 1.6209,
        "grad_norm": 1.8570531606674194,
        "learning_rate": 4.248694838097246e-05,
        "epoch": 0.46597061098221193,
        "step": 3615
    },
    {
        "loss": 2.197,
        "grad_norm": 1.4397343397140503,
        "learning_rate": 4.2437187873801335e-05,
        "epoch": 0.46609951018303686,
        "step": 3616
    },
    {
        "loss": 1.7074,
        "grad_norm": 1.8525139093399048,
        "learning_rate": 4.238744867377884e-05,
        "epoch": 0.46622840938386184,
        "step": 3617
    },
    {
        "loss": 2.1954,
        "grad_norm": 2.598088264465332,
        "learning_rate": 4.23377307993162e-05,
        "epoch": 0.46635730858468677,
        "step": 3618
    },
    {
        "loss": 1.7942,
        "grad_norm": 1.2585643529891968,
        "learning_rate": 4.228803426881678e-05,
        "epoch": 0.46648620778551175,
        "step": 3619
    },
    {
        "loss": 1.69,
        "grad_norm": 2.5632128715515137,
        "learning_rate": 4.223835910067588e-05,
        "epoch": 0.4666151069863367,
        "step": 3620
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.9302036762237549,
        "learning_rate": 4.218870531328108e-05,
        "epoch": 0.46674400618716166,
        "step": 3621
    },
    {
        "loss": 0.4822,
        "grad_norm": 2.4291493892669678,
        "learning_rate": 4.213907292501197e-05,
        "epoch": 0.4668729053879866,
        "step": 3622
    },
    {
        "loss": 2.0833,
        "grad_norm": 1.6109341382980347,
        "learning_rate": 4.208946195424015e-05,
        "epoch": 0.4670018045888116,
        "step": 3623
    },
    {
        "loss": 2.3703,
        "grad_norm": 2.1763241291046143,
        "learning_rate": 4.203987241932943e-05,
        "epoch": 0.4671307037896365,
        "step": 3624
    },
    {
        "loss": 1.31,
        "grad_norm": 2.3940670490264893,
        "learning_rate": 4.199030433863555e-05,
        "epoch": 0.4672596029904615,
        "step": 3625
    },
    {
        "loss": 2.3603,
        "grad_norm": 1.272523283958435,
        "learning_rate": 4.194075773050643e-05,
        "epoch": 0.4673885021912864,
        "step": 3626
    },
    {
        "loss": 2.0311,
        "grad_norm": 1.0992662906646729,
        "learning_rate": 4.1891232613281904e-05,
        "epoch": 0.46751740139211134,
        "step": 3627
    },
    {
        "loss": 2.2829,
        "grad_norm": 1.174959421157837,
        "learning_rate": 4.184172900529402e-05,
        "epoch": 0.4676463005929363,
        "step": 3628
    },
    {
        "loss": 2.5591,
        "grad_norm": 1.4482531547546387,
        "learning_rate": 4.179224692486673e-05,
        "epoch": 0.46777519979376125,
        "step": 3629
    },
    {
        "loss": 2.1262,
        "grad_norm": 1.5225112438201904,
        "learning_rate": 4.1742786390316026e-05,
        "epoch": 0.46790409899458624,
        "step": 3630
    },
    {
        "loss": 1.387,
        "grad_norm": 2.2471697330474854,
        "learning_rate": 4.169334741995002e-05,
        "epoch": 0.46803299819541117,
        "step": 3631
    },
    {
        "loss": 2.5026,
        "grad_norm": 1.7430589199066162,
        "learning_rate": 4.164393003206873e-05,
        "epoch": 0.46816189739623615,
        "step": 3632
    },
    {
        "loss": 1.1445,
        "grad_norm": 1.9152988195419312,
        "learning_rate": 4.1594534244964276e-05,
        "epoch": 0.4682907965970611,
        "step": 3633
    },
    {
        "loss": 2.2326,
        "grad_norm": 1.6533966064453125,
        "learning_rate": 4.154516007692075e-05,
        "epoch": 0.46841969579788606,
        "step": 3634
    },
    {
        "loss": 1.698,
        "grad_norm": 1.6455079317092896,
        "learning_rate": 4.149580754621419e-05,
        "epoch": 0.468548594998711,
        "step": 3635
    },
    {
        "loss": 1.8064,
        "grad_norm": 1.945138692855835,
        "learning_rate": 4.14464766711127e-05,
        "epoch": 0.46867749419953597,
        "step": 3636
    },
    {
        "loss": 2.4558,
        "grad_norm": 1.6769905090332031,
        "learning_rate": 4.139716746987633e-05,
        "epoch": 0.4688063934003609,
        "step": 3637
    },
    {
        "loss": 1.6807,
        "grad_norm": 1.3907063007354736,
        "learning_rate": 4.134787996075715e-05,
        "epoch": 0.4689352926011859,
        "step": 3638
    },
    {
        "loss": 2.3925,
        "grad_norm": 1.774506688117981,
        "learning_rate": 4.129861416199914e-05,
        "epoch": 0.4690641918020108,
        "step": 3639
    },
    {
        "loss": 2.0685,
        "grad_norm": 1.8486822843551636,
        "learning_rate": 4.1249370091838236e-05,
        "epoch": 0.4691930910028358,
        "step": 3640
    },
    {
        "loss": 0.8046,
        "grad_norm": 2.341763734817505,
        "learning_rate": 4.120014776850243e-05,
        "epoch": 0.4693219902036607,
        "step": 3641
    },
    {
        "loss": 1.9315,
        "grad_norm": 1.5631368160247803,
        "learning_rate": 4.115094721021152e-05,
        "epoch": 0.4694508894044857,
        "step": 3642
    },
    {
        "loss": 1.6473,
        "grad_norm": 1.9003660678863525,
        "learning_rate": 4.110176843517741e-05,
        "epoch": 0.46957978860531063,
        "step": 3643
    },
    {
        "loss": 2.154,
        "grad_norm": 1.5822542905807495,
        "learning_rate": 4.105261146160378e-05,
        "epoch": 0.4697086878061356,
        "step": 3644
    },
    {
        "loss": 2.2803,
        "grad_norm": 1.7852779626846313,
        "learning_rate": 4.1003476307686394e-05,
        "epoch": 0.46983758700696054,
        "step": 3645
    },
    {
        "loss": 1.0313,
        "grad_norm": 2.594741106033325,
        "learning_rate": 4.0954362991612796e-05,
        "epoch": 0.4699664862077855,
        "step": 3646
    },
    {
        "loss": 2.2745,
        "grad_norm": 1.4086599349975586,
        "learning_rate": 4.090527153156256e-05,
        "epoch": 0.47009538540861046,
        "step": 3647
    },
    {
        "loss": 1.9509,
        "grad_norm": 2.545043468475342,
        "learning_rate": 4.0856201945707115e-05,
        "epoch": 0.47022428460943544,
        "step": 3648
    },
    {
        "loss": 2.3964,
        "grad_norm": 0.957934558391571,
        "learning_rate": 4.080715425220976e-05,
        "epoch": 0.47035318381026037,
        "step": 3649
    },
    {
        "loss": 2.014,
        "grad_norm": 2.0549674034118652,
        "learning_rate": 4.0758128469225755e-05,
        "epoch": 0.47048208301108535,
        "step": 3650
    },
    {
        "loss": 1.8299,
        "grad_norm": 2.128748893737793,
        "learning_rate": 4.070912461490224e-05,
        "epoch": 0.4706109822119103,
        "step": 3651
    },
    {
        "loss": 2.288,
        "grad_norm": 1.9304182529449463,
        "learning_rate": 4.066014270737826e-05,
        "epoch": 0.47073988141273526,
        "step": 3652
    },
    {
        "loss": 1.3771,
        "grad_norm": 2.5899152755737305,
        "learning_rate": 4.061118276478462e-05,
        "epoch": 0.4708687806135602,
        "step": 3653
    },
    {
        "loss": 2.0158,
        "grad_norm": 2.756335735321045,
        "learning_rate": 4.056224480524412e-05,
        "epoch": 0.4709976798143852,
        "step": 3654
    },
    {
        "loss": 1.8301,
        "grad_norm": 1.0471488237380981,
        "learning_rate": 4.0513328846871415e-05,
        "epoch": 0.4711265790152101,
        "step": 3655
    },
    {
        "loss": 1.2863,
        "grad_norm": 2.160677909851074,
        "learning_rate": 4.0464434907772885e-05,
        "epoch": 0.4712554782160351,
        "step": 3656
    },
    {
        "loss": 1.9113,
        "grad_norm": 1.5563585758209229,
        "learning_rate": 4.041556300604697e-05,
        "epoch": 0.47138437741686,
        "step": 3657
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.955777645111084,
        "learning_rate": 4.0366713159783756e-05,
        "epoch": 0.471513276617685,
        "step": 3658
    },
    {
        "loss": 1.6403,
        "grad_norm": 2.0934524536132812,
        "learning_rate": 4.031788538706523e-05,
        "epoch": 0.4716421758185099,
        "step": 3659
    },
    {
        "loss": 1.4738,
        "grad_norm": 3.1764445304870605,
        "learning_rate": 4.0269079705965315e-05,
        "epoch": 0.4717710750193349,
        "step": 3660
    },
    {
        "loss": 1.7322,
        "grad_norm": 2.0039682388305664,
        "learning_rate": 4.0220296134549565e-05,
        "epoch": 0.47189997422015983,
        "step": 3661
    },
    {
        "loss": 2.0418,
        "grad_norm": 1.9439129829406738,
        "learning_rate": 4.0171534690875525e-05,
        "epoch": 0.47202887342098476,
        "step": 3662
    },
    {
        "loss": 2.3168,
        "grad_norm": 1.318825125694275,
        "learning_rate": 4.0122795392992416e-05,
        "epoch": 0.47215777262180975,
        "step": 3663
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.140349864959717,
        "learning_rate": 4.00740782589414e-05,
        "epoch": 0.4722866718226347,
        "step": 3664
    },
    {
        "loss": 1.2225,
        "grad_norm": 2.282702922821045,
        "learning_rate": 4.002538330675526e-05,
        "epoch": 0.47241557102345966,
        "step": 3665
    },
    {
        "loss": 1.4715,
        "grad_norm": 1.2978562116622925,
        "learning_rate": 3.997671055445871e-05,
        "epoch": 0.4725444702242846,
        "step": 3666
    },
    {
        "loss": 1.7208,
        "grad_norm": 1.4866198301315308,
        "learning_rate": 3.992806002006825e-05,
        "epoch": 0.47267336942510957,
        "step": 3667
    },
    {
        "loss": 1.827,
        "grad_norm": 2.0939154624938965,
        "learning_rate": 3.987943172159201e-05,
        "epoch": 0.4728022686259345,
        "step": 3668
    },
    {
        "loss": 2.2994,
        "grad_norm": 1.2002019882202148,
        "learning_rate": 3.983082567703008e-05,
        "epoch": 0.4729311678267595,
        "step": 3669
    },
    {
        "loss": 2.1018,
        "grad_norm": 2.218325138092041,
        "learning_rate": 3.978224190437413e-05,
        "epoch": 0.4730600670275844,
        "step": 3670
    },
    {
        "loss": 1.7641,
        "grad_norm": 1.0201011896133423,
        "learning_rate": 3.973368042160773e-05,
        "epoch": 0.4731889662284094,
        "step": 3671
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.1108200550079346,
        "learning_rate": 3.968514124670616e-05,
        "epoch": 0.4733178654292343,
        "step": 3672
    },
    {
        "loss": 1.4216,
        "grad_norm": 2.481297492980957,
        "learning_rate": 3.963662439763637e-05,
        "epoch": 0.4734467646300593,
        "step": 3673
    },
    {
        "loss": 2.1367,
        "grad_norm": 1.4030036926269531,
        "learning_rate": 3.958812989235716e-05,
        "epoch": 0.47357566383088423,
        "step": 3674
    },
    {
        "loss": 2.4266,
        "grad_norm": 1.3311609029769897,
        "learning_rate": 3.953965774881897e-05,
        "epoch": 0.4737045630317092,
        "step": 3675
    },
    {
        "loss": 2.2318,
        "grad_norm": 2.068305730819702,
        "learning_rate": 3.9491207984963973e-05,
        "epoch": 0.47383346223253414,
        "step": 3676
    },
    {
        "loss": 2.1141,
        "grad_norm": 1.3777029514312744,
        "learning_rate": 3.9442780618726126e-05,
        "epoch": 0.4739623614333591,
        "step": 3677
    },
    {
        "loss": 1.9382,
        "grad_norm": 2.222184181213379,
        "learning_rate": 3.9394375668031e-05,
        "epoch": 0.47409126063418405,
        "step": 3678
    },
    {
        "loss": 2.254,
        "grad_norm": 1.6834207773208618,
        "learning_rate": 3.934599315079598e-05,
        "epoch": 0.47422015983500904,
        "step": 3679
    },
    {
        "loss": 2.091,
        "grad_norm": 1.4849073886871338,
        "learning_rate": 3.9297633084930006e-05,
        "epoch": 0.47434905903583396,
        "step": 3680
    },
    {
        "loss": 1.5312,
        "grad_norm": 1.994897484779358,
        "learning_rate": 3.9249295488333884e-05,
        "epoch": 0.47447795823665895,
        "step": 3681
    },
    {
        "loss": 2.1183,
        "grad_norm": 2.14516019821167,
        "learning_rate": 3.92009803788999e-05,
        "epoch": 0.4746068574374839,
        "step": 3682
    },
    {
        "loss": 1.5878,
        "grad_norm": 2.4815165996551514,
        "learning_rate": 3.9152687774512175e-05,
        "epoch": 0.47473575663830886,
        "step": 3683
    },
    {
        "loss": 2.0541,
        "grad_norm": 1.2473360300064087,
        "learning_rate": 3.910441769304645e-05,
        "epoch": 0.4748646558391338,
        "step": 3684
    },
    {
        "loss": 1.6508,
        "grad_norm": 2.0736470222473145,
        "learning_rate": 3.905617015237014e-05,
        "epoch": 0.47499355503995877,
        "step": 3685
    },
    {
        "loss": 2.1009,
        "grad_norm": 1.1791410446166992,
        "learning_rate": 3.9007945170342255e-05,
        "epoch": 0.4751224542407837,
        "step": 3686
    },
    {
        "loss": 2.2925,
        "grad_norm": 1.6354104280471802,
        "learning_rate": 3.8959742764813554e-05,
        "epoch": 0.4752513534416087,
        "step": 3687
    },
    {
        "loss": 1.9217,
        "grad_norm": 2.3447909355163574,
        "learning_rate": 3.8911562953626336e-05,
        "epoch": 0.4753802526424336,
        "step": 3688
    },
    {
        "loss": 1.68,
        "grad_norm": 2.4390945434570312,
        "learning_rate": 3.8863405754614644e-05,
        "epoch": 0.4755091518432586,
        "step": 3689
    },
    {
        "loss": 1.6248,
        "grad_norm": 1.6582746505737305,
        "learning_rate": 3.8815271185604016e-05,
        "epoch": 0.4756380510440835,
        "step": 3690
    },
    {
        "loss": 0.9,
        "grad_norm": 2.136042594909668,
        "learning_rate": 3.876715926441178e-05,
        "epoch": 0.4757669502449085,
        "step": 3691
    },
    {
        "loss": 2.1303,
        "grad_norm": 1.5497760772705078,
        "learning_rate": 3.871907000884675e-05,
        "epoch": 0.47589584944573343,
        "step": 3692
    },
    {
        "loss": 1.9877,
        "grad_norm": 1.8039320707321167,
        "learning_rate": 3.8671003436709355e-05,
        "epoch": 0.4760247486465584,
        "step": 3693
    },
    {
        "loss": 2.1981,
        "grad_norm": 1.6770435571670532,
        "learning_rate": 3.862295956579174e-05,
        "epoch": 0.47615364784738334,
        "step": 3694
    },
    {
        "loss": 2.2808,
        "grad_norm": 2.0470035076141357,
        "learning_rate": 3.8574938413877496e-05,
        "epoch": 0.4762825470482083,
        "step": 3695
    },
    {
        "loss": 2.128,
        "grad_norm": 2.0678791999816895,
        "learning_rate": 3.852693999874192e-05,
        "epoch": 0.47641144624903325,
        "step": 3696
    },
    {
        "loss": 1.2823,
        "grad_norm": 1.9173532724380493,
        "learning_rate": 3.847896433815188e-05,
        "epoch": 0.47654034544985824,
        "step": 3697
    },
    {
        "loss": 2.1008,
        "grad_norm": 1.1464412212371826,
        "learning_rate": 3.843101144986574e-05,
        "epoch": 0.47666924465068317,
        "step": 3698
    },
    {
        "loss": 1.8538,
        "grad_norm": 2.0234756469726562,
        "learning_rate": 3.8383081351633554e-05,
        "epoch": 0.4767981438515081,
        "step": 3699
    },
    {
        "loss": 1.4982,
        "grad_norm": 3.7796096801757812,
        "learning_rate": 3.8335174061196796e-05,
        "epoch": 0.4769270430523331,
        "step": 3700
    },
    {
        "loss": 2.0956,
        "grad_norm": 1.4554799795150757,
        "learning_rate": 3.8287289596288635e-05,
        "epoch": 0.477055942253158,
        "step": 3701
    },
    {
        "loss": 1.9058,
        "grad_norm": 1.675108551979065,
        "learning_rate": 3.8239427974633743e-05,
        "epoch": 0.477184841453983,
        "step": 3702
    },
    {
        "loss": 2.2157,
        "grad_norm": 1.4553250074386597,
        "learning_rate": 3.8191589213948276e-05,
        "epoch": 0.4773137406548079,
        "step": 3703
    },
    {
        "loss": 2.0112,
        "grad_norm": 1.628589391708374,
        "learning_rate": 3.8143773331940033e-05,
        "epoch": 0.4774426398556329,
        "step": 3704
    },
    {
        "loss": 1.865,
        "grad_norm": 1.8178486824035645,
        "learning_rate": 3.809598034630822e-05,
        "epoch": 0.47757153905645783,
        "step": 3705
    },
    {
        "loss": 1.9405,
        "grad_norm": 2.5230181217193604,
        "learning_rate": 3.8048210274743736e-05,
        "epoch": 0.4777004382572828,
        "step": 3706
    },
    {
        "loss": 1.4015,
        "grad_norm": 1.804925560951233,
        "learning_rate": 3.80004631349288e-05,
        "epoch": 0.47782933745810774,
        "step": 3707
    },
    {
        "loss": 1.9663,
        "grad_norm": 2.5540285110473633,
        "learning_rate": 3.795273894453731e-05,
        "epoch": 0.4779582366589327,
        "step": 3708
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.9909995794296265,
        "learning_rate": 3.7905037721234573e-05,
        "epoch": 0.47808713585975765,
        "step": 3709
    },
    {
        "loss": 1.9495,
        "grad_norm": 2.285953998565674,
        "learning_rate": 3.7857359482677445e-05,
        "epoch": 0.47821603506058263,
        "step": 3710
    },
    {
        "loss": 1.9509,
        "grad_norm": 1.1552807092666626,
        "learning_rate": 3.780970424651423e-05,
        "epoch": 0.47834493426140756,
        "step": 3711
    },
    {
        "loss": 1.9856,
        "grad_norm": 2.093198776245117,
        "learning_rate": 3.776207203038473e-05,
        "epoch": 0.47847383346223255,
        "step": 3712
    },
    {
        "loss": 1.3187,
        "grad_norm": 2.394580364227295,
        "learning_rate": 3.771446285192025e-05,
        "epoch": 0.4786027326630575,
        "step": 3713
    },
    {
        "loss": 1.3029,
        "grad_norm": 1.9765851497650146,
        "learning_rate": 3.7666876728743595e-05,
        "epoch": 0.47873163186388246,
        "step": 3714
    },
    {
        "loss": 2.3553,
        "grad_norm": 1.6941159963607788,
        "learning_rate": 3.761931367846893e-05,
        "epoch": 0.4788605310647074,
        "step": 3715
    },
    {
        "loss": 1.5597,
        "grad_norm": 2.0782718658447266,
        "learning_rate": 3.7571773718701965e-05,
        "epoch": 0.47898943026553237,
        "step": 3716
    },
    {
        "loss": 2.2386,
        "grad_norm": 1.9650946855545044,
        "learning_rate": 3.752425686703984e-05,
        "epoch": 0.4791183294663573,
        "step": 3717
    },
    {
        "loss": 2.5232,
        "grad_norm": 1.6415671110153198,
        "learning_rate": 3.74767631410712e-05,
        "epoch": 0.4792472286671823,
        "step": 3718
    },
    {
        "loss": 2.3659,
        "grad_norm": 2.0330753326416016,
        "learning_rate": 3.742929255837599e-05,
        "epoch": 0.4793761278680072,
        "step": 3719
    },
    {
        "loss": 1.7485,
        "grad_norm": 1.8543682098388672,
        "learning_rate": 3.738184513652575e-05,
        "epoch": 0.4795050270688322,
        "step": 3720
    },
    {
        "loss": 1.9062,
        "grad_norm": 2.075551986694336,
        "learning_rate": 3.7334420893083324e-05,
        "epoch": 0.4796339262696571,
        "step": 3721
    },
    {
        "loss": 2.3308,
        "grad_norm": 1.3935996294021606,
        "learning_rate": 3.7287019845602975e-05,
        "epoch": 0.4797628254704821,
        "step": 3722
    },
    {
        "loss": 2.077,
        "grad_norm": 1.0584012269973755,
        "learning_rate": 3.7239642011630525e-05,
        "epoch": 0.47989172467130703,
        "step": 3723
    },
    {
        "loss": 1.985,
        "grad_norm": 1.8473317623138428,
        "learning_rate": 3.719228740870302e-05,
        "epoch": 0.480020623872132,
        "step": 3724
    },
    {
        "loss": 1.8099,
        "grad_norm": 2.1429250240325928,
        "learning_rate": 3.714495605434907e-05,
        "epoch": 0.48014952307295694,
        "step": 3725
    },
    {
        "loss": 2.0941,
        "grad_norm": 1.7431808710098267,
        "learning_rate": 3.7097647966088514e-05,
        "epoch": 0.4802784222737819,
        "step": 3726
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.456591248512268,
        "learning_rate": 3.705036316143274e-05,
        "epoch": 0.48040732147460685,
        "step": 3727
    },
    {
        "loss": 1.8938,
        "grad_norm": 1.7617607116699219,
        "learning_rate": 3.700310165788442e-05,
        "epoch": 0.48053622067543184,
        "step": 3728
    },
    {
        "loss": 1.8451,
        "grad_norm": 2.5767858028411865,
        "learning_rate": 3.695586347293755e-05,
        "epoch": 0.48066511987625676,
        "step": 3729
    },
    {
        "loss": 1.9371,
        "grad_norm": 1.5153162479400635,
        "learning_rate": 3.69086486240777e-05,
        "epoch": 0.48079401907708175,
        "step": 3730
    },
    {
        "loss": 2.08,
        "grad_norm": 1.3038570880889893,
        "learning_rate": 3.6861457128781576e-05,
        "epoch": 0.4809229182779067,
        "step": 3731
    },
    {
        "loss": 1.7804,
        "grad_norm": 1.9253780841827393,
        "learning_rate": 3.68142890045174e-05,
        "epoch": 0.48105181747873166,
        "step": 3732
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.0930848121643066,
        "learning_rate": 3.6767144268744604e-05,
        "epoch": 0.4811807166795566,
        "step": 3733
    },
    {
        "loss": 1.1173,
        "grad_norm": 2.386265516281128,
        "learning_rate": 3.672002293891408e-05,
        "epoch": 0.4813096158803815,
        "step": 3734
    },
    {
        "loss": 2.0016,
        "grad_norm": 1.6680339574813843,
        "learning_rate": 3.667292503246804e-05,
        "epoch": 0.4814385150812065,
        "step": 3735
    },
    {
        "loss": 1.9225,
        "grad_norm": 1.785689115524292,
        "learning_rate": 3.662585056683994e-05,
        "epoch": 0.4815674142820314,
        "step": 3736
    },
    {
        "loss": 1.5746,
        "grad_norm": 2.4479496479034424,
        "learning_rate": 3.657879955945467e-05,
        "epoch": 0.4816963134828564,
        "step": 3737
    },
    {
        "loss": 1.0758,
        "grad_norm": 2.432819366455078,
        "learning_rate": 3.6531772027728354e-05,
        "epoch": 0.48182521268368134,
        "step": 3738
    },
    {
        "loss": 2.3842,
        "grad_norm": 1.6744102239608765,
        "learning_rate": 3.648476798906842e-05,
        "epoch": 0.4819541118845063,
        "step": 3739
    },
    {
        "loss": 1.8675,
        "grad_norm": 2.1203203201293945,
        "learning_rate": 3.6437787460873727e-05,
        "epoch": 0.48208301108533125,
        "step": 3740
    },
    {
        "loss": 2.3925,
        "grad_norm": 1.8364797830581665,
        "learning_rate": 3.639083046053425e-05,
        "epoch": 0.48221191028615623,
        "step": 3741
    },
    {
        "loss": 1.7765,
        "grad_norm": 2.5402214527130127,
        "learning_rate": 3.6343897005431414e-05,
        "epoch": 0.48234080948698116,
        "step": 3742
    },
    {
        "loss": 1.2035,
        "grad_norm": 2.2129645347595215,
        "learning_rate": 3.629698711293781e-05,
        "epoch": 0.48246970868780614,
        "step": 3743
    },
    {
        "loss": 1.6504,
        "grad_norm": 2.50605845451355,
        "learning_rate": 3.625010080041741e-05,
        "epoch": 0.48259860788863107,
        "step": 3744
    },
    {
        "loss": 1.9625,
        "grad_norm": 1.4177899360656738,
        "learning_rate": 3.6203238085225335e-05,
        "epoch": 0.48272750708945605,
        "step": 3745
    },
    {
        "loss": 1.5609,
        "grad_norm": 2.6217288970947266,
        "learning_rate": 3.615639898470808e-05,
        "epoch": 0.482856406290281,
        "step": 3746
    },
    {
        "loss": 0.969,
        "grad_norm": 2.592095375061035,
        "learning_rate": 3.610958351620342e-05,
        "epoch": 0.48298530549110597,
        "step": 3747
    },
    {
        "loss": 2.0616,
        "grad_norm": 1.3175722360610962,
        "learning_rate": 3.606279169704021e-05,
        "epoch": 0.4831142046919309,
        "step": 3748
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.251258134841919,
        "learning_rate": 3.601602354453874e-05,
        "epoch": 0.4832431038927559,
        "step": 3749
    },
    {
        "loss": 1.0338,
        "grad_norm": 2.281672716140747,
        "learning_rate": 3.5969279076010464e-05,
        "epoch": 0.4833720030935808,
        "step": 3750
    },
    {
        "loss": 1.2465,
        "grad_norm": 2.979560613632202,
        "learning_rate": 3.5922558308758016e-05,
        "epoch": 0.4835009022944058,
        "step": 3751
    },
    {
        "loss": 1.8694,
        "grad_norm": 2.3709638118743896,
        "learning_rate": 3.587586126007538e-05,
        "epoch": 0.4836298014952307,
        "step": 3752
    },
    {
        "loss": 1.6772,
        "grad_norm": 2.2625820636749268,
        "learning_rate": 3.5829187947247635e-05,
        "epoch": 0.4837587006960557,
        "step": 3753
    },
    {
        "loss": 2.3144,
        "grad_norm": 1.3308473825454712,
        "learning_rate": 3.578253838755118e-05,
        "epoch": 0.4838875998968806,
        "step": 3754
    },
    {
        "loss": 1.3989,
        "grad_norm": 2.9812612533569336,
        "learning_rate": 3.573591259825353e-05,
        "epoch": 0.4840164990977056,
        "step": 3755
    },
    {
        "loss": 1.9968,
        "grad_norm": 1.1953232288360596,
        "learning_rate": 3.5689310596613435e-05,
        "epoch": 0.48414539829853054,
        "step": 3756
    },
    {
        "loss": 1.6703,
        "grad_norm": 1.1294593811035156,
        "learning_rate": 3.56427323998809e-05,
        "epoch": 0.4842742974993555,
        "step": 3757
    },
    {
        "loss": 2.0229,
        "grad_norm": 1.2657073736190796,
        "learning_rate": 3.559617802529699e-05,
        "epoch": 0.48440319670018045,
        "step": 3758
    },
    {
        "loss": 1.7283,
        "grad_norm": 2.1782336235046387,
        "learning_rate": 3.5549647490094074e-05,
        "epoch": 0.48453209590100543,
        "step": 3759
    },
    {
        "loss": 2.1095,
        "grad_norm": 1.7911909818649292,
        "learning_rate": 3.550314081149567e-05,
        "epoch": 0.48466099510183036,
        "step": 3760
    },
    {
        "loss": 2.2717,
        "grad_norm": 1.6687169075012207,
        "learning_rate": 3.545665800671645e-05,
        "epoch": 0.48478989430265534,
        "step": 3761
    },
    {
        "loss": 1.5057,
        "grad_norm": 2.07175350189209,
        "learning_rate": 3.541019909296217e-05,
        "epoch": 0.48491879350348027,
        "step": 3762
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.3211159706115723,
        "learning_rate": 3.536376408742986e-05,
        "epoch": 0.48504769270430526,
        "step": 3763
    },
    {
        "loss": 1.9748,
        "grad_norm": 1.2362849712371826,
        "learning_rate": 3.5317353007307685e-05,
        "epoch": 0.4851765919051302,
        "step": 3764
    },
    {
        "loss": 1.9756,
        "grad_norm": 1.2822052240371704,
        "learning_rate": 3.527096586977493e-05,
        "epoch": 0.48530549110595517,
        "step": 3765
    },
    {
        "loss": 1.81,
        "grad_norm": 1.8303923606872559,
        "learning_rate": 3.5224602692001964e-05,
        "epoch": 0.4854343903067801,
        "step": 3766
    },
    {
        "loss": 1.3787,
        "grad_norm": 1.6799620389938354,
        "learning_rate": 3.517826349115039e-05,
        "epoch": 0.4855632895076051,
        "step": 3767
    },
    {
        "loss": 2.1799,
        "grad_norm": 1.7324140071868896,
        "learning_rate": 3.5131948284372826e-05,
        "epoch": 0.48569218870843,
        "step": 3768
    },
    {
        "loss": 2.567,
        "grad_norm": 1.2354406118392944,
        "learning_rate": 3.508565708881314e-05,
        "epoch": 0.485821087909255,
        "step": 3769
    },
    {
        "loss": 1.9179,
        "grad_norm": 1.1018372774124146,
        "learning_rate": 3.503938992160617e-05,
        "epoch": 0.4859499871100799,
        "step": 3770
    },
    {
        "loss": 2.5335,
        "grad_norm": 1.4222697019577026,
        "learning_rate": 3.4993146799877974e-05,
        "epoch": 0.48607888631090485,
        "step": 3771
    },
    {
        "loss": 1.9852,
        "grad_norm": 1.1895349025726318,
        "learning_rate": 3.494692774074563e-05,
        "epoch": 0.48620778551172983,
        "step": 3772
    },
    {
        "loss": 2.7352,
        "grad_norm": 1.7149780988693237,
        "learning_rate": 3.490073276131738e-05,
        "epoch": 0.48633668471255476,
        "step": 3773
    },
    {
        "loss": 1.1378,
        "grad_norm": 2.2860724925994873,
        "learning_rate": 3.485456187869248e-05,
        "epoch": 0.48646558391337974,
        "step": 3774
    },
    {
        "loss": 2.3713,
        "grad_norm": 1.493672490119934,
        "learning_rate": 3.48084151099613e-05,
        "epoch": 0.48659448311420467,
        "step": 3775
    },
    {
        "loss": 2.3539,
        "grad_norm": 1.931295394897461,
        "learning_rate": 3.476229247220529e-05,
        "epoch": 0.48672338231502965,
        "step": 3776
    },
    {
        "loss": 1.6877,
        "grad_norm": 2.310763359069824,
        "learning_rate": 3.4716193982497015e-05,
        "epoch": 0.4868522815158546,
        "step": 3777
    },
    {
        "loss": 2.5824,
        "grad_norm": 1.332256555557251,
        "learning_rate": 3.467011965789998e-05,
        "epoch": 0.48698118071667956,
        "step": 3778
    },
    {
        "loss": 1.8882,
        "grad_norm": 1.8369159698486328,
        "learning_rate": 3.462406951546884e-05,
        "epoch": 0.4871100799175045,
        "step": 3779
    },
    {
        "loss": 1.961,
        "grad_norm": 1.2678682804107666,
        "learning_rate": 3.457804357224931e-05,
        "epoch": 0.4872389791183295,
        "step": 3780
    },
    {
        "loss": 1.8621,
        "grad_norm": 2.3968453407287598,
        "learning_rate": 3.453204184527805e-05,
        "epoch": 0.4873678783191544,
        "step": 3781
    },
    {
        "loss": 1.9005,
        "grad_norm": 2.2112042903900146,
        "learning_rate": 3.448606435158285e-05,
        "epoch": 0.4874967775199794,
        "step": 3782
    },
    {
        "loss": 2.4335,
        "grad_norm": 1.277195692062378,
        "learning_rate": 3.4440111108182525e-05,
        "epoch": 0.4876256767208043,
        "step": 3783
    },
    {
        "loss": 1.4168,
        "grad_norm": 3.082470655441284,
        "learning_rate": 3.439418213208686e-05,
        "epoch": 0.4877545759216293,
        "step": 3784
    },
    {
        "loss": 1.5254,
        "grad_norm": 2.5435614585876465,
        "learning_rate": 3.434827744029665e-05,
        "epoch": 0.4878834751224542,
        "step": 3785
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.6902498006820679,
        "learning_rate": 3.43023970498038e-05,
        "epoch": 0.4880123743232792,
        "step": 3786
    },
    {
        "loss": 1.4719,
        "grad_norm": 1.9613662958145142,
        "learning_rate": 3.425654097759109e-05,
        "epoch": 0.48814127352410414,
        "step": 3787
    },
    {
        "loss": 1.6836,
        "grad_norm": 2.279853343963623,
        "learning_rate": 3.421070924063243e-05,
        "epoch": 0.4882701727249291,
        "step": 3788
    },
    {
        "loss": 1.9336,
        "grad_norm": 1.4676427841186523,
        "learning_rate": 3.4164901855892585e-05,
        "epoch": 0.48839907192575405,
        "step": 3789
    },
    {
        "loss": 2.0712,
        "grad_norm": 1.250875473022461,
        "learning_rate": 3.411911884032745e-05,
        "epoch": 0.48852797112657903,
        "step": 3790
    },
    {
        "loss": 1.6613,
        "grad_norm": 2.3106913566589355,
        "learning_rate": 3.407336021088379e-05,
        "epoch": 0.48865687032740396,
        "step": 3791
    },
    {
        "loss": 2.0183,
        "grad_norm": 1.2062727212905884,
        "learning_rate": 3.40276259844993e-05,
        "epoch": 0.48878576952822894,
        "step": 3792
    },
    {
        "loss": 1.5201,
        "grad_norm": 2.6697657108306885,
        "learning_rate": 3.398191617810287e-05,
        "epoch": 0.48891466872905387,
        "step": 3793
    },
    {
        "loss": 2.0907,
        "grad_norm": 1.3687703609466553,
        "learning_rate": 3.393623080861413e-05,
        "epoch": 0.48904356792987885,
        "step": 3794
    },
    {
        "loss": 0.7022,
        "grad_norm": 1.9270683526992798,
        "learning_rate": 3.3890569892943704e-05,
        "epoch": 0.4891724671307038,
        "step": 3795
    },
    {
        "loss": 1.607,
        "grad_norm": 1.8883017301559448,
        "learning_rate": 3.3844933447993234e-05,
        "epoch": 0.48930136633152876,
        "step": 3796
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.5915457010269165,
        "learning_rate": 3.379932149065526e-05,
        "epoch": 0.4894302655323537,
        "step": 3797
    },
    {
        "loss": 1.9814,
        "grad_norm": 1.8044918775558472,
        "learning_rate": 3.37537340378133e-05,
        "epoch": 0.4895591647331787,
        "step": 3798
    },
    {
        "loss": 2.1739,
        "grad_norm": 1.1254132986068726,
        "learning_rate": 3.370817110634171e-05,
        "epoch": 0.4896880639340036,
        "step": 3799
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.9627333879470825,
        "learning_rate": 3.366263271310588e-05,
        "epoch": 0.4898169631348286,
        "step": 3800
    },
    {
        "loss": 2.2879,
        "grad_norm": 0.9733515977859497,
        "learning_rate": 3.361711887496204e-05,
        "epoch": 0.4899458623356535,
        "step": 3801
    },
    {
        "loss": 2.2082,
        "grad_norm": 1.1356743574142456,
        "learning_rate": 3.35716296087573e-05,
        "epoch": 0.4900747615364785,
        "step": 3802
    },
    {
        "loss": 1.8573,
        "grad_norm": 2.18827223777771,
        "learning_rate": 3.3526164931329814e-05,
        "epoch": 0.4902036607373034,
        "step": 3803
    },
    {
        "loss": 2.0671,
        "grad_norm": 1.8682026863098145,
        "learning_rate": 3.348072485950848e-05,
        "epoch": 0.4903325599381284,
        "step": 3804
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.9169586896896362,
        "learning_rate": 3.343530941011323e-05,
        "epoch": 0.49046145913895334,
        "step": 3805
    },
    {
        "loss": 1.8716,
        "grad_norm": 1.346052646636963,
        "learning_rate": 3.338991859995472e-05,
        "epoch": 0.4905903583397783,
        "step": 3806
    },
    {
        "loss": 2.2247,
        "grad_norm": 1.5108481645584106,
        "learning_rate": 3.3344552445834655e-05,
        "epoch": 0.49071925754060325,
        "step": 3807
    },
    {
        "loss": 1.7058,
        "grad_norm": 2.1721701622009277,
        "learning_rate": 3.32992109645455e-05,
        "epoch": 0.4908481567414282,
        "step": 3808
    },
    {
        "loss": 2.3024,
        "grad_norm": 1.9118770360946655,
        "learning_rate": 3.325389417287055e-05,
        "epoch": 0.49097705594225316,
        "step": 3809
    },
    {
        "loss": 2.107,
        "grad_norm": 2.597107172012329,
        "learning_rate": 3.320860208758416e-05,
        "epoch": 0.4911059551430781,
        "step": 3810
    },
    {
        "loss": 1.8553,
        "grad_norm": 1.9634008407592773,
        "learning_rate": 3.316333472545132e-05,
        "epoch": 0.49123485434390307,
        "step": 3811
    },
    {
        "loss": 2.0185,
        "grad_norm": 1.2416759729385376,
        "learning_rate": 3.3118092103227995e-05,
        "epoch": 0.491363753544728,
        "step": 3812
    },
    {
        "loss": 2.0005,
        "grad_norm": 1.6773431301116943,
        "learning_rate": 3.307287423766098e-05,
        "epoch": 0.491492652745553,
        "step": 3813
    },
    {
        "loss": 1.5667,
        "grad_norm": 1.8638321161270142,
        "learning_rate": 3.302768114548781e-05,
        "epoch": 0.4916215519463779,
        "step": 3814
    },
    {
        "loss": 2.1744,
        "grad_norm": 1.424742579460144,
        "learning_rate": 3.2982512843437005e-05,
        "epoch": 0.4917504511472029,
        "step": 3815
    },
    {
        "loss": 2.1133,
        "grad_norm": 1.864383339881897,
        "learning_rate": 3.293736934822776e-05,
        "epoch": 0.4918793503480278,
        "step": 3816
    },
    {
        "loss": 1.0712,
        "grad_norm": 2.672334671020508,
        "learning_rate": 3.2892250676570216e-05,
        "epoch": 0.4920082495488528,
        "step": 3817
    },
    {
        "loss": 2.0102,
        "grad_norm": 1.5811457633972168,
        "learning_rate": 3.2847156845165225e-05,
        "epoch": 0.49213714874967773,
        "step": 3818
    },
    {
        "loss": 1.7868,
        "grad_norm": 2.286698818206787,
        "learning_rate": 3.2802087870704455e-05,
        "epoch": 0.4922660479505027,
        "step": 3819
    },
    {
        "loss": 1.8972,
        "grad_norm": 1.369584560394287,
        "learning_rate": 3.275704376987048e-05,
        "epoch": 0.49239494715132764,
        "step": 3820
    },
    {
        "loss": 2.1846,
        "grad_norm": 1.3262568712234497,
        "learning_rate": 3.27120245593365e-05,
        "epoch": 0.49252384635215263,
        "step": 3821
    },
    {
        "loss": 2.0594,
        "grad_norm": 1.3506563901901245,
        "learning_rate": 3.2667030255766626e-05,
        "epoch": 0.49265274555297756,
        "step": 3822
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.4518022537231445,
        "learning_rate": 3.262206087581576e-05,
        "epoch": 0.49278164475380254,
        "step": 3823
    },
    {
        "loss": 1.465,
        "grad_norm": 2.055691957473755,
        "learning_rate": 3.257711643612949e-05,
        "epoch": 0.49291054395462747,
        "step": 3824
    },
    {
        "loss": 1.6868,
        "grad_norm": 2.2361788749694824,
        "learning_rate": 3.2532196953344184e-05,
        "epoch": 0.49303944315545245,
        "step": 3825
    },
    {
        "loss": 1.9779,
        "grad_norm": 2.292342185974121,
        "learning_rate": 3.2487302444087043e-05,
        "epoch": 0.4931683423562774,
        "step": 3826
    },
    {
        "loss": 2.3863,
        "grad_norm": 1.6278812885284424,
        "learning_rate": 3.2442432924976e-05,
        "epoch": 0.49329724155710236,
        "step": 3827
    },
    {
        "loss": 0.9073,
        "grad_norm": 2.1706290245056152,
        "learning_rate": 3.239758841261966e-05,
        "epoch": 0.4934261407579273,
        "step": 3828
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.054748773574829,
        "learning_rate": 3.235276892361747e-05,
        "epoch": 0.4935550399587523,
        "step": 3829
    },
    {
        "loss": 2.2285,
        "grad_norm": 1.4699426889419556,
        "learning_rate": 3.230797447455961e-05,
        "epoch": 0.4936839391595772,
        "step": 3830
    },
    {
        "loss": 0.9935,
        "grad_norm": 3.128713846206665,
        "learning_rate": 3.2263205082026894e-05,
        "epoch": 0.4938128383604022,
        "step": 3831
    },
    {
        "loss": 1.6587,
        "grad_norm": 1.8229857683181763,
        "learning_rate": 3.221846076259097e-05,
        "epoch": 0.4939417375612271,
        "step": 3832
    },
    {
        "loss": 1.9326,
        "grad_norm": 1.1631290912628174,
        "learning_rate": 3.217374153281413e-05,
        "epoch": 0.4940706367620521,
        "step": 3833
    },
    {
        "loss": 1.8059,
        "grad_norm": 1.8707695007324219,
        "learning_rate": 3.2129047409249456e-05,
        "epoch": 0.494199535962877,
        "step": 3834
    },
    {
        "loss": 1.6361,
        "grad_norm": 2.235178232192993,
        "learning_rate": 3.2084378408440616e-05,
        "epoch": 0.494328435163702,
        "step": 3835
    },
    {
        "loss": 1.4613,
        "grad_norm": 1.8451379537582397,
        "learning_rate": 3.203973454692214e-05,
        "epoch": 0.49445733436452693,
        "step": 3836
    },
    {
        "loss": 1.929,
        "grad_norm": 1.4444646835327148,
        "learning_rate": 3.1995115841219126e-05,
        "epoch": 0.4945862335653519,
        "step": 3837
    },
    {
        "loss": 1.3487,
        "grad_norm": 2.4774177074432373,
        "learning_rate": 3.1950522307847354e-05,
        "epoch": 0.49471513276617685,
        "step": 3838
    },
    {
        "loss": 1.4378,
        "grad_norm": 2.083199977874756,
        "learning_rate": 3.190595396331337e-05,
        "epoch": 0.49484403196700183,
        "step": 3839
    },
    {
        "loss": 1.8203,
        "grad_norm": 1.7902544736862183,
        "learning_rate": 3.186141082411441e-05,
        "epoch": 0.49497293116782676,
        "step": 3840
    },
    {
        "loss": 2.1378,
        "grad_norm": 2.011962890625,
        "learning_rate": 3.181689290673827e-05,
        "epoch": 0.49510183036865174,
        "step": 3841
    },
    {
        "loss": 1.7629,
        "grad_norm": 2.077364206314087,
        "learning_rate": 3.1772400227663457e-05,
        "epoch": 0.49523072956947667,
        "step": 3842
    },
    {
        "loss": 1.3832,
        "grad_norm": 2.0218589305877686,
        "learning_rate": 3.1727932803359175e-05,
        "epoch": 0.4953596287703016,
        "step": 3843
    },
    {
        "loss": 1.6212,
        "grad_norm": 2.684781789779663,
        "learning_rate": 3.168349065028524e-05,
        "epoch": 0.4954885279711266,
        "step": 3844
    },
    {
        "loss": 1.8775,
        "grad_norm": 2.0538675785064697,
        "learning_rate": 3.1639073784892136e-05,
        "epoch": 0.4956174271719515,
        "step": 3845
    },
    {
        "loss": 2.0393,
        "grad_norm": 2.3245017528533936,
        "learning_rate": 3.159468222362101e-05,
        "epoch": 0.4957463263727765,
        "step": 3846
    },
    {
        "loss": 1.0219,
        "grad_norm": 2.813481092453003,
        "learning_rate": 3.155031598290358e-05,
        "epoch": 0.4958752255736014,
        "step": 3847
    },
    {
        "loss": 2.3357,
        "grad_norm": 1.6708770990371704,
        "learning_rate": 3.150597507916217e-05,
        "epoch": 0.4960041247744264,
        "step": 3848
    },
    {
        "loss": 2.5051,
        "grad_norm": 1.8335446119308472,
        "learning_rate": 3.1461659528809884e-05,
        "epoch": 0.49613302397525133,
        "step": 3849
    },
    {
        "loss": 1.8385,
        "grad_norm": 2.4014320373535156,
        "learning_rate": 3.141736934825023e-05,
        "epoch": 0.4962619231760763,
        "step": 3850
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.0647997856140137,
        "learning_rate": 3.1373104553877484e-05,
        "epoch": 0.49639082237690124,
        "step": 3851
    },
    {
        "loss": 1.7929,
        "grad_norm": 3.6861348152160645,
        "learning_rate": 3.132886516207645e-05,
        "epoch": 0.4965197215777262,
        "step": 3852
    },
    {
        "loss": 2.0233,
        "grad_norm": 2.23921799659729,
        "learning_rate": 3.128465118922258e-05,
        "epoch": 0.49664862077855115,
        "step": 3853
    },
    {
        "loss": 0.7222,
        "grad_norm": 2.226041316986084,
        "learning_rate": 3.124046265168187e-05,
        "epoch": 0.49677751997937614,
        "step": 3854
    },
    {
        "loss": 1.0008,
        "grad_norm": 3.456470489501953,
        "learning_rate": 3.1196299565810835e-05,
        "epoch": 0.49690641918020106,
        "step": 3855
    },
    {
        "loss": 1.678,
        "grad_norm": 2.419407606124878,
        "learning_rate": 3.1152161947956804e-05,
        "epoch": 0.49703531838102605,
        "step": 3856
    },
    {
        "loss": 2.399,
        "grad_norm": 1.3286205530166626,
        "learning_rate": 3.110804981445744e-05,
        "epoch": 0.497164217581851,
        "step": 3857
    },
    {
        "loss": 1.969,
        "grad_norm": 2.8285200595855713,
        "learning_rate": 3.1063963181641043e-05,
        "epoch": 0.49729311678267596,
        "step": 3858
    },
    {
        "loss": 2.0944,
        "grad_norm": 1.8518677949905396,
        "learning_rate": 3.1019902065826525e-05,
        "epoch": 0.4974220159835009,
        "step": 3859
    },
    {
        "loss": 1.4423,
        "grad_norm": 1.9282240867614746,
        "learning_rate": 3.0975866483323325e-05,
        "epoch": 0.49755091518432587,
        "step": 3860
    },
    {
        "loss": 1.8871,
        "grad_norm": 1.650840401649475,
        "learning_rate": 3.093185645043137e-05,
        "epoch": 0.4976798143851508,
        "step": 3861
    },
    {
        "loss": 2.0808,
        "grad_norm": 2.0297458171844482,
        "learning_rate": 3.088787198344123e-05,
        "epoch": 0.4978087135859758,
        "step": 3862
    },
    {
        "loss": 2.4375,
        "grad_norm": 1.4095940589904785,
        "learning_rate": 3.084391309863399e-05,
        "epoch": 0.4979376127868007,
        "step": 3863
    },
    {
        "loss": 1.7328,
        "grad_norm": 1.5914711952209473,
        "learning_rate": 3.07999798122812e-05,
        "epoch": 0.4980665119876257,
        "step": 3864
    },
    {
        "loss": 1.0557,
        "grad_norm": 1.4514533281326294,
        "learning_rate": 3.075607214064494e-05,
        "epoch": 0.4981954111884506,
        "step": 3865
    },
    {
        "loss": 1.8542,
        "grad_norm": 2.515369415283203,
        "learning_rate": 3.071219009997792e-05,
        "epoch": 0.4983243103892756,
        "step": 3866
    },
    {
        "loss": 1.7931,
        "grad_norm": 1.7013523578643799,
        "learning_rate": 3.066833370652322e-05,
        "epoch": 0.49845320959010053,
        "step": 3867
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.565804123878479,
        "learning_rate": 3.062450297651455e-05,
        "epoch": 0.4985821087909255,
        "step": 3868
    },
    {
        "loss": 1.0302,
        "grad_norm": 2.5255415439605713,
        "learning_rate": 3.058069792617601e-05,
        "epoch": 0.49871100799175044,
        "step": 3869
    },
    {
        "loss": 1.9348,
        "grad_norm": 1.797985553741455,
        "learning_rate": 3.053691857172228e-05,
        "epoch": 0.4988399071925754,
        "step": 3870
    },
    {
        "loss": 1.5015,
        "grad_norm": 1.9254217147827148,
        "learning_rate": 3.0493164929358508e-05,
        "epoch": 0.49896880639340035,
        "step": 3871
    },
    {
        "loss": 0.8142,
        "grad_norm": 2.705711841583252,
        "learning_rate": 3.0449437015280235e-05,
        "epoch": 0.49909770559422534,
        "step": 3872
    },
    {
        "loss": 2.2033,
        "grad_norm": 2.3603217601776123,
        "learning_rate": 3.040573484567366e-05,
        "epoch": 0.49922660479505027,
        "step": 3873
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.2264790534973145,
        "learning_rate": 3.0362058436715314e-05,
        "epoch": 0.49935550399587525,
        "step": 3874
    },
    {
        "loss": 1.4671,
        "grad_norm": 2.4261014461517334,
        "learning_rate": 3.0318407804572192e-05,
        "epoch": 0.4994844031967002,
        "step": 3875
    },
    {
        "loss": 2.0939,
        "grad_norm": 2.0858359336853027,
        "learning_rate": 3.0274782965401794e-05,
        "epoch": 0.49961330239752516,
        "step": 3876
    },
    {
        "loss": 1.7608,
        "grad_norm": 2.191314935684204,
        "learning_rate": 3.023118393535208e-05,
        "epoch": 0.4997422015983501,
        "step": 3877
    },
    {
        "loss": 1.8046,
        "grad_norm": 2.3116374015808105,
        "learning_rate": 3.0187610730561468e-05,
        "epoch": 0.49987110079917507,
        "step": 3878
    },
    {
        "loss": 0.9462,
        "grad_norm": 3.051328182220459,
        "learning_rate": 3.0144063367158727e-05,
        "epoch": 0.5,
        "step": 3879
    },
    {
        "loss": 2.1665,
        "grad_norm": 1.2657943964004517,
        "learning_rate": 3.0100541861263165e-05,
        "epoch": 0.5001288992008249,
        "step": 3880
    },
    {
        "loss": 1.5968,
        "grad_norm": 2.546309232711792,
        "learning_rate": 3.0057046228984464e-05,
        "epoch": 0.5002577984016499,
        "step": 3881
    },
    {
        "loss": 2.2312,
        "grad_norm": 1.7901279926300049,
        "learning_rate": 3.00135764864227e-05,
        "epoch": 0.5003866976024749,
        "step": 3882
    },
    {
        "loss": 2.0208,
        "grad_norm": 1.2759805917739868,
        "learning_rate": 2.997013264966847e-05,
        "epoch": 0.5005155968032998,
        "step": 3883
    },
    {
        "loss": 1.8949,
        "grad_norm": 2.0155224800109863,
        "learning_rate": 2.992671473480264e-05,
        "epoch": 0.5006444960041248,
        "step": 3884
    },
    {
        "loss": 2.0499,
        "grad_norm": 1.8944380283355713,
        "learning_rate": 2.9883322757896613e-05,
        "epoch": 0.5007733952049497,
        "step": 3885
    },
    {
        "loss": 1.1231,
        "grad_norm": 2.5947468280792236,
        "learning_rate": 2.9839956735012144e-05,
        "epoch": 0.5009022944057747,
        "step": 3886
    },
    {
        "loss": 2.1009,
        "grad_norm": 1.600792407989502,
        "learning_rate": 2.979661668220136e-05,
        "epoch": 0.5010311936065996,
        "step": 3887
    },
    {
        "loss": 1.3805,
        "grad_norm": 3.2521822452545166,
        "learning_rate": 2.975330261550675e-05,
        "epoch": 0.5011600928074246,
        "step": 3888
    },
    {
        "loss": 2.5183,
        "grad_norm": 1.3419394493103027,
        "learning_rate": 2.971001455096124e-05,
        "epoch": 0.5012889920082495,
        "step": 3889
    },
    {
        "loss": 1.7081,
        "grad_norm": 2.198486089706421,
        "learning_rate": 2.966675250458817e-05,
        "epoch": 0.5014178912090745,
        "step": 3890
    },
    {
        "loss": 2.2553,
        "grad_norm": 2.0049052238464355,
        "learning_rate": 2.9623516492401105e-05,
        "epoch": 0.5015467904098995,
        "step": 3891
    },
    {
        "loss": 2.0989,
        "grad_norm": 1.1040314435958862,
        "learning_rate": 2.9580306530404102e-05,
        "epoch": 0.5016756896107244,
        "step": 3892
    },
    {
        "loss": 2.3065,
        "grad_norm": 1.7195899486541748,
        "learning_rate": 2.9537122634591564e-05,
        "epoch": 0.5018045888115493,
        "step": 3893
    },
    {
        "loss": 1.8617,
        "grad_norm": 1.367332100868225,
        "learning_rate": 2.9493964820948162e-05,
        "epoch": 0.5019334880123744,
        "step": 3894
    },
    {
        "loss": 1.7195,
        "grad_norm": 1.845504879951477,
        "learning_rate": 2.9450833105449015e-05,
        "epoch": 0.5020623872131993,
        "step": 3895
    },
    {
        "loss": 1.8712,
        "grad_norm": 1.2401541471481323,
        "learning_rate": 2.9407727504059468e-05,
        "epoch": 0.5021912864140242,
        "step": 3896
    },
    {
        "loss": 1.7492,
        "grad_norm": 2.0124526023864746,
        "learning_rate": 2.936464803273534e-05,
        "epoch": 0.5023201856148491,
        "step": 3897
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.369332194328308,
        "learning_rate": 2.932159470742264e-05,
        "epoch": 0.5024490848156742,
        "step": 3898
    },
    {
        "loss": 2.0532,
        "grad_norm": 1.5820519924163818,
        "learning_rate": 2.927856754405782e-05,
        "epoch": 0.5025779840164991,
        "step": 3899
    },
    {
        "loss": 1.3552,
        "grad_norm": 1.542750597000122,
        "learning_rate": 2.923556655856755e-05,
        "epoch": 0.502706883217324,
        "step": 3900
    },
    {
        "loss": 2.2613,
        "grad_norm": 1.8378620147705078,
        "learning_rate": 2.9192591766868795e-05,
        "epoch": 0.502835782418149,
        "step": 3901
    },
    {
        "loss": 2.2499,
        "grad_norm": 1.4764635562896729,
        "learning_rate": 2.9149643184869024e-05,
        "epoch": 0.502964681618974,
        "step": 3902
    },
    {
        "loss": 2.1175,
        "grad_norm": 1.4297826290130615,
        "learning_rate": 2.9106720828465716e-05,
        "epoch": 0.5030935808197989,
        "step": 3903
    },
    {
        "loss": 1.784,
        "grad_norm": 2.084256887435913,
        "learning_rate": 2.906382471354684e-05,
        "epoch": 0.5032224800206239,
        "step": 3904
    },
    {
        "loss": 1.7197,
        "grad_norm": 1.6777480840682983,
        "learning_rate": 2.9020954855990635e-05,
        "epoch": 0.5033513792214488,
        "step": 3905
    },
    {
        "loss": 2.5593,
        "grad_norm": 1.6880533695220947,
        "learning_rate": 2.897811127166552e-05,
        "epoch": 0.5034802784222738,
        "step": 3906
    },
    {
        "loss": 1.7721,
        "grad_norm": 1.7193281650543213,
        "learning_rate": 2.8935293976430266e-05,
        "epoch": 0.5036091776230988,
        "step": 3907
    },
    {
        "loss": 1.7238,
        "grad_norm": 2.2991840839385986,
        "learning_rate": 2.8892502986133952e-05,
        "epoch": 0.5037380768239237,
        "step": 3908
    },
    {
        "loss": 1.9047,
        "grad_norm": 2.5988974571228027,
        "learning_rate": 2.8849738316615793e-05,
        "epoch": 0.5038669760247486,
        "step": 3909
    },
    {
        "loss": 1.653,
        "grad_norm": 3.3555333614349365,
        "learning_rate": 2.8806999983705363e-05,
        "epoch": 0.5039958752255737,
        "step": 3910
    },
    {
        "loss": 2.374,
        "grad_norm": 1.3142709732055664,
        "learning_rate": 2.8764288003222505e-05,
        "epoch": 0.5041247744263986,
        "step": 3911
    },
    {
        "loss": 1.7665,
        "grad_norm": 1.4646446704864502,
        "learning_rate": 2.872160239097721e-05,
        "epoch": 0.5042536736272235,
        "step": 3912
    },
    {
        "loss": 1.997,
        "grad_norm": 1.7053333520889282,
        "learning_rate": 2.8678943162769712e-05,
        "epoch": 0.5043825728280484,
        "step": 3913
    },
    {
        "loss": 2.2015,
        "grad_norm": 1.9171390533447266,
        "learning_rate": 2.863631033439068e-05,
        "epoch": 0.5045114720288735,
        "step": 3914
    },
    {
        "loss": 1.6541,
        "grad_norm": 2.331109046936035,
        "learning_rate": 2.8593703921620705e-05,
        "epoch": 0.5046403712296984,
        "step": 3915
    },
    {
        "loss": 1.8955,
        "grad_norm": 2.379899263381958,
        "learning_rate": 2.855112394023084e-05,
        "epoch": 0.5047692704305233,
        "step": 3916
    },
    {
        "loss": 1.3064,
        "grad_norm": 2.22446346282959,
        "learning_rate": 2.850857040598224e-05,
        "epoch": 0.5048981696313483,
        "step": 3917
    },
    {
        "loss": 2.3242,
        "grad_norm": 1.3377987146377563,
        "learning_rate": 2.8466043334626237e-05,
        "epoch": 0.5050270688321732,
        "step": 3918
    },
    {
        "loss": 2.0626,
        "grad_norm": 2.080071210861206,
        "learning_rate": 2.8423542741904586e-05,
        "epoch": 0.5051559680329982,
        "step": 3919
    },
    {
        "loss": 2.2004,
        "grad_norm": 2.0348498821258545,
        "learning_rate": 2.8381068643548926e-05,
        "epoch": 0.5052848672338232,
        "step": 3920
    },
    {
        "loss": 1.3714,
        "grad_norm": 1.8752155303955078,
        "learning_rate": 2.83386210552813e-05,
        "epoch": 0.5054137664346481,
        "step": 3921
    },
    {
        "loss": 2.3146,
        "grad_norm": 1.5526154041290283,
        "learning_rate": 2.829619999281392e-05,
        "epoch": 0.505542665635473,
        "step": 3922
    },
    {
        "loss": 2.0888,
        "grad_norm": 2.0271520614624023,
        "learning_rate": 2.8253805471849103e-05,
        "epoch": 0.505671564836298,
        "step": 3923
    },
    {
        "loss": 1.4589,
        "grad_norm": 2.3523476123809814,
        "learning_rate": 2.8211437508079385e-05,
        "epoch": 0.505800464037123,
        "step": 3924
    },
    {
        "loss": 2.329,
        "grad_norm": 1.3304259777069092,
        "learning_rate": 2.816909611718751e-05,
        "epoch": 0.5059293632379479,
        "step": 3925
    },
    {
        "loss": 1.8985,
        "grad_norm": 2.047501564025879,
        "learning_rate": 2.8126781314846285e-05,
        "epoch": 0.5060582624387728,
        "step": 3926
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.911298990249634,
        "learning_rate": 2.8084493116718768e-05,
        "epoch": 0.5061871616395979,
        "step": 3927
    },
    {
        "loss": 1.86,
        "grad_norm": 1.6464104652404785,
        "learning_rate": 2.8042231538458162e-05,
        "epoch": 0.5063160608404228,
        "step": 3928
    },
    {
        "loss": 1.7513,
        "grad_norm": 2.1167662143707275,
        "learning_rate": 2.7999996595707777e-05,
        "epoch": 0.5064449600412477,
        "step": 3929
    },
    {
        "loss": 1.1178,
        "grad_norm": 2.9626240730285645,
        "learning_rate": 2.7957788304100995e-05,
        "epoch": 0.5065738592420727,
        "step": 3930
    },
    {
        "loss": 1.4468,
        "grad_norm": 1.8508477210998535,
        "learning_rate": 2.7915606679261576e-05,
        "epoch": 0.5067027584428977,
        "step": 3931
    },
    {
        "loss": 1.5863,
        "grad_norm": 2.4429659843444824,
        "learning_rate": 2.787345173680309e-05,
        "epoch": 0.5068316576437226,
        "step": 3932
    },
    {
        "loss": 2.1379,
        "grad_norm": 1.353934645652771,
        "learning_rate": 2.7831323492329463e-05,
        "epoch": 0.5069605568445475,
        "step": 3933
    },
    {
        "loss": 1.6247,
        "grad_norm": 2.58516526222229,
        "learning_rate": 2.778922196143473e-05,
        "epoch": 0.5070894560453725,
        "step": 3934
    },
    {
        "loss": 2.3401,
        "grad_norm": 0.9857848882675171,
        "learning_rate": 2.7747147159702803e-05,
        "epoch": 0.5072183552461975,
        "step": 3935
    },
    {
        "loss": 1.9819,
        "grad_norm": 2.009434223175049,
        "learning_rate": 2.7705099102708065e-05,
        "epoch": 0.5073472544470224,
        "step": 3936
    },
    {
        "loss": 2.2908,
        "grad_norm": 1.8700599670410156,
        "learning_rate": 2.7663077806014648e-05,
        "epoch": 0.5074761536478474,
        "step": 3937
    },
    {
        "loss": 1.9927,
        "grad_norm": 1.5914511680603027,
        "learning_rate": 2.7621083285176995e-05,
        "epoch": 0.5076050528486723,
        "step": 3938
    },
    {
        "loss": 1.8753,
        "grad_norm": 2.491893768310547,
        "learning_rate": 2.757911555573961e-05,
        "epoch": 0.5077339520494973,
        "step": 3939
    },
    {
        "loss": 1.5985,
        "grad_norm": 2.3736159801483154,
        "learning_rate": 2.753717463323695e-05,
        "epoch": 0.5078628512503223,
        "step": 3940
    },
    {
        "loss": 1.4836,
        "grad_norm": 1.8074586391448975,
        "learning_rate": 2.7495260533193777e-05,
        "epoch": 0.5079917504511472,
        "step": 3941
    },
    {
        "loss": 1.6748,
        "grad_norm": 2.518728256225586,
        "learning_rate": 2.745337327112474e-05,
        "epoch": 0.5081206496519721,
        "step": 3942
    },
    {
        "loss": 1.843,
        "grad_norm": 1.9948077201843262,
        "learning_rate": 2.741151286253456e-05,
        "epoch": 0.5082495488527972,
        "step": 3943
    },
    {
        "loss": 1.7332,
        "grad_norm": 2.6700446605682373,
        "learning_rate": 2.7369679322918097e-05,
        "epoch": 0.5083784480536221,
        "step": 3944
    },
    {
        "loss": 1.8984,
        "grad_norm": 2.8811302185058594,
        "learning_rate": 2.732787266776029e-05,
        "epoch": 0.508507347254447,
        "step": 3945
    },
    {
        "loss": 2.515,
        "grad_norm": 1.0269606113433838,
        "learning_rate": 2.728609291253601e-05,
        "epoch": 0.508636246455272,
        "step": 3946
    },
    {
        "loss": 1.4108,
        "grad_norm": 2.2454376220703125,
        "learning_rate": 2.7244340072710195e-05,
        "epoch": 0.508765145656097,
        "step": 3947
    },
    {
        "loss": 1.8004,
        "grad_norm": 2.4779129028320312,
        "learning_rate": 2.7202614163737995e-05,
        "epoch": 0.5088940448569219,
        "step": 3948
    },
    {
        "loss": 1.2195,
        "grad_norm": 1.9074715375900269,
        "learning_rate": 2.7160915201064307e-05,
        "epoch": 0.5090229440577468,
        "step": 3949
    },
    {
        "loss": 2.1539,
        "grad_norm": 1.8900291919708252,
        "learning_rate": 2.7119243200124257e-05,
        "epoch": 0.5091518432585718,
        "step": 3950
    },
    {
        "loss": 2.417,
        "grad_norm": 1.5493069887161255,
        "learning_rate": 2.7077598176343e-05,
        "epoch": 0.5092807424593968,
        "step": 3951
    },
    {
        "loss": 2.2662,
        "grad_norm": 1.9814426898956299,
        "learning_rate": 2.7035980145135486e-05,
        "epoch": 0.5094096416602217,
        "step": 3952
    },
    {
        "loss": 2.0766,
        "grad_norm": 1.8447421789169312,
        "learning_rate": 2.699438912190698e-05,
        "epoch": 0.5095385408610467,
        "step": 3953
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.8721256256103516,
        "learning_rate": 2.6952825122052517e-05,
        "epoch": 0.5096674400618716,
        "step": 3954
    },
    {
        "loss": 2.5131,
        "grad_norm": 1.6543240547180176,
        "learning_rate": 2.6911288160957237e-05,
        "epoch": 0.5097963392626965,
        "step": 3955
    },
    {
        "loss": 1.5995,
        "grad_norm": 2.9666647911071777,
        "learning_rate": 2.6869778253996214e-05,
        "epoch": 0.5099252384635216,
        "step": 3956
    },
    {
        "loss": 1.5514,
        "grad_norm": 1.8632644414901733,
        "learning_rate": 2.6828295416534566e-05,
        "epoch": 0.5100541376643465,
        "step": 3957
    },
    {
        "loss": 1.8471,
        "grad_norm": 3.0775156021118164,
        "learning_rate": 2.6786839663927378e-05,
        "epoch": 0.5101830368651714,
        "step": 3958
    },
    {
        "loss": 2.3386,
        "grad_norm": 2.8947484493255615,
        "learning_rate": 2.674541101151966e-05,
        "epoch": 0.5103119360659963,
        "step": 3959
    },
    {
        "loss": 1.4702,
        "grad_norm": 1.6732066869735718,
        "learning_rate": 2.670400947464642e-05,
        "epoch": 0.5104408352668214,
        "step": 3960
    },
    {
        "loss": 2.0444,
        "grad_norm": 3.4154343605041504,
        "learning_rate": 2.6662635068632647e-05,
        "epoch": 0.5105697344676463,
        "step": 3961
    },
    {
        "loss": 1.2666,
        "grad_norm": 1.8994827270507812,
        "learning_rate": 2.6621287808793292e-05,
        "epoch": 0.5106986336684712,
        "step": 3962
    },
    {
        "loss": 1.8306,
        "grad_norm": 2.292999505996704,
        "learning_rate": 2.657996771043324e-05,
        "epoch": 0.5108275328692962,
        "step": 3963
    },
    {
        "loss": 1.6039,
        "grad_norm": 2.6463186740875244,
        "learning_rate": 2.653867478884724e-05,
        "epoch": 0.5109564320701212,
        "step": 3964
    },
    {
        "loss": 2.2064,
        "grad_norm": 1.576050877571106,
        "learning_rate": 2.649740905932022e-05,
        "epoch": 0.5110853312709461,
        "step": 3965
    },
    {
        "loss": 1.9355,
        "grad_norm": 2.5918478965759277,
        "learning_rate": 2.6456170537126733e-05,
        "epoch": 0.5112142304717711,
        "step": 3966
    },
    {
        "loss": 2.0803,
        "grad_norm": 1.6736663579940796,
        "learning_rate": 2.641495923753148e-05,
        "epoch": 0.511343129672596,
        "step": 3967
    },
    {
        "loss": 1.6142,
        "grad_norm": 1.6062902212142944,
        "learning_rate": 2.637377517578904e-05,
        "epoch": 0.511472028873421,
        "step": 3968
    },
    {
        "loss": 1.4996,
        "grad_norm": 2.0749671459198,
        "learning_rate": 2.6332618367143848e-05,
        "epoch": 0.511600928074246,
        "step": 3969
    },
    {
        "loss": 2.2061,
        "grad_norm": 1.8731426000595093,
        "learning_rate": 2.6291488826830325e-05,
        "epoch": 0.5117298272750709,
        "step": 3970
    },
    {
        "loss": 1.7398,
        "grad_norm": 2.121023416519165,
        "learning_rate": 2.6250386570072727e-05,
        "epoch": 0.5118587264758958,
        "step": 3971
    },
    {
        "loss": 2.3864,
        "grad_norm": 1.4403139352798462,
        "learning_rate": 2.6209311612085276e-05,
        "epoch": 0.5119876256767208,
        "step": 3972
    },
    {
        "loss": 1.2685,
        "grad_norm": 2.1377146244049072,
        "learning_rate": 2.6168263968072048e-05,
        "epoch": 0.5121165248775458,
        "step": 3973
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.7117260694503784,
        "learning_rate": 2.6127243653227073e-05,
        "epoch": 0.5122454240783707,
        "step": 3974
    },
    {
        "loss": 1.3656,
        "grad_norm": 2.081242799758911,
        "learning_rate": 2.608625068273418e-05,
        "epoch": 0.5123743232791956,
        "step": 3975
    },
    {
        "loss": 2.4766,
        "grad_norm": 1.199115514755249,
        "learning_rate": 2.6045285071767057e-05,
        "epoch": 0.5125032224800207,
        "step": 3976
    },
    {
        "loss": 1.7839,
        "grad_norm": 2.4166226387023926,
        "learning_rate": 2.6004346835489457e-05,
        "epoch": 0.5126321216808456,
        "step": 3977
    },
    {
        "loss": 2.4293,
        "grad_norm": 1.8192269802093506,
        "learning_rate": 2.596343598905472e-05,
        "epoch": 0.5127610208816705,
        "step": 3978
    },
    {
        "loss": 2.2286,
        "grad_norm": 1.6982189416885376,
        "learning_rate": 2.5922552547606272e-05,
        "epoch": 0.5128899200824955,
        "step": 3979
    },
    {
        "loss": 1.6636,
        "grad_norm": 2.2058959007263184,
        "learning_rate": 2.58816965262773e-05,
        "epoch": 0.5130188192833205,
        "step": 3980
    },
    {
        "loss": 1.5574,
        "grad_norm": 2.1254847049713135,
        "learning_rate": 2.5840867940190784e-05,
        "epoch": 0.5131477184841454,
        "step": 3981
    },
    {
        "loss": 2.3938,
        "grad_norm": 1.6828117370605469,
        "learning_rate": 2.5800066804459764e-05,
        "epoch": 0.5132766176849703,
        "step": 3982
    },
    {
        "loss": 1.8011,
        "grad_norm": 2.1197495460510254,
        "learning_rate": 2.5759293134186844e-05,
        "epoch": 0.5134055168857953,
        "step": 3983
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.9270814657211304,
        "learning_rate": 2.5718546944464627e-05,
        "epoch": 0.5135344160866203,
        "step": 3984
    },
    {
        "loss": 1.8213,
        "grad_norm": 2.3321762084960938,
        "learning_rate": 2.5677828250375556e-05,
        "epoch": 0.5136633152874452,
        "step": 3985
    },
    {
        "loss": 1.4707,
        "grad_norm": 2.211944818496704,
        "learning_rate": 2.563713706699179e-05,
        "epoch": 0.5137922144882702,
        "step": 3986
    },
    {
        "loss": 2.2347,
        "grad_norm": 1.7628775835037231,
        "learning_rate": 2.5596473409375388e-05,
        "epoch": 0.5139211136890951,
        "step": 3987
    },
    {
        "loss": 1.3388,
        "grad_norm": 1.9829574823379517,
        "learning_rate": 2.5555837292578212e-05,
        "epoch": 0.51405001288992,
        "step": 3988
    },
    {
        "loss": 1.7907,
        "grad_norm": 1.7301396131515503,
        "learning_rate": 2.5515228731641882e-05,
        "epoch": 0.5141789120907451,
        "step": 3989
    },
    {
        "loss": 1.7331,
        "grad_norm": 1.4745351076126099,
        "learning_rate": 2.5474647741597867e-05,
        "epoch": 0.51430781129157,
        "step": 3990
    },
    {
        "loss": 1.4689,
        "grad_norm": 1.8684571981430054,
        "learning_rate": 2.5434094337467428e-05,
        "epoch": 0.5144367104923949,
        "step": 3991
    },
    {
        "loss": 2.051,
        "grad_norm": 1.377814531326294,
        "learning_rate": 2.5393568534261603e-05,
        "epoch": 0.5145656096932198,
        "step": 3992
    },
    {
        "loss": 0.7986,
        "grad_norm": 2.2799363136291504,
        "learning_rate": 2.5353070346981122e-05,
        "epoch": 0.5146945088940449,
        "step": 3993
    },
    {
        "loss": 2.116,
        "grad_norm": 1.2249642610549927,
        "learning_rate": 2.5312599790616754e-05,
        "epoch": 0.5148234080948698,
        "step": 3994
    },
    {
        "loss": 1.7997,
        "grad_norm": 1.4954628944396973,
        "learning_rate": 2.527215688014869e-05,
        "epoch": 0.5149523072956947,
        "step": 3995
    },
    {
        "loss": 1.5412,
        "grad_norm": 1.9083880186080933,
        "learning_rate": 2.5231741630547155e-05,
        "epoch": 0.5150812064965197,
        "step": 3996
    },
    {
        "loss": 1.9897,
        "grad_norm": 2.490467071533203,
        "learning_rate": 2.5191354056772087e-05,
        "epoch": 0.5152101056973447,
        "step": 3997
    },
    {
        "loss": 2.3294,
        "grad_norm": 2.0514702796936035,
        "learning_rate": 2.5150994173773e-05,
        "epoch": 0.5153390048981696,
        "step": 3998
    },
    {
        "loss": 1.6807,
        "grad_norm": 1.5180699825286865,
        "learning_rate": 2.511066199648947e-05,
        "epoch": 0.5154679040989946,
        "step": 3999
    },
    {
        "loss": 1.5511,
        "grad_norm": 2.375521659851074,
        "learning_rate": 2.5070357539850498e-05,
        "epoch": 0.5155968032998195,
        "step": 4000
    },
    {
        "loss": 2.3756,
        "grad_norm": 1.090847373008728,
        "learning_rate": 2.5030080818775015e-05,
        "epoch": 0.5157257025006445,
        "step": 4001
    },
    {
        "loss": 1.9958,
        "grad_norm": 2.1937057971954346,
        "learning_rate": 2.4989831848171696e-05,
        "epoch": 0.5158546017014695,
        "step": 4002
    },
    {
        "loss": 2.5419,
        "grad_norm": 1.6929175853729248,
        "learning_rate": 2.4949610642938813e-05,
        "epoch": 0.5159835009022944,
        "step": 4003
    },
    {
        "loss": 1.865,
        "grad_norm": 2.1397950649261475,
        "learning_rate": 2.490941721796447e-05,
        "epoch": 0.5161124001031193,
        "step": 4004
    },
    {
        "loss": 2.197,
        "grad_norm": 1.5014532804489136,
        "learning_rate": 2.4869251588126487e-05,
        "epoch": 0.5162412993039444,
        "step": 4005
    },
    {
        "loss": 2.2561,
        "grad_norm": 1.6323347091674805,
        "learning_rate": 2.4829113768292288e-05,
        "epoch": 0.5163701985047693,
        "step": 4006
    },
    {
        "loss": 1.164,
        "grad_norm": 2.202068567276001,
        "learning_rate": 2.478900377331914e-05,
        "epoch": 0.5164990977055942,
        "step": 4007
    },
    {
        "loss": 1.7451,
        "grad_norm": 2.324625253677368,
        "learning_rate": 2.4748921618053945e-05,
        "epoch": 0.5166279969064191,
        "step": 4008
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.4728434085845947,
        "learning_rate": 2.4708867317333284e-05,
        "epoch": 0.5167568961072442,
        "step": 4009
    },
    {
        "loss": 1.6878,
        "grad_norm": 2.27302622795105,
        "learning_rate": 2.46688408859834e-05,
        "epoch": 0.5168857953080691,
        "step": 4010
    },
    {
        "loss": 2.29,
        "grad_norm": 1.992559552192688,
        "learning_rate": 2.46288423388204e-05,
        "epoch": 0.517014694508894,
        "step": 4011
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.5522620677948,
        "learning_rate": 2.4588871690649796e-05,
        "epoch": 0.517143593709719,
        "step": 4012
    },
    {
        "loss": 2.0659,
        "grad_norm": 1.9141511917114258,
        "learning_rate": 2.454892895626697e-05,
        "epoch": 0.517272492910544,
        "step": 4013
    },
    {
        "loss": 1.5266,
        "grad_norm": 2.3452954292297363,
        "learning_rate": 2.450901415045698e-05,
        "epoch": 0.5174013921113689,
        "step": 4014
    },
    {
        "loss": 1.9286,
        "grad_norm": 1.8250389099121094,
        "learning_rate": 2.446912728799432e-05,
        "epoch": 0.5175302913121939,
        "step": 4015
    },
    {
        "loss": 2.1243,
        "grad_norm": 1.4675095081329346,
        "learning_rate": 2.4429268383643467e-05,
        "epoch": 0.5176591905130188,
        "step": 4016
    },
    {
        "loss": 1.7831,
        "grad_norm": 2.813624382019043,
        "learning_rate": 2.4389437452158305e-05,
        "epoch": 0.5177880897138438,
        "step": 4017
    },
    {
        "loss": 1.3439,
        "grad_norm": 1.5787971019744873,
        "learning_rate": 2.4349634508282426e-05,
        "epoch": 0.5179169889146688,
        "step": 4018
    },
    {
        "loss": 1.7108,
        "grad_norm": 1.7346526384353638,
        "learning_rate": 2.4309859566749115e-05,
        "epoch": 0.5180458881154937,
        "step": 4019
    },
    {
        "loss": 1.3355,
        "grad_norm": 2.427950620651245,
        "learning_rate": 2.427011264228124e-05,
        "epoch": 0.5181747873163186,
        "step": 4020
    },
    {
        "loss": 2.084,
        "grad_norm": 1.2423796653747559,
        "learning_rate": 2.4230393749591353e-05,
        "epoch": 0.5183036865171436,
        "step": 4021
    },
    {
        "loss": 2.725,
        "grad_norm": 1.5101165771484375,
        "learning_rate": 2.4190702903381557e-05,
        "epoch": 0.5184325857179686,
        "step": 4022
    },
    {
        "loss": 1.9062,
        "grad_norm": 1.6883037090301514,
        "learning_rate": 2.4151040118343583e-05,
        "epoch": 0.5185614849187935,
        "step": 4023
    },
    {
        "loss": 1.1913,
        "grad_norm": 2.5816221237182617,
        "learning_rate": 2.4111405409158826e-05,
        "epoch": 0.5186903841196184,
        "step": 4024
    },
    {
        "loss": 2.3071,
        "grad_norm": 1.4025607109069824,
        "learning_rate": 2.407179879049829e-05,
        "epoch": 0.5188192833204434,
        "step": 4025
    },
    {
        "loss": 1.4102,
        "grad_norm": 2.342885971069336,
        "learning_rate": 2.4032220277022533e-05,
        "epoch": 0.5189481825212684,
        "step": 4026
    },
    {
        "loss": 1.8894,
        "grad_norm": 2.05751371383667,
        "learning_rate": 2.399266988338166e-05,
        "epoch": 0.5190770817220933,
        "step": 4027
    },
    {
        "loss": 2.4299,
        "grad_norm": 1.9698057174682617,
        "learning_rate": 2.395314762421559e-05,
        "epoch": 0.5192059809229183,
        "step": 4028
    },
    {
        "loss": 1.4625,
        "grad_norm": 2.4816551208496094,
        "learning_rate": 2.391365351415351e-05,
        "epoch": 0.5193348801237432,
        "step": 4029
    },
    {
        "loss": 1.8491,
        "grad_norm": 1.8181244134902954,
        "learning_rate": 2.3874187567814442e-05,
        "epoch": 0.5194637793245682,
        "step": 4030
    },
    {
        "loss": 1.3728,
        "grad_norm": 2.0755226612091064,
        "learning_rate": 2.3834749799806945e-05,
        "epoch": 0.5195926785253931,
        "step": 4031
    },
    {
        "loss": 1.8325,
        "grad_norm": 1.9939017295837402,
        "learning_rate": 2.3795340224728935e-05,
        "epoch": 0.5197215777262181,
        "step": 4032
    },
    {
        "loss": 2.0324,
        "grad_norm": 1.5057644844055176,
        "learning_rate": 2.3755958857168204e-05,
        "epoch": 0.519850476927043,
        "step": 4033
    },
    {
        "loss": 1.215,
        "grad_norm": 2.6624839305877686,
        "learning_rate": 2.371660571170188e-05,
        "epoch": 0.519979376127868,
        "step": 4034
    },
    {
        "loss": 1.4237,
        "grad_norm": 2.539928913116455,
        "learning_rate": 2.367728080289675e-05,
        "epoch": 0.520108275328693,
        "step": 4035
    },
    {
        "loss": 2.1458,
        "grad_norm": 1.2355707883834839,
        "learning_rate": 2.3637984145309077e-05,
        "epoch": 0.5202371745295179,
        "step": 4036
    },
    {
        "loss": 2.1637,
        "grad_norm": 1.4195237159729004,
        "learning_rate": 2.3598715753484734e-05,
        "epoch": 0.5203660737303428,
        "step": 4037
    },
    {
        "loss": 2.3414,
        "grad_norm": 2.138084888458252,
        "learning_rate": 2.3559475641959127e-05,
        "epoch": 0.5204949729311679,
        "step": 4038
    },
    {
        "loss": 2.3389,
        "grad_norm": 1.3068088293075562,
        "learning_rate": 2.3520263825257083e-05,
        "epoch": 0.5206238721319928,
        "step": 4039
    },
    {
        "loss": 2.2667,
        "grad_norm": 1.8245022296905518,
        "learning_rate": 2.348108031789319e-05,
        "epoch": 0.5207527713328177,
        "step": 4040
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.4860098361968994,
        "learning_rate": 2.3441925134371255e-05,
        "epoch": 0.5208816705336426,
        "step": 4041
    },
    {
        "loss": 2.2422,
        "grad_norm": 1.3349815607070923,
        "learning_rate": 2.3402798289184858e-05,
        "epoch": 0.5210105697344677,
        "step": 4042
    },
    {
        "loss": 1.5946,
        "grad_norm": 1.44461190700531,
        "learning_rate": 2.336369979681695e-05,
        "epoch": 0.5211394689352926,
        "step": 4043
    },
    {
        "loss": 1.877,
        "grad_norm": 1.437748908996582,
        "learning_rate": 2.3324629671739963e-05,
        "epoch": 0.5212683681361175,
        "step": 4044
    },
    {
        "loss": 1.5163,
        "grad_norm": 1.5587451457977295,
        "learning_rate": 2.3285587928416043e-05,
        "epoch": 0.5213972673369425,
        "step": 4045
    },
    {
        "loss": 1.9492,
        "grad_norm": 1.6562861204147339,
        "learning_rate": 2.324657458129652e-05,
        "epoch": 0.5215261665377675,
        "step": 4046
    },
    {
        "loss": 1.3009,
        "grad_norm": 2.371227264404297,
        "learning_rate": 2.3207589644822436e-05,
        "epoch": 0.5216550657385924,
        "step": 4047
    },
    {
        "loss": 2.6264,
        "grad_norm": 1.396728754043579,
        "learning_rate": 2.3168633133424278e-05,
        "epoch": 0.5217839649394174,
        "step": 4048
    },
    {
        "loss": 1.8809,
        "grad_norm": 2.494494676589966,
        "learning_rate": 2.312970506152192e-05,
        "epoch": 0.5219128641402423,
        "step": 4049
    },
    {
        "loss": 2.2382,
        "grad_norm": 1.7548102140426636,
        "learning_rate": 2.3090805443524845e-05,
        "epoch": 0.5220417633410673,
        "step": 4050
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.9583784341812134,
        "learning_rate": 2.3051934293831855e-05,
        "epoch": 0.5221706625418923,
        "step": 4051
    },
    {
        "loss": 1.6014,
        "grad_norm": 1.5727211236953735,
        "learning_rate": 2.301309162683134e-05,
        "epoch": 0.5222995617427172,
        "step": 4052
    },
    {
        "loss": 2.191,
        "grad_norm": 2.0512020587921143,
        "learning_rate": 2.2974277456901083e-05,
        "epoch": 0.5224284609435421,
        "step": 4053
    },
    {
        "loss": 2.177,
        "grad_norm": 1.8148341178894043,
        "learning_rate": 2.293549179840837e-05,
        "epoch": 0.5225573601443672,
        "step": 4054
    },
    {
        "loss": 1.6736,
        "grad_norm": 1.4427471160888672,
        "learning_rate": 2.2896734665709875e-05,
        "epoch": 0.5226862593451921,
        "step": 4055
    },
    {
        "loss": 2.1377,
        "grad_norm": 1.421743392944336,
        "learning_rate": 2.285800607315166e-05,
        "epoch": 0.522815158546017,
        "step": 4056
    },
    {
        "loss": 1.2445,
        "grad_norm": 2.4864628314971924,
        "learning_rate": 2.2819306035069444e-05,
        "epoch": 0.5229440577468419,
        "step": 4057
    },
    {
        "loss": 1.8959,
        "grad_norm": 1.8938931226730347,
        "learning_rate": 2.2780634565788095e-05,
        "epoch": 0.523072956947667,
        "step": 4058
    },
    {
        "loss": 1.7428,
        "grad_norm": 2.4020755290985107,
        "learning_rate": 2.27419916796221e-05,
        "epoch": 0.5232018561484919,
        "step": 4059
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.2553725242614746,
        "learning_rate": 2.2703377390875358e-05,
        "epoch": 0.5233307553493168,
        "step": 4060
    },
    {
        "loss": 1.7932,
        "grad_norm": 2.3538150787353516,
        "learning_rate": 2.2664791713840993e-05,
        "epoch": 0.5234596545501418,
        "step": 4061
    },
    {
        "loss": 2.1715,
        "grad_norm": 1.082335114479065,
        "learning_rate": 2.2626234662801848e-05,
        "epoch": 0.5235885537509667,
        "step": 4062
    },
    {
        "loss": 2.1088,
        "grad_norm": 1.6675784587860107,
        "learning_rate": 2.258770625202985e-05,
        "epoch": 0.5237174529517917,
        "step": 4063
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.0018301010131836,
        "learning_rate": 2.2549206495786535e-05,
        "epoch": 0.5238463521526167,
        "step": 4064
    },
    {
        "loss": 2.0377,
        "grad_norm": 1.5787580013275146,
        "learning_rate": 2.2510735408322803e-05,
        "epoch": 0.5239752513534416,
        "step": 4065
    },
    {
        "loss": 1.8242,
        "grad_norm": 1.3419853448867798,
        "learning_rate": 2.247229300387884e-05,
        "epoch": 0.5241041505542665,
        "step": 4066
    },
    {
        "loss": 2.446,
        "grad_norm": 1.6658552885055542,
        "learning_rate": 2.2433879296684333e-05,
        "epoch": 0.5242330497550916,
        "step": 4067
    },
    {
        "loss": 1.4975,
        "grad_norm": 2.377020835876465,
        "learning_rate": 2.239549430095832e-05,
        "epoch": 0.5243619489559165,
        "step": 4068
    },
    {
        "loss": 1.921,
        "grad_norm": 2.5163073539733887,
        "learning_rate": 2.235713803090912e-05,
        "epoch": 0.5244908481567414,
        "step": 4069
    },
    {
        "loss": 1.5896,
        "grad_norm": 3.1987130641937256,
        "learning_rate": 2.2318810500734522e-05,
        "epoch": 0.5246197473575663,
        "step": 4070
    },
    {
        "loss": 1.5684,
        "grad_norm": 2.1087026596069336,
        "learning_rate": 2.228051172462169e-05,
        "epoch": 0.5247486465583914,
        "step": 4071
    },
    {
        "loss": 1.5244,
        "grad_norm": 1.1136696338653564,
        "learning_rate": 2.2242241716747036e-05,
        "epoch": 0.5248775457592163,
        "step": 4072
    },
    {
        "loss": 2.2399,
        "grad_norm": 1.5849072933197021,
        "learning_rate": 2.2204000491276368e-05,
        "epoch": 0.5250064449600412,
        "step": 4073
    },
    {
        "loss": 2.173,
        "grad_norm": 1.2481400966644287,
        "learning_rate": 2.2165788062364967e-05,
        "epoch": 0.5251353441608662,
        "step": 4074
    },
    {
        "loss": 1.2576,
        "grad_norm": 2.2593557834625244,
        "learning_rate": 2.2127604444157226e-05,
        "epoch": 0.5252642433616912,
        "step": 4075
    },
    {
        "loss": 2.0366,
        "grad_norm": 2.2239792346954346,
        "learning_rate": 2.208944965078704e-05,
        "epoch": 0.5253931425625161,
        "step": 4076
    },
    {
        "loss": 2.1708,
        "grad_norm": 1.7301185131072998,
        "learning_rate": 2.2051323696377634e-05,
        "epoch": 0.525522041763341,
        "step": 4077
    },
    {
        "loss": 2.0615,
        "grad_norm": 2.055514335632324,
        "learning_rate": 2.2013226595041393e-05,
        "epoch": 0.525650940964166,
        "step": 4078
    },
    {
        "loss": 1.6027,
        "grad_norm": 2.414097309112549,
        "learning_rate": 2.1975158360880255e-05,
        "epoch": 0.525779840164991,
        "step": 4079
    },
    {
        "loss": 1.2127,
        "grad_norm": 2.56752872467041,
        "learning_rate": 2.1937119007985317e-05,
        "epoch": 0.525908739365816,
        "step": 4080
    },
    {
        "loss": 1.9926,
        "grad_norm": 2.1378087997436523,
        "learning_rate": 2.189910855043701e-05,
        "epoch": 0.5260376385666409,
        "step": 4081
    },
    {
        "loss": 2.2396,
        "grad_norm": 2.043241500854492,
        "learning_rate": 2.1861127002305082e-05,
        "epoch": 0.5261665377674658,
        "step": 4082
    },
    {
        "loss": 2.1532,
        "grad_norm": 2.0474436283111572,
        "learning_rate": 2.1823174377648647e-05,
        "epoch": 0.5262954369682908,
        "step": 4083
    },
    {
        "loss": 2.232,
        "grad_norm": 2.2203760147094727,
        "learning_rate": 2.178525069051597e-05,
        "epoch": 0.5264243361691158,
        "step": 4084
    },
    {
        "loss": 1.8093,
        "grad_norm": 1.8192272186279297,
        "learning_rate": 2.1747355954944736e-05,
        "epoch": 0.5265532353699407,
        "step": 4085
    },
    {
        "loss": 2.305,
        "grad_norm": 1.220859408378601,
        "learning_rate": 2.1709490184961823e-05,
        "epoch": 0.5266821345707656,
        "step": 4086
    },
    {
        "loss": 1.786,
        "grad_norm": 2.098503589630127,
        "learning_rate": 2.1671653394583447e-05,
        "epoch": 0.5268110337715907,
        "step": 4087
    },
    {
        "loss": 1.3769,
        "grad_norm": 1.8866009712219238,
        "learning_rate": 2.1633845597815104e-05,
        "epoch": 0.5269399329724156,
        "step": 4088
    },
    {
        "loss": 1.2202,
        "grad_norm": 1.9303748607635498,
        "learning_rate": 2.1596066808651494e-05,
        "epoch": 0.5270688321732405,
        "step": 4089
    },
    {
        "loss": 1.5682,
        "grad_norm": 1.4910207986831665,
        "learning_rate": 2.155831704107657e-05,
        "epoch": 0.5271977313740654,
        "step": 4090
    },
    {
        "loss": 2.3687,
        "grad_norm": 1.9382550716400146,
        "learning_rate": 2.1520596309063733e-05,
        "epoch": 0.5273266305748905,
        "step": 4091
    },
    {
        "loss": 1.9703,
        "grad_norm": 2.0466220378875732,
        "learning_rate": 2.148290462657534e-05,
        "epoch": 0.5274555297757154,
        "step": 4092
    },
    {
        "loss": 2.1665,
        "grad_norm": 2.0307602882385254,
        "learning_rate": 2.1445242007563205e-05,
        "epoch": 0.5275844289765403,
        "step": 4093
    },
    {
        "loss": 1.5105,
        "grad_norm": 3.0913636684417725,
        "learning_rate": 2.1407608465968376e-05,
        "epoch": 0.5277133281773653,
        "step": 4094
    },
    {
        "loss": 1.1591,
        "grad_norm": 2.307105302810669,
        "learning_rate": 2.1370004015720957e-05,
        "epoch": 0.5278422273781903,
        "step": 4095
    },
    {
        "loss": 2.4583,
        "grad_norm": 1.685575246810913,
        "learning_rate": 2.1332428670740545e-05,
        "epoch": 0.5279711265790152,
        "step": 4096
    },
    {
        "loss": 1.9472,
        "grad_norm": 2.461456060409546,
        "learning_rate": 2.1294882444935795e-05,
        "epoch": 0.5281000257798402,
        "step": 4097
    },
    {
        "loss": 1.9158,
        "grad_norm": 2.080514907836914,
        "learning_rate": 2.125736535220457e-05,
        "epoch": 0.5282289249806651,
        "step": 4098
    },
    {
        "loss": 1.7325,
        "grad_norm": 1.367562174797058,
        "learning_rate": 2.1219877406434042e-05,
        "epoch": 0.52835782418149,
        "step": 4099
    },
    {
        "loss": 1.9924,
        "grad_norm": 2.1668052673339844,
        "learning_rate": 2.1182418621500554e-05,
        "epoch": 0.5284867233823151,
        "step": 4100
    },
    {
        "loss": 2.1327,
        "grad_norm": 1.683834195137024,
        "learning_rate": 2.1144989011269667e-05,
        "epoch": 0.52861562258314,
        "step": 4101
    },
    {
        "loss": 1.4107,
        "grad_norm": 1.660549521446228,
        "learning_rate": 2.1107588589596095e-05,
        "epoch": 0.5287445217839649,
        "step": 4102
    },
    {
        "loss": 1.881,
        "grad_norm": 1.8911359310150146,
        "learning_rate": 2.1070217370323808e-05,
        "epoch": 0.5288734209847898,
        "step": 4103
    },
    {
        "loss": 1.4606,
        "grad_norm": 2.74088191986084,
        "learning_rate": 2.1032875367285922e-05,
        "epoch": 0.5290023201856149,
        "step": 4104
    },
    {
        "loss": 1.7032,
        "grad_norm": 2.0980567932128906,
        "learning_rate": 2.0995562594304772e-05,
        "epoch": 0.5291312193864398,
        "step": 4105
    },
    {
        "loss": 0.9642,
        "grad_norm": 2.616182327270508,
        "learning_rate": 2.0958279065191865e-05,
        "epoch": 0.5292601185872647,
        "step": 4106
    },
    {
        "loss": 2.4979,
        "grad_norm": 1.2106752395629883,
        "learning_rate": 2.0921024793747813e-05,
        "epoch": 0.5293890177880897,
        "step": 4107
    },
    {
        "loss": 2.1707,
        "grad_norm": 1.1064872741699219,
        "learning_rate": 2.0883799793762583e-05,
        "epoch": 0.5295179169889147,
        "step": 4108
    },
    {
        "loss": 0.6149,
        "grad_norm": 1.654646635055542,
        "learning_rate": 2.0846604079015064e-05,
        "epoch": 0.5296468161897396,
        "step": 4109
    },
    {
        "loss": 1.7371,
        "grad_norm": 2.73077130317688,
        "learning_rate": 2.0809437663273478e-05,
        "epoch": 0.5297757153905646,
        "step": 4110
    },
    {
        "loss": 1.8867,
        "grad_norm": 1.558112382888794,
        "learning_rate": 2.07723005602952e-05,
        "epoch": 0.5299046145913895,
        "step": 4111
    },
    {
        "loss": 1.1035,
        "grad_norm": 2.4043262004852295,
        "learning_rate": 2.0735192783826585e-05,
        "epoch": 0.5300335137922145,
        "step": 4112
    },
    {
        "loss": 1.96,
        "grad_norm": 1.1976853609085083,
        "learning_rate": 2.0698114347603357e-05,
        "epoch": 0.5301624129930395,
        "step": 4113
    },
    {
        "loss": 1.6236,
        "grad_norm": 2.0071849822998047,
        "learning_rate": 2.0661065265350234e-05,
        "epoch": 0.5302913121938644,
        "step": 4114
    },
    {
        "loss": 2.1883,
        "grad_norm": 1.8854316473007202,
        "learning_rate": 2.062404555078111e-05,
        "epoch": 0.5304202113946893,
        "step": 4115
    },
    {
        "loss": 0.5945,
        "grad_norm": 1.4634511470794678,
        "learning_rate": 2.058705521759904e-05,
        "epoch": 0.5305491105955144,
        "step": 4116
    },
    {
        "loss": 1.9039,
        "grad_norm": 2.0531632900238037,
        "learning_rate": 2.0550094279496124e-05,
        "epoch": 0.5306780097963393,
        "step": 4117
    },
    {
        "loss": 1.374,
        "grad_norm": 2.0364649295806885,
        "learning_rate": 2.051316275015368e-05,
        "epoch": 0.5308069089971642,
        "step": 4118
    },
    {
        "loss": 1.566,
        "grad_norm": 1.9148470163345337,
        "learning_rate": 2.0476260643242007e-05,
        "epoch": 0.5309358081979891,
        "step": 4119
    },
    {
        "loss": 1.6844,
        "grad_norm": 2.565696954727173,
        "learning_rate": 2.0439387972420733e-05,
        "epoch": 0.5310647073988142,
        "step": 4120
    },
    {
        "loss": 1.6629,
        "grad_norm": 2.7363834381103516,
        "learning_rate": 2.0402544751338316e-05,
        "epoch": 0.5311936065996391,
        "step": 4121
    },
    {
        "loss": 2.0741,
        "grad_norm": 1.1295591592788696,
        "learning_rate": 2.0365730993632503e-05,
        "epoch": 0.531322505800464,
        "step": 4122
    },
    {
        "loss": 1.4721,
        "grad_norm": 2.5256283283233643,
        "learning_rate": 2.0328946712930142e-05,
        "epoch": 0.531451405001289,
        "step": 4123
    },
    {
        "loss": 2.1386,
        "grad_norm": 1.4782696962356567,
        "learning_rate": 2.029219192284696e-05,
        "epoch": 0.531580304202114,
        "step": 4124
    },
    {
        "loss": 2.0614,
        "grad_norm": 1.1657119989395142,
        "learning_rate": 2.0255466636988085e-05,
        "epoch": 0.5317092034029389,
        "step": 4125
    },
    {
        "loss": 2.0697,
        "grad_norm": 1.2715530395507812,
        "learning_rate": 2.0218770868947423e-05,
        "epoch": 0.5318381026037639,
        "step": 4126
    },
    {
        "loss": 2.4788,
        "grad_norm": 2.259101152420044,
        "learning_rate": 2.0182104632308152e-05,
        "epoch": 0.5319670018045888,
        "step": 4127
    },
    {
        "loss": 1.2522,
        "grad_norm": 2.6952826976776123,
        "learning_rate": 2.014546794064247e-05,
        "epoch": 0.5320959010054138,
        "step": 4128
    },
    {
        "loss": 2.0304,
        "grad_norm": 2.039389133453369,
        "learning_rate": 2.0108860807511566e-05,
        "epoch": 0.5322248002062387,
        "step": 4129
    },
    {
        "loss": 2.1263,
        "grad_norm": 1.2302296161651611,
        "learning_rate": 2.0072283246465796e-05,
        "epoch": 0.5323536994070637,
        "step": 4130
    },
    {
        "loss": 2.0009,
        "grad_norm": 1.7878458499908447,
        "learning_rate": 2.0035735271044476e-05,
        "epoch": 0.5324825986078886,
        "step": 4131
    },
    {
        "loss": 1.9452,
        "grad_norm": 1.2682106494903564,
        "learning_rate": 1.999921689477603e-05,
        "epoch": 0.5326114978087136,
        "step": 4132
    },
    {
        "loss": 0.5531,
        "grad_norm": 2.6313629150390625,
        "learning_rate": 1.996272813117791e-05,
        "epoch": 0.5327403970095386,
        "step": 4133
    },
    {
        "loss": 1.9337,
        "grad_norm": 2.18685245513916,
        "learning_rate": 1.9926268993756647e-05,
        "epoch": 0.5328692962103635,
        "step": 4134
    },
    {
        "loss": 1.5326,
        "grad_norm": 2.1915102005004883,
        "learning_rate": 1.9889839496007712e-05,
        "epoch": 0.5329981954111884,
        "step": 4135
    },
    {
        "loss": 2.1256,
        "grad_norm": 2.4630348682403564,
        "learning_rate": 1.985343965141563e-05,
        "epoch": 0.5331270946120134,
        "step": 4136
    },
    {
        "loss": 1.8548,
        "grad_norm": 1.310207724571228,
        "learning_rate": 1.9817069473454086e-05,
        "epoch": 0.5332559938128384,
        "step": 4137
    },
    {
        "loss": 1.5341,
        "grad_norm": 3.0639524459838867,
        "learning_rate": 1.978072897558555e-05,
        "epoch": 0.5333848930136633,
        "step": 4138
    },
    {
        "loss": 1.8429,
        "grad_norm": 1.2502492666244507,
        "learning_rate": 1.9744418171261693e-05,
        "epoch": 0.5335137922144882,
        "step": 4139
    },
    {
        "loss": 1.9233,
        "grad_norm": 1.5434147119522095,
        "learning_rate": 1.970813707392317e-05,
        "epoch": 0.5336426914153132,
        "step": 4140
    },
    {
        "loss": 1.5439,
        "grad_norm": 2.4805431365966797,
        "learning_rate": 1.967188569699948e-05,
        "epoch": 0.5337715906161382,
        "step": 4141
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.5025131702423096,
        "learning_rate": 1.9635664053909365e-05,
        "epoch": 0.5339004898169631,
        "step": 4142
    },
    {
        "loss": 1.9663,
        "grad_norm": 1.4472707509994507,
        "learning_rate": 1.9599472158060405e-05,
        "epoch": 0.5340293890177881,
        "step": 4143
    },
    {
        "loss": 1.7138,
        "grad_norm": 1.49749755859375,
        "learning_rate": 1.9563310022849148e-05,
        "epoch": 0.534158288218613,
        "step": 4144
    },
    {
        "loss": 1.8874,
        "grad_norm": 1.3191744089126587,
        "learning_rate": 1.9527177661661232e-05,
        "epoch": 0.534287187419438,
        "step": 4145
    },
    {
        "loss": 1.8299,
        "grad_norm": 1.4890234470367432,
        "learning_rate": 1.9491075087871224e-05,
        "epoch": 0.534416086620263,
        "step": 4146
    },
    {
        "loss": 0.9716,
        "grad_norm": 1.5031495094299316,
        "learning_rate": 1.945500231484264e-05,
        "epoch": 0.5345449858210879,
        "step": 4147
    },
    {
        "loss": 2.4228,
        "grad_norm": 1.2260065078735352,
        "learning_rate": 1.9418959355928025e-05,
        "epoch": 0.5346738850219128,
        "step": 4148
    },
    {
        "loss": 1.9048,
        "grad_norm": 4.185139179229736,
        "learning_rate": 1.938294622446879e-05,
        "epoch": 0.5348027842227379,
        "step": 4149
    },
    {
        "loss": 1.3719,
        "grad_norm": 2.5656960010528564,
        "learning_rate": 1.934696293379541e-05,
        "epoch": 0.5349316834235628,
        "step": 4150
    },
    {
        "loss": 2.1,
        "grad_norm": 1.4184565544128418,
        "learning_rate": 1.9311009497227283e-05,
        "epoch": 0.5350605826243877,
        "step": 4151
    },
    {
        "loss": 1.4727,
        "grad_norm": 1.9585691690444946,
        "learning_rate": 1.9275085928072733e-05,
        "epoch": 0.5351894818252126,
        "step": 4152
    },
    {
        "loss": 1.8941,
        "grad_norm": 1.685308575630188,
        "learning_rate": 1.9239192239628977e-05,
        "epoch": 0.5353183810260377,
        "step": 4153
    },
    {
        "loss": 2.0318,
        "grad_norm": 1.9625951051712036,
        "learning_rate": 1.9203328445182366e-05,
        "epoch": 0.5354472802268626,
        "step": 4154
    },
    {
        "loss": 1.6681,
        "grad_norm": 2.59230637550354,
        "learning_rate": 1.9167494558007927e-05,
        "epoch": 0.5355761794276875,
        "step": 4155
    },
    {
        "loss": 1.0073,
        "grad_norm": 3.819019079208374,
        "learning_rate": 1.91316905913698e-05,
        "epoch": 0.5357050786285125,
        "step": 4156
    },
    {
        "loss": 2.0499,
        "grad_norm": 1.4551372528076172,
        "learning_rate": 1.909591655852103e-05,
        "epoch": 0.5358339778293375,
        "step": 4157
    },
    {
        "loss": 2.1601,
        "grad_norm": 2.303102493286133,
        "learning_rate": 1.906017247270344e-05,
        "epoch": 0.5359628770301624,
        "step": 4158
    },
    {
        "loss": 0.893,
        "grad_norm": 2.2673022747039795,
        "learning_rate": 1.902445834714798e-05,
        "epoch": 0.5360917762309874,
        "step": 4159
    },
    {
        "loss": 1.3663,
        "grad_norm": 2.235924005508423,
        "learning_rate": 1.8988774195074367e-05,
        "epoch": 0.5362206754318123,
        "step": 4160
    },
    {
        "loss": 2.0748,
        "grad_norm": 2.098482847213745,
        "learning_rate": 1.8953120029691217e-05,
        "epoch": 0.5363495746326373,
        "step": 4161
    },
    {
        "loss": 1.6473,
        "grad_norm": 1.7595300674438477,
        "learning_rate": 1.8917495864196135e-05,
        "epoch": 0.5364784738334623,
        "step": 4162
    },
    {
        "loss": 1.9278,
        "grad_norm": 2.461744785308838,
        "learning_rate": 1.8881901711775584e-05,
        "epoch": 0.5366073730342872,
        "step": 4163
    },
    {
        "loss": 1.7397,
        "grad_norm": 2.5973191261291504,
        "learning_rate": 1.884633758560487e-05,
        "epoch": 0.5367362722351121,
        "step": 4164
    },
    {
        "loss": 1.9443,
        "grad_norm": 1.9855161905288696,
        "learning_rate": 1.881080349884824e-05,
        "epoch": 0.5368651714359371,
        "step": 4165
    },
    {
        "loss": 1.9745,
        "grad_norm": 1.6093792915344238,
        "learning_rate": 1.8775299464658836e-05,
        "epoch": 0.5369940706367621,
        "step": 4166
    },
    {
        "loss": 1.6154,
        "grad_norm": 2.04616641998291,
        "learning_rate": 1.8739825496178597e-05,
        "epoch": 0.537122969837587,
        "step": 4167
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.287602186203003,
        "learning_rate": 1.8704381606538425e-05,
        "epoch": 0.5372518690384119,
        "step": 4168
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.2504348754882812,
        "learning_rate": 1.866896780885802e-05,
        "epoch": 0.537380768239237,
        "step": 4169
    },
    {
        "loss": 1.9522,
        "grad_norm": 1.896285057067871,
        "learning_rate": 1.8633584116245918e-05,
        "epoch": 0.5375096674400619,
        "step": 4170
    },
    {
        "loss": 2.1232,
        "grad_norm": 2.215259552001953,
        "learning_rate": 1.8598230541799688e-05,
        "epoch": 0.5376385666408868,
        "step": 4171
    },
    {
        "loss": 1.4613,
        "grad_norm": 1.8719993829727173,
        "learning_rate": 1.8562907098605498e-05,
        "epoch": 0.5377674658417118,
        "step": 4172
    },
    {
        "loss": 2.051,
        "grad_norm": 2.523185968399048,
        "learning_rate": 1.852761379973853e-05,
        "epoch": 0.5378963650425367,
        "step": 4173
    },
    {
        "loss": 1.6248,
        "grad_norm": 2.232999324798584,
        "learning_rate": 1.8492350658262835e-05,
        "epoch": 0.5380252642433617,
        "step": 4174
    },
    {
        "loss": 1.7559,
        "grad_norm": 2.447765827178955,
        "learning_rate": 1.8457117687231094e-05,
        "epoch": 0.5381541634441867,
        "step": 4175
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.152729868888855,
        "learning_rate": 1.842191489968509e-05,
        "epoch": 0.5382830626450116,
        "step": 4176
    },
    {
        "loss": 2.0737,
        "grad_norm": 1.4115989208221436,
        "learning_rate": 1.8386742308655248e-05,
        "epoch": 0.5384119618458365,
        "step": 4177
    },
    {
        "loss": 1.882,
        "grad_norm": 2.5441527366638184,
        "learning_rate": 1.8351599927160844e-05,
        "epoch": 0.5385408610466615,
        "step": 4178
    },
    {
        "loss": 1.2282,
        "grad_norm": 2.5447888374328613,
        "learning_rate": 1.831648776821002e-05,
        "epoch": 0.5386697602474865,
        "step": 4179
    },
    {
        "loss": 2.1217,
        "grad_norm": 2.3409605026245117,
        "learning_rate": 1.8281405844799716e-05,
        "epoch": 0.5387986594483114,
        "step": 4180
    },
    {
        "loss": 2.6239,
        "grad_norm": 1.4588963985443115,
        "learning_rate": 1.8246354169915703e-05,
        "epoch": 0.5389275586491363,
        "step": 4181
    },
    {
        "loss": 2.3455,
        "grad_norm": 2.1046719551086426,
        "learning_rate": 1.821133275653246e-05,
        "epoch": 0.5390564578499614,
        "step": 4182
    },
    {
        "loss": 1.9584,
        "grad_norm": 2.2309184074401855,
        "learning_rate": 1.8176341617613392e-05,
        "epoch": 0.5391853570507863,
        "step": 4183
    },
    {
        "loss": 2.1568,
        "grad_norm": 1.338005781173706,
        "learning_rate": 1.814138076611057e-05,
        "epoch": 0.5393142562516112,
        "step": 4184
    },
    {
        "loss": 2.4397,
        "grad_norm": 1.4634610414505005,
        "learning_rate": 1.8106450214964965e-05,
        "epoch": 0.5394431554524362,
        "step": 4185
    },
    {
        "loss": 1.8577,
        "grad_norm": 2.088285207748413,
        "learning_rate": 1.8071549977106327e-05,
        "epoch": 0.5395720546532612,
        "step": 4186
    },
    {
        "loss": 1.9138,
        "grad_norm": 2.169543504714966,
        "learning_rate": 1.8036680065453016e-05,
        "epoch": 0.5397009538540861,
        "step": 4187
    },
    {
        "loss": 1.8158,
        "grad_norm": 2.4475080966949463,
        "learning_rate": 1.8001840492912447e-05,
        "epoch": 0.539829853054911,
        "step": 4188
    },
    {
        "loss": 1.6047,
        "grad_norm": 1.6028825044631958,
        "learning_rate": 1.796703127238054e-05,
        "epoch": 0.539958752255736,
        "step": 4189
    },
    {
        "loss": 2.0,
        "grad_norm": 1.460951566696167,
        "learning_rate": 1.7932252416742124e-05,
        "epoch": 0.540087651456561,
        "step": 4190
    },
    {
        "loss": 1.8569,
        "grad_norm": 1.4514803886413574,
        "learning_rate": 1.7897503938870798e-05,
        "epoch": 0.5402165506573859,
        "step": 4191
    },
    {
        "loss": 0.7267,
        "grad_norm": 2.0054850578308105,
        "learning_rate": 1.7862785851628816e-05,
        "epoch": 0.5403454498582109,
        "step": 4192
    },
    {
        "loss": 2.1025,
        "grad_norm": 1.327301263809204,
        "learning_rate": 1.78280981678673e-05,
        "epoch": 0.5404743490590358,
        "step": 4193
    },
    {
        "loss": 1.3239,
        "grad_norm": 2.272108316421509,
        "learning_rate": 1.7793440900426016e-05,
        "epoch": 0.5406032482598608,
        "step": 4194
    },
    {
        "loss": 2.0248,
        "grad_norm": 1.6199424266815186,
        "learning_rate": 1.7758814062133534e-05,
        "epoch": 0.5407321474606858,
        "step": 4195
    },
    {
        "loss": 1.4851,
        "grad_norm": 2.4172372817993164,
        "learning_rate": 1.7724217665807164e-05,
        "epoch": 0.5408610466615107,
        "step": 4196
    },
    {
        "loss": 1.7993,
        "grad_norm": 2.262705087661743,
        "learning_rate": 1.7689651724252876e-05,
        "epoch": 0.5409899458623356,
        "step": 4197
    },
    {
        "loss": 2.3829,
        "grad_norm": 1.8539395332336426,
        "learning_rate": 1.765511625026548e-05,
        "epoch": 0.5411188450631607,
        "step": 4198
    },
    {
        "loss": 1.892,
        "grad_norm": 2.464949369430542,
        "learning_rate": 1.7620611256628373e-05,
        "epoch": 0.5412477442639856,
        "step": 4199
    },
    {
        "loss": 1.8642,
        "grad_norm": 1.8957306146621704,
        "learning_rate": 1.7586136756113846e-05,
        "epoch": 0.5413766434648105,
        "step": 4200
    },
    {
        "loss": 1.9878,
        "grad_norm": 2.0777108669281006,
        "learning_rate": 1.7551692761482685e-05,
        "epoch": 0.5415055426656354,
        "step": 4201
    },
    {
        "loss": 2.113,
        "grad_norm": 1.8295841217041016,
        "learning_rate": 1.7517279285484555e-05,
        "epoch": 0.5416344418664605,
        "step": 4202
    },
    {
        "loss": 1.5666,
        "grad_norm": 2.1136300563812256,
        "learning_rate": 1.7482896340857822e-05,
        "epoch": 0.5417633410672854,
        "step": 4203
    },
    {
        "loss": 1.4858,
        "grad_norm": 2.3611440658569336,
        "learning_rate": 1.7448543940329352e-05,
        "epoch": 0.5418922402681103,
        "step": 4204
    },
    {
        "loss": 1.899,
        "grad_norm": 1.6078150272369385,
        "learning_rate": 1.7414222096614984e-05,
        "epoch": 0.5420211394689353,
        "step": 4205
    },
    {
        "loss": 2.2405,
        "grad_norm": 1.9618239402770996,
        "learning_rate": 1.7379930822419066e-05,
        "epoch": 0.5421500386697602,
        "step": 4206
    },
    {
        "loss": 2.1572,
        "grad_norm": 1.8533587455749512,
        "learning_rate": 1.7345670130434655e-05,
        "epoch": 0.5422789378705852,
        "step": 4207
    },
    {
        "loss": 1.6952,
        "grad_norm": 1.4487371444702148,
        "learning_rate": 1.731144003334354e-05,
        "epoch": 0.5424078370714102,
        "step": 4208
    },
    {
        "loss": 1.8615,
        "grad_norm": 1.5991250276565552,
        "learning_rate": 1.727724054381612e-05,
        "epoch": 0.5425367362722351,
        "step": 4209
    },
    {
        "loss": 1.9707,
        "grad_norm": 1.7848236560821533,
        "learning_rate": 1.7243071674511545e-05,
        "epoch": 0.54266563547306,
        "step": 4210
    },
    {
        "loss": 2.1818,
        "grad_norm": 1.9711573123931885,
        "learning_rate": 1.7208933438077533e-05,
        "epoch": 0.542794534673885,
        "step": 4211
    },
    {
        "loss": 1.7781,
        "grad_norm": 2.1376700401306152,
        "learning_rate": 1.7174825847150545e-05,
        "epoch": 0.54292343387471,
        "step": 4212
    },
    {
        "loss": 1.9381,
        "grad_norm": 1.1788990497589111,
        "learning_rate": 1.7140748914355666e-05,
        "epoch": 0.5430523330755349,
        "step": 4213
    },
    {
        "loss": 1.9122,
        "grad_norm": 1.557984709739685,
        "learning_rate": 1.710670265230665e-05,
        "epoch": 0.5431812322763598,
        "step": 4214
    },
    {
        "loss": 0.7393,
        "grad_norm": 1.553127408027649,
        "learning_rate": 1.7072687073605863e-05,
        "epoch": 0.5433101314771849,
        "step": 4215
    },
    {
        "loss": 1.9641,
        "grad_norm": 1.2896161079406738,
        "learning_rate": 1.703870219084429e-05,
        "epoch": 0.5434390306780098,
        "step": 4216
    },
    {
        "loss": 1.3282,
        "grad_norm": 2.3022007942199707,
        "learning_rate": 1.7004748016601725e-05,
        "epoch": 0.5435679298788347,
        "step": 4217
    },
    {
        "loss": 2.165,
        "grad_norm": 1.3566616773605347,
        "learning_rate": 1.697082456344633e-05,
        "epoch": 0.5436968290796597,
        "step": 4218
    },
    {
        "loss": 1.3216,
        "grad_norm": 2.2567055225372314,
        "learning_rate": 1.693693184393508e-05,
        "epoch": 0.5438257282804847,
        "step": 4219
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.426931381225586,
        "learning_rate": 1.6903069870613585e-05,
        "epoch": 0.5439546274813096,
        "step": 4220
    },
    {
        "loss": 2.2323,
        "grad_norm": 1.114991545677185,
        "learning_rate": 1.6869238656015896e-05,
        "epoch": 0.5440835266821346,
        "step": 4221
    },
    {
        "loss": 1.8893,
        "grad_norm": 2.1698150634765625,
        "learning_rate": 1.683543821266491e-05,
        "epoch": 0.5442124258829595,
        "step": 4222
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.471979022026062,
        "learning_rate": 1.680166855307198e-05,
        "epoch": 0.5443413250837845,
        "step": 4223
    },
    {
        "loss": 1.8618,
        "grad_norm": 2.133702278137207,
        "learning_rate": 1.6767929689737084e-05,
        "epoch": 0.5444702242846094,
        "step": 4224
    },
    {
        "loss": 2.8515,
        "grad_norm": 1.5182417631149292,
        "learning_rate": 1.673422163514884e-05,
        "epoch": 0.5445991234854344,
        "step": 4225
    },
    {
        "loss": 1.5045,
        "grad_norm": 2.320873498916626,
        "learning_rate": 1.670054440178447e-05,
        "epoch": 0.5447280226862593,
        "step": 4226
    },
    {
        "loss": 1.1866,
        "grad_norm": 1.9800028800964355,
        "learning_rate": 1.666689800210971e-05,
        "epoch": 0.5448569218870843,
        "step": 4227
    },
    {
        "loss": 2.1729,
        "grad_norm": 1.748462438583374,
        "learning_rate": 1.663328244857898e-05,
        "epoch": 0.5449858210879093,
        "step": 4228
    },
    {
        "loss": 1.7567,
        "grad_norm": 1.8756533861160278,
        "learning_rate": 1.6599697753635233e-05,
        "epoch": 0.5451147202887342,
        "step": 4229
    },
    {
        "loss": 2.1799,
        "grad_norm": 1.9114012718200684,
        "learning_rate": 1.6566143929709964e-05,
        "epoch": 0.5452436194895591,
        "step": 4230
    },
    {
        "loss": 2.0327,
        "grad_norm": 1.7934406995773315,
        "learning_rate": 1.6532620989223334e-05,
        "epoch": 0.5453725186903842,
        "step": 4231
    },
    {
        "loss": 1.8904,
        "grad_norm": 2.0052707195281982,
        "learning_rate": 1.6499128944584006e-05,
        "epoch": 0.5455014178912091,
        "step": 4232
    },
    {
        "loss": 1.9533,
        "grad_norm": 1.3113523721694946,
        "learning_rate": 1.6465667808189145e-05,
        "epoch": 0.545630317092034,
        "step": 4233
    },
    {
        "loss": 1.967,
        "grad_norm": 1.1091824769973755,
        "learning_rate": 1.6432237592424692e-05,
        "epoch": 0.545759216292859,
        "step": 4234
    },
    {
        "loss": 1.8113,
        "grad_norm": 1.1330980062484741,
        "learning_rate": 1.6398838309664864e-05,
        "epoch": 0.545888115493684,
        "step": 4235
    },
    {
        "loss": 1.9666,
        "grad_norm": 1.3978389501571655,
        "learning_rate": 1.636546997227263e-05,
        "epoch": 0.5460170146945089,
        "step": 4236
    },
    {
        "loss": 1.9185,
        "grad_norm": 1.2517247200012207,
        "learning_rate": 1.633213259259947e-05,
        "epoch": 0.5461459138953338,
        "step": 4237
    },
    {
        "loss": 2.6017,
        "grad_norm": 1.610679268836975,
        "learning_rate": 1.629882618298526e-05,
        "epoch": 0.5462748130961588,
        "step": 4238
    },
    {
        "loss": 1.6404,
        "grad_norm": 2.1301016807556152,
        "learning_rate": 1.626555075575865e-05,
        "epoch": 0.5464037122969838,
        "step": 4239
    },
    {
        "loss": 1.8813,
        "grad_norm": 1.8705646991729736,
        "learning_rate": 1.6232306323236646e-05,
        "epoch": 0.5465326114978087,
        "step": 4240
    },
    {
        "loss": 1.5813,
        "grad_norm": 2.0171430110931396,
        "learning_rate": 1.619909289772481e-05,
        "epoch": 0.5466615106986337,
        "step": 4241
    },
    {
        "loss": 1.4174,
        "grad_norm": 1.6810671091079712,
        "learning_rate": 1.6165910491517266e-05,
        "epoch": 0.5467904098994586,
        "step": 4242
    },
    {
        "loss": 1.9484,
        "grad_norm": 1.2470706701278687,
        "learning_rate": 1.613275911689668e-05,
        "epoch": 0.5469193091002835,
        "step": 4243
    },
    {
        "loss": 1.8499,
        "grad_norm": 3.1481378078460693,
        "learning_rate": 1.609963878613412e-05,
        "epoch": 0.5470482083011086,
        "step": 4244
    },
    {
        "loss": 2.0714,
        "grad_norm": 1.6993064880371094,
        "learning_rate": 1.6066549511489282e-05,
        "epoch": 0.5471771075019335,
        "step": 4245
    },
    {
        "loss": 1.5978,
        "grad_norm": 1.7888669967651367,
        "learning_rate": 1.6033491305210325e-05,
        "epoch": 0.5473060067027584,
        "step": 4246
    },
    {
        "loss": 1.636,
        "grad_norm": 2.183194160461426,
        "learning_rate": 1.600046417953386e-05,
        "epoch": 0.5474349059035833,
        "step": 4247
    },
    {
        "loss": 2.0235,
        "grad_norm": 2.044468402862549,
        "learning_rate": 1.596746814668506e-05,
        "epoch": 0.5475638051044084,
        "step": 4248
    },
    {
        "loss": 2.1229,
        "grad_norm": 1.9743632078170776,
        "learning_rate": 1.5934503218877616e-05,
        "epoch": 0.5476927043052333,
        "step": 4249
    },
    {
        "loss": 1.8776,
        "grad_norm": 1.8642489910125732,
        "learning_rate": 1.5901569408313512e-05,
        "epoch": 0.5478216035060582,
        "step": 4250
    },
    {
        "loss": 1.9501,
        "grad_norm": 1.133246660232544,
        "learning_rate": 1.5868666727183524e-05,
        "epoch": 0.5479505027068832,
        "step": 4251
    },
    {
        "loss": 2.0241,
        "grad_norm": 1.1039936542510986,
        "learning_rate": 1.5835795187666602e-05,
        "epoch": 0.5480794019077082,
        "step": 4252
    },
    {
        "loss": 2.1032,
        "grad_norm": 1.9460018873214722,
        "learning_rate": 1.580295480193035e-05,
        "epoch": 0.5482083011085331,
        "step": 4253
    },
    {
        "loss": 2.0745,
        "grad_norm": 1.6541942358016968,
        "learning_rate": 1.5770145582130847e-05,
        "epoch": 0.5483372003093581,
        "step": 4254
    },
    {
        "loss": 1.6826,
        "grad_norm": 2.9443488121032715,
        "learning_rate": 1.573736754041246e-05,
        "epoch": 0.548466099510183,
        "step": 4255
    },
    {
        "loss": 1.9423,
        "grad_norm": 1.1898859739303589,
        "learning_rate": 1.5704620688908257e-05,
        "epoch": 0.548594998711008,
        "step": 4256
    },
    {
        "loss": 2.0551,
        "grad_norm": 3.115658760070801,
        "learning_rate": 1.5671905039739566e-05,
        "epoch": 0.548723897911833,
        "step": 4257
    },
    {
        "loss": 2.022,
        "grad_norm": 1.4631496667861938,
        "learning_rate": 1.563922060501629e-05,
        "epoch": 0.5488527971126579,
        "step": 4258
    },
    {
        "loss": 1.6057,
        "grad_norm": 1.5999681949615479,
        "learning_rate": 1.560656739683668e-05,
        "epoch": 0.5489816963134828,
        "step": 4259
    },
    {
        "loss": 1.8028,
        "grad_norm": 1.824791431427002,
        "learning_rate": 1.5573945427287505e-05,
        "epoch": 0.5491105955143079,
        "step": 4260
    },
    {
        "loss": 2.1265,
        "grad_norm": 1.251244068145752,
        "learning_rate": 1.5541354708443955e-05,
        "epoch": 0.5492394947151328,
        "step": 4261
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.6795268058776855,
        "learning_rate": 1.55087952523696e-05,
        "epoch": 0.5493683939159577,
        "step": 4262
    },
    {
        "loss": 1.8513,
        "grad_norm": 1.289998173713684,
        "learning_rate": 1.5476267071116533e-05,
        "epoch": 0.5494972931167826,
        "step": 4263
    },
    {
        "loss": 1.6734,
        "grad_norm": 1.8360551595687866,
        "learning_rate": 1.5443770176725148e-05,
        "epoch": 0.5496261923176077,
        "step": 4264
    },
    {
        "loss": 1.8499,
        "grad_norm": 1.7635536193847656,
        "learning_rate": 1.5411304581224363e-05,
        "epoch": 0.5497550915184326,
        "step": 4265
    },
    {
        "loss": 1.8391,
        "grad_norm": 1.4491498470306396,
        "learning_rate": 1.5378870296631508e-05,
        "epoch": 0.5498839907192575,
        "step": 4266
    },
    {
        "loss": 0.9054,
        "grad_norm": 1.8461359739303589,
        "learning_rate": 1.5346467334952185e-05,
        "epoch": 0.5500128899200825,
        "step": 4267
    },
    {
        "loss": 1.4822,
        "grad_norm": 2.7122974395751953,
        "learning_rate": 1.5314095708180607e-05,
        "epoch": 0.5501417891209075,
        "step": 4268
    },
    {
        "loss": 2.0194,
        "grad_norm": 1.336883783340454,
        "learning_rate": 1.528175542829925e-05,
        "epoch": 0.5502706883217324,
        "step": 4269
    },
    {
        "loss": 2.1974,
        "grad_norm": 2.0329673290252686,
        "learning_rate": 1.5249446507278997e-05,
        "epoch": 0.5503995875225574,
        "step": 4270
    },
    {
        "loss": 1.6029,
        "grad_norm": 1.9972950220108032,
        "learning_rate": 1.5217168957079198e-05,
        "epoch": 0.5505284867233823,
        "step": 4271
    },
    {
        "loss": 2.0695,
        "grad_norm": 1.508408784866333,
        "learning_rate": 1.5184922789647483e-05,
        "epoch": 0.5506573859242073,
        "step": 4272
    },
    {
        "loss": 1.5087,
        "grad_norm": 2.3286890983581543,
        "learning_rate": 1.5152708016919981e-05,
        "epoch": 0.5507862851250322,
        "step": 4273
    },
    {
        "loss": 1.2172,
        "grad_norm": 2.0994105339050293,
        "learning_rate": 1.5120524650821088e-05,
        "epoch": 0.5509151843258572,
        "step": 4274
    },
    {
        "loss": 2.6428,
        "grad_norm": 1.9928239583969116,
        "learning_rate": 1.5088372703263664e-05,
        "epoch": 0.5510440835266821,
        "step": 4275
    },
    {
        "loss": 0.9666,
        "grad_norm": 1.7005831003189087,
        "learning_rate": 1.5056252186148923e-05,
        "epoch": 0.5511729827275071,
        "step": 4276
    },
    {
        "loss": 2.229,
        "grad_norm": 1.9179607629776,
        "learning_rate": 1.5024163111366386e-05,
        "epoch": 0.5513018819283321,
        "step": 4277
    },
    {
        "loss": 1.0955,
        "grad_norm": 2.026927947998047,
        "learning_rate": 1.4992105490794017e-05,
        "epoch": 0.551430781129157,
        "step": 4278
    },
    {
        "loss": 2.2965,
        "grad_norm": 1.8752650022506714,
        "learning_rate": 1.4960079336298027e-05,
        "epoch": 0.5515596803299819,
        "step": 4279
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.3342208862304688,
        "learning_rate": 1.4928084659733166e-05,
        "epoch": 0.5516885795308069,
        "step": 4280
    },
    {
        "loss": 1.5933,
        "grad_norm": 3.5366647243499756,
        "learning_rate": 1.4896121472942303e-05,
        "epoch": 0.5518174787316319,
        "step": 4281
    },
    {
        "loss": 1.4929,
        "grad_norm": 2.3380684852600098,
        "learning_rate": 1.486418978775681e-05,
        "epoch": 0.5519463779324568,
        "step": 4282
    },
    {
        "loss": 2.1811,
        "grad_norm": 2.2601983547210693,
        "learning_rate": 1.4832289615996386e-05,
        "epoch": 0.5520752771332817,
        "step": 4283
    },
    {
        "loss": 1.3426,
        "grad_norm": 2.0365238189697266,
        "learning_rate": 1.4800420969468942e-05,
        "epoch": 0.5522041763341067,
        "step": 4284
    },
    {
        "loss": 2.0976,
        "grad_norm": 1.4275989532470703,
        "learning_rate": 1.4768583859970897e-05,
        "epoch": 0.5523330755349317,
        "step": 4285
    },
    {
        "loss": 2.2867,
        "grad_norm": 2.043102264404297,
        "learning_rate": 1.473677829928688e-05,
        "epoch": 0.5524619747357566,
        "step": 4286
    },
    {
        "loss": 1.8289,
        "grad_norm": 2.192741870880127,
        "learning_rate": 1.470500429918984e-05,
        "epoch": 0.5525908739365816,
        "step": 4287
    },
    {
        "loss": 1.5976,
        "grad_norm": 2.02775502204895,
        "learning_rate": 1.4673261871441092e-05,
        "epoch": 0.5527197731374065,
        "step": 4288
    },
    {
        "loss": 2.1849,
        "grad_norm": 1.4006943702697754,
        "learning_rate": 1.4641551027790278e-05,
        "epoch": 0.5528486723382315,
        "step": 4289
    },
    {
        "loss": 2.2681,
        "grad_norm": 2.8579976558685303,
        "learning_rate": 1.4609871779975271e-05,
        "epoch": 0.5529775715390565,
        "step": 4290
    },
    {
        "loss": 2.4947,
        "grad_norm": 1.6677464246749878,
        "learning_rate": 1.457822413972232e-05,
        "epoch": 0.5531064707398814,
        "step": 4291
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.8816804885864258,
        "learning_rate": 1.4546608118745975e-05,
        "epoch": 0.5532353699407063,
        "step": 4292
    },
    {
        "loss": 1.4494,
        "grad_norm": 2.2289671897888184,
        "learning_rate": 1.4515023728749e-05,
        "epoch": 0.5533642691415314,
        "step": 4293
    },
    {
        "loss": 2.0213,
        "grad_norm": 2.151637077331543,
        "learning_rate": 1.4483470981422553e-05,
        "epoch": 0.5534931683423563,
        "step": 4294
    },
    {
        "loss": 2.1358,
        "grad_norm": 1.674565315246582,
        "learning_rate": 1.4451949888446015e-05,
        "epoch": 0.5536220675431812,
        "step": 4295
    },
    {
        "loss": 2.2215,
        "grad_norm": 2.1728861331939697,
        "learning_rate": 1.4420460461487029e-05,
        "epoch": 0.5537509667440061,
        "step": 4296
    },
    {
        "loss": 2.3511,
        "grad_norm": 1.862243890762329,
        "learning_rate": 1.4389002712201671e-05,
        "epoch": 0.5538798659448312,
        "step": 4297
    },
    {
        "loss": 2.02,
        "grad_norm": 2.029050588607788,
        "learning_rate": 1.4357576652234039e-05,
        "epoch": 0.5540087651456561,
        "step": 4298
    },
    {
        "loss": 2.0761,
        "grad_norm": 2.0162816047668457,
        "learning_rate": 1.4326182293216717e-05,
        "epoch": 0.554137664346481,
        "step": 4299
    },
    {
        "loss": 2.1569,
        "grad_norm": 1.3824610710144043,
        "learning_rate": 1.4294819646770497e-05,
        "epoch": 0.554266563547306,
        "step": 4300
    },
    {
        "loss": 1.4092,
        "grad_norm": 2.280081272125244,
        "learning_rate": 1.4263488724504314e-05,
        "epoch": 0.554395462748131,
        "step": 4301
    },
    {
        "loss": 2.01,
        "grad_norm": 1.940639853477478,
        "learning_rate": 1.423218953801556e-05,
        "epoch": 0.5545243619489559,
        "step": 4302
    },
    {
        "loss": 2.4901,
        "grad_norm": 1.3334341049194336,
        "learning_rate": 1.4200922098889746e-05,
        "epoch": 0.5546532611497809,
        "step": 4303
    },
    {
        "loss": 2.0165,
        "grad_norm": 1.8715165853500366,
        "learning_rate": 1.4169686418700623e-05,
        "epoch": 0.5547821603506058,
        "step": 4304
    },
    {
        "loss": 1.6828,
        "grad_norm": 4.498284339904785,
        "learning_rate": 1.4138482509010253e-05,
        "epoch": 0.5549110595514308,
        "step": 4305
    },
    {
        "loss": 2.4447,
        "grad_norm": 1.2518770694732666,
        "learning_rate": 1.4107310381368955e-05,
        "epoch": 0.5550399587522558,
        "step": 4306
    },
    {
        "loss": 1.9046,
        "grad_norm": 1.5514875650405884,
        "learning_rate": 1.4076170047315163e-05,
        "epoch": 0.5551688579530807,
        "step": 4307
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.059218168258667,
        "learning_rate": 1.4045061518375668e-05,
        "epoch": 0.5552977571539056,
        "step": 4308
    },
    {
        "loss": 2.407,
        "grad_norm": 1.1720319986343384,
        "learning_rate": 1.401398480606545e-05,
        "epoch": 0.5554266563547307,
        "step": 4309
    },
    {
        "loss": 2.0514,
        "grad_norm": 2.0426931381225586,
        "learning_rate": 1.398293992188766e-05,
        "epoch": 0.5555555555555556,
        "step": 4310
    },
    {
        "loss": 1.7553,
        "grad_norm": 2.7844655513763428,
        "learning_rate": 1.3951926877333732e-05,
        "epoch": 0.5556844547563805,
        "step": 4311
    },
    {
        "loss": 1.9639,
        "grad_norm": 1.8389703035354614,
        "learning_rate": 1.3920945683883335e-05,
        "epoch": 0.5558133539572054,
        "step": 4312
    },
    {
        "loss": 1.6991,
        "grad_norm": 1.159153699874878,
        "learning_rate": 1.3889996353004198e-05,
        "epoch": 0.5559422531580305,
        "step": 4313
    },
    {
        "loss": 1.6693,
        "grad_norm": 2.0067646503448486,
        "learning_rate": 1.3859078896152488e-05,
        "epoch": 0.5560711523588554,
        "step": 4314
    },
    {
        "loss": 1.6565,
        "grad_norm": 1.5733182430267334,
        "learning_rate": 1.3828193324772354e-05,
        "epoch": 0.5562000515596803,
        "step": 4315
    },
    {
        "loss": 1.808,
        "grad_norm": 1.7606929540634155,
        "learning_rate": 1.379733965029626e-05,
        "epoch": 0.5563289507605053,
        "step": 4316
    },
    {
        "loss": 1.9351,
        "grad_norm": 1.9058607816696167,
        "learning_rate": 1.3766517884144902e-05,
        "epoch": 0.5564578499613302,
        "step": 4317
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.2543059587478638,
        "learning_rate": 1.3735728037726981e-05,
        "epoch": 0.5565867491621552,
        "step": 4318
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.3883743286132812,
        "learning_rate": 1.370497012243962e-05,
        "epoch": 0.5567156483629802,
        "step": 4319
    },
    {
        "loss": 1.4577,
        "grad_norm": 1.6603556871414185,
        "learning_rate": 1.367424414966797e-05,
        "epoch": 0.5568445475638051,
        "step": 4320
    },
    {
        "loss": 2.3939,
        "grad_norm": 1.8091739416122437,
        "learning_rate": 1.3643550130785366e-05,
        "epoch": 0.55697344676463,
        "step": 4321
    },
    {
        "loss": 1.8692,
        "grad_norm": 2.392947196960449,
        "learning_rate": 1.3612888077153363e-05,
        "epoch": 0.557102345965455,
        "step": 4322
    },
    {
        "loss": 2.0251,
        "grad_norm": 1.9044617414474487,
        "learning_rate": 1.3582258000121684e-05,
        "epoch": 0.55723124516628,
        "step": 4323
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.7796175479888916,
        "learning_rate": 1.3551659911028191e-05,
        "epoch": 0.5573601443671049,
        "step": 4324
    },
    {
        "loss": 2.0586,
        "grad_norm": 2.0907723903656006,
        "learning_rate": 1.3521093821198904e-05,
        "epoch": 0.5574890435679298,
        "step": 4325
    },
    {
        "loss": 1.5109,
        "grad_norm": 2.3915748596191406,
        "learning_rate": 1.3490559741948018e-05,
        "epoch": 0.5576179427687549,
        "step": 4326
    },
    {
        "loss": 1.89,
        "grad_norm": 2.0269932746887207,
        "learning_rate": 1.3460057684577843e-05,
        "epoch": 0.5577468419695798,
        "step": 4327
    },
    {
        "loss": 2.3893,
        "grad_norm": 1.2775287628173828,
        "learning_rate": 1.3429587660378873e-05,
        "epoch": 0.5578757411704047,
        "step": 4328
    },
    {
        "loss": 1.7555,
        "grad_norm": 1.9711048603057861,
        "learning_rate": 1.3399149680629785e-05,
        "epoch": 0.5580046403712297,
        "step": 4329
    },
    {
        "loss": 1.9891,
        "grad_norm": 1.3755433559417725,
        "learning_rate": 1.3368743756597224e-05,
        "epoch": 0.5581335395720547,
        "step": 4330
    },
    {
        "loss": 1.57,
        "grad_norm": 2.127080202102661,
        "learning_rate": 1.3338369899536196e-05,
        "epoch": 0.5582624387728796,
        "step": 4331
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.9448243379592896,
        "learning_rate": 1.3308028120689698e-05,
        "epoch": 0.5583913379737045,
        "step": 4332
    },
    {
        "loss": 2.1169,
        "grad_norm": 1.5998903512954712,
        "learning_rate": 1.3277718431288844e-05,
        "epoch": 0.5585202371745295,
        "step": 4333
    },
    {
        "loss": 2.164,
        "grad_norm": 1.0664513111114502,
        "learning_rate": 1.3247440842552982e-05,
        "epoch": 0.5586491363753545,
        "step": 4334
    },
    {
        "loss": 1.639,
        "grad_norm": 1.5469768047332764,
        "learning_rate": 1.3217195365689405e-05,
        "epoch": 0.5587780355761794,
        "step": 4335
    },
    {
        "loss": 1.6804,
        "grad_norm": 2.283276319503784,
        "learning_rate": 1.3186982011893723e-05,
        "epoch": 0.5589069347770044,
        "step": 4336
    },
    {
        "loss": 2.0666,
        "grad_norm": 1.3444989919662476,
        "learning_rate": 1.3156800792349489e-05,
        "epoch": 0.5590358339778293,
        "step": 4337
    },
    {
        "loss": 1.6303,
        "grad_norm": 2.2763559818267822,
        "learning_rate": 1.3126651718228467e-05,
        "epoch": 0.5591647331786543,
        "step": 4338
    },
    {
        "loss": 1.5013,
        "grad_norm": 1.879835605621338,
        "learning_rate": 1.309653480069043e-05,
        "epoch": 0.5592936323794793,
        "step": 4339
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.650559663772583,
        "learning_rate": 1.3066450050883328e-05,
        "epoch": 0.5594225315803042,
        "step": 4340
    },
    {
        "loss": 2.424,
        "grad_norm": 2.150839328765869,
        "learning_rate": 1.3036397479943196e-05,
        "epoch": 0.5595514307811291,
        "step": 4341
    },
    {
        "loss": 1.9588,
        "grad_norm": 2.103534460067749,
        "learning_rate": 1.3006377098994082e-05,
        "epoch": 0.5596803299819542,
        "step": 4342
    },
    {
        "loss": 2.0429,
        "grad_norm": 1.8168423175811768,
        "learning_rate": 1.297638891914823e-05,
        "epoch": 0.5598092291827791,
        "step": 4343
    },
    {
        "loss": 1.9297,
        "grad_norm": 2.0881154537200928,
        "learning_rate": 1.294643295150586e-05,
        "epoch": 0.559938128383604,
        "step": 4344
    },
    {
        "loss": 1.7084,
        "grad_norm": 1.5101484060287476,
        "learning_rate": 1.2916509207155336e-05,
        "epoch": 0.560067027584429,
        "step": 4345
    },
    {
        "loss": 1.4622,
        "grad_norm": 1.3073134422302246,
        "learning_rate": 1.288661769717312e-05,
        "epoch": 0.560195926785254,
        "step": 4346
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.9275060892105103,
        "learning_rate": 1.2856758432623584e-05,
        "epoch": 0.5603248259860789,
        "step": 4347
    },
    {
        "loss": 2.5193,
        "grad_norm": 1.3945386409759521,
        "learning_rate": 1.2826931424559385e-05,
        "epoch": 0.5604537251869038,
        "step": 4348
    },
    {
        "loss": 2.0177,
        "grad_norm": 2.16202712059021,
        "learning_rate": 1.2797136684021094e-05,
        "epoch": 0.5605826243877288,
        "step": 4349
    },
    {
        "loss": 2.2195,
        "grad_norm": 2.1776859760284424,
        "learning_rate": 1.2767374222037354e-05,
        "epoch": 0.5607115235885538,
        "step": 4350
    },
    {
        "loss": 1.9866,
        "grad_norm": 2.6907639503479004,
        "learning_rate": 1.2737644049624898e-05,
        "epoch": 0.5608404227893787,
        "step": 4351
    },
    {
        "loss": 1.1146,
        "grad_norm": 1.7290961742401123,
        "learning_rate": 1.270794617778851e-05,
        "epoch": 0.5609693219902037,
        "step": 4352
    },
    {
        "loss": 1.69,
        "grad_norm": 3.0970473289489746,
        "learning_rate": 1.2678280617520988e-05,
        "epoch": 0.5610982211910286,
        "step": 4353
    },
    {
        "loss": 2.0212,
        "grad_norm": 2.4167122840881348,
        "learning_rate": 1.2648647379803124e-05,
        "epoch": 0.5612271203918535,
        "step": 4354
    },
    {
        "loss": 1.8126,
        "grad_norm": 1.560004711151123,
        "learning_rate": 1.2619046475603902e-05,
        "epoch": 0.5613560195926786,
        "step": 4355
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.0871262550354004,
        "learning_rate": 1.2589477915880165e-05,
        "epoch": 0.5614849187935035,
        "step": 4356
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.5778682231903076,
        "learning_rate": 1.2559941711576896e-05,
        "epoch": 0.5616138179943284,
        "step": 4357
    },
    {
        "loss": 1.4061,
        "grad_norm": 2.3289966583251953,
        "learning_rate": 1.2530437873627043e-05,
        "epoch": 0.5617427171951533,
        "step": 4358
    },
    {
        "loss": 1.1117,
        "grad_norm": 1.7328635454177856,
        "learning_rate": 1.2500966412951553e-05,
        "epoch": 0.5618716163959784,
        "step": 4359
    },
    {
        "loss": 2.085,
        "grad_norm": 1.4071868658065796,
        "learning_rate": 1.2471527340459533e-05,
        "epoch": 0.5620005155968033,
        "step": 4360
    },
    {
        "loss": 2.5634,
        "grad_norm": 1.4420368671417236,
        "learning_rate": 1.2442120667047886e-05,
        "epoch": 0.5621294147976282,
        "step": 4361
    },
    {
        "loss": 1.6221,
        "grad_norm": 2.683095932006836,
        "learning_rate": 1.2412746403601672e-05,
        "epoch": 0.5622583139984532,
        "step": 4362
    },
    {
        "loss": 1.8478,
        "grad_norm": 4.191770076751709,
        "learning_rate": 1.2383404560993961e-05,
        "epoch": 0.5623872131992782,
        "step": 4363
    },
    {
        "loss": 1.9567,
        "grad_norm": 1.776055097579956,
        "learning_rate": 1.2354095150085675e-05,
        "epoch": 0.5625161124001031,
        "step": 4364
    },
    {
        "loss": 1.6433,
        "grad_norm": 2.0220367908477783,
        "learning_rate": 1.2324818181725928e-05,
        "epoch": 0.5626450116009281,
        "step": 4365
    },
    {
        "loss": 1.5606,
        "grad_norm": 1.8117635250091553,
        "learning_rate": 1.2295573666751692e-05,
        "epoch": 0.562773910801753,
        "step": 4366
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.027809977531433,
        "learning_rate": 1.2266361615987931e-05,
        "epoch": 0.562902810002578,
        "step": 4367
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.4392410516738892,
        "learning_rate": 1.2237182040247663e-05,
        "epoch": 0.563031709203403,
        "step": 4368
    },
    {
        "loss": 0.9589,
        "grad_norm": 2.7775020599365234,
        "learning_rate": 1.220803495033186e-05,
        "epoch": 0.5631606084042279,
        "step": 4369
    },
    {
        "loss": 1.7573,
        "grad_norm": 1.6875859498977661,
        "learning_rate": 1.2178920357029421e-05,
        "epoch": 0.5632895076050528,
        "step": 4370
    },
    {
        "loss": 1.5347,
        "grad_norm": 2.4975483417510986,
        "learning_rate": 1.2149838271117253e-05,
        "epoch": 0.5634184068058778,
        "step": 4371
    },
    {
        "loss": 1.185,
        "grad_norm": 2.401928663253784,
        "learning_rate": 1.2120788703360264e-05,
        "epoch": 0.5635473060067028,
        "step": 4372
    },
    {
        "loss": 1.7801,
        "grad_norm": 2.7498581409454346,
        "learning_rate": 1.2091771664511253e-05,
        "epoch": 0.5636762052075277,
        "step": 4373
    },
    {
        "loss": 2.0771,
        "grad_norm": 2.5170273780822754,
        "learning_rate": 1.2062787165311018e-05,
        "epoch": 0.5638051044083526,
        "step": 4374
    },
    {
        "loss": 2.0653,
        "grad_norm": 1.9931919574737549,
        "learning_rate": 1.2033835216488376e-05,
        "epoch": 0.5639340036091777,
        "step": 4375
    },
    {
        "loss": 1.3804,
        "grad_norm": 2.3351752758026123,
        "learning_rate": 1.2004915828759899e-05,
        "epoch": 0.5640629028100026,
        "step": 4376
    },
    {
        "loss": 1.7699,
        "grad_norm": 2.0288772583007812,
        "learning_rate": 1.197602901283037e-05,
        "epoch": 0.5641918020108275,
        "step": 4377
    },
    {
        "loss": 1.6135,
        "grad_norm": 2.730029344558716,
        "learning_rate": 1.1947174779392278e-05,
        "epoch": 0.5643207012116525,
        "step": 4378
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.5692057609558105,
        "learning_rate": 1.1918353139126204e-05,
        "epoch": 0.5644496004124775,
        "step": 4379
    },
    {
        "loss": 2.383,
        "grad_norm": 1.5815818309783936,
        "learning_rate": 1.1889564102700646e-05,
        "epoch": 0.5645784996133024,
        "step": 4380
    },
    {
        "loss": 0.9883,
        "grad_norm": 2.237596035003662,
        "learning_rate": 1.1860807680771902e-05,
        "epoch": 0.5647073988141273,
        "step": 4381
    },
    {
        "loss": 1.3171,
        "grad_norm": 1.6635258197784424,
        "learning_rate": 1.1832083883984402e-05,
        "epoch": 0.5648362980149523,
        "step": 4382
    },
    {
        "loss": 1.6135,
        "grad_norm": 2.298396348953247,
        "learning_rate": 1.1803392722970352e-05,
        "epoch": 0.5649651972157773,
        "step": 4383
    },
    {
        "loss": 2.0474,
        "grad_norm": 1.9032201766967773,
        "learning_rate": 1.1774734208349914e-05,
        "epoch": 0.5650940964166022,
        "step": 4384
    },
    {
        "loss": 1.9703,
        "grad_norm": 2.1778359413146973,
        "learning_rate": 1.1746108350731178e-05,
        "epoch": 0.5652229956174272,
        "step": 4385
    },
    {
        "loss": 2.0513,
        "grad_norm": 1.091760516166687,
        "learning_rate": 1.1717515160710168e-05,
        "epoch": 0.5653518948182521,
        "step": 4386
    },
    {
        "loss": 1.7637,
        "grad_norm": 2.6362080574035645,
        "learning_rate": 1.1688954648870743e-05,
        "epoch": 0.565480794019077,
        "step": 4387
    },
    {
        "loss": 1.4813,
        "grad_norm": 2.282334327697754,
        "learning_rate": 1.1660426825784743e-05,
        "epoch": 0.5656096932199021,
        "step": 4388
    },
    {
        "loss": 1.9223,
        "grad_norm": 2.5419888496398926,
        "learning_rate": 1.16319317020119e-05,
        "epoch": 0.565738592420727,
        "step": 4389
    },
    {
        "loss": 2.3734,
        "grad_norm": 1.5149626731872559,
        "learning_rate": 1.1603469288099766e-05,
        "epoch": 0.5658674916215519,
        "step": 4390
    },
    {
        "loss": 2.5396,
        "grad_norm": 1.3946083784103394,
        "learning_rate": 1.157503959458386e-05,
        "epoch": 0.5659963908223768,
        "step": 4391
    },
    {
        "loss": 1.4385,
        "grad_norm": 2.2555758953094482,
        "learning_rate": 1.1546642631987625e-05,
        "epoch": 0.5661252900232019,
        "step": 4392
    },
    {
        "loss": 1.8043,
        "grad_norm": 1.9854183197021484,
        "learning_rate": 1.1518278410822214e-05,
        "epoch": 0.5662541892240268,
        "step": 4393
    },
    {
        "loss": 1.1881,
        "grad_norm": 1.9872310161590576,
        "learning_rate": 1.1489946941586882e-05,
        "epoch": 0.5663830884248517,
        "step": 4394
    },
    {
        "loss": 0.915,
        "grad_norm": 1.8699722290039062,
        "learning_rate": 1.1461648234768619e-05,
        "epoch": 0.5665119876256767,
        "step": 4395
    },
    {
        "loss": 1.3569,
        "grad_norm": 2.403886556625366,
        "learning_rate": 1.143338230084231e-05,
        "epoch": 0.5666408868265017,
        "step": 4396
    },
    {
        "loss": 1.4828,
        "grad_norm": 1.624916911125183,
        "learning_rate": 1.1405149150270778e-05,
        "epoch": 0.5667697860273266,
        "step": 4397
    },
    {
        "loss": 2.163,
        "grad_norm": 1.3726097345352173,
        "learning_rate": 1.1376948793504572e-05,
        "epoch": 0.5668986852281516,
        "step": 4398
    },
    {
        "loss": 1.2007,
        "grad_norm": 2.170243978500366,
        "learning_rate": 1.1348781240982265e-05,
        "epoch": 0.5670275844289765,
        "step": 4399
    },
    {
        "loss": 1.2671,
        "grad_norm": 2.82055401802063,
        "learning_rate": 1.1320646503130183e-05,
        "epoch": 0.5671564836298015,
        "step": 4400
    },
    {
        "loss": 1.4667,
        "grad_norm": 2.6291873455047607,
        "learning_rate": 1.1292544590362519e-05,
        "epoch": 0.5672853828306265,
        "step": 4401
    },
    {
        "loss": 1.8567,
        "grad_norm": 2.145122528076172,
        "learning_rate": 1.1264475513081341e-05,
        "epoch": 0.5674142820314514,
        "step": 4402
    },
    {
        "loss": 2.239,
        "grad_norm": 1.0918985605239868,
        "learning_rate": 1.123643928167656e-05,
        "epoch": 0.5675431812322763,
        "step": 4403
    },
    {
        "loss": 2.4249,
        "grad_norm": 1.8180745840072632,
        "learning_rate": 1.1208435906525928e-05,
        "epoch": 0.5676720804331014,
        "step": 4404
    },
    {
        "loss": 1.9248,
        "grad_norm": 1.7260717153549194,
        "learning_rate": 1.1180465397994988e-05,
        "epoch": 0.5678009796339263,
        "step": 4405
    },
    {
        "loss": 1.0464,
        "grad_norm": 2.646441698074341,
        "learning_rate": 1.1152527766437214e-05,
        "epoch": 0.5679298788347512,
        "step": 4406
    },
    {
        "loss": 2.2972,
        "grad_norm": 2.083942174911499,
        "learning_rate": 1.1124623022193793e-05,
        "epoch": 0.5680587780355761,
        "step": 4407
    },
    {
        "loss": 1.7961,
        "grad_norm": 2.542046546936035,
        "learning_rate": 1.1096751175593834e-05,
        "epoch": 0.5681876772364012,
        "step": 4408
    },
    {
        "loss": 1.3272,
        "grad_norm": 2.197556734085083,
        "learning_rate": 1.1068912236954265e-05,
        "epoch": 0.5683165764372261,
        "step": 4409
    },
    {
        "loss": 2.4492,
        "grad_norm": 1.5468626022338867,
        "learning_rate": 1.1041106216579699e-05,
        "epoch": 0.568445475638051,
        "step": 4410
    },
    {
        "loss": 1.7638,
        "grad_norm": 1.837560772895813,
        "learning_rate": 1.1013333124762782e-05,
        "epoch": 0.568574374838876,
        "step": 4411
    },
    {
        "loss": 1.9225,
        "grad_norm": 2.068695545196533,
        "learning_rate": 1.0985592971783797e-05,
        "epoch": 0.568703274039701,
        "step": 4412
    },
    {
        "loss": 1.4644,
        "grad_norm": 2.0079665184020996,
        "learning_rate": 1.0957885767910891e-05,
        "epoch": 0.5688321732405259,
        "step": 4413
    },
    {
        "loss": 1.8187,
        "grad_norm": 1.5496610403060913,
        "learning_rate": 1.0930211523400014e-05,
        "epoch": 0.5689610724413509,
        "step": 4414
    },
    {
        "loss": 1.3245,
        "grad_norm": 2.219208002090454,
        "learning_rate": 1.090257024849497e-05,
        "epoch": 0.5690899716421758,
        "step": 4415
    },
    {
        "loss": 2.1375,
        "grad_norm": 0.9373962879180908,
        "learning_rate": 1.0874961953427253e-05,
        "epoch": 0.5692188708430008,
        "step": 4416
    },
    {
        "loss": 1.9475,
        "grad_norm": 2.2594642639160156,
        "learning_rate": 1.0847386648416202e-05,
        "epoch": 0.5693477700438258,
        "step": 4417
    },
    {
        "loss": 1.7612,
        "grad_norm": 1.9243546724319458,
        "learning_rate": 1.0819844343669017e-05,
        "epoch": 0.5694766692446507,
        "step": 4418
    },
    {
        "loss": 1.7796,
        "grad_norm": 1.389273762702942,
        "learning_rate": 1.079233504938053e-05,
        "epoch": 0.5696055684454756,
        "step": 4419
    },
    {
        "loss": 1.9058,
        "grad_norm": 1.832656741142273,
        "learning_rate": 1.0764858775733465e-05,
        "epoch": 0.5697344676463006,
        "step": 4420
    },
    {
        "loss": 1.0693,
        "grad_norm": 2.3539583683013916,
        "learning_rate": 1.0737415532898332e-05,
        "epoch": 0.5698633668471256,
        "step": 4421
    },
    {
        "loss": 1.9297,
        "grad_norm": 1.8714667558670044,
        "learning_rate": 1.0710005331033301e-05,
        "epoch": 0.5699922660479505,
        "step": 4422
    },
    {
        "loss": 1.4986,
        "grad_norm": 2.3668501377105713,
        "learning_rate": 1.0682628180284503e-05,
        "epoch": 0.5701211652487754,
        "step": 4423
    },
    {
        "loss": 1.7809,
        "grad_norm": 2.001197099685669,
        "learning_rate": 1.065528409078561e-05,
        "epoch": 0.5702500644496004,
        "step": 4424
    },
    {
        "loss": 1.8667,
        "grad_norm": 1.805464744567871,
        "learning_rate": 1.0627973072658215e-05,
        "epoch": 0.5703789636504254,
        "step": 4425
    },
    {
        "loss": 2.0296,
        "grad_norm": 3.1163954734802246,
        "learning_rate": 1.0600695136011651e-05,
        "epoch": 0.5705078628512503,
        "step": 4426
    },
    {
        "loss": 2.7311,
        "grad_norm": 2.0593860149383545,
        "learning_rate": 1.0573450290942876e-05,
        "epoch": 0.5706367620520753,
        "step": 4427
    },
    {
        "loss": 2.2606,
        "grad_norm": 1.5113240480422974,
        "learning_rate": 1.0546238547536813e-05,
        "epoch": 0.5707656612529002,
        "step": 4428
    },
    {
        "loss": 1.7112,
        "grad_norm": 1.751350998878479,
        "learning_rate": 1.051905991586597e-05,
        "epoch": 0.5708945604537252,
        "step": 4429
    },
    {
        "loss": 0.892,
        "grad_norm": 2.32904314994812,
        "learning_rate": 1.0491914405990622e-05,
        "epoch": 0.5710234596545501,
        "step": 4430
    },
    {
        "loss": 1.279,
        "grad_norm": 2.7074968814849854,
        "learning_rate": 1.046480202795883e-05,
        "epoch": 0.5711523588553751,
        "step": 4431
    },
    {
        "loss": 2.4235,
        "grad_norm": 1.112349271774292,
        "learning_rate": 1.0437722791806393e-05,
        "epoch": 0.5712812580562,
        "step": 4432
    },
    {
        "loss": 2.3895,
        "grad_norm": 1.1822850704193115,
        "learning_rate": 1.0410676707556798e-05,
        "epoch": 0.571410157257025,
        "step": 4433
    },
    {
        "loss": 2.0447,
        "grad_norm": 1.2492233514785767,
        "learning_rate": 1.0383663785221254e-05,
        "epoch": 0.57153905645785,
        "step": 4434
    },
    {
        "loss": 1.3778,
        "grad_norm": 1.7802518606185913,
        "learning_rate": 1.035668403479878e-05,
        "epoch": 0.5716679556586749,
        "step": 4435
    },
    {
        "loss": 2.2856,
        "grad_norm": 1.3980940580368042,
        "learning_rate": 1.0329737466276013e-05,
        "epoch": 0.5717968548594998,
        "step": 4436
    },
    {
        "loss": 1.8312,
        "grad_norm": 1.711655616760254,
        "learning_rate": 1.0302824089627393e-05,
        "epoch": 0.5719257540603249,
        "step": 4437
    },
    {
        "loss": 0.9022,
        "grad_norm": 2.424970865249634,
        "learning_rate": 1.0275943914815011e-05,
        "epoch": 0.5720546532611498,
        "step": 4438
    },
    {
        "loss": 2.0849,
        "grad_norm": 2.790407419204712,
        "learning_rate": 1.024909695178865e-05,
        "epoch": 0.5721835524619747,
        "step": 4439
    },
    {
        "loss": 1.356,
        "grad_norm": 2.396044969558716,
        "learning_rate": 1.022228321048594e-05,
        "epoch": 0.5723124516627996,
        "step": 4440
    },
    {
        "loss": 1.6958,
        "grad_norm": 2.631075382232666,
        "learning_rate": 1.0195502700832026e-05,
        "epoch": 0.5724413508636247,
        "step": 4441
    },
    {
        "loss": 1.7315,
        "grad_norm": 1.9738510847091675,
        "learning_rate": 1.0168755432739873e-05,
        "epoch": 0.5725702500644496,
        "step": 4442
    },
    {
        "loss": 2.1414,
        "grad_norm": 1.9869462251663208,
        "learning_rate": 1.0142041416110137e-05,
        "epoch": 0.5726991492652745,
        "step": 4443
    },
    {
        "loss": 1.572,
        "grad_norm": 2.427595615386963,
        "learning_rate": 1.0115360660831064e-05,
        "epoch": 0.5728280484660995,
        "step": 4444
    },
    {
        "loss": 2.1872,
        "grad_norm": 1.548008918762207,
        "learning_rate": 1.0088713176778752e-05,
        "epoch": 0.5729569476669245,
        "step": 4445
    },
    {
        "loss": 1.7664,
        "grad_norm": 1.7073986530303955,
        "learning_rate": 1.0062098973816836e-05,
        "epoch": 0.5730858468677494,
        "step": 4446
    },
    {
        "loss": 2.1781,
        "grad_norm": 1.299752950668335,
        "learning_rate": 1.00355180617967e-05,
        "epoch": 0.5732147460685744,
        "step": 4447
    },
    {
        "loss": 2.1349,
        "grad_norm": 1.6373509168624878,
        "learning_rate": 1.0008970450557397e-05,
        "epoch": 0.5733436452693993,
        "step": 4448
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.4741781949996948,
        "learning_rate": 9.982456149925668e-06,
        "epoch": 0.5734725444702243,
        "step": 4449
    },
    {
        "loss": 2.6062,
        "grad_norm": 1.6089390516281128,
        "learning_rate": 9.955975169715881e-06,
        "epoch": 0.5736014436710493,
        "step": 4450
    },
    {
        "loss": 2.1992,
        "grad_norm": 1.9444953203201294,
        "learning_rate": 9.929527519730108e-06,
        "epoch": 0.5737303428718742,
        "step": 4451
    },
    {
        "loss": 1.7119,
        "grad_norm": 2.074289083480835,
        "learning_rate": 9.903113209758096e-06,
        "epoch": 0.5738592420726991,
        "step": 4452
    },
    {
        "loss": 1.981,
        "grad_norm": 1.978346824645996,
        "learning_rate": 9.876732249577181e-06,
        "epoch": 0.5739881412735242,
        "step": 4453
    },
    {
        "loss": 1.9694,
        "grad_norm": 1.98911714553833,
        "learning_rate": 9.850384648952427e-06,
        "epoch": 0.5741170404743491,
        "step": 4454
    },
    {
        "loss": 1.9793,
        "grad_norm": 1.494035005569458,
        "learning_rate": 9.824070417636567e-06,
        "epoch": 0.574245939675174,
        "step": 4455
    },
    {
        "loss": 1.7924,
        "grad_norm": 1.4206130504608154,
        "learning_rate": 9.79778956536983e-06,
        "epoch": 0.5743748388759989,
        "step": 4456
    },
    {
        "loss": 1.6879,
        "grad_norm": 1.797653079032898,
        "learning_rate": 9.771542101880304e-06,
        "epoch": 0.574503738076824,
        "step": 4457
    },
    {
        "loss": 1.0126,
        "grad_norm": 2.327643632888794,
        "learning_rate": 9.745328036883572e-06,
        "epoch": 0.5746326372776489,
        "step": 4458
    },
    {
        "loss": 2.3872,
        "grad_norm": 1.6127468347549438,
        "learning_rate": 9.719147380082872e-06,
        "epoch": 0.5747615364784738,
        "step": 4459
    },
    {
        "loss": 1.9378,
        "grad_norm": 1.7181633710861206,
        "learning_rate": 9.693000141169162e-06,
        "epoch": 0.5748904356792988,
        "step": 4460
    },
    {
        "loss": 1.847,
        "grad_norm": 1.7923314571380615,
        "learning_rate": 9.666886329820868e-06,
        "epoch": 0.5750193348801237,
        "step": 4461
    },
    {
        "loss": 1.6396,
        "grad_norm": 1.774999737739563,
        "learning_rate": 9.640805955704235e-06,
        "epoch": 0.5751482340809487,
        "step": 4462
    },
    {
        "loss": 0.6461,
        "grad_norm": 1.8288754224777222,
        "learning_rate": 9.614759028472997e-06,
        "epoch": 0.5752771332817737,
        "step": 4463
    },
    {
        "loss": 1.8772,
        "grad_norm": 2.0022339820861816,
        "learning_rate": 9.588745557768541e-06,
        "epoch": 0.5754060324825986,
        "step": 4464
    },
    {
        "loss": 2.4753,
        "grad_norm": 2.3324811458587646,
        "learning_rate": 9.562765553219888e-06,
        "epoch": 0.5755349316834235,
        "step": 4465
    },
    {
        "loss": 1.937,
        "grad_norm": 1.5279666185379028,
        "learning_rate": 9.536819024443677e-06,
        "epoch": 0.5756638308842486,
        "step": 4466
    },
    {
        "loss": 2.1177,
        "grad_norm": 1.8929443359375,
        "learning_rate": 9.510905981044104e-06,
        "epoch": 0.5757927300850735,
        "step": 4467
    },
    {
        "loss": 2.5335,
        "grad_norm": 1.2048958539962769,
        "learning_rate": 9.48502643261302e-06,
        "epoch": 0.5759216292858984,
        "step": 4468
    },
    {
        "loss": 2.1493,
        "grad_norm": 1.2295371294021606,
        "learning_rate": 9.459180388729894e-06,
        "epoch": 0.5760505284867233,
        "step": 4469
    },
    {
        "loss": 1.323,
        "grad_norm": 2.066770315170288,
        "learning_rate": 9.433367858961717e-06,
        "epoch": 0.5761794276875484,
        "step": 4470
    },
    {
        "loss": 1.1389,
        "grad_norm": 1.9268419742584229,
        "learning_rate": 9.407588852863126e-06,
        "epoch": 0.5763083268883733,
        "step": 4471
    },
    {
        "loss": 2.0328,
        "grad_norm": 1.564258337020874,
        "learning_rate": 9.381843379976397e-06,
        "epoch": 0.5764372260891982,
        "step": 4472
    },
    {
        "loss": 1.8274,
        "grad_norm": 3.0922865867614746,
        "learning_rate": 9.356131449831241e-06,
        "epoch": 0.5765661252900232,
        "step": 4473
    },
    {
        "loss": 2.7116,
        "grad_norm": 1.2099345922470093,
        "learning_rate": 9.330453071945145e-06,
        "epoch": 0.5766950244908482,
        "step": 4474
    },
    {
        "loss": 1.9538,
        "grad_norm": 1.6782416105270386,
        "learning_rate": 9.30480825582305e-06,
        "epoch": 0.5768239236916731,
        "step": 4475
    },
    {
        "loss": 1.4704,
        "grad_norm": 2.227431535720825,
        "learning_rate": 9.279197010957475e-06,
        "epoch": 0.576952822892498,
        "step": 4476
    },
    {
        "loss": 2.5613,
        "grad_norm": 1.7471356391906738,
        "learning_rate": 9.253619346828568e-06,
        "epoch": 0.577081722093323,
        "step": 4477
    },
    {
        "loss": 0.7323,
        "grad_norm": 1.8357330560684204,
        "learning_rate": 9.228075272904047e-06,
        "epoch": 0.577210621294148,
        "step": 4478
    },
    {
        "loss": 2.1252,
        "grad_norm": 1.1864995956420898,
        "learning_rate": 9.202564798639147e-06,
        "epoch": 0.577339520494973,
        "step": 4479
    },
    {
        "loss": 2.0406,
        "grad_norm": 1.733925461769104,
        "learning_rate": 9.177087933476658e-06,
        "epoch": 0.5774684196957979,
        "step": 4480
    },
    {
        "loss": 2.2568,
        "grad_norm": 1.9312366247177124,
        "learning_rate": 9.151644686847061e-06,
        "epoch": 0.5775973188966228,
        "step": 4481
    },
    {
        "loss": 1.3273,
        "grad_norm": 2.1360630989074707,
        "learning_rate": 9.126235068168198e-06,
        "epoch": 0.5777262180974478,
        "step": 4482
    },
    {
        "loss": 1.1339,
        "grad_norm": 2.3562400341033936,
        "learning_rate": 9.100859086845593e-06,
        "epoch": 0.5778551172982728,
        "step": 4483
    },
    {
        "loss": 2.0885,
        "grad_norm": 1.7885849475860596,
        "learning_rate": 9.075516752272295e-06,
        "epoch": 0.5779840164990977,
        "step": 4484
    },
    {
        "loss": 1.6092,
        "grad_norm": 2.2469630241394043,
        "learning_rate": 9.050208073828881e-06,
        "epoch": 0.5781129156999226,
        "step": 4485
    },
    {
        "loss": 2.1358,
        "grad_norm": 1.7157692909240723,
        "learning_rate": 9.024933060883489e-06,
        "epoch": 0.5782418149007477,
        "step": 4486
    },
    {
        "loss": 2.1157,
        "grad_norm": 1.1666120290756226,
        "learning_rate": 8.99969172279177e-06,
        "epoch": 0.5783707141015726,
        "step": 4487
    },
    {
        "loss": 1.5541,
        "grad_norm": 2.2809674739837646,
        "learning_rate": 8.974484068896926e-06,
        "epoch": 0.5784996133023975,
        "step": 4488
    },
    {
        "loss": 1.1461,
        "grad_norm": 2.5114409923553467,
        "learning_rate": 8.94931010852973e-06,
        "epoch": 0.5786285125032224,
        "step": 4489
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.5955047607421875,
        "learning_rate": 8.924169851008358e-06,
        "epoch": 0.5787574117040475,
        "step": 4490
    },
    {
        "loss": 1.6027,
        "grad_norm": 2.01710844039917,
        "learning_rate": 8.89906330563869e-06,
        "epoch": 0.5788863109048724,
        "step": 4491
    },
    {
        "loss": 2.3132,
        "grad_norm": 1.3254280090332031,
        "learning_rate": 8.873990481713979e-06,
        "epoch": 0.5790152101056973,
        "step": 4492
    },
    {
        "loss": 1.6182,
        "grad_norm": 2.35037899017334,
        "learning_rate": 8.848951388515058e-06,
        "epoch": 0.5791441093065223,
        "step": 4493
    },
    {
        "loss": 2.145,
        "grad_norm": 1.9390125274658203,
        "learning_rate": 8.82394603531026e-06,
        "epoch": 0.5792730085073473,
        "step": 4494
    },
    {
        "loss": 0.7765,
        "grad_norm": 2.3780288696289062,
        "learning_rate": 8.798974431355466e-06,
        "epoch": 0.5794019077081722,
        "step": 4495
    },
    {
        "loss": 1.5451,
        "grad_norm": 2.5258073806762695,
        "learning_rate": 8.774036585894019e-06,
        "epoch": 0.5795308069089972,
        "step": 4496
    },
    {
        "loss": 1.8515,
        "grad_norm": 1.392406940460205,
        "learning_rate": 8.749132508156732e-06,
        "epoch": 0.5796597061098221,
        "step": 4497
    },
    {
        "loss": 1.6526,
        "grad_norm": 2.6207079887390137,
        "learning_rate": 8.724262207362054e-06,
        "epoch": 0.579788605310647,
        "step": 4498
    },
    {
        "loss": 2.2442,
        "grad_norm": 1.1315656900405884,
        "learning_rate": 8.69942569271579e-06,
        "epoch": 0.5799175045114721,
        "step": 4499
    },
    {
        "loss": 0.8989,
        "grad_norm": 2.3438427448272705,
        "learning_rate": 8.674622973411307e-06,
        "epoch": 0.580046403712297,
        "step": 4500
    },
    {
        "loss": 1.9644,
        "grad_norm": 1.08854079246521,
        "learning_rate": 8.649854058629458e-06,
        "epoch": 0.5801753029131219,
        "step": 4501
    },
    {
        "loss": 1.9792,
        "grad_norm": 1.3935372829437256,
        "learning_rate": 8.625118957538525e-06,
        "epoch": 0.5803042021139468,
        "step": 4502
    },
    {
        "loss": 2.1645,
        "grad_norm": 1.6990734338760376,
        "learning_rate": 8.600417679294426e-06,
        "epoch": 0.5804331013147719,
        "step": 4503
    },
    {
        "loss": 1.8651,
        "grad_norm": 1.936448335647583,
        "learning_rate": 8.575750233040358e-06,
        "epoch": 0.5805620005155968,
        "step": 4504
    },
    {
        "loss": 2.198,
        "grad_norm": 1.3446457386016846,
        "learning_rate": 8.55111662790713e-06,
        "epoch": 0.5806908997164217,
        "step": 4505
    },
    {
        "loss": 2.209,
        "grad_norm": 1.2791529893875122,
        "learning_rate": 8.526516873013024e-06,
        "epoch": 0.5808197989172467,
        "step": 4506
    },
    {
        "loss": 2.11,
        "grad_norm": 1.4347926378250122,
        "learning_rate": 8.501950977463657e-06,
        "epoch": 0.5809486981180717,
        "step": 4507
    },
    {
        "loss": 2.1701,
        "grad_norm": 1.562804937362671,
        "learning_rate": 8.477418950352322e-06,
        "epoch": 0.5810775973188966,
        "step": 4508
    },
    {
        "loss": 2.074,
        "grad_norm": 1.5768780708312988,
        "learning_rate": 8.452920800759623e-06,
        "epoch": 0.5812064965197216,
        "step": 4509
    },
    {
        "loss": 2.1396,
        "grad_norm": 1.317150354385376,
        "learning_rate": 8.428456537753637e-06,
        "epoch": 0.5813353957205465,
        "step": 4510
    },
    {
        "loss": 1.7462,
        "grad_norm": 1.8543134927749634,
        "learning_rate": 8.404026170389956e-06,
        "epoch": 0.5814642949213715,
        "step": 4511
    },
    {
        "loss": 1.2578,
        "grad_norm": 2.756058692932129,
        "learning_rate": 8.37962970771161e-06,
        "epoch": 0.5815931941221965,
        "step": 4512
    },
    {
        "loss": 2.1405,
        "grad_norm": 1.4662537574768066,
        "learning_rate": 8.355267158749047e-06,
        "epoch": 0.5817220933230214,
        "step": 4513
    },
    {
        "loss": 1.9763,
        "grad_norm": 1.3161576986312866,
        "learning_rate": 8.33093853252015e-06,
        "epoch": 0.5818509925238463,
        "step": 4514
    },
    {
        "loss": 1.9456,
        "grad_norm": 1.4747023582458496,
        "learning_rate": 8.306643838030348e-06,
        "epoch": 0.5819798917246713,
        "step": 4515
    },
    {
        "loss": 1.9165,
        "grad_norm": 1.3578277826309204,
        "learning_rate": 8.282383084272383e-06,
        "epoch": 0.5821087909254963,
        "step": 4516
    },
    {
        "loss": 2.5329,
        "grad_norm": 1.492876410484314,
        "learning_rate": 8.258156280226515e-06,
        "epoch": 0.5822376901263212,
        "step": 4517
    },
    {
        "loss": 2.0939,
        "grad_norm": 1.025331974029541,
        "learning_rate": 8.233963434860393e-06,
        "epoch": 0.5823665893271461,
        "step": 4518
    },
    {
        "loss": 1.4261,
        "grad_norm": 2.2697670459747314,
        "learning_rate": 8.209804557129097e-06,
        "epoch": 0.5824954885279712,
        "step": 4519
    },
    {
        "loss": 1.3062,
        "grad_norm": 2.858795642852783,
        "learning_rate": 8.185679655975197e-06,
        "epoch": 0.5826243877287961,
        "step": 4520
    },
    {
        "loss": 2.2784,
        "grad_norm": 1.9047034978866577,
        "learning_rate": 8.16158874032863e-06,
        "epoch": 0.582753286929621,
        "step": 4521
    },
    {
        "loss": 1.6771,
        "grad_norm": 2.0291221141815186,
        "learning_rate": 8.137531819106724e-06,
        "epoch": 0.582882186130446,
        "step": 4522
    },
    {
        "loss": 1.4866,
        "grad_norm": 2.821101188659668,
        "learning_rate": 8.113508901214317e-06,
        "epoch": 0.583011085331271,
        "step": 4523
    },
    {
        "loss": 2.2628,
        "grad_norm": 1.964698314666748,
        "learning_rate": 8.089519995543537e-06,
        "epoch": 0.5831399845320959,
        "step": 4524
    },
    {
        "loss": 2.4131,
        "grad_norm": 2.1303458213806152,
        "learning_rate": 8.065565110974061e-06,
        "epoch": 0.5832688837329209,
        "step": 4525
    },
    {
        "loss": 1.833,
        "grad_norm": 1.5903475284576416,
        "learning_rate": 8.041644256372882e-06,
        "epoch": 0.5833977829337458,
        "step": 4526
    },
    {
        "loss": 2.4001,
        "grad_norm": 1.205992579460144,
        "learning_rate": 8.017757440594386e-06,
        "epoch": 0.5835266821345708,
        "step": 4527
    },
    {
        "loss": 2.4521,
        "grad_norm": 1.6358320713043213,
        "learning_rate": 7.993904672480411e-06,
        "epoch": 0.5836555813353957,
        "step": 4528
    },
    {
        "loss": 1.9619,
        "grad_norm": 1.877569317817688,
        "learning_rate": 7.970085960860207e-06,
        "epoch": 0.5837844805362207,
        "step": 4529
    },
    {
        "loss": 1.7374,
        "grad_norm": 1.790357232093811,
        "learning_rate": 7.94630131455033e-06,
        "epoch": 0.5839133797370456,
        "step": 4530
    },
    {
        "loss": 1.892,
        "grad_norm": 1.5111016035079956,
        "learning_rate": 7.922550742354806e-06,
        "epoch": 0.5840422789378706,
        "step": 4531
    },
    {
        "loss": 0.9517,
        "grad_norm": 1.8926688432693481,
        "learning_rate": 7.898834253065034e-06,
        "epoch": 0.5841711781386956,
        "step": 4532
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.810727834701538,
        "learning_rate": 7.87515185545975e-06,
        "epoch": 0.5843000773395205,
        "step": 4533
    },
    {
        "loss": 1.4181,
        "grad_norm": 2.07525897026062,
        "learning_rate": 7.85150355830514e-06,
        "epoch": 0.5844289765403454,
        "step": 4534
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.3599762916564941,
        "learning_rate": 7.827889370354746e-06,
        "epoch": 0.5845578757411704,
        "step": 4535
    },
    {
        "loss": 2.0927,
        "grad_norm": 2.1494128704071045,
        "learning_rate": 7.8043093003494e-06,
        "epoch": 0.5846867749419954,
        "step": 4536
    },
    {
        "loss": 1.9207,
        "grad_norm": 2.055655002593994,
        "learning_rate": 7.780763357017462e-06,
        "epoch": 0.5848156741428203,
        "step": 4537
    },
    {
        "loss": 1.7279,
        "grad_norm": 2.8671915531158447,
        "learning_rate": 7.75725154907453e-06,
        "epoch": 0.5849445733436452,
        "step": 4538
    },
    {
        "loss": 2.1151,
        "grad_norm": 1.204530119895935,
        "learning_rate": 7.733773885223604e-06,
        "epoch": 0.5850734725444702,
        "step": 4539
    },
    {
        "loss": 2.1677,
        "grad_norm": 2.037954092025757,
        "learning_rate": 7.710330374155073e-06,
        "epoch": 0.5852023717452952,
        "step": 4540
    },
    {
        "loss": 2.146,
        "grad_norm": 1.2828816175460815,
        "learning_rate": 7.686921024546668e-06,
        "epoch": 0.5853312709461201,
        "step": 4541
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.015061140060425,
        "learning_rate": 7.663545845063469e-06,
        "epoch": 0.5854601701469451,
        "step": 4542
    },
    {
        "loss": 2.5837,
        "grad_norm": 1.9324370622634888,
        "learning_rate": 7.640204844357868e-06,
        "epoch": 0.58558906934777,
        "step": 4543
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.6780444383621216,
        "learning_rate": 7.616898031069719e-06,
        "epoch": 0.585717968548595,
        "step": 4544
    },
    {
        "loss": 1.8112,
        "grad_norm": 2.187023401260376,
        "learning_rate": 7.5936254138260996e-06,
        "epoch": 0.58584686774942,
        "step": 4545
    },
    {
        "loss": 2.3885,
        "grad_norm": 1.3717120885849,
        "learning_rate": 7.570387001241497e-06,
        "epoch": 0.5859757669502449,
        "step": 4546
    },
    {
        "loss": 2.0466,
        "grad_norm": 1.7797596454620361,
        "learning_rate": 7.547182801917707e-06,
        "epoch": 0.5861046661510698,
        "step": 4547
    },
    {
        "loss": 1.7356,
        "grad_norm": 1.5228526592254639,
        "learning_rate": 7.524012824443871e-06,
        "epoch": 0.5862335653518949,
        "step": 4548
    },
    {
        "loss": 1.5464,
        "grad_norm": 2.153226375579834,
        "learning_rate": 7.5008770773965065e-06,
        "epoch": 0.5863624645527198,
        "step": 4549
    },
    {
        "loss": 2.1731,
        "grad_norm": 1.302058458328247,
        "learning_rate": 7.4777755693393735e-06,
        "epoch": 0.5864913637535447,
        "step": 4550
    },
    {
        "loss": 1.7429,
        "grad_norm": 3.0611467361450195,
        "learning_rate": 7.454708308823621e-06,
        "epoch": 0.5866202629543696,
        "step": 4551
    },
    {
        "loss": 2.0626,
        "grad_norm": 1.6204463243484497,
        "learning_rate": 7.431675304387753e-06,
        "epoch": 0.5867491621551947,
        "step": 4552
    },
    {
        "loss": 1.9626,
        "grad_norm": 1.4860544204711914,
        "learning_rate": 7.4086765645574375e-06,
        "epoch": 0.5868780613560196,
        "step": 4553
    },
    {
        "loss": 0.7142,
        "grad_norm": 2.1286025047302246,
        "learning_rate": 7.385712097845865e-06,
        "epoch": 0.5870069605568445,
        "step": 4554
    },
    {
        "loss": 2.0735,
        "grad_norm": 1.9836509227752686,
        "learning_rate": 7.3627819127534134e-06,
        "epoch": 0.5871358597576695,
        "step": 4555
    },
    {
        "loss": 2.2157,
        "grad_norm": 2.3485803604125977,
        "learning_rate": 7.339886017767783e-06,
        "epoch": 0.5872647589584945,
        "step": 4556
    },
    {
        "loss": 1.7848,
        "grad_norm": 1.253688097000122,
        "learning_rate": 7.317024421364005e-06,
        "epoch": 0.5873936581593194,
        "step": 4557
    },
    {
        "loss": 1.1965,
        "grad_norm": 1.7287695407867432,
        "learning_rate": 7.294197132004421e-06,
        "epoch": 0.5875225573601444,
        "step": 4558
    },
    {
        "loss": 1.9256,
        "grad_norm": 1.9509077072143555,
        "learning_rate": 7.271404158138661e-06,
        "epoch": 0.5876514565609693,
        "step": 4559
    },
    {
        "loss": 1.983,
        "grad_norm": 1.1398693323135376,
        "learning_rate": 7.2486455082036105e-06,
        "epoch": 0.5877803557617943,
        "step": 4560
    },
    {
        "loss": 1.7122,
        "grad_norm": 1.4605544805526733,
        "learning_rate": 7.225921190623574e-06,
        "epoch": 0.5879092549626193,
        "step": 4561
    },
    {
        "loss": 1.6159,
        "grad_norm": 2.3257014751434326,
        "learning_rate": 7.20323121380998e-06,
        "epoch": 0.5880381541634442,
        "step": 4562
    },
    {
        "loss": 1.6367,
        "grad_norm": 1.7997816801071167,
        "learning_rate": 7.180575586161675e-06,
        "epoch": 0.5881670533642691,
        "step": 4563
    },
    {
        "loss": 1.487,
        "grad_norm": 2.4382543563842773,
        "learning_rate": 7.157954316064752e-06,
        "epoch": 0.5882959525650941,
        "step": 4564
    },
    {
        "loss": 1.6025,
        "grad_norm": 1.6241117715835571,
        "learning_rate": 7.1353674118925575e-06,
        "epoch": 0.5884248517659191,
        "step": 4565
    },
    {
        "loss": 1.3562,
        "grad_norm": 1.2971317768096924,
        "learning_rate": 7.1128148820057805e-06,
        "epoch": 0.588553750966744,
        "step": 4566
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.4273711442947388,
        "learning_rate": 7.090296734752289e-06,
        "epoch": 0.5886826501675689,
        "step": 4567
    },
    {
        "loss": 2.4363,
        "grad_norm": 2.147372007369995,
        "learning_rate": 7.067812978467326e-06,
        "epoch": 0.5888115493683939,
        "step": 4568
    },
    {
        "loss": 1.8012,
        "grad_norm": 1.3445039987564087,
        "learning_rate": 7.0453636214733775e-06,
        "epoch": 0.5889404485692189,
        "step": 4569
    },
    {
        "loss": 2.0132,
        "grad_norm": 2.4649875164031982,
        "learning_rate": 7.0229486720800985e-06,
        "epoch": 0.5890693477700438,
        "step": 4570
    },
    {
        "loss": 1.9167,
        "grad_norm": 2.2528257369995117,
        "learning_rate": 7.000568138584574e-06,
        "epoch": 0.5891982469708688,
        "step": 4571
    },
    {
        "loss": 2.2373,
        "grad_norm": 1.6756713390350342,
        "learning_rate": 6.978222029271042e-06,
        "epoch": 0.5893271461716937,
        "step": 4572
    },
    {
        "loss": 1.8685,
        "grad_norm": 1.9144611358642578,
        "learning_rate": 6.955910352410988e-06,
        "epoch": 0.5894560453725187,
        "step": 4573
    },
    {
        "loss": 1.9965,
        "grad_norm": 2.0322864055633545,
        "learning_rate": 6.933633116263217e-06,
        "epoch": 0.5895849445733437,
        "step": 4574
    },
    {
        "loss": 2.17,
        "grad_norm": 1.2287278175354004,
        "learning_rate": 6.911390329073763e-06,
        "epoch": 0.5897138437741686,
        "step": 4575
    },
    {
        "loss": 2.3855,
        "grad_norm": 1.7362362146377563,
        "learning_rate": 6.889181999075878e-06,
        "epoch": 0.5898427429749935,
        "step": 4576
    },
    {
        "loss": 1.8482,
        "grad_norm": 2.363487720489502,
        "learning_rate": 6.86700813449005e-06,
        "epoch": 0.5899716421758185,
        "step": 4577
    },
    {
        "loss": 2.2599,
        "grad_norm": 2.4648079872131348,
        "learning_rate": 6.844868743524114e-06,
        "epoch": 0.5901005413766435,
        "step": 4578
    },
    {
        "loss": 1.9013,
        "grad_norm": 2.186387777328491,
        "learning_rate": 6.822763834373036e-06,
        "epoch": 0.5902294405774684,
        "step": 4579
    },
    {
        "loss": 2.3581,
        "grad_norm": 1.3890371322631836,
        "learning_rate": 6.800693415219029e-06,
        "epoch": 0.5903583397782933,
        "step": 4580
    },
    {
        "loss": 2.0455,
        "grad_norm": 1.8637418746948242,
        "learning_rate": 6.778657494231599e-06,
        "epoch": 0.5904872389791184,
        "step": 4581
    },
    {
        "loss": 1.7215,
        "grad_norm": 1.450545072555542,
        "learning_rate": 6.7566560795674135e-06,
        "epoch": 0.5906161381799433,
        "step": 4582
    },
    {
        "loss": 1.8099,
        "grad_norm": 1.7019357681274414,
        "learning_rate": 6.7346891793704345e-06,
        "epoch": 0.5907450373807682,
        "step": 4583
    },
    {
        "loss": 0.7697,
        "grad_norm": 2.0811471939086914,
        "learning_rate": 6.712756801771814e-06,
        "epoch": 0.5908739365815932,
        "step": 4584
    },
    {
        "loss": 1.7513,
        "grad_norm": 3.3713104724884033,
        "learning_rate": 6.690858954889889e-06,
        "epoch": 0.5910028357824182,
        "step": 4585
    },
    {
        "loss": 1.702,
        "grad_norm": 2.335484504699707,
        "learning_rate": 6.668995646830311e-06,
        "epoch": 0.5911317349832431,
        "step": 4586
    },
    {
        "loss": 2.58,
        "grad_norm": 1.3820117712020874,
        "learning_rate": 6.6471668856858075e-06,
        "epoch": 0.591260634184068,
        "step": 4587
    },
    {
        "loss": 2.1918,
        "grad_norm": 1.7635414600372314,
        "learning_rate": 6.625372679536468e-06,
        "epoch": 0.591389533384893,
        "step": 4588
    },
    {
        "loss": 2.4401,
        "grad_norm": 1.1935697793960571,
        "learning_rate": 6.603613036449497e-06,
        "epoch": 0.591518432585718,
        "step": 4589
    },
    {
        "loss": 2.0068,
        "grad_norm": 1.9371883869171143,
        "learning_rate": 6.581887964479316e-06,
        "epoch": 0.5916473317865429,
        "step": 4590
    },
    {
        "loss": 1.7985,
        "grad_norm": 2.596543312072754,
        "learning_rate": 6.560197471667573e-06,
        "epoch": 0.5917762309873679,
        "step": 4591
    },
    {
        "loss": 2.4955,
        "grad_norm": 1.8419991731643677,
        "learning_rate": 6.538541566043121e-06,
        "epoch": 0.5919051301881928,
        "step": 4592
    },
    {
        "loss": 2.0777,
        "grad_norm": 1.7002372741699219,
        "learning_rate": 6.516920255621972e-06,
        "epoch": 0.5920340293890178,
        "step": 4593
    },
    {
        "loss": 1.4767,
        "grad_norm": 2.1544785499572754,
        "learning_rate": 6.495333548407378e-06,
        "epoch": 0.5921629285898428,
        "step": 4594
    },
    {
        "loss": 1.7857,
        "grad_norm": 1.5685932636260986,
        "learning_rate": 6.473781452389771e-06,
        "epoch": 0.5922918277906677,
        "step": 4595
    },
    {
        "loss": 1.706,
        "grad_norm": 2.203183650970459,
        "learning_rate": 6.452263975546724e-06,
        "epoch": 0.5924207269914926,
        "step": 4596
    },
    {
        "loss": 1.7243,
        "grad_norm": 2.68672513961792,
        "learning_rate": 6.430781125843044e-06,
        "epoch": 0.5925496261923177,
        "step": 4597
    },
    {
        "loss": 1.8729,
        "grad_norm": 2.4254367351531982,
        "learning_rate": 6.40933291123077e-06,
        "epoch": 0.5926785253931426,
        "step": 4598
    },
    {
        "loss": 1.5572,
        "grad_norm": 1.7781517505645752,
        "learning_rate": 6.387919339648951e-06,
        "epoch": 0.5928074245939675,
        "step": 4599
    },
    {
        "loss": 2.2484,
        "grad_norm": 1.5705746412277222,
        "learning_rate": 6.366540419024014e-06,
        "epoch": 0.5929363237947924,
        "step": 4600
    },
    {
        "loss": 1.687,
        "grad_norm": 2.917788028717041,
        "learning_rate": 6.34519615726944e-06,
        "epoch": 0.5930652229956175,
        "step": 4601
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.2910877466201782,
        "learning_rate": 6.323886562285896e-06,
        "epoch": 0.5931941221964424,
        "step": 4602
    },
    {
        "loss": 2.485,
        "grad_norm": 2.3017709255218506,
        "learning_rate": 6.302611641961231e-06,
        "epoch": 0.5933230213972673,
        "step": 4603
    },
    {
        "loss": 1.957,
        "grad_norm": 2.0962131023406982,
        "learning_rate": 6.281371404170489e-06,
        "epoch": 0.5934519205980923,
        "step": 4604
    },
    {
        "loss": 1.6303,
        "grad_norm": 1.4792864322662354,
        "learning_rate": 6.260165856775812e-06,
        "epoch": 0.5935808197989172,
        "step": 4605
    },
    {
        "loss": 1.3489,
        "grad_norm": 2.2870664596557617,
        "learning_rate": 6.238995007626525e-06,
        "epoch": 0.5937097189997422,
        "step": 4606
    },
    {
        "loss": 1.2595,
        "grad_norm": 2.2598838806152344,
        "learning_rate": 6.217858864559167e-06,
        "epoch": 0.5938386182005672,
        "step": 4607
    },
    {
        "loss": 1.9776,
        "grad_norm": 2.2305753231048584,
        "learning_rate": 6.196757435397338e-06,
        "epoch": 0.5939675174013921,
        "step": 4608
    },
    {
        "loss": 2.1606,
        "grad_norm": 1.7099653482437134,
        "learning_rate": 6.175690727951855e-06,
        "epoch": 0.594096416602217,
        "step": 4609
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.6193792819976807,
        "learning_rate": 6.1546587500206475e-06,
        "epoch": 0.594225315803042,
        "step": 4610
    },
    {
        "loss": 1.6649,
        "grad_norm": 1.8029049634933472,
        "learning_rate": 6.133661509388788e-06,
        "epoch": 0.594354215003867,
        "step": 4611
    },
    {
        "loss": 1.6478,
        "grad_norm": 1.892788290977478,
        "learning_rate": 6.11269901382856e-06,
        "epoch": 0.5944831142046919,
        "step": 4612
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.821527123451233,
        "learning_rate": 6.091771271099267e-06,
        "epoch": 0.5946120134055168,
        "step": 4613
    },
    {
        "loss": 1.9397,
        "grad_norm": 1.9125207662582397,
        "learning_rate": 6.070878288947435e-06,
        "epoch": 0.5947409126063419,
        "step": 4614
    },
    {
        "loss": 2.0572,
        "grad_norm": 1.1688772439956665,
        "learning_rate": 6.050020075106744e-06,
        "epoch": 0.5948698118071668,
        "step": 4615
    },
    {
        "loss": 1.6864,
        "grad_norm": 1.816004991531372,
        "learning_rate": 6.029196637297885e-06,
        "epoch": 0.5949987110079917,
        "step": 4616
    },
    {
        "loss": 2.3727,
        "grad_norm": 2.020521640777588,
        "learning_rate": 6.008407983228814e-06,
        "epoch": 0.5951276102088167,
        "step": 4617
    },
    {
        "loss": 2.192,
        "grad_norm": 1.5973223447799683,
        "learning_rate": 5.987654120594521e-06,
        "epoch": 0.5952565094096417,
        "step": 4618
    },
    {
        "loss": 2.0902,
        "grad_norm": 1.7954280376434326,
        "learning_rate": 5.9669350570771384e-06,
        "epoch": 0.5953854086104666,
        "step": 4619
    },
    {
        "loss": 1.3849,
        "grad_norm": 1.9593652486801147,
        "learning_rate": 5.946250800345943e-06,
        "epoch": 0.5955143078112916,
        "step": 4620
    },
    {
        "loss": 1.2012,
        "grad_norm": 1.7218992710113525,
        "learning_rate": 5.925601358057331e-06,
        "epoch": 0.5956432070121165,
        "step": 4621
    },
    {
        "loss": 1.0055,
        "grad_norm": 2.882242202758789,
        "learning_rate": 5.904986737854756e-06,
        "epoch": 0.5957721062129415,
        "step": 4622
    },
    {
        "loss": 2.0858,
        "grad_norm": 1.7315994501113892,
        "learning_rate": 5.884406947368815e-06,
        "epoch": 0.5959010054137664,
        "step": 4623
    },
    {
        "loss": 1.8264,
        "grad_norm": 2.288935422897339,
        "learning_rate": 5.863861994217268e-06,
        "epoch": 0.5960299046145914,
        "step": 4624
    },
    {
        "loss": 1.7325,
        "grad_norm": 2.4291484355926514,
        "learning_rate": 5.843351886004855e-06,
        "epoch": 0.5961588038154163,
        "step": 4625
    },
    {
        "loss": 1.505,
        "grad_norm": 1.6189464330673218,
        "learning_rate": 5.822876630323526e-06,
        "epoch": 0.5962877030162413,
        "step": 4626
    },
    {
        "loss": 1.966,
        "grad_norm": 1.5382964611053467,
        "learning_rate": 5.802436234752317e-06,
        "epoch": 0.5964166022170663,
        "step": 4627
    },
    {
        "loss": 1.4802,
        "grad_norm": 2.1287355422973633,
        "learning_rate": 5.782030706857289e-06,
        "epoch": 0.5965455014178912,
        "step": 4628
    },
    {
        "loss": 2.1858,
        "grad_norm": 1.6907373666763306,
        "learning_rate": 5.7616600541916865e-06,
        "epoch": 0.5966744006187161,
        "step": 4629
    },
    {
        "loss": 2.0349,
        "grad_norm": 1.2715046405792236,
        "learning_rate": 5.74132428429578e-06,
        "epoch": 0.5968032998195412,
        "step": 4630
    },
    {
        "loss": 1.9002,
        "grad_norm": 1.770053505897522,
        "learning_rate": 5.721023404696946e-06,
        "epoch": 0.5969321990203661,
        "step": 4631
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.9223840236663818,
        "learning_rate": 5.700757422909708e-06,
        "epoch": 0.597061098221191,
        "step": 4632
    },
    {
        "loss": 1.9651,
        "grad_norm": 2.50561785697937,
        "learning_rate": 5.680526346435522e-06,
        "epoch": 0.597189997422016,
        "step": 4633
    },
    {
        "loss": 1.5134,
        "grad_norm": 2.7533626556396484,
        "learning_rate": 5.6603301827630965e-06,
        "epoch": 0.597318896622841,
        "step": 4634
    },
    {
        "loss": 1.5172,
        "grad_norm": 1.6956756114959717,
        "learning_rate": 5.640168939368096e-06,
        "epoch": 0.5974477958236659,
        "step": 4635
    },
    {
        "loss": 1.4644,
        "grad_norm": 2.553490400314331,
        "learning_rate": 5.6200426237133175e-06,
        "epoch": 0.5975766950244908,
        "step": 4636
    },
    {
        "loss": 2.0561,
        "grad_norm": 1.7311513423919678,
        "learning_rate": 5.5999512432485915e-06,
        "epoch": 0.5977055942253158,
        "step": 4637
    },
    {
        "loss": 2.1603,
        "grad_norm": 1.098158836364746,
        "learning_rate": 5.579894805410879e-06,
        "epoch": 0.5978344934261408,
        "step": 4638
    },
    {
        "loss": 1.8747,
        "grad_norm": 1.6602668762207031,
        "learning_rate": 5.5598733176241316e-06,
        "epoch": 0.5979633926269657,
        "step": 4639
    },
    {
        "loss": 0.6538,
        "grad_norm": 1.5649000406265259,
        "learning_rate": 5.539886787299387e-06,
        "epoch": 0.5980922918277907,
        "step": 4640
    },
    {
        "loss": 2.3354,
        "grad_norm": 2.367692470550537,
        "learning_rate": 5.519935221834815e-06,
        "epoch": 0.5982211910286156,
        "step": 4641
    },
    {
        "loss": 1.4391,
        "grad_norm": 1.8565592765808105,
        "learning_rate": 5.500018628615511e-06,
        "epoch": 0.5983500902294405,
        "step": 4642
    },
    {
        "loss": 1.6589,
        "grad_norm": 1.850515365600586,
        "learning_rate": 5.480137015013731e-06,
        "epoch": 0.5984789894302656,
        "step": 4643
    },
    {
        "loss": 1.5046,
        "grad_norm": 2.183189630508423,
        "learning_rate": 5.4602903883887554e-06,
        "epoch": 0.5986078886310905,
        "step": 4644
    },
    {
        "loss": 2.1159,
        "grad_norm": 1.093589186668396,
        "learning_rate": 5.440478756086897e-06,
        "epoch": 0.5987367878319154,
        "step": 4645
    },
    {
        "loss": 1.9628,
        "grad_norm": 1.806915521621704,
        "learning_rate": 5.420702125441512e-06,
        "epoch": 0.5988656870327403,
        "step": 4646
    },
    {
        "loss": 1.8299,
        "grad_norm": 1.8736679553985596,
        "learning_rate": 5.400960503773045e-06,
        "epoch": 0.5989945862335654,
        "step": 4647
    },
    {
        "loss": 1.7307,
        "grad_norm": 2.2231040000915527,
        "learning_rate": 5.381253898388927e-06,
        "epoch": 0.5991234854343903,
        "step": 4648
    },
    {
        "loss": 1.8121,
        "grad_norm": 1.5585967302322388,
        "learning_rate": 5.361582316583691e-06,
        "epoch": 0.5992523846352152,
        "step": 4649
    },
    {
        "loss": 1.8078,
        "grad_norm": 2.558049440383911,
        "learning_rate": 5.341945765638778e-06,
        "epoch": 0.5993812838360402,
        "step": 4650
    },
    {
        "loss": 2.3408,
        "grad_norm": 1.0694842338562012,
        "learning_rate": 5.3223442528228526e-06,
        "epoch": 0.5995101830368652,
        "step": 4651
    },
    {
        "loss": 2.3708,
        "grad_norm": 1.4418147802352905,
        "learning_rate": 5.302777785391455e-06,
        "epoch": 0.5996390822376901,
        "step": 4652
    },
    {
        "loss": 2.2498,
        "grad_norm": 1.3526808023452759,
        "learning_rate": 5.283246370587203e-06,
        "epoch": 0.5997679814385151,
        "step": 4653
    },
    {
        "loss": 2.2763,
        "grad_norm": 1.5074540376663208,
        "learning_rate": 5.263750015639745e-06,
        "epoch": 0.59989688063934,
        "step": 4654
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.5722665786743164,
        "learning_rate": 5.244288727765789e-06,
        "epoch": 0.600025779840165,
        "step": 4655
    },
    {
        "loss": 2.152,
        "grad_norm": 1.4366645812988281,
        "learning_rate": 5.224862514168982e-06,
        "epoch": 0.60015467904099,
        "step": 4656
    },
    {
        "loss": 2.1491,
        "grad_norm": 1.858654260635376,
        "learning_rate": 5.205471382040017e-06,
        "epoch": 0.6002835782418149,
        "step": 4657
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.130227565765381,
        "learning_rate": 5.186115338556663e-06,
        "epoch": 0.6004124774426398,
        "step": 4658
    },
    {
        "loss": 1.8338,
        "grad_norm": 1.8695297241210938,
        "learning_rate": 5.166794390883612e-06,
        "epoch": 0.6005413766434649,
        "step": 4659
    },
    {
        "loss": 2.3668,
        "grad_norm": 2.0350284576416016,
        "learning_rate": 5.14750854617263e-06,
        "epoch": 0.6006702758442898,
        "step": 4660
    },
    {
        "loss": 1.9746,
        "grad_norm": 1.8533979654312134,
        "learning_rate": 5.128257811562454e-06,
        "epoch": 0.6007991750451147,
        "step": 4661
    },
    {
        "loss": 2.3692,
        "grad_norm": 1.6897408962249756,
        "learning_rate": 5.109042194178804e-06,
        "epoch": 0.6009280742459396,
        "step": 4662
    },
    {
        "loss": 1.5416,
        "grad_norm": 1.8388563394546509,
        "learning_rate": 5.089861701134491e-06,
        "epoch": 0.6010569734467647,
        "step": 4663
    },
    {
        "loss": 2.3911,
        "grad_norm": 1.555216908454895,
        "learning_rate": 5.070716339529236e-06,
        "epoch": 0.6011858726475896,
        "step": 4664
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.7905688285827637,
        "learning_rate": 5.051606116449781e-06,
        "epoch": 0.6013147718484145,
        "step": 4665
    },
    {
        "loss": 1.9988,
        "grad_norm": 2.119868040084839,
        "learning_rate": 5.032531038969857e-06,
        "epoch": 0.6014436710492395,
        "step": 4666
    },
    {
        "loss": 1.8617,
        "grad_norm": 2.2823846340179443,
        "learning_rate": 5.0134911141502375e-06,
        "epoch": 0.6015725702500645,
        "step": 4667
    },
    {
        "loss": 2.3376,
        "grad_norm": 1.903732419013977,
        "learning_rate": 4.9944863490385965e-06,
        "epoch": 0.6017014694508894,
        "step": 4668
    },
    {
        "loss": 1.7435,
        "grad_norm": 1.415747046470642,
        "learning_rate": 4.975516750669629e-06,
        "epoch": 0.6018303686517144,
        "step": 4669
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.204526901245117,
        "learning_rate": 4.956582326065096e-06,
        "epoch": 0.6019592678525393,
        "step": 4670
    },
    {
        "loss": 1.9369,
        "grad_norm": 1.6564453840255737,
        "learning_rate": 4.937683082233591e-06,
        "epoch": 0.6020881670533643,
        "step": 4671
    },
    {
        "loss": 1.4,
        "grad_norm": 2.5217478275299072,
        "learning_rate": 4.918819026170784e-06,
        "epoch": 0.6022170662541892,
        "step": 4672
    },
    {
        "loss": 1.5396,
        "grad_norm": 2.4826762676239014,
        "learning_rate": 4.899990164859292e-06,
        "epoch": 0.6023459654550142,
        "step": 4673
    },
    {
        "loss": 2.0671,
        "grad_norm": 1.3012341260910034,
        "learning_rate": 4.881196505268704e-06,
        "epoch": 0.6024748646558391,
        "step": 4674
    },
    {
        "loss": 2.0213,
        "grad_norm": 4.259492874145508,
        "learning_rate": 4.862438054355611e-06,
        "epoch": 0.6026037638566641,
        "step": 4675
    },
    {
        "loss": 1.0956,
        "grad_norm": 2.4672436714172363,
        "learning_rate": 4.843714819063494e-06,
        "epoch": 0.6027326630574891,
        "step": 4676
    },
    {
        "loss": 1.9137,
        "grad_norm": 1.3843225240707397,
        "learning_rate": 4.825026806322885e-06,
        "epoch": 0.602861562258314,
        "step": 4677
    },
    {
        "loss": 1.7462,
        "grad_norm": 2.21494460105896,
        "learning_rate": 4.806374023051252e-06,
        "epoch": 0.6029904614591389,
        "step": 4678
    },
    {
        "loss": 2.166,
        "grad_norm": 1.9102466106414795,
        "learning_rate": 4.78775647615296e-06,
        "epoch": 0.6031193606599639,
        "step": 4679
    },
    {
        "loss": 1.7743,
        "grad_norm": 2.768479347229004,
        "learning_rate": 4.76917417251943e-06,
        "epoch": 0.6032482598607889,
        "step": 4680
    },
    {
        "loss": 0.6421,
        "grad_norm": 1.9564350843429565,
        "learning_rate": 4.750627119028972e-06,
        "epoch": 0.6033771590616138,
        "step": 4681
    },
    {
        "loss": 1.5659,
        "grad_norm": 3.180314779281616,
        "learning_rate": 4.732115322546848e-06,
        "epoch": 0.6035060582624387,
        "step": 4682
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.157120704650879,
        "learning_rate": 4.713638789925312e-06,
        "epoch": 0.6036349574632637,
        "step": 4683
    },
    {
        "loss": 1.5002,
        "grad_norm": 3.136023998260498,
        "learning_rate": 4.695197528003547e-06,
        "epoch": 0.6037638566640887,
        "step": 4684
    },
    {
        "loss": 2.0992,
        "grad_norm": 1.6373544931411743,
        "learning_rate": 4.676791543607662e-06,
        "epoch": 0.6038927558649136,
        "step": 4685
    },
    {
        "loss": 1.6923,
        "grad_norm": 1.7988967895507812,
        "learning_rate": 4.658420843550693e-06,
        "epoch": 0.6040216550657386,
        "step": 4686
    },
    {
        "loss": 1.7715,
        "grad_norm": 1.5515048503875732,
        "learning_rate": 4.640085434632702e-06,
        "epoch": 0.6041505542665635,
        "step": 4687
    },
    {
        "loss": 1.1778,
        "grad_norm": 2.0438928604125977,
        "learning_rate": 4.621785323640571e-06,
        "epoch": 0.6042794534673885,
        "step": 4688
    },
    {
        "loss": 1.6388,
        "grad_norm": 2.0798587799072266,
        "learning_rate": 4.603520517348192e-06,
        "epoch": 0.6044083526682135,
        "step": 4689
    },
    {
        "loss": 1.5216,
        "grad_norm": 2.3861536979675293,
        "learning_rate": 4.585291022516392e-06,
        "epoch": 0.6045372518690384,
        "step": 4690
    },
    {
        "loss": 1.6309,
        "grad_norm": 1.0609371662139893,
        "learning_rate": 4.567096845892871e-06,
        "epoch": 0.6046661510698633,
        "step": 4691
    },
    {
        "loss": 1.7337,
        "grad_norm": 1.5197796821594238,
        "learning_rate": 4.54893799421231e-06,
        "epoch": 0.6047950502706884,
        "step": 4692
    },
    {
        "loss": 1.8271,
        "grad_norm": 2.0299007892608643,
        "learning_rate": 4.530814474196288e-06,
        "epoch": 0.6049239494715133,
        "step": 4693
    },
    {
        "loss": 2.1104,
        "grad_norm": 1.9343976974487305,
        "learning_rate": 4.512726292553315e-06,
        "epoch": 0.6050528486723382,
        "step": 4694
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.0416141748428345,
        "learning_rate": 4.4946734559788375e-06,
        "epoch": 0.6051817478731631,
        "step": 4695
    },
    {
        "loss": 2.0718,
        "grad_norm": 1.8834545612335205,
        "learning_rate": 4.476655971155141e-06,
        "epoch": 0.6053106470739882,
        "step": 4696
    },
    {
        "loss": 0.5898,
        "grad_norm": 3.0609822273254395,
        "learning_rate": 4.458673844751537e-06,
        "epoch": 0.6054395462748131,
        "step": 4697
    },
    {
        "loss": 2.3135,
        "grad_norm": 3.4188084602355957,
        "learning_rate": 4.44072708342419e-06,
        "epoch": 0.605568445475638,
        "step": 4698
    },
    {
        "loss": 2.4849,
        "grad_norm": 1.1547865867614746,
        "learning_rate": 4.422815693816151e-06,
        "epoch": 0.605697344676463,
        "step": 4699
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.6501811742782593,
        "learning_rate": 4.4049396825574185e-06,
        "epoch": 0.605826243877288,
        "step": 4700
    },
    {
        "loss": 2.4202,
        "grad_norm": 2.231961250305176,
        "learning_rate": 4.387099056264909e-06,
        "epoch": 0.6059551430781129,
        "step": 4701
    },
    {
        "loss": 1.9982,
        "grad_norm": 2.33423113822937,
        "learning_rate": 4.369293821542386e-06,
        "epoch": 0.6060840422789379,
        "step": 4702
    },
    {
        "loss": 1.1967,
        "grad_norm": 2.4196393489837646,
        "learning_rate": 4.3515239849805215e-06,
        "epoch": 0.6062129414797628,
        "step": 4703
    },
    {
        "loss": 0.8575,
        "grad_norm": 2.2903215885162354,
        "learning_rate": 4.333789553156975e-06,
        "epoch": 0.6063418406805878,
        "step": 4704
    },
    {
        "loss": 1.4886,
        "grad_norm": 2.1002557277679443,
        "learning_rate": 4.316090532636163e-06,
        "epoch": 0.6064707398814128,
        "step": 4705
    },
    {
        "loss": 1.9121,
        "grad_norm": 1.6389660835266113,
        "learning_rate": 4.298426929969479e-06,
        "epoch": 0.6065996390822377,
        "step": 4706
    },
    {
        "loss": 2.2086,
        "grad_norm": 1.3694316148757935,
        "learning_rate": 4.280798751695214e-06,
        "epoch": 0.6067285382830626,
        "step": 4707
    },
    {
        "loss": 0.7569,
        "grad_norm": 1.98800528049469,
        "learning_rate": 4.263206004338493e-06,
        "epoch": 0.6068574374838877,
        "step": 4708
    },
    {
        "loss": 1.6743,
        "grad_norm": 1.7724649906158447,
        "learning_rate": 4.2456486944113746e-06,
        "epoch": 0.6069863366847126,
        "step": 4709
    },
    {
        "loss": 1.7176,
        "grad_norm": 1.4021214246749878,
        "learning_rate": 4.228126828412782e-06,
        "epoch": 0.6071152358855375,
        "step": 4710
    },
    {
        "loss": 1.3527,
        "grad_norm": 1.354992151260376,
        "learning_rate": 4.210640412828493e-06,
        "epoch": 0.6072441350863624,
        "step": 4711
    },
    {
        "loss": 0.367,
        "grad_norm": 1.2284337282180786,
        "learning_rate": 4.193189454131241e-06,
        "epoch": 0.6073730342871875,
        "step": 4712
    },
    {
        "loss": 1.8536,
        "grad_norm": 1.979533076286316,
        "learning_rate": 4.175773958780493e-06,
        "epoch": 0.6075019334880124,
        "step": 4713
    },
    {
        "loss": 1.7111,
        "grad_norm": 2.149765968322754,
        "learning_rate": 4.158393933222759e-06,
        "epoch": 0.6076308326888373,
        "step": 4714
    },
    {
        "loss": 0.7704,
        "grad_norm": 1.4900633096694946,
        "learning_rate": 4.141049383891305e-06,
        "epoch": 0.6077597318896623,
        "step": 4715
    },
    {
        "loss": 2.6791,
        "grad_norm": 1.4707690477371216,
        "learning_rate": 4.123740317206293e-06,
        "epoch": 0.6078886310904872,
        "step": 4716
    },
    {
        "loss": 2.4226,
        "grad_norm": 1.4919942617416382,
        "learning_rate": 4.106466739574754e-06,
        "epoch": 0.6080175302913122,
        "step": 4717
    },
    {
        "loss": 2.209,
        "grad_norm": 2.0907461643218994,
        "learning_rate": 4.08922865739062e-06,
        "epoch": 0.6081464294921372,
        "step": 4718
    },
    {
        "loss": 2.0015,
        "grad_norm": 2.3095757961273193,
        "learning_rate": 4.072026077034619e-06,
        "epoch": 0.6082753286929621,
        "step": 4719
    },
    {
        "loss": 1.5261,
        "grad_norm": 1.971537470817566,
        "learning_rate": 4.054859004874345e-06,
        "epoch": 0.608404227893787,
        "step": 4720
    },
    {
        "loss": 1.2424,
        "grad_norm": 1.8471205234527588,
        "learning_rate": 4.037727447264339e-06,
        "epoch": 0.608533127094612,
        "step": 4721
    },
    {
        "loss": 1.7431,
        "grad_norm": 2.3062593936920166,
        "learning_rate": 4.0206314105458605e-06,
        "epoch": 0.608662026295437,
        "step": 4722
    },
    {
        "loss": 1.9749,
        "grad_norm": 1.5910148620605469,
        "learning_rate": 4.003570901047127e-06,
        "epoch": 0.6087909254962619,
        "step": 4723
    },
    {
        "loss": 1.7024,
        "grad_norm": 2.237152099609375,
        "learning_rate": 3.986545925083163e-06,
        "epoch": 0.6089198246970868,
        "step": 4724
    },
    {
        "loss": 1.8959,
        "grad_norm": 1.6183449029922485,
        "learning_rate": 3.969556488955817e-06,
        "epoch": 0.6090487238979119,
        "step": 4725
    },
    {
        "loss": 1.6957,
        "grad_norm": 1.0977132320404053,
        "learning_rate": 3.952602598953848e-06,
        "epoch": 0.6091776230987368,
        "step": 4726
    },
    {
        "loss": 1.4535,
        "grad_norm": 2.023426055908203,
        "learning_rate": 3.935684261352812e-06,
        "epoch": 0.6093065222995617,
        "step": 4727
    },
    {
        "loss": 1.3891,
        "grad_norm": 1.9542582035064697,
        "learning_rate": 3.918801482415091e-06,
        "epoch": 0.6094354215003867,
        "step": 4728
    },
    {
        "loss": 1.7291,
        "grad_norm": 1.8785697221755981,
        "learning_rate": 3.901954268389929e-06,
        "epoch": 0.6095643207012117,
        "step": 4729
    },
    {
        "loss": 1.6773,
        "grad_norm": 1.83640718460083,
        "learning_rate": 3.885142625513427e-06,
        "epoch": 0.6096932199020366,
        "step": 4730
    },
    {
        "loss": 1.6367,
        "grad_norm": 1.3573694229125977,
        "learning_rate": 3.8683665600084855e-06,
        "epoch": 0.6098221191028615,
        "step": 4731
    },
    {
        "loss": 1.6932,
        "grad_norm": 1.457137942314148,
        "learning_rate": 3.851626078084825e-06,
        "epoch": 0.6099510183036865,
        "step": 4732
    },
    {
        "loss": 2.0809,
        "grad_norm": 1.172783613204956,
        "learning_rate": 3.834921185939022e-06,
        "epoch": 0.6100799175045115,
        "step": 4733
    },
    {
        "loss": 2.084,
        "grad_norm": 2.130826234817505,
        "learning_rate": 3.818251889754488e-06,
        "epoch": 0.6102088167053364,
        "step": 4734
    },
    {
        "loss": 1.8221,
        "grad_norm": 2.663092613220215,
        "learning_rate": 3.8016181957014286e-06,
        "epoch": 0.6103377159061614,
        "step": 4735
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.8371162414550781,
        "learning_rate": 3.7850201099368877e-06,
        "epoch": 0.6104666151069863,
        "step": 4736
    },
    {
        "loss": 2.2994,
        "grad_norm": 1.6296181678771973,
        "learning_rate": 3.7684576386047056e-06,
        "epoch": 0.6105955143078113,
        "step": 4737
    },
    {
        "loss": 2.1163,
        "grad_norm": 1.27338707447052,
        "learning_rate": 3.7519307878356116e-06,
        "epoch": 0.6107244135086363,
        "step": 4738
    },
    {
        "loss": 1.5484,
        "grad_norm": 2.8529560565948486,
        "learning_rate": 3.7354395637470453e-06,
        "epoch": 0.6108533127094612,
        "step": 4739
    },
    {
        "loss": 1.632,
        "grad_norm": 2.2692649364471436,
        "learning_rate": 3.718983972443346e-06,
        "epoch": 0.6109822119102861,
        "step": 4740
    },
    {
        "loss": 2.0804,
        "grad_norm": 1.2760519981384277,
        "learning_rate": 3.7025640200156196e-06,
        "epoch": 0.6111111111111112,
        "step": 4741
    },
    {
        "loss": 1.1885,
        "grad_norm": 1.434112310409546,
        "learning_rate": 3.68617971254176e-06,
        "epoch": 0.6112400103119361,
        "step": 4742
    },
    {
        "loss": 2.0994,
        "grad_norm": 1.68611478805542,
        "learning_rate": 3.6698310560865367e-06,
        "epoch": 0.611368909512761,
        "step": 4743
    },
    {
        "loss": 2.3229,
        "grad_norm": 1.6046719551086426,
        "learning_rate": 3.6535180567014772e-06,
        "epoch": 0.6114978087135859,
        "step": 4744
    },
    {
        "loss": 1.8436,
        "grad_norm": 1.497603178024292,
        "learning_rate": 3.637240720424895e-06,
        "epoch": 0.611626707914411,
        "step": 4745
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.9739617109298706,
        "learning_rate": 3.620999053281937e-06,
        "epoch": 0.6117556071152359,
        "step": 4746
    },
    {
        "loss": 2.0415,
        "grad_norm": 1.5906769037246704,
        "learning_rate": 3.6047930612845395e-06,
        "epoch": 0.6118845063160608,
        "step": 4747
    },
    {
        "loss": 1.6146,
        "grad_norm": 1.876997947692871,
        "learning_rate": 3.588622750431425e-06,
        "epoch": 0.6120134055168858,
        "step": 4748
    },
    {
        "loss": 1.0937,
        "grad_norm": 1.6873910427093506,
        "learning_rate": 3.5724881267080734e-06,
        "epoch": 0.6121423047177108,
        "step": 4749
    },
    {
        "loss": 0.8108,
        "grad_norm": 2.0188651084899902,
        "learning_rate": 3.5563891960868735e-06,
        "epoch": 0.6122712039185357,
        "step": 4750
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.8635538816452026,
        "learning_rate": 3.5403259645268362e-06,
        "epoch": 0.6124001031193607,
        "step": 4751
    },
    {
        "loss": 2.0229,
        "grad_norm": 1.8218815326690674,
        "learning_rate": 3.524298437973883e-06,
        "epoch": 0.6125290023201856,
        "step": 4752
    },
    {
        "loss": 1.9252,
        "grad_norm": 2.831465721130371,
        "learning_rate": 3.508306622360691e-06,
        "epoch": 0.6126579015210105,
        "step": 4753
    },
    {
        "loss": 1.9682,
        "grad_norm": 2.2793233394622803,
        "learning_rate": 3.49235052360668e-06,
        "epoch": 0.6127868007218356,
        "step": 4754
    },
    {
        "loss": 1.8297,
        "grad_norm": 1.856555700302124,
        "learning_rate": 3.4764301476180927e-06,
        "epoch": 0.6129156999226605,
        "step": 4755
    },
    {
        "loss": 1.4025,
        "grad_norm": 1.9947178363800049,
        "learning_rate": 3.460545500287915e-06,
        "epoch": 0.6130445991234854,
        "step": 4756
    },
    {
        "loss": 2.1219,
        "grad_norm": 1.5637545585632324,
        "learning_rate": 3.4446965874959435e-06,
        "epoch": 0.6131734983243103,
        "step": 4757
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.6378376483917236,
        "learning_rate": 3.4288834151087412e-06,
        "epoch": 0.6133023975251354,
        "step": 4758
    },
    {
        "loss": 2.2525,
        "grad_norm": 2.197809934616089,
        "learning_rate": 3.4131059889795704e-06,
        "epoch": 0.6134312967259603,
        "step": 4759
    },
    {
        "loss": 1.6149,
        "grad_norm": 1.8733223676681519,
        "learning_rate": 3.3973643149485814e-06,
        "epoch": 0.6135601959267852,
        "step": 4760
    },
    {
        "loss": 1.2539,
        "grad_norm": 2.2370920181274414,
        "learning_rate": 3.381658398842613e-06,
        "epoch": 0.6136890951276102,
        "step": 4761
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.074411153793335,
        "learning_rate": 3.36598824647526e-06,
        "epoch": 0.6138179943284352,
        "step": 4762
    },
    {
        "loss": 2.1506,
        "grad_norm": 2.1790988445281982,
        "learning_rate": 3.3503538636469267e-06,
        "epoch": 0.6139468935292601,
        "step": 4763
    },
    {
        "loss": 2.1014,
        "grad_norm": 1.3446154594421387,
        "learning_rate": 3.3347552561447615e-06,
        "epoch": 0.6140757927300851,
        "step": 4764
    },
    {
        "loss": 2.5284,
        "grad_norm": 1.2057149410247803,
        "learning_rate": 3.3191924297426568e-06,
        "epoch": 0.61420469193091,
        "step": 4765
    },
    {
        "loss": 1.4135,
        "grad_norm": 2.125669002532959,
        "learning_rate": 3.3036653902012383e-06,
        "epoch": 0.614333591131735,
        "step": 4766
    },
    {
        "loss": 1.5648,
        "grad_norm": 2.0554568767547607,
        "learning_rate": 3.2881741432679636e-06,
        "epoch": 0.61446249033256,
        "step": 4767
    },
    {
        "loss": 2.4028,
        "grad_norm": 1.2149951457977295,
        "learning_rate": 3.272718694676946e-06,
        "epoch": 0.6145913895333849,
        "step": 4768
    },
    {
        "loss": 2.356,
        "grad_norm": 0.9521166086196899,
        "learning_rate": 3.257299050149121e-06,
        "epoch": 0.6147202887342098,
        "step": 4769
    },
    {
        "loss": 1.5395,
        "grad_norm": 3.3120322227478027,
        "learning_rate": 3.2419152153921327e-06,
        "epoch": 0.6148491879350348,
        "step": 4770
    },
    {
        "loss": 2.5349,
        "grad_norm": 2.1709771156311035,
        "learning_rate": 3.226567196100383e-06,
        "epoch": 0.6149780871358598,
        "step": 4771
    },
    {
        "loss": 2.2105,
        "grad_norm": 1.670434594154358,
        "learning_rate": 3.2112549979550044e-06,
        "epoch": 0.6151069863366847,
        "step": 4772
    },
    {
        "loss": 2.1781,
        "grad_norm": 2.448561429977417,
        "learning_rate": 3.1959786266239077e-06,
        "epoch": 0.6152358855375096,
        "step": 4773
    },
    {
        "loss": 1.7523,
        "grad_norm": 1.935990810394287,
        "learning_rate": 3.180738087761681e-06,
        "epoch": 0.6153647847383347,
        "step": 4774
    },
    {
        "loss": 1.744,
        "grad_norm": 1.6514418125152588,
        "learning_rate": 3.1655333870097228e-06,
        "epoch": 0.6154936839391596,
        "step": 4775
    },
    {
        "loss": 1.9363,
        "grad_norm": 1.301825761795044,
        "learning_rate": 3.1503645299960636e-06,
        "epoch": 0.6156225831399845,
        "step": 4776
    },
    {
        "loss": 1.9862,
        "grad_norm": 1.9896924495697021,
        "learning_rate": 3.135231522335591e-06,
        "epoch": 0.6157514823408095,
        "step": 4777
    },
    {
        "loss": 1.9786,
        "grad_norm": 1.663488745689392,
        "learning_rate": 3.1201343696298237e-06,
        "epoch": 0.6158803815416345,
        "step": 4778
    },
    {
        "loss": 1.6063,
        "grad_norm": 2.207350015640259,
        "learning_rate": 3.1050730774670465e-06,
        "epoch": 0.6160092807424594,
        "step": 4779
    },
    {
        "loss": 1.8761,
        "grad_norm": 1.2145702838897705,
        "learning_rate": 3.0900476514222788e-06,
        "epoch": 0.6161381799432843,
        "step": 4780
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.316941499710083,
        "learning_rate": 3.075058097057271e-06,
        "epoch": 0.6162670791441093,
        "step": 4781
    },
    {
        "loss": 1.9961,
        "grad_norm": 1.5401731729507446,
        "learning_rate": 3.06010441992044e-06,
        "epoch": 0.6163959783449343,
        "step": 4782
    },
    {
        "loss": 2.178,
        "grad_norm": 2.108323812484741,
        "learning_rate": 3.0451866255469696e-06,
        "epoch": 0.6165248775457592,
        "step": 4783
    },
    {
        "loss": 1.9877,
        "grad_norm": 1.6947247982025146,
        "learning_rate": 3.0303047194587877e-06,
        "epoch": 0.6166537767465842,
        "step": 4784
    },
    {
        "loss": 2.2695,
        "grad_norm": 2.494218349456787,
        "learning_rate": 3.0154587071644536e-06,
        "epoch": 0.6167826759474091,
        "step": 4785
    },
    {
        "loss": 2.0437,
        "grad_norm": 1.5873903036117554,
        "learning_rate": 3.0006485941593053e-06,
        "epoch": 0.616911575148234,
        "step": 4786
    },
    {
        "loss": 1.4825,
        "grad_norm": 2.513518810272217,
        "learning_rate": 2.9858743859254024e-06,
        "epoch": 0.6170404743490591,
        "step": 4787
    },
    {
        "loss": 1.7721,
        "grad_norm": 2.3959693908691406,
        "learning_rate": 2.971136087931459e-06,
        "epoch": 0.617169373549884,
        "step": 4788
    },
    {
        "loss": 1.8517,
        "grad_norm": 2.0749378204345703,
        "learning_rate": 2.9564337056329216e-06,
        "epoch": 0.6172982727507089,
        "step": 4789
    },
    {
        "loss": 1.743,
        "grad_norm": 2.019468069076538,
        "learning_rate": 2.9417672444719826e-06,
        "epoch": 0.6174271719515338,
        "step": 4790
    },
    {
        "loss": 1.4248,
        "grad_norm": 1.900935411453247,
        "learning_rate": 2.927136709877476e-06,
        "epoch": 0.6175560711523589,
        "step": 4791
    },
    {
        "loss": 2.2096,
        "grad_norm": 1.4927386045455933,
        "learning_rate": 2.9125421072649596e-06,
        "epoch": 0.6176849703531838,
        "step": 4792
    },
    {
        "loss": 2.1438,
        "grad_norm": 1.9487452507019043,
        "learning_rate": 2.897983442036711e-06,
        "epoch": 0.6178138695540087,
        "step": 4793
    },
    {
        "loss": 2.1428,
        "grad_norm": 2.4993698596954346,
        "learning_rate": 2.8834607195816985e-06,
        "epoch": 0.6179427687548337,
        "step": 4794
    },
    {
        "loss": 1.5614,
        "grad_norm": 2.095566749572754,
        "learning_rate": 2.868973945275544e-06,
        "epoch": 0.6180716679556587,
        "step": 4795
    },
    {
        "loss": 2.1543,
        "grad_norm": 1.070689082145691,
        "learning_rate": 2.8545231244806124e-06,
        "epoch": 0.6182005671564836,
        "step": 4796
    },
    {
        "loss": 2.3608,
        "grad_norm": 1.3241479396820068,
        "learning_rate": 2.8401082625459375e-06,
        "epoch": 0.6183294663573086,
        "step": 4797
    },
    {
        "loss": 2.7113,
        "grad_norm": 1.7859469652175903,
        "learning_rate": 2.8257293648072725e-06,
        "epoch": 0.6184583655581335,
        "step": 4798
    },
    {
        "loss": 1.8264,
        "grad_norm": 1.9244375228881836,
        "learning_rate": 2.8113864365870047e-06,
        "epoch": 0.6185872647589585,
        "step": 4799
    },
    {
        "loss": 1.6056,
        "grad_norm": 2.1025686264038086,
        "learning_rate": 2.7970794831942425e-06,
        "epoch": 0.6187161639597835,
        "step": 4800
    },
    {
        "loss": 2.4117,
        "grad_norm": 1.257275938987732,
        "learning_rate": 2.7828085099247945e-06,
        "epoch": 0.6188450631606084,
        "step": 4801
    },
    {
        "loss": 1.3502,
        "grad_norm": 2.5779531002044678,
        "learning_rate": 2.7685735220611244e-06,
        "epoch": 0.6189739623614333,
        "step": 4802
    },
    {
        "loss": 1.7692,
        "grad_norm": 2.553844451904297,
        "learning_rate": 2.7543745248723497e-06,
        "epoch": 0.6191028615622584,
        "step": 4803
    },
    {
        "loss": 1.5923,
        "grad_norm": 2.383054733276367,
        "learning_rate": 2.740211523614333e-06,
        "epoch": 0.6192317607630833,
        "step": 4804
    },
    {
        "loss": 0.8559,
        "grad_norm": 1.9868429899215698,
        "learning_rate": 2.7260845235295484e-06,
        "epoch": 0.6193606599639082,
        "step": 4805
    },
    {
        "loss": 1.9405,
        "grad_norm": 2.0579121112823486,
        "learning_rate": 2.71199352984719e-06,
        "epoch": 0.6194895591647331,
        "step": 4806
    },
    {
        "loss": 1.7569,
        "grad_norm": 2.3764989376068115,
        "learning_rate": 2.6979385477831186e-06,
        "epoch": 0.6196184583655582,
        "step": 4807
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.178474187850952,
        "learning_rate": 2.683919582539818e-06,
        "epoch": 0.6197473575663831,
        "step": 4808
    },
    {
        "loss": 2.395,
        "grad_norm": 1.1660748720169067,
        "learning_rate": 2.6699366393064918e-06,
        "epoch": 0.619876256767208,
        "step": 4809
    },
    {
        "loss": 1.7376,
        "grad_norm": 1.4978668689727783,
        "learning_rate": 2.6559897232590114e-06,
        "epoch": 0.620005155968033,
        "step": 4810
    },
    {
        "loss": 1.6915,
        "grad_norm": 1.911730170249939,
        "learning_rate": 2.6420788395598694e-06,
        "epoch": 0.620134055168858,
        "step": 4811
    },
    {
        "loss": 2.2079,
        "grad_norm": 2.1477625370025635,
        "learning_rate": 2.6282039933582357e-06,
        "epoch": 0.6202629543696829,
        "step": 4812
    },
    {
        "loss": 0.8756,
        "grad_norm": 2.2257280349731445,
        "learning_rate": 2.6143651897900024e-06,
        "epoch": 0.6203918535705079,
        "step": 4813
    },
    {
        "loss": 1.5051,
        "grad_norm": 1.8570998907089233,
        "learning_rate": 2.6005624339776045e-06,
        "epoch": 0.6205207527713328,
        "step": 4814
    },
    {
        "loss": 1.6198,
        "grad_norm": 2.598715305328369,
        "learning_rate": 2.586795731030234e-06,
        "epoch": 0.6206496519721578,
        "step": 4815
    },
    {
        "loss": 2.0492,
        "grad_norm": 1.8084893226623535,
        "learning_rate": 2.5730650860437134e-06,
        "epoch": 0.6207785511729828,
        "step": 4816
    },
    {
        "loss": 1.4106,
        "grad_norm": 1.8645395040512085,
        "learning_rate": 2.5593705041004557e-06,
        "epoch": 0.6209074503738077,
        "step": 4817
    },
    {
        "loss": 1.3352,
        "grad_norm": 2.302062749862671,
        "learning_rate": 2.545711990269628e-06,
        "epoch": 0.6210363495746326,
        "step": 4818
    },
    {
        "loss": 2.1497,
        "grad_norm": 1.996803641319275,
        "learning_rate": 2.5320895496069643e-06,
        "epoch": 0.6211652487754576,
        "step": 4819
    },
    {
        "loss": 0.9868,
        "grad_norm": 2.2444539070129395,
        "learning_rate": 2.518503187154897e-06,
        "epoch": 0.6212941479762826,
        "step": 4820
    },
    {
        "loss": 1.5133,
        "grad_norm": 1.9881210327148438,
        "learning_rate": 2.5049529079424594e-06,
        "epoch": 0.6214230471771075,
        "step": 4821
    },
    {
        "loss": 1.7651,
        "grad_norm": 1.360834002494812,
        "learning_rate": 2.4914387169853613e-06,
        "epoch": 0.6215519463779324,
        "step": 4822
    },
    {
        "loss": 1.7251,
        "grad_norm": 1.9903333187103271,
        "learning_rate": 2.4779606192859463e-06,
        "epoch": 0.6216808455787574,
        "step": 4823
    },
    {
        "loss": 2.0868,
        "grad_norm": 1.810875415802002,
        "learning_rate": 2.4645186198332117e-06,
        "epoch": 0.6218097447795824,
        "step": 4824
    },
    {
        "loss": 1.9288,
        "grad_norm": 1.0291653871536255,
        "learning_rate": 2.4511127236027442e-06,
        "epoch": 0.6219386439804073,
        "step": 4825
    },
    {
        "loss": 1.7793,
        "grad_norm": 2.8113291263580322,
        "learning_rate": 2.4377429355568305e-06,
        "epoch": 0.6220675431812323,
        "step": 4826
    },
    {
        "loss": 1.5602,
        "grad_norm": 3.024362564086914,
        "learning_rate": 2.424409260644345e-06,
        "epoch": 0.6221964423820572,
        "step": 4827
    },
    {
        "loss": 1.9475,
        "grad_norm": 2.0618317127227783,
        "learning_rate": 2.411111703800817e-06,
        "epoch": 0.6223253415828822,
        "step": 4828
    },
    {
        "loss": 1.506,
        "grad_norm": 1.7203494310379028,
        "learning_rate": 2.3978502699483875e-06,
        "epoch": 0.6224542407837071,
        "step": 4829
    },
    {
        "loss": 1.7609,
        "grad_norm": 1.4181023836135864,
        "learning_rate": 2.384624963995874e-06,
        "epoch": 0.6225831399845321,
        "step": 4830
    },
    {
        "loss": 1.6894,
        "grad_norm": 1.388064980506897,
        "learning_rate": 2.3714357908386387e-06,
        "epoch": 0.622712039185357,
        "step": 4831
    },
    {
        "loss": 1.27,
        "grad_norm": 2.492011785507202,
        "learning_rate": 2.3582827553587428e-06,
        "epoch": 0.622840938386182,
        "step": 4832
    },
    {
        "loss": 2.1731,
        "grad_norm": 1.591387152671814,
        "learning_rate": 2.3451658624248587e-06,
        "epoch": 0.622969837587007,
        "step": 4833
    },
    {
        "loss": 2.5481,
        "grad_norm": 1.4458062648773193,
        "learning_rate": 2.332085116892235e-06,
        "epoch": 0.6230987367878319,
        "step": 4834
    },
    {
        "loss": 1.9691,
        "grad_norm": 2.891507625579834,
        "learning_rate": 2.3190405236028e-06,
        "epoch": 0.6232276359886568,
        "step": 4835
    },
    {
        "loss": 1.8876,
        "grad_norm": 1.5611295700073242,
        "learning_rate": 2.306032087385046e-06,
        "epoch": 0.6233565351894819,
        "step": 4836
    },
    {
        "loss": 2.1664,
        "grad_norm": 1.8289388418197632,
        "learning_rate": 2.2930598130541215e-06,
        "epoch": 0.6234854343903068,
        "step": 4837
    },
    {
        "loss": 2.3545,
        "grad_norm": 1.6449986696243286,
        "learning_rate": 2.2801237054117964e-06,
        "epoch": 0.6236143335911317,
        "step": 4838
    },
    {
        "loss": 1.8223,
        "grad_norm": 1.713510274887085,
        "learning_rate": 2.2672237692463847e-06,
        "epoch": 0.6237432327919566,
        "step": 4839
    },
    {
        "loss": 1.5404,
        "grad_norm": 2.1848092079162598,
        "learning_rate": 2.254360009332901e-06,
        "epoch": 0.6238721319927817,
        "step": 4840
    },
    {
        "loss": 2.3196,
        "grad_norm": 1.4358171224594116,
        "learning_rate": 2.241532430432913e-06,
        "epoch": 0.6240010311936066,
        "step": 4841
    },
    {
        "loss": 1.7501,
        "grad_norm": 2.2015628814697266,
        "learning_rate": 2.2287410372945905e-06,
        "epoch": 0.6241299303944315,
        "step": 4842
    },
    {
        "loss": 2.2695,
        "grad_norm": 1.875514268875122,
        "learning_rate": 2.2159858346527563e-06,
        "epoch": 0.6242588295952565,
        "step": 4843
    },
    {
        "loss": 2.0296,
        "grad_norm": 1.9175297021865845,
        "learning_rate": 2.2032668272288113e-06,
        "epoch": 0.6243877287960815,
        "step": 4844
    },
    {
        "loss": 2.0365,
        "grad_norm": 1.8149747848510742,
        "learning_rate": 2.190584019730735e-06,
        "epoch": 0.6245166279969064,
        "step": 4845
    },
    {
        "loss": 1.8111,
        "grad_norm": 1.8640159368515015,
        "learning_rate": 2.1779374168531262e-06,
        "epoch": 0.6246455271977314,
        "step": 4846
    },
    {
        "loss": 2.1279,
        "grad_norm": 1.5436232089996338,
        "learning_rate": 2.1653270232772306e-06,
        "epoch": 0.6247744263985563,
        "step": 4847
    },
    {
        "loss": 1.8115,
        "grad_norm": 1.9021614789962769,
        "learning_rate": 2.152752843670791e-06,
        "epoch": 0.6249033255993813,
        "step": 4848
    },
    {
        "loss": 1.9537,
        "grad_norm": 1.9172979593276978,
        "learning_rate": 2.1402148826882183e-06,
        "epoch": 0.6250322248002063,
        "step": 4849
    },
    {
        "loss": 1.8752,
        "grad_norm": 1.6293352842330933,
        "learning_rate": 2.127713144970511e-06,
        "epoch": 0.6251611240010312,
        "step": 4850
    },
    {
        "loss": 2.1161,
        "grad_norm": 1.2241631746292114,
        "learning_rate": 2.1152476351452346e-06,
        "epoch": 0.6252900232018561,
        "step": 4851
    },
    {
        "loss": 1.6999,
        "grad_norm": 2.2252748012542725,
        "learning_rate": 2.1028183578265525e-06,
        "epoch": 0.6254189224026812,
        "step": 4852
    },
    {
        "loss": 1.4612,
        "grad_norm": 2.0711662769317627,
        "learning_rate": 2.090425317615241e-06,
        "epoch": 0.6255478216035061,
        "step": 4853
    },
    {
        "loss": 1.9625,
        "grad_norm": 2.348602056503296,
        "learning_rate": 2.0780685190986195e-06,
        "epoch": 0.625676720804331,
        "step": 4854
    },
    {
        "loss": 1.7368,
        "grad_norm": 1.566524863243103,
        "learning_rate": 2.0657479668506287e-06,
        "epoch": 0.6258056200051559,
        "step": 4855
    },
    {
        "loss": 1.8983,
        "grad_norm": 1.256838083267212,
        "learning_rate": 2.0534636654317764e-06,
        "epoch": 0.625934519205981,
        "step": 4856
    },
    {
        "loss": 1.2445,
        "grad_norm": 2.104574203491211,
        "learning_rate": 2.0412156193891583e-06,
        "epoch": 0.6260634184068059,
        "step": 4857
    },
    {
        "loss": 1.9784,
        "grad_norm": 1.6674096584320068,
        "learning_rate": 2.0290038332564488e-06,
        "epoch": 0.6261923176076308,
        "step": 4858
    },
    {
        "loss": 2.3287,
        "grad_norm": 1.159015417098999,
        "learning_rate": 2.0168283115538865e-06,
        "epoch": 0.6263212168084558,
        "step": 4859
    },
    {
        "loss": 2.3834,
        "grad_norm": 1.765244960784912,
        "learning_rate": 2.0046890587883005e-06,
        "epoch": 0.6264501160092807,
        "step": 4860
    },
    {
        "loss": 2.0288,
        "grad_norm": 1.2233912944793701,
        "learning_rate": 1.992586079453118e-06,
        "epoch": 0.6265790152101057,
        "step": 4861
    },
    {
        "loss": 1.4639,
        "grad_norm": 2.111222267150879,
        "learning_rate": 1.9805193780282893e-06,
        "epoch": 0.6267079144109307,
        "step": 4862
    },
    {
        "loss": 1.9872,
        "grad_norm": 1.9215501546859741,
        "learning_rate": 1.9684889589803414e-06,
        "epoch": 0.6268368136117556,
        "step": 4863
    },
    {
        "loss": 1.9188,
        "grad_norm": 1.2507861852645874,
        "learning_rate": 1.956494826762456e-06,
        "epoch": 0.6269657128125805,
        "step": 4864
    },
    {
        "loss": 1.7931,
        "grad_norm": 1.5545258522033691,
        "learning_rate": 1.9445369858142494e-06,
        "epoch": 0.6270946120134056,
        "step": 4865
    },
    {
        "loss": 1.9427,
        "grad_norm": 1.8479939699172974,
        "learning_rate": 1.932615440562013e-06,
        "epoch": 0.6272235112142305,
        "step": 4866
    },
    {
        "loss": 2.1415,
        "grad_norm": 1.9074257612228394,
        "learning_rate": 1.920730195418552e-06,
        "epoch": 0.6273524104150554,
        "step": 4867
    },
    {
        "loss": 2.0635,
        "grad_norm": 1.3417381048202515,
        "learning_rate": 1.908881254783246e-06,
        "epoch": 0.6274813096158803,
        "step": 4868
    },
    {
        "loss": 1.9043,
        "grad_norm": 1.3265249729156494,
        "learning_rate": 1.8970686230420308e-06,
        "epoch": 0.6276102088167054,
        "step": 4869
    },
    {
        "loss": 1.4653,
        "grad_norm": 2.359950065612793,
        "learning_rate": 1.885292304567432e-06,
        "epoch": 0.6277391080175303,
        "step": 4870
    },
    {
        "loss": 1.9353,
        "grad_norm": 1.2891650199890137,
        "learning_rate": 1.8735523037184843e-06,
        "epoch": 0.6278680072183552,
        "step": 4871
    },
    {
        "loss": 2.6479,
        "grad_norm": 1.1184591054916382,
        "learning_rate": 1.8618486248408118e-06,
        "epoch": 0.6279969064191802,
        "step": 4872
    },
    {
        "loss": 2.0327,
        "grad_norm": 2.0289249420166016,
        "learning_rate": 1.8501812722666046e-06,
        "epoch": 0.6281258056200052,
        "step": 4873
    },
    {
        "loss": 2.0146,
        "grad_norm": 2.120823621749878,
        "learning_rate": 1.8385502503145635e-06,
        "epoch": 0.6282547048208301,
        "step": 4874
    },
    {
        "loss": 1.9585,
        "grad_norm": 2.1368465423583984,
        "learning_rate": 1.826955563289967e-06,
        "epoch": 0.628383604021655,
        "step": 4875
    },
    {
        "loss": 2.1269,
        "grad_norm": 1.932334542274475,
        "learning_rate": 1.8153972154846822e-06,
        "epoch": 0.62851250322248,
        "step": 4876
    },
    {
        "loss": 2.0294,
        "grad_norm": 2.064884662628174,
        "learning_rate": 1.8038752111770307e-06,
        "epoch": 0.628641402423305,
        "step": 4877
    },
    {
        "loss": 1.6005,
        "grad_norm": 3.4967408180236816,
        "learning_rate": 1.792389554631968e-06,
        "epoch": 0.62877030162413,
        "step": 4878
    },
    {
        "loss": 1.775,
        "grad_norm": 1.6122676134109497,
        "learning_rate": 1.780940250100982e-06,
        "epoch": 0.6288992008249549,
        "step": 4879
    },
    {
        "loss": 2.0722,
        "grad_norm": 1.897159457206726,
        "learning_rate": 1.769527301822027e-06,
        "epoch": 0.6290281000257798,
        "step": 4880
    },
    {
        "loss": 2.4071,
        "grad_norm": 2.133084297180176,
        "learning_rate": 1.758150714019724e-06,
        "epoch": 0.6291569992266048,
        "step": 4881
    },
    {
        "loss": 1.6536,
        "grad_norm": 2.951040267944336,
        "learning_rate": 1.7468104909051486e-06,
        "epoch": 0.6292858984274298,
        "step": 4882
    },
    {
        "loss": 2.3869,
        "grad_norm": 2.002032518386841,
        "learning_rate": 1.735506636675932e-06,
        "epoch": 0.6294147976282547,
        "step": 4883
    },
    {
        "loss": 2.05,
        "grad_norm": 2.139866352081299,
        "learning_rate": 1.7242391555162496e-06,
        "epoch": 0.6295436968290796,
        "step": 4884
    },
    {
        "loss": 1.6169,
        "grad_norm": 1.538939356803894,
        "learning_rate": 1.7130080515968095e-06,
        "epoch": 0.6296725960299047,
        "step": 4885
    },
    {
        "loss": 1.5498,
        "grad_norm": 1.774987816810608,
        "learning_rate": 1.701813329074875e-06,
        "epoch": 0.6298014952307296,
        "step": 4886
    },
    {
        "loss": 1.9186,
        "grad_norm": 2.0771217346191406,
        "learning_rate": 1.6906549920942205e-06,
        "epoch": 0.6299303944315545,
        "step": 4887
    },
    {
        "loss": 1.5474,
        "grad_norm": 2.729550361633301,
        "learning_rate": 1.6795330447851421e-06,
        "epoch": 0.6300592936323794,
        "step": 4888
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.7673141956329346,
        "learning_rate": 1.668447491264491e-06,
        "epoch": 0.6301881928332045,
        "step": 4889
    },
    {
        "loss": 2.1986,
        "grad_norm": 2.0532681941986084,
        "learning_rate": 1.6573983356356514e-06,
        "epoch": 0.6303170920340294,
        "step": 4890
    },
    {
        "loss": 2.1333,
        "grad_norm": 1.929817795753479,
        "learning_rate": 1.6463855819885077e-06,
        "epoch": 0.6304459912348543,
        "step": 4891
    },
    {
        "loss": 1.6741,
        "grad_norm": 1.5727512836456299,
        "learning_rate": 1.6354092343994765e-06,
        "epoch": 0.6305748904356793,
        "step": 4892
    },
    {
        "loss": 2.2975,
        "grad_norm": 1.6332697868347168,
        "learning_rate": 1.62446929693153e-06,
        "epoch": 0.6307037896365043,
        "step": 4893
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.6832756996154785,
        "learning_rate": 1.613565773634096e-06,
        "epoch": 0.6308326888373292,
        "step": 4894
    },
    {
        "loss": 1.7784,
        "grad_norm": 2.354684829711914,
        "learning_rate": 1.6026986685432011e-06,
        "epoch": 0.6309615880381542,
        "step": 4895
    },
    {
        "loss": 1.8669,
        "grad_norm": 1.5422449111938477,
        "learning_rate": 1.5918679856813611e-06,
        "epoch": 0.6310904872389791,
        "step": 4896
    },
    {
        "loss": 1.4389,
        "grad_norm": 2.1367170810699463,
        "learning_rate": 1.5810737290575583e-06,
        "epoch": 0.631219386439804,
        "step": 4897
    },
    {
        "loss": 0.7363,
        "grad_norm": 2.649909734725952,
        "learning_rate": 1.5703159026673852e-06,
        "epoch": 0.6313482856406291,
        "step": 4898
    },
    {
        "loss": 1.7448,
        "grad_norm": 1.9733681678771973,
        "learning_rate": 1.5595945104928788e-06,
        "epoch": 0.631477184841454,
        "step": 4899
    },
    {
        "loss": 1.745,
        "grad_norm": 2.281158447265625,
        "learning_rate": 1.5489095565026313e-06,
        "epoch": 0.6316060840422789,
        "step": 4900
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.6810486316680908,
        "learning_rate": 1.5382610446517231e-06,
        "epoch": 0.6317349832431038,
        "step": 4901
    },
    {
        "loss": 2.1007,
        "grad_norm": 1.1315932273864746,
        "learning_rate": 1.5276489788817238e-06,
        "epoch": 0.6318638824439289,
        "step": 4902
    },
    {
        "loss": 2.3964,
        "grad_norm": 1.9354815483093262,
        "learning_rate": 1.51707336312078e-06,
        "epoch": 0.6319927816447538,
        "step": 4903
    },
    {
        "loss": 1.4878,
        "grad_norm": 2.345643997192383,
        "learning_rate": 1.5065342012834826e-06,
        "epoch": 0.6321216808455787,
        "step": 4904
    },
    {
        "loss": 2.1495,
        "grad_norm": 1.8054150342941284,
        "learning_rate": 1.4960314972709554e-06,
        "epoch": 0.6322505800464037,
        "step": 4905
    },
    {
        "loss": 2.3258,
        "grad_norm": 1.0318397283554077,
        "learning_rate": 1.4855652549708331e-06,
        "epoch": 0.6323794792472287,
        "step": 4906
    },
    {
        "loss": 1.8213,
        "grad_norm": 2.558358907699585,
        "learning_rate": 1.475135478257239e-06,
        "epoch": 0.6325083784480536,
        "step": 4907
    },
    {
        "loss": 1.4737,
        "grad_norm": 2.2116332054138184,
        "learning_rate": 1.4647421709908183e-06,
        "epoch": 0.6326372776488786,
        "step": 4908
    },
    {
        "loss": 1.3486,
        "grad_norm": 1.079724907875061,
        "learning_rate": 1.45438533701866e-06,
        "epoch": 0.6327661768497035,
        "step": 4909
    },
    {
        "loss": 1.8167,
        "grad_norm": 1.245789885520935,
        "learning_rate": 1.4440649801744533e-06,
        "epoch": 0.6328950760505285,
        "step": 4910
    },
    {
        "loss": 1.7539,
        "grad_norm": 1.4025306701660156,
        "learning_rate": 1.4337811042782867e-06,
        "epoch": 0.6330239752513535,
        "step": 4911
    },
    {
        "loss": 2.0678,
        "grad_norm": 1.2482939958572388,
        "learning_rate": 1.4235337131368043e-06,
        "epoch": 0.6331528744521784,
        "step": 4912
    },
    {
        "loss": 1.83,
        "grad_norm": 1.0681401491165161,
        "learning_rate": 1.4133228105431163e-06,
        "epoch": 0.6332817736530033,
        "step": 4913
    },
    {
        "loss": 2.194,
        "grad_norm": 1.0680454969406128,
        "learning_rate": 1.4031484002768436e-06,
        "epoch": 0.6334106728538283,
        "step": 4914
    },
    {
        "loss": 0.7438,
        "grad_norm": 1.4829951524734497,
        "learning_rate": 1.393010486104096e-06,
        "epoch": 0.6335395720546533,
        "step": 4915
    },
    {
        "loss": 2.1289,
        "grad_norm": 1.8321830034255981,
        "learning_rate": 1.3829090717774495e-06,
        "epoch": 0.6336684712554782,
        "step": 4916
    },
    {
        "loss": 1.2291,
        "grad_norm": 1.787033200263977,
        "learning_rate": 1.3728441610360133e-06,
        "epoch": 0.6337973704563031,
        "step": 4917
    },
    {
        "loss": 1.835,
        "grad_norm": 1.9317094087600708,
        "learning_rate": 1.3628157576053624e-06,
        "epoch": 0.6339262696571282,
        "step": 4918
    },
    {
        "loss": 1.7166,
        "grad_norm": 1.7562867403030396,
        "learning_rate": 1.35282386519755e-06,
        "epoch": 0.6340551688579531,
        "step": 4919
    },
    {
        "loss": 2.5077,
        "grad_norm": 1.5538183450698853,
        "learning_rate": 1.3428684875111175e-06,
        "epoch": 0.634184068058778,
        "step": 4920
    },
    {
        "loss": 2.4551,
        "grad_norm": 1.2941887378692627,
        "learning_rate": 1.332949628231106e-06,
        "epoch": 0.634312967259603,
        "step": 4921
    },
    {
        "loss": 2.3761,
        "grad_norm": 1.510314702987671,
        "learning_rate": 1.323067291029023e-06,
        "epoch": 0.634441866460428,
        "step": 4922
    },
    {
        "loss": 2.1492,
        "grad_norm": 2.3416383266448975,
        "learning_rate": 1.3132214795628539e-06,
        "epoch": 0.6345707656612529,
        "step": 4923
    },
    {
        "loss": 1.9348,
        "grad_norm": 1.928720474243164,
        "learning_rate": 1.303412197477094e-06,
        "epoch": 0.6346996648620779,
        "step": 4924
    },
    {
        "loss": 2.6409,
        "grad_norm": 1.7540717124938965,
        "learning_rate": 1.2936394484026836e-06,
        "epoch": 0.6348285640629028,
        "step": 4925
    },
    {
        "loss": 1.4548,
        "grad_norm": 1.9903979301452637,
        "learning_rate": 1.2839032359570402e-06,
        "epoch": 0.6349574632637278,
        "step": 4926
    },
    {
        "loss": 1.9813,
        "grad_norm": 1.849910020828247,
        "learning_rate": 1.2742035637440919e-06,
        "epoch": 0.6350863624645527,
        "step": 4927
    },
    {
        "loss": 1.9801,
        "grad_norm": 1.23283851146698,
        "learning_rate": 1.2645404353542001e-06,
        "epoch": 0.6352152616653777,
        "step": 4928
    },
    {
        "loss": 1.1824,
        "grad_norm": 1.6393327713012695,
        "learning_rate": 1.2549138543642258e-06,
        "epoch": 0.6353441608662026,
        "step": 4929
    },
    {
        "loss": 1.3981,
        "grad_norm": 1.7947800159454346,
        "learning_rate": 1.2453238243375077e-06,
        "epoch": 0.6354730600670276,
        "step": 4930
    },
    {
        "loss": 1.5072,
        "grad_norm": 2.12288498878479,
        "learning_rate": 1.235770348823806e-06,
        "epoch": 0.6356019592678526,
        "step": 4931
    },
    {
        "loss": 2.0073,
        "grad_norm": 2.230677366256714,
        "learning_rate": 1.2262534313594143e-06,
        "epoch": 0.6357308584686775,
        "step": 4932
    },
    {
        "loss": 1.8466,
        "grad_norm": 1.8270235061645508,
        "learning_rate": 1.2167730754670593e-06,
        "epoch": 0.6358597576695024,
        "step": 4933
    },
    {
        "loss": 2.191,
        "grad_norm": 2.297436237335205,
        "learning_rate": 1.2073292846559336e-06,
        "epoch": 0.6359886568703274,
        "step": 4934
    },
    {
        "loss": 1.1888,
        "grad_norm": 2.4629220962524414,
        "learning_rate": 1.1979220624216968e-06,
        "epoch": 0.6361175560711524,
        "step": 4935
    },
    {
        "loss": 1.9151,
        "grad_norm": 1.7709534168243408,
        "learning_rate": 1.1885514122464968e-06,
        "epoch": 0.6362464552719773,
        "step": 4936
    },
    {
        "loss": 1.4154,
        "grad_norm": 2.0726091861724854,
        "learning_rate": 1.1792173375989145e-06,
        "epoch": 0.6363753544728022,
        "step": 4937
    },
    {
        "loss": 1.3415,
        "grad_norm": 5.655329704284668,
        "learning_rate": 1.1699198419339863e-06,
        "epoch": 0.6365042536736272,
        "step": 4938
    },
    {
        "loss": 2.2687,
        "grad_norm": 1.5551220178604126,
        "learning_rate": 1.1606589286932478e-06,
        "epoch": 0.6366331528744522,
        "step": 4939
    },
    {
        "loss": 1.9041,
        "grad_norm": 1.5483818054199219,
        "learning_rate": 1.1514346013046574e-06,
        "epoch": 0.6367620520752771,
        "step": 4940
    },
    {
        "loss": 1.58,
        "grad_norm": 3.138206720352173,
        "learning_rate": 1.1422468631826498e-06,
        "epoch": 0.6368909512761021,
        "step": 4941
    },
    {
        "loss": 1.5407,
        "grad_norm": 2.182060956954956,
        "learning_rate": 1.1330957177281053e-06,
        "epoch": 0.637019850476927,
        "step": 4942
    },
    {
        "loss": 0.8625,
        "grad_norm": 2.17403244972229,
        "learning_rate": 1.1239811683283585e-06,
        "epoch": 0.637148749677752,
        "step": 4943
    },
    {
        "loss": 2.1333,
        "grad_norm": 1.423549771308899,
        "learning_rate": 1.1149032183572217e-06,
        "epoch": 0.637277648878577,
        "step": 4944
    },
    {
        "loss": 2.0463,
        "grad_norm": 1.6253126859664917,
        "learning_rate": 1.105861871174918e-06,
        "epoch": 0.6374065480794019,
        "step": 4945
    },
    {
        "loss": 2.0751,
        "grad_norm": 1.519243836402893,
        "learning_rate": 1.0968571301281594e-06,
        "epoch": 0.6375354472802268,
        "step": 4946
    },
    {
        "loss": 1.7943,
        "grad_norm": 1.3710342645645142,
        "learning_rate": 1.0878889985500906e-06,
        "epoch": 0.6376643464810519,
        "step": 4947
    },
    {
        "loss": 2.1802,
        "grad_norm": 1.9531662464141846,
        "learning_rate": 1.078957479760312e-06,
        "epoch": 0.6377932456818768,
        "step": 4948
    },
    {
        "loss": 1.974,
        "grad_norm": 1.7282276153564453,
        "learning_rate": 1.0700625770648565e-06,
        "epoch": 0.6379221448827017,
        "step": 4949
    },
    {
        "loss": 2.1003,
        "grad_norm": 1.5295262336730957,
        "learning_rate": 1.061204293756235e-06,
        "epoch": 0.6380510440835266,
        "step": 4950
    },
    {
        "loss": 2.3748,
        "grad_norm": 2.1168646812438965,
        "learning_rate": 1.0523826331133579e-06,
        "epoch": 0.6381799432843517,
        "step": 4951
    },
    {
        "loss": 2.0224,
        "grad_norm": 1.7844924926757812,
        "learning_rate": 1.0435975984016245e-06,
        "epoch": 0.6383088424851766,
        "step": 4952
    },
    {
        "loss": 2.3711,
        "grad_norm": 1.5892999172210693,
        "learning_rate": 1.0348491928728443e-06,
        "epoch": 0.6384377416860015,
        "step": 4953
    },
    {
        "loss": 2.1498,
        "grad_norm": 1.3919495344161987,
        "learning_rate": 1.026137419765294e-06,
        "epoch": 0.6385666408868265,
        "step": 4954
    },
    {
        "loss": 0.7732,
        "grad_norm": 2.030967950820923,
        "learning_rate": 1.0174622823036606e-06,
        "epoch": 0.6386955400876515,
        "step": 4955
    },
    {
        "loss": 1.277,
        "grad_norm": 1.6966147422790527,
        "learning_rate": 1.0088237836991088e-06,
        "epoch": 0.6388244392884764,
        "step": 4956
    },
    {
        "loss": 2.4521,
        "grad_norm": 1.859792947769165,
        "learning_rate": 1.000221927149203e-06,
        "epoch": 0.6389533384893014,
        "step": 4957
    },
    {
        "loss": 1.8611,
        "grad_norm": 1.7464849948883057,
        "learning_rate": 9.91656715837963e-07,
        "epoch": 0.6390822376901263,
        "step": 4958
    },
    {
        "loss": 1.7352,
        "grad_norm": 1.363759160041809,
        "learning_rate": 9.831281529358638e-07,
        "epoch": 0.6392111368909513,
        "step": 4959
    },
    {
        "loss": 0.9731,
        "grad_norm": 2.2449557781219482,
        "learning_rate": 9.746362415997578e-07,
        "epoch": 0.6393400360917763,
        "step": 4960
    },
    {
        "loss": 1.6363,
        "grad_norm": 2.2336127758026123,
        "learning_rate": 9.661809849729976e-07,
        "epoch": 0.6394689352926012,
        "step": 4961
    },
    {
        "loss": 1.9382,
        "grad_norm": 2.578212022781372,
        "learning_rate": 9.577623861853125e-07,
        "epoch": 0.6395978344934261,
        "step": 4962
    },
    {
        "loss": 1.8789,
        "grad_norm": 1.2390645742416382,
        "learning_rate": 9.493804483529101e-07,
        "epoch": 0.6397267336942511,
        "step": 4963
    },
    {
        "loss": 1.8123,
        "grad_norm": 2.0658106803894043,
        "learning_rate": 9.410351745783863e-07,
        "epoch": 0.6398556328950761,
        "step": 4964
    },
    {
        "loss": 2.4942,
        "grad_norm": 1.7060836553573608,
        "learning_rate": 9.327265679507813e-07,
        "epoch": 0.639984532095901,
        "step": 4965
    },
    {
        "loss": 1.4614,
        "grad_norm": 3.546854019165039,
        "learning_rate": 9.244546315455904e-07,
        "epoch": 0.6401134312967259,
        "step": 4966
    },
    {
        "loss": 0.8227,
        "grad_norm": 2.549898147583008,
        "learning_rate": 9.162193684246867e-07,
        "epoch": 0.6402423304975509,
        "step": 4967
    },
    {
        "loss": 2.6409,
        "grad_norm": 1.229936122894287,
        "learning_rate": 9.080207816363983e-07,
        "epoch": 0.6403712296983759,
        "step": 4968
    },
    {
        "loss": 2.204,
        "grad_norm": 1.69652259349823,
        "learning_rate": 8.998588742154646e-07,
        "epoch": 0.6405001288992008,
        "step": 4969
    },
    {
        "loss": 2.5463,
        "grad_norm": 1.3851258754730225,
        "learning_rate": 8.917336491830797e-07,
        "epoch": 0.6406290281000258,
        "step": 4970
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.7232404947280884,
        "learning_rate": 8.836451095468157e-07,
        "epoch": 0.6407579273008507,
        "step": 4971
    },
    {
        "loss": 2.0614,
        "grad_norm": 1.4338575601577759,
        "learning_rate": 8.755932583006666e-07,
        "epoch": 0.6408868265016757,
        "step": 4972
    },
    {
        "loss": 0.8456,
        "grad_norm": 1.6087759733200073,
        "learning_rate": 8.675780984251036e-07,
        "epoch": 0.6410157257025006,
        "step": 4973
    },
    {
        "loss": 1.0701,
        "grad_norm": 1.317126989364624,
        "learning_rate": 8.595996328869316e-07,
        "epoch": 0.6411446249033256,
        "step": 4974
    },
    {
        "loss": 0.9061,
        "grad_norm": 3.2089250087738037,
        "learning_rate": 8.516578646394546e-07,
        "epoch": 0.6412735241041505,
        "step": 4975
    },
    {
        "loss": 2.0653,
        "grad_norm": 2.8792948722839355,
        "learning_rate": 8.437527966223435e-07,
        "epoch": 0.6414024233049755,
        "step": 4976
    },
    {
        "loss": 1.9767,
        "grad_norm": 1.8017417192459106,
        "learning_rate": 8.35884431761691e-07,
        "epoch": 0.6415313225058005,
        "step": 4977
    },
    {
        "loss": 1.7009,
        "grad_norm": 2.2232768535614014,
        "learning_rate": 8.280527729700116e-07,
        "epoch": 0.6416602217066254,
        "step": 4978
    },
    {
        "loss": 2.0401,
        "grad_norm": 3.006044626235962,
        "learning_rate": 8.202578231462421e-07,
        "epoch": 0.6417891209074503,
        "step": 4979
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.7062795162200928,
        "learning_rate": 8.124995851756967e-07,
        "epoch": 0.6419180201082754,
        "step": 4980
    },
    {
        "loss": 1.5777,
        "grad_norm": 1.8781771659851074,
        "learning_rate": 8.047780619301559e-07,
        "epoch": 0.6420469193091003,
        "step": 4981
    },
    {
        "loss": 1.9609,
        "grad_norm": 2.5140225887298584,
        "learning_rate": 7.970932562677446e-07,
        "epoch": 0.6421758185099252,
        "step": 4982
    },
    {
        "loss": 1.9364,
        "grad_norm": 2.3809616565704346,
        "learning_rate": 7.894451710330542e-07,
        "epoch": 0.6423047177107502,
        "step": 4983
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.7775168418884277,
        "learning_rate": 7.818338090570643e-07,
        "epoch": 0.6424336169115752,
        "step": 4984
    },
    {
        "loss": 1.8677,
        "grad_norm": 1.6279079914093018,
        "learning_rate": 7.742591731571325e-07,
        "epoch": 0.6425625161124001,
        "step": 4985
    },
    {
        "loss": 2.3317,
        "grad_norm": 1.208672285079956,
        "learning_rate": 7.667212661370494e-07,
        "epoch": 0.642691415313225,
        "step": 4986
    },
    {
        "loss": 1.3755,
        "grad_norm": 1.7492575645446777,
        "learning_rate": 7.592200907870383e-07,
        "epoch": 0.64282031451405,
        "step": 4987
    },
    {
        "loss": 2.1042,
        "grad_norm": 2.0595974922180176,
        "learning_rate": 7.517556498836564e-07,
        "epoch": 0.642949213714875,
        "step": 4988
    },
    {
        "loss": 2.2397,
        "grad_norm": 1.3335257768630981,
        "learning_rate": 7.443279461899155e-07,
        "epoch": 0.6430781129156999,
        "step": 4989
    },
    {
        "loss": 2.5456,
        "grad_norm": 1.3937782049179077,
        "learning_rate": 7.369369824552386e-07,
        "epoch": 0.6432070121165249,
        "step": 4990
    },
    {
        "loss": 1.7526,
        "grad_norm": 1.8769540786743164,
        "learning_rate": 7.295827614153927e-07,
        "epoch": 0.6433359113173498,
        "step": 4991
    },
    {
        "loss": 1.8623,
        "grad_norm": 1.446531891822815,
        "learning_rate": 7.222652857925894e-07,
        "epoch": 0.6434648105181748,
        "step": 4992
    },
    {
        "loss": 1.7902,
        "grad_norm": 2.160386800765991,
        "learning_rate": 7.149845582954284e-07,
        "epoch": 0.6435937097189998,
        "step": 4993
    },
    {
        "loss": 1.914,
        "grad_norm": 2.4810450077056885,
        "learning_rate": 7.077405816189098e-07,
        "epoch": 0.6437226089198247,
        "step": 4994
    },
    {
        "loss": 1.8456,
        "grad_norm": 1.633256435394287,
        "learning_rate": 7.005333584444218e-07,
        "epoch": 0.6438515081206496,
        "step": 4995
    },
    {
        "loss": 1.7529,
        "grad_norm": 2.162508249282837,
        "learning_rate": 6.933628914397528e-07,
        "epoch": 0.6439804073214747,
        "step": 4996
    },
    {
        "loss": 1.8695,
        "grad_norm": 1.795817255973816,
        "learning_rate": 6.862291832590795e-07,
        "epoch": 0.6441093065222996,
        "step": 4997
    },
    {
        "loss": 1.9607,
        "grad_norm": 1.7788739204406738,
        "learning_rate": 6.791322365429898e-07,
        "epoch": 0.6442382057231245,
        "step": 4998
    },
    {
        "loss": 2.13,
        "grad_norm": 1.488289475440979,
        "learning_rate": 6.720720539184489e-07,
        "epoch": 0.6443671049239494,
        "step": 4999
    },
    {
        "loss": 1.6077,
        "grad_norm": 2.3289239406585693,
        "learning_rate": 6.650486379988108e-07,
        "epoch": 0.6444960041247745,
        "step": 5000
    },
    {
        "loss": 2.0072,
        "grad_norm": 1.4887421131134033,
        "learning_rate": 6.580619913838182e-07,
        "epoch": 0.6446249033255994,
        "step": 5001
    },
    {
        "loss": 2.2697,
        "grad_norm": 1.596265196800232,
        "learning_rate": 6.511121166596468e-07,
        "epoch": 0.6447538025264243,
        "step": 5002
    },
    {
        "loss": 1.763,
        "grad_norm": 2.021535634994507,
        "learning_rate": 6.441990163987833e-07,
        "epoch": 0.6448827017272493,
        "step": 5003
    },
    {
        "loss": 2.0726,
        "grad_norm": 2.391374111175537,
        "learning_rate": 6.373226931601583e-07,
        "epoch": 0.6450116009280742,
        "step": 5004
    },
    {
        "loss": 1.8726,
        "grad_norm": 2.8679895401000977,
        "learning_rate": 6.304831494891028e-07,
        "epoch": 0.6451405001288992,
        "step": 5005
    },
    {
        "loss": 1.9247,
        "grad_norm": 1.7253183126449585,
        "learning_rate": 6.236803879172692e-07,
        "epoch": 0.6452693993297242,
        "step": 5006
    },
    {
        "loss": 2.2058,
        "grad_norm": 1.560326099395752,
        "learning_rate": 6.169144109627545e-07,
        "epoch": 0.6453982985305491,
        "step": 5007
    },
    {
        "loss": 1.9568,
        "grad_norm": 1.8319392204284668,
        "learning_rate": 6.101852211299996e-07,
        "epoch": 0.645527197731374,
        "step": 5008
    },
    {
        "loss": 1.8269,
        "grad_norm": 2.0435616970062256,
        "learning_rate": 6.034928209098567e-07,
        "epoch": 0.645656096932199,
        "step": 5009
    },
    {
        "loss": 1.6304,
        "grad_norm": 1.3099621534347534,
        "learning_rate": 5.968372127795552e-07,
        "epoch": 0.645784996133024,
        "step": 5010
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.2402738332748413,
        "learning_rate": 5.902183992026911e-07,
        "epoch": 0.6459138953338489,
        "step": 5011
    },
    {
        "loss": 0.6683,
        "grad_norm": 1.9855623245239258,
        "learning_rate": 5.836363826292601e-07,
        "epoch": 0.6460427945346738,
        "step": 5012
    },
    {
        "loss": 2.0117,
        "grad_norm": 2.3049309253692627,
        "learning_rate": 5.770911654956135e-07,
        "epoch": 0.6461716937354989,
        "step": 5013
    },
    {
        "loss": 2.2828,
        "grad_norm": 1.5190718173980713,
        "learning_rate": 5.70582750224502e-07,
        "epoch": 0.6463005929363238,
        "step": 5014
    },
    {
        "loss": 1.4602,
        "grad_norm": 2.242762804031372,
        "learning_rate": 5.64111139225032e-07,
        "epoch": 0.6464294921371487,
        "step": 5015
    },
    {
        "loss": 1.8045,
        "grad_norm": 2.0387539863586426,
        "learning_rate": 5.576763348927316e-07,
        "epoch": 0.6465583913379737,
        "step": 5016
    },
    {
        "loss": 1.9569,
        "grad_norm": 2.281367778778076,
        "learning_rate": 5.512783396094512e-07,
        "epoch": 0.6466872905387987,
        "step": 5017
    },
    {
        "loss": 1.1383,
        "grad_norm": 2.32088041305542,
        "learning_rate": 5.449171557434408e-07,
        "epoch": 0.6468161897396236,
        "step": 5018
    },
    {
        "loss": 1.7136,
        "grad_norm": 2.061366319656372,
        "learning_rate": 5.38592785649339e-07,
        "epoch": 0.6469450889404486,
        "step": 5019
    },
    {
        "loss": 1.8113,
        "grad_norm": 1.5221298933029175,
        "learning_rate": 5.323052316681177e-07,
        "epoch": 0.6470739881412735,
        "step": 5020
    },
    {
        "loss": 2.0802,
        "grad_norm": 1.8549426794052124,
        "learning_rate": 5.260544961271596e-07,
        "epoch": 0.6472028873420985,
        "step": 5021
    },
    {
        "loss": 2.4969,
        "grad_norm": 1.5397125482559204,
        "learning_rate": 5.19840581340203e-07,
        "epoch": 0.6473317865429234,
        "step": 5022
    },
    {
        "loss": 1.0307,
        "grad_norm": 2.427901029586792,
        "learning_rate": 5.136634896073411e-07,
        "epoch": 0.6474606857437484,
        "step": 5023
    },
    {
        "loss": 1.9282,
        "grad_norm": 1.8040144443511963,
        "learning_rate": 5.075232232150895e-07,
        "epoch": 0.6475895849445733,
        "step": 5024
    },
    {
        "loss": 2.4343,
        "grad_norm": 1.386756420135498,
        "learning_rate": 5.014197844362633e-07,
        "epoch": 0.6477184841453983,
        "step": 5025
    },
    {
        "loss": 2.3016,
        "grad_norm": 1.6041508913040161,
        "learning_rate": 4.953531755300999e-07,
        "epoch": 0.6478473833462233,
        "step": 5026
    },
    {
        "loss": 2.0674,
        "grad_norm": 1.6013176441192627,
        "learning_rate": 4.893233987421697e-07,
        "epoch": 0.6479762825470482,
        "step": 5027
    },
    {
        "loss": 1.8748,
        "grad_norm": 1.9625964164733887,
        "learning_rate": 4.833304563044316e-07,
        "epoch": 0.6481051817478731,
        "step": 5028
    },
    {
        "loss": 1.905,
        "grad_norm": 1.802460789680481,
        "learning_rate": 4.773743504352002e-07,
        "epoch": 0.6482340809486982,
        "step": 5029
    },
    {
        "loss": 1.988,
        "grad_norm": 1.382964849472046,
        "learning_rate": 4.7145508333915625e-07,
        "epoch": 0.6483629801495231,
        "step": 5030
    },
    {
        "loss": 1.6219,
        "grad_norm": 2.1224653720855713,
        "learning_rate": 4.655726572073471e-07,
        "epoch": 0.648491879350348,
        "step": 5031
    },
    {
        "loss": 1.9084,
        "grad_norm": 2.437652349472046,
        "learning_rate": 4.597270742171755e-07,
        "epoch": 0.648620778551173,
        "step": 5032
    },
    {
        "loss": 1.9139,
        "grad_norm": 2.273618698120117,
        "learning_rate": 4.539183365324107e-07,
        "epoch": 0.648749677751998,
        "step": 5033
    },
    {
        "loss": 1.2069,
        "grad_norm": 2.0666329860687256,
        "learning_rate": 4.4814644630318816e-07,
        "epoch": 0.6488785769528229,
        "step": 5034
    },
    {
        "loss": 2.038,
        "grad_norm": 2.9104037284851074,
        "learning_rate": 4.4241140566599894e-07,
        "epoch": 0.6490074761536478,
        "step": 5035
    },
    {
        "loss": 1.2164,
        "grad_norm": 2.3169543743133545,
        "learning_rate": 4.3671321674371157e-07,
        "epoch": 0.6491363753544728,
        "step": 5036
    },
    {
        "loss": 1.8725,
        "grad_norm": 1.8665884733200073,
        "learning_rate": 4.310518816455056e-07,
        "epoch": 0.6492652745552978,
        "step": 5037
    },
    {
        "loss": 2.21,
        "grad_norm": 0.9779009222984314,
        "learning_rate": 4.2542740246697135e-07,
        "epoch": 0.6493941737561227,
        "step": 5038
    },
    {
        "loss": 2.574,
        "grad_norm": 1.5620921850204468,
        "learning_rate": 4.1983978129004344e-07,
        "epoch": 0.6495230729569477,
        "step": 5039
    },
    {
        "loss": 2.2706,
        "grad_norm": 1.589372992515564,
        "learning_rate": 4.1428902018298964e-07,
        "epoch": 0.6496519721577726,
        "step": 5040
    },
    {
        "loss": 2.1852,
        "grad_norm": 2.111513137817383,
        "learning_rate": 4.0877512120045534e-07,
        "epoch": 0.6497808713585975,
        "step": 5041
    },
    {
        "loss": 1.1787,
        "grad_norm": 1.8150441646575928,
        "learning_rate": 4.0329808638344124e-07,
        "epoch": 0.6499097705594226,
        "step": 5042
    },
    {
        "loss": 2.0757,
        "grad_norm": 1.455881953239441,
        "learning_rate": 3.9785791775930337e-07,
        "epoch": 0.6500386697602475,
        "step": 5043
    },
    {
        "loss": 1.9504,
        "grad_norm": 1.3228275775909424,
        "learning_rate": 3.9245461734171984e-07,
        "epoch": 0.6501675689610724,
        "step": 5044
    },
    {
        "loss": 1.6563,
        "grad_norm": 1.685164213180542,
        "learning_rate": 3.8708818713077966e-07,
        "epoch": 0.6502964681618973,
        "step": 5045
    },
    {
        "loss": 1.6388,
        "grad_norm": 1.7811177968978882,
        "learning_rate": 3.8175862911288276e-07,
        "epoch": 0.6504253673627224,
        "step": 5046
    },
    {
        "loss": 2.2583,
        "grad_norm": 1.4726003408432007,
        "learning_rate": 3.764659452607844e-07,
        "epoch": 0.6505542665635473,
        "step": 5047
    },
    {
        "loss": 2.2849,
        "grad_norm": 2.654029369354248,
        "learning_rate": 3.7121013753359525e-07,
        "epoch": 0.6506831657643722,
        "step": 5048
    },
    {
        "loss": 1.4707,
        "grad_norm": 2.121381998062134,
        "learning_rate": 3.659912078767813e-07,
        "epoch": 0.6508120649651972,
        "step": 5049
    },
    {
        "loss": 1.6688,
        "grad_norm": 2.0299484729766846,
        "learning_rate": 3.608091582221529e-07,
        "epoch": 0.6509409641660222,
        "step": 5050
    },
    {
        "loss": 1.7742,
        "grad_norm": 1.1743028163909912,
        "learning_rate": 3.556639904878756e-07,
        "epoch": 0.6510698633668471,
        "step": 5051
    },
    {
        "loss": 1.751,
        "grad_norm": 1.9582840204238892,
        "learning_rate": 3.5055570657844814e-07,
        "epoch": 0.6511987625676721,
        "step": 5052
    },
    {
        "loss": 1.7281,
        "grad_norm": 1.8625893592834473,
        "learning_rate": 3.4548430838473587e-07,
        "epoch": 0.651327661768497,
        "step": 5053
    },
    {
        "loss": 1.2257,
        "grad_norm": 3.702519655227661,
        "learning_rate": 3.4044979778393716e-07,
        "epoch": 0.651456560969322,
        "step": 5054
    },
    {
        "loss": 1.5249,
        "grad_norm": 1.5364676713943481,
        "learning_rate": 3.354521766395946e-07,
        "epoch": 0.651585460170147,
        "step": 5055
    },
    {
        "loss": 1.6074,
        "grad_norm": 2.645268678665161,
        "learning_rate": 3.3049144680161736e-07,
        "epoch": 0.6517143593709719,
        "step": 5056
    },
    {
        "loss": 1.7368,
        "grad_norm": 1.7768429517745972,
        "learning_rate": 3.255676101062255e-07,
        "epoch": 0.6518432585717968,
        "step": 5057
    },
    {
        "loss": 1.9159,
        "grad_norm": 2.1120219230651855,
        "learning_rate": 3.206806683760055e-07,
        "epoch": 0.6519721577726219,
        "step": 5058
    },
    {
        "loss": 2.0293,
        "grad_norm": 1.9930166006088257,
        "learning_rate": 3.1583062341988823e-07,
        "epoch": 0.6521010569734468,
        "step": 5059
    },
    {
        "loss": 1.5285,
        "grad_norm": 2.087517738342285,
        "learning_rate": 3.1101747703313756e-07,
        "epoch": 0.6522299561742717,
        "step": 5060
    },
    {
        "loss": 1.5512,
        "grad_norm": 1.8727627992630005,
        "learning_rate": 3.0624123099735057e-07,
        "epoch": 0.6523588553750966,
        "step": 5061
    },
    {
        "loss": 1.9071,
        "grad_norm": 1.4457073211669922,
        "learning_rate": 3.0150188708050197e-07,
        "epoch": 0.6524877545759217,
        "step": 5062
    },
    {
        "loss": 1.7771,
        "grad_norm": 1.3377918004989624,
        "learning_rate": 2.967994470368662e-07,
        "epoch": 0.6526166537767466,
        "step": 5063
    },
    {
        "loss": 1.807,
        "grad_norm": 1.9162421226501465,
        "learning_rate": 2.9213391260707324e-07,
        "epoch": 0.6527455529775715,
        "step": 5064
    },
    {
        "loss": 1.6052,
        "grad_norm": 2.428570032119751,
        "learning_rate": 2.875052855180971e-07,
        "epoch": 0.6528744521783965,
        "step": 5065
    },
    {
        "loss": 1.6684,
        "grad_norm": 2.7275586128234863,
        "learning_rate": 2.8291356748324506e-07,
        "epoch": 0.6530033513792215,
        "step": 5066
    },
    {
        "loss": 2.3351,
        "grad_norm": 2.841273307800293,
        "learning_rate": 2.7835876020215756e-07,
        "epoch": 0.6531322505800464,
        "step": 5067
    },
    {
        "loss": 1.3089,
        "grad_norm": 2.3828816413879395,
        "learning_rate": 2.738408653608415e-07,
        "epoch": 0.6532611497808714,
        "step": 5068
    },
    {
        "loss": 2.2052,
        "grad_norm": 2.0210254192352295,
        "learning_rate": 2.6935988463158146e-07,
        "epoch": 0.6533900489816963,
        "step": 5069
    },
    {
        "loss": 2.5353,
        "grad_norm": 1.2437424659729004,
        "learning_rate": 2.649158196730617e-07,
        "epoch": 0.6535189481825213,
        "step": 5070
    },
    {
        "loss": 1.8215,
        "grad_norm": 1.8200117349624634,
        "learning_rate": 2.605086721302663e-07,
        "epoch": 0.6536478473833462,
        "step": 5071
    },
    {
        "loss": 2.7003,
        "grad_norm": 1.4361121654510498,
        "learning_rate": 2.561384436345127e-07,
        "epoch": 0.6537767465841712,
        "step": 5072
    },
    {
        "loss": 1.5795,
        "grad_norm": 1.9532761573791504,
        "learning_rate": 2.5180513580347344e-07,
        "epoch": 0.6539056457849961,
        "step": 5073
    },
    {
        "loss": 1.5851,
        "grad_norm": 1.7846341133117676,
        "learning_rate": 2.475087502411433e-07,
        "epoch": 0.6540345449858211,
        "step": 5074
    },
    {
        "loss": 1.3833,
        "grad_norm": 3.1438539028167725,
        "learning_rate": 2.43249288537839e-07,
        "epoch": 0.6541634441866461,
        "step": 5075
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.129587173461914,
        "learning_rate": 2.3902675227022163e-07,
        "epoch": 0.654292343387471,
        "step": 5076
    },
    {
        "loss": 2.1328,
        "grad_norm": 1.7488399744033813,
        "learning_rate": 2.3484114300129646e-07,
        "epoch": 0.6544212425882959,
        "step": 5077
    },
    {
        "loss": 1.4606,
        "grad_norm": 1.563745379447937,
        "learning_rate": 2.3069246228036856e-07,
        "epoch": 0.6545501417891209,
        "step": 5078
    },
    {
        "loss": 1.642,
        "grad_norm": 2.8553338050842285,
        "learning_rate": 2.2658071164310957e-07,
        "epoch": 0.6546790409899459,
        "step": 5079
    },
    {
        "loss": 1.3561,
        "grad_norm": 2.413051128387451,
        "learning_rate": 2.2250589261150202e-07,
        "epoch": 0.6548079401907708,
        "step": 5080
    },
    {
        "loss": 1.6446,
        "grad_norm": 1.2893571853637695,
        "learning_rate": 2.1846800669383938e-07,
        "epoch": 0.6549368393915957,
        "step": 5081
    },
    {
        "loss": 2.2842,
        "grad_norm": 1.109602928161621,
        "learning_rate": 2.1446705538479272e-07,
        "epoch": 0.6550657385924207,
        "step": 5082
    },
    {
        "loss": 1.7367,
        "grad_norm": 1.7819243669509888,
        "learning_rate": 2.1050304016531074e-07,
        "epoch": 0.6551946377932457,
        "step": 5083
    },
    {
        "loss": 1.3251,
        "grad_norm": 1.1475907564163208,
        "learning_rate": 2.0657596250269751e-07,
        "epoch": 0.6553235369940706,
        "step": 5084
    },
    {
        "loss": 1.9462,
        "grad_norm": 1.8021544218063354,
        "learning_rate": 2.026858238505902e-07,
        "epoch": 0.6554524361948956,
        "step": 5085
    },
    {
        "loss": 1.1821,
        "grad_norm": 1.9909497499465942,
        "learning_rate": 1.988326256489259e-07,
        "epoch": 0.6555813353957205,
        "step": 5086
    },
    {
        "loss": 1.6705,
        "grad_norm": 2.028560161590576,
        "learning_rate": 1.9501636932399703e-07,
        "epoch": 0.6557102345965455,
        "step": 5087
    },
    {
        "loss": 2.0249,
        "grad_norm": 2.0287134647369385,
        "learning_rate": 1.9123705628840693e-07,
        "epoch": 0.6558391337973705,
        "step": 5088
    },
    {
        "loss": 1.7722,
        "grad_norm": 1.9945214986801147,
        "learning_rate": 1.8749468794106994e-07,
        "epoch": 0.6559680329981954,
        "step": 5089
    },
    {
        "loss": 2.062,
        "grad_norm": 1.167477011680603,
        "learning_rate": 1.8378926566727793e-07,
        "epoch": 0.6560969321990203,
        "step": 5090
    },
    {
        "loss": 1.4956,
        "grad_norm": 2.0745363235473633,
        "learning_rate": 1.8012079083856715e-07,
        "epoch": 0.6562258313998454,
        "step": 5091
    },
    {
        "loss": 1.0333,
        "grad_norm": 2.438049077987671,
        "learning_rate": 1.7648926481287353e-07,
        "epoch": 0.6563547306006703,
        "step": 5092
    },
    {
        "loss": 1.8393,
        "grad_norm": 1.9233649969100952,
        "learning_rate": 1.7289468893441074e-07,
        "epoch": 0.6564836298014952,
        "step": 5093
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.1878702640533447,
        "learning_rate": 1.6933706453372554e-07,
        "epoch": 0.6566125290023201,
        "step": 5094
    },
    {
        "loss": 2.3182,
        "grad_norm": 1.5566167831420898,
        "learning_rate": 1.658163929276979e-07,
        "epoch": 0.6567414282031452,
        "step": 5095
    },
    {
        "loss": 1.8997,
        "grad_norm": 1.4178217649459839,
        "learning_rate": 1.6233267541951868e-07,
        "epoch": 0.6568703274039701,
        "step": 5096
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.365609049797058,
        "learning_rate": 1.5888591329870083e-07,
        "epoch": 0.656999226604795,
        "step": 5097
    },
    {
        "loss": 1.1884,
        "grad_norm": 1.3724018335342407,
        "learning_rate": 1.5547610784106825e-07,
        "epoch": 0.65712812580562,
        "step": 5098
    },
    {
        "loss": 1.0945,
        "grad_norm": 1.9155769348144531,
        "learning_rate": 1.5210326030880017e-07,
        "epoch": 0.657257025006445,
        "step": 5099
    },
    {
        "loss": 1.6521,
        "grad_norm": 1.9580423831939697,
        "learning_rate": 1.4876737195035352e-07,
        "epoch": 0.6573859242072699,
        "step": 5100
    },
    {
        "loss": 1.1245,
        "grad_norm": 2.2934341430664062,
        "learning_rate": 1.454684440005405e-07,
        "epoch": 0.6575148234080949,
        "step": 5101
    },
    {
        "loss": 2.1101,
        "grad_norm": 2.100080728530884,
        "learning_rate": 1.4220647768045103e-07,
        "epoch": 0.6576437226089198,
        "step": 5102
    },
    {
        "loss": 2.5351,
        "grad_norm": 1.4895212650299072,
        "learning_rate": 1.3898147419753038e-07,
        "epoch": 0.6577726218097448,
        "step": 5103
    },
    {
        "loss": 0.8844,
        "grad_norm": 2.894432783126831,
        "learning_rate": 1.3579343474553475e-07,
        "epoch": 0.6579015210105698,
        "step": 5104
    },
    {
        "loss": 2.3669,
        "grad_norm": 2.0829288959503174,
        "learning_rate": 1.3264236050453127e-07,
        "epoch": 0.6580304202113947,
        "step": 5105
    },
    {
        "loss": 1.69,
        "grad_norm": 2.7464370727539062,
        "learning_rate": 1.2952825264089806e-07,
        "epoch": 0.6581593194122196,
        "step": 5106
    },
    {
        "loss": 1.8484,
        "grad_norm": 1.721290946006775,
        "learning_rate": 1.264511123073353e-07,
        "epoch": 0.6582882186130447,
        "step": 5107
    },
    {
        "loss": 2.1283,
        "grad_norm": 1.5513825416564941,
        "learning_rate": 1.2341094064286517e-07,
        "epoch": 0.6584171178138696,
        "step": 5108
    },
    {
        "loss": 0.7079,
        "grad_norm": 1.6677213907241821,
        "learning_rate": 1.2040773877282085e-07,
        "epoch": 0.6585460170146945,
        "step": 5109
    },
    {
        "loss": 1.1528,
        "grad_norm": 2.877713680267334,
        "learning_rate": 1.1744150780885754e-07,
        "epoch": 0.6586749162155194,
        "step": 5110
    },
    {
        "loss": 1.5955,
        "grad_norm": 2.5798933506011963,
        "learning_rate": 1.145122488489192e-07,
        "epoch": 0.6588038154163445,
        "step": 5111
    },
    {
        "loss": 1.349,
        "grad_norm": 2.5603644847869873,
        "learning_rate": 1.1161996297731625e-07,
        "epoch": 0.6589327146171694,
        "step": 5112
    },
    {
        "loss": 2.4351,
        "grad_norm": 1.2983726263046265,
        "learning_rate": 1.0876465126462565e-07,
        "epoch": 0.6590616138179943,
        "step": 5113
    },
    {
        "loss": 1.4628,
        "grad_norm": 1.878739356994629,
        "learning_rate": 1.0594631476774642e-07,
        "epoch": 0.6591905130188193,
        "step": 5114
    },
    {
        "loss": 1.6237,
        "grad_norm": 2.244640588760376,
        "learning_rate": 1.0316495452991071e-07,
        "epoch": 0.6593194122196442,
        "step": 5115
    },
    {
        "loss": 2.1097,
        "grad_norm": 2.015794515609741,
        "learning_rate": 1.0042057158065055e-07,
        "epoch": 0.6594483114204692,
        "step": 5116
    },
    {
        "loss": 1.7721,
        "grad_norm": 1.8003755807876587,
        "learning_rate": 9.771316693582e-08,
        "epoch": 0.6595772106212942,
        "step": 5117
    },
    {
        "loss": 1.4791,
        "grad_norm": 1.4782395362854004,
        "learning_rate": 9.504274159756188e-08,
        "epoch": 0.6597061098221191,
        "step": 5118
    },
    {
        "loss": 1.8304,
        "grad_norm": 1.9505736827850342,
        "learning_rate": 9.240929655435215e-08,
        "epoch": 0.659835009022944,
        "step": 5119
    },
    {
        "loss": 2.0721,
        "grad_norm": 1.4221163988113403,
        "learning_rate": 8.981283278096664e-08,
        "epoch": 0.659963908223769,
        "step": 5120
    },
    {
        "loss": 1.8465,
        "grad_norm": 1.680845022201538,
        "learning_rate": 8.72533512385254e-08,
        "epoch": 0.660092807424594,
        "step": 5121
    },
    {
        "loss": 2.1611,
        "grad_norm": 1.7721234560012817,
        "learning_rate": 8.473085287440397e-08,
        "epoch": 0.6602217066254189,
        "step": 5122
    },
    {
        "loss": 2.3109,
        "grad_norm": 1.8274503946304321,
        "learning_rate": 8.224533862232208e-08,
        "epoch": 0.6603506058262438,
        "step": 5123
    },
    {
        "loss": 2.2036,
        "grad_norm": 1.5654652118682861,
        "learning_rate": 7.97968094023216e-08,
        "epoch": 0.6604795050270689,
        "step": 5124
    },
    {
        "loss": 2.2141,
        "grad_norm": 1.432992696762085,
        "learning_rate": 7.738526612072194e-08,
        "epoch": 0.6606084042278938,
        "step": 5125
    },
    {
        "loss": 1.6547,
        "grad_norm": 2.2498645782470703,
        "learning_rate": 7.501070967016466e-08,
        "epoch": 0.6607373034287187,
        "step": 5126
    },
    {
        "loss": 1.4694,
        "grad_norm": 2.1559438705444336,
        "learning_rate": 7.267314092961331e-08,
        "epoch": 0.6608662026295437,
        "step": 5127
    },
    {
        "loss": 1.8045,
        "grad_norm": 1.6025563478469849,
        "learning_rate": 7.03725607643313e-08,
        "epoch": 0.6609951018303687,
        "step": 5128
    },
    {
        "loss": 1.7199,
        "grad_norm": 2.187727451324463,
        "learning_rate": 6.81089700258819e-08,
        "epoch": 0.6611240010311936,
        "step": 5129
    },
    {
        "loss": 1.4136,
        "grad_norm": 1.8994346857070923,
        "learning_rate": 6.588236955213934e-08,
        "epoch": 0.6612529002320185,
        "step": 5130
    },
    {
        "loss": 1.7147,
        "grad_norm": 2.1740574836730957,
        "learning_rate": 6.369276016731097e-08,
        "epoch": 0.6613817994328435,
        "step": 5131
    },
    {
        "loss": 2.5517,
        "grad_norm": 1.7366822957992554,
        "learning_rate": 6.15401426818596e-08,
        "epoch": 0.6615106986336685,
        "step": 5132
    },
    {
        "loss": 1.0685,
        "grad_norm": 2.4792208671569824,
        "learning_rate": 5.9424517892614496e-08,
        "epoch": 0.6616395978344934,
        "step": 5133
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.8057814836502075,
        "learning_rate": 5.7345886582682586e-08,
        "epoch": 0.6617684970353184,
        "step": 5134
    },
    {
        "loss": 1.8941,
        "grad_norm": 1.767401933670044,
        "learning_rate": 5.5304249521459515e-08,
        "epoch": 0.6618973962361433,
        "step": 5135
    },
    {
        "loss": 1.9889,
        "grad_norm": 1.7415319681167603,
        "learning_rate": 5.329960746468521e-08,
        "epoch": 0.6620262954369683,
        "step": 5136
    },
    {
        "loss": 1.9628,
        "grad_norm": 1.820715069770813,
        "learning_rate": 5.133196115437722e-08,
        "epoch": 0.6621551946377933,
        "step": 5137
    },
    {
        "loss": 2.1085,
        "grad_norm": 1.9174925088882446,
        "learning_rate": 4.9401311318864054e-08,
        "epoch": 0.6622840938386182,
        "step": 5138
    },
    {
        "loss": 2.1781,
        "grad_norm": 1.9741708040237427,
        "learning_rate": 4.750765867280738e-08,
        "epoch": 0.6624129930394431,
        "step": 5139
    },
    {
        "loss": 1.9634,
        "grad_norm": 1.6003905534744263,
        "learning_rate": 4.565100391713539e-08,
        "epoch": 0.6625418922402682,
        "step": 5140
    },
    {
        "loss": 1.5968,
        "grad_norm": 2.1984310150146484,
        "learning_rate": 4.383134773909836e-08,
        "epoch": 0.6626707914410931,
        "step": 5141
    },
    {
        "loss": 2.3317,
        "grad_norm": 1.5908201932907104,
        "learning_rate": 4.2048690812246385e-08,
        "epoch": 0.662799690641918,
        "step": 5142
    },
    {
        "loss": 1.8736,
        "grad_norm": 2.1683928966522217,
        "learning_rate": 4.0303033796451615e-08,
        "epoch": 0.6629285898427429,
        "step": 5143
    },
    {
        "loss": 2.3925,
        "grad_norm": 1.1184935569763184,
        "learning_rate": 3.8594377337863866e-08,
        "epoch": 0.663057489043568,
        "step": 5144
    },
    {
        "loss": 1.7707,
        "grad_norm": 1.9606776237487793,
        "learning_rate": 3.6922722068966074e-08,
        "epoch": 0.6631863882443929,
        "step": 5145
    },
    {
        "loss": 2.0528,
        "grad_norm": 2.255995273590088,
        "learning_rate": 3.528806860851885e-08,
        "epoch": 0.6633152874452178,
        "step": 5146
    },
    {
        "loss": 2.0414,
        "grad_norm": 1.8320261240005493,
        "learning_rate": 3.3690417561593746e-08,
        "epoch": 0.6634441866460428,
        "step": 5147
    },
    {
        "loss": 1.73,
        "grad_norm": 2.350053548812866,
        "learning_rate": 3.212976951957325e-08,
        "epoch": 0.6635730858468677,
        "step": 5148
    },
    {
        "loss": 1.3472,
        "grad_norm": 2.3683478832244873,
        "learning_rate": 3.060612506013971e-08,
        "epoch": 0.6637019850476927,
        "step": 5149
    },
    {
        "loss": 0.6912,
        "grad_norm": 1.3918166160583496,
        "learning_rate": 2.911948474727533e-08,
        "epoch": 0.6638308842485177,
        "step": 5150
    },
    {
        "loss": 2.3744,
        "grad_norm": 1.9515742063522339,
        "learning_rate": 2.766984913127324e-08,
        "epoch": 0.6639597834493426,
        "step": 5151
    },
    {
        "loss": 1.6846,
        "grad_norm": 2.1093547344207764,
        "learning_rate": 2.625721874871534e-08,
        "epoch": 0.6640886826501675,
        "step": 5152
    },
    {
        "loss": 1.6067,
        "grad_norm": 2.51590633392334,
        "learning_rate": 2.4881594122494467e-08,
        "epoch": 0.6642175818509926,
        "step": 5153
    },
    {
        "loss": 1.4975,
        "grad_norm": 2.490907907485962,
        "learning_rate": 2.354297576180331e-08,
        "epoch": 0.6643464810518175,
        "step": 5154
    },
    {
        "loss": 1.5283,
        "grad_norm": 5.265530586242676,
        "learning_rate": 2.2241364162145506e-08,
        "epoch": 0.6644753802526424,
        "step": 5155
    },
    {
        "loss": 1.2769,
        "grad_norm": 2.3115596771240234,
        "learning_rate": 2.097675980531344e-08,
        "epoch": 0.6646042794534673,
        "step": 5156
    },
    {
        "loss": 1.8336,
        "grad_norm": 2.003574848175049,
        "learning_rate": 1.9749163159410445e-08,
        "epoch": 0.6647331786542924,
        "step": 5157
    },
    {
        "loss": 2.3427,
        "grad_norm": 1.6469624042510986,
        "learning_rate": 1.85585746788286e-08,
        "epoch": 0.6648620778551173,
        "step": 5158
    },
    {
        "loss": 1.4083,
        "grad_norm": 1.5490479469299316,
        "learning_rate": 1.7404994804270936e-08,
        "epoch": 0.6649909770559422,
        "step": 5159
    },
    {
        "loss": 1.8278,
        "grad_norm": 2.10425066947937,
        "learning_rate": 1.628842396275143e-08,
        "epoch": 0.6651198762567672,
        "step": 5160
    },
    {
        "loss": 1.9083,
        "grad_norm": 1.860999584197998,
        "learning_rate": 1.5208862567561712e-08,
        "epoch": 0.6652487754575922,
        "step": 5161
    },
    {
        "loss": 1.0796,
        "grad_norm": 2.364253520965576,
        "learning_rate": 1.4166311018315447e-08,
        "epoch": 0.6653776746584171,
        "step": 5162
    },
    {
        "loss": 0.6319,
        "grad_norm": 1.229235053062439,
        "learning_rate": 1.3160769700915066e-08,
        "epoch": 0.6655065738592421,
        "step": 5163
    },
    {
        "loss": 2.586,
        "grad_norm": 1.502341628074646,
        "learning_rate": 1.2192238987573933e-08,
        "epoch": 0.665635473060067,
        "step": 5164
    },
    {
        "loss": 1.6448,
        "grad_norm": 2.0633299350738525,
        "learning_rate": 1.1260719236783068e-08,
        "epoch": 0.665764372260892,
        "step": 5165
    },
    {
        "loss": 2.1429,
        "grad_norm": 1.4821230173110962,
        "learning_rate": 1.0366210793355535e-08,
        "epoch": 0.665893271461717,
        "step": 5166
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.5302376747131348,
        "learning_rate": 9.50871398840425e-09,
        "epoch": 0.6660221706625419,
        "step": 5167
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.5454434156417847,
        "learning_rate": 8.68822913931977e-09,
        "epoch": 0.6661510698633668,
        "step": 5168
    },
    {
        "loss": 2.2095,
        "grad_norm": 1.9070874452590942,
        "learning_rate": 7.904756549825809e-09,
        "epoch": 0.6662799690641918,
        "step": 5169
    },
    {
        "loss": 0.6999,
        "grad_norm": 1.9580353498458862,
        "learning_rate": 7.158296509923723e-09,
        "epoch": 0.6664088682650168,
        "step": 5170
    },
    {
        "loss": 1.8764,
        "grad_norm": 2.8228983879089355,
        "learning_rate": 6.4488492959147166e-09,
        "epoch": 0.6665377674658417,
        "step": 5171
    },
    {
        "loss": 2.2526,
        "grad_norm": 1.8462095260620117,
        "learning_rate": 5.7764151703998405e-09,
        "epoch": 0.6666666666666666,
        "step": 5172
    },
    {
        "loss": 1.5027,
        "grad_norm": 1.830456018447876,
        "learning_rate": 5.1409943823022e-09,
        "epoch": 0.6667955658674917,
        "step": 5173
    },
    {
        "loss": 1.4306,
        "grad_norm": 1.862607479095459,
        "learning_rate": 4.5425871668114406e-09,
        "epoch": 0.6669244650683166,
        "step": 5174
    },
    {
        "loss": 1.8272,
        "grad_norm": 1.6803982257843018,
        "learning_rate": 3.981193745428158e-09,
        "epoch": 0.6670533642691415,
        "step": 5175
    },
    {
        "loss": 2.5013,
        "grad_norm": 1.2891870737075806,
        "learning_rate": 3.4568143259638973e-09,
        "epoch": 0.6671822634699665,
        "step": 5176
    },
    {
        "loss": 2.1554,
        "grad_norm": 2.0554211139678955,
        "learning_rate": 2.9694491025078487e-09,
        "epoch": 0.6673111626707915,
        "step": 5177
    },
    {
        "loss": 0.9342,
        "grad_norm": 1.8437126874923706,
        "learning_rate": 2.519098255482355e-09,
        "epoch": 0.6674400618716164,
        "step": 5178
    },
    {
        "loss": 2.1224,
        "grad_norm": 3.0646066665649414,
        "learning_rate": 2.105761951565199e-09,
        "epoch": 0.6675689610724413,
        "step": 5179
    },
    {
        "loss": 1.7132,
        "grad_norm": 2.348116159439087,
        "learning_rate": 1.729440343767319e-09,
        "epoch": 0.6676978602732663,
        "step": 5180
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.6721594333648682,
        "learning_rate": 1.3901335713772944e-09,
        "epoch": 0.6678267594740913,
        "step": 5181
    },
    {
        "loss": 1.9005,
        "grad_norm": 3.223703145980835,
        "learning_rate": 1.0878417600057589e-09,
        "epoch": 0.6679556586749162,
        "step": 5182
    },
    {
        "loss": 1.8876,
        "grad_norm": 2.220992088317871,
        "learning_rate": 8.225650215298864e-10,
        "epoch": 0.6680845578757412,
        "step": 5183
    },
    {
        "loss": 2.3762,
        "grad_norm": 1.1089353561401367,
        "learning_rate": 5.943034541489034e-10,
        "epoch": 0.6682134570765661,
        "step": 5184
    },
    {
        "loss": 2.438,
        "grad_norm": 1.9415054321289062,
        "learning_rate": 4.030571423618845e-10,
        "epoch": 0.668342356277391,
        "step": 5185
    },
    {
        "loss": 1.0557,
        "grad_norm": 2.468737840652466,
        "learning_rate": 2.4882615694554746e-10,
        "epoch": 0.6684712554782161,
        "step": 5186
    },
    {
        "loss": 1.3333,
        "grad_norm": 1.6253974437713623,
        "learning_rate": 1.316105550097646e-10,
        "epoch": 0.668600154679041,
        "step": 5187
    },
    {
        "loss": 2.1555,
        "grad_norm": 1.50972580909729,
        "learning_rate": 5.1410379919847316e-11,
        "epoch": 0.6687290538798659,
        "step": 5188
    },
    {
        "loss": 1.9619,
        "grad_norm": 1.8954660892486572,
        "learning_rate": 8.225661374261506e-12,
        "epoch": 0.6688579530806908,
        "step": 5189
    },
    {
        "loss": 1.8988,
        "grad_norm": 1.2373875379562378,
        "learning_rate": 0.00019999999794358464,
        "epoch": 0.6689868522815159,
        "step": 5190
    },
    {
        "loss": 1.7412,
        "grad_norm": 1.8217848539352417,
        "learning_rate": 0.00019999996709735583,
        "epoch": 0.6691157514823408,
        "step": 5191
    },
    {
        "loss": 2.1554,
        "grad_norm": 1.4825519323349,
        "learning_rate": 0.00019999989923566368,
        "epoch": 0.6692446506831657,
        "step": 5192
    },
    {
        "loss": 1.9623,
        "grad_norm": 1.7363382577896118,
        "learning_rate": 0.00019999979435853318,
        "epoch": 0.6693735498839907,
        "step": 5193
    },
    {
        "loss": 1.9656,
        "grad_norm": 1.2217332124710083,
        "learning_rate": 0.00019999965246600328,
        "epoch": 0.6695024490848157,
        "step": 5194
    },
    {
        "loss": 0.698,
        "grad_norm": 2.176466226577759,
        "learning_rate": 0.0001999994735581264,
        "epoch": 0.6696313482856406,
        "step": 5195
    },
    {
        "loss": 2.3364,
        "grad_norm": 1.382645606994629,
        "learning_rate": 0.00019999925763496885,
        "epoch": 0.6697602474864656,
        "step": 5196
    },
    {
        "loss": 2.073,
        "grad_norm": 2.454045534133911,
        "learning_rate": 0.0001999990046966105,
        "epoch": 0.6698891466872905,
        "step": 5197
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.015267014503479,
        "learning_rate": 0.00019999871474314502,
        "epoch": 0.6700180458881155,
        "step": 5198
    },
    {
        "loss": 1.3507,
        "grad_norm": 2.51007342338562,
        "learning_rate": 0.00019999838777467967,
        "epoch": 0.6701469450889405,
        "step": 5199
    },
    {
        "loss": 1.511,
        "grad_norm": 2.953561782836914,
        "learning_rate": 0.00019999802379133555,
        "epoch": 0.6702758442897654,
        "step": 5200
    },
    {
        "loss": 1.5452,
        "grad_norm": 2.45332407951355,
        "learning_rate": 0.00019999762279324736,
        "epoch": 0.6704047434905903,
        "step": 5201
    },
    {
        "loss": 1.0691,
        "grad_norm": 2.3876452445983887,
        "learning_rate": 0.0001999971847805635,
        "epoch": 0.6705336426914154,
        "step": 5202
    },
    {
        "loss": 1.8337,
        "grad_norm": 1.5102816820144653,
        "learning_rate": 0.00019999670975344617,
        "epoch": 0.6706625418922403,
        "step": 5203
    },
    {
        "loss": 2.2298,
        "grad_norm": 1.7540185451507568,
        "learning_rate": 0.00019999619771207114,
        "epoch": 0.6707914410930652,
        "step": 5204
    },
    {
        "loss": 1.7231,
        "grad_norm": 2.3481650352478027,
        "learning_rate": 0.000199995648656628,
        "epoch": 0.6709203402938901,
        "step": 5205
    },
    {
        "loss": 2.4214,
        "grad_norm": 2.7075116634368896,
        "learning_rate": 0.00019999506258731991,
        "epoch": 0.6710492394947152,
        "step": 5206
    },
    {
        "loss": 1.4677,
        "grad_norm": 1.8917584419250488,
        "learning_rate": 0.00019999443950436392,
        "epoch": 0.6711781386955401,
        "step": 5207
    },
    {
        "loss": 1.4249,
        "grad_norm": 2.0748167037963867,
        "learning_rate": 0.00019999377940799055,
        "epoch": 0.671307037896365,
        "step": 5208
    },
    {
        "loss": 2.0893,
        "grad_norm": 2.3886425495147705,
        "learning_rate": 0.00019999308229844422,
        "epoch": 0.67143593709719,
        "step": 5209
    },
    {
        "loss": 1.4734,
        "grad_norm": 1.0969223976135254,
        "learning_rate": 0.00019999234817598294,
        "epoch": 0.671564836298015,
        "step": 5210
    },
    {
        "loss": 1.6411,
        "grad_norm": 1.7137480974197388,
        "learning_rate": 0.0001999915770408784,
        "epoch": 0.6716937354988399,
        "step": 5211
    },
    {
        "loss": 1.6661,
        "grad_norm": 2.9029743671417236,
        "learning_rate": 0.00019999076889341613,
        "epoch": 0.6718226346996649,
        "step": 5212
    },
    {
        "loss": 2.2818,
        "grad_norm": 1.544249415397644,
        "learning_rate": 0.0001999899237338952,
        "epoch": 0.6719515339004898,
        "step": 5213
    },
    {
        "loss": 1.7413,
        "grad_norm": 1.8495529890060425,
        "learning_rate": 0.00019998904156262847,
        "epoch": 0.6720804331013148,
        "step": 5214
    },
    {
        "loss": 1.2447,
        "grad_norm": 3.069666624069214,
        "learning_rate": 0.0001999881223799425,
        "epoch": 0.6722093323021398,
        "step": 5215
    },
    {
        "loss": 2.3996,
        "grad_norm": 1.7437231540679932,
        "learning_rate": 0.00019998716618617755,
        "epoch": 0.6723382315029647,
        "step": 5216
    },
    {
        "loss": 2.0768,
        "grad_norm": 1.5416165590286255,
        "learning_rate": 0.00019998617298168747,
        "epoch": 0.6724671307037896,
        "step": 5217
    },
    {
        "loss": 1.2716,
        "grad_norm": 1.9613782167434692,
        "learning_rate": 0.00019998514276683998,
        "epoch": 0.6725960299046146,
        "step": 5218
    },
    {
        "loss": 2.358,
        "grad_norm": 1.5564472675323486,
        "learning_rate": 0.00019998407554201636,
        "epoch": 0.6727249291054396,
        "step": 5219
    },
    {
        "loss": 1.9931,
        "grad_norm": 1.962722897529602,
        "learning_rate": 0.00019998297130761173,
        "epoch": 0.6728538283062645,
        "step": 5220
    },
    {
        "loss": 2.096,
        "grad_norm": 1.446593165397644,
        "learning_rate": 0.00019998183006403471,
        "epoch": 0.6729827275070894,
        "step": 5221
    },
    {
        "loss": 2.2937,
        "grad_norm": 1.5817815065383911,
        "learning_rate": 0.00019998065181170787,
        "epoch": 0.6731116267079144,
        "step": 5222
    },
    {
        "loss": 2.1746,
        "grad_norm": 2.2313334941864014,
        "learning_rate": 0.00019997943655106725,
        "epoch": 0.6732405259087394,
        "step": 5223
    },
    {
        "loss": 2.2684,
        "grad_norm": 1.3824554681777954,
        "learning_rate": 0.00019997818428256275,
        "epoch": 0.6733694251095643,
        "step": 5224
    },
    {
        "loss": 2.0551,
        "grad_norm": 1.4189414978027344,
        "learning_rate": 0.00019997689500665783,
        "epoch": 0.6734983243103893,
        "step": 5225
    },
    {
        "loss": 2.2783,
        "grad_norm": 1.855666160583496,
        "learning_rate": 0.0001999755687238298,
        "epoch": 0.6736272235112142,
        "step": 5226
    },
    {
        "loss": 1.9814,
        "grad_norm": 1.646847128868103,
        "learning_rate": 0.0001999742054345695,
        "epoch": 0.6737561227120392,
        "step": 5227
    },
    {
        "loss": 1.9607,
        "grad_norm": 1.0863287448883057,
        "learning_rate": 0.00019997280513938163,
        "epoch": 0.6738850219128641,
        "step": 5228
    },
    {
        "loss": 1.8136,
        "grad_norm": 2.398205041885376,
        "learning_rate": 0.00019997136783878448,
        "epoch": 0.6740139211136891,
        "step": 5229
    },
    {
        "loss": 1.52,
        "grad_norm": 1.9808093309402466,
        "learning_rate": 0.00019996989353331012,
        "epoch": 0.674142820314514,
        "step": 5230
    },
    {
        "loss": 1.8698,
        "grad_norm": 1.874275803565979,
        "learning_rate": 0.00019996838222350422,
        "epoch": 0.674271719515339,
        "step": 5231
    },
    {
        "loss": 1.2648,
        "grad_norm": 2.1939854621887207,
        "learning_rate": 0.00019996683390992624,
        "epoch": 0.674400618716164,
        "step": 5232
    },
    {
        "loss": 2.0925,
        "grad_norm": 2.0217130184173584,
        "learning_rate": 0.0001999652485931492,
        "epoch": 0.6745295179169889,
        "step": 5233
    },
    {
        "loss": 1.6454,
        "grad_norm": 2.4376065731048584,
        "learning_rate": 0.0001999636262737601,
        "epoch": 0.6746584171178138,
        "step": 5234
    },
    {
        "loss": 2.3282,
        "grad_norm": 2.063430070877075,
        "learning_rate": 0.00019996196695235924,
        "epoch": 0.6747873163186389,
        "step": 5235
    },
    {
        "loss": 1.2619,
        "grad_norm": 2.6449460983276367,
        "learning_rate": 0.00019996027062956095,
        "epoch": 0.6749162155194638,
        "step": 5236
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.8046270608901978,
        "learning_rate": 0.00019995853730599313,
        "epoch": 0.6750451147202887,
        "step": 5237
    },
    {
        "loss": 2.2717,
        "grad_norm": 1.7514827251434326,
        "learning_rate": 0.0001999567669822973,
        "epoch": 0.6751740139211136,
        "step": 5238
    },
    {
        "loss": 1.5948,
        "grad_norm": 2.1866202354431152,
        "learning_rate": 0.00019995495965912885,
        "epoch": 0.6753029131219387,
        "step": 5239
    },
    {
        "loss": 1.6478,
        "grad_norm": 2.264380693435669,
        "learning_rate": 0.00019995311533715668,
        "epoch": 0.6754318123227636,
        "step": 5240
    },
    {
        "loss": 0.4577,
        "grad_norm": 1.9658546447753906,
        "learning_rate": 0.00019995123401706354,
        "epoch": 0.6755607115235885,
        "step": 5241
    },
    {
        "loss": 1.7598,
        "grad_norm": 1.9478204250335693,
        "learning_rate": 0.0001999493156995458,
        "epoch": 0.6756896107244135,
        "step": 5242
    },
    {
        "loss": 2.4465,
        "grad_norm": 1.2344987392425537,
        "learning_rate": 0.0001999473603853135,
        "epoch": 0.6758185099252385,
        "step": 5243
    },
    {
        "loss": 1.1831,
        "grad_norm": 1.901990294456482,
        "learning_rate": 0.00019994536807509046,
        "epoch": 0.6759474091260634,
        "step": 5244
    },
    {
        "loss": 1.8023,
        "grad_norm": 2.0323610305786133,
        "learning_rate": 0.0001999433387696141,
        "epoch": 0.6760763083268884,
        "step": 5245
    },
    {
        "loss": 2.1553,
        "grad_norm": 2.3526244163513184,
        "learning_rate": 0.0001999412724696356,
        "epoch": 0.6762052075277133,
        "step": 5246
    },
    {
        "loss": 2.0122,
        "grad_norm": 1.8130172491073608,
        "learning_rate": 0.00019993916917591977,
        "epoch": 0.6763341067285383,
        "step": 5247
    },
    {
        "loss": 1.9782,
        "grad_norm": 2.1292526721954346,
        "learning_rate": 0.00019993702888924518,
        "epoch": 0.6764630059293633,
        "step": 5248
    },
    {
        "loss": 2.6051,
        "grad_norm": 1.6972352266311646,
        "learning_rate": 0.0001999348516104041,
        "epoch": 0.6765919051301882,
        "step": 5249
    },
    {
        "loss": 2.5042,
        "grad_norm": 1.1793804168701172,
        "learning_rate": 0.00019993263734020245,
        "epoch": 0.6767208043310131,
        "step": 5250
    },
    {
        "loss": 1.5212,
        "grad_norm": 2.0077285766601562,
        "learning_rate": 0.00019993038607945982,
        "epoch": 0.6768497035318382,
        "step": 5251
    },
    {
        "loss": 2.2308,
        "grad_norm": 2.8297653198242188,
        "learning_rate": 0.00019992809782900953,
        "epoch": 0.6769786027326631,
        "step": 5252
    },
    {
        "loss": 2.2246,
        "grad_norm": 1.5513237714767456,
        "learning_rate": 0.0001999257725896986,
        "epoch": 0.677107501933488,
        "step": 5253
    },
    {
        "loss": 2.5128,
        "grad_norm": 1.3605343103408813,
        "learning_rate": 0.0001999234103623877,
        "epoch": 0.6772364011343129,
        "step": 5254
    },
    {
        "loss": 2.2852,
        "grad_norm": 1.2984516620635986,
        "learning_rate": 0.0001999210111479513,
        "epoch": 0.677365300335138,
        "step": 5255
    },
    {
        "loss": 0.7422,
        "grad_norm": 2.5618603229522705,
        "learning_rate": 0.0001999185749472774,
        "epoch": 0.6774941995359629,
        "step": 5256
    },
    {
        "loss": 1.8717,
        "grad_norm": 1.5130070447921753,
        "learning_rate": 0.0001999161017612678,
        "epoch": 0.6776230987367878,
        "step": 5257
    },
    {
        "loss": 2.1511,
        "grad_norm": 2.1525259017944336,
        "learning_rate": 0.00019991359159083794,
        "epoch": 0.6777519979376128,
        "step": 5258
    },
    {
        "loss": 2.108,
        "grad_norm": 1.7262399196624756,
        "learning_rate": 0.00019991104443691703,
        "epoch": 0.6778808971384377,
        "step": 5259
    },
    {
        "loss": 1.5715,
        "grad_norm": 2.2922112941741943,
        "learning_rate": 0.00019990846030044782,
        "epoch": 0.6780097963392627,
        "step": 5260
    },
    {
        "loss": 2.3813,
        "grad_norm": 1.6711629629135132,
        "learning_rate": 0.00019990583918238694,
        "epoch": 0.6781386955400877,
        "step": 5261
    },
    {
        "loss": 1.1975,
        "grad_norm": 5.536501407623291,
        "learning_rate": 0.00019990318108370456,
        "epoch": 0.6782675947409126,
        "step": 5262
    },
    {
        "loss": 0.9913,
        "grad_norm": 2.4064583778381348,
        "learning_rate": 0.00019990048600538457,
        "epoch": 0.6783964939417375,
        "step": 5263
    },
    {
        "loss": 1.9106,
        "grad_norm": 1.497443437576294,
        "learning_rate": 0.0001998977539484246,
        "epoch": 0.6785253931425625,
        "step": 5264
    },
    {
        "loss": 2.6837,
        "grad_norm": 1.248626708984375,
        "learning_rate": 0.0001998949849138359,
        "epoch": 0.6786542923433875,
        "step": 5265
    },
    {
        "loss": 1.6512,
        "grad_norm": 2.3223021030426025,
        "learning_rate": 0.00019989217890264348,
        "epoch": 0.6787831915442124,
        "step": 5266
    },
    {
        "loss": 1.8089,
        "grad_norm": 2.0394418239593506,
        "learning_rate": 0.00019988933591588595,
        "epoch": 0.6789120907450373,
        "step": 5267
    },
    {
        "loss": 1.9649,
        "grad_norm": 2.2762370109558105,
        "learning_rate": 0.00019988645595461567,
        "epoch": 0.6790409899458624,
        "step": 5268
    },
    {
        "loss": 2.0225,
        "grad_norm": 2.416433334350586,
        "learning_rate": 0.00019988353901989877,
        "epoch": 0.6791698891466873,
        "step": 5269
    },
    {
        "loss": 1.6438,
        "grad_norm": 2.3418664932250977,
        "learning_rate": 0.0001998805851128148,
        "epoch": 0.6792987883475122,
        "step": 5270
    },
    {
        "loss": 2.3315,
        "grad_norm": 1.6550886631011963,
        "learning_rate": 0.0001998775942344573,
        "epoch": 0.6794276875483372,
        "step": 5271
    },
    {
        "loss": 1.4641,
        "grad_norm": 2.863614559173584,
        "learning_rate": 0.00019987456638593325,
        "epoch": 0.6795565867491622,
        "step": 5272
    },
    {
        "loss": 1.2105,
        "grad_norm": 2.7249271869659424,
        "learning_rate": 0.00019987150156836351,
        "epoch": 0.6796854859499871,
        "step": 5273
    },
    {
        "loss": 2.1522,
        "grad_norm": 1.5588176250457764,
        "learning_rate": 0.0001998683997828825,
        "epoch": 0.679814385150812,
        "step": 5274
    },
    {
        "loss": 2.3822,
        "grad_norm": 2.1643640995025635,
        "learning_rate": 0.00019986526103063837,
        "epoch": 0.679943284351637,
        "step": 5275
    },
    {
        "loss": 2.2954,
        "grad_norm": 1.4454426765441895,
        "learning_rate": 0.00019986208531279291,
        "epoch": 0.680072183552462,
        "step": 5276
    },
    {
        "loss": 2.1577,
        "grad_norm": 2.200554847717285,
        "learning_rate": 0.00019985887263052169,
        "epoch": 0.680201082753287,
        "step": 5277
    },
    {
        "loss": 2.1565,
        "grad_norm": 1.4306174516677856,
        "learning_rate": 0.00019985562298501384,
        "epoch": 0.6803299819541119,
        "step": 5278
    },
    {
        "loss": 2.1176,
        "grad_norm": 1.5306282043457031,
        "learning_rate": 0.00019985233637747225,
        "epoch": 0.6804588811549368,
        "step": 5279
    },
    {
        "loss": 1.6527,
        "grad_norm": 2.057373046875,
        "learning_rate": 0.0001998490128091135,
        "epoch": 0.6805877803557618,
        "step": 5280
    },
    {
        "loss": 2.0086,
        "grad_norm": 1.8755587339401245,
        "learning_rate": 0.00019984565228116778,
        "epoch": 0.6807166795565868,
        "step": 5281
    },
    {
        "loss": 0.7447,
        "grad_norm": 1.927973747253418,
        "learning_rate": 0.00019984225479487906,
        "epoch": 0.6808455787574117,
        "step": 5282
    },
    {
        "loss": 1.0178,
        "grad_norm": 1.944680094718933,
        "learning_rate": 0.0001998388203515049,
        "epoch": 0.6809744779582366,
        "step": 5283
    },
    {
        "loss": 2.3153,
        "grad_norm": 2.120164155960083,
        "learning_rate": 0.00019983534895231655,
        "epoch": 0.6811033771590617,
        "step": 5284
    },
    {
        "loss": 2.7022,
        "grad_norm": 1.6222666501998901,
        "learning_rate": 0.000199831840598599,
        "epoch": 0.6812322763598866,
        "step": 5285
    },
    {
        "loss": 2.3249,
        "grad_norm": 2.676206350326538,
        "learning_rate": 0.0001998282952916509,
        "epoch": 0.6813611755607115,
        "step": 5286
    },
    {
        "loss": 1.6559,
        "grad_norm": 1.7051200866699219,
        "learning_rate": 0.0001998247130327845,
        "epoch": 0.6814900747615364,
        "step": 5287
    },
    {
        "loss": 1.9847,
        "grad_norm": 1.9569084644317627,
        "learning_rate": 0.0001998210938233259,
        "epoch": 0.6816189739623615,
        "step": 5288
    },
    {
        "loss": 1.9474,
        "grad_norm": 2.166689157485962,
        "learning_rate": 0.00019981743766461465,
        "epoch": 0.6817478731631864,
        "step": 5289
    },
    {
        "loss": 1.037,
        "grad_norm": 3.150167465209961,
        "learning_rate": 0.00019981374455800416,
        "epoch": 0.6818767723640113,
        "step": 5290
    },
    {
        "loss": 2.1065,
        "grad_norm": 1.3860267400741577,
        "learning_rate": 0.00019981001450486145,
        "epoch": 0.6820056715648363,
        "step": 5291
    },
    {
        "loss": 2.0704,
        "grad_norm": 1.8050811290740967,
        "learning_rate": 0.00019980624750656718,
        "epoch": 0.6821345707656613,
        "step": 5292
    },
    {
        "loss": 1.3122,
        "grad_norm": 2.3930411338806152,
        "learning_rate": 0.00019980244356451575,
        "epoch": 0.6822634699664862,
        "step": 5293
    },
    {
        "loss": 2.2601,
        "grad_norm": 1.7525283098220825,
        "learning_rate": 0.00019979860268011518,
        "epoch": 0.6823923691673112,
        "step": 5294
    },
    {
        "loss": 1.5225,
        "grad_norm": 1.8543707132339478,
        "learning_rate": 0.00019979472485478724,
        "epoch": 0.6825212683681361,
        "step": 5295
    },
    {
        "loss": 1.5257,
        "grad_norm": 1.769687533378601,
        "learning_rate": 0.00019979081008996728,
        "epoch": 0.682650167568961,
        "step": 5296
    },
    {
        "loss": 1.8396,
        "grad_norm": 2.3647985458374023,
        "learning_rate": 0.0001997868583871044,
        "epoch": 0.6827790667697861,
        "step": 5297
    },
    {
        "loss": 1.3669,
        "grad_norm": 2.045776605606079,
        "learning_rate": 0.00019978286974766134,
        "epoch": 0.682907965970611,
        "step": 5298
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.693066120147705,
        "learning_rate": 0.0001997788441731145,
        "epoch": 0.6830368651714359,
        "step": 5299
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.2425414323806763,
        "learning_rate": 0.00019977478166495393,
        "epoch": 0.6831657643722608,
        "step": 5300
    },
    {
        "loss": 1.4917,
        "grad_norm": 3.478890895843506,
        "learning_rate": 0.00019977068222468348,
        "epoch": 0.6832946635730859,
        "step": 5301
    },
    {
        "loss": 1.4639,
        "grad_norm": 2.552577018737793,
        "learning_rate": 0.0001997665458538205,
        "epoch": 0.6834235627739108,
        "step": 5302
    },
    {
        "loss": 0.8955,
        "grad_norm": 2.5293405055999756,
        "learning_rate": 0.00019976237255389607,
        "epoch": 0.6835524619747357,
        "step": 5303
    },
    {
        "loss": 1.8864,
        "grad_norm": 2.170930862426758,
        "learning_rate": 0.00019975816232645503,
        "epoch": 0.6836813611755607,
        "step": 5304
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.48083233833313,
        "learning_rate": 0.00019975391517305577,
        "epoch": 0.6838102603763857,
        "step": 5305
    },
    {
        "loss": 1.2272,
        "grad_norm": 3.0466713905334473,
        "learning_rate": 0.00019974963109527043,
        "epoch": 0.6839391595772106,
        "step": 5306
    },
    {
        "loss": 1.2967,
        "grad_norm": 2.3100409507751465,
        "learning_rate": 0.00019974531009468475,
        "epoch": 0.6840680587780356,
        "step": 5307
    },
    {
        "loss": 1.9486,
        "grad_norm": 2.454699754714966,
        "learning_rate": 0.00019974095217289815,
        "epoch": 0.6841969579788605,
        "step": 5308
    },
    {
        "loss": 1.7241,
        "grad_norm": 1.8607141971588135,
        "learning_rate": 0.00019973655733152379,
        "epoch": 0.6843258571796855,
        "step": 5309
    },
    {
        "loss": 1.7335,
        "grad_norm": 2.6344542503356934,
        "learning_rate": 0.0001997321255721884,
        "epoch": 0.6844547563805105,
        "step": 5310
    },
    {
        "loss": 2.4477,
        "grad_norm": 2.7477259635925293,
        "learning_rate": 0.00019972765689653244,
        "epoch": 0.6845836555813354,
        "step": 5311
    },
    {
        "loss": 1.4882,
        "grad_norm": 2.0973331928253174,
        "learning_rate": 0.00019972315130621003,
        "epoch": 0.6847125547821603,
        "step": 5312
    },
    {
        "loss": 2.0747,
        "grad_norm": 1.9995758533477783,
        "learning_rate": 0.00019971860880288887,
        "epoch": 0.6848414539829853,
        "step": 5313
    },
    {
        "loss": 2.6301,
        "grad_norm": 2.1011414527893066,
        "learning_rate": 0.0001997140293882504,
        "epoch": 0.6849703531838103,
        "step": 5314
    },
    {
        "loss": 0.9134,
        "grad_norm": 3.252976655960083,
        "learning_rate": 0.00019970941306398981,
        "epoch": 0.6850992523846352,
        "step": 5315
    },
    {
        "loss": 2.0775,
        "grad_norm": 2.8164846897125244,
        "learning_rate": 0.00019970475983181572,
        "epoch": 0.6852281515854601,
        "step": 5316
    },
    {
        "loss": 1.87,
        "grad_norm": 2.1837940216064453,
        "learning_rate": 0.00019970006969345067,
        "epoch": 0.6853570507862852,
        "step": 5317
    },
    {
        "loss": 2.6259,
        "grad_norm": 1.7931280136108398,
        "learning_rate": 0.00019969534265063064,
        "epoch": 0.6854859499871101,
        "step": 5318
    },
    {
        "loss": 1.9702,
        "grad_norm": 2.206005334854126,
        "learning_rate": 0.00019969057870510538,
        "epoch": 0.685614849187935,
        "step": 5319
    },
    {
        "loss": 2.4923,
        "grad_norm": 1.0153400897979736,
        "learning_rate": 0.00019968577785863832,
        "epoch": 0.68574374838876,
        "step": 5320
    },
    {
        "loss": 1.9773,
        "grad_norm": 1.5503461360931396,
        "learning_rate": 0.00019968094011300655,
        "epoch": 0.685872647589585,
        "step": 5321
    },
    {
        "loss": 1.981,
        "grad_norm": 1.394054651260376,
        "learning_rate": 0.00019967606547000074,
        "epoch": 0.6860015467904099,
        "step": 5322
    },
    {
        "loss": 1.6726,
        "grad_norm": 2.3469393253326416,
        "learning_rate": 0.0001996711539314252,
        "epoch": 0.6861304459912348,
        "step": 5323
    },
    {
        "loss": 2.0625,
        "grad_norm": 2.409514904022217,
        "learning_rate": 0.00019966620549909808,
        "epoch": 0.6862593451920598,
        "step": 5324
    },
    {
        "loss": 1.9594,
        "grad_norm": 1.983986735343933,
        "learning_rate": 0.000199661220174851,
        "epoch": 0.6863882443928848,
        "step": 5325
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.7424235343933105,
        "learning_rate": 0.00019965619796052928,
        "epoch": 0.6865171435937097,
        "step": 5326
    },
    {
        "loss": 1.2955,
        "grad_norm": 2.510124921798706,
        "learning_rate": 0.000199651138857992,
        "epoch": 0.6866460427945347,
        "step": 5327
    },
    {
        "loss": 1.9215,
        "grad_norm": 1.6073447465896606,
        "learning_rate": 0.00019964604286911168,
        "epoch": 0.6867749419953596,
        "step": 5328
    },
    {
        "loss": 2.0106,
        "grad_norm": 2.2451443672180176,
        "learning_rate": 0.00019964090999577478,
        "epoch": 0.6869038411961846,
        "step": 5329
    },
    {
        "loss": 2.3803,
        "grad_norm": 3.2185957431793213,
        "learning_rate": 0.00019963574023988115,
        "epoch": 0.6870327403970096,
        "step": 5330
    },
    {
        "loss": 1.9292,
        "grad_norm": 2.456970453262329,
        "learning_rate": 0.00019963053360334444,
        "epoch": 0.6871616395978345,
        "step": 5331
    },
    {
        "loss": 1.7915,
        "grad_norm": 3.1061670780181885,
        "learning_rate": 0.00019962529008809185,
        "epoch": 0.6872905387986594,
        "step": 5332
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.8172738552093506,
        "learning_rate": 0.00019962000969606442,
        "epoch": 0.6874194379994844,
        "step": 5333
    },
    {
        "loss": 1.924,
        "grad_norm": 2.4547371864318848,
        "learning_rate": 0.00019961469242921656,
        "epoch": 0.6875483372003094,
        "step": 5334
    },
    {
        "loss": 1.9925,
        "grad_norm": 2.2958638668060303,
        "learning_rate": 0.0001996093382895166,
        "epoch": 0.6876772364011343,
        "step": 5335
    },
    {
        "loss": 1.8039,
        "grad_norm": 2.0993783473968506,
        "learning_rate": 0.00019960394727894633,
        "epoch": 0.6878061356019592,
        "step": 5336
    },
    {
        "loss": 1.595,
        "grad_norm": 1.9520349502563477,
        "learning_rate": 0.00019959851939950132,
        "epoch": 0.6879350348027842,
        "step": 5337
    },
    {
        "loss": 1.7001,
        "grad_norm": 1.7826043367385864,
        "learning_rate": 0.0001995930546531907,
        "epoch": 0.6880639340036092,
        "step": 5338
    },
    {
        "loss": 2.1205,
        "grad_norm": 2.3164608478546143,
        "learning_rate": 0.00019958755304203723,
        "epoch": 0.6881928332044341,
        "step": 5339
    },
    {
        "loss": 1.7153,
        "grad_norm": 2.3409318923950195,
        "learning_rate": 0.0001995820145680774,
        "epoch": 0.6883217324052591,
        "step": 5340
    },
    {
        "loss": 1.9833,
        "grad_norm": 1.944405436515808,
        "learning_rate": 0.00019957643923336127,
        "epoch": 0.688450631606084,
        "step": 5341
    },
    {
        "loss": 1.608,
        "grad_norm": 2.572732448577881,
        "learning_rate": 0.00019957082703995265,
        "epoch": 0.688579530806909,
        "step": 5342
    },
    {
        "loss": 1.021,
        "grad_norm": 2.2746968269348145,
        "learning_rate": 0.00019956517798992883,
        "epoch": 0.688708430007734,
        "step": 5343
    },
    {
        "loss": 1.7712,
        "grad_norm": 2.453510046005249,
        "learning_rate": 0.00019955949208538088,
        "epoch": 0.6888373292085589,
        "step": 5344
    },
    {
        "loss": 1.9089,
        "grad_norm": 1.6307581663131714,
        "learning_rate": 0.00019955376932841344,
        "epoch": 0.6889662284093838,
        "step": 5345
    },
    {
        "loss": 2.377,
        "grad_norm": 1.4143697023391724,
        "learning_rate": 0.00019954800972114484,
        "epoch": 0.6890951276102089,
        "step": 5346
    },
    {
        "loss": 1.1169,
        "grad_norm": 2.3607370853424072,
        "learning_rate": 0.00019954221326570702,
        "epoch": 0.6892240268110338,
        "step": 5347
    },
    {
        "loss": 2.2404,
        "grad_norm": 2.0353403091430664,
        "learning_rate": 0.00019953637996424558,
        "epoch": 0.6893529260118587,
        "step": 5348
    },
    {
        "loss": 2.2184,
        "grad_norm": 1.8389297723770142,
        "learning_rate": 0.00019953050981891973,
        "epoch": 0.6894818252126836,
        "step": 5349
    },
    {
        "loss": 1.0875,
        "grad_norm": 2.1006667613983154,
        "learning_rate": 0.0001995246028319023,
        "epoch": 0.6896107244135087,
        "step": 5350
    },
    {
        "loss": 1.3698,
        "grad_norm": 2.283181667327881,
        "learning_rate": 0.00019951865900537987,
        "epoch": 0.6897396236143336,
        "step": 5351
    },
    {
        "loss": 1.9531,
        "grad_norm": 1.6388956308364868,
        "learning_rate": 0.0001995126783415525,
        "epoch": 0.6898685228151585,
        "step": 5352
    },
    {
        "loss": 1.4798,
        "grad_norm": 3.024989604949951,
        "learning_rate": 0.00019950666084263396,
        "epoch": 0.6899974220159835,
        "step": 5353
    },
    {
        "loss": 2.0067,
        "grad_norm": 1.3843411207199097,
        "learning_rate": 0.0001995006065108517,
        "epoch": 0.6901263212168085,
        "step": 5354
    },
    {
        "loss": 1.5764,
        "grad_norm": 2.7253530025482178,
        "learning_rate": 0.00019949451534844674,
        "epoch": 0.6902552204176334,
        "step": 5355
    },
    {
        "loss": 1.6613,
        "grad_norm": 1.9042539596557617,
        "learning_rate": 0.0001994883873576738,
        "epoch": 0.6903841196184584,
        "step": 5356
    },
    {
        "loss": 2.4394,
        "grad_norm": 1.3993067741394043,
        "learning_rate": 0.00019948222254080107,
        "epoch": 0.6905130188192833,
        "step": 5357
    },
    {
        "loss": 1.915,
        "grad_norm": 2.1093761920928955,
        "learning_rate": 0.0001994760209001106,
        "epoch": 0.6906419180201083,
        "step": 5358
    },
    {
        "loss": 2.1487,
        "grad_norm": 1.6672601699829102,
        "learning_rate": 0.0001994697824378979,
        "epoch": 0.6907708172209333,
        "step": 5359
    },
    {
        "loss": 1.7204,
        "grad_norm": 3.0275213718414307,
        "learning_rate": 0.00019946350715647218,
        "epoch": 0.6908997164217582,
        "step": 5360
    },
    {
        "loss": 1.6686,
        "grad_norm": 2.8898329734802246,
        "learning_rate": 0.00019945719505815626,
        "epoch": 0.6910286156225831,
        "step": 5361
    },
    {
        "loss": 1.8814,
        "grad_norm": 4.9291672706604,
        "learning_rate": 0.0001994508461452866,
        "epoch": 0.6911575148234081,
        "step": 5362
    },
    {
        "loss": 1.4223,
        "grad_norm": 2.690650224685669,
        "learning_rate": 0.00019944446042021328,
        "epoch": 0.6912864140242331,
        "step": 5363
    },
    {
        "loss": 1.6783,
        "grad_norm": 1.9949427843093872,
        "learning_rate": 0.00019943803788529998,
        "epoch": 0.691415313225058,
        "step": 5364
    },
    {
        "loss": 2.0918,
        "grad_norm": 1.7209668159484863,
        "learning_rate": 0.0001994315785429241,
        "epoch": 0.6915442124258829,
        "step": 5365
    },
    {
        "loss": 1.1122,
        "grad_norm": 2.217304229736328,
        "learning_rate": 0.00019942508239547652,
        "epoch": 0.6916731116267079,
        "step": 5366
    },
    {
        "loss": 1.7579,
        "grad_norm": 2.3100197315216064,
        "learning_rate": 0.00019941854944536186,
        "epoch": 0.6918020108275329,
        "step": 5367
    },
    {
        "loss": 2.3804,
        "grad_norm": 1.8137660026550293,
        "learning_rate": 0.0001994119796949983,
        "epoch": 0.6919309100283578,
        "step": 5368
    },
    {
        "loss": 1.3704,
        "grad_norm": 1.9871745109558105,
        "learning_rate": 0.00019940537314681772,
        "epoch": 0.6920598092291828,
        "step": 5369
    },
    {
        "loss": 1.4814,
        "grad_norm": 2.1713669300079346,
        "learning_rate": 0.0001993987298032655,
        "epoch": 0.6921887084300077,
        "step": 5370
    },
    {
        "loss": 2.5602,
        "grad_norm": 2.0285205841064453,
        "learning_rate": 0.00019939204966680073,
        "epoch": 0.6923176076308327,
        "step": 5371
    },
    {
        "loss": 2.2127,
        "grad_norm": 1.3927371501922607,
        "learning_rate": 0.0001993853327398961,
        "epoch": 0.6924465068316576,
        "step": 5372
    },
    {
        "loss": 1.2109,
        "grad_norm": 2.513091564178467,
        "learning_rate": 0.0001993785790250379,
        "epoch": 0.6925754060324826,
        "step": 5373
    },
    {
        "loss": 0.4728,
        "grad_norm": 2.294853687286377,
        "learning_rate": 0.00019937178852472606,
        "epoch": 0.6927043052333075,
        "step": 5374
    },
    {
        "loss": 2.0547,
        "grad_norm": 4.138964653015137,
        "learning_rate": 0.00019936496124147414,
        "epoch": 0.6928332044341325,
        "step": 5375
    },
    {
        "loss": 2.6227,
        "grad_norm": 1.5964927673339844,
        "learning_rate": 0.0001993580971778092,
        "epoch": 0.6929621036349575,
        "step": 5376
    },
    {
        "loss": 1.9611,
        "grad_norm": 1.9955992698669434,
        "learning_rate": 0.0001993511963362721,
        "epoch": 0.6930910028357824,
        "step": 5377
    },
    {
        "loss": 2.2069,
        "grad_norm": 2.615034818649292,
        "learning_rate": 0.0001993442587194172,
        "epoch": 0.6932199020366073,
        "step": 5378
    },
    {
        "loss": 2.054,
        "grad_norm": 1.9708733558654785,
        "learning_rate": 0.00019933728432981247,
        "epoch": 0.6933488012374324,
        "step": 5379
    },
    {
        "loss": 2.0201,
        "grad_norm": 2.560856342315674,
        "learning_rate": 0.00019933027317003954,
        "epoch": 0.6934777004382573,
        "step": 5380
    },
    {
        "loss": 1.9737,
        "grad_norm": 1.7396395206451416,
        "learning_rate": 0.00019932322524269359,
        "epoch": 0.6936065996390822,
        "step": 5381
    },
    {
        "loss": 2.7531,
        "grad_norm": 2.9142343997955322,
        "learning_rate": 0.00019931614055038348,
        "epoch": 0.6937354988399071,
        "step": 5382
    },
    {
        "loss": 1.8063,
        "grad_norm": 2.0943777561187744,
        "learning_rate": 0.00019930901909573161,
        "epoch": 0.6938643980407322,
        "step": 5383
    },
    {
        "loss": 2.5451,
        "grad_norm": 1.2657297849655151,
        "learning_rate": 0.000199301860881374,
        "epoch": 0.6939932972415571,
        "step": 5384
    },
    {
        "loss": 1.834,
        "grad_norm": 1.7074189186096191,
        "learning_rate": 0.0001992946659099604,
        "epoch": 0.694122196442382,
        "step": 5385
    },
    {
        "loss": 0.5624,
        "grad_norm": 2.701646327972412,
        "learning_rate": 0.00019928743418415397,
        "epoch": 0.694251095643207,
        "step": 5386
    },
    {
        "loss": 2.019,
        "grad_norm": 1.3214542865753174,
        "learning_rate": 0.00019928016570663158,
        "epoch": 0.694379994844032,
        "step": 5387
    },
    {
        "loss": 1.7411,
        "grad_norm": 2.4191839694976807,
        "learning_rate": 0.0001992728604800837,
        "epoch": 0.6945088940448569,
        "step": 5388
    },
    {
        "loss": 1.5945,
        "grad_norm": 2.2640504837036133,
        "learning_rate": 0.00019926551850721442,
        "epoch": 0.6946377932456819,
        "step": 5389
    },
    {
        "loss": 1.7333,
        "grad_norm": 1.5617289543151855,
        "learning_rate": 0.00019925813979074138,
        "epoch": 0.6947666924465068,
        "step": 5390
    },
    {
        "loss": 1.4952,
        "grad_norm": 2.443291664123535,
        "learning_rate": 0.00019925072433339583,
        "epoch": 0.6948955916473318,
        "step": 5391
    },
    {
        "loss": 2.0558,
        "grad_norm": 1.5820860862731934,
        "learning_rate": 0.00019924327213792267,
        "epoch": 0.6950244908481568,
        "step": 5392
    },
    {
        "loss": 1.3083,
        "grad_norm": 2.459718704223633,
        "learning_rate": 0.00019923578320708035,
        "epoch": 0.6951533900489817,
        "step": 5393
    },
    {
        "loss": 1.4542,
        "grad_norm": 3.653902530670166,
        "learning_rate": 0.00019922825754364095,
        "epoch": 0.6952822892498066,
        "step": 5394
    },
    {
        "loss": 1.8036,
        "grad_norm": 3.0672342777252197,
        "learning_rate": 0.0001992206951503901,
        "epoch": 0.6954111884506317,
        "step": 5395
    },
    {
        "loss": 1.8693,
        "grad_norm": 2.899610996246338,
        "learning_rate": 0.0001992130960301271,
        "epoch": 0.6955400876514566,
        "step": 5396
    },
    {
        "loss": 2.1889,
        "grad_norm": 1.6399766206741333,
        "learning_rate": 0.00019920546018566476,
        "epoch": 0.6956689868522815,
        "step": 5397
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.6351665258407593,
        "learning_rate": 0.00019919778761982952,
        "epoch": 0.6957978860531064,
        "step": 5398
    },
    {
        "loss": 1.6346,
        "grad_norm": 3.217770576477051,
        "learning_rate": 0.00019919007833546146,
        "epoch": 0.6959267852539315,
        "step": 5399
    },
    {
        "loss": 1.3325,
        "grad_norm": 2.944448709487915,
        "learning_rate": 0.00019918233233541418,
        "epoch": 0.6960556844547564,
        "step": 5400
    },
    {
        "loss": 1.3603,
        "grad_norm": 2.754552125930786,
        "learning_rate": 0.0001991745496225549,
        "epoch": 0.6961845836555813,
        "step": 5401
    },
    {
        "loss": 1.6333,
        "grad_norm": 2.706084728240967,
        "learning_rate": 0.00019916673019976445,
        "epoch": 0.6963134828564063,
        "step": 5402
    },
    {
        "loss": 1.6067,
        "grad_norm": 3.0615086555480957,
        "learning_rate": 0.00019915887406993715,
        "epoch": 0.6964423820572312,
        "step": 5403
    },
    {
        "loss": 1.5852,
        "grad_norm": 1.3679615259170532,
        "learning_rate": 0.00019915098123598107,
        "epoch": 0.6965712812580562,
        "step": 5404
    },
    {
        "loss": 1.4901,
        "grad_norm": 2.028810977935791,
        "learning_rate": 0.00019914305170081774,
        "epoch": 0.6967001804588812,
        "step": 5405
    },
    {
        "loss": 2.1513,
        "grad_norm": 1.9049221277236938,
        "learning_rate": 0.00019913508546738235,
        "epoch": 0.6968290796597061,
        "step": 5406
    },
    {
        "loss": 2.1324,
        "grad_norm": 2.0142762660980225,
        "learning_rate": 0.0001991270825386236,
        "epoch": 0.696957978860531,
        "step": 5407
    },
    {
        "loss": 2.1428,
        "grad_norm": 1.3230700492858887,
        "learning_rate": 0.00019911904291750382,
        "epoch": 0.697086878061356,
        "step": 5408
    },
    {
        "loss": 1.5666,
        "grad_norm": 1.9889562129974365,
        "learning_rate": 0.00019911096660699893,
        "epoch": 0.697215777262181,
        "step": 5409
    },
    {
        "loss": 1.7449,
        "grad_norm": 1.8350530862808228,
        "learning_rate": 0.00019910285361009842,
        "epoch": 0.6973446764630059,
        "step": 5410
    },
    {
        "loss": 1.1574,
        "grad_norm": 2.0129055976867676,
        "learning_rate": 0.00019909470392980532,
        "epoch": 0.6974735756638308,
        "step": 5411
    },
    {
        "loss": 1.2699,
        "grad_norm": 2.4359683990478516,
        "learning_rate": 0.00019908651756913632,
        "epoch": 0.6976024748646559,
        "step": 5412
    },
    {
        "loss": 2.1355,
        "grad_norm": 1.661341667175293,
        "learning_rate": 0.00019907829453112157,
        "epoch": 0.6977313740654808,
        "step": 5413
    },
    {
        "loss": 0.7551,
        "grad_norm": 2.4481260776519775,
        "learning_rate": 0.00019907003481880496,
        "epoch": 0.6978602732663057,
        "step": 5414
    },
    {
        "loss": 1.5734,
        "grad_norm": 2.753680467605591,
        "learning_rate": 0.00019906173843524375,
        "epoch": 0.6979891724671307,
        "step": 5415
    },
    {
        "loss": 1.6835,
        "grad_norm": 5.199987411499023,
        "learning_rate": 0.00019905340538350897,
        "epoch": 0.6981180716679557,
        "step": 5416
    },
    {
        "loss": 1.0179,
        "grad_norm": 3.0076394081115723,
        "learning_rate": 0.00019904503566668514,
        "epoch": 0.6982469708687806,
        "step": 5417
    },
    {
        "loss": 1.6214,
        "grad_norm": 2.228937864303589,
        "learning_rate": 0.0001990366292878703,
        "epoch": 0.6983758700696056,
        "step": 5418
    },
    {
        "loss": 1.4121,
        "grad_norm": 2.4245786666870117,
        "learning_rate": 0.00019902818625017616,
        "epoch": 0.6985047692704305,
        "step": 5419
    },
    {
        "loss": 1.1233,
        "grad_norm": 2.1360456943511963,
        "learning_rate": 0.0001990197065567279,
        "epoch": 0.6986336684712555,
        "step": 5420
    },
    {
        "loss": 2.0937,
        "grad_norm": 2.4558799266815186,
        "learning_rate": 0.00019901119021066438,
        "epoch": 0.6987625676720804,
        "step": 5421
    },
    {
        "loss": 1.6175,
        "grad_norm": 1.3178279399871826,
        "learning_rate": 0.0001990026372151379,
        "epoch": 0.6988914668729054,
        "step": 5422
    },
    {
        "loss": 2.6668,
        "grad_norm": 1.7582037448883057,
        "learning_rate": 0.00019899404757331445,
        "epoch": 0.6990203660737303,
        "step": 5423
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.65721595287323,
        "learning_rate": 0.0001989854212883735,
        "epoch": 0.6991492652745553,
        "step": 5424
    },
    {
        "loss": 1.9996,
        "grad_norm": 1.718239188194275,
        "learning_rate": 0.0001989767583635081,
        "epoch": 0.6992781644753803,
        "step": 5425
    },
    {
        "loss": 1.0759,
        "grad_norm": 2.769137382507324,
        "learning_rate": 0.0001989680588019249,
        "epoch": 0.6994070636762052,
        "step": 5426
    },
    {
        "loss": 1.9137,
        "grad_norm": 1.872527003288269,
        "learning_rate": 0.00019895932260684406,
        "epoch": 0.6995359628770301,
        "step": 5427
    },
    {
        "loss": 2.0834,
        "grad_norm": 1.6112006902694702,
        "learning_rate": 0.0001989505497814993,
        "epoch": 0.6996648620778552,
        "step": 5428
    },
    {
        "loss": 2.471,
        "grad_norm": 2.6810221672058105,
        "learning_rate": 0.00019894174032913803,
        "epoch": 0.6997937612786801,
        "step": 5429
    },
    {
        "loss": 2.4246,
        "grad_norm": 1.209566593170166,
        "learning_rate": 0.000198932894253021,
        "epoch": 0.699922660479505,
        "step": 5430
    },
    {
        "loss": 1.6665,
        "grad_norm": 2.6718642711639404,
        "learning_rate": 0.00019892401155642263,
        "epoch": 0.70005155968033,
        "step": 5431
    },
    {
        "loss": 1.8213,
        "grad_norm": 2.476710319519043,
        "learning_rate": 0.00019891509224263096,
        "epoch": 0.700180458881155,
        "step": 5432
    },
    {
        "loss": 1.6951,
        "grad_norm": 2.9317970275878906,
        "learning_rate": 0.0001989061363149475,
        "epoch": 0.7003093580819799,
        "step": 5433
    },
    {
        "loss": 2.0969,
        "grad_norm": 2.622623920440674,
        "learning_rate": 0.0001988971437766873,
        "epoch": 0.7004382572828048,
        "step": 5434
    },
    {
        "loss": 1.7563,
        "grad_norm": 1.9987919330596924,
        "learning_rate": 0.000198888114631179,
        "epoch": 0.7005671564836298,
        "step": 5435
    },
    {
        "loss": 2.0296,
        "grad_norm": 1.7465211153030396,
        "learning_rate": 0.00019887904888176477,
        "epoch": 0.7006960556844548,
        "step": 5436
    },
    {
        "loss": 1.5923,
        "grad_norm": 2.1864614486694336,
        "learning_rate": 0.00019886994653180035,
        "epoch": 0.7008249548852797,
        "step": 5437
    },
    {
        "loss": 2.3808,
        "grad_norm": 2.0313339233398438,
        "learning_rate": 0.00019886080758465505,
        "epoch": 0.7009538540861047,
        "step": 5438
    },
    {
        "loss": 2.0746,
        "grad_norm": 1.6969538927078247,
        "learning_rate": 0.00019885163204371164,
        "epoch": 0.7010827532869296,
        "step": 5439
    },
    {
        "loss": 1.9108,
        "grad_norm": 1.4771814346313477,
        "learning_rate": 0.00019884241991236653,
        "epoch": 0.7012116524877545,
        "step": 5440
    },
    {
        "loss": 1.0836,
        "grad_norm": 3.139472484588623,
        "learning_rate": 0.0001988331711940296,
        "epoch": 0.7013405516885796,
        "step": 5441
    },
    {
        "loss": 2.0299,
        "grad_norm": 2.327031373977661,
        "learning_rate": 0.00019882388589212438,
        "epoch": 0.7014694508894045,
        "step": 5442
    },
    {
        "loss": 2.5503,
        "grad_norm": 2.311018705368042,
        "learning_rate": 0.00019881456401008778,
        "epoch": 0.7015983500902294,
        "step": 5443
    },
    {
        "loss": 1.7001,
        "grad_norm": 2.060948133468628,
        "learning_rate": 0.0001988052055513704,
        "epoch": 0.7017272492910543,
        "step": 5444
    },
    {
        "loss": 1.6264,
        "grad_norm": 1.9667527675628662,
        "learning_rate": 0.00019879581051943625,
        "epoch": 0.7018561484918794,
        "step": 5445
    },
    {
        "loss": 2.0987,
        "grad_norm": 1.8506278991699219,
        "learning_rate": 0.00019878637891776303,
        "epoch": 0.7019850476927043,
        "step": 5446
    },
    {
        "loss": 2.1007,
        "grad_norm": 2.2471256256103516,
        "learning_rate": 0.00019877691074984184,
        "epoch": 0.7021139468935292,
        "step": 5447
    },
    {
        "loss": 2.2684,
        "grad_norm": 1.9192440509796143,
        "learning_rate": 0.0001987674060191774,
        "epoch": 0.7022428460943542,
        "step": 5448
    },
    {
        "loss": 2.4703,
        "grad_norm": 2.0722720623016357,
        "learning_rate": 0.00019875786472928788,
        "epoch": 0.7023717452951792,
        "step": 5449
    },
    {
        "loss": 2.4868,
        "grad_norm": 1.47196626663208,
        "learning_rate": 0.00019874828688370508,
        "epoch": 0.7025006444960041,
        "step": 5450
    },
    {
        "loss": 2.1531,
        "grad_norm": 2.540980100631714,
        "learning_rate": 0.00019873867248597428,
        "epoch": 0.7026295436968291,
        "step": 5451
    },
    {
        "loss": 1.6312,
        "grad_norm": 2.478107213973999,
        "learning_rate": 0.00019872902153965426,
        "epoch": 0.702758442897654,
        "step": 5452
    },
    {
        "loss": 1.4453,
        "grad_norm": 2.511850118637085,
        "learning_rate": 0.00019871933404831736,
        "epoch": 0.702887342098479,
        "step": 5453
    },
    {
        "loss": 2.1389,
        "grad_norm": 1.5684471130371094,
        "learning_rate": 0.00019870961001554956,
        "epoch": 0.703016241299304,
        "step": 5454
    },
    {
        "loss": 1.2866,
        "grad_norm": 3.4806196689605713,
        "learning_rate": 0.00019869984944495013,
        "epoch": 0.7031451405001289,
        "step": 5455
    },
    {
        "loss": 2.16,
        "grad_norm": 2.083073377609253,
        "learning_rate": 0.00019869005234013205,
        "epoch": 0.7032740397009538,
        "step": 5456
    },
    {
        "loss": 2.2427,
        "grad_norm": 1.180043339729309,
        "learning_rate": 0.00019868021870472172,
        "epoch": 0.7034029389017789,
        "step": 5457
    },
    {
        "loss": 2.3163,
        "grad_norm": 1.3231295347213745,
        "learning_rate": 0.0001986703485423592,
        "epoch": 0.7035318381026038,
        "step": 5458
    },
    {
        "loss": 2.2235,
        "grad_norm": 1.7563732862472534,
        "learning_rate": 0.00019866044185669785,
        "epoch": 0.7036607373034287,
        "step": 5459
    },
    {
        "loss": 1.712,
        "grad_norm": 2.4190495014190674,
        "learning_rate": 0.00019865049865140479,
        "epoch": 0.7037896365042536,
        "step": 5460
    },
    {
        "loss": 2.3464,
        "grad_norm": 1.9004663228988647,
        "learning_rate": 0.0001986405189301605,
        "epoch": 0.7039185357050787,
        "step": 5461
    },
    {
        "loss": 1.3913,
        "grad_norm": 2.483044147491455,
        "learning_rate": 0.000198630502696659,
        "epoch": 0.7040474349059036,
        "step": 5462
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.6150448322296143,
        "learning_rate": 0.00019862044995460788,
        "epoch": 0.7041763341067285,
        "step": 5463
    },
    {
        "loss": 1.7917,
        "grad_norm": 2.6520190238952637,
        "learning_rate": 0.00019861036070772817,
        "epoch": 0.7043052333075535,
        "step": 5464
    },
    {
        "loss": 1.9278,
        "grad_norm": 2.6215085983276367,
        "learning_rate": 0.00019860023495975454,
        "epoch": 0.7044341325083785,
        "step": 5465
    },
    {
        "loss": 2.3159,
        "grad_norm": 2.1598451137542725,
        "learning_rate": 0.00019859007271443496,
        "epoch": 0.7045630317092034,
        "step": 5466
    },
    {
        "loss": 1.1367,
        "grad_norm": 2.24379825592041,
        "learning_rate": 0.0001985798739755311,
        "epoch": 0.7046919309100284,
        "step": 5467
    },
    {
        "loss": 1.2447,
        "grad_norm": 2.442318916320801,
        "learning_rate": 0.0001985696387468181,
        "epoch": 0.7048208301108533,
        "step": 5468
    },
    {
        "loss": 2.3272,
        "grad_norm": 2.1958444118499756,
        "learning_rate": 0.00019855936703208453,
        "epoch": 0.7049497293116783,
        "step": 5469
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.6380090713500977,
        "learning_rate": 0.00019854905883513255,
        "epoch": 0.7050786285125032,
        "step": 5470
    },
    {
        "loss": 1.6553,
        "grad_norm": 2.377497434616089,
        "learning_rate": 0.00019853871415977779,
        "epoch": 0.7052075277133282,
        "step": 5471
    },
    {
        "loss": 1.8747,
        "grad_norm": 2.065178394317627,
        "learning_rate": 0.0001985283330098493,
        "epoch": 0.7053364269141531,
        "step": 5472
    },
    {
        "loss": 1.6758,
        "grad_norm": 1.7556124925613403,
        "learning_rate": 0.0001985179153891898,
        "epoch": 0.7054653261149781,
        "step": 5473
    },
    {
        "loss": 2.3676,
        "grad_norm": 2.1360905170440674,
        "learning_rate": 0.0001985074613016554,
        "epoch": 0.7055942253158031,
        "step": 5474
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.8387027978897095,
        "learning_rate": 0.00019849697075111574,
        "epoch": 0.705723124516628,
        "step": 5475
    },
    {
        "loss": 1.8027,
        "grad_norm": 1.9827402830123901,
        "learning_rate": 0.0001984864437414539,
        "epoch": 0.7058520237174529,
        "step": 5476
    },
    {
        "loss": 2.1619,
        "grad_norm": 1.579009771347046,
        "learning_rate": 0.00019847588027656655,
        "epoch": 0.7059809229182779,
        "step": 5477
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.558015823364258,
        "learning_rate": 0.00019846528036036377,
        "epoch": 0.7061098221191029,
        "step": 5478
    },
    {
        "loss": 1.6864,
        "grad_norm": 2.763911247253418,
        "learning_rate": 0.0001984546439967692,
        "epoch": 0.7062387213199278,
        "step": 5479
    },
    {
        "loss": 1.7649,
        "grad_norm": 2.2029612064361572,
        "learning_rate": 0.00019844397118971995,
        "epoch": 0.7063676205207527,
        "step": 5480
    },
    {
        "loss": 1.2892,
        "grad_norm": 2.217665910720825,
        "learning_rate": 0.00019843326194316657,
        "epoch": 0.7064965197215777,
        "step": 5481
    },
    {
        "loss": 1.4759,
        "grad_norm": 2.1571121215820312,
        "learning_rate": 0.00019842251626107316,
        "epoch": 0.7066254189224027,
        "step": 5482
    },
    {
        "loss": 1.9823,
        "grad_norm": 3.0515565872192383,
        "learning_rate": 0.0001984117341474173,
        "epoch": 0.7067543181232276,
        "step": 5483
    },
    {
        "loss": 2.4435,
        "grad_norm": 1.6255100965499878,
        "learning_rate": 0.00019840091560619,
        "epoch": 0.7068832173240526,
        "step": 5484
    },
    {
        "loss": 2.2223,
        "grad_norm": 2.46024227142334,
        "learning_rate": 0.00019839006064139585,
        "epoch": 0.7070121165248775,
        "step": 5485
    },
    {
        "loss": 2.5795,
        "grad_norm": 1.5024300813674927,
        "learning_rate": 0.0001983791692570528,
        "epoch": 0.7071410157257025,
        "step": 5486
    },
    {
        "loss": 1.9526,
        "grad_norm": 2.3336269855499268,
        "learning_rate": 0.00019836824145719244,
        "epoch": 0.7072699149265275,
        "step": 5487
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.7066519260406494,
        "learning_rate": 0.00019835727724585968,
        "epoch": 0.7073988141273524,
        "step": 5488
    },
    {
        "loss": 1.6212,
        "grad_norm": 2.195200204849243,
        "learning_rate": 0.00019834627662711297,
        "epoch": 0.7075277133281773,
        "step": 5489
    },
    {
        "loss": 1.6188,
        "grad_norm": 2.231985092163086,
        "learning_rate": 0.00019833523960502425,
        "epoch": 0.7076566125290024,
        "step": 5490
    },
    {
        "loss": 0.8879,
        "grad_norm": 2.4437410831451416,
        "learning_rate": 0.00019832416618367893,
        "epoch": 0.7077855117298273,
        "step": 5491
    },
    {
        "loss": 2.2639,
        "grad_norm": 2.438605785369873,
        "learning_rate": 0.0001983130563671759,
        "epoch": 0.7079144109306522,
        "step": 5492
    },
    {
        "loss": 1.4711,
        "grad_norm": 2.2930052280426025,
        "learning_rate": 0.00019830191015962753,
        "epoch": 0.7080433101314771,
        "step": 5493
    },
    {
        "loss": 1.6901,
        "grad_norm": 1.9837018251419067,
        "learning_rate": 0.0001982907275651596,
        "epoch": 0.7081722093323022,
        "step": 5494
    },
    {
        "loss": 1.1856,
        "grad_norm": 1.838621735572815,
        "learning_rate": 0.0001982795085879114,
        "epoch": 0.7083011085331271,
        "step": 5495
    },
    {
        "loss": 1.6164,
        "grad_norm": 2.344829797744751,
        "learning_rate": 0.00019826825323203572,
        "epoch": 0.708430007733952,
        "step": 5496
    },
    {
        "loss": 1.8615,
        "grad_norm": 1.3949053287506104,
        "learning_rate": 0.0001982569615016988,
        "epoch": 0.708558906934777,
        "step": 5497
    },
    {
        "loss": 1.6452,
        "grad_norm": 2.3666319847106934,
        "learning_rate": 0.00019824563340108023,
        "epoch": 0.708687806135602,
        "step": 5498
    },
    {
        "loss": 1.9572,
        "grad_norm": 2.0624029636383057,
        "learning_rate": 0.00019823426893437327,
        "epoch": 0.7088167053364269,
        "step": 5499
    },
    {
        "loss": 2.1006,
        "grad_norm": 1.9942229986190796,
        "learning_rate": 0.0001982228681057845,
        "epoch": 0.7089456045372519,
        "step": 5500
    },
    {
        "loss": 1.7614,
        "grad_norm": 1.5713919401168823,
        "learning_rate": 0.00019821143091953395,
        "epoch": 0.7090745037380768,
        "step": 5501
    },
    {
        "loss": 2.2288,
        "grad_norm": 2.6007370948791504,
        "learning_rate": 0.00019819995737985517,
        "epoch": 0.7092034029389018,
        "step": 5502
    },
    {
        "loss": 2.6771,
        "grad_norm": 2.0310895442962646,
        "learning_rate": 0.00019818844749099518,
        "epoch": 0.7093323021397268,
        "step": 5503
    },
    {
        "loss": 2.1014,
        "grad_norm": 1.7121193408966064,
        "learning_rate": 0.0001981769012572144,
        "epoch": 0.7094612013405517,
        "step": 5504
    },
    {
        "loss": 2.0037,
        "grad_norm": 1.7691197395324707,
        "learning_rate": 0.00019816531868278666,
        "epoch": 0.7095901005413766,
        "step": 5505
    },
    {
        "loss": 1.8227,
        "grad_norm": 3.616682767868042,
        "learning_rate": 0.00019815369977199945,
        "epoch": 0.7097189997422017,
        "step": 5506
    },
    {
        "loss": 2.6797,
        "grad_norm": 1.5389819145202637,
        "learning_rate": 0.00019814204452915342,
        "epoch": 0.7098478989430266,
        "step": 5507
    },
    {
        "loss": 2.3448,
        "grad_norm": 1.956589698791504,
        "learning_rate": 0.00019813035295856288,
        "epoch": 0.7099767981438515,
        "step": 5508
    },
    {
        "loss": 1.6166,
        "grad_norm": 2.948326349258423,
        "learning_rate": 0.0001981186250645555,
        "epoch": 0.7101056973446764,
        "step": 5509
    },
    {
        "loss": 2.1277,
        "grad_norm": 2.2219278812408447,
        "learning_rate": 0.00019810686085147247,
        "epoch": 0.7102345965455015,
        "step": 5510
    },
    {
        "loss": 1.9957,
        "grad_norm": 1.6978402137756348,
        "learning_rate": 0.00019809506032366828,
        "epoch": 0.7103634957463264,
        "step": 5511
    },
    {
        "loss": 2.141,
        "grad_norm": 1.978333830833435,
        "learning_rate": 0.00019808322348551102,
        "epoch": 0.7104923949471513,
        "step": 5512
    },
    {
        "loss": 1.5637,
        "grad_norm": 2.508805513381958,
        "learning_rate": 0.00019807135034138212,
        "epoch": 0.7106212941479763,
        "step": 5513
    },
    {
        "loss": 1.705,
        "grad_norm": 1.929107427597046,
        "learning_rate": 0.0001980594408956765,
        "epoch": 0.7107501933488012,
        "step": 5514
    },
    {
        "loss": 1.9629,
        "grad_norm": 2.906867504119873,
        "learning_rate": 0.00019804749515280246,
        "epoch": 0.7108790925496262,
        "step": 5515
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.5040082931518555,
        "learning_rate": 0.00019803551311718182,
        "epoch": 0.7110079917504512,
        "step": 5516
    },
    {
        "loss": 1.4356,
        "grad_norm": 2.6527247428894043,
        "learning_rate": 0.00019802349479324977,
        "epoch": 0.7111368909512761,
        "step": 5517
    },
    {
        "loss": 2.3524,
        "grad_norm": 1.4320284128189087,
        "learning_rate": 0.00019801144018545495,
        "epoch": 0.711265790152101,
        "step": 5518
    },
    {
        "loss": 1.1979,
        "grad_norm": 2.415153741836548,
        "learning_rate": 0.00019799934929825942,
        "epoch": 0.711394689352926,
        "step": 5519
    },
    {
        "loss": 1.5642,
        "grad_norm": 2.2960832118988037,
        "learning_rate": 0.0001979872221361387,
        "epoch": 0.711523588553751,
        "step": 5520
    },
    {
        "loss": 2.0719,
        "grad_norm": 2.5404138565063477,
        "learning_rate": 0.0001979750587035817,
        "epoch": 0.7116524877545759,
        "step": 5521
    },
    {
        "loss": 1.4283,
        "grad_norm": 1.3828450441360474,
        "learning_rate": 0.00019796285900509079,
        "epoch": 0.7117813869554008,
        "step": 5522
    },
    {
        "loss": 1.5879,
        "grad_norm": 1.8930569887161255,
        "learning_rate": 0.00019795062304518176,
        "epoch": 0.7119102861562259,
        "step": 5523
    },
    {
        "loss": 1.696,
        "grad_norm": 2.1740782260894775,
        "learning_rate": 0.00019793835082838372,
        "epoch": 0.7120391853570508,
        "step": 5524
    },
    {
        "loss": 1.7272,
        "grad_norm": 2.0799570083618164,
        "learning_rate": 0.00019792604235923937,
        "epoch": 0.7121680845578757,
        "step": 5525
    },
    {
        "loss": 1.9732,
        "grad_norm": 1.2843316793441772,
        "learning_rate": 0.00019791369764230477,
        "epoch": 0.7122969837587007,
        "step": 5526
    },
    {
        "loss": 1.9149,
        "grad_norm": 3.023162603378296,
        "learning_rate": 0.00019790131668214928,
        "epoch": 0.7124258829595257,
        "step": 5527
    },
    {
        "loss": 1.9425,
        "grad_norm": 2.9976730346679688,
        "learning_rate": 0.00019788889948335587,
        "epoch": 0.7125547821603506,
        "step": 5528
    },
    {
        "loss": 2.0377,
        "grad_norm": 1.739692211151123,
        "learning_rate": 0.00019787644605052078,
        "epoch": 0.7126836813611755,
        "step": 5529
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.762934684753418,
        "learning_rate": 0.0001978639563882537,
        "epoch": 0.7128125805620005,
        "step": 5530
    },
    {
        "loss": 2.2567,
        "grad_norm": 1.5656158924102783,
        "learning_rate": 0.00019785143050117776,
        "epoch": 0.7129414797628255,
        "step": 5531
    },
    {
        "loss": 2.0396,
        "grad_norm": 1.341674566268921,
        "learning_rate": 0.00019783886839392945,
        "epoch": 0.7130703789636504,
        "step": 5532
    },
    {
        "loss": 1.9905,
        "grad_norm": 2.2498486042022705,
        "learning_rate": 0.0001978262700711587,
        "epoch": 0.7131992781644754,
        "step": 5533
    },
    {
        "loss": 1.8999,
        "grad_norm": 2.1529598236083984,
        "learning_rate": 0.00019781363553752888,
        "epoch": 0.7133281773653003,
        "step": 5534
    },
    {
        "loss": 2.145,
        "grad_norm": 1.4635058641433716,
        "learning_rate": 0.00019780096479771664,
        "epoch": 0.7134570765661253,
        "step": 5535
    },
    {
        "loss": 1.2252,
        "grad_norm": 2.8633534908294678,
        "learning_rate": 0.0001977882578564122,
        "epoch": 0.7135859757669503,
        "step": 5536
    },
    {
        "loss": 1.6194,
        "grad_norm": 3.036207675933838,
        "learning_rate": 0.00019777551471831907,
        "epoch": 0.7137148749677752,
        "step": 5537
    },
    {
        "loss": 2.242,
        "grad_norm": 1.7352545261383057,
        "learning_rate": 0.00019776273538815416,
        "epoch": 0.7138437741686001,
        "step": 5538
    },
    {
        "loss": 1.8145,
        "grad_norm": 1.800930380821228,
        "learning_rate": 0.0001977499198706478,
        "epoch": 0.7139726733694252,
        "step": 5539
    },
    {
        "loss": 1.6097,
        "grad_norm": 2.7129628658294678,
        "learning_rate": 0.00019773706817054375,
        "epoch": 0.7141015725702501,
        "step": 5540
    },
    {
        "loss": 2.1032,
        "grad_norm": 1.3195019960403442,
        "learning_rate": 0.00019772418029259908,
        "epoch": 0.714230471771075,
        "step": 5541
    },
    {
        "loss": 2.4914,
        "grad_norm": 1.8902416229248047,
        "learning_rate": 0.00019771125624158434,
        "epoch": 0.7143593709718999,
        "step": 5542
    },
    {
        "loss": 1.7987,
        "grad_norm": 2.466174364089966,
        "learning_rate": 0.00019769829602228343,
        "epoch": 0.714488270172725,
        "step": 5543
    },
    {
        "loss": 1.3582,
        "grad_norm": 2.15956449508667,
        "learning_rate": 0.0001976852996394936,
        "epoch": 0.7146171693735499,
        "step": 5544
    },
    {
        "loss": 2.0205,
        "grad_norm": 1.3755056858062744,
        "learning_rate": 0.00019767226709802556,
        "epoch": 0.7147460685743748,
        "step": 5545
    },
    {
        "loss": 1.4038,
        "grad_norm": 1.8757526874542236,
        "learning_rate": 0.00019765919840270332,
        "epoch": 0.7148749677751998,
        "step": 5546
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.0562710762023926,
        "learning_rate": 0.0001976460935583644,
        "epoch": 0.7150038669760247,
        "step": 5547
    },
    {
        "loss": 2.1264,
        "grad_norm": 1.9718725681304932,
        "learning_rate": 0.00019763295256985952,
        "epoch": 0.7151327661768497,
        "step": 5548
    },
    {
        "loss": 2.2099,
        "grad_norm": 2.0172057151794434,
        "learning_rate": 0.00019761977544205297,
        "epoch": 0.7152616653776747,
        "step": 5549
    },
    {
        "loss": 0.6507,
        "grad_norm": 3.0384562015533447,
        "learning_rate": 0.00019760656217982224,
        "epoch": 0.7153905645784996,
        "step": 5550
    },
    {
        "loss": 1.3257,
        "grad_norm": 3.199172019958496,
        "learning_rate": 0.00019759331278805837,
        "epoch": 0.7155194637793245,
        "step": 5551
    },
    {
        "loss": 1.7898,
        "grad_norm": 2.582876682281494,
        "learning_rate": 0.00019758002727166563,
        "epoch": 0.7156483629801496,
        "step": 5552
    },
    {
        "loss": 1.8964,
        "grad_norm": 2.5857486724853516,
        "learning_rate": 0.0001975667056355617,
        "epoch": 0.7157772621809745,
        "step": 5553
    },
    {
        "loss": 2.151,
        "grad_norm": 1.4134635925292969,
        "learning_rate": 0.00019755334788467774,
        "epoch": 0.7159061613817994,
        "step": 5554
    },
    {
        "loss": 2.3676,
        "grad_norm": 1.6146354675292969,
        "learning_rate": 0.0001975399540239581,
        "epoch": 0.7160350605826243,
        "step": 5555
    },
    {
        "loss": 1.2672,
        "grad_norm": 2.8855175971984863,
        "learning_rate": 0.0001975265240583606,
        "epoch": 0.7161639597834494,
        "step": 5556
    },
    {
        "loss": 2.1294,
        "grad_norm": 1.6965433359146118,
        "learning_rate": 0.0001975130579928564,
        "epoch": 0.7162928589842743,
        "step": 5557
    },
    {
        "loss": 1.1491,
        "grad_norm": 2.918093204498291,
        "learning_rate": 0.00019749955583243004,
        "epoch": 0.7164217581850992,
        "step": 5558
    },
    {
        "loss": 2.2587,
        "grad_norm": 1.4115012884140015,
        "learning_rate": 0.0001974860175820794,
        "epoch": 0.7165506573859242,
        "step": 5559
    },
    {
        "loss": 2.538,
        "grad_norm": 1.0631128549575806,
        "learning_rate": 0.00019747244324681577,
        "epoch": 0.7166795565867492,
        "step": 5560
    },
    {
        "loss": 2.4119,
        "grad_norm": 1.6013610363006592,
        "learning_rate": 0.00019745883283166368,
        "epoch": 0.7168084557875741,
        "step": 5561
    },
    {
        "loss": 2.4382,
        "grad_norm": 1.8797247409820557,
        "learning_rate": 0.00019744518634166113,
        "epoch": 0.7169373549883991,
        "step": 5562
    },
    {
        "loss": 2.3152,
        "grad_norm": 1.4167240858078003,
        "learning_rate": 0.0001974315037818595,
        "epoch": 0.717066254189224,
        "step": 5563
    },
    {
        "loss": 1.7675,
        "grad_norm": 2.3386318683624268,
        "learning_rate": 0.0001974177851573233,
        "epoch": 0.717195153390049,
        "step": 5564
    },
    {
        "loss": 2.2302,
        "grad_norm": 2.0277438163757324,
        "learning_rate": 0.00019740403047313065,
        "epoch": 0.717324052590874,
        "step": 5565
    },
    {
        "loss": 2.008,
        "grad_norm": 2.271683692932129,
        "learning_rate": 0.00019739023973437293,
        "epoch": 0.7174529517916989,
        "step": 5566
    },
    {
        "loss": 1.7729,
        "grad_norm": 2.0919811725616455,
        "learning_rate": 0.00019737641294615478,
        "epoch": 0.7175818509925238,
        "step": 5567
    },
    {
        "loss": 1.8952,
        "grad_norm": 2.0591366291046143,
        "learning_rate": 0.0001973625501135943,
        "epoch": 0.7177107501933488,
        "step": 5568
    },
    {
        "loss": 1.0625,
        "grad_norm": 2.5691335201263428,
        "learning_rate": 0.00019734865124182286,
        "epoch": 0.7178396493941738,
        "step": 5569
    },
    {
        "loss": 2.5958,
        "grad_norm": 1.1882668733596802,
        "learning_rate": 0.0001973347163359852,
        "epoch": 0.7179685485949987,
        "step": 5570
    },
    {
        "loss": 1.5956,
        "grad_norm": 2.8471972942352295,
        "learning_rate": 0.0001973207454012394,
        "epoch": 0.7180974477958236,
        "step": 5571
    },
    {
        "loss": 2.0847,
        "grad_norm": 1.7749215364456177,
        "learning_rate": 0.00019730673844275685,
        "epoch": 0.7182263469966487,
        "step": 5572
    },
    {
        "loss": 2.0946,
        "grad_norm": 2.024963855743408,
        "learning_rate": 0.0001972926954657223,
        "epoch": 0.7183552461974736,
        "step": 5573
    },
    {
        "loss": 2.1884,
        "grad_norm": 1.3559465408325195,
        "learning_rate": 0.00019727861647533384,
        "epoch": 0.7184841453982985,
        "step": 5574
    },
    {
        "loss": 1.3256,
        "grad_norm": 2.0365419387817383,
        "learning_rate": 0.00019726450147680282,
        "epoch": 0.7186130445991235,
        "step": 5575
    },
    {
        "loss": 1.9509,
        "grad_norm": 1.3323017358779907,
        "learning_rate": 0.000197250350475354,
        "epoch": 0.7187419437999485,
        "step": 5576
    },
    {
        "loss": 1.9573,
        "grad_norm": 1.9517171382904053,
        "learning_rate": 0.00019723616347622552,
        "epoch": 0.7188708430007734,
        "step": 5577
    },
    {
        "loss": 2.0367,
        "grad_norm": 2.1335179805755615,
        "learning_rate": 0.00019722194048466866,
        "epoch": 0.7189997422015983,
        "step": 5578
    },
    {
        "loss": 0.8145,
        "grad_norm": 3.3359532356262207,
        "learning_rate": 0.0001972076815059482,
        "epoch": 0.7191286414024233,
        "step": 5579
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.672245979309082,
        "learning_rate": 0.00019719338654534212,
        "epoch": 0.7192575406032483,
        "step": 5580
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.8452378511428833,
        "learning_rate": 0.00019717905560814178,
        "epoch": 0.7193864398040732,
        "step": 5581
    },
    {
        "loss": 2.2738,
        "grad_norm": 1.668220043182373,
        "learning_rate": 0.00019716468869965183,
        "epoch": 0.7195153390048982,
        "step": 5582
    },
    {
        "loss": 1.9347,
        "grad_norm": 2.155130386352539,
        "learning_rate": 0.0001971502858251903,
        "epoch": 0.7196442382057231,
        "step": 5583
    },
    {
        "loss": 2.0281,
        "grad_norm": 1.1338046789169312,
        "learning_rate": 0.00019713584699008845,
        "epoch": 0.719773137406548,
        "step": 5584
    },
    {
        "loss": 2.1242,
        "grad_norm": 1.3064059019088745,
        "learning_rate": 0.00019712137219969088,
        "epoch": 0.7199020366073731,
        "step": 5585
    },
    {
        "loss": 1.6561,
        "grad_norm": 2.925201654434204,
        "learning_rate": 0.00019710686145935552,
        "epoch": 0.720030935808198,
        "step": 5586
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.1894688606262207,
        "learning_rate": 0.00019709231477445352,
        "epoch": 0.7201598350090229,
        "step": 5587
    },
    {
        "loss": 2.291,
        "grad_norm": 1.1641201972961426,
        "learning_rate": 0.00019707773215036953,
        "epoch": 0.7202887342098478,
        "step": 5588
    },
    {
        "loss": 2.3924,
        "grad_norm": 1.315109372138977,
        "learning_rate": 0.0001970631135925013,
        "epoch": 0.7204176334106729,
        "step": 5589
    },
    {
        "loss": 1.3524,
        "grad_norm": 1.3439947366714478,
        "learning_rate": 0.00019704845910625994,
        "epoch": 0.7205465326114978,
        "step": 5590
    },
    {
        "loss": 1.4173,
        "grad_norm": 2.485050678253174,
        "learning_rate": 0.00019703376869706991,
        "epoch": 0.7206754318123227,
        "step": 5591
    },
    {
        "loss": 2.0694,
        "grad_norm": 2.114553451538086,
        "learning_rate": 0.00019701904237036895,
        "epoch": 0.7208043310131477,
        "step": 5592
    },
    {
        "loss": 1.8647,
        "grad_norm": 2.0370519161224365,
        "learning_rate": 0.00019700428013160801,
        "epoch": 0.7209332302139727,
        "step": 5593
    },
    {
        "loss": 1.1978,
        "grad_norm": 2.4728221893310547,
        "learning_rate": 0.00019698948198625154,
        "epoch": 0.7210621294147976,
        "step": 5594
    },
    {
        "loss": 2.1529,
        "grad_norm": 2.572171688079834,
        "learning_rate": 0.000196974647939777,
        "epoch": 0.7211910286156226,
        "step": 5595
    },
    {
        "loss": 1.8797,
        "grad_norm": 3.108304977416992,
        "learning_rate": 0.00019695977799767534,
        "epoch": 0.7213199278164475,
        "step": 5596
    },
    {
        "loss": 1.4415,
        "grad_norm": 2.3206775188446045,
        "learning_rate": 0.00019694487216545073,
        "epoch": 0.7214488270172725,
        "step": 5597
    },
    {
        "loss": 1.927,
        "grad_norm": 3.1551477909088135,
        "learning_rate": 0.00019692993044862067,
        "epoch": 0.7215777262180975,
        "step": 5598
    },
    {
        "loss": 0.9418,
        "grad_norm": 2.863743782043457,
        "learning_rate": 0.00019691495285271586,
        "epoch": 0.7217066254189224,
        "step": 5599
    },
    {
        "loss": 2.362,
        "grad_norm": 1.442300796508789,
        "learning_rate": 0.00019689993938328036,
        "epoch": 0.7218355246197473,
        "step": 5600
    },
    {
        "loss": 1.4949,
        "grad_norm": 2.1647939682006836,
        "learning_rate": 0.00019688489004587148,
        "epoch": 0.7219644238205724,
        "step": 5601
    },
    {
        "loss": 2.152,
        "grad_norm": 1.8907499313354492,
        "learning_rate": 0.00019686980484605977,
        "epoch": 0.7220933230213973,
        "step": 5602
    },
    {
        "loss": 1.5927,
        "grad_norm": 2.1421782970428467,
        "learning_rate": 0.00019685468378942913,
        "epoch": 0.7222222222222222,
        "step": 5603
    },
    {
        "loss": 2.2439,
        "grad_norm": 1.872589111328125,
        "learning_rate": 0.00019683952688157665,
        "epoch": 0.7223511214230471,
        "step": 5604
    },
    {
        "loss": 2.0313,
        "grad_norm": 1.3211477994918823,
        "learning_rate": 0.00019682433412811278,
        "epoch": 0.7224800206238722,
        "step": 5605
    },
    {
        "loss": 0.849,
        "grad_norm": 2.5517263412475586,
        "learning_rate": 0.00019680910553466113,
        "epoch": 0.7226089198246971,
        "step": 5606
    },
    {
        "loss": 1.5559,
        "grad_norm": 2.5933432579040527,
        "learning_rate": 0.00019679384110685868,
        "epoch": 0.722737819025522,
        "step": 5607
    },
    {
        "loss": 1.721,
        "grad_norm": 1.314632534980774,
        "learning_rate": 0.00019677854085035562,
        "epoch": 0.722866718226347,
        "step": 5608
    },
    {
        "loss": 1.6819,
        "grad_norm": 3.2146267890930176,
        "learning_rate": 0.00019676320477081543,
        "epoch": 0.722995617427172,
        "step": 5609
    },
    {
        "loss": 1.3636,
        "grad_norm": 3.2057008743286133,
        "learning_rate": 0.0001967478328739148,
        "epoch": 0.7231245166279969,
        "step": 5610
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.892871618270874,
        "learning_rate": 0.0001967324251653437,
        "epoch": 0.7232534158288219,
        "step": 5611
    },
    {
        "loss": 2.2632,
        "grad_norm": 2.0859909057617188,
        "learning_rate": 0.00019671698165080542,
        "epoch": 0.7233823150296468,
        "step": 5612
    },
    {
        "loss": 1.984,
        "grad_norm": 1.6399848461151123,
        "learning_rate": 0.00019670150233601642,
        "epoch": 0.7235112142304718,
        "step": 5613
    },
    {
        "loss": 2.0274,
        "grad_norm": 1.4586679935455322,
        "learning_rate": 0.00019668598722670646,
        "epoch": 0.7236401134312967,
        "step": 5614
    },
    {
        "loss": 1.7469,
        "grad_norm": 1.9863636493682861,
        "learning_rate": 0.0001966704363286185,
        "epoch": 0.7237690126321217,
        "step": 5615
    },
    {
        "loss": 1.392,
        "grad_norm": 2.200672149658203,
        "learning_rate": 0.00019665484964750878,
        "epoch": 0.7238979118329466,
        "step": 5616
    },
    {
        "loss": 2.0307,
        "grad_norm": 1.3215556144714355,
        "learning_rate": 0.0001966392271891468,
        "epoch": 0.7240268110337716,
        "step": 5617
    },
    {
        "loss": 2.3914,
        "grad_norm": 1.1894047260284424,
        "learning_rate": 0.0001966235689593153,
        "epoch": 0.7241557102345966,
        "step": 5618
    },
    {
        "loss": 2.5646,
        "grad_norm": 1.8861050605773926,
        "learning_rate": 0.00019660787496381024,
        "epoch": 0.7242846094354215,
        "step": 5619
    },
    {
        "loss": 1.9374,
        "grad_norm": 1.9817347526550293,
        "learning_rate": 0.0001965921452084408,
        "epoch": 0.7244135086362464,
        "step": 5620
    },
    {
        "loss": 1.8208,
        "grad_norm": 1.7862051725387573,
        "learning_rate": 0.00019657637969902947,
        "epoch": 0.7245424078370714,
        "step": 5621
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.906382441520691,
        "learning_rate": 0.00019656057844141188,
        "epoch": 0.7246713070378964,
        "step": 5622
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.133720636367798,
        "learning_rate": 0.00019654474144143697,
        "epoch": 0.7248002062387213,
        "step": 5623
    },
    {
        "loss": 1.5548,
        "grad_norm": 2.7471299171447754,
        "learning_rate": 0.00019652886870496686,
        "epoch": 0.7249291054395463,
        "step": 5624
    },
    {
        "loss": 1.8822,
        "grad_norm": 1.955957055091858,
        "learning_rate": 0.00019651296023787698,
        "epoch": 0.7250580046403712,
        "step": 5625
    },
    {
        "loss": 2.3708,
        "grad_norm": 2.464461088180542,
        "learning_rate": 0.00019649701604605583,
        "epoch": 0.7251869038411962,
        "step": 5626
    },
    {
        "loss": 1.583,
        "grad_norm": 2.2323238849639893,
        "learning_rate": 0.0001964810361354053,
        "epoch": 0.7253158030420211,
        "step": 5627
    },
    {
        "loss": 2.2232,
        "grad_norm": 1.7514373064041138,
        "learning_rate": 0.00019646502051184038,
        "epoch": 0.7254447022428461,
        "step": 5628
    },
    {
        "loss": 1.8048,
        "grad_norm": 2.6184751987457275,
        "learning_rate": 0.00019644896918128942,
        "epoch": 0.725573601443671,
        "step": 5629
    },
    {
        "loss": 1.6496,
        "grad_norm": 2.4611589908599854,
        "learning_rate": 0.00019643288214969378,
        "epoch": 0.725702500644496,
        "step": 5630
    },
    {
        "loss": 2.0572,
        "grad_norm": 2.679743766784668,
        "learning_rate": 0.0001964167594230082,
        "epoch": 0.725831399845321,
        "step": 5631
    },
    {
        "loss": 1.6348,
        "grad_norm": 2.1172561645507812,
        "learning_rate": 0.00019640060100720063,
        "epoch": 0.7259602990461459,
        "step": 5632
    },
    {
        "loss": 1.7803,
        "grad_norm": 1.7228760719299316,
        "learning_rate": 0.0001963844069082521,
        "epoch": 0.7260891982469708,
        "step": 5633
    },
    {
        "loss": 1.5528,
        "grad_norm": 2.20251202583313,
        "learning_rate": 0.000196368177132157,
        "epoch": 0.7262180974477959,
        "step": 5634
    },
    {
        "loss": 1.1833,
        "grad_norm": 2.5830318927764893,
        "learning_rate": 0.00019635191168492283,
        "epoch": 0.7263469966486208,
        "step": 5635
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.172825813293457,
        "learning_rate": 0.0001963356105725703,
        "epoch": 0.7264758958494457,
        "step": 5636
    },
    {
        "loss": 1.0936,
        "grad_norm": 3.0584890842437744,
        "learning_rate": 0.0001963192738011334,
        "epoch": 0.7266047950502706,
        "step": 5637
    },
    {
        "loss": 1.9905,
        "grad_norm": 1.888799786567688,
        "learning_rate": 0.00019630290137665922,
        "epoch": 0.7267336942510957,
        "step": 5638
    },
    {
        "loss": 1.8913,
        "grad_norm": 1.5126107931137085,
        "learning_rate": 0.00019628649330520812,
        "epoch": 0.7268625934519206,
        "step": 5639
    },
    {
        "loss": 1.3941,
        "grad_norm": 2.193737745285034,
        "learning_rate": 0.0001962700495928536,
        "epoch": 0.7269914926527455,
        "step": 5640
    },
    {
        "loss": 1.341,
        "grad_norm": 2.1756162643432617,
        "learning_rate": 0.00019625357024568237,
        "epoch": 0.7271203918535705,
        "step": 5641
    },
    {
        "loss": 2.2411,
        "grad_norm": 2.2104992866516113,
        "learning_rate": 0.0001962370552697944,
        "epoch": 0.7272492910543955,
        "step": 5642
    },
    {
        "loss": 2.0284,
        "grad_norm": 2.022247314453125,
        "learning_rate": 0.00019622050467130267,
        "epoch": 0.7273781902552204,
        "step": 5643
    },
    {
        "loss": 1.7458,
        "grad_norm": 1.6330442428588867,
        "learning_rate": 0.0001962039184563336,
        "epoch": 0.7275070894560454,
        "step": 5644
    },
    {
        "loss": 1.8825,
        "grad_norm": 2.0583279132843018,
        "learning_rate": 0.00019618729663102658,
        "epoch": 0.7276359886568703,
        "step": 5645
    },
    {
        "loss": 2.4423,
        "grad_norm": 1.1395004987716675,
        "learning_rate": 0.0001961706392015343,
        "epoch": 0.7277648878576953,
        "step": 5646
    },
    {
        "loss": 2.2784,
        "grad_norm": 2.6610841751098633,
        "learning_rate": 0.00019615394617402253,
        "epoch": 0.7278937870585203,
        "step": 5647
    },
    {
        "loss": 1.7086,
        "grad_norm": 2.2935597896575928,
        "learning_rate": 0.00019613721755467028,
        "epoch": 0.7280226862593452,
        "step": 5648
    },
    {
        "loss": 2.075,
        "grad_norm": 2.0622305870056152,
        "learning_rate": 0.0001961204533496698,
        "epoch": 0.7281515854601701,
        "step": 5649
    },
    {
        "loss": 1.9835,
        "grad_norm": 2.2804291248321533,
        "learning_rate": 0.00019610365356522634,
        "epoch": 0.7282804846609952,
        "step": 5650
    },
    {
        "loss": 1.8209,
        "grad_norm": 2.3645639419555664,
        "learning_rate": 0.00019608681820755845,
        "epoch": 0.7284093838618201,
        "step": 5651
    },
    {
        "loss": 2.3876,
        "grad_norm": 1.8464552164077759,
        "learning_rate": 0.00019606994728289792,
        "epoch": 0.728538283062645,
        "step": 5652
    },
    {
        "loss": 1.7608,
        "grad_norm": 2.2028255462646484,
        "learning_rate": 0.00019605304079748948,
        "epoch": 0.7286671822634699,
        "step": 5653
    },
    {
        "loss": 2.2599,
        "grad_norm": 3.017925500869751,
        "learning_rate": 0.00019603609875759118,
        "epoch": 0.728796081464295,
        "step": 5654
    },
    {
        "loss": 1.7178,
        "grad_norm": 2.2966368198394775,
        "learning_rate": 0.0001960191211694742,
        "epoch": 0.7289249806651199,
        "step": 5655
    },
    {
        "loss": 2.2584,
        "grad_norm": 1.4046227931976318,
        "learning_rate": 0.0001960021080394229,
        "epoch": 0.7290538798659448,
        "step": 5656
    },
    {
        "loss": 2.1882,
        "grad_norm": 2.1530110836029053,
        "learning_rate": 0.00019598505937373472,
        "epoch": 0.7291827790667698,
        "step": 5657
    },
    {
        "loss": 1.9811,
        "grad_norm": 1.9720609188079834,
        "learning_rate": 0.00019596797517872034,
        "epoch": 0.7293116782675947,
        "step": 5658
    },
    {
        "loss": 1.466,
        "grad_norm": 2.520176649093628,
        "learning_rate": 0.00019595085546070354,
        "epoch": 0.7294405774684197,
        "step": 5659
    },
    {
        "loss": 2.1544,
        "grad_norm": 1.5627527236938477,
        "learning_rate": 0.00019593370022602131,
        "epoch": 0.7295694766692447,
        "step": 5660
    },
    {
        "loss": 1.8855,
        "grad_norm": 2.376805067062378,
        "learning_rate": 0.0001959165094810237,
        "epoch": 0.7296983758700696,
        "step": 5661
    },
    {
        "loss": 1.8193,
        "grad_norm": 2.0586490631103516,
        "learning_rate": 0.00019589928323207395,
        "epoch": 0.7298272750708945,
        "step": 5662
    },
    {
        "loss": 1.7735,
        "grad_norm": 2.040253162384033,
        "learning_rate": 0.00019588202148554843,
        "epoch": 0.7299561742717195,
        "step": 5663
    },
    {
        "loss": 1.6604,
        "grad_norm": 2.5994603633880615,
        "learning_rate": 0.00019586472424783666,
        "epoch": 0.7300850734725445,
        "step": 5664
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.8110147714614868,
        "learning_rate": 0.0001958473915253413,
        "epoch": 0.7302139726733694,
        "step": 5665
    },
    {
        "loss": 2.5888,
        "grad_norm": 1.8031305074691772,
        "learning_rate": 0.00019583002332447817,
        "epoch": 0.7303428718741943,
        "step": 5666
    },
    {
        "loss": 1.4658,
        "grad_norm": 2.0622034072875977,
        "learning_rate": 0.00019581261965167618,
        "epoch": 0.7304717710750194,
        "step": 5667
    },
    {
        "loss": 1.7993,
        "grad_norm": 2.620678663253784,
        "learning_rate": 0.0001957951805133773,
        "epoch": 0.7306006702758443,
        "step": 5668
    },
    {
        "loss": 1.6702,
        "grad_norm": 2.391761064529419,
        "learning_rate": 0.00019577770591603684,
        "epoch": 0.7307295694766692,
        "step": 5669
    },
    {
        "loss": 1.8108,
        "grad_norm": 1.3685719966888428,
        "learning_rate": 0.000195760195866123,
        "epoch": 0.7308584686774942,
        "step": 5670
    },
    {
        "loss": 1.8523,
        "grad_norm": 1.7433723211288452,
        "learning_rate": 0.00019574265037011727,
        "epoch": 0.7309873678783192,
        "step": 5671
    },
    {
        "loss": 2.2365,
        "grad_norm": 1.2134995460510254,
        "learning_rate": 0.00019572506943451416,
        "epoch": 0.7311162670791441,
        "step": 5672
    },
    {
        "loss": 2.6249,
        "grad_norm": 1.390102505683899,
        "learning_rate": 0.0001957074530658214,
        "epoch": 0.731245166279969,
        "step": 5673
    },
    {
        "loss": 1.4526,
        "grad_norm": 1.812673807144165,
        "learning_rate": 0.0001956898012705597,
        "epoch": 0.731374065480794,
        "step": 5674
    },
    {
        "loss": 2.3772,
        "grad_norm": 1.1453191041946411,
        "learning_rate": 0.00019567211405526296,
        "epoch": 0.731502964681619,
        "step": 5675
    },
    {
        "loss": 1.8499,
        "grad_norm": 2.124089241027832,
        "learning_rate": 0.00019565439142647825,
        "epoch": 0.731631863882444,
        "step": 5676
    },
    {
        "loss": 1.9812,
        "grad_norm": 2.092564821243286,
        "learning_rate": 0.00019563663339076563,
        "epoch": 0.7317607630832689,
        "step": 5677
    },
    {
        "loss": 1.6299,
        "grad_norm": 1.6732966899871826,
        "learning_rate": 0.00019561883995469835,
        "epoch": 0.7318896622840938,
        "step": 5678
    },
    {
        "loss": 2.3048,
        "grad_norm": 2.0777759552001953,
        "learning_rate": 0.00019560101112486272,
        "epoch": 0.7320185614849188,
        "step": 5679
    },
    {
        "loss": 1.9015,
        "grad_norm": 1.8071212768554688,
        "learning_rate": 0.00019558314690785818,
        "epoch": 0.7321474606857438,
        "step": 5680
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.876595139503479,
        "learning_rate": 0.00019556524731029722,
        "epoch": 0.7322763598865687,
        "step": 5681
    },
    {
        "loss": 1.9192,
        "grad_norm": 2.2807374000549316,
        "learning_rate": 0.0001955473123388055,
        "epoch": 0.7324052590873936,
        "step": 5682
    },
    {
        "loss": 2.3813,
        "grad_norm": 1.905430793762207,
        "learning_rate": 0.00019552934200002173,
        "epoch": 0.7325341582882187,
        "step": 5683
    },
    {
        "loss": 2.3774,
        "grad_norm": 2.149751901626587,
        "learning_rate": 0.00019551133630059769,
        "epoch": 0.7326630574890436,
        "step": 5684
    },
    {
        "loss": 2.05,
        "grad_norm": 1.4855338335037231,
        "learning_rate": 0.00019549329524719825,
        "epoch": 0.7327919566898685,
        "step": 5685
    },
    {
        "loss": 2.0091,
        "grad_norm": 1.7928316593170166,
        "learning_rate": 0.00019547521884650152,
        "epoch": 0.7329208558906934,
        "step": 5686
    },
    {
        "loss": 1.9131,
        "grad_norm": 2.0781121253967285,
        "learning_rate": 0.00019545710710519842,
        "epoch": 0.7330497550915185,
        "step": 5687
    },
    {
        "loss": 0.9356,
        "grad_norm": 2.714473009109497,
        "learning_rate": 0.0001954389600299932,
        "epoch": 0.7331786542923434,
        "step": 5688
    },
    {
        "loss": 1.5222,
        "grad_norm": 2.388188362121582,
        "learning_rate": 0.000195420777627603,
        "epoch": 0.7333075534931683,
        "step": 5689
    },
    {
        "loss": 1.6718,
        "grad_norm": 2.403517246246338,
        "learning_rate": 0.0001954025599047582,
        "epoch": 0.7334364526939933,
        "step": 5690
    },
    {
        "loss": 1.8586,
        "grad_norm": 2.250373363494873,
        "learning_rate": 0.0001953843068682021,
        "epoch": 0.7335653518948183,
        "step": 5691
    },
    {
        "loss": 2.578,
        "grad_norm": 1.4189156293869019,
        "learning_rate": 0.00019536601852469123,
        "epoch": 0.7336942510956432,
        "step": 5692
    },
    {
        "loss": 1.178,
        "grad_norm": 2.0080063343048096,
        "learning_rate": 0.00019534769488099507,
        "epoch": 0.7338231502964682,
        "step": 5693
    },
    {
        "loss": 1.7731,
        "grad_norm": 2.4051923751831055,
        "learning_rate": 0.00019532933594389618,
        "epoch": 0.7339520494972931,
        "step": 5694
    },
    {
        "loss": 2.2286,
        "grad_norm": 2.0887486934661865,
        "learning_rate": 0.00019531094172019024,
        "epoch": 0.734080948698118,
        "step": 5695
    },
    {
        "loss": 2.065,
        "grad_norm": 1.8489941358566284,
        "learning_rate": 0.00019529251221668594,
        "epoch": 0.7342098478989431,
        "step": 5696
    },
    {
        "loss": 1.8465,
        "grad_norm": 2.7074666023254395,
        "learning_rate": 0.00019527404744020509,
        "epoch": 0.734338747099768,
        "step": 5697
    },
    {
        "loss": 1.9331,
        "grad_norm": 2.1242563724517822,
        "learning_rate": 0.00019525554739758245,
        "epoch": 0.7344676463005929,
        "step": 5698
    },
    {
        "loss": 1.759,
        "grad_norm": 2.4622292518615723,
        "learning_rate": 0.00019523701209566593,
        "epoch": 0.7345965455014178,
        "step": 5699
    },
    {
        "loss": 1.5602,
        "grad_norm": 3.3123762607574463,
        "learning_rate": 0.0001952184415413165,
        "epoch": 0.7347254447022429,
        "step": 5700
    },
    {
        "loss": 2.416,
        "grad_norm": 1.5384384393692017,
        "learning_rate": 0.00019519983574140803,
        "epoch": 0.7348543439030678,
        "step": 5701
    },
    {
        "loss": 2.0573,
        "grad_norm": 1.395433783531189,
        "learning_rate": 0.00019518119470282765,
        "epoch": 0.7349832431038927,
        "step": 5702
    },
    {
        "loss": 2.3369,
        "grad_norm": 1.6417409181594849,
        "learning_rate": 0.00019516251843247542,
        "epoch": 0.7351121423047177,
        "step": 5703
    },
    {
        "loss": 0.6283,
        "grad_norm": 3.050920009613037,
        "learning_rate": 0.0001951438069372644,
        "epoch": 0.7352410415055427,
        "step": 5704
    },
    {
        "loss": 1.6281,
        "grad_norm": 2.057374954223633,
        "learning_rate": 0.00019512506022412076,
        "epoch": 0.7353699407063676,
        "step": 5705
    },
    {
        "loss": 2.2864,
        "grad_norm": 2.0861716270446777,
        "learning_rate": 0.0001951062782999837,
        "epoch": 0.7354988399071926,
        "step": 5706
    },
    {
        "loss": 2.2191,
        "grad_norm": 1.6179096698760986,
        "learning_rate": 0.00019508746117180543,
        "epoch": 0.7356277391080175,
        "step": 5707
    },
    {
        "loss": 2.1034,
        "grad_norm": 1.6535594463348389,
        "learning_rate": 0.0001950686088465512,
        "epoch": 0.7357566383088425,
        "step": 5708
    },
    {
        "loss": 1.3148,
        "grad_norm": 2.2263498306274414,
        "learning_rate": 0.0001950497213311993,
        "epoch": 0.7358855375096675,
        "step": 5709
    },
    {
        "loss": 1.9457,
        "grad_norm": 1.713700771331787,
        "learning_rate": 0.00019503079863274097,
        "epoch": 0.7360144367104924,
        "step": 5710
    },
    {
        "loss": 2.0703,
        "grad_norm": 1.7640445232391357,
        "learning_rate": 0.00019501184075818059,
        "epoch": 0.7361433359113173,
        "step": 5711
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.3033230304718018,
        "learning_rate": 0.0001949928477145355,
        "epoch": 0.7362722351121423,
        "step": 5712
    },
    {
        "loss": 1.7998,
        "grad_norm": 2.4454092979431152,
        "learning_rate": 0.00019497381950883612,
        "epoch": 0.7364011343129673,
        "step": 5713
    },
    {
        "loss": 1.2713,
        "grad_norm": 2.0456669330596924,
        "learning_rate": 0.00019495475614812578,
        "epoch": 0.7365300335137922,
        "step": 5714
    },
    {
        "loss": 2.2544,
        "grad_norm": 1.6361900568008423,
        "learning_rate": 0.00019493565763946085,
        "epoch": 0.7366589327146171,
        "step": 5715
    },
    {
        "loss": 0.5629,
        "grad_norm": 2.5678763389587402,
        "learning_rate": 0.0001949165239899107,
        "epoch": 0.7367878319154422,
        "step": 5716
    },
    {
        "loss": 1.8815,
        "grad_norm": 1.4499320983886719,
        "learning_rate": 0.00019489735520655788,
        "epoch": 0.7369167311162671,
        "step": 5717
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.5600252151489258,
        "learning_rate": 0.00019487815129649768,
        "epoch": 0.737045630317092,
        "step": 5718
    },
    {
        "loss": 1.9984,
        "grad_norm": 2.1554367542266846,
        "learning_rate": 0.00019485891226683858,
        "epoch": 0.737174529517917,
        "step": 5719
    },
    {
        "loss": 2.0559,
        "grad_norm": 1.6080825328826904,
        "learning_rate": 0.000194839638124702,
        "epoch": 0.737303428718742,
        "step": 5720
    },
    {
        "loss": 2.0855,
        "grad_norm": 1.6775459051132202,
        "learning_rate": 0.00019482032887722228,
        "epoch": 0.7374323279195669,
        "step": 5721
    },
    {
        "loss": 0.9011,
        "grad_norm": 2.451282501220703,
        "learning_rate": 0.0001948009845315469,
        "epoch": 0.7375612271203918,
        "step": 5722
    },
    {
        "loss": 2.0109,
        "grad_norm": 2.1928930282592773,
        "learning_rate": 0.0001947816050948363,
        "epoch": 0.7376901263212168,
        "step": 5723
    },
    {
        "loss": 0.9424,
        "grad_norm": 2.765202045440674,
        "learning_rate": 0.00019476219057426377,
        "epoch": 0.7378190255220418,
        "step": 5724
    },
    {
        "loss": 2.3044,
        "grad_norm": 1.3616605997085571,
        "learning_rate": 0.00019474274097701575,
        "epoch": 0.7379479247228667,
        "step": 5725
    },
    {
        "loss": 2.1688,
        "grad_norm": 2.437835216522217,
        "learning_rate": 0.0001947232563102916,
        "epoch": 0.7380768239236917,
        "step": 5726
    },
    {
        "loss": 2.3166,
        "grad_norm": 2.2686121463775635,
        "learning_rate": 0.00019470373658130364,
        "epoch": 0.7382057231245166,
        "step": 5727
    },
    {
        "loss": 2.0792,
        "grad_norm": 2.235335350036621,
        "learning_rate": 0.00019468418179727716,
        "epoch": 0.7383346223253415,
        "step": 5728
    },
    {
        "loss": 2.187,
        "grad_norm": 2.3461992740631104,
        "learning_rate": 0.00019466459196545053,
        "epoch": 0.7384635215261666,
        "step": 5729
    },
    {
        "loss": 1.9141,
        "grad_norm": 1.377698302268982,
        "learning_rate": 0.000194644967093075,
        "epoch": 0.7385924207269915,
        "step": 5730
    },
    {
        "loss": 1.7481,
        "grad_norm": 2.8861618041992188,
        "learning_rate": 0.00019462530718741477,
        "epoch": 0.7387213199278164,
        "step": 5731
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.4305157661437988,
        "learning_rate": 0.0001946056122557471,
        "epoch": 0.7388502191286414,
        "step": 5732
    },
    {
        "loss": 2.1841,
        "grad_norm": 2.084249973297119,
        "learning_rate": 0.0001945858823053621,
        "epoch": 0.7389791183294664,
        "step": 5733
    },
    {
        "loss": 1.1823,
        "grad_norm": 3.2206063270568848,
        "learning_rate": 0.00019456611734356295,
        "epoch": 0.7391080175302913,
        "step": 5734
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.465028762817383,
        "learning_rate": 0.00019454631737766574,
        "epoch": 0.7392369167311162,
        "step": 5735
    },
    {
        "loss": 2.2006,
        "grad_norm": 1.8528339862823486,
        "learning_rate": 0.0001945264824149995,
        "epoch": 0.7393658159319412,
        "step": 5736
    },
    {
        "loss": 1.5213,
        "grad_norm": 2.1042613983154297,
        "learning_rate": 0.0001945066124629063,
        "epoch": 0.7394947151327662,
        "step": 5737
    },
    {
        "loss": 1.3966,
        "grad_norm": 3.494969367980957,
        "learning_rate": 0.000194486707528741,
        "epoch": 0.7396236143335911,
        "step": 5738
    },
    {
        "loss": 1.2719,
        "grad_norm": 2.480532646179199,
        "learning_rate": 0.00019446676761987157,
        "epoch": 0.7397525135344161,
        "step": 5739
    },
    {
        "loss": 1.7134,
        "grad_norm": 2.0116701126098633,
        "learning_rate": 0.00019444679274367886,
        "epoch": 0.739881412735241,
        "step": 5740
    },
    {
        "loss": 0.8631,
        "grad_norm": 2.5599288940429688,
        "learning_rate": 0.00019442678290755664,
        "epoch": 0.740010311936066,
        "step": 5741
    },
    {
        "loss": 2.5987,
        "grad_norm": 1.8755970001220703,
        "learning_rate": 0.00019440673811891164,
        "epoch": 0.740139211136891,
        "step": 5742
    },
    {
        "loss": 2.07,
        "grad_norm": 2.21966814994812,
        "learning_rate": 0.00019438665838516359,
        "epoch": 0.7402681103377159,
        "step": 5743
    },
    {
        "loss": 2.2221,
        "grad_norm": 2.265265464782715,
        "learning_rate": 0.00019436654371374505,
        "epoch": 0.7403970095385408,
        "step": 5744
    },
    {
        "loss": 2.0168,
        "grad_norm": 2.1514832973480225,
        "learning_rate": 0.00019434639411210154,
        "epoch": 0.7405259087393659,
        "step": 5745
    },
    {
        "loss": 1.5684,
        "grad_norm": 2.3652782440185547,
        "learning_rate": 0.00019432620958769159,
        "epoch": 0.7406548079401908,
        "step": 5746
    },
    {
        "loss": 1.6349,
        "grad_norm": 1.6536775827407837,
        "learning_rate": 0.00019430599014798658,
        "epoch": 0.7407837071410157,
        "step": 5747
    },
    {
        "loss": 1.8703,
        "grad_norm": 3.280775547027588,
        "learning_rate": 0.00019428573580047083,
        "epoch": 0.7409126063418406,
        "step": 5748
    },
    {
        "loss": 1.4694,
        "grad_norm": 4.091982364654541,
        "learning_rate": 0.00019426544655264154,
        "epoch": 0.7410415055426657,
        "step": 5749
    },
    {
        "loss": 2.5394,
        "grad_norm": 2.9146738052368164,
        "learning_rate": 0.00019424512241200892,
        "epoch": 0.7411704047434906,
        "step": 5750
    },
    {
        "loss": 1.6468,
        "grad_norm": 2.434004545211792,
        "learning_rate": 0.00019422476338609604,
        "epoch": 0.7412993039443155,
        "step": 5751
    },
    {
        "loss": 2.1442,
        "grad_norm": 2.263679027557373,
        "learning_rate": 0.00019420436948243888,
        "epoch": 0.7414282031451405,
        "step": 5752
    },
    {
        "loss": 1.314,
        "grad_norm": 3.0647010803222656,
        "learning_rate": 0.00019418394070858634,
        "epoch": 0.7415571023459655,
        "step": 5753
    },
    {
        "loss": 2.0629,
        "grad_norm": 2.322598457336426,
        "learning_rate": 0.00019416347707210024,
        "epoch": 0.7416860015467904,
        "step": 5754
    },
    {
        "loss": 2.3075,
        "grad_norm": 1.0681874752044678,
        "learning_rate": 0.0001941429785805553,
        "epoch": 0.7418149007476154,
        "step": 5755
    },
    {
        "loss": 1.9398,
        "grad_norm": 1.9430638551712036,
        "learning_rate": 0.0001941224452415391,
        "epoch": 0.7419437999484403,
        "step": 5756
    },
    {
        "loss": 2.0319,
        "grad_norm": 1.4139659404754639,
        "learning_rate": 0.0001941018770626522,
        "epoch": 0.7420726991492653,
        "step": 5757
    },
    {
        "loss": 1.8584,
        "grad_norm": 2.4848697185516357,
        "learning_rate": 0.00019408127405150797,
        "epoch": 0.7422015983500903,
        "step": 5758
    },
    {
        "loss": 0.9389,
        "grad_norm": 2.974560260772705,
        "learning_rate": 0.0001940606362157327,
        "epoch": 0.7423304975509152,
        "step": 5759
    },
    {
        "loss": 2.1951,
        "grad_norm": 2.7928669452667236,
        "learning_rate": 0.00019403996356296563,
        "epoch": 0.7424593967517401,
        "step": 5760
    },
    {
        "loss": 1.8494,
        "grad_norm": 1.8489866256713867,
        "learning_rate": 0.00019401925610085884,
        "epoch": 0.7425882959525651,
        "step": 5761
    },
    {
        "loss": 2.3965,
        "grad_norm": 2.2161033153533936,
        "learning_rate": 0.00019399851383707724,
        "epoch": 0.7427171951533901,
        "step": 5762
    },
    {
        "loss": 2.1205,
        "grad_norm": 1.3427057266235352,
        "learning_rate": 0.00019397773677929872,
        "epoch": 0.742846094354215,
        "step": 5763
    },
    {
        "loss": 1.4172,
        "grad_norm": 3.064944267272949,
        "learning_rate": 0.000193956924935214,
        "epoch": 0.7429749935550399,
        "step": 5764
    },
    {
        "loss": 2.0173,
        "grad_norm": 4.462638854980469,
        "learning_rate": 0.0001939360783125267,
        "epoch": 0.7431038927558649,
        "step": 5765
    },
    {
        "loss": 1.9706,
        "grad_norm": 2.171642780303955,
        "learning_rate": 0.00019391519691895325,
        "epoch": 0.7432327919566899,
        "step": 5766
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.9899803400039673,
        "learning_rate": 0.00019389428076222302,
        "epoch": 0.7433616911575148,
        "step": 5767
    },
    {
        "loss": 2.3302,
        "grad_norm": 1.69577157497406,
        "learning_rate": 0.00019387332985007825,
        "epoch": 0.7434905903583398,
        "step": 5768
    },
    {
        "loss": 1.5019,
        "grad_norm": 1.7891221046447754,
        "learning_rate": 0.000193852344190274,
        "epoch": 0.7436194895591647,
        "step": 5769
    },
    {
        "loss": 2.2285,
        "grad_norm": 2.133991241455078,
        "learning_rate": 0.00019383132379057817,
        "epoch": 0.7437483887599897,
        "step": 5770
    },
    {
        "loss": 2.4173,
        "grad_norm": 2.2482948303222656,
        "learning_rate": 0.00019381026865877168,
        "epoch": 0.7438772879608146,
        "step": 5771
    },
    {
        "loss": 1.4926,
        "grad_norm": 2.4895777702331543,
        "learning_rate": 0.00019378917880264807,
        "epoch": 0.7440061871616396,
        "step": 5772
    },
    {
        "loss": 2.0031,
        "grad_norm": 2.5889060497283936,
        "learning_rate": 0.00019376805423001388,
        "epoch": 0.7441350863624645,
        "step": 5773
    },
    {
        "loss": 2.1414,
        "grad_norm": 1.8627476692199707,
        "learning_rate": 0.00019374689494868846,
        "epoch": 0.7442639855632895,
        "step": 5774
    },
    {
        "loss": 1.2664,
        "grad_norm": 4.30040979385376,
        "learning_rate": 0.0001937257009665041,
        "epoch": 0.7443928847641145,
        "step": 5775
    },
    {
        "loss": 2.0737,
        "grad_norm": 2.2048468589782715,
        "learning_rate": 0.00019370447229130576,
        "epoch": 0.7445217839649394,
        "step": 5776
    },
    {
        "loss": 1.5803,
        "grad_norm": 2.592451333999634,
        "learning_rate": 0.00019368320893095137,
        "epoch": 0.7446506831657643,
        "step": 5777
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.1333494186401367,
        "learning_rate": 0.00019366191089331168,
        "epoch": 0.7447795823665894,
        "step": 5778
    },
    {
        "loss": 2.1755,
        "grad_norm": 2.3025591373443604,
        "learning_rate": 0.0001936405781862702,
        "epoch": 0.7449084815674143,
        "step": 5779
    },
    {
        "loss": 2.2117,
        "grad_norm": 2.020012140274048,
        "learning_rate": 0.0001936192108177234,
        "epoch": 0.7450373807682392,
        "step": 5780
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.5705139636993408,
        "learning_rate": 0.0001935978087955805,
        "epoch": 0.7451662799690641,
        "step": 5781
    },
    {
        "loss": 2.163,
        "grad_norm": 1.8564260005950928,
        "learning_rate": 0.00019357637212776352,
        "epoch": 0.7452951791698892,
        "step": 5782
    },
    {
        "loss": 2.448,
        "grad_norm": 1.3253504037857056,
        "learning_rate": 0.0001935549008222074,
        "epoch": 0.7454240783707141,
        "step": 5783
    },
    {
        "loss": 1.9569,
        "grad_norm": 1.4045045375823975,
        "learning_rate": 0.0001935333948868598,
        "epoch": 0.745552977571539,
        "step": 5784
    },
    {
        "loss": 1.7268,
        "grad_norm": 3.2829978466033936,
        "learning_rate": 0.00019351185432968127,
        "epoch": 0.745681876772364,
        "step": 5785
    },
    {
        "loss": 1.4154,
        "grad_norm": 2.257805347442627,
        "learning_rate": 0.00019349027915864517,
        "epoch": 0.745810775973189,
        "step": 5786
    },
    {
        "loss": 2.5727,
        "grad_norm": 2.131747245788574,
        "learning_rate": 0.00019346866938173756,
        "epoch": 0.7459396751740139,
        "step": 5787
    },
    {
        "loss": 2.1296,
        "grad_norm": 1.4140981435775757,
        "learning_rate": 0.00019344702500695752,
        "epoch": 0.7460685743748389,
        "step": 5788
    },
    {
        "loss": 2.0645,
        "grad_norm": 1.3984304666519165,
        "learning_rate": 0.00019342534604231673,
        "epoch": 0.7461974735756638,
        "step": 5789
    },
    {
        "loss": 1.9721,
        "grad_norm": 1.6689577102661133,
        "learning_rate": 0.00019340363249583983,
        "epoch": 0.7463263727764888,
        "step": 5790
    },
    {
        "loss": 1.7033,
        "grad_norm": 2.254108428955078,
        "learning_rate": 0.00019338188437556414,
        "epoch": 0.7464552719773138,
        "step": 5791
    },
    {
        "loss": 1.7208,
        "grad_norm": 3.331723928451538,
        "learning_rate": 0.00019336010168953988,
        "epoch": 0.7465841711781387,
        "step": 5792
    },
    {
        "loss": 1.7168,
        "grad_norm": 2.2523229122161865,
        "learning_rate": 0.00019333828444582994,
        "epoch": 0.7467130703789636,
        "step": 5793
    },
    {
        "loss": 2.1803,
        "grad_norm": 1.8454539775848389,
        "learning_rate": 0.00019331643265251014,
        "epoch": 0.7468419695797887,
        "step": 5794
    },
    {
        "loss": 1.9797,
        "grad_norm": 1.7337099313735962,
        "learning_rate": 0.00019329454631766902,
        "epoch": 0.7469708687806136,
        "step": 5795
    },
    {
        "loss": 1.9085,
        "grad_norm": 1.8599140644073486,
        "learning_rate": 0.00019327262544940787,
        "epoch": 0.7470997679814385,
        "step": 5796
    },
    {
        "loss": 1.7586,
        "grad_norm": 2.777467966079712,
        "learning_rate": 0.00019325067005584084,
        "epoch": 0.7472286671822634,
        "step": 5797
    },
    {
        "loss": 1.9742,
        "grad_norm": 2.070195436477661,
        "learning_rate": 0.00019322868014509483,
        "epoch": 0.7473575663830885,
        "step": 5798
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.905021071434021,
        "learning_rate": 0.00019320665572530947,
        "epoch": 0.7474864655839134,
        "step": 5799
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.714703321456909,
        "learning_rate": 0.00019318459680463726,
        "epoch": 0.7476153647847383,
        "step": 5800
    },
    {
        "loss": 2.0917,
        "grad_norm": 2.107940912246704,
        "learning_rate": 0.00019316250339124333,
        "epoch": 0.7477442639855633,
        "step": 5801
    },
    {
        "loss": 1.7626,
        "grad_norm": 2.325395345687866,
        "learning_rate": 0.00019314037549330576,
        "epoch": 0.7478731631863882,
        "step": 5802
    },
    {
        "loss": 1.6595,
        "grad_norm": 1.408963680267334,
        "learning_rate": 0.00019311821311901524,
        "epoch": 0.7480020623872132,
        "step": 5803
    },
    {
        "loss": 1.3021,
        "grad_norm": 2.4607486724853516,
        "learning_rate": 0.00019309601627657527,
        "epoch": 0.7481309615880382,
        "step": 5804
    },
    {
        "loss": 2.1323,
        "grad_norm": 1.8272207975387573,
        "learning_rate": 0.00019307378497420213,
        "epoch": 0.7482598607888631,
        "step": 5805
    },
    {
        "loss": 1.5452,
        "grad_norm": 1.3772703409194946,
        "learning_rate": 0.0001930515192201249,
        "epoch": 0.748388759989688,
        "step": 5806
    },
    {
        "loss": 1.5549,
        "grad_norm": 2.320941925048828,
        "learning_rate": 0.00019302921902258525,
        "epoch": 0.748517659190513,
        "step": 5807
    },
    {
        "loss": 2.1217,
        "grad_norm": 1.3853439092636108,
        "learning_rate": 0.00019300688438983777,
        "epoch": 0.748646558391338,
        "step": 5808
    },
    {
        "loss": 2.381,
        "grad_norm": 1.2642712593078613,
        "learning_rate": 0.00019298451533014975,
        "epoch": 0.7487754575921629,
        "step": 5809
    },
    {
        "loss": 1.4038,
        "grad_norm": 2.2690513134002686,
        "learning_rate": 0.00019296211185180115,
        "epoch": 0.7489043567929878,
        "step": 5810
    },
    {
        "loss": 2.0941,
        "grad_norm": 2.837911367416382,
        "learning_rate": 0.0001929396739630847,
        "epoch": 0.7490332559938129,
        "step": 5811
    },
    {
        "loss": 1.6071,
        "grad_norm": 2.475541353225708,
        "learning_rate": 0.00019291720167230602,
        "epoch": 0.7491621551946378,
        "step": 5812
    },
    {
        "loss": 1.5687,
        "grad_norm": 2.332340717315674,
        "learning_rate": 0.00019289469498778318,
        "epoch": 0.7492910543954627,
        "step": 5813
    },
    {
        "loss": 2.4313,
        "grad_norm": 1.9453027248382568,
        "learning_rate": 0.00019287215391784726,
        "epoch": 0.7494199535962877,
        "step": 5814
    },
    {
        "loss": 1.4003,
        "grad_norm": 2.56777024269104,
        "learning_rate": 0.00019284957847084188,
        "epoch": 0.7495488527971127,
        "step": 5815
    },
    {
        "loss": 1.8345,
        "grad_norm": 1.7187323570251465,
        "learning_rate": 0.00019282696865512345,
        "epoch": 0.7496777519979376,
        "step": 5816
    },
    {
        "loss": 2.2315,
        "grad_norm": 1.8512048721313477,
        "learning_rate": 0.0001928043244790611,
        "epoch": 0.7498066511987626,
        "step": 5817
    },
    {
        "loss": 1.1465,
        "grad_norm": 2.876582145690918,
        "learning_rate": 0.00019278164595103672,
        "epoch": 0.7499355503995875,
        "step": 5818
    },
    {
        "loss": 2.0069,
        "grad_norm": 2.1237564086914062,
        "learning_rate": 0.00019275893307944483,
        "epoch": 0.7500644496004125,
        "step": 5819
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.0567307472229004,
        "learning_rate": 0.00019273618587269272,
        "epoch": 0.7501933488012374,
        "step": 5820
    },
    {
        "loss": 0.931,
        "grad_norm": 2.7218668460845947,
        "learning_rate": 0.0001927134043392004,
        "epoch": 0.7503222480020624,
        "step": 5821
    },
    {
        "loss": 1.9202,
        "grad_norm": 2.2399535179138184,
        "learning_rate": 0.00019269058848740055,
        "epoch": 0.7504511472028873,
        "step": 5822
    },
    {
        "loss": 1.8298,
        "grad_norm": 2.919912815093994,
        "learning_rate": 0.0001926677383257385,
        "epoch": 0.7505800464037123,
        "step": 5823
    },
    {
        "loss": 2.2436,
        "grad_norm": 2.2847349643707275,
        "learning_rate": 0.00019264485386267247,
        "epoch": 0.7507089456045373,
        "step": 5824
    },
    {
        "loss": 1.3487,
        "grad_norm": 1.9080989360809326,
        "learning_rate": 0.00019262193510667314,
        "epoch": 0.7508378448053622,
        "step": 5825
    },
    {
        "loss": 2.6445,
        "grad_norm": 1.6266790628433228,
        "learning_rate": 0.00019259898206622412,
        "epoch": 0.7509667440061871,
        "step": 5826
    },
    {
        "loss": 1.8214,
        "grad_norm": 2.1487069129943848,
        "learning_rate": 0.00019257599474982145,
        "epoch": 0.7510956432070122,
        "step": 5827
    },
    {
        "loss": 2.2946,
        "grad_norm": 1.3436665534973145,
        "learning_rate": 0.00019255297316597404,
        "epoch": 0.7512245424078371,
        "step": 5828
    },
    {
        "loss": 2.4048,
        "grad_norm": 1.8779785633087158,
        "learning_rate": 0.00019252991732320352,
        "epoch": 0.751353441608662,
        "step": 5829
    },
    {
        "loss": 1.0176,
        "grad_norm": 2.205493450164795,
        "learning_rate": 0.00019250682723004402,
        "epoch": 0.751482340809487,
        "step": 5830
    },
    {
        "loss": 1.4626,
        "grad_norm": 2.012071371078491,
        "learning_rate": 0.00019248370289504248,
        "epoch": 0.751611240010312,
        "step": 5831
    },
    {
        "loss": 2.2693,
        "grad_norm": 1.898830533027649,
        "learning_rate": 0.0001924605443267585,
        "epoch": 0.7517401392111369,
        "step": 5832
    },
    {
        "loss": 1.8488,
        "grad_norm": 1.3677903413772583,
        "learning_rate": 0.0001924373515337643,
        "epoch": 0.7518690384119618,
        "step": 5833
    },
    {
        "loss": 2.378,
        "grad_norm": 1.9848459959030151,
        "learning_rate": 0.00019241412452464482,
        "epoch": 0.7519979376127868,
        "step": 5834
    },
    {
        "loss": 1.7393,
        "grad_norm": 2.263951539993286,
        "learning_rate": 0.0001923908633079977,
        "epoch": 0.7521268368136118,
        "step": 5835
    },
    {
        "loss": 1.3546,
        "grad_norm": 1.8928024768829346,
        "learning_rate": 0.0001923675678924331,
        "epoch": 0.7522557360144367,
        "step": 5836
    },
    {
        "loss": 1.7859,
        "grad_norm": 2.0012009143829346,
        "learning_rate": 0.00019234423828657393,
        "epoch": 0.7523846352152617,
        "step": 5837
    },
    {
        "loss": 2.0237,
        "grad_norm": 1.3625799417495728,
        "learning_rate": 0.00019232087449905585,
        "epoch": 0.7525135344160866,
        "step": 5838
    },
    {
        "loss": 1.8599,
        "grad_norm": 2.110102653503418,
        "learning_rate": 0.00019229747653852703,
        "epoch": 0.7526424336169115,
        "step": 5839
    },
    {
        "loss": 1.9544,
        "grad_norm": 2.383470058441162,
        "learning_rate": 0.0001922740444136483,
        "epoch": 0.7527713328177366,
        "step": 5840
    },
    {
        "loss": 1.9914,
        "grad_norm": 1.7094038724899292,
        "learning_rate": 0.00019225057813309322,
        "epoch": 0.7529002320185615,
        "step": 5841
    },
    {
        "loss": 1.8535,
        "grad_norm": 2.385538339614868,
        "learning_rate": 0.0001922270777055479,
        "epoch": 0.7530291312193864,
        "step": 5842
    },
    {
        "loss": 1.4462,
        "grad_norm": 2.24631929397583,
        "learning_rate": 0.00019220354313971121,
        "epoch": 0.7531580304202113,
        "step": 5843
    },
    {
        "loss": 2.6131,
        "grad_norm": 1.441999077796936,
        "learning_rate": 0.00019217997444429446,
        "epoch": 0.7532869296210364,
        "step": 5844
    },
    {
        "loss": 1.7908,
        "grad_norm": 1.9543159008026123,
        "learning_rate": 0.0001921563716280218,
        "epoch": 0.7534158288218613,
        "step": 5845
    },
    {
        "loss": 1.1645,
        "grad_norm": 2.6779534816741943,
        "learning_rate": 0.00019213273469962996,
        "epoch": 0.7535447280226862,
        "step": 5846
    },
    {
        "loss": 2.028,
        "grad_norm": 1.7916828393936157,
        "learning_rate": 0.0001921090636678682,
        "epoch": 0.7536736272235112,
        "step": 5847
    },
    {
        "loss": 2.4223,
        "grad_norm": 1.9808800220489502,
        "learning_rate": 0.00019208535854149843,
        "epoch": 0.7538025264243362,
        "step": 5848
    },
    {
        "loss": 2.153,
        "grad_norm": 1.7190924882888794,
        "learning_rate": 0.00019206161932929533,
        "epoch": 0.7539314256251611,
        "step": 5849
    },
    {
        "loss": 1.8148,
        "grad_norm": 1.4465924501419067,
        "learning_rate": 0.00019203784604004595,
        "epoch": 0.7540603248259861,
        "step": 5850
    },
    {
        "loss": 1.6572,
        "grad_norm": 2.22580623626709,
        "learning_rate": 0.0001920140386825502,
        "epoch": 0.754189224026811,
        "step": 5851
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.7574095726013184,
        "learning_rate": 0.0001919901972656204,
        "epoch": 0.754318123227636,
        "step": 5852
    },
    {
        "loss": 2.1657,
        "grad_norm": 1.5983093976974487,
        "learning_rate": 0.00019196632179808163,
        "epoch": 0.754447022428461,
        "step": 5853
    },
    {
        "loss": 1.757,
        "grad_norm": 2.0398149490356445,
        "learning_rate": 0.00019194241228877147,
        "epoch": 0.7545759216292859,
        "step": 5854
    },
    {
        "loss": 1.4917,
        "grad_norm": 2.4091475009918213,
        "learning_rate": 0.0001919184687465401,
        "epoch": 0.7547048208301108,
        "step": 5855
    },
    {
        "loss": 1.9398,
        "grad_norm": 1.237312912940979,
        "learning_rate": 0.00019189449118025046,
        "epoch": 0.7548337200309359,
        "step": 5856
    },
    {
        "loss": 1.5127,
        "grad_norm": 2.128197193145752,
        "learning_rate": 0.00019187047959877786,
        "epoch": 0.7549626192317608,
        "step": 5857
    },
    {
        "loss": 1.4743,
        "grad_norm": 2.177164077758789,
        "learning_rate": 0.00019184643401101034,
        "epoch": 0.7550915184325857,
        "step": 5858
    },
    {
        "loss": 1.9869,
        "grad_norm": 1.7021443843841553,
        "learning_rate": 0.00019182235442584845,
        "epoch": 0.7552204176334106,
        "step": 5859
    },
    {
        "loss": 2.2239,
        "grad_norm": 1.8966267108917236,
        "learning_rate": 0.0001917982408522054,
        "epoch": 0.7553493168342357,
        "step": 5860
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.150763988494873,
        "learning_rate": 0.00019177409329900692,
        "epoch": 0.7554782160350606,
        "step": 5861
    },
    {
        "loss": 1.0625,
        "grad_norm": 2.9901740550994873,
        "learning_rate": 0.0001917499117751913,
        "epoch": 0.7556071152358855,
        "step": 5862
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.8644118309020996,
        "learning_rate": 0.00019172569628970958,
        "epoch": 0.7557360144367105,
        "step": 5863
    },
    {
        "loss": 2.5361,
        "grad_norm": 1.1969417333602905,
        "learning_rate": 0.00019170144685152514,
        "epoch": 0.7558649136375355,
        "step": 5864
    },
    {
        "loss": 1.9635,
        "grad_norm": 1.5815340280532837,
        "learning_rate": 0.00019167716346961405,
        "epoch": 0.7559938128383604,
        "step": 5865
    },
    {
        "loss": 2.629,
        "grad_norm": 2.172200918197632,
        "learning_rate": 0.0001916528461529649,
        "epoch": 0.7561227120391854,
        "step": 5866
    },
    {
        "loss": 1.3012,
        "grad_norm": 2.2333381175994873,
        "learning_rate": 0.00019162849491057883,
        "epoch": 0.7562516112400103,
        "step": 5867
    },
    {
        "loss": 1.9976,
        "grad_norm": 1.499567985534668,
        "learning_rate": 0.00019160410975146964,
        "epoch": 0.7563805104408353,
        "step": 5868
    },
    {
        "loss": 1.9967,
        "grad_norm": 2.1838040351867676,
        "learning_rate": 0.00019157969068466359,
        "epoch": 0.7565094096416602,
        "step": 5869
    },
    {
        "loss": 2.0379,
        "grad_norm": 1.5235774517059326,
        "learning_rate": 0.0001915552377191995,
        "epoch": 0.7566383088424852,
        "step": 5870
    },
    {
        "loss": 2.7049,
        "grad_norm": 1.3823230266571045,
        "learning_rate": 0.00019153075086412872,
        "epoch": 0.7567672080433101,
        "step": 5871
    },
    {
        "loss": 1.2797,
        "grad_norm": 1.9961127042770386,
        "learning_rate": 0.00019150623012851526,
        "epoch": 0.7568961072441351,
        "step": 5872
    },
    {
        "loss": 2.1997,
        "grad_norm": 2.1683952808380127,
        "learning_rate": 0.00019148167552143552,
        "epoch": 0.7570250064449601,
        "step": 5873
    },
    {
        "loss": 1.8573,
        "grad_norm": 1.4817836284637451,
        "learning_rate": 0.00019145708705197855,
        "epoch": 0.757153905645785,
        "step": 5874
    },
    {
        "loss": 2.1744,
        "grad_norm": 2.0353548526763916,
        "learning_rate": 0.00019143246472924581,
        "epoch": 0.7572828048466099,
        "step": 5875
    },
    {
        "loss": 1.0813,
        "grad_norm": 2.912593364715576,
        "learning_rate": 0.00019140780856235145,
        "epoch": 0.7574117040474349,
        "step": 5876
    },
    {
        "loss": 2.0274,
        "grad_norm": 1.812699317932129,
        "learning_rate": 0.00019138311856042203,
        "epoch": 0.7575406032482599,
        "step": 5877
    },
    {
        "loss": 1.702,
        "grad_norm": 2.650729179382324,
        "learning_rate": 0.0001913583947325967,
        "epoch": 0.7576695024490848,
        "step": 5878
    },
    {
        "loss": 1.1338,
        "grad_norm": 2.8154354095458984,
        "learning_rate": 0.00019133363708802707,
        "epoch": 0.7577984016499097,
        "step": 5879
    },
    {
        "loss": 2.1574,
        "grad_norm": 2.2016539573669434,
        "learning_rate": 0.00019130884563587733,
        "epoch": 0.7579273008507347,
        "step": 5880
    },
    {
        "loss": 2.5334,
        "grad_norm": 1.7698184251785278,
        "learning_rate": 0.0001912840203853241,
        "epoch": 0.7580562000515597,
        "step": 5881
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.925602674484253,
        "learning_rate": 0.00019125916134555665,
        "epoch": 0.7581850992523846,
        "step": 5882
    },
    {
        "loss": 1.0061,
        "grad_norm": 4.22125768661499,
        "learning_rate": 0.0001912342685257766,
        "epoch": 0.7583139984532096,
        "step": 5883
    },
    {
        "loss": 1.4356,
        "grad_norm": 2.2101950645446777,
        "learning_rate": 0.0001912093419351982,
        "epoch": 0.7584428976540345,
        "step": 5884
    },
    {
        "loss": 1.257,
        "grad_norm": 2.0593066215515137,
        "learning_rate": 0.00019118438158304807,
        "epoch": 0.7585717968548595,
        "step": 5885
    },
    {
        "loss": 1.8606,
        "grad_norm": 1.4599213600158691,
        "learning_rate": 0.00019115938747856545,
        "epoch": 0.7587006960556845,
        "step": 5886
    },
    {
        "loss": 2.0403,
        "grad_norm": 2.760983943939209,
        "learning_rate": 0.00019113435963100206,
        "epoch": 0.7588295952565094,
        "step": 5887
    },
    {
        "loss": 1.847,
        "grad_norm": 2.366114854812622,
        "learning_rate": 0.000191109298049622,
        "epoch": 0.7589584944573343,
        "step": 5888
    },
    {
        "loss": 1.8779,
        "grad_norm": 2.613992929458618,
        "learning_rate": 0.000191084202743702,
        "epoch": 0.7590873936581594,
        "step": 5889
    },
    {
        "loss": 2.5598,
        "grad_norm": 2.7432138919830322,
        "learning_rate": 0.00019105907372253121,
        "epoch": 0.7592162928589843,
        "step": 5890
    },
    {
        "loss": 1.6531,
        "grad_norm": 2.467137336730957,
        "learning_rate": 0.00019103391099541123,
        "epoch": 0.7593451920598092,
        "step": 5891
    },
    {
        "loss": 1.6257,
        "grad_norm": 1.6177386045455933,
        "learning_rate": 0.0001910087145716561,
        "epoch": 0.7594740912606341,
        "step": 5892
    },
    {
        "loss": 1.854,
        "grad_norm": 2.1308252811431885,
        "learning_rate": 0.00019098348446059248,
        "epoch": 0.7596029904614592,
        "step": 5893
    },
    {
        "loss": 2.232,
        "grad_norm": 1.6529901027679443,
        "learning_rate": 0.00019095822067155942,
        "epoch": 0.7597318896622841,
        "step": 5894
    },
    {
        "loss": 2.3082,
        "grad_norm": 1.724369764328003,
        "learning_rate": 0.00019093292321390842,
        "epoch": 0.759860788863109,
        "step": 5895
    },
    {
        "loss": 1.7875,
        "grad_norm": 1.9140305519104004,
        "learning_rate": 0.00019090759209700338,
        "epoch": 0.759989688063934,
        "step": 5896
    },
    {
        "loss": 1.688,
        "grad_norm": 2.1933915615081787,
        "learning_rate": 0.00019088222733022083,
        "epoch": 0.760118587264759,
        "step": 5897
    },
    {
        "loss": 2.0943,
        "grad_norm": 2.130903720855713,
        "learning_rate": 0.00019085682892294964,
        "epoch": 0.7602474864655839,
        "step": 5898
    },
    {
        "loss": 1.1153,
        "grad_norm": 2.646995782852173,
        "learning_rate": 0.00019083139688459109,
        "epoch": 0.7603763856664089,
        "step": 5899
    },
    {
        "loss": 2.1119,
        "grad_norm": 2.1058335304260254,
        "learning_rate": 0.000190805931224559,
        "epoch": 0.7605052848672338,
        "step": 5900
    },
    {
        "loss": 2.1507,
        "grad_norm": 2.522416591644287,
        "learning_rate": 0.0001907804319522797,
        "epoch": 0.7606341840680588,
        "step": 5901
    },
    {
        "loss": 2.5973,
        "grad_norm": 1.5704188346862793,
        "learning_rate": 0.00019075489907719172,
        "epoch": 0.7607630832688838,
        "step": 5902
    },
    {
        "loss": 1.2758,
        "grad_norm": 2.2979447841644287,
        "learning_rate": 0.00019072933260874626,
        "epoch": 0.7608919824697087,
        "step": 5903
    },
    {
        "loss": 1.6916,
        "grad_norm": 2.863968849182129,
        "learning_rate": 0.00019070373255640686,
        "epoch": 0.7610208816705336,
        "step": 5904
    },
    {
        "loss": 1.7899,
        "grad_norm": 2.3468921184539795,
        "learning_rate": 0.00019067809892964947,
        "epoch": 0.7611497808713586,
        "step": 5905
    },
    {
        "loss": 2.1332,
        "grad_norm": 2.1079232692718506,
        "learning_rate": 0.00019065243173796255,
        "epoch": 0.7612786800721836,
        "step": 5906
    },
    {
        "loss": 2.0081,
        "grad_norm": 1.3619043827056885,
        "learning_rate": 0.0001906267309908469,
        "epoch": 0.7614075792730085,
        "step": 5907
    },
    {
        "loss": 2.0073,
        "grad_norm": 1.9944974184036255,
        "learning_rate": 0.00019060099669781578,
        "epoch": 0.7615364784738334,
        "step": 5908
    },
    {
        "loss": 1.2109,
        "grad_norm": 2.521376132965088,
        "learning_rate": 0.00019057522886839484,
        "epoch": 0.7616653776746585,
        "step": 5909
    },
    {
        "loss": 1.7118,
        "grad_norm": 3.173194408416748,
        "learning_rate": 0.00019054942751212215,
        "epoch": 0.7617942768754834,
        "step": 5910
    },
    {
        "loss": 1.3099,
        "grad_norm": 1.8148155212402344,
        "learning_rate": 0.0001905235926385483,
        "epoch": 0.7619231760763083,
        "step": 5911
    },
    {
        "loss": 2.0816,
        "grad_norm": 2.180680274963379,
        "learning_rate": 0.00019049772425723608,
        "epoch": 0.7620520752771333,
        "step": 5912
    },
    {
        "loss": 2.0089,
        "grad_norm": 2.500981092453003,
        "learning_rate": 0.00019047182237776083,
        "epoch": 0.7621809744779582,
        "step": 5913
    },
    {
        "loss": 2.0245,
        "grad_norm": 1.778838872909546,
        "learning_rate": 0.00019044588700971033,
        "epoch": 0.7623098736787832,
        "step": 5914
    },
    {
        "loss": 1.6634,
        "grad_norm": 2.2478389739990234,
        "learning_rate": 0.00019041991816268458,
        "epoch": 0.7624387728796082,
        "step": 5915
    },
    {
        "loss": 1.9658,
        "grad_norm": 1.685780644416809,
        "learning_rate": 0.00019039391584629614,
        "epoch": 0.7625676720804331,
        "step": 5916
    },
    {
        "loss": 1.3043,
        "grad_norm": 1.9056140184402466,
        "learning_rate": 0.0001903678800701698,
        "epoch": 0.762696571281258,
        "step": 5917
    },
    {
        "loss": 1.6587,
        "grad_norm": 1.638067603111267,
        "learning_rate": 0.00019034181084394297,
        "epoch": 0.762825470482083,
        "step": 5918
    },
    {
        "loss": 1.3251,
        "grad_norm": 2.8661937713623047,
        "learning_rate": 0.00019031570817726516,
        "epoch": 0.762954369682908,
        "step": 5919
    },
    {
        "loss": 2.1153,
        "grad_norm": 2.528437376022339,
        "learning_rate": 0.00019028957207979844,
        "epoch": 0.7630832688837329,
        "step": 5920
    },
    {
        "loss": 1.9307,
        "grad_norm": 2.340027332305908,
        "learning_rate": 0.00019026340256121728,
        "epoch": 0.7632121680845578,
        "step": 5921
    },
    {
        "loss": 2.172,
        "grad_norm": 1.7773994207382202,
        "learning_rate": 0.00019023719963120838,
        "epoch": 0.7633410672853829,
        "step": 5922
    },
    {
        "loss": 2.0594,
        "grad_norm": 1.8082119226455688,
        "learning_rate": 0.00019021096329947085,
        "epoch": 0.7634699664862078,
        "step": 5923
    },
    {
        "loss": 1.5956,
        "grad_norm": 2.301619052886963,
        "learning_rate": 0.00019018469357571624,
        "epoch": 0.7635988656870327,
        "step": 5924
    },
    {
        "loss": 2.0579,
        "grad_norm": 1.2572486400604248,
        "learning_rate": 0.00019015839046966846,
        "epoch": 0.7637277648878577,
        "step": 5925
    },
    {
        "loss": 2.4287,
        "grad_norm": 1.6526784896850586,
        "learning_rate": 0.00019013205399106363,
        "epoch": 0.7638566640886827,
        "step": 5926
    },
    {
        "loss": 1.6149,
        "grad_norm": 2.088555097579956,
        "learning_rate": 0.00019010568414965038,
        "epoch": 0.7639855632895076,
        "step": 5927
    },
    {
        "loss": 2.1836,
        "grad_norm": 1.9542062282562256,
        "learning_rate": 0.00019007928095518962,
        "epoch": 0.7641144624903325,
        "step": 5928
    },
    {
        "loss": 2.178,
        "grad_norm": 2.3420755863189697,
        "learning_rate": 0.00019005284441745465,
        "epoch": 0.7642433616911575,
        "step": 5929
    },
    {
        "loss": 2.3962,
        "grad_norm": 2.244800329208374,
        "learning_rate": 0.000190026374546231,
        "epoch": 0.7643722608919825,
        "step": 5930
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.7838891744613647,
        "learning_rate": 0.00018999987135131675,
        "epoch": 0.7645011600928074,
        "step": 5931
    },
    {
        "loss": 1.9355,
        "grad_norm": 1.6529946327209473,
        "learning_rate": 0.00018997333484252206,
        "epoch": 0.7646300592936324,
        "step": 5932
    },
    {
        "loss": 1.716,
        "grad_norm": 1.761147379875183,
        "learning_rate": 0.00018994676502966958,
        "epoch": 0.7647589584944573,
        "step": 5933
    },
    {
        "loss": 2.3353,
        "grad_norm": 1.8346750736236572,
        "learning_rate": 0.00018992016192259423,
        "epoch": 0.7648878576952823,
        "step": 5934
    },
    {
        "loss": 0.7703,
        "grad_norm": 3.5033841133117676,
        "learning_rate": 0.0001898935255311434,
        "epoch": 0.7650167568961073,
        "step": 5935
    },
    {
        "loss": 2.0271,
        "grad_norm": 1.1208696365356445,
        "learning_rate": 0.00018986685586517652,
        "epoch": 0.7651456560969322,
        "step": 5936
    },
    {
        "loss": 2.2243,
        "grad_norm": 1.8799283504486084,
        "learning_rate": 0.00018984015293456552,
        "epoch": 0.7652745552977571,
        "step": 5937
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.122753620147705,
        "learning_rate": 0.00018981341674919477,
        "epoch": 0.7654034544985822,
        "step": 5938
    },
    {
        "loss": 1.5724,
        "grad_norm": 1.8494211435317993,
        "learning_rate": 0.0001897866473189606,
        "epoch": 0.7655323536994071,
        "step": 5939
    },
    {
        "loss": 1.8555,
        "grad_norm": 2.879648447036743,
        "learning_rate": 0.00018975984465377193,
        "epoch": 0.765661252900232,
        "step": 5940
    },
    {
        "loss": 2.1256,
        "grad_norm": 2.148677349090576,
        "learning_rate": 0.00018973300876354992,
        "epoch": 0.7657901521010569,
        "step": 5941
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.9254122972488403,
        "learning_rate": 0.00018970613965822795,
        "epoch": 0.765919051301882,
        "step": 5942
    },
    {
        "loss": 1.8055,
        "grad_norm": 2.9485082626342773,
        "learning_rate": 0.00018967923734775175,
        "epoch": 0.7660479505027069,
        "step": 5943
    },
    {
        "loss": 2.1672,
        "grad_norm": 2.608494758605957,
        "learning_rate": 0.00018965230184207936,
        "epoch": 0.7661768497035318,
        "step": 5944
    },
    {
        "loss": 1.3882,
        "grad_norm": 3.880763053894043,
        "learning_rate": 0.0001896253331511811,
        "epoch": 0.7663057489043568,
        "step": 5945
    },
    {
        "loss": 1.1956,
        "grad_norm": 2.0011725425720215,
        "learning_rate": 0.00018959833128503952,
        "epoch": 0.7664346481051817,
        "step": 5946
    },
    {
        "loss": 1.3449,
        "grad_norm": 2.697441339492798,
        "learning_rate": 0.00018957129625364955,
        "epoch": 0.7665635473060067,
        "step": 5947
    },
    {
        "loss": 2.1254,
        "grad_norm": 2.5903708934783936,
        "learning_rate": 0.00018954422806701827,
        "epoch": 0.7666924465068317,
        "step": 5948
    },
    {
        "loss": 1.62,
        "grad_norm": 2.3328864574432373,
        "learning_rate": 0.00018951712673516514,
        "epoch": 0.7668213457076566,
        "step": 5949
    },
    {
        "loss": 2.1142,
        "grad_norm": 1.6528071165084839,
        "learning_rate": 0.0001894899922681218,
        "epoch": 0.7669502449084815,
        "step": 5950
    },
    {
        "loss": 1.718,
        "grad_norm": 3.0116007328033447,
        "learning_rate": 0.00018946282467593225,
        "epoch": 0.7670791441093066,
        "step": 5951
    },
    {
        "loss": 1.938,
        "grad_norm": 1.852110505104065,
        "learning_rate": 0.00018943562396865268,
        "epoch": 0.7672080433101315,
        "step": 5952
    },
    {
        "loss": 1.6305,
        "grad_norm": 1.921129822731018,
        "learning_rate": 0.00018940839015635158,
        "epoch": 0.7673369425109564,
        "step": 5953
    },
    {
        "loss": 2.3253,
        "grad_norm": 1.835094690322876,
        "learning_rate": 0.0001893811232491096,
        "epoch": 0.7674658417117813,
        "step": 5954
    },
    {
        "loss": 1.4928,
        "grad_norm": 2.5139987468719482,
        "learning_rate": 0.00018935382325701986,
        "epoch": 0.7675947409126064,
        "step": 5955
    },
    {
        "loss": 1.8175,
        "grad_norm": 1.6877169609069824,
        "learning_rate": 0.00018932649019018746,
        "epoch": 0.7677236401134313,
        "step": 5956
    },
    {
        "loss": 1.2033,
        "grad_norm": 2.335378408432007,
        "learning_rate": 0.0001892991240587299,
        "epoch": 0.7678525393142562,
        "step": 5957
    },
    {
        "loss": 1.8272,
        "grad_norm": 2.52439546585083,
        "learning_rate": 0.0001892717248727769,
        "epoch": 0.7679814385150812,
        "step": 5958
    },
    {
        "loss": 1.6503,
        "grad_norm": 1.531506896018982,
        "learning_rate": 0.00018924429264247038,
        "epoch": 0.7681103377159062,
        "step": 5959
    },
    {
        "loss": 2.1844,
        "grad_norm": 1.522202730178833,
        "learning_rate": 0.00018921682737796448,
        "epoch": 0.7682392369167311,
        "step": 5960
    },
    {
        "loss": 2.2188,
        "grad_norm": 1.1385602951049805,
        "learning_rate": 0.0001891893290894257,
        "epoch": 0.768368136117556,
        "step": 5961
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.371920108795166,
        "learning_rate": 0.00018916179778703256,
        "epoch": 0.768497035318381,
        "step": 5962
    },
    {
        "loss": 2.186,
        "grad_norm": 2.169036626815796,
        "learning_rate": 0.0001891342334809759,
        "epoch": 0.768625934519206,
        "step": 5963
    },
    {
        "loss": 1.9082,
        "grad_norm": 2.337217092514038,
        "learning_rate": 0.0001891066361814589,
        "epoch": 0.768754833720031,
        "step": 5964
    },
    {
        "loss": 1.8461,
        "grad_norm": 2.4560859203338623,
        "learning_rate": 0.00018907900589869672,
        "epoch": 0.7688837329208559,
        "step": 5965
    },
    {
        "loss": 2.2982,
        "grad_norm": 1.5476428270339966,
        "learning_rate": 0.0001890513426429169,
        "epoch": 0.7690126321216808,
        "step": 5966
    },
    {
        "loss": 1.82,
        "grad_norm": 2.5522212982177734,
        "learning_rate": 0.00018902364642435905,
        "epoch": 0.7691415313225058,
        "step": 5967
    },
    {
        "loss": 1.8487,
        "grad_norm": 2.4195005893707275,
        "learning_rate": 0.00018899591725327511,
        "epoch": 0.7692704305233308,
        "step": 5968
    },
    {
        "loss": 2.1417,
        "grad_norm": 1.6878303289413452,
        "learning_rate": 0.0001889681551399292,
        "epoch": 0.7693993297241557,
        "step": 5969
    },
    {
        "loss": 2.126,
        "grad_norm": 2.8474674224853516,
        "learning_rate": 0.00018894036009459747,
        "epoch": 0.7695282289249806,
        "step": 5970
    },
    {
        "loss": 2.0535,
        "grad_norm": 2.1929025650024414,
        "learning_rate": 0.00018891253212756853,
        "epoch": 0.7696571281258057,
        "step": 5971
    },
    {
        "loss": 2.1452,
        "grad_norm": 1.6014423370361328,
        "learning_rate": 0.000188884671249143,
        "epoch": 0.7697860273266306,
        "step": 5972
    },
    {
        "loss": 2.4231,
        "grad_norm": 1.4029347896575928,
        "learning_rate": 0.00018885677746963364,
        "epoch": 0.7699149265274555,
        "step": 5973
    },
    {
        "loss": 2.3274,
        "grad_norm": 1.7199712991714478,
        "learning_rate": 0.00018882885079936557,
        "epoch": 0.7700438257282805,
        "step": 5974
    },
    {
        "loss": 2.1858,
        "grad_norm": 1.7321356534957886,
        "learning_rate": 0.00018880089124867592,
        "epoch": 0.7701727249291055,
        "step": 5975
    },
    {
        "loss": 1.4426,
        "grad_norm": 2.494311571121216,
        "learning_rate": 0.00018877289882791404,
        "epoch": 0.7703016241299304,
        "step": 5976
    },
    {
        "loss": 2.2411,
        "grad_norm": 2.4204294681549072,
        "learning_rate": 0.0001887448735474415,
        "epoch": 0.7704305233307553,
        "step": 5977
    },
    {
        "loss": 2.5198,
        "grad_norm": 1.2799636125564575,
        "learning_rate": 0.00018871681541763195,
        "epoch": 0.7705594225315803,
        "step": 5978
    },
    {
        "loss": 2.0192,
        "grad_norm": 2.1013026237487793,
        "learning_rate": 0.00018868872444887126,
        "epoch": 0.7706883217324053,
        "step": 5979
    },
    {
        "loss": 2.4286,
        "grad_norm": 1.4551550149917603,
        "learning_rate": 0.00018866060065155744,
        "epoch": 0.7708172209332302,
        "step": 5980
    },
    {
        "loss": 1.9919,
        "grad_norm": 1.6821262836456299,
        "learning_rate": 0.00018863244403610066,
        "epoch": 0.7709461201340552,
        "step": 5981
    },
    {
        "loss": 2.2363,
        "grad_norm": 1.5382215976715088,
        "learning_rate": 0.0001886042546129232,
        "epoch": 0.7710750193348801,
        "step": 5982
    },
    {
        "loss": 2.1798,
        "grad_norm": 2.0960772037506104,
        "learning_rate": 0.00018857603239245952,
        "epoch": 0.771203918535705,
        "step": 5983
    },
    {
        "loss": 1.7401,
        "grad_norm": 2.2563834190368652,
        "learning_rate": 0.00018854777738515623,
        "epoch": 0.7713328177365301,
        "step": 5984
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.2640507221221924,
        "learning_rate": 0.00018851948960147197,
        "epoch": 0.771461716937355,
        "step": 5985
    },
    {
        "loss": 1.3969,
        "grad_norm": 3.7645199298858643,
        "learning_rate": 0.00018849116905187772,
        "epoch": 0.7715906161381799,
        "step": 5986
    },
    {
        "loss": 2.4939,
        "grad_norm": 1.1347084045410156,
        "learning_rate": 0.00018846281574685638,
        "epoch": 0.7717195153390048,
        "step": 5987
    },
    {
        "loss": 1.6341,
        "grad_norm": 2.4392800331115723,
        "learning_rate": 0.00018843442969690302,
        "epoch": 0.7718484145398299,
        "step": 5988
    },
    {
        "loss": 2.1627,
        "grad_norm": 1.515891671180725,
        "learning_rate": 0.00018840601091252505,
        "epoch": 0.7719773137406548,
        "step": 5989
    },
    {
        "loss": 1.7283,
        "grad_norm": 1.6251298189163208,
        "learning_rate": 0.00018837755940424164,
        "epoch": 0.7721062129414797,
        "step": 5990
    },
    {
        "loss": 1.1348,
        "grad_norm": 2.864414691925049,
        "learning_rate": 0.00018834907518258434,
        "epoch": 0.7722351121423047,
        "step": 5991
    },
    {
        "loss": 2.2702,
        "grad_norm": 2.0208845138549805,
        "learning_rate": 0.0001883205582580967,
        "epoch": 0.7723640113431297,
        "step": 5992
    },
    {
        "loss": 1.9502,
        "grad_norm": 2.5797760486602783,
        "learning_rate": 0.0001882920086413344,
        "epoch": 0.7724929105439546,
        "step": 5993
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.6752521991729736,
        "learning_rate": 0.00018826342634286516,
        "epoch": 0.7726218097447796,
        "step": 5994
    },
    {
        "loss": 2.2579,
        "grad_norm": 2.3242032527923584,
        "learning_rate": 0.00018823481137326896,
        "epoch": 0.7727507089456045,
        "step": 5995
    },
    {
        "loss": 1.9649,
        "grad_norm": 2.752134323120117,
        "learning_rate": 0.00018820616374313768,
        "epoch": 0.7728796081464295,
        "step": 5996
    },
    {
        "loss": 1.9039,
        "grad_norm": 2.4913809299468994,
        "learning_rate": 0.00018817748346307538,
        "epoch": 0.7730085073472545,
        "step": 5997
    },
    {
        "loss": 2.2347,
        "grad_norm": 2.2697582244873047,
        "learning_rate": 0.00018814877054369825,
        "epoch": 0.7731374065480794,
        "step": 5998
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.5661656856536865,
        "learning_rate": 0.00018812002499563453,
        "epoch": 0.7732663057489043,
        "step": 5999
    },
    {
        "loss": 1.3713,
        "grad_norm": 1.607793927192688,
        "learning_rate": 0.00018809124682952444,
        "epoch": 0.7733952049497294,
        "step": 6000
    },
    {
        "loss": 1.0876,
        "grad_norm": 2.914071559906006,
        "learning_rate": 0.0001880624360560204,
        "epoch": 0.7735241041505543,
        "step": 6001
    },
    {
        "loss": 1.6107,
        "grad_norm": 2.622655153274536,
        "learning_rate": 0.00018803359268578684,
        "epoch": 0.7736530033513792,
        "step": 6002
    },
    {
        "loss": 1.0445,
        "grad_norm": 1.8629710674285889,
        "learning_rate": 0.00018800471672950032,
        "epoch": 0.7737819025522041,
        "step": 6003
    },
    {
        "loss": 2.5677,
        "grad_norm": 2.139589309692383,
        "learning_rate": 0.00018797580819784936,
        "epoch": 0.7739108017530292,
        "step": 6004
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.8014767169952393,
        "learning_rate": 0.0001879468671015346,
        "epoch": 0.7740397009538541,
        "step": 6005
    },
    {
        "loss": 2.483,
        "grad_norm": 1.715447187423706,
        "learning_rate": 0.00018791789345126874,
        "epoch": 0.774168600154679,
        "step": 6006
    },
    {
        "loss": 2.0,
        "grad_norm": 2.31705379486084,
        "learning_rate": 0.00018788888725777651,
        "epoch": 0.774297499355504,
        "step": 6007
    },
    {
        "loss": 2.2311,
        "grad_norm": 1.3767900466918945,
        "learning_rate": 0.0001878598485317947,
        "epoch": 0.774426398556329,
        "step": 6008
    },
    {
        "loss": 2.0829,
        "grad_norm": 2.3897573947906494,
        "learning_rate": 0.00018783077728407206,
        "epoch": 0.7745552977571539,
        "step": 6009
    },
    {
        "loss": 2.1573,
        "grad_norm": 1.5394278764724731,
        "learning_rate": 0.0001878016735253696,
        "epoch": 0.7746841969579789,
        "step": 6010
    },
    {
        "loss": 1.53,
        "grad_norm": 1.3794362545013428,
        "learning_rate": 0.00018777253726646003,
        "epoch": 0.7748130961588038,
        "step": 6011
    },
    {
        "loss": 2.5164,
        "grad_norm": 1.8577862977981567,
        "learning_rate": 0.0001877433685181284,
        "epoch": 0.7749419953596288,
        "step": 6012
    },
    {
        "loss": 1.9959,
        "grad_norm": 1.5742735862731934,
        "learning_rate": 0.00018771416729117165,
        "epoch": 0.7750708945604537,
        "step": 6013
    },
    {
        "loss": 2.2873,
        "grad_norm": 2.0980916023254395,
        "learning_rate": 0.0001876849335963987,
        "epoch": 0.7751997937612787,
        "step": 6014
    },
    {
        "loss": 1.4428,
        "grad_norm": 3.008646249771118,
        "learning_rate": 0.0001876556674446306,
        "epoch": 0.7753286929621036,
        "step": 6015
    },
    {
        "loss": 1.7764,
        "grad_norm": 2.31064772605896,
        "learning_rate": 0.00018762636884670032,
        "epoch": 0.7754575921629286,
        "step": 6016
    },
    {
        "loss": 1.0227,
        "grad_norm": 3.537860631942749,
        "learning_rate": 0.0001875970378134529,
        "epoch": 0.7755864913637536,
        "step": 6017
    },
    {
        "loss": 1.7077,
        "grad_norm": 2.662137269973755,
        "learning_rate": 0.00018756767435574535,
        "epoch": 0.7757153905645785,
        "step": 6018
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.1567156314849854,
        "learning_rate": 0.00018753827848444664,
        "epoch": 0.7758442897654034,
        "step": 6019
    },
    {
        "loss": 2.5222,
        "grad_norm": 1.2878456115722656,
        "learning_rate": 0.00018750885021043788,
        "epoch": 0.7759731889662284,
        "step": 6020
    },
    {
        "loss": 1.9125,
        "grad_norm": 2.3926446437835693,
        "learning_rate": 0.00018747938954461203,
        "epoch": 0.7761020881670534,
        "step": 6021
    },
    {
        "loss": 2.0651,
        "grad_norm": 1.3588378429412842,
        "learning_rate": 0.0001874498964978741,
        "epoch": 0.7762309873678783,
        "step": 6022
    },
    {
        "loss": 1.7042,
        "grad_norm": 1.8229094743728638,
        "learning_rate": 0.00018742037108114112,
        "epoch": 0.7763598865687033,
        "step": 6023
    },
    {
        "loss": 1.2746,
        "grad_norm": 2.5086991786956787,
        "learning_rate": 0.00018739081330534203,
        "epoch": 0.7764887857695282,
        "step": 6024
    },
    {
        "loss": 2.0811,
        "grad_norm": 1.7538143396377563,
        "learning_rate": 0.0001873612231814178,
        "epoch": 0.7766176849703532,
        "step": 6025
    },
    {
        "loss": 1.8032,
        "grad_norm": 1.9563060998916626,
        "learning_rate": 0.0001873316007203213,
        "epoch": 0.7767465841711781,
        "step": 6026
    },
    {
        "loss": 1.4105,
        "grad_norm": 2.0052335262298584,
        "learning_rate": 0.00018730194593301756,
        "epoch": 0.7768754833720031,
        "step": 6027
    },
    {
        "loss": 2.2017,
        "grad_norm": 1.7130943536758423,
        "learning_rate": 0.00018727225883048324,
        "epoch": 0.777004382572828,
        "step": 6028
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.719651460647583,
        "learning_rate": 0.0001872425394237073,
        "epoch": 0.777133281773653,
        "step": 6029
    },
    {
        "loss": 1.0884,
        "grad_norm": 3.36251163482666,
        "learning_rate": 0.00018721278772369053,
        "epoch": 0.777262180974478,
        "step": 6030
    },
    {
        "loss": 2.0418,
        "grad_norm": 1.730711817741394,
        "learning_rate": 0.00018718300374144558,
        "epoch": 0.7773910801753029,
        "step": 6031
    },
    {
        "loss": 2.0986,
        "grad_norm": 1.8617185354232788,
        "learning_rate": 0.0001871531874879972,
        "epoch": 0.7775199793761278,
        "step": 6032
    },
    {
        "loss": 2.4482,
        "grad_norm": 1.695380449295044,
        "learning_rate": 0.000187123338974382,
        "epoch": 0.7776488785769529,
        "step": 6033
    },
    {
        "loss": 2.5638,
        "grad_norm": 1.6905308961868286,
        "learning_rate": 0.00018709345821164855,
        "epoch": 0.7777777777777778,
        "step": 6034
    },
    {
        "loss": 1.4152,
        "grad_norm": 2.3189830780029297,
        "learning_rate": 0.00018706354521085734,
        "epoch": 0.7779066769786027,
        "step": 6035
    },
    {
        "loss": 1.2233,
        "grad_norm": 2.6985907554626465,
        "learning_rate": 0.0001870335999830808,
        "epoch": 0.7780355761794276,
        "step": 6036
    },
    {
        "loss": 1.6744,
        "grad_norm": 1.4302241802215576,
        "learning_rate": 0.0001870036225394033,
        "epoch": 0.7781644753802527,
        "step": 6037
    },
    {
        "loss": 1.2309,
        "grad_norm": 2.718101978302002,
        "learning_rate": 0.0001869736128909212,
        "epoch": 0.7782933745810776,
        "step": 6038
    },
    {
        "loss": 2.3255,
        "grad_norm": 1.4481459856033325,
        "learning_rate": 0.00018694357104874262,
        "epoch": 0.7784222737819025,
        "step": 6039
    },
    {
        "loss": 2.0821,
        "grad_norm": 1.8183616399765015,
        "learning_rate": 0.00018691349702398778,
        "epoch": 0.7785511729827275,
        "step": 6040
    },
    {
        "loss": 1.9199,
        "grad_norm": 1.3880833387374878,
        "learning_rate": 0.00018688339082778866,
        "epoch": 0.7786800721835525,
        "step": 6041
    },
    {
        "loss": 1.9536,
        "grad_norm": 2.423358201980591,
        "learning_rate": 0.00018685325247128924,
        "epoch": 0.7788089713843774,
        "step": 6042
    },
    {
        "loss": 1.9944,
        "grad_norm": 1.760040521621704,
        "learning_rate": 0.00018682308196564534,
        "epoch": 0.7789378705852024,
        "step": 6043
    },
    {
        "loss": 1.5072,
        "grad_norm": 1.4360308647155762,
        "learning_rate": 0.00018679287932202483,
        "epoch": 0.7790667697860273,
        "step": 6044
    },
    {
        "loss": 1.9092,
        "grad_norm": 1.6673730611801147,
        "learning_rate": 0.00018676264455160717,
        "epoch": 0.7791956689868523,
        "step": 6045
    },
    {
        "loss": 1.6771,
        "grad_norm": 2.634056568145752,
        "learning_rate": 0.00018673237766558403,
        "epoch": 0.7793245681876773,
        "step": 6046
    },
    {
        "loss": 1.5066,
        "grad_norm": 2.474634885787964,
        "learning_rate": 0.00018670207867515888,
        "epoch": 0.7794534673885022,
        "step": 6047
    },
    {
        "loss": 2.486,
        "grad_norm": 2.976652145385742,
        "learning_rate": 0.0001866717475915469,
        "epoch": 0.7795823665893271,
        "step": 6048
    },
    {
        "loss": 1.8215,
        "grad_norm": 3.1618359088897705,
        "learning_rate": 0.0001866413844259754,
        "epoch": 0.7797112657901522,
        "step": 6049
    },
    {
        "loss": 1.5654,
        "grad_norm": 2.243886947631836,
        "learning_rate": 0.00018661098918968343,
        "epoch": 0.7798401649909771,
        "step": 6050
    },
    {
        "loss": 1.1251,
        "grad_norm": 2.0556790828704834,
        "learning_rate": 0.0001865805618939219,
        "epoch": 0.779969064191802,
        "step": 6051
    },
    {
        "loss": 1.4462,
        "grad_norm": 3.049795389175415,
        "learning_rate": 0.0001865501025499536,
        "epoch": 0.7800979633926269,
        "step": 6052
    },
    {
        "loss": 1.2742,
        "grad_norm": 2.9297149181365967,
        "learning_rate": 0.00018651961116905325,
        "epoch": 0.780226862593452,
        "step": 6053
    },
    {
        "loss": 2.0557,
        "grad_norm": 2.1020050048828125,
        "learning_rate": 0.0001864890877625074,
        "epoch": 0.7803557617942769,
        "step": 6054
    },
    {
        "loss": 2.2465,
        "grad_norm": 1.6482129096984863,
        "learning_rate": 0.00018645853234161436,
        "epoch": 0.7804846609951018,
        "step": 6055
    },
    {
        "loss": 1.5807,
        "grad_norm": 2.182426691055298,
        "learning_rate": 0.00018642794491768438,
        "epoch": 0.7806135601959268,
        "step": 6056
    },
    {
        "loss": 1.7218,
        "grad_norm": 2.058666229248047,
        "learning_rate": 0.0001863973255020396,
        "epoch": 0.7807424593967517,
        "step": 6057
    },
    {
        "loss": 1.3771,
        "grad_norm": 3.0713727474212646,
        "learning_rate": 0.00018636667410601389,
        "epoch": 0.7808713585975767,
        "step": 6058
    },
    {
        "loss": 1.7493,
        "grad_norm": 2.5052330493927,
        "learning_rate": 0.00018633599074095302,
        "epoch": 0.7810002577984017,
        "step": 6059
    },
    {
        "loss": 2.5053,
        "grad_norm": 1.4546353816986084,
        "learning_rate": 0.00018630527541821457,
        "epoch": 0.7811291569992266,
        "step": 6060
    },
    {
        "loss": 2.1424,
        "grad_norm": 1.9636214971542358,
        "learning_rate": 0.00018627452814916806,
        "epoch": 0.7812580562000515,
        "step": 6061
    },
    {
        "loss": 1.5569,
        "grad_norm": 1.7790802717208862,
        "learning_rate": 0.00018624374894519457,
        "epoch": 0.7813869554008765,
        "step": 6062
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.0499513149261475,
        "learning_rate": 0.00018621293781768725,
        "epoch": 0.7815158546017015,
        "step": 6063
    },
    {
        "loss": 1.4098,
        "grad_norm": 3.312540054321289,
        "learning_rate": 0.00018618209477805104,
        "epoch": 0.7816447538025264,
        "step": 6064
    },
    {
        "loss": 1.9587,
        "grad_norm": 2.1635830402374268,
        "learning_rate": 0.00018615121983770255,
        "epoch": 0.7817736530033513,
        "step": 6065
    },
    {
        "loss": 1.3282,
        "grad_norm": 3.039936065673828,
        "learning_rate": 0.00018612031300807038,
        "epoch": 0.7819025522041764,
        "step": 6066
    },
    {
        "loss": 1.9565,
        "grad_norm": 1.7140429019927979,
        "learning_rate": 0.00018608937430059474,
        "epoch": 0.7820314514050013,
        "step": 6067
    },
    {
        "loss": 1.6527,
        "grad_norm": 1.9248113632202148,
        "learning_rate": 0.00018605840372672782,
        "epoch": 0.7821603506058262,
        "step": 6068
    },
    {
        "loss": 2.1963,
        "grad_norm": 2.1671295166015625,
        "learning_rate": 0.0001860274012979334,
        "epoch": 0.7822892498066512,
        "step": 6069
    },
    {
        "loss": 1.9248,
        "grad_norm": 2.5336415767669678,
        "learning_rate": 0.00018599636702568736,
        "epoch": 0.7824181490074762,
        "step": 6070
    },
    {
        "loss": 2.313,
        "grad_norm": 1.493435263633728,
        "learning_rate": 0.0001859653009214771,
        "epoch": 0.7825470482083011,
        "step": 6071
    },
    {
        "loss": 1.9223,
        "grad_norm": 2.0723049640655518,
        "learning_rate": 0.0001859342029968018,
        "epoch": 0.782675947409126,
        "step": 6072
    },
    {
        "loss": 2.6517,
        "grad_norm": 1.927611231803894,
        "learning_rate": 0.0001859030732631726,
        "epoch": 0.782804846609951,
        "step": 6073
    },
    {
        "loss": 1.902,
        "grad_norm": 1.7953451871871948,
        "learning_rate": 0.00018587191173211236,
        "epoch": 0.782933745810776,
        "step": 6074
    },
    {
        "loss": 1.6847,
        "grad_norm": 2.5098185539245605,
        "learning_rate": 0.00018584071841515557,
        "epoch": 0.7830626450116009,
        "step": 6075
    },
    {
        "loss": 2.2071,
        "grad_norm": 1.634285569190979,
        "learning_rate": 0.00018580949332384862,
        "epoch": 0.7831915442124259,
        "step": 6076
    },
    {
        "loss": 1.5291,
        "grad_norm": 2.7696382999420166,
        "learning_rate": 0.00018577823646974962,
        "epoch": 0.7833204434132508,
        "step": 6077
    },
    {
        "loss": 2.4375,
        "grad_norm": 1.5732241868972778,
        "learning_rate": 0.0001857469478644285,
        "epoch": 0.7834493426140758,
        "step": 6078
    },
    {
        "loss": 2.0445,
        "grad_norm": 1.5478200912475586,
        "learning_rate": 0.0001857156275194668,
        "epoch": 0.7835782418149008,
        "step": 6079
    },
    {
        "loss": 1.402,
        "grad_norm": 2.245267629623413,
        "learning_rate": 0.00018568427544645789,
        "epoch": 0.7837071410157257,
        "step": 6080
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.94813072681427,
        "learning_rate": 0.000185652891657007,
        "epoch": 0.7838360402165506,
        "step": 6081
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.1681575775146484,
        "learning_rate": 0.0001856214761627309,
        "epoch": 0.7839649394173757,
        "step": 6082
    },
    {
        "loss": 1.7948,
        "grad_norm": 2.5750067234039307,
        "learning_rate": 0.00018559002897525822,
        "epoch": 0.7840938386182006,
        "step": 6083
    },
    {
        "loss": 2.2515,
        "grad_norm": 2.756138801574707,
        "learning_rate": 0.00018555855010622923,
        "epoch": 0.7842227378190255,
        "step": 6084
    },
    {
        "loss": 1.6146,
        "grad_norm": 2.6971523761749268,
        "learning_rate": 0.00018552703956729606,
        "epoch": 0.7843516370198504,
        "step": 6085
    },
    {
        "loss": 1.8399,
        "grad_norm": 1.927980661392212,
        "learning_rate": 0.00018549549737012245,
        "epoch": 0.7844805362206755,
        "step": 6086
    },
    {
        "loss": 0.8452,
        "grad_norm": 2.405311107635498,
        "learning_rate": 0.0001854639235263839,
        "epoch": 0.7846094354215004,
        "step": 6087
    },
    {
        "loss": 1.4832,
        "grad_norm": 2.1292619705200195,
        "learning_rate": 0.0001854323180477676,
        "epoch": 0.7847383346223253,
        "step": 6088
    },
    {
        "loss": 1.1358,
        "grad_norm": 1.6246442794799805,
        "learning_rate": 0.00018540068094597244,
        "epoch": 0.7848672338231503,
        "step": 6089
    },
    {
        "loss": 1.3258,
        "grad_norm": 2.4936749935150146,
        "learning_rate": 0.00018536901223270916,
        "epoch": 0.7849961330239753,
        "step": 6090
    },
    {
        "loss": 1.9156,
        "grad_norm": 1.8360669612884521,
        "learning_rate": 0.00018533731191969994,
        "epoch": 0.7851250322248002,
        "step": 6091
    },
    {
        "loss": 1.5797,
        "grad_norm": 2.0207653045654297,
        "learning_rate": 0.00018530558001867892,
        "epoch": 0.7852539314256252,
        "step": 6092
    },
    {
        "loss": 2.0018,
        "grad_norm": 2.147063732147217,
        "learning_rate": 0.00018527381654139178,
        "epoch": 0.7853828306264501,
        "step": 6093
    },
    {
        "loss": 1.2503,
        "grad_norm": 1.4726225137710571,
        "learning_rate": 0.00018524202149959584,
        "epoch": 0.785511729827275,
        "step": 6094
    },
    {
        "loss": 1.843,
        "grad_norm": 2.2833080291748047,
        "learning_rate": 0.00018521019490506038,
        "epoch": 0.7856406290281001,
        "step": 6095
    },
    {
        "loss": 2.0432,
        "grad_norm": 1.8767871856689453,
        "learning_rate": 0.0001851783367695659,
        "epoch": 0.785769528228925,
        "step": 6096
    },
    {
        "loss": 1.994,
        "grad_norm": 1.7342852354049683,
        "learning_rate": 0.000185146447104905,
        "epoch": 0.7858984274297499,
        "step": 6097
    },
    {
        "loss": 1.9317,
        "grad_norm": 1.7692134380340576,
        "learning_rate": 0.00018511452592288188,
        "epoch": 0.7860273266305748,
        "step": 6098
    },
    {
        "loss": 1.6686,
        "grad_norm": 1.238249659538269,
        "learning_rate": 0.0001850825732353121,
        "epoch": 0.7861562258313999,
        "step": 6099
    },
    {
        "loss": 1.2004,
        "grad_norm": 1.983303189277649,
        "learning_rate": 0.00018505058905402325,
        "epoch": 0.7862851250322248,
        "step": 6100
    },
    {
        "loss": 2.5025,
        "grad_norm": 1.3786360025405884,
        "learning_rate": 0.00018501857339085437,
        "epoch": 0.7864140242330497,
        "step": 6101
    },
    {
        "loss": 2.0301,
        "grad_norm": 1.536906361579895,
        "learning_rate": 0.00018498652625765623,
        "epoch": 0.7865429234338747,
        "step": 6102
    },
    {
        "loss": 1.773,
        "grad_norm": 2.833922863006592,
        "learning_rate": 0.0001849544476662912,
        "epoch": 0.7866718226346997,
        "step": 6103
    },
    {
        "loss": 1.349,
        "grad_norm": 2.741760015487671,
        "learning_rate": 0.00018492233762863337,
        "epoch": 0.7868007218355246,
        "step": 6104
    },
    {
        "loss": 1.4561,
        "grad_norm": 2.967653274536133,
        "learning_rate": 0.00018489019615656842,
        "epoch": 0.7869296210363496,
        "step": 6105
    },
    {
        "loss": 1.4837,
        "grad_norm": 2.591853380203247,
        "learning_rate": 0.0001848580232619936,
        "epoch": 0.7870585202371745,
        "step": 6106
    },
    {
        "loss": 1.5437,
        "grad_norm": 3.3675551414489746,
        "learning_rate": 0.00018482581895681795,
        "epoch": 0.7871874194379995,
        "step": 6107
    },
    {
        "loss": 2.2299,
        "grad_norm": 2.0321946144104004,
        "learning_rate": 0.00018479358325296198,
        "epoch": 0.7873163186388245,
        "step": 6108
    },
    {
        "loss": 1.9036,
        "grad_norm": 2.5591795444488525,
        "learning_rate": 0.00018476131616235793,
        "epoch": 0.7874452178396494,
        "step": 6109
    },
    {
        "loss": 2.5271,
        "grad_norm": 1.9781633615493774,
        "learning_rate": 0.0001847290176969496,
        "epoch": 0.7875741170404743,
        "step": 6110
    },
    {
        "loss": 1.5741,
        "grad_norm": 2.410367250442505,
        "learning_rate": 0.00018469668786869238,
        "epoch": 0.7877030162412993,
        "step": 6111
    },
    {
        "loss": 1.9162,
        "grad_norm": 3.615849256515503,
        "learning_rate": 0.0001846643266895534,
        "epoch": 0.7878319154421243,
        "step": 6112
    },
    {
        "loss": 1.7568,
        "grad_norm": 2.1573798656463623,
        "learning_rate": 0.00018463193417151124,
        "epoch": 0.7879608146429492,
        "step": 6113
    },
    {
        "loss": 2.1718,
        "grad_norm": 2.183628797531128,
        "learning_rate": 0.00018459951032655613,
        "epoch": 0.7880897138437741,
        "step": 6114
    },
    {
        "loss": 1.7747,
        "grad_norm": 1.7682552337646484,
        "learning_rate": 0.00018456705516668997,
        "epoch": 0.7882186130445992,
        "step": 6115
    },
    {
        "loss": 1.5107,
        "grad_norm": 2.706516742706299,
        "learning_rate": 0.00018453456870392616,
        "epoch": 0.7883475122454241,
        "step": 6116
    },
    {
        "loss": 1.589,
        "grad_norm": 2.775592088699341,
        "learning_rate": 0.00018450205095028974,
        "epoch": 0.788476411446249,
        "step": 6117
    },
    {
        "loss": 2.073,
        "grad_norm": 1.745248794555664,
        "learning_rate": 0.00018446950191781727,
        "epoch": 0.788605310647074,
        "step": 6118
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.6304882764816284,
        "learning_rate": 0.00018443692161855698,
        "epoch": 0.788734209847899,
        "step": 6119
    },
    {
        "loss": 1.9707,
        "grad_norm": 2.665256977081299,
        "learning_rate": 0.00018440431006456852,
        "epoch": 0.7888631090487239,
        "step": 6120
    },
    {
        "loss": 2.1021,
        "grad_norm": 1.7763336896896362,
        "learning_rate": 0.00018437166726792333,
        "epoch": 0.7889920082495488,
        "step": 6121
    },
    {
        "loss": 1.4546,
        "grad_norm": 2.7790589332580566,
        "learning_rate": 0.0001843389932407043,
        "epoch": 0.7891209074503738,
        "step": 6122
    },
    {
        "loss": 2.3223,
        "grad_norm": 1.9479124546051025,
        "learning_rate": 0.00018430628799500574,
        "epoch": 0.7892498066511988,
        "step": 6123
    },
    {
        "loss": 1.4574,
        "grad_norm": 2.481478214263916,
        "learning_rate": 0.00018427355154293376,
        "epoch": 0.7893787058520237,
        "step": 6124
    },
    {
        "loss": 2.1892,
        "grad_norm": 1.3196251392364502,
        "learning_rate": 0.00018424078389660595,
        "epoch": 0.7895076050528487,
        "step": 6125
    },
    {
        "loss": 1.6941,
        "grad_norm": 2.346487283706665,
        "learning_rate": 0.00018420798506815132,
        "epoch": 0.7896365042536736,
        "step": 6126
    },
    {
        "loss": 1.9321,
        "grad_norm": 1.4140955209732056,
        "learning_rate": 0.00018417515506971056,
        "epoch": 0.7897654034544985,
        "step": 6127
    },
    {
        "loss": 1.7813,
        "grad_norm": 2.171872138977051,
        "learning_rate": 0.0001841422939134358,
        "epoch": 0.7898943026553236,
        "step": 6128
    },
    {
        "loss": 1.6841,
        "grad_norm": 2.331925630569458,
        "learning_rate": 0.00018410940161149084,
        "epoch": 0.7900232018561485,
        "step": 6129
    },
    {
        "loss": 1.7778,
        "grad_norm": 3.1267130374908447,
        "learning_rate": 0.00018407647817605084,
        "epoch": 0.7901521010569734,
        "step": 6130
    },
    {
        "loss": 0.8616,
        "grad_norm": 1.8858261108398438,
        "learning_rate": 0.0001840435236193026,
        "epoch": 0.7902810002577983,
        "step": 6131
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.3935068845748901,
        "learning_rate": 0.0001840105379534444,
        "epoch": 0.7904098994586234,
        "step": 6132
    },
    {
        "loss": 1.7409,
        "grad_norm": 1.8369947671890259,
        "learning_rate": 0.00018397752119068606,
        "epoch": 0.7905387986594483,
        "step": 6133
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.5401256084442139,
        "learning_rate": 0.0001839444733432489,
        "epoch": 0.7906676978602732,
        "step": 6134
    },
    {
        "loss": 1.6182,
        "grad_norm": 2.4459497928619385,
        "learning_rate": 0.00018391139442336564,
        "epoch": 0.7907965970610982,
        "step": 6135
    },
    {
        "loss": 2.1839,
        "grad_norm": 1.7246124744415283,
        "learning_rate": 0.00018387828444328078,
        "epoch": 0.7909254962619232,
        "step": 6136
    },
    {
        "loss": 1.4303,
        "grad_norm": 1.9045252799987793,
        "learning_rate": 0.00018384514341524994,
        "epoch": 0.7910543954627481,
        "step": 6137
    },
    {
        "loss": 1.6375,
        "grad_norm": 1.4741491079330444,
        "learning_rate": 0.00018381197135154052,
        "epoch": 0.7911832946635731,
        "step": 6138
    },
    {
        "loss": 2.0221,
        "grad_norm": 1.7516143321990967,
        "learning_rate": 0.00018377876826443138,
        "epoch": 0.791312193864398,
        "step": 6139
    },
    {
        "loss": 1.5459,
        "grad_norm": 1.7427030801773071,
        "learning_rate": 0.0001837455341662127,
        "epoch": 0.791441093065223,
        "step": 6140
    },
    {
        "loss": 1.6313,
        "grad_norm": 2.389725685119629,
        "learning_rate": 0.00018371226906918628,
        "epoch": 0.791569992266048,
        "step": 6141
    },
    {
        "loss": 2.1297,
        "grad_norm": 2.2131431102752686,
        "learning_rate": 0.00018367897298566534,
        "epoch": 0.7916988914668729,
        "step": 6142
    },
    {
        "loss": 1.9524,
        "grad_norm": 2.7378296852111816,
        "learning_rate": 0.00018364564592797465,
        "epoch": 0.7918277906676978,
        "step": 6143
    },
    {
        "loss": 2.6729,
        "grad_norm": 1.7328171730041504,
        "learning_rate": 0.00018361228790845027,
        "epoch": 0.7919566898685229,
        "step": 6144
    },
    {
        "loss": 2.282,
        "grad_norm": 1.482491135597229,
        "learning_rate": 0.00018357889893943986,
        "epoch": 0.7920855890693478,
        "step": 6145
    },
    {
        "loss": 1.836,
        "grad_norm": 2.0390853881835938,
        "learning_rate": 0.00018354547903330257,
        "epoch": 0.7922144882701727,
        "step": 6146
    },
    {
        "loss": 1.2035,
        "grad_norm": 2.382716178894043,
        "learning_rate": 0.0001835120282024089,
        "epoch": 0.7923433874709976,
        "step": 6147
    },
    {
        "loss": 1.7723,
        "grad_norm": 1.674308180809021,
        "learning_rate": 0.00018347854645914082,
        "epoch": 0.7924722866718227,
        "step": 6148
    },
    {
        "loss": 1.3068,
        "grad_norm": 2.6843185424804688,
        "learning_rate": 0.00018344503381589178,
        "epoch": 0.7926011858726476,
        "step": 6149
    },
    {
        "loss": 1.5232,
        "grad_norm": 3.0642783641815186,
        "learning_rate": 0.00018341149028506658,
        "epoch": 0.7927300850734725,
        "step": 6150
    },
    {
        "loss": 1.2999,
        "grad_norm": 4.200061798095703,
        "learning_rate": 0.00018337791587908164,
        "epoch": 0.7928589842742975,
        "step": 6151
    },
    {
        "loss": 1.1377,
        "grad_norm": 2.718302011489868,
        "learning_rate": 0.00018334431061036455,
        "epoch": 0.7929878834751225,
        "step": 6152
    },
    {
        "loss": 1.9848,
        "grad_norm": 1.172876000404358,
        "learning_rate": 0.0001833106744913546,
        "epoch": 0.7931167826759474,
        "step": 6153
    },
    {
        "loss": 1.4787,
        "grad_norm": 2.8637022972106934,
        "learning_rate": 0.00018327700753450224,
        "epoch": 0.7932456818767724,
        "step": 6154
    },
    {
        "loss": 1.9691,
        "grad_norm": 1.9754419326782227,
        "learning_rate": 0.00018324330975226944,
        "epoch": 0.7933745810775973,
        "step": 6155
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.972165822982788,
        "learning_rate": 0.00018320958115712976,
        "epoch": 0.7935034802784223,
        "step": 6156
    },
    {
        "loss": 1.1189,
        "grad_norm": 2.6229493618011475,
        "learning_rate": 0.00018317582176156783,
        "epoch": 0.7936323794792473,
        "step": 6157
    },
    {
        "loss": 1.8164,
        "grad_norm": 2.164740562438965,
        "learning_rate": 0.00018314203157807993,
        "epoch": 0.7937612786800722,
        "step": 6158
    },
    {
        "loss": 2.0136,
        "grad_norm": 2.1681594848632812,
        "learning_rate": 0.00018310821061917363,
        "epoch": 0.7938901778808971,
        "step": 6159
    },
    {
        "loss": 2.5415,
        "grad_norm": 1.7730687856674194,
        "learning_rate": 0.00018307435889736794,
        "epoch": 0.7940190770817221,
        "step": 6160
    },
    {
        "loss": 0.9295,
        "grad_norm": 3.326582670211792,
        "learning_rate": 0.00018304047642519325,
        "epoch": 0.7941479762825471,
        "step": 6161
    },
    {
        "loss": 1.1246,
        "grad_norm": 4.372028350830078,
        "learning_rate": 0.00018300656321519122,
        "epoch": 0.794276875483372,
        "step": 6162
    },
    {
        "loss": 2.5724,
        "grad_norm": 1.3011784553527832,
        "learning_rate": 0.0001829726192799151,
        "epoch": 0.7944057746841969,
        "step": 6163
    },
    {
        "loss": 2.5357,
        "grad_norm": 1.2018191814422607,
        "learning_rate": 0.00018293864463192938,
        "epoch": 0.7945346738850219,
        "step": 6164
    },
    {
        "loss": 1.2656,
        "grad_norm": 3.102179765701294,
        "learning_rate": 0.00018290463928380984,
        "epoch": 0.7946635730858469,
        "step": 6165
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.402998924255371,
        "learning_rate": 0.0001828706032481439,
        "epoch": 0.7947924722866718,
        "step": 6166
    },
    {
        "loss": 1.7557,
        "grad_norm": 1.634960412979126,
        "learning_rate": 0.00018283653653753,
        "epoch": 0.7949213714874968,
        "step": 6167
    },
    {
        "loss": 2.155,
        "grad_norm": 2.113731622695923,
        "learning_rate": 0.0001828024391645782,
        "epoch": 0.7950502706883217,
        "step": 6168
    },
    {
        "loss": 1.9101,
        "grad_norm": 2.2080891132354736,
        "learning_rate": 0.00018276831114190966,
        "epoch": 0.7951791698891467,
        "step": 6169
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.7697925567626953,
        "learning_rate": 0.00018273415248215726,
        "epoch": 0.7953080690899716,
        "step": 6170
    },
    {
        "loss": 1.2083,
        "grad_norm": 2.5346462726593018,
        "learning_rate": 0.00018269996319796475,
        "epoch": 0.7954369682907966,
        "step": 6171
    },
    {
        "loss": 2.0301,
        "grad_norm": 1.938208818435669,
        "learning_rate": 0.00018266574330198755,
        "epoch": 0.7955658674916215,
        "step": 6172
    },
    {
        "loss": 1.8273,
        "grad_norm": 2.1721770763397217,
        "learning_rate": 0.00018263149280689246,
        "epoch": 0.7956947666924465,
        "step": 6173
    },
    {
        "loss": 2.1021,
        "grad_norm": 2.234264373779297,
        "learning_rate": 0.0001825972117253572,
        "epoch": 0.7958236658932715,
        "step": 6174
    },
    {
        "loss": 2.1342,
        "grad_norm": 1.4400122165679932,
        "learning_rate": 0.00018256290007007125,
        "epoch": 0.7959525650940964,
        "step": 6175
    },
    {
        "loss": 1.508,
        "grad_norm": 1.8899224996566772,
        "learning_rate": 0.00018252855785373524,
        "epoch": 0.7960814642949213,
        "step": 6176
    },
    {
        "loss": 2.0744,
        "grad_norm": 2.1314799785614014,
        "learning_rate": 0.00018249418508906105,
        "epoch": 0.7962103634957464,
        "step": 6177
    },
    {
        "loss": 1.8354,
        "grad_norm": 2.136155366897583,
        "learning_rate": 0.00018245978178877184,
        "epoch": 0.7963392626965713,
        "step": 6178
    },
    {
        "loss": 0.9212,
        "grad_norm": 2.971359968185425,
        "learning_rate": 0.00018242534796560233,
        "epoch": 0.7964681618973962,
        "step": 6179
    },
    {
        "loss": 2.2335,
        "grad_norm": 1.493343710899353,
        "learning_rate": 0.0001823908836322983,
        "epoch": 0.7965970610982211,
        "step": 6180
    },
    {
        "loss": 1.7015,
        "grad_norm": 1.443225383758545,
        "learning_rate": 0.0001823563888016168,
        "epoch": 0.7967259602990462,
        "step": 6181
    },
    {
        "loss": 1.6943,
        "grad_norm": 2.41711688041687,
        "learning_rate": 0.00018232186348632632,
        "epoch": 0.7968548594998711,
        "step": 6182
    },
    {
        "loss": 1.8628,
        "grad_norm": 2.362086772918701,
        "learning_rate": 0.00018228730769920658,
        "epoch": 0.796983758700696,
        "step": 6183
    },
    {
        "loss": 2.0783,
        "grad_norm": 2.4560511112213135,
        "learning_rate": 0.0001822527214530486,
        "epoch": 0.797112657901521,
        "step": 6184
    },
    {
        "loss": 1.5545,
        "grad_norm": 3.692837953567505,
        "learning_rate": 0.00018221810476065455,
        "epoch": 0.797241557102346,
        "step": 6185
    },
    {
        "loss": 1.885,
        "grad_norm": 2.2025790214538574,
        "learning_rate": 0.00018218345763483802,
        "epoch": 0.7973704563031709,
        "step": 6186
    },
    {
        "loss": 1.9405,
        "grad_norm": 2.5562326908111572,
        "learning_rate": 0.00018214878008842388,
        "epoch": 0.7974993555039959,
        "step": 6187
    },
    {
        "loss": 2.1493,
        "grad_norm": 2.1171767711639404,
        "learning_rate": 0.000182114072134248,
        "epoch": 0.7976282547048208,
        "step": 6188
    },
    {
        "loss": 1.3614,
        "grad_norm": 1.940049171447754,
        "learning_rate": 0.00018207933378515785,
        "epoch": 0.7977571539056458,
        "step": 6189
    },
    {
        "loss": 2.4721,
        "grad_norm": 1.506780982017517,
        "learning_rate": 0.00018204456505401198,
        "epoch": 0.7978860531064708,
        "step": 6190
    },
    {
        "loss": 2.0529,
        "grad_norm": 2.112098455429077,
        "learning_rate": 0.00018200976595368013,
        "epoch": 0.7980149523072957,
        "step": 6191
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.9109874963760376,
        "learning_rate": 0.00018197493649704344,
        "epoch": 0.7981438515081206,
        "step": 6192
    },
    {
        "loss": 2.481,
        "grad_norm": 1.5326083898544312,
        "learning_rate": 0.00018194007669699414,
        "epoch": 0.7982727507089457,
        "step": 6193
    },
    {
        "loss": 1.8601,
        "grad_norm": 2.8746836185455322,
        "learning_rate": 0.00018190518656643578,
        "epoch": 0.7984016499097706,
        "step": 6194
    },
    {
        "loss": 2.432,
        "grad_norm": 1.4119864702224731,
        "learning_rate": 0.00018187026611828303,
        "epoch": 0.7985305491105955,
        "step": 6195
    },
    {
        "loss": 1.1539,
        "grad_norm": 3.371081590652466,
        "learning_rate": 0.000181835315365462,
        "epoch": 0.7986594483114204,
        "step": 6196
    },
    {
        "loss": 2.4182,
        "grad_norm": 1.2706314325332642,
        "learning_rate": 0.00018180033432090977,
        "epoch": 0.7987883475122455,
        "step": 6197
    },
    {
        "loss": 2.2795,
        "grad_norm": 2.452115774154663,
        "learning_rate": 0.00018176532299757475,
        "epoch": 0.7989172467130704,
        "step": 6198
    },
    {
        "loss": 2.1721,
        "grad_norm": 1.3839746713638306,
        "learning_rate": 0.0001817302814084166,
        "epoch": 0.7990461459138953,
        "step": 6199
    },
    {
        "loss": 1.5851,
        "grad_norm": 2.333738327026367,
        "learning_rate": 0.00018169520956640609,
        "epoch": 0.7991750451147203,
        "step": 6200
    },
    {
        "loss": 1.8425,
        "grad_norm": 1.7682456970214844,
        "learning_rate": 0.00018166010748452523,
        "epoch": 0.7993039443155452,
        "step": 6201
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.415353775024414,
        "learning_rate": 0.00018162497517576725,
        "epoch": 0.7994328435163702,
        "step": 6202
    },
    {
        "loss": 1.7476,
        "grad_norm": 2.29117751121521,
        "learning_rate": 0.00018158981265313647,
        "epoch": 0.7995617427171952,
        "step": 6203
    },
    {
        "loss": 1.4892,
        "grad_norm": 3.0275726318359375,
        "learning_rate": 0.00018155461992964863,
        "epoch": 0.7996906419180201,
        "step": 6204
    },
    {
        "loss": 1.6158,
        "grad_norm": 2.221057415008545,
        "learning_rate": 0.00018151939701833026,
        "epoch": 0.799819541118845,
        "step": 6205
    },
    {
        "loss": 1.879,
        "grad_norm": 2.0885658264160156,
        "learning_rate": 0.00018148414393221943,
        "epoch": 0.79994844031967,
        "step": 6206
    },
    {
        "loss": 2.0996,
        "grad_norm": 1.3669368028640747,
        "learning_rate": 0.00018144886068436526,
        "epoch": 0.800077339520495,
        "step": 6207
    },
    {
        "loss": 1.4952,
        "grad_norm": 2.6480374336242676,
        "learning_rate": 0.00018141354728782786,
        "epoch": 0.8002062387213199,
        "step": 6208
    },
    {
        "loss": 2.2897,
        "grad_norm": 2.0961508750915527,
        "learning_rate": 0.00018137820375567883,
        "epoch": 0.8003351379221448,
        "step": 6209
    },
    {
        "loss": 0.4897,
        "grad_norm": 1.7574383020401,
        "learning_rate": 0.00018134283010100066,
        "epoch": 0.8004640371229699,
        "step": 6210
    },
    {
        "loss": 1.6978,
        "grad_norm": 1.9604965448379517,
        "learning_rate": 0.00018130742633688706,
        "epoch": 0.8005929363237948,
        "step": 6211
    },
    {
        "loss": 2.549,
        "grad_norm": 1.8404146432876587,
        "learning_rate": 0.0001812719924764429,
        "epoch": 0.8007218355246197,
        "step": 6212
    },
    {
        "loss": 1.9716,
        "grad_norm": 2.3730781078338623,
        "learning_rate": 0.00018123652853278428,
        "epoch": 0.8008507347254447,
        "step": 6213
    },
    {
        "loss": 2.6578,
        "grad_norm": 1.758042335510254,
        "learning_rate": 0.00018120103451903824,
        "epoch": 0.8009796339262697,
        "step": 6214
    },
    {
        "loss": 1.4425,
        "grad_norm": 4.321037769317627,
        "learning_rate": 0.00018116551044834308,
        "epoch": 0.8011085331270946,
        "step": 6215
    },
    {
        "loss": 2.1138,
        "grad_norm": 1.8508514165878296,
        "learning_rate": 0.00018112995633384827,
        "epoch": 0.8012374323279196,
        "step": 6216
    },
    {
        "loss": 2.1679,
        "grad_norm": 2.089111566543579,
        "learning_rate": 0.0001810943721887143,
        "epoch": 0.8013663315287445,
        "step": 6217
    },
    {
        "loss": 1.673,
        "grad_norm": 2.401723861694336,
        "learning_rate": 0.0001810587580261128,
        "epoch": 0.8014952307295695,
        "step": 6218
    },
    {
        "loss": 1.0369,
        "grad_norm": 3.221217155456543,
        "learning_rate": 0.00018102311385922654,
        "epoch": 0.8016241299303944,
        "step": 6219
    },
    {
        "loss": 1.3607,
        "grad_norm": 1.9194622039794922,
        "learning_rate": 0.00018098743970124928,
        "epoch": 0.8017530291312194,
        "step": 6220
    },
    {
        "loss": 2.161,
        "grad_norm": 1.8756263256072998,
        "learning_rate": 0.00018095173556538616,
        "epoch": 0.8018819283320443,
        "step": 6221
    },
    {
        "loss": 2.1076,
        "grad_norm": 2.0319278240203857,
        "learning_rate": 0.00018091600146485305,
        "epoch": 0.8020108275328693,
        "step": 6222
    },
    {
        "loss": 0.8715,
        "grad_norm": 3.01366925239563,
        "learning_rate": 0.0001808802374128772,
        "epoch": 0.8021397267336943,
        "step": 6223
    },
    {
        "loss": 1.8011,
        "grad_norm": 1.5649998188018799,
        "learning_rate": 0.00018084444342269688,
        "epoch": 0.8022686259345192,
        "step": 6224
    },
    {
        "loss": 2.2106,
        "grad_norm": 1.807403802871704,
        "learning_rate": 0.0001808086195075613,
        "epoch": 0.8023975251353441,
        "step": 6225
    },
    {
        "loss": 1.3179,
        "grad_norm": 1.6003402471542358,
        "learning_rate": 0.00018077276568073092,
        "epoch": 0.8025264243361692,
        "step": 6226
    },
    {
        "loss": 2.2252,
        "grad_norm": 2.6331446170806885,
        "learning_rate": 0.0001807368819554772,
        "epoch": 0.8026553235369941,
        "step": 6227
    },
    {
        "loss": 2.103,
        "grad_norm": 1.56288480758667,
        "learning_rate": 0.00018070096834508267,
        "epoch": 0.802784222737819,
        "step": 6228
    },
    {
        "loss": 1.7872,
        "grad_norm": 2.7840158939361572,
        "learning_rate": 0.00018066502486284084,
        "epoch": 0.802913121938644,
        "step": 6229
    },
    {
        "loss": 1.5823,
        "grad_norm": 2.7684051990509033,
        "learning_rate": 0.00018062905152205653,
        "epoch": 0.803042021139469,
        "step": 6230
    },
    {
        "loss": 1.8944,
        "grad_norm": 2.1351163387298584,
        "learning_rate": 0.00018059304833604531,
        "epoch": 0.8031709203402939,
        "step": 6231
    },
    {
        "loss": 2.0571,
        "grad_norm": 2.3284618854522705,
        "learning_rate": 0.00018055701531813397,
        "epoch": 0.8032998195411188,
        "step": 6232
    },
    {
        "loss": 1.8926,
        "grad_norm": 1.622673749923706,
        "learning_rate": 0.0001805209524816603,
        "epoch": 0.8034287187419438,
        "step": 6233
    },
    {
        "loss": 1.5348,
        "grad_norm": 1.5723477602005005,
        "learning_rate": 0.00018048485983997314,
        "epoch": 0.8035576179427688,
        "step": 6234
    },
    {
        "loss": 0.6912,
        "grad_norm": 4.851700782775879,
        "learning_rate": 0.00018044873740643235,
        "epoch": 0.8036865171435937,
        "step": 6235
    },
    {
        "loss": 2.1352,
        "grad_norm": 2.683899402618408,
        "learning_rate": 0.00018041258519440882,
        "epoch": 0.8038154163444187,
        "step": 6236
    },
    {
        "loss": 1.7617,
        "grad_norm": 1.1769639253616333,
        "learning_rate": 0.00018037640321728443,
        "epoch": 0.8039443155452436,
        "step": 6237
    },
    {
        "loss": 1.8265,
        "grad_norm": 2.2951433658599854,
        "learning_rate": 0.00018034019148845215,
        "epoch": 0.8040732147460685,
        "step": 6238
    },
    {
        "loss": 1.9226,
        "grad_norm": 2.903670072555542,
        "learning_rate": 0.00018030395002131594,
        "epoch": 0.8042021139468936,
        "step": 6239
    },
    {
        "loss": 0.9276,
        "grad_norm": 2.970273733139038,
        "learning_rate": 0.0001802676788292907,
        "epoch": 0.8043310131477185,
        "step": 6240
    },
    {
        "loss": 2.2842,
        "grad_norm": 1.455633521080017,
        "learning_rate": 0.00018023137792580243,
        "epoch": 0.8044599123485434,
        "step": 6241
    },
    {
        "loss": 1.6584,
        "grad_norm": 2.2203562259674072,
        "learning_rate": 0.00018019504732428806,
        "epoch": 0.8045888115493683,
        "step": 6242
    },
    {
        "loss": 1.8374,
        "grad_norm": 5.289165496826172,
        "learning_rate": 0.00018015868703819555,
        "epoch": 0.8047177107501934,
        "step": 6243
    },
    {
        "loss": 2.0962,
        "grad_norm": 1.5698362588882446,
        "learning_rate": 0.00018012229708098382,
        "epoch": 0.8048466099510183,
        "step": 6244
    },
    {
        "loss": 2.0801,
        "grad_norm": 2.256840705871582,
        "learning_rate": 0.00018008587746612276,
        "epoch": 0.8049755091518432,
        "step": 6245
    },
    {
        "loss": 2.0396,
        "grad_norm": 2.6234960556030273,
        "learning_rate": 0.00018004942820709326,
        "epoch": 0.8051044083526682,
        "step": 6246
    },
    {
        "loss": 2.3226,
        "grad_norm": 1.860467791557312,
        "learning_rate": 0.00018001294931738727,
        "epoch": 0.8052333075534932,
        "step": 6247
    },
    {
        "loss": 2.0196,
        "grad_norm": 1.4921058416366577,
        "learning_rate": 0.00017997644081050755,
        "epoch": 0.8053622067543181,
        "step": 6248
    },
    {
        "loss": 2.0768,
        "grad_norm": 1.5801647901535034,
        "learning_rate": 0.0001799399026999679,
        "epoch": 0.8054911059551431,
        "step": 6249
    },
    {
        "loss": 1.9763,
        "grad_norm": 2.1215429306030273,
        "learning_rate": 0.00017990333499929312,
        "epoch": 0.805620005155968,
        "step": 6250
    },
    {
        "loss": 2.7626,
        "grad_norm": 1.9679946899414062,
        "learning_rate": 0.0001798667377220189,
        "epoch": 0.805748904356793,
        "step": 6251
    },
    {
        "loss": 1.92,
        "grad_norm": 1.720231294631958,
        "learning_rate": 0.00017983011088169187,
        "epoch": 0.805877803557618,
        "step": 6252
    },
    {
        "loss": 1.3667,
        "grad_norm": 1.7628730535507202,
        "learning_rate": 0.00017979345449186965,
        "epoch": 0.8060067027584429,
        "step": 6253
    },
    {
        "loss": 2.1223,
        "grad_norm": 2.0113887786865234,
        "learning_rate": 0.00017975676856612073,
        "epoch": 0.8061356019592678,
        "step": 6254
    },
    {
        "loss": 2.5401,
        "grad_norm": 1.542482614517212,
        "learning_rate": 0.00017972005311802468,
        "epoch": 0.8062645011600929,
        "step": 6255
    },
    {
        "loss": 1.7405,
        "grad_norm": 1.3341158628463745,
        "learning_rate": 0.00017968330816117183,
        "epoch": 0.8063934003609178,
        "step": 6256
    },
    {
        "loss": 2.2824,
        "grad_norm": 1.7633209228515625,
        "learning_rate": 0.0001796465337091635,
        "epoch": 0.8065222995617427,
        "step": 6257
    },
    {
        "loss": 2.1811,
        "grad_norm": 2.504948377609253,
        "learning_rate": 0.00017960972977561194,
        "epoch": 0.8066511987625676,
        "step": 6258
    },
    {
        "loss": 1.5199,
        "grad_norm": 3.1232805252075195,
        "learning_rate": 0.00017957289637414033,
        "epoch": 0.8067800979633927,
        "step": 6259
    },
    {
        "loss": 2.1145,
        "grad_norm": 1.8284175395965576,
        "learning_rate": 0.00017953603351838267,
        "epoch": 0.8069089971642176,
        "step": 6260
    },
    {
        "loss": 1.7978,
        "grad_norm": 1.6273590326309204,
        "learning_rate": 0.00017949914122198393,
        "epoch": 0.8070378963650425,
        "step": 6261
    },
    {
        "loss": 1.5457,
        "grad_norm": 2.2172343730926514,
        "learning_rate": 0.00017946221949860007,
        "epoch": 0.8071667955658675,
        "step": 6262
    },
    {
        "loss": 0.9429,
        "grad_norm": 2.217510461807251,
        "learning_rate": 0.00017942526836189768,
        "epoch": 0.8072956947666925,
        "step": 6263
    },
    {
        "loss": 2.2529,
        "grad_norm": 1.4192259311676025,
        "learning_rate": 0.0001793882878255545,
        "epoch": 0.8074245939675174,
        "step": 6264
    },
    {
        "loss": 2.0111,
        "grad_norm": 2.499446153640747,
        "learning_rate": 0.00017935127790325906,
        "epoch": 0.8075534931683424,
        "step": 6265
    },
    {
        "loss": 1.1863,
        "grad_norm": 1.8881586790084839,
        "learning_rate": 0.0001793142386087107,
        "epoch": 0.8076823923691673,
        "step": 6266
    },
    {
        "loss": 2.2956,
        "grad_norm": 1.555473804473877,
        "learning_rate": 0.00017927716995561977,
        "epoch": 0.8078112915699923,
        "step": 6267
    },
    {
        "loss": 2.2072,
        "grad_norm": 1.4132479429244995,
        "learning_rate": 0.00017924007195770734,
        "epoch": 0.8079401907708172,
        "step": 6268
    },
    {
        "loss": 1.1089,
        "grad_norm": 2.9012982845306396,
        "learning_rate": 0.00017920294462870539,
        "epoch": 0.8080690899716422,
        "step": 6269
    },
    {
        "loss": 2.4327,
        "grad_norm": 1.5962395668029785,
        "learning_rate": 0.00017916578798235686,
        "epoch": 0.8081979891724671,
        "step": 6270
    },
    {
        "loss": 1.9394,
        "grad_norm": 2.3512020111083984,
        "learning_rate": 0.0001791286020324154,
        "epoch": 0.8083268883732921,
        "step": 6271
    },
    {
        "loss": 0.896,
        "grad_norm": 2.676837205886841,
        "learning_rate": 0.00017909138679264558,
        "epoch": 0.8084557875741171,
        "step": 6272
    },
    {
        "loss": 2.3098,
        "grad_norm": 2.0660645961761475,
        "learning_rate": 0.00017905414227682283,
        "epoch": 0.808584686774942,
        "step": 6273
    },
    {
        "loss": 1.3692,
        "grad_norm": 1.7261135578155518,
        "learning_rate": 0.0001790168684987333,
        "epoch": 0.8087135859757669,
        "step": 6274
    },
    {
        "loss": 2.738,
        "grad_norm": 1.599764108657837,
        "learning_rate": 0.00017897956547217417,
        "epoch": 0.8088424851765919,
        "step": 6275
    },
    {
        "loss": 1.9077,
        "grad_norm": 1.0700873136520386,
        "learning_rate": 0.00017894223321095326,
        "epoch": 0.8089713843774169,
        "step": 6276
    },
    {
        "loss": 1.6669,
        "grad_norm": 2.6083569526672363,
        "learning_rate": 0.0001789048717288893,
        "epoch": 0.8091002835782418,
        "step": 6277
    },
    {
        "loss": 1.6453,
        "grad_norm": 1.4927196502685547,
        "learning_rate": 0.00017886748103981175,
        "epoch": 0.8092291827790667,
        "step": 6278
    },
    {
        "loss": 0.3651,
        "grad_norm": 1.9772236347198486,
        "learning_rate": 0.00017883006115756113,
        "epoch": 0.8093580819798917,
        "step": 6279
    },
    {
        "loss": 2.0835,
        "grad_norm": 2.139434814453125,
        "learning_rate": 0.00017879261209598842,
        "epoch": 0.8094869811807167,
        "step": 6280
    },
    {
        "loss": 1.5085,
        "grad_norm": 1.659852385520935,
        "learning_rate": 0.00017875513386895556,
        "epoch": 0.8096158803815416,
        "step": 6281
    },
    {
        "loss": 2.131,
        "grad_norm": 1.5771697759628296,
        "learning_rate": 0.0001787176264903355,
        "epoch": 0.8097447795823666,
        "step": 6282
    },
    {
        "loss": 1.5315,
        "grad_norm": 2.7642860412597656,
        "learning_rate": 0.00017868008997401155,
        "epoch": 0.8098736787831915,
        "step": 6283
    },
    {
        "loss": 1.9636,
        "grad_norm": 2.300949811935425,
        "learning_rate": 0.00017864252433387817,
        "epoch": 0.8100025779840165,
        "step": 6284
    },
    {
        "loss": 2.035,
        "grad_norm": 1.7513421773910522,
        "learning_rate": 0.00017860492958384034,
        "epoch": 0.8101314771848415,
        "step": 6285
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.5784568786621094,
        "learning_rate": 0.00017856730573781405,
        "epoch": 0.8102603763856664,
        "step": 6286
    },
    {
        "loss": 1.7619,
        "grad_norm": 1.6744831800460815,
        "learning_rate": 0.0001785296528097259,
        "epoch": 0.8103892755864913,
        "step": 6287
    },
    {
        "loss": 1.9347,
        "grad_norm": 4.1005167961120605,
        "learning_rate": 0.00017849197081351323,
        "epoch": 0.8105181747873164,
        "step": 6288
    },
    {
        "loss": 1.6575,
        "grad_norm": 1.0875203609466553,
        "learning_rate": 0.00017845425976312432,
        "epoch": 0.8106470739881413,
        "step": 6289
    },
    {
        "loss": 2.1096,
        "grad_norm": 3.082768440246582,
        "learning_rate": 0.00017841651967251808,
        "epoch": 0.8107759731889662,
        "step": 6290
    },
    {
        "loss": 2.2071,
        "grad_norm": 2.1337831020355225,
        "learning_rate": 0.0001783787505556641,
        "epoch": 0.8109048723897911,
        "step": 6291
    },
    {
        "loss": 1.9702,
        "grad_norm": 1.8055272102355957,
        "learning_rate": 0.00017834095242654288,
        "epoch": 0.8110337715906162,
        "step": 6292
    },
    {
        "loss": 0.8545,
        "grad_norm": 1.5379455089569092,
        "learning_rate": 0.00017830312529914556,
        "epoch": 0.8111626707914411,
        "step": 6293
    },
    {
        "loss": 1.7895,
        "grad_norm": 2.033458948135376,
        "learning_rate": 0.000178265269187474,
        "epoch": 0.811291569992266,
        "step": 6294
    },
    {
        "loss": 1.1839,
        "grad_norm": 3.1972901821136475,
        "learning_rate": 0.00017822738410554084,
        "epoch": 0.811420469193091,
        "step": 6295
    },
    {
        "loss": 1.7234,
        "grad_norm": 1.6458091735839844,
        "learning_rate": 0.0001781894700673695,
        "epoch": 0.811549368393916,
        "step": 6296
    },
    {
        "loss": 2.145,
        "grad_norm": 2.0972769260406494,
        "learning_rate": 0.00017815152708699388,
        "epoch": 0.8116782675947409,
        "step": 6297
    },
    {
        "loss": 1.509,
        "grad_norm": 2.58318829536438,
        "learning_rate": 0.00017811355517845882,
        "epoch": 0.8118071667955659,
        "step": 6298
    },
    {
        "loss": 1.7317,
        "grad_norm": 2.184507131576538,
        "learning_rate": 0.00017807555435581993,
        "epoch": 0.8119360659963908,
        "step": 6299
    },
    {
        "loss": 2.2208,
        "grad_norm": 1.7613462209701538,
        "learning_rate": 0.0001780375246331432,
        "epoch": 0.8120649651972158,
        "step": 6300
    },
    {
        "loss": 2.0002,
        "grad_norm": 1.7892309427261353,
        "learning_rate": 0.00017799946602450563,
        "epoch": 0.8121938643980408,
        "step": 6301
    },
    {
        "loss": 1.6468,
        "grad_norm": 1.8536555767059326,
        "learning_rate": 0.00017796137854399476,
        "epoch": 0.8123227635988657,
        "step": 6302
    },
    {
        "loss": 2.1721,
        "grad_norm": 1.583103895187378,
        "learning_rate": 0.00017792326220570888,
        "epoch": 0.8124516627996906,
        "step": 6303
    },
    {
        "loss": 1.3449,
        "grad_norm": 2.593470811843872,
        "learning_rate": 0.0001778851170237569,
        "epoch": 0.8125805620005156,
        "step": 6304
    },
    {
        "loss": 1.7945,
        "grad_norm": 1.624977707862854,
        "learning_rate": 0.00017784694301225845,
        "epoch": 0.8127094612013406,
        "step": 6305
    },
    {
        "loss": 1.8566,
        "grad_norm": 1.8525159358978271,
        "learning_rate": 0.00017780874018534385,
        "epoch": 0.8128383604021655,
        "step": 6306
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.5330288410186768,
        "learning_rate": 0.000177770508557154,
        "epoch": 0.8129672596029904,
        "step": 6307
    },
    {
        "loss": 2.0289,
        "grad_norm": 1.459329605102539,
        "learning_rate": 0.00017773224814184054,
        "epoch": 0.8130961588038154,
        "step": 6308
    },
    {
        "loss": 2.0315,
        "grad_norm": 1.6052591800689697,
        "learning_rate": 0.0001776939589535658,
        "epoch": 0.8132250580046404,
        "step": 6309
    },
    {
        "loss": 2.5343,
        "grad_norm": 1.4602972269058228,
        "learning_rate": 0.00017765564100650263,
        "epoch": 0.8133539572054653,
        "step": 6310
    },
    {
        "loss": 0.9166,
        "grad_norm": 2.9204049110412598,
        "learning_rate": 0.00017761729431483466,
        "epoch": 0.8134828564062903,
        "step": 6311
    },
    {
        "loss": 1.8324,
        "grad_norm": 2.6687216758728027,
        "learning_rate": 0.00017757891889275603,
        "epoch": 0.8136117556071152,
        "step": 6312
    },
    {
        "loss": 2.001,
        "grad_norm": 4.175172805786133,
        "learning_rate": 0.0001775405147544717,
        "epoch": 0.8137406548079402,
        "step": 6313
    },
    {
        "loss": 1.3612,
        "grad_norm": 2.9737184047698975,
        "learning_rate": 0.000177502081914197,
        "epoch": 0.8138695540087652,
        "step": 6314
    },
    {
        "loss": 2.2066,
        "grad_norm": 2.068894386291504,
        "learning_rate": 0.00017746362038615807,
        "epoch": 0.8139984532095901,
        "step": 6315
    },
    {
        "loss": 1.9213,
        "grad_norm": 2.3147521018981934,
        "learning_rate": 0.00017742513018459177,
        "epoch": 0.814127352410415,
        "step": 6316
    },
    {
        "loss": 1.929,
        "grad_norm": 2.6477150917053223,
        "learning_rate": 0.00017738661132374523,
        "epoch": 0.81425625161124,
        "step": 6317
    },
    {
        "loss": 1.844,
        "grad_norm": 2.241198778152466,
        "learning_rate": 0.00017734806381787652,
        "epoch": 0.814385150812065,
        "step": 6318
    },
    {
        "loss": 2.2282,
        "grad_norm": 2.147855758666992,
        "learning_rate": 0.00017730948768125416,
        "epoch": 0.8145140500128899,
        "step": 6319
    },
    {
        "loss": 1.9758,
        "grad_norm": 2.2759485244750977,
        "learning_rate": 0.00017727088292815725,
        "epoch": 0.8146429492137148,
        "step": 6320
    },
    {
        "loss": 1.445,
        "grad_norm": 1.5868974924087524,
        "learning_rate": 0.0001772322495728755,
        "epoch": 0.8147718484145399,
        "step": 6321
    },
    {
        "loss": 0.6878,
        "grad_norm": 3.595644235610962,
        "learning_rate": 0.00017719358762970932,
        "epoch": 0.8149007476153648,
        "step": 6322
    },
    {
        "loss": 1.748,
        "grad_norm": 2.1025781631469727,
        "learning_rate": 0.0001771548971129696,
        "epoch": 0.8150296468161897,
        "step": 6323
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.6131972074508667,
        "learning_rate": 0.00017711617803697771,
        "epoch": 0.8151585460170147,
        "step": 6324
    },
    {
        "loss": 1.7212,
        "grad_norm": 2.3974688053131104,
        "learning_rate": 0.0001770774304160658,
        "epoch": 0.8152874452178397,
        "step": 6325
    },
    {
        "loss": 1.8469,
        "grad_norm": 1.290850281715393,
        "learning_rate": 0.0001770386542645765,
        "epoch": 0.8154163444186646,
        "step": 6326
    },
    {
        "loss": 2.2074,
        "grad_norm": 2.4169673919677734,
        "learning_rate": 0.00017699984959686295,
        "epoch": 0.8155452436194895,
        "step": 6327
    },
    {
        "loss": 2.043,
        "grad_norm": 1.6163301467895508,
        "learning_rate": 0.00017696101642728884,
        "epoch": 0.8156741428203145,
        "step": 6328
    },
    {
        "loss": 1.5136,
        "grad_norm": 2.6591012477874756,
        "learning_rate": 0.0001769221547702285,
        "epoch": 0.8158030420211395,
        "step": 6329
    },
    {
        "loss": 1.296,
        "grad_norm": 1.5798277854919434,
        "learning_rate": 0.0001768832646400668,
        "epoch": 0.8159319412219644,
        "step": 6330
    },
    {
        "loss": 1.5729,
        "grad_norm": 1.9799726009368896,
        "learning_rate": 0.00017684434605119898,
        "epoch": 0.8160608404227894,
        "step": 6331
    },
    {
        "loss": 1.551,
        "grad_norm": 2.2764153480529785,
        "learning_rate": 0.000176805399018031,
        "epoch": 0.8161897396236143,
        "step": 6332
    },
    {
        "loss": 2.255,
        "grad_norm": 2.0526340007781982,
        "learning_rate": 0.00017676642355497944,
        "epoch": 0.8163186388244393,
        "step": 6333
    },
    {
        "loss": 2.2003,
        "grad_norm": 2.0896337032318115,
        "learning_rate": 0.000176727419676471,
        "epoch": 0.8164475380252643,
        "step": 6334
    },
    {
        "loss": 0.6467,
        "grad_norm": 2.9781620502471924,
        "learning_rate": 0.00017668838739694334,
        "epoch": 0.8165764372260892,
        "step": 6335
    },
    {
        "loss": 1.9649,
        "grad_norm": 2.8566994667053223,
        "learning_rate": 0.00017664932673084436,
        "epoch": 0.8167053364269141,
        "step": 6336
    },
    {
        "loss": 1.2465,
        "grad_norm": 4.3944573402404785,
        "learning_rate": 0.00017661023769263256,
        "epoch": 0.8168342356277392,
        "step": 6337
    },
    {
        "loss": 1.1869,
        "grad_norm": 2.499112606048584,
        "learning_rate": 0.0001765711202967769,
        "epoch": 0.8169631348285641,
        "step": 6338
    },
    {
        "loss": 2.2943,
        "grad_norm": 3.035238265991211,
        "learning_rate": 0.00017653197455775696,
        "epoch": 0.817092034029389,
        "step": 6339
    },
    {
        "loss": 1.2419,
        "grad_norm": 2.4186220169067383,
        "learning_rate": 0.00017649280049006268,
        "epoch": 0.8172209332302139,
        "step": 6340
    },
    {
        "loss": 1.9411,
        "grad_norm": 1.8708335161209106,
        "learning_rate": 0.00017645359810819445,
        "epoch": 0.817349832431039,
        "step": 6341
    },
    {
        "loss": 2.6369,
        "grad_norm": 1.320798635482788,
        "learning_rate": 0.00017641436742666335,
        "epoch": 0.8174787316318639,
        "step": 6342
    },
    {
        "loss": 1.9629,
        "grad_norm": 2.394052505493164,
        "learning_rate": 0.00017637510845999076,
        "epoch": 0.8176076308326888,
        "step": 6343
    },
    {
        "loss": 2.1979,
        "grad_norm": 1.3670663833618164,
        "learning_rate": 0.00017633582122270853,
        "epoch": 0.8177365300335138,
        "step": 6344
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.558569312095642,
        "learning_rate": 0.00017629650572935902,
        "epoch": 0.8178654292343387,
        "step": 6345
    },
    {
        "loss": 2.088,
        "grad_norm": 2.312474250793457,
        "learning_rate": 0.00017625716199449508,
        "epoch": 0.8179943284351637,
        "step": 6346
    },
    {
        "loss": 1.4204,
        "grad_norm": 2.7593631744384766,
        "learning_rate": 0.00017621779003268,
        "epoch": 0.8181232276359887,
        "step": 6347
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.9094555377960205,
        "learning_rate": 0.00017617838985848746,
        "epoch": 0.8182521268368136,
        "step": 6348
    },
    {
        "loss": 2.1098,
        "grad_norm": 1.9164434671401978,
        "learning_rate": 0.00017613896148650153,
        "epoch": 0.8183810260376385,
        "step": 6349
    },
    {
        "loss": 1.9875,
        "grad_norm": 1.6224623918533325,
        "learning_rate": 0.00017609950493131706,
        "epoch": 0.8185099252384636,
        "step": 6350
    },
    {
        "loss": 1.4238,
        "grad_norm": 2.2799620628356934,
        "learning_rate": 0.0001760600202075388,
        "epoch": 0.8186388244392885,
        "step": 6351
    },
    {
        "loss": 2.3355,
        "grad_norm": 1.78542959690094,
        "learning_rate": 0.00017602050732978244,
        "epoch": 0.8187677236401134,
        "step": 6352
    },
    {
        "loss": 2.134,
        "grad_norm": 1.9530857801437378,
        "learning_rate": 0.00017598096631267374,
        "epoch": 0.8188966228409383,
        "step": 6353
    },
    {
        "loss": 1.4214,
        "grad_norm": 1.9878430366516113,
        "learning_rate": 0.00017594139717084899,
        "epoch": 0.8190255220417634,
        "step": 6354
    },
    {
        "loss": 1.8669,
        "grad_norm": 1.2551794052124023,
        "learning_rate": 0.0001759017999189549,
        "epoch": 0.8191544212425883,
        "step": 6355
    },
    {
        "loss": 2.2421,
        "grad_norm": 2.053067445755005,
        "learning_rate": 0.00017586217457164863,
        "epoch": 0.8192833204434132,
        "step": 6356
    },
    {
        "loss": 2.2839,
        "grad_norm": 1.5699526071548462,
        "learning_rate": 0.00017582252114359766,
        "epoch": 0.8194122196442382,
        "step": 6357
    },
    {
        "loss": 1.638,
        "grad_norm": 3.288114070892334,
        "learning_rate": 0.0001757828396494799,
        "epoch": 0.8195411188450632,
        "step": 6358
    },
    {
        "loss": 2.2441,
        "grad_norm": 2.8203718662261963,
        "learning_rate": 0.00017574313010398362,
        "epoch": 0.8196700180458881,
        "step": 6359
    },
    {
        "loss": 2.2554,
        "grad_norm": 1.529496669769287,
        "learning_rate": 0.00017570339252180755,
        "epoch": 0.819798917246713,
        "step": 6360
    },
    {
        "loss": 1.4015,
        "grad_norm": 2.651395320892334,
        "learning_rate": 0.00017566362691766072,
        "epoch": 0.819927816447538,
        "step": 6361
    },
    {
        "loss": 1.4255,
        "grad_norm": 2.7079224586486816,
        "learning_rate": 0.00017562383330626253,
        "epoch": 0.820056715648363,
        "step": 6362
    },
    {
        "loss": 1.9746,
        "grad_norm": 1.7418142557144165,
        "learning_rate": 0.00017558401170234277,
        "epoch": 0.820185614849188,
        "step": 6363
    },
    {
        "loss": 1.6844,
        "grad_norm": 3.7566699981689453,
        "learning_rate": 0.00017554416212064164,
        "epoch": 0.8203145140500129,
        "step": 6364
    },
    {
        "loss": 2.345,
        "grad_norm": 2.5499298572540283,
        "learning_rate": 0.00017550428457590965,
        "epoch": 0.8204434132508378,
        "step": 6365
    },
    {
        "loss": 1.852,
        "grad_norm": 1.2606921195983887,
        "learning_rate": 0.00017546437908290757,
        "epoch": 0.8205723124516628,
        "step": 6366
    },
    {
        "loss": 1.8143,
        "grad_norm": 1.8698581457138062,
        "learning_rate": 0.00017542444565640678,
        "epoch": 0.8207012116524878,
        "step": 6367
    },
    {
        "loss": 1.3785,
        "grad_norm": 1.8352056741714478,
        "learning_rate": 0.0001753844843111887,
        "epoch": 0.8208301108533127,
        "step": 6368
    },
    {
        "loss": 2.3384,
        "grad_norm": 2.8044824600219727,
        "learning_rate": 0.00017534449506204528,
        "epoch": 0.8209590100541376,
        "step": 6369
    },
    {
        "loss": 1.1959,
        "grad_norm": 2.348604202270508,
        "learning_rate": 0.00017530447792377866,
        "epoch": 0.8210879092549627,
        "step": 6370
    },
    {
        "loss": 1.8849,
        "grad_norm": 2.2005386352539062,
        "learning_rate": 0.00017526443291120142,
        "epoch": 0.8212168084557876,
        "step": 6371
    },
    {
        "loss": 1.4332,
        "grad_norm": 2.4892823696136475,
        "learning_rate": 0.00017522436003913634,
        "epoch": 0.8213457076566125,
        "step": 6372
    },
    {
        "loss": 1.9888,
        "grad_norm": 2.331197500228882,
        "learning_rate": 0.0001751842593224167,
        "epoch": 0.8214746068574375,
        "step": 6373
    },
    {
        "loss": 1.4598,
        "grad_norm": 1.966872215270996,
        "learning_rate": 0.00017514413077588593,
        "epoch": 0.8216035060582625,
        "step": 6374
    },
    {
        "loss": 2.2984,
        "grad_norm": 2.1948635578155518,
        "learning_rate": 0.00017510397441439774,
        "epoch": 0.8217324052590874,
        "step": 6375
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.8748480081558228,
        "learning_rate": 0.00017506379025281627,
        "epoch": 0.8218613044599123,
        "step": 6376
    },
    {
        "loss": 1.5606,
        "grad_norm": 2.9113996028900146,
        "learning_rate": 0.0001750235783060159,
        "epoch": 0.8219902036607373,
        "step": 6377
    },
    {
        "loss": 2.3023,
        "grad_norm": 2.6523733139038086,
        "learning_rate": 0.0001749833385888812,
        "epoch": 0.8221191028615623,
        "step": 6378
    },
    {
        "loss": 1.99,
        "grad_norm": 2.3155534267425537,
        "learning_rate": 0.0001749430711163071,
        "epoch": 0.8222480020623872,
        "step": 6379
    },
    {
        "loss": 1.5416,
        "grad_norm": 2.931598663330078,
        "learning_rate": 0.0001749027759031988,
        "epoch": 0.8223769012632122,
        "step": 6380
    },
    {
        "loss": 1.4976,
        "grad_norm": 2.506971836090088,
        "learning_rate": 0.0001748624529644718,
        "epoch": 0.8225058004640371,
        "step": 6381
    },
    {
        "loss": 1.5032,
        "grad_norm": 2.4772188663482666,
        "learning_rate": 0.00017482210231505184,
        "epoch": 0.822634699664862,
        "step": 6382
    },
    {
        "loss": 1.4459,
        "grad_norm": 2.4768271446228027,
        "learning_rate": 0.0001747817239698748,
        "epoch": 0.8227635988656871,
        "step": 6383
    },
    {
        "loss": 1.9075,
        "grad_norm": 1.7271217107772827,
        "learning_rate": 0.00017474131794388705,
        "epoch": 0.822892498066512,
        "step": 6384
    },
    {
        "loss": 1.8389,
        "grad_norm": 2.1143155097961426,
        "learning_rate": 0.00017470088425204503,
        "epoch": 0.8230213972673369,
        "step": 6385
    },
    {
        "loss": 2.3943,
        "grad_norm": 2.385758399963379,
        "learning_rate": 0.00017466042290931543,
        "epoch": 0.8231502964681618,
        "step": 6386
    },
    {
        "loss": 1.8562,
        "grad_norm": 2.0219109058380127,
        "learning_rate": 0.0001746199339306752,
        "epoch": 0.8232791956689869,
        "step": 6387
    },
    {
        "loss": 1.0026,
        "grad_norm": 2.980020046234131,
        "learning_rate": 0.00017457941733111163,
        "epoch": 0.8234080948698118,
        "step": 6388
    },
    {
        "loss": 1.3545,
        "grad_norm": 2.1404147148132324,
        "learning_rate": 0.00017453887312562195,
        "epoch": 0.8235369940706367,
        "step": 6389
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.470160722732544,
        "learning_rate": 0.00017449830132921395,
        "epoch": 0.8236658932714617,
        "step": 6390
    },
    {
        "loss": 2.3306,
        "grad_norm": 2.3614468574523926,
        "learning_rate": 0.0001744577019569054,
        "epoch": 0.8237947924722867,
        "step": 6391
    },
    {
        "loss": 2.2076,
        "grad_norm": 1.9345725774765015,
        "learning_rate": 0.00017441707502372436,
        "epoch": 0.8239236916731116,
        "step": 6392
    },
    {
        "loss": 1.7155,
        "grad_norm": 2.789936065673828,
        "learning_rate": 0.00017437642054470912,
        "epoch": 0.8240525908739366,
        "step": 6393
    },
    {
        "loss": 2.2896,
        "grad_norm": 1.8862441778182983,
        "learning_rate": 0.00017433573853490813,
        "epoch": 0.8241814900747615,
        "step": 6394
    },
    {
        "loss": 1.5589,
        "grad_norm": 2.293689489364624,
        "learning_rate": 0.00017429502900937998,
        "epoch": 0.8243103892755865,
        "step": 6395
    },
    {
        "loss": 1.5546,
        "grad_norm": 2.3635685443878174,
        "learning_rate": 0.0001742542919831935,
        "epoch": 0.8244392884764115,
        "step": 6396
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.049271821975708,
        "learning_rate": 0.00017421352747142764,
        "epoch": 0.8245681876772364,
        "step": 6397
    },
    {
        "loss": 1.8682,
        "grad_norm": 2.277477264404297,
        "learning_rate": 0.00017417273548917173,
        "epoch": 0.8246970868780613,
        "step": 6398
    },
    {
        "loss": 2.431,
        "grad_norm": 1.8560283184051514,
        "learning_rate": 0.00017413191605152503,
        "epoch": 0.8248259860788864,
        "step": 6399
    },
    {
        "loss": 1.388,
        "grad_norm": 2.0678162574768066,
        "learning_rate": 0.00017409106917359702,
        "epoch": 0.8249548852797113,
        "step": 6400
    },
    {
        "loss": 1.203,
        "grad_norm": 2.734819173812866,
        "learning_rate": 0.00017405019487050743,
        "epoch": 0.8250837844805362,
        "step": 6401
    },
    {
        "loss": 1.9081,
        "grad_norm": 1.894489049911499,
        "learning_rate": 0.00017400929315738604,
        "epoch": 0.8252126836813611,
        "step": 6402
    },
    {
        "loss": 1.3979,
        "grad_norm": 2.6686413288116455,
        "learning_rate": 0.00017396836404937283,
        "epoch": 0.8253415828821862,
        "step": 6403
    },
    {
        "loss": 2.2231,
        "grad_norm": 2.3958027362823486,
        "learning_rate": 0.00017392740756161785,
        "epoch": 0.8254704820830111,
        "step": 6404
    },
    {
        "loss": 1.499,
        "grad_norm": 1.7268757820129395,
        "learning_rate": 0.00017388642370928147,
        "epoch": 0.825599381283836,
        "step": 6405
    },
    {
        "loss": 1.7524,
        "grad_norm": 2.363773822784424,
        "learning_rate": 0.00017384541250753385,
        "epoch": 0.825728280484661,
        "step": 6406
    },
    {
        "loss": 0.7925,
        "grad_norm": 3.2287235260009766,
        "learning_rate": 0.00017380437397155566,
        "epoch": 0.825857179685486,
        "step": 6407
    },
    {
        "loss": 2.8085,
        "grad_norm": 1.4956414699554443,
        "learning_rate": 0.0001737633081165375,
        "epoch": 0.8259860788863109,
        "step": 6408
    },
    {
        "loss": 2.0246,
        "grad_norm": 2.10438871383667,
        "learning_rate": 0.00017372221495767995,
        "epoch": 0.8261149780871359,
        "step": 6409
    },
    {
        "loss": 1.3281,
        "grad_norm": 1.8425737619400024,
        "learning_rate": 0.00017368109451019402,
        "epoch": 0.8262438772879608,
        "step": 6410
    },
    {
        "loss": 2.0622,
        "grad_norm": 2.6672134399414062,
        "learning_rate": 0.00017363994678930048,
        "epoch": 0.8263727764887858,
        "step": 6411
    },
    {
        "loss": 1.999,
        "grad_norm": 1.8135583400726318,
        "learning_rate": 0.0001735987718102305,
        "epoch": 0.8265016756896107,
        "step": 6412
    },
    {
        "loss": 1.7655,
        "grad_norm": 1.8558721542358398,
        "learning_rate": 0.00017355756958822505,
        "epoch": 0.8266305748904357,
        "step": 6413
    },
    {
        "loss": 1.3366,
        "grad_norm": 2.771801471710205,
        "learning_rate": 0.0001735163401385354,
        "epoch": 0.8267594740912606,
        "step": 6414
    },
    {
        "loss": 2.1066,
        "grad_norm": 1.6974947452545166,
        "learning_rate": 0.00017347508347642282,
        "epoch": 0.8268883732920856,
        "step": 6415
    },
    {
        "loss": 2.1068,
        "grad_norm": 1.7545721530914307,
        "learning_rate": 0.00017343379961715872,
        "epoch": 0.8270172724929106,
        "step": 6416
    },
    {
        "loss": 1.2724,
        "grad_norm": 2.2515785694122314,
        "learning_rate": 0.00017339248857602436,
        "epoch": 0.8271461716937355,
        "step": 6417
    },
    {
        "loss": 1.8614,
        "grad_norm": 1.2898625135421753,
        "learning_rate": 0.0001733511503683114,
        "epoch": 0.8272750708945604,
        "step": 6418
    },
    {
        "loss": 1.2229,
        "grad_norm": 4.500651836395264,
        "learning_rate": 0.0001733097850093213,
        "epoch": 0.8274039700953854,
        "step": 6419
    },
    {
        "loss": 2.0571,
        "grad_norm": 1.8288164138793945,
        "learning_rate": 0.00017326839251436562,
        "epoch": 0.8275328692962104,
        "step": 6420
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.5808823108673096,
        "learning_rate": 0.00017322697289876598,
        "epoch": 0.8276617684970353,
        "step": 6421
    },
    {
        "loss": 1.8405,
        "grad_norm": 1.876222848892212,
        "learning_rate": 0.00017318552617785413,
        "epoch": 0.8277906676978602,
        "step": 6422
    },
    {
        "loss": 1.9158,
        "grad_norm": 2.9825267791748047,
        "learning_rate": 0.00017314405236697165,
        "epoch": 0.8279195668986852,
        "step": 6423
    },
    {
        "loss": 2.0635,
        "grad_norm": 1.556760549545288,
        "learning_rate": 0.0001731025514814703,
        "epoch": 0.8280484660995102,
        "step": 6424
    },
    {
        "loss": 1.8016,
        "grad_norm": 2.4172847270965576,
        "learning_rate": 0.000173061023536712,
        "epoch": 0.8281773653003351,
        "step": 6425
    },
    {
        "loss": 2.0159,
        "grad_norm": 2.3753514289855957,
        "learning_rate": 0.00017301946854806824,
        "epoch": 0.8283062645011601,
        "step": 6426
    },
    {
        "loss": 2.0939,
        "grad_norm": 2.421976089477539,
        "learning_rate": 0.00017297788653092101,
        "epoch": 0.828435163701985,
        "step": 6427
    },
    {
        "loss": 2.4342,
        "grad_norm": 1.9389065504074097,
        "learning_rate": 0.00017293627750066204,
        "epoch": 0.82856406290281,
        "step": 6428
    },
    {
        "loss": 0.8647,
        "grad_norm": 2.336333751678467,
        "learning_rate": 0.00017289464147269307,
        "epoch": 0.828692962103635,
        "step": 6429
    },
    {
        "loss": 2.323,
        "grad_norm": 1.3862793445587158,
        "learning_rate": 0.0001728529784624259,
        "epoch": 0.8288218613044599,
        "step": 6430
    },
    {
        "loss": 1.8863,
        "grad_norm": 2.3432278633117676,
        "learning_rate": 0.0001728112884852822,
        "epoch": 0.8289507605052848,
        "step": 6431
    },
    {
        "loss": 2.0471,
        "grad_norm": 1.0611162185668945,
        "learning_rate": 0.00017276957155669387,
        "epoch": 0.8290796597061099,
        "step": 6432
    },
    {
        "loss": 0.8031,
        "grad_norm": 2.0176384449005127,
        "learning_rate": 0.00017272782769210253,
        "epoch": 0.8292085589069348,
        "step": 6433
    },
    {
        "loss": 2.2554,
        "grad_norm": 1.5088564157485962,
        "learning_rate": 0.0001726860569069599,
        "epoch": 0.8293374581077597,
        "step": 6434
    },
    {
        "loss": 0.9025,
        "grad_norm": 3.174309492111206,
        "learning_rate": 0.00017264425921672763,
        "epoch": 0.8294663573085846,
        "step": 6435
    },
    {
        "loss": 1.3236,
        "grad_norm": 2.195686101913452,
        "learning_rate": 0.00017260243463687735,
        "epoch": 0.8295952565094097,
        "step": 6436
    },
    {
        "loss": 2.3968,
        "grad_norm": 1.768433928489685,
        "learning_rate": 0.0001725605831828906,
        "epoch": 0.8297241557102346,
        "step": 6437
    },
    {
        "loss": 1.98,
        "grad_norm": 2.47621488571167,
        "learning_rate": 0.0001725187048702589,
        "epoch": 0.8298530549110595,
        "step": 6438
    },
    {
        "loss": 1.0663,
        "grad_norm": 2.244654655456543,
        "learning_rate": 0.00017247679971448378,
        "epoch": 0.8299819541118845,
        "step": 6439
    },
    {
        "loss": 1.696,
        "grad_norm": 2.276906728744507,
        "learning_rate": 0.00017243486773107646,
        "epoch": 0.8301108533127095,
        "step": 6440
    },
    {
        "loss": 1.8453,
        "grad_norm": 2.7586395740509033,
        "learning_rate": 0.00017239290893555838,
        "epoch": 0.8302397525135344,
        "step": 6441
    },
    {
        "loss": 2.1071,
        "grad_norm": 2.1982762813568115,
        "learning_rate": 0.00017235092334346087,
        "epoch": 0.8303686517143594,
        "step": 6442
    },
    {
        "loss": 2.2141,
        "grad_norm": 1.7911107540130615,
        "learning_rate": 0.00017230891097032492,
        "epoch": 0.8304975509151843,
        "step": 6443
    },
    {
        "loss": 1.0992,
        "grad_norm": 2.2202560901641846,
        "learning_rate": 0.00017226687183170171,
        "epoch": 0.8306264501160093,
        "step": 6444
    },
    {
        "loss": 1.2368,
        "grad_norm": 1.721732497215271,
        "learning_rate": 0.00017222480594315223,
        "epoch": 0.8307553493168343,
        "step": 6445
    },
    {
        "loss": 1.4134,
        "grad_norm": 2.3374545574188232,
        "learning_rate": 0.00017218271332024738,
        "epoch": 0.8308842485176592,
        "step": 6446
    },
    {
        "loss": 1.8919,
        "grad_norm": 1.6129398345947266,
        "learning_rate": 0.00017214059397856782,
        "epoch": 0.8310131477184841,
        "step": 6447
    },
    {
        "loss": 1.2973,
        "grad_norm": 1.7998533248901367,
        "learning_rate": 0.00017209844793370437,
        "epoch": 0.8311420469193092,
        "step": 6448
    },
    {
        "loss": 1.9302,
        "grad_norm": 1.870303988456726,
        "learning_rate": 0.00017205627520125756,
        "epoch": 0.8312709461201341,
        "step": 6449
    },
    {
        "loss": 2.307,
        "grad_norm": 2.2430379390716553,
        "learning_rate": 0.00017201407579683773,
        "epoch": 0.831399845320959,
        "step": 6450
    },
    {
        "loss": 1.9561,
        "grad_norm": 2.220132827758789,
        "learning_rate": 0.00017197184973606532,
        "epoch": 0.8315287445217839,
        "step": 6451
    },
    {
        "loss": 2.0499,
        "grad_norm": 2.768059253692627,
        "learning_rate": 0.00017192959703457046,
        "epoch": 0.831657643722609,
        "step": 6452
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.655574321746826,
        "learning_rate": 0.00017188731770799315,
        "epoch": 0.8317865429234339,
        "step": 6453
    },
    {
        "loss": 2.4337,
        "grad_norm": 1.663326382637024,
        "learning_rate": 0.00017184501177198338,
        "epoch": 0.8319154421242588,
        "step": 6454
    },
    {
        "loss": 1.8934,
        "grad_norm": 1.6882007122039795,
        "learning_rate": 0.0001718026792422007,
        "epoch": 0.8320443413250838,
        "step": 6455
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.4074640274047852,
        "learning_rate": 0.00017176032013431497,
        "epoch": 0.8321732405259087,
        "step": 6456
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.3671413660049438,
        "learning_rate": 0.00017171793446400536,
        "epoch": 0.8323021397267337,
        "step": 6457
    },
    {
        "loss": 1.9288,
        "grad_norm": 1.8679078817367554,
        "learning_rate": 0.00017167552224696126,
        "epoch": 0.8324310389275587,
        "step": 6458
    },
    {
        "loss": 2.7879,
        "grad_norm": 1.4324662685394287,
        "learning_rate": 0.00017163308349888177,
        "epoch": 0.8325599381283836,
        "step": 6459
    },
    {
        "loss": 2.1328,
        "grad_norm": 1.5667210817337036,
        "learning_rate": 0.0001715906182354757,
        "epoch": 0.8326888373292085,
        "step": 6460
    },
    {
        "loss": 2.0148,
        "grad_norm": 2.6962859630584717,
        "learning_rate": 0.0001715481264724619,
        "epoch": 0.8328177365300335,
        "step": 6461
    },
    {
        "loss": 1.766,
        "grad_norm": 1.8290058374404907,
        "learning_rate": 0.00017150560822556878,
        "epoch": 0.8329466357308585,
        "step": 6462
    },
    {
        "loss": 1.7641,
        "grad_norm": 2.4622511863708496,
        "learning_rate": 0.0001714630635105347,
        "epoch": 0.8330755349316834,
        "step": 6463
    },
    {
        "loss": 2.1597,
        "grad_norm": 1.6530994176864624,
        "learning_rate": 0.0001714204923431078,
        "epoch": 0.8332044341325083,
        "step": 6464
    },
    {
        "loss": 1.6861,
        "grad_norm": 2.0794687271118164,
        "learning_rate": 0.00017137789473904602,
        "epoch": 0.8333333333333334,
        "step": 6465
    },
    {
        "loss": 1.9829,
        "grad_norm": 3.9031646251678467,
        "learning_rate": 0.00017133527071411706,
        "epoch": 0.8334622325341583,
        "step": 6466
    },
    {
        "loss": 1.7897,
        "grad_norm": 1.9508529901504517,
        "learning_rate": 0.00017129262028409834,
        "epoch": 0.8335911317349832,
        "step": 6467
    },
    {
        "loss": 1.3669,
        "grad_norm": 2.722912549972534,
        "learning_rate": 0.00017124994346477722,
        "epoch": 0.8337200309358082,
        "step": 6468
    },
    {
        "loss": 1.5824,
        "grad_norm": 1.6305216550827026,
        "learning_rate": 0.0001712072402719507,
        "epoch": 0.8338489301366332,
        "step": 6469
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.4509382247924805,
        "learning_rate": 0.0001711645107214255,
        "epoch": 0.8339778293374581,
        "step": 6470
    },
    {
        "loss": 1.9801,
        "grad_norm": 1.378042459487915,
        "learning_rate": 0.00017112175482901826,
        "epoch": 0.834106728538283,
        "step": 6471
    },
    {
        "loss": 1.274,
        "grad_norm": 3.6512324810028076,
        "learning_rate": 0.00017107897261055518,
        "epoch": 0.834235627739108,
        "step": 6472
    },
    {
        "loss": 1.7565,
        "grad_norm": 2.2102859020233154,
        "learning_rate": 0.00017103616408187247,
        "epoch": 0.834364526939933,
        "step": 6473
    },
    {
        "loss": 1.8818,
        "grad_norm": 2.3588955402374268,
        "learning_rate": 0.0001709933292588157,
        "epoch": 0.8344934261407579,
        "step": 6474
    },
    {
        "loss": 1.4076,
        "grad_norm": 2.3264613151550293,
        "learning_rate": 0.00017095046815724044,
        "epoch": 0.8346223253415829,
        "step": 6475
    },
    {
        "loss": 2.3502,
        "grad_norm": 2.3128767013549805,
        "learning_rate": 0.0001709075807930121,
        "epoch": 0.8347512245424078,
        "step": 6476
    },
    {
        "loss": 1.4068,
        "grad_norm": 2.2642152309417725,
        "learning_rate": 0.0001708646671820054,
        "epoch": 0.8348801237432328,
        "step": 6477
    },
    {
        "loss": 2.1849,
        "grad_norm": 2.3745439052581787,
        "learning_rate": 0.00017082172734010517,
        "epoch": 0.8350090229440578,
        "step": 6478
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.1017067432403564,
        "learning_rate": 0.00017077876128320578,
        "epoch": 0.8351379221448827,
        "step": 6479
    },
    {
        "loss": 1.3026,
        "grad_norm": 2.6658127307891846,
        "learning_rate": 0.00017073576902721128,
        "epoch": 0.8352668213457076,
        "step": 6480
    },
    {
        "loss": 1.6731,
        "grad_norm": 2.0855746269226074,
        "learning_rate": 0.00017069275058803545,
        "epoch": 0.8353957205465327,
        "step": 6481
    },
    {
        "loss": 2.1088,
        "grad_norm": 2.757793426513672,
        "learning_rate": 0.0001706497059816018,
        "epoch": 0.8355246197473576,
        "step": 6482
    },
    {
        "loss": 2.1102,
        "grad_norm": 2.310497760772705,
        "learning_rate": 0.00017060663522384354,
        "epoch": 0.8356535189481825,
        "step": 6483
    },
    {
        "loss": 2.1947,
        "grad_norm": 1.3984521627426147,
        "learning_rate": 0.00017056353833070338,
        "epoch": 0.8357824181490074,
        "step": 6484
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.77812659740448,
        "learning_rate": 0.00017052041531813396,
        "epoch": 0.8359113173498325,
        "step": 6485
    },
    {
        "loss": 1.4891,
        "grad_norm": 2.8168418407440186,
        "learning_rate": 0.00017047726620209746,
        "epoch": 0.8360402165506574,
        "step": 6486
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.3665252923965454,
        "learning_rate": 0.0001704340909985657,
        "epoch": 0.8361691157514823,
        "step": 6487
    },
    {
        "loss": 1.8423,
        "grad_norm": 1.1133724451065063,
        "learning_rate": 0.0001703908897235202,
        "epoch": 0.8362980149523073,
        "step": 6488
    },
    {
        "loss": 2.0148,
        "grad_norm": 1.7944258451461792,
        "learning_rate": 0.00017034766239295207,
        "epoch": 0.8364269141531323,
        "step": 6489
    },
    {
        "loss": 1.435,
        "grad_norm": 2.668952703475952,
        "learning_rate": 0.00017030440902286218,
        "epoch": 0.8365558133539572,
        "step": 6490
    },
    {
        "loss": 1.9,
        "grad_norm": 1.5352932214736938,
        "learning_rate": 0.00017026112962926094,
        "epoch": 0.8366847125547822,
        "step": 6491
    },
    {
        "loss": 2.1094,
        "grad_norm": 1.1848307847976685,
        "learning_rate": 0.00017021782422816837,
        "epoch": 0.8368136117556071,
        "step": 6492
    },
    {
        "loss": 1.73,
        "grad_norm": 2.11883282661438,
        "learning_rate": 0.00017017449283561432,
        "epoch": 0.836942510956432,
        "step": 6493
    },
    {
        "loss": 1.9082,
        "grad_norm": 1.9706852436065674,
        "learning_rate": 0.00017013113546763794,
        "epoch": 0.8370714101572571,
        "step": 6494
    },
    {
        "loss": 1.7902,
        "grad_norm": 1.762709379196167,
        "learning_rate": 0.00017008775214028828,
        "epoch": 0.837200309358082,
        "step": 6495
    },
    {
        "loss": 1.1357,
        "grad_norm": 2.0605409145355225,
        "learning_rate": 0.00017004434286962386,
        "epoch": 0.8373292085589069,
        "step": 6496
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.0040526390075684,
        "learning_rate": 0.0001700009076717128,
        "epoch": 0.8374581077597318,
        "step": 6497
    },
    {
        "loss": 2.2543,
        "grad_norm": 1.213452696800232,
        "learning_rate": 0.0001699574465626328,
        "epoch": 0.8375870069605569,
        "step": 6498
    },
    {
        "loss": 1.5308,
        "grad_norm": 2.606337070465088,
        "learning_rate": 0.00016991395955847133,
        "epoch": 0.8377159061613818,
        "step": 6499
    },
    {
        "loss": 1.0738,
        "grad_norm": 2.5308785438537598,
        "learning_rate": 0.00016987044667532523,
        "epoch": 0.8378448053622067,
        "step": 6500
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.8108316659927368,
        "learning_rate": 0.00016982690792930095,
        "epoch": 0.8379737045630317,
        "step": 6501
    },
    {
        "loss": 2.0565,
        "grad_norm": 1.15647292137146,
        "learning_rate": 0.0001697833433365147,
        "epoch": 0.8381026037638567,
        "step": 6502
    },
    {
        "loss": 1.6837,
        "grad_norm": 2.589550733566284,
        "learning_rate": 0.000169739752913092,
        "epoch": 0.8382315029646816,
        "step": 6503
    },
    {
        "loss": 1.8614,
        "grad_norm": 1.7393543720245361,
        "learning_rate": 0.00016969613667516811,
        "epoch": 0.8383604021655066,
        "step": 6504
    },
    {
        "loss": 2.2414,
        "grad_norm": 1.9248141050338745,
        "learning_rate": 0.00016965249463888783,
        "epoch": 0.8384893013663315,
        "step": 6505
    },
    {
        "loss": 1.6014,
        "grad_norm": 1.9344733953475952,
        "learning_rate": 0.0001696088268204053,
        "epoch": 0.8386182005671565,
        "step": 6506
    },
    {
        "loss": 1.7997,
        "grad_norm": 2.5709195137023926,
        "learning_rate": 0.00016956513323588453,
        "epoch": 0.8387470997679815,
        "step": 6507
    },
    {
        "loss": 1.6787,
        "grad_norm": 2.2996044158935547,
        "learning_rate": 0.00016952141390149885,
        "epoch": 0.8388759989688064,
        "step": 6508
    },
    {
        "loss": 1.7281,
        "grad_norm": 2.6624345779418945,
        "learning_rate": 0.0001694776688334312,
        "epoch": 0.8390048981696313,
        "step": 6509
    },
    {
        "loss": 2.1932,
        "grad_norm": 1.8918027877807617,
        "learning_rate": 0.00016943389804787402,
        "epoch": 0.8391337973704563,
        "step": 6510
    },
    {
        "loss": 1.5213,
        "grad_norm": 2.3814504146575928,
        "learning_rate": 0.00016939010156102926,
        "epoch": 0.8392626965712813,
        "step": 6511
    },
    {
        "loss": 2.1816,
        "grad_norm": 2.6202244758605957,
        "learning_rate": 0.00016934627938910843,
        "epoch": 0.8393915957721062,
        "step": 6512
    },
    {
        "loss": 2.1307,
        "grad_norm": 2.162287473678589,
        "learning_rate": 0.00016930243154833244,
        "epoch": 0.8395204949729311,
        "step": 6513
    },
    {
        "loss": 1.7204,
        "grad_norm": 2.179145097732544,
        "learning_rate": 0.00016925855805493192,
        "epoch": 0.8396493941737562,
        "step": 6514
    },
    {
        "loss": 2.0628,
        "grad_norm": 2.068450927734375,
        "learning_rate": 0.00016921465892514665,
        "epoch": 0.8397782933745811,
        "step": 6515
    },
    {
        "loss": 1.699,
        "grad_norm": 1.849570631980896,
        "learning_rate": 0.00016917073417522626,
        "epoch": 0.839907192575406,
        "step": 6516
    },
    {
        "loss": 0.7488,
        "grad_norm": 1.993855595588684,
        "learning_rate": 0.0001691267838214297,
        "epoch": 0.840036091776231,
        "step": 6517
    },
    {
        "loss": 1.6084,
        "grad_norm": 2.369844913482666,
        "learning_rate": 0.00016908280788002528,
        "epoch": 0.840164990977056,
        "step": 6518
    },
    {
        "loss": 2.2781,
        "grad_norm": 1.2530227899551392,
        "learning_rate": 0.00016903880636729102,
        "epoch": 0.8402938901778809,
        "step": 6519
    },
    {
        "loss": 1.9738,
        "grad_norm": 2.2244925498962402,
        "learning_rate": 0.00016899477929951427,
        "epoch": 0.8404227893787058,
        "step": 6520
    },
    {
        "loss": 1.9098,
        "grad_norm": 1.6900925636291504,
        "learning_rate": 0.00016895072669299185,
        "epoch": 0.8405516885795308,
        "step": 6521
    },
    {
        "loss": 1.826,
        "grad_norm": 1.808212161064148,
        "learning_rate": 0.00016890664856403004,
        "epoch": 0.8406805877803558,
        "step": 6522
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.5991812944412231,
        "learning_rate": 0.00016886254492894448,
        "epoch": 0.8408094869811807,
        "step": 6523
    },
    {
        "loss": 1.017,
        "grad_norm": 1.9468085765838623,
        "learning_rate": 0.0001688184158040605,
        "epoch": 0.8409383861820057,
        "step": 6524
    },
    {
        "loss": 2.0201,
        "grad_norm": 1.619256854057312,
        "learning_rate": 0.0001687742612057126,
        "epoch": 0.8410672853828306,
        "step": 6525
    },
    {
        "loss": 1.9964,
        "grad_norm": 1.3868106603622437,
        "learning_rate": 0.00016873008115024485,
        "epoch": 0.8411961845836555,
        "step": 6526
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.499265670776367,
        "learning_rate": 0.00016868587565401072,
        "epoch": 0.8413250837844806,
        "step": 6527
    },
    {
        "loss": 1.6048,
        "grad_norm": 1.8850593566894531,
        "learning_rate": 0.00016864164473337305,
        "epoch": 0.8414539829853055,
        "step": 6528
    },
    {
        "loss": 1.8781,
        "grad_norm": 2.3162641525268555,
        "learning_rate": 0.00016859738840470414,
        "epoch": 0.8415828821861304,
        "step": 6529
    },
    {
        "loss": 2.0802,
        "grad_norm": 2.8622968196868896,
        "learning_rate": 0.00016855310668438563,
        "epoch": 0.8417117813869553,
        "step": 6530
    },
    {
        "loss": 1.9646,
        "grad_norm": 2.2467691898345947,
        "learning_rate": 0.00016850879958880879,
        "epoch": 0.8418406805877804,
        "step": 6531
    },
    {
        "loss": 2.0457,
        "grad_norm": 2.423441171646118,
        "learning_rate": 0.00016846446713437386,
        "epoch": 0.8419695797886053,
        "step": 6532
    },
    {
        "loss": 1.8684,
        "grad_norm": 2.2616093158721924,
        "learning_rate": 0.0001684201093374908,
        "epoch": 0.8420984789894302,
        "step": 6533
    },
    {
        "loss": 2.1492,
        "grad_norm": 2.048941135406494,
        "learning_rate": 0.000168375726214579,
        "epoch": 0.8422273781902552,
        "step": 6534
    },
    {
        "loss": 2.2157,
        "grad_norm": 2.5785722732543945,
        "learning_rate": 0.00016833131778206685,
        "epoch": 0.8423562773910802,
        "step": 6535
    },
    {
        "loss": 2.0299,
        "grad_norm": 2.56885027885437,
        "learning_rate": 0.0001682868840563925,
        "epoch": 0.8424851765919051,
        "step": 6536
    },
    {
        "loss": 2.4593,
        "grad_norm": 1.3057574033737183,
        "learning_rate": 0.00016824242505400329,
        "epoch": 0.8426140757927301,
        "step": 6537
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.3744105100631714,
        "learning_rate": 0.0001681979407913559,
        "epoch": 0.842742974993555,
        "step": 6538
    },
    {
        "loss": 2.667,
        "grad_norm": 1.6890208721160889,
        "learning_rate": 0.0001681534312849164,
        "epoch": 0.84287187419438,
        "step": 6539
    },
    {
        "loss": 1.7217,
        "grad_norm": 2.4419145584106445,
        "learning_rate": 0.00016810889655116015,
        "epoch": 0.843000773395205,
        "step": 6540
    },
    {
        "loss": 1.9987,
        "grad_norm": 1.7048932313919067,
        "learning_rate": 0.00016806433660657196,
        "epoch": 0.8431296725960299,
        "step": 6541
    },
    {
        "loss": 1.4119,
        "grad_norm": 3.0425446033477783,
        "learning_rate": 0.0001680197514676459,
        "epoch": 0.8432585717968548,
        "step": 6542
    },
    {
        "loss": 1.6826,
        "grad_norm": 2.3439481258392334,
        "learning_rate": 0.00016797514115088525,
        "epoch": 0.8433874709976799,
        "step": 6543
    },
    {
        "loss": 1.5797,
        "grad_norm": 2.0507652759552,
        "learning_rate": 0.00016793050567280292,
        "epoch": 0.8435163701985048,
        "step": 6544
    },
    {
        "loss": 1.7761,
        "grad_norm": 1.994795799255371,
        "learning_rate": 0.00016788584504992088,
        "epoch": 0.8436452693993297,
        "step": 6545
    },
    {
        "loss": 1.7329,
        "grad_norm": 1.395871639251709,
        "learning_rate": 0.0001678411592987704,
        "epoch": 0.8437741686001546,
        "step": 6546
    },
    {
        "loss": 1.9765,
        "grad_norm": 1.7625787258148193,
        "learning_rate": 0.00016779644843589216,
        "epoch": 0.8439030678009797,
        "step": 6547
    },
    {
        "loss": 1.6725,
        "grad_norm": 2.117854356765747,
        "learning_rate": 0.0001677517124778362,
        "epoch": 0.8440319670018046,
        "step": 6548
    },
    {
        "loss": 1.6939,
        "grad_norm": 2.2937440872192383,
        "learning_rate": 0.00016770695144116156,
        "epoch": 0.8441608662026295,
        "step": 6549
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.1986513137817383,
        "learning_rate": 0.00016766216534243683,
        "epoch": 0.8442897654034545,
        "step": 6550
    },
    {
        "loss": 1.8532,
        "grad_norm": 2.1062278747558594,
        "learning_rate": 0.0001676173541982399,
        "epoch": 0.8444186646042795,
        "step": 6551
    },
    {
        "loss": 2.6108,
        "grad_norm": 2.416903018951416,
        "learning_rate": 0.00016757251802515768,
        "epoch": 0.8445475638051044,
        "step": 6552
    },
    {
        "loss": 1.6692,
        "grad_norm": 3.6907546520233154,
        "learning_rate": 0.0001675276568397866,
        "epoch": 0.8446764630059294,
        "step": 6553
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.590510606765747,
        "learning_rate": 0.00016748277065873216,
        "epoch": 0.8448053622067543,
        "step": 6554
    },
    {
        "loss": 1.9219,
        "grad_norm": 3.260964870452881,
        "learning_rate": 0.0001674378594986093,
        "epoch": 0.8449342614075793,
        "step": 6555
    },
    {
        "loss": 1.6512,
        "grad_norm": 2.6439716815948486,
        "learning_rate": 0.00016739292337604198,
        "epoch": 0.8450631606084043,
        "step": 6556
    },
    {
        "loss": 2.1083,
        "grad_norm": 2.0652027130126953,
        "learning_rate": 0.00016734796230766352,
        "epoch": 0.8451920598092292,
        "step": 6557
    },
    {
        "loss": 2.0528,
        "grad_norm": 1.8495830297470093,
        "learning_rate": 0.0001673029763101166,
        "epoch": 0.8453209590100541,
        "step": 6558
    },
    {
        "loss": 2.6259,
        "grad_norm": 1.94564688205719,
        "learning_rate": 0.0001672579654000529,
        "epoch": 0.8454498582108791,
        "step": 6559
    },
    {
        "loss": 1.1315,
        "grad_norm": 2.2348363399505615,
        "learning_rate": 0.0001672129295941334,
        "epoch": 0.8455787574117041,
        "step": 6560
    },
    {
        "loss": 1.1149,
        "grad_norm": 2.412259817123413,
        "learning_rate": 0.00016716786890902844,
        "epoch": 0.845707656612529,
        "step": 6561
    },
    {
        "loss": 1.9909,
        "grad_norm": 2.587381601333618,
        "learning_rate": 0.00016712278336141734,
        "epoch": 0.8458365558133539,
        "step": 6562
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.412514567375183,
        "learning_rate": 0.00016707767296798878,
        "epoch": 0.8459654550141789,
        "step": 6563
    },
    {
        "loss": 2.3996,
        "grad_norm": 1.5658848285675049,
        "learning_rate": 0.0001670325377454405,
        "epoch": 0.8460943542150039,
        "step": 6564
    },
    {
        "loss": 1.7948,
        "grad_norm": 1.6823973655700684,
        "learning_rate": 0.0001669873777104797,
        "epoch": 0.8462232534158288,
        "step": 6565
    },
    {
        "loss": 1.5631,
        "grad_norm": 2.306180715560913,
        "learning_rate": 0.00016694219287982236,
        "epoch": 0.8463521526166538,
        "step": 6566
    },
    {
        "loss": 2.2592,
        "grad_norm": 1.3159452676773071,
        "learning_rate": 0.00016689698327019395,
        "epoch": 0.8464810518174787,
        "step": 6567
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.425092935562134,
        "learning_rate": 0.00016685174889832918,
        "epoch": 0.8466099510183037,
        "step": 6568
    },
    {
        "loss": 1.9433,
        "grad_norm": 1.5555987358093262,
        "learning_rate": 0.00016680648978097148,
        "epoch": 0.8467388502191286,
        "step": 6569
    },
    {
        "loss": 1.603,
        "grad_norm": 2.5458245277404785,
        "learning_rate": 0.00016676120593487394,
        "epoch": 0.8468677494199536,
        "step": 6570
    },
    {
        "loss": 2.0092,
        "grad_norm": 2.4783177375793457,
        "learning_rate": 0.00016671589737679855,
        "epoch": 0.8469966486207785,
        "step": 6571
    },
    {
        "loss": 1.1907,
        "grad_norm": 2.652780055999756,
        "learning_rate": 0.00016667056412351646,
        "epoch": 0.8471255478216035,
        "step": 6572
    },
    {
        "loss": 2.2815,
        "grad_norm": 2.209149122238159,
        "learning_rate": 0.00016662520619180793,
        "epoch": 0.8472544470224285,
        "step": 6573
    },
    {
        "loss": 1.5557,
        "grad_norm": 3.0589792728424072,
        "learning_rate": 0.0001665798235984625,
        "epoch": 0.8473833462232534,
        "step": 6574
    },
    {
        "loss": 0.9608,
        "grad_norm": 2.2699310779571533,
        "learning_rate": 0.00016653441636027875,
        "epoch": 0.8475122454240783,
        "step": 6575
    },
    {
        "loss": 1.5438,
        "grad_norm": 2.1633992195129395,
        "learning_rate": 0.00016648898449406432,
        "epoch": 0.8476411446249034,
        "step": 6576
    },
    {
        "loss": 2.0141,
        "grad_norm": 1.3080697059631348,
        "learning_rate": 0.00016644352801663614,
        "epoch": 0.8477700438257283,
        "step": 6577
    },
    {
        "loss": 1.6642,
        "grad_norm": 2.1351912021636963,
        "learning_rate": 0.00016639804694482005,
        "epoch": 0.8478989430265532,
        "step": 6578
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.6753082275390625,
        "learning_rate": 0.00016635254129545113,
        "epoch": 0.8480278422273781,
        "step": 6579
    },
    {
        "loss": 1.9678,
        "grad_norm": 2.306973934173584,
        "learning_rate": 0.00016630701108537348,
        "epoch": 0.8481567414282032,
        "step": 6580
    },
    {
        "loss": 1.8336,
        "grad_norm": 2.9184820652008057,
        "learning_rate": 0.00016626145633144029,
        "epoch": 0.8482856406290281,
        "step": 6581
    },
    {
        "loss": 1.7716,
        "grad_norm": 2.3385493755340576,
        "learning_rate": 0.00016621587705051405,
        "epoch": 0.848414539829853,
        "step": 6582
    },
    {
        "loss": 1.7216,
        "grad_norm": 1.416779637336731,
        "learning_rate": 0.00016617027325946588,
        "epoch": 0.848543439030678,
        "step": 6583
    },
    {
        "loss": 1.6909,
        "grad_norm": 2.6837284564971924,
        "learning_rate": 0.00016612464497517637,
        "epoch": 0.848672338231503,
        "step": 6584
    },
    {
        "loss": 2.1611,
        "grad_norm": 1.9574542045593262,
        "learning_rate": 0.00016607899221453515,
        "epoch": 0.8488012374323279,
        "step": 6585
    },
    {
        "loss": 0.9849,
        "grad_norm": 2.454132556915283,
        "learning_rate": 0.00016603331499444063,
        "epoch": 0.8489301366331529,
        "step": 6586
    },
    {
        "loss": 1.4414,
        "grad_norm": 2.059727907180786,
        "learning_rate": 0.00016598761333180058,
        "epoch": 0.8490590358339778,
        "step": 6587
    },
    {
        "loss": 2.1001,
        "grad_norm": 1.8833832740783691,
        "learning_rate": 0.00016594188724353162,
        "epoch": 0.8491879350348028,
        "step": 6588
    },
    {
        "loss": 1.0821,
        "grad_norm": 2.148024082183838,
        "learning_rate": 0.0001658961367465595,
        "epoch": 0.8493168342356278,
        "step": 6589
    },
    {
        "loss": 2.4234,
        "grad_norm": 2.2529408931732178,
        "learning_rate": 0.00016585036185781893,
        "epoch": 0.8494457334364527,
        "step": 6590
    },
    {
        "loss": 2.246,
        "grad_norm": 2.1962459087371826,
        "learning_rate": 0.00016580456259425382,
        "epoch": 0.8495746326372776,
        "step": 6591
    },
    {
        "loss": 1.8292,
        "grad_norm": 2.573777914047241,
        "learning_rate": 0.0001657587389728169,
        "epoch": 0.8497035318381027,
        "step": 6592
    },
    {
        "loss": 2.5622,
        "grad_norm": 1.684670090675354,
        "learning_rate": 0.00016571289101046996,
        "epoch": 0.8498324310389276,
        "step": 6593
    },
    {
        "loss": 1.896,
        "grad_norm": 2.754626989364624,
        "learning_rate": 0.00016566701872418394,
        "epoch": 0.8499613302397525,
        "step": 6594
    },
    {
        "loss": 1.872,
        "grad_norm": 2.0208487510681152,
        "learning_rate": 0.00016562112213093868,
        "epoch": 0.8500902294405774,
        "step": 6595
    },
    {
        "loss": 2.2748,
        "grad_norm": 2.6531877517700195,
        "learning_rate": 0.00016557520124772297,
        "epoch": 0.8502191286414025,
        "step": 6596
    },
    {
        "loss": 1.4172,
        "grad_norm": 3.409259796142578,
        "learning_rate": 0.00016552925609153465,
        "epoch": 0.8503480278422274,
        "step": 6597
    },
    {
        "loss": 1.811,
        "grad_norm": 1.5359939336776733,
        "learning_rate": 0.0001654832866793805,
        "epoch": 0.8504769270430523,
        "step": 6598
    },
    {
        "loss": 1.8459,
        "grad_norm": 2.289804697036743,
        "learning_rate": 0.0001654372930282765,
        "epoch": 0.8506058262438773,
        "step": 6599
    },
    {
        "loss": 2.1282,
        "grad_norm": 1.6886069774627686,
        "learning_rate": 0.00016539127515524714,
        "epoch": 0.8507347254447022,
        "step": 6600
    },
    {
        "loss": 1.5042,
        "grad_norm": 2.4198293685913086,
        "learning_rate": 0.00016534523307732633,
        "epoch": 0.8508636246455272,
        "step": 6601
    },
    {
        "loss": 2.1946,
        "grad_norm": 2.0384457111358643,
        "learning_rate": 0.0001652991668115568,
        "epoch": 0.8509925238463522,
        "step": 6602
    },
    {
        "loss": 2.0774,
        "grad_norm": 1.6387338638305664,
        "learning_rate": 0.00016525307637499004,
        "epoch": 0.8511214230471771,
        "step": 6603
    },
    {
        "loss": 2.1031,
        "grad_norm": 2.4505410194396973,
        "learning_rate": 0.0001652069617846868,
        "epoch": 0.851250322248002,
        "step": 6604
    },
    {
        "loss": 2.1397,
        "grad_norm": 2.591932535171509,
        "learning_rate": 0.00016516082305771654,
        "epoch": 0.851379221448827,
        "step": 6605
    },
    {
        "loss": 2.121,
        "grad_norm": 1.5583449602127075,
        "learning_rate": 0.00016511466021115772,
        "epoch": 0.851508120649652,
        "step": 6606
    },
    {
        "loss": 1.8851,
        "grad_norm": 1.7012310028076172,
        "learning_rate": 0.00016506847326209774,
        "epoch": 0.8516370198504769,
        "step": 6607
    },
    {
        "loss": 1.7274,
        "grad_norm": 1.6991313695907593,
        "learning_rate": 0.00016502226222763294,
        "epoch": 0.8517659190513018,
        "step": 6608
    },
    {
        "loss": 2.0185,
        "grad_norm": 1.9591444730758667,
        "learning_rate": 0.0001649760271248686,
        "epoch": 0.8518948182521269,
        "step": 6609
    },
    {
        "loss": 2.0989,
        "grad_norm": 1.4165353775024414,
        "learning_rate": 0.00016492976797091873,
        "epoch": 0.8520237174529518,
        "step": 6610
    },
    {
        "loss": 1.7183,
        "grad_norm": 1.582120656967163,
        "learning_rate": 0.00016488348478290653,
        "epoch": 0.8521526166537767,
        "step": 6611
    },
    {
        "loss": 2.1978,
        "grad_norm": 2.1312713623046875,
        "learning_rate": 0.00016483717757796384,
        "epoch": 0.8522815158546017,
        "step": 6612
    },
    {
        "loss": 1.9163,
        "grad_norm": 1.6806354522705078,
        "learning_rate": 0.0001647908463732315,
        "epoch": 0.8524104150554267,
        "step": 6613
    },
    {
        "loss": 2.0422,
        "grad_norm": 1.7121084928512573,
        "learning_rate": 0.0001647444911858593,
        "epoch": 0.8525393142562516,
        "step": 6614
    },
    {
        "loss": 1.0891,
        "grad_norm": 2.901839017868042,
        "learning_rate": 0.00016469811203300572,
        "epoch": 0.8526682134570766,
        "step": 6615
    },
    {
        "loss": 1.7643,
        "grad_norm": 2.2538273334503174,
        "learning_rate": 0.00016465170893183832,
        "epoch": 0.8527971126579015,
        "step": 6616
    },
    {
        "loss": 1.63,
        "grad_norm": 2.6944596767425537,
        "learning_rate": 0.00016460528189953338,
        "epoch": 0.8529260118587265,
        "step": 6617
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.9407167434692383,
        "learning_rate": 0.00016455883095327606,
        "epoch": 0.8530549110595514,
        "step": 6618
    },
    {
        "loss": 1.9121,
        "grad_norm": 2.7629780769348145,
        "learning_rate": 0.00016451235611026057,
        "epoch": 0.8531838102603764,
        "step": 6619
    },
    {
        "loss": 1.819,
        "grad_norm": 1.6370078325271606,
        "learning_rate": 0.00016446585738768952,
        "epoch": 0.8533127094612013,
        "step": 6620
    },
    {
        "loss": 2.1249,
        "grad_norm": 1.5326025485992432,
        "learning_rate": 0.00016441933480277484,
        "epoch": 0.8534416086620263,
        "step": 6621
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.4574558734893799,
        "learning_rate": 0.00016437278837273702,
        "epoch": 0.8535705078628513,
        "step": 6622
    },
    {
        "loss": 1.5974,
        "grad_norm": 2.830514430999756,
        "learning_rate": 0.00016432621811480547,
        "epoch": 0.8536994070636762,
        "step": 6623
    },
    {
        "loss": 1.9115,
        "grad_norm": 2.1137077808380127,
        "learning_rate": 0.0001642796240462183,
        "epoch": 0.8538283062645011,
        "step": 6624
    },
    {
        "loss": 1.5284,
        "grad_norm": 2.175687551498413,
        "learning_rate": 0.0001642330061842226,
        "epoch": 0.8539572054653262,
        "step": 6625
    },
    {
        "loss": 1.8404,
        "grad_norm": 2.2060017585754395,
        "learning_rate": 0.00016418636454607427,
        "epoch": 0.8540861046661511,
        "step": 6626
    },
    {
        "loss": 2.2953,
        "grad_norm": 2.353151321411133,
        "learning_rate": 0.00016413969914903773,
        "epoch": 0.854215003866976,
        "step": 6627
    },
    {
        "loss": 0.8439,
        "grad_norm": 2.044492483139038,
        "learning_rate": 0.0001640930100103866,
        "epoch": 0.854343903067801,
        "step": 6628
    },
    {
        "loss": 2.4271,
        "grad_norm": 1.6087305545806885,
        "learning_rate": 0.000164046297147403,
        "epoch": 0.854472802268626,
        "step": 6629
    },
    {
        "loss": 1.7186,
        "grad_norm": 2.0901236534118652,
        "learning_rate": 0.0001639995605773779,
        "epoch": 0.8546017014694509,
        "step": 6630
    },
    {
        "loss": 1.4679,
        "grad_norm": 1.5240917205810547,
        "learning_rate": 0.00016395280031761112,
        "epoch": 0.8547306006702758,
        "step": 6631
    },
    {
        "loss": 1.8894,
        "grad_norm": 1.3740864992141724,
        "learning_rate": 0.0001639060163854111,
        "epoch": 0.8548594998711008,
        "step": 6632
    },
    {
        "loss": 1.3399,
        "grad_norm": 2.161787748336792,
        "learning_rate": 0.00016385920879809525,
        "epoch": 0.8549883990719258,
        "step": 6633
    },
    {
        "loss": 1.7368,
        "grad_norm": 2.210176467895508,
        "learning_rate": 0.00016381237757298958,
        "epoch": 0.8551172982727507,
        "step": 6634
    },
    {
        "loss": 2.4375,
        "grad_norm": 1.5940327644348145,
        "learning_rate": 0.0001637655227274288,
        "epoch": 0.8552461974735757,
        "step": 6635
    },
    {
        "loss": 0.8849,
        "grad_norm": 2.217949390411377,
        "learning_rate": 0.00016371864427875657,
        "epoch": 0.8553750966744006,
        "step": 6636
    },
    {
        "loss": 2.0931,
        "grad_norm": 1.56646728515625,
        "learning_rate": 0.00016367174224432516,
        "epoch": 0.8555039958752255,
        "step": 6637
    },
    {
        "loss": 0.9383,
        "grad_norm": 2.767096519470215,
        "learning_rate": 0.00016362481664149555,
        "epoch": 0.8556328950760506,
        "step": 6638
    },
    {
        "loss": 2.1502,
        "grad_norm": 1.58704674243927,
        "learning_rate": 0.00016357786748763742,
        "epoch": 0.8557617942768755,
        "step": 6639
    },
    {
        "loss": 1.8866,
        "grad_norm": 2.258650302886963,
        "learning_rate": 0.00016353089480012937,
        "epoch": 0.8558906934777004,
        "step": 6640
    },
    {
        "loss": 1.9551,
        "grad_norm": 1.7235349416732788,
        "learning_rate": 0.0001634838985963584,
        "epoch": 0.8560195926785253,
        "step": 6641
    },
    {
        "loss": 2.4053,
        "grad_norm": 1.3081105947494507,
        "learning_rate": 0.00016343687889372048,
        "epoch": 0.8561484918793504,
        "step": 6642
    },
    {
        "loss": 0.8774,
        "grad_norm": 2.6639535427093506,
        "learning_rate": 0.00016338983570962016,
        "epoch": 0.8562773910801753,
        "step": 6643
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.6702070236206055,
        "learning_rate": 0.00016334276906147064,
        "epoch": 0.8564062902810002,
        "step": 6644
    },
    {
        "loss": 1.5871,
        "grad_norm": 2.757249593734741,
        "learning_rate": 0.00016329567896669394,
        "epoch": 0.8565351894818252,
        "step": 6645
    },
    {
        "loss": 2.2271,
        "grad_norm": 1.1310516595840454,
        "learning_rate": 0.00016324856544272067,
        "epoch": 0.8566640886826502,
        "step": 6646
    },
    {
        "loss": 1.7262,
        "grad_norm": 2.9614553451538086,
        "learning_rate": 0.00016320142850699012,
        "epoch": 0.8567929878834751,
        "step": 6647
    },
    {
        "loss": 2.6574,
        "grad_norm": 1.9819767475128174,
        "learning_rate": 0.00016315426817695023,
        "epoch": 0.8569218870843001,
        "step": 6648
    },
    {
        "loss": 2.156,
        "grad_norm": 3.421156406402588,
        "learning_rate": 0.00016310708447005756,
        "epoch": 0.857050786285125,
        "step": 6649
    },
    {
        "loss": 1.4591,
        "grad_norm": 1.9618663787841797,
        "learning_rate": 0.00016305987740377752,
        "epoch": 0.85717968548595,
        "step": 6650
    },
    {
        "loss": 2.2967,
        "grad_norm": 1.4297661781311035,
        "learning_rate": 0.00016301264699558398,
        "epoch": 0.857308584686775,
        "step": 6651
    },
    {
        "loss": 1.8812,
        "grad_norm": 1.4822386503219604,
        "learning_rate": 0.00016296539326295938,
        "epoch": 0.8574374838875999,
        "step": 6652
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.4217689037323,
        "learning_rate": 0.00016291811622339513,
        "epoch": 0.8575663830884248,
        "step": 6653
    },
    {
        "loss": 2.4077,
        "grad_norm": 2.348052978515625,
        "learning_rate": 0.0001628708158943909,
        "epoch": 0.8576952822892498,
        "step": 6654
    },
    {
        "loss": 2.5248,
        "grad_norm": 2.313347339630127,
        "learning_rate": 0.0001628234922934552,
        "epoch": 0.8578241814900748,
        "step": 6655
    },
    {
        "loss": 2.0133,
        "grad_norm": 1.465254783630371,
        "learning_rate": 0.00016277614543810497,
        "epoch": 0.8579530806908997,
        "step": 6656
    },
    {
        "loss": 1.5013,
        "grad_norm": 1.9736371040344238,
        "learning_rate": 0.00016272877534586607,
        "epoch": 0.8580819798917246,
        "step": 6657
    },
    {
        "loss": 1.682,
        "grad_norm": 1.537615418434143,
        "learning_rate": 0.00016268138203427257,
        "epoch": 0.8582108790925497,
        "step": 6658
    },
    {
        "loss": 2.1525,
        "grad_norm": 1.6706727743148804,
        "learning_rate": 0.0001626339655208674,
        "epoch": 0.8583397782933746,
        "step": 6659
    },
    {
        "loss": 1.0469,
        "grad_norm": 3.6506989002227783,
        "learning_rate": 0.00016258652582320214,
        "epoch": 0.8584686774941995,
        "step": 6660
    },
    {
        "loss": 1.8124,
        "grad_norm": 2.1840927600860596,
        "learning_rate": 0.0001625390629588366,
        "epoch": 0.8585975766950245,
        "step": 6661
    },
    {
        "loss": 1.8474,
        "grad_norm": 1.4284183979034424,
        "learning_rate": 0.0001624915769453395,
        "epoch": 0.8587264758958495,
        "step": 6662
    },
    {
        "loss": 2.1019,
        "grad_norm": 1.8880878686904907,
        "learning_rate": 0.00016244406780028804,
        "epoch": 0.8588553750966744,
        "step": 6663
    },
    {
        "loss": 1.4003,
        "grad_norm": 1.8173158168792725,
        "learning_rate": 0.0001623965355412679,
        "epoch": 0.8589842742974994,
        "step": 6664
    },
    {
        "loss": 1.9378,
        "grad_norm": 2.8441522121429443,
        "learning_rate": 0.0001623489801858734,
        "epoch": 0.8591131734983243,
        "step": 6665
    },
    {
        "loss": 2.3574,
        "grad_norm": 1.5362131595611572,
        "learning_rate": 0.00016230140175170732,
        "epoch": 0.8592420726991493,
        "step": 6666
    },
    {
        "loss": 2.0661,
        "grad_norm": 1.7707215547561646,
        "learning_rate": 0.0001622538002563811,
        "epoch": 0.8593709718999742,
        "step": 6667
    },
    {
        "loss": 1.8981,
        "grad_norm": 2.7878258228302,
        "learning_rate": 0.0001622061757175147,
        "epoch": 0.8594998711007992,
        "step": 6668
    },
    {
        "loss": 1.2925,
        "grad_norm": 3.2584855556488037,
        "learning_rate": 0.00016215852815273647,
        "epoch": 0.8596287703016241,
        "step": 6669
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.3199546337127686,
        "learning_rate": 0.00016211085757968346,
        "epoch": 0.8597576695024491,
        "step": 6670
    },
    {
        "loss": 2.3965,
        "grad_norm": 1.6083855628967285,
        "learning_rate": 0.00016206316401600116,
        "epoch": 0.8598865687032741,
        "step": 6671
    },
    {
        "loss": 2.0708,
        "grad_norm": 1.4720494747161865,
        "learning_rate": 0.0001620154474793435,
        "epoch": 0.860015467904099,
        "step": 6672
    },
    {
        "loss": 1.8432,
        "grad_norm": 2.355947971343994,
        "learning_rate": 0.00016196770798737302,
        "epoch": 0.8601443671049239,
        "step": 6673
    },
    {
        "loss": 1.8791,
        "grad_norm": 2.1092417240142822,
        "learning_rate": 0.0001619199455577608,
        "epoch": 0.8602732663057489,
        "step": 6674
    },
    {
        "loss": 1.9352,
        "grad_norm": 2.1982202529907227,
        "learning_rate": 0.0001618721602081862,
        "epoch": 0.8604021655065739,
        "step": 6675
    },
    {
        "loss": 1.6115,
        "grad_norm": 2.1715328693389893,
        "learning_rate": 0.00016182435195633723,
        "epoch": 0.8605310647073988,
        "step": 6676
    },
    {
        "loss": 1.9837,
        "grad_norm": 2.4371860027313232,
        "learning_rate": 0.00016177652081991045,
        "epoch": 0.8606599639082237,
        "step": 6677
    },
    {
        "loss": 2.1965,
        "grad_norm": 1.505894422531128,
        "learning_rate": 0.00016172866681661062,
        "epoch": 0.8607888631090487,
        "step": 6678
    },
    {
        "loss": 1.8733,
        "grad_norm": 1.5138224363327026,
        "learning_rate": 0.00016168078996415123,
        "epoch": 0.8609177623098737,
        "step": 6679
    },
    {
        "loss": 1.8545,
        "grad_norm": 1.595238208770752,
        "learning_rate": 0.0001616328902802541,
        "epoch": 0.8610466615106986,
        "step": 6680
    },
    {
        "loss": 1.6117,
        "grad_norm": 2.203798532485962,
        "learning_rate": 0.00016158496778264953,
        "epoch": 0.8611755607115236,
        "step": 6681
    },
    {
        "loss": 2.0196,
        "grad_norm": 1.787076473236084,
        "learning_rate": 0.00016153702248907626,
        "epoch": 0.8613044599123485,
        "step": 6682
    },
    {
        "loss": 1.7383,
        "grad_norm": 2.368018865585327,
        "learning_rate": 0.0001614890544172814,
        "epoch": 0.8614333591131735,
        "step": 6683
    },
    {
        "loss": 1.8203,
        "grad_norm": 1.6955989599227905,
        "learning_rate": 0.00016144106358502067,
        "epoch": 0.8615622583139985,
        "step": 6684
    },
    {
        "loss": 0.7927,
        "grad_norm": 2.6032774448394775,
        "learning_rate": 0.0001613930500100581,
        "epoch": 0.8616911575148234,
        "step": 6685
    },
    {
        "loss": 2.515,
        "grad_norm": 1.8536595106124878,
        "learning_rate": 0.00016134501371016597,
        "epoch": 0.8618200567156483,
        "step": 6686
    },
    {
        "loss": 1.5022,
        "grad_norm": 2.2347970008850098,
        "learning_rate": 0.00016129695470312537,
        "epoch": 0.8619489559164734,
        "step": 6687
    },
    {
        "loss": 1.4989,
        "grad_norm": 1.8067185878753662,
        "learning_rate": 0.0001612488730067255,
        "epoch": 0.8620778551172983,
        "step": 6688
    },
    {
        "loss": 2.3419,
        "grad_norm": 1.483502745628357,
        "learning_rate": 0.00016120076863876395,
        "epoch": 0.8622067543181232,
        "step": 6689
    },
    {
        "loss": 1.7414,
        "grad_norm": 2.649282217025757,
        "learning_rate": 0.0001611526416170468,
        "epoch": 0.8623356535189481,
        "step": 6690
    },
    {
        "loss": 2.4147,
        "grad_norm": 2.1653635501861572,
        "learning_rate": 0.00016110449195938867,
        "epoch": 0.8624645527197732,
        "step": 6691
    },
    {
        "loss": 1.8374,
        "grad_norm": 1.2367169857025146,
        "learning_rate": 0.00016105631968361213,
        "epoch": 0.8625934519205981,
        "step": 6692
    },
    {
        "loss": 1.9414,
        "grad_norm": 2.3788177967071533,
        "learning_rate": 0.00016100812480754847,
        "epoch": 0.862722351121423,
        "step": 6693
    },
    {
        "loss": 1.659,
        "grad_norm": 2.822157382965088,
        "learning_rate": 0.00016095990734903736,
        "epoch": 0.862851250322248,
        "step": 6694
    },
    {
        "loss": 1.2148,
        "grad_norm": 3.6266825199127197,
        "learning_rate": 0.00016091166732592658,
        "epoch": 0.862980149523073,
        "step": 6695
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.9480432271957397,
        "learning_rate": 0.00016086340475607246,
        "epoch": 0.8631090487238979,
        "step": 6696
    },
    {
        "loss": 2.0368,
        "grad_norm": 2.5838561058044434,
        "learning_rate": 0.00016081511965733967,
        "epoch": 0.8632379479247229,
        "step": 6697
    },
    {
        "loss": 1.3453,
        "grad_norm": 2.4062161445617676,
        "learning_rate": 0.00016076681204760108,
        "epoch": 0.8633668471255478,
        "step": 6698
    },
    {
        "loss": 2.3877,
        "grad_norm": 1.4064316749572754,
        "learning_rate": 0.00016071848194473799,
        "epoch": 0.8634957463263728,
        "step": 6699
    },
    {
        "loss": 1.7902,
        "grad_norm": 2.932405471801758,
        "learning_rate": 0.00016067012936664009,
        "epoch": 0.8636246455271978,
        "step": 6700
    },
    {
        "loss": 2.0276,
        "grad_norm": 1.7108665704727173,
        "learning_rate": 0.00016062175433120528,
        "epoch": 0.8637535447280227,
        "step": 6701
    },
    {
        "loss": 0.7423,
        "grad_norm": 2.522878408432007,
        "learning_rate": 0.00016057335685633973,
        "epoch": 0.8638824439288476,
        "step": 6702
    },
    {
        "loss": 2.2001,
        "grad_norm": 2.4079296588897705,
        "learning_rate": 0.00016052493695995816,
        "epoch": 0.8640113431296726,
        "step": 6703
    },
    {
        "loss": 1.7054,
        "grad_norm": 1.1891509294509888,
        "learning_rate": 0.0001604764946599833,
        "epoch": 0.8641402423304976,
        "step": 6704
    },
    {
        "loss": 1.663,
        "grad_norm": 2.6170129776000977,
        "learning_rate": 0.00016042802997434636,
        "epoch": 0.8642691415313225,
        "step": 6705
    },
    {
        "loss": 2.4885,
        "grad_norm": 1.1949450969696045,
        "learning_rate": 0.00016037954292098672,
        "epoch": 0.8643980407321474,
        "step": 6706
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.6274460554122925,
        "learning_rate": 0.00016033103351785212,
        "epoch": 0.8645269399329724,
        "step": 6707
    },
    {
        "loss": 1.1994,
        "grad_norm": 3.861100673675537,
        "learning_rate": 0.00016028250178289855,
        "epoch": 0.8646558391337974,
        "step": 6708
    },
    {
        "loss": 1.3247,
        "grad_norm": 1.6680774688720703,
        "learning_rate": 0.00016023394773409023,
        "epoch": 0.8647847383346223,
        "step": 6709
    },
    {
        "loss": 2.2528,
        "grad_norm": 1.6414153575897217,
        "learning_rate": 0.0001601853713893997,
        "epoch": 0.8649136375354473,
        "step": 6710
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.103912115097046,
        "learning_rate": 0.0001601367727668078,
        "epoch": 0.8650425367362722,
        "step": 6711
    },
    {
        "loss": 1.8802,
        "grad_norm": 1.8313593864440918,
        "learning_rate": 0.00016008815188430338,
        "epoch": 0.8651714359370972,
        "step": 6712
    },
    {
        "loss": 1.7556,
        "grad_norm": 2.1310434341430664,
        "learning_rate": 0.00016003950875988383,
        "epoch": 0.8653003351379221,
        "step": 6713
    },
    {
        "loss": 1.8147,
        "grad_norm": 3.0460081100463867,
        "learning_rate": 0.00015999084341155454,
        "epoch": 0.8654292343387471,
        "step": 6714
    },
    {
        "loss": 1.9722,
        "grad_norm": 1.751857876777649,
        "learning_rate": 0.0001599421558573293,
        "epoch": 0.865558133539572,
        "step": 6715
    },
    {
        "loss": 2.0665,
        "grad_norm": 1.709204912185669,
        "learning_rate": 0.00015989344611522994,
        "epoch": 0.865687032740397,
        "step": 6716
    },
    {
        "loss": 2.3582,
        "grad_norm": 1.761049509048462,
        "learning_rate": 0.00015984471420328673,
        "epoch": 0.865815931941222,
        "step": 6717
    },
    {
        "loss": 1.4801,
        "grad_norm": 2.2342021465301514,
        "learning_rate": 0.00015979596013953793,
        "epoch": 0.8659448311420469,
        "step": 6718
    },
    {
        "loss": 2.097,
        "grad_norm": 2.1525304317474365,
        "learning_rate": 0.00015974718394203008,
        "epoch": 0.8660737303428718,
        "step": 6719
    },
    {
        "loss": 2.0924,
        "grad_norm": 1.8410309553146362,
        "learning_rate": 0.00015969838562881797,
        "epoch": 0.8662026295436969,
        "step": 6720
    },
    {
        "loss": 2.1962,
        "grad_norm": 1.2923215627670288,
        "learning_rate": 0.00015964956521796454,
        "epoch": 0.8663315287445218,
        "step": 6721
    },
    {
        "loss": 2.1514,
        "grad_norm": 2.291935920715332,
        "learning_rate": 0.00015960072272754088,
        "epoch": 0.8664604279453467,
        "step": 6722
    },
    {
        "loss": 1.975,
        "grad_norm": 1.7578063011169434,
        "learning_rate": 0.0001595518581756262,
        "epoch": 0.8665893271461717,
        "step": 6723
    },
    {
        "loss": 2.135,
        "grad_norm": 1.2331514358520508,
        "learning_rate": 0.00015950297158030802,
        "epoch": 0.8667182263469967,
        "step": 6724
    },
    {
        "loss": 1.2142,
        "grad_norm": 1.442784070968628,
        "learning_rate": 0.000159454062959682,
        "epoch": 0.8668471255478216,
        "step": 6725
    },
    {
        "loss": 1.4895,
        "grad_norm": 1.9216275215148926,
        "learning_rate": 0.00015940513233185172,
        "epoch": 0.8669760247486465,
        "step": 6726
    },
    {
        "loss": 1.9534,
        "grad_norm": 2.5329508781433105,
        "learning_rate": 0.0001593561797149292,
        "epoch": 0.8671049239494715,
        "step": 6727
    },
    {
        "loss": 1.7808,
        "grad_norm": 1.8465192317962646,
        "learning_rate": 0.00015930720512703458,
        "epoch": 0.8672338231502965,
        "step": 6728
    },
    {
        "loss": 2.1079,
        "grad_norm": 1.876288652420044,
        "learning_rate": 0.00015925820858629584,
        "epoch": 0.8673627223511214,
        "step": 6729
    },
    {
        "loss": 2.2633,
        "grad_norm": 2.347815990447998,
        "learning_rate": 0.0001592091901108494,
        "epoch": 0.8674916215519464,
        "step": 6730
    },
    {
        "loss": 1.8766,
        "grad_norm": 1.663097620010376,
        "learning_rate": 0.0001591601497188397,
        "epoch": 0.8676205207527713,
        "step": 6731
    },
    {
        "loss": 1.8576,
        "grad_norm": 2.20322322845459,
        "learning_rate": 0.0001591110874284192,
        "epoch": 0.8677494199535963,
        "step": 6732
    },
    {
        "loss": 2.4932,
        "grad_norm": 1.6069471836090088,
        "learning_rate": 0.00015906200325774853,
        "epoch": 0.8678783191544213,
        "step": 6733
    },
    {
        "loss": 1.6368,
        "grad_norm": 2.066715955734253,
        "learning_rate": 0.00015901289722499652,
        "epoch": 0.8680072183552462,
        "step": 6734
    },
    {
        "loss": 2.3296,
        "grad_norm": 1.206684947013855,
        "learning_rate": 0.00015896376934833995,
        "epoch": 0.8681361175560711,
        "step": 6735
    },
    {
        "loss": 2.3469,
        "grad_norm": 1.1248644590377808,
        "learning_rate": 0.00015891461964596368,
        "epoch": 0.8682650167568962,
        "step": 6736
    },
    {
        "loss": 1.7626,
        "grad_norm": 1.3592677116394043,
        "learning_rate": 0.00015886544813606084,
        "epoch": 0.8683939159577211,
        "step": 6737
    },
    {
        "loss": 1.9746,
        "grad_norm": 2.6283881664276123,
        "learning_rate": 0.0001588162548368324,
        "epoch": 0.868522815158546,
        "step": 6738
    },
    {
        "loss": 1.6768,
        "grad_norm": 2.226851224899292,
        "learning_rate": 0.00015876703976648755,
        "epoch": 0.8686517143593709,
        "step": 6739
    },
    {
        "loss": 1.411,
        "grad_norm": 1.8415178060531616,
        "learning_rate": 0.00015871780294324342,
        "epoch": 0.868780613560196,
        "step": 6740
    },
    {
        "loss": 1.4555,
        "grad_norm": 2.136949300765991,
        "learning_rate": 0.00015866854438532522,
        "epoch": 0.8689095127610209,
        "step": 6741
    },
    {
        "loss": 1.8694,
        "grad_norm": 2.5135202407836914,
        "learning_rate": 0.0001586192641109664,
        "epoch": 0.8690384119618458,
        "step": 6742
    },
    {
        "loss": 0.9608,
        "grad_norm": 2.0695574283599854,
        "learning_rate": 0.00015856996213840815,
        "epoch": 0.8691673111626708,
        "step": 6743
    },
    {
        "loss": 2.4349,
        "grad_norm": 1.4741381406784058,
        "learning_rate": 0.00015852063848589983,
        "epoch": 0.8692962103634957,
        "step": 6744
    },
    {
        "loss": 2.4003,
        "grad_norm": 1.6769710779190063,
        "learning_rate": 0.00015847129317169903,
        "epoch": 0.8694251095643207,
        "step": 6745
    },
    {
        "loss": 1.5202,
        "grad_norm": 2.413407325744629,
        "learning_rate": 0.0001584219262140708,
        "epoch": 0.8695540087651457,
        "step": 6746
    },
    {
        "loss": 1.741,
        "grad_norm": Infinity,
        "learning_rate": 0.0001584219262140708,
        "epoch": 0.8696829079659706,
        "step": 6747
    },
    {
        "loss": 1.9021,
        "grad_norm": 2.1693291664123535,
        "learning_rate": 0.00015837253763128882,
        "epoch": 0.8698118071667955,
        "step": 6748
    },
    {
        "loss": 1.5558,
        "grad_norm": 2.475979804992676,
        "learning_rate": 0.00015832312744163446,
        "epoch": 0.8699407063676206,
        "step": 6749
    },
    {
        "loss": 1.5914,
        "grad_norm": 2.285703659057617,
        "learning_rate": 0.0001582736956633971,
        "epoch": 0.8700696055684455,
        "step": 6750
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.6161468029022217,
        "learning_rate": 0.0001582242423148741,
        "epoch": 0.8701985047692704,
        "step": 6751
    },
    {
        "loss": 1.3821,
        "grad_norm": 2.595951795578003,
        "learning_rate": 0.00015817476741437095,
        "epoch": 0.8703274039700953,
        "step": 6752
    },
    {
        "loss": 1.6892,
        "grad_norm": 3.488093614578247,
        "learning_rate": 0.000158125270980201,
        "epoch": 0.8704563031709204,
        "step": 6753
    },
    {
        "loss": 1.4379,
        "grad_norm": 1.6497459411621094,
        "learning_rate": 0.00015807575303068548,
        "epoch": 0.8705852023717453,
        "step": 6754
    },
    {
        "loss": 1.9573,
        "grad_norm": 2.281900405883789,
        "learning_rate": 0.00015802621358415386,
        "epoch": 0.8707141015725702,
        "step": 6755
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.333894729614258,
        "learning_rate": 0.00015797665265894332,
        "epoch": 0.8708430007733952,
        "step": 6756
    },
    {
        "loss": 1.9701,
        "grad_norm": 2.065795421600342,
        "learning_rate": 0.0001579270702733991,
        "epoch": 0.8709718999742202,
        "step": 6757
    },
    {
        "loss": 1.3305,
        "grad_norm": 2.5612521171569824,
        "learning_rate": 0.00015787746644587428,
        "epoch": 0.8711007991750451,
        "step": 6758
    },
    {
        "loss": 2.0373,
        "grad_norm": 1.9567855596542358,
        "learning_rate": 0.00015782784119473,
        "epoch": 0.87122969837587,
        "step": 6759
    },
    {
        "loss": 1.9337,
        "grad_norm": 2.0497918128967285,
        "learning_rate": 0.00015777819453833528,
        "epoch": 0.871358597576695,
        "step": 6760
    },
    {
        "loss": 2.2132,
        "grad_norm": 2.270145893096924,
        "learning_rate": 0.00015772852649506715,
        "epoch": 0.87148749677752,
        "step": 6761
    },
    {
        "loss": 1.8142,
        "grad_norm": 2.6131441593170166,
        "learning_rate": 0.0001576788370833103,
        "epoch": 0.871616395978345,
        "step": 6762
    },
    {
        "loss": 2.3142,
        "grad_norm": 1.6481759548187256,
        "learning_rate": 0.00015762912632145767,
        "epoch": 0.8717452951791699,
        "step": 6763
    },
    {
        "loss": 1.8536,
        "grad_norm": 2.2598369121551514,
        "learning_rate": 0.00015757939422790984,
        "epoch": 0.8718741943799948,
        "step": 6764
    },
    {
        "loss": 1.6791,
        "grad_norm": 2.371873617172241,
        "learning_rate": 0.00015752964082107543,
        "epoch": 0.8720030935808198,
        "step": 6765
    },
    {
        "loss": 2.0785,
        "grad_norm": 2.642340898513794,
        "learning_rate": 0.00015747986611937083,
        "epoch": 0.8721319927816448,
        "step": 6766
    },
    {
        "loss": 2.1866,
        "grad_norm": 2.0017521381378174,
        "learning_rate": 0.00015743007014122048,
        "epoch": 0.8722608919824697,
        "step": 6767
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.1006810665130615,
        "learning_rate": 0.0001573802529050565,
        "epoch": 0.8723897911832946,
        "step": 6768
    },
    {
        "loss": 2.4441,
        "grad_norm": 1.7811180353164673,
        "learning_rate": 0.00015733041442931905,
        "epoch": 0.8725186903841197,
        "step": 6769
    },
    {
        "loss": 2.3345,
        "grad_norm": 1.9023258686065674,
        "learning_rate": 0.00015728055473245605,
        "epoch": 0.8726475895849446,
        "step": 6770
    },
    {
        "loss": 1.0664,
        "grad_norm": 3.1714999675750732,
        "learning_rate": 0.00015723067383292325,
        "epoch": 0.8727764887857695,
        "step": 6771
    },
    {
        "loss": 2.0942,
        "grad_norm": 1.889968752861023,
        "learning_rate": 0.0001571807717491844,
        "epoch": 0.8729053879865944,
        "step": 6772
    },
    {
        "loss": 2.478,
        "grad_norm": 1.4813281297683716,
        "learning_rate": 0.00015713084849971098,
        "epoch": 0.8730342871874195,
        "step": 6773
    },
    {
        "loss": 2.1656,
        "grad_norm": 1.7515265941619873,
        "learning_rate": 0.0001570809041029823,
        "epoch": 0.8731631863882444,
        "step": 6774
    },
    {
        "loss": 1.3866,
        "grad_norm": 3.1360926628112793,
        "learning_rate": 0.00015703093857748543,
        "epoch": 0.8732920855890693,
        "step": 6775
    },
    {
        "loss": 2.1359,
        "grad_norm": 1.6035231351852417,
        "learning_rate": 0.00015698095194171543,
        "epoch": 0.8734209847898943,
        "step": 6776
    },
    {
        "loss": 1.8846,
        "grad_norm": 1.7677814960479736,
        "learning_rate": 0.00015693094421417514,
        "epoch": 0.8735498839907193,
        "step": 6777
    },
    {
        "loss": 1.2834,
        "grad_norm": 1.4869399070739746,
        "learning_rate": 0.00015688091541337507,
        "epoch": 0.8736787831915442,
        "step": 6778
    },
    {
        "loss": 2.1985,
        "grad_norm": 2.290869951248169,
        "learning_rate": 0.0001568308655578336,
        "epoch": 0.8738076823923692,
        "step": 6779
    },
    {
        "loss": 2.0366,
        "grad_norm": 2.102703094482422,
        "learning_rate": 0.00015678079466607706,
        "epoch": 0.8739365815931941,
        "step": 6780
    },
    {
        "loss": 2.2671,
        "grad_norm": 1.6894034147262573,
        "learning_rate": 0.00015673070275663928,
        "epoch": 0.874065480794019,
        "step": 6781
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.7136727571487427,
        "learning_rate": 0.00015668058984806213,
        "epoch": 0.8741943799948441,
        "step": 6782
    },
    {
        "loss": 1.5991,
        "grad_norm": 2.179818630218506,
        "learning_rate": 0.000156630455958895,
        "epoch": 0.874323279195669,
        "step": 6783
    },
    {
        "loss": 1.6625,
        "grad_norm": 2.018002510070801,
        "learning_rate": 0.00015658030110769544,
        "epoch": 0.8744521783964939,
        "step": 6784
    },
    {
        "loss": 1.2899,
        "grad_norm": 1.6958237886428833,
        "learning_rate": 0.0001565301253130282,
        "epoch": 0.8745810775973188,
        "step": 6785
    },
    {
        "loss": 1.2632,
        "grad_norm": 1.9581490755081177,
        "learning_rate": 0.0001564799285934662,
        "epoch": 0.8747099767981439,
        "step": 6786
    },
    {
        "loss": 2.476,
        "grad_norm": 1.4484196901321411,
        "learning_rate": 0.0001564297109675902,
        "epoch": 0.8748388759989688,
        "step": 6787
    },
    {
        "loss": 1.4528,
        "grad_norm": 2.839931011199951,
        "learning_rate": 0.00015637947245398815,
        "epoch": 0.8749677751997937,
        "step": 6788
    },
    {
        "loss": 2.0555,
        "grad_norm": 1.6852065324783325,
        "learning_rate": 0.00015632921307125636,
        "epoch": 0.8750966744006187,
        "step": 6789
    },
    {
        "loss": 2.0239,
        "grad_norm": 1.5136321783065796,
        "learning_rate": 0.00015627893283799843,
        "epoch": 0.8752255736014437,
        "step": 6790
    },
    {
        "loss": 2.6072,
        "grad_norm": 1.9204235076904297,
        "learning_rate": 0.00015622863177282588,
        "epoch": 0.8753544728022686,
        "step": 6791
    },
    {
        "loss": 1.6659,
        "grad_norm": 2.332843065261841,
        "learning_rate": 0.00015617830989435785,
        "epoch": 0.8754833720030936,
        "step": 6792
    },
    {
        "loss": 0.8823,
        "grad_norm": 3.551316022872925,
        "learning_rate": 0.0001561279672212212,
        "epoch": 0.8756122712039185,
        "step": 6793
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.4182546138763428,
        "learning_rate": 0.0001560776037720506,
        "epoch": 0.8757411704047435,
        "step": 6794
    },
    {
        "loss": 1.9767,
        "grad_norm": 2.018104314804077,
        "learning_rate": 0.00015602721956548828,
        "epoch": 0.8758700696055685,
        "step": 6795
    },
    {
        "loss": 1.723,
        "grad_norm": 1.6370834112167358,
        "learning_rate": 0.00015597681462018413,
        "epoch": 0.8759989688063934,
        "step": 6796
    },
    {
        "loss": 2.375,
        "grad_norm": 1.2532949447631836,
        "learning_rate": 0.0001559263889547959,
        "epoch": 0.8761278680072183,
        "step": 6797
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.4160548448562622,
        "learning_rate": 0.00015587594258798884,
        "epoch": 0.8762567672080434,
        "step": 6798
    },
    {
        "loss": 2.2525,
        "grad_norm": 2.083439588546753,
        "learning_rate": 0.0001558254755384359,
        "epoch": 0.8763856664088683,
        "step": 6799
    },
    {
        "loss": 2.2591,
        "grad_norm": 1.6158736944198608,
        "learning_rate": 0.00015577498782481762,
        "epoch": 0.8765145656096932,
        "step": 6800
    },
    {
        "loss": 2.0856,
        "grad_norm": 2.795212745666504,
        "learning_rate": 0.0001557244794658225,
        "epoch": 0.8766434648105181,
        "step": 6801
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.5676629543304443,
        "learning_rate": 0.0001556739504801461,
        "epoch": 0.8767723640113432,
        "step": 6802
    },
    {
        "loss": 0.7949,
        "grad_norm": 2.1926987171173096,
        "learning_rate": 0.00015562340088649224,
        "epoch": 0.8769012632121681,
        "step": 6803
    },
    {
        "loss": 0.9165,
        "grad_norm": 2.1672096252441406,
        "learning_rate": 0.00015557283070357207,
        "epoch": 0.877030162412993,
        "step": 6804
    },
    {
        "loss": 1.8794,
        "grad_norm": 1.7270771265029907,
        "learning_rate": 0.00015552223995010425,
        "epoch": 0.877159061613818,
        "step": 6805
    },
    {
        "loss": 2.3089,
        "grad_norm": 2.3567583560943604,
        "learning_rate": 0.00015547162864481534,
        "epoch": 0.877287960814643,
        "step": 6806
    },
    {
        "loss": 1.144,
        "grad_norm": 2.42396879196167,
        "learning_rate": 0.0001554209968064392,
        "epoch": 0.8774168600154679,
        "step": 6807
    },
    {
        "loss": 1.3001,
        "grad_norm": 2.3966264724731445,
        "learning_rate": 0.00015537034445371758,
        "epoch": 0.8775457592162929,
        "step": 6808
    },
    {
        "loss": 2.365,
        "grad_norm": 1.8097511529922485,
        "learning_rate": 0.00015531967160539964,
        "epoch": 0.8776746584171178,
        "step": 6809
    },
    {
        "loss": 2.0232,
        "grad_norm": 1.703622817993164,
        "learning_rate": 0.0001552689782802421,
        "epoch": 0.8778035576179428,
        "step": 6810
    },
    {
        "loss": 1.3466,
        "grad_norm": 2.4771506786346436,
        "learning_rate": 0.00015521826449700944,
        "epoch": 0.8779324568187677,
        "step": 6811
    },
    {
        "loss": 0.6718,
        "grad_norm": 1.6572790145874023,
        "learning_rate": 0.00015516753027447358,
        "epoch": 0.8780613560195927,
        "step": 6812
    },
    {
        "loss": 2.2115,
        "grad_norm": 2.1265015602111816,
        "learning_rate": 0.00015511677563141403,
        "epoch": 0.8781902552204176,
        "step": 6813
    },
    {
        "loss": 2.2615,
        "grad_norm": 1.5347877740859985,
        "learning_rate": 0.00015506600058661784,
        "epoch": 0.8783191544212426,
        "step": 6814
    },
    {
        "loss": 1.897,
        "grad_norm": 2.0531809329986572,
        "learning_rate": 0.00015501520515887968,
        "epoch": 0.8784480536220676,
        "step": 6815
    },
    {
        "loss": 1.5618,
        "grad_norm": 3.205047369003296,
        "learning_rate": 0.0001549643893670017,
        "epoch": 0.8785769528228925,
        "step": 6816
    },
    {
        "loss": 2.2234,
        "grad_norm": 1.840258002281189,
        "learning_rate": 0.00015491355322979356,
        "epoch": 0.8787058520237174,
        "step": 6817
    },
    {
        "loss": 1.5506,
        "grad_norm": 2.1266286373138428,
        "learning_rate": 0.00015486269676607262,
        "epoch": 0.8788347512245424,
        "step": 6818
    },
    {
        "loss": 1.782,
        "grad_norm": 2.5748937129974365,
        "learning_rate": 0.00015481181999466346,
        "epoch": 0.8789636504253674,
        "step": 6819
    },
    {
        "loss": 2.1035,
        "grad_norm": 1.4070030450820923,
        "learning_rate": 0.00015476092293439844,
        "epoch": 0.8790925496261923,
        "step": 6820
    },
    {
        "loss": 0.7732,
        "grad_norm": 3.198906898498535,
        "learning_rate": 0.00015471000560411753,
        "epoch": 0.8792214488270172,
        "step": 6821
    },
    {
        "loss": 1.09,
        "grad_norm": 2.552530527114868,
        "learning_rate": 0.0001546590680226677,
        "epoch": 0.8793503480278422,
        "step": 6822
    },
    {
        "loss": 1.6116,
        "grad_norm": 1.6874918937683105,
        "learning_rate": 0.00015460811020890397,
        "epoch": 0.8794792472286672,
        "step": 6823
    },
    {
        "loss": 0.8155,
        "grad_norm": 2.114781618118286,
        "learning_rate": 0.00015455713218168855,
        "epoch": 0.8796081464294921,
        "step": 6824
    },
    {
        "loss": 2.6966,
        "grad_norm": 2.600400447845459,
        "learning_rate": 0.00015450613395989117,
        "epoch": 0.8797370456303171,
        "step": 6825
    },
    {
        "loss": 0.9992,
        "grad_norm": 2.307119607925415,
        "learning_rate": 0.00015445511556238904,
        "epoch": 0.879865944831142,
        "step": 6826
    },
    {
        "loss": 1.5803,
        "grad_norm": 2.626375198364258,
        "learning_rate": 0.000154404077008067,
        "epoch": 0.879994844031967,
        "step": 6827
    },
    {
        "loss": 2.1959,
        "grad_norm": 1.1016225814819336,
        "learning_rate": 0.0001543530183158171,
        "epoch": 0.880123743232792,
        "step": 6828
    },
    {
        "loss": 2.2211,
        "grad_norm": 2.0286896228790283,
        "learning_rate": 0.00015430193950453892,
        "epoch": 0.8802526424336169,
        "step": 6829
    },
    {
        "loss": 1.9199,
        "grad_norm": 2.220414876937866,
        "learning_rate": 0.00015425084059313965,
        "epoch": 0.8803815416344418,
        "step": 6830
    },
    {
        "loss": 2.1491,
        "grad_norm": 2.2135517597198486,
        "learning_rate": 0.0001541997216005337,
        "epoch": 0.8805104408352669,
        "step": 6831
    },
    {
        "loss": 1.9969,
        "grad_norm": 1.818323016166687,
        "learning_rate": 0.0001541485825456431,
        "epoch": 0.8806393400360918,
        "step": 6832
    },
    {
        "loss": 1.6134,
        "grad_norm": 1.5360735654830933,
        "learning_rate": 0.00015409742344739714,
        "epoch": 0.8807682392369167,
        "step": 6833
    },
    {
        "loss": 1.5358,
        "grad_norm": 1.400961995124817,
        "learning_rate": 0.00015404624432473253,
        "epoch": 0.8808971384377416,
        "step": 6834
    },
    {
        "loss": 1.2822,
        "grad_norm": 2.567573308944702,
        "learning_rate": 0.00015399504519659368,
        "epoch": 0.8810260376385667,
        "step": 6835
    },
    {
        "loss": 2.2737,
        "grad_norm": 1.4338226318359375,
        "learning_rate": 0.00015394382608193197,
        "epoch": 0.8811549368393916,
        "step": 6836
    },
    {
        "loss": 2.2988,
        "grad_norm": 2.665513515472412,
        "learning_rate": 0.00015389258699970647,
        "epoch": 0.8812838360402165,
        "step": 6837
    },
    {
        "loss": 1.8751,
        "grad_norm": 2.209425210952759,
        "learning_rate": 0.0001538413279688837,
        "epoch": 0.8814127352410415,
        "step": 6838
    },
    {
        "loss": 2.0793,
        "grad_norm": 1.8922284841537476,
        "learning_rate": 0.00015379004900843723,
        "epoch": 0.8815416344418665,
        "step": 6839
    },
    {
        "loss": 2.1802,
        "grad_norm": 2.284821033477783,
        "learning_rate": 0.0001537387501373484,
        "epoch": 0.8816705336426914,
        "step": 6840
    },
    {
        "loss": 1.8949,
        "grad_norm": 2.205904006958008,
        "learning_rate": 0.00015368743137460559,
        "epoch": 0.8817994328435164,
        "step": 6841
    },
    {
        "loss": 2.0404,
        "grad_norm": 1.1444422006607056,
        "learning_rate": 0.00015363609273920473,
        "epoch": 0.8819283320443413,
        "step": 6842
    },
    {
        "loss": 1.554,
        "grad_norm": 2.260557174682617,
        "learning_rate": 0.00015358473425014903,
        "epoch": 0.8820572312451663,
        "step": 6843
    },
    {
        "loss": 1.0675,
        "grad_norm": 2.2591888904571533,
        "learning_rate": 0.00015353335592644917,
        "epoch": 0.8821861304459913,
        "step": 6844
    },
    {
        "loss": 1.405,
        "grad_norm": 2.713513135910034,
        "learning_rate": 0.000153481957787123,
        "epoch": 0.8823150296468162,
        "step": 6845
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.5498292446136475,
        "learning_rate": 0.0001534305398511958,
        "epoch": 0.8824439288476411,
        "step": 6846
    },
    {
        "loss": 2.0033,
        "grad_norm": 1.605804443359375,
        "learning_rate": 0.0001533791021377002,
        "epoch": 0.8825728280484662,
        "step": 6847
    },
    {
        "loss": 1.8546,
        "grad_norm": 1.6025710105895996,
        "learning_rate": 0.0001533276446656761,
        "epoch": 0.8827017272492911,
        "step": 6848
    },
    {
        "loss": 2.1027,
        "grad_norm": 2.101823568344116,
        "learning_rate": 0.0001532761674541707,
        "epoch": 0.882830626450116,
        "step": 6849
    },
    {
        "loss": 2.1902,
        "grad_norm": 1.2321702241897583,
        "learning_rate": 0.00015322467052223854,
        "epoch": 0.8829595256509409,
        "step": 6850
    },
    {
        "loss": 2.2732,
        "grad_norm": 2.555436134338379,
        "learning_rate": 0.00015317315388894143,
        "epoch": 0.883088424851766,
        "step": 6851
    },
    {
        "loss": 2.0097,
        "grad_norm": 1.5463768243789673,
        "learning_rate": 0.00015312161757334862,
        "epoch": 0.8832173240525909,
        "step": 6852
    },
    {
        "loss": 1.9592,
        "grad_norm": 1.66218101978302,
        "learning_rate": 0.00015307006159453635,
        "epoch": 0.8833462232534158,
        "step": 6853
    },
    {
        "loss": 1.1189,
        "grad_norm": 3.1034505367279053,
        "learning_rate": 0.00015301848597158837,
        "epoch": 0.8834751224542408,
        "step": 6854
    },
    {
        "loss": 1.7788,
        "grad_norm": 2.408499240875244,
        "learning_rate": 0.0001529668907235958,
        "epoch": 0.8836040216550657,
        "step": 6855
    },
    {
        "loss": 2.5792,
        "grad_norm": 1.3654329776763916,
        "learning_rate": 0.0001529152758696566,
        "epoch": 0.8837329208558907,
        "step": 6856
    },
    {
        "loss": 1.6986,
        "grad_norm": 1.2187869548797607,
        "learning_rate": 0.00015286364142887645,
        "epoch": 0.8838618200567157,
        "step": 6857
    },
    {
        "loss": 1.4251,
        "grad_norm": 1.6592869758605957,
        "learning_rate": 0.00015281198742036803,
        "epoch": 0.8839907192575406,
        "step": 6858
    },
    {
        "loss": 1.7043,
        "grad_norm": 2.4571118354797363,
        "learning_rate": 0.0001527603138632513,
        "epoch": 0.8841196184583655,
        "step": 6859
    },
    {
        "loss": 2.3768,
        "grad_norm": 2.5467607975006104,
        "learning_rate": 0.0001527086207766534,
        "epoch": 0.8842485176591905,
        "step": 6860
    },
    {
        "loss": 1.0619,
        "grad_norm": 2.717036008834839,
        "learning_rate": 0.00015265690817970894,
        "epoch": 0.8843774168600155,
        "step": 6861
    },
    {
        "loss": 1.7839,
        "grad_norm": 1.3176769018173218,
        "learning_rate": 0.00015260517609155949,
        "epoch": 0.8845063160608404,
        "step": 6862
    },
    {
        "loss": 2.4408,
        "grad_norm": 2.167048215866089,
        "learning_rate": 0.00015255342453135385,
        "epoch": 0.8846352152616653,
        "step": 6863
    },
    {
        "loss": 2.3194,
        "grad_norm": 1.5838373899459839,
        "learning_rate": 0.0001525016535182483,
        "epoch": 0.8847641144624904,
        "step": 6864
    },
    {
        "loss": 2.4842,
        "grad_norm": 1.6486045122146606,
        "learning_rate": 0.00015244986307140596,
        "epoch": 0.8848930136633153,
        "step": 6865
    },
    {
        "loss": 0.7131,
        "grad_norm": 2.937615394592285,
        "learning_rate": 0.00015239805320999738,
        "epoch": 0.8850219128641402,
        "step": 6866
    },
    {
        "loss": 1.806,
        "grad_norm": 1.781788945198059,
        "learning_rate": 0.00015234622395320023,
        "epoch": 0.8851508120649652,
        "step": 6867
    },
    {
        "loss": 1.9373,
        "grad_norm": 1.958962321281433,
        "learning_rate": 0.00015229437532019927,
        "epoch": 0.8852797112657902,
        "step": 6868
    },
    {
        "loss": 2.1023,
        "grad_norm": 2.5692546367645264,
        "learning_rate": 0.00015224250733018665,
        "epoch": 0.8854086104666151,
        "step": 6869
    },
    {
        "loss": 1.6106,
        "grad_norm": 2.528871774673462,
        "learning_rate": 0.0001521906200023615,
        "epoch": 0.88553750966744,
        "step": 6870
    },
    {
        "loss": 1.7664,
        "grad_norm": 2.092292308807373,
        "learning_rate": 0.00015213871335593008,
        "epoch": 0.885666408868265,
        "step": 6871
    },
    {
        "loss": 2.0978,
        "grad_norm": 1.844186544418335,
        "learning_rate": 0.0001520867874101061,
        "epoch": 0.88579530806909,
        "step": 6872
    },
    {
        "loss": 1.6175,
        "grad_norm": 2.4748928546905518,
        "learning_rate": 0.00015203484218410991,
        "epoch": 0.8859242072699149,
        "step": 6873
    },
    {
        "loss": 2.0611,
        "grad_norm": 2.488161087036133,
        "learning_rate": 0.00015198287769716947,
        "epoch": 0.8860531064707399,
        "step": 6874
    },
    {
        "loss": 1.2972,
        "grad_norm": 1.6008329391479492,
        "learning_rate": 0.00015193089396851962,
        "epoch": 0.8861820056715648,
        "step": 6875
    },
    {
        "loss": 1.9591,
        "grad_norm": 2.5653929710388184,
        "learning_rate": 0.00015187889101740245,
        "epoch": 0.8863109048723898,
        "step": 6876
    },
    {
        "loss": 1.5583,
        "grad_norm": 2.5458791255950928,
        "learning_rate": 0.00015182686886306695,
        "epoch": 0.8864398040732148,
        "step": 6877
    },
    {
        "loss": 1.4337,
        "grad_norm": 2.522144079208374,
        "learning_rate": 0.00015177482752476952,
        "epoch": 0.8865687032740397,
        "step": 6878
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.5340030193328857,
        "learning_rate": 0.00015172276702177346,
        "epoch": 0.8866976024748646,
        "step": 6879
    },
    {
        "loss": 0.9029,
        "grad_norm": 2.655601978302002,
        "learning_rate": 0.00015167068737334915,
        "epoch": 0.8868265016756897,
        "step": 6880
    },
    {
        "loss": 1.7955,
        "grad_norm": 2.28790283203125,
        "learning_rate": 0.00015161858859877428,
        "epoch": 0.8869554008765146,
        "step": 6881
    },
    {
        "loss": 1.7163,
        "grad_norm": 2.971407413482666,
        "learning_rate": 0.0001515664707173333,
        "epoch": 0.8870843000773395,
        "step": 6882
    },
    {
        "loss": 2.2047,
        "grad_norm": 1.4605005979537964,
        "learning_rate": 0.00015151433374831796,
        "epoch": 0.8872131992781644,
        "step": 6883
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.187307834625244,
        "learning_rate": 0.00015146217771102699,
        "epoch": 0.8873420984789895,
        "step": 6884
    },
    {
        "loss": 2.3468,
        "grad_norm": 1.230464220046997,
        "learning_rate": 0.00015141000262476617,
        "epoch": 0.8874709976798144,
        "step": 6885
    },
    {
        "loss": 1.4652,
        "grad_norm": 2.7341771125793457,
        "learning_rate": 0.0001513578085088484,
        "epoch": 0.8875998968806393,
        "step": 6886
    },
    {
        "loss": 1.4442,
        "grad_norm": 2.5861403942108154,
        "learning_rate": 0.0001513055953825936,
        "epoch": 0.8877287960814643,
        "step": 6887
    },
    {
        "loss": 2.0961,
        "grad_norm": 2.2921838760375977,
        "learning_rate": 0.0001512533632653286,
        "epoch": 0.8878576952822892,
        "step": 6888
    },
    {
        "loss": 2.0087,
        "grad_norm": 2.131453037261963,
        "learning_rate": 0.00015120111217638751,
        "epoch": 0.8879865944831142,
        "step": 6889
    },
    {
        "loss": 1.5382,
        "grad_norm": 1.6662561893463135,
        "learning_rate": 0.00015114884213511128,
        "epoch": 0.8881154936839392,
        "step": 6890
    },
    {
        "loss": 2.1638,
        "grad_norm": 1.9686752557754517,
        "learning_rate": 0.00015109655316084783,
        "epoch": 0.8882443928847641,
        "step": 6891
    },
    {
        "loss": 1.3919,
        "grad_norm": 2.352891206741333,
        "learning_rate": 0.0001510442452729522,
        "epoch": 0.888373292085589,
        "step": 6892
    },
    {
        "loss": 1.4494,
        "grad_norm": 1.8400417566299438,
        "learning_rate": 0.0001509919184907865,
        "epoch": 0.8885021912864141,
        "step": 6893
    },
    {
        "loss": 1.3318,
        "grad_norm": 2.591933012008667,
        "learning_rate": 0.0001509395728337196,
        "epoch": 0.888631090487239,
        "step": 6894
    },
    {
        "loss": 1.6199,
        "grad_norm": 2.7435665130615234,
        "learning_rate": 0.00015088720832112765,
        "epoch": 0.8887599896880639,
        "step": 6895
    },
    {
        "loss": 1.3938,
        "grad_norm": 1.736853003501892,
        "learning_rate": 0.00015083482497239346,
        "epoch": 0.8888888888888888,
        "step": 6896
    },
    {
        "loss": 1.772,
        "grad_norm": 2.4703924655914307,
        "learning_rate": 0.00015078242280690702,
        "epoch": 0.8890177880897139,
        "step": 6897
    },
    {
        "loss": 1.8015,
        "grad_norm": 2.1153416633605957,
        "learning_rate": 0.0001507300018440653,
        "epoch": 0.8891466872905388,
        "step": 6898
    },
    {
        "loss": 0.9581,
        "grad_norm": 1.9281269311904907,
        "learning_rate": 0.0001506775621032722,
        "epoch": 0.8892755864913637,
        "step": 6899
    },
    {
        "loss": 1.5259,
        "grad_norm": 2.6055831909179688,
        "learning_rate": 0.00015062510360393843,
        "epoch": 0.8894044856921887,
        "step": 6900
    },
    {
        "loss": 2.2175,
        "grad_norm": 2.009746551513672,
        "learning_rate": 0.00015057262636548183,
        "epoch": 0.8895333848930137,
        "step": 6901
    },
    {
        "loss": 1.5702,
        "grad_norm": 1.136338710784912,
        "learning_rate": 0.000150520130407327,
        "epoch": 0.8896622840938386,
        "step": 6902
    },
    {
        "loss": 2.0731,
        "grad_norm": 1.6782010793685913,
        "learning_rate": 0.0001504676157489057,
        "epoch": 0.8897911832946636,
        "step": 6903
    },
    {
        "loss": 2.3232,
        "grad_norm": 1.5744585990905762,
        "learning_rate": 0.0001504150824096564,
        "epoch": 0.8899200824954885,
        "step": 6904
    },
    {
        "loss": 1.1782,
        "grad_norm": 2.535144567489624,
        "learning_rate": 0.00015036253040902456,
        "epoch": 0.8900489816963135,
        "step": 6905
    },
    {
        "loss": 2.5764,
        "grad_norm": 1.2608786821365356,
        "learning_rate": 0.00015030995976646262,
        "epoch": 0.8901778808971385,
        "step": 6906
    },
    {
        "loss": 1.6303,
        "grad_norm": 1.925826907157898,
        "learning_rate": 0.0001502573705014298,
        "epoch": 0.8903067800979634,
        "step": 6907
    },
    {
        "loss": 2.1422,
        "grad_norm": 3.2227282524108887,
        "learning_rate": 0.0001502047626333923,
        "epoch": 0.8904356792987883,
        "step": 6908
    },
    {
        "loss": 2.3141,
        "grad_norm": 2.3658053874969482,
        "learning_rate": 0.00015015213618182308,
        "epoch": 0.8905645784996133,
        "step": 6909
    },
    {
        "loss": 2.2612,
        "grad_norm": 1.767874836921692,
        "learning_rate": 0.00015009949116620227,
        "epoch": 0.8906934777004383,
        "step": 6910
    },
    {
        "loss": 1.6501,
        "grad_norm": 2.1507534980773926,
        "learning_rate": 0.00015004682760601644,
        "epoch": 0.8908223769012632,
        "step": 6911
    },
    {
        "loss": 1.8171,
        "grad_norm": 2.0423200130462646,
        "learning_rate": 0.00014999414552075932,
        "epoch": 0.8909512761020881,
        "step": 6912
    },
    {
        "loss": 2.5827,
        "grad_norm": 1.7176281213760376,
        "learning_rate": 0.00014994144492993164,
        "epoch": 0.8910801753029132,
        "step": 6913
    },
    {
        "loss": 1.0473,
        "grad_norm": 1.6253628730773926,
        "learning_rate": 0.00014988872585304047,
        "epoch": 0.8912090745037381,
        "step": 6914
    },
    {
        "loss": 1.7013,
        "grad_norm": 2.678906202316284,
        "learning_rate": 0.00014983598830960022,
        "epoch": 0.891337973704563,
        "step": 6915
    },
    {
        "loss": 2.0088,
        "grad_norm": 3.0530898571014404,
        "learning_rate": 0.00014978323231913188,
        "epoch": 0.891466872905388,
        "step": 6916
    },
    {
        "loss": 1.0828,
        "grad_norm": 1.7678589820861816,
        "learning_rate": 0.0001497304579011634,
        "epoch": 0.891595772106213,
        "step": 6917
    },
    {
        "loss": 1.1946,
        "grad_norm": 2.2941904067993164,
        "learning_rate": 0.00014967766507522937,
        "epoch": 0.8917246713070379,
        "step": 6918
    },
    {
        "loss": 2.2672,
        "grad_norm": 1.188194751739502,
        "learning_rate": 0.00014962485386087133,
        "epoch": 0.8918535705078628,
        "step": 6919
    },
    {
        "loss": 1.128,
        "grad_norm": 4.454037666320801,
        "learning_rate": 0.00014957202427763768,
        "epoch": 0.8919824697086878,
        "step": 6920
    },
    {
        "loss": 2.4631,
        "grad_norm": 1.9435653686523438,
        "learning_rate": 0.00014951917634508348,
        "epoch": 0.8921113689095128,
        "step": 6921
    },
    {
        "loss": 1.3032,
        "grad_norm": 2.6233699321746826,
        "learning_rate": 0.00014946631008277059,
        "epoch": 0.8922402681103377,
        "step": 6922
    },
    {
        "loss": 2.0724,
        "grad_norm": 1.4077907800674438,
        "learning_rate": 0.00014941342551026785,
        "epoch": 0.8923691673111627,
        "step": 6923
    },
    {
        "loss": 2.2954,
        "grad_norm": 1.2262459993362427,
        "learning_rate": 0.00014936052264715065,
        "epoch": 0.8924980665119876,
        "step": 6924
    },
    {
        "loss": 1.6046,
        "grad_norm": 2.9215970039367676,
        "learning_rate": 0.00014930760151300127,
        "epoch": 0.8926269657128125,
        "step": 6925
    },
    {
        "loss": 2.0663,
        "grad_norm": 2.3708248138427734,
        "learning_rate": 0.00014925466212740862,
        "epoch": 0.8927558649136376,
        "step": 6926
    },
    {
        "loss": 1.4156,
        "grad_norm": 1.5522229671478271,
        "learning_rate": 0.00014920170450996866,
        "epoch": 0.8928847641144625,
        "step": 6927
    },
    {
        "loss": 1.043,
        "grad_norm": 2.78467059135437,
        "learning_rate": 0.0001491487286802837,
        "epoch": 0.8930136633152874,
        "step": 6928
    },
    {
        "loss": 1.7719,
        "grad_norm": 1.704325556755066,
        "learning_rate": 0.00014909573465796304,
        "epoch": 0.8931425625161123,
        "step": 6929
    },
    {
        "loss": 1.8498,
        "grad_norm": 1.7558716535568237,
        "learning_rate": 0.00014904272246262282,
        "epoch": 0.8932714617169374,
        "step": 6930
    },
    {
        "loss": 2.1904,
        "grad_norm": 1.9648184776306152,
        "learning_rate": 0.00014898969211388555,
        "epoch": 0.8934003609177623,
        "step": 6931
    },
    {
        "loss": 1.3145,
        "grad_norm": 2.1908655166625977,
        "learning_rate": 0.00014893664363138082,
        "epoch": 0.8935292601185872,
        "step": 6932
    },
    {
        "loss": 1.7728,
        "grad_norm": 1.8232706785202026,
        "learning_rate": 0.0001488835770347447,
        "epoch": 0.8936581593194122,
        "step": 6933
    },
    {
        "loss": 1.9525,
        "grad_norm": 1.9399304389953613,
        "learning_rate": 0.00014883049234362008,
        "epoch": 0.8937870585202372,
        "step": 6934
    },
    {
        "loss": 1.9572,
        "grad_norm": 2.736426591873169,
        "learning_rate": 0.0001487773895776565,
        "epoch": 0.8939159577210621,
        "step": 6935
    },
    {
        "loss": 2.4255,
        "grad_norm": 1.595658302307129,
        "learning_rate": 0.00014872426875651016,
        "epoch": 0.8940448569218871,
        "step": 6936
    },
    {
        "loss": 0.9638,
        "grad_norm": 1.922974944114685,
        "learning_rate": 0.00014867112989984404,
        "epoch": 0.894173756122712,
        "step": 6937
    },
    {
        "loss": 2.1427,
        "grad_norm": 1.8161439895629883,
        "learning_rate": 0.00014861797302732774,
        "epoch": 0.894302655323537,
        "step": 6938
    },
    {
        "loss": 1.8551,
        "grad_norm": 2.0216116905212402,
        "learning_rate": 0.00014856479815863745,
        "epoch": 0.894431554524362,
        "step": 6939
    },
    {
        "loss": 1.2654,
        "grad_norm": 2.892524480819702,
        "learning_rate": 0.0001485116053134562,
        "epoch": 0.8945604537251869,
        "step": 6940
    },
    {
        "loss": 2.1533,
        "grad_norm": 1.542838215827942,
        "learning_rate": 0.00014845839451147356,
        "epoch": 0.8946893529260118,
        "step": 6941
    },
    {
        "loss": 1.9956,
        "grad_norm": 2.8149945735931396,
        "learning_rate": 0.00014840516577238573,
        "epoch": 0.8948182521268369,
        "step": 6942
    },
    {
        "loss": 1.3049,
        "grad_norm": 1.5651720762252808,
        "learning_rate": 0.00014835191911589552,
        "epoch": 0.8949471513276618,
        "step": 6943
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.921049952507019,
        "learning_rate": 0.0001482986545617126,
        "epoch": 0.8950760505284867,
        "step": 6944
    },
    {
        "loss": 1.4821,
        "grad_norm": 2.2613542079925537,
        "learning_rate": 0.00014824537212955292,
        "epoch": 0.8952049497293116,
        "step": 6945
    },
    {
        "loss": 1.5699,
        "grad_norm": 2.805152416229248,
        "learning_rate": 0.00014819207183913927,
        "epoch": 0.8953338489301367,
        "step": 6946
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.1936514377593994,
        "learning_rate": 0.0001481387537102012,
        "epoch": 0.8954627481309616,
        "step": 6947
    },
    {
        "loss": 1.3606,
        "grad_norm": 2.3046555519104004,
        "learning_rate": 0.00014808541776247436,
        "epoch": 0.8955916473317865,
        "step": 6948
    },
    {
        "loss": 2.4893,
        "grad_norm": 2.073557138442993,
        "learning_rate": 0.00014803206401570152,
        "epoch": 0.8957205465326115,
        "step": 6949
    },
    {
        "loss": 2.3984,
        "grad_norm": 1.5094175338745117,
        "learning_rate": 0.00014797869248963177,
        "epoch": 0.8958494457334365,
        "step": 6950
    },
    {
        "loss": 2.4377,
        "grad_norm": 1.8594999313354492,
        "learning_rate": 0.0001479253032040208,
        "epoch": 0.8959783449342614,
        "step": 6951
    },
    {
        "loss": 2.1855,
        "grad_norm": 2.230342388153076,
        "learning_rate": 0.0001478718961786309,
        "epoch": 0.8961072441350864,
        "step": 6952
    },
    {
        "loss": 2.4997,
        "grad_norm": 1.457450032234192,
        "learning_rate": 0.000147818471433231,
        "epoch": 0.8962361433359113,
        "step": 6953
    },
    {
        "loss": 1.7596,
        "grad_norm": 3.0510735511779785,
        "learning_rate": 0.00014776502898759648,
        "epoch": 0.8963650425367363,
        "step": 6954
    },
    {
        "loss": 1.4597,
        "grad_norm": 3.6052987575531006,
        "learning_rate": 0.00014771156886150933,
        "epoch": 0.8964939417375613,
        "step": 6955
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.3875401020050049,
        "learning_rate": 0.00014765809107475798,
        "epoch": 0.8966228409383862,
        "step": 6956
    },
    {
        "loss": 1.604,
        "grad_norm": 2.774752140045166,
        "learning_rate": 0.00014760459564713767,
        "epoch": 0.8967517401392111,
        "step": 6957
    },
    {
        "loss": 1.5084,
        "grad_norm": 2.0755231380462646,
        "learning_rate": 0.0001475510825984499,
        "epoch": 0.8968806393400361,
        "step": 6958
    },
    {
        "loss": 2.426,
        "grad_norm": 1.4784791469573975,
        "learning_rate": 0.0001474975519485027,
        "epoch": 0.8970095385408611,
        "step": 6959
    },
    {
        "loss": 1.8961,
        "grad_norm": 2.02547025680542,
        "learning_rate": 0.00014744400371711076,
        "epoch": 0.897138437741686,
        "step": 6960
    },
    {
        "loss": 1.4842,
        "grad_norm": 1.522190809249878,
        "learning_rate": 0.0001473904379240953,
        "epoch": 0.8972673369425109,
        "step": 6961
    },
    {
        "loss": 2.0238,
        "grad_norm": 1.321280598640442,
        "learning_rate": 0.00014733685458928375,
        "epoch": 0.8973962361433359,
        "step": 6962
    },
    {
        "loss": 1.6815,
        "grad_norm": 1.825755000114441,
        "learning_rate": 0.00014728325373251035,
        "epoch": 0.8975251353441609,
        "step": 6963
    },
    {
        "loss": 2.1506,
        "grad_norm": 1.9037201404571533,
        "learning_rate": 0.00014722963537361586,
        "epoch": 0.8976540345449858,
        "step": 6964
    },
    {
        "loss": 1.7105,
        "grad_norm": 1.7426739931106567,
        "learning_rate": 0.00014717599953244708,
        "epoch": 0.8977829337458108,
        "step": 6965
    },
    {
        "loss": 1.8993,
        "grad_norm": 1.679595708847046,
        "learning_rate": 0.00014712234622885778,
        "epoch": 0.8979118329466357,
        "step": 6966
    },
    {
        "loss": 1.5254,
        "grad_norm": 2.211135149002075,
        "learning_rate": 0.00014706867548270792,
        "epoch": 0.8980407321474607,
        "step": 6967
    },
    {
        "loss": 2.0864,
        "grad_norm": 1.3511171340942383,
        "learning_rate": 0.00014701498731386402,
        "epoch": 0.8981696313482856,
        "step": 6968
    },
    {
        "loss": 1.6919,
        "grad_norm": 1.9238288402557373,
        "learning_rate": 0.00014696128174219888,
        "epoch": 0.8982985305491106,
        "step": 6969
    },
    {
        "loss": 1.6969,
        "grad_norm": 2.5240306854248047,
        "learning_rate": 0.000146907558787592,
        "epoch": 0.8984274297499355,
        "step": 6970
    },
    {
        "loss": 1.682,
        "grad_norm": 2.6599271297454834,
        "learning_rate": 0.00014685381846992924,
        "epoch": 0.8985563289507605,
        "step": 6971
    },
    {
        "loss": 1.6306,
        "grad_norm": 1.736168384552002,
        "learning_rate": 0.00014680006080910262,
        "epoch": 0.8986852281515855,
        "step": 6972
    },
    {
        "loss": 1.7421,
        "grad_norm": 1.5723254680633545,
        "learning_rate": 0.000146746285825011,
        "epoch": 0.8988141273524104,
        "step": 6973
    },
    {
        "loss": 1.6093,
        "grad_norm": 2.7715625762939453,
        "learning_rate": 0.0001466924935375594,
        "epoch": 0.8989430265532353,
        "step": 6974
    },
    {
        "loss": 2.0014,
        "grad_norm": 1.3665679693222046,
        "learning_rate": 0.00014663868396665923,
        "epoch": 0.8990719257540604,
        "step": 6975
    },
    {
        "loss": 2.3447,
        "grad_norm": 1.9596548080444336,
        "learning_rate": 0.00014658485713222837,
        "epoch": 0.8992008249548853,
        "step": 6976
    },
    {
        "loss": 0.5043,
        "grad_norm": 1.2901140451431274,
        "learning_rate": 0.00014653101305419107,
        "epoch": 0.8993297241557102,
        "step": 6977
    },
    {
        "loss": 1.6839,
        "grad_norm": 2.2276463508605957,
        "learning_rate": 0.00014647715175247812,
        "epoch": 0.8994586233565351,
        "step": 6978
    },
    {
        "loss": 1.9978,
        "grad_norm": 1.788028359413147,
        "learning_rate": 0.00014642327324702626,
        "epoch": 0.8995875225573602,
        "step": 6979
    },
    {
        "loss": 1.5212,
        "grad_norm": 2.8557145595550537,
        "learning_rate": 0.00014636937755777908,
        "epoch": 0.8997164217581851,
        "step": 6980
    },
    {
        "loss": 0.4876,
        "grad_norm": 2.068380355834961,
        "learning_rate": 0.00014631546470468632,
        "epoch": 0.89984532095901,
        "step": 6981
    },
    {
        "loss": 2.0026,
        "grad_norm": 1.9052897691726685,
        "learning_rate": 0.000146261534707704,
        "epoch": 0.899974220159835,
        "step": 6982
    },
    {
        "loss": 2.0099,
        "grad_norm": 2.527653932571411,
        "learning_rate": 0.00014620758758679463,
        "epoch": 0.90010311936066,
        "step": 6983
    },
    {
        "loss": 2.0608,
        "grad_norm": 1.744552731513977,
        "learning_rate": 0.00014615362336192696,
        "epoch": 0.9002320185614849,
        "step": 6984
    },
    {
        "loss": 2.2968,
        "grad_norm": 2.0914275646209717,
        "learning_rate": 0.00014609964205307608,
        "epoch": 0.9003609177623099,
        "step": 6985
    },
    {
        "loss": 1.7142,
        "grad_norm": 2.1929197311401367,
        "learning_rate": 0.00014604564368022344,
        "epoch": 0.9004898169631348,
        "step": 6986
    },
    {
        "loss": 1.8738,
        "grad_norm": 2.0674562454223633,
        "learning_rate": 0.00014599162826335686,
        "epoch": 0.9006187161639598,
        "step": 6987
    },
    {
        "loss": 1.9364,
        "grad_norm": 2.6421074867248535,
        "learning_rate": 0.00014593759582247036,
        "epoch": 0.9007476153647848,
        "step": 6988
    },
    {
        "loss": 2.0478,
        "grad_norm": 1.5856103897094727,
        "learning_rate": 0.0001458835463775642,
        "epoch": 0.9008765145656097,
        "step": 6989
    },
    {
        "loss": 1.703,
        "grad_norm": 2.9744510650634766,
        "learning_rate": 0.00014582947994864527,
        "epoch": 0.9010054137664346,
        "step": 6990
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.369509220123291,
        "learning_rate": 0.00014577539655572633,
        "epoch": 0.9011343129672597,
        "step": 6991
    },
    {
        "loss": 1.5949,
        "grad_norm": 1.8521445989608765,
        "learning_rate": 0.00014572129621882667,
        "epoch": 0.9012632121680846,
        "step": 6992
    },
    {
        "loss": 2.0776,
        "grad_norm": 1.4059687852859497,
        "learning_rate": 0.00014566717895797177,
        "epoch": 0.9013921113689095,
        "step": 6993
    },
    {
        "loss": 1.8144,
        "grad_norm": 1.9109445810317993,
        "learning_rate": 0.00014561304479319338,
        "epoch": 0.9015210105697344,
        "step": 6994
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.3750925064086914,
        "learning_rate": 0.00014555889374452953,
        "epoch": 0.9016499097705595,
        "step": 6995
    },
    {
        "loss": 2.2208,
        "grad_norm": 2.0976245403289795,
        "learning_rate": 0.00014550472583202455,
        "epoch": 0.9017788089713844,
        "step": 6996
    },
    {
        "loss": 1.6845,
        "grad_norm": 1.9252252578735352,
        "learning_rate": 0.0001454505410757288,
        "epoch": 0.9019077081722093,
        "step": 6997
    },
    {
        "loss": 0.731,
        "grad_norm": 2.468592405319214,
        "learning_rate": 0.00014539633949569927,
        "epoch": 0.9020366073730343,
        "step": 6998
    },
    {
        "loss": 2.2823,
        "grad_norm": 2.33566951751709,
        "learning_rate": 0.00014534212111199865,
        "epoch": 0.9021655065738592,
        "step": 6999
    },
    {
        "loss": 2.379,
        "grad_norm": 2.810969591140747,
        "learning_rate": 0.00014528788594469631,
        "epoch": 0.9022944057746842,
        "step": 7000
    },
    {
        "loss": 1.5029,
        "grad_norm": 2.5897419452667236,
        "learning_rate": 0.00014523363401386762,
        "epoch": 0.9024233049755092,
        "step": 7001
    },
    {
        "loss": 2.0028,
        "grad_norm": 1.7337571382522583,
        "learning_rate": 0.0001451793653395942,
        "epoch": 0.9025522041763341,
        "step": 7002
    },
    {
        "loss": 1.7719,
        "grad_norm": 2.369310140609741,
        "learning_rate": 0.00014512507994196376,
        "epoch": 0.902681103377159,
        "step": 7003
    },
    {
        "loss": 1.9218,
        "grad_norm": 2.304908514022827,
        "learning_rate": 0.00014507077784107044,
        "epoch": 0.902810002577984,
        "step": 7004
    },
    {
        "loss": 2.0008,
        "grad_norm": 1.9329100847244263,
        "learning_rate": 0.00014501645905701436,
        "epoch": 0.902938901778809,
        "step": 7005
    },
    {
        "loss": 2.092,
        "grad_norm": 1.4765408039093018,
        "learning_rate": 0.0001449621236099018,
        "epoch": 0.9030678009796339,
        "step": 7006
    },
    {
        "loss": 1.1181,
        "grad_norm": 2.128549337387085,
        "learning_rate": 0.00014490777151984543,
        "epoch": 0.9031967001804588,
        "step": 7007
    },
    {
        "loss": 1.7084,
        "grad_norm": 1.9974091053009033,
        "learning_rate": 0.00014485340280696385,
        "epoch": 0.9033255993812839,
        "step": 7008
    },
    {
        "loss": 1.2708,
        "grad_norm": 2.660754680633545,
        "learning_rate": 0.00014479901749138194,
        "epoch": 0.9034544985821088,
        "step": 7009
    },
    {
        "loss": 1.9521,
        "grad_norm": 1.7050901651382446,
        "learning_rate": 0.00014474461559323064,
        "epoch": 0.9035833977829337,
        "step": 7010
    },
    {
        "loss": 2.0938,
        "grad_norm": 1.2437989711761475,
        "learning_rate": 0.000144690197132647,
        "epoch": 0.9037122969837587,
        "step": 7011
    },
    {
        "loss": 1.0839,
        "grad_norm": 1.5303133726119995,
        "learning_rate": 0.00014463576212977446,
        "epoch": 0.9038411961845837,
        "step": 7012
    },
    {
        "loss": 2.0149,
        "grad_norm": 2.0192246437072754,
        "learning_rate": 0.00014458131060476225,
        "epoch": 0.9039700953854086,
        "step": 7013
    },
    {
        "loss": 1.7684,
        "grad_norm": 2.1635940074920654,
        "learning_rate": 0.00014452684257776588,
        "epoch": 0.9040989945862336,
        "step": 7014
    },
    {
        "loss": 1.2519,
        "grad_norm": 3.4218902587890625,
        "learning_rate": 0.000144472358068947,
        "epoch": 0.9042278937870585,
        "step": 7015
    },
    {
        "loss": 1.2718,
        "grad_norm": 2.346264600753784,
        "learning_rate": 0.00014441785709847331,
        "epoch": 0.9043567929878835,
        "step": 7016
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.503051996231079,
        "learning_rate": 0.00014436333968651855,
        "epoch": 0.9044856921887084,
        "step": 7017
    },
    {
        "loss": 1.5491,
        "grad_norm": 2.337467670440674,
        "learning_rate": 0.00014430880585326265,
        "epoch": 0.9046145913895334,
        "step": 7018
    },
    {
        "loss": 0.9351,
        "grad_norm": 3.1404714584350586,
        "learning_rate": 0.00014425425561889155,
        "epoch": 0.9047434905903583,
        "step": 7019
    },
    {
        "loss": 1.928,
        "grad_norm": 2.468435525894165,
        "learning_rate": 0.00014419968900359717,
        "epoch": 0.9048723897911833,
        "step": 7020
    },
    {
        "loss": 2.4516,
        "grad_norm": 1.421595811843872,
        "learning_rate": 0.00014414510602757781,
        "epoch": 0.9050012889920083,
        "step": 7021
    },
    {
        "loss": 1.7339,
        "grad_norm": 2.5907750129699707,
        "learning_rate": 0.00014409050671103748,
        "epoch": 0.9051301881928332,
        "step": 7022
    },
    {
        "loss": 0.6306,
        "grad_norm": 3.3663463592529297,
        "learning_rate": 0.00014403589107418635,
        "epoch": 0.9052590873936581,
        "step": 7023
    },
    {
        "loss": 2.0868,
        "grad_norm": 2.254685640335083,
        "learning_rate": 0.00014398125913724078,
        "epoch": 0.9053879865944832,
        "step": 7024
    },
    {
        "loss": 1.2958,
        "grad_norm": 2.8677642345428467,
        "learning_rate": 0.00014392661092042298,
        "epoch": 0.9055168857953081,
        "step": 7025
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.2613577842712402,
        "learning_rate": 0.00014387194644396124,
        "epoch": 0.905645784996133,
        "step": 7026
    },
    {
        "loss": 1.6004,
        "grad_norm": 2.045278549194336,
        "learning_rate": 0.0001438172657280899,
        "epoch": 0.905774684196958,
        "step": 7027
    },
    {
        "loss": 1.8199,
        "grad_norm": 2.0727500915527344,
        "learning_rate": 0.00014376256879304917,
        "epoch": 0.905903583397783,
        "step": 7028
    },
    {
        "loss": 1.6845,
        "grad_norm": 2.293853282928467,
        "learning_rate": 0.00014370785565908555,
        "epoch": 0.9060324825986079,
        "step": 7029
    },
    {
        "loss": 1.6685,
        "grad_norm": 2.3089919090270996,
        "learning_rate": 0.00014365312634645133,
        "epoch": 0.9061613817994328,
        "step": 7030
    },
    {
        "loss": 1.4955,
        "grad_norm": 2.448561668395996,
        "learning_rate": 0.0001435983808754047,
        "epoch": 0.9062902810002578,
        "step": 7031
    },
    {
        "loss": 1.9154,
        "grad_norm": 2.292421340942383,
        "learning_rate": 0.0001435436192662101,
        "epoch": 0.9064191802010828,
        "step": 7032
    },
    {
        "loss": 2.2537,
        "grad_norm": 2.048051595687866,
        "learning_rate": 0.00014348884153913776,
        "epoch": 0.9065480794019077,
        "step": 7033
    },
    {
        "loss": 2.23,
        "grad_norm": 1.5128289461135864,
        "learning_rate": 0.00014343404771446393,
        "epoch": 0.9066769786027327,
        "step": 7034
    },
    {
        "loss": 2.184,
        "grad_norm": 1.7511928081512451,
        "learning_rate": 0.00014337923781247068,
        "epoch": 0.9068058778035576,
        "step": 7035
    },
    {
        "loss": 1.4382,
        "grad_norm": 2.373277187347412,
        "learning_rate": 0.00014332441185344635,
        "epoch": 0.9069347770043825,
        "step": 7036
    },
    {
        "loss": 2.023,
        "grad_norm": 2.079076051712036,
        "learning_rate": 0.00014326956985768487,
        "epoch": 0.9070636762052076,
        "step": 7037
    },
    {
        "loss": 2.1636,
        "grad_norm": 1.6725045442581177,
        "learning_rate": 0.00014321471184548633,
        "epoch": 0.9071925754060325,
        "step": 7038
    },
    {
        "loss": 1.7451,
        "grad_norm": 2.57275128364563,
        "learning_rate": 0.00014315983783715674,
        "epoch": 0.9073214746068574,
        "step": 7039
    },
    {
        "loss": 1.5313,
        "grad_norm": 2.3386127948760986,
        "learning_rate": 0.00014310494785300785,
        "epoch": 0.9074503738076823,
        "step": 7040
    },
    {
        "loss": 2.1688,
        "grad_norm": 2.696373224258423,
        "learning_rate": 0.00014305004191335759,
        "epoch": 0.9075792730085074,
        "step": 7041
    },
    {
        "loss": 2.3318,
        "grad_norm": 1.887473464012146,
        "learning_rate": 0.00014299512003852953,
        "epoch": 0.9077081722093323,
        "step": 7042
    },
    {
        "loss": 1.5768,
        "grad_norm": 2.1356587409973145,
        "learning_rate": 0.00014294018224885336,
        "epoch": 0.9078370714101572,
        "step": 7043
    },
    {
        "loss": 1.4772,
        "grad_norm": 3.0594241619110107,
        "learning_rate": 0.0001428852285646645,
        "epoch": 0.9079659706109822,
        "step": 7044
    },
    {
        "loss": 1.8269,
        "grad_norm": 1.6062989234924316,
        "learning_rate": 0.00014283025900630424,
        "epoch": 0.9080948698118072,
        "step": 7045
    },
    {
        "loss": 1.8764,
        "grad_norm": 2.6157917976379395,
        "learning_rate": 0.00014277527359412003,
        "epoch": 0.9082237690126321,
        "step": 7046
    },
    {
        "loss": 1.8927,
        "grad_norm": 2.138063907623291,
        "learning_rate": 0.00014272027234846487,
        "epoch": 0.9083526682134571,
        "step": 7047
    },
    {
        "loss": 2.041,
        "grad_norm": 2.3591513633728027,
        "learning_rate": 0.00014266525528969766,
        "epoch": 0.908481567414282,
        "step": 7048
    },
    {
        "loss": 1.3327,
        "grad_norm": 2.2689435482025146,
        "learning_rate": 0.00014261022243818334,
        "epoch": 0.908610466615107,
        "step": 7049
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.362281084060669,
        "learning_rate": 0.00014255517381429254,
        "epoch": 0.908739365815932,
        "step": 7050
    },
    {
        "loss": 2.2511,
        "grad_norm": 2.002734899520874,
        "learning_rate": 0.00014250010943840183,
        "epoch": 0.9088682650167569,
        "step": 7051
    },
    {
        "loss": 2.2406,
        "grad_norm": 1.6950640678405762,
        "learning_rate": 0.0001424450293308934,
        "epoch": 0.9089971642175818,
        "step": 7052
    },
    {
        "loss": 2.0518,
        "grad_norm": 1.427337408065796,
        "learning_rate": 0.0001423899335121556,
        "epoch": 0.9091260634184068,
        "step": 7053
    },
    {
        "loss": 1.9896,
        "grad_norm": 1.3963513374328613,
        "learning_rate": 0.00014233482200258222,
        "epoch": 0.9092549626192318,
        "step": 7054
    },
    {
        "loss": 2.0975,
        "grad_norm": 1.5522656440734863,
        "learning_rate": 0.00014227969482257313,
        "epoch": 0.9093838618200567,
        "step": 7055
    },
    {
        "loss": 0.9395,
        "grad_norm": 1.9981461763381958,
        "learning_rate": 0.00014222455199253408,
        "epoch": 0.9095127610208816,
        "step": 7056
    },
    {
        "loss": 1.2154,
        "grad_norm": 2.7553951740264893,
        "learning_rate": 0.00014216939353287614,
        "epoch": 0.9096416602217067,
        "step": 7057
    },
    {
        "loss": 0.9956,
        "grad_norm": 2.415433645248413,
        "learning_rate": 0.00014211421946401672,
        "epoch": 0.9097705594225316,
        "step": 7058
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.6006051301956177,
        "learning_rate": 0.00014205902980637868,
        "epoch": 0.9098994586233565,
        "step": 7059
    },
    {
        "loss": 1.4964,
        "grad_norm": 2.1824841499328613,
        "learning_rate": 0.00014200382458039076,
        "epoch": 0.9100283578241815,
        "step": 7060
    },
    {
        "loss": 1.6656,
        "grad_norm": 2.752126455307007,
        "learning_rate": 0.00014194860380648737,
        "epoch": 0.9101572570250065,
        "step": 7061
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.439117908477783,
        "learning_rate": 0.00014189336750510876,
        "epoch": 0.9102861562258314,
        "step": 7062
    },
    {
        "loss": 2.1471,
        "grad_norm": 1.3298747539520264,
        "learning_rate": 0.00014183811569670098,
        "epoch": 0.9104150554266563,
        "step": 7063
    },
    {
        "loss": 2.0763,
        "grad_norm": 2.6089279651641846,
        "learning_rate": 0.0001417828484017157,
        "epoch": 0.9105439546274813,
        "step": 7064
    },
    {
        "loss": 2.3091,
        "grad_norm": 1.7170957326889038,
        "learning_rate": 0.00014172756564061033,
        "epoch": 0.9106728538283063,
        "step": 7065
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.3379546403884888,
        "learning_rate": 0.00014167226743384812,
        "epoch": 0.9108017530291312,
        "step": 7066
    },
    {
        "loss": 1.3228,
        "grad_norm": 3.5005290508270264,
        "learning_rate": 0.00014161695380189796,
        "epoch": 0.9109306522299562,
        "step": 7067
    },
    {
        "loss": 1.7406,
        "grad_norm": 2.697139263153076,
        "learning_rate": 0.00014156162476523446,
        "epoch": 0.9110595514307811,
        "step": 7068
    },
    {
        "loss": 1.5995,
        "grad_norm": 1.9296668767929077,
        "learning_rate": 0.0001415062803443378,
        "epoch": 0.9111884506316061,
        "step": 7069
    },
    {
        "loss": 1.6683,
        "grad_norm": 2.7403671741485596,
        "learning_rate": 0.0001414509205596942,
        "epoch": 0.9113173498324311,
        "step": 7070
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.9990897178649902,
        "learning_rate": 0.00014139554543179513,
        "epoch": 0.911446249033256,
        "step": 7071
    },
    {
        "loss": 1.0427,
        "grad_norm": 1.8556820154190063,
        "learning_rate": 0.000141340154981138,
        "epoch": 0.9115751482340809,
        "step": 7072
    },
    {
        "loss": 1.9557,
        "grad_norm": 3.1064326763153076,
        "learning_rate": 0.00014128474922822605,
        "epoch": 0.9117040474349059,
        "step": 7073
    },
    {
        "loss": 2.1606,
        "grad_norm": 2.258598804473877,
        "learning_rate": 0.00014122932819356777,
        "epoch": 0.9118329466357309,
        "step": 7074
    },
    {
        "loss": 0.8526,
        "grad_norm": 3.3095624446868896,
        "learning_rate": 0.00014117389189767756,
        "epoch": 0.9119618458365558,
        "step": 7075
    },
    {
        "loss": 2.0237,
        "grad_norm": 1.714758038520813,
        "learning_rate": 0.00014111844036107548,
        "epoch": 0.9120907450373807,
        "step": 7076
    },
    {
        "loss": 1.3382,
        "grad_norm": 2.920553207397461,
        "learning_rate": 0.00014106297360428718,
        "epoch": 0.9122196442382057,
        "step": 7077
    },
    {
        "loss": 1.4109,
        "grad_norm": 2.315258741378784,
        "learning_rate": 0.00014100749164784382,
        "epoch": 0.9123485434390307,
        "step": 7078
    },
    {
        "loss": 2.518,
        "grad_norm": 1.7031415700912476,
        "learning_rate": 0.00014095199451228248,
        "epoch": 0.9124774426398556,
        "step": 7079
    },
    {
        "loss": 2.3157,
        "grad_norm": 1.7729239463806152,
        "learning_rate": 0.0001408964822181456,
        "epoch": 0.9126063418406806,
        "step": 7080
    },
    {
        "loss": 1.9323,
        "grad_norm": 2.8558974266052246,
        "learning_rate": 0.00014084095478598133,
        "epoch": 0.9127352410415055,
        "step": 7081
    },
    {
        "loss": 1.9897,
        "grad_norm": 2.393963098526001,
        "learning_rate": 0.0001407854122363434,
        "epoch": 0.9128641402423305,
        "step": 7082
    },
    {
        "loss": 2.1053,
        "grad_norm": 1.5343117713928223,
        "learning_rate": 0.00014072985458979119,
        "epoch": 0.9129930394431555,
        "step": 7083
    },
    {
        "loss": 1.4272,
        "grad_norm": 2.2134463787078857,
        "learning_rate": 0.0001406742818668896,
        "epoch": 0.9131219386439804,
        "step": 7084
    },
    {
        "loss": 2.1618,
        "grad_norm": 1.0369020700454712,
        "learning_rate": 0.00014061869408820918,
        "epoch": 0.9132508378448053,
        "step": 7085
    },
    {
        "loss": 0.8468,
        "grad_norm": 2.919133424758911,
        "learning_rate": 0.00014056309127432587,
        "epoch": 0.9133797370456304,
        "step": 7086
    },
    {
        "loss": 2.0748,
        "grad_norm": 1.2878178358078003,
        "learning_rate": 0.00014050747344582158,
        "epoch": 0.9135086362464553,
        "step": 7087
    },
    {
        "loss": 2.038,
        "grad_norm": 1.4015843868255615,
        "learning_rate": 0.0001404518406232832,
        "epoch": 0.9136375354472802,
        "step": 7088
    },
    {
        "loss": 1.8626,
        "grad_norm": 1.9241464138031006,
        "learning_rate": 0.00014039619282730362,
        "epoch": 0.9137664346481051,
        "step": 7089
    },
    {
        "loss": 1.9354,
        "grad_norm": 1.2512489557266235,
        "learning_rate": 0.0001403405300784813,
        "epoch": 0.9138953338489302,
        "step": 7090
    },
    {
        "loss": 2.1596,
        "grad_norm": 1.541580080986023,
        "learning_rate": 0.0001402848523974198,
        "epoch": 0.9140242330497551,
        "step": 7091
    },
    {
        "loss": 1.1503,
        "grad_norm": 2.7887938022613525,
        "learning_rate": 0.00014022915980472866,
        "epoch": 0.91415313225058,
        "step": 7092
    },
    {
        "loss": 1.2731,
        "grad_norm": 2.4504621028900146,
        "learning_rate": 0.0001401734523210227,
        "epoch": 0.914282031451405,
        "step": 7093
    },
    {
        "loss": 1.5128,
        "grad_norm": 2.4248955249786377,
        "learning_rate": 0.0001401177299669223,
        "epoch": 0.91441093065223,
        "step": 7094
    },
    {
        "loss": 2.151,
        "grad_norm": 1.5027152299880981,
        "learning_rate": 0.00014006199276305332,
        "epoch": 0.9145398298530549,
        "step": 7095
    },
    {
        "loss": 2.6778,
        "grad_norm": 1.2495086193084717,
        "learning_rate": 0.00014000624073004725,
        "epoch": 0.9146687290538799,
        "step": 7096
    },
    {
        "loss": 1.5472,
        "grad_norm": 2.2158889770507812,
        "learning_rate": 0.0001399504738885409,
        "epoch": 0.9147976282547048,
        "step": 7097
    },
    {
        "loss": 2.0733,
        "grad_norm": 2.457857847213745,
        "learning_rate": 0.0001398946922591766,
        "epoch": 0.9149265274555298,
        "step": 7098
    },
    {
        "loss": 1.7205,
        "grad_norm": 3.635845422744751,
        "learning_rate": 0.0001398388958626023,
        "epoch": 0.9150554266563548,
        "step": 7099
    },
    {
        "loss": 1.732,
        "grad_norm": 2.249110460281372,
        "learning_rate": 0.00013978308471947122,
        "epoch": 0.9151843258571797,
        "step": 7100
    },
    {
        "loss": 1.4222,
        "grad_norm": 2.1154468059539795,
        "learning_rate": 0.00013972725885044214,
        "epoch": 0.9153132250580046,
        "step": 7101
    },
    {
        "loss": 1.6946,
        "grad_norm": 2.254786491394043,
        "learning_rate": 0.00013967141827617922,
        "epoch": 0.9154421242588296,
        "step": 7102
    },
    {
        "loss": 1.9016,
        "grad_norm": 1.6618162393569946,
        "learning_rate": 0.00013961556301735217,
        "epoch": 0.9155710234596546,
        "step": 7103
    },
    {
        "loss": 1.5348,
        "grad_norm": 2.961920976638794,
        "learning_rate": 0.00013955969309463614,
        "epoch": 0.9156999226604795,
        "step": 7104
    },
    {
        "loss": 1.0841,
        "grad_norm": 2.700044870376587,
        "learning_rate": 0.00013950380852871148,
        "epoch": 0.9158288218613044,
        "step": 7105
    },
    {
        "loss": 2.2663,
        "grad_norm": 1.3978694677352905,
        "learning_rate": 0.0001394479093402642,
        "epoch": 0.9159577210621294,
        "step": 7106
    },
    {
        "loss": 2.1121,
        "grad_norm": 1.9644403457641602,
        "learning_rate": 0.00013939199554998584,
        "epoch": 0.9160866202629544,
        "step": 7107
    },
    {
        "loss": 1.8172,
        "grad_norm": 1.3952271938323975,
        "learning_rate": 0.0001393360671785728,
        "epoch": 0.9162155194637793,
        "step": 7108
    },
    {
        "loss": 2.2204,
        "grad_norm": 1.907164454460144,
        "learning_rate": 0.00013928012424672755,
        "epoch": 0.9163444186646043,
        "step": 7109
    },
    {
        "loss": 2.3039,
        "grad_norm": 1.9184964895248413,
        "learning_rate": 0.00013922416677515748,
        "epoch": 0.9164733178654292,
        "step": 7110
    },
    {
        "loss": 1.78,
        "grad_norm": 1.8674589395523071,
        "learning_rate": 0.00013916819478457552,
        "epoch": 0.9166022170662542,
        "step": 7111
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.6509606838226318,
        "learning_rate": 0.00013911220829569994,
        "epoch": 0.9167311162670791,
        "step": 7112
    },
    {
        "loss": 1.9819,
        "grad_norm": 2.0188283920288086,
        "learning_rate": 0.00013905620732925452,
        "epoch": 0.9168600154679041,
        "step": 7113
    },
    {
        "loss": 1.5648,
        "grad_norm": 1.8859704732894897,
        "learning_rate": 0.0001390001919059682,
        "epoch": 0.916988914668729,
        "step": 7114
    },
    {
        "loss": 1.7842,
        "grad_norm": 1.6558297872543335,
        "learning_rate": 0.00013894416204657533,
        "epoch": 0.917117813869554,
        "step": 7115
    },
    {
        "loss": 0.9548,
        "grad_norm": 2.434767484664917,
        "learning_rate": 0.00013888811777181573,
        "epoch": 0.917246713070379,
        "step": 7116
    },
    {
        "loss": 2.0638,
        "grad_norm": 1.4040457010269165,
        "learning_rate": 0.00013883205910243437,
        "epoch": 0.9173756122712039,
        "step": 7117
    },
    {
        "loss": 2.0413,
        "grad_norm": 1.9383959770202637,
        "learning_rate": 0.0001387759860591817,
        "epoch": 0.9175045114720288,
        "step": 7118
    },
    {
        "loss": 1.423,
        "grad_norm": 2.999173402786255,
        "learning_rate": 0.0001387198986628133,
        "epoch": 0.9176334106728539,
        "step": 7119
    },
    {
        "loss": 1.4115,
        "grad_norm": 2.935433864593506,
        "learning_rate": 0.0001386637969340903,
        "epoch": 0.9177623098736788,
        "step": 7120
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.960740566253662,
        "learning_rate": 0.00013860768089377903,
        "epoch": 0.9178912090745037,
        "step": 7121
    },
    {
        "loss": 1.4872,
        "grad_norm": 1.3135044574737549,
        "learning_rate": 0.00013855155056265102,
        "epoch": 0.9180201082753287,
        "step": 7122
    },
    {
        "loss": 0.9469,
        "grad_norm": 2.145808696746826,
        "learning_rate": 0.00013849540596148321,
        "epoch": 0.9181490074761537,
        "step": 7123
    },
    {
        "loss": 1.5333,
        "grad_norm": 2.1807730197906494,
        "learning_rate": 0.00013843924711105796,
        "epoch": 0.9182779066769786,
        "step": 7124
    },
    {
        "loss": 1.7458,
        "grad_norm": 2.4583780765533447,
        "learning_rate": 0.0001383830740321624,
        "epoch": 0.9184068058778035,
        "step": 7125
    },
    {
        "loss": 1.7956,
        "grad_norm": 2.3255250453948975,
        "learning_rate": 0.00013832688674558956,
        "epoch": 0.9185357050786285,
        "step": 7126
    },
    {
        "loss": 1.9855,
        "grad_norm": 3.2460038661956787,
        "learning_rate": 0.00013827068527213726,
        "epoch": 0.9186646042794535,
        "step": 7127
    },
    {
        "loss": 2.3551,
        "grad_norm": 2.7029056549072266,
        "learning_rate": 0.00013821446963260887,
        "epoch": 0.9187935034802784,
        "step": 7128
    },
    {
        "loss": 1.3555,
        "grad_norm": 2.3228232860565186,
        "learning_rate": 0.00013815823984781271,
        "epoch": 0.9189224026811034,
        "step": 7129
    },
    {
        "loss": 1.8131,
        "grad_norm": 1.8157204389572144,
        "learning_rate": 0.00013810199593856264,
        "epoch": 0.9190513018819283,
        "step": 7130
    },
    {
        "loss": 1.3558,
        "grad_norm": 2.3486595153808594,
        "learning_rate": 0.00013804573792567762,
        "epoch": 0.9191802010827533,
        "step": 7131
    },
    {
        "loss": 1.5062,
        "grad_norm": 2.772451400756836,
        "learning_rate": 0.00013798946582998167,
        "epoch": 0.9193091002835783,
        "step": 7132
    },
    {
        "loss": 1.8358,
        "grad_norm": 3.695333957672119,
        "learning_rate": 0.00013793317967230437,
        "epoch": 0.9194379994844032,
        "step": 7133
    },
    {
        "loss": 0.5147,
        "grad_norm": 1.7742362022399902,
        "learning_rate": 0.0001378768794734802,
        "epoch": 0.9195668986852281,
        "step": 7134
    },
    {
        "loss": 1.5613,
        "grad_norm": 2.849604368209839,
        "learning_rate": 0.00013782056525434895,
        "epoch": 0.9196957978860532,
        "step": 7135
    },
    {
        "loss": 2.3897,
        "grad_norm": 1.932665467262268,
        "learning_rate": 0.0001377642370357556,
        "epoch": 0.9198246970868781,
        "step": 7136
    },
    {
        "loss": 2.0119,
        "grad_norm": 1.5551159381866455,
        "learning_rate": 0.00013770789483855028,
        "epoch": 0.919953596287703,
        "step": 7137
    },
    {
        "loss": 1.1966,
        "grad_norm": 1.975355863571167,
        "learning_rate": 0.0001376515386835884,
        "epoch": 0.9200824954885279,
        "step": 7138
    },
    {
        "loss": 2.0792,
        "grad_norm": 2.0476489067077637,
        "learning_rate": 0.00013759516859173043,
        "epoch": 0.920211394689353,
        "step": 7139
    },
    {
        "loss": 1.7973,
        "grad_norm": 1.2210112810134888,
        "learning_rate": 0.00013753878458384194,
        "epoch": 0.9203402938901779,
        "step": 7140
    },
    {
        "loss": 1.2574,
        "grad_norm": 2.533344030380249,
        "learning_rate": 0.00013748238668079388,
        "epoch": 0.9204691930910028,
        "step": 7141
    },
    {
        "loss": 1.9262,
        "grad_norm": 2.285187244415283,
        "learning_rate": 0.00013742597490346212,
        "epoch": 0.9205980922918278,
        "step": 7142
    },
    {
        "loss": 1.6427,
        "grad_norm": 2.1801018714904785,
        "learning_rate": 0.00013736954927272778,
        "epoch": 0.9207269914926527,
        "step": 7143
    },
    {
        "loss": 2.6581,
        "grad_norm": 2.1459529399871826,
        "learning_rate": 0.00013731310980947704,
        "epoch": 0.9208558906934777,
        "step": 7144
    },
    {
        "loss": 2.117,
        "grad_norm": 2.2352373600006104,
        "learning_rate": 0.00013725665653460128,
        "epoch": 0.9209847898943027,
        "step": 7145
    },
    {
        "loss": 1.7698,
        "grad_norm": 1.145686149597168,
        "learning_rate": 0.0001372001894689968,
        "epoch": 0.9211136890951276,
        "step": 7146
    },
    {
        "loss": 1.68,
        "grad_norm": 2.638554811477661,
        "learning_rate": 0.0001371437086335654,
        "epoch": 0.9212425882959525,
        "step": 7147
    },
    {
        "loss": 1.5775,
        "grad_norm": 2.8580949306488037,
        "learning_rate": 0.0001370872140492136,
        "epoch": 0.9213714874967776,
        "step": 7148
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.894168734550476,
        "learning_rate": 0.00013703070573685303,
        "epoch": 0.9215003866976025,
        "step": 7149
    },
    {
        "loss": 1.8823,
        "grad_norm": 2.2277426719665527,
        "learning_rate": 0.00013697418371740074,
        "epoch": 0.9216292858984274,
        "step": 7150
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.7671242952346802,
        "learning_rate": 0.00013691764801177848,
        "epoch": 0.9217581850992523,
        "step": 7151
    },
    {
        "loss": 1.6069,
        "grad_norm": 2.228459596633911,
        "learning_rate": 0.00013686109864091326,
        "epoch": 0.9218870843000774,
        "step": 7152
    },
    {
        "loss": 1.9133,
        "grad_norm": 2.024789810180664,
        "learning_rate": 0.00013680453562573704,
        "epoch": 0.9220159835009023,
        "step": 7153
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.4447389841079712,
        "learning_rate": 0.0001367479589871869,
        "epoch": 0.9221448827017272,
        "step": 7154
    },
    {
        "loss": 1.3684,
        "grad_norm": 1.9329241514205933,
        "learning_rate": 0.000136691368746205,
        "epoch": 0.9222737819025522,
        "step": 7155
    },
    {
        "loss": 1.455,
        "grad_norm": 2.420851707458496,
        "learning_rate": 0.0001366347649237385,
        "epoch": 0.9224026811033772,
        "step": 7156
    },
    {
        "loss": 1.736,
        "grad_norm": 2.2895348072052,
        "learning_rate": 0.00013657814754073947,
        "epoch": 0.9225315803042021,
        "step": 7157
    },
    {
        "loss": 1.7402,
        "grad_norm": 2.312865972518921,
        "learning_rate": 0.0001365215166181652,
        "epoch": 0.922660479505027,
        "step": 7158
    },
    {
        "loss": 1.5698,
        "grad_norm": 2.9982032775878906,
        "learning_rate": 0.00013646487217697795,
        "epoch": 0.922789378705852,
        "step": 7159
    },
    {
        "loss": 2.1417,
        "grad_norm": 1.8944019079208374,
        "learning_rate": 0.00013640821423814475,
        "epoch": 0.922918277906677,
        "step": 7160
    },
    {
        "loss": 2.3872,
        "grad_norm": 1.2766517400741577,
        "learning_rate": 0.00013635154282263788,
        "epoch": 0.923047177107502,
        "step": 7161
    },
    {
        "loss": 2.2319,
        "grad_norm": 1.572074055671692,
        "learning_rate": 0.00013629485795143472,
        "epoch": 0.9231760763083269,
        "step": 7162
    },
    {
        "loss": 1.8792,
        "grad_norm": 2.8952648639678955,
        "learning_rate": 0.00013623815964551708,
        "epoch": 0.9233049755091518,
        "step": 7163
    },
    {
        "loss": 1.6747,
        "grad_norm": 2.6703593730926514,
        "learning_rate": 0.00013618144792587237,
        "epoch": 0.9234338747099768,
        "step": 7164
    },
    {
        "loss": 2.0306,
        "grad_norm": 2.3459267616271973,
        "learning_rate": 0.00013612472281349276,
        "epoch": 0.9235627739108018,
        "step": 7165
    },
    {
        "loss": 1.5632,
        "grad_norm": 1.6834133863449097,
        "learning_rate": 0.00013606798432937503,
        "epoch": 0.9236916731116267,
        "step": 7166
    },
    {
        "loss": 2.4022,
        "grad_norm": 1.8135764598846436,
        "learning_rate": 0.00013601123249452147,
        "epoch": 0.9238205723124516,
        "step": 7167
    },
    {
        "loss": 1.9207,
        "grad_norm": 2.280303478240967,
        "learning_rate": 0.00013595446732993894,
        "epoch": 0.9239494715132767,
        "step": 7168
    },
    {
        "loss": 2.0448,
        "grad_norm": 2.001688003540039,
        "learning_rate": 0.00013589768885663932,
        "epoch": 0.9240783707141016,
        "step": 7169
    },
    {
        "loss": 1.1191,
        "grad_norm": 2.139092445373535,
        "learning_rate": 0.00013584089709563947,
        "epoch": 0.9242072699149265,
        "step": 7170
    },
    {
        "loss": 1.5142,
        "grad_norm": 2.719148635864258,
        "learning_rate": 0.00013578409206796104,
        "epoch": 0.9243361691157514,
        "step": 7171
    },
    {
        "loss": 1.4379,
        "grad_norm": 1.6846479177474976,
        "learning_rate": 0.0001357272737946308,
        "epoch": 0.9244650683165765,
        "step": 7172
    },
    {
        "loss": 1.2183,
        "grad_norm": 2.9207658767700195,
        "learning_rate": 0.0001356704422966803,
        "epoch": 0.9245939675174014,
        "step": 7173
    },
    {
        "loss": 2.0453,
        "grad_norm": 2.1755247116088867,
        "learning_rate": 0.00013561359759514584,
        "epoch": 0.9247228667182263,
        "step": 7174
    },
    {
        "loss": 1.8471,
        "grad_norm": 2.7511439323425293,
        "learning_rate": 0.00013555673971106896,
        "epoch": 0.9248517659190513,
        "step": 7175
    },
    {
        "loss": 1.3483,
        "grad_norm": 2.7007198333740234,
        "learning_rate": 0.00013549986866549576,
        "epoch": 0.9249806651198763,
        "step": 7176
    },
    {
        "loss": 1.9416,
        "grad_norm": 2.2069449424743652,
        "learning_rate": 0.00013544298447947732,
        "epoch": 0.9251095643207012,
        "step": 7177
    },
    {
        "loss": 1.7994,
        "grad_norm": 4.09548807144165,
        "learning_rate": 0.00013538608717406957,
        "epoch": 0.9252384635215262,
        "step": 7178
    },
    {
        "loss": 1.9346,
        "grad_norm": 1.8168973922729492,
        "learning_rate": 0.0001353291767703335,
        "epoch": 0.9253673627223511,
        "step": 7179
    },
    {
        "loss": 1.426,
        "grad_norm": 3.6153767108917236,
        "learning_rate": 0.0001352722532893345,
        "epoch": 0.925496261923176,
        "step": 7180
    },
    {
        "loss": 1.767,
        "grad_norm": 2.136561632156372,
        "learning_rate": 0.0001352153167521432,
        "epoch": 0.9256251611240011,
        "step": 7181
    },
    {
        "loss": 1.4975,
        "grad_norm": 1.5675843954086304,
        "learning_rate": 0.00013515836717983506,
        "epoch": 0.925754060324826,
        "step": 7182
    },
    {
        "loss": 1.5971,
        "grad_norm": 1.5744681358337402,
        "learning_rate": 0.00013510140459349,
        "epoch": 0.9258829595256509,
        "step": 7183
    },
    {
        "loss": 0.8671,
        "grad_norm": 2.4644877910614014,
        "learning_rate": 0.00013504442901419314,
        "epoch": 0.9260118587264758,
        "step": 7184
    },
    {
        "loss": 1.3134,
        "grad_norm": 3.596038341522217,
        "learning_rate": 0.0001349874404630342,
        "epoch": 0.9261407579273009,
        "step": 7185
    },
    {
        "loss": 1.997,
        "grad_norm": 2.3297858238220215,
        "learning_rate": 0.0001349304389611078,
        "epoch": 0.9262696571281258,
        "step": 7186
    },
    {
        "loss": 1.6781,
        "grad_norm": 2.331373453140259,
        "learning_rate": 0.0001348734245295133,
        "epoch": 0.9263985563289507,
        "step": 7187
    },
    {
        "loss": 2.1166,
        "grad_norm": 2.6941113471984863,
        "learning_rate": 0.0001348163971893548,
        "epoch": 0.9265274555297757,
        "step": 7188
    },
    {
        "loss": 1.6528,
        "grad_norm": 2.218844413757324,
        "learning_rate": 0.00013475935696174136,
        "epoch": 0.9266563547306007,
        "step": 7189
    },
    {
        "loss": 2.1974,
        "grad_norm": 2.3681082725524902,
        "learning_rate": 0.00013470230386778664,
        "epoch": 0.9267852539314256,
        "step": 7190
    },
    {
        "loss": 1.262,
        "grad_norm": 2.226520299911499,
        "learning_rate": 0.00013464523792860905,
        "epoch": 0.9269141531322506,
        "step": 7191
    },
    {
        "loss": 2.2149,
        "grad_norm": 2.085017681121826,
        "learning_rate": 0.00013458815916533192,
        "epoch": 0.9270430523330755,
        "step": 7192
    },
    {
        "loss": 2.2008,
        "grad_norm": 1.9331058263778687,
        "learning_rate": 0.0001345310675990832,
        "epoch": 0.9271719515339005,
        "step": 7193
    },
    {
        "loss": 1.7749,
        "grad_norm": 2.313173770904541,
        "learning_rate": 0.00013447396325099557,
        "epoch": 0.9273008507347255,
        "step": 7194
    },
    {
        "loss": 1.5806,
        "grad_norm": 1.209957480430603,
        "learning_rate": 0.00013441684614220645,
        "epoch": 0.9274297499355504,
        "step": 7195
    },
    {
        "loss": 2.2547,
        "grad_norm": 1.6535454988479614,
        "learning_rate": 0.00013435971629385815,
        "epoch": 0.9275586491363753,
        "step": 7196
    },
    {
        "loss": 1.2977,
        "grad_norm": 2.2730824947357178,
        "learning_rate": 0.00013430257372709737,
        "epoch": 0.9276875483372004,
        "step": 7197
    },
    {
        "loss": 1.9192,
        "grad_norm": 1.47370445728302,
        "learning_rate": 0.00013424541846307575,
        "epoch": 0.9278164475380253,
        "step": 7198
    },
    {
        "loss": 1.641,
        "grad_norm": 2.131542205810547,
        "learning_rate": 0.00013418825052294978,
        "epoch": 0.9279453467388502,
        "step": 7199
    },
    {
        "loss": 2.2747,
        "grad_norm": 2.372105121612549,
        "learning_rate": 0.00013413106992788017,
        "epoch": 0.9280742459396751,
        "step": 7200
    },
    {
        "loss": 1.8815,
        "grad_norm": 2.09600830078125,
        "learning_rate": 0.00013407387669903275,
        "epoch": 0.9282031451405002,
        "step": 7201
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.792868733406067,
        "learning_rate": 0.00013401667085757786,
        "epoch": 0.9283320443413251,
        "step": 7202
    },
    {
        "loss": 1.3733,
        "grad_norm": 2.2513339519500732,
        "learning_rate": 0.00013395945242469047,
        "epoch": 0.92846094354215,
        "step": 7203
    },
    {
        "loss": 1.9349,
        "grad_norm": 3.3452677726745605,
        "learning_rate": 0.00013390222142155013,
        "epoch": 0.928589842742975,
        "step": 7204
    },
    {
        "loss": 1.8983,
        "grad_norm": 2.5527453422546387,
        "learning_rate": 0.00013384497786934145,
        "epoch": 0.9287187419438,
        "step": 7205
    },
    {
        "loss": 1.8156,
        "grad_norm": 2.101489782333374,
        "learning_rate": 0.00013378772178925322,
        "epoch": 0.9288476411446249,
        "step": 7206
    },
    {
        "loss": 2.0443,
        "grad_norm": 2.098416805267334,
        "learning_rate": 0.00013373045320247905,
        "epoch": 0.9289765403454499,
        "step": 7207
    },
    {
        "loss": 1.6643,
        "grad_norm": 1.600392460823059,
        "learning_rate": 0.0001336731721302172,
        "epoch": 0.9291054395462748,
        "step": 7208
    },
    {
        "loss": 1.2808,
        "grad_norm": 2.5998423099517822,
        "learning_rate": 0.00013361587859367062,
        "epoch": 0.9292343387470998,
        "step": 7209
    },
    {
        "loss": 1.3011,
        "grad_norm": 2.6004638671875,
        "learning_rate": 0.00013355857261404666,
        "epoch": 0.9293632379479247,
        "step": 7210
    },
    {
        "loss": 1.9383,
        "grad_norm": 2.4652323722839355,
        "learning_rate": 0.00013350125421255748,
        "epoch": 0.9294921371487497,
        "step": 7211
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.9582828283309937,
        "learning_rate": 0.00013344392341041965,
        "epoch": 0.9296210363495746,
        "step": 7212
    },
    {
        "loss": 1.9985,
        "grad_norm": 2.2986714839935303,
        "learning_rate": 0.00013338658022885467,
        "epoch": 0.9297499355503996,
        "step": 7213
    },
    {
        "loss": 1.8754,
        "grad_norm": 2.0143632888793945,
        "learning_rate": 0.00013332922468908812,
        "epoch": 0.9298788347512246,
        "step": 7214
    },
    {
        "loss": 1.9597,
        "grad_norm": 2.599447011947632,
        "learning_rate": 0.0001332718568123505,
        "epoch": 0.9300077339520495,
        "step": 7215
    },
    {
        "loss": 1.2095,
        "grad_norm": 1.412550449371338,
        "learning_rate": 0.00013321447661987702,
        "epoch": 0.9301366331528744,
        "step": 7216
    },
    {
        "loss": 2.3204,
        "grad_norm": 2.1111721992492676,
        "learning_rate": 0.00013315708413290693,
        "epoch": 0.9302655323536994,
        "step": 7217
    },
    {
        "loss": 1.8153,
        "grad_norm": 1.8095053434371948,
        "learning_rate": 0.0001330996793726845,
        "epoch": 0.9303944315545244,
        "step": 7218
    },
    {
        "loss": 1.5971,
        "grad_norm": 2.1329538822174072,
        "learning_rate": 0.00013304226236045835,
        "epoch": 0.9305233307553493,
        "step": 7219
    },
    {
        "loss": 1.5302,
        "grad_norm": 1.7114728689193726,
        "learning_rate": 0.00013298483311748166,
        "epoch": 0.9306522299561742,
        "step": 7220
    },
    {
        "loss": 1.7445,
        "grad_norm": 2.209714889526367,
        "learning_rate": 0.00013292739166501205,
        "epoch": 0.9307811291569992,
        "step": 7221
    },
    {
        "loss": 1.4813,
        "grad_norm": 2.267300605773926,
        "learning_rate": 0.00013286993802431188,
        "epoch": 0.9309100283578242,
        "step": 7222
    },
    {
        "loss": 1.5019,
        "grad_norm": 3.014502763748169,
        "learning_rate": 0.00013281247221664784,
        "epoch": 0.9310389275586491,
        "step": 7223
    },
    {
        "loss": 1.7387,
        "grad_norm": 1.8718513250350952,
        "learning_rate": 0.00013275499426329107,
        "epoch": 0.9311678267594741,
        "step": 7224
    },
    {
        "loss": 2.6416,
        "grad_norm": 2.8682212829589844,
        "learning_rate": 0.0001326975041855175,
        "epoch": 0.931296725960299,
        "step": 7225
    },
    {
        "loss": 1.6778,
        "grad_norm": 2.697192430496216,
        "learning_rate": 0.00013264000200460723,
        "epoch": 0.931425625161124,
        "step": 7226
    },
    {
        "loss": 1.1038,
        "grad_norm": 2.7794339656829834,
        "learning_rate": 0.000132582487741845,
        "epoch": 0.931554524361949,
        "step": 7227
    },
    {
        "loss": 2.2224,
        "grad_norm": 1.9863313436508179,
        "learning_rate": 0.00013252496141852,
        "epoch": 0.9316834235627739,
        "step": 7228
    },
    {
        "loss": 1.2642,
        "grad_norm": 3.1043858528137207,
        "learning_rate": 0.00013246742305592573,
        "epoch": 0.9318123227635988,
        "step": 7229
    },
    {
        "loss": 2.3742,
        "grad_norm": 2.0661540031433105,
        "learning_rate": 0.0001324098726753606,
        "epoch": 0.9319412219644239,
        "step": 7230
    },
    {
        "loss": 1.0164,
        "grad_norm": 1.950165033340454,
        "learning_rate": 0.00013235231029812682,
        "epoch": 0.9320701211652488,
        "step": 7231
    },
    {
        "loss": 1.3781,
        "grad_norm": 2.498819351196289,
        "learning_rate": 0.0001322947359455315,
        "epoch": 0.9321990203660737,
        "step": 7232
    },
    {
        "loss": 1.7869,
        "grad_norm": 2.546086549758911,
        "learning_rate": 0.00013223714963888624,
        "epoch": 0.9323279195668986,
        "step": 7233
    },
    {
        "loss": 0.9903,
        "grad_norm": 2.619906425476074,
        "learning_rate": 0.00013217955139950658,
        "epoch": 0.9324568187677237,
        "step": 7234
    },
    {
        "loss": 2.0094,
        "grad_norm": 2.742424249649048,
        "learning_rate": 0.000132121941248713,
        "epoch": 0.9325857179685486,
        "step": 7235
    },
    {
        "loss": 2.148,
        "grad_norm": 3.4255099296569824,
        "learning_rate": 0.00013206431920783013,
        "epoch": 0.9327146171693735,
        "step": 7236
    },
    {
        "loss": 2.4297,
        "grad_norm": 2.691279888153076,
        "learning_rate": 0.00013200668529818698,
        "epoch": 0.9328435163701985,
        "step": 7237
    },
    {
        "loss": 1.7863,
        "grad_norm": 2.27748441696167,
        "learning_rate": 0.000131949039541117,
        "epoch": 0.9329724155710235,
        "step": 7238
    },
    {
        "loss": 1.497,
        "grad_norm": 2.048200845718384,
        "learning_rate": 0.00013189138195795818,
        "epoch": 0.9331013147718484,
        "step": 7239
    },
    {
        "loss": 2.4802,
        "grad_norm": 1.5870267152786255,
        "learning_rate": 0.00013183371257005264,
        "epoch": 0.9332302139726734,
        "step": 7240
    },
    {
        "loss": 0.7338,
        "grad_norm": 2.2766358852386475,
        "learning_rate": 0.00013177603139874694,
        "epoch": 0.9333591131734983,
        "step": 7241
    },
    {
        "loss": 1.9521,
        "grad_norm": 2.0682199001312256,
        "learning_rate": 0.00013171833846539218,
        "epoch": 0.9334880123743233,
        "step": 7242
    },
    {
        "loss": 1.5636,
        "grad_norm": 1.618903398513794,
        "learning_rate": 0.00013166063379134357,
        "epoch": 0.9336169115751483,
        "step": 7243
    },
    {
        "loss": 1.5377,
        "grad_norm": 2.4620022773742676,
        "learning_rate": 0.0001316029173979608,
        "epoch": 0.9337458107759732,
        "step": 7244
    },
    {
        "loss": 1.2712,
        "grad_norm": 1.7721924781799316,
        "learning_rate": 0.00013154518930660785,
        "epoch": 0.9338747099767981,
        "step": 7245
    },
    {
        "loss": 1.5431,
        "grad_norm": 2.165597915649414,
        "learning_rate": 0.00013148744953865303,
        "epoch": 0.9340036091776232,
        "step": 7246
    },
    {
        "loss": 1.3845,
        "grad_norm": 2.6203973293304443,
        "learning_rate": 0.00013142969811546904,
        "epoch": 0.9341325083784481,
        "step": 7247
    },
    {
        "loss": 2.3401,
        "grad_norm": 1.6064631938934326,
        "learning_rate": 0.0001313719350584328,
        "epoch": 0.934261407579273,
        "step": 7248
    },
    {
        "loss": 2.1512,
        "grad_norm": 2.180891990661621,
        "learning_rate": 0.00013131416038892558,
        "epoch": 0.9343903067800979,
        "step": 7249
    },
    {
        "loss": 2.2144,
        "grad_norm": 2.45145583152771,
        "learning_rate": 0.0001312563741283331,
        "epoch": 0.934519205980923,
        "step": 7250
    },
    {
        "loss": 1.5559,
        "grad_norm": 2.4856514930725098,
        "learning_rate": 0.00013119857629804494,
        "epoch": 0.9346481051817479,
        "step": 7251
    },
    {
        "loss": 2.018,
        "grad_norm": 1.9491766691207886,
        "learning_rate": 0.00013114076691945548,
        "epoch": 0.9347770043825728,
        "step": 7252
    },
    {
        "loss": 1.9591,
        "grad_norm": 2.4616055488586426,
        "learning_rate": 0.00013108294601396305,
        "epoch": 0.9349059035833978,
        "step": 7253
    },
    {
        "loss": 1.8928,
        "grad_norm": 2.3001108169555664,
        "learning_rate": 0.00013102511360297032,
        "epoch": 0.9350348027842227,
        "step": 7254
    },
    {
        "loss": 1.2399,
        "grad_norm": 2.772040367126465,
        "learning_rate": 0.00013096726970788418,
        "epoch": 0.9351637019850477,
        "step": 7255
    },
    {
        "loss": 1.5759,
        "grad_norm": 1.7353969812393188,
        "learning_rate": 0.00013090941435011595,
        "epoch": 0.9352926011858727,
        "step": 7256
    },
    {
        "loss": 0.7095,
        "grad_norm": 1.8187953233718872,
        "learning_rate": 0.00013085154755108097,
        "epoch": 0.9354215003866976,
        "step": 7257
    },
    {
        "loss": 1.5127,
        "grad_norm": 2.5089409351348877,
        "learning_rate": 0.0001307936693321989,
        "epoch": 0.9355503995875225,
        "step": 7258
    },
    {
        "loss": 1.7077,
        "grad_norm": 1.7214529514312744,
        "learning_rate": 0.00013073577971489377,
        "epoch": 0.9356792987883475,
        "step": 7259
    },
    {
        "loss": 2.054,
        "grad_norm": 1.8199970722198486,
        "learning_rate": 0.00013067787872059357,
        "epoch": 0.9358081979891725,
        "step": 7260
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.4442517757415771,
        "learning_rate": 0.0001306199663707307,
        "epoch": 0.9359370971899974,
        "step": 7261
    },
    {
        "loss": 1.6174,
        "grad_norm": 2.4443118572235107,
        "learning_rate": 0.00013056204268674165,
        "epoch": 0.9360659963908223,
        "step": 7262
    },
    {
        "loss": 1.6498,
        "grad_norm": 3.434185266494751,
        "learning_rate": 0.00013050410769006708,
        "epoch": 0.9361948955916474,
        "step": 7263
    },
    {
        "loss": 1.4224,
        "grad_norm": 1.7499185800552368,
        "learning_rate": 0.00013044616140215205,
        "epoch": 0.9363237947924723,
        "step": 7264
    },
    {
        "loss": 2.0644,
        "grad_norm": 2.980297803878784,
        "learning_rate": 0.00013038820384444555,
        "epoch": 0.9364526939932972,
        "step": 7265
    },
    {
        "loss": 2.1108,
        "grad_norm": 1.9563897848129272,
        "learning_rate": 0.00013033023503840086,
        "epoch": 0.9365815931941222,
        "step": 7266
    },
    {
        "loss": 2.4969,
        "grad_norm": 1.3373773097991943,
        "learning_rate": 0.00013027225500547546,
        "epoch": 0.9367104923949472,
        "step": 7267
    },
    {
        "loss": 2.8547,
        "grad_norm": 2.287569284439087,
        "learning_rate": 0.00013021426376713094,
        "epoch": 0.9368393915957721,
        "step": 7268
    },
    {
        "loss": 2.404,
        "grad_norm": 1.8827979564666748,
        "learning_rate": 0.00013015626134483296,
        "epoch": 0.936968290796597,
        "step": 7269
    },
    {
        "loss": 1.9519,
        "grad_norm": 2.62213397026062,
        "learning_rate": 0.00013009824776005145,
        "epoch": 0.937097189997422,
        "step": 7270
    },
    {
        "loss": 1.886,
        "grad_norm": 1.5917973518371582,
        "learning_rate": 0.00013004022303426038,
        "epoch": 0.937226089198247,
        "step": 7271
    },
    {
        "loss": 1.4996,
        "grad_norm": 2.0907392501831055,
        "learning_rate": 0.00012998218718893786,
        "epoch": 0.9373549883990719,
        "step": 7272
    },
    {
        "loss": 2.4106,
        "grad_norm": 2.05521559715271,
        "learning_rate": 0.00012992414024556615,
        "epoch": 0.9374838875998969,
        "step": 7273
    },
    {
        "loss": 1.46,
        "grad_norm": 1.9396684169769287,
        "learning_rate": 0.00012986608222563168,
        "epoch": 0.9376127868007218,
        "step": 7274
    },
    {
        "loss": 1.8853,
        "grad_norm": 2.9522624015808105,
        "learning_rate": 0.0001298080131506248,
        "epoch": 0.9377416860015468,
        "step": 7275
    },
    {
        "loss": 2.8096,
        "grad_norm": 1.8307543992996216,
        "learning_rate": 0.00012974993304204008,
        "epoch": 0.9378705852023718,
        "step": 7276
    },
    {
        "loss": 1.3302,
        "grad_norm": 2.9909534454345703,
        "learning_rate": 0.00012969184192137623,
        "epoch": 0.9379994844031967,
        "step": 7277
    },
    {
        "loss": 1.5843,
        "grad_norm": 1.656044840812683,
        "learning_rate": 0.00012963373981013585,
        "epoch": 0.9381283836040216,
        "step": 7278
    },
    {
        "loss": 1.1902,
        "grad_norm": 2.199462890625,
        "learning_rate": 0.00012957562672982572,
        "epoch": 0.9382572828048467,
        "step": 7279
    },
    {
        "loss": 1.9515,
        "grad_norm": 3.0249533653259277,
        "learning_rate": 0.0001295175027019567,
        "epoch": 0.9383861820056716,
        "step": 7280
    },
    {
        "loss": 2.2685,
        "grad_norm": 1.318885087966919,
        "learning_rate": 0.0001294593677480437,
        "epoch": 0.9385150812064965,
        "step": 7281
    },
    {
        "loss": 0.75,
        "grad_norm": 2.336622476577759,
        "learning_rate": 0.00012940122188960566,
        "epoch": 0.9386439804073214,
        "step": 7282
    },
    {
        "loss": 1.3574,
        "grad_norm": 1.9555519819259644,
        "learning_rate": 0.00012934306514816542,
        "epoch": 0.9387728796081465,
        "step": 7283
    },
    {
        "loss": 1.324,
        "grad_norm": 3.208918333053589,
        "learning_rate": 0.00012928489754525016,
        "epoch": 0.9389017788089714,
        "step": 7284
    },
    {
        "loss": 1.8263,
        "grad_norm": 2.7260632514953613,
        "learning_rate": 0.00012922671910239078,
        "epoch": 0.9390306780097963,
        "step": 7285
    },
    {
        "loss": 2.2921,
        "grad_norm": 1.4090083837509155,
        "learning_rate": 0.00012916852984112235,
        "epoch": 0.9391595772106213,
        "step": 7286
    },
    {
        "loss": 1.9709,
        "grad_norm": 1.8276255130767822,
        "learning_rate": 0.0001291103297829838,
        "epoch": 0.9392884764114462,
        "step": 7287
    },
    {
        "loss": 1.7649,
        "grad_norm": 1.5673688650131226,
        "learning_rate": 0.00012905211894951836,
        "epoch": 0.9394173756122712,
        "step": 7288
    },
    {
        "loss": 2.5911,
        "grad_norm": 1.4135892391204834,
        "learning_rate": 0.00012899389736227283,
        "epoch": 0.9395462748130962,
        "step": 7289
    },
    {
        "loss": 2.3808,
        "grad_norm": 1.4167433977127075,
        "learning_rate": 0.00012893566504279828,
        "epoch": 0.9396751740139211,
        "step": 7290
    },
    {
        "loss": 1.9656,
        "grad_norm": 1.4223034381866455,
        "learning_rate": 0.00012887742201264982,
        "epoch": 0.939804073214746,
        "step": 7291
    },
    {
        "loss": 2.011,
        "grad_norm": 1.4909887313842773,
        "learning_rate": 0.00012881916829338616,
        "epoch": 0.939932972415571,
        "step": 7292
    },
    {
        "loss": 1.0198,
        "grad_norm": 2.635317325592041,
        "learning_rate": 0.0001287609039065704,
        "epoch": 0.940061871616396,
        "step": 7293
    },
    {
        "loss": 2.3516,
        "grad_norm": 1.739296555519104,
        "learning_rate": 0.00012870262887376924,
        "epoch": 0.9401907708172209,
        "step": 7294
    },
    {
        "loss": 1.1139,
        "grad_norm": 2.6243021488189697,
        "learning_rate": 0.00012864434321655348,
        "epoch": 0.9403196700180458,
        "step": 7295
    },
    {
        "loss": 2.0468,
        "grad_norm": 1.6001648902893066,
        "learning_rate": 0.00012858604695649789,
        "epoch": 0.9404485692188709,
        "step": 7296
    },
    {
        "loss": 2.1649,
        "grad_norm": 2.087636709213257,
        "learning_rate": 0.00012852774011518097,
        "epoch": 0.9405774684196958,
        "step": 7297
    },
    {
        "loss": 1.9687,
        "grad_norm": 2.4531090259552,
        "learning_rate": 0.00012846942271418547,
        "epoch": 0.9407063676205207,
        "step": 7298
    },
    {
        "loss": 1.8983,
        "grad_norm": 1.4220598936080933,
        "learning_rate": 0.00012841109477509775,
        "epoch": 0.9408352668213457,
        "step": 7299
    },
    {
        "loss": 2.4471,
        "grad_norm": 1.7195597887039185,
        "learning_rate": 0.00012835275631950811,
        "epoch": 0.9409641660221707,
        "step": 7300
    },
    {
        "loss": 1.759,
        "grad_norm": 1.879830002784729,
        "learning_rate": 0.00012829440736901096,
        "epoch": 0.9410930652229956,
        "step": 7301
    },
    {
        "loss": 2.2005,
        "grad_norm": 1.537838101387024,
        "learning_rate": 0.0001282360479452044,
        "epoch": 0.9412219644238206,
        "step": 7302
    },
    {
        "loss": 1.6419,
        "grad_norm": 2.0926215648651123,
        "learning_rate": 0.00012817767806969034,
        "epoch": 0.9413508636246455,
        "step": 7303
    },
    {
        "loss": 1.9548,
        "grad_norm": 1.780490756034851,
        "learning_rate": 0.0001281192977640747,
        "epoch": 0.9414797628254705,
        "step": 7304
    },
    {
        "loss": 1.481,
        "grad_norm": 1.9707077741622925,
        "learning_rate": 0.0001280609070499674,
        "epoch": 0.9416086620262955,
        "step": 7305
    },
    {
        "loss": 2.4243,
        "grad_norm": 1.4772734642028809,
        "learning_rate": 0.00012800250594898177,
        "epoch": 0.9417375612271204,
        "step": 7306
    },
    {
        "loss": 1.2873,
        "grad_norm": 1.7155004739761353,
        "learning_rate": 0.00012794409448273542,
        "epoch": 0.9418664604279453,
        "step": 7307
    },
    {
        "loss": 1.6471,
        "grad_norm": 2.0767440795898438,
        "learning_rate": 0.00012788567267284969,
        "epoch": 0.9419953596287703,
        "step": 7308
    },
    {
        "loss": 2.1399,
        "grad_norm": 1.879607081413269,
        "learning_rate": 0.00012782724054094947,
        "epoch": 0.9421242588295953,
        "step": 7309
    },
    {
        "loss": 1.7621,
        "grad_norm": 2.620797634124756,
        "learning_rate": 0.00012776879810866393,
        "epoch": 0.9422531580304202,
        "step": 7310
    },
    {
        "loss": 1.7295,
        "grad_norm": 2.6864001750946045,
        "learning_rate": 0.00012771034539762568,
        "epoch": 0.9423820572312451,
        "step": 7311
    },
    {
        "loss": 1.6711,
        "grad_norm": 1.6821178197860718,
        "learning_rate": 0.0001276518824294713,
        "epoch": 0.9425109564320702,
        "step": 7312
    },
    {
        "loss": 1.7699,
        "grad_norm": 2.1245906352996826,
        "learning_rate": 0.00012759340922584116,
        "epoch": 0.9426398556328951,
        "step": 7313
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.5131521224975586,
        "learning_rate": 0.00012753492580837926,
        "epoch": 0.94276875483372,
        "step": 7314
    },
    {
        "loss": 1.0188,
        "grad_norm": 1.399369478225708,
        "learning_rate": 0.0001274764321987337,
        "epoch": 0.942897654034545,
        "step": 7315
    },
    {
        "loss": 2.4217,
        "grad_norm": 1.8919757604599,
        "learning_rate": 0.00012741792841855612,
        "epoch": 0.94302655323537,
        "step": 7316
    },
    {
        "loss": 1.915,
        "grad_norm": 1.7774062156677246,
        "learning_rate": 0.00012735941448950187,
        "epoch": 0.9431554524361949,
        "step": 7317
    },
    {
        "loss": 1.195,
        "grad_norm": 1.814347267150879,
        "learning_rate": 0.00012730089043323027,
        "epoch": 0.9432843516370198,
        "step": 7318
    },
    {
        "loss": 1.3776,
        "grad_norm": 2.03987193107605,
        "learning_rate": 0.0001272423562714043,
        "epoch": 0.9434132508378448,
        "step": 7319
    },
    {
        "loss": 1.3666,
        "grad_norm": 2.6834299564361572,
        "learning_rate": 0.0001271838120256905,
        "epoch": 0.9435421500386698,
        "step": 7320
    },
    {
        "loss": 1.956,
        "grad_norm": 1.73870849609375,
        "learning_rate": 0.00012712525771775942,
        "epoch": 0.9436710492394947,
        "step": 7321
    },
    {
        "loss": 2.0259,
        "grad_norm": 1.8405823707580566,
        "learning_rate": 0.0001270666933692853,
        "epoch": 0.9437999484403197,
        "step": 7322
    },
    {
        "loss": 1.7259,
        "grad_norm": 2.3356823921203613,
        "learning_rate": 0.0001270081190019458,
        "epoch": 0.9439288476411446,
        "step": 7323
    },
    {
        "loss": 1.377,
        "grad_norm": 1.960175633430481,
        "learning_rate": 0.00012694953463742256,
        "epoch": 0.9440577468419695,
        "step": 7324
    },
    {
        "loss": 1.6884,
        "grad_norm": 2.5971786975860596,
        "learning_rate": 0.00012689094029740109,
        "epoch": 0.9441866460427946,
        "step": 7325
    },
    {
        "loss": 1.7085,
        "grad_norm": 2.05912184715271,
        "learning_rate": 0.00012683233600357,
        "epoch": 0.9443155452436195,
        "step": 7326
    },
    {
        "loss": 1.6705,
        "grad_norm": 2.54314923286438,
        "learning_rate": 0.00012677372177762224,
        "epoch": 0.9444444444444444,
        "step": 7327
    },
    {
        "loss": 1.9563,
        "grad_norm": 2.7856760025024414,
        "learning_rate": 0.000126715097641254,
        "epoch": 0.9445733436452693,
        "step": 7328
    },
    {
        "loss": 1.7061,
        "grad_norm": 2.027906894683838,
        "learning_rate": 0.00012665646361616534,
        "epoch": 0.9447022428460944,
        "step": 7329
    },
    {
        "loss": 0.9725,
        "grad_norm": 1.2910025119781494,
        "learning_rate": 0.00012659781972405984,
        "epoch": 0.9448311420469193,
        "step": 7330
    },
    {
        "loss": 2.4515,
        "grad_norm": 1.562268614768982,
        "learning_rate": 0.00012653916598664492,
        "epoch": 0.9449600412477442,
        "step": 7331
    },
    {
        "loss": 2.019,
        "grad_norm": 1.2089543342590332,
        "learning_rate": 0.00012648050242563153,
        "epoch": 0.9450889404485692,
        "step": 7332
    },
    {
        "loss": 1.5648,
        "grad_norm": 1.9336642026901245,
        "learning_rate": 0.0001264218290627342,
        "epoch": 0.9452178396493942,
        "step": 7333
    },
    {
        "loss": 2.0162,
        "grad_norm": 2.024951696395874,
        "learning_rate": 0.00012636314591967114,
        "epoch": 0.9453467388502191,
        "step": 7334
    },
    {
        "loss": 1.4568,
        "grad_norm": 2.2338783740997314,
        "learning_rate": 0.00012630445301816428,
        "epoch": 0.9454756380510441,
        "step": 7335
    },
    {
        "loss": 1.919,
        "grad_norm": 2.2873218059539795,
        "learning_rate": 0.00012624575037993907,
        "epoch": 0.945604537251869,
        "step": 7336
    },
    {
        "loss": 0.9935,
        "grad_norm": 1.9249933958053589,
        "learning_rate": 0.00012618703802672453,
        "epoch": 0.945733436452694,
        "step": 7337
    },
    {
        "loss": 2.1165,
        "grad_norm": 1.152252197265625,
        "learning_rate": 0.00012612831598025328,
        "epoch": 0.945862335653519,
        "step": 7338
    },
    {
        "loss": 2.098,
        "grad_norm": 2.336487054824829,
        "learning_rate": 0.00012606958426226173,
        "epoch": 0.9459912348543439,
        "step": 7339
    },
    {
        "loss": 1.3255,
        "grad_norm": 2.3487465381622314,
        "learning_rate": 0.00012601084289448944,
        "epoch": 0.9461201340551688,
        "step": 7340
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.8481608629226685,
        "learning_rate": 0.00012595209189867997,
        "epoch": 0.9462490332559939,
        "step": 7341
    },
    {
        "loss": 2.4431,
        "grad_norm": 1.649013638496399,
        "learning_rate": 0.00012589333129658036,
        "epoch": 0.9463779324568188,
        "step": 7342
    },
    {
        "loss": 2.0827,
        "grad_norm": 1.4941028356552124,
        "learning_rate": 0.00012583456110994092,
        "epoch": 0.9465068316576437,
        "step": 7343
    },
    {
        "loss": 2.4681,
        "grad_norm": 1.575881838798523,
        "learning_rate": 0.0001257757813605159,
        "epoch": 0.9466357308584686,
        "step": 7344
    },
    {
        "loss": 1.8844,
        "grad_norm": 1.2478135824203491,
        "learning_rate": 0.0001257169920700628,
        "epoch": 0.9467646300592937,
        "step": 7345
    },
    {
        "loss": 1.5147,
        "grad_norm": 2.63122820854187,
        "learning_rate": 0.0001256581932603428,
        "epoch": 0.9468935292601186,
        "step": 7346
    },
    {
        "loss": 1.5186,
        "grad_norm": 2.4813923835754395,
        "learning_rate": 0.00012559938495312046,
        "epoch": 0.9470224284609435,
        "step": 7347
    },
    {
        "loss": 2.1133,
        "grad_norm": 2.0022339820861816,
        "learning_rate": 0.00012554056717016413,
        "epoch": 0.9471513276617685,
        "step": 7348
    },
    {
        "loss": 1.7342,
        "grad_norm": 1.9901245832443237,
        "learning_rate": 0.00012548173993324539,
        "epoch": 0.9472802268625935,
        "step": 7349
    },
    {
        "loss": 1.9329,
        "grad_norm": 1.4094197750091553,
        "learning_rate": 0.00012542290326413935,
        "epoch": 0.9474091260634184,
        "step": 7350
    },
    {
        "loss": 1.9479,
        "grad_norm": 2.0228664875030518,
        "learning_rate": 0.00012536405718462486,
        "epoch": 0.9475380252642434,
        "step": 7351
    },
    {
        "loss": 1.2236,
        "grad_norm": 1.6909018754959106,
        "learning_rate": 0.00012530520171648394,
        "epoch": 0.9476669244650683,
        "step": 7352
    },
    {
        "loss": 1.6751,
        "grad_norm": 2.246299982070923,
        "learning_rate": 0.00012524633688150227,
        "epoch": 0.9477958236658933,
        "step": 7353
    },
    {
        "loss": 1.723,
        "grad_norm": 2.519010305404663,
        "learning_rate": 0.00012518746270146896,
        "epoch": 0.9479247228667183,
        "step": 7354
    },
    {
        "loss": 1.6413,
        "grad_norm": 1.3495419025421143,
        "learning_rate": 0.00012512857919817647,
        "epoch": 0.9480536220675432,
        "step": 7355
    },
    {
        "loss": 1.7726,
        "grad_norm": 2.499871253967285,
        "learning_rate": 0.00012506968639342097,
        "epoch": 0.9481825212683681,
        "step": 7356
    },
    {
        "loss": 2.081,
        "grad_norm": 1.4588737487792969,
        "learning_rate": 0.00012501078430900175,
        "epoch": 0.9483114204691931,
        "step": 7357
    },
    {
        "loss": 0.9177,
        "grad_norm": 2.443812131881714,
        "learning_rate": 0.00012495187296672174,
        "epoch": 0.9484403196700181,
        "step": 7358
    },
    {
        "loss": 1.3563,
        "grad_norm": 2.013399839401245,
        "learning_rate": 0.00012489295238838746,
        "epoch": 0.948569218870843,
        "step": 7359
    },
    {
        "loss": 1.8338,
        "grad_norm": 1.8773554563522339,
        "learning_rate": 0.00012483402259580833,
        "epoch": 0.9486981180716679,
        "step": 7360
    },
    {
        "loss": 0.6697,
        "grad_norm": 1.5781731605529785,
        "learning_rate": 0.00012477508361079766,
        "epoch": 0.9488270172724929,
        "step": 7361
    },
    {
        "loss": 1.6332,
        "grad_norm": 2.389024496078491,
        "learning_rate": 0.000124716135455172,
        "epoch": 0.9489559164733179,
        "step": 7362
    },
    {
        "loss": 2.1914,
        "grad_norm": 2.739591121673584,
        "learning_rate": 0.00012465717815075125,
        "epoch": 0.9490848156741428,
        "step": 7363
    },
    {
        "loss": 2.3844,
        "grad_norm": 1.565078616142273,
        "learning_rate": 0.00012459821171935867,
        "epoch": 0.9492137148749678,
        "step": 7364
    },
    {
        "loss": 1.9636,
        "grad_norm": 1.8674410581588745,
        "learning_rate": 0.0001245392361828211,
        "epoch": 0.9493426140757927,
        "step": 7365
    },
    {
        "loss": 1.8637,
        "grad_norm": 1.5457473993301392,
        "learning_rate": 0.0001244802515629686,
        "epoch": 0.9494715132766177,
        "step": 7366
    },
    {
        "loss": 2.441,
        "grad_norm": 1.2463107109069824,
        "learning_rate": 0.00012442125788163444,
        "epoch": 0.9496004124774426,
        "step": 7367
    },
    {
        "loss": 1.1696,
        "grad_norm": 2.647529363632202,
        "learning_rate": 0.00012436225516065564,
        "epoch": 0.9497293116782676,
        "step": 7368
    },
    {
        "loss": 1.4194,
        "grad_norm": 1.3080781698226929,
        "learning_rate": 0.00012430324342187218,
        "epoch": 0.9498582108790925,
        "step": 7369
    },
    {
        "loss": 1.8086,
        "grad_norm": 2.2622995376586914,
        "learning_rate": 0.00012424422268712766,
        "epoch": 0.9499871100799175,
        "step": 7370
    },
    {
        "loss": 2.0549,
        "grad_norm": 2.9268317222595215,
        "learning_rate": 0.00012418519297826877,
        "epoch": 0.9501160092807425,
        "step": 7371
    },
    {
        "loss": 1.913,
        "grad_norm": 3.205841541290283,
        "learning_rate": 0.0001241261543171456,
        "epoch": 0.9502449084815674,
        "step": 7372
    },
    {
        "loss": 1.269,
        "grad_norm": 1.430877923965454,
        "learning_rate": 0.00012406710672561178,
        "epoch": 0.9503738076823923,
        "step": 7373
    },
    {
        "loss": 1.2266,
        "grad_norm": 2.162151575088501,
        "learning_rate": 0.00012400805022552393,
        "epoch": 0.9505027068832174,
        "step": 7374
    },
    {
        "loss": 1.7042,
        "grad_norm": 2.423362970352173,
        "learning_rate": 0.00012394898483874205,
        "epoch": 0.9506316060840423,
        "step": 7375
    },
    {
        "loss": 1.6096,
        "grad_norm": 2.514627695083618,
        "learning_rate": 0.00012388991058712967,
        "epoch": 0.9507605052848672,
        "step": 7376
    },
    {
        "loss": 2.5062,
        "grad_norm": 1.5247492790222168,
        "learning_rate": 0.00012383082749255312,
        "epoch": 0.9508894044856921,
        "step": 7377
    },
    {
        "loss": 1.332,
        "grad_norm": 2.769378662109375,
        "learning_rate": 0.00012377173557688253,
        "epoch": 0.9510183036865172,
        "step": 7378
    },
    {
        "loss": 1.5839,
        "grad_norm": 1.297675609588623,
        "learning_rate": 0.00012371263486199096,
        "epoch": 0.9511472028873421,
        "step": 7379
    },
    {
        "loss": 1.6609,
        "grad_norm": 2.677305221557617,
        "learning_rate": 0.00012365352536975482,
        "epoch": 0.951276102088167,
        "step": 7380
    },
    {
        "loss": 2.1789,
        "grad_norm": 1.6577622890472412,
        "learning_rate": 0.0001235944071220537,
        "epoch": 0.951405001288992,
        "step": 7381
    },
    {
        "loss": 1.4977,
        "grad_norm": 3.120018720626831,
        "learning_rate": 0.00012353528014077066,
        "epoch": 0.951533900489817,
        "step": 7382
    },
    {
        "loss": 2.2326,
        "grad_norm": 2.2428576946258545,
        "learning_rate": 0.00012347614444779172,
        "epoch": 0.9516627996906419,
        "step": 7383
    },
    {
        "loss": 1.5616,
        "grad_norm": 2.7456350326538086,
        "learning_rate": 0.00012341700006500623,
        "epoch": 0.9517916988914669,
        "step": 7384
    },
    {
        "loss": 1.9223,
        "grad_norm": 1.853533387184143,
        "learning_rate": 0.00012335784701430686,
        "epoch": 0.9519205980922918,
        "step": 7385
    },
    {
        "loss": 0.785,
        "grad_norm": 2.7204813957214355,
        "learning_rate": 0.00012329868531758934,
        "epoch": 0.9520494972931168,
        "step": 7386
    },
    {
        "loss": 1.9805,
        "grad_norm": 2.302358388900757,
        "learning_rate": 0.00012323951499675263,
        "epoch": 0.9521783964939418,
        "step": 7387
    },
    {
        "loss": 2.1431,
        "grad_norm": 1.168057918548584,
        "learning_rate": 0.00012318033607369895,
        "epoch": 0.9523072956947667,
        "step": 7388
    },
    {
        "loss": 2.234,
        "grad_norm": 2.0651140213012695,
        "learning_rate": 0.00012312114857033351,
        "epoch": 0.9524361948955916,
        "step": 7389
    },
    {
        "loss": 1.9216,
        "grad_norm": 2.0692923069000244,
        "learning_rate": 0.0001230619525085651,
        "epoch": 0.9525650940964167,
        "step": 7390
    },
    {
        "loss": 1.2862,
        "grad_norm": 3.23486590385437,
        "learning_rate": 0.00012300274791030526,
        "epoch": 0.9526939932972416,
        "step": 7391
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.2159922122955322,
        "learning_rate": 0.00012294353479746882,
        "epoch": 0.9528228924980665,
        "step": 7392
    },
    {
        "loss": 1.4617,
        "grad_norm": 2.6385722160339355,
        "learning_rate": 0.00012288431319197395,
        "epoch": 0.9529517916988914,
        "step": 7393
    },
    {
        "loss": 1.3284,
        "grad_norm": 1.577153205871582,
        "learning_rate": 0.00012282508311574168,
        "epoch": 0.9530806908997165,
        "step": 7394
    },
    {
        "loss": 1.6666,
        "grad_norm": 2.648153305053711,
        "learning_rate": 0.00012276584459069637,
        "epoch": 0.9532095901005414,
        "step": 7395
    },
    {
        "loss": 1.0953,
        "grad_norm": 2.6373324394226074,
        "learning_rate": 0.00012270659763876545,
        "epoch": 0.9533384893013663,
        "step": 7396
    },
    {
        "loss": 2.1185,
        "grad_norm": 1.952515959739685,
        "learning_rate": 0.0001226473422818794,
        "epoch": 0.9534673885021913,
        "step": 7397
    },
    {
        "loss": 2.26,
        "grad_norm": 1.8322954177856445,
        "learning_rate": 0.00012258807854197178,
        "epoch": 0.9535962877030162,
        "step": 7398
    },
    {
        "loss": 1.6344,
        "grad_norm": 3.204491138458252,
        "learning_rate": 0.0001225288064409796,
        "epoch": 0.9537251869038412,
        "step": 7399
    },
    {
        "loss": 1.8457,
        "grad_norm": 2.0895800590515137,
        "learning_rate": 0.00012246952600084254,
        "epoch": 0.9538540861046662,
        "step": 7400
    },
    {
        "loss": 1.8444,
        "grad_norm": 1.7545543909072876,
        "learning_rate": 0.00012241023724350353,
        "epoch": 0.9539829853054911,
        "step": 7401
    },
    {
        "loss": 1.6014,
        "grad_norm": 2.332756996154785,
        "learning_rate": 0.00012235094019090868,
        "epoch": 0.954111884506316,
        "step": 7402
    },
    {
        "loss": 2.1069,
        "grad_norm": 1.795655608177185,
        "learning_rate": 0.000122291634865007,
        "epoch": 0.954240783707141,
        "step": 7403
    },
    {
        "loss": 1.5378,
        "grad_norm": 1.877022385597229,
        "learning_rate": 0.0001222323212877507,
        "epoch": 0.954369682907966,
        "step": 7404
    },
    {
        "loss": 1.919,
        "grad_norm": 1.491188406944275,
        "learning_rate": 0.00012217299948109493,
        "epoch": 0.9544985821087909,
        "step": 7405
    },
    {
        "loss": 1.8727,
        "grad_norm": 1.8195396661758423,
        "learning_rate": 0.0001221136694669979,
        "epoch": 0.9546274813096158,
        "step": 7406
    },
    {
        "loss": 2.2078,
        "grad_norm": 1.8940117359161377,
        "learning_rate": 0.00012205433126742101,
        "epoch": 0.9547563805104409,
        "step": 7407
    },
    {
        "loss": 1.8235,
        "grad_norm": 2.6409971714019775,
        "learning_rate": 0.00012199498490432847,
        "epoch": 0.9548852797112658,
        "step": 7408
    },
    {
        "loss": 1.4644,
        "grad_norm": 2.9948575496673584,
        "learning_rate": 0.00012193563039968767,
        "epoch": 0.9550141789120907,
        "step": 7409
    },
    {
        "loss": 2.3103,
        "grad_norm": 2.4423277378082275,
        "learning_rate": 0.000121876267775469,
        "epoch": 0.9551430781129157,
        "step": 7410
    },
    {
        "loss": 1.5383,
        "grad_norm": 2.522820472717285,
        "learning_rate": 0.00012181689705364576,
        "epoch": 0.9552719773137407,
        "step": 7411
    },
    {
        "loss": 1.8976,
        "grad_norm": 2.580397129058838,
        "learning_rate": 0.00012175751825619433,
        "epoch": 0.9554008765145656,
        "step": 7412
    },
    {
        "loss": 2.4103,
        "grad_norm": 1.463331937789917,
        "learning_rate": 0.00012169813140509396,
        "epoch": 0.9555297757153906,
        "step": 7413
    },
    {
        "loss": 2.0165,
        "grad_norm": 2.4425008296966553,
        "learning_rate": 0.00012163873652232722,
        "epoch": 0.9556586749162155,
        "step": 7414
    },
    {
        "loss": 2.6307,
        "grad_norm": 1.725080966949463,
        "learning_rate": 0.00012157933362987912,
        "epoch": 0.9557875741170405,
        "step": 7415
    },
    {
        "loss": 1.6672,
        "grad_norm": 2.262378215789795,
        "learning_rate": 0.00012151992274973803,
        "epoch": 0.9559164733178654,
        "step": 7416
    },
    {
        "loss": 1.8735,
        "grad_norm": 1.4011626243591309,
        "learning_rate": 0.00012146050390389534,
        "epoch": 0.9560453725186904,
        "step": 7417
    },
    {
        "loss": 2.4711,
        "grad_norm": 1.3711720705032349,
        "learning_rate": 0.00012140107711434492,
        "epoch": 0.9561742717195153,
        "step": 7418
    },
    {
        "loss": 1.8274,
        "grad_norm": 2.908756971359253,
        "learning_rate": 0.0001213416424030841,
        "epoch": 0.9563031709203403,
        "step": 7419
    },
    {
        "loss": 2.2006,
        "grad_norm": 2.608630895614624,
        "learning_rate": 0.00012128219979211286,
        "epoch": 0.9564320701211653,
        "step": 7420
    },
    {
        "loss": 1.8264,
        "grad_norm": 2.0655524730682373,
        "learning_rate": 0.00012122274930343416,
        "epoch": 0.9565609693219902,
        "step": 7421
    },
    {
        "loss": 1.6023,
        "grad_norm": 2.3980820178985596,
        "learning_rate": 0.00012116329095905384,
        "epoch": 0.9566898685228151,
        "step": 7422
    },
    {
        "loss": 1.5366,
        "grad_norm": 1.4188029766082764,
        "learning_rate": 0.0001211038247809807,
        "epoch": 0.9568187677236402,
        "step": 7423
    },
    {
        "loss": 1.7712,
        "grad_norm": 2.6653189659118652,
        "learning_rate": 0.00012104435079122649,
        "epoch": 0.9569476669244651,
        "step": 7424
    },
    {
        "loss": 1.7126,
        "grad_norm": 1.8396236896514893,
        "learning_rate": 0.00012098486901180577,
        "epoch": 0.95707656612529,
        "step": 7425
    },
    {
        "loss": 2.0938,
        "grad_norm": 2.2010889053344727,
        "learning_rate": 0.00012092537946473592,
        "epoch": 0.957205465326115,
        "step": 7426
    },
    {
        "loss": 1.7964,
        "grad_norm": 2.395911455154419,
        "learning_rate": 0.00012086588217203738,
        "epoch": 0.95733436452694,
        "step": 7427
    },
    {
        "loss": 1.263,
        "grad_norm": 2.7247912883758545,
        "learning_rate": 0.00012080637715573337,
        "epoch": 0.9574632637277649,
        "step": 7428
    },
    {
        "loss": 1.7981,
        "grad_norm": 1.5676828622817993,
        "learning_rate": 0.00012074686443784989,
        "epoch": 0.9575921629285898,
        "step": 7429
    },
    {
        "loss": 1.8528,
        "grad_norm": 2.250586986541748,
        "learning_rate": 0.00012068734404041578,
        "epoch": 0.9577210621294148,
        "step": 7430
    },
    {
        "loss": 1.8376,
        "grad_norm": 1.8338719606399536,
        "learning_rate": 0.00012062781598546305,
        "epoch": 0.9578499613302398,
        "step": 7431
    },
    {
        "loss": 1.8908,
        "grad_norm": 1.4255027770996094,
        "learning_rate": 0.000120568280295026,
        "epoch": 0.9579788605310647,
        "step": 7432
    },
    {
        "loss": 1.7565,
        "grad_norm": 1.8026080131530762,
        "learning_rate": 0.00012050873699114215,
        "epoch": 0.9581077597318897,
        "step": 7433
    },
    {
        "loss": 1.8881,
        "grad_norm": 2.3135743141174316,
        "learning_rate": 0.00012044918609585194,
        "epoch": 0.9582366589327146,
        "step": 7434
    },
    {
        "loss": 1.8783,
        "grad_norm": 1.9164788722991943,
        "learning_rate": 0.0001203896276311981,
        "epoch": 0.9583655581335395,
        "step": 7435
    },
    {
        "loss": 2.1754,
        "grad_norm": 1.9084588289260864,
        "learning_rate": 0.00012033006161922668,
        "epoch": 0.9584944573343646,
        "step": 7436
    },
    {
        "loss": 1.9975,
        "grad_norm": 1.7359031438827515,
        "learning_rate": 0.00012027048808198628,
        "epoch": 0.9586233565351895,
        "step": 7437
    },
    {
        "loss": 1.6893,
        "grad_norm": 1.703967571258545,
        "learning_rate": 0.00012021090704152831,
        "epoch": 0.9587522557360144,
        "step": 7438
    },
    {
        "loss": 2.2762,
        "grad_norm": 1.889270544052124,
        "learning_rate": 0.00012015131851990699,
        "epoch": 0.9588811549368393,
        "step": 7439
    },
    {
        "loss": 1.9732,
        "grad_norm": 3.3537983894348145,
        "learning_rate": 0.00012009172253917922,
        "epoch": 0.9590100541376644,
        "step": 7440
    },
    {
        "loss": 1.9469,
        "grad_norm": 1.8625446557998657,
        "learning_rate": 0.00012003211912140484,
        "epoch": 0.9591389533384893,
        "step": 7441
    },
    {
        "loss": 1.7497,
        "grad_norm": 2.3732259273529053,
        "learning_rate": 0.00011997250828864634,
        "epoch": 0.9592678525393142,
        "step": 7442
    },
    {
        "loss": 1.77,
        "grad_norm": 2.80735445022583,
        "learning_rate": 0.00011991289006296884,
        "epoch": 0.9593967517401392,
        "step": 7443
    },
    {
        "loss": 2.198,
        "grad_norm": 1.3451310396194458,
        "learning_rate": 0.00011985326446644043,
        "epoch": 0.9595256509409642,
        "step": 7444
    },
    {
        "loss": 1.4013,
        "grad_norm": 1.7825813293457031,
        "learning_rate": 0.00011979363152113177,
        "epoch": 0.9596545501417891,
        "step": 7445
    },
    {
        "loss": 1.6613,
        "grad_norm": 2.449951171875,
        "learning_rate": 0.00011973399124911626,
        "epoch": 0.9597834493426141,
        "step": 7446
    },
    {
        "loss": 1.4966,
        "grad_norm": 2.7901546955108643,
        "learning_rate": 0.00011967434367246996,
        "epoch": 0.959912348543439,
        "step": 7447
    },
    {
        "loss": 2.4214,
        "grad_norm": 1.6456360816955566,
        "learning_rate": 0.00011961468881327196,
        "epoch": 0.960041247744264,
        "step": 7448
    },
    {
        "loss": 2.0189,
        "grad_norm": 2.074626922607422,
        "learning_rate": 0.00011955502669360344,
        "epoch": 0.960170146945089,
        "step": 7449
    },
    {
        "loss": 2.0818,
        "grad_norm": 2.0250325202941895,
        "learning_rate": 0.00011949535733554881,
        "epoch": 0.9602990461459139,
        "step": 7450
    },
    {
        "loss": 2.0032,
        "grad_norm": 1.8785263299942017,
        "learning_rate": 0.0001194356807611951,
        "epoch": 0.9604279453467388,
        "step": 7451
    },
    {
        "loss": 2.0274,
        "grad_norm": 3.623718738555908,
        "learning_rate": 0.00011937599699263156,
        "epoch": 0.9605568445475638,
        "step": 7452
    },
    {
        "loss": 2.3501,
        "grad_norm": 2.2999651432037354,
        "learning_rate": 0.00011931630605195065,
        "epoch": 0.9606857437483888,
        "step": 7453
    },
    {
        "loss": 1.906,
        "grad_norm": 1.5767145156860352,
        "learning_rate": 0.00011925660796124722,
        "epoch": 0.9608146429492137,
        "step": 7454
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.2790600061416626,
        "learning_rate": 0.00011919690274261874,
        "epoch": 0.9609435421500386,
        "step": 7455
    },
    {
        "loss": 1.7901,
        "grad_norm": 2.475351333618164,
        "learning_rate": 0.00011913719041816537,
        "epoch": 0.9610724413508637,
        "step": 7456
    },
    {
        "loss": 1.9792,
        "grad_norm": 1.7118905782699585,
        "learning_rate": 0.00011907747100999006,
        "epoch": 0.9612013405516886,
        "step": 7457
    },
    {
        "loss": 1.8202,
        "grad_norm": 2.348684072494507,
        "learning_rate": 0.00011901774454019805,
        "epoch": 0.9613302397525135,
        "step": 7458
    },
    {
        "loss": 1.8863,
        "grad_norm": 2.292415142059326,
        "learning_rate": 0.0001189580110308975,
        "epoch": 0.9614591389533385,
        "step": 7459
    },
    {
        "loss": 1.8449,
        "grad_norm": 1.9718551635742188,
        "learning_rate": 0.00011889827050419893,
        "epoch": 0.9615880381541635,
        "step": 7460
    },
    {
        "loss": 1.7076,
        "grad_norm": 3.1393020153045654,
        "learning_rate": 0.00011883852298221574,
        "epoch": 0.9617169373549884,
        "step": 7461
    },
    {
        "loss": 1.939,
        "grad_norm": 2.562546968460083,
        "learning_rate": 0.00011877876848706366,
        "epoch": 0.9618458365558133,
        "step": 7462
    },
    {
        "loss": 2.0028,
        "grad_norm": 1.7492741346359253,
        "learning_rate": 0.00011871900704086113,
        "epoch": 0.9619747357566383,
        "step": 7463
    },
    {
        "loss": 1.7402,
        "grad_norm": 2.9084510803222656,
        "learning_rate": 0.00011865923866572903,
        "epoch": 0.9621036349574633,
        "step": 7464
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.929175853729248,
        "learning_rate": 0.00011859946338379117,
        "epoch": 0.9622325341582882,
        "step": 7465
    },
    {
        "loss": 2.0749,
        "grad_norm": 3.7250096797943115,
        "learning_rate": 0.00011853968121717331,
        "epoch": 0.9623614333591132,
        "step": 7466
    },
    {
        "loss": 1.466,
        "grad_norm": 2.291468381881714,
        "learning_rate": 0.00011847989218800433,
        "epoch": 0.9624903325599381,
        "step": 7467
    },
    {
        "loss": 1.5693,
        "grad_norm": 1.6832540035247803,
        "learning_rate": 0.00011842009631841546,
        "epoch": 0.962619231760763,
        "step": 7468
    },
    {
        "loss": 2.0317,
        "grad_norm": 1.9306422472000122,
        "learning_rate": 0.00011836029363054022,
        "epoch": 0.9627481309615881,
        "step": 7469
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.7466741800308228,
        "learning_rate": 0.00011830048414651505,
        "epoch": 0.962877030162413,
        "step": 7470
    },
    {
        "loss": 1.6101,
        "grad_norm": 2.5660314559936523,
        "learning_rate": 0.0001182406678884786,
        "epoch": 0.9630059293632379,
        "step": 7471
    },
    {
        "loss": 1.0842,
        "grad_norm": 2.4516489505767822,
        "learning_rate": 0.00011818084487857224,
        "epoch": 0.9631348285640629,
        "step": 7472
    },
    {
        "loss": 2.0582,
        "grad_norm": 1.335270643234253,
        "learning_rate": 0.00011812101513893958,
        "epoch": 0.9632637277648879,
        "step": 7473
    },
    {
        "loss": 1.1743,
        "grad_norm": 3.7670681476593018,
        "learning_rate": 0.00011806117869172706,
        "epoch": 0.9633926269657128,
        "step": 7474
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.334092140197754,
        "learning_rate": 0.00011800133555908337,
        "epoch": 0.9635215261665377,
        "step": 7475
    },
    {
        "loss": 2.1734,
        "grad_norm": 1.4846516847610474,
        "learning_rate": 0.00011794148576315964,
        "epoch": 0.9636504253673627,
        "step": 7476
    },
    {
        "loss": 1.4702,
        "grad_norm": 1.1961824893951416,
        "learning_rate": 0.00011788162932610969,
        "epoch": 0.9637793245681877,
        "step": 7477
    },
    {
        "loss": 2.0853,
        "grad_norm": 1.5049301385879517,
        "learning_rate": 0.00011782176627008961,
        "epoch": 0.9639082237690126,
        "step": 7478
    },
    {
        "loss": 2.1321,
        "grad_norm": 1.5383843183517456,
        "learning_rate": 0.000117761896617258,
        "epoch": 0.9640371229698376,
        "step": 7479
    },
    {
        "loss": 1.4125,
        "grad_norm": 2.3667266368865967,
        "learning_rate": 0.00011770202038977587,
        "epoch": 0.9641660221706625,
        "step": 7480
    },
    {
        "loss": 1.3832,
        "grad_norm": 2.9471840858459473,
        "learning_rate": 0.00011764213760980664,
        "epoch": 0.9642949213714875,
        "step": 7481
    },
    {
        "loss": 1.5619,
        "grad_norm": 3.2252016067504883,
        "learning_rate": 0.00011758224829951643,
        "epoch": 0.9644238205723125,
        "step": 7482
    },
    {
        "loss": 1.5954,
        "grad_norm": 2.2121336460113525,
        "learning_rate": 0.00011752235248107322,
        "epoch": 0.9645527197731374,
        "step": 7483
    },
    {
        "loss": 1.9203,
        "grad_norm": 2.1371679306030273,
        "learning_rate": 0.00011746245017664795,
        "epoch": 0.9646816189739623,
        "step": 7484
    },
    {
        "loss": 2.432,
        "grad_norm": 1.722935438156128,
        "learning_rate": 0.00011740254140841379,
        "epoch": 0.9648105181747874,
        "step": 7485
    },
    {
        "loss": 1.4575,
        "grad_norm": 2.1571521759033203,
        "learning_rate": 0.00011734262619854606,
        "epoch": 0.9649394173756123,
        "step": 7486
    },
    {
        "loss": 1.096,
        "grad_norm": 4.11930513381958,
        "learning_rate": 0.00011728270456922284,
        "epoch": 0.9650683165764372,
        "step": 7487
    },
    {
        "loss": 2.5868,
        "grad_norm": 1.8801542520523071,
        "learning_rate": 0.00011722277654262431,
        "epoch": 0.9651972157772621,
        "step": 7488
    },
    {
        "loss": 1.6187,
        "grad_norm": 1.9735647439956665,
        "learning_rate": 0.00011716284214093316,
        "epoch": 0.9653261149780872,
        "step": 7489
    },
    {
        "loss": 1.8847,
        "grad_norm": 2.2731266021728516,
        "learning_rate": 0.00011710290138633428,
        "epoch": 0.9654550141789121,
        "step": 7490
    },
    {
        "loss": 2.2496,
        "grad_norm": 1.4856607913970947,
        "learning_rate": 0.0001170429543010152,
        "epoch": 0.965583913379737,
        "step": 7491
    },
    {
        "loss": 1.2277,
        "grad_norm": 2.1056392192840576,
        "learning_rate": 0.00011698300090716553,
        "epoch": 0.965712812580562,
        "step": 7492
    },
    {
        "loss": 2.0163,
        "grad_norm": 2.1455118656158447,
        "learning_rate": 0.00011692304122697721,
        "epoch": 0.965841711781387,
        "step": 7493
    },
    {
        "loss": 2.3358,
        "grad_norm": 1.8560311794281006,
        "learning_rate": 0.00011686307528264479,
        "epoch": 0.9659706109822119,
        "step": 7494
    },
    {
        "loss": 1.5617,
        "grad_norm": 2.753744602203369,
        "learning_rate": 0.00011680310309636489,
        "epoch": 0.9660995101830369,
        "step": 7495
    },
    {
        "loss": 1.3493,
        "grad_norm": 1.6627618074417114,
        "learning_rate": 0.00011674312469033642,
        "epoch": 0.9662284093838618,
        "step": 7496
    },
    {
        "loss": 1.9947,
        "grad_norm": 2.260667562484741,
        "learning_rate": 0.00011668314008676073,
        "epoch": 0.9663573085846868,
        "step": 7497
    },
    {
        "loss": 1.9768,
        "grad_norm": 2.1597814559936523,
        "learning_rate": 0.00011662314930784136,
        "epoch": 0.9664862077855118,
        "step": 7498
    },
    {
        "loss": 2.398,
        "grad_norm": 3.342622756958008,
        "learning_rate": 0.00011656315237578423,
        "epoch": 0.9666151069863367,
        "step": 7499
    },
    {
        "loss": 1.8078,
        "grad_norm": 1.9752641916275024,
        "learning_rate": 0.00011650314931279754,
        "epoch": 0.9667440061871616,
        "step": 7500
    },
    {
        "loss": 2.2095,
        "grad_norm": 1.8431421518325806,
        "learning_rate": 0.00011644314014109152,
        "epoch": 0.9668729053879866,
        "step": 7501
    },
    {
        "loss": 1.6013,
        "grad_norm": 2.08442759513855,
        "learning_rate": 0.00011638312488287914,
        "epoch": 0.9670018045888116,
        "step": 7502
    },
    {
        "loss": 1.9209,
        "grad_norm": 2.0648398399353027,
        "learning_rate": 0.00011632310356037506,
        "epoch": 0.9671307037896365,
        "step": 7503
    },
    {
        "loss": 1.4735,
        "grad_norm": 2.0323188304901123,
        "learning_rate": 0.00011626307619579659,
        "epoch": 0.9672596029904614,
        "step": 7504
    },
    {
        "loss": 1.5259,
        "grad_norm": 2.721869707107544,
        "learning_rate": 0.00011620304281136316,
        "epoch": 0.9673885021912864,
        "step": 7505
    },
    {
        "loss": 1.9227,
        "grad_norm": 1.7451072931289673,
        "learning_rate": 0.00011614300342929639,
        "epoch": 0.9675174013921114,
        "step": 7506
    },
    {
        "loss": 2.1392,
        "grad_norm": 1.385953426361084,
        "learning_rate": 0.00011608295807182002,
        "epoch": 0.9676463005929363,
        "step": 7507
    },
    {
        "loss": 2.4238,
        "grad_norm": 1.4968385696411133,
        "learning_rate": 0.00011602290676116032,
        "epoch": 0.9677751997937613,
        "step": 7508
    },
    {
        "loss": 1.9329,
        "grad_norm": 2.5192055702209473,
        "learning_rate": 0.00011596284951954547,
        "epoch": 0.9679040989945862,
        "step": 7509
    },
    {
        "loss": 1.9336,
        "grad_norm": 2.3514416217803955,
        "learning_rate": 0.00011590278636920588,
        "epoch": 0.9680329981954112,
        "step": 7510
    },
    {
        "loss": 2.204,
        "grad_norm": 2.8540940284729004,
        "learning_rate": 0.0001158427173323744,
        "epoch": 0.9681618973962361,
        "step": 7511
    },
    {
        "loss": 1.4906,
        "grad_norm": 2.650002956390381,
        "learning_rate": 0.00011578264243128574,
        "epoch": 0.9682907965970611,
        "step": 7512
    },
    {
        "loss": 1.1834,
        "grad_norm": 2.2159674167633057,
        "learning_rate": 0.00011572256168817687,
        "epoch": 0.968419695797886,
        "step": 7513
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.1155002117156982,
        "learning_rate": 0.00011566247512528707,
        "epoch": 0.968548594998711,
        "step": 7514
    },
    {
        "loss": 2.012,
        "grad_norm": 2.4898295402526855,
        "learning_rate": 0.00011560238276485752,
        "epoch": 0.968677494199536,
        "step": 7515
    },
    {
        "loss": 1.3626,
        "grad_norm": 1.9484946727752686,
        "learning_rate": 0.00011554228462913191,
        "epoch": 0.9688063934003609,
        "step": 7516
    },
    {
        "loss": 1.5073,
        "grad_norm": 2.2712655067443848,
        "learning_rate": 0.00011548218074035568,
        "epoch": 0.9689352926011858,
        "step": 7517
    },
    {
        "loss": 0.9612,
        "grad_norm": 2.2045679092407227,
        "learning_rate": 0.00011542207112077657,
        "epoch": 0.9690641918020109,
        "step": 7518
    },
    {
        "loss": 2.2819,
        "grad_norm": 1.5619165897369385,
        "learning_rate": 0.00011536195579264454,
        "epoch": 0.9691930910028358,
        "step": 7519
    },
    {
        "loss": 1.6429,
        "grad_norm": 1.9655619859695435,
        "learning_rate": 0.00011530183477821155,
        "epoch": 0.9693219902036607,
        "step": 7520
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.702193260192871,
        "learning_rate": 0.00011524170809973163,
        "epoch": 0.9694508894044856,
        "step": 7521
    },
    {
        "loss": 2.0991,
        "grad_norm": 1.4009950160980225,
        "learning_rate": 0.00011518157577946098,
        "epoch": 0.9695797886053107,
        "step": 7522
    },
    {
        "loss": 1.5515,
        "grad_norm": 2.423375368118286,
        "learning_rate": 0.00011512143783965785,
        "epoch": 0.9697086878061356,
        "step": 7523
    },
    {
        "loss": 0.685,
        "grad_norm": 2.5061099529266357,
        "learning_rate": 0.0001150612943025825,
        "epoch": 0.9698375870069605,
        "step": 7524
    },
    {
        "loss": 2.0537,
        "grad_norm": 1.1920777559280396,
        "learning_rate": 0.00011500114519049754,
        "epoch": 0.9699664862077855,
        "step": 7525
    },
    {
        "loss": 2.4099,
        "grad_norm": 1.6516870260238647,
        "learning_rate": 0.00011494099052566731,
        "epoch": 0.9700953854086105,
        "step": 7526
    },
    {
        "loss": 1.4932,
        "grad_norm": 1.2199363708496094,
        "learning_rate": 0.00011488083033035833,
        "epoch": 0.9702242846094354,
        "step": 7527
    },
    {
        "loss": 1.0523,
        "grad_norm": 1.804609775543213,
        "learning_rate": 0.00011482066462683931,
        "epoch": 0.9703531838102604,
        "step": 7528
    },
    {
        "loss": 2.2957,
        "grad_norm": 1.5877870321273804,
        "learning_rate": 0.00011476049343738078,
        "epoch": 0.9704820830110853,
        "step": 7529
    },
    {
        "loss": 1.2146,
        "grad_norm": 2.0474298000335693,
        "learning_rate": 0.00011470031678425541,
        "epoch": 0.9706109822119103,
        "step": 7530
    },
    {
        "loss": 2.2258,
        "grad_norm": 1.6444908380508423,
        "learning_rate": 0.00011464013468973789,
        "epoch": 0.9707398814127353,
        "step": 7531
    },
    {
        "loss": 1.7587,
        "grad_norm": 2.043581962585449,
        "learning_rate": 0.0001145799471761048,
        "epoch": 0.9708687806135602,
        "step": 7532
    },
    {
        "loss": 2.1617,
        "grad_norm": 1.2511831521987915,
        "learning_rate": 0.00011451975426563501,
        "epoch": 0.9709976798143851,
        "step": 7533
    },
    {
        "loss": 1.5061,
        "grad_norm": 1.660789132118225,
        "learning_rate": 0.00011445955598060914,
        "epoch": 0.9711265790152102,
        "step": 7534
    },
    {
        "loss": 0.5348,
        "grad_norm": 1.8489487171173096,
        "learning_rate": 0.00011439935234330973,
        "epoch": 0.9712554782160351,
        "step": 7535
    },
    {
        "loss": 1.1341,
        "grad_norm": 3.3513262271881104,
        "learning_rate": 0.0001143391433760217,
        "epoch": 0.97138437741686,
        "step": 7536
    },
    {
        "loss": 2.1907,
        "grad_norm": 2.349461317062378,
        "learning_rate": 0.00011427892910103152,
        "epoch": 0.9715132766176849,
        "step": 7537
    },
    {
        "loss": 2.1214,
        "grad_norm": 1.428763747215271,
        "learning_rate": 0.00011421870954062785,
        "epoch": 0.97164217581851,
        "step": 7538
    },
    {
        "loss": 2.0269,
        "grad_norm": 2.111321449279785,
        "learning_rate": 0.00011415848471710116,
        "epoch": 0.9717710750193349,
        "step": 7539
    },
    {
        "loss": 1.1026,
        "grad_norm": 2.0692248344421387,
        "learning_rate": 0.00011409825465274413,
        "epoch": 0.9718999742201598,
        "step": 7540
    },
    {
        "loss": 1.6948,
        "grad_norm": 1.3821537494659424,
        "learning_rate": 0.00011403801936985097,
        "epoch": 0.9720288734209848,
        "step": 7541
    },
    {
        "loss": 2.1032,
        "grad_norm": 1.7291339635849,
        "learning_rate": 0.00011397777889071824,
        "epoch": 0.9721577726218097,
        "step": 7542
    },
    {
        "loss": 1.9846,
        "grad_norm": 2.194802761077881,
        "learning_rate": 0.00011391753323764419,
        "epoch": 0.9722866718226347,
        "step": 7543
    },
    {
        "loss": 2.2864,
        "grad_norm": 1.3734463453292847,
        "learning_rate": 0.00011385728243292895,
        "epoch": 0.9724155710234597,
        "step": 7544
    },
    {
        "loss": 1.6738,
        "grad_norm": 1.977763295173645,
        "learning_rate": 0.00011379702649887479,
        "epoch": 0.9725444702242846,
        "step": 7545
    },
    {
        "loss": 1.8357,
        "grad_norm": 1.976637840270996,
        "learning_rate": 0.00011373676545778563,
        "epoch": 0.9726733694251095,
        "step": 7546
    },
    {
        "loss": 1.2031,
        "grad_norm": 2.5306241512298584,
        "learning_rate": 0.00011367649933196742,
        "epoch": 0.9728022686259346,
        "step": 7547
    },
    {
        "loss": 1.7452,
        "grad_norm": 2.6089773178100586,
        "learning_rate": 0.00011361622814372797,
        "epoch": 0.9729311678267595,
        "step": 7548
    },
    {
        "loss": 2.0278,
        "grad_norm": 1.242028832435608,
        "learning_rate": 0.0001135559519153768,
        "epoch": 0.9730600670275844,
        "step": 7549
    },
    {
        "loss": 1.5799,
        "grad_norm": 2.370213031768799,
        "learning_rate": 0.00011349567066922566,
        "epoch": 0.9731889662284093,
        "step": 7550
    },
    {
        "loss": 2.5172,
        "grad_norm": 2.545254945755005,
        "learning_rate": 0.00011343538442758783,
        "epoch": 0.9733178654292344,
        "step": 7551
    },
    {
        "loss": 2.2941,
        "grad_norm": 2.058178186416626,
        "learning_rate": 0.00011337509321277851,
        "epoch": 0.9734467646300593,
        "step": 7552
    },
    {
        "loss": 1.4242,
        "grad_norm": 2.3995308876037598,
        "learning_rate": 0.00011331479704711489,
        "epoch": 0.9735756638308842,
        "step": 7553
    },
    {
        "loss": 2.2734,
        "grad_norm": 3.3692240715026855,
        "learning_rate": 0.00011325449595291579,
        "epoch": 0.9737045630317092,
        "step": 7554
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.676891803741455,
        "learning_rate": 0.00011319418995250201,
        "epoch": 0.9738334622325342,
        "step": 7555
    },
    {
        "loss": 2.3707,
        "grad_norm": 1.597311019897461,
        "learning_rate": 0.00011313387906819602,
        "epoch": 0.9739623614333591,
        "step": 7556
    },
    {
        "loss": 2.3392,
        "grad_norm": 1.507336974143982,
        "learning_rate": 0.00011307356332232231,
        "epoch": 0.974091260634184,
        "step": 7557
    },
    {
        "loss": 1.8488,
        "grad_norm": 1.755810260772705,
        "learning_rate": 0.00011301324273720688,
        "epoch": 0.974220159835009,
        "step": 7558
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.8054457902908325,
        "learning_rate": 0.00011295291733517772,
        "epoch": 0.974349059035834,
        "step": 7559
    },
    {
        "loss": 1.5641,
        "grad_norm": 1.9203808307647705,
        "learning_rate": 0.0001128925871385648,
        "epoch": 0.974477958236659,
        "step": 7560
    },
    {
        "loss": 1.603,
        "grad_norm": 1.9759702682495117,
        "learning_rate": 0.00011283225216969926,
        "epoch": 0.9746068574374839,
        "step": 7561
    },
    {
        "loss": 1.7507,
        "grad_norm": 2.248856544494629,
        "learning_rate": 0.00011277191245091461,
        "epoch": 0.9747357566383088,
        "step": 7562
    },
    {
        "loss": 2.0069,
        "grad_norm": 2.2519991397857666,
        "learning_rate": 0.00011271156800454585,
        "epoch": 0.9748646558391338,
        "step": 7563
    },
    {
        "loss": 1.0149,
        "grad_norm": 1.815016269683838,
        "learning_rate": 0.00011265121885292973,
        "epoch": 0.9749935550399588,
        "step": 7564
    },
    {
        "loss": 2.2697,
        "grad_norm": 1.8814160823822021,
        "learning_rate": 0.00011259086501840478,
        "epoch": 0.9751224542407837,
        "step": 7565
    },
    {
        "loss": 2.1329,
        "grad_norm": 2.4934234619140625,
        "learning_rate": 0.00011253050652331123,
        "epoch": 0.9752513534416086,
        "step": 7566
    },
    {
        "loss": 1.8978,
        "grad_norm": 1.5136076211929321,
        "learning_rate": 0.00011247014338999114,
        "epoch": 0.9753802526424337,
        "step": 7567
    },
    {
        "loss": 1.5194,
        "grad_norm": 2.1749088764190674,
        "learning_rate": 0.00011240977564078816,
        "epoch": 0.9755091518432586,
        "step": 7568
    },
    {
        "loss": 2.0208,
        "grad_norm": 2.8361430168151855,
        "learning_rate": 0.00011234940329804766,
        "epoch": 0.9756380510440835,
        "step": 7569
    },
    {
        "loss": 2.3106,
        "grad_norm": 2.1069698333740234,
        "learning_rate": 0.00011228902638411685,
        "epoch": 0.9757669502449084,
        "step": 7570
    },
    {
        "loss": 2.0046,
        "grad_norm": 1.5929090976715088,
        "learning_rate": 0.0001122286449213445,
        "epoch": 0.9758958494457335,
        "step": 7571
    },
    {
        "loss": 1.4216,
        "grad_norm": 2.358647346496582,
        "learning_rate": 0.00011216825893208107,
        "epoch": 0.9760247486465584,
        "step": 7572
    },
    {
        "loss": 2.0182,
        "grad_norm": 1.4505879878997803,
        "learning_rate": 0.00011210786843867864,
        "epoch": 0.9761536478473833,
        "step": 7573
    },
    {
        "loss": 2.0263,
        "grad_norm": 2.4346091747283936,
        "learning_rate": 0.00011204747346349129,
        "epoch": 0.9762825470482083,
        "step": 7574
    },
    {
        "loss": 1.7341,
        "grad_norm": 2.4111709594726562,
        "learning_rate": 0.0001119870740288742,
        "epoch": 0.9764114462490333,
        "step": 7575
    },
    {
        "loss": 2.0795,
        "grad_norm": 1.8039249181747437,
        "learning_rate": 0.00011192667015718465,
        "epoch": 0.9765403454498582,
        "step": 7576
    },
    {
        "loss": 1.1027,
        "grad_norm": 2.846804141998291,
        "learning_rate": 0.00011186626187078156,
        "epoch": 0.9766692446506832,
        "step": 7577
    },
    {
        "loss": 1.0464,
        "grad_norm": 2.3659276962280273,
        "learning_rate": 0.00011180584919202508,
        "epoch": 0.9767981438515081,
        "step": 7578
    },
    {
        "loss": 0.8493,
        "grad_norm": 2.666593074798584,
        "learning_rate": 0.0001117454321432774,
        "epoch": 0.976927043052333,
        "step": 7579
    },
    {
        "loss": 1.8041,
        "grad_norm": 1.5857406854629517,
        "learning_rate": 0.00011168501074690219,
        "epoch": 0.9770559422531581,
        "step": 7580
    },
    {
        "loss": 2.0267,
        "grad_norm": 1.761904239654541,
        "learning_rate": 0.0001116245850252647,
        "epoch": 0.977184841453983,
        "step": 7581
    },
    {
        "loss": 1.9388,
        "grad_norm": 1.4256181716918945,
        "learning_rate": 0.00011156415500073166,
        "epoch": 0.9773137406548079,
        "step": 7582
    },
    {
        "loss": 2.5722,
        "grad_norm": 1.3354724645614624,
        "learning_rate": 0.00011150372069567174,
        "epoch": 0.9774426398556328,
        "step": 7583
    },
    {
        "loss": 1.055,
        "grad_norm": 3.1260502338409424,
        "learning_rate": 0.0001114432821324549,
        "epoch": 0.9775715390564579,
        "step": 7584
    },
    {
        "loss": 2.1197,
        "grad_norm": 2.1830430030822754,
        "learning_rate": 0.00011138283933345275,
        "epoch": 0.9777004382572828,
        "step": 7585
    },
    {
        "loss": 1.95,
        "grad_norm": 1.5605331659317017,
        "learning_rate": 0.00011132239232103845,
        "epoch": 0.9778293374581077,
        "step": 7586
    },
    {
        "loss": 1.5204,
        "grad_norm": 2.4662132263183594,
        "learning_rate": 0.0001112619411175868,
        "epoch": 0.9779582366589327,
        "step": 7587
    },
    {
        "loss": 1.704,
        "grad_norm": 1.9077425003051758,
        "learning_rate": 0.00011120148574547414,
        "epoch": 0.9780871358597577,
        "step": 7588
    },
    {
        "loss": 2.3516,
        "grad_norm": 1.3978877067565918,
        "learning_rate": 0.00011114102622707825,
        "epoch": 0.9782160350605826,
        "step": 7589
    },
    {
        "loss": 1.5654,
        "grad_norm": 2.0224878787994385,
        "learning_rate": 0.00011108056258477842,
        "epoch": 0.9783449342614076,
        "step": 7590
    },
    {
        "loss": 1.2729,
        "grad_norm": 2.84860897064209,
        "learning_rate": 0.00011102009484095581,
        "epoch": 0.9784738334622325,
        "step": 7591
    },
    {
        "loss": 1.9322,
        "grad_norm": 1.8540453910827637,
        "learning_rate": 0.00011095962301799255,
        "epoch": 0.9786027326630575,
        "step": 7592
    },
    {
        "loss": 1.3535,
        "grad_norm": 3.152169704437256,
        "learning_rate": 0.0001108991471382727,
        "epoch": 0.9787316318638825,
        "step": 7593
    },
    {
        "loss": 1.7726,
        "grad_norm": 2.136173725128174,
        "learning_rate": 0.00011083866722418184,
        "epoch": 0.9788605310647074,
        "step": 7594
    },
    {
        "loss": 1.5992,
        "grad_norm": 2.2548890113830566,
        "learning_rate": 0.00011077818329810661,
        "epoch": 0.9789894302655323,
        "step": 7595
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.111724853515625,
        "learning_rate": 0.00011071769538243568,
        "epoch": 0.9791183294663574,
        "step": 7596
    },
    {
        "loss": 1.7593,
        "grad_norm": 1.724570631980896,
        "learning_rate": 0.00011065720349955882,
        "epoch": 0.9792472286671823,
        "step": 7597
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.1094894409179688,
        "learning_rate": 0.00011059670767186738,
        "epoch": 0.9793761278680072,
        "step": 7598
    },
    {
        "loss": 1.3819,
        "grad_norm": 2.2337300777435303,
        "learning_rate": 0.00011053620792175417,
        "epoch": 0.9795050270688321,
        "step": 7599
    },
    {
        "loss": 2.3932,
        "grad_norm": 2.064189910888672,
        "learning_rate": 0.00011047570427161354,
        "epoch": 0.9796339262696572,
        "step": 7600
    },
    {
        "loss": 1.608,
        "grad_norm": 2.7502827644348145,
        "learning_rate": 0.00011041519674384113,
        "epoch": 0.9797628254704821,
        "step": 7601
    },
    {
        "loss": 2.4317,
        "grad_norm": 1.9468361139297485,
        "learning_rate": 0.00011035468536083409,
        "epoch": 0.979891724671307,
        "step": 7602
    },
    {
        "loss": 1.9469,
        "grad_norm": 1.4844255447387695,
        "learning_rate": 0.00011029417014499104,
        "epoch": 0.980020623872132,
        "step": 7603
    },
    {
        "loss": 0.5939,
        "grad_norm": 1.8029732704162598,
        "learning_rate": 0.00011023365111871201,
        "epoch": 0.980149523072957,
        "step": 7604
    },
    {
        "loss": 1.6719,
        "grad_norm": 2.1004865169525146,
        "learning_rate": 0.0001101731283043983,
        "epoch": 0.9802784222737819,
        "step": 7605
    },
    {
        "loss": 1.4567,
        "grad_norm": 2.9940004348754883,
        "learning_rate": 0.0001101126017244528,
        "epoch": 0.9804073214746069,
        "step": 7606
    },
    {
        "loss": 2.0088,
        "grad_norm": 1.7646241188049316,
        "learning_rate": 0.0001100520714012796,
        "epoch": 0.9805362206754318,
        "step": 7607
    },
    {
        "loss": 2.0073,
        "grad_norm": 2.0395631790161133,
        "learning_rate": 0.0001099915373572845,
        "epoch": 0.9806651198762568,
        "step": 7608
    },
    {
        "loss": 2.1261,
        "grad_norm": 2.168849468231201,
        "learning_rate": 0.00010993099961487416,
        "epoch": 0.9807940190770817,
        "step": 7609
    },
    {
        "loss": 1.7631,
        "grad_norm": 2.3723597526550293,
        "learning_rate": 0.00010987045819645714,
        "epoch": 0.9809229182779067,
        "step": 7610
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.9795522689819336,
        "learning_rate": 0.0001098099131244432,
        "epoch": 0.9810518174787316,
        "step": 7611
    },
    {
        "loss": 1.3338,
        "grad_norm": 2.422170639038086,
        "learning_rate": 0.00010974936442124311,
        "epoch": 0.9811807166795566,
        "step": 7612
    },
    {
        "loss": 1.1234,
        "grad_norm": 2.4528918266296387,
        "learning_rate": 0.00010968881210926949,
        "epoch": 0.9813096158803816,
        "step": 7613
    },
    {
        "loss": 2.4659,
        "grad_norm": 1.6059813499450684,
        "learning_rate": 0.000109628256210936,
        "epoch": 0.9814385150812065,
        "step": 7614
    },
    {
        "loss": 1.8306,
        "grad_norm": 1.6511791944503784,
        "learning_rate": 0.00010956769674865766,
        "epoch": 0.9815674142820314,
        "step": 7615
    },
    {
        "loss": 2.0977,
        "grad_norm": 2.6179893016815186,
        "learning_rate": 0.00010950713374485083,
        "epoch": 0.9816963134828564,
        "step": 7616
    },
    {
        "loss": 1.893,
        "grad_norm": 2.094348430633545,
        "learning_rate": 0.00010944656722193328,
        "epoch": 0.9818252126836814,
        "step": 7617
    },
    {
        "loss": 2.0007,
        "grad_norm": 2.394674777984619,
        "learning_rate": 0.00010938599720232394,
        "epoch": 0.9819541118845063,
        "step": 7618
    },
    {
        "loss": 1.0998,
        "grad_norm": 2.0613021850585938,
        "learning_rate": 0.00010932542370844305,
        "epoch": 0.9820830110853312,
        "step": 7619
    },
    {
        "loss": 1.7733,
        "grad_norm": 2.9018001556396484,
        "learning_rate": 0.00010926484676271226,
        "epoch": 0.9822119102861562,
        "step": 7620
    },
    {
        "loss": 1.4005,
        "grad_norm": 1.3434849977493286,
        "learning_rate": 0.00010920426638755444,
        "epoch": 0.9823408094869812,
        "step": 7621
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.5154902935028076,
        "learning_rate": 0.0001091436826053936,
        "epoch": 0.9824697086878061,
        "step": 7622
    },
    {
        "loss": 1.8613,
        "grad_norm": 2.31807017326355,
        "learning_rate": 0.00010908309543865516,
        "epoch": 0.9825986078886311,
        "step": 7623
    },
    {
        "loss": 1.7932,
        "grad_norm": 1.2001322507858276,
        "learning_rate": 0.00010902250490976569,
        "epoch": 0.982727507089456,
        "step": 7624
    },
    {
        "loss": 1.8903,
        "grad_norm": 2.4318902492523193,
        "learning_rate": 0.00010896191104115319,
        "epoch": 0.982856406290281,
        "step": 7625
    },
    {
        "loss": 2.1848,
        "grad_norm": 2.0334599018096924,
        "learning_rate": 0.00010890131385524666,
        "epoch": 0.982985305491106,
        "step": 7626
    },
    {
        "loss": 1.9716,
        "grad_norm": 3.3467278480529785,
        "learning_rate": 0.00010884071337447646,
        "epoch": 0.9831142046919309,
        "step": 7627
    },
    {
        "loss": 2.2118,
        "grad_norm": 1.7393029928207397,
        "learning_rate": 0.00010878010962127427,
        "epoch": 0.9832431038927558,
        "step": 7628
    },
    {
        "loss": 2.0424,
        "grad_norm": 2.0405666828155518,
        "learning_rate": 0.0001087195026180726,
        "epoch": 0.9833720030935809,
        "step": 7629
    },
    {
        "loss": 2.3068,
        "grad_norm": 2.7692923545837402,
        "learning_rate": 0.00010865889238730567,
        "epoch": 0.9835009022944058,
        "step": 7630
    },
    {
        "loss": 2.2519,
        "grad_norm": 2.0378434658050537,
        "learning_rate": 0.0001085982789514085,
        "epoch": 0.9836298014952307,
        "step": 7631
    },
    {
        "loss": 1.7235,
        "grad_norm": 1.4536908864974976,
        "learning_rate": 0.00010853766233281752,
        "epoch": 0.9837587006960556,
        "step": 7632
    },
    {
        "loss": 2.1277,
        "grad_norm": 1.2846230268478394,
        "learning_rate": 0.00010847704255397015,
        "epoch": 0.9838875998968807,
        "step": 7633
    },
    {
        "loss": 1.9896,
        "grad_norm": 2.384856939315796,
        "learning_rate": 0.00010841641963730522,
        "epoch": 0.9840164990977056,
        "step": 7634
    },
    {
        "loss": 1.7797,
        "grad_norm": 2.587364435195923,
        "learning_rate": 0.00010835579360526254,
        "epoch": 0.9841453982985305,
        "step": 7635
    },
    {
        "loss": 0.9633,
        "grad_norm": 3.389021635055542,
        "learning_rate": 0.00010829516448028305,
        "epoch": 0.9842742974993555,
        "step": 7636
    },
    {
        "loss": 2.0049,
        "grad_norm": 1.3020514249801636,
        "learning_rate": 0.00010823453228480904,
        "epoch": 0.9844031967001805,
        "step": 7637
    },
    {
        "loss": 2.1681,
        "grad_norm": 1.945755958557129,
        "learning_rate": 0.00010817389704128378,
        "epoch": 0.9845320959010054,
        "step": 7638
    },
    {
        "loss": 2.1997,
        "grad_norm": 2.1815593242645264,
        "learning_rate": 0.00010811325877215162,
        "epoch": 0.9846609951018304,
        "step": 7639
    },
    {
        "loss": 1.528,
        "grad_norm": 2.9188339710235596,
        "learning_rate": 0.00010805261749985816,
        "epoch": 0.9847898943026553,
        "step": 7640
    },
    {
        "loss": 1.5386,
        "grad_norm": 3.3101601600646973,
        "learning_rate": 0.00010799197324684995,
        "epoch": 0.9849187935034803,
        "step": 7641
    },
    {
        "loss": 1.3797,
        "grad_norm": 1.7514832019805908,
        "learning_rate": 0.0001079313260355749,
        "epoch": 0.9850476927043053,
        "step": 7642
    },
    {
        "loss": 2.5911,
        "grad_norm": 1.6136515140533447,
        "learning_rate": 0.00010787067588848183,
        "epoch": 0.9851765919051302,
        "step": 7643
    },
    {
        "loss": 1.5694,
        "grad_norm": 2.6147634983062744,
        "learning_rate": 0.00010781002282802057,
        "epoch": 0.9853054911059551,
        "step": 7644
    },
    {
        "loss": 1.8641,
        "grad_norm": 2.0352799892425537,
        "learning_rate": 0.00010774936687664225,
        "epoch": 0.9854343903067802,
        "step": 7645
    },
    {
        "loss": 2.1466,
        "grad_norm": 1.3233699798583984,
        "learning_rate": 0.00010768870805679899,
        "epoch": 0.9855632895076051,
        "step": 7646
    },
    {
        "loss": 1.3884,
        "grad_norm": 3.007849931716919,
        "learning_rate": 0.00010762804639094384,
        "epoch": 0.98569218870843,
        "step": 7647
    },
    {
        "loss": 1.394,
        "grad_norm": 1.3170397281646729,
        "learning_rate": 0.00010756738190153101,
        "epoch": 0.9858210879092549,
        "step": 7648
    },
    {
        "loss": 1.987,
        "grad_norm": 1.82674241065979,
        "learning_rate": 0.00010750671461101585,
        "epoch": 0.98594998711008,
        "step": 7649
    },
    {
        "loss": 1.5105,
        "grad_norm": 2.066859722137451,
        "learning_rate": 0.00010744604454185445,
        "epoch": 0.9860788863109049,
        "step": 7650
    },
    {
        "loss": 2.0506,
        "grad_norm": 1.3321037292480469,
        "learning_rate": 0.00010738537171650435,
        "epoch": 0.9862077855117298,
        "step": 7651
    },
    {
        "loss": 2.2038,
        "grad_norm": 2.679100513458252,
        "learning_rate": 0.00010732469615742377,
        "epoch": 0.9863366847125548,
        "step": 7652
    },
    {
        "loss": 1.7747,
        "grad_norm": 1.5096893310546875,
        "learning_rate": 0.00010726401788707201,
        "epoch": 0.9864655839133797,
        "step": 7653
    },
    {
        "loss": 1.5944,
        "grad_norm": 2.4211153984069824,
        "learning_rate": 0.00010720333692790956,
        "epoch": 0.9865944831142047,
        "step": 7654
    },
    {
        "loss": 2.039,
        "grad_norm": 2.0172975063323975,
        "learning_rate": 0.0001071426533023977,
        "epoch": 0.9867233823150297,
        "step": 7655
    },
    {
        "loss": 1.2193,
        "grad_norm": 3.3160810470581055,
        "learning_rate": 0.00010708196703299874,
        "epoch": 0.9868522815158546,
        "step": 7656
    },
    {
        "loss": 1.5915,
        "grad_norm": 2.1289665699005127,
        "learning_rate": 0.00010702127814217599,
        "epoch": 0.9869811807166795,
        "step": 7657
    },
    {
        "loss": 2.3425,
        "grad_norm": 1.6539095640182495,
        "learning_rate": 0.0001069605866523937,
        "epoch": 0.9871100799175045,
        "step": 7658
    },
    {
        "loss": 2.0775,
        "grad_norm": 1.5550745725631714,
        "learning_rate": 0.0001068998925861172,
        "epoch": 0.9872389791183295,
        "step": 7659
    },
    {
        "loss": 1.6141,
        "grad_norm": 2.579578399658203,
        "learning_rate": 0.00010683919596581267,
        "epoch": 0.9873678783191544,
        "step": 7660
    },
    {
        "loss": 1.8787,
        "grad_norm": 1.6634087562561035,
        "learning_rate": 0.00010677849681394712,
        "epoch": 0.9874967775199793,
        "step": 7661
    },
    {
        "loss": 1.8472,
        "grad_norm": 1.3214820623397827,
        "learning_rate": 0.00010671779515298884,
        "epoch": 0.9876256767208044,
        "step": 7662
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.577028751373291,
        "learning_rate": 0.00010665709100540673,
        "epoch": 0.9877545759216293,
        "step": 7663
    },
    {
        "loss": 1.687,
        "grad_norm": 1.8103370666503906,
        "learning_rate": 0.00010659638439367072,
        "epoch": 0.9878834751224542,
        "step": 7664
    },
    {
        "loss": 1.0266,
        "grad_norm": 1.4748467206954956,
        "learning_rate": 0.0001065356753402516,
        "epoch": 0.9880123743232792,
        "step": 7665
    },
    {
        "loss": 2.3283,
        "grad_norm": 1.6602641344070435,
        "learning_rate": 0.00010647496386762131,
        "epoch": 0.9881412735241042,
        "step": 7666
    },
    {
        "loss": 1.0757,
        "grad_norm": 2.486220359802246,
        "learning_rate": 0.00010641424999825218,
        "epoch": 0.9882701727249291,
        "step": 7667
    },
    {
        "loss": 1.5472,
        "grad_norm": 1.781368613243103,
        "learning_rate": 0.00010635353375461801,
        "epoch": 0.988399071925754,
        "step": 7668
    },
    {
        "loss": 1.8824,
        "grad_norm": 1.6267099380493164,
        "learning_rate": 0.0001062928151591931,
        "epoch": 0.988527971126579,
        "step": 7669
    },
    {
        "loss": 1.4285,
        "grad_norm": 2.1359477043151855,
        "learning_rate": 0.00010623209423445266,
        "epoch": 0.988656870327404,
        "step": 7670
    },
    {
        "loss": 1.7686,
        "grad_norm": 2.034501075744629,
        "learning_rate": 0.00010617137100287298,
        "epoch": 0.9887857695282289,
        "step": 7671
    },
    {
        "loss": 1.6982,
        "grad_norm": 3.4258406162261963,
        "learning_rate": 0.00010611064548693093,
        "epoch": 0.9889146687290539,
        "step": 7672
    },
    {
        "loss": 1.5865,
        "grad_norm": 2.6424520015716553,
        "learning_rate": 0.00010604991770910444,
        "epoch": 0.9890435679298788,
        "step": 7673
    },
    {
        "loss": 1.3787,
        "grad_norm": 3.6841115951538086,
        "learning_rate": 0.00010598918769187211,
        "epoch": 0.9891724671307038,
        "step": 7674
    },
    {
        "loss": 2.0544,
        "grad_norm": 1.7690396308898926,
        "learning_rate": 0.00010592845545771342,
        "epoch": 0.9893013663315288,
        "step": 7675
    },
    {
        "loss": 0.9534,
        "grad_norm": 2.9333677291870117,
        "learning_rate": 0.00010586772102910882,
        "epoch": 0.9894302655323537,
        "step": 7676
    },
    {
        "loss": 1.7573,
        "grad_norm": 2.8810651302337646,
        "learning_rate": 0.00010580698442853937,
        "epoch": 0.9895591647331786,
        "step": 7677
    },
    {
        "loss": 2.0817,
        "grad_norm": 2.009430170059204,
        "learning_rate": 0.00010574624567848694,
        "epoch": 0.9896880639340037,
        "step": 7678
    },
    {
        "loss": 1.1647,
        "grad_norm": 1.8450371026992798,
        "learning_rate": 0.00010568550480143445,
        "epoch": 0.9898169631348286,
        "step": 7679
    },
    {
        "loss": 1.3806,
        "grad_norm": 2.5811617374420166,
        "learning_rate": 0.00010562476181986529,
        "epoch": 0.9899458623356535,
        "step": 7680
    },
    {
        "loss": 2.2691,
        "grad_norm": 1.6064895391464233,
        "learning_rate": 0.00010556401675626377,
        "epoch": 0.9900747615364784,
        "step": 7681
    },
    {
        "loss": 1.9516,
        "grad_norm": 1.7946313619613647,
        "learning_rate": 0.00010550326963311494,
        "epoch": 0.9902036607373035,
        "step": 7682
    },
    {
        "loss": 1.033,
        "grad_norm": 2.5008037090301514,
        "learning_rate": 0.00010544252047290484,
        "epoch": 0.9903325599381284,
        "step": 7683
    },
    {
        "loss": 1.959,
        "grad_norm": 2.4040887355804443,
        "learning_rate": 0.00010538176929811974,
        "epoch": 0.9904614591389533,
        "step": 7684
    },
    {
        "loss": 1.3874,
        "grad_norm": 2.379903554916382,
        "learning_rate": 0.00010532101613124717,
        "epoch": 0.9905903583397783,
        "step": 7685
    },
    {
        "loss": 1.8034,
        "grad_norm": 2.5287559032440186,
        "learning_rate": 0.00010526026099477526,
        "epoch": 0.9907192575406032,
        "step": 7686
    },
    {
        "loss": 1.2765,
        "grad_norm": 2.528217315673828,
        "learning_rate": 0.00010519950391119262,
        "epoch": 0.9908481567414282,
        "step": 7687
    },
    {
        "loss": 1.6845,
        "grad_norm": 2.960197687149048,
        "learning_rate": 0.00010513874490298896,
        "epoch": 0.9909770559422532,
        "step": 7688
    },
    {
        "loss": 1.9009,
        "grad_norm": 1.5056686401367188,
        "learning_rate": 0.0001050779839926544,
        "epoch": 0.9911059551430781,
        "step": 7689
    },
    {
        "loss": 1.694,
        "grad_norm": 2.117734909057617,
        "learning_rate": 0.00010501722120267997,
        "epoch": 0.991234854343903,
        "step": 7690
    },
    {
        "loss": 2.3693,
        "grad_norm": 2.246577262878418,
        "learning_rate": 0.00010495645655555721,
        "epoch": 0.991363753544728,
        "step": 7691
    },
    {
        "loss": 2.2779,
        "grad_norm": 2.049011707305908,
        "learning_rate": 0.00010489569007377842,
        "epoch": 0.991492652745553,
        "step": 7692
    },
    {
        "loss": 1.9563,
        "grad_norm": 2.834601402282715,
        "learning_rate": 0.00010483492177983674,
        "epoch": 0.9916215519463779,
        "step": 7693
    },
    {
        "loss": 1.726,
        "grad_norm": 2.6273353099823,
        "learning_rate": 0.00010477415169622578,
        "epoch": 0.9917504511472028,
        "step": 7694
    },
    {
        "loss": 1.1461,
        "grad_norm": 1.6370978355407715,
        "learning_rate": 0.00010471337984543977,
        "epoch": 0.9918793503480279,
        "step": 7695
    },
    {
        "loss": 1.4906,
        "grad_norm": 1.9206030368804932,
        "learning_rate": 0.00010465260624997389,
        "epoch": 0.9920082495488528,
        "step": 7696
    },
    {
        "loss": 0.9309,
        "grad_norm": 3.2275445461273193,
        "learning_rate": 0.00010459183093232366,
        "epoch": 0.9921371487496777,
        "step": 7697
    },
    {
        "loss": 2.574,
        "grad_norm": 1.2044848203659058,
        "learning_rate": 0.00010453105391498538,
        "epoch": 0.9922660479505027,
        "step": 7698
    },
    {
        "loss": 2.0951,
        "grad_norm": 1.7149626016616821,
        "learning_rate": 0.00010447027522045589,
        "epoch": 0.9923949471513277,
        "step": 7699
    },
    {
        "loss": 1.5623,
        "grad_norm": 1.472854495048523,
        "learning_rate": 0.00010440949487123288,
        "epoch": 0.9925238463521526,
        "step": 7700
    },
    {
        "loss": 0.6833,
        "grad_norm": 2.943838119506836,
        "learning_rate": 0.00010434871288981428,
        "epoch": 0.9926527455529776,
        "step": 7701
    },
    {
        "loss": 1.9806,
        "grad_norm": 2.9233248233795166,
        "learning_rate": 0.00010428792929869886,
        "epoch": 0.9927816447538025,
        "step": 7702
    },
    {
        "loss": 2.2787,
        "grad_norm": 2.3588249683380127,
        "learning_rate": 0.00010422714412038619,
        "epoch": 0.9929105439546275,
        "step": 7703
    },
    {
        "loss": 2.4645,
        "grad_norm": 2.7016561031341553,
        "learning_rate": 0.0001041663573773759,
        "epoch": 0.9930394431554525,
        "step": 7704
    },
    {
        "loss": 1.5392,
        "grad_norm": 2.0484344959259033,
        "learning_rate": 0.00010410556909216865,
        "epoch": 0.9931683423562774,
        "step": 7705
    },
    {
        "loss": 1.5204,
        "grad_norm": 2.031106948852539,
        "learning_rate": 0.00010404477928726548,
        "epoch": 0.9932972415571023,
        "step": 7706
    },
    {
        "loss": 1.8966,
        "grad_norm": 1.7962281703948975,
        "learning_rate": 0.00010398398798516804,
        "epoch": 0.9934261407579273,
        "step": 7707
    },
    {
        "loss": 2.1321,
        "grad_norm": 1.8981584310531616,
        "learning_rate": 0.00010392319520837841,
        "epoch": 0.9935550399587523,
        "step": 7708
    },
    {
        "loss": 0.8462,
        "grad_norm": 2.6248321533203125,
        "learning_rate": 0.0001038624009793995,
        "epoch": 0.9936839391595772,
        "step": 7709
    },
    {
        "loss": 2.1256,
        "grad_norm": 1.3433799743652344,
        "learning_rate": 0.00010380160532073451,
        "epoch": 0.9938128383604021,
        "step": 7710
    },
    {
        "loss": 2.147,
        "grad_norm": 1.682348370552063,
        "learning_rate": 0.00010374080825488719,
        "epoch": 0.9939417375612272,
        "step": 7711
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.5449689626693726,
        "learning_rate": 0.00010368000980436188,
        "epoch": 0.9940706367620521,
        "step": 7712
    },
    {
        "loss": 1.7085,
        "grad_norm": 2.17462158203125,
        "learning_rate": 0.0001036192099916635,
        "epoch": 0.994199535962877,
        "step": 7713
    },
    {
        "loss": 1.6053,
        "grad_norm": 1.935724139213562,
        "learning_rate": 0.00010355840883929731,
        "epoch": 0.994328435163702,
        "step": 7714
    },
    {
        "loss": 2.2941,
        "grad_norm": 1.9349178075790405,
        "learning_rate": 0.00010349760636976918,
        "epoch": 0.994457334364527,
        "step": 7715
    },
    {
        "loss": 2.1762,
        "grad_norm": 1.500218152999878,
        "learning_rate": 0.00010343680260558532,
        "epoch": 0.9945862335653519,
        "step": 7716
    },
    {
        "loss": 2.0659,
        "grad_norm": 2.257262945175171,
        "learning_rate": 0.00010337599756925281,
        "epoch": 0.9947151327661768,
        "step": 7717
    },
    {
        "loss": 2.0463,
        "grad_norm": 3.5905959606170654,
        "learning_rate": 0.00010331519128327857,
        "epoch": 0.9948440319670018,
        "step": 7718
    },
    {
        "loss": 1.7733,
        "grad_norm": 1.5817272663116455,
        "learning_rate": 0.00010325438377017053,
        "epoch": 0.9949729311678268,
        "step": 7719
    },
    {
        "loss": 0.8333,
        "grad_norm": 3.157869815826416,
        "learning_rate": 0.00010319357505243701,
        "epoch": 0.9951018303686517,
        "step": 7720
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.3108789920806885,
        "learning_rate": 0.00010313276515258637,
        "epoch": 0.9952307295694767,
        "step": 7721
    },
    {
        "loss": 0.9552,
        "grad_norm": 3.66772723197937,
        "learning_rate": 0.00010307195409312782,
        "epoch": 0.9953596287703016,
        "step": 7722
    },
    {
        "loss": 1.2537,
        "grad_norm": 2.711988687515259,
        "learning_rate": 0.0001030111418965709,
        "epoch": 0.9954885279711265,
        "step": 7723
    },
    {
        "loss": 2.1846,
        "grad_norm": 2.2220869064331055,
        "learning_rate": 0.00010295032858542548,
        "epoch": 0.9956174271719516,
        "step": 7724
    },
    {
        "loss": 1.6434,
        "grad_norm": 1.923853874206543,
        "learning_rate": 0.00010288951418220183,
        "epoch": 0.9957463263727765,
        "step": 7725
    },
    {
        "loss": 2.5456,
        "grad_norm": 1.6266056299209595,
        "learning_rate": 0.00010282869870941083,
        "epoch": 0.9958752255736014,
        "step": 7726
    },
    {
        "loss": 1.4079,
        "grad_norm": 3.9546005725860596,
        "learning_rate": 0.00010276788218956357,
        "epoch": 0.9960041247744263,
        "step": 7727
    },
    {
        "loss": 1.2332,
        "grad_norm": 2.5108988285064697,
        "learning_rate": 0.00010270706464517144,
        "epoch": 0.9961330239752514,
        "step": 7728
    },
    {
        "loss": 1.7034,
        "grad_norm": 1.781768798828125,
        "learning_rate": 0.00010264624609874656,
        "epoch": 0.9962619231760763,
        "step": 7729
    },
    {
        "loss": 2.0002,
        "grad_norm": 2.446800708770752,
        "learning_rate": 0.00010258542657280108,
        "epoch": 0.9963908223769012,
        "step": 7730
    },
    {
        "loss": 2.3966,
        "grad_norm": 1.4568238258361816,
        "learning_rate": 0.00010252460608984769,
        "epoch": 0.9965197215777262,
        "step": 7731
    },
    {
        "loss": 2.039,
        "grad_norm": 1.4490890502929688,
        "learning_rate": 0.00010246378467239935,
        "epoch": 0.9966486207785512,
        "step": 7732
    },
    {
        "loss": 2.0903,
        "grad_norm": 1.7083122730255127,
        "learning_rate": 0.00010240296234296928,
        "epoch": 0.9967775199793761,
        "step": 7733
    },
    {
        "loss": 0.9004,
        "grad_norm": 2.2218596935272217,
        "learning_rate": 0.00010234213912407147,
        "epoch": 0.9969064191802011,
        "step": 7734
    },
    {
        "loss": 1.7801,
        "grad_norm": 2.420180559158325,
        "learning_rate": 0.00010228131503821956,
        "epoch": 0.997035318381026,
        "step": 7735
    },
    {
        "loss": 1.5274,
        "grad_norm": 1.810054898262024,
        "learning_rate": 0.00010222049010792802,
        "epoch": 0.997164217581851,
        "step": 7736
    },
    {
        "loss": 1.7182,
        "grad_norm": 1.4170820713043213,
        "learning_rate": 0.00010215966435571167,
        "epoch": 0.997293116782676,
        "step": 7737
    },
    {
        "loss": 2.031,
        "grad_norm": 2.215838670730591,
        "learning_rate": 0.00010209883780408511,
        "epoch": 0.9974220159835009,
        "step": 7738
    },
    {
        "loss": 1.5179,
        "grad_norm": 2.456042766571045,
        "learning_rate": 0.00010203801047556387,
        "epoch": 0.9975509151843258,
        "step": 7739
    },
    {
        "loss": 2.2357,
        "grad_norm": 1.7586637735366821,
        "learning_rate": 0.00010197718239266337,
        "epoch": 0.9976798143851509,
        "step": 7740
    },
    {
        "loss": 1.9036,
        "grad_norm": 2.055851459503174,
        "learning_rate": 0.00010191635357789937,
        "epoch": 0.9978087135859758,
        "step": 7741
    },
    {
        "loss": 1.7457,
        "grad_norm": 1.6012004613876343,
        "learning_rate": 0.0001018555240537879,
        "epoch": 0.9979376127868007,
        "step": 7742
    },
    {
        "loss": 1.3919,
        "grad_norm": 3.094083547592163,
        "learning_rate": 0.00010179469384284548,
        "epoch": 0.9980665119876256,
        "step": 7743
    },
    {
        "loss": 1.5968,
        "grad_norm": 1.4627939462661743,
        "learning_rate": 0.00010173386296758858,
        "epoch": 0.9981954111884507,
        "step": 7744
    },
    {
        "loss": 1.9494,
        "grad_norm": 2.9222331047058105,
        "learning_rate": 0.00010167303145053399,
        "epoch": 0.9983243103892756,
        "step": 7745
    },
    {
        "loss": 2.114,
        "grad_norm": 2.161181688308716,
        "learning_rate": 0.00010161219931419894,
        "epoch": 0.9984532095901005,
        "step": 7746
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.298415422439575,
        "learning_rate": 0.00010155136658110063,
        "epoch": 0.9985821087909255,
        "step": 7747
    },
    {
        "loss": 0.9285,
        "grad_norm": 2.676795721054077,
        "learning_rate": 0.0001014905332737566,
        "epoch": 0.9987110079917505,
        "step": 7748
    },
    {
        "loss": 1.8199,
        "grad_norm": 2.638735771179199,
        "learning_rate": 0.00010142969941468461,
        "epoch": 0.9988399071925754,
        "step": 7749
    },
    {
        "loss": 1.5265,
        "grad_norm": 2.404189109802246,
        "learning_rate": 0.00010136886502640249,
        "epoch": 0.9989688063934004,
        "step": 7750
    },
    {
        "loss": 1.6256,
        "grad_norm": 1.999901294708252,
        "learning_rate": 0.00010130803013142853,
        "epoch": 0.9990977055942253,
        "step": 7751
    },
    {
        "loss": 1.1848,
        "grad_norm": 2.889782667160034,
        "learning_rate": 0.000101247194752281,
        "epoch": 0.9992266047950503,
        "step": 7752
    },
    {
        "loss": 1.4861,
        "grad_norm": 1.2644411325454712,
        "learning_rate": 0.00010118635891147837,
        "epoch": 0.9993555039958752,
        "step": 7753
    },
    {
        "loss": 1.908,
        "grad_norm": 1.8541784286499023,
        "learning_rate": 0.00010112552263153945,
        "epoch": 0.9994844031967002,
        "step": 7754
    },
    {
        "loss": 2.2881,
        "grad_norm": 1.3570001125335693,
        "learning_rate": 0.00010106468593498284,
        "epoch": 0.9996133023975251,
        "step": 7755
    },
    {
        "loss": 2.415,
        "grad_norm": 2.6012790203094482,
        "learning_rate": 0.00010100384884432774,
        "epoch": 0.9997422015983501,
        "step": 7756
    },
    {
        "loss": 2.3008,
        "grad_norm": 1.3941140174865723,
        "learning_rate": 0.00010094301138209324,
        "epoch": 0.9998711007991751,
        "step": 7757
    },
    {
        "loss": 1.9198,
        "grad_norm": 1.9425601959228516,
        "learning_rate": 0.00010088217357079855,
        "epoch": 1.0,
        "step": 7758
    },
    {
        "loss": 1.7551,
        "grad_norm": 2.0754008293151855,
        "learning_rate": 0.00010082133543296308,
        "epoch": 1.000128899200825,
        "step": 7759
    },
    {
        "loss": 0.9157,
        "grad_norm": 2.373415231704712,
        "learning_rate": 0.00010076049699110646,
        "epoch": 1.0002577984016499,
        "step": 7760
    },
    {
        "loss": 2.0974,
        "grad_norm": 1.159433364868164,
        "learning_rate": 0.00010069965826774823,
        "epoch": 1.000386697602475,
        "step": 7761
    },
    {
        "loss": 1.6973,
        "grad_norm": 2.1969478130340576,
        "learning_rate": 0.00010063881928540815,
        "epoch": 1.0005155968032997,
        "step": 7762
    },
    {
        "loss": 2.0194,
        "grad_norm": 1.2613744735717773,
        "learning_rate": 0.00010057798006660609,
        "epoch": 1.0006444960041248,
        "step": 7763
    },
    {
        "loss": 0.8596,
        "grad_norm": 2.386392593383789,
        "learning_rate": 0.00010051714063386199,
        "epoch": 1.0007733952049498,
        "step": 7764
    },
    {
        "loss": 1.592,
        "grad_norm": 2.1963534355163574,
        "learning_rate": 0.00010045630100969584,
        "epoch": 1.0009022944057746,
        "step": 7765
    },
    {
        "loss": 1.634,
        "grad_norm": 1.5367751121520996,
        "learning_rate": 0.00010039546121662764,
        "epoch": 1.0010311936065996,
        "step": 7766
    },
    {
        "loss": 1.5511,
        "grad_norm": 2.0124006271362305,
        "learning_rate": 0.00010033462127717759,
        "epoch": 1.0011600928074247,
        "step": 7767
    },
    {
        "loss": 1.3193,
        "grad_norm": 2.7641971111297607,
        "learning_rate": 0.00010027378121386593,
        "epoch": 1.0012889920082495,
        "step": 7768
    },
    {
        "loss": 2.0696,
        "grad_norm": 1.790076732635498,
        "learning_rate": 0.00010021294104921282,
        "epoch": 1.0014178912090745,
        "step": 7769
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.9911199808120728,
        "learning_rate": 0.00010015210080573854,
        "epoch": 1.0015467904098994,
        "step": 7770
    },
    {
        "loss": 1.9484,
        "grad_norm": 1.3967496156692505,
        "learning_rate": 0.00010009126050596344,
        "epoch": 1.0016756896107244,
        "step": 7771
    },
    {
        "loss": 1.5896,
        "grad_norm": 2.046945810317993,
        "learning_rate": 0.0001000304201724079,
        "epoch": 1.0018045888115494,
        "step": 7772
    },
    {
        "loss": 1.8653,
        "grad_norm": 2.7936830520629883,
        "learning_rate": 9.996957982759214e-05,
        "epoch": 1.0019334880123743,
        "step": 7773
    },
    {
        "loss": 1.7373,
        "grad_norm": 2.257378339767456,
        "learning_rate": 9.990873949403657e-05,
        "epoch": 1.0020623872131993,
        "step": 7774
    },
    {
        "loss": 0.917,
        "grad_norm": 4.131269931793213,
        "learning_rate": 9.98478991942615e-05,
        "epoch": 1.0021912864140243,
        "step": 7775
    },
    {
        "loss": 1.4332,
        "grad_norm": 2.868159294128418,
        "learning_rate": 9.97870589507872e-05,
        "epoch": 1.0023201856148491,
        "step": 7776
    },
    {
        "loss": 2.1144,
        "grad_norm": 1.8280689716339111,
        "learning_rate": 9.972621878613404e-05,
        "epoch": 1.0024490848156742,
        "step": 7777
    },
    {
        "loss": 1.8775,
        "grad_norm": 1.7395086288452148,
        "learning_rate": 9.966537872282238e-05,
        "epoch": 1.002577984016499,
        "step": 7778
    },
    {
        "loss": 2.0295,
        "grad_norm": 2.6186532974243164,
        "learning_rate": 9.960453878337244e-05,
        "epoch": 1.002706883217324,
        "step": 7779
    },
    {
        "loss": 2.256,
        "grad_norm": 2.198852300643921,
        "learning_rate": 9.954369899030419e-05,
        "epoch": 1.002835782418149,
        "step": 7780
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.5726524591445923,
        "learning_rate": 9.948285936613803e-05,
        "epoch": 1.002964681618974,
        "step": 7781
    },
    {
        "loss": 1.162,
        "grad_norm": 2.254011869430542,
        "learning_rate": 9.942201993339392e-05,
        "epoch": 1.003093580819799,
        "step": 7782
    },
    {
        "loss": 1.7436,
        "grad_norm": 2.0314173698425293,
        "learning_rate": 9.936118071459182e-05,
        "epoch": 1.003222480020624,
        "step": 7783
    },
    {
        "loss": 0.7042,
        "grad_norm": 4.596053123474121,
        "learning_rate": 9.930034173225185e-05,
        "epoch": 1.0033513792214488,
        "step": 7784
    },
    {
        "loss": 2.1084,
        "grad_norm": 1.4413408041000366,
        "learning_rate": 9.923950300889366e-05,
        "epoch": 1.0034802784222738,
        "step": 7785
    },
    {
        "loss": 1.4398,
        "grad_norm": 2.447291612625122,
        "learning_rate": 9.917866456703694e-05,
        "epoch": 1.0036091776230986,
        "step": 7786
    },
    {
        "loss": 2.3992,
        "grad_norm": 1.7899121046066284,
        "learning_rate": 9.911782642920146e-05,
        "epoch": 1.0037380768239237,
        "step": 7787
    },
    {
        "loss": 2.0141,
        "grad_norm": 1.93634033203125,
        "learning_rate": 9.905698861790679e-05,
        "epoch": 1.0038669760247487,
        "step": 7788
    },
    {
        "loss": 1.1103,
        "grad_norm": 2.553446054458618,
        "learning_rate": 9.899615115567227e-05,
        "epoch": 1.0039958752255735,
        "step": 7789
    },
    {
        "loss": 1.8526,
        "grad_norm": 2.8436639308929443,
        "learning_rate": 9.893531406501718e-05,
        "epoch": 1.0041247744263986,
        "step": 7790
    },
    {
        "loss": 2.0562,
        "grad_norm": 1.8023256063461304,
        "learning_rate": 9.887447736846064e-05,
        "epoch": 1.0042536736272236,
        "step": 7791
    },
    {
        "loss": 2.2228,
        "grad_norm": 2.071402072906494,
        "learning_rate": 9.881364108852166e-05,
        "epoch": 1.0043825728280484,
        "step": 7792
    },
    {
        "loss": 1.6191,
        "grad_norm": 2.438610315322876,
        "learning_rate": 9.875280524771902e-05,
        "epoch": 1.0045114720288735,
        "step": 7793
    },
    {
        "loss": 1.9288,
        "grad_norm": 1.5134762525558472,
        "learning_rate": 9.869196986857142e-05,
        "epoch": 1.0046403712296983,
        "step": 7794
    },
    {
        "loss": 1.6688,
        "grad_norm": 2.1362760066986084,
        "learning_rate": 9.863113497359748e-05,
        "epoch": 1.0047692704305233,
        "step": 7795
    },
    {
        "loss": 1.4827,
        "grad_norm": 1.802645206451416,
        "learning_rate": 9.857030058531551e-05,
        "epoch": 1.0048981696313484,
        "step": 7796
    },
    {
        "loss": 1.4887,
        "grad_norm": 2.667821168899536,
        "learning_rate": 9.850946672624343e-05,
        "epoch": 1.0050270688321732,
        "step": 7797
    },
    {
        "loss": 1.1296,
        "grad_norm": 2.297153949737549,
        "learning_rate": 9.844863341889941e-05,
        "epoch": 1.0051559680329982,
        "step": 7798
    },
    {
        "loss": 1.0145,
        "grad_norm": 1.424680233001709,
        "learning_rate": 9.838780068580107e-05,
        "epoch": 1.005284867233823,
        "step": 7799
    },
    {
        "loss": 1.9281,
        "grad_norm": 3.852419853210449,
        "learning_rate": 9.832696854946596e-05,
        "epoch": 1.005413766434648,
        "step": 7800
    },
    {
        "loss": 1.3109,
        "grad_norm": 2.1899402141571045,
        "learning_rate": 9.826613703241135e-05,
        "epoch": 1.0055426656354731,
        "step": 7801
    },
    {
        "loss": 1.946,
        "grad_norm": 1.6722612380981445,
        "learning_rate": 9.82053061571546e-05,
        "epoch": 1.005671564836298,
        "step": 7802
    },
    {
        "loss": 1.3407,
        "grad_norm": 3.080038070678711,
        "learning_rate": 9.814447594621212e-05,
        "epoch": 1.005800464037123,
        "step": 7803
    },
    {
        "loss": 0.7944,
        "grad_norm": 2.6503403186798096,
        "learning_rate": 9.808364642210065e-05,
        "epoch": 1.005929363237948,
        "step": 7804
    },
    {
        "loss": 1.1909,
        "grad_norm": 3.2975494861602783,
        "learning_rate": 9.802281760733666e-05,
        "epoch": 1.0060582624387728,
        "step": 7805
    },
    {
        "loss": 1.3838,
        "grad_norm": 1.648417353630066,
        "learning_rate": 9.796198952443615e-05,
        "epoch": 1.0061871616395979,
        "step": 7806
    },
    {
        "loss": 1.3286,
        "grad_norm": 2.4257240295410156,
        "learning_rate": 9.79011621959149e-05,
        "epoch": 1.0063160608404227,
        "step": 7807
    },
    {
        "loss": 1.2196,
        "grad_norm": 2.319929361343384,
        "learning_rate": 9.784033564428843e-05,
        "epoch": 1.0064449600412477,
        "step": 7808
    },
    {
        "loss": 1.5903,
        "grad_norm": 2.5533905029296875,
        "learning_rate": 9.7779509892072e-05,
        "epoch": 1.0065738592420728,
        "step": 7809
    },
    {
        "loss": 1.574,
        "grad_norm": 2.187194585800171,
        "learning_rate": 9.771868496178047e-05,
        "epoch": 1.0067027584428976,
        "step": 7810
    },
    {
        "loss": 1.615,
        "grad_norm": 1.727223515510559,
        "learning_rate": 9.765786087592856e-05,
        "epoch": 1.0068316576437226,
        "step": 7811
    },
    {
        "loss": 2.3184,
        "grad_norm": 2.0989456176757812,
        "learning_rate": 9.759703765703066e-05,
        "epoch": 1.0069605568445477,
        "step": 7812
    },
    {
        "loss": 1.3208,
        "grad_norm": 2.462705135345459,
        "learning_rate": 9.753621532760074e-05,
        "epoch": 1.0070894560453725,
        "step": 7813
    },
    {
        "loss": 1.5733,
        "grad_norm": 1.9838306903839111,
        "learning_rate": 9.747539391015234e-05,
        "epoch": 1.0072183552461975,
        "step": 7814
    },
    {
        "loss": 1.6363,
        "grad_norm": 2.5714452266693115,
        "learning_rate": 9.741457342719893e-05,
        "epoch": 1.0073472544470223,
        "step": 7815
    },
    {
        "loss": 1.7701,
        "grad_norm": 2.319126605987549,
        "learning_rate": 9.735375390125345e-05,
        "epoch": 1.0074761536478474,
        "step": 7816
    },
    {
        "loss": 1.6165,
        "grad_norm": 1.796899437904358,
        "learning_rate": 9.72929353548285e-05,
        "epoch": 1.0076050528486724,
        "step": 7817
    },
    {
        "loss": 1.5219,
        "grad_norm": 2.4609334468841553,
        "learning_rate": 9.72321178104364e-05,
        "epoch": 1.0077339520494972,
        "step": 7818
    },
    {
        "loss": 1.4171,
        "grad_norm": 2.504707098007202,
        "learning_rate": 9.717130129058926e-05,
        "epoch": 1.0078628512503223,
        "step": 7819
    },
    {
        "loss": 1.237,
        "grad_norm": 1.8236349821090698,
        "learning_rate": 9.71104858177982e-05,
        "epoch": 1.0079917504511473,
        "step": 7820
    },
    {
        "loss": 2.5992,
        "grad_norm": 1.4917240142822266,
        "learning_rate": 9.704967141457454e-05,
        "epoch": 1.0081206496519721,
        "step": 7821
    },
    {
        "loss": 2.2431,
        "grad_norm": 1.386838436126709,
        "learning_rate": 9.698885810342912e-05,
        "epoch": 1.0082495488527972,
        "step": 7822
    },
    {
        "loss": 1.5305,
        "grad_norm": 1.418285846710205,
        "learning_rate": 9.69280459068722e-05,
        "epoch": 1.008378448053622,
        "step": 7823
    },
    {
        "loss": 1.8458,
        "grad_norm": 1.8312572240829468,
        "learning_rate": 9.68672348474136e-05,
        "epoch": 1.008507347254447,
        "step": 7824
    },
    {
        "loss": 1.1174,
        "grad_norm": 3.012305974960327,
        "learning_rate": 9.680642494756308e-05,
        "epoch": 1.008636246455272,
        "step": 7825
    },
    {
        "loss": 1.2507,
        "grad_norm": 1.9605056047439575,
        "learning_rate": 9.674561622982949e-05,
        "epoch": 1.0087651456560969,
        "step": 7826
    },
    {
        "loss": 1.7076,
        "grad_norm": 2.5304229259490967,
        "learning_rate": 9.668480871672144e-05,
        "epoch": 1.008894044856922,
        "step": 7827
    },
    {
        "loss": 1.445,
        "grad_norm": 2.139338970184326,
        "learning_rate": 9.662400243074723e-05,
        "epoch": 1.009022944057747,
        "step": 7828
    },
    {
        "loss": 1.716,
        "grad_norm": 1.889532208442688,
        "learning_rate": 9.656319739441462e-05,
        "epoch": 1.0091518432585718,
        "step": 7829
    },
    {
        "loss": 1.3487,
        "grad_norm": 1.742870569229126,
        "learning_rate": 9.650239363023093e-05,
        "epoch": 1.0092807424593968,
        "step": 7830
    },
    {
        "loss": 1.1398,
        "grad_norm": 1.7503557205200195,
        "learning_rate": 9.644159116070273e-05,
        "epoch": 1.0094096416602216,
        "step": 7831
    },
    {
        "loss": 1.9437,
        "grad_norm": 2.252609968185425,
        "learning_rate": 9.638079000833654e-05,
        "epoch": 1.0095385408610467,
        "step": 7832
    },
    {
        "loss": 1.7636,
        "grad_norm": 2.0645570755004883,
        "learning_rate": 9.631999019563814e-05,
        "epoch": 1.0096674400618717,
        "step": 7833
    },
    {
        "loss": 2.0617,
        "grad_norm": 1.6732757091522217,
        "learning_rate": 9.625919174511283e-05,
        "epoch": 1.0097963392626965,
        "step": 7834
    },
    {
        "loss": 1.3147,
        "grad_norm": 3.1060869693756104,
        "learning_rate": 9.619839467926544e-05,
        "epoch": 1.0099252384635216,
        "step": 7835
    },
    {
        "loss": 2.4941,
        "grad_norm": 1.1386834383010864,
        "learning_rate": 9.613759902060058e-05,
        "epoch": 1.0100541376643464,
        "step": 7836
    },
    {
        "loss": 1.0555,
        "grad_norm": 2.359661817550659,
        "learning_rate": 9.607680479162162e-05,
        "epoch": 1.0101830368651714,
        "step": 7837
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.11210298538208,
        "learning_rate": 9.601601201483198e-05,
        "epoch": 1.0103119360659965,
        "step": 7838
    },
    {
        "loss": 1.6814,
        "grad_norm": 1.86351478099823,
        "learning_rate": 9.595522071273455e-05,
        "epoch": 1.0104408352668213,
        "step": 7839
    },
    {
        "loss": 1.9205,
        "grad_norm": 3.7073705196380615,
        "learning_rate": 9.589443090783138e-05,
        "epoch": 1.0105697344676463,
        "step": 7840
    },
    {
        "loss": 1.2454,
        "grad_norm": 3.679842472076416,
        "learning_rate": 9.583364262262406e-05,
        "epoch": 1.0106986336684713,
        "step": 7841
    },
    {
        "loss": 1.7673,
        "grad_norm": 3.060051441192627,
        "learning_rate": 9.57728558796139e-05,
        "epoch": 1.0108275328692962,
        "step": 7842
    },
    {
        "loss": 2.2239,
        "grad_norm": 2.6711502075195312,
        "learning_rate": 9.571207070130116e-05,
        "epoch": 1.0109564320701212,
        "step": 7843
    },
    {
        "loss": 1.0271,
        "grad_norm": 2.1701431274414062,
        "learning_rate": 9.565128711018576e-05,
        "epoch": 1.011085331270946,
        "step": 7844
    },
    {
        "loss": 1.7319,
        "grad_norm": 2.461016893386841,
        "learning_rate": 9.559050512876714e-05,
        "epoch": 1.011214230471771,
        "step": 7845
    },
    {
        "loss": 1.3694,
        "grad_norm": 1.4235587120056152,
        "learning_rate": 9.552972477954407e-05,
        "epoch": 1.011343129672596,
        "step": 7846
    },
    {
        "loss": 2.2921,
        "grad_norm": 1.7990458011627197,
        "learning_rate": 9.54689460850147e-05,
        "epoch": 1.011472028873421,
        "step": 7847
    },
    {
        "loss": 1.9746,
        "grad_norm": 2.457502603530884,
        "learning_rate": 9.540816906767637e-05,
        "epoch": 1.011600928074246,
        "step": 7848
    },
    {
        "loss": 1.2323,
        "grad_norm": 3.136681318283081,
        "learning_rate": 9.534739375002614e-05,
        "epoch": 1.011729827275071,
        "step": 7849
    },
    {
        "loss": 1.4775,
        "grad_norm": 1.9244518280029297,
        "learning_rate": 9.528662015456024e-05,
        "epoch": 1.0118587264758958,
        "step": 7850
    },
    {
        "loss": 1.6377,
        "grad_norm": 2.518298864364624,
        "learning_rate": 9.522584830377424e-05,
        "epoch": 1.0119876256767208,
        "step": 7851
    },
    {
        "loss": 1.8339,
        "grad_norm": 1.966167688369751,
        "learning_rate": 9.516507822016322e-05,
        "epoch": 1.0121165248775457,
        "step": 7852
    },
    {
        "loss": 1.8294,
        "grad_norm": 3.174560308456421,
        "learning_rate": 9.510430992622168e-05,
        "epoch": 1.0122454240783707,
        "step": 7853
    },
    {
        "loss": 1.1695,
        "grad_norm": 2.9758520126342773,
        "learning_rate": 9.504354344444288e-05,
        "epoch": 1.0123743232791957,
        "step": 7854
    },
    {
        "loss": 1.3507,
        "grad_norm": 2.7485525608062744,
        "learning_rate": 9.498277879732007e-05,
        "epoch": 1.0125032224800206,
        "step": 7855
    },
    {
        "loss": 1.3451,
        "grad_norm": 2.462315559387207,
        "learning_rate": 9.492201600734562e-05,
        "epoch": 1.0126321216808456,
        "step": 7856
    },
    {
        "loss": 2.3103,
        "grad_norm": 1.3821731805801392,
        "learning_rate": 9.486125509701106e-05,
        "epoch": 1.0127610208816706,
        "step": 7857
    },
    {
        "loss": 0.6144,
        "grad_norm": 2.3321280479431152,
        "learning_rate": 9.480049608880734e-05,
        "epoch": 1.0128899200824955,
        "step": 7858
    },
    {
        "loss": 2.1498,
        "grad_norm": 1.853347659111023,
        "learning_rate": 9.473973900522484e-05,
        "epoch": 1.0130188192833205,
        "step": 7859
    },
    {
        "loss": 1.8576,
        "grad_norm": 1.754721760749817,
        "learning_rate": 9.467898386875287e-05,
        "epoch": 1.0131477184841453,
        "step": 7860
    },
    {
        "loss": 1.1721,
        "grad_norm": 2.713369369506836,
        "learning_rate": 9.461823070188028e-05,
        "epoch": 1.0132766176849703,
        "step": 7861
    },
    {
        "loss": 0.7277,
        "grad_norm": 2.735255002975464,
        "learning_rate": 9.455747952709518e-05,
        "epoch": 1.0134055168857954,
        "step": 7862
    },
    {
        "loss": 1.5992,
        "grad_norm": 2.3510727882385254,
        "learning_rate": 9.4496730366885e-05,
        "epoch": 1.0135344160866202,
        "step": 7863
    },
    {
        "loss": 1.7039,
        "grad_norm": 2.2334771156311035,
        "learning_rate": 9.443598324373625e-05,
        "epoch": 1.0136633152874452,
        "step": 7864
    },
    {
        "loss": 1.8669,
        "grad_norm": 1.6934587955474854,
        "learning_rate": 9.437523818013474e-05,
        "epoch": 1.0137922144882703,
        "step": 7865
    },
    {
        "loss": 1.7536,
        "grad_norm": 2.111476182937622,
        "learning_rate": 9.431449519856559e-05,
        "epoch": 1.013921113689095,
        "step": 7866
    },
    {
        "loss": 0.9785,
        "grad_norm": 2.6260392665863037,
        "learning_rate": 9.425375432151308e-05,
        "epoch": 1.0140500128899201,
        "step": 7867
    },
    {
        "loss": 0.5421,
        "grad_norm": 2.4463284015655518,
        "learning_rate": 9.419301557146064e-05,
        "epoch": 1.014178912090745,
        "step": 7868
    },
    {
        "loss": 1.3794,
        "grad_norm": 1.9273059368133545,
        "learning_rate": 9.413227897089113e-05,
        "epoch": 1.01430781129157,
        "step": 7869
    },
    {
        "loss": 1.9727,
        "grad_norm": 2.419936418533325,
        "learning_rate": 9.407154454228667e-05,
        "epoch": 1.014436710492395,
        "step": 7870
    },
    {
        "loss": 1.4464,
        "grad_norm": 2.46598219871521,
        "learning_rate": 9.401081230812796e-05,
        "epoch": 1.0145656096932198,
        "step": 7871
    },
    {
        "loss": 1.3891,
        "grad_norm": 3.757674217224121,
        "learning_rate": 9.39500822908956e-05,
        "epoch": 1.0146945088940449,
        "step": 7872
    },
    {
        "loss": 1.014,
        "grad_norm": 3.6748673915863037,
        "learning_rate": 9.388935451306908e-05,
        "epoch": 1.0148234080948697,
        "step": 7873
    },
    {
        "loss": 1.8622,
        "grad_norm": 2.5545248985290527,
        "learning_rate": 9.382862899712706e-05,
        "epoch": 1.0149523072956947,
        "step": 7874
    },
    {
        "loss": 1.7801,
        "grad_norm": 2.072096586227417,
        "learning_rate": 9.376790576554731e-05,
        "epoch": 1.0150812064965198,
        "step": 7875
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.8371127843856812,
        "learning_rate": 9.370718484080701e-05,
        "epoch": 1.0152101056973446,
        "step": 7876
    },
    {
        "loss": 2.0979,
        "grad_norm": 1.8768954277038574,
        "learning_rate": 9.364646624538208e-05,
        "epoch": 1.0153390048981696,
        "step": 7877
    },
    {
        "loss": 1.4946,
        "grad_norm": 2.306666374206543,
        "learning_rate": 9.358575000174785e-05,
        "epoch": 1.0154679040989947,
        "step": 7878
    },
    {
        "loss": 1.6505,
        "grad_norm": 2.894681930541992,
        "learning_rate": 9.352503613237872e-05,
        "epoch": 1.0155968032998195,
        "step": 7879
    },
    {
        "loss": 1.6404,
        "grad_norm": 2.5501060485839844,
        "learning_rate": 9.346432465974836e-05,
        "epoch": 1.0157257025006445,
        "step": 7880
    },
    {
        "loss": 1.3124,
        "grad_norm": 2.3907222747802734,
        "learning_rate": 9.340361560632931e-05,
        "epoch": 1.0158546017014694,
        "step": 7881
    },
    {
        "loss": 2.2321,
        "grad_norm": 1.4694106578826904,
        "learning_rate": 9.334290899459328e-05,
        "epoch": 1.0159835009022944,
        "step": 7882
    },
    {
        "loss": 0.73,
        "grad_norm": 2.2937164306640625,
        "learning_rate": 9.328220484701117e-05,
        "epoch": 1.0161124001031194,
        "step": 7883
    },
    {
        "loss": 1.7508,
        "grad_norm": 3.0958526134490967,
        "learning_rate": 9.322150318605289e-05,
        "epoch": 1.0162412993039442,
        "step": 7884
    },
    {
        "loss": 1.3199,
        "grad_norm": 2.3696014881134033,
        "learning_rate": 9.316080403418737e-05,
        "epoch": 1.0163701985047693,
        "step": 7885
    },
    {
        "loss": 1.8685,
        "grad_norm": 2.47163987159729,
        "learning_rate": 9.310010741388276e-05,
        "epoch": 1.0164990977055943,
        "step": 7886
    },
    {
        "loss": 1.6927,
        "grad_norm": 2.528679609298706,
        "learning_rate": 9.303941334760627e-05,
        "epoch": 1.0166279969064191,
        "step": 7887
    },
    {
        "loss": 1.1955,
        "grad_norm": 2.238290309906006,
        "learning_rate": 9.29787218578241e-05,
        "epoch": 1.0167568961072442,
        "step": 7888
    },
    {
        "loss": 2.0275,
        "grad_norm": 1.2037498950958252,
        "learning_rate": 9.29180329670013e-05,
        "epoch": 1.016885795308069,
        "step": 7889
    },
    {
        "loss": 0.8646,
        "grad_norm": 1.9601436853408813,
        "learning_rate": 9.285734669760232e-05,
        "epoch": 1.017014694508894,
        "step": 7890
    },
    {
        "loss": 0.5264,
        "grad_norm": 3.0480246543884277,
        "learning_rate": 9.279666307209045e-05,
        "epoch": 1.017143593709719,
        "step": 7891
    },
    {
        "loss": 1.9061,
        "grad_norm": 2.0908539295196533,
        "learning_rate": 9.273598211292793e-05,
        "epoch": 1.017272492910544,
        "step": 7892
    },
    {
        "loss": 0.6453,
        "grad_norm": 2.724431276321411,
        "learning_rate": 9.26753038425763e-05,
        "epoch": 1.017401392111369,
        "step": 7893
    },
    {
        "loss": 1.4187,
        "grad_norm": 2.2282707691192627,
        "learning_rate": 9.261462828349574e-05,
        "epoch": 1.017530291312194,
        "step": 7894
    },
    {
        "loss": 1.7283,
        "grad_norm": 1.1685309410095215,
        "learning_rate": 9.255395545814558e-05,
        "epoch": 1.0176591905130188,
        "step": 7895
    },
    {
        "loss": 1.6895,
        "grad_norm": 2.122028112411499,
        "learning_rate": 9.249328538898418e-05,
        "epoch": 1.0177880897138438,
        "step": 7896
    },
    {
        "loss": 2.0392,
        "grad_norm": 1.4083309173583984,
        "learning_rate": 9.2432618098469e-05,
        "epoch": 1.0179169889146686,
        "step": 7897
    },
    {
        "loss": 1.912,
        "grad_norm": 1.4727174043655396,
        "learning_rate": 9.237195360905618e-05,
        "epoch": 1.0180458881154937,
        "step": 7898
    },
    {
        "loss": 1.5789,
        "grad_norm": 2.318182945251465,
        "learning_rate": 9.231129194320105e-05,
        "epoch": 1.0181747873163187,
        "step": 7899
    },
    {
        "loss": 1.4925,
        "grad_norm": 1.9719353914260864,
        "learning_rate": 9.225063312335776e-05,
        "epoch": 1.0183036865171435,
        "step": 7900
    },
    {
        "loss": 0.806,
        "grad_norm": 2.6698203086853027,
        "learning_rate": 9.218997717197946e-05,
        "epoch": 1.0184325857179686,
        "step": 7901
    },
    {
        "loss": 1.3338,
        "grad_norm": 2.298689126968384,
        "learning_rate": 9.21293241115182e-05,
        "epoch": 1.0185614849187936,
        "step": 7902
    },
    {
        "loss": 2.0349,
        "grad_norm": 2.757486581802368,
        "learning_rate": 9.206867396442503e-05,
        "epoch": 1.0186903841196184,
        "step": 7903
    },
    {
        "loss": 1.4338,
        "grad_norm": 2.505526065826416,
        "learning_rate": 9.200802675315e-05,
        "epoch": 1.0188192833204435,
        "step": 7904
    },
    {
        "loss": 1.4001,
        "grad_norm": 2.1648707389831543,
        "learning_rate": 9.194738250014194e-05,
        "epoch": 1.0189481825212683,
        "step": 7905
    },
    {
        "loss": 1.9461,
        "grad_norm": 2.3681631088256836,
        "learning_rate": 9.188674122784842e-05,
        "epoch": 1.0190770817220933,
        "step": 7906
    },
    {
        "loss": 2.0329,
        "grad_norm": 2.2357230186462402,
        "learning_rate": 9.182610295871626e-05,
        "epoch": 1.0192059809229184,
        "step": 7907
    },
    {
        "loss": 1.7941,
        "grad_norm": 1.9609510898590088,
        "learning_rate": 9.176546771519097e-05,
        "epoch": 1.0193348801237432,
        "step": 7908
    },
    {
        "loss": 0.9135,
        "grad_norm": 3.12235164642334,
        "learning_rate": 9.170483551971689e-05,
        "epoch": 1.0194637793245682,
        "step": 7909
    },
    {
        "loss": 2.2567,
        "grad_norm": 3.080970048904419,
        "learning_rate": 9.164420639473754e-05,
        "epoch": 1.019592678525393,
        "step": 7910
    },
    {
        "loss": 1.8206,
        "grad_norm": 1.8060531616210938,
        "learning_rate": 9.158358036269488e-05,
        "epoch": 1.019721577726218,
        "step": 7911
    },
    {
        "loss": 1.653,
        "grad_norm": 1.5324510335922241,
        "learning_rate": 9.152295744602988e-05,
        "epoch": 1.0198504769270431,
        "step": 7912
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.6776323318481445,
        "learning_rate": 9.14623376671825e-05,
        "epoch": 1.019979376127868,
        "step": 7913
    },
    {
        "loss": 1.806,
        "grad_norm": 2.221808671951294,
        "learning_rate": 9.140172104859152e-05,
        "epoch": 1.020108275328693,
        "step": 7914
    },
    {
        "loss": 2.3908,
        "grad_norm": 1.6887072324752808,
        "learning_rate": 9.134110761269437e-05,
        "epoch": 1.020237174529518,
        "step": 7915
    },
    {
        "loss": 1.8989,
        "grad_norm": 3.2986700534820557,
        "learning_rate": 9.128049738192743e-05,
        "epoch": 1.0203660737303428,
        "step": 7916
    },
    {
        "loss": 1.4826,
        "grad_norm": 2.07120680809021,
        "learning_rate": 9.121989037872584e-05,
        "epoch": 1.0204949729311679,
        "step": 7917
    },
    {
        "loss": 1.3105,
        "grad_norm": 2.775505304336548,
        "learning_rate": 9.115928662552357e-05,
        "epoch": 1.0206238721319927,
        "step": 7918
    },
    {
        "loss": 1.9728,
        "grad_norm": 2.786362886428833,
        "learning_rate": 9.109868614475335e-05,
        "epoch": 1.0207527713328177,
        "step": 7919
    },
    {
        "loss": 1.5649,
        "grad_norm": 2.0748605728149414,
        "learning_rate": 9.103808895884678e-05,
        "epoch": 1.0208816705336428,
        "step": 7920
    },
    {
        "loss": 1.7164,
        "grad_norm": 1.8597419261932373,
        "learning_rate": 9.097749509023428e-05,
        "epoch": 1.0210105697344676,
        "step": 7921
    },
    {
        "loss": 1.8924,
        "grad_norm": 1.7329609394073486,
        "learning_rate": 9.091690456134494e-05,
        "epoch": 1.0211394689352926,
        "step": 7922
    },
    {
        "loss": 1.6097,
        "grad_norm": 2.117201566696167,
        "learning_rate": 9.085631739460643e-05,
        "epoch": 1.0212683681361177,
        "step": 7923
    },
    {
        "loss": 1.5531,
        "grad_norm": 2.356830358505249,
        "learning_rate": 9.07957336124456e-05,
        "epoch": 1.0213972673369425,
        "step": 7924
    },
    {
        "loss": 1.6557,
        "grad_norm": 3.4709091186523438,
        "learning_rate": 9.073515323728775e-05,
        "epoch": 1.0215261665377675,
        "step": 7925
    },
    {
        "loss": 1.8932,
        "grad_norm": 1.286119818687439,
        "learning_rate": 9.06745762915569e-05,
        "epoch": 1.0216550657385923,
        "step": 7926
    },
    {
        "loss": 1.4269,
        "grad_norm": 3.8975257873535156,
        "learning_rate": 9.061400279767601e-05,
        "epoch": 1.0217839649394174,
        "step": 7927
    },
    {
        "loss": 1.5938,
        "grad_norm": 2.5408215522766113,
        "learning_rate": 9.055343277806683e-05,
        "epoch": 1.0219128641402424,
        "step": 7928
    },
    {
        "loss": 2.4037,
        "grad_norm": 1.2065544128417969,
        "learning_rate": 9.04928662551492e-05,
        "epoch": 1.0220417633410672,
        "step": 7929
    },
    {
        "loss": 1.0258,
        "grad_norm": 2.203899621963501,
        "learning_rate": 9.043230325134236e-05,
        "epoch": 1.0221706625418923,
        "step": 7930
    },
    {
        "loss": 1.8409,
        "grad_norm": 1.217591404914856,
        "learning_rate": 9.037174378906402e-05,
        "epoch": 1.0222995617427173,
        "step": 7931
    },
    {
        "loss": 1.3065,
        "grad_norm": 2.706144332885742,
        "learning_rate": 9.031118789073053e-05,
        "epoch": 1.0224284609435421,
        "step": 7932
    },
    {
        "loss": 1.2969,
        "grad_norm": 5.255606651306152,
        "learning_rate": 9.025063557875691e-05,
        "epoch": 1.0225573601443672,
        "step": 7933
    },
    {
        "loss": 1.6787,
        "grad_norm": 2.574427366256714,
        "learning_rate": 9.01900868755569e-05,
        "epoch": 1.022686259345192,
        "step": 7934
    },
    {
        "loss": 1.8862,
        "grad_norm": 2.202481746673584,
        "learning_rate": 9.012954180354288e-05,
        "epoch": 1.022815158546017,
        "step": 7935
    },
    {
        "loss": 1.8225,
        "grad_norm": 1.600670576095581,
        "learning_rate": 9.006900038512584e-05,
        "epoch": 1.022944057746842,
        "step": 7936
    },
    {
        "loss": 2.0339,
        "grad_norm": 1.7835763692855835,
        "learning_rate": 9.000846264271553e-05,
        "epoch": 1.0230729569476669,
        "step": 7937
    },
    {
        "loss": 1.32,
        "grad_norm": 2.557889223098755,
        "learning_rate": 8.994792859872036e-05,
        "epoch": 1.023201856148492,
        "step": 7938
    },
    {
        "loss": 1.5476,
        "grad_norm": 2.5564939975738525,
        "learning_rate": 8.988739827554731e-05,
        "epoch": 1.023330755349317,
        "step": 7939
    },
    {
        "loss": 1.432,
        "grad_norm": 1.679050326347351,
        "learning_rate": 8.98268716956017e-05,
        "epoch": 1.0234596545501418,
        "step": 7940
    },
    {
        "loss": 2.0984,
        "grad_norm": 1.3750264644622803,
        "learning_rate": 8.976634888128803e-05,
        "epoch": 1.0235885537509668,
        "step": 7941
    },
    {
        "loss": 2.2316,
        "grad_norm": 1.9172770977020264,
        "learning_rate": 8.970582985500897e-05,
        "epoch": 1.0237174529517916,
        "step": 7942
    },
    {
        "loss": 2.4565,
        "grad_norm": 1.8159204721450806,
        "learning_rate": 8.964531463916586e-05,
        "epoch": 1.0238463521526167,
        "step": 7943
    },
    {
        "loss": 1.555,
        "grad_norm": 1.8333427906036377,
        "learning_rate": 8.958480325615883e-05,
        "epoch": 1.0239752513534417,
        "step": 7944
    },
    {
        "loss": 1.4236,
        "grad_norm": 1.6092478036880493,
        "learning_rate": 8.952429572838657e-05,
        "epoch": 1.0241041505542665,
        "step": 7945
    },
    {
        "loss": 1.7397,
        "grad_norm": 2.679786443710327,
        "learning_rate": 8.946379207824585e-05,
        "epoch": 1.0242330497550916,
        "step": 7946
    },
    {
        "loss": 2.0762,
        "grad_norm": 2.015146255493164,
        "learning_rate": 8.940329232813263e-05,
        "epoch": 1.0243619489559164,
        "step": 7947
    },
    {
        "loss": 1.1429,
        "grad_norm": 2.547994613647461,
        "learning_rate": 8.934279650044119e-05,
        "epoch": 1.0244908481567414,
        "step": 7948
    },
    {
        "loss": 1.3141,
        "grad_norm": 2.199091672897339,
        "learning_rate": 8.928230461756434e-05,
        "epoch": 1.0246197473575664,
        "step": 7949
    },
    {
        "loss": 1.3875,
        "grad_norm": 1.857905626296997,
        "learning_rate": 8.922181670189331e-05,
        "epoch": 1.0247486465583913,
        "step": 7950
    },
    {
        "loss": 1.3548,
        "grad_norm": 1.4387104511260986,
        "learning_rate": 8.916133277581824e-05,
        "epoch": 1.0248775457592163,
        "step": 7951
    },
    {
        "loss": 1.3917,
        "grad_norm": 2.400562047958374,
        "learning_rate": 8.910085286172734e-05,
        "epoch": 1.0250064449600413,
        "step": 7952
    },
    {
        "loss": 1.6704,
        "grad_norm": 2.1360206604003906,
        "learning_rate": 8.904037698200746e-05,
        "epoch": 1.0251353441608662,
        "step": 7953
    },
    {
        "loss": 1.3538,
        "grad_norm": 2.8284056186676025,
        "learning_rate": 8.897990515904422e-05,
        "epoch": 1.0252642433616912,
        "step": 7954
    },
    {
        "loss": 2.5897,
        "grad_norm": 1.2149897813796997,
        "learning_rate": 8.891943741522154e-05,
        "epoch": 1.025393142562516,
        "step": 7955
    },
    {
        "loss": 1.5925,
        "grad_norm": 2.399057626724243,
        "learning_rate": 8.885897377292184e-05,
        "epoch": 1.025522041763341,
        "step": 7956
    },
    {
        "loss": 1.4958,
        "grad_norm": 2.189920663833618,
        "learning_rate": 8.879851425452588e-05,
        "epoch": 1.025650940964166,
        "step": 7957
    },
    {
        "loss": 1.8116,
        "grad_norm": 2.065427303314209,
        "learning_rate": 8.873805888241322e-05,
        "epoch": 1.025779840164991,
        "step": 7958
    },
    {
        "loss": 2.0485,
        "grad_norm": 1.5274351835250854,
        "learning_rate": 8.867760767896159e-05,
        "epoch": 1.025908739365816,
        "step": 7959
    },
    {
        "loss": 1.3862,
        "grad_norm": 3.3609774112701416,
        "learning_rate": 8.861716066654725e-05,
        "epoch": 1.026037638566641,
        "step": 7960
    },
    {
        "loss": 1.8909,
        "grad_norm": 1.6007914543151855,
        "learning_rate": 8.855671786754504e-05,
        "epoch": 1.0261665377674658,
        "step": 7961
    },
    {
        "loss": 1.2643,
        "grad_norm": 1.6674565076828003,
        "learning_rate": 8.849627930432834e-05,
        "epoch": 1.0262954369682908,
        "step": 7962
    },
    {
        "loss": 1.5202,
        "grad_norm": 2.133765935897827,
        "learning_rate": 8.843584499926835e-05,
        "epoch": 1.0264243361691157,
        "step": 7963
    },
    {
        "loss": 1.8543,
        "grad_norm": 2.482175350189209,
        "learning_rate": 8.837541497473534e-05,
        "epoch": 1.0265532353699407,
        "step": 7964
    },
    {
        "loss": 1.6265,
        "grad_norm": 2.062587261199951,
        "learning_rate": 8.831498925309782e-05,
        "epoch": 1.0266821345707657,
        "step": 7965
    },
    {
        "loss": 1.789,
        "grad_norm": 2.4295647144317627,
        "learning_rate": 8.825456785672261e-05,
        "epoch": 1.0268110337715906,
        "step": 7966
    },
    {
        "loss": 1.4217,
        "grad_norm": 2.142930030822754,
        "learning_rate": 8.819415080797489e-05,
        "epoch": 1.0269399329724156,
        "step": 7967
    },
    {
        "loss": 1.8985,
        "grad_norm": 1.629417896270752,
        "learning_rate": 8.813373812921853e-05,
        "epoch": 1.0270688321732406,
        "step": 7968
    },
    {
        "loss": 2.5176,
        "grad_norm": 1.477325677871704,
        "learning_rate": 8.807332984281538e-05,
        "epoch": 1.0271977313740654,
        "step": 7969
    },
    {
        "loss": 1.4508,
        "grad_norm": 2.335446357727051,
        "learning_rate": 8.801292597112581e-05,
        "epoch": 1.0273266305748905,
        "step": 7970
    },
    {
        "loss": 1.253,
        "grad_norm": 1.398478627204895,
        "learning_rate": 8.795252653650873e-05,
        "epoch": 1.0274555297757153,
        "step": 7971
    },
    {
        "loss": 2.1179,
        "grad_norm": 2.809992551803589,
        "learning_rate": 8.78921315613213e-05,
        "epoch": 1.0275844289765403,
        "step": 7972
    },
    {
        "loss": 1.1991,
        "grad_norm": 2.1159543991088867,
        "learning_rate": 8.7831741067919e-05,
        "epoch": 1.0277133281773654,
        "step": 7973
    },
    {
        "loss": 1.6358,
        "grad_norm": 1.778306245803833,
        "learning_rate": 8.777135507865552e-05,
        "epoch": 1.0278422273781902,
        "step": 7974
    },
    {
        "loss": 1.9943,
        "grad_norm": 2.339921712875366,
        "learning_rate": 8.771097361588316e-05,
        "epoch": 1.0279711265790152,
        "step": 7975
    },
    {
        "loss": 0.8549,
        "grad_norm": 1.8075485229492188,
        "learning_rate": 8.765059670195236e-05,
        "epoch": 1.0281000257798403,
        "step": 7976
    },
    {
        "loss": 1.3664,
        "grad_norm": 2.3528707027435303,
        "learning_rate": 8.759022435921185e-05,
        "epoch": 1.028228924980665,
        "step": 7977
    },
    {
        "loss": 1.7088,
        "grad_norm": 2.659120559692383,
        "learning_rate": 8.75298566100088e-05,
        "epoch": 1.0283578241814901,
        "step": 7978
    },
    {
        "loss": 1.2518,
        "grad_norm": 2.5606367588043213,
        "learning_rate": 8.746949347668887e-05,
        "epoch": 1.028486723382315,
        "step": 7979
    },
    {
        "loss": 2.0058,
        "grad_norm": 1.5388195514678955,
        "learning_rate": 8.740913498159531e-05,
        "epoch": 1.02861562258314,
        "step": 7980
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.584256887435913,
        "learning_rate": 8.734878114707028e-05,
        "epoch": 1.028744521783965,
        "step": 7981
    },
    {
        "loss": 1.8132,
        "grad_norm": 1.984829068183899,
        "learning_rate": 8.728843199545417e-05,
        "epoch": 1.0288734209847898,
        "step": 7982
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.1108014583587646,
        "learning_rate": 8.722808754908541e-05,
        "epoch": 1.0290023201856149,
        "step": 7983
    },
    {
        "loss": 2.137,
        "grad_norm": 2.095276117324829,
        "learning_rate": 8.716774783030071e-05,
        "epoch": 1.0291312193864397,
        "step": 7984
    },
    {
        "loss": 0.9387,
        "grad_norm": 1.9496058225631714,
        "learning_rate": 8.710741286143531e-05,
        "epoch": 1.0292601185872647,
        "step": 7985
    },
    {
        "loss": 2.2813,
        "grad_norm": 1.9314453601837158,
        "learning_rate": 8.70470826648223e-05,
        "epoch": 1.0293890177880898,
        "step": 7986
    },
    {
        "loss": 1.7431,
        "grad_norm": 3.631524085998535,
        "learning_rate": 8.698675726279315e-05,
        "epoch": 1.0295179169889146,
        "step": 7987
    },
    {
        "loss": 1.5195,
        "grad_norm": 2.7046072483062744,
        "learning_rate": 8.69264366776777e-05,
        "epoch": 1.0296468161897396,
        "step": 7988
    },
    {
        "loss": 1.7244,
        "grad_norm": 2.142559289932251,
        "learning_rate": 8.686612093180395e-05,
        "epoch": 1.0297757153905647,
        "step": 7989
    },
    {
        "loss": 1.3259,
        "grad_norm": 2.724270820617676,
        "learning_rate": 8.680581004749801e-05,
        "epoch": 1.0299046145913895,
        "step": 7990
    },
    {
        "loss": 1.2454,
        "grad_norm": 2.864931344985962,
        "learning_rate": 8.674550404708422e-05,
        "epoch": 1.0300335137922145,
        "step": 7991
    },
    {
        "loss": 1.5894,
        "grad_norm": 2.3134233951568604,
        "learning_rate": 8.668520295288513e-05,
        "epoch": 1.0301624129930393,
        "step": 7992
    },
    {
        "loss": 1.9437,
        "grad_norm": 2.1516976356506348,
        "learning_rate": 8.662490678722151e-05,
        "epoch": 1.0302913121938644,
        "step": 7993
    },
    {
        "loss": 1.6863,
        "grad_norm": 1.757102370262146,
        "learning_rate": 8.65646155724122e-05,
        "epoch": 1.0304202113946894,
        "step": 7994
    },
    {
        "loss": 1.745,
        "grad_norm": 1.753846287727356,
        "learning_rate": 8.650432933077428e-05,
        "epoch": 1.0305491105955142,
        "step": 7995
    },
    {
        "loss": 1.8742,
        "grad_norm": 2.8486077785491943,
        "learning_rate": 8.644404808462328e-05,
        "epoch": 1.0306780097963393,
        "step": 7996
    },
    {
        "loss": 1.0936,
        "grad_norm": 1.3176846504211426,
        "learning_rate": 8.638377185627215e-05,
        "epoch": 1.0308069089971643,
        "step": 7997
    },
    {
        "loss": 1.9836,
        "grad_norm": 1.4529300928115845,
        "learning_rate": 8.632350066803259e-05,
        "epoch": 1.0309358081979891,
        "step": 7998
    },
    {
        "loss": 1.6881,
        "grad_norm": 2.0425145626068115,
        "learning_rate": 8.62632345422144e-05,
        "epoch": 1.0310647073988142,
        "step": 7999
    },
    {
        "loss": 1.1878,
        "grad_norm": 2.4620842933654785,
        "learning_rate": 8.620297350112524e-05,
        "epoch": 1.031193606599639,
        "step": 8000
    },
    {
        "loss": 1.3698,
        "grad_norm": 2.577070713043213,
        "learning_rate": 8.614271756707102e-05,
        "epoch": 1.031322505800464,
        "step": 8001
    },
    {
        "loss": 1.4451,
        "grad_norm": 2.6838674545288086,
        "learning_rate": 8.608246676235592e-05,
        "epoch": 1.031451405001289,
        "step": 8002
    },
    {
        "loss": 2.0492,
        "grad_norm": 1.8347736597061157,
        "learning_rate": 8.602222110928184e-05,
        "epoch": 1.0315803042021139,
        "step": 8003
    },
    {
        "loss": 1.5324,
        "grad_norm": 2.6094512939453125,
        "learning_rate": 8.596198063014906e-05,
        "epoch": 1.031709203402939,
        "step": 8004
    },
    {
        "loss": 1.5462,
        "grad_norm": 1.2214386463165283,
        "learning_rate": 8.590174534725588e-05,
        "epoch": 1.031838102603764,
        "step": 8005
    },
    {
        "loss": 2.2844,
        "grad_norm": 1.314810037612915,
        "learning_rate": 8.584151528289881e-05,
        "epoch": 1.0319670018045888,
        "step": 8006
    },
    {
        "loss": 1.6259,
        "grad_norm": 2.49150013923645,
        "learning_rate": 8.578129045937218e-05,
        "epoch": 1.0320959010054138,
        "step": 8007
    },
    {
        "loss": 1.7097,
        "grad_norm": 2.7815518379211426,
        "learning_rate": 8.57210708989685e-05,
        "epoch": 1.0322248002062386,
        "step": 8008
    },
    {
        "loss": 1.1348,
        "grad_norm": 4.557991027832031,
        "learning_rate": 8.566085662397832e-05,
        "epoch": 1.0323536994070637,
        "step": 8009
    },
    {
        "loss": 1.633,
        "grad_norm": 2.9884092807769775,
        "learning_rate": 8.560064765669028e-05,
        "epoch": 1.0324825986078887,
        "step": 8010
    },
    {
        "loss": 1.5152,
        "grad_norm": 2.0217978954315186,
        "learning_rate": 8.55404440193909e-05,
        "epoch": 1.0326114978087135,
        "step": 8011
    },
    {
        "loss": 1.9615,
        "grad_norm": 2.29435133934021,
        "learning_rate": 8.548024573436495e-05,
        "epoch": 1.0327403970095386,
        "step": 8012
    },
    {
        "loss": 1.3032,
        "grad_norm": 2.044612407684326,
        "learning_rate": 8.542005282389515e-05,
        "epoch": 1.0328692962103636,
        "step": 8013
    },
    {
        "loss": 1.599,
        "grad_norm": 3.0571067333221436,
        "learning_rate": 8.535986531026219e-05,
        "epoch": 1.0329981954111884,
        "step": 8014
    },
    {
        "loss": 1.4637,
        "grad_norm": 2.3687610626220703,
        "learning_rate": 8.52996832157446e-05,
        "epoch": 1.0331270946120135,
        "step": 8015
    },
    {
        "loss": 1.3991,
        "grad_norm": 2.7619893550872803,
        "learning_rate": 8.523950656261925e-05,
        "epoch": 1.0332559938128383,
        "step": 8016
    },
    {
        "loss": 0.7595,
        "grad_norm": 2.5653698444366455,
        "learning_rate": 8.517933537316071e-05,
        "epoch": 1.0333848930136633,
        "step": 8017
    },
    {
        "loss": 2.381,
        "grad_norm": 1.3188072443008423,
        "learning_rate": 8.511916966964161e-05,
        "epoch": 1.0335137922144884,
        "step": 8018
    },
    {
        "loss": 1.7431,
        "grad_norm": 2.3657915592193604,
        "learning_rate": 8.505900947433279e-05,
        "epoch": 1.0336426914153132,
        "step": 8019
    },
    {
        "loss": 1.2331,
        "grad_norm": 2.083097219467163,
        "learning_rate": 8.499885480950256e-05,
        "epoch": 1.0337715906161382,
        "step": 8020
    },
    {
        "loss": 1.3839,
        "grad_norm": 1.5584330558776855,
        "learning_rate": 8.493870569741751e-05,
        "epoch": 1.033900489816963,
        "step": 8021
    },
    {
        "loss": 1.7741,
        "grad_norm": 1.6338306665420532,
        "learning_rate": 8.487856216034219e-05,
        "epoch": 1.034029389017788,
        "step": 8022
    },
    {
        "loss": 1.5843,
        "grad_norm": 2.4498291015625,
        "learning_rate": 8.481842422053905e-05,
        "epoch": 1.034158288218613,
        "step": 8023
    },
    {
        "loss": 1.6366,
        "grad_norm": 1.3607019186019897,
        "learning_rate": 8.475829190026839e-05,
        "epoch": 1.034287187419438,
        "step": 8024
    },
    {
        "loss": 0.8422,
        "grad_norm": 3.0913913249969482,
        "learning_rate": 8.469816522178846e-05,
        "epoch": 1.034416086620263,
        "step": 8025
    },
    {
        "loss": 1.7323,
        "grad_norm": 2.1995739936828613,
        "learning_rate": 8.463804420735547e-05,
        "epoch": 1.034544985821088,
        "step": 8026
    },
    {
        "loss": 1.5899,
        "grad_norm": 2.5749852657318115,
        "learning_rate": 8.457792887922346e-05,
        "epoch": 1.0346738850219128,
        "step": 8027
    },
    {
        "loss": 2.0307,
        "grad_norm": 2.685319423675537,
        "learning_rate": 8.451781925964433e-05,
        "epoch": 1.0348027842227379,
        "step": 8028
    },
    {
        "loss": 1.7864,
        "grad_norm": 2.358917474746704,
        "learning_rate": 8.445771537086806e-05,
        "epoch": 1.0349316834235627,
        "step": 8029
    },
    {
        "loss": 1.8048,
        "grad_norm": 3.468247890472412,
        "learning_rate": 8.43976172351424e-05,
        "epoch": 1.0350605826243877,
        "step": 8030
    },
    {
        "loss": 1.1011,
        "grad_norm": 2.5668561458587646,
        "learning_rate": 8.4337524874713e-05,
        "epoch": 1.0351894818252128,
        "step": 8031
    },
    {
        "loss": 1.9895,
        "grad_norm": 2.8093514442443848,
        "learning_rate": 8.427743831182314e-05,
        "epoch": 1.0353183810260376,
        "step": 8032
    },
    {
        "loss": 1.061,
        "grad_norm": 3.322746753692627,
        "learning_rate": 8.42173575687143e-05,
        "epoch": 1.0354472802268626,
        "step": 8033
    },
    {
        "loss": 1.5724,
        "grad_norm": 3.749342918395996,
        "learning_rate": 8.415728266762564e-05,
        "epoch": 1.0355761794276876,
        "step": 8034
    },
    {
        "loss": 2.3274,
        "grad_norm": 2.3939733505249023,
        "learning_rate": 8.409721363079406e-05,
        "epoch": 1.0357050786285125,
        "step": 8035
    },
    {
        "loss": 2.0454,
        "grad_norm": 1.634129524230957,
        "learning_rate": 8.40371504804546e-05,
        "epoch": 1.0358339778293375,
        "step": 8036
    },
    {
        "loss": 1.7017,
        "grad_norm": 2.7280964851379395,
        "learning_rate": 8.397709323883977e-05,
        "epoch": 1.0359628770301623,
        "step": 8037
    },
    {
        "loss": 1.3717,
        "grad_norm": 2.1875076293945312,
        "learning_rate": 8.391704192818002e-05,
        "epoch": 1.0360917762309874,
        "step": 8038
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.4118494987487793,
        "learning_rate": 8.385699657070365e-05,
        "epoch": 1.0362206754318124,
        "step": 8039
    },
    {
        "loss": 1.9817,
        "grad_norm": 1.878490924835205,
        "learning_rate": 8.379695718863686e-05,
        "epoch": 1.0363495746326372,
        "step": 8040
    },
    {
        "loss": 1.7203,
        "grad_norm": 2.146740198135376,
        "learning_rate": 8.373692380420342e-05,
        "epoch": 1.0364784738334623,
        "step": 8041
    },
    {
        "loss": 0.7028,
        "grad_norm": 1.8761341571807861,
        "learning_rate": 8.367689643962497e-05,
        "epoch": 1.0366073730342873,
        "step": 8042
    },
    {
        "loss": 2.3117,
        "grad_norm": 1.6628038883209229,
        "learning_rate": 8.361687511712093e-05,
        "epoch": 1.036736272235112,
        "step": 8043
    },
    {
        "loss": 1.5122,
        "grad_norm": 2.4572672843933105,
        "learning_rate": 8.355685985890849e-05,
        "epoch": 1.0368651714359371,
        "step": 8044
    },
    {
        "loss": 1.5206,
        "grad_norm": 2.130671977996826,
        "learning_rate": 8.34968506872025e-05,
        "epoch": 1.036994070636762,
        "step": 8045
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.2107062339782715,
        "learning_rate": 8.343684762421572e-05,
        "epoch": 1.037122969837587,
        "step": 8046
    },
    {
        "loss": 1.7767,
        "grad_norm": 1.9547138214111328,
        "learning_rate": 8.33768506921586e-05,
        "epoch": 1.037251869038412,
        "step": 8047
    },
    {
        "loss": 1.5055,
        "grad_norm": 3.0153326988220215,
        "learning_rate": 8.331685991323936e-05,
        "epoch": 1.0373807682392369,
        "step": 8048
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.4177260398864746,
        "learning_rate": 8.325687530966362e-05,
        "epoch": 1.037509667440062,
        "step": 8049
    },
    {
        "loss": 1.6172,
        "grad_norm": 2.248544931411743,
        "learning_rate": 8.319689690363515e-05,
        "epoch": 1.037638566640887,
        "step": 8050
    },
    {
        "loss": 1.6431,
        "grad_norm": 1.4464131593704224,
        "learning_rate": 8.313692471735523e-05,
        "epoch": 1.0377674658417118,
        "step": 8051
    },
    {
        "loss": 2.3263,
        "grad_norm": 1.4688180685043335,
        "learning_rate": 8.307695877302273e-05,
        "epoch": 1.0378963650425368,
        "step": 8052
    },
    {
        "loss": 1.702,
        "grad_norm": 1.4926012754440308,
        "learning_rate": 8.301699909283443e-05,
        "epoch": 1.0380252642433616,
        "step": 8053
    },
    {
        "loss": 1.7186,
        "grad_norm": 3.230570077896118,
        "learning_rate": 8.29570456989849e-05,
        "epoch": 1.0381541634441867,
        "step": 8054
    },
    {
        "loss": 1.2299,
        "grad_norm": 1.7199623584747314,
        "learning_rate": 8.289709861366574e-05,
        "epoch": 1.0382830626450117,
        "step": 8055
    },
    {
        "loss": 1.4815,
        "grad_norm": 1.6884733438491821,
        "learning_rate": 8.283715785906688e-05,
        "epoch": 1.0384119618458365,
        "step": 8056
    },
    {
        "loss": 1.8431,
        "grad_norm": 2.6702880859375,
        "learning_rate": 8.277722345737571e-05,
        "epoch": 1.0385408610466615,
        "step": 8057
    },
    {
        "loss": 1.5664,
        "grad_norm": 2.8008673191070557,
        "learning_rate": 8.271729543077717e-05,
        "epoch": 1.0386697602474864,
        "step": 8058
    },
    {
        "loss": 1.6902,
        "grad_norm": 2.309434175491333,
        "learning_rate": 8.265737380145395e-05,
        "epoch": 1.0387986594483114,
        "step": 8059
    },
    {
        "loss": 1.6174,
        "grad_norm": 2.15842604637146,
        "learning_rate": 8.259745859158629e-05,
        "epoch": 1.0389275586491364,
        "step": 8060
    },
    {
        "loss": 1.3943,
        "grad_norm": 2.800467014312744,
        "learning_rate": 8.253754982335208e-05,
        "epoch": 1.0390564578499613,
        "step": 8061
    },
    {
        "loss": 0.9213,
        "grad_norm": 2.268195867538452,
        "learning_rate": 8.24776475189268e-05,
        "epoch": 1.0391853570507863,
        "step": 8062
    },
    {
        "loss": 2.1537,
        "grad_norm": 3.1633613109588623,
        "learning_rate": 8.24177517004836e-05,
        "epoch": 1.0393142562516113,
        "step": 8063
    },
    {
        "loss": 1.1731,
        "grad_norm": 2.4312119483947754,
        "learning_rate": 8.235786239019331e-05,
        "epoch": 1.0394431554524362,
        "step": 8064
    },
    {
        "loss": 1.7475,
        "grad_norm": 1.9704101085662842,
        "learning_rate": 8.229797961022423e-05,
        "epoch": 1.0395720546532612,
        "step": 8065
    },
    {
        "loss": 1.7071,
        "grad_norm": 3.2258551120758057,
        "learning_rate": 8.223810338274203e-05,
        "epoch": 1.039700953854086,
        "step": 8066
    },
    {
        "loss": 1.5018,
        "grad_norm": 1.4746514558792114,
        "learning_rate": 8.21782337299104e-05,
        "epoch": 1.039829853054911,
        "step": 8067
    },
    {
        "loss": 0.9574,
        "grad_norm": 3.442511558532715,
        "learning_rate": 8.211837067389033e-05,
        "epoch": 1.039958752255736,
        "step": 8068
    },
    {
        "loss": 1.5283,
        "grad_norm": 4.257328033447266,
        "learning_rate": 8.20585142368403e-05,
        "epoch": 1.040087651456561,
        "step": 8069
    },
    {
        "loss": 0.563,
        "grad_norm": 2.437492847442627,
        "learning_rate": 8.199866444091659e-05,
        "epoch": 1.040216550657386,
        "step": 8070
    },
    {
        "loss": 1.1687,
        "grad_norm": 1.6956446170806885,
        "learning_rate": 8.193882130827302e-05,
        "epoch": 1.040345449858211,
        "step": 8071
    },
    {
        "loss": 1.8001,
        "grad_norm": 2.4434356689453125,
        "learning_rate": 8.187898486106043e-05,
        "epoch": 1.0404743490590358,
        "step": 8072
    },
    {
        "loss": 2.2066,
        "grad_norm": 2.675621509552002,
        "learning_rate": 8.18191551214278e-05,
        "epoch": 1.0406032482598608,
        "step": 8073
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.7720625400543213,
        "learning_rate": 8.175933211152142e-05,
        "epoch": 1.0407321474606857,
        "step": 8074
    },
    {
        "loss": 1.0897,
        "grad_norm": 1.3497213125228882,
        "learning_rate": 8.169951585348499e-05,
        "epoch": 1.0408610466615107,
        "step": 8075
    },
    {
        "loss": 0.7363,
        "grad_norm": 1.6661919355392456,
        "learning_rate": 8.163970636945975e-05,
        "epoch": 1.0409899458623357,
        "step": 8076
    },
    {
        "loss": 1.3035,
        "grad_norm": 2.1335015296936035,
        "learning_rate": 8.157990368158464e-05,
        "epoch": 1.0411188450631605,
        "step": 8077
    },
    {
        "loss": 2.0445,
        "grad_norm": 1.595445990562439,
        "learning_rate": 8.15201078119957e-05,
        "epoch": 1.0412477442639856,
        "step": 8078
    },
    {
        "loss": 1.3974,
        "grad_norm": 2.6287832260131836,
        "learning_rate": 8.146031878282671e-05,
        "epoch": 1.0413766434648106,
        "step": 8079
    },
    {
        "loss": 0.8682,
        "grad_norm": 2.641953229904175,
        "learning_rate": 8.140053661620885e-05,
        "epoch": 1.0415055426656354,
        "step": 8080
    },
    {
        "loss": 1.855,
        "grad_norm": 1.5419036149978638,
        "learning_rate": 8.134076133427091e-05,
        "epoch": 1.0416344418664605,
        "step": 8081
    },
    {
        "loss": 1.8525,
        "grad_norm": 2.001818895339966,
        "learning_rate": 8.128099295913897e-05,
        "epoch": 1.0417633410672853,
        "step": 8082
    },
    {
        "loss": 1.4042,
        "grad_norm": 2.7626028060913086,
        "learning_rate": 8.122123151293636e-05,
        "epoch": 1.0418922402681103,
        "step": 8083
    },
    {
        "loss": 2.3939,
        "grad_norm": 2.7692818641662598,
        "learning_rate": 8.11614770177843e-05,
        "epoch": 1.0420211394689354,
        "step": 8084
    },
    {
        "loss": 2.101,
        "grad_norm": 2.1327805519104004,
        "learning_rate": 8.11017294958011e-05,
        "epoch": 1.0421500386697602,
        "step": 8085
    },
    {
        "loss": 1.8599,
        "grad_norm": 2.9909114837646484,
        "learning_rate": 8.10419889691025e-05,
        "epoch": 1.0422789378705852,
        "step": 8086
    },
    {
        "loss": 1.8363,
        "grad_norm": 1.803848147392273,
        "learning_rate": 8.09822554598019e-05,
        "epoch": 1.0424078370714103,
        "step": 8087
    },
    {
        "loss": 1.9271,
        "grad_norm": 1.9081279039382935,
        "learning_rate": 8.092252899001008e-05,
        "epoch": 1.042536736272235,
        "step": 8088
    },
    {
        "loss": 2.4391,
        "grad_norm": 1.332071304321289,
        "learning_rate": 8.086280958183464e-05,
        "epoch": 1.0426656354730601,
        "step": 8089
    },
    {
        "loss": 1.6893,
        "grad_norm": 2.0333285331726074,
        "learning_rate": 8.080309725738127e-05,
        "epoch": 1.042794534673885,
        "step": 8090
    },
    {
        "loss": 1.3766,
        "grad_norm": 2.4386184215545654,
        "learning_rate": 8.07433920387528e-05,
        "epoch": 1.04292343387471,
        "step": 8091
    },
    {
        "loss": 1.8146,
        "grad_norm": 2.2392735481262207,
        "learning_rate": 8.068369394804936e-05,
        "epoch": 1.043052333075535,
        "step": 8092
    },
    {
        "loss": 1.711,
        "grad_norm": 2.7911148071289062,
        "learning_rate": 8.062400300736841e-05,
        "epoch": 1.0431812322763598,
        "step": 8093
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.5261775255203247,
        "learning_rate": 8.0564319238805e-05,
        "epoch": 1.0433101314771849,
        "step": 8094
    },
    {
        "loss": 1.0338,
        "grad_norm": 2.164499044418335,
        "learning_rate": 8.05046426644512e-05,
        "epoch": 1.0434390306780097,
        "step": 8095
    },
    {
        "loss": 0.9884,
        "grad_norm": 1.7797791957855225,
        "learning_rate": 8.044497330639657e-05,
        "epoch": 1.0435679298788347,
        "step": 8096
    },
    {
        "loss": 1.5344,
        "grad_norm": 2.276052474975586,
        "learning_rate": 8.038531118672806e-05,
        "epoch": 1.0436968290796598,
        "step": 8097
    },
    {
        "loss": 1.1448,
        "grad_norm": 2.3363828659057617,
        "learning_rate": 8.032565632752999e-05,
        "epoch": 1.0438257282804846,
        "step": 8098
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.2991750240325928,
        "learning_rate": 8.026600875088384e-05,
        "epoch": 1.0439546274813096,
        "step": 8099
    },
    {
        "loss": 2.2881,
        "grad_norm": 1.2239760160446167,
        "learning_rate": 8.020636847886826e-05,
        "epoch": 1.0440835266821347,
        "step": 8100
    },
    {
        "loss": 0.8534,
        "grad_norm": 3.1753385066986084,
        "learning_rate": 8.01467355335596e-05,
        "epoch": 1.0442124258829595,
        "step": 8101
    },
    {
        "loss": 1.628,
        "grad_norm": 1.910301685333252,
        "learning_rate": 8.008710993703119e-05,
        "epoch": 1.0443413250837845,
        "step": 8102
    },
    {
        "loss": 1.8235,
        "grad_norm": 2.513866901397705,
        "learning_rate": 8.002749171135368e-05,
        "epoch": 1.0444702242846093,
        "step": 8103
    },
    {
        "loss": 1.4412,
        "grad_norm": 2.102084159851074,
        "learning_rate": 7.996788087859508e-05,
        "epoch": 1.0445991234854344,
        "step": 8104
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.650092601776123,
        "learning_rate": 7.990827746082086e-05,
        "epoch": 1.0447280226862594,
        "step": 8105
    },
    {
        "loss": 1.8948,
        "grad_norm": 2.228524684906006,
        "learning_rate": 7.984868148009309e-05,
        "epoch": 1.0448569218870842,
        "step": 8106
    },
    {
        "loss": 1.5238,
        "grad_norm": 2.138376474380493,
        "learning_rate": 7.978909295847171e-05,
        "epoch": 1.0449858210879093,
        "step": 8107
    },
    {
        "loss": 0.6407,
        "grad_norm": 2.9445512294769287,
        "learning_rate": 7.972951191801373e-05,
        "epoch": 1.0451147202887343,
        "step": 8108
    },
    {
        "loss": 1.0995,
        "grad_norm": 1.924491286277771,
        "learning_rate": 7.966993838077334e-05,
        "epoch": 1.0452436194895591,
        "step": 8109
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.3353244066238403,
        "learning_rate": 7.961037236880184e-05,
        "epoch": 1.0453725186903842,
        "step": 8110
    },
    {
        "loss": 1.0063,
        "grad_norm": 2.0747666358947754,
        "learning_rate": 7.955081390414813e-05,
        "epoch": 1.045501417891209,
        "step": 8111
    },
    {
        "loss": 1.6543,
        "grad_norm": 1.7030324935913086,
        "learning_rate": 7.949126300885786e-05,
        "epoch": 1.045630317092034,
        "step": 8112
    },
    {
        "loss": 1.1539,
        "grad_norm": 2.4863035678863525,
        "learning_rate": 7.943171970497402e-05,
        "epoch": 1.045759216292859,
        "step": 8113
    },
    {
        "loss": 1.7708,
        "grad_norm": 2.1810433864593506,
        "learning_rate": 7.937218401453696e-05,
        "epoch": 1.0458881154936839,
        "step": 8114
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.5429482460021973,
        "learning_rate": 7.931265595958417e-05,
        "epoch": 1.046017014694509,
        "step": 8115
    },
    {
        "loss": 1.0494,
        "grad_norm": 2.6732075214385986,
        "learning_rate": 7.925313556215014e-05,
        "epoch": 1.0461459138953337,
        "step": 8116
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.547616958618164,
        "learning_rate": 7.919362284426665e-05,
        "epoch": 1.0462748130961588,
        "step": 8117
    },
    {
        "loss": 2.038,
        "grad_norm": 1.5621861219406128,
        "learning_rate": 7.913411782796263e-05,
        "epoch": 1.0464037122969838,
        "step": 8118
    },
    {
        "loss": 0.898,
        "grad_norm": 2.4177401065826416,
        "learning_rate": 7.90746205352641e-05,
        "epoch": 1.0465326114978086,
        "step": 8119
    },
    {
        "loss": 1.3555,
        "grad_norm": 2.4393959045410156,
        "learning_rate": 7.901513098819426e-05,
        "epoch": 1.0466615106986337,
        "step": 8120
    },
    {
        "loss": 1.7761,
        "grad_norm": 2.1475749015808105,
        "learning_rate": 7.895564920877346e-05,
        "epoch": 1.0467904098994587,
        "step": 8121
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.4533843994140625,
        "learning_rate": 7.88961752190194e-05,
        "epoch": 1.0469193091002835,
        "step": 8122
    },
    {
        "loss": 1.3345,
        "grad_norm": 2.843001365661621,
        "learning_rate": 7.883670904094624e-05,
        "epoch": 1.0470482083011086,
        "step": 8123
    },
    {
        "loss": 2.546,
        "grad_norm": 1.1253982782363892,
        "learning_rate": 7.877725069656586e-05,
        "epoch": 1.0471771075019336,
        "step": 8124
    },
    {
        "loss": 1.8954,
        "grad_norm": 2.094968557357788,
        "learning_rate": 7.871780020788716e-05,
        "epoch": 1.0473060067027584,
        "step": 8125
    },
    {
        "loss": 1.2606,
        "grad_norm": 2.950861692428589,
        "learning_rate": 7.865835759691592e-05,
        "epoch": 1.0474349059035835,
        "step": 8126
    },
    {
        "loss": 1.8494,
        "grad_norm": 1.831329107284546,
        "learning_rate": 7.859892288565502e-05,
        "epoch": 1.0475638051044083,
        "step": 8127
    },
    {
        "loss": 1.6642,
        "grad_norm": 3.1397247314453125,
        "learning_rate": 7.853949609610476e-05,
        "epoch": 1.0476927043052333,
        "step": 8128
    },
    {
        "loss": 1.5206,
        "grad_norm": 2.160743474960327,
        "learning_rate": 7.8480077250262e-05,
        "epoch": 1.0478216035060584,
        "step": 8129
    },
    {
        "loss": 0.8826,
        "grad_norm": 2.644979238510132,
        "learning_rate": 7.842066637012089e-05,
        "epoch": 1.0479505027068832,
        "step": 8130
    },
    {
        "loss": 1.6346,
        "grad_norm": 3.2595860958099365,
        "learning_rate": 7.83612634776728e-05,
        "epoch": 1.0480794019077082,
        "step": 8131
    },
    {
        "loss": 0.3337,
        "grad_norm": 2.838778257369995,
        "learning_rate": 7.830186859490597e-05,
        "epoch": 1.048208301108533,
        "step": 8132
    },
    {
        "loss": 0.8634,
        "grad_norm": 3.1165521144866943,
        "learning_rate": 7.824248174380568e-05,
        "epoch": 1.048337200309358,
        "step": 8133
    },
    {
        "loss": 1.2364,
        "grad_norm": 2.063279151916504,
        "learning_rate": 7.818310294635427e-05,
        "epoch": 1.048466099510183,
        "step": 8134
    },
    {
        "loss": 1.7258,
        "grad_norm": 1.9691855907440186,
        "learning_rate": 7.812373222453102e-05,
        "epoch": 1.048594998711008,
        "step": 8135
    },
    {
        "loss": 0.7691,
        "grad_norm": 3.053530693054199,
        "learning_rate": 7.806436960031235e-05,
        "epoch": 1.048723897911833,
        "step": 8136
    },
    {
        "loss": 0.8434,
        "grad_norm": 2.8166840076446533,
        "learning_rate": 7.800501509567153e-05,
        "epoch": 1.048852797112658,
        "step": 8137
    },
    {
        "loss": 1.6749,
        "grad_norm": 2.0428433418273926,
        "learning_rate": 7.794566873257896e-05,
        "epoch": 1.0489816963134828,
        "step": 8138
    },
    {
        "loss": 1.7083,
        "grad_norm": 1.9776668548583984,
        "learning_rate": 7.788633053300206e-05,
        "epoch": 1.0491105955143079,
        "step": 8139
    },
    {
        "loss": 1.4709,
        "grad_norm": 2.3930954933166504,
        "learning_rate": 7.782700051890516e-05,
        "epoch": 1.0492394947151327,
        "step": 8140
    },
    {
        "loss": 1.6036,
        "grad_norm": 1.5177321434020996,
        "learning_rate": 7.776767871224931e-05,
        "epoch": 1.0493683939159577,
        "step": 8141
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.539168357849121,
        "learning_rate": 7.7708365134993e-05,
        "epoch": 1.0494972931167827,
        "step": 8142
    },
    {
        "loss": 0.9057,
        "grad_norm": 3.3237993717193604,
        "learning_rate": 7.764905980909136e-05,
        "epoch": 1.0496261923176076,
        "step": 8143
    },
    {
        "loss": 1.3979,
        "grad_norm": 2.2997307777404785,
        "learning_rate": 7.758976275649644e-05,
        "epoch": 1.0497550915184326,
        "step": 8144
    },
    {
        "loss": 1.7307,
        "grad_norm": 2.1665046215057373,
        "learning_rate": 7.753047399915755e-05,
        "epoch": 1.0498839907192576,
        "step": 8145
    },
    {
        "loss": 0.4924,
        "grad_norm": 2.105848550796509,
        "learning_rate": 7.74711935590205e-05,
        "epoch": 1.0500128899200825,
        "step": 8146
    },
    {
        "loss": 1.9674,
        "grad_norm": 1.705976128578186,
        "learning_rate": 7.741192145802823e-05,
        "epoch": 1.0501417891209075,
        "step": 8147
    },
    {
        "loss": 2.3585,
        "grad_norm": 2.1277835369110107,
        "learning_rate": 7.735265771812066e-05,
        "epoch": 1.0502706883217323,
        "step": 8148
    },
    {
        "loss": 1.9477,
        "grad_norm": 2.837686061859131,
        "learning_rate": 7.729340236123459e-05,
        "epoch": 1.0503995875225574,
        "step": 8149
    },
    {
        "loss": 1.2454,
        "grad_norm": 3.0503029823303223,
        "learning_rate": 7.723415540930364e-05,
        "epoch": 1.0505284867233824,
        "step": 8150
    },
    {
        "loss": 1.6862,
        "grad_norm": 1.4237571954727173,
        "learning_rate": 7.717491688425833e-05,
        "epoch": 1.0506573859242072,
        "step": 8151
    },
    {
        "loss": 1.5247,
        "grad_norm": 1.8543535470962524,
        "learning_rate": 7.711568680802608e-05,
        "epoch": 1.0507862851250322,
        "step": 8152
    },
    {
        "loss": 1.8782,
        "grad_norm": 2.0729482173919678,
        "learning_rate": 7.70564652025312e-05,
        "epoch": 1.050915184325857,
        "step": 8153
    },
    {
        "loss": 0.843,
        "grad_norm": 3.8136391639709473,
        "learning_rate": 7.699725208969475e-05,
        "epoch": 1.051044083526682,
        "step": 8154
    },
    {
        "loss": 1.0444,
        "grad_norm": 2.2812962532043457,
        "learning_rate": 7.693804749143486e-05,
        "epoch": 1.0511729827275071,
        "step": 8155
    },
    {
        "loss": 2.3623,
        "grad_norm": 1.5271060466766357,
        "learning_rate": 7.687885142966643e-05,
        "epoch": 1.051301881928332,
        "step": 8156
    },
    {
        "loss": 2.1753,
        "grad_norm": 1.6235995292663574,
        "learning_rate": 7.681966392630114e-05,
        "epoch": 1.051430781129157,
        "step": 8157
    },
    {
        "loss": 0.6245,
        "grad_norm": 2.0276834964752197,
        "learning_rate": 7.676048500324739e-05,
        "epoch": 1.051559680329982,
        "step": 8158
    },
    {
        "loss": 1.6532,
        "grad_norm": 3.058014154434204,
        "learning_rate": 7.67013146824107e-05,
        "epoch": 1.0516885795308069,
        "step": 8159
    },
    {
        "loss": 2.0588,
        "grad_norm": 2.931428909301758,
        "learning_rate": 7.664215298569315e-05,
        "epoch": 1.051817478731632,
        "step": 8160
    },
    {
        "loss": 1.7892,
        "grad_norm": 3.0254671573638916,
        "learning_rate": 7.658299993499371e-05,
        "epoch": 1.051946377932457,
        "step": 8161
    },
    {
        "loss": 1.8953,
        "grad_norm": 2.527639389038086,
        "learning_rate": 7.652385555220836e-05,
        "epoch": 1.0520752771332817,
        "step": 8162
    },
    {
        "loss": 0.4123,
        "grad_norm": 2.3699004650115967,
        "learning_rate": 7.646471985922943e-05,
        "epoch": 1.0522041763341068,
        "step": 8163
    },
    {
        "loss": 0.7423,
        "grad_norm": 3.0806314945220947,
        "learning_rate": 7.640559287794632e-05,
        "epoch": 1.0523330755349316,
        "step": 8164
    },
    {
        "loss": 0.9304,
        "grad_norm": 4.792182445526123,
        "learning_rate": 7.63464746302452e-05,
        "epoch": 1.0524619747357566,
        "step": 8165
    },
    {
        "loss": 1.3287,
        "grad_norm": 2.043921709060669,
        "learning_rate": 7.628736513800907e-05,
        "epoch": 1.0525908739365817,
        "step": 8166
    },
    {
        "loss": 0.9881,
        "grad_norm": 2.5282063484191895,
        "learning_rate": 7.62282644231175e-05,
        "epoch": 1.0527197731374065,
        "step": 8167
    },
    {
        "loss": 1.4387,
        "grad_norm": 2.850613832473755,
        "learning_rate": 7.616917250744688e-05,
        "epoch": 1.0528486723382315,
        "step": 8168
    },
    {
        "loss": 1.7697,
        "grad_norm": 1.812563180923462,
        "learning_rate": 7.611008941287042e-05,
        "epoch": 1.0529775715390564,
        "step": 8169
    },
    {
        "loss": 1.5175,
        "grad_norm": 2.0088794231414795,
        "learning_rate": 7.605101516125799e-05,
        "epoch": 1.0531064707398814,
        "step": 8170
    },
    {
        "loss": 1.814,
        "grad_norm": 1.4826158285140991,
        "learning_rate": 7.599194977447609e-05,
        "epoch": 1.0532353699407064,
        "step": 8171
    },
    {
        "loss": 1.2952,
        "grad_norm": 1.892922043800354,
        "learning_rate": 7.593289327438818e-05,
        "epoch": 1.0533642691415313,
        "step": 8172
    },
    {
        "loss": 1.65,
        "grad_norm": 1.4848555326461792,
        "learning_rate": 7.587384568285435e-05,
        "epoch": 1.0534931683423563,
        "step": 8173
    },
    {
        "loss": 0.8446,
        "grad_norm": 7.549549102783203,
        "learning_rate": 7.581480702173132e-05,
        "epoch": 1.0536220675431813,
        "step": 8174
    },
    {
        "loss": 1.1029,
        "grad_norm": 4.815518379211426,
        "learning_rate": 7.575577731287238e-05,
        "epoch": 1.0537509667440061,
        "step": 8175
    },
    {
        "loss": 1.4864,
        "grad_norm": 1.4339889287948608,
        "learning_rate": 7.569675657812783e-05,
        "epoch": 1.0538798659448312,
        "step": 8176
    },
    {
        "loss": 2.2239,
        "grad_norm": 1.9684892892837524,
        "learning_rate": 7.563774483934438e-05,
        "epoch": 1.054008765145656,
        "step": 8177
    },
    {
        "loss": 1.6574,
        "grad_norm": 1.6970877647399902,
        "learning_rate": 7.557874211836551e-05,
        "epoch": 1.054137664346481,
        "step": 8178
    },
    {
        "loss": 1.4594,
        "grad_norm": 2.070754289627075,
        "learning_rate": 7.551974843703136e-05,
        "epoch": 1.054266563547306,
        "step": 8179
    },
    {
        "loss": 0.9871,
        "grad_norm": 1.3614338636398315,
        "learning_rate": 7.546076381717897e-05,
        "epoch": 1.054395462748131,
        "step": 8180
    },
    {
        "loss": 2.216,
        "grad_norm": 1.890039324760437,
        "learning_rate": 7.540178828064136e-05,
        "epoch": 1.054524361948956,
        "step": 8181
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.1509735584259033,
        "learning_rate": 7.534282184924877e-05,
        "epoch": 1.054653261149781,
        "step": 8182
    },
    {
        "loss": 2.29,
        "grad_norm": 3.1491429805755615,
        "learning_rate": 7.528386454482802e-05,
        "epoch": 1.0547821603506058,
        "step": 8183
    },
    {
        "loss": 1.4312,
        "grad_norm": 2.400548219680786,
        "learning_rate": 7.522491638920237e-05,
        "epoch": 1.0549110595514308,
        "step": 8184
    },
    {
        "loss": 2.4553,
        "grad_norm": 2.1475820541381836,
        "learning_rate": 7.51659774041917e-05,
        "epoch": 1.0550399587522556,
        "step": 8185
    },
    {
        "loss": 1.2993,
        "grad_norm": 2.4612951278686523,
        "learning_rate": 7.510704761161262e-05,
        "epoch": 1.0551688579530807,
        "step": 8186
    },
    {
        "loss": 1.7515,
        "grad_norm": 2.4530773162841797,
        "learning_rate": 7.504812703327827e-05,
        "epoch": 1.0552977571539057,
        "step": 8187
    },
    {
        "loss": 2.2838,
        "grad_norm": 2.4708006381988525,
        "learning_rate": 7.498921569099826e-05,
        "epoch": 1.0554266563547305,
        "step": 8188
    },
    {
        "loss": 1.318,
        "grad_norm": 2.3921539783477783,
        "learning_rate": 7.493031360657904e-05,
        "epoch": 1.0555555555555556,
        "step": 8189
    },
    {
        "loss": 1.3971,
        "grad_norm": 2.7797908782958984,
        "learning_rate": 7.48714208018235e-05,
        "epoch": 1.0556844547563804,
        "step": 8190
    },
    {
        "loss": 1.3295,
        "grad_norm": 2.6039392948150635,
        "learning_rate": 7.481253729853113e-05,
        "epoch": 1.0558133539572054,
        "step": 8191
    },
    {
        "loss": 1.7724,
        "grad_norm": 2.047518253326416,
        "learning_rate": 7.475366311849776e-05,
        "epoch": 1.0559422531580305,
        "step": 8192
    },
    {
        "loss": 1.6226,
        "grad_norm": 2.3668949604034424,
        "learning_rate": 7.469479828351606e-05,
        "epoch": 1.0560711523588553,
        "step": 8193
    },
    {
        "loss": 1.0521,
        "grad_norm": 2.3171212673187256,
        "learning_rate": 7.463594281537518e-05,
        "epoch": 1.0562000515596803,
        "step": 8194
    },
    {
        "loss": 1.8201,
        "grad_norm": 2.1675803661346436,
        "learning_rate": 7.45770967358606e-05,
        "epoch": 1.0563289507605054,
        "step": 8195
    },
    {
        "loss": 1.7854,
        "grad_norm": 2.0298638343811035,
        "learning_rate": 7.451826006675456e-05,
        "epoch": 1.0564578499613302,
        "step": 8196
    },
    {
        "loss": 1.7659,
        "grad_norm": 2.152512311935425,
        "learning_rate": 7.445943282983594e-05,
        "epoch": 1.0565867491621552,
        "step": 8197
    },
    {
        "loss": 1.4434,
        "grad_norm": 1.930467128753662,
        "learning_rate": 7.440061504687953e-05,
        "epoch": 1.05671564836298,
        "step": 8198
    },
    {
        "loss": 2.0813,
        "grad_norm": 2.229320526123047,
        "learning_rate": 7.43418067396572e-05,
        "epoch": 1.056844547563805,
        "step": 8199
    },
    {
        "loss": 1.9071,
        "grad_norm": 2.1430492401123047,
        "learning_rate": 7.42830079299372e-05,
        "epoch": 1.0569734467646301,
        "step": 8200
    },
    {
        "loss": 1.6363,
        "grad_norm": 1.9563016891479492,
        "learning_rate": 7.422421863948412e-05,
        "epoch": 1.057102345965455,
        "step": 8201
    },
    {
        "loss": 1.5209,
        "grad_norm": 2.192396640777588,
        "learning_rate": 7.416543889005903e-05,
        "epoch": 1.05723124516628,
        "step": 8202
    },
    {
        "loss": 1.6082,
        "grad_norm": 2.6098556518554688,
        "learning_rate": 7.410666870341973e-05,
        "epoch": 1.057360144367105,
        "step": 8203
    },
    {
        "loss": 1.2787,
        "grad_norm": 3.8136284351348877,
        "learning_rate": 7.404790810132006e-05,
        "epoch": 1.0574890435679298,
        "step": 8204
    },
    {
        "loss": 1.8416,
        "grad_norm": 2.355848789215088,
        "learning_rate": 7.39891571055106e-05,
        "epoch": 1.0576179427687549,
        "step": 8205
    },
    {
        "loss": 2.1033,
        "grad_norm": 2.0915234088897705,
        "learning_rate": 7.393041573773831e-05,
        "epoch": 1.0577468419695797,
        "step": 8206
    },
    {
        "loss": 0.9691,
        "grad_norm": 1.8677071332931519,
        "learning_rate": 7.387168401974669e-05,
        "epoch": 1.0578757411704047,
        "step": 8207
    },
    {
        "loss": 1.8703,
        "grad_norm": 2.0974316596984863,
        "learning_rate": 7.381296197327554e-05,
        "epoch": 1.0580046403712298,
        "step": 8208
    },
    {
        "loss": 1.835,
        "grad_norm": 2.0553929805755615,
        "learning_rate": 7.375424962006095e-05,
        "epoch": 1.0581335395720546,
        "step": 8209
    },
    {
        "loss": 1.1748,
        "grad_norm": 2.6220767498016357,
        "learning_rate": 7.369554698183571e-05,
        "epoch": 1.0582624387728796,
        "step": 8210
    },
    {
        "loss": 1.6569,
        "grad_norm": 1.6502269506454468,
        "learning_rate": 7.36368540803289e-05,
        "epoch": 1.0583913379737047,
        "step": 8211
    },
    {
        "loss": 1.4772,
        "grad_norm": 1.9695439338684082,
        "learning_rate": 7.357817093726583e-05,
        "epoch": 1.0585202371745295,
        "step": 8212
    },
    {
        "loss": 0.7805,
        "grad_norm": 2.551511764526367,
        "learning_rate": 7.351949757436845e-05,
        "epoch": 1.0586491363753545,
        "step": 8213
    },
    {
        "loss": 2.057,
        "grad_norm": 1.8057750463485718,
        "learning_rate": 7.346083401335517e-05,
        "epoch": 1.0587780355761793,
        "step": 8214
    },
    {
        "loss": 2.3665,
        "grad_norm": 2.477105140686035,
        "learning_rate": 7.340218027594019e-05,
        "epoch": 1.0589069347770044,
        "step": 8215
    },
    {
        "loss": 1.9611,
        "grad_norm": 1.6814743280410767,
        "learning_rate": 7.334353638383469e-05,
        "epoch": 1.0590358339778294,
        "step": 8216
    },
    {
        "loss": 1.7677,
        "grad_norm": 1.7704827785491943,
        "learning_rate": 7.328490235874603e-05,
        "epoch": 1.0591647331786542,
        "step": 8217
    },
    {
        "loss": 1.3741,
        "grad_norm": 5.071721076965332,
        "learning_rate": 7.32262782223778e-05,
        "epoch": 1.0592936323794793,
        "step": 8218
    },
    {
        "loss": 1.6087,
        "grad_norm": 1.4096122980117798,
        "learning_rate": 7.316766399642996e-05,
        "epoch": 1.0594225315803043,
        "step": 8219
    },
    {
        "loss": 1.3939,
        "grad_norm": 3.320789337158203,
        "learning_rate": 7.310905970259902e-05,
        "epoch": 1.0595514307811291,
        "step": 8220
    },
    {
        "loss": 1.205,
        "grad_norm": 2.3099865913391113,
        "learning_rate": 7.305046536257745e-05,
        "epoch": 1.0596803299819542,
        "step": 8221
    },
    {
        "loss": 1.4992,
        "grad_norm": 2.7200329303741455,
        "learning_rate": 7.299188099805425e-05,
        "epoch": 1.059809229182779,
        "step": 8222
    },
    {
        "loss": 2.1436,
        "grad_norm": 1.703475832939148,
        "learning_rate": 7.293330663071472e-05,
        "epoch": 1.059938128383604,
        "step": 8223
    },
    {
        "loss": 1.3046,
        "grad_norm": 1.6870282888412476,
        "learning_rate": 7.287474228224054e-05,
        "epoch": 1.060067027584429,
        "step": 8224
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.611783027648926,
        "learning_rate": 7.281618797430955e-05,
        "epoch": 1.0601959267852539,
        "step": 8225
    },
    {
        "loss": 1.6845,
        "grad_norm": 2.7008163928985596,
        "learning_rate": 7.275764372859575e-05,
        "epoch": 1.060324825986079,
        "step": 8226
    },
    {
        "loss": 1.9632,
        "grad_norm": 1.5262387990951538,
        "learning_rate": 7.269910956676974e-05,
        "epoch": 1.0604537251869037,
        "step": 8227
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.811119318008423,
        "learning_rate": 7.264058551049817e-05,
        "epoch": 1.0605826243877288,
        "step": 8228
    },
    {
        "loss": 1.3275,
        "grad_norm": 2.6887667179107666,
        "learning_rate": 7.258207158144389e-05,
        "epoch": 1.0607115235885538,
        "step": 8229
    },
    {
        "loss": 1.5532,
        "grad_norm": 1.9435679912567139,
        "learning_rate": 7.252356780126623e-05,
        "epoch": 1.0608404227893786,
        "step": 8230
    },
    {
        "loss": 2.1743,
        "grad_norm": 2.478539228439331,
        "learning_rate": 7.246507419162083e-05,
        "epoch": 1.0609693219902037,
        "step": 8231
    },
    {
        "loss": 1.3321,
        "grad_norm": 5.391146183013916,
        "learning_rate": 7.240659077415893e-05,
        "epoch": 1.0610982211910287,
        "step": 8232
    },
    {
        "loss": 2.1972,
        "grad_norm": 2.127692461013794,
        "learning_rate": 7.234811757052872e-05,
        "epoch": 1.0612271203918535,
        "step": 8233
    },
    {
        "loss": 1.1432,
        "grad_norm": 2.174936294555664,
        "learning_rate": 7.228965460237434e-05,
        "epoch": 1.0613560195926786,
        "step": 8234
    },
    {
        "loss": 2.2094,
        "grad_norm": 1.5197807550430298,
        "learning_rate": 7.22312018913361e-05,
        "epoch": 1.0614849187935034,
        "step": 8235
    },
    {
        "loss": 1.3722,
        "grad_norm": 1.7006714344024658,
        "learning_rate": 7.217275945905047e-05,
        "epoch": 1.0616138179943284,
        "step": 8236
    },
    {
        "loss": 1.5805,
        "grad_norm": 2.0934810638427734,
        "learning_rate": 7.211432732715039e-05,
        "epoch": 1.0617427171951535,
        "step": 8237
    },
    {
        "loss": 1.8358,
        "grad_norm": 2.1385796070098877,
        "learning_rate": 7.205590551726462e-05,
        "epoch": 1.0618716163959783,
        "step": 8238
    },
    {
        "loss": 1.1543,
        "grad_norm": 2.5459392070770264,
        "learning_rate": 7.199749405101826e-05,
        "epoch": 1.0620005155968033,
        "step": 8239
    },
    {
        "loss": 1.7441,
        "grad_norm": 2.37711238861084,
        "learning_rate": 7.19390929500326e-05,
        "epoch": 1.0621294147976283,
        "step": 8240
    },
    {
        "loss": 2.4347,
        "grad_norm": 1.3971717357635498,
        "learning_rate": 7.188070223592525e-05,
        "epoch": 1.0622583139984532,
        "step": 8241
    },
    {
        "loss": 1.6428,
        "grad_norm": 2.8268754482269287,
        "learning_rate": 7.182232193030969e-05,
        "epoch": 1.0623872131992782,
        "step": 8242
    },
    {
        "loss": 1.6603,
        "grad_norm": 1.338958978652954,
        "learning_rate": 7.176395205479563e-05,
        "epoch": 1.062516112400103,
        "step": 8243
    },
    {
        "loss": 1.7406,
        "grad_norm": 2.429065465927124,
        "learning_rate": 7.170559263098903e-05,
        "epoch": 1.062645011600928,
        "step": 8244
    },
    {
        "loss": 1.5108,
        "grad_norm": 2.751246929168701,
        "learning_rate": 7.16472436804919e-05,
        "epoch": 1.062773910801753,
        "step": 8245
    },
    {
        "loss": 1.7966,
        "grad_norm": 1.2945870161056519,
        "learning_rate": 7.158890522490226e-05,
        "epoch": 1.062902810002578,
        "step": 8246
    },
    {
        "loss": 1.8642,
        "grad_norm": 2.239466905593872,
        "learning_rate": 7.153057728581449e-05,
        "epoch": 1.063031709203403,
        "step": 8247
    },
    {
        "loss": 1.2652,
        "grad_norm": 2.0796029567718506,
        "learning_rate": 7.147225988481912e-05,
        "epoch": 1.063160608404228,
        "step": 8248
    },
    {
        "loss": 1.3678,
        "grad_norm": 3.8892011642456055,
        "learning_rate": 7.14139530435022e-05,
        "epoch": 1.0632895076050528,
        "step": 8249
    },
    {
        "loss": 1.3456,
        "grad_norm": 2.031827449798584,
        "learning_rate": 7.135565678344655e-05,
        "epoch": 1.0634184068058778,
        "step": 8250
    },
    {
        "loss": 1.6812,
        "grad_norm": 2.109499931335449,
        "learning_rate": 7.12973711262308e-05,
        "epoch": 1.0635473060067027,
        "step": 8251
    },
    {
        "loss": 2.0223,
        "grad_norm": 2.0647716522216797,
        "learning_rate": 7.123909609342964e-05,
        "epoch": 1.0636762052075277,
        "step": 8252
    },
    {
        "loss": 1.1673,
        "grad_norm": 2.119079828262329,
        "learning_rate": 7.118083170661378e-05,
        "epoch": 1.0638051044083527,
        "step": 8253
    },
    {
        "loss": 0.4784,
        "grad_norm": 2.145869731903076,
        "learning_rate": 7.112257798735024e-05,
        "epoch": 1.0639340036091776,
        "step": 8254
    },
    {
        "loss": 1.7262,
        "grad_norm": 1.6220735311508179,
        "learning_rate": 7.106433495720176e-05,
        "epoch": 1.0640629028100026,
        "step": 8255
    },
    {
        "loss": 1.5241,
        "grad_norm": 1.980102777481079,
        "learning_rate": 7.100610263772719e-05,
        "epoch": 1.0641918020108276,
        "step": 8256
    },
    {
        "loss": 1.932,
        "grad_norm": 1.896329641342163,
        "learning_rate": 7.094788105048165e-05,
        "epoch": 1.0643207012116525,
        "step": 8257
    },
    {
        "loss": 1.6546,
        "grad_norm": 1.9260791540145874,
        "learning_rate": 7.088967021701617e-05,
        "epoch": 1.0644496004124775,
        "step": 8258
    },
    {
        "loss": 1.3898,
        "grad_norm": 2.469133138656616,
        "learning_rate": 7.083147015887768e-05,
        "epoch": 1.0645784996133023,
        "step": 8259
    },
    {
        "loss": 1.7425,
        "grad_norm": 2.508236885070801,
        "learning_rate": 7.077328089760924e-05,
        "epoch": 1.0647073988141273,
        "step": 8260
    },
    {
        "loss": 1.8566,
        "grad_norm": 1.4889284372329712,
        "learning_rate": 7.071510245474987e-05,
        "epoch": 1.0648362980149524,
        "step": 8261
    },
    {
        "loss": 1.6881,
        "grad_norm": 1.6146941184997559,
        "learning_rate": 7.065693485183457e-05,
        "epoch": 1.0649651972157772,
        "step": 8262
    },
    {
        "loss": 1.7663,
        "grad_norm": 2.688429117202759,
        "learning_rate": 7.059877811039437e-05,
        "epoch": 1.0650940964166022,
        "step": 8263
    },
    {
        "loss": 1.8982,
        "grad_norm": 2.2987112998962402,
        "learning_rate": 7.054063225195625e-05,
        "epoch": 1.065222995617427,
        "step": 8264
    },
    {
        "loss": 1.6522,
        "grad_norm": 2.6597628593444824,
        "learning_rate": 7.048249729804326e-05,
        "epoch": 1.065351894818252,
        "step": 8265
    },
    {
        "loss": 1.6283,
        "grad_norm": 2.185716152191162,
        "learning_rate": 7.042437327017435e-05,
        "epoch": 1.0654807940190771,
        "step": 8266
    },
    {
        "loss": 1.8464,
        "grad_norm": 2.7196109294891357,
        "learning_rate": 7.036626018986417e-05,
        "epoch": 1.065609693219902,
        "step": 8267
    },
    {
        "loss": 1.7574,
        "grad_norm": 3.441068649291992,
        "learning_rate": 7.03081580786238e-05,
        "epoch": 1.065738592420727,
        "step": 8268
    },
    {
        "loss": 1.4787,
        "grad_norm": 2.767587184906006,
        "learning_rate": 7.025006695795994e-05,
        "epoch": 1.065867491621552,
        "step": 8269
    },
    {
        "loss": 1.8103,
        "grad_norm": 3.364046812057495,
        "learning_rate": 7.019198684937519e-05,
        "epoch": 1.0659963908223768,
        "step": 8270
    },
    {
        "loss": 1.8766,
        "grad_norm": 1.6256988048553467,
        "learning_rate": 7.01339177743684e-05,
        "epoch": 1.0661252900232019,
        "step": 8271
    },
    {
        "loss": 1.2443,
        "grad_norm": 2.3685293197631836,
        "learning_rate": 7.007585975443392e-05,
        "epoch": 1.066254189224027,
        "step": 8272
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.1023848056793213,
        "learning_rate": 7.00178128110622e-05,
        "epoch": 1.0663830884248517,
        "step": 8273
    },
    {
        "loss": 1.4785,
        "grad_norm": 3.0286359786987305,
        "learning_rate": 6.995977696573964e-05,
        "epoch": 1.0665119876256768,
        "step": 8274
    },
    {
        "loss": 1.5976,
        "grad_norm": 2.5610358715057373,
        "learning_rate": 6.990175223994859e-05,
        "epoch": 1.0666408868265016,
        "step": 8275
    },
    {
        "loss": 1.1728,
        "grad_norm": 2.3693840503692627,
        "learning_rate": 6.984373865516705e-05,
        "epoch": 1.0667697860273266,
        "step": 8276
    },
    {
        "loss": 1.4038,
        "grad_norm": 2.1918344497680664,
        "learning_rate": 6.978573623286909e-05,
        "epoch": 1.0668986852281517,
        "step": 8277
    },
    {
        "loss": 1.8867,
        "grad_norm": 1.4294320344924927,
        "learning_rate": 6.972774499452456e-05,
        "epoch": 1.0670275844289765,
        "step": 8278
    },
    {
        "loss": 1.0299,
        "grad_norm": 2.4274966716766357,
        "learning_rate": 6.966976496159916e-05,
        "epoch": 1.0671564836298015,
        "step": 8279
    },
    {
        "loss": 1.3995,
        "grad_norm": 1.6133816242218018,
        "learning_rate": 6.961179615555447e-05,
        "epoch": 1.0672853828306264,
        "step": 8280
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.5137828588485718,
        "learning_rate": 6.955383859784792e-05,
        "epoch": 1.0674142820314514,
        "step": 8281
    },
    {
        "loss": 2.491,
        "grad_norm": 1.912843108177185,
        "learning_rate": 6.949589230993289e-05,
        "epoch": 1.0675431812322764,
        "step": 8282
    },
    {
        "loss": 1.6721,
        "grad_norm": 2.42379093170166,
        "learning_rate": 6.943795731325846e-05,
        "epoch": 1.0676720804331012,
        "step": 8283
    },
    {
        "loss": 2.1906,
        "grad_norm": 1.2196725606918335,
        "learning_rate": 6.938003362926931e-05,
        "epoch": 1.0678009796339263,
        "step": 8284
    },
    {
        "loss": 2.267,
        "grad_norm": 1.6886649131774902,
        "learning_rate": 6.932212127940642e-05,
        "epoch": 1.0679298788347513,
        "step": 8285
    },
    {
        "loss": 1.8886,
        "grad_norm": 1.7185393571853638,
        "learning_rate": 6.926422028510626e-05,
        "epoch": 1.0680587780355761,
        "step": 8286
    },
    {
        "loss": 2.0374,
        "grad_norm": 2.2624566555023193,
        "learning_rate": 6.920633066780103e-05,
        "epoch": 1.0681876772364012,
        "step": 8287
    },
    {
        "loss": 1.5811,
        "grad_norm": 1.6212471723556519,
        "learning_rate": 6.914845244891909e-05,
        "epoch": 1.068316576437226,
        "step": 8288
    },
    {
        "loss": 1.7965,
        "grad_norm": 1.5195773839950562,
        "learning_rate": 6.909058564988416e-05,
        "epoch": 1.068445475638051,
        "step": 8289
    },
    {
        "loss": 1.0363,
        "grad_norm": 2.800152063369751,
        "learning_rate": 6.903273029211584e-05,
        "epoch": 1.068574374838876,
        "step": 8290
    },
    {
        "loss": 1.5525,
        "grad_norm": 2.0941720008850098,
        "learning_rate": 6.897488639702972e-05,
        "epoch": 1.068703274039701,
        "step": 8291
    },
    {
        "loss": 1.3149,
        "grad_norm": 2.569725275039673,
        "learning_rate": 6.891705398603696e-05,
        "epoch": 1.068832173240526,
        "step": 8292
    },
    {
        "loss": 2.0522,
        "grad_norm": 3.241962432861328,
        "learning_rate": 6.885923308054453e-05,
        "epoch": 1.068961072441351,
        "step": 8293
    },
    {
        "loss": 2.5941,
        "grad_norm": 1.7671446800231934,
        "learning_rate": 6.880142370195505e-05,
        "epoch": 1.0690899716421758,
        "step": 8294
    },
    {
        "loss": 1.7759,
        "grad_norm": 2.2041561603546143,
        "learning_rate": 6.874362587166697e-05,
        "epoch": 1.0692188708430008,
        "step": 8295
    },
    {
        "loss": 1.0692,
        "grad_norm": 2.4577434062957764,
        "learning_rate": 6.868583961107442e-05,
        "epoch": 1.0693477700438256,
        "step": 8296
    },
    {
        "loss": 2.0875,
        "grad_norm": 1.780397653579712,
        "learning_rate": 6.86280649415672e-05,
        "epoch": 1.0694766692446507,
        "step": 8297
    },
    {
        "loss": 1.4745,
        "grad_norm": 2.548034191131592,
        "learning_rate": 6.857030188453091e-05,
        "epoch": 1.0696055684454757,
        "step": 8298
    },
    {
        "loss": 2.0091,
        "grad_norm": 2.6291186809539795,
        "learning_rate": 6.851255046134695e-05,
        "epoch": 1.0697344676463005,
        "step": 8299
    },
    {
        "loss": 1.9841,
        "grad_norm": 3.2037689685821533,
        "learning_rate": 6.845481069339224e-05,
        "epoch": 1.0698633668471256,
        "step": 8300
    },
    {
        "loss": 1.9205,
        "grad_norm": 2.2207889556884766,
        "learning_rate": 6.839708260203922e-05,
        "epoch": 1.0699922660479504,
        "step": 8301
    },
    {
        "loss": 0.9789,
        "grad_norm": 1.974155068397522,
        "learning_rate": 6.833936620865647e-05,
        "epoch": 1.0701211652487754,
        "step": 8302
    },
    {
        "loss": 0.8028,
        "grad_norm": 2.8516039848327637,
        "learning_rate": 6.828166153460785e-05,
        "epoch": 1.0702500644496005,
        "step": 8303
    },
    {
        "loss": 1.3006,
        "grad_norm": 2.953380823135376,
        "learning_rate": 6.822396860125301e-05,
        "epoch": 1.0703789636504253,
        "step": 8304
    },
    {
        "loss": 1.7116,
        "grad_norm": 2.1310153007507324,
        "learning_rate": 6.816628742994731e-05,
        "epoch": 1.0705078628512503,
        "step": 8305
    },
    {
        "loss": 1.933,
        "grad_norm": 1.6003539562225342,
        "learning_rate": 6.81086180420419e-05,
        "epoch": 1.0706367620520754,
        "step": 8306
    },
    {
        "loss": 1.3592,
        "grad_norm": 1.6913692951202393,
        "learning_rate": 6.8050960458883e-05,
        "epoch": 1.0707656612529002,
        "step": 8307
    },
    {
        "loss": 1.6915,
        "grad_norm": 2.7415781021118164,
        "learning_rate": 6.799331470181303e-05,
        "epoch": 1.0708945604537252,
        "step": 8308
    },
    {
        "loss": 1.9102,
        "grad_norm": 1.791860818862915,
        "learning_rate": 6.793568079216989e-05,
        "epoch": 1.0710234596545503,
        "step": 8309
    },
    {
        "loss": 1.556,
        "grad_norm": 2.841087818145752,
        "learning_rate": 6.787805875128701e-05,
        "epoch": 1.071152358855375,
        "step": 8310
    },
    {
        "loss": 2.3661,
        "grad_norm": 2.032198667526245,
        "learning_rate": 6.782044860049343e-05,
        "epoch": 1.0712812580562001,
        "step": 8311
    },
    {
        "loss": 1.4753,
        "grad_norm": 3.0237698554992676,
        "learning_rate": 6.776285036111385e-05,
        "epoch": 1.071410157257025,
        "step": 8312
    },
    {
        "loss": 2.0834,
        "grad_norm": 1.7616409063339233,
        "learning_rate": 6.770526405446852e-05,
        "epoch": 1.07153905645785,
        "step": 8313
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.081113338470459,
        "learning_rate": 6.764768970187321e-05,
        "epoch": 1.071667955658675,
        "step": 8314
    },
    {
        "loss": 1.4795,
        "grad_norm": 2.350461959838867,
        "learning_rate": 6.759012732463943e-05,
        "epoch": 1.0717968548594998,
        "step": 8315
    },
    {
        "loss": 1.8468,
        "grad_norm": 2.7287044525146484,
        "learning_rate": 6.753257694407422e-05,
        "epoch": 1.0719257540603249,
        "step": 8316
    },
    {
        "loss": 1.8022,
        "grad_norm": 3.3533430099487305,
        "learning_rate": 6.747503858148009e-05,
        "epoch": 1.0720546532611497,
        "step": 8317
    },
    {
        "loss": 1.8064,
        "grad_norm": 2.3964271545410156,
        "learning_rate": 6.741751225815502e-05,
        "epoch": 1.0721835524619747,
        "step": 8318
    },
    {
        "loss": 1.4223,
        "grad_norm": 2.5388495922088623,
        "learning_rate": 6.73599979953928e-05,
        "epoch": 1.0723124516627998,
        "step": 8319
    },
    {
        "loss": 1.0039,
        "grad_norm": 2.7075157165527344,
        "learning_rate": 6.730249581448253e-05,
        "epoch": 1.0724413508636246,
        "step": 8320
    },
    {
        "loss": 2.0889,
        "grad_norm": 1.963248610496521,
        "learning_rate": 6.724500573670889e-05,
        "epoch": 1.0725702500644496,
        "step": 8321
    },
    {
        "loss": 1.2237,
        "grad_norm": 2.4345664978027344,
        "learning_rate": 6.718752778335212e-05,
        "epoch": 1.0726991492652747,
        "step": 8322
    },
    {
        "loss": 1.4561,
        "grad_norm": 2.8250410556793213,
        "learning_rate": 6.71300619756882e-05,
        "epoch": 1.0728280484660995,
        "step": 8323
    },
    {
        "loss": 1.2703,
        "grad_norm": 1.6631637811660767,
        "learning_rate": 6.707260833498799e-05,
        "epoch": 1.0729569476669245,
        "step": 8324
    },
    {
        "loss": 0.7502,
        "grad_norm": 2.0613014698028564,
        "learning_rate": 6.701516688251836e-05,
        "epoch": 1.0730858468677493,
        "step": 8325
    },
    {
        "loss": 1.4815,
        "grad_norm": 2.7750627994537354,
        "learning_rate": 6.695773763954167e-05,
        "epoch": 1.0732147460685744,
        "step": 8326
    },
    {
        "loss": 1.7635,
        "grad_norm": 2.4690334796905518,
        "learning_rate": 6.690032062731552e-05,
        "epoch": 1.0733436452693994,
        "step": 8327
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.782296061515808,
        "learning_rate": 6.68429158670931e-05,
        "epoch": 1.0734725444702242,
        "step": 8328
    },
    {
        "loss": 0.9363,
        "grad_norm": 2.6256251335144043,
        "learning_rate": 6.678552338012307e-05,
        "epoch": 1.0736014436710493,
        "step": 8329
    },
    {
        "loss": 2.0402,
        "grad_norm": 3.3900177478790283,
        "learning_rate": 6.672814318764951e-05,
        "epoch": 1.0737303428718743,
        "step": 8330
    },
    {
        "loss": 1.9073,
        "grad_norm": 1.4534351825714111,
        "learning_rate": 6.667077531091192e-05,
        "epoch": 1.0738592420726991,
        "step": 8331
    },
    {
        "loss": 2.576,
        "grad_norm": 1.839958906173706,
        "learning_rate": 6.661341977114534e-05,
        "epoch": 1.0739881412735242,
        "step": 8332
    },
    {
        "loss": 0.8479,
        "grad_norm": 2.2054824829101562,
        "learning_rate": 6.655607658958029e-05,
        "epoch": 1.074117040474349,
        "step": 8333
    },
    {
        "loss": 0.8247,
        "grad_norm": 2.7829384803771973,
        "learning_rate": 6.64987457874426e-05,
        "epoch": 1.074245939675174,
        "step": 8334
    },
    {
        "loss": 1.9482,
        "grad_norm": 1.8371973037719727,
        "learning_rate": 6.644142738595337e-05,
        "epoch": 1.074374838875999,
        "step": 8335
    },
    {
        "loss": 1.9407,
        "grad_norm": 1.7314047813415527,
        "learning_rate": 6.638412140632941e-05,
        "epoch": 1.0745037380768239,
        "step": 8336
    },
    {
        "loss": 2.14,
        "grad_norm": 1.8573329448699951,
        "learning_rate": 6.632682786978282e-05,
        "epoch": 1.074632637277649,
        "step": 8337
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.456779718399048,
        "learning_rate": 6.626954679752096e-05,
        "epoch": 1.0747615364784737,
        "step": 8338
    },
    {
        "loss": 1.0421,
        "grad_norm": 2.612545967102051,
        "learning_rate": 6.621227821074674e-05,
        "epoch": 1.0748904356792988,
        "step": 8339
    },
    {
        "loss": 0.9451,
        "grad_norm": 3.5076217651367188,
        "learning_rate": 6.615502213065865e-05,
        "epoch": 1.0750193348801238,
        "step": 8340
    },
    {
        "loss": 1.9378,
        "grad_norm": 2.3138298988342285,
        "learning_rate": 6.609777857844988e-05,
        "epoch": 1.0751482340809486,
        "step": 8341
    },
    {
        "loss": 2.1212,
        "grad_norm": 1.597099781036377,
        "learning_rate": 6.604054757530957e-05,
        "epoch": 1.0752771332817737,
        "step": 8342
    },
    {
        "loss": 1.1187,
        "grad_norm": 2.7126715183258057,
        "learning_rate": 6.598332914242217e-05,
        "epoch": 1.0754060324825987,
        "step": 8343
    },
    {
        "loss": 0.9464,
        "grad_norm": 2.390991449356079,
        "learning_rate": 6.592612330096726e-05,
        "epoch": 1.0755349316834235,
        "step": 8344
    },
    {
        "loss": 1.851,
        "grad_norm": 2.5265846252441406,
        "learning_rate": 6.586893007211978e-05,
        "epoch": 1.0756638308842486,
        "step": 8345
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.080451726913452,
        "learning_rate": 6.581174947705028e-05,
        "epoch": 1.0757927300850736,
        "step": 8346
    },
    {
        "loss": 1.2913,
        "grad_norm": 2.458292007446289,
        "learning_rate": 6.575458153692427e-05,
        "epoch": 1.0759216292858984,
        "step": 8347
    },
    {
        "loss": 1.2911,
        "grad_norm": 2.1687798500061035,
        "learning_rate": 6.569742627290266e-05,
        "epoch": 1.0760505284867234,
        "step": 8348
    },
    {
        "loss": 1.7025,
        "grad_norm": 2.4709112644195557,
        "learning_rate": 6.564028370614186e-05,
        "epoch": 1.0761794276875483,
        "step": 8349
    },
    {
        "loss": 1.9108,
        "grad_norm": 2.510079860687256,
        "learning_rate": 6.558315385779352e-05,
        "epoch": 1.0763083268883733,
        "step": 8350
    },
    {
        "loss": 1.909,
        "grad_norm": 1.778490424156189,
        "learning_rate": 6.552603674900452e-05,
        "epoch": 1.0764372260891983,
        "step": 8351
    },
    {
        "loss": 1.6904,
        "grad_norm": 2.2192630767822266,
        "learning_rate": 6.546893240091686e-05,
        "epoch": 1.0765661252900232,
        "step": 8352
    },
    {
        "loss": 1.7153,
        "grad_norm": 3.1171395778656006,
        "learning_rate": 6.541184083466811e-05,
        "epoch": 1.0766950244908482,
        "step": 8353
    },
    {
        "loss": 1.9989,
        "grad_norm": 2.7918171882629395,
        "learning_rate": 6.5354762071391e-05,
        "epoch": 1.076823923691673,
        "step": 8354
    },
    {
        "loss": 1.5634,
        "grad_norm": 2.130967855453491,
        "learning_rate": 6.529769613221339e-05,
        "epoch": 1.076952822892498,
        "step": 8355
    },
    {
        "loss": 0.9325,
        "grad_norm": 2.051492929458618,
        "learning_rate": 6.52406430382586e-05,
        "epoch": 1.077081722093323,
        "step": 8356
    },
    {
        "loss": 1.196,
        "grad_norm": 3.473158836364746,
        "learning_rate": 6.51836028106453e-05,
        "epoch": 1.077210621294148,
        "step": 8357
    },
    {
        "loss": 1.8095,
        "grad_norm": 1.9959895610809326,
        "learning_rate": 6.512657547048679e-05,
        "epoch": 1.077339520494973,
        "step": 8358
    },
    {
        "loss": 1.8395,
        "grad_norm": 3.0656418800354004,
        "learning_rate": 6.506956103889222e-05,
        "epoch": 1.077468419695798,
        "step": 8359
    },
    {
        "loss": 2.0566,
        "grad_norm": 1.7656922340393066,
        "learning_rate": 6.501255953696584e-05,
        "epoch": 1.0775973188966228,
        "step": 8360
    },
    {
        "loss": 1.9444,
        "grad_norm": 2.4738852977752686,
        "learning_rate": 6.495557098580688e-05,
        "epoch": 1.0777262180974478,
        "step": 8361
    },
    {
        "loss": 1.2606,
        "grad_norm": 3.0545661449432373,
        "learning_rate": 6.489859540650997e-05,
        "epoch": 1.0778551172982727,
        "step": 8362
    },
    {
        "loss": 0.7738,
        "grad_norm": 2.7809946537017822,
        "learning_rate": 6.484163282016501e-05,
        "epoch": 1.0779840164990977,
        "step": 8363
    },
    {
        "loss": 1.7591,
        "grad_norm": 2.3784632682800293,
        "learning_rate": 6.47846832478568e-05,
        "epoch": 1.0781129156999227,
        "step": 8364
    },
    {
        "loss": 1.8836,
        "grad_norm": 2.1897497177124023,
        "learning_rate": 6.472774671066553e-05,
        "epoch": 1.0782418149007476,
        "step": 8365
    },
    {
        "loss": 1.943,
        "grad_norm": 2.6818654537200928,
        "learning_rate": 6.467082322966651e-05,
        "epoch": 1.0783707141015726,
        "step": 8366
    },
    {
        "loss": 1.163,
        "grad_norm": 3.734673023223877,
        "learning_rate": 6.461391282593039e-05,
        "epoch": 1.0784996133023976,
        "step": 8367
    },
    {
        "loss": 2.3249,
        "grad_norm": 1.8609890937805176,
        "learning_rate": 6.455701552052272e-05,
        "epoch": 1.0786285125032224,
        "step": 8368
    },
    {
        "loss": 1.4521,
        "grad_norm": 2.0252685546875,
        "learning_rate": 6.450013133450428e-05,
        "epoch": 1.0787574117040475,
        "step": 8369
    },
    {
        "loss": 1.8795,
        "grad_norm": 2.490957498550415,
        "learning_rate": 6.444326028893108e-05,
        "epoch": 1.0788863109048723,
        "step": 8370
    },
    {
        "loss": 1.6116,
        "grad_norm": 4.985834121704102,
        "learning_rate": 6.438640240485418e-05,
        "epoch": 1.0790152101056973,
        "step": 8371
    },
    {
        "loss": 1.5516,
        "grad_norm": 2.279550313949585,
        "learning_rate": 6.432955770331972e-05,
        "epoch": 1.0791441093065224,
        "step": 8372
    },
    {
        "loss": 1.4602,
        "grad_norm": 3.0382637977600098,
        "learning_rate": 6.427272620536914e-05,
        "epoch": 1.0792730085073472,
        "step": 8373
    },
    {
        "loss": 1.8138,
        "grad_norm": 2.6110315322875977,
        "learning_rate": 6.421590793203905e-05,
        "epoch": 1.0794019077081722,
        "step": 8374
    },
    {
        "loss": 1.0939,
        "grad_norm": 1.4522840976715088,
        "learning_rate": 6.41591029043606e-05,
        "epoch": 1.079530806908997,
        "step": 8375
    },
    {
        "loss": 1.7194,
        "grad_norm": 1.4823671579360962,
        "learning_rate": 6.41023111433607e-05,
        "epoch": 1.079659706109822,
        "step": 8376
    },
    {
        "loss": 1.8362,
        "grad_norm": 2.547117233276367,
        "learning_rate": 6.40455326700611e-05,
        "epoch": 1.0797886053106471,
        "step": 8377
    },
    {
        "loss": 0.7846,
        "grad_norm": 1.882412314414978,
        "learning_rate": 6.398876750547855e-05,
        "epoch": 1.079917504511472,
        "step": 8378
    },
    {
        "loss": 0.9253,
        "grad_norm": 2.753906726837158,
        "learning_rate": 6.393201567062492e-05,
        "epoch": 1.080046403712297,
        "step": 8379
    },
    {
        "loss": 1.6652,
        "grad_norm": 2.515770673751831,
        "learning_rate": 6.387527718650736e-05,
        "epoch": 1.080175302913122,
        "step": 8380
    },
    {
        "loss": 0.5685,
        "grad_norm": 1.6689683198928833,
        "learning_rate": 6.381855207412766e-05,
        "epoch": 1.0803042021139468,
        "step": 8381
    },
    {
        "loss": 1.4042,
        "grad_norm": 1.8622863292694092,
        "learning_rate": 6.376184035448293e-05,
        "epoch": 1.0804331013147719,
        "step": 8382
    },
    {
        "loss": 1.8779,
        "grad_norm": 2.118119239807129,
        "learning_rate": 6.370514204856531e-05,
        "epoch": 1.080562000515597,
        "step": 8383
    },
    {
        "loss": 1.6971,
        "grad_norm": 1.9979362487792969,
        "learning_rate": 6.364845717736206e-05,
        "epoch": 1.0806908997164217,
        "step": 8384
    },
    {
        "loss": 1.751,
        "grad_norm": 1.4483177661895752,
        "learning_rate": 6.359178576185526e-05,
        "epoch": 1.0808197989172468,
        "step": 8385
    },
    {
        "loss": 0.9939,
        "grad_norm": 2.329646587371826,
        "learning_rate": 6.353512782302209e-05,
        "epoch": 1.0809486981180716,
        "step": 8386
    },
    {
        "loss": 1.6458,
        "grad_norm": 2.6992552280426025,
        "learning_rate": 6.34784833818348e-05,
        "epoch": 1.0810775973188966,
        "step": 8387
    },
    {
        "loss": 1.2802,
        "grad_norm": 2.2462713718414307,
        "learning_rate": 6.342185245926056e-05,
        "epoch": 1.0812064965197217,
        "step": 8388
    },
    {
        "loss": 1.3768,
        "grad_norm": 2.1254031658172607,
        "learning_rate": 6.33652350762615e-05,
        "epoch": 1.0813353957205465,
        "step": 8389
    },
    {
        "loss": 1.4798,
        "grad_norm": 2.1553168296813965,
        "learning_rate": 6.330863125379495e-05,
        "epoch": 1.0814642949213715,
        "step": 8390
    },
    {
        "loss": 0.7888,
        "grad_norm": 2.3587396144866943,
        "learning_rate": 6.32520410128132e-05,
        "epoch": 1.0815931941221963,
        "step": 8391
    },
    {
        "loss": 1.8744,
        "grad_norm": 1.6289440393447876,
        "learning_rate": 6.319546437426302e-05,
        "epoch": 1.0817220933230214,
        "step": 8392
    },
    {
        "loss": 1.231,
        "grad_norm": 3.9025256633758545,
        "learning_rate": 6.313890135908678e-05,
        "epoch": 1.0818509925238464,
        "step": 8393
    },
    {
        "loss": 1.8386,
        "grad_norm": 2.1417076587677,
        "learning_rate": 6.308235198822152e-05,
        "epoch": 1.0819798917246712,
        "step": 8394
    },
    {
        "loss": 2.0709,
        "grad_norm": 3.4189512729644775,
        "learning_rate": 6.302581628259928e-05,
        "epoch": 1.0821087909254963,
        "step": 8395
    },
    {
        "loss": 2.1137,
        "grad_norm": 3.2205774784088135,
        "learning_rate": 6.296929426314692e-05,
        "epoch": 1.0822376901263213,
        "step": 8396
    },
    {
        "loss": 1.6976,
        "grad_norm": 1.9255362749099731,
        "learning_rate": 6.291278595078649e-05,
        "epoch": 1.0823665893271461,
        "step": 8397
    },
    {
        "loss": 1.8697,
        "grad_norm": 2.820370674133301,
        "learning_rate": 6.285629136643468e-05,
        "epoch": 1.0824954885279712,
        "step": 8398
    },
    {
        "loss": 2.0252,
        "grad_norm": 2.067195415496826,
        "learning_rate": 6.27998105310032e-05,
        "epoch": 1.082624387728796,
        "step": 8399
    },
    {
        "loss": 1.0488,
        "grad_norm": 4.952310085296631,
        "learning_rate": 6.274334346539875e-05,
        "epoch": 1.082753286929621,
        "step": 8400
    },
    {
        "loss": 1.4581,
        "grad_norm": 2.6878299713134766,
        "learning_rate": 6.268689019052298e-05,
        "epoch": 1.082882186130446,
        "step": 8401
    },
    {
        "loss": 1.3595,
        "grad_norm": 3.4673290252685547,
        "learning_rate": 6.263045072727223e-05,
        "epoch": 1.0830110853312709,
        "step": 8402
    },
    {
        "loss": 1.7501,
        "grad_norm": 2.817678213119507,
        "learning_rate": 6.257402509653789e-05,
        "epoch": 1.083139984532096,
        "step": 8403
    },
    {
        "loss": 1.5916,
        "grad_norm": 1.3063899278640747,
        "learning_rate": 6.251761331920616e-05,
        "epoch": 1.083268883732921,
        "step": 8404
    },
    {
        "loss": 1.409,
        "grad_norm": 1.6433649063110352,
        "learning_rate": 6.246121541615808e-05,
        "epoch": 1.0833977829337458,
        "step": 8405
    },
    {
        "loss": 0.7349,
        "grad_norm": 3.211792469024658,
        "learning_rate": 6.24048314082696e-05,
        "epoch": 1.0835266821345708,
        "step": 8406
    },
    {
        "loss": 1.8234,
        "grad_norm": 2.2668261528015137,
        "learning_rate": 6.234846131641156e-05,
        "epoch": 1.0836555813353956,
        "step": 8407
    },
    {
        "loss": 1.7807,
        "grad_norm": 1.9738974571228027,
        "learning_rate": 6.229210516144968e-05,
        "epoch": 1.0837844805362207,
        "step": 8408
    },
    {
        "loss": 1.6615,
        "grad_norm": 1.6230316162109375,
        "learning_rate": 6.223576296424447e-05,
        "epoch": 1.0839133797370457,
        "step": 8409
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.0895063877105713,
        "learning_rate": 6.217943474565109e-05,
        "epoch": 1.0840422789378705,
        "step": 8410
    },
    {
        "loss": 1.9482,
        "grad_norm": 1.3230676651000977,
        "learning_rate": 6.212312052651982e-05,
        "epoch": 1.0841711781386956,
        "step": 8411
    },
    {
        "loss": 2.0065,
        "grad_norm": 2.0751419067382812,
        "learning_rate": 6.206682032769566e-05,
        "epoch": 1.0843000773395204,
        "step": 8412
    },
    {
        "loss": 1.3407,
        "grad_norm": 2.6369943618774414,
        "learning_rate": 6.201053417001827e-05,
        "epoch": 1.0844289765403454,
        "step": 8413
    },
    {
        "loss": 1.4719,
        "grad_norm": 1.8965345621109009,
        "learning_rate": 6.195426207432247e-05,
        "epoch": 1.0845578757411705,
        "step": 8414
    },
    {
        "loss": 1.6046,
        "grad_norm": 2.790236473083496,
        "learning_rate": 6.189800406143743e-05,
        "epoch": 1.0846867749419953,
        "step": 8415
    },
    {
        "loss": 2.055,
        "grad_norm": 3.141918897628784,
        "learning_rate": 6.184176015218731e-05,
        "epoch": 1.0848156741428203,
        "step": 8416
    },
    {
        "loss": 1.3358,
        "grad_norm": 2.748197317123413,
        "learning_rate": 6.178553036739116e-05,
        "epoch": 1.0849445733436454,
        "step": 8417
    },
    {
        "loss": 0.9491,
        "grad_norm": 2.5273196697235107,
        "learning_rate": 6.172931472786274e-05,
        "epoch": 1.0850734725444702,
        "step": 8418
    },
    {
        "loss": 1.6959,
        "grad_norm": 2.902557611465454,
        "learning_rate": 6.167311325441048e-05,
        "epoch": 1.0852023717452952,
        "step": 8419
    },
    {
        "loss": 2.3103,
        "grad_norm": 1.3396247625350952,
        "learning_rate": 6.161692596783762e-05,
        "epoch": 1.08533127094612,
        "step": 8420
    },
    {
        "loss": 0.9156,
        "grad_norm": 3.328272819519043,
        "learning_rate": 6.156075288894214e-05,
        "epoch": 1.085460170146945,
        "step": 8421
    },
    {
        "loss": 1.9825,
        "grad_norm": 1.8341634273529053,
        "learning_rate": 6.150459403851678e-05,
        "epoch": 1.08558906934777,
        "step": 8422
    },
    {
        "loss": 1.8958,
        "grad_norm": 2.075039863586426,
        "learning_rate": 6.144844943734898e-05,
        "epoch": 1.085717968548595,
        "step": 8423
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.635455846786499,
        "learning_rate": 6.139231910622094e-05,
        "epoch": 1.08584686774942,
        "step": 8424
    },
    {
        "loss": 1.8078,
        "grad_norm": 1.8743488788604736,
        "learning_rate": 6.133620306590967e-05,
        "epoch": 1.085975766950245,
        "step": 8425
    },
    {
        "loss": 1.9739,
        "grad_norm": 1.7529501914978027,
        "learning_rate": 6.128010133718675e-05,
        "epoch": 1.0861046661510698,
        "step": 8426
    },
    {
        "loss": 1.0693,
        "grad_norm": 2.476017713546753,
        "learning_rate": 6.122401394081833e-05,
        "epoch": 1.0862335653518949,
        "step": 8427
    },
    {
        "loss": 1.0995,
        "grad_norm": 3.5629141330718994,
        "learning_rate": 6.116794089756566e-05,
        "epoch": 1.0863624645527197,
        "step": 8428
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.5717692375183105,
        "learning_rate": 6.111188222818431e-05,
        "epoch": 1.0864913637535447,
        "step": 8429
    },
    {
        "loss": 1.4939,
        "grad_norm": 2.658074140548706,
        "learning_rate": 6.105583795342463e-05,
        "epoch": 1.0866202629543698,
        "step": 8430
    },
    {
        "loss": 1.3407,
        "grad_norm": 2.407329559326172,
        "learning_rate": 6.0999808094031764e-05,
        "epoch": 1.0867491621551946,
        "step": 8431
    },
    {
        "loss": 1.9897,
        "grad_norm": 2.373518466949463,
        "learning_rate": 6.094379267074558e-05,
        "epoch": 1.0868780613560196,
        "step": 8432
    },
    {
        "loss": 1.6879,
        "grad_norm": 3.1965606212615967,
        "learning_rate": 6.088779170430008e-05,
        "epoch": 1.0870069605568446,
        "step": 8433
    },
    {
        "loss": 1.3933,
        "grad_norm": 2.1787519454956055,
        "learning_rate": 6.083180521542451e-05,
        "epoch": 1.0871358597576695,
        "step": 8434
    },
    {
        "loss": 0.7866,
        "grad_norm": 2.7326927185058594,
        "learning_rate": 6.077583322484255e-05,
        "epoch": 1.0872647589584945,
        "step": 8435
    },
    {
        "loss": 1.3827,
        "grad_norm": 2.4787039756774902,
        "learning_rate": 6.0719875753272485e-05,
        "epoch": 1.0873936581593193,
        "step": 8436
    },
    {
        "loss": 1.9931,
        "grad_norm": 2.7280921936035156,
        "learning_rate": 6.0663932821427203e-05,
        "epoch": 1.0875225573601444,
        "step": 8437
    },
    {
        "loss": 1.7276,
        "grad_norm": 2.870199203491211,
        "learning_rate": 6.060800445001426e-05,
        "epoch": 1.0876514565609694,
        "step": 8438
    },
    {
        "loss": 1.3917,
        "grad_norm": 1.3602930307388306,
        "learning_rate": 6.0552090659735814e-05,
        "epoch": 1.0877803557617942,
        "step": 8439
    },
    {
        "loss": 1.7444,
        "grad_norm": 1.87626314163208,
        "learning_rate": 6.0496191471288557e-05,
        "epoch": 1.0879092549626193,
        "step": 8440
    },
    {
        "loss": 1.3781,
        "grad_norm": 2.0513298511505127,
        "learning_rate": 6.0440306905363865e-05,
        "epoch": 1.0880381541634443,
        "step": 8441
    },
    {
        "loss": 1.0871,
        "grad_norm": 2.519287586212158,
        "learning_rate": 6.03844369826478e-05,
        "epoch": 1.088167053364269,
        "step": 8442
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.4195144176483154,
        "learning_rate": 6.0328581723820856e-05,
        "epoch": 1.0882959525650941,
        "step": 8443
    },
    {
        "loss": 1.417,
        "grad_norm": 2.4478063583374023,
        "learning_rate": 6.02727411495579e-05,
        "epoch": 1.088424851765919,
        "step": 8444
    },
    {
        "loss": 1.6735,
        "grad_norm": 3.338266372680664,
        "learning_rate": 6.021691528052882e-05,
        "epoch": 1.088553750966744,
        "step": 8445
    },
    {
        "loss": 1.2723,
        "grad_norm": 2.823659896850586,
        "learning_rate": 6.016110413739775e-05,
        "epoch": 1.088682650167569,
        "step": 8446
    },
    {
        "loss": 2.1031,
        "grad_norm": 2.8500378131866455,
        "learning_rate": 6.010530774082336e-05,
        "epoch": 1.0888115493683939,
        "step": 8447
    },
    {
        "loss": 1.6569,
        "grad_norm": 2.605518102645874,
        "learning_rate": 6.0049526111459066e-05,
        "epoch": 1.088940448569219,
        "step": 8448
    },
    {
        "loss": 2.1951,
        "grad_norm": 2.4621593952178955,
        "learning_rate": 5.999375926995285e-05,
        "epoch": 1.0890693477700437,
        "step": 8449
    },
    {
        "loss": 1.6159,
        "grad_norm": 1.986709475517273,
        "learning_rate": 5.99380072369467e-05,
        "epoch": 1.0891982469708688,
        "step": 8450
    },
    {
        "loss": 1.4301,
        "grad_norm": 1.9356600046157837,
        "learning_rate": 5.988227003307771e-05,
        "epoch": 1.0893271461716938,
        "step": 8451
    },
    {
        "loss": 1.5841,
        "grad_norm": 2.3988306522369385,
        "learning_rate": 5.982654767897731e-05,
        "epoch": 1.0894560453725186,
        "step": 8452
    },
    {
        "loss": 1.8329,
        "grad_norm": 2.9430811405181885,
        "learning_rate": 5.977084019527135e-05,
        "epoch": 1.0895849445733437,
        "step": 8453
    },
    {
        "loss": 0.6812,
        "grad_norm": 2.079594850540161,
        "learning_rate": 5.971514760258021e-05,
        "epoch": 1.0897138437741687,
        "step": 8454
    },
    {
        "loss": 0.7772,
        "grad_norm": 2.5131471157073975,
        "learning_rate": 5.9659469921518764e-05,
        "epoch": 1.0898427429749935,
        "step": 8455
    },
    {
        "loss": 1.2284,
        "grad_norm": 1.8402042388916016,
        "learning_rate": 5.9603807172696377e-05,
        "epoch": 1.0899716421758185,
        "step": 8456
    },
    {
        "loss": 1.4967,
        "grad_norm": 2.5386250019073486,
        "learning_rate": 5.954815937671681e-05,
        "epoch": 1.0901005413766434,
        "step": 8457
    },
    {
        "loss": 1.8311,
        "grad_norm": 2.1879422664642334,
        "learning_rate": 5.949252655417845e-05,
        "epoch": 1.0902294405774684,
        "step": 8458
    },
    {
        "loss": 1.2186,
        "grad_norm": 3.0002546310424805,
        "learning_rate": 5.943690872567408e-05,
        "epoch": 1.0903583397782934,
        "step": 8459
    },
    {
        "loss": 1.2557,
        "grad_norm": 4.293703079223633,
        "learning_rate": 5.9381305911790893e-05,
        "epoch": 1.0904872389791183,
        "step": 8460
    },
    {
        "loss": 1.7617,
        "grad_norm": 1.9046193361282349,
        "learning_rate": 5.93257181331104e-05,
        "epoch": 1.0906161381799433,
        "step": 8461
    },
    {
        "loss": 1.5064,
        "grad_norm": 2.1530978679656982,
        "learning_rate": 5.927014541020883e-05,
        "epoch": 1.0907450373807683,
        "step": 8462
    },
    {
        "loss": 2.116,
        "grad_norm": 2.2689783573150635,
        "learning_rate": 5.9214587763656626e-05,
        "epoch": 1.0908739365815932,
        "step": 8463
    },
    {
        "loss": 1.6138,
        "grad_norm": 1.7254314422607422,
        "learning_rate": 5.915904521401867e-05,
        "epoch": 1.0910028357824182,
        "step": 8464
    },
    {
        "loss": 1.7813,
        "grad_norm": 2.36727237701416,
        "learning_rate": 5.9103517781854345e-05,
        "epoch": 1.091131734983243,
        "step": 8465
    },
    {
        "loss": 1.5446,
        "grad_norm": 2.328148126602173,
        "learning_rate": 5.904800548771761e-05,
        "epoch": 1.091260634184068,
        "step": 8466
    },
    {
        "loss": 2.231,
        "grad_norm": 1.4437278509140015,
        "learning_rate": 5.8992508352156214e-05,
        "epoch": 1.091389533384893,
        "step": 8467
    },
    {
        "loss": 1.2457,
        "grad_norm": 2.054593801498413,
        "learning_rate": 5.893702639571286e-05,
        "epoch": 1.091518432585718,
        "step": 8468
    },
    {
        "loss": 1.2713,
        "grad_norm": 2.4254727363586426,
        "learning_rate": 5.8881559638924545e-05,
        "epoch": 1.091647331786543,
        "step": 8469
    },
    {
        "loss": 1.5525,
        "grad_norm": 1.5623080730438232,
        "learning_rate": 5.882610810232247e-05,
        "epoch": 1.091776230987368,
        "step": 8470
    },
    {
        "loss": 2.3089,
        "grad_norm": 2.249894857406616,
        "learning_rate": 5.877067180643221e-05,
        "epoch": 1.0919051301881928,
        "step": 8471
    },
    {
        "loss": 2.1199,
        "grad_norm": 3.2480051517486572,
        "learning_rate": 5.871525077177402e-05,
        "epoch": 1.0920340293890178,
        "step": 8472
    },
    {
        "loss": 1.1565,
        "grad_norm": 3.0978550910949707,
        "learning_rate": 5.8659845018862014e-05,
        "epoch": 1.0921629285898427,
        "step": 8473
    },
    {
        "loss": 2.3339,
        "grad_norm": 3.2730040550231934,
        "learning_rate": 5.860445456820491e-05,
        "epoch": 1.0922918277906677,
        "step": 8474
    },
    {
        "loss": 1.9173,
        "grad_norm": 2.53074312210083,
        "learning_rate": 5.854907944030583e-05,
        "epoch": 1.0924207269914927,
        "step": 8475
    },
    {
        "loss": 0.9296,
        "grad_norm": 2.692328691482544,
        "learning_rate": 5.8493719655662174e-05,
        "epoch": 1.0925496261923175,
        "step": 8476
    },
    {
        "loss": 1.3455,
        "grad_norm": 3.228550672531128,
        "learning_rate": 5.843837523476563e-05,
        "epoch": 1.0926785253931426,
        "step": 8477
    },
    {
        "loss": 1.4342,
        "grad_norm": 2.9828860759735107,
        "learning_rate": 5.838304619810205e-05,
        "epoch": 1.0928074245939676,
        "step": 8478
    },
    {
        "loss": 1.689,
        "grad_norm": 2.4211201667785645,
        "learning_rate": 5.832773256615189e-05,
        "epoch": 1.0929363237947924,
        "step": 8479
    },
    {
        "loss": 1.7271,
        "grad_norm": 1.7755720615386963,
        "learning_rate": 5.827243435938972e-05,
        "epoch": 1.0930652229956175,
        "step": 8480
    },
    {
        "loss": 2.1213,
        "grad_norm": 1.4643900394439697,
        "learning_rate": 5.821715159828432e-05,
        "epoch": 1.0931941221964423,
        "step": 8481
    },
    {
        "loss": 1.778,
        "grad_norm": 2.312565326690674,
        "learning_rate": 5.816188430329898e-05,
        "epoch": 1.0933230213972673,
        "step": 8482
    },
    {
        "loss": 1.5548,
        "grad_norm": 2.0953469276428223,
        "learning_rate": 5.810663249489132e-05,
        "epoch": 1.0934519205980924,
        "step": 8483
    },
    {
        "loss": 1.2689,
        "grad_norm": 1.6856813430786133,
        "learning_rate": 5.805139619351267e-05,
        "epoch": 1.0935808197989172,
        "step": 8484
    },
    {
        "loss": 1.76,
        "grad_norm": 2.5374503135681152,
        "learning_rate": 5.799617541960928e-05,
        "epoch": 1.0937097189997422,
        "step": 8485
    },
    {
        "loss": 1.88,
        "grad_norm": 1.2849406003952026,
        "learning_rate": 5.79409701936213e-05,
        "epoch": 1.093838618200567,
        "step": 8486
    },
    {
        "loss": 2.3433,
        "grad_norm": 1.9231983423233032,
        "learning_rate": 5.788578053598327e-05,
        "epoch": 1.093967517401392,
        "step": 8487
    },
    {
        "loss": 1.7202,
        "grad_norm": 2.2870168685913086,
        "learning_rate": 5.7830606467123815e-05,
        "epoch": 1.0940964166022171,
        "step": 8488
    },
    {
        "loss": 1.2684,
        "grad_norm": 2.9606592655181885,
        "learning_rate": 5.777544800746601e-05,
        "epoch": 1.094225315803042,
        "step": 8489
    },
    {
        "loss": 1.8419,
        "grad_norm": 1.4717400074005127,
        "learning_rate": 5.7720305177426884e-05,
        "epoch": 1.094354215003867,
        "step": 8490
    },
    {
        "loss": 1.0098,
        "grad_norm": 2.5834269523620605,
        "learning_rate": 5.7665177997417794e-05,
        "epoch": 1.094483114204692,
        "step": 8491
    },
    {
        "loss": 1.9521,
        "grad_norm": 3.4123120307922363,
        "learning_rate": 5.76100664878444e-05,
        "epoch": 1.0946120134055168,
        "step": 8492
    },
    {
        "loss": 0.8133,
        "grad_norm": 3.1023647785186768,
        "learning_rate": 5.755497066910657e-05,
        "epoch": 1.0947409126063419,
        "step": 8493
    },
    {
        "loss": 2.0459,
        "grad_norm": 1.6456173658370972,
        "learning_rate": 5.74998905615982e-05,
        "epoch": 1.0948698118071667,
        "step": 8494
    },
    {
        "loss": 1.7356,
        "grad_norm": 2.3298180103302,
        "learning_rate": 5.7444826185707454e-05,
        "epoch": 1.0949987110079917,
        "step": 8495
    },
    {
        "loss": 1.6772,
        "grad_norm": 3.616953134536743,
        "learning_rate": 5.7389777561816694e-05,
        "epoch": 1.0951276102088168,
        "step": 8496
    },
    {
        "loss": 1.6236,
        "grad_norm": 1.5251532793045044,
        "learning_rate": 5.73347447103024e-05,
        "epoch": 1.0952565094096416,
        "step": 8497
    },
    {
        "loss": 1.7374,
        "grad_norm": 2.5020453929901123,
        "learning_rate": 5.7279727651535175e-05,
        "epoch": 1.0953854086104666,
        "step": 8498
    },
    {
        "loss": 1.0074,
        "grad_norm": 2.908796548843384,
        "learning_rate": 5.7224726405879925e-05,
        "epoch": 1.0955143078112917,
        "step": 8499
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.2439942359924316,
        "learning_rate": 5.716974099369583e-05,
        "epoch": 1.0956432070121165,
        "step": 8500
    },
    {
        "loss": 1.7936,
        "grad_norm": 2.606886386871338,
        "learning_rate": 5.711477143533559e-05,
        "epoch": 1.0957721062129415,
        "step": 8501
    },
    {
        "loss": 1.2232,
        "grad_norm": 1.9460166692733765,
        "learning_rate": 5.70598177511467e-05,
        "epoch": 1.0959010054137663,
        "step": 8502
    },
    {
        "loss": 2.0168,
        "grad_norm": 2.9541778564453125,
        "learning_rate": 5.700487996147048e-05,
        "epoch": 1.0960299046145914,
        "step": 8503
    },
    {
        "loss": 2.0514,
        "grad_norm": 1.7119399309158325,
        "learning_rate": 5.694995808664245e-05,
        "epoch": 1.0961588038154164,
        "step": 8504
    },
    {
        "loss": 1.2087,
        "grad_norm": 3.1756532192230225,
        "learning_rate": 5.689505214699209e-05,
        "epoch": 1.0962877030162412,
        "step": 8505
    },
    {
        "loss": 2.1974,
        "grad_norm": 1.7446339130401611,
        "learning_rate": 5.684016216284334e-05,
        "epoch": 1.0964166022170663,
        "step": 8506
    },
    {
        "loss": 1.9352,
        "grad_norm": 2.3830933570861816,
        "learning_rate": 5.6785288154513725e-05,
        "epoch": 1.0965455014178913,
        "step": 8507
    },
    {
        "loss": 1.8631,
        "grad_norm": 2.362191677093506,
        "learning_rate": 5.673043014231515e-05,
        "epoch": 1.0966744006187161,
        "step": 8508
    },
    {
        "loss": 1.9649,
        "grad_norm": 2.7282328605651855,
        "learning_rate": 5.667558814655364e-05,
        "epoch": 1.0968032998195412,
        "step": 8509
    },
    {
        "loss": 0.5155,
        "grad_norm": 1.9703232049942017,
        "learning_rate": 5.662076218752929e-05,
        "epoch": 1.096932199020366,
        "step": 8510
    },
    {
        "loss": 2.0037,
        "grad_norm": 2.4284448623657227,
        "learning_rate": 5.6565952285536117e-05,
        "epoch": 1.097061098221191,
        "step": 8511
    },
    {
        "loss": 1.7309,
        "grad_norm": 1.79698646068573,
        "learning_rate": 5.6511158460862265e-05,
        "epoch": 1.097189997422016,
        "step": 8512
    },
    {
        "loss": 1.5844,
        "grad_norm": 1.882303237915039,
        "learning_rate": 5.645638073378994e-05,
        "epoch": 1.0973188966228409,
        "step": 8513
    },
    {
        "loss": 2.2635,
        "grad_norm": 2.007215976715088,
        "learning_rate": 5.640161912459532e-05,
        "epoch": 1.097447795823666,
        "step": 8514
    },
    {
        "loss": 2.0491,
        "grad_norm": 1.619998574256897,
        "learning_rate": 5.6346873653548715e-05,
        "epoch": 1.097576695024491,
        "step": 8515
    },
    {
        "loss": 1.5573,
        "grad_norm": 1.7194480895996094,
        "learning_rate": 5.62921443409144e-05,
        "epoch": 1.0977055942253158,
        "step": 8516
    },
    {
        "loss": 1.6853,
        "grad_norm": 2.776886463165283,
        "learning_rate": 5.623743120695091e-05,
        "epoch": 1.0978344934261408,
        "step": 8517
    },
    {
        "loss": 1.3443,
        "grad_norm": 2.211824893951416,
        "learning_rate": 5.6182734271910186e-05,
        "epoch": 1.0979633926269656,
        "step": 8518
    },
    {
        "loss": 0.75,
        "grad_norm": 2.5762641429901123,
        "learning_rate": 5.612805355603876e-05,
        "epoch": 1.0980922918277907,
        "step": 8519
    },
    {
        "loss": 2.2046,
        "grad_norm": 1.8030738830566406,
        "learning_rate": 5.6073389079577045e-05,
        "epoch": 1.0982211910286157,
        "step": 8520
    },
    {
        "loss": 1.1043,
        "grad_norm": 2.2107107639312744,
        "learning_rate": 5.601874086275925e-05,
        "epoch": 1.0983500902294405,
        "step": 8521
    },
    {
        "loss": 1.6801,
        "grad_norm": 1.3055435419082642,
        "learning_rate": 5.596410892581361e-05,
        "epoch": 1.0984789894302656,
        "step": 8522
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.3429416418075562,
        "learning_rate": 5.5909493288962625e-05,
        "epoch": 1.0986078886310904,
        "step": 8523
    },
    {
        "loss": 1.9465,
        "grad_norm": 1.8598397970199585,
        "learning_rate": 5.585489397242232e-05,
        "epoch": 1.0987367878319154,
        "step": 8524
    },
    {
        "loss": 1.8995,
        "grad_norm": 2.34751558303833,
        "learning_rate": 5.580031099640286e-05,
        "epoch": 1.0988656870327405,
        "step": 8525
    },
    {
        "loss": 1.8847,
        "grad_norm": 1.2711225748062134,
        "learning_rate": 5.5745744381108486e-05,
        "epoch": 1.0989945862335653,
        "step": 8526
    },
    {
        "loss": 1.5842,
        "grad_norm": 1.9915670156478882,
        "learning_rate": 5.569119414673737e-05,
        "epoch": 1.0991234854343903,
        "step": 8527
    },
    {
        "loss": 1.3355,
        "grad_norm": 3.67179274559021,
        "learning_rate": 5.563666031348147e-05,
        "epoch": 1.0992523846352154,
        "step": 8528
    },
    {
        "loss": 1.3514,
        "grad_norm": 2.8337490558624268,
        "learning_rate": 5.558214290152672e-05,
        "epoch": 1.0993812838360402,
        "step": 8529
    },
    {
        "loss": 1.7063,
        "grad_norm": 2.5721561908721924,
        "learning_rate": 5.552764193105303e-05,
        "epoch": 1.0995101830368652,
        "step": 8530
    },
    {
        "loss": 0.804,
        "grad_norm": 2.379611015319824,
        "learning_rate": 5.547315742223413e-05,
        "epoch": 1.09963908223769,
        "step": 8531
    },
    {
        "loss": 1.5844,
        "grad_norm": 1.905895709991455,
        "learning_rate": 5.541868939523778e-05,
        "epoch": 1.099767981438515,
        "step": 8532
    },
    {
        "loss": 2.3478,
        "grad_norm": 1.7494515180587769,
        "learning_rate": 5.536423787022551e-05,
        "epoch": 1.09989688063934,
        "step": 8533
    },
    {
        "loss": 2.4368,
        "grad_norm": 1.6120914220809937,
        "learning_rate": 5.530980286735296e-05,
        "epoch": 1.100025779840165,
        "step": 8534
    },
    {
        "loss": 0.9564,
        "grad_norm": 2.3634262084960938,
        "learning_rate": 5.525538440676945e-05,
        "epoch": 1.10015467904099,
        "step": 8535
    },
    {
        "loss": 2.0088,
        "grad_norm": 2.303230047225952,
        "learning_rate": 5.520098250861807e-05,
        "epoch": 1.100283578241815,
        "step": 8536
    },
    {
        "loss": 1.359,
        "grad_norm": 3.060476779937744,
        "learning_rate": 5.514659719303614e-05,
        "epoch": 1.1004124774426398,
        "step": 8537
    },
    {
        "loss": 2.196,
        "grad_norm": 2.3091564178466797,
        "learning_rate": 5.509222848015458e-05,
        "epoch": 1.1005413766434649,
        "step": 8538
    },
    {
        "loss": 1.1193,
        "grad_norm": 4.503228664398193,
        "learning_rate": 5.503787639009814e-05,
        "epoch": 1.1006702758442897,
        "step": 8539
    },
    {
        "loss": 1.9132,
        "grad_norm": 1.7481935024261475,
        "learning_rate": 5.498354094298574e-05,
        "epoch": 1.1007991750451147,
        "step": 8540
    },
    {
        "loss": 1.1401,
        "grad_norm": 1.9811203479766846,
        "learning_rate": 5.492922215892966e-05,
        "epoch": 1.1009280742459397,
        "step": 8541
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.3610092401504517,
        "learning_rate": 5.487492005803627e-05,
        "epoch": 1.1010569734467646,
        "step": 8542
    },
    {
        "loss": 1.8257,
        "grad_norm": 2.2514567375183105,
        "learning_rate": 5.482063466040582e-05,
        "epoch": 1.1011858726475896,
        "step": 8543
    },
    {
        "loss": 1.8104,
        "grad_norm": 2.44785475730896,
        "learning_rate": 5.4766365986132395e-05,
        "epoch": 1.1013147718484146,
        "step": 8544
    },
    {
        "loss": 2.1283,
        "grad_norm": 2.593508720397949,
        "learning_rate": 5.471211405530372e-05,
        "epoch": 1.1014436710492395,
        "step": 8545
    },
    {
        "loss": 1.9907,
        "grad_norm": 1.4369930028915405,
        "learning_rate": 5.46578788880014e-05,
        "epoch": 1.1015725702500645,
        "step": 8546
    },
    {
        "loss": 1.0407,
        "grad_norm": 2.726557970046997,
        "learning_rate": 5.46036605043008e-05,
        "epoch": 1.1017014694508893,
        "step": 8547
    },
    {
        "loss": 0.7801,
        "grad_norm": 2.222264528274536,
        "learning_rate": 5.45494589242712e-05,
        "epoch": 1.1018303686517144,
        "step": 8548
    },
    {
        "loss": 1.9874,
        "grad_norm": 2.810422420501709,
        "learning_rate": 5.44952741679755e-05,
        "epoch": 1.1019592678525394,
        "step": 8549
    },
    {
        "loss": 1.2312,
        "grad_norm": 2.333174228668213,
        "learning_rate": 5.444110625547043e-05,
        "epoch": 1.1020881670533642,
        "step": 8550
    },
    {
        "loss": 2.0404,
        "grad_norm": 1.6198992729187012,
        "learning_rate": 5.4386955206806614e-05,
        "epoch": 1.1022170662541892,
        "step": 8551
    },
    {
        "loss": 2.102,
        "grad_norm": 1.3971198797225952,
        "learning_rate": 5.4332821042028304e-05,
        "epoch": 1.1023459654550143,
        "step": 8552
    },
    {
        "loss": 1.1768,
        "grad_norm": 2.64986252784729,
        "learning_rate": 5.4278703781173345e-05,
        "epoch": 1.102474864655839,
        "step": 8553
    },
    {
        "loss": 1.9057,
        "grad_norm": 2.3982841968536377,
        "learning_rate": 5.422460344427369e-05,
        "epoch": 1.1026037638566641,
        "step": 8554
    },
    {
        "loss": 2.4418,
        "grad_norm": 1.3787460327148438,
        "learning_rate": 5.417052005135477e-05,
        "epoch": 1.102732663057489,
        "step": 8555
    },
    {
        "loss": 2.0273,
        "grad_norm": 1.7948617935180664,
        "learning_rate": 5.4116453622435725e-05,
        "epoch": 1.102861562258314,
        "step": 8556
    },
    {
        "loss": 1.045,
        "grad_norm": 3.399108409881592,
        "learning_rate": 5.4062404177529625e-05,
        "epoch": 1.102990461459139,
        "step": 8557
    },
    {
        "loss": 1.787,
        "grad_norm": 2.5304737091064453,
        "learning_rate": 5.4008371736643206e-05,
        "epoch": 1.1031193606599639,
        "step": 8558
    },
    {
        "loss": 0.8168,
        "grad_norm": 3.453700304031372,
        "learning_rate": 5.395435631977659e-05,
        "epoch": 1.103248259860789,
        "step": 8559
    },
    {
        "loss": 1.654,
        "grad_norm": 2.941823720932007,
        "learning_rate": 5.390035794692392e-05,
        "epoch": 1.1033771590616137,
        "step": 8560
    },
    {
        "loss": 1.6884,
        "grad_norm": 1.846627116203308,
        "learning_rate": 5.3846376638073085e-05,
        "epoch": 1.1035060582624387,
        "step": 8561
    },
    {
        "loss": 1.6097,
        "grad_norm": 1.3428442478179932,
        "learning_rate": 5.3792412413205416e-05,
        "epoch": 1.1036349574632638,
        "step": 8562
    },
    {
        "loss": 1.4876,
        "grad_norm": 2.7663028240203857,
        "learning_rate": 5.3738465292296045e-05,
        "epoch": 1.1037638566640886,
        "step": 8563
    },
    {
        "loss": 1.2966,
        "grad_norm": 3.3592069149017334,
        "learning_rate": 5.368453529531372e-05,
        "epoch": 1.1038927558649136,
        "step": 8564
    },
    {
        "loss": 1.5127,
        "grad_norm": 2.9117140769958496,
        "learning_rate": 5.363062244222097e-05,
        "epoch": 1.1040216550657387,
        "step": 8565
    },
    {
        "loss": 2.0067,
        "grad_norm": 1.423323154449463,
        "learning_rate": 5.357672675297375e-05,
        "epoch": 1.1041505542665635,
        "step": 8566
    },
    {
        "loss": 1.4974,
        "grad_norm": 2.222188949584961,
        "learning_rate": 5.352284824752194e-05,
        "epoch": 1.1042794534673885,
        "step": 8567
    },
    {
        "loss": 1.7412,
        "grad_norm": 3.1512885093688965,
        "learning_rate": 5.346898694580893e-05,
        "epoch": 1.1044083526682134,
        "step": 8568
    },
    {
        "loss": 1.8208,
        "grad_norm": 1.2881547212600708,
        "learning_rate": 5.341514286777172e-05,
        "epoch": 1.1045372518690384,
        "step": 8569
    },
    {
        "loss": 1.4116,
        "grad_norm": 2.3683862686157227,
        "learning_rate": 5.33613160333408e-05,
        "epoch": 1.1046661510698634,
        "step": 8570
    },
    {
        "loss": 0.5761,
        "grad_norm": 1.9359062910079956,
        "learning_rate": 5.330750646244065e-05,
        "epoch": 1.1047950502706883,
        "step": 8571
    },
    {
        "loss": 1.7268,
        "grad_norm": 2.0755064487457275,
        "learning_rate": 5.3253714174989024e-05,
        "epoch": 1.1049239494715133,
        "step": 8572
    },
    {
        "loss": 1.6009,
        "grad_norm": 3.3361167907714844,
        "learning_rate": 5.319993919089734e-05,
        "epoch": 1.1050528486723383,
        "step": 8573
    },
    {
        "loss": 1.0347,
        "grad_norm": 2.2479629516601562,
        "learning_rate": 5.3146181530070715e-05,
        "epoch": 1.1051817478731631,
        "step": 8574
    },
    {
        "loss": 1.1111,
        "grad_norm": 1.216446042060852,
        "learning_rate": 5.309244121240804e-05,
        "epoch": 1.1053106470739882,
        "step": 8575
    },
    {
        "loss": 1.3792,
        "grad_norm": 2.3348281383514404,
        "learning_rate": 5.303871825780116e-05,
        "epoch": 1.105439546274813,
        "step": 8576
    },
    {
        "loss": 1.6619,
        "grad_norm": 2.0922656059265137,
        "learning_rate": 5.298501268613602e-05,
        "epoch": 1.105568445475638,
        "step": 8577
    },
    {
        "loss": 1.735,
        "grad_norm": 2.230201482772827,
        "learning_rate": 5.293132451729209e-05,
        "epoch": 1.105697344676463,
        "step": 8578
    },
    {
        "loss": 1.471,
        "grad_norm": 2.602957010269165,
        "learning_rate": 5.2877653771142246e-05,
        "epoch": 1.105826243877288,
        "step": 8579
    },
    {
        "loss": 1.2076,
        "grad_norm": 1.9137272834777832,
        "learning_rate": 5.282400046755291e-05,
        "epoch": 1.105955143078113,
        "step": 8580
    },
    {
        "loss": 2.2083,
        "grad_norm": 1.8414386510849,
        "learning_rate": 5.277036462638422e-05,
        "epoch": 1.106084042278938,
        "step": 8581
    },
    {
        "loss": 1.4108,
        "grad_norm": 3.7923147678375244,
        "learning_rate": 5.271674626748966e-05,
        "epoch": 1.1062129414797628,
        "step": 8582
    },
    {
        "loss": 1.0412,
        "grad_norm": 1.1809486150741577,
        "learning_rate": 5.266314541071625e-05,
        "epoch": 1.1063418406805878,
        "step": 8583
    },
    {
        "loss": 1.6665,
        "grad_norm": 2.458651304244995,
        "learning_rate": 5.260956207590474e-05,
        "epoch": 1.1064707398814126,
        "step": 8584
    },
    {
        "loss": 1.4425,
        "grad_norm": 1.9206151962280273,
        "learning_rate": 5.2555996282889196e-05,
        "epoch": 1.1065996390822377,
        "step": 8585
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.5171762704849243,
        "learning_rate": 5.250244805149738e-05,
        "epoch": 1.1067285382830627,
        "step": 8586
    },
    {
        "loss": 1.2199,
        "grad_norm": 2.890152931213379,
        "learning_rate": 5.244891740155014e-05,
        "epoch": 1.1068574374838875,
        "step": 8587
    },
    {
        "loss": 1.2723,
        "grad_norm": 2.953129768371582,
        "learning_rate": 5.239540435286235e-05,
        "epoch": 1.1069863366847126,
        "step": 8588
    },
    {
        "loss": 1.8064,
        "grad_norm": 1.8778263330459595,
        "learning_rate": 5.2341908925242037e-05,
        "epoch": 1.1071152358855376,
        "step": 8589
    },
    {
        "loss": 0.5843,
        "grad_norm": 1.866607904434204,
        "learning_rate": 5.228843113849069e-05,
        "epoch": 1.1072441350863624,
        "step": 8590
    },
    {
        "loss": 1.6958,
        "grad_norm": 1.9665452241897583,
        "learning_rate": 5.223497101240348e-05,
        "epoch": 1.1073730342871875,
        "step": 8591
    },
    {
        "loss": 1.5189,
        "grad_norm": 2.4945719242095947,
        "learning_rate": 5.218152856676909e-05,
        "epoch": 1.1075019334880123,
        "step": 8592
    },
    {
        "loss": 0.6696,
        "grad_norm": 2.465872049331665,
        "learning_rate": 5.212810382136916e-05,
        "epoch": 1.1076308326888373,
        "step": 8593
    },
    {
        "loss": 1.5767,
        "grad_norm": 1.2286959886550903,
        "learning_rate": 5.207469679597923e-05,
        "epoch": 1.1077597318896624,
        "step": 8594
    },
    {
        "loss": 1.7056,
        "grad_norm": 2.8113350868225098,
        "learning_rate": 5.2021307510368266e-05,
        "epoch": 1.1078886310904872,
        "step": 8595
    },
    {
        "loss": 1.83,
        "grad_norm": 2.017077922821045,
        "learning_rate": 5.196793598429852e-05,
        "epoch": 1.1080175302913122,
        "step": 8596
    },
    {
        "loss": 1.037,
        "grad_norm": 1.7859768867492676,
        "learning_rate": 5.191458223752561e-05,
        "epoch": 1.108146429492137,
        "step": 8597
    },
    {
        "loss": 1.9093,
        "grad_norm": 2.1942477226257324,
        "learning_rate": 5.186124628979887e-05,
        "epoch": 1.108275328692962,
        "step": 8598
    },
    {
        "loss": 2.1322,
        "grad_norm": 2.9303088188171387,
        "learning_rate": 5.180792816086073e-05,
        "epoch": 1.1084042278937871,
        "step": 8599
    },
    {
        "loss": 1.8511,
        "grad_norm": 2.3553359508514404,
        "learning_rate": 5.1754627870447094e-05,
        "epoch": 1.108533127094612,
        "step": 8600
    },
    {
        "loss": 1.454,
        "grad_norm": 3.287776231765747,
        "learning_rate": 5.1701345438287416e-05,
        "epoch": 1.108662026295437,
        "step": 8601
    },
    {
        "loss": 1.8361,
        "grad_norm": 2.322827100753784,
        "learning_rate": 5.164808088410443e-05,
        "epoch": 1.108790925496262,
        "step": 8602
    },
    {
        "loss": 1.8964,
        "grad_norm": 2.622238874435425,
        "learning_rate": 5.159483422761438e-05,
        "epoch": 1.1089198246970868,
        "step": 8603
    },
    {
        "loss": 2.2095,
        "grad_norm": 2.615898370742798,
        "learning_rate": 5.154160548852647e-05,
        "epoch": 1.1090487238979119,
        "step": 8604
    },
    {
        "loss": 1.3141,
        "grad_norm": 2.5689918994903564,
        "learning_rate": 5.148839468654383e-05,
        "epoch": 1.1091776230987367,
        "step": 8605
    },
    {
        "loss": 1.1142,
        "grad_norm": 2.229764938354492,
        "learning_rate": 5.143520184136259e-05,
        "epoch": 1.1093065222995617,
        "step": 8606
    },
    {
        "loss": 1.726,
        "grad_norm": 2.529168128967285,
        "learning_rate": 5.1382026972672295e-05,
        "epoch": 1.1094354215003868,
        "step": 8607
    },
    {
        "loss": 1.8086,
        "grad_norm": 3.6457035541534424,
        "learning_rate": 5.13288701001559e-05,
        "epoch": 1.1095643207012116,
        "step": 8608
    },
    {
        "loss": 1.6608,
        "grad_norm": 2.823772668838501,
        "learning_rate": 5.127573124348992e-05,
        "epoch": 1.1096932199020366,
        "step": 8609
    },
    {
        "loss": 1.2185,
        "grad_norm": 1.818219542503357,
        "learning_rate": 5.1222610422343584e-05,
        "epoch": 1.1098221191028617,
        "step": 8610
    },
    {
        "loss": 2.1235,
        "grad_norm": 2.6621108055114746,
        "learning_rate": 5.116950765637994e-05,
        "epoch": 1.1099510183036865,
        "step": 8611
    },
    {
        "loss": 1.7783,
        "grad_norm": 2.6655848026275635,
        "learning_rate": 5.111642296525532e-05,
        "epoch": 1.1100799175045115,
        "step": 8612
    },
    {
        "loss": 1.3877,
        "grad_norm": 3.052178144454956,
        "learning_rate": 5.106335636861919e-05,
        "epoch": 1.1102088167053363,
        "step": 8613
    },
    {
        "loss": 1.2958,
        "grad_norm": 1.7843186855316162,
        "learning_rate": 5.1010307886114425e-05,
        "epoch": 1.1103377159061614,
        "step": 8614
    },
    {
        "loss": 1.6005,
        "grad_norm": 2.7313754558563232,
        "learning_rate": 5.0957277537377256e-05,
        "epoch": 1.1104666151069864,
        "step": 8615
    },
    {
        "loss": 0.697,
        "grad_norm": 3.5379531383514404,
        "learning_rate": 5.0904265342037004e-05,
        "epoch": 1.1105955143078112,
        "step": 8616
    },
    {
        "loss": 1.6315,
        "grad_norm": 2.7694950103759766,
        "learning_rate": 5.085127131971634e-05,
        "epoch": 1.1107244135086363,
        "step": 8617
    },
    {
        "loss": 1.0782,
        "grad_norm": 2.154200315475464,
        "learning_rate": 5.0798295490031346e-05,
        "epoch": 1.1108533127094613,
        "step": 8618
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.1100046634674072,
        "learning_rate": 5.0745337872591324e-05,
        "epoch": 1.1109822119102861,
        "step": 8619
    },
    {
        "loss": 1.4002,
        "grad_norm": 2.1120643615722656,
        "learning_rate": 5.069239848699875e-05,
        "epoch": 1.1111111111111112,
        "step": 8620
    },
    {
        "loss": 1.6794,
        "grad_norm": 2.2789688110351562,
        "learning_rate": 5.063947735284936e-05,
        "epoch": 1.111240010311936,
        "step": 8621
    },
    {
        "loss": 1.937,
        "grad_norm": 1.7102148532867432,
        "learning_rate": 5.058657448973217e-05,
        "epoch": 1.111368909512761,
        "step": 8622
    },
    {
        "loss": 1.4273,
        "grad_norm": 2.87239408493042,
        "learning_rate": 5.0533689917229444e-05,
        "epoch": 1.111497808713586,
        "step": 8623
    },
    {
        "loss": 0.4599,
        "grad_norm": 1.4704434871673584,
        "learning_rate": 5.048082365491655e-05,
        "epoch": 1.1116267079144109,
        "step": 8624
    },
    {
        "loss": 0.9598,
        "grad_norm": 2.8862719535827637,
        "learning_rate": 5.0427975722362285e-05,
        "epoch": 1.111755607115236,
        "step": 8625
    },
    {
        "loss": 2.106,
        "grad_norm": 1.864256739616394,
        "learning_rate": 5.037514613912876e-05,
        "epoch": 1.111884506316061,
        "step": 8626
    },
    {
        "loss": 1.6465,
        "grad_norm": 2.685525894165039,
        "learning_rate": 5.0322334924770694e-05,
        "epoch": 1.1120134055168858,
        "step": 8627
    },
    {
        "loss": 1.7374,
        "grad_norm": 1.9047578573226929,
        "learning_rate": 5.0269542098836655e-05,
        "epoch": 1.1121423047177108,
        "step": 8628
    },
    {
        "loss": 1.473,
        "grad_norm": 1.9569834470748901,
        "learning_rate": 5.0216767680868136e-05,
        "epoch": 1.1122712039185356,
        "step": 8629
    },
    {
        "loss": 1.1742,
        "grad_norm": 1.9506521224975586,
        "learning_rate": 5.0164011690399784e-05,
        "epoch": 1.1124001031193607,
        "step": 8630
    },
    {
        "loss": 1.3569,
        "grad_norm": 2.3908557891845703,
        "learning_rate": 5.011127414695951e-05,
        "epoch": 1.1125290023201857,
        "step": 8631
    },
    {
        "loss": 1.4951,
        "grad_norm": 2.0041520595550537,
        "learning_rate": 5.005855507006845e-05,
        "epoch": 1.1126579015210105,
        "step": 8632
    },
    {
        "loss": 1.3875,
        "grad_norm": 2.8496034145355225,
        "learning_rate": 5.000585447924069e-05,
        "epoch": 1.1127868007218356,
        "step": 8633
    },
    {
        "loss": 1.3449,
        "grad_norm": 3.4976720809936523,
        "learning_rate": 4.9953172393983584e-05,
        "epoch": 1.1129156999226604,
        "step": 8634
    },
    {
        "loss": 1.2558,
        "grad_norm": 2.6711645126342773,
        "learning_rate": 4.9900508833797735e-05,
        "epoch": 1.1130445991234854,
        "step": 8635
    },
    {
        "loss": 1.0191,
        "grad_norm": 1.9167197942733765,
        "learning_rate": 4.9847863818176865e-05,
        "epoch": 1.1131734983243105,
        "step": 8636
    },
    {
        "loss": 1.4482,
        "grad_norm": 2.34527587890625,
        "learning_rate": 4.979523736660772e-05,
        "epoch": 1.1133023975251353,
        "step": 8637
    },
    {
        "loss": 1.4413,
        "grad_norm": 3.709913730621338,
        "learning_rate": 4.974262949857021e-05,
        "epoch": 1.1134312967259603,
        "step": 8638
    },
    {
        "loss": 1.5994,
        "grad_norm": 3.9531028270721436,
        "learning_rate": 4.9690040233537415e-05,
        "epoch": 1.1135601959267853,
        "step": 8639
    },
    {
        "loss": 1.7883,
        "grad_norm": 2.6496832370758057,
        "learning_rate": 4.9637469590975486e-05,
        "epoch": 1.1136890951276102,
        "step": 8640
    },
    {
        "loss": 1.6641,
        "grad_norm": 3.5918195247650146,
        "learning_rate": 4.958491759034362e-05,
        "epoch": 1.1138179943284352,
        "step": 8641
    },
    {
        "loss": 1.6445,
        "grad_norm": 2.252098798751831,
        "learning_rate": 4.953238425109427e-05,
        "epoch": 1.11394689352926,
        "step": 8642
    },
    {
        "loss": 1.4793,
        "grad_norm": 2.313211441040039,
        "learning_rate": 4.9479869592673075e-05,
        "epoch": 1.114075792730085,
        "step": 8643
    },
    {
        "loss": 1.8636,
        "grad_norm": 1.2618495225906372,
        "learning_rate": 4.9427373634518235e-05,
        "epoch": 1.11420469193091,
        "step": 8644
    },
    {
        "loss": 0.9541,
        "grad_norm": 2.5649423599243164,
        "learning_rate": 4.93748963960616e-05,
        "epoch": 1.114333591131735,
        "step": 8645
    },
    {
        "loss": 0.6413,
        "grad_norm": 3.5317115783691406,
        "learning_rate": 4.9322437896727816e-05,
        "epoch": 1.11446249033256,
        "step": 8646
    },
    {
        "loss": 1.2839,
        "grad_norm": 2.974905490875244,
        "learning_rate": 4.926999815593468e-05,
        "epoch": 1.114591389533385,
        "step": 8647
    },
    {
        "loss": 1.9106,
        "grad_norm": 3.0109713077545166,
        "learning_rate": 4.921757719309297e-05,
        "epoch": 1.1147202887342098,
        "step": 8648
    },
    {
        "loss": 1.3633,
        "grad_norm": 2.0146522521972656,
        "learning_rate": 4.9165175027606626e-05,
        "epoch": 1.1148491879350348,
        "step": 8649
    },
    {
        "loss": 0.8082,
        "grad_norm": 2.566765069961548,
        "learning_rate": 4.911279167887247e-05,
        "epoch": 1.1149780871358597,
        "step": 8650
    },
    {
        "loss": 1.9708,
        "grad_norm": 2.435495138168335,
        "learning_rate": 4.90604271662804e-05,
        "epoch": 1.1151069863366847,
        "step": 8651
    },
    {
        "loss": 1.0105,
        "grad_norm": 2.14462947845459,
        "learning_rate": 4.900808150921348e-05,
        "epoch": 1.1152358855375097,
        "step": 8652
    },
    {
        "loss": 2.0373,
        "grad_norm": 2.5211613178253174,
        "learning_rate": 4.895575472704775e-05,
        "epoch": 1.1153647847383346,
        "step": 8653
    },
    {
        "loss": 1.6563,
        "grad_norm": 5.353921413421631,
        "learning_rate": 4.8903446839152176e-05,
        "epoch": 1.1154936839391596,
        "step": 8654
    },
    {
        "loss": 2.1326,
        "grad_norm": 1.843644380569458,
        "learning_rate": 4.885115786488876e-05,
        "epoch": 1.1156225831399846,
        "step": 8655
    },
    {
        "loss": 0.6658,
        "grad_norm": 2.39516019821167,
        "learning_rate": 4.87988878236125e-05,
        "epoch": 1.1157514823408095,
        "step": 8656
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.855823278427124,
        "learning_rate": 4.874663673467138e-05,
        "epoch": 1.1158803815416345,
        "step": 8657
    },
    {
        "loss": 2.1457,
        "grad_norm": 1.7721949815750122,
        "learning_rate": 4.869440461740641e-05,
        "epoch": 1.1160092807424593,
        "step": 8658
    },
    {
        "loss": 1.1713,
        "grad_norm": 1.8052717447280884,
        "learning_rate": 4.8642191491151535e-05,
        "epoch": 1.1161381799432843,
        "step": 8659
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.7931246757507324,
        "learning_rate": 4.85899973752338e-05,
        "epoch": 1.1162670791441094,
        "step": 8660
    },
    {
        "loss": 1.5794,
        "grad_norm": 3.3943231105804443,
        "learning_rate": 4.8537822288973064e-05,
        "epoch": 1.1163959783449342,
        "step": 8661
    },
    {
        "loss": 1.5103,
        "grad_norm": 2.14343523979187,
        "learning_rate": 4.8485666251682074e-05,
        "epoch": 1.1165248775457592,
        "step": 8662
    },
    {
        "loss": 0.9469,
        "grad_norm": 2.4494709968566895,
        "learning_rate": 4.8433529282666715e-05,
        "epoch": 1.1166537767465843,
        "step": 8663
    },
    {
        "loss": 0.8514,
        "grad_norm": 3.0631766319274902,
        "learning_rate": 4.838141140122575e-05,
        "epoch": 1.116782675947409,
        "step": 8664
    },
    {
        "loss": 1.7326,
        "grad_norm": 1.863889217376709,
        "learning_rate": 4.832931262665078e-05,
        "epoch": 1.1169115751482341,
        "step": 8665
    },
    {
        "loss": 2.0222,
        "grad_norm": 1.8315904140472412,
        "learning_rate": 4.8277232978226614e-05,
        "epoch": 1.117040474349059,
        "step": 8666
    },
    {
        "loss": 1.9822,
        "grad_norm": 1.980428695678711,
        "learning_rate": 4.8225172475230576e-05,
        "epoch": 1.117169373549884,
        "step": 8667
    },
    {
        "loss": 1.1791,
        "grad_norm": 1.7596700191497803,
        "learning_rate": 4.817313113693308e-05,
        "epoch": 1.117298272750709,
        "step": 8668
    },
    {
        "loss": 1.5583,
        "grad_norm": 1.2616772651672363,
        "learning_rate": 4.8121108982597587e-05,
        "epoch": 1.1174271719515338,
        "step": 8669
    },
    {
        "loss": 0.9323,
        "grad_norm": 2.766037702560425,
        "learning_rate": 4.806910603148039e-05,
        "epoch": 1.1175560711523589,
        "step": 8670
    },
    {
        "loss": 1.7104,
        "grad_norm": 2.1142096519470215,
        "learning_rate": 4.801712230283057e-05,
        "epoch": 1.1176849703531837,
        "step": 8671
    },
    {
        "loss": 1.8341,
        "grad_norm": 1.4521257877349854,
        "learning_rate": 4.7965157815890136e-05,
        "epoch": 1.1178138695540087,
        "step": 8672
    },
    {
        "loss": 1.3768,
        "grad_norm": 2.8598809242248535,
        "learning_rate": 4.7913212589894006e-05,
        "epoch": 1.1179427687548338,
        "step": 8673
    },
    {
        "loss": 1.4613,
        "grad_norm": 2.0052428245544434,
        "learning_rate": 4.786128664406991e-05,
        "epoch": 1.1180716679556586,
        "step": 8674
    },
    {
        "loss": 1.3361,
        "grad_norm": 1.664120078086853,
        "learning_rate": 4.780937999763853e-05,
        "epoch": 1.1182005671564836,
        "step": 8675
    },
    {
        "loss": 1.2785,
        "grad_norm": 5.247436046600342,
        "learning_rate": 4.7757492669813295e-05,
        "epoch": 1.1183294663573087,
        "step": 8676
    },
    {
        "loss": 2.0414,
        "grad_norm": 2.6339941024780273,
        "learning_rate": 4.770562467980069e-05,
        "epoch": 1.1184583655581335,
        "step": 8677
    },
    {
        "loss": 1.4026,
        "grad_norm": 2.2844879627227783,
        "learning_rate": 4.765377604679984e-05,
        "epoch": 1.1185872647589585,
        "step": 8678
    },
    {
        "loss": 1.8511,
        "grad_norm": 2.311725378036499,
        "learning_rate": 4.760194679000261e-05,
        "epoch": 1.1187161639597833,
        "step": 8679
    },
    {
        "loss": 2.6175,
        "grad_norm": 1.556787371635437,
        "learning_rate": 4.7550136928594044e-05,
        "epoch": 1.1188450631606084,
        "step": 8680
    },
    {
        "loss": 1.8528,
        "grad_norm": 2.0404772758483887,
        "learning_rate": 4.7498346481751735e-05,
        "epoch": 1.1189739623614334,
        "step": 8681
    },
    {
        "loss": 1.6487,
        "grad_norm": 2.240492582321167,
        "learning_rate": 4.744657546864608e-05,
        "epoch": 1.1191028615622582,
        "step": 8682
    },
    {
        "loss": 2.035,
        "grad_norm": 1.9862065315246582,
        "learning_rate": 4.739482390844049e-05,
        "epoch": 1.1192317607630833,
        "step": 8683
    },
    {
        "loss": 1.3006,
        "grad_norm": 2.0721428394317627,
        "learning_rate": 4.7343091820291154e-05,
        "epoch": 1.1193606599639083,
        "step": 8684
    },
    {
        "loss": 1.7227,
        "grad_norm": 2.4595534801483154,
        "learning_rate": 4.729137922334661e-05,
        "epoch": 1.1194895591647331,
        "step": 8685
    },
    {
        "loss": 0.9591,
        "grad_norm": 2.9175209999084473,
        "learning_rate": 4.7239686136748715e-05,
        "epoch": 1.1196184583655582,
        "step": 8686
    },
    {
        "loss": 2.1,
        "grad_norm": 1.7573766708374023,
        "learning_rate": 4.7188012579631994e-05,
        "epoch": 1.119747357566383,
        "step": 8687
    },
    {
        "loss": 1.7361,
        "grad_norm": 2.212129831314087,
        "learning_rate": 4.7136358571123576e-05,
        "epoch": 1.119876256767208,
        "step": 8688
    },
    {
        "loss": 1.4244,
        "grad_norm": 2.561445474624634,
        "learning_rate": 4.708472413034343e-05,
        "epoch": 1.120005155968033,
        "step": 8689
    },
    {
        "loss": 0.9337,
        "grad_norm": 2.002847671508789,
        "learning_rate": 4.7033109276404266e-05,
        "epoch": 1.120134055168858,
        "step": 8690
    },
    {
        "loss": 1.5173,
        "grad_norm": 2.9533252716064453,
        "learning_rate": 4.6981514028411636e-05,
        "epoch": 1.120262954369683,
        "step": 8691
    },
    {
        "loss": 1.1565,
        "grad_norm": 2.997523546218872,
        "learning_rate": 4.692993840546368e-05,
        "epoch": 1.1203918535705077,
        "step": 8692
    },
    {
        "loss": 1.4957,
        "grad_norm": 2.362220287322998,
        "learning_rate": 4.687838242665139e-05,
        "epoch": 1.1205207527713328,
        "step": 8693
    },
    {
        "loss": 1.9783,
        "grad_norm": 1.5009589195251465,
        "learning_rate": 4.6826846111058545e-05,
        "epoch": 1.1206496519721578,
        "step": 8694
    },
    {
        "loss": 1.45,
        "grad_norm": 3.784184455871582,
        "learning_rate": 4.677532947776153e-05,
        "epoch": 1.1207785511729826,
        "step": 8695
    },
    {
        "loss": 1.2687,
        "grad_norm": 0.8663632273674011,
        "learning_rate": 4.6723832545829314e-05,
        "epoch": 1.1209074503738077,
        "step": 8696
    },
    {
        "loss": 1.5435,
        "grad_norm": 2.838750123977661,
        "learning_rate": 4.6672355334323925e-05,
        "epoch": 1.1210363495746327,
        "step": 8697
    },
    {
        "loss": 1.7436,
        "grad_norm": 1.713463306427002,
        "learning_rate": 4.662089786229982e-05,
        "epoch": 1.1211652487754575,
        "step": 8698
    },
    {
        "loss": 1.7068,
        "grad_norm": 2.1245388984680176,
        "learning_rate": 4.656946014880416e-05,
        "epoch": 1.1212941479762826,
        "step": 8699
    },
    {
        "loss": 1.502,
        "grad_norm": 2.3412928581237793,
        "learning_rate": 4.6518042212876956e-05,
        "epoch": 1.1214230471771076,
        "step": 8700
    },
    {
        "loss": 0.9816,
        "grad_norm": 1.5680800676345825,
        "learning_rate": 4.646664407355093e-05,
        "epoch": 1.1215519463779324,
        "step": 8701
    },
    {
        "loss": 1.6894,
        "grad_norm": 1.8708909749984741,
        "learning_rate": 4.641526574985099e-05,
        "epoch": 1.1216808455787575,
        "step": 8702
    },
    {
        "loss": 1.8292,
        "grad_norm": 2.804840087890625,
        "learning_rate": 4.636390726079527e-05,
        "epoch": 1.1218097447795823,
        "step": 8703
    },
    {
        "loss": 1.7956,
        "grad_norm": 1.677200436592102,
        "learning_rate": 4.631256862539444e-05,
        "epoch": 1.1219386439804073,
        "step": 8704
    },
    {
        "loss": 1.6097,
        "grad_norm": 1.9677602052688599,
        "learning_rate": 4.626124986265164e-05,
        "epoch": 1.1220675431812324,
        "step": 8705
    },
    {
        "loss": 1.5747,
        "grad_norm": 2.980710029602051,
        "learning_rate": 4.620995099156279e-05,
        "epoch": 1.1221964423820572,
        "step": 8706
    },
    {
        "loss": 0.5381,
        "grad_norm": 1.5045784711837769,
        "learning_rate": 4.6158672031116344e-05,
        "epoch": 1.1223253415828822,
        "step": 8707
    },
    {
        "loss": 1.7987,
        "grad_norm": 2.174055576324463,
        "learning_rate": 4.610741300029354e-05,
        "epoch": 1.122454240783707,
        "step": 8708
    },
    {
        "loss": 1.3504,
        "grad_norm": 1.4538450241088867,
        "learning_rate": 4.6056173918068045e-05,
        "epoch": 1.122583139984532,
        "step": 8709
    },
    {
        "loss": 1.8658,
        "grad_norm": 1.7084213495254517,
        "learning_rate": 4.6004954803406344e-05,
        "epoch": 1.1227120391853571,
        "step": 8710
    },
    {
        "loss": 1.6178,
        "grad_norm": 2.3695614337921143,
        "learning_rate": 4.595375567526745e-05,
        "epoch": 1.122840938386182,
        "step": 8711
    },
    {
        "loss": 1.4441,
        "grad_norm": 2.6627068519592285,
        "learning_rate": 4.590257655260296e-05,
        "epoch": 1.122969837587007,
        "step": 8712
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.9210201501846313,
        "learning_rate": 4.5851417454356915e-05,
        "epoch": 1.123098736787832,
        "step": 8713
    },
    {
        "loss": 1.9602,
        "grad_norm": 3.695232391357422,
        "learning_rate": 4.5800278399466304e-05,
        "epoch": 1.1232276359886568,
        "step": 8714
    },
    {
        "loss": 1.5473,
        "grad_norm": 3.008596420288086,
        "learning_rate": 4.5749159406860384e-05,
        "epoch": 1.1233565351894819,
        "step": 8715
    },
    {
        "loss": 1.994,
        "grad_norm": 4.758644104003906,
        "learning_rate": 4.5698060495461046e-05,
        "epoch": 1.1234854343903067,
        "step": 8716
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.353813409805298,
        "learning_rate": 4.5646981684182853e-05,
        "epoch": 1.1236143335911317,
        "step": 8717
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.4963085651397705,
        "learning_rate": 4.559592299193308e-05,
        "epoch": 1.1237432327919568,
        "step": 8718
    },
    {
        "loss": 0.9192,
        "grad_norm": 2.822248697280884,
        "learning_rate": 4.554488443761098e-05,
        "epoch": 1.1238721319927816,
        "step": 8719
    },
    {
        "loss": 1.0896,
        "grad_norm": 1.9835610389709473,
        "learning_rate": 4.549386604010885e-05,
        "epoch": 1.1240010311936066,
        "step": 8720
    },
    {
        "loss": 2.0293,
        "grad_norm": 2.815781354904175,
        "learning_rate": 4.54428678183115e-05,
        "epoch": 1.1241299303944317,
        "step": 8721
    },
    {
        "loss": 1.4302,
        "grad_norm": 2.1784372329711914,
        "learning_rate": 4.5391889791096075e-05,
        "epoch": 1.1242588295952565,
        "step": 8722
    },
    {
        "loss": 1.6646,
        "grad_norm": 2.2512447834014893,
        "learning_rate": 4.5340931977332266e-05,
        "epoch": 1.1243877287960815,
        "step": 8723
    },
    {
        "loss": 0.4864,
        "grad_norm": 2.22570538520813,
        "learning_rate": 4.5289994395882564e-05,
        "epoch": 1.1245166279969063,
        "step": 8724
    },
    {
        "loss": 1.3867,
        "grad_norm": 2.8863699436187744,
        "learning_rate": 4.5239077065601566e-05,
        "epoch": 1.1246455271977314,
        "step": 8725
    },
    {
        "loss": 2.0422,
        "grad_norm": 1.4662902355194092,
        "learning_rate": 4.5188180005336545e-05,
        "epoch": 1.1247744263985564,
        "step": 8726
    },
    {
        "loss": 1.3225,
        "grad_norm": 2.267277479171753,
        "learning_rate": 4.513730323392741e-05,
        "epoch": 1.1249033255993812,
        "step": 8727
    },
    {
        "loss": 1.9532,
        "grad_norm": 2.0973973274230957,
        "learning_rate": 4.508644677020644e-05,
        "epoch": 1.1250322248002063,
        "step": 8728
    },
    {
        "loss": 1.9969,
        "grad_norm": 2.429934024810791,
        "learning_rate": 4.5035610632998374e-05,
        "epoch": 1.125161124001031,
        "step": 8729
    },
    {
        "loss": 1.8692,
        "grad_norm": 2.1058685779571533,
        "learning_rate": 4.4984794841120326e-05,
        "epoch": 1.1252900232018561,
        "step": 8730
    },
    {
        "loss": 2.1739,
        "grad_norm": 2.2524116039276123,
        "learning_rate": 4.4933999413382176e-05,
        "epoch": 1.1254189224026812,
        "step": 8731
    },
    {
        "loss": 0.9891,
        "grad_norm": 2.073028802871704,
        "learning_rate": 4.488322436858602e-05,
        "epoch": 1.125547821603506,
        "step": 8732
    },
    {
        "loss": 1.6224,
        "grad_norm": 2.534769058227539,
        "learning_rate": 4.483246972552641e-05,
        "epoch": 1.125676720804331,
        "step": 8733
    },
    {
        "loss": 1.2807,
        "grad_norm": 2.4780001640319824,
        "learning_rate": 4.478173550299049e-05,
        "epoch": 1.125805620005156,
        "step": 8734
    },
    {
        "loss": 1.1893,
        "grad_norm": 2.2198426723480225,
        "learning_rate": 4.473102171975797e-05,
        "epoch": 1.1259345192059809,
        "step": 8735
    },
    {
        "loss": 2.2888,
        "grad_norm": 1.8145382404327393,
        "learning_rate": 4.468032839460046e-05,
        "epoch": 1.126063418406806,
        "step": 8736
    },
    {
        "loss": 1.1675,
        "grad_norm": 1.5650103092193604,
        "learning_rate": 4.4629655546282435e-05,
        "epoch": 1.126192317607631,
        "step": 8737
    },
    {
        "loss": 2.071,
        "grad_norm": 2.4866161346435547,
        "learning_rate": 4.457900319356081e-05,
        "epoch": 1.1263212168084558,
        "step": 8738
    },
    {
        "loss": 2.2811,
        "grad_norm": 2.3792338371276855,
        "learning_rate": 4.4528371355184726e-05,
        "epoch": 1.1264501160092808,
        "step": 8739
    },
    {
        "loss": 1.3263,
        "grad_norm": 2.2130162715911865,
        "learning_rate": 4.447776004989571e-05,
        "epoch": 1.1265790152101056,
        "step": 8740
    },
    {
        "loss": 1.8666,
        "grad_norm": 3.25203013420105,
        "learning_rate": 4.4427169296427985e-05,
        "epoch": 1.1267079144109307,
        "step": 8741
    },
    {
        "loss": 2.1732,
        "grad_norm": 2.1228742599487305,
        "learning_rate": 4.437659911350779e-05,
        "epoch": 1.1268368136117557,
        "step": 8742
    },
    {
        "loss": 1.7053,
        "grad_norm": 3.455095052719116,
        "learning_rate": 4.4326049519853884e-05,
        "epoch": 1.1269657128125805,
        "step": 8743
    },
    {
        "loss": 1.2462,
        "grad_norm": 2.4234986305236816,
        "learning_rate": 4.427552053417756e-05,
        "epoch": 1.1270946120134056,
        "step": 8744
    },
    {
        "loss": 1.5964,
        "grad_norm": 1.8840510845184326,
        "learning_rate": 4.422501217518232e-05,
        "epoch": 1.1272235112142304,
        "step": 8745
    },
    {
        "loss": 2.2187,
        "grad_norm": 1.6133652925491333,
        "learning_rate": 4.417452446156412e-05,
        "epoch": 1.1273524104150554,
        "step": 8746
    },
    {
        "loss": 1.5155,
        "grad_norm": 2.4341869354248047,
        "learning_rate": 4.4124057412011166e-05,
        "epoch": 1.1274813096158804,
        "step": 8747
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.898206353187561,
        "learning_rate": 4.4073611045204096e-05,
        "epoch": 1.1276102088167053,
        "step": 8748
    },
    {
        "loss": 2.1921,
        "grad_norm": 1.31862211227417,
        "learning_rate": 4.402318537981587e-05,
        "epoch": 1.1277391080175303,
        "step": 8749
    },
    {
        "loss": 0.398,
        "grad_norm": 1.5983973741531372,
        "learning_rate": 4.397278043451172e-05,
        "epoch": 1.1278680072183553,
        "step": 8750
    },
    {
        "loss": 0.9367,
        "grad_norm": 3.0368659496307373,
        "learning_rate": 4.392239622794933e-05,
        "epoch": 1.1279969064191802,
        "step": 8751
    },
    {
        "loss": 1.4866,
        "grad_norm": 2.405015707015991,
        "learning_rate": 4.387203277877886e-05,
        "epoch": 1.1281258056200052,
        "step": 8752
    },
    {
        "loss": 1.8902,
        "grad_norm": 2.8900418281555176,
        "learning_rate": 4.3821690105642234e-05,
        "epoch": 1.1282547048208302,
        "step": 8753
    },
    {
        "loss": 1.8922,
        "grad_norm": 1.9870916604995728,
        "learning_rate": 4.3771368227174147e-05,
        "epoch": 1.128383604021655,
        "step": 8754
    },
    {
        "loss": 1.8578,
        "grad_norm": 1.2668670415878296,
        "learning_rate": 4.37210671620016e-05,
        "epoch": 1.12851250322248,
        "step": 8755
    },
    {
        "loss": 1.5034,
        "grad_norm": 2.945359706878662,
        "learning_rate": 4.3670786928743645e-05,
        "epoch": 1.128641402423305,
        "step": 8756
    },
    {
        "loss": 0.9874,
        "grad_norm": 3.020750045776367,
        "learning_rate": 4.3620527546011804e-05,
        "epoch": 1.12877030162413,
        "step": 8757
    },
    {
        "loss": 2.076,
        "grad_norm": 1.446962594985962,
        "learning_rate": 4.357028903240988e-05,
        "epoch": 1.128899200824955,
        "step": 8758
    },
    {
        "loss": 1.144,
        "grad_norm": 2.327178955078125,
        "learning_rate": 4.35200714065338e-05,
        "epoch": 1.1290281000257798,
        "step": 8759
    },
    {
        "loss": 1.5536,
        "grad_norm": 1.9456605911254883,
        "learning_rate": 4.3469874686971826e-05,
        "epoch": 1.1291569992266048,
        "step": 8760
    },
    {
        "loss": 1.8201,
        "grad_norm": 1.7034871578216553,
        "learning_rate": 4.341969889230462e-05,
        "epoch": 1.1292858984274297,
        "step": 8761
    },
    {
        "loss": 0.4852,
        "grad_norm": 2.1614830493927,
        "learning_rate": 4.3369544041104936e-05,
        "epoch": 1.1294147976282547,
        "step": 8762
    },
    {
        "loss": 1.7688,
        "grad_norm": 1.6066190004348755,
        "learning_rate": 4.331941015193788e-05,
        "epoch": 1.1295436968290797,
        "step": 8763
    },
    {
        "loss": 1.2256,
        "grad_norm": 1.7135939598083496,
        "learning_rate": 4.326929724336072e-05,
        "epoch": 1.1296725960299046,
        "step": 8764
    },
    {
        "loss": 1.5579,
        "grad_norm": 2.1341021060943604,
        "learning_rate": 4.321920533392299e-05,
        "epoch": 1.1298014952307296,
        "step": 8765
    },
    {
        "loss": 1.1558,
        "grad_norm": 3.307621717453003,
        "learning_rate": 4.3169134442166424e-05,
        "epoch": 1.1299303944315544,
        "step": 8766
    },
    {
        "loss": 0.7975,
        "grad_norm": 2.840167760848999,
        "learning_rate": 4.311908458662495e-05,
        "epoch": 1.1300592936323794,
        "step": 8767
    },
    {
        "loss": 1.6005,
        "grad_norm": 1.6594934463500977,
        "learning_rate": 4.306905578582483e-05,
        "epoch": 1.1301881928332045,
        "step": 8768
    },
    {
        "loss": 1.9784,
        "grad_norm": 2.2061097621917725,
        "learning_rate": 4.3019048058284636e-05,
        "epoch": 1.1303170920340293,
        "step": 8769
    },
    {
        "loss": 1.7122,
        "grad_norm": 1.5494048595428467,
        "learning_rate": 4.296906142251461e-05,
        "epoch": 1.1304459912348543,
        "step": 8770
    },
    {
        "loss": 1.8929,
        "grad_norm": 2.276224374771118,
        "learning_rate": 4.291909589701776e-05,
        "epoch": 1.1305748904356794,
        "step": 8771
    },
    {
        "loss": 1.595,
        "grad_norm": 2.2680625915527344,
        "learning_rate": 4.2869151500289065e-05,
        "epoch": 1.1307037896365042,
        "step": 8772
    },
    {
        "loss": 1.5616,
        "grad_norm": 3.235044240951538,
        "learning_rate": 4.281922825081559e-05,
        "epoch": 1.1308326888373292,
        "step": 8773
    },
    {
        "loss": 2.1559,
        "grad_norm": 2.1907196044921875,
        "learning_rate": 4.276932616707673e-05,
        "epoch": 1.1309615880381543,
        "step": 8774
    },
    {
        "loss": 1.2092,
        "grad_norm": 3.096113443374634,
        "learning_rate": 4.271944526754402e-05,
        "epoch": 1.131090487238979,
        "step": 8775
    },
    {
        "loss": 1.0339,
        "grad_norm": 3.1331288814544678,
        "learning_rate": 4.2669585570681036e-05,
        "epoch": 1.1312193864398041,
        "step": 8776
    },
    {
        "loss": 1.4469,
        "grad_norm": 3.6003851890563965,
        "learning_rate": 4.261974709494352e-05,
        "epoch": 1.131348285640629,
        "step": 8777
    },
    {
        "loss": 1.0181,
        "grad_norm": 3.038457155227661,
        "learning_rate": 4.25699298587795e-05,
        "epoch": 1.131477184841454,
        "step": 8778
    },
    {
        "loss": 1.3897,
        "grad_norm": 2.6555533409118652,
        "learning_rate": 4.2520133880629144e-05,
        "epoch": 1.131606084042279,
        "step": 8779
    },
    {
        "loss": 2.1052,
        "grad_norm": 3.3166325092315674,
        "learning_rate": 4.247035917892458e-05,
        "epoch": 1.1317349832431038,
        "step": 8780
    },
    {
        "loss": 1.4968,
        "grad_norm": 2.0840914249420166,
        "learning_rate": 4.2420605772090175e-05,
        "epoch": 1.1318638824439289,
        "step": 8781
    },
    {
        "loss": 1.7354,
        "grad_norm": 2.2302865982055664,
        "learning_rate": 4.2370873678542364e-05,
        "epoch": 1.1319927816447537,
        "step": 8782
    },
    {
        "loss": 2.1689,
        "grad_norm": 1.587743878364563,
        "learning_rate": 4.2321162916689725e-05,
        "epoch": 1.1321216808455787,
        "step": 8783
    },
    {
        "loss": 1.6693,
        "grad_norm": 2.3638341426849365,
        "learning_rate": 4.227147350493288e-05,
        "epoch": 1.1322505800464038,
        "step": 8784
    },
    {
        "loss": 2.3662,
        "grad_norm": 2.0883164405822754,
        "learning_rate": 4.222180546166465e-05,
        "epoch": 1.1323794792472286,
        "step": 8785
    },
    {
        "loss": 1.7377,
        "grad_norm": 1.711682677268982,
        "learning_rate": 4.217215880526998e-05,
        "epoch": 1.1325083784480536,
        "step": 8786
    },
    {
        "loss": 2.0195,
        "grad_norm": 2.7872321605682373,
        "learning_rate": 4.2122533554125775e-05,
        "epoch": 1.1326372776488787,
        "step": 8787
    },
    {
        "loss": 1.3834,
        "grad_norm": 2.9451253414154053,
        "learning_rate": 4.2072929726600964e-05,
        "epoch": 1.1327661768497035,
        "step": 8788
    },
    {
        "loss": 0.981,
        "grad_norm": 3.5156826972961426,
        "learning_rate": 4.202334734105669e-05,
        "epoch": 1.1328950760505285,
        "step": 8789
    },
    {
        "loss": 1.0893,
        "grad_norm": 1.7727795839309692,
        "learning_rate": 4.197378641584615e-05,
        "epoch": 1.1330239752513536,
        "step": 8790
    },
    {
        "loss": 1.8055,
        "grad_norm": 3.3347110748291016,
        "learning_rate": 4.1924246969314506e-05,
        "epoch": 1.1331528744521784,
        "step": 8791
    },
    {
        "loss": 2.288,
        "grad_norm": 1.9171737432479858,
        "learning_rate": 4.18747290197991e-05,
        "epoch": 1.1332817736530034,
        "step": 8792
    },
    {
        "loss": 1.9468,
        "grad_norm": 1.992344617843628,
        "learning_rate": 4.1825232585629135e-05,
        "epoch": 1.1334106728538282,
        "step": 8793
    },
    {
        "loss": 1.1676,
        "grad_norm": 2.61820912361145,
        "learning_rate": 4.177575768512593e-05,
        "epoch": 1.1335395720546533,
        "step": 8794
    },
    {
        "loss": 1.234,
        "grad_norm": 2.247572898864746,
        "learning_rate": 4.172630433660291e-05,
        "epoch": 1.1336684712554783,
        "step": 8795
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.2746551036834717,
        "learning_rate": 4.1676872558365546e-05,
        "epoch": 1.1337973704563031,
        "step": 8796
    },
    {
        "loss": 1.6596,
        "grad_norm": 2.1233699321746826,
        "learning_rate": 4.1627462368711175e-05,
        "epoch": 1.1339262696571282,
        "step": 8797
    },
    {
        "loss": 1.9868,
        "grad_norm": 1.4364513158798218,
        "learning_rate": 4.157807378592922e-05,
        "epoch": 1.134055168857953,
        "step": 8798
    },
    {
        "loss": 1.9909,
        "grad_norm": 2.4262115955352783,
        "learning_rate": 4.1528706828301075e-05,
        "epoch": 1.134184068058778,
        "step": 8799
    },
    {
        "loss": 1.8254,
        "grad_norm": 2.467960834503174,
        "learning_rate": 4.147936151410018e-05,
        "epoch": 1.134312967259603,
        "step": 8800
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.35827374458313,
        "learning_rate": 4.143003786159186e-05,
        "epoch": 1.1344418664604279,
        "step": 8801
    },
    {
        "loss": 2.3459,
        "grad_norm": 1.7444387674331665,
        "learning_rate": 4.138073588903356e-05,
        "epoch": 1.134570765661253,
        "step": 8802
    },
    {
        "loss": 0.8841,
        "grad_norm": 2.5723893642425537,
        "learning_rate": 4.133145561467473e-05,
        "epoch": 1.1346996648620777,
        "step": 8803
    },
    {
        "loss": 1.5953,
        "grad_norm": 1.5291606187820435,
        "learning_rate": 4.128219705675666e-05,
        "epoch": 1.1348285640629028,
        "step": 8804
    },
    {
        "loss": 1.6367,
        "grad_norm": 2.2863333225250244,
        "learning_rate": 4.1232960233512496e-05,
        "epoch": 1.1349574632637278,
        "step": 8805
    },
    {
        "loss": 0.8079,
        "grad_norm": 2.1622679233551025,
        "learning_rate": 4.1183745163167595e-05,
        "epoch": 1.1350863624645526,
        "step": 8806
    },
    {
        "loss": 1.5344,
        "grad_norm": 1.974989414215088,
        "learning_rate": 4.1134551863939186e-05,
        "epoch": 1.1352152616653777,
        "step": 8807
    },
    {
        "loss": 1.7728,
        "grad_norm": 1.7790457010269165,
        "learning_rate": 4.108538035403626e-05,
        "epoch": 1.1353441608662027,
        "step": 8808
    },
    {
        "loss": 0.6933,
        "grad_norm": 1.0862879753112793,
        "learning_rate": 4.103623065166002e-05,
        "epoch": 1.1354730600670275,
        "step": 8809
    },
    {
        "loss": 1.9548,
        "grad_norm": 2.03842830657959,
        "learning_rate": 4.0987102775003574e-05,
        "epoch": 1.1356019592678526,
        "step": 8810
    },
    {
        "loss": 1.9358,
        "grad_norm": 1.8879910707473755,
        "learning_rate": 4.0937996742251505e-05,
        "epoch": 1.1357308584686776,
        "step": 8811
    },
    {
        "loss": 2.089,
        "grad_norm": 1.8728313446044922,
        "learning_rate": 4.0888912571580815e-05,
        "epoch": 1.1358597576695024,
        "step": 8812
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.698582410812378,
        "learning_rate": 4.083985028116033e-05,
        "epoch": 1.1359886568703275,
        "step": 8813
    },
    {
        "loss": 1.9852,
        "grad_norm": 1.803303837776184,
        "learning_rate": 4.0790809889150604e-05,
        "epoch": 1.1361175560711523,
        "step": 8814
    },
    {
        "loss": 2.1342,
        "grad_norm": 1.716212511062622,
        "learning_rate": 4.074179141370419e-05,
        "epoch": 1.1362464552719773,
        "step": 8815
    },
    {
        "loss": 1.2147,
        "grad_norm": 2.6212735176086426,
        "learning_rate": 4.06927948729655e-05,
        "epoch": 1.1363753544728024,
        "step": 8816
    },
    {
        "loss": 1.5345,
        "grad_norm": 2.4818079471588135,
        "learning_rate": 4.06438202850708e-05,
        "epoch": 1.1365042536736272,
        "step": 8817
    },
    {
        "loss": 1.4156,
        "grad_norm": 2.174504518508911,
        "learning_rate": 4.059486766814829e-05,
        "epoch": 1.1366331528744522,
        "step": 8818
    },
    {
        "loss": 1.0817,
        "grad_norm": 1.8517377376556396,
        "learning_rate": 4.054593704031802e-05,
        "epoch": 1.136762052075277,
        "step": 8819
    },
    {
        "loss": 1.2294,
        "grad_norm": 2.9394631385803223,
        "learning_rate": 4.049702841969195e-05,
        "epoch": 1.136890951276102,
        "step": 8820
    },
    {
        "loss": 1.9646,
        "grad_norm": 3.2681360244750977,
        "learning_rate": 4.044814182437383e-05,
        "epoch": 1.137019850476927,
        "step": 8821
    },
    {
        "loss": 1.1919,
        "grad_norm": 2.899143934249878,
        "learning_rate": 4.039927727245913e-05,
        "epoch": 1.137148749677752,
        "step": 8822
    },
    {
        "loss": 2.1594,
        "grad_norm": 2.978684902191162,
        "learning_rate": 4.035043478203545e-05,
        "epoch": 1.137277648878577,
        "step": 8823
    },
    {
        "loss": 2.298,
        "grad_norm": 1.906219720840454,
        "learning_rate": 4.030161437118203e-05,
        "epoch": 1.137406548079402,
        "step": 8824
    },
    {
        "loss": 1.7127,
        "grad_norm": 1.343375563621521,
        "learning_rate": 4.025281605796988e-05,
        "epoch": 1.1375354472802268,
        "step": 8825
    },
    {
        "loss": 1.4645,
        "grad_norm": 2.3057281970977783,
        "learning_rate": 4.020403986046204e-05,
        "epoch": 1.1376643464810519,
        "step": 8826
    },
    {
        "loss": 0.8269,
        "grad_norm": 2.1011571884155273,
        "learning_rate": 4.015528579671336e-05,
        "epoch": 1.137793245681877,
        "step": 8827
    },
    {
        "loss": 1.5688,
        "grad_norm": 2.9502382278442383,
        "learning_rate": 4.010655388477007e-05,
        "epoch": 1.1379221448827017,
        "step": 8828
    },
    {
        "loss": 0.5445,
        "grad_norm": 2.49021053314209,
        "learning_rate": 4.0057844142670694e-05,
        "epoch": 1.1380510440835268,
        "step": 8829
    },
    {
        "loss": 2.2556,
        "grad_norm": 1.8288192749023438,
        "learning_rate": 4.000915658844546e-05,
        "epoch": 1.1381799432843516,
        "step": 8830
    },
    {
        "loss": 1.3442,
        "grad_norm": 2.8593146800994873,
        "learning_rate": 3.99604912401162e-05,
        "epoch": 1.1383088424851766,
        "step": 8831
    },
    {
        "loss": 1.5993,
        "grad_norm": 2.3373565673828125,
        "learning_rate": 3.991184811569666e-05,
        "epoch": 1.1384377416860016,
        "step": 8832
    },
    {
        "loss": 2.3573,
        "grad_norm": 2.142613172531128,
        "learning_rate": 3.986322723319229e-05,
        "epoch": 1.1385666408868265,
        "step": 8833
    },
    {
        "loss": 1.4602,
        "grad_norm": 2.415590763092041,
        "learning_rate": 3.9814628610600315e-05,
        "epoch": 1.1386955400876515,
        "step": 8834
    },
    {
        "loss": 0.9582,
        "grad_norm": 3.3102610111236572,
        "learning_rate": 3.976605226590979e-05,
        "epoch": 1.1388244392884763,
        "step": 8835
    },
    {
        "loss": 1.8095,
        "grad_norm": 2.5111448764801025,
        "learning_rate": 3.9717498217101444e-05,
        "epoch": 1.1389533384893014,
        "step": 8836
    },
    {
        "loss": 2.1159,
        "grad_norm": 1.5349622964859009,
        "learning_rate": 3.966896648214789e-05,
        "epoch": 1.1390822376901264,
        "step": 8837
    },
    {
        "loss": 1.359,
        "grad_norm": 2.626552104949951,
        "learning_rate": 3.962045707901334e-05,
        "epoch": 1.1392111368909512,
        "step": 8838
    },
    {
        "loss": 1.9635,
        "grad_norm": 2.246290445327759,
        "learning_rate": 3.957197002565365e-05,
        "epoch": 1.1393400360917763,
        "step": 8839
    },
    {
        "loss": 2.1193,
        "grad_norm": 3.0749645233154297,
        "learning_rate": 3.95235053400167e-05,
        "epoch": 1.139468935292601,
        "step": 8840
    },
    {
        "loss": 1.1777,
        "grad_norm": 2.2538092136383057,
        "learning_rate": 3.947506304004186e-05,
        "epoch": 1.139597834493426,
        "step": 8841
    },
    {
        "loss": 1.4857,
        "grad_norm": 2.3046443462371826,
        "learning_rate": 3.94266431436602e-05,
        "epoch": 1.1397267336942511,
        "step": 8842
    },
    {
        "loss": 2.271,
        "grad_norm": 1.980629563331604,
        "learning_rate": 3.93782456687947e-05,
        "epoch": 1.139855632895076,
        "step": 8843
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.693706750869751,
        "learning_rate": 3.932987063336e-05,
        "epoch": 1.139984532095901,
        "step": 8844
    },
    {
        "loss": 1.405,
        "grad_norm": 2.6254568099975586,
        "learning_rate": 3.928151805526203e-05,
        "epoch": 1.140113431296726,
        "step": 8845
    },
    {
        "loss": 2.3635,
        "grad_norm": 2.239469051361084,
        "learning_rate": 3.923318795239893e-05,
        "epoch": 1.1402423304975509,
        "step": 8846
    },
    {
        "loss": 1.7356,
        "grad_norm": 3.162508010864258,
        "learning_rate": 3.918488034266036e-05,
        "epoch": 1.140371229698376,
        "step": 8847
    },
    {
        "loss": 1.6405,
        "grad_norm": 1.6648801565170288,
        "learning_rate": 3.913659524392755e-05,
        "epoch": 1.140500128899201,
        "step": 8848
    },
    {
        "loss": 1.8423,
        "grad_norm": 1.4665501117706299,
        "learning_rate": 3.908833267407339e-05,
        "epoch": 1.1406290281000258,
        "step": 8849
    },
    {
        "loss": 0.8522,
        "grad_norm": 2.2739176750183105,
        "learning_rate": 3.904009265096269e-05,
        "epoch": 1.1407579273008508,
        "step": 8850
    },
    {
        "loss": 1.749,
        "grad_norm": 1.7770298719406128,
        "learning_rate": 3.899187519245156e-05,
        "epoch": 1.1408868265016756,
        "step": 8851
    },
    {
        "loss": 1.0654,
        "grad_norm": 2.210207462310791,
        "learning_rate": 3.8943680316387924e-05,
        "epoch": 1.1410157257025006,
        "step": 8852
    },
    {
        "loss": 1.5737,
        "grad_norm": 1.972881555557251,
        "learning_rate": 3.889550804061135e-05,
        "epoch": 1.1411446249033257,
        "step": 8853
    },
    {
        "loss": 1.5444,
        "grad_norm": 1.851414442062378,
        "learning_rate": 3.8847358382953145e-05,
        "epoch": 1.1412735241041505,
        "step": 8854
    },
    {
        "loss": 1.6096,
        "grad_norm": 2.37884259223938,
        "learning_rate": 3.879923136123611e-05,
        "epoch": 1.1414024233049755,
        "step": 8855
    },
    {
        "loss": 1.3428,
        "grad_norm": 1.2735264301300049,
        "learning_rate": 3.875112699327452e-05,
        "epoch": 1.1415313225058004,
        "step": 8856
    },
    {
        "loss": 1.5159,
        "grad_norm": 1.7488486766815186,
        "learning_rate": 3.870304529687463e-05,
        "epoch": 1.1416602217066254,
        "step": 8857
    },
    {
        "loss": 1.2463,
        "grad_norm": 3.7895376682281494,
        "learning_rate": 3.865498628983404e-05,
        "epoch": 1.1417891209074504,
        "step": 8858
    },
    {
        "loss": 1.4071,
        "grad_norm": 2.8052377700805664,
        "learning_rate": 3.860694998994193e-05,
        "epoch": 1.1419180201082753,
        "step": 8859
    },
    {
        "loss": 1.8829,
        "grad_norm": 1.9968546628952026,
        "learning_rate": 3.8558936414979296e-05,
        "epoch": 1.1420469193091003,
        "step": 8860
    },
    {
        "loss": 1.4399,
        "grad_norm": 2.9963064193725586,
        "learning_rate": 3.851094558271865e-05,
        "epoch": 1.1421758185099253,
        "step": 8861
    },
    {
        "loss": 1.2542,
        "grad_norm": 2.6352908611297607,
        "learning_rate": 3.8462977510923814e-05,
        "epoch": 1.1423047177107502,
        "step": 8862
    },
    {
        "loss": 1.9081,
        "grad_norm": 2.633497476577759,
        "learning_rate": 3.8415032217350475e-05,
        "epoch": 1.1424336169115752,
        "step": 8863
    },
    {
        "loss": 2.0648,
        "grad_norm": 1.7806843519210815,
        "learning_rate": 3.836710971974592e-05,
        "epoch": 1.1425625161124,
        "step": 8864
    },
    {
        "loss": 1.5413,
        "grad_norm": 1.9274462461471558,
        "learning_rate": 3.83192100358488e-05,
        "epoch": 1.142691415313225,
        "step": 8865
    },
    {
        "loss": 1.2971,
        "grad_norm": 2.1627213954925537,
        "learning_rate": 3.8271333183389346e-05,
        "epoch": 1.14282031451405,
        "step": 8866
    },
    {
        "loss": 1.7433,
        "grad_norm": 1.8003193140029907,
        "learning_rate": 3.822347918008959e-05,
        "epoch": 1.142949213714875,
        "step": 8867
    },
    {
        "loss": 1.7,
        "grad_norm": 1.7436144351959229,
        "learning_rate": 3.817564804366278e-05,
        "epoch": 1.1430781129157,
        "step": 8868
    },
    {
        "loss": 1.2749,
        "grad_norm": 1.9796234369277954,
        "learning_rate": 3.812783979181379e-05,
        "epoch": 1.143207012116525,
        "step": 8869
    },
    {
        "loss": 1.945,
        "grad_norm": 1.76274836063385,
        "learning_rate": 3.808005444223919e-05,
        "epoch": 1.1433359113173498,
        "step": 8870
    },
    {
        "loss": 0.979,
        "grad_norm": 1.8792678117752075,
        "learning_rate": 3.803229201262695e-05,
        "epoch": 1.1434648105181748,
        "step": 8871
    },
    {
        "loss": 1.2765,
        "grad_norm": 2.682701587677002,
        "learning_rate": 3.798455252065648e-05,
        "epoch": 1.1435937097189997,
        "step": 8872
    },
    {
        "loss": 1.6065,
        "grad_norm": 1.7712445259094238,
        "learning_rate": 3.7936835983998854e-05,
        "epoch": 1.1437226089198247,
        "step": 8873
    },
    {
        "loss": 2.3404,
        "grad_norm": 2.0576770305633545,
        "learning_rate": 3.788914242031655e-05,
        "epoch": 1.1438515081206497,
        "step": 8874
    },
    {
        "loss": 1.1764,
        "grad_norm": 3.6353909969329834,
        "learning_rate": 3.7841471847263565e-05,
        "epoch": 1.1439804073214745,
        "step": 8875
    },
    {
        "loss": 1.5893,
        "grad_norm": 2.1650290489196777,
        "learning_rate": 3.7793824282485315e-05,
        "epoch": 1.1441093065222996,
        "step": 8876
    },
    {
        "loss": 2.0301,
        "grad_norm": 2.0060269832611084,
        "learning_rate": 3.7746199743618835e-05,
        "epoch": 1.1442382057231244,
        "step": 8877
    },
    {
        "loss": 0.9227,
        "grad_norm": 3.662780284881592,
        "learning_rate": 3.769859824829276e-05,
        "epoch": 1.1443671049239494,
        "step": 8878
    },
    {
        "loss": 1.7064,
        "grad_norm": 2.9088985919952393,
        "learning_rate": 3.765101981412669e-05,
        "epoch": 1.1444960041247745,
        "step": 8879
    },
    {
        "loss": 1.8234,
        "grad_norm": 1.8035714626312256,
        "learning_rate": 3.760346445873212e-05,
        "epoch": 1.1446249033255993,
        "step": 8880
    },
    {
        "loss": 1.391,
        "grad_norm": 3.526853561401367,
        "learning_rate": 3.755593219971198e-05,
        "epoch": 1.1447538025264243,
        "step": 8881
    },
    {
        "loss": 1.0751,
        "grad_norm": 1.75784432888031,
        "learning_rate": 3.7508423054660525e-05,
        "epoch": 1.1448827017272494,
        "step": 8882
    },
    {
        "loss": 1.7097,
        "grad_norm": 2.8807332515716553,
        "learning_rate": 3.746093704116339e-05,
        "epoch": 1.1450116009280742,
        "step": 8883
    },
    {
        "loss": 1.5869,
        "grad_norm": 2.3508310317993164,
        "learning_rate": 3.741347417679793e-05,
        "epoch": 1.1451405001288992,
        "step": 8884
    },
    {
        "loss": 1.2753,
        "grad_norm": 3.3815340995788574,
        "learning_rate": 3.736603447913262e-05,
        "epoch": 1.1452693993297243,
        "step": 8885
    },
    {
        "loss": 1.2165,
        "grad_norm": 1.2934578657150269,
        "learning_rate": 3.731861796572743e-05,
        "epoch": 1.145398298530549,
        "step": 8886
    },
    {
        "loss": 1.6411,
        "grad_norm": 2.375615358352661,
        "learning_rate": 3.727122465413395e-05,
        "epoch": 1.1455271977313741,
        "step": 8887
    },
    {
        "loss": 1.7222,
        "grad_norm": 2.1227762699127197,
        "learning_rate": 3.7223854561894974e-05,
        "epoch": 1.145656096932199,
        "step": 8888
    },
    {
        "loss": 1.6402,
        "grad_norm": 1.756697654724121,
        "learning_rate": 3.717650770654482e-05,
        "epoch": 1.145784996133024,
        "step": 8889
    },
    {
        "loss": 1.1209,
        "grad_norm": 2.8536460399627686,
        "learning_rate": 3.7129184105609104e-05,
        "epoch": 1.145913895333849,
        "step": 8890
    },
    {
        "loss": 2.0262,
        "grad_norm": 3.629093647003174,
        "learning_rate": 3.70818837766049e-05,
        "epoch": 1.1460427945346738,
        "step": 8891
    },
    {
        "loss": 1.2311,
        "grad_norm": 3.617426872253418,
        "learning_rate": 3.703460673704062e-05,
        "epoch": 1.1461716937354989,
        "step": 8892
    },
    {
        "loss": 1.3424,
        "grad_norm": 3.1101484298706055,
        "learning_rate": 3.6987353004416046e-05,
        "epoch": 1.1463005929363237,
        "step": 8893
    },
    {
        "loss": 1.0627,
        "grad_norm": 3.122781991958618,
        "learning_rate": 3.694012259622243e-05,
        "epoch": 1.1464294921371487,
        "step": 8894
    },
    {
        "loss": 1.5877,
        "grad_norm": 2.5252132415771484,
        "learning_rate": 3.689291552994249e-05,
        "epoch": 1.1465583913379738,
        "step": 8895
    },
    {
        "loss": 1.7496,
        "grad_norm": 1.1926493644714355,
        "learning_rate": 3.684573182304987e-05,
        "epoch": 1.1466872905387986,
        "step": 8896
    },
    {
        "loss": 1.8535,
        "grad_norm": 2.5008347034454346,
        "learning_rate": 3.67985714930099e-05,
        "epoch": 1.1468161897396236,
        "step": 8897
    },
    {
        "loss": 1.5332,
        "grad_norm": 1.394774079322815,
        "learning_rate": 3.675143455727935e-05,
        "epoch": 1.1469450889404487,
        "step": 8898
    },
    {
        "loss": 1.7633,
        "grad_norm": 1.4919970035552979,
        "learning_rate": 3.6704321033306085e-05,
        "epoch": 1.1470739881412735,
        "step": 8899
    },
    {
        "loss": 1.9898,
        "grad_norm": 1.9744157791137695,
        "learning_rate": 3.665723093852934e-05,
        "epoch": 1.1472028873420985,
        "step": 8900
    },
    {
        "loss": 2.0802,
        "grad_norm": 2.8544676303863525,
        "learning_rate": 3.66101642903799e-05,
        "epoch": 1.1473317865429233,
        "step": 8901
    },
    {
        "loss": 1.2792,
        "grad_norm": 2.2684855461120605,
        "learning_rate": 3.656312110627959e-05,
        "epoch": 1.1474606857437484,
        "step": 8902
    },
    {
        "loss": 2.3164,
        "grad_norm": 1.513604760169983,
        "learning_rate": 3.6516101403641615e-05,
        "epoch": 1.1475895849445734,
        "step": 8903
    },
    {
        "loss": 2.0737,
        "grad_norm": 2.0547006130218506,
        "learning_rate": 3.6469105199870644e-05,
        "epoch": 1.1477184841453982,
        "step": 8904
    },
    {
        "loss": 2.1126,
        "grad_norm": 2.8032848834991455,
        "learning_rate": 3.642213251236253e-05,
        "epoch": 1.1478473833462233,
        "step": 8905
    },
    {
        "loss": 2.029,
        "grad_norm": 1.44632887840271,
        "learning_rate": 3.637518335850446e-05,
        "epoch": 1.1479762825470483,
        "step": 8906
    },
    {
        "loss": 1.5162,
        "grad_norm": 2.3964931964874268,
        "learning_rate": 3.632825775567485e-05,
        "epoch": 1.1481051817478731,
        "step": 8907
    },
    {
        "loss": 2.0864,
        "grad_norm": 1.477338194847107,
        "learning_rate": 3.628135572124344e-05,
        "epoch": 1.1482340809486982,
        "step": 8908
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.88593590259552,
        "learning_rate": 3.6234477272571235e-05,
        "epoch": 1.148362980149523,
        "step": 8909
    },
    {
        "loss": 1.3417,
        "grad_norm": 3.2524943351745605,
        "learning_rate": 3.618762242701045e-05,
        "epoch": 1.148491879350348,
        "step": 8910
    },
    {
        "loss": 1.7196,
        "grad_norm": 1.1583307981491089,
        "learning_rate": 3.61407912019047e-05,
        "epoch": 1.148620778551173,
        "step": 8911
    },
    {
        "loss": 1.4482,
        "grad_norm": 2.4158966541290283,
        "learning_rate": 3.6093983614588855e-05,
        "epoch": 1.1487496777519979,
        "step": 8912
    },
    {
        "loss": 1.8218,
        "grad_norm": 1.4971287250518799,
        "learning_rate": 3.6047199682388967e-05,
        "epoch": 1.148878576952823,
        "step": 8913
    },
    {
        "loss": 1.562,
        "grad_norm": 3.0249664783477783,
        "learning_rate": 3.600043942262211e-05,
        "epoch": 1.1490074761536477,
        "step": 8914
    },
    {
        "loss": 1.2802,
        "grad_norm": 3.0850350856781006,
        "learning_rate": 3.595370285259704e-05,
        "epoch": 1.1491363753544728,
        "step": 8915
    },
    {
        "loss": 1.7038,
        "grad_norm": 2.1822168827056885,
        "learning_rate": 3.5906989989613404e-05,
        "epoch": 1.1492652745552978,
        "step": 8916
    },
    {
        "loss": 1.3646,
        "grad_norm": 2.4794087409973145,
        "learning_rate": 3.5860300850962245e-05,
        "epoch": 1.1493941737561226,
        "step": 8917
    },
    {
        "loss": 1.5434,
        "grad_norm": 2.501426935195923,
        "learning_rate": 3.581363545392582e-05,
        "epoch": 1.1495230729569477,
        "step": 8918
    },
    {
        "loss": 1.5329,
        "grad_norm": 1.3116388320922852,
        "learning_rate": 3.576699381577745e-05,
        "epoch": 1.1496519721577727,
        "step": 8919
    },
    {
        "loss": 1.9939,
        "grad_norm": 1.5913546085357666,
        "learning_rate": 3.572037595378173e-05,
        "epoch": 1.1497808713585975,
        "step": 8920
    },
    {
        "loss": 1.5998,
        "grad_norm": 1.799028992652893,
        "learning_rate": 3.5673781885194545e-05,
        "epoch": 1.1499097705594226,
        "step": 8921
    },
    {
        "loss": 1.8832,
        "grad_norm": 2.015814781188965,
        "learning_rate": 3.562721162726298e-05,
        "epoch": 1.1500386697602476,
        "step": 8922
    },
    {
        "loss": 1.1343,
        "grad_norm": 1.3618003129959106,
        "learning_rate": 3.558066519722517e-05,
        "epoch": 1.1501675689610724,
        "step": 8923
    },
    {
        "loss": 1.7246,
        "grad_norm": 1.9181448221206665,
        "learning_rate": 3.553414261231049e-05,
        "epoch": 1.1502964681618975,
        "step": 8924
    },
    {
        "loss": 2.1028,
        "grad_norm": 2.2417752742767334,
        "learning_rate": 3.548764388973952e-05,
        "epoch": 1.1504253673627223,
        "step": 8925
    },
    {
        "loss": 2.0203,
        "grad_norm": 1.9452606439590454,
        "learning_rate": 3.5441169046723954e-05,
        "epoch": 1.1505542665635473,
        "step": 8926
    },
    {
        "loss": 1.575,
        "grad_norm": 2.003833532333374,
        "learning_rate": 3.539471810046663e-05,
        "epoch": 1.1506831657643724,
        "step": 8927
    },
    {
        "loss": 2.1497,
        "grad_norm": 1.3062950372695923,
        "learning_rate": 3.534829106816165e-05,
        "epoch": 1.1508120649651972,
        "step": 8928
    },
    {
        "loss": 1.459,
        "grad_norm": 1.9234663248062134,
        "learning_rate": 3.5301887966994266e-05,
        "epoch": 1.1509409641660222,
        "step": 8929
    },
    {
        "loss": 1.4893,
        "grad_norm": 2.617793321609497,
        "learning_rate": 3.525550881414076e-05,
        "epoch": 1.151069863366847,
        "step": 8930
    },
    {
        "loss": 1.5479,
        "grad_norm": 1.3258553743362427,
        "learning_rate": 3.520915362676851e-05,
        "epoch": 1.151198762567672,
        "step": 8931
    },
    {
        "loss": 0.859,
        "grad_norm": 2.0432095527648926,
        "learning_rate": 3.5162822422036214e-05,
        "epoch": 1.151327661768497,
        "step": 8932
    },
    {
        "loss": 1.4971,
        "grad_norm": 1.9440478086471558,
        "learning_rate": 3.5116515217093504e-05,
        "epoch": 1.151456560969322,
        "step": 8933
    },
    {
        "loss": 2.2074,
        "grad_norm": 1.6082607507705688,
        "learning_rate": 3.507023202908125e-05,
        "epoch": 1.151585460170147,
        "step": 8934
    },
    {
        "loss": 1.9566,
        "grad_norm": 1.9996248483657837,
        "learning_rate": 3.502397287513138e-05,
        "epoch": 1.151714359370972,
        "step": 8935
    },
    {
        "loss": 1.3161,
        "grad_norm": 2.618849039077759,
        "learning_rate": 3.497773777236711e-05,
        "epoch": 1.1518432585717968,
        "step": 8936
    },
    {
        "loss": 1.3616,
        "grad_norm": 2.3801112174987793,
        "learning_rate": 3.4931526737902287e-05,
        "epoch": 1.1519721577726219,
        "step": 8937
    },
    {
        "loss": 1.9818,
        "grad_norm": 2.5665228366851807,
        "learning_rate": 3.4885339788842275e-05,
        "epoch": 1.1521010569734467,
        "step": 8938
    },
    {
        "loss": 1.4731,
        "grad_norm": 3.3509726524353027,
        "learning_rate": 3.483917694228348e-05,
        "epoch": 1.1522299561742717,
        "step": 8939
    },
    {
        "loss": 1.3216,
        "grad_norm": 2.2834959030151367,
        "learning_rate": 3.479303821531322e-05,
        "epoch": 1.1523588553750967,
        "step": 8940
    },
    {
        "loss": 1.2987,
        "grad_norm": 2.8199729919433594,
        "learning_rate": 3.474692362500997e-05,
        "epoch": 1.1524877545759216,
        "step": 8941
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.4914538860321045,
        "learning_rate": 3.470083318844327e-05,
        "epoch": 1.1526166537767466,
        "step": 8942
    },
    {
        "loss": 1.4475,
        "grad_norm": 2.2164244651794434,
        "learning_rate": 3.465476692267372e-05,
        "epoch": 1.1527455529775716,
        "step": 8943
    },
    {
        "loss": 1.6341,
        "grad_norm": 1.5123423337936401,
        "learning_rate": 3.460872484475288e-05,
        "epoch": 1.1528744521783965,
        "step": 8944
    },
    {
        "loss": 1.8238,
        "grad_norm": 2.177835464477539,
        "learning_rate": 3.4562706971723526e-05,
        "epoch": 1.1530033513792215,
        "step": 8945
    },
    {
        "loss": 1.1172,
        "grad_norm": 2.7664825916290283,
        "learning_rate": 3.451671332061946e-05,
        "epoch": 1.1531322505800463,
        "step": 8946
    },
    {
        "loss": 0.5226,
        "grad_norm": 2.2493927478790283,
        "learning_rate": 3.44707439084654e-05,
        "epoch": 1.1532611497808714,
        "step": 8947
    },
    {
        "loss": 0.8386,
        "grad_norm": 3.2479588985443115,
        "learning_rate": 3.442479875227706e-05,
        "epoch": 1.1533900489816964,
        "step": 8948
    },
    {
        "loss": 1.6182,
        "grad_norm": 1.3388324975967407,
        "learning_rate": 3.437887786906132e-05,
        "epoch": 1.1535189481825212,
        "step": 8949
    },
    {
        "loss": 1.4953,
        "grad_norm": 2.3887553215026855,
        "learning_rate": 3.433298127581606e-05,
        "epoch": 1.1536478473833462,
        "step": 8950
    },
    {
        "loss": 1.5651,
        "grad_norm": 2.3391928672790527,
        "learning_rate": 3.4287108989530026e-05,
        "epoch": 1.153776746584171,
        "step": 8951
    },
    {
        "loss": 1.3692,
        "grad_norm": 2.7962231636047363,
        "learning_rate": 3.424126102718308e-05,
        "epoch": 1.153905645784996,
        "step": 8952
    },
    {
        "loss": 2.0373,
        "grad_norm": 1.7124075889587402,
        "learning_rate": 3.419543740574627e-05,
        "epoch": 1.1540345449858211,
        "step": 8953
    },
    {
        "loss": 1.6047,
        "grad_norm": 1.7822140455245972,
        "learning_rate": 3.414963814218109e-05,
        "epoch": 1.154163444186646,
        "step": 8954
    },
    {
        "loss": 1.3283,
        "grad_norm": 2.148203134536743,
        "learning_rate": 3.410386325344051e-05,
        "epoch": 1.154292343387471,
        "step": 8955
    },
    {
        "loss": 2.3139,
        "grad_norm": 1.277080774307251,
        "learning_rate": 3.4058112756468386e-05,
        "epoch": 1.154421242588296,
        "step": 8956
    },
    {
        "loss": 2.4209,
        "grad_norm": 1.7909003496170044,
        "learning_rate": 3.401238666819944e-05,
        "epoch": 1.1545501417891209,
        "step": 8957
    },
    {
        "loss": 0.1624,
        "grad_norm": 1.5846967697143555,
        "learning_rate": 3.396668500555938e-05,
        "epoch": 1.154679040989946,
        "step": 8958
    },
    {
        "loss": 1.8023,
        "grad_norm": 3.08113169670105,
        "learning_rate": 3.392100778546492e-05,
        "epoch": 1.154807940190771,
        "step": 8959
    },
    {
        "loss": 1.3493,
        "grad_norm": 2.128434658050537,
        "learning_rate": 3.387535502482363e-05,
        "epoch": 1.1549368393915957,
        "step": 8960
    },
    {
        "loss": 2.1158,
        "grad_norm": 1.8393582105636597,
        "learning_rate": 3.3829726740534163e-05,
        "epoch": 1.1550657385924208,
        "step": 8961
    },
    {
        "loss": 1.8019,
        "grad_norm": 1.9401168823242188,
        "learning_rate": 3.378412294948599e-05,
        "epoch": 1.1551946377932456,
        "step": 8962
    },
    {
        "loss": 1.7013,
        "grad_norm": 2.5705113410949707,
        "learning_rate": 3.3738543668559676e-05,
        "epoch": 1.1553235369940706,
        "step": 8963
    },
    {
        "loss": 1.9399,
        "grad_norm": 1.7887822389602661,
        "learning_rate": 3.369298891462658e-05,
        "epoch": 1.1554524361948957,
        "step": 8964
    },
    {
        "loss": 1.989,
        "grad_norm": 2.498304843902588,
        "learning_rate": 3.3647458704548905e-05,
        "epoch": 1.1555813353957205,
        "step": 8965
    },
    {
        "loss": 1.8156,
        "grad_norm": 1.8522332906723022,
        "learning_rate": 3.360195305517996e-05,
        "epoch": 1.1557102345965455,
        "step": 8966
    },
    {
        "loss": 1.5816,
        "grad_norm": 2.4778759479522705,
        "learning_rate": 3.355647198336388e-05,
        "epoch": 1.1558391337973704,
        "step": 8967
    },
    {
        "loss": 1.1992,
        "grad_norm": 3.122076988220215,
        "learning_rate": 3.3511015505935625e-05,
        "epoch": 1.1559680329981954,
        "step": 8968
    },
    {
        "loss": 1.8675,
        "grad_norm": 1.8822174072265625,
        "learning_rate": 3.346558363972122e-05,
        "epoch": 1.1560969321990204,
        "step": 8969
    },
    {
        "loss": 1.8932,
        "grad_norm": 1.5830448865890503,
        "learning_rate": 3.342017640153757e-05,
        "epoch": 1.1562258313998452,
        "step": 8970
    },
    {
        "loss": 1.5473,
        "grad_norm": 3.0668365955352783,
        "learning_rate": 3.337479380819211e-05,
        "epoch": 1.1563547306006703,
        "step": 8971
    },
    {
        "loss": 1.6211,
        "grad_norm": 2.918043851852417,
        "learning_rate": 3.332943587648356e-05,
        "epoch": 1.1564836298014953,
        "step": 8972
    },
    {
        "loss": 0.9178,
        "grad_norm": 2.377378463745117,
        "learning_rate": 3.328410262320146e-05,
        "epoch": 1.1566125290023201,
        "step": 8973
    },
    {
        "loss": 0.8619,
        "grad_norm": 2.3727288246154785,
        "learning_rate": 3.323879406512607e-05,
        "epoch": 1.1567414282031452,
        "step": 8974
    },
    {
        "loss": 1.809,
        "grad_norm": 2.5424692630767822,
        "learning_rate": 3.319351021902847e-05,
        "epoch": 1.15687032740397,
        "step": 8975
    },
    {
        "loss": 2.5996,
        "grad_norm": 1.9777462482452393,
        "learning_rate": 3.314825110167091e-05,
        "epoch": 1.156999226604795,
        "step": 8976
    },
    {
        "loss": 2.0394,
        "grad_norm": 2.3509514331817627,
        "learning_rate": 3.310301672980605e-05,
        "epoch": 1.15712812580562,
        "step": 8977
    },
    {
        "loss": 1.3295,
        "grad_norm": 2.5454092025756836,
        "learning_rate": 3.305780712017768e-05,
        "epoch": 1.157257025006445,
        "step": 8978
    },
    {
        "loss": 1.8201,
        "grad_norm": 1.5798693895339966,
        "learning_rate": 3.3012622289520323e-05,
        "epoch": 1.15738592420727,
        "step": 8979
    },
    {
        "loss": 1.5428,
        "grad_norm": 2.185471773147583,
        "learning_rate": 3.296746225455948e-05,
        "epoch": 1.157514823408095,
        "step": 8980
    },
    {
        "loss": 1.8684,
        "grad_norm": 2.1337671279907227,
        "learning_rate": 3.292232703201129e-05,
        "epoch": 1.1576437226089198,
        "step": 8981
    },
    {
        "loss": 1.1261,
        "grad_norm": 1.6972284317016602,
        "learning_rate": 3.287721663858267e-05,
        "epoch": 1.1577726218097448,
        "step": 8982
    },
    {
        "loss": 1.3211,
        "grad_norm": 2.7450058460235596,
        "learning_rate": 3.283213109097159e-05,
        "epoch": 1.1579015210105696,
        "step": 8983
    },
    {
        "loss": 1.2572,
        "grad_norm": 1.6940284967422485,
        "learning_rate": 3.2787070405866605e-05,
        "epoch": 1.1580304202113947,
        "step": 8984
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.32870614528656,
        "learning_rate": 3.27420345999471e-05,
        "epoch": 1.1581593194122197,
        "step": 8985
    },
    {
        "loss": 1.5691,
        "grad_norm": 2.520400285720825,
        "learning_rate": 3.269702368988339e-05,
        "epoch": 1.1582882186130445,
        "step": 8986
    },
    {
        "loss": 2.2046,
        "grad_norm": 2.108781576156616,
        "learning_rate": 3.2652037692336555e-05,
        "epoch": 1.1584171178138696,
        "step": 8987
    },
    {
        "loss": 1.8861,
        "grad_norm": 2.130422592163086,
        "learning_rate": 3.26070766239581e-05,
        "epoch": 1.1585460170146944,
        "step": 8988
    },
    {
        "loss": 1.3768,
        "grad_norm": 2.063124656677246,
        "learning_rate": 3.2562140501390725e-05,
        "epoch": 1.1586749162155194,
        "step": 8989
    },
    {
        "loss": 2.2776,
        "grad_norm": 1.607203483581543,
        "learning_rate": 3.251722934126784e-05,
        "epoch": 1.1588038154163445,
        "step": 8990
    },
    {
        "loss": 1.9542,
        "grad_norm": 2.8405914306640625,
        "learning_rate": 3.247234316021344e-05,
        "epoch": 1.1589327146171693,
        "step": 8991
    },
    {
        "loss": 1.0337,
        "grad_norm": 2.773921012878418,
        "learning_rate": 3.2427481974842286e-05,
        "epoch": 1.1590616138179943,
        "step": 8992
    },
    {
        "loss": 1.7346,
        "grad_norm": 2.810330867767334,
        "learning_rate": 3.238264580176015e-05,
        "epoch": 1.1591905130188194,
        "step": 8993
    },
    {
        "loss": 1.1602,
        "grad_norm": 1.2222347259521484,
        "learning_rate": 3.233783465756319e-05,
        "epoch": 1.1593194122196442,
        "step": 8994
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.5669586658477783,
        "learning_rate": 3.22930485588385e-05,
        "epoch": 1.1594483114204692,
        "step": 8995
    },
    {
        "loss": 2.0264,
        "grad_norm": 2.097487211227417,
        "learning_rate": 3.2248287522163833e-05,
        "epoch": 1.1595772106212943,
        "step": 8996
    },
    {
        "loss": 1.3183,
        "grad_norm": 1.8783894777297974,
        "learning_rate": 3.2203551564107826e-05,
        "epoch": 1.159706109822119,
        "step": 8997
    },
    {
        "loss": 2.4468,
        "grad_norm": 1.1614980697631836,
        "learning_rate": 3.215884070122962e-05,
        "epoch": 1.1598350090229441,
        "step": 8998
    },
    {
        "loss": 2.0738,
        "grad_norm": 1.9419622421264648,
        "learning_rate": 3.211415495007913e-05,
        "epoch": 1.159963908223769,
        "step": 8999
    },
    {
        "loss": 1.8348,
        "grad_norm": 2.170199155807495,
        "learning_rate": 3.206949432719708e-05,
        "epoch": 1.160092807424594,
        "step": 9000
    },
    {
        "loss": 2.0927,
        "grad_norm": 2.0697245597839355,
        "learning_rate": 3.202485884911475e-05,
        "epoch": 1.160221706625419,
        "step": 9001
    },
    {
        "loss": 1.41,
        "grad_norm": 1.5086253881454468,
        "learning_rate": 3.198024853235413e-05,
        "epoch": 1.1603506058262438,
        "step": 9002
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.0171093940734863,
        "learning_rate": 3.1935663393428026e-05,
        "epoch": 1.1604795050270689,
        "step": 9003
    },
    {
        "loss": 1.2939,
        "grad_norm": 1.74634850025177,
        "learning_rate": 3.1891103448839945e-05,
        "epoch": 1.1606084042278937,
        "step": 9004
    },
    {
        "loss": 2.2015,
        "grad_norm": 2.7950751781463623,
        "learning_rate": 3.1846568715083666e-05,
        "epoch": 1.1607373034287187,
        "step": 9005
    },
    {
        "loss": 0.8439,
        "grad_norm": 4.0727410316467285,
        "learning_rate": 3.1802059208644096e-05,
        "epoch": 1.1608662026295438,
        "step": 9006
    },
    {
        "loss": 1.7072,
        "grad_norm": 1.500643253326416,
        "learning_rate": 3.175757494599672e-05,
        "epoch": 1.1609951018303686,
        "step": 9007
    },
    {
        "loss": 1.509,
        "grad_norm": 2.012322187423706,
        "learning_rate": 3.17131159436075e-05,
        "epoch": 1.1611240010311936,
        "step": 9008
    },
    {
        "loss": 1.4532,
        "grad_norm": 1.858556866645813,
        "learning_rate": 3.166868221793311e-05,
        "epoch": 1.1612529002320184,
        "step": 9009
    },
    {
        "loss": 1.571,
        "grad_norm": 2.78397536277771,
        "learning_rate": 3.1624273785421066e-05,
        "epoch": 1.1613817994328435,
        "step": 9010
    },
    {
        "loss": 1.7724,
        "grad_norm": 1.5787928104400635,
        "learning_rate": 3.157989066250922e-05,
        "epoch": 1.1615106986336685,
        "step": 9011
    },
    {
        "loss": 2.1416,
        "grad_norm": 1.9450989961624146,
        "learning_rate": 3.153553286562619e-05,
        "epoch": 1.1616395978344933,
        "step": 9012
    },
    {
        "loss": 0.5099,
        "grad_norm": 3.99143385887146,
        "learning_rate": 3.1491200411191244e-05,
        "epoch": 1.1617684970353184,
        "step": 9013
    },
    {
        "loss": 1.9109,
        "grad_norm": 1.5453547239303589,
        "learning_rate": 3.144689331561434e-05,
        "epoch": 1.1618973962361434,
        "step": 9014
    },
    {
        "loss": 1.7739,
        "grad_norm": 1.6053071022033691,
        "learning_rate": 3.140261159529587e-05,
        "epoch": 1.1620262954369682,
        "step": 9015
    },
    {
        "loss": 1.3038,
        "grad_norm": 1.7264524698257446,
        "learning_rate": 3.135835526662697e-05,
        "epoch": 1.1621551946377933,
        "step": 9016
    },
    {
        "loss": 0.9213,
        "grad_norm": 2.8587210178375244,
        "learning_rate": 3.1314124345989314e-05,
        "epoch": 1.1622840938386183,
        "step": 9017
    },
    {
        "loss": 1.7394,
        "grad_norm": 1.854492425918579,
        "learning_rate": 3.126991884975519e-05,
        "epoch": 1.1624129930394431,
        "step": 9018
    },
    {
        "loss": 1.7398,
        "grad_norm": 2.4882264137268066,
        "learning_rate": 3.12257387942874e-05,
        "epoch": 1.1625418922402682,
        "step": 9019
    },
    {
        "loss": 1.3852,
        "grad_norm": 1.8754510879516602,
        "learning_rate": 3.118158419593945e-05,
        "epoch": 1.162670791441093,
        "step": 9020
    },
    {
        "loss": 0.7738,
        "grad_norm": 4.242190361022949,
        "learning_rate": 3.113745507105556e-05,
        "epoch": 1.162799690641918,
        "step": 9021
    },
    {
        "loss": 1.6171,
        "grad_norm": 2.589437246322632,
        "learning_rate": 3.1093351435970056e-05,
        "epoch": 1.162928589842743,
        "step": 9022
    },
    {
        "loss": 2.0663,
        "grad_norm": 2.573431968688965,
        "learning_rate": 3.104927330700816e-05,
        "epoch": 1.1630574890435679,
        "step": 9023
    },
    {
        "loss": 1.147,
        "grad_norm": 3.065337896347046,
        "learning_rate": 3.100522070048575e-05,
        "epoch": 1.163186388244393,
        "step": 9024
    },
    {
        "loss": 0.7836,
        "grad_norm": 2.271648645401001,
        "learning_rate": 3.0961193632709e-05,
        "epoch": 1.1633152874452177,
        "step": 9025
    },
    {
        "loss": 1.6147,
        "grad_norm": 2.124962329864502,
        "learning_rate": 3.09171921199747e-05,
        "epoch": 1.1634441866460428,
        "step": 9026
    },
    {
        "loss": 1.1132,
        "grad_norm": 1.4393000602722168,
        "learning_rate": 3.087321617857037e-05,
        "epoch": 1.1635730858468678,
        "step": 9027
    },
    {
        "loss": 2.2097,
        "grad_norm": 3.4409451484680176,
        "learning_rate": 3.082926582477379e-05,
        "epoch": 1.1637019850476926,
        "step": 9028
    },
    {
        "loss": 1.175,
        "grad_norm": 2.1423730850219727,
        "learning_rate": 3.0785341074853356e-05,
        "epoch": 1.1638308842485177,
        "step": 9029
    },
    {
        "loss": 1.2979,
        "grad_norm": 2.976229190826416,
        "learning_rate": 3.0741441945068116e-05,
        "epoch": 1.1639597834493427,
        "step": 9030
    },
    {
        "loss": 1.3582,
        "grad_norm": 3.2602219581604004,
        "learning_rate": 3.069756845166756e-05,
        "epoch": 1.1640886826501675,
        "step": 9031
    },
    {
        "loss": 1.904,
        "grad_norm": 3.6863760948181152,
        "learning_rate": 3.065372061089159e-05,
        "epoch": 1.1642175818509926,
        "step": 9032
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.791490077972412,
        "learning_rate": 3.060989843897075e-05,
        "epoch": 1.1643464810518176,
        "step": 9033
    },
    {
        "loss": 2.472,
        "grad_norm": 1.91757071018219,
        "learning_rate": 3.0566101952126014e-05,
        "epoch": 1.1644753802526424,
        "step": 9034
    },
    {
        "loss": 2.0788,
        "grad_norm": 2.529205083847046,
        "learning_rate": 3.052233116656884e-05,
        "epoch": 1.1646042794534675,
        "step": 9035
    },
    {
        "loss": 1.8918,
        "grad_norm": 1.5862075090408325,
        "learning_rate": 3.0478586098501148e-05,
        "epoch": 1.1647331786542923,
        "step": 9036
    },
    {
        "loss": 1.8805,
        "grad_norm": 1.2786661386489868,
        "learning_rate": 3.0434866764115434e-05,
        "epoch": 1.1648620778551173,
        "step": 9037
    },
    {
        "loss": 1.5469,
        "grad_norm": 1.8616477251052856,
        "learning_rate": 3.039117317959468e-05,
        "epoch": 1.1649909770559423,
        "step": 9038
    },
    {
        "loss": 1.5003,
        "grad_norm": 1.3810094594955444,
        "learning_rate": 3.0347505361112284e-05,
        "epoch": 1.1651198762567672,
        "step": 9039
    },
    {
        "loss": 1.2716,
        "grad_norm": 3.1389036178588867,
        "learning_rate": 3.0303863324831892e-05,
        "epoch": 1.1652487754575922,
        "step": 9040
    },
    {
        "loss": 2.1078,
        "grad_norm": 2.7092888355255127,
        "learning_rate": 3.026024708690801e-05,
        "epoch": 1.165377674658417,
        "step": 9041
    },
    {
        "loss": 1.4954,
        "grad_norm": 1.8793673515319824,
        "learning_rate": 3.0216656663485344e-05,
        "epoch": 1.165506573859242,
        "step": 9042
    },
    {
        "loss": 1.0445,
        "grad_norm": 2.790083408355713,
        "learning_rate": 3.0173092070699017e-05,
        "epoch": 1.165635473060067,
        "step": 9043
    },
    {
        "loss": 1.2649,
        "grad_norm": 2.689319372177124,
        "learning_rate": 3.012955332467482e-05,
        "epoch": 1.165764372260892,
        "step": 9044
    },
    {
        "loss": 1.358,
        "grad_norm": 2.921820878982544,
        "learning_rate": 3.008604044152873e-05,
        "epoch": 1.165893271461717,
        "step": 9045
    },
    {
        "loss": 0.4894,
        "grad_norm": 2.72865891456604,
        "learning_rate": 3.0042553437367194e-05,
        "epoch": 1.1660221706625418,
        "step": 9046
    },
    {
        "loss": 2.2743,
        "grad_norm": 2.004915237426758,
        "learning_rate": 2.9999092328287226e-05,
        "epoch": 1.1661510698633668,
        "step": 9047
    },
    {
        "loss": 2.4753,
        "grad_norm": 2.485305070877075,
        "learning_rate": 2.9955657130376146e-05,
        "epoch": 1.1662799690641918,
        "step": 9048
    },
    {
        "loss": 2.1501,
        "grad_norm": 1.8823893070220947,
        "learning_rate": 2.9912247859711716e-05,
        "epoch": 1.1664088682650167,
        "step": 9049
    },
    {
        "loss": 0.8004,
        "grad_norm": 2.069424867630005,
        "learning_rate": 2.9868864532362063e-05,
        "epoch": 1.1665377674658417,
        "step": 9050
    },
    {
        "loss": 1.3442,
        "grad_norm": 2.1377995014190674,
        "learning_rate": 2.9825507164385734e-05,
        "epoch": 1.1666666666666667,
        "step": 9051
    },
    {
        "loss": 1.3793,
        "grad_norm": 2.874730348587036,
        "learning_rate": 2.9782175771831654e-05,
        "epoch": 1.1667955658674916,
        "step": 9052
    },
    {
        "loss": 1.5145,
        "grad_norm": 1.9731885194778442,
        "learning_rate": 2.973887037073908e-05,
        "epoch": 1.1669244650683166,
        "step": 9053
    },
    {
        "loss": 1.4964,
        "grad_norm": 1.885582447052002,
        "learning_rate": 2.9695590977137787e-05,
        "epoch": 1.1670533642691416,
        "step": 9054
    },
    {
        "loss": 0.966,
        "grad_norm": 2.655745267868042,
        "learning_rate": 2.965233760704791e-05,
        "epoch": 1.1671822634699665,
        "step": 9055
    },
    {
        "loss": 1.1483,
        "grad_norm": 1.854169249534607,
        "learning_rate": 2.960911027647988e-05,
        "epoch": 1.1673111626707915,
        "step": 9056
    },
    {
        "loss": 1.223,
        "grad_norm": 2.530343532562256,
        "learning_rate": 2.956590900143431e-05,
        "epoch": 1.1674400618716163,
        "step": 9057
    },
    {
        "loss": 0.8918,
        "grad_norm": 2.081714391708374,
        "learning_rate": 2.9522733797902568e-05,
        "epoch": 1.1675689610724413,
        "step": 9058
    },
    {
        "loss": 1.3602,
        "grad_norm": 1.9606226682662964,
        "learning_rate": 2.9479584681866033e-05,
        "epoch": 1.1676978602732664,
        "step": 9059
    },
    {
        "loss": 1.6388,
        "grad_norm": 1.3839472532272339,
        "learning_rate": 2.943646166929661e-05,
        "epoch": 1.1678267594740912,
        "step": 9060
    },
    {
        "loss": 0.9136,
        "grad_norm": 4.207770824432373,
        "learning_rate": 2.9393364776156453e-05,
        "epoch": 1.1679556586749162,
        "step": 9061
    },
    {
        "loss": 2.2947,
        "grad_norm": 2.190396785736084,
        "learning_rate": 2.935029401839826e-05,
        "epoch": 1.168084557875741,
        "step": 9062
    },
    {
        "loss": 1.1145,
        "grad_norm": 1.9750115871429443,
        "learning_rate": 2.9307249411964578e-05,
        "epoch": 1.168213457076566,
        "step": 9063
    },
    {
        "loss": 1.4029,
        "grad_norm": 1.1696391105651855,
        "learning_rate": 2.9264230972788753e-05,
        "epoch": 1.1683423562773911,
        "step": 9064
    },
    {
        "loss": 0.4816,
        "grad_norm": 1.644740343093872,
        "learning_rate": 2.922123871679423e-05,
        "epoch": 1.168471255478216,
        "step": 9065
    },
    {
        "loss": 1.5519,
        "grad_norm": 2.6892144680023193,
        "learning_rate": 2.9178272659894833e-05,
        "epoch": 1.168600154679041,
        "step": 9066
    },
    {
        "loss": 1.4456,
        "grad_norm": 2.474418878555298,
        "learning_rate": 2.913533281799462e-05,
        "epoch": 1.168729053879866,
        "step": 9067
    },
    {
        "loss": 1.3598,
        "grad_norm": 2.225452184677124,
        "learning_rate": 2.9092419206987976e-05,
        "epoch": 1.1688579530806908,
        "step": 9068
    },
    {
        "loss": 1.3206,
        "grad_norm": 3.3521313667297363,
        "learning_rate": 2.904953184275958e-05,
        "epoch": 1.1689868522815159,
        "step": 9069
    },
    {
        "loss": 1.6763,
        "grad_norm": 2.297102928161621,
        "learning_rate": 2.9006670741184338e-05,
        "epoch": 1.169115751482341,
        "step": 9070
    },
    {
        "loss": 2.1995,
        "grad_norm": 2.3824329376220703,
        "learning_rate": 2.8963835918127534e-05,
        "epoch": 1.1692446506831657,
        "step": 9071
    },
    {
        "loss": 0.8531,
        "grad_norm": 1.879014253616333,
        "learning_rate": 2.8921027389444767e-05,
        "epoch": 1.1693735498839908,
        "step": 9072
    },
    {
        "loss": 1.5236,
        "grad_norm": 1.7379401922225952,
        "learning_rate": 2.8878245170981766e-05,
        "epoch": 1.1695024490848156,
        "step": 9073
    },
    {
        "loss": 1.014,
        "grad_norm": 2.0850601196289062,
        "learning_rate": 2.88354892785745e-05,
        "epoch": 1.1696313482856406,
        "step": 9074
    },
    {
        "loss": 1.9478,
        "grad_norm": 1.7618424892425537,
        "learning_rate": 2.8792759728049346e-05,
        "epoch": 1.1697602474864657,
        "step": 9075
    },
    {
        "loss": 1.6822,
        "grad_norm": 2.3895108699798584,
        "learning_rate": 2.875005653522278e-05,
        "epoch": 1.1698891466872905,
        "step": 9076
    },
    {
        "loss": 1.6008,
        "grad_norm": 3.03731632232666,
        "learning_rate": 2.870737971590164e-05,
        "epoch": 1.1700180458881155,
        "step": 9077
    },
    {
        "loss": 1.4912,
        "grad_norm": 2.039116144180298,
        "learning_rate": 2.8664729285882906e-05,
        "epoch": 1.1701469450889403,
        "step": 9078
    },
    {
        "loss": 1.9229,
        "grad_norm": 2.0608057975769043,
        "learning_rate": 2.8622105260954048e-05,
        "epoch": 1.1702758442897654,
        "step": 9079
    },
    {
        "loss": 1.8323,
        "grad_norm": 1.8111673593521118,
        "learning_rate": 2.8579507656892223e-05,
        "epoch": 1.1704047434905904,
        "step": 9080
    },
    {
        "loss": 2.4553,
        "grad_norm": 2.4483766555786133,
        "learning_rate": 2.8536936489465293e-05,
        "epoch": 1.1705336426914152,
        "step": 9081
    },
    {
        "loss": 1.8533,
        "grad_norm": 1.3990181684494019,
        "learning_rate": 2.849439177443124e-05,
        "epoch": 1.1706625418922403,
        "step": 9082
    },
    {
        "loss": 1.7752,
        "grad_norm": 2.261364221572876,
        "learning_rate": 2.845187352753813e-05,
        "epoch": 1.170791441093065,
        "step": 9083
    },
    {
        "loss": 1.4482,
        "grad_norm": 2.493063449859619,
        "learning_rate": 2.8409381764524302e-05,
        "epoch": 1.1709203402938901,
        "step": 9084
    },
    {
        "loss": 2.1831,
        "grad_norm": 2.958854913711548,
        "learning_rate": 2.836691650111828e-05,
        "epoch": 1.1710492394947152,
        "step": 9085
    },
    {
        "loss": 1.6612,
        "grad_norm": 2.6694512367248535,
        "learning_rate": 2.8324477753038792e-05,
        "epoch": 1.17117813869554,
        "step": 9086
    },
    {
        "loss": 1.1433,
        "grad_norm": 2.7224056720733643,
        "learning_rate": 2.8282065535994652e-05,
        "epoch": 1.171307037896365,
        "step": 9087
    },
    {
        "loss": 1.7755,
        "grad_norm": 2.3831605911254883,
        "learning_rate": 2.8239679865685042e-05,
        "epoch": 1.17143593709719,
        "step": 9088
    },
    {
        "loss": 2.1118,
        "grad_norm": 2.296875476837158,
        "learning_rate": 2.819732075779925e-05,
        "epoch": 1.1715648362980149,
        "step": 9089
    },
    {
        "loss": 1.7925,
        "grad_norm": 1.8728042840957642,
        "learning_rate": 2.81549882280167e-05,
        "epoch": 1.17169373549884,
        "step": 9090
    },
    {
        "loss": 1.2707,
        "grad_norm": 1.8469752073287964,
        "learning_rate": 2.8112682292006853e-05,
        "epoch": 1.171822634699665,
        "step": 9091
    },
    {
        "loss": 1.8739,
        "grad_norm": 2.0615155696868896,
        "learning_rate": 2.8070402965429543e-05,
        "epoch": 1.1719515339004898,
        "step": 9092
    },
    {
        "loss": 1.6815,
        "grad_norm": 2.3764641284942627,
        "learning_rate": 2.8028150263934684e-05,
        "epoch": 1.1720804331013148,
        "step": 9093
    },
    {
        "loss": 1.4428,
        "grad_norm": 2.5083131790161133,
        "learning_rate": 2.798592420316225e-05,
        "epoch": 1.1722093323021396,
        "step": 9094
    },
    {
        "loss": 1.3563,
        "grad_norm": 3.8326950073242188,
        "learning_rate": 2.7943724798742434e-05,
        "epoch": 1.1723382315029647,
        "step": 9095
    },
    {
        "loss": 1.4753,
        "grad_norm": 1.4496378898620605,
        "learning_rate": 2.79015520662957e-05,
        "epoch": 1.1724671307037897,
        "step": 9096
    },
    {
        "loss": 1.9549,
        "grad_norm": 1.5081110000610352,
        "learning_rate": 2.78594060214322e-05,
        "epoch": 1.1725960299046145,
        "step": 9097
    },
    {
        "loss": 1.0894,
        "grad_norm": Infinity,
        "learning_rate": 2.78594060214322e-05,
        "epoch": 1.1727249291054396,
        "step": 9098
    },
    {
        "loss": 1.3254,
        "grad_norm": 2.2734556198120117,
        "learning_rate": 2.781728667975264e-05,
        "epoch": 1.1728538283062644,
        "step": 9099
    },
    {
        "loss": 1.8587,
        "grad_norm": 1.846671462059021,
        "learning_rate": 2.7775194056847764e-05,
        "epoch": 1.1729827275070894,
        "step": 9100
    },
    {
        "loss": 1.999,
        "grad_norm": 1.7465729713439941,
        "learning_rate": 2.7733128168298295e-05,
        "epoch": 1.1731116267079145,
        "step": 9101
    },
    {
        "loss": 1.9209,
        "grad_norm": 1.5266038179397583,
        "learning_rate": 2.7691089029675044e-05,
        "epoch": 1.1732405259087393,
        "step": 9102
    },
    {
        "loss": 2.3565,
        "grad_norm": 2.804313898086548,
        "learning_rate": 2.76490766565392e-05,
        "epoch": 1.1733694251095643,
        "step": 9103
    },
    {
        "loss": 2.0794,
        "grad_norm": 2.1215267181396484,
        "learning_rate": 2.7607091064441647e-05,
        "epoch": 1.1734983243103894,
        "step": 9104
    },
    {
        "loss": 1.4376,
        "grad_norm": 3.2140345573425293,
        "learning_rate": 2.7565132268923564e-05,
        "epoch": 1.1736272235112142,
        "step": 9105
    },
    {
        "loss": 1.6349,
        "grad_norm": 2.875645160675049,
        "learning_rate": 2.7523200285516247e-05,
        "epoch": 1.1737561227120392,
        "step": 9106
    },
    {
        "loss": 1.723,
        "grad_norm": 1.7577191591262817,
        "learning_rate": 2.748129512974108e-05,
        "epoch": 1.1738850219128643,
        "step": 9107
    },
    {
        "loss": 1.535,
        "grad_norm": 2.1811466217041016,
        "learning_rate": 2.7439416817109442e-05,
        "epoch": 1.174013921113689,
        "step": 9108
    },
    {
        "loss": 1.6792,
        "grad_norm": 2.172098398208618,
        "learning_rate": 2.739756536312268e-05,
        "epoch": 1.1741428203145141,
        "step": 9109
    },
    {
        "loss": 2.1695,
        "grad_norm": 2.1488568782806396,
        "learning_rate": 2.7355740783272375e-05,
        "epoch": 1.174271719515339,
        "step": 9110
    },
    {
        "loss": 1.3529,
        "grad_norm": 2.589205265045166,
        "learning_rate": 2.731394309304013e-05,
        "epoch": 1.174400618716164,
        "step": 9111
    },
    {
        "loss": 1.6019,
        "grad_norm": 2.8671183586120605,
        "learning_rate": 2.7272172307897492e-05,
        "epoch": 1.174529517916989,
        "step": 9112
    },
    {
        "loss": 0.9675,
        "grad_norm": 2.8208200931549072,
        "learning_rate": 2.723042844330611e-05,
        "epoch": 1.1746584171178138,
        "step": 9113
    },
    {
        "loss": 1.6647,
        "grad_norm": 2.1903579235076904,
        "learning_rate": 2.718871151471788e-05,
        "epoch": 1.1747873163186389,
        "step": 9114
    },
    {
        "loss": 1.806,
        "grad_norm": 2.4495127201080322,
        "learning_rate": 2.714702153757419e-05,
        "epoch": 1.1749162155194637,
        "step": 9115
    },
    {
        "loss": 1.4796,
        "grad_norm": 2.706916332244873,
        "learning_rate": 2.7105358527306946e-05,
        "epoch": 1.1750451147202887,
        "step": 9116
    },
    {
        "loss": 1.6846,
        "grad_norm": 1.9519476890563965,
        "learning_rate": 2.7063722499337984e-05,
        "epoch": 1.1751740139211138,
        "step": 9117
    },
    {
        "loss": 2.0112,
        "grad_norm": 2.2844903469085693,
        "learning_rate": 2.702211346907899e-05,
        "epoch": 1.1753029131219386,
        "step": 9118
    },
    {
        "loss": 1.6702,
        "grad_norm": 3.235301971435547,
        "learning_rate": 2.69805314519317e-05,
        "epoch": 1.1754318123227636,
        "step": 9119
    },
    {
        "loss": 2.0889,
        "grad_norm": 2.0284550189971924,
        "learning_rate": 2.693897646328808e-05,
        "epoch": 1.1755607115235884,
        "step": 9120
    },
    {
        "loss": 0.9189,
        "grad_norm": 2.260838270187378,
        "learning_rate": 2.689744851852969e-05,
        "epoch": 1.1756896107244135,
        "step": 9121
    },
    {
        "loss": 1.129,
        "grad_norm": 2.445739269256592,
        "learning_rate": 2.685594763302839e-05,
        "epoch": 1.1758185099252385,
        "step": 9122
    },
    {
        "loss": 0.6794,
        "grad_norm": 2.987636089324951,
        "learning_rate": 2.681447382214589e-05,
        "epoch": 1.1759474091260633,
        "step": 9123
    },
    {
        "loss": 1.3038,
        "grad_norm": 3.6045570373535156,
        "learning_rate": 2.6773027101234017e-05,
        "epoch": 1.1760763083268884,
        "step": 9124
    },
    {
        "loss": 1.8207,
        "grad_norm": 2.3630473613739014,
        "learning_rate": 2.6731607485634413e-05,
        "epoch": 1.1762052075277134,
        "step": 9125
    },
    {
        "loss": 1.6982,
        "grad_norm": 2.1191020011901855,
        "learning_rate": 2.669021499067871e-05,
        "epoch": 1.1763341067285382,
        "step": 9126
    },
    {
        "loss": 2.0146,
        "grad_norm": 1.93538498878479,
        "learning_rate": 2.66488496316886e-05,
        "epoch": 1.1764630059293633,
        "step": 9127
    },
    {
        "loss": 1.5513,
        "grad_norm": 1.6483298540115356,
        "learning_rate": 2.660751142397564e-05,
        "epoch": 1.1765919051301883,
        "step": 9128
    },
    {
        "loss": 1.7923,
        "grad_norm": 2.3507096767425537,
        "learning_rate": 2.65662003828413e-05,
        "epoch": 1.1767208043310131,
        "step": 9129
    },
    {
        "loss": 1.1568,
        "grad_norm": 1.801939606666565,
        "learning_rate": 2.6524916523577148e-05,
        "epoch": 1.1768497035318382,
        "step": 9130
    },
    {
        "loss": 2.0279,
        "grad_norm": 1.9185967445373535,
        "learning_rate": 2.6483659861464694e-05,
        "epoch": 1.176978602732663,
        "step": 9131
    },
    {
        "loss": 2.1893,
        "grad_norm": 2.7358438968658447,
        "learning_rate": 2.644243041177501e-05,
        "epoch": 1.177107501933488,
        "step": 9132
    },
    {
        "loss": 1.1498,
        "grad_norm": 2.197514533996582,
        "learning_rate": 2.6401228189769533e-05,
        "epoch": 1.177236401134313,
        "step": 9133
    },
    {
        "loss": 2.0807,
        "grad_norm": 1.3464508056640625,
        "learning_rate": 2.636005321069952e-05,
        "epoch": 1.1773653003351379,
        "step": 9134
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.8878487348556519,
        "learning_rate": 2.6318905489806024e-05,
        "epoch": 1.177494199535963,
        "step": 9135
    },
    {
        "loss": 1.7321,
        "grad_norm": 1.4737766981124878,
        "learning_rate": 2.6277785042320002e-05,
        "epoch": 1.1776230987367877,
        "step": 9136
    },
    {
        "loss": 0.6822,
        "grad_norm": 1.6984992027282715,
        "learning_rate": 2.6236691883462582e-05,
        "epoch": 1.1777519979376128,
        "step": 9137
    },
    {
        "loss": 1.447,
        "grad_norm": 2.308835744857788,
        "learning_rate": 2.6195626028444354e-05,
        "epoch": 1.1778808971384378,
        "step": 9138
    },
    {
        "loss": 1.8206,
        "grad_norm": 3.1525418758392334,
        "learning_rate": 2.6154587492466155e-05,
        "epoch": 1.1780097963392626,
        "step": 9139
    },
    {
        "loss": 2.1485,
        "grad_norm": 1.8573484420776367,
        "learning_rate": 2.6113576290718554e-05,
        "epoch": 1.1781386955400877,
        "step": 9140
    },
    {
        "loss": 1.4905,
        "grad_norm": 2.778674602508545,
        "learning_rate": 2.6072592438382125e-05,
        "epoch": 1.1782675947409127,
        "step": 9141
    },
    {
        "loss": 0.4984,
        "grad_norm": 1.4406975507736206,
        "learning_rate": 2.6031635950627197e-05,
        "epoch": 1.1783964939417375,
        "step": 9142
    },
    {
        "loss": 1.0472,
        "grad_norm": 3.1764345169067383,
        "learning_rate": 2.5990706842613954e-05,
        "epoch": 1.1785253931425625,
        "step": 9143
    },
    {
        "loss": 1.5603,
        "grad_norm": 2.998203754425049,
        "learning_rate": 2.5949805129492578e-05,
        "epoch": 1.1786542923433876,
        "step": 9144
    },
    {
        "loss": 1.5113,
        "grad_norm": 2.68135142326355,
        "learning_rate": 2.590893082640299e-05,
        "epoch": 1.1787831915442124,
        "step": 9145
    },
    {
        "loss": 1.342,
        "grad_norm": 2.9145045280456543,
        "learning_rate": 2.5868083948474962e-05,
        "epoch": 1.1789120907450374,
        "step": 9146
    },
    {
        "loss": 1.8223,
        "grad_norm": 4.537139415740967,
        "learning_rate": 2.582726451082823e-05,
        "epoch": 1.1790409899458623,
        "step": 9147
    },
    {
        "loss": 1.2093,
        "grad_norm": 1.9448959827423096,
        "learning_rate": 2.5786472528572413e-05,
        "epoch": 1.1791698891466873,
        "step": 9148
    },
    {
        "loss": 1.4977,
        "grad_norm": 1.9213985204696655,
        "learning_rate": 2.574570801680658e-05,
        "epoch": 1.1792987883475123,
        "step": 9149
    },
    {
        "loss": 1.3505,
        "grad_norm": 1.253480315208435,
        "learning_rate": 2.5704970990620058e-05,
        "epoch": 1.1794276875483372,
        "step": 9150
    },
    {
        "loss": 1.1848,
        "grad_norm": 4.357680320739746,
        "learning_rate": 2.566426146509191e-05,
        "epoch": 1.1795565867491622,
        "step": 9151
    },
    {
        "loss": 2.0096,
        "grad_norm": 3.133765935897827,
        "learning_rate": 2.56235794552909e-05,
        "epoch": 1.179685485949987,
        "step": 9152
    },
    {
        "loss": 1.82,
        "grad_norm": 4.209023952484131,
        "learning_rate": 2.5582924976275614e-05,
        "epoch": 1.179814385150812,
        "step": 9153
    },
    {
        "loss": 1.7149,
        "grad_norm": 1.2208223342895508,
        "learning_rate": 2.5542298043094636e-05,
        "epoch": 1.179943284351637,
        "step": 9154
    },
    {
        "loss": 1.304,
        "grad_norm": 2.73860239982605,
        "learning_rate": 2.550169867078611e-05,
        "epoch": 1.180072183552462,
        "step": 9155
    },
    {
        "loss": 1.4805,
        "grad_norm": 1.9494224786758423,
        "learning_rate": 2.5461126874378095e-05,
        "epoch": 1.180201082753287,
        "step": 9156
    },
    {
        "loss": 1.7508,
        "grad_norm": 1.6769667863845825,
        "learning_rate": 2.5420582668888416e-05,
        "epoch": 1.1803299819541118,
        "step": 9157
    },
    {
        "loss": 2.087,
        "grad_norm": 2.659712076187134,
        "learning_rate": 2.5380066069324803e-05,
        "epoch": 1.1804588811549368,
        "step": 9158
    },
    {
        "loss": 1.7215,
        "grad_norm": 2.6305253505706787,
        "learning_rate": 2.5339577090684575e-05,
        "epoch": 1.1805877803557618,
        "step": 9159
    },
    {
        "loss": 2.133,
        "grad_norm": 1.5604231357574463,
        "learning_rate": 2.5299115747954982e-05,
        "epoch": 1.1807166795565867,
        "step": 9160
    },
    {
        "loss": 1.6811,
        "grad_norm": 3.5898046493530273,
        "learning_rate": 2.5258682056112948e-05,
        "epoch": 1.1808455787574117,
        "step": 9161
    },
    {
        "loss": 1.0385,
        "grad_norm": 2.7931714057922363,
        "learning_rate": 2.521827603012521e-05,
        "epoch": 1.1809744779582367,
        "step": 9162
    },
    {
        "loss": 2.0242,
        "grad_norm": 2.443264961242676,
        "learning_rate": 2.517789768494817e-05,
        "epoch": 1.1811033771590616,
        "step": 9163
    },
    {
        "loss": 1.0523,
        "grad_norm": 3.769008159637451,
        "learning_rate": 2.5137547035528177e-05,
        "epoch": 1.1812322763598866,
        "step": 9164
    },
    {
        "loss": 1.1537,
        "grad_norm": 2.41200590133667,
        "learning_rate": 2.5097224096801175e-05,
        "epoch": 1.1813611755607116,
        "step": 9165
    },
    {
        "loss": 2.0983,
        "grad_norm": 2.476396083831787,
        "learning_rate": 2.5056928883692964e-05,
        "epoch": 1.1814900747615364,
        "step": 9166
    },
    {
        "loss": 1.6719,
        "grad_norm": 4.088692665100098,
        "learning_rate": 2.5016661411118836e-05,
        "epoch": 1.1816189739623615,
        "step": 9167
    },
    {
        "loss": 0.8661,
        "grad_norm": 2.3303489685058594,
        "learning_rate": 2.497642169398414e-05,
        "epoch": 1.1817478731631863,
        "step": 9168
    },
    {
        "loss": 1.4679,
        "grad_norm": 2.681429386138916,
        "learning_rate": 2.4936209747183736e-05,
        "epoch": 1.1818767723640113,
        "step": 9169
    },
    {
        "loss": 0.9637,
        "grad_norm": 2.8651607036590576,
        "learning_rate": 2.4896025585602234e-05,
        "epoch": 1.1820056715648364,
        "step": 9170
    },
    {
        "loss": 1.2297,
        "grad_norm": 3.237957239151001,
        "learning_rate": 2.4855869224114115e-05,
        "epoch": 1.1821345707656612,
        "step": 9171
    },
    {
        "loss": 0.9241,
        "grad_norm": 1.676257848739624,
        "learning_rate": 2.4815740677583342e-05,
        "epoch": 1.1822634699664862,
        "step": 9172
    },
    {
        "loss": 1.8369,
        "grad_norm": 2.9483838081359863,
        "learning_rate": 2.477563996086365e-05,
        "epoch": 1.182392369167311,
        "step": 9173
    },
    {
        "loss": 1.313,
        "grad_norm": 2.2424700260162354,
        "learning_rate": 2.4735567088798605e-05,
        "epoch": 1.182521268368136,
        "step": 9174
    },
    {
        "loss": 1.4121,
        "grad_norm": 2.9127793312072754,
        "learning_rate": 2.4695522076221365e-05,
        "epoch": 1.1826501675689611,
        "step": 9175
    },
    {
        "loss": 1.265,
        "grad_norm": 2.3122658729553223,
        "learning_rate": 2.4655504937954733e-05,
        "epoch": 1.182779066769786,
        "step": 9176
    },
    {
        "loss": 0.7852,
        "grad_norm": 1.7548422813415527,
        "learning_rate": 2.46155156888113e-05,
        "epoch": 1.182907965970611,
        "step": 9177
    },
    {
        "loss": 2.2643,
        "grad_norm": 1.463443636894226,
        "learning_rate": 2.4575554343593233e-05,
        "epoch": 1.183036865171436,
        "step": 9178
    },
    {
        "loss": 1.1331,
        "grad_norm": 2.258984327316284,
        "learning_rate": 2.453562091709243e-05,
        "epoch": 1.1831657643722608,
        "step": 9179
    },
    {
        "loss": 1.8731,
        "grad_norm": 1.9561372995376587,
        "learning_rate": 2.449571542409037e-05,
        "epoch": 1.1832946635730859,
        "step": 9180
    },
    {
        "loss": 1.8104,
        "grad_norm": 3.371405839920044,
        "learning_rate": 2.445583787935832e-05,
        "epoch": 1.183423562773911,
        "step": 9181
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.9663441181182861,
        "learning_rate": 2.4415988297657212e-05,
        "epoch": 1.1835524619747357,
        "step": 9182
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.2416812181472778,
        "learning_rate": 2.4376166693737547e-05,
        "epoch": 1.1836813611755608,
        "step": 9183
    },
    {
        "loss": 2.4127,
        "grad_norm": 2.233720302581787,
        "learning_rate": 2.4336373082339313e-05,
        "epoch": 1.1838102603763856,
        "step": 9184
    },
    {
        "loss": 1.2725,
        "grad_norm": 1.6948838233947754,
        "learning_rate": 2.429660747819248e-05,
        "epoch": 1.1839391595772106,
        "step": 9185
    },
    {
        "loss": 1.5829,
        "grad_norm": 2.636312961578369,
        "learning_rate": 2.425686989601641e-05,
        "epoch": 1.1840680587780357,
        "step": 9186
    },
    {
        "loss": 1.635,
        "grad_norm": 2.855403184890747,
        "learning_rate": 2.4217160350520106e-05,
        "epoch": 1.1841969579788605,
        "step": 9187
    },
    {
        "loss": 1.6481,
        "grad_norm": 3.135040044784546,
        "learning_rate": 2.4177478856402304e-05,
        "epoch": 1.1843258571796855,
        "step": 9188
    },
    {
        "loss": 1.4134,
        "grad_norm": 2.6824285984039307,
        "learning_rate": 2.4137825428351425e-05,
        "epoch": 1.1844547563805103,
        "step": 9189
    },
    {
        "loss": 0.784,
        "grad_norm": 2.600088596343994,
        "learning_rate": 2.4098200081045108e-05,
        "epoch": 1.1845836555813354,
        "step": 9190
    },
    {
        "loss": 1.956,
        "grad_norm": 1.6890571117401123,
        "learning_rate": 2.4058602829151034e-05,
        "epoch": 1.1847125547821604,
        "step": 9191
    },
    {
        "loss": 1.1987,
        "grad_norm": 2.367863416671753,
        "learning_rate": 2.4019033687326276e-05,
        "epoch": 1.1848414539829852,
        "step": 9192
    },
    {
        "loss": 1.1578,
        "grad_norm": 1.7078241109848022,
        "learning_rate": 2.3979492670217575e-05,
        "epoch": 1.1849703531838103,
        "step": 9193
    },
    {
        "loss": 1.8867,
        "grad_norm": 1.9954277276992798,
        "learning_rate": 2.3939979792461186e-05,
        "epoch": 1.185099252384635,
        "step": 9194
    },
    {
        "loss": 1.696,
        "grad_norm": 2.216384172439575,
        "learning_rate": 2.3900495068683004e-05,
        "epoch": 1.1852281515854601,
        "step": 9195
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.503488302230835,
        "learning_rate": 2.386103851349848e-05,
        "epoch": 1.1853570507862852,
        "step": 9196
    },
    {
        "loss": 1.2887,
        "grad_norm": 2.6808745861053467,
        "learning_rate": 2.3821610141512585e-05,
        "epoch": 1.18548594998711,
        "step": 9197
    },
    {
        "loss": 1.8971,
        "grad_norm": 1.449936866760254,
        "learning_rate": 2.378220996731999e-05,
        "epoch": 1.185614849187935,
        "step": 9198
    },
    {
        "loss": 1.2796,
        "grad_norm": 2.403341293334961,
        "learning_rate": 2.3742838005504897e-05,
        "epoch": 1.18574374838876,
        "step": 9199
    },
    {
        "loss": 2.2837,
        "grad_norm": 2.2924883365631104,
        "learning_rate": 2.3703494270641034e-05,
        "epoch": 1.1858726475895849,
        "step": 9200
    },
    {
        "loss": 1.9584,
        "grad_norm": 2.254056930541992,
        "learning_rate": 2.366417877729149e-05,
        "epoch": 1.18600154679041,
        "step": 9201
    },
    {
        "loss": 1.6321,
        "grad_norm": 2.498669147491455,
        "learning_rate": 2.3624891540009274e-05,
        "epoch": 1.186130445991235,
        "step": 9202
    },
    {
        "loss": 1.4587,
        "grad_norm": 1.9831688404083252,
        "learning_rate": 2.358563257333667e-05,
        "epoch": 1.1862593451920598,
        "step": 9203
    },
    {
        "loss": 0.9211,
        "grad_norm": 2.5460472106933594,
        "learning_rate": 2.3546401891805515e-05,
        "epoch": 1.1863882443928848,
        "step": 9204
    },
    {
        "loss": 0.8285,
        "grad_norm": 2.671252965927124,
        "learning_rate": 2.3507199509937295e-05,
        "epoch": 1.1865171435937096,
        "step": 9205
    },
    {
        "loss": 1.9117,
        "grad_norm": 2.567950487136841,
        "learning_rate": 2.3468025442243093e-05,
        "epoch": 1.1866460427945347,
        "step": 9206
    },
    {
        "loss": 0.8436,
        "grad_norm": 1.9999301433563232,
        "learning_rate": 2.3428879703223105e-05,
        "epoch": 1.1867749419953597,
        "step": 9207
    },
    {
        "loss": 1.2685,
        "grad_norm": 2.383429527282715,
        "learning_rate": 2.3389762307367467e-05,
        "epoch": 1.1869038411961845,
        "step": 9208
    },
    {
        "loss": 2.0155,
        "grad_norm": 2.6506834030151367,
        "learning_rate": 2.3350673269155655e-05,
        "epoch": 1.1870327403970096,
        "step": 9209
    },
    {
        "loss": 1.0004,
        "grad_norm": 2.738161325454712,
        "learning_rate": 2.3311612603056676e-05,
        "epoch": 1.1871616395978344,
        "step": 9210
    },
    {
        "loss": 1.4729,
        "grad_norm": 1.8575184345245361,
        "learning_rate": 2.3272580323529e-05,
        "epoch": 1.1872905387986594,
        "step": 9211
    },
    {
        "loss": 1.3711,
        "grad_norm": 2.098470687866211,
        "learning_rate": 2.3233576445020622e-05,
        "epoch": 1.1874194379994845,
        "step": 9212
    },
    {
        "loss": 0.9105,
        "grad_norm": 2.3912320137023926,
        "learning_rate": 2.3194600981969006e-05,
        "epoch": 1.1875483372003093,
        "step": 9213
    },
    {
        "loss": 1.7541,
        "grad_norm": 2.0281031131744385,
        "learning_rate": 2.3155653948801036e-05,
        "epoch": 1.1876772364011343,
        "step": 9214
    },
    {
        "loss": 1.8584,
        "grad_norm": 1.8915047645568848,
        "learning_rate": 2.3116735359933218e-05,
        "epoch": 1.1878061356019594,
        "step": 9215
    },
    {
        "loss": 1.7715,
        "grad_norm": 2.3841006755828857,
        "learning_rate": 2.3077845229771488e-05,
        "epoch": 1.1879350348027842,
        "step": 9216
    },
    {
        "loss": 1.4809,
        "grad_norm": 2.3357136249542236,
        "learning_rate": 2.3038983572711225e-05,
        "epoch": 1.1880639340036092,
        "step": 9217
    },
    {
        "loss": 1.374,
        "grad_norm": 1.9012243747711182,
        "learning_rate": 2.3000150403137088e-05,
        "epoch": 1.1881928332044343,
        "step": 9218
    },
    {
        "loss": 1.8602,
        "grad_norm": 3.5192060470581055,
        "learning_rate": 2.2961345735423535e-05,
        "epoch": 1.188321732405259,
        "step": 9219
    },
    {
        "loss": 1.9486,
        "grad_norm": 1.6372469663619995,
        "learning_rate": 2.2922569583934195e-05,
        "epoch": 1.188450631606084,
        "step": 9220
    },
    {
        "loss": 1.1083,
        "grad_norm": 2.8258161544799805,
        "learning_rate": 2.2883821963022278e-05,
        "epoch": 1.188579530806909,
        "step": 9221
    },
    {
        "loss": 1.093,
        "grad_norm": 3.6536076068878174,
        "learning_rate": 2.284510288703039e-05,
        "epoch": 1.188708430007734,
        "step": 9222
    },
    {
        "loss": 2.0108,
        "grad_norm": 1.6435651779174805,
        "learning_rate": 2.280641237029073e-05,
        "epoch": 1.188837329208559,
        "step": 9223
    },
    {
        "loss": 2.1294,
        "grad_norm": 2.220231771469116,
        "learning_rate": 2.27677504271245e-05,
        "epoch": 1.1889662284093838,
        "step": 9224
    },
    {
        "loss": 2.1499,
        "grad_norm": 1.3821535110473633,
        "learning_rate": 2.2729117071842755e-05,
        "epoch": 1.1890951276102089,
        "step": 9225
    },
    {
        "loss": 2.0401,
        "grad_norm": 1.7485032081604004,
        "learning_rate": 2.269051231874585e-05,
        "epoch": 1.1892240268110337,
        "step": 9226
    },
    {
        "loss": 1.7599,
        "grad_norm": 2.5180156230926514,
        "learning_rate": 2.265193618212348e-05,
        "epoch": 1.1893529260118587,
        "step": 9227
    },
    {
        "loss": 2.2492,
        "grad_norm": 1.5213584899902344,
        "learning_rate": 2.261338867625472e-05,
        "epoch": 1.1894818252126838,
        "step": 9228
    },
    {
        "loss": 1.8303,
        "grad_norm": 3.703030586242676,
        "learning_rate": 2.2574869815408283e-05,
        "epoch": 1.1896107244135086,
        "step": 9229
    },
    {
        "loss": 0.8172,
        "grad_norm": 3.5307159423828125,
        "learning_rate": 2.253637961384194e-05,
        "epoch": 1.1897396236143336,
        "step": 9230
    },
    {
        "loss": 1.0641,
        "grad_norm": 2.6643471717834473,
        "learning_rate": 2.2497918085803028e-05,
        "epoch": 1.1898685228151584,
        "step": 9231
    },
    {
        "loss": 1.8398,
        "grad_norm": 1.6237515211105347,
        "learning_rate": 2.2459485245528312e-05,
        "epoch": 1.1899974220159835,
        "step": 9232
    },
    {
        "loss": 1.5489,
        "grad_norm": 2.818493604660034,
        "learning_rate": 2.2421081107243945e-05,
        "epoch": 1.1901263212168085,
        "step": 9233
    },
    {
        "loss": 1.3739,
        "grad_norm": 2.2001500129699707,
        "learning_rate": 2.2382705685165383e-05,
        "epoch": 1.1902552204176333,
        "step": 9234
    },
    {
        "loss": 1.7939,
        "grad_norm": 2.961416006088257,
        "learning_rate": 2.2344358993497383e-05,
        "epoch": 1.1903841196184584,
        "step": 9235
    },
    {
        "loss": 1.1058,
        "grad_norm": 2.152592420578003,
        "learning_rate": 2.2306041046434246e-05,
        "epoch": 1.1905130188192834,
        "step": 9236
    },
    {
        "loss": 1.2347,
        "grad_norm": 3.700955867767334,
        "learning_rate": 2.226775185815947e-05,
        "epoch": 1.1906419180201082,
        "step": 9237
    },
    {
        "loss": 1.6239,
        "grad_norm": 1.8482846021652222,
        "learning_rate": 2.2229491442846016e-05,
        "epoch": 1.1907708172209333,
        "step": 9238
    },
    {
        "loss": 1.3305,
        "grad_norm": 2.2316460609436035,
        "learning_rate": 2.219125981465614e-05,
        "epoch": 1.1908997164217583,
        "step": 9239
    },
    {
        "loss": 1.289,
        "grad_norm": 4.122227668762207,
        "learning_rate": 2.2153056987741616e-05,
        "epoch": 1.191028615622583,
        "step": 9240
    },
    {
        "loss": 1.5222,
        "grad_norm": 2.516321897506714,
        "learning_rate": 2.2114882976243122e-05,
        "epoch": 1.1911575148234081,
        "step": 9241
    },
    {
        "loss": 2.3471,
        "grad_norm": 1.505396842956543,
        "learning_rate": 2.2076737794291102e-05,
        "epoch": 1.191286414024233,
        "step": 9242
    },
    {
        "loss": 1.9161,
        "grad_norm": 1.542052149772644,
        "learning_rate": 2.2038621456005225e-05,
        "epoch": 1.191415313225058,
        "step": 9243
    },
    {
        "loss": 2.0391,
        "grad_norm": 1.5251960754394531,
        "learning_rate": 2.2000533975494375e-05,
        "epoch": 1.191544212425883,
        "step": 9244
    },
    {
        "loss": 0.7844,
        "grad_norm": 2.501077651977539,
        "learning_rate": 2.1962475366856773e-05,
        "epoch": 1.1916731116267079,
        "step": 9245
    },
    {
        "loss": 1.4019,
        "grad_norm": 2.8281099796295166,
        "learning_rate": 2.1924445644180147e-05,
        "epoch": 1.191802010827533,
        "step": 9246
    },
    {
        "loss": 0.6371,
        "grad_norm": 2.7286977767944336,
        "learning_rate": 2.1886444821541208e-05,
        "epoch": 1.1919309100283577,
        "step": 9247
    },
    {
        "loss": 1.7625,
        "grad_norm": 1.9299601316452026,
        "learning_rate": 2.1848472913006158e-05,
        "epoch": 1.1920598092291828,
        "step": 9248
    },
    {
        "loss": 1.4651,
        "grad_norm": 3.271362781524658,
        "learning_rate": 2.181052993263052e-05,
        "epoch": 1.1921887084300078,
        "step": 9249
    },
    {
        "loss": 1.8284,
        "grad_norm": 1.3345736265182495,
        "learning_rate": 2.1772615894459135e-05,
        "epoch": 1.1923176076308326,
        "step": 9250
    },
    {
        "loss": 2.3414,
        "grad_norm": 1.9322165250778198,
        "learning_rate": 2.1734730812526007e-05,
        "epoch": 1.1924465068316576,
        "step": 9251
    },
    {
        "loss": 1.5401,
        "grad_norm": 2.152777671813965,
        "learning_rate": 2.1696874700854476e-05,
        "epoch": 1.1925754060324827,
        "step": 9252
    },
    {
        "loss": 1.5126,
        "grad_norm": 4.244509220123291,
        "learning_rate": 2.1659047573457137e-05,
        "epoch": 1.1927043052333075,
        "step": 9253
    },
    {
        "loss": 1.008,
        "grad_norm": 3.313015937805176,
        "learning_rate": 2.1621249444335923e-05,
        "epoch": 1.1928332044341325,
        "step": 9254
    },
    {
        "loss": 1.5477,
        "grad_norm": 2.8874118328094482,
        "learning_rate": 2.1583480327481965e-05,
        "epoch": 1.1929621036349576,
        "step": 9255
    },
    {
        "loss": 1.43,
        "grad_norm": 2.7640905380249023,
        "learning_rate": 2.1545740236875656e-05,
        "epoch": 1.1930910028357824,
        "step": 9256
    },
    {
        "loss": 1.2194,
        "grad_norm": 2.7073476314544678,
        "learning_rate": 2.1508029186486833e-05,
        "epoch": 1.1932199020366074,
        "step": 9257
    },
    {
        "loss": 2.0814,
        "grad_norm": 1.8422932624816895,
        "learning_rate": 2.1470347190274166e-05,
        "epoch": 1.1933488012374323,
        "step": 9258
    },
    {
        "loss": 1.1889,
        "grad_norm": 2.1962368488311768,
        "learning_rate": 2.143269426218596e-05,
        "epoch": 1.1934777004382573,
        "step": 9259
    },
    {
        "loss": 2.2491,
        "grad_norm": 2.344390392303467,
        "learning_rate": 2.1395070416159656e-05,
        "epoch": 1.1936065996390823,
        "step": 9260
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.4003686904907227,
        "learning_rate": 2.1357475666121873e-05,
        "epoch": 1.1937354988399071,
        "step": 9261
    },
    {
        "loss": 0.8416,
        "grad_norm": 2.483386278152466,
        "learning_rate": 2.131991002598842e-05,
        "epoch": 1.1938643980407322,
        "step": 9262
    },
    {
        "loss": 0.8945,
        "grad_norm": 3.325791835784912,
        "learning_rate": 2.1282373509664553e-05,
        "epoch": 1.193993297241557,
        "step": 9263
    },
    {
        "loss": 2.0,
        "grad_norm": 2.402855157852173,
        "learning_rate": 2.1244866131044417e-05,
        "epoch": 1.194122196442382,
        "step": 9264
    },
    {
        "loss": 1.0924,
        "grad_norm": 1.3217145204544067,
        "learning_rate": 2.120738790401162e-05,
        "epoch": 1.194251095643207,
        "step": 9265
    },
    {
        "loss": 1.7553,
        "grad_norm": 5.181466102600098,
        "learning_rate": 2.1169938842438874e-05,
        "epoch": 1.194379994844032,
        "step": 9266
    },
    {
        "loss": 2.0821,
        "grad_norm": 2.445561647415161,
        "learning_rate": 2.1132518960188218e-05,
        "epoch": 1.194508894044857,
        "step": 9267
    },
    {
        "loss": 1.9446,
        "grad_norm": 1.8805934190750122,
        "learning_rate": 2.109512827111074e-05,
        "epoch": 1.1946377932456818,
        "step": 9268
    },
    {
        "loss": 1.3654,
        "grad_norm": 2.6600983142852783,
        "learning_rate": 2.1057766789046797e-05,
        "epoch": 1.1947666924465068,
        "step": 9269
    },
    {
        "loss": 1.3658,
        "grad_norm": 3.9505012035369873,
        "learning_rate": 2.102043452782586e-05,
        "epoch": 1.1948955916473318,
        "step": 9270
    },
    {
        "loss": 1.6071,
        "grad_norm": 2.4342844486236572,
        "learning_rate": 2.0983131501266716e-05,
        "epoch": 1.1950244908481567,
        "step": 9271
    },
    {
        "loss": 0.6016,
        "grad_norm": 2.062739849090576,
        "learning_rate": 2.0945857723177185e-05,
        "epoch": 1.1951533900489817,
        "step": 9272
    },
    {
        "loss": 0.9797,
        "grad_norm": 3.182208299636841,
        "learning_rate": 2.0908613207354398e-05,
        "epoch": 1.1952822892498067,
        "step": 9273
    },
    {
        "loss": 0.6778,
        "grad_norm": 2.194485902786255,
        "learning_rate": 2.0871397967584682e-05,
        "epoch": 1.1954111884506315,
        "step": 9274
    },
    {
        "loss": 1.8193,
        "grad_norm": 3.835235118865967,
        "learning_rate": 2.0834212017643185e-05,
        "epoch": 1.1955400876514566,
        "step": 9275
    },
    {
        "loss": 1.6677,
        "grad_norm": 3.0380942821502686,
        "learning_rate": 2.0797055371294603e-05,
        "epoch": 1.1956689868522816,
        "step": 9276
    },
    {
        "loss": 1.9249,
        "grad_norm": 1.9772268533706665,
        "learning_rate": 2.0759928042292698e-05,
        "epoch": 1.1957978860531064,
        "step": 9277
    },
    {
        "loss": 1.8224,
        "grad_norm": 2.339242458343506,
        "learning_rate": 2.072283004438027e-05,
        "epoch": 1.1959267852539315,
        "step": 9278
    },
    {
        "loss": 1.6689,
        "grad_norm": 2.3694467544555664,
        "learning_rate": 2.068576139128926e-05,
        "epoch": 1.1960556844547563,
        "step": 9279
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.7605500221252441,
        "learning_rate": 2.0648722096740993e-05,
        "epoch": 1.1961845836555813,
        "step": 9280
    },
    {
        "loss": 1.8179,
        "grad_norm": 1.557922601699829,
        "learning_rate": 2.0611712174445518e-05,
        "epoch": 1.1963134828564064,
        "step": 9281
    },
    {
        "loss": 1.0311,
        "grad_norm": 2.186389923095703,
        "learning_rate": 2.057473163810234e-05,
        "epoch": 1.1964423820572312,
        "step": 9282
    },
    {
        "loss": 1.548,
        "grad_norm": 2.1493120193481445,
        "learning_rate": 2.053778050139994e-05,
        "epoch": 1.1965712812580562,
        "step": 9283
    },
    {
        "loss": 2.5931,
        "grad_norm": 1.7077109813690186,
        "learning_rate": 2.0500858778016043e-05,
        "epoch": 1.196700180458881,
        "step": 9284
    },
    {
        "loss": 1.3519,
        "grad_norm": 3.324707269668579,
        "learning_rate": 2.0463966481617346e-05,
        "epoch": 1.196829079659706,
        "step": 9285
    },
    {
        "loss": 1.8188,
        "grad_norm": 1.7414430379867554,
        "learning_rate": 2.042710362585969e-05,
        "epoch": 1.1969579788605311,
        "step": 9286
    },
    {
        "loss": 1.6046,
        "grad_norm": 1.6238473653793335,
        "learning_rate": 2.0390270224388065e-05,
        "epoch": 1.197086878061356,
        "step": 9287
    },
    {
        "loss": 1.8917,
        "grad_norm": 1.60904860496521,
        "learning_rate": 2.0353466290836533e-05,
        "epoch": 1.197215777262181,
        "step": 9288
    },
    {
        "loss": 0.9137,
        "grad_norm": 1.4347349405288696,
        "learning_rate": 2.0316691838828173e-05,
        "epoch": 1.197344676463006,
        "step": 9289
    },
    {
        "loss": 1.7442,
        "grad_norm": 1.9558926820755005,
        "learning_rate": 2.0279946881975297e-05,
        "epoch": 1.1974735756638308,
        "step": 9290
    },
    {
        "loss": 1.9438,
        "grad_norm": 2.464495897293091,
        "learning_rate": 2.0243231433879218e-05,
        "epoch": 1.1976024748646559,
        "step": 9291
    },
    {
        "loss": 1.216,
        "grad_norm": 2.129539728164673,
        "learning_rate": 2.0206545508130413e-05,
        "epoch": 1.197731374065481,
        "step": 9292
    },
    {
        "loss": 1.2725,
        "grad_norm": 2.3070528507232666,
        "learning_rate": 2.016988911830815e-05,
        "epoch": 1.1978602732663057,
        "step": 9293
    },
    {
        "loss": 1.4054,
        "grad_norm": 3.252143621444702,
        "learning_rate": 2.0133262277981135e-05,
        "epoch": 1.1979891724671308,
        "step": 9294
    },
    {
        "loss": 1.5332,
        "grad_norm": 1.3285704851150513,
        "learning_rate": 2.0096665000706904e-05,
        "epoch": 1.1981180716679556,
        "step": 9295
    },
    {
        "loss": 1.9358,
        "grad_norm": 2.6143102645874023,
        "learning_rate": 2.0060097300032077e-05,
        "epoch": 1.1982469708687806,
        "step": 9296
    },
    {
        "loss": 1.8064,
        "grad_norm": 1.8386722803115845,
        "learning_rate": 2.0023559189492492e-05,
        "epoch": 1.1983758700696057,
        "step": 9297
    },
    {
        "loss": 1.4873,
        "grad_norm": 1.5649261474609375,
        "learning_rate": 1.9987050682612784e-05,
        "epoch": 1.1985047692704305,
        "step": 9298
    },
    {
        "loss": 2.2428,
        "grad_norm": 2.5531935691833496,
        "learning_rate": 1.995057179290677e-05,
        "epoch": 1.1986336684712555,
        "step": 9299
    },
    {
        "loss": 1.5921,
        "grad_norm": 3.7269127368927,
        "learning_rate": 1.9914122533877257e-05,
        "epoch": 1.1987625676720803,
        "step": 9300
    },
    {
        "loss": 1.4067,
        "grad_norm": 2.756418466567993,
        "learning_rate": 1.9877702919016216e-05,
        "epoch": 1.1988914668729054,
        "step": 9301
    },
    {
        "loss": 1.9139,
        "grad_norm": 2.9104645252227783,
        "learning_rate": 1.984131296180448e-05,
        "epoch": 1.1990203660737304,
        "step": 9302
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.4842793941497803,
        "learning_rate": 1.980495267571194e-05,
        "epoch": 1.1991492652745552,
        "step": 9303
    },
    {
        "loss": 1.8318,
        "grad_norm": 1.6689176559448242,
        "learning_rate": 1.9768622074197585e-05,
        "epoch": 1.1992781644753803,
        "step": 9304
    },
    {
        "loss": 1.4239,
        "grad_norm": 2.2565267086029053,
        "learning_rate": 1.973232117070931e-05,
        "epoch": 1.199407063676205,
        "step": 9305
    },
    {
        "loss": 1.8134,
        "grad_norm": 1.8482354879379272,
        "learning_rate": 1.969604997868405e-05,
        "epoch": 1.1995359628770301,
        "step": 9306
    },
    {
        "loss": 1.7473,
        "grad_norm": 2.7115089893341064,
        "learning_rate": 1.9659808511547818e-05,
        "epoch": 1.1996648620778552,
        "step": 9307
    },
    {
        "loss": 1.7525,
        "grad_norm": 3.166391134262085,
        "learning_rate": 1.9623596782715535e-05,
        "epoch": 1.19979376127868,
        "step": 9308
    },
    {
        "loss": 1.4989,
        "grad_norm": 3.109764575958252,
        "learning_rate": 1.958741480559123e-05,
        "epoch": 1.199922660479505,
        "step": 9309
    },
    {
        "loss": 1.8437,
        "grad_norm": 1.822635293006897,
        "learning_rate": 1.9551262593567655e-05,
        "epoch": 1.20005155968033,
        "step": 9310
    },
    {
        "loss": 1.7127,
        "grad_norm": 2.109358072280884,
        "learning_rate": 1.9515140160026867e-05,
        "epoch": 1.2001804588811549,
        "step": 9311
    },
    {
        "loss": 1.8174,
        "grad_norm": 1.9971339702606201,
        "learning_rate": 1.947904751833972e-05,
        "epoch": 1.20030935808198,
        "step": 9312
    },
    {
        "loss": 1.1047,
        "grad_norm": 1.3026270866394043,
        "learning_rate": 1.944298468186603e-05,
        "epoch": 1.200438257282805,
        "step": 9313
    },
    {
        "loss": 1.2169,
        "grad_norm": 2.5832479000091553,
        "learning_rate": 1.940695166395464e-05,
        "epoch": 1.2005671564836298,
        "step": 9314
    },
    {
        "loss": 1.8563,
        "grad_norm": 2.318145513534546,
        "learning_rate": 1.937094847794353e-05,
        "epoch": 1.2006960556844548,
        "step": 9315
    },
    {
        "loss": 2.0096,
        "grad_norm": 2.2075395584106445,
        "learning_rate": 1.9334975137159163e-05,
        "epoch": 1.2008249548852796,
        "step": 9316
    },
    {
        "loss": 1.7036,
        "grad_norm": 3.0240588188171387,
        "learning_rate": 1.9299031654917355e-05,
        "epoch": 1.2009538540861047,
        "step": 9317
    },
    {
        "loss": 0.6721,
        "grad_norm": 2.116940975189209,
        "learning_rate": 1.926311804452282e-05,
        "epoch": 1.2010827532869297,
        "step": 9318
    },
    {
        "loss": 1.7271,
        "grad_norm": 2.036945104598999,
        "learning_rate": 1.9227234319269084e-05,
        "epoch": 1.2012116524877545,
        "step": 9319
    },
    {
        "loss": 0.9282,
        "grad_norm": 2.844409465789795,
        "learning_rate": 1.9191380492438714e-05,
        "epoch": 1.2013405516885796,
        "step": 9320
    },
    {
        "loss": 1.2725,
        "grad_norm": 2.836214065551758,
        "learning_rate": 1.915555657730317e-05,
        "epoch": 1.2014694508894044,
        "step": 9321
    },
    {
        "loss": 2.4499,
        "grad_norm": 1.913554310798645,
        "learning_rate": 1.9119762587122824e-05,
        "epoch": 1.2015983500902294,
        "step": 9322
    },
    {
        "loss": 2.1344,
        "grad_norm": 2.557345390319824,
        "learning_rate": 1.9083998535146963e-05,
        "epoch": 1.2017272492910545,
        "step": 9323
    },
    {
        "loss": 1.5279,
        "grad_norm": 2.2532799243927,
        "learning_rate": 1.904826443461385e-05,
        "epoch": 1.2018561484918793,
        "step": 9324
    },
    {
        "loss": 0.6296,
        "grad_norm": 2.496899366378784,
        "learning_rate": 1.901256029875069e-05,
        "epoch": 1.2019850476927043,
        "step": 9325
    },
    {
        "loss": 1.5174,
        "grad_norm": 3.7531039714813232,
        "learning_rate": 1.897688614077354e-05,
        "epoch": 1.2021139468935294,
        "step": 9326
    },
    {
        "loss": 1.6802,
        "grad_norm": 2.3095858097076416,
        "learning_rate": 1.8941241973887213e-05,
        "epoch": 1.2022428460943542,
        "step": 9327
    },
    {
        "loss": 1.9555,
        "grad_norm": 2.010969877243042,
        "learning_rate": 1.890562781128571e-05,
        "epoch": 1.2023717452951792,
        "step": 9328
    },
    {
        "loss": 1.8085,
        "grad_norm": 3.1194324493408203,
        "learning_rate": 1.8870043666151736e-05,
        "epoch": 1.2025006444960042,
        "step": 9329
    },
    {
        "loss": 0.8664,
        "grad_norm": 3.6330533027648926,
        "learning_rate": 1.8834489551656875e-05,
        "epoch": 1.202629543696829,
        "step": 9330
    },
    {
        "loss": 1.7371,
        "grad_norm": 1.5584299564361572,
        "learning_rate": 1.8798965480961718e-05,
        "epoch": 1.202758442897654,
        "step": 9331
    },
    {
        "loss": 1.2562,
        "grad_norm": 2.7791380882263184,
        "learning_rate": 1.8763471467215786e-05,
        "epoch": 1.202887342098479,
        "step": 9332
    },
    {
        "loss": 1.8635,
        "grad_norm": 1.958785057067871,
        "learning_rate": 1.872800752355709e-05,
        "epoch": 1.203016241299304,
        "step": 9333
    },
    {
        "loss": 1.8898,
        "grad_norm": 1.806545615196228,
        "learning_rate": 1.8692573663112957e-05,
        "epoch": 1.203145140500129,
        "step": 9334
    },
    {
        "loss": 1.308,
        "grad_norm": 2.9826152324676514,
        "learning_rate": 1.865716989899937e-05,
        "epoch": 1.2032740397009538,
        "step": 9335
    },
    {
        "loss": 1.5741,
        "grad_norm": 2.707066059112549,
        "learning_rate": 1.8621796244321176e-05,
        "epoch": 1.2034029389017789,
        "step": 9336
    },
    {
        "loss": 2.3284,
        "grad_norm": 2.191509962081909,
        "learning_rate": 1.858645271217213e-05,
        "epoch": 1.2035318381026037,
        "step": 9337
    },
    {
        "loss": 1.6516,
        "grad_norm": 2.1126255989074707,
        "learning_rate": 1.8551139315634803e-05,
        "epoch": 1.2036607373034287,
        "step": 9338
    },
    {
        "loss": 2.2692,
        "grad_norm": 2.0788304805755615,
        "learning_rate": 1.8515856067780603e-05,
        "epoch": 1.2037896365042537,
        "step": 9339
    },
    {
        "loss": 1.6408,
        "grad_norm": 3.3449106216430664,
        "learning_rate": 1.8480602981669738e-05,
        "epoch": 1.2039185357050786,
        "step": 9340
    },
    {
        "loss": 1.9736,
        "grad_norm": 1.5089935064315796,
        "learning_rate": 1.844538007035137e-05,
        "epoch": 1.2040474349059036,
        "step": 9341
    },
    {
        "loss": 0.7313,
        "grad_norm": 1.3200290203094482,
        "learning_rate": 1.841018734686347e-05,
        "epoch": 1.2041763341067284,
        "step": 9342
    },
    {
        "loss": 1.2351,
        "grad_norm": 2.5835037231445312,
        "learning_rate": 1.83750248242328e-05,
        "epoch": 1.2043052333075535,
        "step": 9343
    },
    {
        "loss": 1.7995,
        "grad_norm": 2.7521800994873047,
        "learning_rate": 1.8339892515474777e-05,
        "epoch": 1.2044341325083785,
        "step": 9344
    },
    {
        "loss": 1.1633,
        "grad_norm": 2.3075263500213623,
        "learning_rate": 1.8304790433593934e-05,
        "epoch": 1.2045630317092033,
        "step": 9345
    },
    {
        "loss": 1.8111,
        "grad_norm": 2.5330286026000977,
        "learning_rate": 1.826971859158344e-05,
        "epoch": 1.2046919309100284,
        "step": 9346
    },
    {
        "loss": 1.6779,
        "grad_norm": 3.2423629760742188,
        "learning_rate": 1.823467700242524e-05,
        "epoch": 1.2048208301108534,
        "step": 9347
    },
    {
        "loss": 0.8536,
        "grad_norm": 1.273943305015564,
        "learning_rate": 1.8199665679090206e-05,
        "epoch": 1.2049497293116782,
        "step": 9348
    },
    {
        "loss": 1.4968,
        "grad_norm": 2.005542516708374,
        "learning_rate": 1.8164684634538065e-05,
        "epoch": 1.2050786285125032,
        "step": 9349
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.8427654504776,
        "learning_rate": 1.8129733881716958e-05,
        "epoch": 1.2052075277133283,
        "step": 9350
    },
    {
        "loss": 0.7843,
        "grad_norm": 3.5293967723846436,
        "learning_rate": 1.809481343356425e-05,
        "epoch": 1.205336426914153,
        "step": 9351
    },
    {
        "loss": 1.058,
        "grad_norm": 1.3467625379562378,
        "learning_rate": 1.8059923303005853e-05,
        "epoch": 1.2054653261149781,
        "step": 9352
    },
    {
        "loss": 1.0754,
        "grad_norm": 2.184377670288086,
        "learning_rate": 1.8025063502956573e-05,
        "epoch": 1.205594225315803,
        "step": 9353
    },
    {
        "loss": 1.3676,
        "grad_norm": 2.9816813468933105,
        "learning_rate": 1.7990234046319853e-05,
        "epoch": 1.205723124516628,
        "step": 9354
    },
    {
        "loss": 1.5299,
        "grad_norm": 2.3651506900787354,
        "learning_rate": 1.795543494598806e-05,
        "epoch": 1.205852023717453,
        "step": 9355
    },
    {
        "loss": 1.2078,
        "grad_norm": 3.0870513916015625,
        "learning_rate": 1.7920666214842185e-05,
        "epoch": 1.2059809229182779,
        "step": 9356
    },
    {
        "loss": 2.1038,
        "grad_norm": 1.9763225317001343,
        "learning_rate": 1.7885927865752018e-05,
        "epoch": 1.206109822119103,
        "step": 9357
    },
    {
        "loss": 1.7143,
        "grad_norm": 1.4625449180603027,
        "learning_rate": 1.7851219911576144e-05,
        "epoch": 1.2062387213199277,
        "step": 9358
    },
    {
        "loss": 1.0449,
        "grad_norm": 2.7450413703918457,
        "learning_rate": 1.7816542365161958e-05,
        "epoch": 1.2063676205207527,
        "step": 9359
    },
    {
        "loss": 1.3089,
        "grad_norm": 2.1329991817474365,
        "learning_rate": 1.7781895239345513e-05,
        "epoch": 1.2064965197215778,
        "step": 9360
    },
    {
        "loss": 1.1187,
        "grad_norm": 2.5553932189941406,
        "learning_rate": 1.774727854695144e-05,
        "epoch": 1.2066254189224026,
        "step": 9361
    },
    {
        "loss": 1.882,
        "grad_norm": 2.167625904083252,
        "learning_rate": 1.7712692300793443e-05,
        "epoch": 1.2067543181232276,
        "step": 9362
    },
    {
        "loss": 1.1734,
        "grad_norm": 2.9618306159973145,
        "learning_rate": 1.7678136513673703e-05,
        "epoch": 1.2068832173240527,
        "step": 9363
    },
    {
        "loss": 1.4157,
        "grad_norm": 2.3131189346313477,
        "learning_rate": 1.7643611198383226e-05,
        "epoch": 1.2070121165248775,
        "step": 9364
    },
    {
        "loss": 2.4269,
        "grad_norm": 1.7239638566970825,
        "learning_rate": 1.7609116367701716e-05,
        "epoch": 1.2071410157257025,
        "step": 9365
    },
    {
        "loss": 1.4562,
        "grad_norm": 2.48224139213562,
        "learning_rate": 1.757465203439772e-05,
        "epoch": 1.2072699149265276,
        "step": 9366
    },
    {
        "loss": 1.3854,
        "grad_norm": 2.283423900604248,
        "learning_rate": 1.7540218211228155e-05,
        "epoch": 1.2073988141273524,
        "step": 9367
    },
    {
        "loss": 1.7433,
        "grad_norm": 2.6303274631500244,
        "learning_rate": 1.750581491093899e-05,
        "epoch": 1.2075277133281774,
        "step": 9368
    },
    {
        "loss": 1.4509,
        "grad_norm": 2.5155906677246094,
        "learning_rate": 1.747144214626476e-05,
        "epoch": 1.2076566125290022,
        "step": 9369
    },
    {
        "loss": 1.7017,
        "grad_norm": 3.3917996883392334,
        "learning_rate": 1.7437099929928735e-05,
        "epoch": 1.2077855117298273,
        "step": 9370
    },
    {
        "loss": 1.2476,
        "grad_norm": 2.0212879180908203,
        "learning_rate": 1.7402788274642755e-05,
        "epoch": 1.2079144109306523,
        "step": 9371
    },
    {
        "loss": 1.0768,
        "grad_norm": 2.180968761444092,
        "learning_rate": 1.736850719310762e-05,
        "epoch": 1.2080433101314771,
        "step": 9372
    },
    {
        "loss": 1.6241,
        "grad_norm": 2.4442548751831055,
        "learning_rate": 1.7334256698012463e-05,
        "epoch": 1.2081722093323022,
        "step": 9373
    },
    {
        "loss": 2.2765,
        "grad_norm": 2.3837852478027344,
        "learning_rate": 1.730003680203527e-05,
        "epoch": 1.208301108533127,
        "step": 9374
    },
    {
        "loss": 1.4054,
        "grad_norm": 2.950507640838623,
        "learning_rate": 1.726584751784276e-05,
        "epoch": 1.208430007733952,
        "step": 9375
    },
    {
        "loss": 1.6549,
        "grad_norm": 1.6510241031646729,
        "learning_rate": 1.72316888580903e-05,
        "epoch": 1.208558906934777,
        "step": 9376
    },
    {
        "loss": 2.0302,
        "grad_norm": 2.5213911533355713,
        "learning_rate": 1.7197560835421833e-05,
        "epoch": 1.208687806135602,
        "step": 9377
    },
    {
        "loss": 1.8761,
        "grad_norm": 1.5162147283554077,
        "learning_rate": 1.716346346247001e-05,
        "epoch": 1.208816705336427,
        "step": 9378
    },
    {
        "loss": 0.8135,
        "grad_norm": 2.2067980766296387,
        "learning_rate": 1.712939675185613e-05,
        "epoch": 1.2089456045372518,
        "step": 9379
    },
    {
        "loss": 1.1427,
        "grad_norm": 1.3396615982055664,
        "learning_rate": 1.7095360716190134e-05,
        "epoch": 1.2090745037380768,
        "step": 9380
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.9458322525024414,
        "learning_rate": 1.706135536807064e-05,
        "epoch": 1.2092034029389018,
        "step": 9381
    },
    {
        "loss": 1.354,
        "grad_norm": 2.5046470165252686,
        "learning_rate": 1.702738072008486e-05,
        "epoch": 1.2093323021397266,
        "step": 9382
    },
    {
        "loss": 1.4285,
        "grad_norm": 2.2248008251190186,
        "learning_rate": 1.6993436784808837e-05,
        "epoch": 1.2094612013405517,
        "step": 9383
    },
    {
        "loss": 1.1915,
        "grad_norm": 1.7313176393508911,
        "learning_rate": 1.6959523574806813e-05,
        "epoch": 1.2095901005413767,
        "step": 9384
    },
    {
        "loss": 2.0164,
        "grad_norm": 1.6206879615783691,
        "learning_rate": 1.692564110263205e-05,
        "epoch": 1.2097189997422015,
        "step": 9385
    },
    {
        "loss": 1.534,
        "grad_norm": 1.8587735891342163,
        "learning_rate": 1.6891789380826372e-05,
        "epoch": 1.2098478989430266,
        "step": 9386
    },
    {
        "loss": 2.3618,
        "grad_norm": 1.9073224067687988,
        "learning_rate": 1.685796842192009e-05,
        "epoch": 1.2099767981438516,
        "step": 9387
    },
    {
        "loss": 1.5696,
        "grad_norm": 1.5009446144104004,
        "learning_rate": 1.6824178238432152e-05,
        "epoch": 1.2101056973446764,
        "step": 9388
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.9202783107757568,
        "learning_rate": 1.6790418842870292e-05,
        "epoch": 1.2102345965455015,
        "step": 9389
    },
    {
        "loss": 1.7741,
        "grad_norm": 2.3465123176574707,
        "learning_rate": 1.675669024773058e-05,
        "epoch": 1.2103634957463263,
        "step": 9390
    },
    {
        "loss": 1.4579,
        "grad_norm": 3.2580602169036865,
        "learning_rate": 1.67229924654978e-05,
        "epoch": 1.2104923949471513,
        "step": 9391
    },
    {
        "loss": 1.5987,
        "grad_norm": 2.420175552368164,
        "learning_rate": 1.6689325508645404e-05,
        "epoch": 1.2106212941479764,
        "step": 9392
    },
    {
        "loss": 1.9572,
        "grad_norm": 1.7927956581115723,
        "learning_rate": 1.665568938963542e-05,
        "epoch": 1.2107501933488012,
        "step": 9393
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.699869155883789,
        "learning_rate": 1.662208412091838e-05,
        "epoch": 1.2108790925496262,
        "step": 9394
    },
    {
        "loss": 1.8342,
        "grad_norm": 2.193075656890869,
        "learning_rate": 1.658850971493342e-05,
        "epoch": 1.211007991750451,
        "step": 9395
    },
    {
        "loss": 1.3554,
        "grad_norm": 1.2002792358398438,
        "learning_rate": 1.6554966184108246e-05,
        "epoch": 1.211136890951276,
        "step": 9396
    },
    {
        "loss": 1.7464,
        "grad_norm": 2.9235265254974365,
        "learning_rate": 1.65214535408592e-05,
        "epoch": 1.2112657901521011,
        "step": 9397
    },
    {
        "loss": 1.8063,
        "grad_norm": 3.2173707485198975,
        "learning_rate": 1.648797179759112e-05,
        "epoch": 1.211394689352926,
        "step": 9398
    },
    {
        "loss": 1.3691,
        "grad_norm": 2.9194250106811523,
        "learning_rate": 1.6454520966697396e-05,
        "epoch": 1.211523588553751,
        "step": 9399
    },
    {
        "loss": 1.7917,
        "grad_norm": 1.795566439628601,
        "learning_rate": 1.6421101060560183e-05,
        "epoch": 1.211652487754576,
        "step": 9400
    },
    {
        "loss": 1.5677,
        "grad_norm": 2.5394792556762695,
        "learning_rate": 1.638771209154978e-05,
        "epoch": 1.2117813869554008,
        "step": 9401
    },
    {
        "loss": 1.4165,
        "grad_norm": 2.5766713619232178,
        "learning_rate": 1.6354354072025368e-05,
        "epoch": 1.2119102861562259,
        "step": 9402
    },
    {
        "loss": 0.6931,
        "grad_norm": 2.1461021900177,
        "learning_rate": 1.6321027014334656e-05,
        "epoch": 1.212039185357051,
        "step": 9403
    },
    {
        "loss": 1.8527,
        "grad_norm": 1.9329410791397095,
        "learning_rate": 1.6287730930813737e-05,
        "epoch": 1.2121680845578757,
        "step": 9404
    },
    {
        "loss": 1.928,
        "grad_norm": 2.8974153995513916,
        "learning_rate": 1.6254465833787293e-05,
        "epoch": 1.2122969837587008,
        "step": 9405
    },
    {
        "loss": 0.9826,
        "grad_norm": 2.0424249172210693,
        "learning_rate": 1.6221231735568674e-05,
        "epoch": 1.2124258829595256,
        "step": 9406
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.6165741682052612,
        "learning_rate": 1.618802864845952e-05,
        "epoch": 1.2125547821603506,
        "step": 9407
    },
    {
        "loss": 1.9436,
        "grad_norm": 1.7760593891143799,
        "learning_rate": 1.615485658475009e-05,
        "epoch": 1.2126836813611757,
        "step": 9408
    },
    {
        "loss": 1.7391,
        "grad_norm": 2.2628214359283447,
        "learning_rate": 1.612171555671924e-05,
        "epoch": 1.2128125805620005,
        "step": 9409
    },
    {
        "loss": 1.8975,
        "grad_norm": 1.7411401271820068,
        "learning_rate": 1.6088605576634342e-05,
        "epoch": 1.2129414797628255,
        "step": 9410
    },
    {
        "loss": 2.2053,
        "grad_norm": 1.9109764099121094,
        "learning_rate": 1.605552665675114e-05,
        "epoch": 1.2130703789636503,
        "step": 9411
    },
    {
        "loss": 2.07,
        "grad_norm": 1.3969649076461792,
        "learning_rate": 1.6022478809313958e-05,
        "epoch": 1.2131992781644754,
        "step": 9412
    },
    {
        "loss": 1.1547,
        "grad_norm": 1.8365000486373901,
        "learning_rate": 1.5989462046555603e-05,
        "epoch": 1.2133281773653004,
        "step": 9413
    },
    {
        "loss": 0.6618,
        "grad_norm": 1.9502036571502686,
        "learning_rate": 1.5956476380697427e-05,
        "epoch": 1.2134570765661252,
        "step": 9414
    },
    {
        "loss": 1.9436,
        "grad_norm": 1.7588955163955688,
        "learning_rate": 1.5923521823949184e-05,
        "epoch": 1.2135859757669503,
        "step": 9415
    },
    {
        "loss": 1.5883,
        "grad_norm": 2.8144264221191406,
        "learning_rate": 1.589059838850915e-05,
        "epoch": 1.213714874967775,
        "step": 9416
    },
    {
        "loss": 1.5074,
        "grad_norm": 2.4349987506866455,
        "learning_rate": 1.5857706086564174e-05,
        "epoch": 1.2138437741686001,
        "step": 9417
    },
    {
        "loss": 1.1945,
        "grad_norm": 2.594564437866211,
        "learning_rate": 1.5824844930289484e-05,
        "epoch": 1.2139726733694252,
        "step": 9418
    },
    {
        "loss": 1.5381,
        "grad_norm": 2.6161108016967773,
        "learning_rate": 1.5792014931848676e-05,
        "epoch": 1.21410157257025,
        "step": 9419
    },
    {
        "loss": 1.7544,
        "grad_norm": 1.253472089767456,
        "learning_rate": 1.575921610339406e-05,
        "epoch": 1.214230471771075,
        "step": 9420
    },
    {
        "loss": 0.9848,
        "grad_norm": 2.6818158626556396,
        "learning_rate": 1.5726448457066223e-05,
        "epoch": 1.2143593709719,
        "step": 9421
    },
    {
        "loss": 1.0308,
        "grad_norm": 1.3715304136276245,
        "learning_rate": 1.5693712004994233e-05,
        "epoch": 1.2144882701727249,
        "step": 9422
    },
    {
        "loss": 2.1784,
        "grad_norm": 1.8063493967056274,
        "learning_rate": 1.566100675929578e-05,
        "epoch": 1.21461716937355,
        "step": 9423
    },
    {
        "loss": 1.4504,
        "grad_norm": 2.6336662769317627,
        "learning_rate": 1.562833273207669e-05,
        "epoch": 1.214746068574375,
        "step": 9424
    },
    {
        "loss": 0.5977,
        "grad_norm": 1.5912508964538574,
        "learning_rate": 1.559568993543149e-05,
        "epoch": 1.2148749677751998,
        "step": 9425
    },
    {
        "loss": 1.7194,
        "grad_norm": 2.3048832416534424,
        "learning_rate": 1.5563078381443043e-05,
        "epoch": 1.2150038669760248,
        "step": 9426
    },
    {
        "loss": 1.1975,
        "grad_norm": 2.3851478099823,
        "learning_rate": 1.5530498082182743e-05,
        "epoch": 1.2151327661768496,
        "step": 9427
    },
    {
        "loss": 1.1159,
        "grad_norm": 2.3529887199401855,
        "learning_rate": 1.549794904971028e-05,
        "epoch": 1.2152616653776747,
        "step": 9428
    },
    {
        "loss": 1.2353,
        "grad_norm": 2.566295623779297,
        "learning_rate": 1.546543129607384e-05,
        "epoch": 1.2153905645784997,
        "step": 9429
    },
    {
        "loss": 1.4229,
        "grad_norm": 1.5111886262893677,
        "learning_rate": 1.5432944833310037e-05,
        "epoch": 1.2155194637793245,
        "step": 9430
    },
    {
        "loss": 1.6277,
        "grad_norm": 2.182302236557007,
        "learning_rate": 1.5400489673443885e-05,
        "epoch": 1.2156483629801496,
        "step": 9431
    },
    {
        "loss": 1.5585,
        "grad_norm": 1.8648864030838013,
        "learning_rate": 1.5368065828488777e-05,
        "epoch": 1.2157772621809744,
        "step": 9432
    },
    {
        "loss": 1.3467,
        "grad_norm": 3.203070640563965,
        "learning_rate": 1.5335673310446597e-05,
        "epoch": 1.2159061613817994,
        "step": 9433
    },
    {
        "loss": 2.2158,
        "grad_norm": 1.6042332649230957,
        "learning_rate": 1.5303312131307612e-05,
        "epoch": 1.2160350605826244,
        "step": 9434
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.8558626174926758,
        "learning_rate": 1.5270982303050463e-05,
        "epoch": 1.2161639597834493,
        "step": 9435
    },
    {
        "loss": 1.3715,
        "grad_norm": 3.866283893585205,
        "learning_rate": 1.5238683837642087e-05,
        "epoch": 1.2162928589842743,
        "step": 9436
    },
    {
        "loss": 1.9004,
        "grad_norm": 1.565291404724121,
        "learning_rate": 1.5206416747038044e-05,
        "epoch": 1.2164217581850993,
        "step": 9437
    },
    {
        "loss": 1.5026,
        "grad_norm": 2.6680872440338135,
        "learning_rate": 1.517418104318209e-05,
        "epoch": 1.2165506573859242,
        "step": 9438
    },
    {
        "loss": 1.8524,
        "grad_norm": 1.9724125862121582,
        "learning_rate": 1.5141976738006391e-05,
        "epoch": 1.2166795565867492,
        "step": 9439
    },
    {
        "loss": 0.9048,
        "grad_norm": 1.4406533241271973,
        "learning_rate": 1.5109803843431647e-05,
        "epoch": 1.2168084557875742,
        "step": 9440
    },
    {
        "loss": 1.4819,
        "grad_norm": 1.778135895729065,
        "learning_rate": 1.5077662371366663e-05,
        "epoch": 1.216937354988399,
        "step": 9441
    },
    {
        "loss": 1.7284,
        "grad_norm": 2.3412725925445557,
        "learning_rate": 1.5045552333708824e-05,
        "epoch": 1.217066254189224,
        "step": 9442
    },
    {
        "loss": 2.5167,
        "grad_norm": 1.7509642839431763,
        "learning_rate": 1.5013473742343775e-05,
        "epoch": 1.217195153390049,
        "step": 9443
    },
    {
        "loss": 1.6144,
        "grad_norm": 2.5879313945770264,
        "learning_rate": 1.4981426609145655e-05,
        "epoch": 1.217324052590874,
        "step": 9444
    },
    {
        "loss": 1.4892,
        "grad_norm": 3.692291021347046,
        "learning_rate": 1.4949410945976794e-05,
        "epoch": 1.217452951791699,
        "step": 9445
    },
    {
        "loss": 1.5356,
        "grad_norm": 1.6310665607452393,
        "learning_rate": 1.4917426764687914e-05,
        "epoch": 1.2175818509925238,
        "step": 9446
    },
    {
        "loss": 1.3693,
        "grad_norm": 1.4431241750717163,
        "learning_rate": 1.4885474077118188e-05,
        "epoch": 1.2177107501933488,
        "step": 9447
    },
    {
        "loss": 1.0308,
        "grad_norm": 2.847937822341919,
        "learning_rate": 1.4853552895095003e-05,
        "epoch": 1.2178396493941737,
        "step": 9448
    },
    {
        "loss": 1.528,
        "grad_norm": 2.8237085342407227,
        "learning_rate": 1.4821663230434102e-05,
        "epoch": 1.2179685485949987,
        "step": 9449
    },
    {
        "loss": 1.6742,
        "grad_norm": 2.7543373107910156,
        "learning_rate": 1.4789805094939668e-05,
        "epoch": 1.2180974477958237,
        "step": 9450
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.4755971431732178,
        "learning_rate": 1.4757978500404112e-05,
        "epoch": 1.2182263469966486,
        "step": 9451
    },
    {
        "loss": 0.9648,
        "grad_norm": 3.113942861557007,
        "learning_rate": 1.4726183458608278e-05,
        "epoch": 1.2183552461974736,
        "step": 9452
    },
    {
        "loss": 1.2032,
        "grad_norm": 2.4406819343566895,
        "learning_rate": 1.4694419981321073e-05,
        "epoch": 1.2184841453982984,
        "step": 9453
    },
    {
        "loss": 1.9121,
        "grad_norm": 3.0947537422180176,
        "learning_rate": 1.4662688080300058e-05,
        "epoch": 1.2186130445991235,
        "step": 9454
    },
    {
        "loss": 1.6536,
        "grad_norm": 1.2569849491119385,
        "learning_rate": 1.4630987767290893e-05,
        "epoch": 1.2187419437999485,
        "step": 9455
    },
    {
        "loss": 1.2438,
        "grad_norm": 2.4012563228607178,
        "learning_rate": 1.4599319054027539e-05,
        "epoch": 1.2188708430007733,
        "step": 9456
    },
    {
        "loss": 1.5926,
        "grad_norm": 1.9909917116165161,
        "learning_rate": 1.4567681952232392e-05,
        "epoch": 1.2189997422015983,
        "step": 9457
    },
    {
        "loss": 1.798,
        "grad_norm": 1.3439520597457886,
        "learning_rate": 1.4536076473616167e-05,
        "epoch": 1.2191286414024234,
        "step": 9458
    },
    {
        "loss": 1.43,
        "grad_norm": 2.271272659301758,
        "learning_rate": 1.4504502629877593e-05,
        "epoch": 1.2192575406032482,
        "step": 9459
    },
    {
        "loss": 2.279,
        "grad_norm": 1.7788677215576172,
        "learning_rate": 1.4472960432703941e-05,
        "epoch": 1.2193864398040732,
        "step": 9460
    },
    {
        "loss": 1.5718,
        "grad_norm": 1.450474500656128,
        "learning_rate": 1.4441449893770776e-05,
        "epoch": 1.2195153390048983,
        "step": 9461
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.7010509967803955,
        "learning_rate": 1.4409971024741809e-05,
        "epoch": 1.219644238205723,
        "step": 9462
    },
    {
        "loss": 1.7751,
        "grad_norm": 1.4964237213134766,
        "learning_rate": 1.437852383726912e-05,
        "epoch": 1.2197731374065481,
        "step": 9463
    },
    {
        "loss": 2.089,
        "grad_norm": 1.574230432510376,
        "learning_rate": 1.4347108342993031e-05,
        "epoch": 1.219902036607373,
        "step": 9464
    },
    {
        "loss": 2.2414,
        "grad_norm": 2.28128981590271,
        "learning_rate": 1.4315724553542131e-05,
        "epoch": 1.220030935808198,
        "step": 9465
    },
    {
        "loss": 1.2323,
        "grad_norm": 1.6564760208129883,
        "learning_rate": 1.4284372480533236e-05,
        "epoch": 1.220159835009023,
        "step": 9466
    },
    {
        "loss": 2.2459,
        "grad_norm": 1.3086557388305664,
        "learning_rate": 1.4253052135571533e-05,
        "epoch": 1.2202887342098478,
        "step": 9467
    },
    {
        "loss": 0.7739,
        "grad_norm": 3.183570623397827,
        "learning_rate": 1.4221763530250366e-05,
        "epoch": 1.2204176334106729,
        "step": 9468
    },
    {
        "loss": 1.959,
        "grad_norm": 2.430882215499878,
        "learning_rate": 1.4190506676151427e-05,
        "epoch": 1.2205465326114977,
        "step": 9469
    },
    {
        "loss": 1.9374,
        "grad_norm": 1.6335046291351318,
        "learning_rate": 1.4159281584844452e-05,
        "epoch": 1.2206754318123227,
        "step": 9470
    },
    {
        "loss": 1.5532,
        "grad_norm": 2.5319790840148926,
        "learning_rate": 1.4128088267887673e-05,
        "epoch": 1.2208043310131478,
        "step": 9471
    },
    {
        "loss": 1.9595,
        "grad_norm": 1.4758950471878052,
        "learning_rate": 1.4096926736827398e-05,
        "epoch": 1.2209332302139726,
        "step": 9472
    },
    {
        "loss": 1.8946,
        "grad_norm": 2.766389846801758,
        "learning_rate": 1.4065797003198188e-05,
        "epoch": 1.2210621294147976,
        "step": 9473
    },
    {
        "loss": 1.2381,
        "grad_norm": 2.526845693588257,
        "learning_rate": 1.4034699078522895e-05,
        "epoch": 1.2211910286156227,
        "step": 9474
    },
    {
        "loss": 1.3094,
        "grad_norm": 3.962216854095459,
        "learning_rate": 1.4003632974312675e-05,
        "epoch": 1.2213199278164475,
        "step": 9475
    },
    {
        "loss": 0.9122,
        "grad_norm": 2.4980380535125732,
        "learning_rate": 1.3972598702066576e-05,
        "epoch": 1.2214488270172725,
        "step": 9476
    },
    {
        "loss": 0.9014,
        "grad_norm": 3.257249116897583,
        "learning_rate": 1.3941596273272217e-05,
        "epoch": 1.2215777262180976,
        "step": 9477
    },
    {
        "loss": 1.8178,
        "grad_norm": 3.125849723815918,
        "learning_rate": 1.3910625699405278e-05,
        "epoch": 1.2217066254189224,
        "step": 9478
    },
    {
        "loss": 0.9047,
        "grad_norm": 2.7864575386047363,
        "learning_rate": 1.3879686991929641e-05,
        "epoch": 1.2218355246197474,
        "step": 9479
    },
    {
        "loss": 1.4984,
        "grad_norm": 2.6034302711486816,
        "learning_rate": 1.3848780162297426e-05,
        "epoch": 1.2219644238205722,
        "step": 9480
    },
    {
        "loss": 1.6238,
        "grad_norm": 1.7734309434890747,
        "learning_rate": 1.381790522194899e-05,
        "epoch": 1.2220933230213973,
        "step": 9481
    },
    {
        "loss": 2.4469,
        "grad_norm": 2.0379858016967773,
        "learning_rate": 1.3787062182312783e-05,
        "epoch": 1.2222222222222223,
        "step": 9482
    },
    {
        "loss": 0.8864,
        "grad_norm": 3.1714982986450195,
        "learning_rate": 1.3756251054805458e-05,
        "epoch": 1.2223511214230471,
        "step": 9483
    },
    {
        "loss": 1.8785,
        "grad_norm": 1.9716548919677734,
        "learning_rate": 1.3725471850831961e-05,
        "epoch": 1.2224800206238722,
        "step": 9484
    },
    {
        "loss": 1.0005,
        "grad_norm": 2.7699100971221924,
        "learning_rate": 1.3694724581785402e-05,
        "epoch": 1.222608919824697,
        "step": 9485
    },
    {
        "loss": 1.7965,
        "grad_norm": 3.2989578247070312,
        "learning_rate": 1.3664009259047029e-05,
        "epoch": 1.222737819025522,
        "step": 9486
    },
    {
        "loss": 2.0453,
        "grad_norm": 2.616180896759033,
        "learning_rate": 1.3633325893986127e-05,
        "epoch": 1.222866718226347,
        "step": 9487
    },
    {
        "loss": 0.8814,
        "grad_norm": 2.4842896461486816,
        "learning_rate": 1.3602674497960422e-05,
        "epoch": 1.2229956174271719,
        "step": 9488
    },
    {
        "loss": 2.1436,
        "grad_norm": 3.4502155780792236,
        "learning_rate": 1.3572055082315638e-05,
        "epoch": 1.223124516627997,
        "step": 9489
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.653928518295288,
        "learning_rate": 1.3541467658385654e-05,
        "epoch": 1.2232534158288217,
        "step": 9490
    },
    {
        "loss": 1.0744,
        "grad_norm": 3.3491504192352295,
        "learning_rate": 1.351091223749259e-05,
        "epoch": 1.2233823150296468,
        "step": 9491
    },
    {
        "loss": 1.0916,
        "grad_norm": 2.617711305618286,
        "learning_rate": 1.3480388830946788e-05,
        "epoch": 1.2235112142304718,
        "step": 9492
    },
    {
        "loss": 0.8892,
        "grad_norm": 4.274776935577393,
        "learning_rate": 1.3449897450046411e-05,
        "epoch": 1.2236401134312966,
        "step": 9493
    },
    {
        "loss": 1.464,
        "grad_norm": 1.921118140220642,
        "learning_rate": 1.341943810607812e-05,
        "epoch": 1.2237690126321217,
        "step": 9494
    },
    {
        "loss": 1.9653,
        "grad_norm": 1.9762459993362427,
        "learning_rate": 1.3389010810316572e-05,
        "epoch": 1.2238979118329467,
        "step": 9495
    },
    {
        "loss": 0.9452,
        "grad_norm": 2.131514072418213,
        "learning_rate": 1.3358615574024601e-05,
        "epoch": 1.2240268110337715,
        "step": 9496
    },
    {
        "loss": 0.887,
        "grad_norm": 2.165628671646118,
        "learning_rate": 1.332825240845309e-05,
        "epoch": 1.2241557102345966,
        "step": 9497
    },
    {
        "loss": 1.9952,
        "grad_norm": 2.948643684387207,
        "learning_rate": 1.3297921324841178e-05,
        "epoch": 1.2242846094354216,
        "step": 9498
    },
    {
        "loss": 1.664,
        "grad_norm": 2.607574462890625,
        "learning_rate": 1.3267622334415997e-05,
        "epoch": 1.2244135086362464,
        "step": 9499
    },
    {
        "loss": 1.4525,
        "grad_norm": 2.6705517768859863,
        "learning_rate": 1.3237355448392852e-05,
        "epoch": 1.2245424078370715,
        "step": 9500
    },
    {
        "loss": 1.0403,
        "grad_norm": 2.1098852157592773,
        "learning_rate": 1.3207120677975194e-05,
        "epoch": 1.2246713070378963,
        "step": 9501
    },
    {
        "loss": 1.3382,
        "grad_norm": 2.2216684818267822,
        "learning_rate": 1.3176918034354635e-05,
        "epoch": 1.2248002062387213,
        "step": 9502
    },
    {
        "loss": 1.8692,
        "grad_norm": 1.8830256462097168,
        "learning_rate": 1.3146747528710813e-05,
        "epoch": 1.2249291054395464,
        "step": 9503
    },
    {
        "loss": 2.0528,
        "grad_norm": 2.3550429344177246,
        "learning_rate": 1.3116609172211346e-05,
        "epoch": 1.2250580046403712,
        "step": 9504
    },
    {
        "loss": 1.2509,
        "grad_norm": 3.888655185699463,
        "learning_rate": 1.308650297601225e-05,
        "epoch": 1.2251869038411962,
        "step": 9505
    },
    {
        "loss": 1.8789,
        "grad_norm": 1.736791729927063,
        "learning_rate": 1.3056428951257405e-05,
        "epoch": 1.225315803042021,
        "step": 9506
    },
    {
        "loss": 2.5221,
        "grad_norm": 2.240980625152588,
        "learning_rate": 1.302638710907882e-05,
        "epoch": 1.225444702242846,
        "step": 9507
    },
    {
        "loss": 1.6674,
        "grad_norm": 2.3393137454986572,
        "learning_rate": 1.2996377460596664e-05,
        "epoch": 1.2255736014436711,
        "step": 9508
    },
    {
        "loss": 2.0905,
        "grad_norm": 2.841080665588379,
        "learning_rate": 1.2966400016919278e-05,
        "epoch": 1.225702500644496,
        "step": 9509
    },
    {
        "loss": 1.318,
        "grad_norm": 2.572636604309082,
        "learning_rate": 1.2936454789142716e-05,
        "epoch": 1.225831399845321,
        "step": 9510
    },
    {
        "loss": 0.6477,
        "grad_norm": 2.5457396507263184,
        "learning_rate": 1.2906541788351489e-05,
        "epoch": 1.225960299046146,
        "step": 9511
    },
    {
        "loss": 1.6762,
        "grad_norm": 1.5985443592071533,
        "learning_rate": 1.2876661025618008e-05,
        "epoch": 1.2260891982469708,
        "step": 9512
    },
    {
        "loss": 1.9911,
        "grad_norm": 3.897989273071289,
        "learning_rate": 1.2846812512002816e-05,
        "epoch": 1.2262180974477959,
        "step": 9513
    },
    {
        "loss": 0.8272,
        "grad_norm": 1.4304887056350708,
        "learning_rate": 1.2816996258554415e-05,
        "epoch": 1.226346996648621,
        "step": 9514
    },
    {
        "loss": 0.6487,
        "grad_norm": 1.6294195652008057,
        "learning_rate": 1.2787212276309523e-05,
        "epoch": 1.2264758958494457,
        "step": 9515
    },
    {
        "loss": 1.5608,
        "grad_norm": 2.9947335720062256,
        "learning_rate": 1.2757460576292735e-05,
        "epoch": 1.2266047950502708,
        "step": 9516
    },
    {
        "loss": 0.7303,
        "grad_norm": 2.1069600582122803,
        "learning_rate": 1.2727741169516782e-05,
        "epoch": 1.2267336942510956,
        "step": 9517
    },
    {
        "loss": 2.0449,
        "grad_norm": 2.6895956993103027,
        "learning_rate": 1.2698054066982478e-05,
        "epoch": 1.2268625934519206,
        "step": 9518
    },
    {
        "loss": 0.7793,
        "grad_norm": 2.521435260772705,
        "learning_rate": 1.2668399279678678e-05,
        "epoch": 1.2269914926527457,
        "step": 9519
    },
    {
        "loss": 1.6339,
        "grad_norm": 2.297126054763794,
        "learning_rate": 1.2638776818582221e-05,
        "epoch": 1.2271203918535705,
        "step": 9520
    },
    {
        "loss": 0.8652,
        "grad_norm": 2.9321212768554688,
        "learning_rate": 1.2609186694657993e-05,
        "epoch": 1.2272492910543955,
        "step": 9521
    },
    {
        "loss": 1.6619,
        "grad_norm": 2.625178575515747,
        "learning_rate": 1.257962891885891e-05,
        "epoch": 1.2273781902552203,
        "step": 9522
    },
    {
        "loss": 1.7996,
        "grad_norm": 2.439070224761963,
        "learning_rate": 1.255010350212591e-05,
        "epoch": 1.2275070894560454,
        "step": 9523
    },
    {
        "loss": 1.6003,
        "grad_norm": 2.5508031845092773,
        "learning_rate": 1.2520610455387983e-05,
        "epoch": 1.2276359886568704,
        "step": 9524
    },
    {
        "loss": 1.6679,
        "grad_norm": 2.7820680141448975,
        "learning_rate": 1.2491149789562106e-05,
        "epoch": 1.2277648878576952,
        "step": 9525
    },
    {
        "loss": 1.8252,
        "grad_norm": 2.7610220909118652,
        "learning_rate": 1.2461721515553416e-05,
        "epoch": 1.2278937870585203,
        "step": 9526
    },
    {
        "loss": 2.1996,
        "grad_norm": 2.142382860183716,
        "learning_rate": 1.2432325644254706e-05,
        "epoch": 1.228022686259345,
        "step": 9527
    },
    {
        "loss": 1.4068,
        "grad_norm": 1.771170973777771,
        "learning_rate": 1.2402962186547107e-05,
        "epoch": 1.2281515854601701,
        "step": 9528
    },
    {
        "loss": 1.9058,
        "grad_norm": 1.7629361152648926,
        "learning_rate": 1.2373631153299681e-05,
        "epoch": 1.2282804846609952,
        "step": 9529
    },
    {
        "loss": 1.8176,
        "grad_norm": 2.24080753326416,
        "learning_rate": 1.2344332555369409e-05,
        "epoch": 1.22840938386182,
        "step": 9530
    },
    {
        "loss": 1.9066,
        "grad_norm": 2.509197235107422,
        "learning_rate": 1.2315066403601273e-05,
        "epoch": 1.228538283062645,
        "step": 9531
    },
    {
        "loss": 1.7089,
        "grad_norm": 1.6198604106903076,
        "learning_rate": 1.2285832708828393e-05,
        "epoch": 1.22866718226347,
        "step": 9532
    },
    {
        "loss": 1.5916,
        "grad_norm": 2.437624931335449,
        "learning_rate": 1.2256631481871638e-05,
        "epoch": 1.2287960814642949,
        "step": 9533
    },
    {
        "loss": 1.5426,
        "grad_norm": 2.2016985416412354,
        "learning_rate": 1.2227462733539986e-05,
        "epoch": 1.22892498066512,
        "step": 9534
    },
    {
        "loss": 1.7249,
        "grad_norm": 4.149383544921875,
        "learning_rate": 1.2198326474630429e-05,
        "epoch": 1.229053879865945,
        "step": 9535
    },
    {
        "loss": 1.0241,
        "grad_norm": 2.6113946437835693,
        "learning_rate": 1.216922271592792e-05,
        "epoch": 1.2291827790667698,
        "step": 9536
    },
    {
        "loss": 1.4003,
        "grad_norm": 2.352818250656128,
        "learning_rate": 1.214015146820533e-05,
        "epoch": 1.2293116782675948,
        "step": 9537
    },
    {
        "loss": 1.1327,
        "grad_norm": 3.12007999420166,
        "learning_rate": 1.2111112742223507e-05,
        "epoch": 1.2294405774684196,
        "step": 9538
    },
    {
        "loss": 0.4208,
        "grad_norm": 1.2607134580612183,
        "learning_rate": 1.2082106548731286e-05,
        "epoch": 1.2295694766692447,
        "step": 9539
    },
    {
        "loss": 1.7421,
        "grad_norm": 2.2171688079833984,
        "learning_rate": 1.2053132898465414e-05,
        "epoch": 1.2296983758700697,
        "step": 9540
    },
    {
        "loss": 1.3239,
        "grad_norm": 6.757174491882324,
        "learning_rate": 1.2024191802150653e-05,
        "epoch": 1.2298272750708945,
        "step": 9541
    },
    {
        "loss": 2.0678,
        "grad_norm": 1.8851279020309448,
        "learning_rate": 1.1995283270499657e-05,
        "epoch": 1.2299561742717195,
        "step": 9542
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.361527681350708,
        "learning_rate": 1.1966407314213135e-05,
        "epoch": 1.2300850734725444,
        "step": 9543
    },
    {
        "loss": 0.7526,
        "grad_norm": 2.917412042617798,
        "learning_rate": 1.1937563943979623e-05,
        "epoch": 1.2302139726733694,
        "step": 9544
    },
    {
        "loss": 1.9693,
        "grad_norm": 1.9387246370315552,
        "learning_rate": 1.1908753170475562e-05,
        "epoch": 1.2303428718741944,
        "step": 9545
    },
    {
        "loss": 0.6575,
        "grad_norm": 1.397904396057129,
        "learning_rate": 1.1879975004365496e-05,
        "epoch": 1.2304717710750193,
        "step": 9546
    },
    {
        "loss": 1.321,
        "grad_norm": 3.012385606765747,
        "learning_rate": 1.1851229456301748e-05,
        "epoch": 1.2306006702758443,
        "step": 9547
    },
    {
        "loss": 1.8432,
        "grad_norm": 1.4700381755828857,
        "learning_rate": 1.18225165369246e-05,
        "epoch": 1.2307295694766693,
        "step": 9548
    },
    {
        "loss": 1.8467,
        "grad_norm": 2.57726788520813,
        "learning_rate": 1.1793836256862378e-05,
        "epoch": 1.2308584686774942,
        "step": 9549
    },
    {
        "loss": 1.6617,
        "grad_norm": 2.61078143119812,
        "learning_rate": 1.1765188626731117e-05,
        "epoch": 1.2309873678783192,
        "step": 9550
    },
    {
        "loss": 1.3317,
        "grad_norm": 2.78942608833313,
        "learning_rate": 1.1736573657134854e-05,
        "epoch": 1.2311162670791442,
        "step": 9551
    },
    {
        "loss": 2.0373,
        "grad_norm": 2.0144052505493164,
        "learning_rate": 1.1707991358665616e-05,
        "epoch": 1.231245166279969,
        "step": 9552
    },
    {
        "loss": 1.7591,
        "grad_norm": 2.354527711868286,
        "learning_rate": 1.1679441741903308e-05,
        "epoch": 1.231374065480794,
        "step": 9553
    },
    {
        "loss": 1.6695,
        "grad_norm": 1.5898257493972778,
        "learning_rate": 1.1650924817415676e-05,
        "epoch": 1.231502964681619,
        "step": 9554
    },
    {
        "loss": 1.183,
        "grad_norm": 3.0279951095581055,
        "learning_rate": 1.1622440595758377e-05,
        "epoch": 1.231631863882444,
        "step": 9555
    },
    {
        "loss": 1.2881,
        "grad_norm": 2.000554084777832,
        "learning_rate": 1.1593989087474977e-05,
        "epoch": 1.231760763083269,
        "step": 9556
    },
    {
        "loss": 1.5273,
        "grad_norm": 2.8179097175598145,
        "learning_rate": 1.1565570303096962e-05,
        "epoch": 1.2318896622840938,
        "step": 9557
    },
    {
        "loss": 1.9569,
        "grad_norm": 1.3773828744888306,
        "learning_rate": 1.1537184253143662e-05,
        "epoch": 1.2320185614849188,
        "step": 9558
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.7356168031692505,
        "learning_rate": 1.1508830948122285e-05,
        "epoch": 1.2321474606857437,
        "step": 9559
    },
    {
        "loss": 1.713,
        "grad_norm": 2.0734763145446777,
        "learning_rate": 1.1480510398528021e-05,
        "epoch": 1.2322763598865687,
        "step": 9560
    },
    {
        "loss": 1.6813,
        "grad_norm": 2.486938238143921,
        "learning_rate": 1.1452222614843832e-05,
        "epoch": 1.2324052590873937,
        "step": 9561
    },
    {
        "loss": 1.9082,
        "grad_norm": 2.1572909355163574,
        "learning_rate": 1.1423967607540476e-05,
        "epoch": 1.2325341582882186,
        "step": 9562
    },
    {
        "loss": 1.5369,
        "grad_norm": 2.6038904190063477,
        "learning_rate": 1.1395745387076806e-05,
        "epoch": 1.2326630574890436,
        "step": 9563
    },
    {
        "loss": 1.4695,
        "grad_norm": 2.738420248031616,
        "learning_rate": 1.1367555963899357e-05,
        "epoch": 1.2327919566898684,
        "step": 9564
    },
    {
        "loss": 0.7684,
        "grad_norm": 3.0645506381988525,
        "learning_rate": 1.1339399348442537e-05,
        "epoch": 1.2329208558906934,
        "step": 9565
    },
    {
        "loss": 1.8727,
        "grad_norm": 1.7010767459869385,
        "learning_rate": 1.1311275551128775e-05,
        "epoch": 1.2330497550915185,
        "step": 9566
    },
    {
        "loss": 1.8773,
        "grad_norm": 1.8035907745361328,
        "learning_rate": 1.1283184582368111e-05,
        "epoch": 1.2331786542923433,
        "step": 9567
    },
    {
        "loss": 1.3662,
        "grad_norm": 2.6993415355682373,
        "learning_rate": 1.125512645255854e-05,
        "epoch": 1.2333075534931683,
        "step": 9568
    },
    {
        "loss": 0.7942,
        "grad_norm": 2.043694496154785,
        "learning_rate": 1.1227101172085964e-05,
        "epoch": 1.2334364526939934,
        "step": 9569
    },
    {
        "loss": 1.0824,
        "grad_norm": 2.136744976043701,
        "learning_rate": 1.1199108751324105e-05,
        "epoch": 1.2335653518948182,
        "step": 9570
    },
    {
        "loss": 1.634,
        "grad_norm": 1.9761669635772705,
        "learning_rate": 1.1171149200634456e-05,
        "epoch": 1.2336942510956432,
        "step": 9571
    },
    {
        "loss": 2.1474,
        "grad_norm": 2.095978021621704,
        "learning_rate": 1.1143222530366382e-05,
        "epoch": 1.2338231502964683,
        "step": 9572
    },
    {
        "loss": 1.1963,
        "grad_norm": 2.8430123329162598,
        "learning_rate": 1.1115328750857045e-05,
        "epoch": 1.233952049497293,
        "step": 9573
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.062556028366089,
        "learning_rate": 1.1087467872431501e-05,
        "epoch": 1.2340809486981181,
        "step": 9574
    },
    {
        "loss": 0.8928,
        "grad_norm": 3.4208552837371826,
        "learning_rate": 1.105963990540253e-05,
        "epoch": 1.234209847898943,
        "step": 9575
    },
    {
        "loss": 1.4099,
        "grad_norm": 2.022294759750366,
        "learning_rate": 1.103184486007084e-05,
        "epoch": 1.234338747099768,
        "step": 9576
    },
    {
        "loss": 0.8901,
        "grad_norm": 2.3985488414764404,
        "learning_rate": 1.1004082746724908e-05,
        "epoch": 1.234467646300593,
        "step": 9577
    },
    {
        "loss": 1.7959,
        "grad_norm": 1.5181840658187866,
        "learning_rate": 1.0976353575641008e-05,
        "epoch": 1.2345965455014178,
        "step": 9578
    },
    {
        "loss": 2.2858,
        "grad_norm": 1.8796809911727905,
        "learning_rate": 1.0948657357083136e-05,
        "epoch": 1.2347254447022429,
        "step": 9579
    },
    {
        "loss": 2.2561,
        "grad_norm": 1.6723426580429077,
        "learning_rate": 1.0920994101303295e-05,
        "epoch": 1.2348543439030677,
        "step": 9580
    },
    {
        "loss": 1.688,
        "grad_norm": 2.616647958755493,
        "learning_rate": 1.0893363818541124e-05,
        "epoch": 1.2349832431038927,
        "step": 9581
    },
    {
        "loss": 1.9044,
        "grad_norm": 2.0407204627990723,
        "learning_rate": 1.0865766519024057e-05,
        "epoch": 1.2351121423047178,
        "step": 9582
    },
    {
        "loss": 1.9909,
        "grad_norm": 2.2041547298431396,
        "learning_rate": 1.0838202212967418e-05,
        "epoch": 1.2352410415055426,
        "step": 9583
    },
    {
        "loss": 1.8409,
        "grad_norm": 2.122255563735962,
        "learning_rate": 1.081067091057435e-05,
        "epoch": 1.2353699407063676,
        "step": 9584
    },
    {
        "loss": 2.0416,
        "grad_norm": 1.3967037200927734,
        "learning_rate": 1.0783172622035532e-05,
        "epoch": 1.2354988399071927,
        "step": 9585
    },
    {
        "loss": 1.2831,
        "grad_norm": 4.354557991027832,
        "learning_rate": 1.0755707357529643e-05,
        "epoch": 1.2356277391080175,
        "step": 9586
    },
    {
        "loss": 1.4619,
        "grad_norm": 1.3339499235153198,
        "learning_rate": 1.0728275127223131e-05,
        "epoch": 1.2357566383088425,
        "step": 9587
    },
    {
        "loss": 2.2078,
        "grad_norm": 1.640108346939087,
        "learning_rate": 1.0700875941270127e-05,
        "epoch": 1.2358855375096676,
        "step": 9588
    },
    {
        "loss": 1.0645,
        "grad_norm": 2.4497783184051514,
        "learning_rate": 1.0673509809812564e-05,
        "epoch": 1.2360144367104924,
        "step": 9589
    },
    {
        "loss": 1.8595,
        "grad_norm": 2.5914597511291504,
        "learning_rate": 1.0646176742980185e-05,
        "epoch": 1.2361433359113174,
        "step": 9590
    },
    {
        "loss": 1.793,
        "grad_norm": 3.631413698196411,
        "learning_rate": 1.0618876750890417e-05,
        "epoch": 1.2362722351121422,
        "step": 9591
    },
    {
        "loss": 1.3839,
        "grad_norm": 2.7515811920166016,
        "learning_rate": 1.0591609843648453e-05,
        "epoch": 1.2364011343129673,
        "step": 9592
    },
    {
        "loss": 0.6645,
        "grad_norm": 1.8272486925125122,
        "learning_rate": 1.0564376031347333e-05,
        "epoch": 1.2365300335137923,
        "step": 9593
    },
    {
        "loss": 1.648,
        "grad_norm": 2.3520023822784424,
        "learning_rate": 1.053717532406775e-05,
        "epoch": 1.2366589327146171,
        "step": 9594
    },
    {
        "loss": 1.3716,
        "grad_norm": 2.197357654571533,
        "learning_rate": 1.0510007731878246e-05,
        "epoch": 1.2367878319154422,
        "step": 9595
    },
    {
        "loss": 1.3098,
        "grad_norm": 2.1142005920410156,
        "learning_rate": 1.048287326483488e-05,
        "epoch": 1.236916731116267,
        "step": 9596
    },
    {
        "loss": 1.142,
        "grad_norm": 1.9271899461746216,
        "learning_rate": 1.0455771932981751e-05,
        "epoch": 1.237045630317092,
        "step": 9597
    },
    {
        "loss": 1.3383,
        "grad_norm": 3.1322476863861084,
        "learning_rate": 1.0428703746350476e-05,
        "epoch": 1.237174529517917,
        "step": 9598
    },
    {
        "loss": 2.1868,
        "grad_norm": 2.0897064208984375,
        "learning_rate": 1.0401668714960455e-05,
        "epoch": 1.2373034287187419,
        "step": 9599
    },
    {
        "loss": 2.1167,
        "grad_norm": 1.6693305969238281,
        "learning_rate": 1.037466684881886e-05,
        "epoch": 1.237432327919567,
        "step": 9600
    },
    {
        "loss": 1.0347,
        "grad_norm": 3.1067614555358887,
        "learning_rate": 1.034769815792066e-05,
        "epoch": 1.2375612271203917,
        "step": 9601
    },
    {
        "loss": 2.0554,
        "grad_norm": 1.523825764656067,
        "learning_rate": 1.0320762652248272e-05,
        "epoch": 1.2376901263212168,
        "step": 9602
    },
    {
        "loss": 1.4085,
        "grad_norm": 2.5278892517089844,
        "learning_rate": 1.0293860341772065e-05,
        "epoch": 1.2378190255220418,
        "step": 9603
    },
    {
        "loss": 1.8261,
        "grad_norm": 3.034090757369995,
        "learning_rate": 1.0266991236450108e-05,
        "epoch": 1.2379479247228666,
        "step": 9604
    },
    {
        "loss": 2.2517,
        "grad_norm": 1.6664503812789917,
        "learning_rate": 1.0240155346228086e-05,
        "epoch": 1.2380768239236917,
        "step": 9605
    },
    {
        "loss": 1.5595,
        "grad_norm": 1.9632798433303833,
        "learning_rate": 1.02133526810394e-05,
        "epoch": 1.2382057231245167,
        "step": 9606
    },
    {
        "loss": 1.8757,
        "grad_norm": 1.7147632837295532,
        "learning_rate": 1.0186583250805282e-05,
        "epoch": 1.2383346223253415,
        "step": 9607
    },
    {
        "loss": 1.4568,
        "grad_norm": 3.056367874145508,
        "learning_rate": 1.015984706543448e-05,
        "epoch": 1.2384635215261666,
        "step": 9608
    },
    {
        "loss": 1.9705,
        "grad_norm": 2.989415168762207,
        "learning_rate": 1.0133144134823503e-05,
        "epoch": 1.2385924207269916,
        "step": 9609
    },
    {
        "loss": 0.5652,
        "grad_norm": 1.868617296218872,
        "learning_rate": 1.010647446885663e-05,
        "epoch": 1.2387213199278164,
        "step": 9610
    },
    {
        "loss": 1.868,
        "grad_norm": 2.3182566165924072,
        "learning_rate": 1.0079838077405735e-05,
        "epoch": 1.2388502191286415,
        "step": 9611
    },
    {
        "loss": 1.8511,
        "grad_norm": 2.2501797676086426,
        "learning_rate": 1.0053234970330472e-05,
        "epoch": 1.2389791183294663,
        "step": 9612
    },
    {
        "loss": 1.0163,
        "grad_norm": 2.935704469680786,
        "learning_rate": 1.0026665157477976e-05,
        "epoch": 1.2391080175302913,
        "step": 9613
    },
    {
        "loss": 1.8475,
        "grad_norm": 3.2279582023620605,
        "learning_rate": 1.0000128648683294e-05,
        "epoch": 1.2392369167311164,
        "step": 9614
    },
    {
        "loss": 1.7682,
        "grad_norm": 2.467956781387329,
        "learning_rate": 9.973625453769008e-06,
        "epoch": 1.2393658159319412,
        "step": 9615
    },
    {
        "loss": 0.9746,
        "grad_norm": 2.6376380920410156,
        "learning_rate": 9.947155582545365e-06,
        "epoch": 1.2394947151327662,
        "step": 9616
    },
    {
        "loss": 1.9013,
        "grad_norm": 2.773922920227051,
        "learning_rate": 9.920719044810345e-06,
        "epoch": 1.239623614333591,
        "step": 9617
    },
    {
        "loss": 2.3398,
        "grad_norm": 2.0730907917022705,
        "learning_rate": 9.894315850349666e-06,
        "epoch": 1.239752513534416,
        "step": 9618
    },
    {
        "loss": 1.9166,
        "grad_norm": 1.3931983709335327,
        "learning_rate": 9.867946008936413e-06,
        "epoch": 1.239881412735241,
        "step": 9619
    },
    {
        "loss": 1.7375,
        "grad_norm": 1.9125436544418335,
        "learning_rate": 9.841609530331564e-06,
        "epoch": 1.240010311936066,
        "step": 9620
    },
    {
        "loss": 1.8278,
        "grad_norm": 1.2936992645263672,
        "learning_rate": 9.815306424283766e-06,
        "epoch": 1.240139211136891,
        "step": 9621
    },
    {
        "loss": 1.6247,
        "grad_norm": 2.9871232509613037,
        "learning_rate": 9.789036700529164e-06,
        "epoch": 1.240268110337716,
        "step": 9622
    },
    {
        "loss": 1.1904,
        "grad_norm": 3.343087911605835,
        "learning_rate": 9.762800368791648e-06,
        "epoch": 1.2403970095385408,
        "step": 9623
    },
    {
        "loss": 1.8712,
        "grad_norm": 2.064966917037964,
        "learning_rate": 9.736597438782746e-06,
        "epoch": 1.2405259087393659,
        "step": 9624
    },
    {
        "loss": 1.305,
        "grad_norm": 2.6433472633361816,
        "learning_rate": 9.71042792020157e-06,
        "epoch": 1.2406548079401907,
        "step": 9625
    },
    {
        "loss": 1.1466,
        "grad_norm": 2.1275389194488525,
        "learning_rate": 9.684291822734837e-06,
        "epoch": 1.2407837071410157,
        "step": 9626
    },
    {
        "loss": 1.3341,
        "grad_norm": 2.2887094020843506,
        "learning_rate": 9.658189156057018e-06,
        "epoch": 1.2409126063418408,
        "step": 9627
    },
    {
        "loss": 1.3999,
        "grad_norm": 2.7722675800323486,
        "learning_rate": 9.632119929830153e-06,
        "epoch": 1.2410415055426656,
        "step": 9628
    },
    {
        "loss": 1.1564,
        "grad_norm": 1.742918610572815,
        "learning_rate": 9.606084153703899e-06,
        "epoch": 1.2411704047434906,
        "step": 9629
    },
    {
        "loss": 1.822,
        "grad_norm": 2.4206180572509766,
        "learning_rate": 9.580081837315414e-06,
        "epoch": 1.2412993039443156,
        "step": 9630
    },
    {
        "loss": 1.5345,
        "grad_norm": 2.46852970123291,
        "learning_rate": 9.554112990289688e-06,
        "epoch": 1.2414282031451405,
        "step": 9631
    },
    {
        "loss": 2.0714,
        "grad_norm": 2.1593711376190186,
        "learning_rate": 9.528177622239165e-06,
        "epoch": 1.2415571023459655,
        "step": 9632
    },
    {
        "loss": 1.5185,
        "grad_norm": 2.9250447750091553,
        "learning_rate": 9.502275742763922e-06,
        "epoch": 1.2416860015467903,
        "step": 9633
    },
    {
        "loss": 1.8689,
        "grad_norm": 2.2815065383911133,
        "learning_rate": 9.4764073614517e-06,
        "epoch": 1.2418149007476154,
        "step": 9634
    },
    {
        "loss": 1.6046,
        "grad_norm": 2.4413723945617676,
        "learning_rate": 9.450572487877895e-06,
        "epoch": 1.2419437999484404,
        "step": 9635
    },
    {
        "loss": 1.8998,
        "grad_norm": 2.4819447994232178,
        "learning_rate": 9.424771131605214e-06,
        "epoch": 1.2420726991492652,
        "step": 9636
    },
    {
        "loss": 0.9753,
        "grad_norm": 2.127084732055664,
        "learning_rate": 9.399003302184261e-06,
        "epoch": 1.2422015983500903,
        "step": 9637
    },
    {
        "loss": 2.0189,
        "grad_norm": 2.693359136581421,
        "learning_rate": 9.37326900915314e-06,
        "epoch": 1.242330497550915,
        "step": 9638
    },
    {
        "loss": 1.3415,
        "grad_norm": 2.6198627948760986,
        "learning_rate": 9.347568262037465e-06,
        "epoch": 1.24245939675174,
        "step": 9639
    },
    {
        "loss": 1.5462,
        "grad_norm": 2.227959394454956,
        "learning_rate": 9.321901070350525e-06,
        "epoch": 1.2425882959525651,
        "step": 9640
    },
    {
        "loss": 1.4694,
        "grad_norm": 2.4860732555389404,
        "learning_rate": 9.296267443593176e-06,
        "epoch": 1.24271719515339,
        "step": 9641
    },
    {
        "loss": 1.5498,
        "grad_norm": 2.6194612979888916,
        "learning_rate": 9.270667391253774e-06,
        "epoch": 1.242846094354215,
        "step": 9642
    },
    {
        "loss": 1.1851,
        "grad_norm": 2.8084919452667236,
        "learning_rate": 9.245100922808292e-06,
        "epoch": 1.24297499355504,
        "step": 9643
    },
    {
        "loss": 1.4645,
        "grad_norm": 2.2622556686401367,
        "learning_rate": 9.219568047720294e-06,
        "epoch": 1.2431038927558649,
        "step": 9644
    },
    {
        "loss": 2.2923,
        "grad_norm": 1.5036375522613525,
        "learning_rate": 9.194068775440967e-06,
        "epoch": 1.24323279195669,
        "step": 9645
    },
    {
        "loss": 1.9145,
        "grad_norm": 2.708320140838623,
        "learning_rate": 9.168603115408914e-06,
        "epoch": 1.243361691157515,
        "step": 9646
    },
    {
        "loss": 1.0494,
        "grad_norm": 2.326348066329956,
        "learning_rate": 9.143171077050406e-06,
        "epoch": 1.2434905903583398,
        "step": 9647
    },
    {
        "loss": 2.13,
        "grad_norm": 2.2802631855010986,
        "learning_rate": 9.117772669779202e-06,
        "epoch": 1.2436194895591648,
        "step": 9648
    },
    {
        "loss": 1.0292,
        "grad_norm": 2.6630165576934814,
        "learning_rate": 9.092407902996658e-06,
        "epoch": 1.2437483887599896,
        "step": 9649
    },
    {
        "loss": 1.5711,
        "grad_norm": 2.9841365814208984,
        "learning_rate": 9.067076786091621e-06,
        "epoch": 1.2438772879608146,
        "step": 9650
    },
    {
        "loss": 2.1238,
        "grad_norm": 3.1272616386413574,
        "learning_rate": 9.041779328440569e-06,
        "epoch": 1.2440061871616397,
        "step": 9651
    },
    {
        "loss": 1.6268,
        "grad_norm": 2.3255040645599365,
        "learning_rate": 9.016515539407556e-06,
        "epoch": 1.2441350863624645,
        "step": 9652
    },
    {
        "loss": 1.9966,
        "grad_norm": 1.894806981086731,
        "learning_rate": 8.991285428343922e-06,
        "epoch": 1.2442639855632895,
        "step": 9653
    },
    {
        "loss": 1.4643,
        "grad_norm": 2.6918368339538574,
        "learning_rate": 8.966089004588818e-06,
        "epoch": 1.2443928847641144,
        "step": 9654
    },
    {
        "loss": 1.9729,
        "grad_norm": 2.172116279602051,
        "learning_rate": 8.940926277468797e-06,
        "epoch": 1.2445217839649394,
        "step": 9655
    },
    {
        "loss": 2.0506,
        "grad_norm": 1.6351797580718994,
        "learning_rate": 8.915797256297998e-06,
        "epoch": 1.2446506831657644,
        "step": 9656
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.6421453952789307,
        "learning_rate": 8.89070195037799e-06,
        "epoch": 1.2447795823665893,
        "step": 9657
    },
    {
        "loss": 1.4072,
        "grad_norm": 1.6472102403640747,
        "learning_rate": 8.865640368997974e-06,
        "epoch": 1.2449084815674143,
        "step": 9658
    },
    {
        "loss": 1.5221,
        "grad_norm": 2.4338796138763428,
        "learning_rate": 8.840612521434578e-06,
        "epoch": 1.2450373807682393,
        "step": 9659
    },
    {
        "loss": 1.6653,
        "grad_norm": 2.4393906593322754,
        "learning_rate": 8.815618416951955e-06,
        "epoch": 1.2451662799690641,
        "step": 9660
    },
    {
        "loss": 1.2691,
        "grad_norm": 2.1495022773742676,
        "learning_rate": 8.790658064801816e-06,
        "epoch": 1.2452951791698892,
        "step": 9661
    },
    {
        "loss": 1.7065,
        "grad_norm": 2.072204828262329,
        "learning_rate": 8.765731474223392e-06,
        "epoch": 1.245424078370714,
        "step": 9662
    },
    {
        "loss": 1.0369,
        "grad_norm": 1.5423418283462524,
        "learning_rate": 8.740838654443363e-06,
        "epoch": 1.245552977571539,
        "step": 9663
    },
    {
        "loss": 1.4061,
        "grad_norm": 2.5570895671844482,
        "learning_rate": 8.715979614675896e-06,
        "epoch": 1.245681876772364,
        "step": 9664
    },
    {
        "loss": 1.8234,
        "grad_norm": 2.506859540939331,
        "learning_rate": 8.6911543641227e-06,
        "epoch": 1.245810775973189,
        "step": 9665
    },
    {
        "loss": 0.9193,
        "grad_norm": 2.6973652839660645,
        "learning_rate": 8.66636291197296e-06,
        "epoch": 1.245939675174014,
        "step": 9666
    },
    {
        "loss": 2.1724,
        "grad_norm": 1.4026702642440796,
        "learning_rate": 8.641605267403308e-06,
        "epoch": 1.246068574374839,
        "step": 9667
    },
    {
        "loss": 2.4585,
        "grad_norm": 2.501054286956787,
        "learning_rate": 8.61688143957794e-06,
        "epoch": 1.2461974735756638,
        "step": 9668
    },
    {
        "loss": 1.9694,
        "grad_norm": 3.043686628341675,
        "learning_rate": 8.592191437648545e-06,
        "epoch": 1.2463263727764888,
        "step": 9669
    },
    {
        "loss": 1.3887,
        "grad_norm": 2.442239999771118,
        "learning_rate": 8.567535270754212e-06,
        "epoch": 1.2464552719773137,
        "step": 9670
    },
    {
        "loss": 1.4999,
        "grad_norm": 1.46708083152771,
        "learning_rate": 8.542912948021498e-06,
        "epoch": 1.2465841711781387,
        "step": 9671
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.4077231884002686,
        "learning_rate": 8.518324478564488e-06,
        "epoch": 1.2467130703789637,
        "step": 9672
    },
    {
        "loss": 0.9313,
        "grad_norm": 2.538588762283325,
        "learning_rate": 8.493769871484757e-06,
        "epoch": 1.2468419695797885,
        "step": 9673
    },
    {
        "loss": 0.7895,
        "grad_norm": 2.112853527069092,
        "learning_rate": 8.46924913587126e-06,
        "epoch": 1.2469708687806136,
        "step": 9674
    },
    {
        "loss": 1.9883,
        "grad_norm": 1.653571605682373,
        "learning_rate": 8.444762280800555e-06,
        "epoch": 1.2470997679814384,
        "step": 9675
    },
    {
        "loss": 1.6904,
        "grad_norm": 1.8611880540847778,
        "learning_rate": 8.420309315336473e-06,
        "epoch": 1.2472286671822634,
        "step": 9676
    },
    {
        "loss": 1.4775,
        "grad_norm": 2.550975799560547,
        "learning_rate": 8.39589024853038e-06,
        "epoch": 1.2473575663830885,
        "step": 9677
    },
    {
        "loss": 1.7092,
        "grad_norm": 1.5298902988433838,
        "learning_rate": 8.371505089421161e-06,
        "epoch": 1.2474864655839133,
        "step": 9678
    },
    {
        "loss": 1.6395,
        "grad_norm": 2.072719097137451,
        "learning_rate": 8.347153847035138e-06,
        "epoch": 1.2476153647847383,
        "step": 9679
    },
    {
        "loss": 2.1799,
        "grad_norm": 2.699528932571411,
        "learning_rate": 8.32283653038598e-06,
        "epoch": 1.2477442639855634,
        "step": 9680
    },
    {
        "loss": 1.4523,
        "grad_norm": 2.00494384765625,
        "learning_rate": 8.298553148474886e-06,
        "epoch": 1.2478731631863882,
        "step": 9681
    },
    {
        "loss": 1.0193,
        "grad_norm": 2.289142370223999,
        "learning_rate": 8.274303710290443e-06,
        "epoch": 1.2480020623872132,
        "step": 9682
    },
    {
        "loss": 0.6372,
        "grad_norm": 2.283177375793457,
        "learning_rate": 8.250088224808683e-06,
        "epoch": 1.2481309615880383,
        "step": 9683
    },
    {
        "loss": 1.5195,
        "grad_norm": 1.9497768878936768,
        "learning_rate": 8.225906700993114e-06,
        "epoch": 1.248259860788863,
        "step": 9684
    },
    {
        "loss": 1.796,
        "grad_norm": 2.986049175262451,
        "learning_rate": 8.201759147794608e-06,
        "epoch": 1.2483887599896881,
        "step": 9685
    },
    {
        "loss": 0.8793,
        "grad_norm": 2.995805263519287,
        "learning_rate": 8.17764557415156e-06,
        "epoch": 1.248517659190513,
        "step": 9686
    },
    {
        "loss": 1.9293,
        "grad_norm": 2.6296579837799072,
        "learning_rate": 8.153565988989708e-06,
        "epoch": 1.248646558391338,
        "step": 9687
    },
    {
        "loss": 2.0966,
        "grad_norm": 2.2749271392822266,
        "learning_rate": 8.129520401222135e-06,
        "epoch": 1.248775457592163,
        "step": 9688
    },
    {
        "loss": 1.699,
        "grad_norm": 2.0987744331359863,
        "learning_rate": 8.10550881974954e-06,
        "epoch": 1.2489043567929878,
        "step": 9689
    },
    {
        "loss": 1.4189,
        "grad_norm": 1.704566478729248,
        "learning_rate": 8.081531253459885e-06,
        "epoch": 1.2490332559938129,
        "step": 9690
    },
    {
        "loss": 1.8287,
        "grad_norm": 2.5527780055999756,
        "learning_rate": 8.057587711228531e-06,
        "epoch": 1.2491621551946377,
        "step": 9691
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.316769599914551,
        "learning_rate": 8.033678201918415e-06,
        "epoch": 1.2492910543954627,
        "step": 9692
    },
    {
        "loss": 2.4508,
        "grad_norm": 2.1649975776672363,
        "learning_rate": 8.009802734379645e-06,
        "epoch": 1.2494199535962878,
        "step": 9693
    },
    {
        "loss": 0.767,
        "grad_norm": 2.962712049484253,
        "learning_rate": 7.985961317449842e-06,
        "epoch": 1.2495488527971126,
        "step": 9694
    },
    {
        "loss": 2.2429,
        "grad_norm": 2.3729019165039062,
        "learning_rate": 7.962153959954045e-06,
        "epoch": 1.2496777519979376,
        "step": 9695
    },
    {
        "loss": 1.5862,
        "grad_norm": 1.421507716178894,
        "learning_rate": 7.938380670704703e-06,
        "epoch": 1.2498066511987627,
        "step": 9696
    },
    {
        "loss": 1.5483,
        "grad_norm": 2.8453142642974854,
        "learning_rate": 7.914641458501581e-06,
        "epoch": 1.2499355503995875,
        "step": 9697
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.7122634649276733,
        "learning_rate": 7.890936332131849e-06,
        "epoch": 1.2500644496004125,
        "step": 9698
    },
    {
        "loss": 2.4122,
        "grad_norm": 2.337526559829712,
        "learning_rate": 7.867265300370086e-06,
        "epoch": 1.2501933488012376,
        "step": 9699
    },
    {
        "loss": 0.9537,
        "grad_norm": 2.2573022842407227,
        "learning_rate": 7.843628371978207e-06,
        "epoch": 1.2503222480020624,
        "step": 9700
    },
    {
        "loss": 1.6821,
        "grad_norm": 2.5672881603240967,
        "learning_rate": 7.820025555705557e-06,
        "epoch": 1.2504511472028874,
        "step": 9701
    },
    {
        "loss": 0.5097,
        "grad_norm": 1.809712529182434,
        "learning_rate": 7.796456860288815e-06,
        "epoch": 1.2505800464037122,
        "step": 9702
    },
    {
        "loss": 1.457,
        "grad_norm": 2.2001395225524902,
        "learning_rate": 7.7729222944521e-06,
        "epoch": 1.2507089456045373,
        "step": 9703
    },
    {
        "loss": 1.1589,
        "grad_norm": 1.9258408546447754,
        "learning_rate": 7.749421866906813e-06,
        "epoch": 1.2508378448053623,
        "step": 9704
    },
    {
        "loss": 1.0563,
        "grad_norm": 2.1528806686401367,
        "learning_rate": 7.725955586351696e-06,
        "epoch": 1.2509667440061871,
        "step": 9705
    },
    {
        "loss": 1.6142,
        "grad_norm": 2.4427034854888916,
        "learning_rate": 7.702523461472988e-06,
        "epoch": 1.2510956432070122,
        "step": 9706
    },
    {
        "loss": 1.9275,
        "grad_norm": 2.063915252685547,
        "learning_rate": 7.67912550094415e-06,
        "epoch": 1.251224542407837,
        "step": 9707
    },
    {
        "loss": 0.9967,
        "grad_norm": 2.032313585281372,
        "learning_rate": 7.655761713426035e-06,
        "epoch": 1.251353441608662,
        "step": 9708
    },
    {
        "loss": 1.3356,
        "grad_norm": 3.061159133911133,
        "learning_rate": 7.632432107566912e-06,
        "epoch": 1.251482340809487,
        "step": 9709
    },
    {
        "loss": 1.7223,
        "grad_norm": 2.131389856338501,
        "learning_rate": 7.609136692002372e-06,
        "epoch": 1.2516112400103119,
        "step": 9710
    },
    {
        "loss": 1.1559,
        "grad_norm": 2.6369152069091797,
        "learning_rate": 7.585875475355186e-06,
        "epoch": 1.251740139211137,
        "step": 9711
    },
    {
        "loss": 1.2483,
        "grad_norm": 2.5465099811553955,
        "learning_rate": 7.5626484662357e-06,
        "epoch": 1.2518690384119617,
        "step": 9712
    },
    {
        "loss": 1.4477,
        "grad_norm": 2.9807846546173096,
        "learning_rate": 7.539455673241525e-06,
        "epoch": 1.2519979376127868,
        "step": 9713
    },
    {
        "loss": 0.8609,
        "grad_norm": 1.347801685333252,
        "learning_rate": 7.516297104957548e-06,
        "epoch": 1.2521268368136118,
        "step": 9714
    },
    {
        "loss": 1.6129,
        "grad_norm": 2.4829061031341553,
        "learning_rate": 7.493172769956014e-06,
        "epoch": 1.2522557360144368,
        "step": 9715
    },
    {
        "loss": 1.4748,
        "grad_norm": 1.5056939125061035,
        "learning_rate": 7.470082676796508e-06,
        "epoch": 1.2523846352152617,
        "step": 9716
    },
    {
        "loss": 1.6356,
        "grad_norm": 2.8788235187530518,
        "learning_rate": 7.447026834025972e-06,
        "epoch": 1.2525135344160865,
        "step": 9717
    },
    {
        "loss": 1.5916,
        "grad_norm": 1.3480244874954224,
        "learning_rate": 7.424005250178578e-06,
        "epoch": 1.2526424336169115,
        "step": 9718
    },
    {
        "loss": 1.2127,
        "grad_norm": 1.6557193994522095,
        "learning_rate": 7.4010179337759e-06,
        "epoch": 1.2527713328177366,
        "step": 9719
    },
    {
        "loss": 1.9496,
        "grad_norm": 1.6709650754928589,
        "learning_rate": 7.378064893326841e-06,
        "epoch": 1.2529002320185616,
        "step": 9720
    },
    {
        "loss": 1.3931,
        "grad_norm": 1.9694396257400513,
        "learning_rate": 7.355146137327562e-06,
        "epoch": 1.2530291312193864,
        "step": 9721
    },
    {
        "loss": 1.1301,
        "grad_norm": 2.3499274253845215,
        "learning_rate": 7.332261674261487e-06,
        "epoch": 1.2531580304202115,
        "step": 9722
    },
    {
        "loss": 1.1956,
        "grad_norm": 2.8781578540802,
        "learning_rate": 7.309411512599484e-06,
        "epoch": 1.2532869296210363,
        "step": 9723
    },
    {
        "loss": 1.1981,
        "grad_norm": 2.467369318008423,
        "learning_rate": 7.286595660799634e-06,
        "epoch": 1.2534158288218613,
        "step": 9724
    },
    {
        "loss": 1.6506,
        "grad_norm": 1.785764455795288,
        "learning_rate": 7.263814127307278e-06,
        "epoch": 1.2535447280226863,
        "step": 9725
    },
    {
        "loss": 0.8374,
        "grad_norm": 3.024115562438965,
        "learning_rate": 7.241066920555151e-06,
        "epoch": 1.2536736272235112,
        "step": 9726
    },
    {
        "loss": 1.3155,
        "grad_norm": 2.2388901710510254,
        "learning_rate": 7.218354048963316e-06,
        "epoch": 1.2538025264243362,
        "step": 9727
    },
    {
        "loss": 2.2377,
        "grad_norm": 3.1052610874176025,
        "learning_rate": 7.195675520938916e-06,
        "epoch": 1.253931425625161,
        "step": 9728
    },
    {
        "loss": 2.1561,
        "grad_norm": 2.4994266033172607,
        "learning_rate": 7.173031344876557e-06,
        "epoch": 1.254060324825986,
        "step": 9729
    },
    {
        "loss": 1.9692,
        "grad_norm": 1.893626093864441,
        "learning_rate": 7.150421529158136e-06,
        "epoch": 1.254189224026811,
        "step": 9730
    },
    {
        "loss": 1.4326,
        "grad_norm": 2.4066810607910156,
        "learning_rate": 7.127846082152756e-06,
        "epoch": 1.254318123227636,
        "step": 9731
    },
    {
        "loss": 1.2184,
        "grad_norm": 1.650977611541748,
        "learning_rate": 7.10530501221679e-06,
        "epoch": 1.254447022428461,
        "step": 9732
    },
    {
        "loss": 1.2123,
        "grad_norm": 2.423217296600342,
        "learning_rate": 7.082798327694007e-06,
        "epoch": 1.2545759216292858,
        "step": 9733
    },
    {
        "loss": 1.9817,
        "grad_norm": 2.5556588172912598,
        "learning_rate": 7.060326036915288e-06,
        "epoch": 1.2547048208301108,
        "step": 9734
    },
    {
        "loss": 1.4838,
        "grad_norm": 2.2999162673950195,
        "learning_rate": 7.037888148198857e-06,
        "epoch": 1.2548337200309359,
        "step": 9735
    },
    {
        "loss": 1.9621,
        "grad_norm": 1.9158042669296265,
        "learning_rate": 7.015484669850248e-06,
        "epoch": 1.254962619231761,
        "step": 9736
    },
    {
        "loss": 1.8555,
        "grad_norm": 1.6637250185012817,
        "learning_rate": 6.993115610162215e-06,
        "epoch": 1.2550915184325857,
        "step": 9737
    },
    {
        "loss": 1.1974,
        "grad_norm": 2.7287542819976807,
        "learning_rate": 6.97078097741477e-06,
        "epoch": 1.2552204176334107,
        "step": 9738
    },
    {
        "loss": 0.5325,
        "grad_norm": 1.564177393913269,
        "learning_rate": 6.948480779875111e-06,
        "epoch": 1.2553493168342356,
        "step": 9739
    },
    {
        "loss": 1.5704,
        "grad_norm": 2.310814142227173,
        "learning_rate": 6.9262150257978555e-06,
        "epoch": 1.2554782160350606,
        "step": 9740
    },
    {
        "loss": 1.5606,
        "grad_norm": 2.407680034637451,
        "learning_rate": 6.903983723424745e-06,
        "epoch": 1.2556071152358856,
        "step": 9741
    },
    {
        "loss": 0.676,
        "grad_norm": 2.5058703422546387,
        "learning_rate": 6.8817868809847645e-06,
        "epoch": 1.2557360144367105,
        "step": 9742
    },
    {
        "loss": 1.5061,
        "grad_norm": 2.888136625289917,
        "learning_rate": 6.859624506694218e-06,
        "epoch": 1.2558649136375355,
        "step": 9743
    },
    {
        "loss": 1.9985,
        "grad_norm": 2.835949659347534,
        "learning_rate": 6.837496608756688e-06,
        "epoch": 1.2559938128383603,
        "step": 9744
    },
    {
        "loss": 1.2171,
        "grad_norm": 2.9360506534576416,
        "learning_rate": 6.81540319536278e-06,
        "epoch": 1.2561227120391854,
        "step": 9745
    },
    {
        "loss": 1.5723,
        "grad_norm": 2.798675060272217,
        "learning_rate": 6.793344274690528e-06,
        "epoch": 1.2562516112400104,
        "step": 9746
    },
    {
        "loss": 1.4332,
        "grad_norm": 2.646937847137451,
        "learning_rate": 6.7713198549051895e-06,
        "epoch": 1.2563805104408352,
        "step": 9747
    },
    {
        "loss": 1.5985,
        "grad_norm": 2.4866364002227783,
        "learning_rate": 6.749329944159177e-06,
        "epoch": 1.2565094096416602,
        "step": 9748
    },
    {
        "loss": 1.3625,
        "grad_norm": 2.964348077774048,
        "learning_rate": 6.727374550592125e-06,
        "epoch": 1.256638308842485,
        "step": 9749
    },
    {
        "loss": 2.0398,
        "grad_norm": 1.9034806489944458,
        "learning_rate": 6.7054536823310085e-06,
        "epoch": 1.25676720804331,
        "step": 9750
    },
    {
        "loss": 0.7643,
        "grad_norm": 2.710085868835449,
        "learning_rate": 6.683567347489883e-06,
        "epoch": 1.2568961072441351,
        "step": 9751
    },
    {
        "loss": 0.8114,
        "grad_norm": 1.6515371799468994,
        "learning_rate": 6.661715554170067e-06,
        "epoch": 1.25702500644496,
        "step": 9752
    },
    {
        "loss": 2.0888,
        "grad_norm": 1.8807575702667236,
        "learning_rate": 6.639898310460135e-06,
        "epoch": 1.257153905645785,
        "step": 9753
    },
    {
        "loss": 1.3807,
        "grad_norm": 1.9879002571105957,
        "learning_rate": 6.618115624435839e-06,
        "epoch": 1.2572828048466098,
        "step": 9754
    },
    {
        "loss": 1.8487,
        "grad_norm": 1.9012870788574219,
        "learning_rate": 6.596367504160195e-06,
        "epoch": 1.2574117040474349,
        "step": 9755
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.580810546875,
        "learning_rate": 6.574653957683263e-06,
        "epoch": 1.25754060324826,
        "step": 9756
    },
    {
        "loss": 1.1783,
        "grad_norm": 2.880082845687866,
        "learning_rate": 6.552974993042504e-06,
        "epoch": 1.257669502449085,
        "step": 9757
    },
    {
        "loss": 1.3722,
        "grad_norm": 2.2792701721191406,
        "learning_rate": 6.531330618262454e-06,
        "epoch": 1.2577984016499097,
        "step": 9758
    },
    {
        "loss": 1.8791,
        "grad_norm": 1.9143871068954468,
        "learning_rate": 6.5097208413548604e-06,
        "epoch": 1.2579273008507348,
        "step": 9759
    },
    {
        "loss": 1.3209,
        "grad_norm": 2.904461622238159,
        "learning_rate": 6.488145670318713e-06,
        "epoch": 1.2580562000515596,
        "step": 9760
    },
    {
        "loss": 0.9958,
        "grad_norm": 2.825788736343384,
        "learning_rate": 6.466605113140234e-06,
        "epoch": 1.2581850992523846,
        "step": 9761
    },
    {
        "loss": 0.8896,
        "grad_norm": 1.8492661714553833,
        "learning_rate": 6.445099177792635e-06,
        "epoch": 1.2583139984532097,
        "step": 9762
    },
    {
        "loss": 1.535,
        "grad_norm": 2.3911960124969482,
        "learning_rate": 6.4236278722364815e-06,
        "epoch": 1.2584428976540345,
        "step": 9763
    },
    {
        "loss": 1.1527,
        "grad_norm": 3.192045211791992,
        "learning_rate": 6.402191204419528e-06,
        "epoch": 1.2585717968548595,
        "step": 9764
    },
    {
        "loss": 2.2734,
        "grad_norm": 2.942837715148926,
        "learning_rate": 6.380789182276626e-06,
        "epoch": 1.2587006960556844,
        "step": 9765
    },
    {
        "loss": 2.3623,
        "grad_norm": 2.553138017654419,
        "learning_rate": 6.359421813729804e-06,
        "epoch": 1.2588295952565094,
        "step": 9766
    },
    {
        "loss": 2.1427,
        "grad_norm": 1.3779624700546265,
        "learning_rate": 6.338089106688372e-06,
        "epoch": 1.2589584944573344,
        "step": 9767
    },
    {
        "loss": 1.8803,
        "grad_norm": 1.4348143339157104,
        "learning_rate": 6.316791069048666e-06,
        "epoch": 1.2590873936581592,
        "step": 9768
    },
    {
        "loss": 0.6681,
        "grad_norm": 2.877464771270752,
        "learning_rate": 6.29552770869426e-06,
        "epoch": 1.2592162928589843,
        "step": 9769
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.638347864151001,
        "learning_rate": 6.274299033495912e-06,
        "epoch": 1.259345192059809,
        "step": 9770
    },
    {
        "loss": 1.8877,
        "grad_norm": 2.2634482383728027,
        "learning_rate": 6.253105051311514e-06,
        "epoch": 1.2594740912606341,
        "step": 9771
    },
    {
        "loss": 1.7494,
        "grad_norm": 1.778404712677002,
        "learning_rate": 6.231945769986136e-06,
        "epoch": 1.2596029904614592,
        "step": 9772
    },
    {
        "loss": 1.665,
        "grad_norm": 2.4216320514678955,
        "learning_rate": 6.210821197351957e-06,
        "epoch": 1.2597318896622842,
        "step": 9773
    },
    {
        "loss": 2.0066,
        "grad_norm": 1.9386577606201172,
        "learning_rate": 6.189731341228355e-06,
        "epoch": 1.259860788863109,
        "step": 9774
    },
    {
        "loss": 1.5021,
        "grad_norm": 2.6712470054626465,
        "learning_rate": 6.168676209421831e-06,
        "epoch": 1.259989688063934,
        "step": 9775
    },
    {
        "loss": 0.2536,
        "grad_norm": 1.1787227392196655,
        "learning_rate": 6.147655809726016e-06,
        "epoch": 1.260118587264759,
        "step": 9776
    },
    {
        "loss": 2.4115,
        "grad_norm": 1.8939546346664429,
        "learning_rate": 6.126670149921731e-06,
        "epoch": 1.260247486465584,
        "step": 9777
    },
    {
        "loss": 1.8583,
        "grad_norm": 1.624829649925232,
        "learning_rate": 6.105719237777019e-06,
        "epoch": 1.260376385666409,
        "step": 9778
    },
    {
        "loss": 1.7293,
        "grad_norm": 1.4510823488235474,
        "learning_rate": 6.084803081046775e-06,
        "epoch": 1.2605052848672338,
        "step": 9779
    },
    {
        "loss": 1.7844,
        "grad_norm": 1.7006447315216064,
        "learning_rate": 6.063921687473329e-06,
        "epoch": 1.2606341840680588,
        "step": 9780
    },
    {
        "loss": 0.9266,
        "grad_norm": 1.7227656841278076,
        "learning_rate": 6.0430750647860194e-06,
        "epoch": 1.2607630832688836,
        "step": 9781
    },
    {
        "loss": 1.5322,
        "grad_norm": 2.3433837890625,
        "learning_rate": 6.022263220701286e-06,
        "epoch": 1.2608919824697087,
        "step": 9782
    },
    {
        "loss": 0.9867,
        "grad_norm": 2.362598180770874,
        "learning_rate": 6.001486162922765e-06,
        "epoch": 1.2610208816705337,
        "step": 9783
    },
    {
        "loss": 1.4256,
        "grad_norm": 3.139265775680542,
        "learning_rate": 5.980743899141195e-06,
        "epoch": 1.2611497808713585,
        "step": 9784
    },
    {
        "loss": 1.4767,
        "grad_norm": 3.4581525325775146,
        "learning_rate": 5.960036437034388e-06,
        "epoch": 1.2612786800721836,
        "step": 9785
    },
    {
        "loss": 2.1983,
        "grad_norm": 3.693295478820801,
        "learning_rate": 5.939363784267293e-06,
        "epoch": 1.2614075792730084,
        "step": 9786
    },
    {
        "loss": 1.581,
        "grad_norm": 2.083160877227783,
        "learning_rate": 5.918725948492021e-06,
        "epoch": 1.2615364784738334,
        "step": 9787
    },
    {
        "loss": 1.3285,
        "grad_norm": 2.7827765941619873,
        "learning_rate": 5.898122937347794e-06,
        "epoch": 1.2616653776746585,
        "step": 9788
    },
    {
        "loss": 1.2498,
        "grad_norm": 2.559595823287964,
        "learning_rate": 5.87755475846089e-06,
        "epoch": 1.2617942768754833,
        "step": 9789
    },
    {
        "loss": 1.7357,
        "grad_norm": 2.2569804191589355,
        "learning_rate": 5.857021419444708e-06,
        "epoch": 1.2619231760763083,
        "step": 9790
    },
    {
        "loss": 1.1605,
        "grad_norm": 2.7876241207122803,
        "learning_rate": 5.836522927899768e-06,
        "epoch": 1.2620520752771331,
        "step": 9791
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.1253767013549805,
        "learning_rate": 5.816059291413678e-06,
        "epoch": 1.2621809744779582,
        "step": 9792
    },
    {
        "loss": 1.8883,
        "grad_norm": 3.151606321334839,
        "learning_rate": 5.7956305175611234e-06,
        "epoch": 1.2623098736787832,
        "step": 9793
    },
    {
        "loss": 1.7315,
        "grad_norm": 2.90370774269104,
        "learning_rate": 5.775236613903945e-06,
        "epoch": 1.2624387728796083,
        "step": 9794
    },
    {
        "loss": 1.3436,
        "grad_norm": 2.804492235183716,
        "learning_rate": 5.754877587991081e-06,
        "epoch": 1.262567672080433,
        "step": 9795
    },
    {
        "loss": 1.4033,
        "grad_norm": 3.6418542861938477,
        "learning_rate": 5.734553447358482e-06,
        "epoch": 1.2626965712812581,
        "step": 9796
    },
    {
        "loss": 1.8997,
        "grad_norm": 2.4571571350097656,
        "learning_rate": 5.714264199529207e-06,
        "epoch": 1.262825470482083,
        "step": 9797
    },
    {
        "loss": 1.7544,
        "grad_norm": 1.5794318914413452,
        "learning_rate": 5.694009852013449e-06,
        "epoch": 1.262954369682908,
        "step": 9798
    },
    {
        "loss": 1.7865,
        "grad_norm": 2.1552679538726807,
        "learning_rate": 5.673790412308422e-06,
        "epoch": 1.263083268883733,
        "step": 9799
    },
    {
        "loss": 1.609,
        "grad_norm": 3.634003162384033,
        "learning_rate": 5.653605887898461e-06,
        "epoch": 1.2632121680845578,
        "step": 9800
    },
    {
        "loss": 1.917,
        "grad_norm": 2.602121114730835,
        "learning_rate": 5.633456286255001e-06,
        "epoch": 1.2633410672853829,
        "step": 9801
    },
    {
        "loss": 2.068,
        "grad_norm": 1.519482970237732,
        "learning_rate": 5.613341614836454e-06,
        "epoch": 1.2634699664862077,
        "step": 9802
    },
    {
        "loss": 0.6628,
        "grad_norm": 2.725752592086792,
        "learning_rate": 5.593261881088363e-06,
        "epoch": 1.2635988656870327,
        "step": 9803
    },
    {
        "loss": 1.5467,
        "grad_norm": 2.453558921813965,
        "learning_rate": 5.573217092443362e-06,
        "epoch": 1.2637277648878578,
        "step": 9804
    },
    {
        "loss": 2.2954,
        "grad_norm": 1.2722920179367065,
        "learning_rate": 5.553207256321147e-06,
        "epoch": 1.2638566640886826,
        "step": 9805
    },
    {
        "loss": 2.4995,
        "grad_norm": 1.6973375082015991,
        "learning_rate": 5.533232380128439e-06,
        "epoch": 1.2639855632895076,
        "step": 9806
    },
    {
        "loss": 2.2386,
        "grad_norm": 2.508596658706665,
        "learning_rate": 5.513292471259013e-06,
        "epoch": 1.2641144624903324,
        "step": 9807
    },
    {
        "loss": 1.4828,
        "grad_norm": 3.3808188438415527,
        "learning_rate": 5.493387537093742e-06,
        "epoch": 1.2642433616911575,
        "step": 9808
    },
    {
        "loss": 1.508,
        "grad_norm": 2.3174595832824707,
        "learning_rate": 5.473517585000509e-06,
        "epoch": 1.2643722608919825,
        "step": 9809
    },
    {
        "loss": 1.2657,
        "grad_norm": 1.8844695091247559,
        "learning_rate": 5.453682622334266e-06,
        "epoch": 1.2645011600928076,
        "step": 9810
    },
    {
        "loss": 1.1979,
        "grad_norm": 1.7212424278259277,
        "learning_rate": 5.43388265643704e-06,
        "epoch": 1.2646300592936324,
        "step": 9811
    },
    {
        "loss": 1.1339,
        "grad_norm": 2.487882375717163,
        "learning_rate": 5.414117694637899e-06,
        "epoch": 1.2647589584944574,
        "step": 9812
    },
    {
        "loss": 1.6999,
        "grad_norm": 2.49216628074646,
        "learning_rate": 5.3943877442529376e-06,
        "epoch": 1.2648878576952822,
        "step": 9813
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.0021841526031494,
        "learning_rate": 5.3746928125852355e-06,
        "epoch": 1.2650167568961073,
        "step": 9814
    },
    {
        "loss": 2.093,
        "grad_norm": 2.418163299560547,
        "learning_rate": 5.355032906925006e-06,
        "epoch": 1.2651456560969323,
        "step": 9815
    },
    {
        "loss": 1.096,
        "grad_norm": 2.408045768737793,
        "learning_rate": 5.335408034549472e-06,
        "epoch": 1.2652745552977571,
        "step": 9816
    },
    {
        "loss": 0.636,
        "grad_norm": 1.7195193767547607,
        "learning_rate": 5.315818202722833e-06,
        "epoch": 1.2654034544985822,
        "step": 9817
    },
    {
        "loss": 2.0405,
        "grad_norm": 2.961185932159424,
        "learning_rate": 5.296263418696412e-06,
        "epoch": 1.265532353699407,
        "step": 9818
    },
    {
        "loss": 1.8141,
        "grad_norm": 3.159313201904297,
        "learning_rate": 5.276743689708452e-06,
        "epoch": 1.265661252900232,
        "step": 9819
    },
    {
        "loss": 0.2726,
        "grad_norm": 1.2460200786590576,
        "learning_rate": 5.257259022984262e-06,
        "epoch": 1.265790152101057,
        "step": 9820
    },
    {
        "loss": 1.4351,
        "grad_norm": 2.922513961791992,
        "learning_rate": 5.237809425736229e-06,
        "epoch": 1.2659190513018819,
        "step": 9821
    },
    {
        "loss": 2.1051,
        "grad_norm": 2.149942398071289,
        "learning_rate": 5.218394905163715e-06,
        "epoch": 1.266047950502707,
        "step": 9822
    },
    {
        "loss": 1.2518,
        "grad_norm": 2.243159055709839,
        "learning_rate": 5.199015468453083e-06,
        "epoch": 1.2661768497035317,
        "step": 9823
    },
    {
        "loss": 2.0574,
        "grad_norm": 1.4462988376617432,
        "learning_rate": 5.179671122777729e-06,
        "epoch": 1.2663057489043568,
        "step": 9824
    },
    {
        "loss": 1.6843,
        "grad_norm": 2.846200466156006,
        "learning_rate": 5.160361875298059e-06,
        "epoch": 1.2664346481051818,
        "step": 9825
    },
    {
        "loss": 1.7651,
        "grad_norm": 2.3010475635528564,
        "learning_rate": 5.141087733161443e-06,
        "epoch": 1.2665635473060066,
        "step": 9826
    },
    {
        "loss": 1.1808,
        "grad_norm": 2.4710912704467773,
        "learning_rate": 5.12184870350233e-06,
        "epoch": 1.2666924465068317,
        "step": 9827
    },
    {
        "loss": 0.527,
        "grad_norm": 1.411291241645813,
        "learning_rate": 5.102644793442124e-06,
        "epoch": 1.2668213457076565,
        "step": 9828
    },
    {
        "loss": 1.2455,
        "grad_norm": 2.4089839458465576,
        "learning_rate": 5.083476010089272e-06,
        "epoch": 1.2669502449084815,
        "step": 9829
    },
    {
        "loss": 0.7185,
        "grad_norm": 1.8474358320236206,
        "learning_rate": 5.064342360539199e-06,
        "epoch": 1.2670791441093066,
        "step": 9830
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.800181269645691,
        "learning_rate": 5.0452438518742485e-06,
        "epoch": 1.2672080433101316,
        "step": 9831
    },
    {
        "loss": 2.0246,
        "grad_norm": 2.1574411392211914,
        "learning_rate": 5.026180491163879e-06,
        "epoch": 1.2673369425109564,
        "step": 9832
    },
    {
        "loss": 0.6083,
        "grad_norm": 2.4880170822143555,
        "learning_rate": 5.007152285464478e-06,
        "epoch": 1.2674658417117814,
        "step": 9833
    },
    {
        "loss": 1.4405,
        "grad_norm": 2.6475830078125,
        "learning_rate": 4.98815924181939e-06,
        "epoch": 1.2675947409126063,
        "step": 9834
    },
    {
        "loss": 0.8399,
        "grad_norm": 3.3536581993103027,
        "learning_rate": 4.9692013672590246e-06,
        "epoch": 1.2677236401134313,
        "step": 9835
    },
    {
        "loss": 1.5996,
        "grad_norm": 1.5781607627868652,
        "learning_rate": 4.950278668800767e-06,
        "epoch": 1.2678525393142563,
        "step": 9836
    },
    {
        "loss": 1.2039,
        "grad_norm": 1.1381938457489014,
        "learning_rate": 4.931391153448828e-06,
        "epoch": 1.2679814385150812,
        "step": 9837
    },
    {
        "loss": 1.708,
        "grad_norm": 2.2916383743286133,
        "learning_rate": 4.912538828194568e-06,
        "epoch": 1.2681103377159062,
        "step": 9838
    },
    {
        "loss": 1.9738,
        "grad_norm": 1.5186313390731812,
        "learning_rate": 4.893721700016296e-06,
        "epoch": 1.268239236916731,
        "step": 9839
    },
    {
        "loss": 1.5232,
        "grad_norm": 3.2492287158966064,
        "learning_rate": 4.87493977587925e-06,
        "epoch": 1.268368136117556,
        "step": 9840
    },
    {
        "loss": 1.31,
        "grad_norm": 1.9091873168945312,
        "learning_rate": 4.856193062735626e-06,
        "epoch": 1.268497035318381,
        "step": 9841
    },
    {
        "loss": 2.4349,
        "grad_norm": 2.3933165073394775,
        "learning_rate": 4.837481567524615e-06,
        "epoch": 1.268625934519206,
        "step": 9842
    },
    {
        "loss": 0.9961,
        "grad_norm": 2.028881549835205,
        "learning_rate": 4.818805297172357e-06,
        "epoch": 1.268754833720031,
        "step": 9843
    },
    {
        "loss": 1.4427,
        "grad_norm": 2.186279058456421,
        "learning_rate": 4.800164258591977e-06,
        "epoch": 1.2688837329208558,
        "step": 9844
    },
    {
        "loss": 0.8678,
        "grad_norm": 2.2739946842193604,
        "learning_rate": 4.781558458683522e-06,
        "epoch": 1.2690126321216808,
        "step": 9845
    },
    {
        "loss": 1.6533,
        "grad_norm": 1.9959477186203003,
        "learning_rate": 4.762987904334071e-06,
        "epoch": 1.2691415313225058,
        "step": 9846
    },
    {
        "loss": 0.8897,
        "grad_norm": 1.3178468942642212,
        "learning_rate": 4.744452602417582e-06,
        "epoch": 1.2692704305233309,
        "step": 9847
    },
    {
        "loss": 2.0272,
        "grad_norm": 2.4196465015411377,
        "learning_rate": 4.7259525597949325e-06,
        "epoch": 1.2693993297241557,
        "step": 9848
    },
    {
        "loss": 1.1941,
        "grad_norm": 2.5366201400756836,
        "learning_rate": 4.7074877833140574e-06,
        "epoch": 1.2695282289249807,
        "step": 9849
    },
    {
        "loss": 1.8186,
        "grad_norm": 2.606442451477051,
        "learning_rate": 4.689058279809777e-06,
        "epoch": 1.2696571281258056,
        "step": 9850
    },
    {
        "loss": 1.7516,
        "grad_norm": 3.7092859745025635,
        "learning_rate": 4.6706640561038124e-06,
        "epoch": 1.2697860273266306,
        "step": 9851
    },
    {
        "loss": 2.0958,
        "grad_norm": 1.2297471761703491,
        "learning_rate": 4.65230511900493e-06,
        "epoch": 1.2699149265274556,
        "step": 9852
    },
    {
        "loss": 1.3897,
        "grad_norm": 2.0512328147888184,
        "learning_rate": 4.633981475308802e-06,
        "epoch": 1.2700438257282805,
        "step": 9853
    },
    {
        "loss": 1.7291,
        "grad_norm": 1.8806723356246948,
        "learning_rate": 4.6156931317979045e-06,
        "epoch": 1.2701727249291055,
        "step": 9854
    },
    {
        "loss": 1.2218,
        "grad_norm": 1.911052942276001,
        "learning_rate": 4.5974400952418115e-06,
        "epoch": 1.2703016241299303,
        "step": 9855
    },
    {
        "loss": 0.8089,
        "grad_norm": 2.215167284011841,
        "learning_rate": 4.579222372396996e-06,
        "epoch": 1.2704305233307553,
        "step": 9856
    },
    {
        "loss": 1.6176,
        "grad_norm": 2.585949659347534,
        "learning_rate": 4.5610399700068304e-06,
        "epoch": 1.2705594225315804,
        "step": 9857
    },
    {
        "loss": 0.7271,
        "grad_norm": 2.1574413776397705,
        "learning_rate": 4.542892894801565e-06,
        "epoch": 1.2706883217324052,
        "step": 9858
    },
    {
        "loss": 1.8413,
        "grad_norm": 1.8399121761322021,
        "learning_rate": 4.524781153498503e-06,
        "epoch": 1.2708172209332302,
        "step": 9859
    },
    {
        "loss": 2.179,
        "grad_norm": 3.4467315673828125,
        "learning_rate": 4.506704752801738e-06,
        "epoch": 1.270946120134055,
        "step": 9860
    },
    {
        "loss": 1.5262,
        "grad_norm": 3.346719741821289,
        "learning_rate": 4.48866369940234e-06,
        "epoch": 1.27107501933488,
        "step": 9861
    },
    {
        "loss": 1.8776,
        "grad_norm": 2.0223894119262695,
        "learning_rate": 4.470657999978289e-06,
        "epoch": 1.2712039185357051,
        "step": 9862
    },
    {
        "loss": 1.597,
        "grad_norm": 3.0345118045806885,
        "learning_rate": 4.45268766119451e-06,
        "epoch": 1.27133281773653,
        "step": 9863
    },
    {
        "loss": 0.7913,
        "grad_norm": 1.6887911558151245,
        "learning_rate": 4.434752689702804e-06,
        "epoch": 1.271461716937355,
        "step": 9864
    },
    {
        "loss": 0.6947,
        "grad_norm": 1.7827680110931396,
        "learning_rate": 4.416853092141837e-06,
        "epoch": 1.2715906161381798,
        "step": 9865
    },
    {
        "loss": 2.0839,
        "grad_norm": 1.7952531576156616,
        "learning_rate": 4.398988875137289e-06,
        "epoch": 1.2717195153390048,
        "step": 9866
    },
    {
        "loss": 2.1509,
        "grad_norm": 2.243717670440674,
        "learning_rate": 4.3811600453016685e-06,
        "epoch": 1.2718484145398299,
        "step": 9867
    },
    {
        "loss": 1.5778,
        "grad_norm": 2.039599895477295,
        "learning_rate": 4.363366609234365e-06,
        "epoch": 1.271977313740655,
        "step": 9868
    },
    {
        "loss": 1.9023,
        "grad_norm": 2.751969575881958,
        "learning_rate": 4.345608573521742e-06,
        "epoch": 1.2721062129414797,
        "step": 9869
    },
    {
        "loss": 1.2257,
        "grad_norm": 3.7293334007263184,
        "learning_rate": 4.327885944737065e-06,
        "epoch": 1.2722351121423048,
        "step": 9870
    },
    {
        "loss": 1.0012,
        "grad_norm": 2.477490186691284,
        "learning_rate": 4.310198729440329e-06,
        "epoch": 1.2723640113431296,
        "step": 9871
    },
    {
        "loss": 1.0714,
        "grad_norm": 2.4947948455810547,
        "learning_rate": 4.292546934178621e-06,
        "epoch": 1.2724929105439546,
        "step": 9872
    },
    {
        "loss": 0.9447,
        "grad_norm": 2.0365381240844727,
        "learning_rate": 4.274930565485835e-06,
        "epoch": 1.2726218097447797,
        "step": 9873
    },
    {
        "loss": 1.6133,
        "grad_norm": 2.036452531814575,
        "learning_rate": 4.257349629882745e-06,
        "epoch": 1.2727507089456045,
        "step": 9874
    },
    {
        "loss": 1.7691,
        "grad_norm": 2.1871180534362793,
        "learning_rate": 4.239804133877001e-06,
        "epoch": 1.2728796081464295,
        "step": 9875
    },
    {
        "loss": 1.4224,
        "grad_norm": 1.459313154220581,
        "learning_rate": 4.222294083963197e-06,
        "epoch": 1.2730085073472543,
        "step": 9876
    },
    {
        "loss": 1.8823,
        "grad_norm": 2.325073719024658,
        "learning_rate": 4.204819486622713e-06,
        "epoch": 1.2731374065480794,
        "step": 9877
    },
    {
        "loss": 1.8065,
        "grad_norm": 2.036951780319214,
        "learning_rate": 4.187380348323855e-06,
        "epoch": 1.2732663057489044,
        "step": 9878
    },
    {
        "loss": 1.6652,
        "grad_norm": 2.5783331394195557,
        "learning_rate": 4.169976675521836e-06,
        "epoch": 1.2733952049497292,
        "step": 9879
    },
    {
        "loss": 1.825,
        "grad_norm": 3.052482843399048,
        "learning_rate": 4.1526084746586925e-06,
        "epoch": 1.2735241041505543,
        "step": 9880
    },
    {
        "loss": 0.5773,
        "grad_norm": 2.110130548477173,
        "learning_rate": 4.13527575216337e-06,
        "epoch": 1.273653003351379,
        "step": 9881
    },
    {
        "loss": 1.4136,
        "grad_norm": 2.28289532661438,
        "learning_rate": 4.117978514451592e-06,
        "epoch": 1.2737819025522041,
        "step": 9882
    },
    {
        "loss": 1.5347,
        "grad_norm": 2.909929037094116,
        "learning_rate": 4.100716767926083e-06,
        "epoch": 1.2739108017530292,
        "step": 9883
    },
    {
        "loss": 1.6475,
        "grad_norm": 3.313575029373169,
        "learning_rate": 4.08349051897633e-06,
        "epoch": 1.2740397009538542,
        "step": 9884
    },
    {
        "loss": 1.1297,
        "grad_norm": 3.4847426414489746,
        "learning_rate": 4.0662997739786875e-06,
        "epoch": 1.274168600154679,
        "step": 9885
    },
    {
        "loss": 2.0352,
        "grad_norm": 1.256049633026123,
        "learning_rate": 4.049144539296423e-06,
        "epoch": 1.274297499355504,
        "step": 9886
    },
    {
        "loss": 0.7366,
        "grad_norm": 2.1843984127044678,
        "learning_rate": 4.032024821279689e-06,
        "epoch": 1.2744263985563289,
        "step": 9887
    },
    {
        "loss": 0.9729,
        "grad_norm": 2.6613552570343018,
        "learning_rate": 4.014940626265307e-06,
        "epoch": 1.274555297757154,
        "step": 9888
    },
    {
        "loss": 2.1951,
        "grad_norm": 2.766901731491089,
        "learning_rate": 3.997891960577127e-06,
        "epoch": 1.274684196957979,
        "step": 9889
    },
    {
        "loss": 1.7649,
        "grad_norm": 2.2899506092071533,
        "learning_rate": 3.9808788305258134e-06,
        "epoch": 1.2748130961588038,
        "step": 9890
    },
    {
        "loss": 1.9635,
        "grad_norm": 1.8534735441207886,
        "learning_rate": 3.963901242408852e-06,
        "epoch": 1.2749419953596288,
        "step": 9891
    },
    {
        "loss": 1.8493,
        "grad_norm": 1.7313616275787354,
        "learning_rate": 3.946959202510536e-06,
        "epoch": 1.2750708945604536,
        "step": 9892
    },
    {
        "loss": 1.5772,
        "grad_norm": 2.8554413318634033,
        "learning_rate": 3.930052717102107e-06,
        "epoch": 1.2751997937612787,
        "step": 9893
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.6990970373153687,
        "learning_rate": 3.913181792441545e-06,
        "epoch": 1.2753286929621037,
        "step": 9894
    },
    {
        "loss": 2.1732,
        "grad_norm": 1.3969957828521729,
        "learning_rate": 3.896346434773679e-06,
        "epoch": 1.2754575921629285,
        "step": 9895
    },
    {
        "loss": 1.4438,
        "grad_norm": 2.6136245727539062,
        "learning_rate": 3.879546650330235e-06,
        "epoch": 1.2755864913637536,
        "step": 9896
    },
    {
        "loss": 1.4762,
        "grad_norm": 2.6354472637176514,
        "learning_rate": 3.8627824453297265e-06,
        "epoch": 1.2757153905645784,
        "step": 9897
    },
    {
        "loss": 1.0021,
        "grad_norm": 2.555293560028076,
        "learning_rate": 3.846053825977491e-06,
        "epoch": 1.2758442897654034,
        "step": 9898
    },
    {
        "loss": 1.5469,
        "grad_norm": 1.8879027366638184,
        "learning_rate": 3.829360798465731e-06,
        "epoch": 1.2759731889662285,
        "step": 9899
    },
    {
        "loss": 1.329,
        "grad_norm": 2.6025400161743164,
        "learning_rate": 3.812703368973425e-06,
        "epoch": 1.2761020881670533,
        "step": 9900
    },
    {
        "loss": 1.4231,
        "grad_norm": 1.8109805583953857,
        "learning_rate": 3.796081543666408e-06,
        "epoch": 1.2762309873678783,
        "step": 9901
    },
    {
        "loss": 1.7889,
        "grad_norm": 1.9407925605773926,
        "learning_rate": 3.7794953286973135e-06,
        "epoch": 1.2763598865687031,
        "step": 9902
    },
    {
        "loss": 1.3909,
        "grad_norm": 2.5704538822174072,
        "learning_rate": 3.7629447302056086e-06,
        "epoch": 1.2764887857695282,
        "step": 9903
    },
    {
        "loss": 1.9205,
        "grad_norm": 2.7097091674804688,
        "learning_rate": 3.7464297543176598e-06,
        "epoch": 1.2766176849703532,
        "step": 9904
    },
    {
        "loss": 2.0504,
        "grad_norm": 1.8177542686462402,
        "learning_rate": 3.729950407146443e-06,
        "epoch": 1.2767465841711783,
        "step": 9905
    },
    {
        "loss": 1.2766,
        "grad_norm": 1.2181977033615112,
        "learning_rate": 3.7135066947919017e-06,
        "epoch": 1.276875483372003,
        "step": 9906
    },
    {
        "loss": 1.339,
        "grad_norm": 3.967256784439087,
        "learning_rate": 3.6970986233408002e-06,
        "epoch": 1.2770043825728281,
        "step": 9907
    },
    {
        "loss": 1.2464,
        "grad_norm": 2.717282772064209,
        "learning_rate": 3.680726198866624e-06,
        "epoch": 1.277133281773653,
        "step": 9908
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.367637872695923,
        "learning_rate": 3.6643894274297044e-06,
        "epoch": 1.277262180974478,
        "step": 9909
    },
    {
        "loss": 1.5412,
        "grad_norm": 4.297516822814941,
        "learning_rate": 3.6480883150772027e-06,
        "epoch": 1.277391080175303,
        "step": 9910
    },
    {
        "loss": 0.7534,
        "grad_norm": 1.914154291152954,
        "learning_rate": 3.6318228678430376e-06,
        "epoch": 1.2775199793761278,
        "step": 9911
    },
    {
        "loss": 1.2476,
        "grad_norm": 2.1581296920776367,
        "learning_rate": 3.615593091747904e-06,
        "epoch": 1.2776488785769529,
        "step": 9912
    },
    {
        "loss": 1.7767,
        "grad_norm": 2.2317473888397217,
        "learning_rate": 3.5993989927993856e-06,
        "epoch": 1.2777777777777777,
        "step": 9913
    },
    {
        "loss": 0.8238,
        "grad_norm": 2.9232661724090576,
        "learning_rate": 3.5832405769917863e-06,
        "epoch": 1.2779066769786027,
        "step": 9914
    },
    {
        "loss": 1.7815,
        "grad_norm": 1.7098023891448975,
        "learning_rate": 3.567117850306223e-06,
        "epoch": 1.2780355761794278,
        "step": 9915
    },
    {
        "loss": 1.8264,
        "grad_norm": 1.5817660093307495,
        "learning_rate": 3.551030818710599e-06,
        "epoch": 1.2781644753802526,
        "step": 9916
    },
    {
        "loss": 1.8566,
        "grad_norm": 2.7452199459075928,
        "learning_rate": 3.5349794881596066e-06,
        "epoch": 1.2782933745810776,
        "step": 9917
    },
    {
        "loss": 2.178,
        "grad_norm": 2.0081844329833984,
        "learning_rate": 3.5189638645947153e-06,
        "epoch": 1.2784222737819024,
        "step": 9918
    },
    {
        "loss": 1.6445,
        "grad_norm": 3.0261759757995605,
        "learning_rate": 3.5029839539441723e-06,
        "epoch": 1.2785511729827275,
        "step": 9919
    },
    {
        "loss": 1.6405,
        "grad_norm": 2.0276126861572266,
        "learning_rate": 3.4870397621230233e-06,
        "epoch": 1.2786800721835525,
        "step": 9920
    },
    {
        "loss": 1.1113,
        "grad_norm": 3.485738754272461,
        "learning_rate": 3.471131295033114e-06,
        "epoch": 1.2788089713843775,
        "step": 9921
    },
    {
        "loss": 1.9529,
        "grad_norm": 1.912222146987915,
        "learning_rate": 3.455258558563057e-06,
        "epoch": 1.2789378705852024,
        "step": 9922
    },
    {
        "loss": 1.5397,
        "grad_norm": 2.38594913482666,
        "learning_rate": 3.4394215585881294e-06,
        "epoch": 1.2790667697860274,
        "step": 9923
    },
    {
        "loss": 1.1118,
        "grad_norm": 2.914154052734375,
        "learning_rate": 3.4236203009705537e-06,
        "epoch": 1.2791956689868522,
        "step": 9924
    },
    {
        "loss": 1.2304,
        "grad_norm": 1.7783774137496948,
        "learning_rate": 3.407854791559206e-06,
        "epoch": 1.2793245681876773,
        "step": 9925
    },
    {
        "loss": 0.7043,
        "grad_norm": 2.0070135593414307,
        "learning_rate": 3.392125036189775e-06,
        "epoch": 1.2794534673885023,
        "step": 9926
    },
    {
        "loss": 1.5549,
        "grad_norm": 2.9620397090911865,
        "learning_rate": 3.3764310406847244e-06,
        "epoch": 1.2795823665893271,
        "step": 9927
    },
    {
        "loss": 1.4511,
        "grad_norm": 2.436893939971924,
        "learning_rate": 3.3607728108532187e-06,
        "epoch": 1.2797112657901522,
        "step": 9928
    },
    {
        "loss": 1.6569,
        "grad_norm": 2.1337859630584717,
        "learning_rate": 3.345150352491233e-06,
        "epoch": 1.279840164990977,
        "step": 9929
    },
    {
        "loss": 1.4105,
        "grad_norm": 1.9416255950927734,
        "learning_rate": 3.329563671381508e-06,
        "epoch": 1.279969064191802,
        "step": 9930
    },
    {
        "loss": 0.6288,
        "grad_norm": 1.717085838317871,
        "learning_rate": 3.314012773293551e-06,
        "epoch": 1.280097963392627,
        "step": 9931
    },
    {
        "loss": 1.0745,
        "grad_norm": 2.3997116088867188,
        "learning_rate": 3.2984976639835797e-06,
        "epoch": 1.2802268625934519,
        "step": 9932
    },
    {
        "loss": 1.8906,
        "grad_norm": 2.6603634357452393,
        "learning_rate": 3.2830183491945886e-06,
        "epoch": 1.280355761794277,
        "step": 9933
    },
    {
        "loss": 1.2956,
        "grad_norm": 2.2472615242004395,
        "learning_rate": 3.2675748346563064e-06,
        "epoch": 1.2804846609951017,
        "step": 9934
    },
    {
        "loss": 1.3728,
        "grad_norm": 1.2050546407699585,
        "learning_rate": 3.2521671260852373e-06,
        "epoch": 1.2806135601959268,
        "step": 9935
    },
    {
        "loss": 1.3343,
        "grad_norm": 2.7871108055114746,
        "learning_rate": 3.236795229184586e-06,
        "epoch": 1.2807424593967518,
        "step": 9936
    },
    {
        "loss": 1.3106,
        "grad_norm": 2.2996535301208496,
        "learning_rate": 3.221459149644368e-06,
        "epoch": 1.2808713585975766,
        "step": 9937
    },
    {
        "loss": 1.2313,
        "grad_norm": 2.8135876655578613,
        "learning_rate": 3.206158893141309e-06,
        "epoch": 1.2810002577984017,
        "step": 9938
    },
    {
        "loss": 2.4761,
        "grad_norm": 2.566617488861084,
        "learning_rate": 3.1908944653388784e-06,
        "epoch": 1.2811291569992265,
        "step": 9939
    },
    {
        "loss": 1.268,
        "grad_norm": 2.400589942932129,
        "learning_rate": 3.1756658718872457e-06,
        "epoch": 1.2812580562000515,
        "step": 9940
    },
    {
        "loss": 1.3986,
        "grad_norm": 1.9343745708465576,
        "learning_rate": 3.160473118423357e-06,
        "epoch": 1.2813869554008765,
        "step": 9941
    },
    {
        "loss": 1.8172,
        "grad_norm": 2.2584381103515625,
        "learning_rate": 3.145316210570892e-06,
        "epoch": 1.2815158546017016,
        "step": 9942
    },
    {
        "loss": 2.2708,
        "grad_norm": 1.1861673593521118,
        "learning_rate": 3.1301951539402297e-06,
        "epoch": 1.2816447538025264,
        "step": 9943
    },
    {
        "loss": 1.1148,
        "grad_norm": 2.1167140007019043,
        "learning_rate": 3.1151099541285478e-06,
        "epoch": 1.2817736530033514,
        "step": 9944
    },
    {
        "loss": 1.5521,
        "grad_norm": 3.595519781112671,
        "learning_rate": 3.100060616719669e-06,
        "epoch": 1.2819025522041763,
        "step": 9945
    },
    {
        "loss": 2.6898,
        "grad_norm": 2.170103073120117,
        "learning_rate": 3.085047147284148e-06,
        "epoch": 1.2820314514050013,
        "step": 9946
    },
    {
        "loss": 1.5395,
        "grad_norm": 1.828355073928833,
        "learning_rate": 3.0700695513793397e-06,
        "epoch": 1.2821603506058263,
        "step": 9947
    },
    {
        "loss": 1.1363,
        "grad_norm": 2.158149480819702,
        "learning_rate": 3.055127834549276e-06,
        "epoch": 1.2822892498066512,
        "step": 9948
    },
    {
        "loss": 1.759,
        "grad_norm": 3.095010280609131,
        "learning_rate": 3.0402220023246886e-06,
        "epoch": 1.2824181490074762,
        "step": 9949
    },
    {
        "loss": 1.6162,
        "grad_norm": 3.3395557403564453,
        "learning_rate": 3.025352060223041e-06,
        "epoch": 1.282547048208301,
        "step": 9950
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.8447800874710083,
        "learning_rate": 3.010518013748509e-06,
        "epoch": 1.282675947409126,
        "step": 9951
    },
    {
        "loss": 1.6262,
        "grad_norm": 2.34672212600708,
        "learning_rate": 2.9957198683919887e-06,
        "epoch": 1.282804846609951,
        "step": 9952
    },
    {
        "loss": 1.4003,
        "grad_norm": 2.5927085876464844,
        "learning_rate": 2.980957629631076e-06,
        "epoch": 1.282933745810776,
        "step": 9953
    },
    {
        "loss": 2.0263,
        "grad_norm": 2.4417216777801514,
        "learning_rate": 2.966231302930089e-06,
        "epoch": 1.283062645011601,
        "step": 9954
    },
    {
        "loss": 1.4745,
        "grad_norm": 2.799442768096924,
        "learning_rate": 2.951540893740068e-06,
        "epoch": 1.2831915442124258,
        "step": 9955
    },
    {
        "loss": 0.8893,
        "grad_norm": 3.338129997253418,
        "learning_rate": 2.936886407498729e-06,
        "epoch": 1.2833204434132508,
        "step": 9956
    },
    {
        "loss": 1.6168,
        "grad_norm": 1.1556940078735352,
        "learning_rate": 2.9222678496304777e-06,
        "epoch": 1.2834493426140758,
        "step": 9957
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.0357413291931152,
        "learning_rate": 2.9076852255464525e-06,
        "epoch": 1.2835782418149009,
        "step": 9958
    },
    {
        "loss": 2.034,
        "grad_norm": 1.3116540908813477,
        "learning_rate": 2.893138540644502e-06,
        "epoch": 1.2837071410157257,
        "step": 9959
    },
    {
        "loss": 1.3528,
        "grad_norm": 2.238537311553955,
        "learning_rate": 2.8786278003091305e-06,
        "epoch": 1.2838360402165507,
        "step": 9960
    },
    {
        "loss": 1.0828,
        "grad_norm": 2.559330701828003,
        "learning_rate": 2.864153009911541e-06,
        "epoch": 1.2839649394173756,
        "step": 9961
    },
    {
        "loss": 1.3932,
        "grad_norm": 2.258695602416992,
        "learning_rate": 2.8497141748097143e-06,
        "epoch": 1.2840938386182006,
        "step": 9962
    },
    {
        "loss": 0.6733,
        "grad_norm": 2.8485000133514404,
        "learning_rate": 2.8353113003481645e-06,
        "epoch": 1.2842227378190256,
        "step": 9963
    },
    {
        "loss": 1.5157,
        "grad_norm": 2.2825610637664795,
        "learning_rate": 2.8209443918582266e-06,
        "epoch": 1.2843516370198504,
        "step": 9964
    },
    {
        "loss": 1.6971,
        "grad_norm": 2.8239476680755615,
        "learning_rate": 2.8066134546578916e-06,
        "epoch": 1.2844805362206755,
        "step": 9965
    },
    {
        "loss": 1.3015,
        "grad_norm": 1.5518993139266968,
        "learning_rate": 2.7923184940518154e-06,
        "epoch": 1.2846094354215003,
        "step": 9966
    },
    {
        "loss": 1.1648,
        "grad_norm": 3.131256580352783,
        "learning_rate": 2.778059515331344e-06,
        "epoch": 1.2847383346223253,
        "step": 9967
    },
    {
        "loss": 1.8226,
        "grad_norm": 1.9093965291976929,
        "learning_rate": 2.763836523774499e-06,
        "epoch": 1.2848672338231504,
        "step": 9968
    },
    {
        "loss": 1.4981,
        "grad_norm": 2.1209144592285156,
        "learning_rate": 2.749649524646003e-06,
        "epoch": 1.2849961330239752,
        "step": 9969
    },
    {
        "loss": 1.9806,
        "grad_norm": 1.6682671308517456,
        "learning_rate": 2.7354985231971996e-06,
        "epoch": 1.2851250322248002,
        "step": 9970
    },
    {
        "loss": 2.0402,
        "grad_norm": 2.8459160327911377,
        "learning_rate": 2.721383524666188e-06,
        "epoch": 1.285253931425625,
        "step": 9971
    },
    {
        "loss": 2.3211,
        "grad_norm": 3.1790661811828613,
        "learning_rate": 2.7073045342777214e-06,
        "epoch": 1.28538283062645,
        "step": 9972
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.906327247619629,
        "learning_rate": 2.6932615572431873e-06,
        "epoch": 1.2855117298272751,
        "step": 9973
    },
    {
        "loss": 1.334,
        "grad_norm": 1.891493320465088,
        "learning_rate": 2.6792545987606277e-06,
        "epoch": 1.2856406290281,
        "step": 9974
    },
    {
        "loss": 2.248,
        "grad_norm": 1.5094660520553589,
        "learning_rate": 2.665283664014806e-06,
        "epoch": 1.285769528228925,
        "step": 9975
    },
    {
        "loss": 2.4386,
        "grad_norm": 2.3200950622558594,
        "learning_rate": 2.6513487581771523e-06,
        "epoch": 1.2858984274297498,
        "step": 9976
    },
    {
        "loss": 1.4683,
        "grad_norm": 3.1382699012756348,
        "learning_rate": 2.6374498864056964e-06,
        "epoch": 1.2860273266305748,
        "step": 9977
    },
    {
        "loss": 1.7076,
        "grad_norm": 2.22992205619812,
        "learning_rate": 2.6235870538452e-06,
        "epoch": 1.2861562258313999,
        "step": 9978
    },
    {
        "loss": 1.2405,
        "grad_norm": 2.765908718109131,
        "learning_rate": 2.6097602656271035e-06,
        "epoch": 1.286285125032225,
        "step": 9979
    },
    {
        "loss": 1.5786,
        "grad_norm": 3.245736837387085,
        "learning_rate": 2.5959695268693463e-06,
        "epoch": 1.2864140242330497,
        "step": 9980
    },
    {
        "loss": 0.7016,
        "grad_norm": 2.3363208770751953,
        "learning_rate": 2.5822148426767e-06,
        "epoch": 1.2865429234338748,
        "step": 9981
    },
    {
        "loss": 1.9284,
        "grad_norm": 1.9021849632263184,
        "learning_rate": 2.568496218140537e-06,
        "epoch": 1.2866718226346996,
        "step": 9982
    },
    {
        "loss": 1.6174,
        "grad_norm": 2.533740282058716,
        "learning_rate": 2.5548136583388618e-06,
        "epoch": 1.2868007218355246,
        "step": 9983
    },
    {
        "loss": 1.031,
        "grad_norm": 2.7096190452575684,
        "learning_rate": 2.5411671683363113e-06,
        "epoch": 1.2869296210363497,
        "step": 9984
    },
    {
        "loss": 1.6922,
        "grad_norm": 2.413055419921875,
        "learning_rate": 2.527556753184268e-06,
        "epoch": 1.2870585202371745,
        "step": 9985
    },
    {
        "loss": 1.6219,
        "grad_norm": 2.8521058559417725,
        "learning_rate": 2.513982417920613e-06,
        "epoch": 1.2871874194379995,
        "step": 9986
    },
    {
        "loss": 1.3897,
        "grad_norm": 1.7199251651763916,
        "learning_rate": 2.5004441675699706e-06,
        "epoch": 1.2873163186388243,
        "step": 9987
    },
    {
        "loss": 1.8167,
        "grad_norm": 2.2355716228485107,
        "learning_rate": 2.4869420071436e-06,
        "epoch": 1.2874452178396494,
        "step": 9988
    },
    {
        "loss": 1.4502,
        "grad_norm": 2.3718137741088867,
        "learning_rate": 2.4734759416394137e-06,
        "epoch": 1.2875741170404744,
        "step": 9989
    },
    {
        "loss": 1.9602,
        "grad_norm": 1.7602978944778442,
        "learning_rate": 2.460045976041925e-06,
        "epoch": 1.2877030162412992,
        "step": 9990
    },
    {
        "loss": 1.0981,
        "grad_norm": 1.7950639724731445,
        "learning_rate": 2.4466521153222675e-06,
        "epoch": 1.2878319154421243,
        "step": 9991
    },
    {
        "loss": 1.1468,
        "grad_norm": 2.384915351867676,
        "learning_rate": 2.433294364438288e-06,
        "epoch": 1.287960814642949,
        "step": 9992
    },
    {
        "loss": 1.4235,
        "grad_norm": 2.797865390777588,
        "learning_rate": 2.419972728334385e-06,
        "epoch": 1.2880897138437741,
        "step": 9993
    },
    {
        "loss": 1.4918,
        "grad_norm": 2.4593069553375244,
        "learning_rate": 2.4066872119416385e-06,
        "epoch": 1.2882186130445992,
        "step": 9994
    },
    {
        "loss": 1.6099,
        "grad_norm": 3.0235390663146973,
        "learning_rate": 2.3934378201777463e-06,
        "epoch": 1.2883475122454242,
        "step": 9995
    },
    {
        "loss": 1.0701,
        "grad_norm": 2.770181894302368,
        "learning_rate": 2.380224557947075e-06,
        "epoch": 1.288476411446249,
        "step": 9996
    },
    {
        "loss": 1.2856,
        "grad_norm": 2.1500654220581055,
        "learning_rate": 2.36704743014049e-06,
        "epoch": 1.288605310647074,
        "step": 9997
    },
    {
        "loss": 1.6154,
        "grad_norm": 2.438278913497925,
        "learning_rate": 2.353906441635623e-06,
        "epoch": 1.2887342098478989,
        "step": 9998
    },
    {
        "loss": 1.7428,
        "grad_norm": 1.8672051429748535,
        "learning_rate": 2.3408015972966715e-06,
        "epoch": 1.288863109048724,
        "step": 9999
    },
    {
        "loss": 1.0755,
        "grad_norm": 2.848388195037842,
        "learning_rate": 2.3277329019744555e-06,
        "epoch": 1.288992008249549,
        "step": 10000
    },
    {
        "loss": 1.6015,
        "grad_norm": 3.2130043506622314,
        "learning_rate": 2.3147003605063945e-06,
        "epoch": 1.2891209074503738,
        "step": 10001
    },
    {
        "loss": 0.8858,
        "grad_norm": 2.03206729888916,
        "learning_rate": 2.301703977716596e-06,
        "epoch": 1.2892498066511988,
        "step": 10002
    },
    {
        "loss": 1.8501,
        "grad_norm": 3.222193717956543,
        "learning_rate": 2.2887437584156678e-06,
        "epoch": 1.2893787058520236,
        "step": 10003
    },
    {
        "loss": 1.8112,
        "grad_norm": 2.8071882724761963,
        "learning_rate": 2.2758197074009167e-06,
        "epoch": 1.2895076050528487,
        "step": 10004
    },
    {
        "loss": 1.9598,
        "grad_norm": 2.8321478366851807,
        "learning_rate": 2.2629318294562498e-06,
        "epoch": 1.2896365042536737,
        "step": 10005
    },
    {
        "loss": 0.7689,
        "grad_norm": 1.6709338426589966,
        "learning_rate": 2.2500801293521943e-06,
        "epoch": 1.2897654034544985,
        "step": 10006
    },
    {
        "loss": 1.6786,
        "grad_norm": 2.9553143978118896,
        "learning_rate": 2.2372646118458685e-06,
        "epoch": 1.2898943026553236,
        "step": 10007
    },
    {
        "loss": 1.0292,
        "grad_norm": 2.372105598449707,
        "learning_rate": 2.2244852816809437e-06,
        "epoch": 1.2900232018561484,
        "step": 10008
    },
    {
        "loss": 2.0398,
        "grad_norm": 2.4065093994140625,
        "learning_rate": 2.2117421435878028e-06,
        "epoch": 1.2901521010569734,
        "step": 10009
    },
    {
        "loss": 0.9198,
        "grad_norm": 2.8557381629943848,
        "learning_rate": 2.1990352022833614e-06,
        "epoch": 1.2902810002577985,
        "step": 10010
    },
    {
        "loss": 1.2802,
        "grad_norm": 2.093278169631958,
        "learning_rate": 2.1863644624711354e-06,
        "epoch": 1.2904098994586233,
        "step": 10011
    },
    {
        "loss": 1.6499,
        "grad_norm": 1.4983688592910767,
        "learning_rate": 2.173729928841295e-06,
        "epoch": 1.2905387986594483,
        "step": 10012
    },
    {
        "loss": 1.3445,
        "grad_norm": 1.7606827020645142,
        "learning_rate": 2.1611316060705875e-06,
        "epoch": 1.2906676978602731,
        "step": 10013
    },
    {
        "loss": 0.8123,
        "grad_norm": 2.070702314376831,
        "learning_rate": 2.1485694988222726e-06,
        "epoch": 1.2907965970610982,
        "step": 10014
    },
    {
        "loss": 1.6605,
        "grad_norm": 2.006101369857788,
        "learning_rate": 2.136043611746308e-06,
        "epoch": 1.2909254962619232,
        "step": 10015
    },
    {
        "loss": 0.7185,
        "grad_norm": 2.0562827587127686,
        "learning_rate": 2.123553949479229e-06,
        "epoch": 1.2910543954627482,
        "step": 10016
    },
    {
        "loss": 1.1877,
        "grad_norm": 2.7144336700439453,
        "learning_rate": 2.111100516644138e-06,
        "epoch": 1.291183294663573,
        "step": 10017
    },
    {
        "loss": 1.9011,
        "grad_norm": 1.4602729082107544,
        "learning_rate": 2.0986833178507026e-06,
        "epoch": 1.291312193864398,
        "step": 10018
    },
    {
        "loss": 1.456,
        "grad_norm": 2.620197057723999,
        "learning_rate": 2.086302357695258e-06,
        "epoch": 1.291441093065223,
        "step": 10019
    },
    {
        "loss": 0.9402,
        "grad_norm": 2.2245829105377197,
        "learning_rate": 2.0739576407606377e-06,
        "epoch": 1.291569992266048,
        "step": 10020
    },
    {
        "loss": 1.4789,
        "grad_norm": 2.3568332195281982,
        "learning_rate": 2.061649171616298e-06,
        "epoch": 1.291698891466873,
        "step": 10021
    },
    {
        "loss": 1.7774,
        "grad_norm": 2.6903042793273926,
        "learning_rate": 2.049376954818272e-06,
        "epoch": 1.2918277906676978,
        "step": 10022
    },
    {
        "loss": 0.5631,
        "grad_norm": 1.917797565460205,
        "learning_rate": 2.0371409949092147e-06,
        "epoch": 1.2919566898685229,
        "step": 10023
    },
    {
        "loss": 1.9238,
        "grad_norm": 2.384429931640625,
        "learning_rate": 2.024941296418292e-06,
        "epoch": 1.2920855890693477,
        "step": 10024
    },
    {
        "loss": 2.3214,
        "grad_norm": 2.348700523376465,
        "learning_rate": 2.0127778638612926e-06,
        "epoch": 1.2922144882701727,
        "step": 10025
    },
    {
        "loss": 1.2347,
        "grad_norm": 3.198389768600464,
        "learning_rate": 2.0006507017405808e-06,
        "epoch": 1.2923433874709978,
        "step": 10026
    },
    {
        "loss": 2.0151,
        "grad_norm": 2.4116625785827637,
        "learning_rate": 1.9885598145450657e-06,
        "epoch": 1.2924722866718226,
        "step": 10027
    },
    {
        "loss": 1.2573,
        "grad_norm": 1.9856709241867065,
        "learning_rate": 1.976505206750234e-06,
        "epoch": 1.2926011858726476,
        "step": 10028
    },
    {
        "loss": 0.9201,
        "grad_norm": 1.5242984294891357,
        "learning_rate": 1.964486882818173e-06,
        "epoch": 1.2927300850734724,
        "step": 10029
    },
    {
        "loss": 1.9334,
        "grad_norm": 2.42181658744812,
        "learning_rate": 1.952504847197567e-06,
        "epoch": 1.2928589842742975,
        "step": 10030
    },
    {
        "loss": 2.01,
        "grad_norm": 2.1127214431762695,
        "learning_rate": 1.940559104323547e-06,
        "epoch": 1.2929878834751225,
        "step": 10031
    },
    {
        "loss": 1.1033,
        "grad_norm": 1.946540355682373,
        "learning_rate": 1.9286496586178982e-06,
        "epoch": 1.2931167826759475,
        "step": 10032
    },
    {
        "loss": 1.4587,
        "grad_norm": 1.8993347883224487,
        "learning_rate": 1.916776514489005e-06,
        "epoch": 1.2932456818767724,
        "step": 10033
    },
    {
        "loss": 0.6692,
        "grad_norm": 2.258527994155884,
        "learning_rate": 1.9049396763317406e-06,
        "epoch": 1.2933745810775974,
        "step": 10034
    },
    {
        "loss": 1.5426,
        "grad_norm": 2.4797096252441406,
        "learning_rate": 1.8931391485275563e-06,
        "epoch": 1.2935034802784222,
        "step": 10035
    },
    {
        "loss": 2.0647,
        "grad_norm": 1.6716314554214478,
        "learning_rate": 1.881374935444513e-06,
        "epoch": 1.2936323794792473,
        "step": 10036
    },
    {
        "loss": 1.4878,
        "grad_norm": 4.551547050476074,
        "learning_rate": 1.8696470414371502e-06,
        "epoch": 1.2937612786800723,
        "step": 10037
    },
    {
        "loss": 1.3589,
        "grad_norm": 1.8939250707626343,
        "learning_rate": 1.8579554708465951e-06,
        "epoch": 1.293890177880897,
        "step": 10038
    },
    {
        "loss": 0.9726,
        "grad_norm": 2.372407913208008,
        "learning_rate": 1.8463002280005747e-06,
        "epoch": 1.2940190770817221,
        "step": 10039
    },
    {
        "loss": 1.9189,
        "grad_norm": 1.7386788129806519,
        "learning_rate": 1.834681317213316e-06,
        "epoch": 1.294147976282547,
        "step": 10040
    },
    {
        "loss": 1.3476,
        "grad_norm": 2.0269815921783447,
        "learning_rate": 1.8230987427856227e-06,
        "epoch": 1.294276875483372,
        "step": 10041
    },
    {
        "loss": 1.418,
        "grad_norm": 2.887514114379883,
        "learning_rate": 1.8115525090048325e-06,
        "epoch": 1.294405774684197,
        "step": 10042
    },
    {
        "loss": 1.4639,
        "grad_norm": 3.2924959659576416,
        "learning_rate": 1.8000426201448373e-06,
        "epoch": 1.2945346738850219,
        "step": 10043
    },
    {
        "loss": 0.7911,
        "grad_norm": 2.4877986907958984,
        "learning_rate": 1.7885690804660737e-06,
        "epoch": 1.294663573085847,
        "step": 10044
    },
    {
        "loss": 1.5899,
        "grad_norm": 1.740478754043579,
        "learning_rate": 1.7771318942155224e-06,
        "epoch": 1.2947924722866717,
        "step": 10045
    },
    {
        "loss": 1.5199,
        "grad_norm": 1.5916637182235718,
        "learning_rate": 1.76573106562673e-06,
        "epoch": 1.2949213714874968,
        "step": 10046
    },
    {
        "loss": 1.5948,
        "grad_norm": 2.456293821334839,
        "learning_rate": 1.7543665989197656e-06,
        "epoch": 1.2950502706883218,
        "step": 10047
    },
    {
        "loss": 1.204,
        "grad_norm": 2.632575511932373,
        "learning_rate": 1.7430384983012527e-06,
        "epoch": 1.2951791698891466,
        "step": 10048
    },
    {
        "loss": 1.4559,
        "grad_norm": 2.8989791870117188,
        "learning_rate": 1.731746767964293e-06,
        "epoch": 1.2953080690899716,
        "step": 10049
    },
    {
        "loss": 0.7922,
        "grad_norm": 2.7171669006347656,
        "learning_rate": 1.7204914120886094e-06,
        "epoch": 1.2954369682907965,
        "step": 10050
    },
    {
        "loss": 1.4691,
        "grad_norm": 1.8546338081359863,
        "learning_rate": 1.7092724348404365e-06,
        "epoch": 1.2955658674916215,
        "step": 10051
    },
    {
        "loss": 1.1181,
        "grad_norm": 2.4845616817474365,
        "learning_rate": 1.6980898403724854e-06,
        "epoch": 1.2956947666924465,
        "step": 10052
    },
    {
        "loss": 1.9592,
        "grad_norm": 1.5922722816467285,
        "learning_rate": 1.6869436328241118e-06,
        "epoch": 1.2958236658932716,
        "step": 10053
    },
    {
        "loss": 1.1161,
        "grad_norm": 2.404628038406372,
        "learning_rate": 1.675833816321093e-06,
        "epoch": 1.2959525650940964,
        "step": 10054
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.5412832498550415,
        "learning_rate": 1.6647603949757729e-06,
        "epoch": 1.2960814642949214,
        "step": 10055
    },
    {
        "loss": 1.89,
        "grad_norm": 1.7376705408096313,
        "learning_rate": 1.6537233728870504e-06,
        "epoch": 1.2962103634957463,
        "step": 10056
    },
    {
        "loss": 1.0181,
        "grad_norm": 3.470816135406494,
        "learning_rate": 1.6427227541403357e-06,
        "epoch": 1.2963392626965713,
        "step": 10057
    },
    {
        "loss": 1.4926,
        "grad_norm": 2.353195905685425,
        "learning_rate": 1.6317585428075711e-06,
        "epoch": 1.2964681618973963,
        "step": 10058
    },
    {
        "loss": 1.797,
        "grad_norm": 3.4240925312042236,
        "learning_rate": 1.620830742947177e-06,
        "epoch": 1.2965970610982211,
        "step": 10059
    },
    {
        "loss": 1.535,
        "grad_norm": 2.1224498748779297,
        "learning_rate": 1.6099393586041622e-06,
        "epoch": 1.2967259602990462,
        "step": 10060
    },
    {
        "loss": 1.1162,
        "grad_norm": 3.1409595012664795,
        "learning_rate": 1.599084393810002e-06,
        "epoch": 1.296854859499871,
        "step": 10061
    },
    {
        "loss": 0.7136,
        "grad_norm": 2.6332828998565674,
        "learning_rate": 1.5882658525827154e-06,
        "epoch": 1.296983758700696,
        "step": 10062
    },
    {
        "loss": 1.1208,
        "grad_norm": 2.133971691131592,
        "learning_rate": 1.5774837389268326e-06,
        "epoch": 1.297112657901521,
        "step": 10063
    },
    {
        "loss": 1.3835,
        "grad_norm": 2.5467653274536133,
        "learning_rate": 1.5667380568334389e-06,
        "epoch": 1.297241557102346,
        "step": 10064
    },
    {
        "loss": 2.3533,
        "grad_norm": 2.83984637260437,
        "learning_rate": 1.5560288102800858e-06,
        "epoch": 1.297370456303171,
        "step": 10065
    },
    {
        "loss": 1.5257,
        "grad_norm": 2.8625996112823486,
        "learning_rate": 1.5453560032308023e-06,
        "epoch": 1.2974993555039958,
        "step": 10066
    },
    {
        "loss": 1.5622,
        "grad_norm": 2.980759382247925,
        "learning_rate": 1.5347196396362506e-06,
        "epoch": 1.2976282547048208,
        "step": 10067
    },
    {
        "loss": 0.7725,
        "grad_norm": 2.963871955871582,
        "learning_rate": 1.5241197234334814e-06,
        "epoch": 1.2977571539056458,
        "step": 10068
    },
    {
        "loss": 1.9757,
        "grad_norm": 2.339829921722412,
        "learning_rate": 1.5135562585461227e-06,
        "epoch": 1.2978860531064709,
        "step": 10069
    },
    {
        "loss": 1.9433,
        "grad_norm": 2.422698974609375,
        "learning_rate": 1.5030292488843022e-06,
        "epoch": 1.2980149523072957,
        "step": 10070
    },
    {
        "loss": 0.7829,
        "grad_norm": 2.369840145111084,
        "learning_rate": 1.4925386983446254e-06,
        "epoch": 1.2981438515081207,
        "step": 10071
    },
    {
        "loss": 0.5906,
        "grad_norm": 2.2189135551452637,
        "learning_rate": 1.4820846108102082e-06,
        "epoch": 1.2982727507089455,
        "step": 10072
    },
    {
        "loss": 1.3001,
        "grad_norm": 4.833611488342285,
        "learning_rate": 1.4716669901507108e-06,
        "epoch": 1.2984016499097706,
        "step": 10073
    },
    {
        "loss": 1.2018,
        "grad_norm": 2.9568989276885986,
        "learning_rate": 1.4612858402222373e-06,
        "epoch": 1.2985305491105956,
        "step": 10074
    },
    {
        "loss": 1.3503,
        "grad_norm": 1.7850289344787598,
        "learning_rate": 1.4509411648674587e-06,
        "epoch": 1.2986594483114204,
        "step": 10075
    },
    {
        "loss": 1.6727,
        "grad_norm": 2.9853551387786865,
        "learning_rate": 1.4406329679154674e-06,
        "epoch": 1.2987883475122455,
        "step": 10076
    },
    {
        "loss": 1.3635,
        "grad_norm": 2.302267074584961,
        "learning_rate": 1.430361253181911e-06,
        "epoch": 1.2989172467130703,
        "step": 10077
    },
    {
        "loss": 1.2634,
        "grad_norm": 3.093257427215576,
        "learning_rate": 1.420126024468904e-06,
        "epoch": 1.2990461459138953,
        "step": 10078
    },
    {
        "loss": 1.9422,
        "grad_norm": 1.7523828744888306,
        "learning_rate": 1.4099272855650491e-06,
        "epoch": 1.2991750451147204,
        "step": 10079
    },
    {
        "loss": 1.2415,
        "grad_norm": 2.821117401123047,
        "learning_rate": 1.399765040245482e-06,
        "epoch": 1.2993039443155452,
        "step": 10080
    },
    {
        "loss": 2.4277,
        "grad_norm": 1.815407395362854,
        "learning_rate": 1.389639292271827e-06,
        "epoch": 1.2994328435163702,
        "step": 10081
    },
    {
        "loss": 1.9558,
        "grad_norm": 3.5006446838378906,
        "learning_rate": 1.3795500453921418e-06,
        "epoch": 1.299561742717195,
        "step": 10082
    },
    {
        "loss": 1.7702,
        "grad_norm": 1.3799254894256592,
        "learning_rate": 1.3694973033410163e-06,
        "epoch": 1.29969064191802,
        "step": 10083
    },
    {
        "loss": 1.905,
        "grad_norm": 2.361116886138916,
        "learning_rate": 1.35948106983953e-06,
        "epoch": 1.2998195411188451,
        "step": 10084
    },
    {
        "loss": 2.1259,
        "grad_norm": 2.102739095687866,
        "learning_rate": 1.3495013485952168e-06,
        "epoch": 1.29994844031967,
        "step": 10085
    },
    {
        "loss": 0.4413,
        "grad_norm": 2.2827370166778564,
        "learning_rate": 1.3395581433021442e-06,
        "epoch": 1.300077339520495,
        "step": 10086
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.9010599851608276,
        "learning_rate": 1.3296514576408236e-06,
        "epoch": 1.3002062387213198,
        "step": 10087
    },
    {
        "loss": 1.9005,
        "grad_norm": 2.6533656120300293,
        "learning_rate": 1.3197812952782885e-06,
        "epoch": 1.3003351379221448,
        "step": 10088
    },
    {
        "loss": 0.6479,
        "grad_norm": 1.9984829425811768,
        "learning_rate": 1.3099476598679715e-06,
        "epoch": 1.3004640371229699,
        "step": 10089
    },
    {
        "loss": 1.837,
        "grad_norm": 1.1467621326446533,
        "learning_rate": 1.3001505550498838e-06,
        "epoch": 1.300592936323795,
        "step": 10090
    },
    {
        "loss": 1.5348,
        "grad_norm": 3.0568668842315674,
        "learning_rate": 1.2903899844504575e-06,
        "epoch": 1.3007218355246197,
        "step": 10091
    },
    {
        "loss": 1.9318,
        "grad_norm": 3.883544921875,
        "learning_rate": 1.2806659516826246e-06,
        "epoch": 1.3008507347254448,
        "step": 10092
    },
    {
        "loss": 0.9102,
        "grad_norm": 2.4100916385650635,
        "learning_rate": 1.270978460345762e-06,
        "epoch": 1.3009796339262696,
        "step": 10093
    },
    {
        "loss": 2.0241,
        "grad_norm": 2.416632652282715,
        "learning_rate": 1.2613275140257563e-06,
        "epoch": 1.3011085331270946,
        "step": 10094
    },
    {
        "loss": 1.8872,
        "grad_norm": 2.2930517196655273,
        "learning_rate": 1.2517131162949502e-06,
        "epoch": 1.3012374323279197,
        "step": 10095
    },
    {
        "loss": 1.9351,
        "grad_norm": 2.1691970825195312,
        "learning_rate": 1.2421352707121302e-06,
        "epoch": 1.3013663315287445,
        "step": 10096
    },
    {
        "loss": 1.9211,
        "grad_norm": 2.639047622680664,
        "learning_rate": 1.2325939808226162e-06,
        "epoch": 1.3014952307295695,
        "step": 10097
    },
    {
        "loss": 1.2077,
        "grad_norm": 2.2208056449890137,
        "learning_rate": 1.2230892501581603e-06,
        "epoch": 1.3016241299303943,
        "step": 10098
    },
    {
        "loss": 1.8315,
        "grad_norm": 1.7480649948120117,
        "learning_rate": 1.213621082236982e-06,
        "epoch": 1.3017530291312194,
        "step": 10099
    },
    {
        "loss": 0.709,
        "grad_norm": 2.122055768966675,
        "learning_rate": 1.2041894805637443e-06,
        "epoch": 1.3018819283320444,
        "step": 10100
    },
    {
        "loss": 1.3012,
        "grad_norm": 2.069401502609253,
        "learning_rate": 1.1947944486296326e-06,
        "epoch": 1.3020108275328692,
        "step": 10101
    },
    {
        "loss": 2.1002,
        "grad_norm": 2.583994150161743,
        "learning_rate": 1.1854359899122313e-06,
        "epoch": 1.3021397267336943,
        "step": 10102
    },
    {
        "loss": 2.069,
        "grad_norm": 3.147033214569092,
        "learning_rate": 1.1761141078756365e-06,
        "epoch": 1.302268625934519,
        "step": 10103
    },
    {
        "loss": 1.5647,
        "grad_norm": 1.6500517129898071,
        "learning_rate": 1.166828805970377e-06,
        "epoch": 1.3023975251353441,
        "step": 10104
    },
    {
        "loss": 2.2057,
        "grad_norm": 2.1050052642822266,
        "learning_rate": 1.157580087633492e-06,
        "epoch": 1.3025264243361692,
        "step": 10105
    },
    {
        "loss": 1.2015,
        "grad_norm": 3.3523664474487305,
        "learning_rate": 1.1483679562883653e-06,
        "epoch": 1.3026553235369942,
        "step": 10106
    },
    {
        "loss": 2.3517,
        "grad_norm": 2.5622124671936035,
        "learning_rate": 1.1391924153449696e-06,
        "epoch": 1.302784222737819,
        "step": 10107
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.4807097911834717,
        "learning_rate": 1.1300534681996543e-06,
        "epoch": 1.302913121938644,
        "step": 10108
    },
    {
        "loss": 0.9948,
        "grad_norm": 3.549715280532837,
        "learning_rate": 1.1209511182352472e-06,
        "epoch": 1.3030420211394689,
        "step": 10109
    },
    {
        "loss": 1.8067,
        "grad_norm": 2.9544448852539062,
        "learning_rate": 1.1118853688210195e-06,
        "epoch": 1.303170920340294,
        "step": 10110
    },
    {
        "loss": 1.8462,
        "grad_norm": 2.8592545986175537,
        "learning_rate": 1.1028562233127315e-06,
        "epoch": 1.303299819541119,
        "step": 10111
    },
    {
        "loss": 1.9229,
        "grad_norm": 1.2801059484481812,
        "learning_rate": 1.0938636850525318e-06,
        "epoch": 1.3034287187419438,
        "step": 10112
    },
    {
        "loss": 1.2348,
        "grad_norm": 2.788126230239868,
        "learning_rate": 1.0849077573690358e-06,
        "epoch": 1.3035576179427688,
        "step": 10113
    },
    {
        "loss": 1.7022,
        "grad_norm": 2.3489341735839844,
        "learning_rate": 1.0759884435773583e-06,
        "epoch": 1.3036865171435936,
        "step": 10114
    },
    {
        "loss": 0.7395,
        "grad_norm": 2.457252025604248,
        "learning_rate": 1.0671057469790246e-06,
        "epoch": 1.3038154163444187,
        "step": 10115
    },
    {
        "loss": 1.2985,
        "grad_norm": 2.7018589973449707,
        "learning_rate": 1.058259670861994e-06,
        "epoch": 1.3039443155452437,
        "step": 10116
    },
    {
        "loss": 1.7445,
        "grad_norm": 2.1515848636627197,
        "learning_rate": 1.0494502185006804e-06,
        "epoch": 1.3040732147460685,
        "step": 10117
    },
    {
        "loss": 1.7145,
        "grad_norm": 2.563657522201538,
        "learning_rate": 1.0406773931559533e-06,
        "epoch": 1.3042021139468936,
        "step": 10118
    },
    {
        "loss": 1.3224,
        "grad_norm": 1.7882574796676636,
        "learning_rate": 1.0319411980751148e-06,
        "epoch": 1.3043310131477184,
        "step": 10119
    },
    {
        "loss": 1.6266,
        "grad_norm": 2.4363653659820557,
        "learning_rate": 1.0232416364919007e-06,
        "epoch": 1.3044599123485434,
        "step": 10120
    },
    {
        "loss": 1.6208,
        "grad_norm": 2.270066022872925,
        "learning_rate": 1.0145787116265015e-06,
        "epoch": 1.3045888115493685,
        "step": 10121
    },
    {
        "loss": 2.0763,
        "grad_norm": 3.7293002605438232,
        "learning_rate": 1.0059524266855635e-06,
        "epoch": 1.3047177107501933,
        "step": 10122
    },
    {
        "loss": 1.8682,
        "grad_norm": 1.6914870738983154,
        "learning_rate": 9.973627848620992e-07,
        "epoch": 1.3048466099510183,
        "step": 10123
    },
    {
        "loss": 1.3787,
        "grad_norm": 5.331116676330566,
        "learning_rate": 9.88809789335632e-07,
        "epoch": 1.3049755091518431,
        "step": 10124
    },
    {
        "loss": 1.9099,
        "grad_norm": 2.6698598861694336,
        "learning_rate": 9.802934432720956e-07,
        "epoch": 1.3051044083526682,
        "step": 10125
    },
    {
        "loss": 1.9401,
        "grad_norm": 2.1439003944396973,
        "learning_rate": 9.718137498238577e-07,
        "epoch": 1.3052333075534932,
        "step": 10126
    },
    {
        "loss": 1.5728,
        "grad_norm": 2.1573824882507324,
        "learning_rate": 9.633707121296964e-07,
        "epoch": 1.3053622067543182,
        "step": 10127
    },
    {
        "loss": 1.014,
        "grad_norm": 2.649937391281128,
        "learning_rate": 9.549643333148784e-07,
        "epoch": 1.305491105955143,
        "step": 10128
    },
    {
        "loss": 1.3801,
        "grad_norm": 2.757483720779419,
        "learning_rate": 9.465946164910367e-07,
        "epoch": 1.305620005155968,
        "step": 10129
    },
    {
        "loss": 1.982,
        "grad_norm": 1.6021416187286377,
        "learning_rate": 9.382615647562598e-07,
        "epoch": 1.305748904356793,
        "step": 10130
    },
    {
        "loss": 1.9014,
        "grad_norm": 1.7891006469726562,
        "learning_rate": 9.299651811950694e-07,
        "epoch": 1.305877803557618,
        "step": 10131
    },
    {
        "loss": 1.4204,
        "grad_norm": 2.38114070892334,
        "learning_rate": 9.21705468878431e-07,
        "epoch": 1.306006702758443,
        "step": 10132
    },
    {
        "loss": 1.5998,
        "grad_norm": 2.311244487762451,
        "learning_rate": 9.134824308637102e-07,
        "epoch": 1.3061356019592678,
        "step": 10133
    },
    {
        "loss": 1.2117,
        "grad_norm": 2.382709503173828,
        "learning_rate": 9.052960701946833e-07,
        "epoch": 1.3062645011600929,
        "step": 10134
    },
    {
        "loss": 1.5646,
        "grad_norm": 3.2103002071380615,
        "learning_rate": 8.971463899015931e-07,
        "epoch": 1.3063934003609177,
        "step": 10135
    },
    {
        "loss": 1.9838,
        "grad_norm": 1.7844289541244507,
        "learning_rate": 8.890333930010708e-07,
        "epoch": 1.3065222995617427,
        "step": 10136
    },
    {
        "loss": 1.0788,
        "grad_norm": 2.4111721515655518,
        "learning_rate": 8.8095708249617e-07,
        "epoch": 1.3066511987625677,
        "step": 10137
    },
    {
        "loss": 0.9533,
        "grad_norm": 4.297857761383057,
        "learning_rate": 8.729174613763991e-07,
        "epoch": 1.3067800979633926,
        "step": 10138
    },
    {
        "loss": 1.7723,
        "grad_norm": 1.7566838264465332,
        "learning_rate": 8.649145326176667e-07,
        "epoch": 1.3069089971642176,
        "step": 10139
    },
    {
        "loss": 1.1307,
        "grad_norm": 3.111279010772705,
        "learning_rate": 8.569482991822586e-07,
        "epoch": 1.3070378963650424,
        "step": 10140
    },
    {
        "loss": 1.4638,
        "grad_norm": 1.8840712308883667,
        "learning_rate": 8.490187640189273e-07,
        "epoch": 1.3071667955658675,
        "step": 10141
    },
    {
        "loss": 1.4405,
        "grad_norm": 2.4562015533447266,
        "learning_rate": 8.411259300628582e-07,
        "epoch": 1.3072956947666925,
        "step": 10142
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.950944423675537,
        "learning_rate": 8.332698002355921e-07,
        "epoch": 1.3074245939675175,
        "step": 10143
    },
    {
        "loss": 2.191,
        "grad_norm": 2.153499126434326,
        "learning_rate": 8.254503774451028e-07,
        "epoch": 1.3075534931683424,
        "step": 10144
    },
    {
        "loss": 2.0093,
        "grad_norm": 2.3857526779174805,
        "learning_rate": 8.176676645858306e-07,
        "epoch": 1.3076823923691674,
        "step": 10145
    },
    {
        "loss": 2.1858,
        "grad_norm": 1.3848963975906372,
        "learning_rate": 8.099216645385488e-07,
        "epoch": 1.3078112915699922,
        "step": 10146
    },
    {
        "loss": 1.009,
        "grad_norm": 2.030552864074707,
        "learning_rate": 8.022123801704751e-07,
        "epoch": 1.3079401907708172,
        "step": 10147
    },
    {
        "loss": 1.2183,
        "grad_norm": 2.551386594772339,
        "learning_rate": 7.945398143352378e-07,
        "epoch": 1.3080690899716423,
        "step": 10148
    },
    {
        "loss": 2.3029,
        "grad_norm": 1.4744964838027954,
        "learning_rate": 7.869039698728986e-07,
        "epoch": 1.308197989172467,
        "step": 10149
    },
    {
        "loss": 0.5448,
        "grad_norm": 2.996954917907715,
        "learning_rate": 7.793048496098964e-07,
        "epoch": 1.3083268883732921,
        "step": 10150
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.929111957550049,
        "learning_rate": 7.717424563590481e-07,
        "epoch": 1.308455787574117,
        "step": 10151
    },
    {
        "loss": 2.1766,
        "grad_norm": 3.0568952560424805,
        "learning_rate": 7.642167929196476e-07,
        "epoch": 1.308584686774942,
        "step": 10152
    },
    {
        "loss": 1.3584,
        "grad_norm": 2.932260036468506,
        "learning_rate": 7.567278620773332e-07,
        "epoch": 1.308713585975767,
        "step": 10153
    },
    {
        "loss": 2.1428,
        "grad_norm": 2.4040017127990723,
        "learning_rate": 7.492756666041767e-07,
        "epoch": 1.3088424851765919,
        "step": 10154
    },
    {
        "loss": 1.5287,
        "grad_norm": 1.8235396146774292,
        "learning_rate": 7.418602092586379e-07,
        "epoch": 1.308971384377417,
        "step": 10155
    },
    {
        "loss": 1.534,
        "grad_norm": 2.7741730213165283,
        "learning_rate": 7.344814927856103e-07,
        "epoch": 1.3091002835782417,
        "step": 10156
    },
    {
        "loss": 1.9157,
        "grad_norm": 1.2928210496902466,
        "learning_rate": 7.271395199163089e-07,
        "epoch": 1.3092291827790667,
        "step": 10157
    },
    {
        "loss": 1.1841,
        "grad_norm": 3.1469151973724365,
        "learning_rate": 7.198342933684266e-07,
        "epoch": 1.3093580819798918,
        "step": 10158
    },
    {
        "loss": 1.6471,
        "grad_norm": 2.073934316635132,
        "learning_rate": 7.125658158460446e-07,
        "epoch": 1.3094869811807166,
        "step": 10159
    },
    {
        "loss": 1.55,
        "grad_norm": 2.650006055831909,
        "learning_rate": 7.05334090039611e-07,
        "epoch": 1.3096158803815416,
        "step": 10160
    },
    {
        "loss": 2.1874,
        "grad_norm": 3.029625415802002,
        "learning_rate": 6.981391186259734e-07,
        "epoch": 1.3097447795823665,
        "step": 10161
    },
    {
        "loss": 1.8151,
        "grad_norm": 2.411074161529541,
        "learning_rate": 6.909809042684124e-07,
        "epoch": 1.3098736787831915,
        "step": 10162
    },
    {
        "loss": 1.1235,
        "grad_norm": 2.9770865440368652,
        "learning_rate": 6.838594496165418e-07,
        "epoch": 1.3100025779840165,
        "step": 10163
    },
    {
        "loss": 1.8218,
        "grad_norm": 2.6862871646881104,
        "learning_rate": 6.767747573064198e-07,
        "epoch": 1.3101314771848416,
        "step": 10164
    },
    {
        "loss": 0.7549,
        "grad_norm": 2.5465729236602783,
        "learning_rate": 6.697268299604709e-07,
        "epoch": 1.3102603763856664,
        "step": 10165
    },
    {
        "loss": 1.5917,
        "grad_norm": 3.2018141746520996,
        "learning_rate": 6.627156701875304e-07,
        "epoch": 1.3103892755864914,
        "step": 10166
    },
    {
        "loss": 1.4799,
        "grad_norm": 1.7624526023864746,
        "learning_rate": 6.557412805828e-07,
        "epoch": 1.3105181747873162,
        "step": 10167
    },
    {
        "loss": 1.576,
        "grad_norm": 1.9712486267089844,
        "learning_rate": 6.488036637278927e-07,
        "epoch": 1.3106470739881413,
        "step": 10168
    },
    {
        "loss": 2.0878,
        "grad_norm": 2.1007559299468994,
        "learning_rate": 6.419028221907985e-07,
        "epoch": 1.3107759731889663,
        "step": 10169
    },
    {
        "loss": 1.8737,
        "grad_norm": 1.9722176790237427,
        "learning_rate": 6.350387585258965e-07,
        "epoch": 1.3109048723897911,
        "step": 10170
    },
    {
        "loss": 0.654,
        "grad_norm": 3.0943546295166016,
        "learning_rate": 6.282114752739431e-07,
        "epoch": 1.3110337715906162,
        "step": 10171
    },
    {
        "loss": 1.1481,
        "grad_norm": 2.226498603820801,
        "learning_rate": 6.214209749620947e-07,
        "epoch": 1.311162670791441,
        "step": 10172
    },
    {
        "loss": 1.2408,
        "grad_norm": 2.458031415939331,
        "learning_rate": 6.146672601038961e-07,
        "epoch": 1.311291569992266,
        "step": 10173
    },
    {
        "loss": 1.4287,
        "grad_norm": 2.70279860496521,
        "learning_rate": 6.079503331992809e-07,
        "epoch": 1.311420469193091,
        "step": 10174
    },
    {
        "loss": 1.2333,
        "grad_norm": 2.7028355598449707,
        "learning_rate": 6.012701967345047e-07,
        "epoch": 1.311549368393916,
        "step": 10175
    },
    {
        "loss": 1.8549,
        "grad_norm": 2.3413925170898438,
        "learning_rate": 5.946268531822896e-07,
        "epoch": 1.311678267594741,
        "step": 10176
    },
    {
        "loss": 1.0793,
        "grad_norm": 1.7346006631851196,
        "learning_rate": 5.880203050016908e-07,
        "epoch": 1.3118071667955657,
        "step": 10177
    },
    {
        "loss": 2.1862,
        "grad_norm": 1.5385539531707764,
        "learning_rate": 5.81450554638141e-07,
        "epoch": 1.3119360659963908,
        "step": 10178
    },
    {
        "loss": 1.4399,
        "grad_norm": 1.8651752471923828,
        "learning_rate": 5.74917604523495e-07,
        "epoch": 1.3120649651972158,
        "step": 10179
    },
    {
        "loss": 1.8063,
        "grad_norm": 2.365063428878784,
        "learning_rate": 5.684214570759295e-07,
        "epoch": 1.3121938643980409,
        "step": 10180
    },
    {
        "loss": 1.1151,
        "grad_norm": 2.944544792175293,
        "learning_rate": 5.619621147000209e-07,
        "epoch": 1.3123227635988657,
        "step": 10181
    },
    {
        "loss": 1.1569,
        "grad_norm": 2.5068013668060303,
        "learning_rate": 5.555395797867347e-07,
        "epoch": 1.3124516627996907,
        "step": 10182
    },
    {
        "loss": 1.7955,
        "grad_norm": 4.1190714836120605,
        "learning_rate": 5.491538547134134e-07,
        "epoch": 1.3125805620005155,
        "step": 10183
    },
    {
        "loss": 2.377,
        "grad_norm": 1.4696215391159058,
        "learning_rate": 5.428049418437553e-07,
        "epoch": 1.3127094612013406,
        "step": 10184
    },
    {
        "loss": 1.2521,
        "grad_norm": 1.8179212808609009,
        "learning_rate": 5.36492843527836e-07,
        "epoch": 1.3128383604021656,
        "step": 10185
    },
    {
        "loss": 1.4261,
        "grad_norm": 2.0338330268859863,
        "learning_rate": 5.3021756210212e-07,
        "epoch": 1.3129672596029904,
        "step": 10186
    },
    {
        "loss": 1.2694,
        "grad_norm": 3.180885076522827,
        "learning_rate": 5.239790998894156e-07,
        "epoch": 1.3130961588038155,
        "step": 10187
    },
    {
        "loss": 1.6886,
        "grad_norm": 3.2833895683288574,
        "learning_rate": 5.177774591989315e-07,
        "epoch": 1.3132250580046403,
        "step": 10188
    },
    {
        "loss": 1.5683,
        "grad_norm": 2.628485679626465,
        "learning_rate": 5.116126423262201e-07,
        "epoch": 1.3133539572054653,
        "step": 10189
    },
    {
        "loss": 1.528,
        "grad_norm": 1.9748303890228271,
        "learning_rate": 5.054846515532452e-07,
        "epoch": 1.3134828564062904,
        "step": 10190
    },
    {
        "loss": 1.3395,
        "grad_norm": 3.0857155323028564,
        "learning_rate": 4.993934891483032e-07,
        "epoch": 1.3136117556071152,
        "step": 10191
    },
    {
        "loss": 1.5632,
        "grad_norm": 3.490356683731079,
        "learning_rate": 4.933391573660351e-07,
        "epoch": 1.3137406548079402,
        "step": 10192
    },
    {
        "loss": 1.1746,
        "grad_norm": 1.8057421445846558,
        "learning_rate": 4.87321658447526e-07,
        "epoch": 1.313869554008765,
        "step": 10193
    },
    {
        "loss": 2.1709,
        "grad_norm": 2.302168846130371,
        "learning_rate": 4.813409946201497e-07,
        "epoch": 1.31399845320959,
        "step": 10194
    },
    {
        "loss": 1.1527,
        "grad_norm": 3.612898826599121,
        "learning_rate": 4.7539716809767987e-07,
        "epoch": 1.3141273524104151,
        "step": 10195
    },
    {
        "loss": 1.3148,
        "grad_norm": 3.125763177871704,
        "learning_rate": 4.6949018108027874e-07,
        "epoch": 1.31425625161124,
        "step": 10196
    },
    {
        "loss": 2.1373,
        "grad_norm": 1.642852544784546,
        "learning_rate": 4.6362003575441957e-07,
        "epoch": 1.314385150812065,
        "step": 10197
    },
    {
        "loss": 1.9119,
        "grad_norm": 1.957679271697998,
        "learning_rate": 4.577867342929643e-07,
        "epoch": 1.3145140500128898,
        "step": 10198
    },
    {
        "loss": 1.2792,
        "grad_norm": 2.287954807281494,
        "learning_rate": 4.519902788551522e-07,
        "epoch": 1.3146429492137148,
        "step": 10199
    },
    {
        "loss": 1.2473,
        "grad_norm": 3.192249059677124,
        "learning_rate": 4.462306715865672e-07,
        "epoch": 1.3147718484145399,
        "step": 10200
    },
    {
        "loss": 1.2612,
        "grad_norm": 1.9211360216140747,
        "learning_rate": 4.405079146191371e-07,
        "epoch": 1.314900747615365,
        "step": 10201
    },
    {
        "loss": 1.2954,
        "grad_norm": 3.126883029937744,
        "learning_rate": 4.3482201007118974e-07,
        "epoch": 1.3150296468161897,
        "step": 10202
    },
    {
        "loss": 1.2092,
        "grad_norm": 2.5045595169067383,
        "learning_rate": 4.2917296004738595e-07,
        "epoch": 1.3151585460170148,
        "step": 10203
    },
    {
        "loss": 2.0233,
        "grad_norm": 2.754368543624878,
        "learning_rate": 4.23560766638742e-07,
        "epoch": 1.3152874452178396,
        "step": 10204
    },
    {
        "loss": 1.9311,
        "grad_norm": 1.5673741102218628,
        "learning_rate": 4.179854319226184e-07,
        "epoch": 1.3154163444186646,
        "step": 10205
    },
    {
        "loss": 2.1655,
        "grad_norm": 2.847648859024048,
        "learning_rate": 4.1244695796278655e-07,
        "epoch": 1.3155452436194897,
        "step": 10206
    },
    {
        "loss": 1.3355,
        "grad_norm": 1.9965271949768066,
        "learning_rate": 4.0694534680932874e-07,
        "epoch": 1.3156741428203145,
        "step": 10207
    },
    {
        "loss": 1.6366,
        "grad_norm": Infinity,
        "learning_rate": 4.0694534680932874e-07,
        "epoch": 1.3158030420211395,
        "step": 10208
    },
    {
        "loss": 2.0088,
        "grad_norm": 1.6705764532089233,
        "learning_rate": 4.0148060049869374e-07,
        "epoch": 1.3159319412219643,
        "step": 10209
    },
    {
        "loss": 2.0056,
        "grad_norm": 2.4694840908050537,
        "learning_rate": 3.9605272105366353e-07,
        "epoch": 1.3160608404227894,
        "step": 10210
    },
    {
        "loss": 1.2388,
        "grad_norm": 3.0771548748016357,
        "learning_rate": 3.9066171048340873e-07,
        "epoch": 1.3161897396236144,
        "step": 10211
    },
    {
        "loss": 2.1221,
        "grad_norm": 1.5891398191452026,
        "learning_rate": 3.8530757078344415e-07,
        "epoch": 1.3163186388244392,
        "step": 10212
    },
    {
        "loss": 1.4864,
        "grad_norm": 3.087886333465576,
        "learning_rate": 3.7999030393561787e-07,
        "epoch": 1.3164475380252643,
        "step": 10213
    },
    {
        "loss": 1.325,
        "grad_norm": 2.274995803833008,
        "learning_rate": 3.747099119081443e-07,
        "epoch": 1.316576437226089,
        "step": 10214
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.6051185131073,
        "learning_rate": 3.6946639665560444e-07,
        "epoch": 1.3167053364269141,
        "step": 10215
    },
    {
        "loss": 1.278,
        "grad_norm": 1.6975640058517456,
        "learning_rate": 3.642597601188791e-07,
        "epoch": 1.3168342356277392,
        "step": 10216
    },
    {
        "loss": 1.2972,
        "grad_norm": 4.018031120300293,
        "learning_rate": 3.5909000422524875e-07,
        "epoch": 1.3169631348285642,
        "step": 10217
    },
    {
        "loss": 1.5559,
        "grad_norm": 2.567798376083374,
        "learning_rate": 3.5395713088830494e-07,
        "epoch": 1.317092034029389,
        "step": 10218
    },
    {
        "loss": 0.9611,
        "grad_norm": 2.397648811340332,
        "learning_rate": 3.48861142008039e-07,
        "epoch": 1.317220933230214,
        "step": 10219
    },
    {
        "loss": 1.5563,
        "grad_norm": 2.0559301376342773,
        "learning_rate": 3.4380203947073086e-07,
        "epoch": 1.3173498324310389,
        "step": 10220
    },
    {
        "loss": 1.1264,
        "grad_norm": 2.832371234893799,
        "learning_rate": 3.3877982514902705e-07,
        "epoch": 1.317478731631864,
        "step": 10221
    },
    {
        "loss": 0.8357,
        "grad_norm": 2.4095921516418457,
        "learning_rate": 3.337945009019405e-07,
        "epoch": 1.317607630832689,
        "step": 10222
    },
    {
        "loss": 1.8219,
        "grad_norm": 1.9154232740402222,
        "learning_rate": 3.2884606857479514e-07,
        "epoch": 1.3177365300335138,
        "step": 10223
    },
    {
        "loss": 1.6079,
        "grad_norm": 1.964020013809204,
        "learning_rate": 3.239345299992924e-07,
        "epoch": 1.3178654292343388,
        "step": 10224
    },
    {
        "loss": 2.1506,
        "grad_norm": 2.539123773574829,
        "learning_rate": 3.190598869934558e-07,
        "epoch": 1.3179943284351636,
        "step": 10225
    },
    {
        "loss": 1.6121,
        "grad_norm": 2.0527074337005615,
        "learning_rate": 3.142221413616753e-07,
        "epoch": 1.3181232276359887,
        "step": 10226
    },
    {
        "loss": 2.3326,
        "grad_norm": 3.5743179321289062,
        "learning_rate": 3.094212948946185e-07,
        "epoch": 1.3182521268368137,
        "step": 10227
    },
    {
        "loss": 1.6752,
        "grad_norm": 1.983881950378418,
        "learning_rate": 3.0465734936938603e-07,
        "epoch": 1.3183810260376385,
        "step": 10228
    },
    {
        "loss": 1.1971,
        "grad_norm": 1.9461170434951782,
        "learning_rate": 2.999303065493564e-07,
        "epoch": 1.3185099252384636,
        "step": 10229
    },
    {
        "loss": 2.0274,
        "grad_norm": 2.0465481281280518,
        "learning_rate": 2.9524016818427426e-07,
        "epoch": 1.3186388244392884,
        "step": 10230
    },
    {
        "loss": 0.964,
        "grad_norm": 2.586244821548462,
        "learning_rate": 2.905869360102065e-07,
        "epoch": 1.3187677236401134,
        "step": 10231
    },
    {
        "loss": 0.966,
        "grad_norm": 2.310500144958496,
        "learning_rate": 2.859706117495864e-07,
        "epoch": 1.3188966228409384,
        "step": 10232
    },
    {
        "loss": 2.0106,
        "grad_norm": 3.6482667922973633,
        "learning_rate": 2.8139119711114714e-07,
        "epoch": 1.3190255220417633,
        "step": 10233
    },
    {
        "loss": 1.9843,
        "grad_norm": 1.7280734777450562,
        "learning_rate": 2.7684869378999943e-07,
        "epoch": 1.3191544212425883,
        "step": 10234
    },
    {
        "loss": 1.762,
        "grad_norm": 3.1493330001831055,
        "learning_rate": 2.7234310346755387e-07,
        "epoch": 1.3192833204434131,
        "step": 10235
    },
    {
        "loss": 0.6571,
        "grad_norm": 2.298595905303955,
        "learning_rate": 2.6787442781158747e-07,
        "epoch": 1.3194122196442382,
        "step": 10236
    },
    {
        "loss": 1.9311,
        "grad_norm": 2.4119954109191895,
        "learning_rate": 2.6344266847619925e-07,
        "epoch": 1.3195411188450632,
        "step": 10237
    },
    {
        "loss": 1.724,
        "grad_norm": 2.3447277545928955,
        "learning_rate": 2.5904782710184373e-07,
        "epoch": 1.3196700180458882,
        "step": 10238
    },
    {
        "loss": 1.9642,
        "grad_norm": 1.383378028869629,
        "learning_rate": 2.5468990531526406e-07,
        "epoch": 1.319798917246713,
        "step": 10239
    },
    {
        "loss": 0.7337,
        "grad_norm": 2.0836095809936523,
        "learning_rate": 2.5036890472958096e-07,
        "epoch": 1.319927816447538,
        "step": 10240
    },
    {
        "loss": 2.1485,
        "grad_norm": 2.060776948928833,
        "learning_rate": 2.460848269442262e-07,
        "epoch": 1.320056715648363,
        "step": 10241
    },
    {
        "loss": 1.1136,
        "grad_norm": 2.8957679271698,
        "learning_rate": 2.418376735449757e-07,
        "epoch": 1.320185614849188,
        "step": 10242
    },
    {
        "loss": 1.4511,
        "grad_norm": 1.6626383066177368,
        "learning_rate": 2.376274461039385e-07,
        "epoch": 1.320314514050013,
        "step": 10243
    },
    {
        "loss": 1.568,
        "grad_norm": 1.9904284477233887,
        "learning_rate": 2.3345414617953477e-07,
        "epoch": 1.3204434132508378,
        "step": 10244
    },
    {
        "loss": 0.4252,
        "grad_norm": 2.810062885284424,
        "learning_rate": 2.2931777531655098e-07,
        "epoch": 1.3205723124516628,
        "step": 10245
    },
    {
        "loss": 1.5801,
        "grad_norm": 2.5267341136932373,
        "learning_rate": 2.2521833504607348e-07,
        "epoch": 1.3207012116524877,
        "step": 10246
    },
    {
        "loss": 1.5328,
        "grad_norm": 2.8680567741394043,
        "learning_rate": 2.2115582688553293e-07,
        "epoch": 1.3208301108533127,
        "step": 10247
    },
    {
        "loss": 1.5192,
        "grad_norm": 2.78735613822937,
        "learning_rate": 2.171302523386709e-07,
        "epoch": 1.3209590100541377,
        "step": 10248
    },
    {
        "loss": 1.6437,
        "grad_norm": 2.9161996841430664,
        "learning_rate": 2.1314161289560651e-07,
        "epoch": 1.3210879092549626,
        "step": 10249
    },
    {
        "loss": 1.1977,
        "grad_norm": 2.0927562713623047,
        "learning_rate": 2.091899100327255e-07,
        "epoch": 1.3212168084557876,
        "step": 10250
    },
    {
        "loss": 1.3322,
        "grad_norm": 2.2942864894866943,
        "learning_rate": 2.0527514521276881e-07,
        "epoch": 1.3213457076566124,
        "step": 10251
    },
    {
        "loss": 1.4907,
        "grad_norm": 2.380282402038574,
        "learning_rate": 2.013973198848218e-07,
        "epoch": 1.3214746068574375,
        "step": 10252
    },
    {
        "loss": 0.4989,
        "grad_norm": 1.2653639316558838,
        "learning_rate": 1.975564354842696e-07,
        "epoch": 1.3216035060582625,
        "step": 10253
    },
    {
        "loss": 1.7237,
        "grad_norm": 1.917484998703003,
        "learning_rate": 1.9375249343283052e-07,
        "epoch": 1.3217324052590875,
        "step": 10254
    },
    {
        "loss": 1.3784,
        "grad_norm": 3.4514310359954834,
        "learning_rate": 1.8998549513857823e-07,
        "epoch": 1.3218613044599123,
        "step": 10255
    },
    {
        "loss": 1.8106,
        "grad_norm": 2.3242783546447754,
        "learning_rate": 1.8625544199585288e-07,
        "epoch": 1.3219902036607374,
        "step": 10256
    },
    {
        "loss": 1.7665,
        "grad_norm": 2.604989767074585,
        "learning_rate": 1.8256233538535006e-07,
        "epoch": 1.3221191028615622,
        "step": 10257
    },
    {
        "loss": 1.388,
        "grad_norm": 2.3308873176574707,
        "learning_rate": 1.7890617667412068e-07,
        "epoch": 1.3222480020623872,
        "step": 10258
    },
    {
        "loss": 2.0124,
        "grad_norm": 2.6480419635772705,
        "learning_rate": 1.752869672154822e-07,
        "epoch": 1.3223769012632123,
        "step": 10259
    },
    {
        "loss": 0.6583,
        "grad_norm": 2.5537075996398926,
        "learning_rate": 1.7170470834911857e-07,
        "epoch": 1.322505800464037,
        "step": 10260
    },
    {
        "loss": 1.8125,
        "grad_norm": 2.0597267150878906,
        "learning_rate": 1.681594014010135e-07,
        "epoch": 1.3226346996648621,
        "step": 10261
    },
    {
        "loss": 2.4302,
        "grad_norm": 2.3452508449554443,
        "learning_rate": 1.6465104768347284e-07,
        "epoch": 1.322763598865687,
        "step": 10262
    },
    {
        "loss": 1.5817,
        "grad_norm": 2.256645441055298,
        "learning_rate": 1.6117964849514667e-07,
        "epoch": 1.322892498066512,
        "step": 10263
    },
    {
        "loss": 0.9501,
        "grad_norm": 2.8222885131835938,
        "learning_rate": 1.577452051209738e-07,
        "epoch": 1.323021397267337,
        "step": 10264
    },
    {
        "loss": 1.4088,
        "grad_norm": 1.7107211351394653,
        "learning_rate": 1.543477188322262e-07,
        "epoch": 1.3231502964681618,
        "step": 10265
    },
    {
        "loss": 1.7946,
        "grad_norm": 2.107915163040161,
        "learning_rate": 1.5098719088653123e-07,
        "epoch": 1.3232791956689869,
        "step": 10266
    },
    {
        "loss": 0.9129,
        "grad_norm": 1.7369599342346191,
        "learning_rate": 1.4766362252776055e-07,
        "epoch": 1.3234080948698117,
        "step": 10267
    },
    {
        "loss": 2.2286,
        "grad_norm": 2.559436559677124,
        "learning_rate": 1.443770149861856e-07,
        "epoch": 1.3235369940706367,
        "step": 10268
    },
    {
        "loss": 1.5119,
        "grad_norm": 2.4883804321289062,
        "learning_rate": 1.4112736947833328e-07,
        "epoch": 1.3236658932714618,
        "step": 10269
    },
    {
        "loss": 1.3048,
        "grad_norm": 1.6307648420333862,
        "learning_rate": 1.3791468720709689e-07,
        "epoch": 1.3237947924722866,
        "step": 10270
    },
    {
        "loss": 1.8033,
        "grad_norm": 1.8631402254104614,
        "learning_rate": 1.3473896936165852e-07,
        "epoch": 1.3239236916731116,
        "step": 10271
    },
    {
        "loss": 2.4144,
        "grad_norm": 1.2684468030929565,
        "learning_rate": 1.3160021711752236e-07,
        "epoch": 1.3240525908739365,
        "step": 10272
    },
    {
        "loss": 1.6413,
        "grad_norm": 3.390209913253784,
        "learning_rate": 1.2849843163650343e-07,
        "epoch": 1.3241814900747615,
        "step": 10273
    },
    {
        "loss": 1.123,
        "grad_norm": 2.0795681476593018,
        "learning_rate": 1.2543361406676114e-07,
        "epoch": 1.3243103892755865,
        "step": 10274
    },
    {
        "loss": 1.5511,
        "grad_norm": 2.9601492881774902,
        "learning_rate": 1.2240576554273243e-07,
        "epoch": 1.3244392884764116,
        "step": 10275
    },
    {
        "loss": 2.1496,
        "grad_norm": 2.476628065109253,
        "learning_rate": 1.1941488718520966e-07,
        "epoch": 1.3245681876772364,
        "step": 10276
    },
    {
        "loss": 2.0561,
        "grad_norm": 2.2961692810058594,
        "learning_rate": 1.1646098010126284e-07,
        "epoch": 1.3246970868780614,
        "step": 10277
    },
    {
        "loss": 1.2664,
        "grad_norm": 3.183413028717041,
        "learning_rate": 1.1354404538431729e-07,
        "epoch": 1.3248259860788862,
        "step": 10278
    },
    {
        "loss": 2.0396,
        "grad_norm": 2.279454469680786,
        "learning_rate": 1.1066408411406493e-07,
        "epoch": 1.3249548852797113,
        "step": 10279
    },
    {
        "loss": 1.811,
        "grad_norm": 2.093775749206543,
        "learning_rate": 1.0782109735655299e-07,
        "epoch": 1.3250837844805363,
        "step": 10280
    },
    {
        "loss": 1.6597,
        "grad_norm": 2.776301383972168,
        "learning_rate": 1.0501508616411748e-07,
        "epoch": 1.3252126836813611,
        "step": 10281
    },
    {
        "loss": 2.0996,
        "grad_norm": 1.63345468044281,
        "learning_rate": 1.0224605157542755e-07,
        "epoch": 1.3253415828821862,
        "step": 10282
    },
    {
        "loss": 2.6015,
        "grad_norm": 2.219332218170166,
        "learning_rate": 9.951399461545219e-08,
        "epoch": 1.325470482083011,
        "step": 10283
    },
    {
        "loss": 2.1619,
        "grad_norm": 2.6450355052948,
        "learning_rate": 9.681891629546025e-08,
        "epoch": 1.325599381283836,
        "step": 10284
    },
    {
        "loss": 1.0838,
        "grad_norm": 2.7872095108032227,
        "learning_rate": 9.416081761306483e-08,
        "epoch": 1.325728280484661,
        "step": 10285
    },
    {
        "loss": 1.4917,
        "grad_norm": 2.4935314655303955,
        "learning_rate": 9.153969955216779e-08,
        "epoch": 1.3258571796854859,
        "step": 10286
    },
    {
        "loss": 2.2272,
        "grad_norm": 3.609971761703491,
        "learning_rate": 8.895556308298192e-08,
        "epoch": 1.325986078886311,
        "step": 10287
    },
    {
        "loss": 1.4657,
        "grad_norm": 2.311631917953491,
        "learning_rate": 8.640840916205317e-08,
        "epoch": 1.3261149780871357,
        "step": 10288
    },
    {
        "loss": 1.7396,
        "grad_norm": 1.4361207485198975,
        "learning_rate": 8.389823873221625e-08,
        "epoch": 1.3262438772879608,
        "step": 10289
    },
    {
        "loss": 1.5733,
        "grad_norm": 1.8067723512649536,
        "learning_rate": 8.14250527226168e-08,
        "epoch": 1.3263727764887858,
        "step": 10290
    },
    {
        "loss": 0.925,
        "grad_norm": 2.701911211013794,
        "learning_rate": 7.898885204871142e-08,
        "epoch": 1.3265016756896109,
        "step": 10291
    },
    {
        "loss": 1.1365,
        "grad_norm": 2.5199244022369385,
        "learning_rate": 7.658963761228988e-08,
        "epoch": 1.3266305748904357,
        "step": 10292
    },
    {
        "loss": 1.5332,
        "grad_norm": 2.9612460136413574,
        "learning_rate": 7.422741030141956e-08,
        "epoch": 1.3267594740912607,
        "step": 10293
    },
    {
        "loss": 1.346,
        "grad_norm": 2.542140483856201,
        "learning_rate": 7.19021709904899e-08,
        "epoch": 1.3268883732920855,
        "step": 10294
    },
    {
        "loss": 2.2278,
        "grad_norm": 3.0705769062042236,
        "learning_rate": 6.961392054020133e-08,
        "epoch": 1.3270172724929106,
        "step": 10295
    },
    {
        "loss": 1.9351,
        "grad_norm": 3.722515821456909,
        "learning_rate": 6.736265979757628e-08,
        "epoch": 1.3271461716937356,
        "step": 10296
    },
    {
        "loss": 2.2733,
        "grad_norm": 2.741774559020996,
        "learning_rate": 6.514838959590375e-08,
        "epoch": 1.3272750708945604,
        "step": 10297
    },
    {
        "loss": 2.2988,
        "grad_norm": 2.6925697326660156,
        "learning_rate": 6.297111075481699e-08,
        "epoch": 1.3274039700953855,
        "step": 10298
    },
    {
        "loss": 2.077,
        "grad_norm": 2.9781863689422607,
        "learning_rate": 6.08308240802491e-08,
        "epoch": 1.3275328692962103,
        "step": 10299
    },
    {
        "loss": 1.7583,
        "grad_norm": 7.650848388671875,
        "learning_rate": 5.872753036443301e-08,
        "epoch": 1.3276617684970353,
        "step": 10300
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.038267135620117,
        "learning_rate": 5.666123038592375e-08,
        "epoch": 1.3277906676978604,
        "step": 10301
    },
    {
        "loss": 2.3693,
        "grad_norm": 3.0015745162963867,
        "learning_rate": 5.4631924909553936e-08,
        "epoch": 1.3279195668986852,
        "step": 10302
    },
    {
        "loss": 1.2069,
        "grad_norm": 3.5507211685180664,
        "learning_rate": 5.2639614686500473e-08,
        "epoch": 1.3280484660995102,
        "step": 10303
    },
    {
        "loss": 2.1391,
        "grad_norm": 1.8094769716262817,
        "learning_rate": 5.06843004542068e-08,
        "epoch": 1.328177365300335,
        "step": 10304
    },
    {
        "loss": 1.5761,
        "grad_norm": 2.6095213890075684,
        "learning_rate": 4.876598293646062e-08,
        "epoch": 1.32830626450116,
        "step": 10305
    },
    {
        "loss": 1.4857,
        "grad_norm": 2.810866594314575,
        "learning_rate": 4.6884662843316164e-08,
        "epoch": 1.3284351637019851,
        "step": 10306
    },
    {
        "loss": 1.9297,
        "grad_norm": 1.8861720561981201,
        "learning_rate": 4.5040340871171925e-08,
        "epoch": 1.32856406290281,
        "step": 10307
    },
    {
        "loss": 1.2731,
        "grad_norm": 3.0495595932006836,
        "learning_rate": 4.323301770270405e-08,
        "epoch": 1.328692962103635,
        "step": 10308
    },
    {
        "loss": 1.2886,
        "grad_norm": 1.914003610610962,
        "learning_rate": 4.146269400689961e-08,
        "epoch": 1.3288218613044598,
        "step": 10309
    },
    {
        "loss": 1.8159,
        "grad_norm": 2.4270315170288086,
        "learning_rate": 3.9729370439056666e-08,
        "epoch": 1.3289507605052848,
        "step": 10310
    },
    {
        "loss": 2.1825,
        "grad_norm": 1.4942939281463623,
        "learning_rate": 3.803304764076199e-08,
        "epoch": 1.3290796597061099,
        "step": 10311
    },
    {
        "loss": 1.016,
        "grad_norm": 2.3940987586975098,
        "learning_rate": 3.637372623993551e-08,
        "epoch": 1.329208558906935,
        "step": 10312
    },
    {
        "loss": 2.0305,
        "grad_norm": 4.2220306396484375,
        "learning_rate": 3.475140685077482e-08,
        "epoch": 1.3293374581077597,
        "step": 10313
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.9190123081207275,
        "learning_rate": 3.316609007378846e-08,
        "epoch": 1.3294663573085848,
        "step": 10314
    },
    {
        "loss": 2.1009,
        "grad_norm": 2.56001877784729,
        "learning_rate": 3.161777649578479e-08,
        "epoch": 1.3295952565094096,
        "step": 10315
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.5148736238479614,
        "learning_rate": 3.010646668988315e-08,
        "epoch": 1.3297241557102346,
        "step": 10316
    },
    {
        "loss": 2.1163,
        "grad_norm": 2.3586549758911133,
        "learning_rate": 2.863216121551382e-08,
        "epoch": 1.3298530549110597,
        "step": 10317
    },
    {
        "loss": 1.7528,
        "grad_norm": 1.7714089155197144,
        "learning_rate": 2.719486061837362e-08,
        "epoch": 1.3299819541118845,
        "step": 10318
    },
    {
        "loss": 1.3826,
        "grad_norm": 1.3008713722229004,
        "learning_rate": 2.5794565430503626e-08,
        "epoch": 1.3301108533127095,
        "step": 10319
    },
    {
        "loss": 1.6963,
        "grad_norm": 1.6832045316696167,
        "learning_rate": 2.4431276170233665e-08,
        "epoch": 1.3302397525135343,
        "step": 10320
    },
    {
        "loss": 1.6592,
        "grad_norm": 2.111973762512207,
        "learning_rate": 2.31049933421712e-08,
        "epoch": 1.3303686517143594,
        "step": 10321
    },
    {
        "loss": 1.839,
        "grad_norm": 1.6353862285614014,
        "learning_rate": 2.1815717437267958e-08,
        "epoch": 1.3304975509151844,
        "step": 10322
    },
    {
        "loss": 1.6449,
        "grad_norm": 2.6331288814544678,
        "learning_rate": 2.0563448932742204e-08,
        "epoch": 1.3306264501160092,
        "step": 10323
    },
    {
        "loss": 1.232,
        "grad_norm": 2.3581485748291016,
        "learning_rate": 1.934818829213425e-08,
        "epoch": 1.3307553493168343,
        "step": 10324
    },
    {
        "loss": 1.6434,
        "grad_norm": 2.2939817905426025,
        "learning_rate": 1.8169935965273167e-08,
        "epoch": 1.330884248517659,
        "step": 10325
    },
    {
        "loss": 2.1698,
        "grad_norm": 1.8193777799606323,
        "learning_rate": 1.7028692388287858e-08,
        "epoch": 1.3310131477184841,
        "step": 10326
    },
    {
        "loss": 1.9258,
        "grad_norm": 1.351168155670166,
        "learning_rate": 1.5924457983640394e-08,
        "epoch": 1.3311420469193092,
        "step": 10327
    },
    {
        "loss": 1.5839,
        "grad_norm": 2.4430418014526367,
        "learning_rate": 1.4857233160037176e-08,
        "epoch": 1.3312709461201342,
        "step": 10328
    },
    {
        "loss": 2.2831,
        "grad_norm": 2.2941958904266357,
        "learning_rate": 1.3827018312539963e-08,
        "epoch": 1.331399845320959,
        "step": 10329
    },
    {
        "loss": 1.5366,
        "grad_norm": 2.1087229251861572,
        "learning_rate": 1.283381382247706e-08,
        "epoch": 1.331528744521784,
        "step": 10330
    },
    {
        "loss": 1.2426,
        "grad_norm": 2.552964210510254,
        "learning_rate": 1.1877620057498817e-08,
        "epoch": 1.3316576437226089,
        "step": 10331
    },
    {
        "loss": 1.5258,
        "grad_norm": 2.7574310302734375,
        "learning_rate": 1.0958437371522134e-08,
        "epoch": 1.331786542923434,
        "step": 10332
    },
    {
        "loss": 1.5574,
        "grad_norm": 3.169647693634033,
        "learning_rate": 1.0076266104808163e-08,
        "epoch": 1.331915442124259,
        "step": 10333
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.944011926651001,
        "learning_rate": 9.231106583884597e-09,
        "epoch": 1.3320443413250838,
        "step": 10334
    },
    {
        "loss": 1.0832,
        "grad_norm": 2.1199660301208496,
        "learning_rate": 8.422959121601182e-09,
        "epoch": 1.3321732405259088,
        "step": 10335
    },
    {
        "loss": 1.6525,
        "grad_norm": 1.605658769607544,
        "learning_rate": 7.651824017085308e-09,
        "epoch": 1.3323021397267336,
        "step": 10336
    },
    {
        "loss": 2.0896,
        "grad_norm": 1.7924638986587524,
        "learning_rate": 6.917701555797518e-09,
        "epoch": 1.3324310389275587,
        "step": 10337
    },
    {
        "loss": 1.2149,
        "grad_norm": 1.2358983755111694,
        "learning_rate": 6.220592009442694e-09,
        "epoch": 1.3325599381283837,
        "step": 10338
    },
    {
        "loss": 1.5859,
        "grad_norm": 1.7608063220977783,
        "learning_rate": 5.560495636092178e-09,
        "epoch": 1.3326888373292085,
        "step": 10339
    },
    {
        "loss": 0.7805,
        "grad_norm": 2.3777031898498535,
        "learning_rate": 4.937412680072751e-09,
        "epoch": 1.3328177365300335,
        "step": 10340
    },
    {
        "loss": 1.4415,
        "grad_norm": 8.896608352661133,
        "learning_rate": 4.351343372011041e-09,
        "epoch": 1.3329466357308584,
        "step": 10341
    },
    {
        "loss": 1.0739,
        "grad_norm": 1.374567985534668,
        "learning_rate": 3.80228792885573e-09,
        "epoch": 1.3330755349316834,
        "step": 10342
    },
    {
        "loss": 2.0611,
        "grad_norm": 1.4969836473464966,
        "learning_rate": 3.290246553844245e-09,
        "epoch": 1.3332044341325084,
        "step": 10343
    },
    {
        "loss": 1.9915,
        "grad_norm": 2.152554750442505,
        "learning_rate": 2.8152194365027582e-09,
        "epoch": 1.3333333333333333,
        "step": 10344
    },
    {
        "loss": 1.9616,
        "grad_norm": 1.9528614282608032,
        "learning_rate": 2.3772067526572905e-09,
        "epoch": 1.3334622325341583,
        "step": 10345
    },
    {
        "loss": 1.2494,
        "grad_norm": 1.4954724311828613,
        "learning_rate": 1.976208664455914e-09,
        "epoch": 1.3335911317349831,
        "step": 10346
    },
    {
        "loss": 2.105,
        "grad_norm": 1.5010966062545776,
        "learning_rate": 1.6122253203354475e-09,
        "epoch": 1.3337200309358082,
        "step": 10347
    },
    {
        "loss": 1.8357,
        "grad_norm": 1.9292234182357788,
        "learning_rate": 1.2852568549992505e-09,
        "epoch": 1.3338489301366332,
        "step": 10348
    },
    {
        "loss": 1.3601,
        "grad_norm": 1.6253222227096558,
        "learning_rate": 9.953033895060415e-10,
        "epoch": 1.3339778293374582,
        "step": 10349
    },
    {
        "loss": 1.4531,
        "grad_norm": 2.6347453594207764,
        "learning_rate": 7.423650311588759e-10,
        "epoch": 1.334106728538283,
        "step": 10350
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.4107908010482788,
        "learning_rate": 5.264418735939635e-10,
        "epoch": 1.334235627739108,
        "step": 10351
    },
    {
        "loss": 1.4031,
        "grad_norm": 3.5549776554107666,
        "learning_rate": 3.4753399674736233e-10,
        "epoch": 1.334364526939933,
        "step": 10352
    },
    {
        "loss": 2.2784,
        "grad_norm": 2.7665748596191406,
        "learning_rate": 2.0564146682167108e-10,
        "epoch": 1.334493426140758,
        "step": 10353
    },
    {
        "loss": 1.3651,
        "grad_norm": 4.23495626449585,
        "learning_rate": 1.0076433635264337e-10,
        "epoch": 1.334622325341583,
        "step": 10354
    },
    {
        "loss": 1.4817,
        "grad_norm": 2.1952948570251465,
        "learning_rate": 3.2902644164778396e-11,
        "epoch": 1.3347512245424078,
        "step": 10355
    },
    {
        "loss": 2.1568,
        "grad_norm": 1.9681403636932373,
        "learning_rate": 2.0564153713209523e-12,
        "epoch": 1.3348801237432328,
        "step": 10356
    },
    {
        "loss": 1.9153,
        "grad_norm": 1.8804011344909668,
        "learning_rate": 0.00019999999177433865,
        "epoch": 1.3350090229440577,
        "step": 10357
    },
    {
        "loss": 1.9652,
        "grad_norm": 2.5033986568450928,
        "learning_rate": 0.0001999999485896201,
        "epoch": 1.3351379221448827,
        "step": 10358
    },
    {
        "loss": 1.7148,
        "grad_norm": 2.5688297748565674,
        "learning_rate": 0.000199999868389445,
        "epoch": 1.3352668213457077,
        "step": 10359
    },
    {
        "loss": 1.7116,
        "grad_norm": 1.8305156230926514,
        "learning_rate": 0.00019999975117384307,
        "epoch": 1.3353957205465325,
        "step": 10360
    },
    {
        "loss": 1.422,
        "grad_norm": 2.2285876274108887,
        "learning_rate": 0.00019999959694285763,
        "epoch": 1.3355246197473576,
        "step": 10361
    },
    {
        "loss": 1.6174,
        "grad_norm": 1.633359432220459,
        "learning_rate": 0.00019999940569654588,
        "epoch": 1.3356535189481824,
        "step": 10362
    },
    {
        "loss": 1.6383,
        "grad_norm": 1.8884636163711548,
        "learning_rate": 0.0001999991774349785,
        "epoch": 1.3357824181490074,
        "step": 10363
    },
    {
        "loss": 1.7188,
        "grad_norm": 2.103245735168457,
        "learning_rate": 0.00019999891215824,
        "epoch": 1.3359113173498325,
        "step": 10364
    },
    {
        "loss": 2.0639,
        "grad_norm": 1.6701523065567017,
        "learning_rate": 0.00019999860986642863,
        "epoch": 1.3360402165506575,
        "step": 10365
    },
    {
        "loss": 1.898,
        "grad_norm": 2.4555251598358154,
        "learning_rate": 0.00019999827055965623,
        "epoch": 1.3361691157514823,
        "step": 10366
    },
    {
        "loss": 1.4567,
        "grad_norm": 2.8487820625305176,
        "learning_rate": 0.00019999789423804845,
        "epoch": 1.3362980149523074,
        "step": 10367
    },
    {
        "loss": 1.6046,
        "grad_norm": 2.782776117324829,
        "learning_rate": 0.00019999748090174452,
        "epoch": 1.3364269141531322,
        "step": 10368
    },
    {
        "loss": 1.6476,
        "grad_norm": 2.185274600982666,
        "learning_rate": 0.0001999970305508975,
        "epoch": 1.3365558133539572,
        "step": 10369
    },
    {
        "loss": 1.6493,
        "grad_norm": 2.2229902744293213,
        "learning_rate": 0.00019999654318567404,
        "epoch": 1.3366847125547823,
        "step": 10370
    },
    {
        "loss": 1.9484,
        "grad_norm": 2.3339476585388184,
        "learning_rate": 0.00019999601880625457,
        "epoch": 1.336813611755607,
        "step": 10371
    },
    {
        "loss": 1.5016,
        "grad_norm": 2.0382463932037354,
        "learning_rate": 0.00019999545741283318,
        "epoch": 1.3369425109564321,
        "step": 10372
    },
    {
        "loss": 1.3661,
        "grad_norm": 1.7679389715194702,
        "learning_rate": 0.00019999485900561772,
        "epoch": 1.337071410157257,
        "step": 10373
    },
    {
        "loss": 1.1121,
        "grad_norm": 3.2967331409454346,
        "learning_rate": 0.0001999942235848296,
        "epoch": 1.337200309358082,
        "step": 10374
    },
    {
        "loss": 2.286,
        "grad_norm": 2.1457343101501465,
        "learning_rate": 0.0001999935511507041,
        "epoch": 1.337329208558907,
        "step": 10375
    },
    {
        "loss": 2.7198,
        "grad_norm": 1.7769113779067993,
        "learning_rate": 0.00019999284170349007,
        "epoch": 1.3374581077597318,
        "step": 10376
    },
    {
        "loss": 0.8836,
        "grad_norm": 2.149381160736084,
        "learning_rate": 0.0001999920952434502,
        "epoch": 1.3375870069605569,
        "step": 10377
    },
    {
        "loss": 2.0352,
        "grad_norm": 2.253319025039673,
        "learning_rate": 0.00019999131177086068,
        "epoch": 1.3377159061613817,
        "step": 10378
    },
    {
        "loss": 1.2723,
        "grad_norm": 3.0100677013397217,
        "learning_rate": 0.0001999904912860116,
        "epoch": 1.3378448053622067,
        "step": 10379
    },
    {
        "loss": 0.6503,
        "grad_norm": 2.2518911361694336,
        "learning_rate": 0.00019998963378920666,
        "epoch": 1.3379737045630318,
        "step": 10380
    },
    {
        "loss": 1.9235,
        "grad_norm": 1.6093653440475464,
        "learning_rate": 0.00019998873928076322,
        "epoch": 1.3381026037638566,
        "step": 10381
    },
    {
        "loss": 1.8003,
        "grad_norm": 2.2061214447021484,
        "learning_rate": 0.00019998780776101245,
        "epoch": 1.3382315029646816,
        "step": 10382
    },
    {
        "loss": 1.7046,
        "grad_norm": 2.8156042098999023,
        "learning_rate": 0.00019998683923029908,
        "epoch": 1.3383604021655064,
        "step": 10383
    },
    {
        "loss": 1.8828,
        "grad_norm": 2.5639097690582275,
        "learning_rate": 0.0001999858336889817,
        "epoch": 1.3384893013663315,
        "step": 10384
    },
    {
        "loss": 2.047,
        "grad_norm": 1.7779090404510498,
        "learning_rate": 0.00019998479113743245,
        "epoch": 1.3386182005671565,
        "step": 10385
    },
    {
        "loss": 1.8125,
        "grad_norm": 1.5110194683074951,
        "learning_rate": 0.00019998371157603726,
        "epoch": 1.3387470997679816,
        "step": 10386
    },
    {
        "loss": 1.8261,
        "grad_norm": 1.9683666229248047,
        "learning_rate": 0.00019998259500519575,
        "epoch": 1.3388759989688064,
        "step": 10387
    },
    {
        "loss": 1.9043,
        "grad_norm": 1.8666889667510986,
        "learning_rate": 0.0001999814414253212,
        "epoch": 1.3390048981696314,
        "step": 10388
    },
    {
        "loss": 1.0559,
        "grad_norm": 3.0810916423797607,
        "learning_rate": 0.0001999802508368406,
        "epoch": 1.3391337973704562,
        "step": 10389
    },
    {
        "loss": 0.8978,
        "grad_norm": 2.906494379043579,
        "learning_rate": 0.0001999790232401947,
        "epoch": 1.3392626965712813,
        "step": 10390
    },
    {
        "loss": 1.298,
        "grad_norm": 3.8158669471740723,
        "learning_rate": 0.00019997775863583788,
        "epoch": 1.3393915957721063,
        "step": 10391
    },
    {
        "loss": 0.9442,
        "grad_norm": 4.305890083312988,
        "learning_rate": 0.0001999764570242382,
        "epoch": 1.3395204949729311,
        "step": 10392
    },
    {
        "loss": 1.2486,
        "grad_norm": 3.843827486038208,
        "learning_rate": 0.00019997511840587752,
        "epoch": 1.3396493941737562,
        "step": 10393
    },
    {
        "loss": 0.9564,
        "grad_norm": 3.1707663536071777,
        "learning_rate": 0.0001999737427812513,
        "epoch": 1.339778293374581,
        "step": 10394
    },
    {
        "loss": 2.0005,
        "grad_norm": 2.924283027648926,
        "learning_rate": 0.00019997233015086872,
        "epoch": 1.339907192575406,
        "step": 10395
    },
    {
        "loss": 2.1771,
        "grad_norm": 1.478740930557251,
        "learning_rate": 0.00019997088051525275,
        "epoch": 1.340036091776231,
        "step": 10396
    },
    {
        "loss": 0.6833,
        "grad_norm": 4.742752552032471,
        "learning_rate": 0.00019996939387493986,
        "epoch": 1.3401649909770559,
        "step": 10397
    },
    {
        "loss": 2.1594,
        "grad_norm": 1.8686667680740356,
        "learning_rate": 0.00019996787023048045,
        "epoch": 1.340293890177881,
        "step": 10398
    },
    {
        "loss": 1.8461,
        "grad_norm": 1.4789116382598877,
        "learning_rate": 0.0001999663095824384,
        "epoch": 1.3404227893787057,
        "step": 10399
    },
    {
        "loss": 1.1627,
        "grad_norm": 3.4530956745147705,
        "learning_rate": 0.00019996471193139148,
        "epoch": 1.3405516885795308,
        "step": 10400
    },
    {
        "loss": 2.1158,
        "grad_norm": 1.377679705619812,
        "learning_rate": 0.00019996307727793104,
        "epoch": 1.3406805877803558,
        "step": 10401
    },
    {
        "loss": 1.791,
        "grad_norm": 1.3737314939498901,
        "learning_rate": 0.00019996140562266213,
        "epoch": 1.3408094869811809,
        "step": 10402
    },
    {
        "loss": 1.5449,
        "grad_norm": 2.4459614753723145,
        "learning_rate": 0.00019995969696620357,
        "epoch": 1.3409383861820057,
        "step": 10403
    },
    {
        "loss": 1.5027,
        "grad_norm": 1.8823292255401611,
        "learning_rate": 0.00019995795130918777,
        "epoch": 1.3410672853828307,
        "step": 10404
    },
    {
        "loss": 1.6401,
        "grad_norm": 2.9328858852386475,
        "learning_rate": 0.0001999561686522609,
        "epoch": 1.3411961845836555,
        "step": 10405
    },
    {
        "loss": 1.7082,
        "grad_norm": 2.6799488067626953,
        "learning_rate": 0.00019995434899608286,
        "epoch": 1.3413250837844806,
        "step": 10406
    },
    {
        "loss": 2.2112,
        "grad_norm": 2.6697487831115723,
        "learning_rate": 0.0001999524923413272,
        "epoch": 1.3414539829853056,
        "step": 10407
    },
    {
        "loss": 2.8758,
        "grad_norm": 1.7965080738067627,
        "learning_rate": 0.00019995059868868113,
        "epoch": 1.3415828821861304,
        "step": 10408
    },
    {
        "loss": 2.2551,
        "grad_norm": 1.920234203338623,
        "learning_rate": 0.00019994866803884565,
        "epoch": 1.3417117813869555,
        "step": 10409
    },
    {
        "loss": 1.1055,
        "grad_norm": 1.9251958131790161,
        "learning_rate": 0.0001999467003925353,
        "epoch": 1.3418406805877803,
        "step": 10410
    },
    {
        "loss": 1.9367,
        "grad_norm": 2.32961106300354,
        "learning_rate": 0.00019994469575047857,
        "epoch": 1.3419695797886053,
        "step": 10411
    },
    {
        "loss": 1.858,
        "grad_norm": 1.537739872932434,
        "learning_rate": 0.00019994265411341734,
        "epoch": 1.3420984789894304,
        "step": 10412
    },
    {
        "loss": 1.7364,
        "grad_norm": 2.0685462951660156,
        "learning_rate": 0.0001999405754821074,
        "epoch": 1.3422273781902552,
        "step": 10413
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.8965864181518555,
        "learning_rate": 0.00019993845985731814,
        "epoch": 1.3423562773910802,
        "step": 10414
    },
    {
        "loss": 2.335,
        "grad_norm": 1.6684417724609375,
        "learning_rate": 0.00019993630723983273,
        "epoch": 1.342485176591905,
        "step": 10415
    },
    {
        "loss": 1.5796,
        "grad_norm": 2.6446478366851807,
        "learning_rate": 0.00019993411763044786,
        "epoch": 1.34261407579273,
        "step": 10416
    },
    {
        "loss": 1.6338,
        "grad_norm": 3.0170392990112305,
        "learning_rate": 0.00019993189102997415,
        "epoch": 1.342742974993555,
        "step": 10417
    },
    {
        "loss": 2.1714,
        "grad_norm": 2.6465694904327393,
        "learning_rate": 0.00019992962743923566,
        "epoch": 1.34287187419438,
        "step": 10418
    },
    {
        "loss": 1.2394,
        "grad_norm": 3.270404815673828,
        "learning_rate": 0.0001999273268590704,
        "epoch": 1.343000773395205,
        "step": 10419
    },
    {
        "loss": 1.8849,
        "grad_norm": 2.133791446685791,
        "learning_rate": 0.00019992498929032984,
        "epoch": 1.3431296725960298,
        "step": 10420
    },
    {
        "loss": 0.3707,
        "grad_norm": 2.465163469314575,
        "learning_rate": 0.0001999226147338793,
        "epoch": 1.3432585717968548,
        "step": 10421
    },
    {
        "loss": 1.7013,
        "grad_norm": 2.184915542602539,
        "learning_rate": 0.0001999202031905977,
        "epoch": 1.3433874709976799,
        "step": 10422
    },
    {
        "loss": 1.8583,
        "grad_norm": 2.1436352729797363,
        "learning_rate": 0.0001999177546613777,
        "epoch": 1.343516370198505,
        "step": 10423
    },
    {
        "loss": 1.6831,
        "grad_norm": 2.038566827774048,
        "learning_rate": 0.0001999152691471256,
        "epoch": 1.3436452693993297,
        "step": 10424
    },
    {
        "loss": 1.7449,
        "grad_norm": 2.977456569671631,
        "learning_rate": 0.0001999127466487615,
        "epoch": 1.3437741686001548,
        "step": 10425
    },
    {
        "loss": 1.7815,
        "grad_norm": 2.4400343894958496,
        "learning_rate": 0.00019991018716721904,
        "epoch": 1.3439030678009796,
        "step": 10426
    },
    {
        "loss": 1.2027,
        "grad_norm": 2.2242538928985596,
        "learning_rate": 0.00019990759070344564,
        "epoch": 1.3440319670018046,
        "step": 10427
    },
    {
        "loss": 0.5887,
        "grad_norm": 2.3166134357452393,
        "learning_rate": 0.00019990495725840247,
        "epoch": 1.3441608662026296,
        "step": 10428
    },
    {
        "loss": 1.7121,
        "grad_norm": 1.6054350137710571,
        "learning_rate": 0.00019990228683306417,
        "epoch": 1.3442897654034545,
        "step": 10429
    },
    {
        "loss": 2.3378,
        "grad_norm": 2.0768609046936035,
        "learning_rate": 0.00019989957942841935,
        "epoch": 1.3444186646042795,
        "step": 10430
    },
    {
        "loss": 2.215,
        "grad_norm": 2.065943956375122,
        "learning_rate": 0.0001998968350454701,
        "epoch": 1.3445475638051043,
        "step": 10431
    },
    {
        "loss": 1.767,
        "grad_norm": 2.1843345165252686,
        "learning_rate": 0.00019989405368523225,
        "epoch": 1.3446764630059294,
        "step": 10432
    },
    {
        "loss": 1.0008,
        "grad_norm": 3.1665430068969727,
        "learning_rate": 0.0001998912353487354,
        "epoch": 1.3448053622067544,
        "step": 10433
    },
    {
        "loss": 1.0441,
        "grad_norm": 3.0750882625579834,
        "learning_rate": 0.0001998883800370227,
        "epoch": 1.3449342614075792,
        "step": 10434
    },
    {
        "loss": 1.8464,
        "grad_norm": 2.2383005619049072,
        "learning_rate": 0.0001998854877511511,
        "epoch": 1.3450631606084043,
        "step": 10435
    },
    {
        "loss": 2.0702,
        "grad_norm": 1.8604824542999268,
        "learning_rate": 0.00019988255849219116,
        "epoch": 1.345192059809229,
        "step": 10436
    },
    {
        "loss": 1.0205,
        "grad_norm": 2.949404239654541,
        "learning_rate": 0.00019987959226122718,
        "epoch": 1.345320959010054,
        "step": 10437
    },
    {
        "loss": 1.3736,
        "grad_norm": 3.1310439109802246,
        "learning_rate": 0.00019987658905935715,
        "epoch": 1.3454498582108791,
        "step": 10438
    },
    {
        "loss": 0.5701,
        "grad_norm": 2.0924875736236572,
        "learning_rate": 0.00019987354888769266,
        "epoch": 1.3455787574117042,
        "step": 10439
    },
    {
        "loss": 0.8936,
        "grad_norm": 3.0456252098083496,
        "learning_rate": 0.00019987047174735914,
        "epoch": 1.345707656612529,
        "step": 10440
    },
    {
        "loss": 1.7906,
        "grad_norm": 2.5459859371185303,
        "learning_rate": 0.00019986735763949547,
        "epoch": 1.3458365558133538,
        "step": 10441
    },
    {
        "loss": 1.7949,
        "grad_norm": 3.286588668823242,
        "learning_rate": 0.00019986420656525447,
        "epoch": 1.3459654550141789,
        "step": 10442
    },
    {
        "loss": 1.0916,
        "grad_norm": 2.1356494426727295,
        "learning_rate": 0.0001998610185258025,
        "epoch": 1.346094354215004,
        "step": 10443
    },
    {
        "loss": 0.6449,
        "grad_norm": 3.2351431846618652,
        "learning_rate": 0.00019985779352231956,
        "epoch": 1.346223253415829,
        "step": 10444
    },
    {
        "loss": 1.6237,
        "grad_norm": 3.075019121170044,
        "learning_rate": 0.00019985453155599948,
        "epoch": 1.3463521526166538,
        "step": 10445
    },
    {
        "loss": 1.1946,
        "grad_norm": 3.748739242553711,
        "learning_rate": 0.00019985123262804966,
        "epoch": 1.3464810518174788,
        "step": 10446
    },
    {
        "loss": 1.3777,
        "grad_norm": 3.8087894916534424,
        "learning_rate": 0.0001998478967396912,
        "epoch": 1.3466099510183036,
        "step": 10447
    },
    {
        "loss": 2.1498,
        "grad_norm": 2.7217330932617188,
        "learning_rate": 0.00019984452389215895,
        "epoch": 1.3467388502191286,
        "step": 10448
    },
    {
        "loss": 1.3349,
        "grad_norm": 3.894495964050293,
        "learning_rate": 0.00019984111408670132,
        "epoch": 1.3468677494199537,
        "step": 10449
    },
    {
        "loss": 0.8885,
        "grad_norm": 2.927621364593506,
        "learning_rate": 0.0001998376673245805,
        "epoch": 1.3469966486207785,
        "step": 10450
    },
    {
        "loss": 2.4629,
        "grad_norm": 2.0503013134002686,
        "learning_rate": 0.0001998341836070723,
        "epoch": 1.3471255478216035,
        "step": 10451
    },
    {
        "loss": 1.8752,
        "grad_norm": 1.95259690284729,
        "learning_rate": 0.00019983066293546628,
        "epoch": 1.3472544470224284,
        "step": 10452
    },
    {
        "loss": 1.9158,
        "grad_norm": 1.9764609336853027,
        "learning_rate": 0.00019982710531106558,
        "epoch": 1.3473833462232534,
        "step": 10453
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.4269475936889648,
        "learning_rate": 0.00019982351073518712,
        "epoch": 1.3475122454240784,
        "step": 10454
    },
    {
        "loss": 1.9985,
        "grad_norm": 2.5185658931732178,
        "learning_rate": 0.00019981987920916145,
        "epoch": 1.3476411446249033,
        "step": 10455
    },
    {
        "loss": 1.1192,
        "grad_norm": 2.4618170261383057,
        "learning_rate": 0.00019981621073433275,
        "epoch": 1.3477700438257283,
        "step": 10456
    },
    {
        "loss": 1.9632,
        "grad_norm": 1.8492540121078491,
        "learning_rate": 0.00019981250531205894,
        "epoch": 1.347898943026553,
        "step": 10457
    },
    {
        "loss": 1.4346,
        "grad_norm": 2.256101131439209,
        "learning_rate": 0.0001998087629437116,
        "epoch": 1.3480278422273781,
        "step": 10458
    },
    {
        "loss": 1.5171,
        "grad_norm": 2.3325445652008057,
        "learning_rate": 0.000199804983630676,
        "epoch": 1.3481567414282032,
        "step": 10459
    },
    {
        "loss": 2.4464,
        "grad_norm": 1.50002920627594,
        "learning_rate": 0.00019980116737435108,
        "epoch": 1.3482856406290282,
        "step": 10460
    },
    {
        "loss": 1.7553,
        "grad_norm": 2.322039842605591,
        "learning_rate": 0.0001997973141761494,
        "epoch": 1.348414539829853,
        "step": 10461
    },
    {
        "loss": 1.3254,
        "grad_norm": 2.424004077911377,
        "learning_rate": 0.0001997934240374973,
        "epoch": 1.348543439030678,
        "step": 10462
    },
    {
        "loss": 1.1285,
        "grad_norm": 3.2043240070343018,
        "learning_rate": 0.00019978949695983471,
        "epoch": 1.348672338231503,
        "step": 10463
    },
    {
        "loss": 1.8985,
        "grad_norm": 1.8728581666946411,
        "learning_rate": 0.0001997855329446152,
        "epoch": 1.348801237432328,
        "step": 10464
    },
    {
        "loss": 1.7358,
        "grad_norm": 1.9774415493011475,
        "learning_rate": 0.00019978153199330616,
        "epoch": 1.348930136633153,
        "step": 10465
    },
    {
        "loss": 1.8153,
        "grad_norm": 3.743497133255005,
        "learning_rate": 0.00019977749410738852,
        "epoch": 1.3490590358339778,
        "step": 10466
    },
    {
        "loss": 1.3459,
        "grad_norm": 3.483884572982788,
        "learning_rate": 0.00019977341928835692,
        "epoch": 1.3491879350348028,
        "step": 10467
    },
    {
        "loss": 1.4788,
        "grad_norm": 2.5663418769836426,
        "learning_rate": 0.00019976930753771966,
        "epoch": 1.3493168342356276,
        "step": 10468
    },
    {
        "loss": 1.8731,
        "grad_norm": 1.9341802597045898,
        "learning_rate": 0.0001997651588569987,
        "epoch": 1.3494457334364527,
        "step": 10469
    },
    {
        "loss": 1.4703,
        "grad_norm": 2.59586238861084,
        "learning_rate": 0.00019976097324772977,
        "epoch": 1.3495746326372777,
        "step": 10470
    },
    {
        "loss": 2.0626,
        "grad_norm": 1.8366953134536743,
        "learning_rate": 0.00019975675071146217,
        "epoch": 1.3497035318381025,
        "step": 10471
    },
    {
        "loss": 1.2009,
        "grad_norm": 3.47750186920166,
        "learning_rate": 0.00019975249124975887,
        "epoch": 1.3498324310389276,
        "step": 10472
    },
    {
        "loss": 1.8573,
        "grad_norm": 2.17789363861084,
        "learning_rate": 0.00019974819486419653,
        "epoch": 1.3499613302397524,
        "step": 10473
    },
    {
        "loss": 1.7851,
        "grad_norm": 2.1155624389648438,
        "learning_rate": 0.0001997438615563655,
        "epoch": 1.3500902294405774,
        "step": 10474
    },
    {
        "loss": 1.2946,
        "grad_norm": 2.394707441329956,
        "learning_rate": 0.00019973949132786977,
        "epoch": 1.3502191286414025,
        "step": 10475
    },
    {
        "loss": 1.6857,
        "grad_norm": 2.82717227935791,
        "learning_rate": 0.00019973508418032695,
        "epoch": 1.3503480278422275,
        "step": 10476
    },
    {
        "loss": 1.5659,
        "grad_norm": 2.0309507846832275,
        "learning_rate": 0.0001997306401153684,
        "epoch": 1.3504769270430523,
        "step": 10477
    },
    {
        "loss": 1.8115,
        "grad_norm": 3.318126678466797,
        "learning_rate": 0.00019972615913463917,
        "epoch": 1.3506058262438772,
        "step": 10478
    },
    {
        "loss": 0.8707,
        "grad_norm": 2.7695066928863525,
        "learning_rate": 0.00019972164123979784,
        "epoch": 1.3507347254447022,
        "step": 10479
    },
    {
        "loss": 1.5953,
        "grad_norm": 2.326368570327759,
        "learning_rate": 0.00019971708643251677,
        "epoch": 1.3508636246455272,
        "step": 10480
    },
    {
        "loss": 1.2101,
        "grad_norm": 2.4368374347686768,
        "learning_rate": 0.0001997124947144819,
        "epoch": 1.3509925238463523,
        "step": 10481
    },
    {
        "loss": 0.8445,
        "grad_norm": 5.739652633666992,
        "learning_rate": 0.00019970786608739294,
        "epoch": 1.351121423047177,
        "step": 10482
    },
    {
        "loss": 2.1082,
        "grad_norm": 2.284942865371704,
        "learning_rate": 0.00019970320055296315,
        "epoch": 1.3512503222480021,
        "step": 10483
    },
    {
        "loss": 1.1136,
        "grad_norm": 2.5089192390441895,
        "learning_rate": 0.0001996984981129195,
        "epoch": 1.351379221448827,
        "step": 10484
    },
    {
        "loss": 0.5342,
        "grad_norm": 2.4225265979766846,
        "learning_rate": 0.00019969375876900265,
        "epoch": 1.351508120649652,
        "step": 10485
    },
    {
        "loss": 1.389,
        "grad_norm": 2.379443645477295,
        "learning_rate": 0.00019968898252296685,
        "epoch": 1.351637019850477,
        "step": 10486
    },
    {
        "loss": 1.4219,
        "grad_norm": 1.8117928504943848,
        "learning_rate": 0.0001996841693765801,
        "epoch": 1.3517659190513018,
        "step": 10487
    },
    {
        "loss": 2.2183,
        "grad_norm": 2.384181261062622,
        "learning_rate": 0.000199679319331624,
        "epoch": 1.3518948182521269,
        "step": 10488
    },
    {
        "loss": 0.7844,
        "grad_norm": 3.132316827774048,
        "learning_rate": 0.00019967443238989377,
        "epoch": 1.3520237174529517,
        "step": 10489
    },
    {
        "loss": 2.0292,
        "grad_norm": 3.6024880409240723,
        "learning_rate": 0.00019966950855319842,
        "epoch": 1.3521526166537767,
        "step": 10490
    },
    {
        "loss": 1.5528,
        "grad_norm": 2.4762511253356934,
        "learning_rate": 0.0001996645478233604,
        "epoch": 1.3522815158546018,
        "step": 10491
    },
    {
        "loss": 1.4797,
        "grad_norm": 2.952211618423462,
        "learning_rate": 0.00019965955020221607,
        "epoch": 1.3524104150554266,
        "step": 10492
    },
    {
        "loss": 1.167,
        "grad_norm": 2.5493836402893066,
        "learning_rate": 0.0001996545156916153,
        "epoch": 1.3525393142562516,
        "step": 10493
    },
    {
        "loss": 2.0235,
        "grad_norm": 1.680148720741272,
        "learning_rate": 0.00019964944429342156,
        "epoch": 1.3526682134570764,
        "step": 10494
    },
    {
        "loss": 1.4094,
        "grad_norm": 2.791550636291504,
        "learning_rate": 0.00019964433600951214,
        "epoch": 1.3527971126579015,
        "step": 10495
    },
    {
        "loss": 1.7261,
        "grad_norm": 2.003253221511841,
        "learning_rate": 0.00019963919084177784,
        "epoch": 1.3529260118587265,
        "step": 10496
    },
    {
        "loss": 1.4489,
        "grad_norm": 2.394580364227295,
        "learning_rate": 0.00019963400879212322,
        "epoch": 1.3530549110595516,
        "step": 10497
    },
    {
        "loss": 1.9957,
        "grad_norm": 2.5887844562530518,
        "learning_rate": 0.00019962878986246643,
        "epoch": 1.3531838102603764,
        "step": 10498
    },
    {
        "loss": 1.4252,
        "grad_norm": 2.390113592147827,
        "learning_rate": 0.00019962353405473924,
        "epoch": 1.3533127094612014,
        "step": 10499
    },
    {
        "loss": 0.9035,
        "grad_norm": 2.1495473384857178,
        "learning_rate": 0.00019961824137088713,
        "epoch": 1.3534416086620262,
        "step": 10500
    },
    {
        "loss": 1.4316,
        "grad_norm": 2.692852735519409,
        "learning_rate": 0.00019961291181286924,
        "epoch": 1.3535705078628513,
        "step": 10501
    },
    {
        "loss": 2.0826,
        "grad_norm": 1.9679123163223267,
        "learning_rate": 0.00019960754538265828,
        "epoch": 1.3536994070636763,
        "step": 10502
    },
    {
        "loss": 1.6773,
        "grad_norm": 2.6694114208221436,
        "learning_rate": 0.0001996021420822407,
        "epoch": 1.3538283062645011,
        "step": 10503
    },
    {
        "loss": 2.2463,
        "grad_norm": 1.852254867553711,
        "learning_rate": 0.00019959670191361658,
        "epoch": 1.3539572054653262,
        "step": 10504
    },
    {
        "loss": 1.6884,
        "grad_norm": 2.4475011825561523,
        "learning_rate": 0.00019959122487879955,
        "epoch": 1.354086104666151,
        "step": 10505
    },
    {
        "loss": 1.295,
        "grad_norm": 3.499140977859497,
        "learning_rate": 0.00019958571097981702,
        "epoch": 1.354215003866976,
        "step": 10506
    },
    {
        "loss": 2.0894,
        "grad_norm": 1.9278030395507812,
        "learning_rate": 0.00019958016021870997,
        "epoch": 1.354343903067801,
        "step": 10507
    },
    {
        "loss": 0.5298,
        "grad_norm": 3.1372623443603516,
        "learning_rate": 0.00019957457259753305,
        "epoch": 1.3544728022686259,
        "step": 10508
    },
    {
        "loss": 1.1114,
        "grad_norm": 2.3204989433288574,
        "learning_rate": 0.0001995689481183545,
        "epoch": 1.354601701469451,
        "step": 10509
    },
    {
        "loss": 1.3251,
        "grad_norm": 2.183371067047119,
        "learning_rate": 0.00019956328678325628,
        "epoch": 1.3547306006702757,
        "step": 10510
    },
    {
        "loss": 1.4578,
        "grad_norm": 2.797659397125244,
        "learning_rate": 0.000199557588594334,
        "epoch": 1.3548594998711008,
        "step": 10511
    },
    {
        "loss": 1.4659,
        "grad_norm": 2.8153395652770996,
        "learning_rate": 0.00019955185355369684,
        "epoch": 1.3549883990719258,
        "step": 10512
    },
    {
        "loss": 1.3595,
        "grad_norm": 2.096158742904663,
        "learning_rate": 0.0001995460816634676,
        "epoch": 1.3551172982727508,
        "step": 10513
    },
    {
        "loss": 1.518,
        "grad_norm": 1.8537169694900513,
        "learning_rate": 0.00019954027292578283,
        "epoch": 1.3552461974735757,
        "step": 10514
    },
    {
        "loss": 1.8971,
        "grad_norm": 2.6352691650390625,
        "learning_rate": 0.00019953442734279268,
        "epoch": 1.3553750966744005,
        "step": 10515
    },
    {
        "loss": 1.8851,
        "grad_norm": 2.0023202896118164,
        "learning_rate": 0.00019952854491666084,
        "epoch": 1.3555039958752255,
        "step": 10516
    },
    {
        "loss": 1.2121,
        "grad_norm": 3.1489369869232178,
        "learning_rate": 0.0001995226256495648,
        "epoch": 1.3556328950760506,
        "step": 10517
    },
    {
        "loss": 1.6304,
        "grad_norm": 1.4979182481765747,
        "learning_rate": 0.00019951666954369558,
        "epoch": 1.3557617942768756,
        "step": 10518
    },
    {
        "loss": 0.9218,
        "grad_norm": 3.156139373779297,
        "learning_rate": 0.00019951067660125787,
        "epoch": 1.3558906934777004,
        "step": 10519
    },
    {
        "loss": 1.046,
        "grad_norm": 2.841679573059082,
        "learning_rate": 0.0001995046468244699,
        "epoch": 1.3560195926785255,
        "step": 10520
    },
    {
        "loss": 1.597,
        "grad_norm": 3.1624884605407715,
        "learning_rate": 0.00019949858021556374,
        "epoch": 1.3561484918793503,
        "step": 10521
    },
    {
        "loss": 0.7382,
        "grad_norm": 3.1953444480895996,
        "learning_rate": 0.00019949247677678492,
        "epoch": 1.3562773910801753,
        "step": 10522
    },
    {
        "loss": 1.3245,
        "grad_norm": 2.874931573867798,
        "learning_rate": 0.00019948633651039262,
        "epoch": 1.3564062902810003,
        "step": 10523
    },
    {
        "loss": 1.5891,
        "grad_norm": 4.547674179077148,
        "learning_rate": 0.0001994801594186598,
        "epoch": 1.3565351894818252,
        "step": 10524
    },
    {
        "loss": 1.5063,
        "grad_norm": 2.3888790607452393,
        "learning_rate": 0.00019947394550387287,
        "epoch": 1.3566640886826502,
        "step": 10525
    },
    {
        "loss": 1.3996,
        "grad_norm": 3.3185365200042725,
        "learning_rate": 0.0001994676947683319,
        "epoch": 1.356792987883475,
        "step": 10526
    },
    {
        "loss": 0.831,
        "grad_norm": 2.759023427963257,
        "learning_rate": 0.00019946140721435068,
        "epoch": 1.3569218870843,
        "step": 10527
    },
    {
        "loss": 2.2438,
        "grad_norm": 2.6913976669311523,
        "learning_rate": 0.00019945508284425657,
        "epoch": 1.357050786285125,
        "step": 10528
    },
    {
        "loss": 1.7998,
        "grad_norm": 3.529301404953003,
        "learning_rate": 0.00019944872166039054,
        "epoch": 1.35717968548595,
        "step": 10529
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.0588648319244385,
        "learning_rate": 0.00019944232366510728,
        "epoch": 1.357308584686775,
        "step": 10530
    },
    {
        "loss": 2.0982,
        "grad_norm": 3.404435157775879,
        "learning_rate": 0.00019943588886077497,
        "epoch": 1.3574374838875998,
        "step": 10531
    },
    {
        "loss": 0.5293,
        "grad_norm": 3.0331318378448486,
        "learning_rate": 0.00019942941724977552,
        "epoch": 1.3575663830884248,
        "step": 10532
    },
    {
        "loss": 1.4498,
        "grad_norm": 2.5130674839019775,
        "learning_rate": 0.00019942290883450441,
        "epoch": 1.3576952822892498,
        "step": 10533
    },
    {
        "loss": 1.4285,
        "grad_norm": 2.5433707237243652,
        "learning_rate": 0.00019941636361737075,
        "epoch": 1.3578241814900749,
        "step": 10534
    },
    {
        "loss": 1.8014,
        "grad_norm": 3.0257725715637207,
        "learning_rate": 0.00019940978160079732,
        "epoch": 1.3579530806908997,
        "step": 10535
    },
    {
        "loss": 1.7038,
        "grad_norm": 3.6030187606811523,
        "learning_rate": 0.00019940316278722046,
        "epoch": 1.3580819798917247,
        "step": 10536
    },
    {
        "loss": 2.1955,
        "grad_norm": 2.34775710105896,
        "learning_rate": 0.00019939650717909016,
        "epoch": 1.3582108790925496,
        "step": 10537
    },
    {
        "loss": 1.4291,
        "grad_norm": 3.188375234603882,
        "learning_rate": 0.00019938981477887,
        "epoch": 1.3583397782933746,
        "step": 10538
    },
    {
        "loss": 1.5925,
        "grad_norm": 2.8154265880584717,
        "learning_rate": 0.00019938308558903727,
        "epoch": 1.3584686774941996,
        "step": 10539
    },
    {
        "loss": 0.4084,
        "grad_norm": 2.5371251106262207,
        "learning_rate": 0.00019937631961208273,
        "epoch": 1.3585975766950245,
        "step": 10540
    },
    {
        "loss": 1.6098,
        "grad_norm": 3.2512664794921875,
        "learning_rate": 0.00019936951685051093,
        "epoch": 1.3587264758958495,
        "step": 10541
    },
    {
        "loss": 0.7934,
        "grad_norm": 2.572063446044922,
        "learning_rate": 0.00019936267730683987,
        "epoch": 1.3588553750966743,
        "step": 10542
    },
    {
        "loss": 0.6132,
        "grad_norm": 2.5691263675689697,
        "learning_rate": 0.00019935580098360121,
        "epoch": 1.3589842742974994,
        "step": 10543
    },
    {
        "loss": 1.2243,
        "grad_norm": 2.248074531555176,
        "learning_rate": 0.00019934888788334035,
        "epoch": 1.3591131734983244,
        "step": 10544
    },
    {
        "loss": 1.0302,
        "grad_norm": 3.71077823638916,
        "learning_rate": 0.0001993419380086162,
        "epoch": 1.3592420726991492,
        "step": 10545
    },
    {
        "loss": 1.8761,
        "grad_norm": 2.396596908569336,
        "learning_rate": 0.0001993349513620012,
        "epoch": 1.3593709718999742,
        "step": 10546
    },
    {
        "loss": 1.604,
        "grad_norm": 3.235095739364624,
        "learning_rate": 0.00019932792794608159,
        "epoch": 1.359499871100799,
        "step": 10547
    },
    {
        "loss": 1.3396,
        "grad_norm": 2.2418935298919678,
        "learning_rate": 0.00019932086776345702,
        "epoch": 1.359628770301624,
        "step": 10548
    },
    {
        "loss": 1.7465,
        "grad_norm": 1.9890893697738647,
        "learning_rate": 0.00019931377081674093,
        "epoch": 1.3597576695024491,
        "step": 10549
    },
    {
        "loss": 1.9834,
        "grad_norm": 2.1717629432678223,
        "learning_rate": 0.00019930663710856025,
        "epoch": 1.359886568703274,
        "step": 10550
    },
    {
        "loss": 1.604,
        "grad_norm": 3.6551949977874756,
        "learning_rate": 0.00019929946664155556,
        "epoch": 1.360015467904099,
        "step": 10551
    },
    {
        "loss": 1.6732,
        "grad_norm": 2.1249184608459473,
        "learning_rate": 0.00019929225941838112,
        "epoch": 1.3601443671049238,
        "step": 10552
    },
    {
        "loss": 1.6653,
        "grad_norm": 2.507570505142212,
        "learning_rate": 0.00019928501544170458,
        "epoch": 1.3602732663057489,
        "step": 10553
    },
    {
        "loss": 1.8621,
        "grad_norm": 1.6460812091827393,
        "learning_rate": 0.0001992777347142074,
        "epoch": 1.360402165506574,
        "step": 10554
    },
    {
        "loss": 1.2982,
        "grad_norm": 2.4395639896392822,
        "learning_rate": 0.00019927041723858462,
        "epoch": 1.360531064707399,
        "step": 10555
    },
    {
        "loss": 2.3623,
        "grad_norm": 2.6599156856536865,
        "learning_rate": 0.00019926306301754477,
        "epoch": 1.3606599639082237,
        "step": 10556
    },
    {
        "loss": 1.916,
        "grad_norm": 3.0419886112213135,
        "learning_rate": 0.00019925567205381008,
        "epoch": 1.3607888631090488,
        "step": 10557
    },
    {
        "loss": 2.0069,
        "grad_norm": 1.9200390577316284,
        "learning_rate": 0.00019924824435011638,
        "epoch": 1.3609177623098736,
        "step": 10558
    },
    {
        "loss": 2.4518,
        "grad_norm": 2.145151376724243,
        "learning_rate": 0.000199240779909213,
        "epoch": 1.3610466615106986,
        "step": 10559
    },
    {
        "loss": 1.7641,
        "grad_norm": 2.763152599334717,
        "learning_rate": 0.00019923327873386294,
        "epoch": 1.3611755607115237,
        "step": 10560
    },
    {
        "loss": 1.9335,
        "grad_norm": 1.825730323791504,
        "learning_rate": 0.00019922574082684288,
        "epoch": 1.3613044599123485,
        "step": 10561
    },
    {
        "loss": 1.5078,
        "grad_norm": 2.6296818256378174,
        "learning_rate": 0.00019921816619094295,
        "epoch": 1.3614333591131735,
        "step": 10562
    },
    {
        "loss": 1.6494,
        "grad_norm": 2.5455002784729004,
        "learning_rate": 0.00019921055482896696,
        "epoch": 1.3615622583139984,
        "step": 10563
    },
    {
        "loss": 0.8027,
        "grad_norm": 3.4636824131011963,
        "learning_rate": 0.00019920290674373226,
        "epoch": 1.3616911575148234,
        "step": 10564
    },
    {
        "loss": 1.045,
        "grad_norm": 4.156403541564941,
        "learning_rate": 0.00019919522193806986,
        "epoch": 1.3618200567156484,
        "step": 10565
    },
    {
        "loss": 1.5766,
        "grad_norm": 2.2366600036621094,
        "learning_rate": 0.0001991875004148243,
        "epoch": 1.3619489559164732,
        "step": 10566
    },
    {
        "loss": 1.8112,
        "grad_norm": 2.6596577167510986,
        "learning_rate": 0.00019917974217685379,
        "epoch": 1.3620778551172983,
        "step": 10567
    },
    {
        "loss": 1.3997,
        "grad_norm": 2.647467851638794,
        "learning_rate": 0.00019917194722703,
        "epoch": 1.362206754318123,
        "step": 10568
    },
    {
        "loss": 1.243,
        "grad_norm": 3.3871572017669678,
        "learning_rate": 0.0001991641155682383,
        "epoch": 1.3623356535189481,
        "step": 10569
    },
    {
        "loss": 0.9835,
        "grad_norm": 2.4581539630889893,
        "learning_rate": 0.0001991562472033777,
        "epoch": 1.3624645527197732,
        "step": 10570
    },
    {
        "loss": 0.7158,
        "grad_norm": 3.320168972015381,
        "learning_rate": 0.00019914834213536055,
        "epoch": 1.3625934519205982,
        "step": 10571
    },
    {
        "loss": 1.552,
        "grad_norm": 2.4951114654541016,
        "learning_rate": 0.00019914040036711307,
        "epoch": 1.362722351121423,
        "step": 10572
    },
    {
        "loss": 1.7944,
        "grad_norm": 2.6734774112701416,
        "learning_rate": 0.00019913242190157493,
        "epoch": 1.362851250322248,
        "step": 10573
    },
    {
        "loss": 0.7357,
        "grad_norm": 1.9273955821990967,
        "learning_rate": 0.00019912440674169934,
        "epoch": 1.362980149523073,
        "step": 10574
    },
    {
        "loss": 1.9888,
        "grad_norm": 2.4720041751861572,
        "learning_rate": 0.00019911635489045323,
        "epoch": 1.363109048723898,
        "step": 10575
    },
    {
        "loss": 1.6065,
        "grad_norm": 3.2293686866760254,
        "learning_rate": 0.00019910826635081695,
        "epoch": 1.363237947924723,
        "step": 10576
    },
    {
        "loss": 1.5344,
        "grad_norm": 3.0596554279327393,
        "learning_rate": 0.00019910014112578455,
        "epoch": 1.3633668471255478,
        "step": 10577
    },
    {
        "loss": 1.8354,
        "grad_norm": 2.343524932861328,
        "learning_rate": 0.0001990919792183636,
        "epoch": 1.3634957463263728,
        "step": 10578
    },
    {
        "loss": 1.6858,
        "grad_norm": 2.47169828414917,
        "learning_rate": 0.00019908378063157532,
        "epoch": 1.3636246455271976,
        "step": 10579
    },
    {
        "loss": 1.0845,
        "grad_norm": 2.366420269012451,
        "learning_rate": 0.00019907554536845442,
        "epoch": 1.3637535447280227,
        "step": 10580
    },
    {
        "loss": 2.3542,
        "grad_norm": 2.5264668464660645,
        "learning_rate": 0.00019906727343204923,
        "epoch": 1.3638824439288477,
        "step": 10581
    },
    {
        "loss": 1.4169,
        "grad_norm": 3.3370254039764404,
        "learning_rate": 0.00019905896482542163,
        "epoch": 1.3640113431296725,
        "step": 10582
    },
    {
        "loss": 1.6229,
        "grad_norm": 3.240161418914795,
        "learning_rate": 0.0001990506195516471,
        "epoch": 1.3641402423304976,
        "step": 10583
    },
    {
        "loss": 1.3525,
        "grad_norm": 2.5483927726745605,
        "learning_rate": 0.00019904223761381472,
        "epoch": 1.3642691415313224,
        "step": 10584
    },
    {
        "loss": 2.5001,
        "grad_norm": 2.107562780380249,
        "learning_rate": 0.000199033819015027,
        "epoch": 1.3643980407321474,
        "step": 10585
    },
    {
        "loss": 1.8381,
        "grad_norm": 2.7564890384674072,
        "learning_rate": 0.00019902536375840025,
        "epoch": 1.3645269399329725,
        "step": 10586
    },
    {
        "loss": 1.7211,
        "grad_norm": 2.680549383163452,
        "learning_rate": 0.00019901687184706415,
        "epoch": 1.3646558391337973,
        "step": 10587
    },
    {
        "loss": 0.9715,
        "grad_norm": 3.8971455097198486,
        "learning_rate": 0.00019900834328416203,
        "epoch": 1.3647847383346223,
        "step": 10588
    },
    {
        "loss": 1.2021,
        "grad_norm": 3.1087939739227295,
        "learning_rate": 0.0001989997780728508,
        "epoch": 1.3649136375354471,
        "step": 10589
    },
    {
        "loss": 1.4138,
        "grad_norm": 2.3894407749176025,
        "learning_rate": 0.0001989911762163009,
        "epoch": 1.3650425367362722,
        "step": 10590
    },
    {
        "loss": 1.4721,
        "grad_norm": 2.4141552448272705,
        "learning_rate": 0.00019898253771769632,
        "epoch": 1.3651714359370972,
        "step": 10591
    },
    {
        "loss": 1.7526,
        "grad_norm": 2.262747049331665,
        "learning_rate": 0.0001989738625802347,
        "epoch": 1.3653003351379223,
        "step": 10592
    },
    {
        "loss": 1.8114,
        "grad_norm": 3.1486592292785645,
        "learning_rate": 0.00019896515080712715,
        "epoch": 1.365429234338747,
        "step": 10593
    },
    {
        "loss": 1.834,
        "grad_norm": 1.8937023878097534,
        "learning_rate": 0.0001989564024015984,
        "epoch": 1.3655581335395721,
        "step": 10594
    },
    {
        "loss": 0.7809,
        "grad_norm": 2.9278783798217773,
        "learning_rate": 0.00019894761736688666,
        "epoch": 1.365687032740397,
        "step": 10595
    },
    {
        "loss": 1.1201,
        "grad_norm": 3.394256591796875,
        "learning_rate": 0.00019893879570624377,
        "epoch": 1.365815931941222,
        "step": 10596
    },
    {
        "loss": 1.9897,
        "grad_norm": 1.3958407640457153,
        "learning_rate": 0.00019892993742293515,
        "epoch": 1.365944831142047,
        "step": 10597
    },
    {
        "loss": 1.9655,
        "grad_norm": 1.8355257511138916,
        "learning_rate": 0.00019892104252023968,
        "epoch": 1.3660737303428718,
        "step": 10598
    },
    {
        "loss": 0.7376,
        "grad_norm": 3.35719633102417,
        "learning_rate": 0.00019891211100144993,
        "epoch": 1.3662026295436969,
        "step": 10599
    },
    {
        "loss": 1.9572,
        "grad_norm": 2.2704765796661377,
        "learning_rate": 0.00019890314286987186,
        "epoch": 1.3663315287445217,
        "step": 10600
    },
    {
        "loss": 1.785,
        "grad_norm": 2.517739772796631,
        "learning_rate": 0.00019889413812882508,
        "epoch": 1.3664604279453467,
        "step": 10601
    },
    {
        "loss": 1.4532,
        "grad_norm": 2.741222620010376,
        "learning_rate": 0.00019888509678164278,
        "epoch": 1.3665893271461718,
        "step": 10602
    },
    {
        "loss": 1.5433,
        "grad_norm": 2.943516969680786,
        "learning_rate": 0.00019887601883167164,
        "epoch": 1.3667182263469966,
        "step": 10603
    },
    {
        "loss": 1.4867,
        "grad_norm": 2.3024842739105225,
        "learning_rate": 0.00019886690428227194,
        "epoch": 1.3668471255478216,
        "step": 10604
    },
    {
        "loss": 2.0123,
        "grad_norm": 2.208508014678955,
        "learning_rate": 0.00019885775313681735,
        "epoch": 1.3669760247486464,
        "step": 10605
    },
    {
        "loss": 1.8567,
        "grad_norm": 2.7057878971099854,
        "learning_rate": 0.00019884856539869534,
        "epoch": 1.3671049239494715,
        "step": 10606
    },
    {
        "loss": 1.4629,
        "grad_norm": 3.2280282974243164,
        "learning_rate": 0.00019883934107130675,
        "epoch": 1.3672338231502965,
        "step": 10607
    },
    {
        "loss": 1.1658,
        "grad_norm": 3.9061622619628906,
        "learning_rate": 0.000198830080158066,
        "epoch": 1.3673627223511216,
        "step": 10608
    },
    {
        "loss": 2.2015,
        "grad_norm": 1.8187663555145264,
        "learning_rate": 0.00019882078266240109,
        "epoch": 1.3674916215519464,
        "step": 10609
    },
    {
        "loss": 1.6114,
        "grad_norm": 3.6726231575012207,
        "learning_rate": 0.00019881144858775352,
        "epoch": 1.3676205207527714,
        "step": 10610
    },
    {
        "loss": 1.3512,
        "grad_norm": 2.566499710083008,
        "learning_rate": 0.0001988020779375783,
        "epoch": 1.3677494199535962,
        "step": 10611
    },
    {
        "loss": 2.2289,
        "grad_norm": 1.8153691291809082,
        "learning_rate": 0.00019879267071534407,
        "epoch": 1.3678783191544213,
        "step": 10612
    },
    {
        "loss": 1.6369,
        "grad_norm": 2.2456912994384766,
        "learning_rate": 0.00019878322692453294,
        "epoch": 1.3680072183552463,
        "step": 10613
    },
    {
        "loss": 1.8564,
        "grad_norm": 2.605085611343384,
        "learning_rate": 0.0001987737465686406,
        "epoch": 1.3681361175560711,
        "step": 10614
    },
    {
        "loss": 2.3299,
        "grad_norm": 1.5966047048568726,
        "learning_rate": 0.0001987642296511762,
        "epoch": 1.3682650167568962,
        "step": 10615
    },
    {
        "loss": 1.8064,
        "grad_norm": 2.106419324874878,
        "learning_rate": 0.00019875467617566251,
        "epoch": 1.368393915957721,
        "step": 10616
    },
    {
        "loss": 1.7158,
        "grad_norm": 2.362347364425659,
        "learning_rate": 0.00019874508614563577,
        "epoch": 1.368522815158546,
        "step": 10617
    },
    {
        "loss": 1.1288,
        "grad_norm": 3.3373987674713135,
        "learning_rate": 0.00019873545956464582,
        "epoch": 1.368651714359371,
        "step": 10618
    },
    {
        "loss": 1.0814,
        "grad_norm": 2.243767261505127,
        "learning_rate": 0.00019872579643625594,
        "epoch": 1.3687806135601959,
        "step": 10619
    },
    {
        "loss": 1.7713,
        "grad_norm": 2.707076072692871,
        "learning_rate": 0.00019871609676404297,
        "epoch": 1.368909512761021,
        "step": 10620
    },
    {
        "loss": 1.065,
        "grad_norm": 2.299307346343994,
        "learning_rate": 0.00019870636055159736,
        "epoch": 1.3690384119618457,
        "step": 10621
    },
    {
        "loss": 2.0141,
        "grad_norm": 2.110002279281616,
        "learning_rate": 0.0001986965878025229,
        "epoch": 1.3691673111626708,
        "step": 10622
    },
    {
        "loss": 0.9891,
        "grad_norm": 1.3907829523086548,
        "learning_rate": 0.00019868677852043716,
        "epoch": 1.3692962103634958,
        "step": 10623
    },
    {
        "loss": 2.3421,
        "grad_norm": 2.8274731636047363,
        "learning_rate": 0.000198676932708971,
        "epoch": 1.3694251095643206,
        "step": 10624
    },
    {
        "loss": 1.4072,
        "grad_norm": 2.9158172607421875,
        "learning_rate": 0.0001986670503717689,
        "epoch": 1.3695540087651457,
        "step": 10625
    },
    {
        "loss": 1.6527,
        "grad_norm": 2.54668927192688,
        "learning_rate": 0.0001986571315124889,
        "epoch": 1.3696829079659705,
        "step": 10626
    },
    {
        "loss": 1.7676,
        "grad_norm": 2.0861294269561768,
        "learning_rate": 0.00019864717613480245,
        "epoch": 1.3698118071667955,
        "step": 10627
    },
    {
        "loss": 2.0476,
        "grad_norm": 2.2606921195983887,
        "learning_rate": 0.00019863718424239463,
        "epoch": 1.3699407063676206,
        "step": 10628
    },
    {
        "loss": 1.368,
        "grad_norm": 3.00273060798645,
        "learning_rate": 0.000198627155838964,
        "epoch": 1.3700696055684456,
        "step": 10629
    },
    {
        "loss": 2.0147,
        "grad_norm": 2.344874143600464,
        "learning_rate": 0.00019861709092822255,
        "epoch": 1.3701985047692704,
        "step": 10630
    },
    {
        "loss": 1.3652,
        "grad_norm": 2.5704333782196045,
        "learning_rate": 0.0001986069895138959,
        "epoch": 1.3703274039700954,
        "step": 10631
    },
    {
        "loss": 1.4081,
        "grad_norm": 2.185457468032837,
        "learning_rate": 0.00019859685159972316,
        "epoch": 1.3704563031709203,
        "step": 10632
    },
    {
        "loss": 2.2338,
        "grad_norm": 2.3796212673187256,
        "learning_rate": 0.00019858667718945688,
        "epoch": 1.3705852023717453,
        "step": 10633
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.0868873596191406,
        "learning_rate": 0.0001985764662868632,
        "epoch": 1.3707141015725703,
        "step": 10634
    },
    {
        "loss": 1.7446,
        "grad_norm": 3.0974111557006836,
        "learning_rate": 0.00019856621889572173,
        "epoch": 1.3708430007733952,
        "step": 10635
    },
    {
        "loss": 1.6463,
        "grad_norm": 2.878629684448242,
        "learning_rate": 0.00019855593501982553,
        "epoch": 1.3709718999742202,
        "step": 10636
    },
    {
        "loss": 1.4477,
        "grad_norm": 1.5315786600112915,
        "learning_rate": 0.00019854561466298135,
        "epoch": 1.371100799175045,
        "step": 10637
    },
    {
        "loss": 1.4297,
        "grad_norm": 2.5002102851867676,
        "learning_rate": 0.0001985352578290092,
        "epoch": 1.37122969837587,
        "step": 10638
    },
    {
        "loss": 2.0233,
        "grad_norm": 3.0060274600982666,
        "learning_rate": 0.00019852486452174277,
        "epoch": 1.371358597576695,
        "step": 10639
    },
    {
        "loss": 1.3375,
        "grad_norm": 2.7898988723754883,
        "learning_rate": 0.00019851443474502918,
        "epoch": 1.37148749677752,
        "step": 10640
    },
    {
        "loss": 1.902,
        "grad_norm": 2.3420276641845703,
        "learning_rate": 0.00019850396850272904,
        "epoch": 1.371616395978345,
        "step": 10641
    },
    {
        "loss": 1.7012,
        "grad_norm": 2.6798110008239746,
        "learning_rate": 0.00019849346579871652,
        "epoch": 1.3717452951791698,
        "step": 10642
    },
    {
        "loss": 2.2152,
        "grad_norm": 2.074026107788086,
        "learning_rate": 0.00019848292663687922,
        "epoch": 1.3718741943799948,
        "step": 10643
    },
    {
        "loss": 2.1128,
        "grad_norm": 2.1705803871154785,
        "learning_rate": 0.0001984723510211183,
        "epoch": 1.3720030935808198,
        "step": 10644
    },
    {
        "loss": 1.156,
        "grad_norm": 2.7719430923461914,
        "learning_rate": 0.0001984617389553483,
        "epoch": 1.3721319927816449,
        "step": 10645
    },
    {
        "loss": 1.8594,
        "grad_norm": 3.8310437202453613,
        "learning_rate": 0.00019845109044349738,
        "epoch": 1.3722608919824697,
        "step": 10646
    },
    {
        "loss": 1.9728,
        "grad_norm": 2.1353282928466797,
        "learning_rate": 0.00019844040548950713,
        "epoch": 1.3723897911832947,
        "step": 10647
    },
    {
        "loss": 1.0838,
        "grad_norm": 3.249664068222046,
        "learning_rate": 0.00019842968409733262,
        "epoch": 1.3725186903841196,
        "step": 10648
    },
    {
        "loss": 1.6903,
        "grad_norm": 2.779414653778076,
        "learning_rate": 0.00019841892627094244,
        "epoch": 1.3726475895849446,
        "step": 10649
    },
    {
        "loss": 1.8789,
        "grad_norm": 1.471785306930542,
        "learning_rate": 0.00019840813201431868,
        "epoch": 1.3727764887857696,
        "step": 10650
    },
    {
        "loss": 1.6925,
        "grad_norm": 2.118299961090088,
        "learning_rate": 0.0001983973013314568,
        "epoch": 1.3729053879865944,
        "step": 10651
    },
    {
        "loss": 1.9221,
        "grad_norm": 2.0262928009033203,
        "learning_rate": 0.00019838643422636593,
        "epoch": 1.3730342871874195,
        "step": 10652
    },
    {
        "loss": 1.4863,
        "grad_norm": 2.322993040084839,
        "learning_rate": 0.0001983755307030685,
        "epoch": 1.3731631863882443,
        "step": 10653
    },
    {
        "loss": 1.7549,
        "grad_norm": 2.578127384185791,
        "learning_rate": 0.00019836459076560053,
        "epoch": 1.3732920855890693,
        "step": 10654
    },
    {
        "loss": 1.9704,
        "grad_norm": 2.033707857131958,
        "learning_rate": 0.0001983536144180115,
        "epoch": 1.3734209847898944,
        "step": 10655
    },
    {
        "loss": 1.938,
        "grad_norm": 2.6305150985717773,
        "learning_rate": 0.00019834260166436436,
        "epoch": 1.3735498839907192,
        "step": 10656
    },
    {
        "loss": 0.717,
        "grad_norm": 2.864579439163208,
        "learning_rate": 0.0001983315525087355,
        "epoch": 1.3736787831915442,
        "step": 10657
    },
    {
        "loss": 1.7711,
        "grad_norm": 2.6850388050079346,
        "learning_rate": 0.00019832046695521488,
        "epoch": 1.373807682392369,
        "step": 10658
    },
    {
        "loss": 1.6057,
        "grad_norm": 1.7331831455230713,
        "learning_rate": 0.0001983093450079058,
        "epoch": 1.373936581593194,
        "step": 10659
    },
    {
        "loss": 1.4773,
        "grad_norm": 2.7900285720825195,
        "learning_rate": 0.00019829818667092513,
        "epoch": 1.3740654807940191,
        "step": 10660
    },
    {
        "loss": 2.2727,
        "grad_norm": 1.868661880493164,
        "learning_rate": 0.0001982869919484032,
        "epoch": 1.374194379994844,
        "step": 10661
    },
    {
        "loss": 1.1485,
        "grad_norm": 3.4813640117645264,
        "learning_rate": 0.00019827576084448375,
        "epoch": 1.374323279195669,
        "step": 10662
    },
    {
        "loss": 2.1123,
        "grad_norm": 1.7914538383483887,
        "learning_rate": 0.00019826449336332407,
        "epoch": 1.3744521783964938,
        "step": 10663
    },
    {
        "loss": 1.7507,
        "grad_norm": 1.4401105642318726,
        "learning_rate": 0.00019825318950909487,
        "epoch": 1.3745810775973188,
        "step": 10664
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.118940830230713,
        "learning_rate": 0.0001982418492859803,
        "epoch": 1.3747099767981439,
        "step": 10665
    },
    {
        "loss": 1.6896,
        "grad_norm": 1.8700764179229736,
        "learning_rate": 0.000198230472698178,
        "epoch": 1.374838875998969,
        "step": 10666
    },
    {
        "loss": 1.7057,
        "grad_norm": 1.7287946939468384,
        "learning_rate": 0.00019821905974989906,
        "epoch": 1.3749677751997937,
        "step": 10667
    },
    {
        "loss": 1.6194,
        "grad_norm": 2.9708659648895264,
        "learning_rate": 0.00019820761044536808,
        "epoch": 1.3750966744006188,
        "step": 10668
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.756169557571411,
        "learning_rate": 0.00019819612478882298,
        "epoch": 1.3752255736014436,
        "step": 10669
    },
    {
        "loss": 1.5418,
        "grad_norm": 2.7481722831726074,
        "learning_rate": 0.00019818460278451535,
        "epoch": 1.3753544728022686,
        "step": 10670
    },
    {
        "loss": 1.6281,
        "grad_norm": 2.084843635559082,
        "learning_rate": 0.00019817304443671004,
        "epoch": 1.3754833720030937,
        "step": 10671
    },
    {
        "loss": 1.2109,
        "grad_norm": 3.0114376544952393,
        "learning_rate": 0.00019816144974968544,
        "epoch": 1.3756122712039185,
        "step": 10672
    },
    {
        "loss": 1.425,
        "grad_norm": 2.4029293060302734,
        "learning_rate": 0.0001981498187277334,
        "epoch": 1.3757411704047435,
        "step": 10673
    },
    {
        "loss": 1.4429,
        "grad_norm": 3.273010730743408,
        "learning_rate": 0.0001981381513751592,
        "epoch": 1.3758700696055683,
        "step": 10674
    },
    {
        "loss": 2.2903,
        "grad_norm": 1.7664775848388672,
        "learning_rate": 0.00019812644769628152,
        "epoch": 1.3759989688063934,
        "step": 10675
    },
    {
        "loss": 1.4819,
        "grad_norm": 1.886861801147461,
        "learning_rate": 0.00019811470769543258,
        "epoch": 1.3761278680072184,
        "step": 10676
    },
    {
        "loss": 1.0175,
        "grad_norm": 3.2167775630950928,
        "learning_rate": 0.00019810293137695797,
        "epoch": 1.3762567672080432,
        "step": 10677
    },
    {
        "loss": 1.2814,
        "grad_norm": 3.159041404724121,
        "learning_rate": 0.0001980911187452168,
        "epoch": 1.3763856664088683,
        "step": 10678
    },
    {
        "loss": 1.109,
        "grad_norm": 2.580082654953003,
        "learning_rate": 0.00019807926980458147,
        "epoch": 1.376514565609693,
        "step": 10679
    },
    {
        "loss": 2.4799,
        "grad_norm": 2.072554588317871,
        "learning_rate": 0.000198067384559438,
        "epoch": 1.3766434648105181,
        "step": 10680
    },
    {
        "loss": 0.9335,
        "grad_norm": 2.1745071411132812,
        "learning_rate": 0.00019805546301418576,
        "epoch": 1.3767723640113432,
        "step": 10681
    },
    {
        "loss": 0.798,
        "grad_norm": 3.672658920288086,
        "learning_rate": 0.00019804350517323757,
        "epoch": 1.3769012632121682,
        "step": 10682
    },
    {
        "loss": 1.6635,
        "grad_norm": 2.0930135250091553,
        "learning_rate": 0.00019803151104101966,
        "epoch": 1.377030162412993,
        "step": 10683
    },
    {
        "loss": 2.2509,
        "grad_norm": 1.8981379270553589,
        "learning_rate": 0.00019801948062197174,
        "epoch": 1.377159061613818,
        "step": 10684
    },
    {
        "loss": 1.3637,
        "grad_norm": 2.7185096740722656,
        "learning_rate": 0.00019800741392054692,
        "epoch": 1.3772879608146429,
        "step": 10685
    },
    {
        "loss": 0.7477,
        "grad_norm": 2.2832517623901367,
        "learning_rate": 0.00019799531094121173,
        "epoch": 1.377416860015468,
        "step": 10686
    },
    {
        "loss": 1.8467,
        "grad_norm": 2.6078438758850098,
        "learning_rate": 0.0001979831716884461,
        "epoch": 1.377545759216293,
        "step": 10687
    },
    {
        "loss": 1.3701,
        "grad_norm": 6.217695713043213,
        "learning_rate": 0.00019797099616674358,
        "epoch": 1.3776746584171178,
        "step": 10688
    },
    {
        "loss": 1.7059,
        "grad_norm": 2.303427219390869,
        "learning_rate": 0.00019795878438061087,
        "epoch": 1.3778035576179428,
        "step": 10689
    },
    {
        "loss": 1.8591,
        "grad_norm": 2.2207937240600586,
        "learning_rate": 0.00019794653633456823,
        "epoch": 1.3779324568187676,
        "step": 10690
    },
    {
        "loss": 1.4382,
        "grad_norm": 2.764880657196045,
        "learning_rate": 0.0001979342520331494,
        "epoch": 1.3780613560195927,
        "step": 10691
    },
    {
        "loss": 2.0822,
        "grad_norm": 2.0587165355682373,
        "learning_rate": 0.0001979219314809014,
        "epoch": 1.3781902552204177,
        "step": 10692
    },
    {
        "loss": 1.7574,
        "grad_norm": 2.1574089527130127,
        "learning_rate": 0.00019790957468238476,
        "epoch": 1.3783191544212425,
        "step": 10693
    },
    {
        "loss": 2.0814,
        "grad_norm": 1.9867805242538452,
        "learning_rate": 0.00019789718164217344,
        "epoch": 1.3784480536220676,
        "step": 10694
    },
    {
        "loss": 1.7966,
        "grad_norm": 2.4996817111968994,
        "learning_rate": 0.00019788475236485477,
        "epoch": 1.3785769528228924,
        "step": 10695
    },
    {
        "loss": 1.1482,
        "grad_norm": 3.172222137451172,
        "learning_rate": 0.0001978722868550295,
        "epoch": 1.3787058520237174,
        "step": 10696
    },
    {
        "loss": 1.2392,
        "grad_norm": 2.360825538635254,
        "learning_rate": 0.0001978597851173118,
        "epoch": 1.3788347512245425,
        "step": 10697
    },
    {
        "loss": 2.0093,
        "grad_norm": 2.048654079437256,
        "learning_rate": 0.0001978472471563292,
        "epoch": 1.3789636504253673,
        "step": 10698
    },
    {
        "loss": 1.1084,
        "grad_norm": 3.2221927642822266,
        "learning_rate": 0.00019783467297672277,
        "epoch": 1.3790925496261923,
        "step": 10699
    },
    {
        "loss": 2.1309,
        "grad_norm": 2.6407408714294434,
        "learning_rate": 0.0001978220625831469,
        "epoch": 1.3792214488270171,
        "step": 10700
    },
    {
        "loss": 1.8586,
        "grad_norm": 1.6233166456222534,
        "learning_rate": 0.0001978094159802693,
        "epoch": 1.3793503480278422,
        "step": 10701
    },
    {
        "loss": 1.296,
        "grad_norm": 6.234931945800781,
        "learning_rate": 0.00019779673317277122,
        "epoch": 1.3794792472286672,
        "step": 10702
    },
    {
        "loss": 1.4982,
        "grad_norm": 2.962768793106079,
        "learning_rate": 0.00019778401416534726,
        "epoch": 1.3796081464294923,
        "step": 10703
    },
    {
        "loss": 1.0389,
        "grad_norm": 3.164734125137329,
        "learning_rate": 0.0001977712589627054,
        "epoch": 1.379737045630317,
        "step": 10704
    },
    {
        "loss": 2.1234,
        "grad_norm": 1.736432433128357,
        "learning_rate": 0.0001977584675695671,
        "epoch": 1.379865944831142,
        "step": 10705
    },
    {
        "loss": 2.0784,
        "grad_norm": 2.1560819149017334,
        "learning_rate": 0.0001977456399906671,
        "epoch": 1.379994844031967,
        "step": 10706
    },
    {
        "loss": 0.8564,
        "grad_norm": 2.774643898010254,
        "learning_rate": 0.0001977327762307536,
        "epoch": 1.380123743232792,
        "step": 10707
    },
    {
        "loss": 2.5995,
        "grad_norm": 1.4973008632659912,
        "learning_rate": 0.00019771987629458823,
        "epoch": 1.380252642433617,
        "step": 10708
    },
    {
        "loss": 0.9029,
        "grad_norm": 3.633539915084839,
        "learning_rate": 0.0001977069401869459,
        "epoch": 1.3803815416344418,
        "step": 10709
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.4703545570373535,
        "learning_rate": 0.00019769396791261495,
        "epoch": 1.3805104408352669,
        "step": 10710
    },
    {
        "loss": 2.1133,
        "grad_norm": 1.7407644987106323,
        "learning_rate": 0.00019768095947639722,
        "epoch": 1.3806393400360917,
        "step": 10711
    },
    {
        "loss": 1.6645,
        "grad_norm": 2.061347007751465,
        "learning_rate": 0.00019766791488310775,
        "epoch": 1.3807682392369167,
        "step": 10712
    },
    {
        "loss": 1.6408,
        "grad_norm": 2.336507558822632,
        "learning_rate": 0.00019765483413757516,
        "epoch": 1.3808971384377418,
        "step": 10713
    },
    {
        "loss": 1.4697,
        "grad_norm": 2.7185471057891846,
        "learning_rate": 0.00019764171724464126,
        "epoch": 1.3810260376385666,
        "step": 10714
    },
    {
        "loss": 1.5532,
        "grad_norm": 2.401308298110962,
        "learning_rate": 0.00019762856420916136,
        "epoch": 1.3811549368393916,
        "step": 10715
    },
    {
        "loss": 1.2111,
        "grad_norm": 2.6900057792663574,
        "learning_rate": 0.00019761537503600415,
        "epoch": 1.3812838360402164,
        "step": 10716
    },
    {
        "loss": 2.1009,
        "grad_norm": 2.327009916305542,
        "learning_rate": 0.0001976021497300516,
        "epoch": 1.3814127352410415,
        "step": 10717
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.553701400756836,
        "learning_rate": 0.0001975888882961992,
        "epoch": 1.3815416344418665,
        "step": 10718
    },
    {
        "loss": 1.8484,
        "grad_norm": 3.11714243888855,
        "learning_rate": 0.00019757559073935567,
        "epoch": 1.3816705336426915,
        "step": 10719
    },
    {
        "loss": 1.9193,
        "grad_norm": 2.493805408477783,
        "learning_rate": 0.0001975622570644432,
        "epoch": 1.3817994328435164,
        "step": 10720
    },
    {
        "loss": 1.6052,
        "grad_norm": 3.474025249481201,
        "learning_rate": 0.00019754888727639727,
        "epoch": 1.3819283320443414,
        "step": 10721
    },
    {
        "loss": 1.58,
        "grad_norm": 2.485995292663574,
        "learning_rate": 0.0001975354813801668,
        "epoch": 1.3820572312451662,
        "step": 10722
    },
    {
        "loss": 1.9547,
        "grad_norm": 2.1048026084899902,
        "learning_rate": 0.00019752203938071405,
        "epoch": 1.3821861304459913,
        "step": 10723
    },
    {
        "loss": 1.3449,
        "grad_norm": 2.2745769023895264,
        "learning_rate": 0.00019750856128301466,
        "epoch": 1.3823150296468163,
        "step": 10724
    },
    {
        "loss": 0.6699,
        "grad_norm": 3.001002311706543,
        "learning_rate": 0.00019749504709205758,
        "epoch": 1.3824439288476411,
        "step": 10725
    },
    {
        "loss": 1.5514,
        "grad_norm": 2.965644359588623,
        "learning_rate": 0.00019748149681284512,
        "epoch": 1.3825728280484662,
        "step": 10726
    },
    {
        "loss": 1.2433,
        "grad_norm": 2.7113335132598877,
        "learning_rate": 0.00019746791045039303,
        "epoch": 1.382701727249291,
        "step": 10727
    },
    {
        "loss": 1.5123,
        "grad_norm": 1.7004724740982056,
        "learning_rate": 0.00019745428800973036,
        "epoch": 1.382830626450116,
        "step": 10728
    },
    {
        "loss": 0.548,
        "grad_norm": 3.8638429641723633,
        "learning_rate": 0.00019744062949589953,
        "epoch": 1.382959525650941,
        "step": 10729
    },
    {
        "loss": 2.0041,
        "grad_norm": 1.454460620880127,
        "learning_rate": 0.00019742693491395631,
        "epoch": 1.3830884248517659,
        "step": 10730
    },
    {
        "loss": 1.6705,
        "grad_norm": 2.793400764465332,
        "learning_rate": 0.00019741320426896976,
        "epoch": 1.383217324052591,
        "step": 10731
    },
    {
        "loss": 1.5857,
        "grad_norm": 3.326897382736206,
        "learning_rate": 0.0001973994375660224,
        "epoch": 1.3833462232534157,
        "step": 10732
    },
    {
        "loss": 2.2703,
        "grad_norm": 2.1757419109344482,
        "learning_rate": 0.00019738563481021002,
        "epoch": 1.3834751224542408,
        "step": 10733
    },
    {
        "loss": 2.3496,
        "grad_norm": 1.7797701358795166,
        "learning_rate": 0.00019737179600664176,
        "epoch": 1.3836040216550658,
        "step": 10734
    },
    {
        "loss": 0.4886,
        "grad_norm": 2.7982380390167236,
        "learning_rate": 0.0001973579211604401,
        "epoch": 1.3837329208558906,
        "step": 10735
    },
    {
        "loss": 1.5257,
        "grad_norm": 2.610387086868286,
        "learning_rate": 0.00019734401027674103,
        "epoch": 1.3838618200567157,
        "step": 10736
    },
    {
        "loss": 2.5657,
        "grad_norm": 2.2413086891174316,
        "learning_rate": 0.00019733006336069352,
        "epoch": 1.3839907192575405,
        "step": 10737
    },
    {
        "loss": 1.3959,
        "grad_norm": 2.8545045852661133,
        "learning_rate": 0.00019731608041746016,
        "epoch": 1.3841196184583655,
        "step": 10738
    },
    {
        "loss": 2.0259,
        "grad_norm": 2.609917163848877,
        "learning_rate": 0.00019730206145221688,
        "epoch": 1.3842485176591905,
        "step": 10739
    },
    {
        "loss": 1.3681,
        "grad_norm": 3.0151777267456055,
        "learning_rate": 0.0001972880064701528,
        "epoch": 1.3843774168600156,
        "step": 10740
    },
    {
        "loss": 1.3521,
        "grad_norm": 2.024779796600342,
        "learning_rate": 0.00019727391547647046,
        "epoch": 1.3845063160608404,
        "step": 10741
    },
    {
        "loss": 1.5868,
        "grad_norm": 1.4275341033935547,
        "learning_rate": 0.0001972597884763857,
        "epoch": 1.3846352152616654,
        "step": 10742
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.9875305891036987,
        "learning_rate": 0.0001972456254751277,
        "epoch": 1.3847641144624903,
        "step": 10743
    },
    {
        "loss": 1.9857,
        "grad_norm": 2.8209786415100098,
        "learning_rate": 0.0001972314264779389,
        "epoch": 1.3848930136633153,
        "step": 10744
    },
    {
        "loss": 1.5122,
        "grad_norm": 2.4339675903320312,
        "learning_rate": 0.0001972171914900752,
        "epoch": 1.3850219128641403,
        "step": 10745
    },
    {
        "loss": 1.7015,
        "grad_norm": 3.531738519668579,
        "learning_rate": 0.00019720292051680577,
        "epoch": 1.3851508120649652,
        "step": 10746
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.6609790325164795,
        "learning_rate": 0.00019718861356341302,
        "epoch": 1.3852797112657902,
        "step": 10747
    },
    {
        "loss": 0.5323,
        "grad_norm": 2.4095053672790527,
        "learning_rate": 0.00019717427063519277,
        "epoch": 1.385408610466615,
        "step": 10748
    },
    {
        "loss": 1.7921,
        "grad_norm": 2.011709690093994,
        "learning_rate": 0.00019715989173745406,
        "epoch": 1.38553750966744,
        "step": 10749
    },
    {
        "loss": 2.0832,
        "grad_norm": 2.850379228591919,
        "learning_rate": 0.0001971454768755194,
        "epoch": 1.385666408868265,
        "step": 10750
    },
    {
        "loss": 1.5616,
        "grad_norm": 2.671144723892212,
        "learning_rate": 0.00019713102605472448,
        "epoch": 1.38579530806909,
        "step": 10751
    },
    {
        "loss": 2.0843,
        "grad_norm": 1.8977665901184082,
        "learning_rate": 0.0001971165392804183,
        "epoch": 1.385924207269915,
        "step": 10752
    },
    {
        "loss": 0.4271,
        "grad_norm": 3.5704495906829834,
        "learning_rate": 0.0001971020165579633,
        "epoch": 1.3860531064707398,
        "step": 10753
    },
    {
        "loss": 1.0144,
        "grad_norm": 3.6544876098632812,
        "learning_rate": 0.00019708745789273507,
        "epoch": 1.3861820056715648,
        "step": 10754
    },
    {
        "loss": 1.5992,
        "grad_norm": 2.8269565105438232,
        "learning_rate": 0.0001970728632901225,
        "epoch": 1.3863109048723898,
        "step": 10755
    },
    {
        "loss": 2.4438,
        "grad_norm": 1.7924275398254395,
        "learning_rate": 0.00019705823275552802,
        "epoch": 1.3864398040732149,
        "step": 10756
    },
    {
        "loss": 2.3421,
        "grad_norm": 2.4422054290771484,
        "learning_rate": 0.00019704356629436708,
        "epoch": 1.3865687032740397,
        "step": 10757
    },
    {
        "loss": 1.9405,
        "grad_norm": 3.58713436126709,
        "learning_rate": 0.00019702886391206854,
        "epoch": 1.3866976024748647,
        "step": 10758
    },
    {
        "loss": 1.8062,
        "grad_norm": 1.8481166362762451,
        "learning_rate": 0.00019701412561407464,
        "epoch": 1.3868265016756895,
        "step": 10759
    },
    {
        "loss": 1.2025,
        "grad_norm": 2.1134250164031982,
        "learning_rate": 0.0001969993514058407,
        "epoch": 1.3869554008765146,
        "step": 10760
    },
    {
        "loss": 1.0156,
        "grad_norm": 4.319789886474609,
        "learning_rate": 0.00019698454129283554,
        "epoch": 1.3870843000773396,
        "step": 10761
    },
    {
        "loss": 1.1216,
        "grad_norm": 4.248374938964844,
        "learning_rate": 0.00019696969528054122,
        "epoch": 1.3872131992781644,
        "step": 10762
    },
    {
        "loss": 1.6457,
        "grad_norm": 3.6453680992126465,
        "learning_rate": 0.00019695481337445302,
        "epoch": 1.3873420984789895,
        "step": 10763
    },
    {
        "loss": 1.6184,
        "grad_norm": 2.0041897296905518,
        "learning_rate": 0.00019693989558007956,
        "epoch": 1.3874709976798143,
        "step": 10764
    },
    {
        "loss": 0.9827,
        "grad_norm": 3.700777530670166,
        "learning_rate": 0.00019692494190294275,
        "epoch": 1.3875998968806393,
        "step": 10765
    },
    {
        "loss": 1.8742,
        "grad_norm": 2.4052622318267822,
        "learning_rate": 0.00019690995234857773,
        "epoch": 1.3877287960814644,
        "step": 10766
    },
    {
        "loss": 1.359,
        "grad_norm": 2.270899772644043,
        "learning_rate": 0.00019689492692253298,
        "epoch": 1.3878576952822892,
        "step": 10767
    },
    {
        "loss": 2.1945,
        "grad_norm": 1.6775236129760742,
        "learning_rate": 0.0001968798656303702,
        "epoch": 1.3879865944831142,
        "step": 10768
    },
    {
        "loss": 1.738,
        "grad_norm": 1.8797285556793213,
        "learning_rate": 0.00019686476847766442,
        "epoch": 1.388115493683939,
        "step": 10769
    },
    {
        "loss": 1.9159,
        "grad_norm": 1.388758659362793,
        "learning_rate": 0.00019684963547000395,
        "epoch": 1.388244392884764,
        "step": 10770
    },
    {
        "loss": 1.2012,
        "grad_norm": 3.038853406906128,
        "learning_rate": 0.00019683446661299033,
        "epoch": 1.3883732920855891,
        "step": 10771
    },
    {
        "loss": 2.2824,
        "grad_norm": 2.1726863384246826,
        "learning_rate": 0.00019681926191223833,
        "epoch": 1.388502191286414,
        "step": 10772
    },
    {
        "loss": 1.9411,
        "grad_norm": 2.410606622695923,
        "learning_rate": 0.0001968040213733761,
        "epoch": 1.388631090487239,
        "step": 10773
    },
    {
        "loss": 1.8639,
        "grad_norm": 2.348902702331543,
        "learning_rate": 0.00019678874500204501,
        "epoch": 1.3887599896880638,
        "step": 10774
    },
    {
        "loss": 1.8331,
        "grad_norm": 2.7084691524505615,
        "learning_rate": 0.00019677343280389962,
        "epoch": 1.3888888888888888,
        "step": 10775
    },
    {
        "loss": 1.6796,
        "grad_norm": 2.013458728790283,
        "learning_rate": 0.00019675808478460788,
        "epoch": 1.3890177880897139,
        "step": 10776
    },
    {
        "loss": 0.3717,
        "grad_norm": 1.8787481784820557,
        "learning_rate": 0.0001967427009498509,
        "epoch": 1.389146687290539,
        "step": 10777
    },
    {
        "loss": 1.4252,
        "grad_norm": 2.4534149169921875,
        "learning_rate": 0.00019672728130532306,
        "epoch": 1.3892755864913637,
        "step": 10778
    },
    {
        "loss": 1.417,
        "grad_norm": 1.9679105281829834,
        "learning_rate": 0.00019671182585673203,
        "epoch": 1.3894044856921888,
        "step": 10779
    },
    {
        "loss": 1.7124,
        "grad_norm": 1.8622859716415405,
        "learning_rate": 0.00019669633460979875,
        "epoch": 1.3895333848930136,
        "step": 10780
    },
    {
        "loss": 1.7754,
        "grad_norm": 2.9632537364959717,
        "learning_rate": 0.00019668080757025735,
        "epoch": 1.3896622840938386,
        "step": 10781
    },
    {
        "loss": 1.7431,
        "grad_norm": 3.39819598197937,
        "learning_rate": 0.00019666524474385523,
        "epoch": 1.3897911832946637,
        "step": 10782
    },
    {
        "loss": 1.6487,
        "grad_norm": 1.961337685585022,
        "learning_rate": 0.00019664964613635307,
        "epoch": 1.3899200824954885,
        "step": 10783
    },
    {
        "loss": 2.0464,
        "grad_norm": 2.492330551147461,
        "learning_rate": 0.00019663401175352475,
        "epoch": 1.3900489816963135,
        "step": 10784
    },
    {
        "loss": 1.8712,
        "grad_norm": 1.8650153875350952,
        "learning_rate": 0.00019661834160115743,
        "epoch": 1.3901778808971383,
        "step": 10785
    },
    {
        "loss": 2.5583,
        "grad_norm": 3.3668832778930664,
        "learning_rate": 0.0001966026356850514,
        "epoch": 1.3903067800979634,
        "step": 10786
    },
    {
        "loss": 1.1324,
        "grad_norm": 2.6243069171905518,
        "learning_rate": 0.00019658689401102046,
        "epoch": 1.3904356792987884,
        "step": 10787
    },
    {
        "loss": 1.2982,
        "grad_norm": 4.776968955993652,
        "learning_rate": 0.0001965711165848913,
        "epoch": 1.3905645784996132,
        "step": 10788
    },
    {
        "loss": 1.876,
        "grad_norm": 2.2828147411346436,
        "learning_rate": 0.00019655530341250406,
        "epoch": 1.3906934777004383,
        "step": 10789
    },
    {
        "loss": 0.9336,
        "grad_norm": 2.6280605792999268,
        "learning_rate": 0.0001965394544997121,
        "epoch": 1.390822376901263,
        "step": 10790
    },
    {
        "loss": 1.2401,
        "grad_norm": 2.54670786857605,
        "learning_rate": 0.0001965235698523819,
        "epoch": 1.3909512761020881,
        "step": 10791
    },
    {
        "loss": 2.3161,
        "grad_norm": 1.668601155281067,
        "learning_rate": 0.0001965076494763933,
        "epoch": 1.3910801753029132,
        "step": 10792
    },
    {
        "loss": 1.8318,
        "grad_norm": 2.9850306510925293,
        "learning_rate": 0.00019649169337763932,
        "epoch": 1.3912090745037382,
        "step": 10793
    },
    {
        "loss": 1.676,
        "grad_norm": 3.580932140350342,
        "learning_rate": 0.00019647570156202613,
        "epoch": 1.391337973704563,
        "step": 10794
    },
    {
        "loss": 1.2864,
        "grad_norm": 2.967090606689453,
        "learning_rate": 0.00019645967403547317,
        "epoch": 1.391466872905388,
        "step": 10795
    },
    {
        "loss": 0.844,
        "grad_norm": 3.4229283332824707,
        "learning_rate": 0.00019644361080391315,
        "epoch": 1.3915957721062129,
        "step": 10796
    },
    {
        "loss": 1.7185,
        "grad_norm": 2.35296368598938,
        "learning_rate": 0.0001964275118732919,
        "epoch": 1.391724671307038,
        "step": 10797
    },
    {
        "loss": 0.5527,
        "grad_norm": 2.3898229598999023,
        "learning_rate": 0.0001964113772495686,
        "epoch": 1.391853570507863,
        "step": 10798
    },
    {
        "loss": 1.7206,
        "grad_norm": 1.5460656881332397,
        "learning_rate": 0.00019639520693871546,
        "epoch": 1.3919824697086878,
        "step": 10799
    },
    {
        "loss": 1.7332,
        "grad_norm": 2.6865732669830322,
        "learning_rate": 0.00019637900094671807,
        "epoch": 1.3921113689095128,
        "step": 10800
    },
    {
        "loss": 1.5308,
        "grad_norm": 1.450163722038269,
        "learning_rate": 0.00019636275927957511,
        "epoch": 1.3922402681103376,
        "step": 10801
    },
    {
        "loss": 1.3567,
        "grad_norm": 2.5405631065368652,
        "learning_rate": 0.00019634648194329858,
        "epoch": 1.3923691673111627,
        "step": 10802
    },
    {
        "loss": 2.1147,
        "grad_norm": 2.4055373668670654,
        "learning_rate": 0.00019633016894391345,
        "epoch": 1.3924980665119877,
        "step": 10803
    },
    {
        "loss": 1.963,
        "grad_norm": 3.1240291595458984,
        "learning_rate": 0.00019631382028745828,
        "epoch": 1.3926269657128125,
        "step": 10804
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.7330126762390137,
        "learning_rate": 0.00019629743597998444,
        "epoch": 1.3927558649136376,
        "step": 10805
    },
    {
        "loss": 1.1566,
        "grad_norm": 3.0776288509368896,
        "learning_rate": 0.00019628101602755667,
        "epoch": 1.3928847641144624,
        "step": 10806
    },
    {
        "loss": 2.1165,
        "grad_norm": 2.2588729858398438,
        "learning_rate": 0.00019626456043625296,
        "epoch": 1.3930136633152874,
        "step": 10807
    },
    {
        "loss": 2.0714,
        "grad_norm": 2.3800888061523438,
        "learning_rate": 0.0001962480692121644,
        "epoch": 1.3931425625161125,
        "step": 10808
    },
    {
        "loss": 1.0801,
        "grad_norm": 3.6874828338623047,
        "learning_rate": 0.00019623154236139527,
        "epoch": 1.3932714617169373,
        "step": 10809
    },
    {
        "loss": 1.4587,
        "grad_norm": 2.079655647277832,
        "learning_rate": 0.00019621497989006314,
        "epoch": 1.3934003609177623,
        "step": 10810
    },
    {
        "loss": 1.5181,
        "grad_norm": 1.9868090152740479,
        "learning_rate": 0.0001961983818042986,
        "epoch": 1.3935292601185871,
        "step": 10811
    },
    {
        "loss": 1.8149,
        "grad_norm": 2.3154985904693604,
        "learning_rate": 0.00019618174811024552,
        "epoch": 1.3936581593194122,
        "step": 10812
    },
    {
        "loss": 0.7291,
        "grad_norm": 2.921384811401367,
        "learning_rate": 0.00019616507881406098,
        "epoch": 1.3937870585202372,
        "step": 10813
    },
    {
        "loss": 0.9751,
        "grad_norm": 2.0807414054870605,
        "learning_rate": 0.00019614837392191518,
        "epoch": 1.3939159577210622,
        "step": 10814
    },
    {
        "loss": 0.5282,
        "grad_norm": 2.1407856941223145,
        "learning_rate": 0.00019613163343999154,
        "epoch": 1.394044856921887,
        "step": 10815
    },
    {
        "loss": 1.506,
        "grad_norm": 2.1825814247131348,
        "learning_rate": 0.0001961148573744866,
        "epoch": 1.394173756122712,
        "step": 10816
    },
    {
        "loss": 1.3359,
        "grad_norm": 2.763895273208618,
        "learning_rate": 0.0001960980457316101,
        "epoch": 1.394302655323537,
        "step": 10817
    },
    {
        "loss": 1.3189,
        "grad_norm": 3.636704444885254,
        "learning_rate": 0.00019608119851758493,
        "epoch": 1.394431554524362,
        "step": 10818
    },
    {
        "loss": 2.124,
        "grad_norm": 2.8959970474243164,
        "learning_rate": 0.00019606431573864722,
        "epoch": 1.394560453725187,
        "step": 10819
    },
    {
        "loss": 1.2565,
        "grad_norm": 2.383298873901367,
        "learning_rate": 0.00019604739740104613,
        "epoch": 1.3946893529260118,
        "step": 10820
    },
    {
        "loss": 2.2204,
        "grad_norm": 3.1277432441711426,
        "learning_rate": 0.0001960304435110442,
        "epoch": 1.3948182521268369,
        "step": 10821
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.0062339305877686,
        "learning_rate": 0.0001960134540749169,
        "epoch": 1.3949471513276617,
        "step": 10822
    },
    {
        "loss": 1.2401,
        "grad_norm": 3.27777361869812,
        "learning_rate": 0.0001959964290989529,
        "epoch": 1.3950760505284867,
        "step": 10823
    },
    {
        "loss": 1.1531,
        "grad_norm": 2.8512518405914307,
        "learning_rate": 0.00019597936858945417,
        "epoch": 1.3952049497293117,
        "step": 10824
    },
    {
        "loss": 1.9994,
        "grad_norm": 2.254051446914673,
        "learning_rate": 0.0001959622725527357,
        "epoch": 1.3953338489301366,
        "step": 10825
    },
    {
        "loss": 2.4309,
        "grad_norm": 1.7001769542694092,
        "learning_rate": 0.00019594514099512562,
        "epoch": 1.3954627481309616,
        "step": 10826
    },
    {
        "loss": 1.1607,
        "grad_norm": 3.435765027999878,
        "learning_rate": 0.00019592797392296542,
        "epoch": 1.3955916473317864,
        "step": 10827
    },
    {
        "loss": 1.7664,
        "grad_norm": 2.24991512298584,
        "learning_rate": 0.00019591077134260943,
        "epoch": 1.3957205465326115,
        "step": 10828
    },
    {
        "loss": 1.8769,
        "grad_norm": 3.2434144020080566,
        "learning_rate": 0.00019589353326042525,
        "epoch": 1.3958494457334365,
        "step": 10829
    },
    {
        "loss": 1.7392,
        "grad_norm": 1.832999348640442,
        "learning_rate": 0.0001958762596827937,
        "epoch": 1.3959783449342615,
        "step": 10830
    },
    {
        "loss": 1.3668,
        "grad_norm": 3.3149254322052,
        "learning_rate": 0.00019585895061610873,
        "epoch": 1.3961072441350864,
        "step": 10831
    },
    {
        "loss": 1.0615,
        "grad_norm": 3.4735586643218994,
        "learning_rate": 0.00019584160606677726,
        "epoch": 1.3962361433359114,
        "step": 10832
    },
    {
        "loss": 1.9023,
        "grad_norm": 2.8615477085113525,
        "learning_rate": 0.00019582422604121953,
        "epoch": 1.3963650425367362,
        "step": 10833
    },
    {
        "loss": 0.9335,
        "grad_norm": 2.779709577560425,
        "learning_rate": 0.00019580681054586882,
        "epoch": 1.3964939417375613,
        "step": 10834
    },
    {
        "loss": 1.0252,
        "grad_norm": 3.452031135559082,
        "learning_rate": 0.00019578935958717154,
        "epoch": 1.3966228409383863,
        "step": 10835
    },
    {
        "loss": 1.4839,
        "grad_norm": 2.4042720794677734,
        "learning_rate": 0.00019577187317158725,
        "epoch": 1.396751740139211,
        "step": 10836
    },
    {
        "loss": 2.3607,
        "grad_norm": 2.142091751098633,
        "learning_rate": 0.00019575435130558862,
        "epoch": 1.3968806393400361,
        "step": 10837
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.8038623332977295,
        "learning_rate": 0.0001957367939956615,
        "epoch": 1.397009538540861,
        "step": 10838
    },
    {
        "loss": 0.8569,
        "grad_norm": 2.6732687950134277,
        "learning_rate": 0.00019571920124830482,
        "epoch": 1.397138437741686,
        "step": 10839
    },
    {
        "loss": 1.8139,
        "grad_norm": 2.344843864440918,
        "learning_rate": 0.00019570157307003053,
        "epoch": 1.397267336942511,
        "step": 10840
    },
    {
        "loss": 1.1671,
        "grad_norm": 2.7418696880340576,
        "learning_rate": 0.00019568390946736385,
        "epoch": 1.3973962361433359,
        "step": 10841
    },
    {
        "loss": 1.9229,
        "grad_norm": 1.855092167854309,
        "learning_rate": 0.00019566621044684305,
        "epoch": 1.397525135344161,
        "step": 10842
    },
    {
        "loss": 1.8565,
        "grad_norm": 2.4057583808898926,
        "learning_rate": 0.00019564847601501946,
        "epoch": 1.3976540345449857,
        "step": 10843
    },
    {
        "loss": 1.8229,
        "grad_norm": 2.9608802795410156,
        "learning_rate": 0.0001956307061784576,
        "epoch": 1.3977829337458108,
        "step": 10844
    },
    {
        "loss": 1.9419,
        "grad_norm": 2.997875452041626,
        "learning_rate": 0.00019561290094373513,
        "epoch": 1.3979118329466358,
        "step": 10845
    },
    {
        "loss": 2.1755,
        "grad_norm": 3.3494153022766113,
        "learning_rate": 0.00019559506031744257,
        "epoch": 1.3980407321474606,
        "step": 10846
    },
    {
        "loss": 1.5251,
        "grad_norm": 2.58408522605896,
        "learning_rate": 0.00019557718430618387,
        "epoch": 1.3981696313482856,
        "step": 10847
    },
    {
        "loss": 1.5769,
        "grad_norm": 3.429558515548706,
        "learning_rate": 0.00019555927291657583,
        "epoch": 1.3982985305491105,
        "step": 10848
    },
    {
        "loss": 1.5134,
        "grad_norm": 1.9675617218017578,
        "learning_rate": 0.00019554132615524848,
        "epoch": 1.3984274297499355,
        "step": 10849
    },
    {
        "loss": 1.9282,
        "grad_norm": 1.341343641281128,
        "learning_rate": 0.00019552334402884487,
        "epoch": 1.3985563289507605,
        "step": 10850
    },
    {
        "loss": 1.6209,
        "grad_norm": 1.4904669523239136,
        "learning_rate": 0.0001955053265440212,
        "epoch": 1.3986852281515856,
        "step": 10851
    },
    {
        "loss": 1.9808,
        "grad_norm": 1.9673470258712769,
        "learning_rate": 0.0001954872737074467,
        "epoch": 1.3988141273524104,
        "step": 10852
    },
    {
        "loss": 1.9125,
        "grad_norm": 1.662987470626831,
        "learning_rate": 0.00019546918552580374,
        "epoch": 1.3989430265532354,
        "step": 10853
    },
    {
        "loss": 1.7355,
        "grad_norm": 2.0323240756988525,
        "learning_rate": 0.00019545106200578768,
        "epoch": 1.3990719257540603,
        "step": 10854
    },
    {
        "loss": 1.7243,
        "grad_norm": 2.4218978881835938,
        "learning_rate": 0.0001954329031541071,
        "epoch": 1.3992008249548853,
        "step": 10855
    },
    {
        "loss": 1.8416,
        "grad_norm": 3.5825600624084473,
        "learning_rate": 0.00019541470897748366,
        "epoch": 1.3993297241557103,
        "step": 10856
    },
    {
        "loss": 2.0657,
        "grad_norm": 3.0347423553466797,
        "learning_rate": 0.0001953964794826518,
        "epoch": 1.3994586233565351,
        "step": 10857
    },
    {
        "loss": 0.801,
        "grad_norm": 2.4441659450531006,
        "learning_rate": 0.00019537821467635946,
        "epoch": 1.3995875225573602,
        "step": 10858
    },
    {
        "loss": 1.5427,
        "grad_norm": 3.40586519241333,
        "learning_rate": 0.00019535991456536732,
        "epoch": 1.399716421758185,
        "step": 10859
    },
    {
        "loss": 2.0593,
        "grad_norm": 2.2383949756622314,
        "learning_rate": 0.00019534157915644928,
        "epoch": 1.39984532095901,
        "step": 10860
    },
    {
        "loss": 1.4534,
        "grad_norm": 1.5264936685562134,
        "learning_rate": 0.00019532320845639233,
        "epoch": 1.399974220159835,
        "step": 10861
    },
    {
        "loss": 2.0046,
        "grad_norm": 2.5754966735839844,
        "learning_rate": 0.00019530480247199647,
        "epoch": 1.40010311936066,
        "step": 10862
    },
    {
        "loss": 1.9431,
        "grad_norm": 2.045546770095825,
        "learning_rate": 0.00019528636121007468,
        "epoch": 1.400232018561485,
        "step": 10863
    },
    {
        "loss": 1.8197,
        "grad_norm": 2.5427470207214355,
        "learning_rate": 0.00019526788467745317,
        "epoch": 1.4003609177623098,
        "step": 10864
    },
    {
        "loss": 1.6905,
        "grad_norm": 3.1138055324554443,
        "learning_rate": 0.00019524937288097104,
        "epoch": 1.4004898169631348,
        "step": 10865
    },
    {
        "loss": 2.3239,
        "grad_norm": 2.1705868244171143,
        "learning_rate": 0.0001952308258274806,
        "epoch": 1.4006187161639598,
        "step": 10866
    },
    {
        "loss": 1.6377,
        "grad_norm": 2.0597801208496094,
        "learning_rate": 0.00019521224352384703,
        "epoch": 1.4007476153647849,
        "step": 10867
    },
    {
        "loss": 1.1121,
        "grad_norm": 2.5604419708251953,
        "learning_rate": 0.00019519362597694877,
        "epoch": 1.4008765145656097,
        "step": 10868
    },
    {
        "loss": 2.4734,
        "grad_norm": 1.7202168703079224,
        "learning_rate": 0.00019517497319367712,
        "epoch": 1.4010054137664347,
        "step": 10869
    },
    {
        "loss": 0.5432,
        "grad_norm": 2.6574559211730957,
        "learning_rate": 0.00019515628518093655,
        "epoch": 1.4011343129672595,
        "step": 10870
    },
    {
        "loss": 1.6962,
        "grad_norm": 2.7933592796325684,
        "learning_rate": 0.00019513756194564438,
        "epoch": 1.4012632121680846,
        "step": 10871
    },
    {
        "loss": 2.0857,
        "grad_norm": 2.0967562198638916,
        "learning_rate": 0.0001951188034947313,
        "epoch": 1.4013921113689096,
        "step": 10872
    },
    {
        "loss": 1.6166,
        "grad_norm": 2.0952160358428955,
        "learning_rate": 0.00019510000983514076,
        "epoch": 1.4015210105697344,
        "step": 10873
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.0860543251037598,
        "learning_rate": 0.00019508118097382924,
        "epoch": 1.4016499097705595,
        "step": 10874
    },
    {
        "loss": 1.49,
        "grad_norm": 3.1500635147094727,
        "learning_rate": 0.00019506231691776641,
        "epoch": 1.4017788089713843,
        "step": 10875
    },
    {
        "loss": 2.4979,
        "grad_norm": 1.8404573202133179,
        "learning_rate": 0.00019504341767393493,
        "epoch": 1.4019077081722093,
        "step": 10876
    },
    {
        "loss": 2.05,
        "grad_norm": 2.4498610496520996,
        "learning_rate": 0.00019502448324933033,
        "epoch": 1.4020366073730344,
        "step": 10877
    },
    {
        "loss": 1.658,
        "grad_norm": 3.2222208976745605,
        "learning_rate": 0.00019500551365096139,
        "epoch": 1.4021655065738592,
        "step": 10878
    },
    {
        "loss": 0.7737,
        "grad_norm": 2.995199203491211,
        "learning_rate": 0.0001949865088858498,
        "epoch": 1.4022944057746842,
        "step": 10879
    },
    {
        "loss": 1.4438,
        "grad_norm": 2.768556594848633,
        "learning_rate": 0.00019496746896103013,
        "epoch": 1.402423304975509,
        "step": 10880
    },
    {
        "loss": 1.015,
        "grad_norm": 3.381783962249756,
        "learning_rate": 0.00019494839388355022,
        "epoch": 1.402552204176334,
        "step": 10881
    },
    {
        "loss": 1.387,
        "grad_norm": 2.241021156311035,
        "learning_rate": 0.00019492928366047075,
        "epoch": 1.4026811033771591,
        "step": 10882
    },
    {
        "loss": 2.292,
        "grad_norm": 2.091148853302002,
        "learning_rate": 0.00019491013829886554,
        "epoch": 1.402810002577984,
        "step": 10883
    },
    {
        "loss": 1.7115,
        "grad_norm": 2.3769609928131104,
        "learning_rate": 0.0001948909578058212,
        "epoch": 1.402938901778809,
        "step": 10884
    },
    {
        "loss": 1.5833,
        "grad_norm": 2.8675692081451416,
        "learning_rate": 0.00019487174218843758,
        "epoch": 1.4030678009796338,
        "step": 10885
    },
    {
        "loss": 1.2211,
        "grad_norm": 2.9029176235198975,
        "learning_rate": 0.0001948524914538274,
        "epoch": 1.4031967001804588,
        "step": 10886
    },
    {
        "loss": 1.2376,
        "grad_norm": 3.490523099899292,
        "learning_rate": 0.00019483320560911643,
        "epoch": 1.4033255993812839,
        "step": 10887
    },
    {
        "loss": 1.0846,
        "grad_norm": 3.1487467288970947,
        "learning_rate": 0.00019481388466144334,
        "epoch": 1.403454498582109,
        "step": 10888
    },
    {
        "loss": 0.9456,
        "grad_norm": 2.2545764446258545,
        "learning_rate": 0.00019479452861795998,
        "epoch": 1.4035833977829337,
        "step": 10889
    },
    {
        "loss": 2.1446,
        "grad_norm": 1.8706483840942383,
        "learning_rate": 0.0001947751374858311,
        "epoch": 1.4037122969837588,
        "step": 10890
    },
    {
        "loss": 1.5561,
        "grad_norm": 2.5492281913757324,
        "learning_rate": 0.00019475571127223424,
        "epoch": 1.4038411961845836,
        "step": 10891
    },
    {
        "loss": 1.5373,
        "grad_norm": 2.723525047302246,
        "learning_rate": 0.00019473624998436027,
        "epoch": 1.4039700953854086,
        "step": 10892
    },
    {
        "loss": 1.2202,
        "grad_norm": 3.9067277908325195,
        "learning_rate": 0.0001947167536294128,
        "epoch": 1.4040989945862337,
        "step": 10893
    },
    {
        "loss": 1.7372,
        "grad_norm": 1.841542363166809,
        "learning_rate": 0.00019469722221460853,
        "epoch": 1.4042278937870585,
        "step": 10894
    },
    {
        "loss": 1.4347,
        "grad_norm": 4.522255897521973,
        "learning_rate": 0.00019467765574717713,
        "epoch": 1.4043567929878835,
        "step": 10895
    },
    {
        "loss": 1.7965,
        "grad_norm": 2.1787357330322266,
        "learning_rate": 0.00019465805423436126,
        "epoch": 1.4044856921887083,
        "step": 10896
    },
    {
        "loss": 1.4962,
        "grad_norm": 2.6082427501678467,
        "learning_rate": 0.00019463841768341633,
        "epoch": 1.4046145913895334,
        "step": 10897
    },
    {
        "loss": 1.3294,
        "grad_norm": 5.282494068145752,
        "learning_rate": 0.00019461874610161108,
        "epoch": 1.4047434905903584,
        "step": 10898
    },
    {
        "loss": 1.0465,
        "grad_norm": 2.4426145553588867,
        "learning_rate": 0.00019459903949622698,
        "epoch": 1.4048723897911832,
        "step": 10899
    },
    {
        "loss": 1.5566,
        "grad_norm": 2.6355538368225098,
        "learning_rate": 0.0001945792978745585,
        "epoch": 1.4050012889920083,
        "step": 10900
    },
    {
        "loss": 2.0125,
        "grad_norm": 2.8539061546325684,
        "learning_rate": 0.00019455952124391312,
        "epoch": 1.405130188192833,
        "step": 10901
    },
    {
        "loss": 0.8424,
        "grad_norm": 2.7781131267547607,
        "learning_rate": 0.00019453970961161128,
        "epoch": 1.4052590873936581,
        "step": 10902
    },
    {
        "loss": 1.9848,
        "grad_norm": 1.7529548406600952,
        "learning_rate": 0.0001945198629849863,
        "epoch": 1.4053879865944832,
        "step": 10903
    },
    {
        "loss": 1.7507,
        "grad_norm": 2.9685356616973877,
        "learning_rate": 0.00019449998137138454,
        "epoch": 1.4055168857953082,
        "step": 10904
    },
    {
        "loss": 1.5709,
        "grad_norm": 2.716402769088745,
        "learning_rate": 0.0001944800647781652,
        "epoch": 1.405645784996133,
        "step": 10905
    },
    {
        "loss": 1.7034,
        "grad_norm": 2.7469072341918945,
        "learning_rate": 0.00019446011321270059,
        "epoch": 1.405774684196958,
        "step": 10906
    },
    {
        "loss": 2.0437,
        "grad_norm": 1.6188253164291382,
        "learning_rate": 0.00019444012668237587,
        "epoch": 1.4059035833977829,
        "step": 10907
    },
    {
        "loss": 1.9678,
        "grad_norm": 2.637787103652954,
        "learning_rate": 0.00019442010519458912,
        "epoch": 1.406032482598608,
        "step": 10908
    },
    {
        "loss": 1.367,
        "grad_norm": 3.242121696472168,
        "learning_rate": 0.0001944000487567514,
        "epoch": 1.406161381799433,
        "step": 10909
    },
    {
        "loss": 1.2976,
        "grad_norm": 2.600308418273926,
        "learning_rate": 0.0001943799573762867,
        "epoch": 1.4062902810002578,
        "step": 10910
    },
    {
        "loss": 1.7127,
        "grad_norm": 2.6507582664489746,
        "learning_rate": 0.0001943598310606319,
        "epoch": 1.4064191802010828,
        "step": 10911
    },
    {
        "loss": 1.7724,
        "grad_norm": 2.761974334716797,
        "learning_rate": 0.0001943396698172369,
        "epoch": 1.4065480794019076,
        "step": 10912
    },
    {
        "loss": 0.9876,
        "grad_norm": 2.175487995147705,
        "learning_rate": 0.0001943194736535645,
        "epoch": 1.4066769786027327,
        "step": 10913
    },
    {
        "loss": 1.9365,
        "grad_norm": 1.5958040952682495,
        "learning_rate": 0.00019429924257709032,
        "epoch": 1.4068058778035577,
        "step": 10914
    },
    {
        "loss": 1.4416,
        "grad_norm": 1.9310580492019653,
        "learning_rate": 0.00019427897659530304,
        "epoch": 1.4069347770043825,
        "step": 10915
    },
    {
        "loss": 1.778,
        "grad_norm": 2.548110008239746,
        "learning_rate": 0.00019425867571570425,
        "epoch": 1.4070636762052076,
        "step": 10916
    },
    {
        "loss": 1.5757,
        "grad_norm": 2.9218409061431885,
        "learning_rate": 0.00019423833994580833,
        "epoch": 1.4071925754060324,
        "step": 10917
    },
    {
        "loss": 1.8231,
        "grad_norm": 2.467437505722046,
        "learning_rate": 0.00019421796929314274,
        "epoch": 1.4073214746068574,
        "step": 10918
    },
    {
        "loss": 2.0165,
        "grad_norm": 2.595947265625,
        "learning_rate": 0.0001941975637652477,
        "epoch": 1.4074503738076825,
        "step": 10919
    },
    {
        "loss": 1.358,
        "grad_norm": 3.744006395339966,
        "learning_rate": 0.0001941771233696765,
        "epoch": 1.4075792730085073,
        "step": 10920
    },
    {
        "loss": 1.9145,
        "grad_norm": 2.6476504802703857,
        "learning_rate": 0.0001941566481139952,
        "epoch": 1.4077081722093323,
        "step": 10921
    },
    {
        "loss": 1.7022,
        "grad_norm": 3.226213216781616,
        "learning_rate": 0.00019413613800578274,
        "epoch": 1.4078370714101571,
        "step": 10922
    },
    {
        "loss": 1.7468,
        "grad_norm": 2.2191336154937744,
        "learning_rate": 0.0001941155930526312,
        "epoch": 1.4079659706109822,
        "step": 10923
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.3103110790252686,
        "learning_rate": 0.00019409501326214527,
        "epoch": 1.4080948698118072,
        "step": 10924
    },
    {
        "loss": 1.7002,
        "grad_norm": 3.348782539367676,
        "learning_rate": 0.0001940743986419427,
        "epoch": 1.4082237690126322,
        "step": 10925
    },
    {
        "loss": 1.3535,
        "grad_norm": 4.13986349105835,
        "learning_rate": 0.00019405374919965408,
        "epoch": 1.408352668213457,
        "step": 10926
    },
    {
        "loss": 0.9924,
        "grad_norm": 2.8568201065063477,
        "learning_rate": 0.00019403306494292287,
        "epoch": 1.408481567414282,
        "step": 10927
    },
    {
        "loss": 2.3507,
        "grad_norm": 2.3959836959838867,
        "learning_rate": 0.00019401234587940546,
        "epoch": 1.408610466615107,
        "step": 10928
    },
    {
        "loss": 1.2463,
        "grad_norm": 2.3148441314697266,
        "learning_rate": 0.00019399159201677118,
        "epoch": 1.408739365815932,
        "step": 10929
    },
    {
        "loss": 1.906,
        "grad_norm": 3.2860307693481445,
        "learning_rate": 0.00019397080336270214,
        "epoch": 1.408868265016757,
        "step": 10930
    },
    {
        "loss": 1.3396,
        "grad_norm": 1.3603790998458862,
        "learning_rate": 0.00019394997992489325,
        "epoch": 1.4089971642175818,
        "step": 10931
    },
    {
        "loss": 1.2914,
        "grad_norm": 2.3609635829925537,
        "learning_rate": 0.00019392912171105255,
        "epoch": 1.4091260634184068,
        "step": 10932
    },
    {
        "loss": 1.9509,
        "grad_norm": 2.5672714710235596,
        "learning_rate": 0.00019390822872890074,
        "epoch": 1.4092549626192317,
        "step": 10933
    },
    {
        "loss": 1.1727,
        "grad_norm": 1.8957966566085815,
        "learning_rate": 0.00019388730098617146,
        "epoch": 1.4093838618200567,
        "step": 10934
    },
    {
        "loss": 1.3417,
        "grad_norm": 3.1910953521728516,
        "learning_rate": 0.00019386633849061122,
        "epoch": 1.4095127610208817,
        "step": 10935
    },
    {
        "loss": 1.9194,
        "grad_norm": 2.4883439540863037,
        "learning_rate": 0.0001938453412499794,
        "epoch": 1.4096416602217066,
        "step": 10936
    },
    {
        "loss": 1.9129,
        "grad_norm": 2.863607406616211,
        "learning_rate": 0.0001938243092720482,
        "epoch": 1.4097705594225316,
        "step": 10937
    },
    {
        "loss": 1.5618,
        "grad_norm": 3.3137216567993164,
        "learning_rate": 0.00019380324256460272,
        "epoch": 1.4098994586233564,
        "step": 10938
    },
    {
        "loss": 0.6863,
        "grad_norm": 2.2669646739959717,
        "learning_rate": 0.00019378214113544085,
        "epoch": 1.4100283578241815,
        "step": 10939
    },
    {
        "loss": 0.7408,
        "grad_norm": 3.615388870239258,
        "learning_rate": 0.0001937610049923735,
        "epoch": 1.4101572570250065,
        "step": 10940
    },
    {
        "loss": 1.283,
        "grad_norm": 3.097452402114868,
        "learning_rate": 0.0001937398341432242,
        "epoch": 1.4102861562258315,
        "step": 10941
    },
    {
        "loss": 1.5028,
        "grad_norm": 3.1428003311157227,
        "learning_rate": 0.00019371862859582955,
        "epoch": 1.4104150554266563,
        "step": 10942
    },
    {
        "loss": 0.9709,
        "grad_norm": 3.0977516174316406,
        "learning_rate": 0.00019369738835803876,
        "epoch": 1.4105439546274814,
        "step": 10943
    },
    {
        "loss": 1.8092,
        "grad_norm": 2.050037384033203,
        "learning_rate": 0.00019367611343771413,
        "epoch": 1.4106728538283062,
        "step": 10944
    },
    {
        "loss": 0.7531,
        "grad_norm": 4.127904891967773,
        "learning_rate": 0.00019365480384273055,
        "epoch": 1.4108017530291312,
        "step": 10945
    },
    {
        "loss": 0.8699,
        "grad_norm": 2.411618947982788,
        "learning_rate": 0.00019363345958097597,
        "epoch": 1.4109306522299563,
        "step": 10946
    },
    {
        "loss": 2.2288,
        "grad_norm": 1.9737945795059204,
        "learning_rate": 0.00019361208066035103,
        "epoch": 1.411059551430781,
        "step": 10947
    },
    {
        "loss": 2.1632,
        "grad_norm": 3.4233551025390625,
        "learning_rate": 0.00019359066708876926,
        "epoch": 1.4111884506316061,
        "step": 10948
    },
    {
        "loss": 1.8446,
        "grad_norm": 2.6104092597961426,
        "learning_rate": 0.00019356921887415696,
        "epoch": 1.411317349832431,
        "step": 10949
    },
    {
        "loss": 1.6616,
        "grad_norm": 2.687121629714966,
        "learning_rate": 0.0001935477360244533,
        "epoch": 1.411446249033256,
        "step": 10950
    },
    {
        "loss": 1.3916,
        "grad_norm": 2.5292720794677734,
        "learning_rate": 0.00019352621854761023,
        "epoch": 1.411575148234081,
        "step": 10951
    },
    {
        "loss": 2.0694,
        "grad_norm": 1.6410002708435059,
        "learning_rate": 0.00019350466645159264,
        "epoch": 1.4117040474349059,
        "step": 10952
    },
    {
        "loss": 2.1551,
        "grad_norm": 1.9126349687576294,
        "learning_rate": 0.00019348307974437808,
        "epoch": 1.411832946635731,
        "step": 10953
    },
    {
        "loss": 1.8189,
        "grad_norm": 2.188169240951538,
        "learning_rate": 0.00019346145843395692,
        "epoch": 1.4119618458365557,
        "step": 10954
    },
    {
        "loss": 1.9893,
        "grad_norm": 2.000356674194336,
        "learning_rate": 0.00019343980252833247,
        "epoch": 1.4120907450373807,
        "step": 10955
    },
    {
        "loss": 0.9265,
        "grad_norm": 1.9890693426132202,
        "learning_rate": 0.0001934181120355207,
        "epoch": 1.4122196442382058,
        "step": 10956
    },
    {
        "loss": 0.6486,
        "grad_norm": 2.7996678352355957,
        "learning_rate": 0.00019339638696355053,
        "epoch": 1.4123485434390306,
        "step": 10957
    },
    {
        "loss": 0.9893,
        "grad_norm": 3.1415047645568848,
        "learning_rate": 0.00019337462732046357,
        "epoch": 1.4124774426398556,
        "step": 10958
    },
    {
        "loss": 1.3595,
        "grad_norm": 3.6020915508270264,
        "learning_rate": 0.0001933528331143142,
        "epoch": 1.4126063418406805,
        "step": 10959
    },
    {
        "loss": 2.3196,
        "grad_norm": 2.5188097953796387,
        "learning_rate": 0.00019333100435316974,
        "epoch": 1.4127352410415055,
        "step": 10960
    },
    {
        "loss": 1.443,
        "grad_norm": 2.7990403175354004,
        "learning_rate": 0.00019330914104511012,
        "epoch": 1.4128641402423305,
        "step": 10961
    },
    {
        "loss": 1.553,
        "grad_norm": 1.847602367401123,
        "learning_rate": 0.00019328724319822818,
        "epoch": 1.4129930394431556,
        "step": 10962
    },
    {
        "loss": 2.0227,
        "grad_norm": 1.940003752708435,
        "learning_rate": 0.00019326531082062954,
        "epoch": 1.4131219386439804,
        "step": 10963
    },
    {
        "loss": 1.8864,
        "grad_norm": 2.7110776901245117,
        "learning_rate": 0.00019324334392043259,
        "epoch": 1.4132508378448054,
        "step": 10964
    },
    {
        "loss": 1.9466,
        "grad_norm": 1.5699610710144043,
        "learning_rate": 0.00019322134250576842,
        "epoch": 1.4133797370456302,
        "step": 10965
    },
    {
        "loss": 2.4054,
        "grad_norm": 2.815617322921753,
        "learning_rate": 0.000193199306584781,
        "epoch": 1.4135086362464553,
        "step": 10966
    },
    {
        "loss": 2.2525,
        "grad_norm": 2.9108054637908936,
        "learning_rate": 0.000193177236165627,
        "epoch": 1.4136375354472803,
        "step": 10967
    },
    {
        "loss": 1.7257,
        "grad_norm": 1.908469796180725,
        "learning_rate": 0.0001931551312564759,
        "epoch": 1.4137664346481051,
        "step": 10968
    },
    {
        "loss": 1.2919,
        "grad_norm": 2.8245673179626465,
        "learning_rate": 0.00019313299186550994,
        "epoch": 1.4138953338489302,
        "step": 10969
    },
    {
        "loss": 1.3455,
        "grad_norm": 3.115006685256958,
        "learning_rate": 0.00019311081800092413,
        "epoch": 1.414024233049755,
        "step": 10970
    },
    {
        "loss": 1.16,
        "grad_norm": 2.5468075275421143,
        "learning_rate": 0.0001930886096709263,
        "epoch": 1.41415313225058,
        "step": 10971
    },
    {
        "loss": 1.4029,
        "grad_norm": 3.148409366607666,
        "learning_rate": 0.00019306636688373684,
        "epoch": 1.414282031451405,
        "step": 10972
    },
    {
        "loss": 1.5532,
        "grad_norm": 2.7871792316436768,
        "learning_rate": 0.000193044089647589,
        "epoch": 1.41441093065223,
        "step": 10973
    },
    {
        "loss": 0.6519,
        "grad_norm": 2.8188061714172363,
        "learning_rate": 0.00019302177797072897,
        "epoch": 1.414539829853055,
        "step": 10974
    },
    {
        "loss": 2.3722,
        "grad_norm": 2.0930893421173096,
        "learning_rate": 0.00019299943186141546,
        "epoch": 1.4146687290538797,
        "step": 10975
    },
    {
        "loss": 1.3844,
        "grad_norm": 2.910529136657715,
        "learning_rate": 0.00019297705132791993,
        "epoch": 1.4147976282547048,
        "step": 10976
    },
    {
        "loss": 1.0837,
        "grad_norm": 2.035010814666748,
        "learning_rate": 0.00019295463637852665,
        "epoch": 1.4149265274555298,
        "step": 10977
    },
    {
        "loss": 2.377,
        "grad_norm": 2.066856861114502,
        "learning_rate": 0.0001929321870215327,
        "epoch": 1.4150554266563549,
        "step": 10978
    },
    {
        "loss": 1.8488,
        "grad_norm": 2.9669885635375977,
        "learning_rate": 0.0001929097032652477,
        "epoch": 1.4151843258571797,
        "step": 10979
    },
    {
        "loss": 1.9552,
        "grad_norm": 2.3642773628234863,
        "learning_rate": 0.00019288718511799421,
        "epoch": 1.4153132250580047,
        "step": 10980
    },
    {
        "loss": 1.3823,
        "grad_norm": 2.0554442405700684,
        "learning_rate": 0.00019286463258810744,
        "epoch": 1.4154421242588295,
        "step": 10981
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.2466185092926025,
        "learning_rate": 0.00019284204568393527,
        "epoch": 1.4155710234596546,
        "step": 10982
    },
    {
        "loss": 1.5155,
        "grad_norm": 3.0667600631713867,
        "learning_rate": 0.00019281942441383833,
        "epoch": 1.4156999226604796,
        "step": 10983
    },
    {
        "loss": 1.7639,
        "grad_norm": 3.722505569458008,
        "learning_rate": 0.00019279676878619005,
        "epoch": 1.4158288218613044,
        "step": 10984
    },
    {
        "loss": 2.0149,
        "grad_norm": 2.1961417198181152,
        "learning_rate": 0.00019277407880937645,
        "epoch": 1.4159577210621295,
        "step": 10985
    },
    {
        "loss": 1.3932,
        "grad_norm": 2.357285499572754,
        "learning_rate": 0.0001927513544917964,
        "epoch": 1.4160866202629543,
        "step": 10986
    },
    {
        "loss": 2.0184,
        "grad_norm": 1.78895103931427,
        "learning_rate": 0.00019272859584186134,
        "epoch": 1.4162155194637793,
        "step": 10987
    },
    {
        "loss": 1.0442,
        "grad_norm": 3.394559860229492,
        "learning_rate": 0.00019270580286799563,
        "epoch": 1.4163444186646044,
        "step": 10988
    },
    {
        "loss": 1.9225,
        "grad_norm": 2.553600311279297,
        "learning_rate": 0.00019268297557863603,
        "epoch": 1.4164733178654292,
        "step": 10989
    },
    {
        "loss": 2.3288,
        "grad_norm": 1.6499634981155396,
        "learning_rate": 0.0001926601139822322,
        "epoch": 1.4166022170662542,
        "step": 10990
    },
    {
        "loss": 1.9115,
        "grad_norm": 3.170663356781006,
        "learning_rate": 0.0001926372180872466,
        "epoch": 1.416731116267079,
        "step": 10991
    },
    {
        "loss": 2.1305,
        "grad_norm": 1.3538439273834229,
        "learning_rate": 0.00019261428790215415,
        "epoch": 1.416860015467904,
        "step": 10992
    },
    {
        "loss": 1.4539,
        "grad_norm": 2.1597414016723633,
        "learning_rate": 0.00019259132343544259,
        "epoch": 1.4169889146687291,
        "step": 10993
    },
    {
        "loss": 2.3392,
        "grad_norm": 2.326538562774658,
        "learning_rate": 0.0001925683246956123,
        "epoch": 1.417117813869554,
        "step": 10994
    },
    {
        "loss": 1.9193,
        "grad_norm": 3.430314302444458,
        "learning_rate": 0.0001925452916911764,
        "epoch": 1.417246713070379,
        "step": 10995
    },
    {
        "loss": 2.1766,
        "grad_norm": 2.177412271499634,
        "learning_rate": 0.00019252222443066062,
        "epoch": 1.4173756122712038,
        "step": 10996
    },
    {
        "loss": 2.1521,
        "grad_norm": 2.2148449420928955,
        "learning_rate": 0.0001924991229226035,
        "epoch": 1.4175045114720288,
        "step": 10997
    },
    {
        "loss": 1.6502,
        "grad_norm": 1.916503667831421,
        "learning_rate": 0.0001924759871755561,
        "epoch": 1.4176334106728539,
        "step": 10998
    },
    {
        "loss": 2.0436,
        "grad_norm": 1.713609218597412,
        "learning_rate": 0.0001924528171980823,
        "epoch": 1.417762309873679,
        "step": 10999
    },
    {
        "loss": 2.7478,
        "grad_norm": 1.5025615692138672,
        "learning_rate": 0.00019242961299875853,
        "epoch": 1.4178912090745037,
        "step": 11000
    },
    {
        "loss": 1.7979,
        "grad_norm": 1.9483177661895752,
        "learning_rate": 0.00019240637458617393,
        "epoch": 1.4180201082753288,
        "step": 11001
    },
    {
        "loss": 0.8671,
        "grad_norm": 3.9961652755737305,
        "learning_rate": 0.0001923831019689303,
        "epoch": 1.4181490074761536,
        "step": 11002
    },
    {
        "loss": 2.0502,
        "grad_norm": 2.428804397583008,
        "learning_rate": 0.00019235979515564214,
        "epoch": 1.4182779066769786,
        "step": 11003
    },
    {
        "loss": 2.1331,
        "grad_norm": 1.7353990077972412,
        "learning_rate": 0.00019233645415493654,
        "epoch": 1.4184068058778037,
        "step": 11004
    },
    {
        "loss": 1.1952,
        "grad_norm": 2.569812774658203,
        "learning_rate": 0.00019231307897545337,
        "epoch": 1.4185357050786285,
        "step": 11005
    },
    {
        "loss": 2.5249,
        "grad_norm": 1.4556889533996582,
        "learning_rate": 0.00019228966962584498,
        "epoch": 1.4186646042794535,
        "step": 11006
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.9718421697616577,
        "learning_rate": 0.0001922662261147764,
        "epoch": 1.4187935034802783,
        "step": 11007
    },
    {
        "loss": 1.2422,
        "grad_norm": 3.3327338695526123,
        "learning_rate": 0.0001922427484509255,
        "epoch": 1.4189224026811034,
        "step": 11008
    },
    {
        "loss": 1.953,
        "grad_norm": 2.06022572517395,
        "learning_rate": 0.00019221923664298256,
        "epoch": 1.4190513018819284,
        "step": 11009
    },
    {
        "loss": 2.3862,
        "grad_norm": 2.9674887657165527,
        "learning_rate": 0.00019219569069965056,
        "epoch": 1.4191802010827532,
        "step": 11010
    },
    {
        "loss": 1.8693,
        "grad_norm": 2.6367084980010986,
        "learning_rate": 0.00019217211062964529,
        "epoch": 1.4193091002835783,
        "step": 11011
    },
    {
        "loss": 1.4567,
        "grad_norm": 3.283674955368042,
        "learning_rate": 0.00019214849644169489,
        "epoch": 1.419437999484403,
        "step": 11012
    },
    {
        "loss": 1.8529,
        "grad_norm": 3.6974480152130127,
        "learning_rate": 0.00019212484814454023,
        "epoch": 1.4195668986852281,
        "step": 11013
    },
    {
        "loss": 1.5788,
        "grad_norm": 2.3955109119415283,
        "learning_rate": 0.00019210116574693497,
        "epoch": 1.4196957978860532,
        "step": 11014
    },
    {
        "loss": 1.4758,
        "grad_norm": 3.176130533218384,
        "learning_rate": 0.0001920774492576452,
        "epoch": 1.4198246970868782,
        "step": 11015
    },
    {
        "loss": 1.7015,
        "grad_norm": 2.4785659313201904,
        "learning_rate": 0.00019205369868544966,
        "epoch": 1.419953596287703,
        "step": 11016
    },
    {
        "loss": 1.9439,
        "grad_norm": 1.741265058517456,
        "learning_rate": 0.00019202991403913981,
        "epoch": 1.420082495488528,
        "step": 11017
    },
    {
        "loss": 1.3538,
        "grad_norm": 3.275564193725586,
        "learning_rate": 0.0001920060953275196,
        "epoch": 1.4202113946893529,
        "step": 11018
    },
    {
        "loss": 2.3113,
        "grad_norm": 1.4581716060638428,
        "learning_rate": 0.00019198224255940563,
        "epoch": 1.420340293890178,
        "step": 11019
    },
    {
        "loss": 1.3941,
        "grad_norm": 2.673862934112549,
        "learning_rate": 0.00019195835574362718,
        "epoch": 1.420469193091003,
        "step": 11020
    },
    {
        "loss": 1.701,
        "grad_norm": 2.6898467540740967,
        "learning_rate": 0.0001919344348890259,
        "epoch": 1.4205980922918278,
        "step": 11021
    },
    {
        "loss": 1.1915,
        "grad_norm": 3.5193469524383545,
        "learning_rate": 0.0001919104800044565,
        "epoch": 1.4207269914926528,
        "step": 11022
    },
    {
        "loss": 1.6807,
        "grad_norm": 2.8037121295928955,
        "learning_rate": 0.00019188649109878574,
        "epoch": 1.4208558906934776,
        "step": 11023
    },
    {
        "loss": 1.6946,
        "grad_norm": 2.79097843170166,
        "learning_rate": 0.00019186246818089328,
        "epoch": 1.4209847898943027,
        "step": 11024
    },
    {
        "loss": 0.9535,
        "grad_norm": 2.995225429534912,
        "learning_rate": 0.0001918384112596714,
        "epoch": 1.4211136890951277,
        "step": 11025
    },
    {
        "loss": 1.6615,
        "grad_norm": 2.6199324131011963,
        "learning_rate": 0.0001918143203440248,
        "epoch": 1.4212425882959525,
        "step": 11026
    },
    {
        "loss": 2.176,
        "grad_norm": 1.6349390745162964,
        "learning_rate": 0.00019179019544287088,
        "epoch": 1.4213714874967776,
        "step": 11027
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.935276508331299,
        "learning_rate": 0.00019176603656513965,
        "epoch": 1.4215003866976024,
        "step": 11028
    },
    {
        "loss": 0.7849,
        "grad_norm": 2.6562211513519287,
        "learning_rate": 0.00019174184371977354,
        "epoch": 1.4216292858984274,
        "step": 11029
    },
    {
        "loss": 2.152,
        "grad_norm": 1.6727172136306763,
        "learning_rate": 0.00019171761691572762,
        "epoch": 1.4217581850992524,
        "step": 11030
    },
    {
        "loss": 2.6858,
        "grad_norm": 1.7365329265594482,
        "learning_rate": 0.00019169335616196965,
        "epoch": 1.4218870843000773,
        "step": 11031
    },
    {
        "loss": 1.985,
        "grad_norm": 1.3033084869384766,
        "learning_rate": 0.00019166906146747984,
        "epoch": 1.4220159835009023,
        "step": 11032
    },
    {
        "loss": 1.5217,
        "grad_norm": 3.5179452896118164,
        "learning_rate": 0.00019164473284125096,
        "epoch": 1.4221448827017271,
        "step": 11033
    },
    {
        "loss": 1.4527,
        "grad_norm": 3.690096616744995,
        "learning_rate": 0.0001916203702922884,
        "epoch": 1.4222737819025522,
        "step": 11034
    },
    {
        "loss": 1.8851,
        "grad_norm": 1.5595664978027344,
        "learning_rate": 0.00019159597382961008,
        "epoch": 1.4224026811033772,
        "step": 11035
    },
    {
        "loss": 1.9445,
        "grad_norm": 1.8725156784057617,
        "learning_rate": 0.0001915715434622464,
        "epoch": 1.4225315803042022,
        "step": 11036
    },
    {
        "loss": 1.7912,
        "grad_norm": 2.4645142555236816,
        "learning_rate": 0.00019154707919924043,
        "epoch": 1.422660479505027,
        "step": 11037
    },
    {
        "loss": 0.9795,
        "grad_norm": 2.9773480892181396,
        "learning_rate": 0.00019152258104964767,
        "epoch": 1.422789378705852,
        "step": 11038
    },
    {
        "loss": 2.284,
        "grad_norm": 1.7923747301101685,
        "learning_rate": 0.0001914980490225364,
        "epoch": 1.422918277906677,
        "step": 11039
    },
    {
        "loss": 0.8727,
        "grad_norm": 2.2939207553863525,
        "learning_rate": 0.00019147348312698704,
        "epoch": 1.423047177107502,
        "step": 11040
    },
    {
        "loss": 2.3029,
        "grad_norm": 1.8512517213821411,
        "learning_rate": 0.00019144888337209288,
        "epoch": 1.423176076308327,
        "step": 11041
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.7899904251098633,
        "learning_rate": 0.00019142424976695965,
        "epoch": 1.4233049755091518,
        "step": 11042
    },
    {
        "loss": 1.8328,
        "grad_norm": 3.5176243782043457,
        "learning_rate": 0.0001913995823207056,
        "epoch": 1.4234338747099768,
        "step": 11043
    },
    {
        "loss": 1.1626,
        "grad_norm": 4.598174571990967,
        "learning_rate": 0.00019137488104246141,
        "epoch": 1.4235627739108017,
        "step": 11044
    },
    {
        "loss": 2.127,
        "grad_norm": 2.3367035388946533,
        "learning_rate": 0.0001913501459413706,
        "epoch": 1.4236916731116267,
        "step": 11045
    },
    {
        "loss": 1.8662,
        "grad_norm": 1.4643789529800415,
        "learning_rate": 0.00019132537702658872,
        "epoch": 1.4238205723124517,
        "step": 11046
    },
    {
        "loss": 1.5564,
        "grad_norm": 2.436495542526245,
        "learning_rate": 0.0001913005743072842,
        "epoch": 1.4239494715132766,
        "step": 11047
    },
    {
        "loss": 1.6018,
        "grad_norm": 1.9423816204071045,
        "learning_rate": 0.00019127573779263794,
        "epoch": 1.4240783707141016,
        "step": 11048
    },
    {
        "loss": 1.2839,
        "grad_norm": 3.233806610107422,
        "learning_rate": 0.00019125086749184327,
        "epoch": 1.4242072699149264,
        "step": 11049
    },
    {
        "loss": 1.9382,
        "grad_norm": 2.1329052448272705,
        "learning_rate": 0.00019122596341410598,
        "epoch": 1.4243361691157514,
        "step": 11050
    },
    {
        "loss": 1.0758,
        "grad_norm": 4.028637409210205,
        "learning_rate": 0.00019120102556864453,
        "epoch": 1.4244650683165765,
        "step": 11051
    },
    {
        "loss": 1.1528,
        "grad_norm": 3.7313878536224365,
        "learning_rate": 0.00019117605396468977,
        "epoch": 1.4245939675174015,
        "step": 11052
    },
    {
        "loss": 2.3156,
        "grad_norm": 2.5131661891937256,
        "learning_rate": 0.00019115104861148495,
        "epoch": 1.4247228667182263,
        "step": 11053
    },
    {
        "loss": 1.7598,
        "grad_norm": 2.2177364826202393,
        "learning_rate": 0.00019112600951828607,
        "epoch": 1.4248517659190514,
        "step": 11054
    },
    {
        "loss": 1.5525,
        "grad_norm": 2.346665859222412,
        "learning_rate": 0.0001911009366943613,
        "epoch": 1.4249806651198762,
        "step": 11055
    },
    {
        "loss": 1.0098,
        "grad_norm": 3.0072507858276367,
        "learning_rate": 0.00019107583014899167,
        "epoch": 1.4251095643207012,
        "step": 11056
    },
    {
        "loss": 1.504,
        "grad_norm": 2.820594310760498,
        "learning_rate": 0.00019105068989147035,
        "epoch": 1.4252384635215263,
        "step": 11057
    },
    {
        "loss": 1.4208,
        "grad_norm": 1.917127013206482,
        "learning_rate": 0.00019102551593110306,
        "epoch": 1.425367362722351,
        "step": 11058
    },
    {
        "loss": 1.5122,
        "grad_norm": 2.1657536029815674,
        "learning_rate": 0.00019100030827720824,
        "epoch": 1.4254962619231761,
        "step": 11059
    },
    {
        "loss": 2.0566,
        "grad_norm": 1.8889541625976562,
        "learning_rate": 0.00019097506693911654,
        "epoch": 1.425625161124001,
        "step": 11060
    },
    {
        "loss": 1.6304,
        "grad_norm": 2.159776210784912,
        "learning_rate": 0.0001909497919261711,
        "epoch": 1.425754060324826,
        "step": 11061
    },
    {
        "loss": 0.9508,
        "grad_norm": 2.9800057411193848,
        "learning_rate": 0.00019092448324772773,
        "epoch": 1.425882959525651,
        "step": 11062
    },
    {
        "loss": 1.5497,
        "grad_norm": 3.330035924911499,
        "learning_rate": 0.00019089914091315445,
        "epoch": 1.4260118587264758,
        "step": 11063
    },
    {
        "loss": 1.7398,
        "grad_norm": 3.120896577835083,
        "learning_rate": 0.0001908737649318318,
        "epoch": 1.4261407579273009,
        "step": 11064
    },
    {
        "loss": 2.1633,
        "grad_norm": 2.0185160636901855,
        "learning_rate": 0.00019084835531315295,
        "epoch": 1.4262696571281257,
        "step": 11065
    },
    {
        "loss": 1.2371,
        "grad_norm": 3.850231647491455,
        "learning_rate": 0.00019082291206652335,
        "epoch": 1.4263985563289507,
        "step": 11066
    },
    {
        "loss": 1.7567,
        "grad_norm": 3.1812198162078857,
        "learning_rate": 0.00019079743520136088,
        "epoch": 1.4265274555297758,
        "step": 11067
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.6728005409240723,
        "learning_rate": 0.00019077192472709598,
        "epoch": 1.4266563547306006,
        "step": 11068
    },
    {
        "loss": 1.2267,
        "grad_norm": 2.2467429637908936,
        "learning_rate": 0.00019074638065317147,
        "epoch": 1.4267852539314256,
        "step": 11069
    },
    {
        "loss": 1.0368,
        "grad_norm": 3.9839391708374023,
        "learning_rate": 0.00019072080298904254,
        "epoch": 1.4269141531322505,
        "step": 11070
    },
    {
        "loss": 1.6446,
        "grad_norm": 2.03057861328125,
        "learning_rate": 0.00019069519174417702,
        "epoch": 1.4270430523330755,
        "step": 11071
    },
    {
        "loss": 1.5116,
        "grad_norm": 3.290149688720703,
        "learning_rate": 0.00019066954692805482,
        "epoch": 1.4271719515339005,
        "step": 11072
    },
    {
        "loss": 1.6006,
        "grad_norm": 3.1362903118133545,
        "learning_rate": 0.00019064386855016874,
        "epoch": 1.4273008507347256,
        "step": 11073
    },
    {
        "loss": 1.3604,
        "grad_norm": 2.441467046737671,
        "learning_rate": 0.00019061815662002368,
        "epoch": 1.4274297499355504,
        "step": 11074
    },
    {
        "loss": 1.9369,
        "grad_norm": 2.4784457683563232,
        "learning_rate": 0.00019059241114713685,
        "epoch": 1.4275586491363754,
        "step": 11075
    },
    {
        "loss": 1.7335,
        "grad_norm": 3.017503261566162,
        "learning_rate": 0.0001905666321410383,
        "epoch": 1.4276875483372002,
        "step": 11076
    },
    {
        "loss": 1.9901,
        "grad_norm": 2.334789991378784,
        "learning_rate": 0.00019054081961127013,
        "epoch": 1.4278164475380253,
        "step": 11077
    },
    {
        "loss": 0.6785,
        "grad_norm": 3.674050807952881,
        "learning_rate": 0.00019051497356738692,
        "epoch": 1.4279453467388503,
        "step": 11078
    },
    {
        "loss": 1.8333,
        "grad_norm": 1.9509296417236328,
        "learning_rate": 0.00019048909401895594,
        "epoch": 1.4280742459396751,
        "step": 11079
    },
    {
        "loss": 1.8165,
        "grad_norm": 2.5920116901397705,
        "learning_rate": 0.00019046318097555638,
        "epoch": 1.4282031451405002,
        "step": 11080
    },
    {
        "loss": 1.4249,
        "grad_norm": 2.8889963626861572,
        "learning_rate": 0.0001904372344467801,
        "epoch": 1.428332044341325,
        "step": 11081
    },
    {
        "loss": 1.5231,
        "grad_norm": 3.1763856410980225,
        "learning_rate": 0.00019041125444223148,
        "epoch": 1.42846094354215,
        "step": 11082
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.0152323246002197,
        "learning_rate": 0.00019038524097152702,
        "epoch": 1.428589842742975,
        "step": 11083
    },
    {
        "loss": 2.1807,
        "grad_norm": 3.348790407180786,
        "learning_rate": 0.0001903591940442958,
        "epoch": 1.4287187419437999,
        "step": 11084
    },
    {
        "loss": 1.3204,
        "grad_norm": 2.2603609561920166,
        "learning_rate": 0.00019033311367017914,
        "epoch": 1.428847641144625,
        "step": 11085
    },
    {
        "loss": 1.2005,
        "grad_norm": 3.848599433898926,
        "learning_rate": 0.00019030699985883088,
        "epoch": 1.4289765403454497,
        "step": 11086
    },
    {
        "loss": 1.182,
        "grad_norm": 3.937462568283081,
        "learning_rate": 0.00019028085261991716,
        "epoch": 1.4291054395462748,
        "step": 11087
    },
    {
        "loss": 1.7921,
        "grad_norm": 2.562241315841675,
        "learning_rate": 0.00019025467196311647,
        "epoch": 1.4292343387470998,
        "step": 11088
    },
    {
        "loss": 2.1095,
        "grad_norm": 1.7905269861221313,
        "learning_rate": 0.00019022845789811967,
        "epoch": 1.4293632379479249,
        "step": 11089
    },
    {
        "loss": 1.2788,
        "grad_norm": 2.3768670558929443,
        "learning_rate": 0.00019020221043463016,
        "epoch": 1.4294921371487497,
        "step": 11090
    },
    {
        "loss": 1.9523,
        "grad_norm": 2.3348000049591064,
        "learning_rate": 0.00019017592958236353,
        "epoch": 1.4296210363495747,
        "step": 11091
    },
    {
        "loss": 1.933,
        "grad_norm": 1.9739168882369995,
        "learning_rate": 0.00019014961535104757,
        "epoch": 1.4297499355503995,
        "step": 11092
    },
    {
        "loss": 0.9745,
        "grad_norm": 2.913668155670166,
        "learning_rate": 0.00019012326775042282,
        "epoch": 1.4298788347512246,
        "step": 11093
    },
    {
        "loss": 1.4235,
        "grad_norm": 5.240665912628174,
        "learning_rate": 0.00019009688679024194,
        "epoch": 1.4300077339520496,
        "step": 11094
    },
    {
        "loss": 2.2983,
        "grad_norm": 1.7931153774261475,
        "learning_rate": 0.00019007047248026986,
        "epoch": 1.4301366331528744,
        "step": 11095
    },
    {
        "loss": 2.4332,
        "grad_norm": 2.512887716293335,
        "learning_rate": 0.0001900440248302841,
        "epoch": 1.4302655323536995,
        "step": 11096
    },
    {
        "loss": 1.8797,
        "grad_norm": 1.9757447242736816,
        "learning_rate": 0.00019001754385007438,
        "epoch": 1.4303944315545243,
        "step": 11097
    },
    {
        "loss": 1.5238,
        "grad_norm": 2.258699893951416,
        "learning_rate": 0.0001899910295494426,
        "epoch": 1.4305233307553493,
        "step": 11098
    },
    {
        "loss": 1.5788,
        "grad_norm": 2.697117567062378,
        "learning_rate": 0.0001899644819382033,
        "epoch": 1.4306522299561744,
        "step": 11099
    },
    {
        "loss": 1.6794,
        "grad_norm": 2.3108012676239014,
        "learning_rate": 0.00018993790102618318,
        "epoch": 1.4307811291569992,
        "step": 11100
    },
    {
        "loss": 0.7325,
        "grad_norm": 3.8060145378112793,
        "learning_rate": 0.00018991128682322125,
        "epoch": 1.4309100283578242,
        "step": 11101
    },
    {
        "loss": 0.8787,
        "grad_norm": 2.908064842224121,
        "learning_rate": 0.00018988463933916893,
        "epoch": 1.431038927558649,
        "step": 11102
    },
    {
        "loss": 1.7735,
        "grad_norm": 1.7742329835891724,
        "learning_rate": 0.0001898579585838899,
        "epoch": 1.431167826759474,
        "step": 11103
    },
    {
        "loss": 1.788,
        "grad_norm": 1.941637396812439,
        "learning_rate": 0.00018983124456726017,
        "epoch": 1.431296725960299,
        "step": 11104
    },
    {
        "loss": 0.8359,
        "grad_norm": 4.303328037261963,
        "learning_rate": 0.00018980449729916803,
        "epoch": 1.431425625161124,
        "step": 11105
    },
    {
        "loss": 1.3812,
        "grad_norm": 3.127021551132202,
        "learning_rate": 0.00018977771678951405,
        "epoch": 1.431554524361949,
        "step": 11106
    },
    {
        "loss": 0.5426,
        "grad_norm": 3.5529706478118896,
        "learning_rate": 0.00018975090304821133,
        "epoch": 1.4316834235627738,
        "step": 11107
    },
    {
        "loss": 1.7938,
        "grad_norm": 1.6341936588287354,
        "learning_rate": 0.00018972405608518505,
        "epoch": 1.4318123227635988,
        "step": 11108
    },
    {
        "loss": 2.4263,
        "grad_norm": 2.3841962814331055,
        "learning_rate": 0.00018969717591037262,
        "epoch": 1.4319412219644239,
        "step": 11109
    },
    {
        "loss": 1.0616,
        "grad_norm": 2.419954538345337,
        "learning_rate": 0.00018967026253372397,
        "epoch": 1.432070121165249,
        "step": 11110
    },
    {
        "loss": 2.2789,
        "grad_norm": 2.3143599033355713,
        "learning_rate": 0.00018964331596520124,
        "epoch": 1.4321990203660737,
        "step": 11111
    },
    {
        "loss": 1.8414,
        "grad_norm": 2.092162847518921,
        "learning_rate": 0.0001896163362147787,
        "epoch": 1.4323279195668988,
        "step": 11112
    },
    {
        "loss": 1.5322,
        "grad_norm": 3.839996576309204,
        "learning_rate": 0.00018958932329244318,
        "epoch": 1.4324568187677236,
        "step": 11113
    },
    {
        "loss": 1.9202,
        "grad_norm": 2.1514408588409424,
        "learning_rate": 0.00018956227720819365,
        "epoch": 1.4325857179685486,
        "step": 11114
    },
    {
        "loss": 1.045,
        "grad_norm": 6.06141471862793,
        "learning_rate": 0.00018953519797204117,
        "epoch": 1.4327146171693736,
        "step": 11115
    },
    {
        "loss": 2.3497,
        "grad_norm": 2.43485689163208,
        "learning_rate": 0.00018950808559400938,
        "epoch": 1.4328435163701985,
        "step": 11116
    },
    {
        "loss": 2.2146,
        "grad_norm": 2.5242977142333984,
        "learning_rate": 0.00018948094008413405,
        "epoch": 1.4329724155710235,
        "step": 11117
    },
    {
        "loss": 1.9337,
        "grad_norm": 2.783848285675049,
        "learning_rate": 0.00018945376145246322,
        "epoch": 1.4331013147718483,
        "step": 11118
    },
    {
        "loss": 1.4366,
        "grad_norm": 2.7212750911712646,
        "learning_rate": 0.00018942654970905714,
        "epoch": 1.4332302139726734,
        "step": 11119
    },
    {
        "loss": 1.7368,
        "grad_norm": 2.73937726020813,
        "learning_rate": 0.0001893993048639884,
        "epoch": 1.4333591131734984,
        "step": 11120
    },
    {
        "loss": 1.9306,
        "grad_norm": 1.8980865478515625,
        "learning_rate": 0.00018937202692734183,
        "epoch": 1.4334880123743232,
        "step": 11121
    },
    {
        "loss": 2.0241,
        "grad_norm": 2.561950206756592,
        "learning_rate": 0.00018934471590921443,
        "epoch": 1.4336169115751483,
        "step": 11122
    },
    {
        "loss": 2.4017,
        "grad_norm": 1.6846774816513062,
        "learning_rate": 0.0001893173718197155,
        "epoch": 1.433745810775973,
        "step": 11123
    },
    {
        "loss": 0.653,
        "grad_norm": 2.649280071258545,
        "learning_rate": 0.00018928999466896667,
        "epoch": 1.4338747099767981,
        "step": 11124
    },
    {
        "loss": 1.9389,
        "grad_norm": 1.6719233989715576,
        "learning_rate": 0.00018926258446710174,
        "epoch": 1.4340036091776232,
        "step": 11125
    },
    {
        "loss": 0.8897,
        "grad_norm": 1.933913230895996,
        "learning_rate": 0.00018923514122426652,
        "epoch": 1.4341325083784482,
        "step": 11126
    },
    {
        "loss": 2.0954,
        "grad_norm": 1.656718373298645,
        "learning_rate": 0.0001892076649506195,
        "epoch": 1.434261407579273,
        "step": 11127
    },
    {
        "loss": 1.1234,
        "grad_norm": 3.212830066680908,
        "learning_rate": 0.00018918015565633103,
        "epoch": 1.434390306780098,
        "step": 11128
    },
    {
        "loss": 1.7196,
        "grad_norm": 2.2898812294006348,
        "learning_rate": 0.00018915261335158374,
        "epoch": 1.4345192059809229,
        "step": 11129
    },
    {
        "loss": 0.9547,
        "grad_norm": 3.0237011909484863,
        "learning_rate": 0.00018912503804657273,
        "epoch": 1.434648105181748,
        "step": 11130
    },
    {
        "loss": 1.608,
        "grad_norm": 3.027916193008423,
        "learning_rate": 0.00018909742975150508,
        "epoch": 1.434777004382573,
        "step": 11131
    },
    {
        "loss": 1.2687,
        "grad_norm": 2.7320756912231445,
        "learning_rate": 0.00018906978847659994,
        "epoch": 1.4349059035833978,
        "step": 11132
    },
    {
        "loss": 1.8608,
        "grad_norm": 2.8551127910614014,
        "learning_rate": 0.00018904211423208912,
        "epoch": 1.4350348027842228,
        "step": 11133
    },
    {
        "loss": 2.1088,
        "grad_norm": 2.4045703411102295,
        "learning_rate": 0.00018901440702821622,
        "epoch": 1.4351637019850476,
        "step": 11134
    },
    {
        "loss": 2.0698,
        "grad_norm": 1.655948519706726,
        "learning_rate": 0.00018898666687523722,
        "epoch": 1.4352926011858727,
        "step": 11135
    },
    {
        "loss": 2.2389,
        "grad_norm": 1.53984797000885,
        "learning_rate": 0.0001889588937834203,
        "epoch": 1.4354215003866977,
        "step": 11136
    },
    {
        "loss": 1.978,
        "grad_norm": 2.881946563720703,
        "learning_rate": 0.00018893108776304578,
        "epoch": 1.4355503995875225,
        "step": 11137
    },
    {
        "loss": 0.6843,
        "grad_norm": 3.048196792602539,
        "learning_rate": 0.0001889032488244062,
        "epoch": 1.4356792987883475,
        "step": 11138
    },
    {
        "loss": 1.1191,
        "grad_norm": 2.8507182598114014,
        "learning_rate": 0.00018887537697780625,
        "epoch": 1.4358081979891724,
        "step": 11139
    },
    {
        "loss": 1.4408,
        "grad_norm": 2.922541379928589,
        "learning_rate": 0.0001888474722335628,
        "epoch": 1.4359370971899974,
        "step": 11140
    },
    {
        "loss": 1.6369,
        "grad_norm": 2.665717840194702,
        "learning_rate": 0.000188819534602005,
        "epoch": 1.4360659963908224,
        "step": 11141
    },
    {
        "loss": 1.6461,
        "grad_norm": 2.022299289703369,
        "learning_rate": 0.00018879156409347414,
        "epoch": 1.4361948955916473,
        "step": 11142
    },
    {
        "loss": 1.353,
        "grad_norm": 2.977297067642212,
        "learning_rate": 0.00018876356071832344,
        "epoch": 1.4363237947924723,
        "step": 11143
    },
    {
        "loss": 2.1613,
        "grad_norm": 1.8350417613983154,
        "learning_rate": 0.00018873552448691867,
        "epoch": 1.4364526939932971,
        "step": 11144
    },
    {
        "loss": 1.6145,
        "grad_norm": 2.234764337539673,
        "learning_rate": 0.00018870745540963753,
        "epoch": 1.4365815931941222,
        "step": 11145
    },
    {
        "loss": 2.0071,
        "grad_norm": 3.235614061355591,
        "learning_rate": 0.0001886793534968698,
        "epoch": 1.4367104923949472,
        "step": 11146
    },
    {
        "loss": 1.8963,
        "grad_norm": 2.4810986518859863,
        "learning_rate": 0.00018865121875901773,
        "epoch": 1.4368393915957722,
        "step": 11147
    },
    {
        "loss": 1.9179,
        "grad_norm": 1.933508038520813,
        "learning_rate": 0.0001886230512064955,
        "epoch": 1.436968290796597,
        "step": 11148
    },
    {
        "loss": 1.6941,
        "grad_norm": 1.8100194931030273,
        "learning_rate": 0.00018859485084972923,
        "epoch": 1.437097189997422,
        "step": 11149
    },
    {
        "loss": 1.9598,
        "grad_norm": 3.5434329509735107,
        "learning_rate": 0.00018856661769915768,
        "epoch": 1.437226089198247,
        "step": 11150
    },
    {
        "loss": 2.1932,
        "grad_norm": 2.0204923152923584,
        "learning_rate": 0.0001885383517652314,
        "epoch": 1.437354988399072,
        "step": 11151
    },
    {
        "loss": 1.2162,
        "grad_norm": 3.129870891571045,
        "learning_rate": 0.00018851005305841313,
        "epoch": 1.437483887599897,
        "step": 11152
    },
    {
        "loss": 1.5125,
        "grad_norm": 3.4906764030456543,
        "learning_rate": 0.0001884817215891778,
        "epoch": 1.4376127868007218,
        "step": 11153
    },
    {
        "loss": 1.5197,
        "grad_norm": 2.2992053031921387,
        "learning_rate": 0.00018845335736801243,
        "epoch": 1.4377416860015468,
        "step": 11154
    },
    {
        "loss": 1.9211,
        "grad_norm": 2.5677244663238525,
        "learning_rate": 0.00018842496040541616,
        "epoch": 1.4378705852023717,
        "step": 11155
    },
    {
        "loss": 1.5902,
        "grad_norm": 2.4090054035186768,
        "learning_rate": 0.00018839653071190028,
        "epoch": 1.4379994844031967,
        "step": 11156
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.33556866645813,
        "learning_rate": 0.0001883680682979881,
        "epoch": 1.4381283836040217,
        "step": 11157
    },
    {
        "loss": 0.8801,
        "grad_norm": 2.735609531402588,
        "learning_rate": 0.00018833957317421524,
        "epoch": 1.4382572828048465,
        "step": 11158
    },
    {
        "loss": 2.207,
        "grad_norm": 1.7374491691589355,
        "learning_rate": 0.00018831104535112924,
        "epoch": 1.4383861820056716,
        "step": 11159
    },
    {
        "loss": 2.3812,
        "grad_norm": 1.7101140022277832,
        "learning_rate": 0.00018828248483928985,
        "epoch": 1.4385150812064964,
        "step": 11160
    },
    {
        "loss": 1.2275,
        "grad_norm": 2.7929320335388184,
        "learning_rate": 0.00018825389164926884,
        "epoch": 1.4386439804073214,
        "step": 11161
    },
    {
        "loss": 1.5956,
        "grad_norm": 3.258626699447632,
        "learning_rate": 0.00018822526579165012,
        "epoch": 1.4387728796081465,
        "step": 11162
    },
    {
        "loss": 2.0878,
        "grad_norm": 3.0815742015838623,
        "learning_rate": 0.00018819660727702965,
        "epoch": 1.4389017788089715,
        "step": 11163
    },
    {
        "loss": 1.7249,
        "grad_norm": 2.0535454750061035,
        "learning_rate": 0.00018816791611601558,
        "epoch": 1.4390306780097963,
        "step": 11164
    },
    {
        "loss": 1.4656,
        "grad_norm": 2.218538761138916,
        "learning_rate": 0.00018813919231922815,
        "epoch": 1.4391595772106214,
        "step": 11165
    },
    {
        "loss": 1.4489,
        "grad_norm": 2.952483892440796,
        "learning_rate": 0.0001881104358972994,
        "epoch": 1.4392884764114462,
        "step": 11166
    },
    {
        "loss": 0.3619,
        "grad_norm": 1.552699089050293,
        "learning_rate": 0.0001880816468608738,
        "epoch": 1.4394173756122712,
        "step": 11167
    },
    {
        "loss": 2.0079,
        "grad_norm": 2.4954757690429688,
        "learning_rate": 0.00018805282522060774,
        "epoch": 1.4395462748130963,
        "step": 11168
    },
    {
        "loss": 2.5287,
        "grad_norm": 1.9924653768539429,
        "learning_rate": 0.00018802397098716965,
        "epoch": 1.439675174013921,
        "step": 11169
    },
    {
        "loss": 2.1057,
        "grad_norm": 2.6366443634033203,
        "learning_rate": 0.00018799508417124012,
        "epoch": 1.4398040732147461,
        "step": 11170
    },
    {
        "loss": 0.8267,
        "grad_norm": 2.7213242053985596,
        "learning_rate": 0.00018796616478351167,
        "epoch": 1.439932972415571,
        "step": 11171
    },
    {
        "loss": 1.7052,
        "grad_norm": 2.362182855606079,
        "learning_rate": 0.000187937212834689,
        "epoch": 1.440061871616396,
        "step": 11172
    },
    {
        "loss": 2.0991,
        "grad_norm": 2.7563674449920654,
        "learning_rate": 0.0001879082283354888,
        "epoch": 1.440190770817221,
        "step": 11173
    },
    {
        "loss": 1.9108,
        "grad_norm": 3.141321897506714,
        "learning_rate": 0.00018787921129663973,
        "epoch": 1.4403196700180458,
        "step": 11174
    },
    {
        "loss": 1.5826,
        "grad_norm": 2.8016257286071777,
        "learning_rate": 0.0001878501617288827,
        "epoch": 1.4404485692188709,
        "step": 11175
    },
    {
        "loss": 1.8475,
        "grad_norm": 2.468478202819824,
        "learning_rate": 0.0001878210796429706,
        "epoch": 1.4405774684196957,
        "step": 11176
    },
    {
        "loss": 1.4276,
        "grad_norm": 2.4438376426696777,
        "learning_rate": 0.00018779196504966817,
        "epoch": 1.4407063676205207,
        "step": 11177
    },
    {
        "loss": 1.9123,
        "grad_norm": 1.4199802875518799,
        "learning_rate": 0.00018776281795975238,
        "epoch": 1.4408352668213458,
        "step": 11178
    },
    {
        "loss": 1.082,
        "grad_norm": 4.0800700187683105,
        "learning_rate": 0.0001877336383840121,
        "epoch": 1.4409641660221706,
        "step": 11179
    },
    {
        "loss": 1.1802,
        "grad_norm": 3.0558488368988037,
        "learning_rate": 0.00018770442633324832,
        "epoch": 1.4410930652229956,
        "step": 11180
    },
    {
        "loss": 2.0644,
        "grad_norm": 2.448957920074463,
        "learning_rate": 0.00018767518181827405,
        "epoch": 1.4412219644238204,
        "step": 11181
    },
    {
        "loss": 2.0838,
        "grad_norm": 2.0507004261016846,
        "learning_rate": 0.00018764590484991438,
        "epoch": 1.4413508636246455,
        "step": 11182
    },
    {
        "loss": 2.3689,
        "grad_norm": 1.4751728773117065,
        "learning_rate": 0.00018761659543900606,
        "epoch": 1.4414797628254705,
        "step": 11183
    },
    {
        "loss": 2.0066,
        "grad_norm": 2.178640127182007,
        "learning_rate": 0.00018758725359639832,
        "epoch": 1.4416086620262956,
        "step": 11184
    },
    {
        "loss": 1.1583,
        "grad_norm": 2.556607484817505,
        "learning_rate": 0.00018755787933295214,
        "epoch": 1.4417375612271204,
        "step": 11185
    },
    {
        "loss": 2.0514,
        "grad_norm": 1.8087341785430908,
        "learning_rate": 0.0001875284726595405,
        "epoch": 1.4418664604279454,
        "step": 11186
    },
    {
        "loss": 1.751,
        "grad_norm": 1.8829454183578491,
        "learning_rate": 0.00018749903358704844,
        "epoch": 1.4419953596287702,
        "step": 11187
    },
    {
        "loss": 1.378,
        "grad_norm": 2.723275899887085,
        "learning_rate": 0.000187469562126373,
        "epoch": 1.4421242588295953,
        "step": 11188
    },
    {
        "loss": 1.7306,
        "grad_norm": 4.149873733520508,
        "learning_rate": 0.00018744005828842314,
        "epoch": 1.4422531580304203,
        "step": 11189
    },
    {
        "loss": 1.7367,
        "grad_norm": 2.270510673522949,
        "learning_rate": 0.00018741052208411988,
        "epoch": 1.4423820572312451,
        "step": 11190
    },
    {
        "loss": 1.5954,
        "grad_norm": 1.81801176071167,
        "learning_rate": 0.0001873809535243961,
        "epoch": 1.4425109564320702,
        "step": 11191
    },
    {
        "loss": 1.4533,
        "grad_norm": 2.94313907623291,
        "learning_rate": 0.00018735135262019684,
        "epoch": 1.442639855632895,
        "step": 11192
    },
    {
        "loss": 1.9657,
        "grad_norm": 1.7244848012924194,
        "learning_rate": 0.00018732171938247905,
        "epoch": 1.44276875483372,
        "step": 11193
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.740248441696167,
        "learning_rate": 0.0001872920538222115,
        "epoch": 1.442897654034545,
        "step": 11194
    },
    {
        "loss": 1.5157,
        "grad_norm": 2.4864816665649414,
        "learning_rate": 0.00018726235595037513,
        "epoch": 1.4430265532353699,
        "step": 11195
    },
    {
        "loss": 1.8615,
        "grad_norm": 2.079970598220825,
        "learning_rate": 0.0001872326257779627,
        "epoch": 1.443155452436195,
        "step": 11196
    },
    {
        "loss": 1.4738,
        "grad_norm": 2.870457172393799,
        "learning_rate": 0.0001872028633159789,
        "epoch": 1.4432843516370197,
        "step": 11197
    },
    {
        "loss": 1.5081,
        "grad_norm": 2.9813969135284424,
        "learning_rate": 0.00018717306857544058,
        "epoch": 1.4434132508378448,
        "step": 11198
    },
    {
        "loss": 1.8803,
        "grad_norm": 2.263481855392456,
        "learning_rate": 0.0001871432415673764,
        "epoch": 1.4435421500386698,
        "step": 11199
    },
    {
        "loss": 1.6238,
        "grad_norm": 2.525239944458008,
        "learning_rate": 0.0001871133823028269,
        "epoch": 1.4436710492394949,
        "step": 11200
    },
    {
        "loss": 2.3168,
        "grad_norm": 1.7623045444488525,
        "learning_rate": 0.00018708349079284465,
        "epoch": 1.4437999484403197,
        "step": 11201
    },
    {
        "loss": 2.137,
        "grad_norm": 2.704254388809204,
        "learning_rate": 0.00018705356704849414,
        "epoch": 1.4439288476411445,
        "step": 11202
    },
    {
        "loss": 1.6562,
        "grad_norm": 2.069575071334839,
        "learning_rate": 0.0001870236110808518,
        "epoch": 1.4440577468419695,
        "step": 11203
    },
    {
        "loss": 1.026,
        "grad_norm": 2.788426399230957,
        "learning_rate": 0.00018699362290100594,
        "epoch": 1.4441866460427946,
        "step": 11204
    },
    {
        "loss": 0.8869,
        "grad_norm": 3.182554244995117,
        "learning_rate": 0.00018696360252005686,
        "epoch": 1.4443155452436196,
        "step": 11205
    },
    {
        "loss": 1.8042,
        "grad_norm": 2.857240915298462,
        "learning_rate": 0.0001869335499491167,
        "epoch": 1.4444444444444444,
        "step": 11206
    },
    {
        "loss": 2.1973,
        "grad_norm": 2.694972515106201,
        "learning_rate": 0.00018690346519930964,
        "epoch": 1.4445733436452695,
        "step": 11207
    },
    {
        "loss": 1.7052,
        "grad_norm": 2.1318178176879883,
        "learning_rate": 0.00018687334828177153,
        "epoch": 1.4447022428460943,
        "step": 11208
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.2668206691741943,
        "learning_rate": 0.00018684319920765052,
        "epoch": 1.4448311420469193,
        "step": 11209
    },
    {
        "loss": 1.6715,
        "grad_norm": 2.546107053756714,
        "learning_rate": 0.00018681301798810629,
        "epoch": 1.4449600412477444,
        "step": 11210
    },
    {
        "loss": 2.0893,
        "grad_norm": 2.787616014480591,
        "learning_rate": 0.00018678280463431063,
        "epoch": 1.4450889404485692,
        "step": 11211
    },
    {
        "loss": 1.212,
        "grad_norm": 2.9328291416168213,
        "learning_rate": 0.00018675255915744706,
        "epoch": 1.4452178396493942,
        "step": 11212
    },
    {
        "loss": 1.8922,
        "grad_norm": 1.593990445137024,
        "learning_rate": 0.0001867222815687112,
        "epoch": 1.445346738850219,
        "step": 11213
    },
    {
        "loss": 2.1693,
        "grad_norm": 2.5452494621276855,
        "learning_rate": 0.0001866919718793103,
        "epoch": 1.445475638051044,
        "step": 11214
    },
    {
        "loss": 1.9694,
        "grad_norm": 2.6560747623443604,
        "learning_rate": 0.00018666163010046377,
        "epoch": 1.445604537251869,
        "step": 11215
    },
    {
        "loss": 2.0651,
        "grad_norm": 2.086756467819214,
        "learning_rate": 0.00018663125624340275,
        "epoch": 1.445733436452694,
        "step": 11216
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.9936041831970215,
        "learning_rate": 0.00018660085031937024,
        "epoch": 1.445862335653519,
        "step": 11217
    },
    {
        "loss": 1.9415,
        "grad_norm": 2.164093494415283,
        "learning_rate": 0.0001865704123396211,
        "epoch": 1.4459912348543438,
        "step": 11218
    },
    {
        "loss": 1.9948,
        "grad_norm": 2.256565809249878,
        "learning_rate": 0.00018653994231542217,
        "epoch": 1.4461201340551688,
        "step": 11219
    },
    {
        "loss": 1.5429,
        "grad_norm": 2.585127115249634,
        "learning_rate": 0.000186509440258052,
        "epoch": 1.4462490332559939,
        "step": 11220
    },
    {
        "loss": 1.6958,
        "grad_norm": 2.558722972869873,
        "learning_rate": 0.00018647890617880112,
        "epoch": 1.446377932456819,
        "step": 11221
    },
    {
        "loss": 1.8054,
        "grad_norm": 2.8516170978546143,
        "learning_rate": 0.0001864483400889718,
        "epoch": 1.4465068316576437,
        "step": 11222
    },
    {
        "loss": 2.21,
        "grad_norm": 1.3146415948867798,
        "learning_rate": 0.00018641774199987837,
        "epoch": 1.4466357308584687,
        "step": 11223
    },
    {
        "loss": 1.5709,
        "grad_norm": 2.0454823970794678,
        "learning_rate": 0.00018638711192284669,
        "epoch": 1.4467646300592936,
        "step": 11224
    },
    {
        "loss": 1.4421,
        "grad_norm": 2.670927047729492,
        "learning_rate": 0.00018635644986921465,
        "epoch": 1.4468935292601186,
        "step": 11225
    },
    {
        "loss": 1.3529,
        "grad_norm": 2.1991569995880127,
        "learning_rate": 0.00018632575585033205,
        "epoch": 1.4470224284609436,
        "step": 11226
    },
    {
        "loss": 1.6877,
        "grad_norm": 2.581239700317383,
        "learning_rate": 0.0001862950298775604,
        "epoch": 1.4471513276617685,
        "step": 11227
    },
    {
        "loss": 1.3657,
        "grad_norm": 3.5158679485321045,
        "learning_rate": 0.00018626427196227305,
        "epoch": 1.4472802268625935,
        "step": 11228
    },
    {
        "loss": 1.665,
        "grad_norm": 2.1856517791748047,
        "learning_rate": 0.00018623348211585516,
        "epoch": 1.4474091260634183,
        "step": 11229
    },
    {
        "loss": 1.1425,
        "grad_norm": 3.26623797416687,
        "learning_rate": 0.00018620266034970377,
        "epoch": 1.4475380252642434,
        "step": 11230
    },
    {
        "loss": 0.9089,
        "grad_norm": 2.8587162494659424,
        "learning_rate": 0.00018617180667522765,
        "epoch": 1.4476669244650684,
        "step": 11231
    },
    {
        "loss": 1.9846,
        "grad_norm": 1.7417048215866089,
        "learning_rate": 0.00018614092110384752,
        "epoch": 1.4477958236658932,
        "step": 11232
    },
    {
        "loss": 2.2441,
        "grad_norm": 1.8178480863571167,
        "learning_rate": 0.00018611000364699578,
        "epoch": 1.4479247228667183,
        "step": 11233
    },
    {
        "loss": 1.4317,
        "grad_norm": 2.7799935340881348,
        "learning_rate": 0.00018607905431611667,
        "epoch": 1.448053622067543,
        "step": 11234
    },
    {
        "loss": 1.893,
        "grad_norm": 2.3553218841552734,
        "learning_rate": 0.00018604807312266625,
        "epoch": 1.448182521268368,
        "step": 11235
    },
    {
        "loss": 1.863,
        "grad_norm": 2.197305202484131,
        "learning_rate": 0.00018601706007811234,
        "epoch": 1.4483114204691931,
        "step": 11236
    },
    {
        "loss": 1.8547,
        "grad_norm": 2.0530178546905518,
        "learning_rate": 0.0001859860151939346,
        "epoch": 1.4484403196700182,
        "step": 11237
    },
    {
        "loss": 2.263,
        "grad_norm": 2.930985450744629,
        "learning_rate": 0.00018595493848162436,
        "epoch": 1.448569218870843,
        "step": 11238
    },
    {
        "loss": 1.5449,
        "grad_norm": 2.1201088428497314,
        "learning_rate": 0.0001859238299526848,
        "epoch": 1.4486981180716678,
        "step": 11239
    },
    {
        "loss": 1.8348,
        "grad_norm": 2.5902411937713623,
        "learning_rate": 0.0001858926896186311,
        "epoch": 1.4488270172724929,
        "step": 11240
    },
    {
        "loss": 1.6868,
        "grad_norm": 1.6763681173324585,
        "learning_rate": 0.0001858615174909898,
        "epoch": 1.448955916473318,
        "step": 11241
    },
    {
        "loss": 0.3162,
        "grad_norm": 2.558234930038452,
        "learning_rate": 0.00018583031358129938,
        "epoch": 1.449084815674143,
        "step": 11242
    },
    {
        "loss": 2.0124,
        "grad_norm": 1.9063471555709839,
        "learning_rate": 0.0001857990779011103,
        "epoch": 1.4492137148749678,
        "step": 11243
    },
    {
        "loss": 2.03,
        "grad_norm": 2.5654335021972656,
        "learning_rate": 0.00018576781046198446,
        "epoch": 1.4493426140757928,
        "step": 11244
    },
    {
        "loss": 1.6907,
        "grad_norm": 2.9881460666656494,
        "learning_rate": 0.00018573651127549572,
        "epoch": 1.4494715132766176,
        "step": 11245
    },
    {
        "loss": 1.6801,
        "grad_norm": 3.102742910385132,
        "learning_rate": 0.00018570518035322957,
        "epoch": 1.4496004124774426,
        "step": 11246
    },
    {
        "loss": 1.5488,
        "grad_norm": 1.5591952800750732,
        "learning_rate": 0.0001856738177067833,
        "epoch": 1.4497293116782677,
        "step": 11247
    },
    {
        "loss": 1.7141,
        "grad_norm": 2.4585378170013428,
        "learning_rate": 0.00018564242334776594,
        "epoch": 1.4498582108790925,
        "step": 11248
    },
    {
        "loss": 0.3217,
        "grad_norm": 3.155667543411255,
        "learning_rate": 0.0001856109972877983,
        "epoch": 1.4499871100799175,
        "step": 11249
    },
    {
        "loss": 1.798,
        "grad_norm": 3.1791796684265137,
        "learning_rate": 0.0001855795395385129,
        "epoch": 1.4501160092807424,
        "step": 11250
    },
    {
        "loss": 2.1517,
        "grad_norm": 2.490144968032837,
        "learning_rate": 0.000185548050111554,
        "epoch": 1.4502449084815674,
        "step": 11251
    },
    {
        "loss": 1.7592,
        "grad_norm": 2.297685384750366,
        "learning_rate": 0.00018551652901857747,
        "epoch": 1.4503738076823924,
        "step": 11252
    },
    {
        "loss": 1.5168,
        "grad_norm": 5.45648717880249,
        "learning_rate": 0.00018548497627125102,
        "epoch": 1.4505027068832173,
        "step": 11253
    },
    {
        "loss": 0.912,
        "grad_norm": 1.6974209547042847,
        "learning_rate": 0.00018545339188125407,
        "epoch": 1.4506316060840423,
        "step": 11254
    },
    {
        "loss": 1.3226,
        "grad_norm": 2.893761396408081,
        "learning_rate": 0.0001854217758602777,
        "epoch": 1.450760505284867,
        "step": 11255
    },
    {
        "loss": 2.1544,
        "grad_norm": 2.156956434249878,
        "learning_rate": 0.00018539012822002467,
        "epoch": 1.4508894044856921,
        "step": 11256
    },
    {
        "loss": 2.4538,
        "grad_norm": 2.2008581161499023,
        "learning_rate": 0.00018535844897220978,
        "epoch": 1.4510183036865172,
        "step": 11257
    },
    {
        "loss": 1.7955,
        "grad_norm": 1.4813778400421143,
        "learning_rate": 0.00018532673812855896,
        "epoch": 1.4511472028873422,
        "step": 11258
    },
    {
        "loss": 2.016,
        "grad_norm": 2.1600217819213867,
        "learning_rate": 0.00018529499570081018,
        "epoch": 1.451276102088167,
        "step": 11259
    },
    {
        "loss": 1.5219,
        "grad_norm": 2.2093393802642822,
        "learning_rate": 0.00018526322170071312,
        "epoch": 1.451405001288992,
        "step": 11260
    },
    {
        "loss": 2.191,
        "grad_norm": 1.2401286363601685,
        "learning_rate": 0.00018523141614002912,
        "epoch": 1.451533900489817,
        "step": 11261
    },
    {
        "loss": 1.1487,
        "grad_norm": 2.218722105026245,
        "learning_rate": 0.000185199579030531,
        "epoch": 1.451662799690642,
        "step": 11262
    },
    {
        "loss": 1.7179,
        "grad_norm": 1.7594236135482788,
        "learning_rate": 0.00018516771038400368,
        "epoch": 1.451791698891467,
        "step": 11263
    },
    {
        "loss": 2.3803,
        "grad_norm": 1.2743059396743774,
        "learning_rate": 0.00018513581021224323,
        "epoch": 1.4519205980922918,
        "step": 11264
    },
    {
        "loss": 1.3143,
        "grad_norm": 2.490342378616333,
        "learning_rate": 0.00018510387852705765,
        "epoch": 1.4520494972931168,
        "step": 11265
    },
    {
        "loss": 2.0718,
        "grad_norm": 3.203526496887207,
        "learning_rate": 0.00018507191534026683,
        "epoch": 1.4521783964939416,
        "step": 11266
    },
    {
        "loss": 1.6359,
        "grad_norm": 2.2975211143493652,
        "learning_rate": 0.00018503992066370195,
        "epoch": 1.4523072956947667,
        "step": 11267
    },
    {
        "loss": 1.5563,
        "grad_norm": 2.8033788204193115,
        "learning_rate": 0.000185007894509206,
        "epoch": 1.4524361948955917,
        "step": 11268
    },
    {
        "loss": 1.281,
        "grad_norm": 2.3621835708618164,
        "learning_rate": 0.00018497583688863363,
        "epoch": 1.4525650940964165,
        "step": 11269
    },
    {
        "loss": 0.8243,
        "grad_norm": 2.114736557006836,
        "learning_rate": 0.00018494374781385112,
        "epoch": 1.4526939932972416,
        "step": 11270
    },
    {
        "loss": 1.6514,
        "grad_norm": 1.9821925163269043,
        "learning_rate": 0.00018491162729673634,
        "epoch": 1.4528228924980664,
        "step": 11271
    },
    {
        "loss": 2.1465,
        "grad_norm": 1.739828109741211,
        "learning_rate": 0.00018487947534917895,
        "epoch": 1.4529517916988914,
        "step": 11272
    },
    {
        "loss": 1.8154,
        "grad_norm": 2.5283923149108887,
        "learning_rate": 0.00018484729198308002,
        "epoch": 1.4530806908997165,
        "step": 11273
    },
    {
        "loss": 0.9934,
        "grad_norm": 3.651188373565674,
        "learning_rate": 0.00018481507721035257,
        "epoch": 1.4532095901005415,
        "step": 11274
    },
    {
        "loss": 1.7883,
        "grad_norm": 3.0621204376220703,
        "learning_rate": 0.0001847828310429209,
        "epoch": 1.4533384893013663,
        "step": 11275
    },
    {
        "loss": 0.9755,
        "grad_norm": 2.867266893386841,
        "learning_rate": 0.00018475055349272102,
        "epoch": 1.4534673885021911,
        "step": 11276
    },
    {
        "loss": 2.1495,
        "grad_norm": 2.473836660385132,
        "learning_rate": 0.00018471824457170078,
        "epoch": 1.4535962877030162,
        "step": 11277
    },
    {
        "loss": 2.0225,
        "grad_norm": 2.2531673908233643,
        "learning_rate": 0.0001846859042918194,
        "epoch": 1.4537251869038412,
        "step": 11278
    },
    {
        "loss": 1.4756,
        "grad_norm": 2.527406692504883,
        "learning_rate": 0.00018465353266504777,
        "epoch": 1.4538540861046663,
        "step": 11279
    },
    {
        "loss": 1.4573,
        "grad_norm": 3.2249608039855957,
        "learning_rate": 0.00018462112970336856,
        "epoch": 1.453982985305491,
        "step": 11280
    },
    {
        "loss": 1.2841,
        "grad_norm": 4.464466094970703,
        "learning_rate": 0.00018458869541877567,
        "epoch": 1.4541118845063161,
        "step": 11281
    },
    {
        "loss": 1.0658,
        "grad_norm": 3.0601518154144287,
        "learning_rate": 0.00018455622982327483,
        "epoch": 1.454240783707141,
        "step": 11282
    },
    {
        "loss": 2.2293,
        "grad_norm": 2.464477300643921,
        "learning_rate": 0.00018452373292888346,
        "epoch": 1.454369682907966,
        "step": 11283
    },
    {
        "loss": 2.2614,
        "grad_norm": 2.5032150745391846,
        "learning_rate": 0.00018449120474763037,
        "epoch": 1.454498582108791,
        "step": 11284
    },
    {
        "loss": 2.0856,
        "grad_norm": 1.904062032699585,
        "learning_rate": 0.00018445864529155606,
        "epoch": 1.4546274813096158,
        "step": 11285
    },
    {
        "loss": 2.1346,
        "grad_norm": 2.7514684200286865,
        "learning_rate": 0.00018442605457271252,
        "epoch": 1.4547563805104409,
        "step": 11286
    },
    {
        "loss": 1.9759,
        "grad_norm": 2.645109176635742,
        "learning_rate": 0.00018439343260316334,
        "epoch": 1.4548852797112657,
        "step": 11287
    },
    {
        "loss": 1.1829,
        "grad_norm": 2.9211816787719727,
        "learning_rate": 0.00018436077939498375,
        "epoch": 1.4550141789120907,
        "step": 11288
    },
    {
        "loss": 2.1218,
        "grad_norm": 2.53499174118042,
        "learning_rate": 0.00018432809496026045,
        "epoch": 1.4551430781129158,
        "step": 11289
    },
    {
        "loss": 1.9186,
        "grad_norm": 2.106630325317383,
        "learning_rate": 0.00018429537931109174,
        "epoch": 1.4552719773137406,
        "step": 11290
    },
    {
        "loss": 2.0982,
        "grad_norm": 2.610023260116577,
        "learning_rate": 0.00018426263245958758,
        "epoch": 1.4554008765145656,
        "step": 11291
    },
    {
        "loss": 1.3813,
        "grad_norm": 2.9132981300354004,
        "learning_rate": 0.00018422985441786925,
        "epoch": 1.4555297757153904,
        "step": 11292
    },
    {
        "loss": 1.2195,
        "grad_norm": 3.2692372798919678,
        "learning_rate": 0.00018419704519806964,
        "epoch": 1.4556586749162155,
        "step": 11293
    },
    {
        "loss": 1.8653,
        "grad_norm": 2.4380602836608887,
        "learning_rate": 0.00018416420481233341,
        "epoch": 1.4557875741170405,
        "step": 11294
    },
    {
        "loss": 1.4563,
        "grad_norm": 2.5114240646362305,
        "learning_rate": 0.00018413133327281652,
        "epoch": 1.4559164733178656,
        "step": 11295
    },
    {
        "loss": 0.8362,
        "grad_norm": 3.2750864028930664,
        "learning_rate": 0.0001840984305916864,
        "epoch": 1.4560453725186904,
        "step": 11296
    },
    {
        "loss": 2.309,
        "grad_norm": 1.304358959197998,
        "learning_rate": 0.00018406549678112244,
        "epoch": 1.4561742717195154,
        "step": 11297
    },
    {
        "loss": 1.6519,
        "grad_norm": 1.9827390909194946,
        "learning_rate": 0.00018403253185331496,
        "epoch": 1.4563031709203402,
        "step": 11298
    },
    {
        "loss": 1.9452,
        "grad_norm": 1.6504485607147217,
        "learning_rate": 0.0001839995358204661,
        "epoch": 1.4564320701211653,
        "step": 11299
    },
    {
        "loss": 1.4096,
        "grad_norm": 2.653578758239746,
        "learning_rate": 0.00018396650869478967,
        "epoch": 1.4565609693219903,
        "step": 11300
    },
    {
        "loss": 1.3511,
        "grad_norm": 2.6088175773620605,
        "learning_rate": 0.00018393345048851073,
        "epoch": 1.4566898685228151,
        "step": 11301
    },
    {
        "loss": 1.5163,
        "grad_norm": 1.431609034538269,
        "learning_rate": 0.0001839003612138659,
        "epoch": 1.4568187677236402,
        "step": 11302
    },
    {
        "loss": 1.9452,
        "grad_norm": 2.893878698348999,
        "learning_rate": 0.00018386724088310333,
        "epoch": 1.456947666924465,
        "step": 11303
    },
    {
        "loss": 1.4095,
        "grad_norm": 2.1444199085235596,
        "learning_rate": 0.00018383408950848275,
        "epoch": 1.45707656612529,
        "step": 11304
    },
    {
        "loss": 0.7582,
        "grad_norm": 2.672379732131958,
        "learning_rate": 0.00018380090710227523,
        "epoch": 1.457205465326115,
        "step": 11305
    },
    {
        "loss": 2.6068,
        "grad_norm": 2.8256287574768066,
        "learning_rate": 0.00018376769367676343,
        "epoch": 1.4573343645269399,
        "step": 11306
    },
    {
        "loss": 2.0665,
        "grad_norm": 1.7753615379333496,
        "learning_rate": 0.00018373444924424135,
        "epoch": 1.457463263727765,
        "step": 11307
    },
    {
        "loss": 1.7334,
        "grad_norm": 1.9177649021148682,
        "learning_rate": 0.0001837011738170148,
        "epoch": 1.4575921629285897,
        "step": 11308
    },
    {
        "loss": 1.0871,
        "grad_norm": 3.3429160118103027,
        "learning_rate": 0.00018366786740740064,
        "epoch": 1.4577210621294148,
        "step": 11309
    },
    {
        "loss": 1.965,
        "grad_norm": 2.4436213970184326,
        "learning_rate": 0.00018363453002772737,
        "epoch": 1.4578499613302398,
        "step": 11310
    },
    {
        "loss": 2.0211,
        "grad_norm": 2.94419527053833,
        "learning_rate": 0.00018360116169033515,
        "epoch": 1.4579788605310646,
        "step": 11311
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.3567222356796265,
        "learning_rate": 0.00018356776240757534,
        "epoch": 1.4581077597318897,
        "step": 11312
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.7156126499176025,
        "learning_rate": 0.00018353433219181077,
        "epoch": 1.4582366589327145,
        "step": 11313
    },
    {
        "loss": 1.7793,
        "grad_norm": 2.492523193359375,
        "learning_rate": 0.00018350087105541604,
        "epoch": 1.4583655581335395,
        "step": 11314
    },
    {
        "loss": 1.6786,
        "grad_norm": 1.9318387508392334,
        "learning_rate": 0.0001834673790107767,
        "epoch": 1.4584944573343646,
        "step": 11315
    },
    {
        "loss": 2.1211,
        "grad_norm": 1.7279245853424072,
        "learning_rate": 0.00018343385607029,
        "epoch": 1.4586233565351896,
        "step": 11316
    },
    {
        "loss": 1.4386,
        "grad_norm": 3.5427987575531006,
        "learning_rate": 0.00018340030224636475,
        "epoch": 1.4587522557360144,
        "step": 11317
    },
    {
        "loss": 2.3237,
        "grad_norm": 2.006314754486084,
        "learning_rate": 0.000183366717551421,
        "epoch": 1.4588811549368395,
        "step": 11318
    },
    {
        "loss": 1.1109,
        "grad_norm": 3.651162624359131,
        "learning_rate": 0.0001833331019978903,
        "epoch": 1.4590100541376643,
        "step": 11319
    },
    {
        "loss": 1.6332,
        "grad_norm": 2.8804495334625244,
        "learning_rate": 0.00018329945559821556,
        "epoch": 1.4591389533384893,
        "step": 11320
    },
    {
        "loss": 1.7965,
        "grad_norm": 2.6287217140197754,
        "learning_rate": 0.00018326577836485117,
        "epoch": 1.4592678525393143,
        "step": 11321
    },
    {
        "loss": 1.5073,
        "grad_norm": 1.9456291198730469,
        "learning_rate": 0.00018323207031026298,
        "epoch": 1.4593967517401392,
        "step": 11322
    },
    {
        "loss": 1.8114,
        "grad_norm": 2.084920883178711,
        "learning_rate": 0.0001831983314469281,
        "epoch": 1.4595256509409642,
        "step": 11323
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.3414521217346191,
        "learning_rate": 0.00018316456178733509,
        "epoch": 1.459654550141789,
        "step": 11324
    },
    {
        "loss": 1.7661,
        "grad_norm": 1.6288093328475952,
        "learning_rate": 0.00018313076134398408,
        "epoch": 1.459783449342614,
        "step": 11325
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.095871686935425,
        "learning_rate": 0.0001830969301293865,
        "epoch": 1.459912348543439,
        "step": 11326
    },
    {
        "loss": 0.5075,
        "grad_norm": 3.3422863483428955,
        "learning_rate": 0.0001830630681560649,
        "epoch": 1.460041247744264,
        "step": 11327
    },
    {
        "loss": 1.9426,
        "grad_norm": 2.2664802074432373,
        "learning_rate": 0.00018302917543655368,
        "epoch": 1.460170146945089,
        "step": 11328
    },
    {
        "loss": 2.1962,
        "grad_norm": 1.7392348051071167,
        "learning_rate": 0.00018299525198339832,
        "epoch": 1.4602990461459138,
        "step": 11329
    },
    {
        "loss": 1.8869,
        "grad_norm": 1.7753702402114868,
        "learning_rate": 0.00018296129780915563,
        "epoch": 1.4604279453467388,
        "step": 11330
    },
    {
        "loss": 2.3132,
        "grad_norm": 1.7419915199279785,
        "learning_rate": 0.0001829273129263942,
        "epoch": 1.4605568445475638,
        "step": 11331
    },
    {
        "loss": 1.364,
        "grad_norm": 2.5077288150787354,
        "learning_rate": 0.0001828932973476934,
        "epoch": 1.4606857437483889,
        "step": 11332
    },
    {
        "loss": 2.502,
        "grad_norm": 1.9036036729812622,
        "learning_rate": 0.0001828592510856443,
        "epoch": 1.4608146429492137,
        "step": 11333
    },
    {
        "loss": 1.5928,
        "grad_norm": 3.4504427909851074,
        "learning_rate": 0.00018282517415284946,
        "epoch": 1.4609435421500387,
        "step": 11334
    },
    {
        "loss": 1.263,
        "grad_norm": 2.252696990966797,
        "learning_rate": 0.0001827910665619225,
        "epoch": 1.4610724413508636,
        "step": 11335
    },
    {
        "loss": 2.477,
        "grad_norm": 2.085805654525757,
        "learning_rate": 0.00018275692832548849,
        "epoch": 1.4612013405516886,
        "step": 11336
    },
    {
        "loss": 1.846,
        "grad_norm": 3.539017915725708,
        "learning_rate": 0.0001827227594561839,
        "epoch": 1.4613302397525136,
        "step": 11337
    },
    {
        "loss": 1.5562,
        "grad_norm": 2.096544027328491,
        "learning_rate": 0.00018268855996665654,
        "epoch": 1.4614591389533385,
        "step": 11338
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.5975617170333862,
        "learning_rate": 0.0001826543298695654,
        "epoch": 1.4615880381541635,
        "step": 11339
    },
    {
        "loss": 0.9045,
        "grad_norm": 2.6720731258392334,
        "learning_rate": 0.000182620069177581,
        "epoch": 1.4617169373549883,
        "step": 11340
    },
    {
        "loss": 1.6952,
        "grad_norm": 1.8130828142166138,
        "learning_rate": 0.000182585777903385,
        "epoch": 1.4618458365558133,
        "step": 11341
    },
    {
        "loss": 0.9093,
        "grad_norm": 2.95063853263855,
        "learning_rate": 0.0001825514560596706,
        "epoch": 1.4619747357566384,
        "step": 11342
    },
    {
        "loss": 1.8674,
        "grad_norm": 2.851433515548706,
        "learning_rate": 0.00018251710365914227,
        "epoch": 1.4621036349574632,
        "step": 11343
    },
    {
        "loss": 2.0025,
        "grad_norm": 1.6294944286346436,
        "learning_rate": 0.00018248272071451544,
        "epoch": 1.4622325341582882,
        "step": 11344
    },
    {
        "loss": 1.213,
        "grad_norm": 3.4484057426452637,
        "learning_rate": 0.00018244830723851733,
        "epoch": 1.462361433359113,
        "step": 11345
    },
    {
        "loss": 1.498,
        "grad_norm": 1.6493453979492188,
        "learning_rate": 0.00018241386324388618,
        "epoch": 1.462490332559938,
        "step": 11346
    },
    {
        "loss": 1.1547,
        "grad_norm": 2.3199570178985596,
        "learning_rate": 0.00018237938874337157,
        "epoch": 1.4626192317607631,
        "step": 11347
    },
    {
        "loss": 1.2971,
        "grad_norm": 2.273559331893921,
        "learning_rate": 0.00018234488374973447,
        "epoch": 1.462748130961588,
        "step": 11348
    },
    {
        "loss": 2.0453,
        "grad_norm": 2.3824286460876465,
        "learning_rate": 0.00018231034827574714,
        "epoch": 1.462877030162413,
        "step": 11349
    },
    {
        "loss": 2.0039,
        "grad_norm": 2.89792799949646,
        "learning_rate": 0.00018227578233419282,
        "epoch": 1.4630059293632378,
        "step": 11350
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.824547529220581,
        "learning_rate": 0.00018224118593786645,
        "epoch": 1.4631348285640629,
        "step": 11351
    },
    {
        "loss": 2.0182,
        "grad_norm": 1.9904487133026123,
        "learning_rate": 0.00018220655909957398,
        "epoch": 1.463263727764888,
        "step": 11352
    },
    {
        "loss": 2.1862,
        "grad_norm": 2.600131034851074,
        "learning_rate": 0.00018217190183213272,
        "epoch": 1.463392626965713,
        "step": 11353
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.063236713409424,
        "learning_rate": 0.0001821372141483712,
        "epoch": 1.4635215261665377,
        "step": 11354
    },
    {
        "loss": 1.9084,
        "grad_norm": 2.2181148529052734,
        "learning_rate": 0.00018210249606112923,
        "epoch": 1.4636504253673628,
        "step": 11355
    },
    {
        "loss": 1.311,
        "grad_norm": 2.9118432998657227,
        "learning_rate": 0.00018206774758325792,
        "epoch": 1.4637793245681876,
        "step": 11356
    },
    {
        "loss": 1.6895,
        "grad_norm": 2.8647916316986084,
        "learning_rate": 0.00018203296872761952,
        "epoch": 1.4639082237690126,
        "step": 11357
    },
    {
        "loss": 0.6557,
        "grad_norm": 3.347635269165039,
        "learning_rate": 0.00018199815950708755,
        "epoch": 1.4640371229698377,
        "step": 11358
    },
    {
        "loss": 1.2443,
        "grad_norm": 2.3251469135284424,
        "learning_rate": 0.00018196331993454697,
        "epoch": 1.4641660221706625,
        "step": 11359
    },
    {
        "loss": 0.8863,
        "grad_norm": 3.233797311782837,
        "learning_rate": 0.00018192845002289377,
        "epoch": 1.4642949213714875,
        "step": 11360
    },
    {
        "loss": 1.7832,
        "grad_norm": 3.0547709465026855,
        "learning_rate": 0.00018189354978503502,
        "epoch": 1.4644238205723124,
        "step": 11361
    },
    {
        "loss": 1.8101,
        "grad_norm": 1.4491175413131714,
        "learning_rate": 0.00018185861923388945,
        "epoch": 1.4645527197731374,
        "step": 11362
    },
    {
        "loss": 2.1715,
        "grad_norm": 2.1391515731811523,
        "learning_rate": 0.00018182365838238665,
        "epoch": 1.4646816189739624,
        "step": 11363
    },
    {
        "loss": 0.9844,
        "grad_norm": 2.071295976638794,
        "learning_rate": 0.00018178866724346748,
        "epoch": 1.4648105181747872,
        "step": 11364
    },
    {
        "loss": 1.5787,
        "grad_norm": 2.3612825870513916,
        "learning_rate": 0.00018175364583008428,
        "epoch": 1.4649394173756123,
        "step": 11365
    },
    {
        "loss": 1.6654,
        "grad_norm": 2.7434074878692627,
        "learning_rate": 0.00018171859415520034,
        "epoch": 1.465068316576437,
        "step": 11366
    },
    {
        "loss": 1.2133,
        "grad_norm": 3.418426513671875,
        "learning_rate": 0.00018168351223178997,
        "epoch": 1.4651972157772621,
        "step": 11367
    },
    {
        "loss": 1.8954,
        "grad_norm": 1.772760272026062,
        "learning_rate": 0.00018164840007283916,
        "epoch": 1.4653261149780872,
        "step": 11368
    },
    {
        "loss": 1.8277,
        "grad_norm": 1.9516681432724,
        "learning_rate": 0.00018161325769134478,
        "epoch": 1.4654550141789122,
        "step": 11369
    },
    {
        "loss": 0.9563,
        "grad_norm": 2.237215042114258,
        "learning_rate": 0.00018157808510031493,
        "epoch": 1.465583913379737,
        "step": 11370
    },
    {
        "loss": 1.282,
        "grad_norm": 3.1575136184692383,
        "learning_rate": 0.0001815428823127689,
        "epoch": 1.465712812580562,
        "step": 11371
    },
    {
        "loss": 1.7043,
        "grad_norm": 1.6708449125289917,
        "learning_rate": 0.00018150764934173722,
        "epoch": 1.465841711781387,
        "step": 11372
    },
    {
        "loss": 2.3107,
        "grad_norm": 1.8370448350906372,
        "learning_rate": 0.0001814723862002615,
        "epoch": 1.465970610982212,
        "step": 11373
    },
    {
        "loss": 1.6299,
        "grad_norm": 1.5742868185043335,
        "learning_rate": 0.00018143709290139456,
        "epoch": 1.466099510183037,
        "step": 11374
    },
    {
        "loss": 1.4815,
        "grad_norm": 5.02257776260376,
        "learning_rate": 0.00018140176945820031,
        "epoch": 1.4662284093838618,
        "step": 11375
    },
    {
        "loss": 0.7526,
        "grad_norm": 2.41605544090271,
        "learning_rate": 0.00018136641588375408,
        "epoch": 1.4663573085846868,
        "step": 11376
    },
    {
        "loss": 2.121,
        "grad_norm": 1.6015043258666992,
        "learning_rate": 0.00018133103219114207,
        "epoch": 1.4664862077855116,
        "step": 11377
    },
    {
        "loss": 1.1432,
        "grad_norm": 2.151495933532715,
        "learning_rate": 0.0001812956183934616,
        "epoch": 1.4666151069863367,
        "step": 11378
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.286153554916382,
        "learning_rate": 0.00018126017450382142,
        "epoch": 1.4667440061871617,
        "step": 11379
    },
    {
        "loss": 1.0011,
        "grad_norm": 2.474193572998047,
        "learning_rate": 0.00018122470053534117,
        "epoch": 1.4668729053879865,
        "step": 11380
    },
    {
        "loss": 1.552,
        "grad_norm": 3.1888208389282227,
        "learning_rate": 0.00018118919650115172,
        "epoch": 1.4670018045888116,
        "step": 11381
    },
    {
        "loss": 1.2176,
        "grad_norm": 3.682650089263916,
        "learning_rate": 0.0001811536624143951,
        "epoch": 1.4671307037896364,
        "step": 11382
    },
    {
        "loss": 1.2931,
        "grad_norm": 4.457698345184326,
        "learning_rate": 0.00018111809828822448,
        "epoch": 1.4672596029904614,
        "step": 11383
    },
    {
        "loss": 2.231,
        "grad_norm": 2.027414321899414,
        "learning_rate": 0.00018108250413580382,
        "epoch": 1.4673885021912865,
        "step": 11384
    },
    {
        "loss": 1.8018,
        "grad_norm": 3.318471908569336,
        "learning_rate": 0.00018104687997030878,
        "epoch": 1.4675174013921113,
        "step": 11385
    },
    {
        "loss": 1.5587,
        "grad_norm": 3.28358793258667,
        "learning_rate": 0.00018101122580492567,
        "epoch": 1.4676463005929363,
        "step": 11386
    },
    {
        "loss": 1.7551,
        "grad_norm": 3.350522994995117,
        "learning_rate": 0.00018097554165285204,
        "epoch": 1.4677751997937611,
        "step": 11387
    },
    {
        "loss": 1.1765,
        "grad_norm": 3.9371566772460938,
        "learning_rate": 0.00018093982752729658,
        "epoch": 1.4679040989945862,
        "step": 11388
    },
    {
        "loss": 1.6445,
        "grad_norm": 1.7998409271240234,
        "learning_rate": 0.00018090408344147903,
        "epoch": 1.4680329981954112,
        "step": 11389
    },
    {
        "loss": 2.7016,
        "grad_norm": 2.0622177124023438,
        "learning_rate": 0.00018086830940863022,
        "epoch": 1.4681618973962363,
        "step": 11390
    },
    {
        "loss": 1.9873,
        "grad_norm": 2.4141342639923096,
        "learning_rate": 0.00018083250544199213,
        "epoch": 1.468290796597061,
        "step": 11391
    },
    {
        "loss": 1.7667,
        "grad_norm": 2.7506978511810303,
        "learning_rate": 0.00018079667155481764,
        "epoch": 1.4684196957978861,
        "step": 11392
    },
    {
        "loss": 1.994,
        "grad_norm": 1.9329278469085693,
        "learning_rate": 0.00018076080776037099,
        "epoch": 1.468548594998711,
        "step": 11393
    },
    {
        "loss": 2.2528,
        "grad_norm": 2.293748378753662,
        "learning_rate": 0.0001807249140719274,
        "epoch": 1.468677494199536,
        "step": 11394
    },
    {
        "loss": 1.6717,
        "grad_norm": 3.167363405227661,
        "learning_rate": 0.0001806889905027727,
        "epoch": 1.468806393400361,
        "step": 11395
    },
    {
        "loss": 1.919,
        "grad_norm": 1.991060495376587,
        "learning_rate": 0.0001806530370662046,
        "epoch": 1.4689352926011858,
        "step": 11396
    },
    {
        "loss": 1.9147,
        "grad_norm": 1.8973952531814575,
        "learning_rate": 0.00018061705377553125,
        "epoch": 1.4690641918020109,
        "step": 11397
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.156287431716919,
        "learning_rate": 0.00018058104064407195,
        "epoch": 1.4691930910028357,
        "step": 11398
    },
    {
        "loss": 1.0376,
        "grad_norm": 2.788132429122925,
        "learning_rate": 0.00018054499768515735,
        "epoch": 1.4693219902036607,
        "step": 11399
    },
    {
        "loss": 0.9744,
        "grad_norm": 1.40066397190094,
        "learning_rate": 0.00018050892491212883,
        "epoch": 1.4694508894044858,
        "step": 11400
    },
    {
        "loss": 1.9726,
        "grad_norm": 2.0708253383636475,
        "learning_rate": 0.00018047282233833874,
        "epoch": 1.4695797886053106,
        "step": 11401
    },
    {
        "loss": 0.8594,
        "grad_norm": 2.488619327545166,
        "learning_rate": 0.00018043668997715084,
        "epoch": 1.4697086878061356,
        "step": 11402
    },
    {
        "loss": 1.8062,
        "grad_norm": 1.6996692419052124,
        "learning_rate": 0.0001804005278419396,
        "epoch": 1.4698375870069604,
        "step": 11403
    },
    {
        "loss": 2.3514,
        "grad_norm": 2.0323402881622314,
        "learning_rate": 0.00018036433594609066,
        "epoch": 1.4699664862077855,
        "step": 11404
    },
    {
        "loss": 1.8814,
        "grad_norm": 2.1517136096954346,
        "learning_rate": 0.00018032811430300055,
        "epoch": 1.4700953854086105,
        "step": 11405
    },
    {
        "loss": 2.2096,
        "grad_norm": 2.0387308597564697,
        "learning_rate": 0.00018029186292607688,
        "epoch": 1.4702242846094355,
        "step": 11406
    },
    {
        "loss": 1.0155,
        "grad_norm": 2.856523275375366,
        "learning_rate": 0.00018025558182873836,
        "epoch": 1.4703531838102604,
        "step": 11407
    },
    {
        "loss": 0.7161,
        "grad_norm": 5.089748859405518,
        "learning_rate": 0.00018021927102441452,
        "epoch": 1.4704820830110854,
        "step": 11408
    },
    {
        "loss": 1.4995,
        "grad_norm": 2.704812526702881,
        "learning_rate": 0.0001801829305265459,
        "epoch": 1.4706109822119102,
        "step": 11409
    },
    {
        "loss": 1.4952,
        "grad_norm": 2.7476840019226074,
        "learning_rate": 0.00018014656034858435,
        "epoch": 1.4707398814127353,
        "step": 11410
    },
    {
        "loss": 0.8222,
        "grad_norm": 2.611250400543213,
        "learning_rate": 0.0001801101605039923,
        "epoch": 1.4708687806135603,
        "step": 11411
    },
    {
        "loss": 1.7658,
        "grad_norm": 1.6213045120239258,
        "learning_rate": 0.00018007373100624335,
        "epoch": 1.4709976798143851,
        "step": 11412
    },
    {
        "loss": 1.7245,
        "grad_norm": 2.2096848487854004,
        "learning_rate": 0.00018003727186882208,
        "epoch": 1.4711265790152102,
        "step": 11413
    },
    {
        "loss": 1.9447,
        "grad_norm": 2.809318780899048,
        "learning_rate": 0.000180000783105224,
        "epoch": 1.471255478216035,
        "step": 11414
    },
    {
        "loss": 0.7806,
        "grad_norm": 3.486651659011841,
        "learning_rate": 0.00017996426472895549,
        "epoch": 1.47138437741686,
        "step": 11415
    },
    {
        "loss": 2.6095,
        "grad_norm": 2.371967315673828,
        "learning_rate": 0.00017992771675353418,
        "epoch": 1.471513276617685,
        "step": 11416
    },
    {
        "loss": 2.2498,
        "grad_norm": 2.0533835887908936,
        "learning_rate": 0.0001798911391924885,
        "epoch": 1.4716421758185099,
        "step": 11417
    },
    {
        "loss": 2.1898,
        "grad_norm": 1.5807088613510132,
        "learning_rate": 0.00017985453205935753,
        "epoch": 1.471771075019335,
        "step": 11418
    },
    {
        "loss": 2.3488,
        "grad_norm": 1.860845923423767,
        "learning_rate": 0.00017981789536769186,
        "epoch": 1.4718999742201597,
        "step": 11419
    },
    {
        "loss": 1.9523,
        "grad_norm": 2.016967296600342,
        "learning_rate": 0.0001797812291310526,
        "epoch": 1.4720288734209848,
        "step": 11420
    },
    {
        "loss": 1.7511,
        "grad_norm": 2.2868568897247314,
        "learning_rate": 0.00017974453336301195,
        "epoch": 1.4721577726218098,
        "step": 11421
    },
    {
        "loss": 2.0567,
        "grad_norm": 2.1684021949768066,
        "learning_rate": 0.00017970780807715305,
        "epoch": 1.4722866718226346,
        "step": 11422
    },
    {
        "loss": 1.0478,
        "grad_norm": 3.1755592823028564,
        "learning_rate": 0.00017967105328706991,
        "epoch": 1.4724155710234597,
        "step": 11423
    },
    {
        "loss": 2.1688,
        "grad_norm": 1.7260632514953613,
        "learning_rate": 0.00017963426900636752,
        "epoch": 1.4725444702242845,
        "step": 11424
    },
    {
        "loss": 0.9236,
        "grad_norm": 2.4362504482269287,
        "learning_rate": 0.00017959745524866175,
        "epoch": 1.4726733694251095,
        "step": 11425
    },
    {
        "loss": 1.9062,
        "grad_norm": 1.892177700996399,
        "learning_rate": 0.00017956061202757925,
        "epoch": 1.4728022686259346,
        "step": 11426
    },
    {
        "loss": 2.5046,
        "grad_norm": 1.3140132427215576,
        "learning_rate": 0.00017952373935675796,
        "epoch": 1.4729311678267596,
        "step": 11427
    },
    {
        "loss": 1.6804,
        "grad_norm": 1.828601360321045,
        "learning_rate": 0.00017948683724984635,
        "epoch": 1.4730600670275844,
        "step": 11428
    },
    {
        "loss": 1.3667,
        "grad_norm": 2.7837793827056885,
        "learning_rate": 0.00017944990572050387,
        "epoch": 1.4731889662284094,
        "step": 11429
    },
    {
        "loss": 2.5233,
        "grad_norm": 1.683581829071045,
        "learning_rate": 0.000179412944782401,
        "epoch": 1.4733178654292343,
        "step": 11430
    },
    {
        "loss": 1.9197,
        "grad_norm": 1.834660291671753,
        "learning_rate": 0.00017937595444921894,
        "epoch": 1.4734467646300593,
        "step": 11431
    },
    {
        "loss": 1.9616,
        "grad_norm": 2.3062198162078857,
        "learning_rate": 0.00017933893473464973,
        "epoch": 1.4735756638308843,
        "step": 11432
    },
    {
        "loss": 1.8073,
        "grad_norm": 2.3165652751922607,
        "learning_rate": 0.00017930188565239662,
        "epoch": 1.4737045630317092,
        "step": 11433
    },
    {
        "loss": 1.8053,
        "grad_norm": 1.663765549659729,
        "learning_rate": 0.00017926480721617346,
        "epoch": 1.4738334622325342,
        "step": 11434
    },
    {
        "loss": 1.3987,
        "grad_norm": 2.9108364582061768,
        "learning_rate": 0.00017922769943970484,
        "epoch": 1.473962361433359,
        "step": 11435
    },
    {
        "loss": 2.0335,
        "grad_norm": 3.381913423538208,
        "learning_rate": 0.00017919056233672652,
        "epoch": 1.474091260634184,
        "step": 11436
    },
    {
        "loss": 2.0266,
        "grad_norm": 2.7647180557250977,
        "learning_rate": 0.00017915339592098497,
        "epoch": 1.474220159835009,
        "step": 11437
    },
    {
        "loss": 1.0229,
        "grad_norm": 2.8709685802459717,
        "learning_rate": 0.00017911620020623746,
        "epoch": 1.474349059035834,
        "step": 11438
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.8850862979888916,
        "learning_rate": 0.00017907897520625217,
        "epoch": 1.474477958236659,
        "step": 11439
    },
    {
        "loss": 1.9099,
        "grad_norm": 2.220576286315918,
        "learning_rate": 0.0001790417209348082,
        "epoch": 1.4746068574374838,
        "step": 11440
    },
    {
        "loss": 2.1139,
        "grad_norm": 2.600238084793091,
        "learning_rate": 0.0001790044374056953,
        "epoch": 1.4747357566383088,
        "step": 11441
    },
    {
        "loss": 2.0065,
        "grad_norm": 2.083319902420044,
        "learning_rate": 0.00017896712463271413,
        "epoch": 1.4748646558391338,
        "step": 11442
    },
    {
        "loss": 1.8267,
        "grad_norm": 1.6832188367843628,
        "learning_rate": 0.00017892978262967616,
        "epoch": 1.4749935550399589,
        "step": 11443
    },
    {
        "loss": 1.9392,
        "grad_norm": 2.481841802597046,
        "learning_rate": 0.0001788924114104039,
        "epoch": 1.4751224542407837,
        "step": 11444
    },
    {
        "loss": 1.8141,
        "grad_norm": 2.188973903656006,
        "learning_rate": 0.00017885501098873033,
        "epoch": 1.4752513534416087,
        "step": 11445
    },
    {
        "loss": 1.1321,
        "grad_norm": 3.8833324909210205,
        "learning_rate": 0.00017881758137849944,
        "epoch": 1.4753802526424336,
        "step": 11446
    },
    {
        "loss": 1.798,
        "grad_norm": 2.1114747524261475,
        "learning_rate": 0.000178780122593566,
        "epoch": 1.4755091518432586,
        "step": 11447
    },
    {
        "loss": 1.5572,
        "grad_norm": 1.895944595336914,
        "learning_rate": 0.0001787426346477955,
        "epoch": 1.4756380510440836,
        "step": 11448
    },
    {
        "loss": 1.6375,
        "grad_norm": 1.8925368785858154,
        "learning_rate": 0.0001787051175550642,
        "epoch": 1.4757669502449084,
        "step": 11449
    },
    {
        "loss": 2.2781,
        "grad_norm": 2.3603265285491943,
        "learning_rate": 0.00017866757132925943,
        "epoch": 1.4758958494457335,
        "step": 11450
    },
    {
        "loss": 1.8942,
        "grad_norm": 1.9610533714294434,
        "learning_rate": 0.00017862999598427902,
        "epoch": 1.4760247486465583,
        "step": 11451
    },
    {
        "loss": 2.222,
        "grad_norm": 1.90128755569458,
        "learning_rate": 0.00017859239153403164,
        "epoch": 1.4761536478473833,
        "step": 11452
    },
    {
        "loss": 1.7301,
        "grad_norm": 2.276944637298584,
        "learning_rate": 0.0001785547579924368,
        "epoch": 1.4762825470482084,
        "step": 11453
    },
    {
        "loss": 1.8056,
        "grad_norm": 4.0453619956970215,
        "learning_rate": 0.00017851709537342468,
        "epoch": 1.4764114462490332,
        "step": 11454
    },
    {
        "loss": 2.055,
        "grad_norm": 2.194465160369873,
        "learning_rate": 0.0001784794036909363,
        "epoch": 1.4765403454498582,
        "step": 11455
    },
    {
        "loss": 1.7517,
        "grad_norm": 2.4410083293914795,
        "learning_rate": 0.00017844168295892346,
        "epoch": 1.476669244650683,
        "step": 11456
    },
    {
        "loss": 1.7699,
        "grad_norm": 2.7695484161376953,
        "learning_rate": 0.00017840393319134857,
        "epoch": 1.476798143851508,
        "step": 11457
    },
    {
        "loss": 0.4498,
        "grad_norm": 2.867600917816162,
        "learning_rate": 0.00017836615440218497,
        "epoch": 1.4769270430523331,
        "step": 11458
    },
    {
        "loss": 1.665,
        "grad_norm": 2.5801753997802734,
        "learning_rate": 0.0001783283466054166,
        "epoch": 1.477055942253158,
        "step": 11459
    },
    {
        "loss": 1.2009,
        "grad_norm": 2.687861919403076,
        "learning_rate": 0.00017829050981503817,
        "epoch": 1.477184841453983,
        "step": 11460
    },
    {
        "loss": 1.8658,
        "grad_norm": 2.538485050201416,
        "learning_rate": 0.0001782526440450553,
        "epoch": 1.4773137406548078,
        "step": 11461
    },
    {
        "loss": 2.2468,
        "grad_norm": 2.4406137466430664,
        "learning_rate": 0.00017821474930948403,
        "epoch": 1.4774426398556328,
        "step": 11462
    },
    {
        "loss": 1.7106,
        "grad_norm": 3.0467960834503174,
        "learning_rate": 0.00017817682562235138,
        "epoch": 1.4775715390564579,
        "step": 11463
    },
    {
        "loss": 1.3772,
        "grad_norm": 3.0058083534240723,
        "learning_rate": 0.00017813887299769493,
        "epoch": 1.477700438257283,
        "step": 11464
    },
    {
        "loss": 2.0593,
        "grad_norm": 1.4648094177246094,
        "learning_rate": 0.00017810089144956303,
        "epoch": 1.4778293374581077,
        "step": 11465
    },
    {
        "loss": 1.674,
        "grad_norm": 2.767263174057007,
        "learning_rate": 0.00017806288099201468,
        "epoch": 1.4779582366589328,
        "step": 11466
    },
    {
        "loss": 1.0086,
        "grad_norm": 3.17358136177063,
        "learning_rate": 0.00017802484163911971,
        "epoch": 1.4780871358597576,
        "step": 11467
    },
    {
        "loss": 1.301,
        "grad_norm": 3.1857409477233887,
        "learning_rate": 0.0001779867734049586,
        "epoch": 1.4782160350605826,
        "step": 11468
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.7813150882720947,
        "learning_rate": 0.0001779486763036224,
        "epoch": 1.4783449342614077,
        "step": 11469
    },
    {
        "loss": 2.3428,
        "grad_norm": 2.372058153152466,
        "learning_rate": 0.00017791055034921296,
        "epoch": 1.4784738334622325,
        "step": 11470
    },
    {
        "loss": 1.8215,
        "grad_norm": 3.066746473312378,
        "learning_rate": 0.00017787239555584278,
        "epoch": 1.4786027326630575,
        "step": 11471
    },
    {
        "loss": 1.3026,
        "grad_norm": 2.0352394580841064,
        "learning_rate": 0.00017783421193763506,
        "epoch": 1.4787316318638823,
        "step": 11472
    },
    {
        "loss": 2.2966,
        "grad_norm": 1.9779610633850098,
        "learning_rate": 0.00017779599950872363,
        "epoch": 1.4788605310647074,
        "step": 11473
    },
    {
        "loss": 2.187,
        "grad_norm": 2.3650333881378174,
        "learning_rate": 0.00017775775828325293,
        "epoch": 1.4789894302655324,
        "step": 11474
    },
    {
        "loss": 1.1577,
        "grad_norm": 3.010220527648926,
        "learning_rate": 0.00017771948827537837,
        "epoch": 1.4791183294663572,
        "step": 11475
    },
    {
        "loss": 0.9206,
        "grad_norm": 2.8954594135284424,
        "learning_rate": 0.00017768118949926554,
        "epoch": 1.4792472286671823,
        "step": 11476
    },
    {
        "loss": 0.9316,
        "grad_norm": 2.598444938659668,
        "learning_rate": 0.00017764286196909085,
        "epoch": 1.479376127868007,
        "step": 11477
    },
    {
        "loss": 1.5574,
        "grad_norm": 3.4172439575195312,
        "learning_rate": 0.0001776045056990417,
        "epoch": 1.4795050270688321,
        "step": 11478
    },
    {
        "loss": 1.9682,
        "grad_norm": 2.4111316204071045,
        "learning_rate": 0.00017756612070331566,
        "epoch": 1.4796339262696572,
        "step": 11479
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.801509141921997,
        "learning_rate": 0.00017752770699612118,
        "epoch": 1.4797628254704822,
        "step": 11480
    },
    {
        "loss": 2.0084,
        "grad_norm": 2.0460121631622314,
        "learning_rate": 0.00017748926459167726,
        "epoch": 1.479891724671307,
        "step": 11481
    },
    {
        "loss": 2.3342,
        "grad_norm": 1.5503265857696533,
        "learning_rate": 0.0001774507935042135,
        "epoch": 1.480020623872132,
        "step": 11482
    },
    {
        "loss": 0.9258,
        "grad_norm": 3.763627290725708,
        "learning_rate": 0.00017741229374797012,
        "epoch": 1.4801495230729569,
        "step": 11483
    },
    {
        "loss": 2.3367,
        "grad_norm": 2.6449599266052246,
        "learning_rate": 0.00017737376533719817,
        "epoch": 1.480278422273782,
        "step": 11484
    },
    {
        "loss": 1.9502,
        "grad_norm": 1.8248103857040405,
        "learning_rate": 0.00017733520828615897,
        "epoch": 1.480407321474607,
        "step": 11485
    },
    {
        "loss": 2.3251,
        "grad_norm": 1.5133159160614014,
        "learning_rate": 0.00017729662260912465,
        "epoch": 1.4805362206754318,
        "step": 11486
    },
    {
        "loss": 1.619,
        "grad_norm": 2.414595365524292,
        "learning_rate": 0.00017725800832037788,
        "epoch": 1.4806651198762568,
        "step": 11487
    },
    {
        "loss": 1.6968,
        "grad_norm": 2.691520929336548,
        "learning_rate": 0.00017721936543421191,
        "epoch": 1.4807940190770816,
        "step": 11488
    },
    {
        "loss": 1.1798,
        "grad_norm": 2.4170167446136475,
        "learning_rate": 0.00017718069396493059,
        "epoch": 1.4809229182779067,
        "step": 11489
    },
    {
        "loss": 1.2351,
        "grad_norm": 2.6395485401153564,
        "learning_rate": 0.00017714199392684834,
        "epoch": 1.4810518174787317,
        "step": 11490
    },
    {
        "loss": 1.5394,
        "grad_norm": 2.1887457370758057,
        "learning_rate": 0.0001771032653342901,
        "epoch": 1.4811807166795565,
        "step": 11491
    },
    {
        "loss": 2.1259,
        "grad_norm": 1.4665071964263916,
        "learning_rate": 0.00017706450820159167,
        "epoch": 1.4813096158803816,
        "step": 11492
    },
    {
        "loss": 1.8425,
        "grad_norm": 1.978186845779419,
        "learning_rate": 0.00017702572254309895,
        "epoch": 1.4814385150812064,
        "step": 11493
    },
    {
        "loss": 1.9311,
        "grad_norm": 1.778607964515686,
        "learning_rate": 0.00017698690837316864,
        "epoch": 1.4815674142820314,
        "step": 11494
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.9967058897018433,
        "learning_rate": 0.00017694806570616814,
        "epoch": 1.4816963134828565,
        "step": 11495
    },
    {
        "loss": 0.9819,
        "grad_norm": 4.932183742523193,
        "learning_rate": 0.0001769091945564752,
        "epoch": 1.4818252126836813,
        "step": 11496
    },
    {
        "loss": 0.9654,
        "grad_norm": 3.9104716777801514,
        "learning_rate": 0.0001768702949384781,
        "epoch": 1.4819541118845063,
        "step": 11497
    },
    {
        "loss": 1.4218,
        "grad_norm": 4.537994384765625,
        "learning_rate": 0.0001768313668665758,
        "epoch": 1.4820830110853311,
        "step": 11498
    },
    {
        "loss": 2.1653,
        "grad_norm": 2.1583588123321533,
        "learning_rate": 0.00017679241035517762,
        "epoch": 1.4822119102861562,
        "step": 11499
    },
    {
        "loss": 1.9006,
        "grad_norm": 2.7539074420928955,
        "learning_rate": 0.00017675342541870345,
        "epoch": 1.4823408094869812,
        "step": 11500
    },
    {
        "loss": 1.4103,
        "grad_norm": 2.998716354370117,
        "learning_rate": 0.00017671441207158396,
        "epoch": 1.4824697086878063,
        "step": 11501
    },
    {
        "loss": 1.7382,
        "grad_norm": 2.411093235015869,
        "learning_rate": 0.00017667537032826,
        "epoch": 1.482598607888631,
        "step": 11502
    },
    {
        "loss": 1.4675,
        "grad_norm": 2.828824043273926,
        "learning_rate": 0.00017663630020318307,
        "epoch": 1.482727507089456,
        "step": 11503
    },
    {
        "loss": 1.8056,
        "grad_norm": 2.611865282058716,
        "learning_rate": 0.00017659720171081514,
        "epoch": 1.482856406290281,
        "step": 11504
    },
    {
        "loss": 1.9721,
        "grad_norm": 1.236245036125183,
        "learning_rate": 0.00017655807486562875,
        "epoch": 1.482985305491106,
        "step": 11505
    },
    {
        "loss": 1.8204,
        "grad_norm": 2.140552043914795,
        "learning_rate": 0.00017651891968210683,
        "epoch": 1.483114204691931,
        "step": 11506
    },
    {
        "loss": 1.997,
        "grad_norm": 2.2718753814697266,
        "learning_rate": 0.0001764797361747429,
        "epoch": 1.4832431038927558,
        "step": 11507
    },
    {
        "loss": 1.7524,
        "grad_norm": 1.4870421886444092,
        "learning_rate": 0.00017644052435804083,
        "epoch": 1.4833720030935809,
        "step": 11508
    },
    {
        "loss": 0.7107,
        "grad_norm": 3.441727638244629,
        "learning_rate": 0.00017640128424651532,
        "epoch": 1.4835009022944057,
        "step": 11509
    },
    {
        "loss": 1.3709,
        "grad_norm": 2.45505952835083,
        "learning_rate": 0.000176362015854691,
        "epoch": 1.4836298014952307,
        "step": 11510
    },
    {
        "loss": 2.3128,
        "grad_norm": 1.7688218355178833,
        "learning_rate": 0.00017632271919710326,
        "epoch": 1.4837587006960558,
        "step": 11511
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.596510171890259,
        "learning_rate": 0.00017628339428829812,
        "epoch": 1.4838875998968806,
        "step": 11512
    },
    {
        "loss": 1.8135,
        "grad_norm": 1.8094919919967651,
        "learning_rate": 0.0001762440411428318,
        "epoch": 1.4840164990977056,
        "step": 11513
    },
    {
        "loss": 0.9217,
        "grad_norm": 2.5942981243133545,
        "learning_rate": 0.000176204659775271,
        "epoch": 1.4841453982985304,
        "step": 11514
    },
    {
        "loss": 1.198,
        "grad_norm": 3.1395814418792725,
        "learning_rate": 0.00017616525020019316,
        "epoch": 1.4842742974993555,
        "step": 11515
    },
    {
        "loss": 2.005,
        "grad_norm": 2.307874917984009,
        "learning_rate": 0.0001761258124321856,
        "epoch": 1.4844031967001805,
        "step": 11516
    },
    {
        "loss": 2.3193,
        "grad_norm": 1.7577953338623047,
        "learning_rate": 0.00017608634648584646,
        "epoch": 1.4845320959010055,
        "step": 11517
    },
    {
        "loss": 1.9245,
        "grad_norm": 2.4853217601776123,
        "learning_rate": 0.00017604685237578442,
        "epoch": 1.4846609951018304,
        "step": 11518
    },
    {
        "loss": 1.2973,
        "grad_norm": 1.6777604818344116,
        "learning_rate": 0.00017600733011661828,
        "epoch": 1.4847898943026554,
        "step": 11519
    },
    {
        "loss": 0.9929,
        "grad_norm": 4.245969295501709,
        "learning_rate": 0.0001759677797229775,
        "epoch": 1.4849187935034802,
        "step": 11520
    },
    {
        "loss": 2.2282,
        "grad_norm": 2.0953991413116455,
        "learning_rate": 0.00017592820120950172,
        "epoch": 1.4850476927043053,
        "step": 11521
    },
    {
        "loss": 0.6431,
        "grad_norm": 1.7289496660232544,
        "learning_rate": 0.00017588859459084118,
        "epoch": 1.4851765919051303,
        "step": 11522
    },
    {
        "loss": 2.3177,
        "grad_norm": 1.326492428779602,
        "learning_rate": 0.00017584895988165643,
        "epoch": 1.4853054911059551,
        "step": 11523
    },
    {
        "loss": 1.7895,
        "grad_norm": 3.0623350143432617,
        "learning_rate": 0.0001758092970966185,
        "epoch": 1.4854343903067802,
        "step": 11524
    },
    {
        "loss": 2.2312,
        "grad_norm": 1.6935780048370361,
        "learning_rate": 0.00017576960625040862,
        "epoch": 1.485563289507605,
        "step": 11525
    },
    {
        "loss": 1.5268,
        "grad_norm": 2.123420238494873,
        "learning_rate": 0.0001757298873577188,
        "epoch": 1.48569218870843,
        "step": 11526
    },
    {
        "loss": 0.9771,
        "grad_norm": 2.602726697921753,
        "learning_rate": 0.00017569014043325096,
        "epoch": 1.485821087909255,
        "step": 11527
    },
    {
        "loss": 2.1753,
        "grad_norm": 1.887874960899353,
        "learning_rate": 0.00017565036549171755,
        "epoch": 1.4859499871100799,
        "step": 11528
    },
    {
        "loss": 2.2694,
        "grad_norm": 1.4808967113494873,
        "learning_rate": 0.00017561056254784173,
        "epoch": 1.486078886310905,
        "step": 11529
    },
    {
        "loss": 1.1942,
        "grad_norm": 3.3434839248657227,
        "learning_rate": 0.00017557073161635658,
        "epoch": 1.4862077855117297,
        "step": 11530
    },
    {
        "loss": 2.1552,
        "grad_norm": 1.764204978942871,
        "learning_rate": 0.0001755308727120056,
        "epoch": 1.4863366847125548,
        "step": 11531
    },
    {
        "loss": 2.4617,
        "grad_norm": 2.144937753677368,
        "learning_rate": 0.00017549098584954307,
        "epoch": 1.4864655839133798,
        "step": 11532
    },
    {
        "loss": 1.0673,
        "grad_norm": 2.900853157043457,
        "learning_rate": 0.00017545107104373307,
        "epoch": 1.4865944831142046,
        "step": 11533
    },
    {
        "loss": 2.5513,
        "grad_norm": 1.757536768913269,
        "learning_rate": 0.00017541112830935017,
        "epoch": 1.4867233823150297,
        "step": 11534
    },
    {
        "loss": 2.4419,
        "grad_norm": 2.2202744483947754,
        "learning_rate": 0.0001753711576611796,
        "epoch": 1.4868522815158545,
        "step": 11535
    },
    {
        "loss": 2.2986,
        "grad_norm": 2.081997871398926,
        "learning_rate": 0.00017533115911401656,
        "epoch": 1.4869811807166795,
        "step": 11536
    },
    {
        "loss": 1.7167,
        "grad_norm": 2.4107542037963867,
        "learning_rate": 0.00017529113268266672,
        "epoch": 1.4871100799175045,
        "step": 11537
    },
    {
        "loss": 1.5053,
        "grad_norm": 2.7632386684417725,
        "learning_rate": 0.00017525107838194605,
        "epoch": 1.4872389791183296,
        "step": 11538
    },
    {
        "loss": 2.044,
        "grad_norm": 2.204709053039551,
        "learning_rate": 0.0001752109962266809,
        "epoch": 1.4873678783191544,
        "step": 11539
    },
    {
        "loss": 1.8629,
        "grad_norm": 1.4445948600769043,
        "learning_rate": 0.00017517088623170775,
        "epoch": 1.4874967775199794,
        "step": 11540
    },
    {
        "loss": 0.6687,
        "grad_norm": 1.9029213190078735,
        "learning_rate": 0.00017513074841187356,
        "epoch": 1.4876256767208043,
        "step": 11541
    },
    {
        "loss": 2.17,
        "grad_norm": 2.313755750656128,
        "learning_rate": 0.0001750905827820355,
        "epoch": 1.4877545759216293,
        "step": 11542
    },
    {
        "loss": 1.6344,
        "grad_norm": 2.0805931091308594,
        "learning_rate": 0.00017505038935706127,
        "epoch": 1.4878834751224543,
        "step": 11543
    },
    {
        "loss": 1.2606,
        "grad_norm": 3.550687074661255,
        "learning_rate": 0.0001750101681518284,
        "epoch": 1.4880123743232792,
        "step": 11544
    },
    {
        "loss": 1.6244,
        "grad_norm": 2.63723087310791,
        "learning_rate": 0.00017496991918122497,
        "epoch": 1.4881412735241042,
        "step": 11545
    },
    {
        "loss": 1.053,
        "grad_norm": 2.982912302017212,
        "learning_rate": 0.00017492964246014953,
        "epoch": 1.488270172724929,
        "step": 11546
    },
    {
        "loss": 1.25,
        "grad_norm": 2.612139940261841,
        "learning_rate": 0.00017488933800351058,
        "epoch": 1.488399071925754,
        "step": 11547
    },
    {
        "loss": 1.8723,
        "grad_norm": 2.3411917686462402,
        "learning_rate": 0.00017484900582622693,
        "epoch": 1.488527971126579,
        "step": 11548
    },
    {
        "loss": 1.9014,
        "grad_norm": 2.742771625518799,
        "learning_rate": 0.00017480864594322798,
        "epoch": 1.488656870327404,
        "step": 11549
    },
    {
        "loss": 1.5351,
        "grad_norm": 2.9569313526153564,
        "learning_rate": 0.0001747682583694529,
        "epoch": 1.488785769528229,
        "step": 11550
    },
    {
        "loss": 1.5557,
        "grad_norm": 2.657729148864746,
        "learning_rate": 0.00017472784311985126,
        "epoch": 1.4889146687290538,
        "step": 11551
    },
    {
        "loss": 1.6061,
        "grad_norm": 3.465579032897949,
        "learning_rate": 0.00017468740020938324,
        "epoch": 1.4890435679298788,
        "step": 11552
    },
    {
        "loss": 1.2475,
        "grad_norm": 3.157338857650757,
        "learning_rate": 0.00017464692965301883,
        "epoch": 1.4891724671307038,
        "step": 11553
    },
    {
        "loss": 1.831,
        "grad_norm": 2.6218655109405518,
        "learning_rate": 0.00017460643146573844,
        "epoch": 1.4893013663315289,
        "step": 11554
    },
    {
        "loss": 1.9283,
        "grad_norm": 4.412965774536133,
        "learning_rate": 0.00017456590566253258,
        "epoch": 1.4894302655323537,
        "step": 11555
    },
    {
        "loss": 1.4908,
        "grad_norm": 2.5771067142486572,
        "learning_rate": 0.00017452535225840214,
        "epoch": 1.4895591647331787,
        "step": 11556
    },
    {
        "loss": 0.9321,
        "grad_norm": 3.376814365386963,
        "learning_rate": 0.00017448477126835816,
        "epoch": 1.4896880639340035,
        "step": 11557
    },
    {
        "loss": 1.9443,
        "grad_norm": 2.8791370391845703,
        "learning_rate": 0.00017444416270742186,
        "epoch": 1.4898169631348286,
        "step": 11558
    },
    {
        "loss": 1.7732,
        "grad_norm": 2.1394731998443604,
        "learning_rate": 0.00017440352659062457,
        "epoch": 1.4899458623356536,
        "step": 11559
    },
    {
        "loss": 1.8007,
        "grad_norm": 2.623716354370117,
        "learning_rate": 0.0001743628629330083,
        "epoch": 1.4900747615364784,
        "step": 11560
    },
    {
        "loss": 1.7699,
        "grad_norm": 2.2481257915496826,
        "learning_rate": 0.00017432217174962453,
        "epoch": 1.4902036607373035,
        "step": 11561
    },
    {
        "loss": 0.5211,
        "grad_norm": 3.1342623233795166,
        "learning_rate": 0.00017428145305553537,
        "epoch": 1.4903325599381283,
        "step": 11562
    },
    {
        "loss": 1.5468,
        "grad_norm": 3.185492753982544,
        "learning_rate": 0.00017424070686581318,
        "epoch": 1.4904614591389533,
        "step": 11563
    },
    {
        "loss": 1.907,
        "grad_norm": 2.1047518253326416,
        "learning_rate": 0.00017419993319554026,
        "epoch": 1.4905903583397784,
        "step": 11564
    },
    {
        "loss": 2.2792,
        "grad_norm": 1.8582106828689575,
        "learning_rate": 0.00017415913205980914,
        "epoch": 1.4907192575406032,
        "step": 11565
    },
    {
        "loss": 1.8271,
        "grad_norm": 2.2057547569274902,
        "learning_rate": 0.00017411830347372278,
        "epoch": 1.4908481567414282,
        "step": 11566
    },
    {
        "loss": 2.0911,
        "grad_norm": 2.5087387561798096,
        "learning_rate": 0.00017407744745239382,
        "epoch": 1.490977055942253,
        "step": 11567
    },
    {
        "loss": 1.833,
        "grad_norm": 2.340010643005371,
        "learning_rate": 0.00017403656401094523,
        "epoch": 1.491105955143078,
        "step": 11568
    },
    {
        "loss": 1.8235,
        "grad_norm": 2.1242716312408447,
        "learning_rate": 0.00017399565316451055,
        "epoch": 1.4912348543439031,
        "step": 11569
    },
    {
        "loss": 2.3278,
        "grad_norm": 1.726171851158142,
        "learning_rate": 0.00017395471492823292,
        "epoch": 1.491363753544728,
        "step": 11570
    },
    {
        "loss": 1.1303,
        "grad_norm": 2.782193422317505,
        "learning_rate": 0.00017391374931726583,
        "epoch": 1.491492652745553,
        "step": 11571
    },
    {
        "loss": 1.4238,
        "grad_norm": 2.3367719650268555,
        "learning_rate": 0.00017387275634677294,
        "epoch": 1.4916215519463778,
        "step": 11572
    },
    {
        "loss": 1.5427,
        "grad_norm": 2.609757423400879,
        "learning_rate": 0.00017383173603192795,
        "epoch": 1.4917504511472028,
        "step": 11573
    },
    {
        "loss": 0.7718,
        "grad_norm": 3.2047011852264404,
        "learning_rate": 0.00017379068838791473,
        "epoch": 1.4918793503480279,
        "step": 11574
    },
    {
        "loss": 1.5069,
        "grad_norm": 2.262718915939331,
        "learning_rate": 0.00017374961342992733,
        "epoch": 1.492008249548853,
        "step": 11575
    },
    {
        "loss": 1.61,
        "grad_norm": 2.0032262802124023,
        "learning_rate": 0.00017370851117316964,
        "epoch": 1.4921371487496777,
        "step": 11576
    },
    {
        "loss": 1.025,
        "grad_norm": 3.2701942920684814,
        "learning_rate": 0.00017366738163285614,
        "epoch": 1.4922660479505028,
        "step": 11577
    },
    {
        "loss": 1.2296,
        "grad_norm": 3.6974103450775146,
        "learning_rate": 0.00017362622482421105,
        "epoch": 1.4923949471513276,
        "step": 11578
    },
    {
        "loss": 1.257,
        "grad_norm": 3.1393673419952393,
        "learning_rate": 0.00017358504076246854,
        "epoch": 1.4925238463521526,
        "step": 11579
    },
    {
        "loss": 2.2539,
        "grad_norm": 2.7516281604766846,
        "learning_rate": 0.0001735438294628733,
        "epoch": 1.4926527455529777,
        "step": 11580
    },
    {
        "loss": 1.5161,
        "grad_norm": 2.3956799507141113,
        "learning_rate": 0.00017350259094067982,
        "epoch": 1.4927816447538025,
        "step": 11581
    },
    {
        "loss": 1.3192,
        "grad_norm": 2.4535937309265137,
        "learning_rate": 0.00017346132521115268,
        "epoch": 1.4929105439546275,
        "step": 11582
    },
    {
        "loss": 2.0385,
        "grad_norm": 1.5056216716766357,
        "learning_rate": 0.00017342003228956683,
        "epoch": 1.4930394431554523,
        "step": 11583
    },
    {
        "loss": 1.088,
        "grad_norm": 4.803350925445557,
        "learning_rate": 0.00017337871219120677,
        "epoch": 1.4931683423562774,
        "step": 11584
    },
    {
        "loss": 2.3304,
        "grad_norm": 3.0497567653656006,
        "learning_rate": 0.00017333736493136732,
        "epoch": 1.4932972415571024,
        "step": 11585
    },
    {
        "loss": 1.9868,
        "grad_norm": 2.1431221961975098,
        "learning_rate": 0.00017329599052535357,
        "epoch": 1.4934261407579272,
        "step": 11586
    },
    {
        "loss": 1.821,
        "grad_norm": 3.8218204975128174,
        "learning_rate": 0.00017325458898848035,
        "epoch": 1.4935550399587523,
        "step": 11587
    },
    {
        "loss": 1.9987,
        "grad_norm": 2.0199899673461914,
        "learning_rate": 0.00017321316033607262,
        "epoch": 1.493683939159577,
        "step": 11588
    },
    {
        "loss": 1.4034,
        "grad_norm": 2.7706751823425293,
        "learning_rate": 0.00017317170458346545,
        "epoch": 1.4938128383604021,
        "step": 11589
    },
    {
        "loss": 1.8961,
        "grad_norm": 1.765871524810791,
        "learning_rate": 0.00017313022174600382,
        "epoch": 1.4939417375612272,
        "step": 11590
    },
    {
        "loss": 1.5989,
        "grad_norm": 2.203509569168091,
        "learning_rate": 0.00017308871183904282,
        "epoch": 1.4940706367620522,
        "step": 11591
    },
    {
        "loss": 1.5253,
        "grad_norm": 4.4272918701171875,
        "learning_rate": 0.00017304717487794756,
        "epoch": 1.494199535962877,
        "step": 11592
    },
    {
        "loss": 0.8366,
        "grad_norm": 3.4636592864990234,
        "learning_rate": 0.00017300561087809298,
        "epoch": 1.494328435163702,
        "step": 11593
    },
    {
        "loss": 1.3965,
        "grad_norm": 3.570253372192383,
        "learning_rate": 0.00017296401985486448,
        "epoch": 1.4944573343645269,
        "step": 11594
    },
    {
        "loss": 2.0508,
        "grad_norm": 3.053891897201538,
        "learning_rate": 0.00017292240182365713,
        "epoch": 1.494586233565352,
        "step": 11595
    },
    {
        "loss": 2.2971,
        "grad_norm": 2.3365111351013184,
        "learning_rate": 0.0001728807567998757,
        "epoch": 1.494715132766177,
        "step": 11596
    },
    {
        "loss": 1.7315,
        "grad_norm": 2.890618324279785,
        "learning_rate": 0.0001728390847989357,
        "epoch": 1.4948440319670018,
        "step": 11597
    },
    {
        "loss": 1.9344,
        "grad_norm": 1.8354341983795166,
        "learning_rate": 0.00017279738583626206,
        "epoch": 1.4949729311678268,
        "step": 11598
    },
    {
        "loss": 1.5917,
        "grad_norm": 1.982662558555603,
        "learning_rate": 0.00017275565992728973,
        "epoch": 1.4951018303686516,
        "step": 11599
    },
    {
        "loss": 1.5191,
        "grad_norm": 2.294701337814331,
        "learning_rate": 0.00017271390708746396,
        "epoch": 1.4952307295694767,
        "step": 11600
    },
    {
        "loss": 1.3016,
        "grad_norm": 1.9658337831497192,
        "learning_rate": 0.00017267212733223977,
        "epoch": 1.4953596287703017,
        "step": 11601
    },
    {
        "loss": 1.7281,
        "grad_norm": 1.9928592443466187,
        "learning_rate": 0.00017263032067708185,
        "epoch": 1.4954885279711265,
        "step": 11602
    },
    {
        "loss": 1.8675,
        "grad_norm": 1.513599157333374,
        "learning_rate": 0.00017258848713746542,
        "epoch": 1.4956174271719516,
        "step": 11603
    },
    {
        "loss": 1.3805,
        "grad_norm": 3.4036166667938232,
        "learning_rate": 0.00017254662672887527,
        "epoch": 1.4957463263727764,
        "step": 11604
    },
    {
        "loss": 1.0782,
        "grad_norm": 4.103806018829346,
        "learning_rate": 0.00017250473946680623,
        "epoch": 1.4958752255736014,
        "step": 11605
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.6380085945129395,
        "learning_rate": 0.00017246282536676307,
        "epoch": 1.4960041247744265,
        "step": 11606
    },
    {
        "loss": 1.4032,
        "grad_norm": 1.8899496793746948,
        "learning_rate": 0.00017242088444426043,
        "epoch": 1.4961330239752513,
        "step": 11607
    },
    {
        "loss": 0.897,
        "grad_norm": 2.2905662059783936,
        "learning_rate": 0.00017237891671482305,
        "epoch": 1.4962619231760763,
        "step": 11608
    },
    {
        "loss": 1.4966,
        "grad_norm": 2.0716755390167236,
        "learning_rate": 0.00017233692219398544,
        "epoch": 1.4963908223769011,
        "step": 11609
    },
    {
        "loss": 1.5319,
        "grad_norm": 1.553043007850647,
        "learning_rate": 0.00017229490089729193,
        "epoch": 1.4965197215777262,
        "step": 11610
    },
    {
        "loss": 2.0368,
        "grad_norm": 3.359290361404419,
        "learning_rate": 0.00017225285284029718,
        "epoch": 1.4966486207785512,
        "step": 11611
    },
    {
        "loss": 2.156,
        "grad_norm": 2.1502368450164795,
        "learning_rate": 0.00017221077803856538,
        "epoch": 1.4967775199793762,
        "step": 11612
    },
    {
        "loss": 1.9703,
        "grad_norm": 1.7739359140396118,
        "learning_rate": 0.00017216867650767052,
        "epoch": 1.496906419180201,
        "step": 11613
    },
    {
        "loss": 1.1763,
        "grad_norm": 3.53588604927063,
        "learning_rate": 0.00017212654826319695,
        "epoch": 1.497035318381026,
        "step": 11614
    },
    {
        "loss": 1.5586,
        "grad_norm": 2.562344551086426,
        "learning_rate": 0.00017208439332073847,
        "epoch": 1.497164217581851,
        "step": 11615
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.4378530979156494,
        "learning_rate": 0.00017204221169589895,
        "epoch": 1.497293116782676,
        "step": 11616
    },
    {
        "loss": 1.6732,
        "grad_norm": 2.3595778942108154,
        "learning_rate": 0.0001720000034042922,
        "epoch": 1.497422015983501,
        "step": 11617
    },
    {
        "loss": 1.7617,
        "grad_norm": 3.0415549278259277,
        "learning_rate": 0.00017195776846154188,
        "epoch": 1.4975509151843258,
        "step": 11618
    },
    {
        "loss": 2.3252,
        "grad_norm": 3.6860265731811523,
        "learning_rate": 0.00017191550688328117,
        "epoch": 1.4976798143851509,
        "step": 11619
    },
    {
        "loss": 1.2996,
        "grad_norm": 2.877432346343994,
        "learning_rate": 0.0001718732186851537,
        "epoch": 1.4978087135859757,
        "step": 11620
    },
    {
        "loss": 1.4858,
        "grad_norm": 2.3968441486358643,
        "learning_rate": 0.0001718309038828125,
        "epoch": 1.4979376127868007,
        "step": 11621
    },
    {
        "loss": 1.5316,
        "grad_norm": 1.5392646789550781,
        "learning_rate": 0.0001717885624919206,
        "epoch": 1.4980665119876257,
        "step": 11622
    },
    {
        "loss": 2.1736,
        "grad_norm": 1.2398617267608643,
        "learning_rate": 0.00017174619452815093,
        "epoch": 1.4981954111884506,
        "step": 11623
    },
    {
        "loss": 1.7834,
        "grad_norm": 3.0259063243865967,
        "learning_rate": 0.00017170380000718614,
        "epoch": 1.4983243103892756,
        "step": 11624
    },
    {
        "loss": 1.7508,
        "grad_norm": 2.7051143646240234,
        "learning_rate": 0.00017166137894471874,
        "epoch": 1.4984532095901004,
        "step": 11625
    },
    {
        "loss": 1.7933,
        "grad_norm": 1.8006130456924438,
        "learning_rate": 0.00017161893135645117,
        "epoch": 1.4985821087909255,
        "step": 11626
    },
    {
        "loss": 1.3995,
        "grad_norm": 2.8630664348602295,
        "learning_rate": 0.0001715764572580954,
        "epoch": 1.4987110079917505,
        "step": 11627
    },
    {
        "loss": 1.5596,
        "grad_norm": 2.276637077331543,
        "learning_rate": 0.00017153395666537374,
        "epoch": 1.4988399071925755,
        "step": 11628
    },
    {
        "loss": 1.6289,
        "grad_norm": 1.8728078603744507,
        "learning_rate": 0.0001714914295940179,
        "epoch": 1.4989688063934004,
        "step": 11629
    },
    {
        "loss": 1.6914,
        "grad_norm": 1.884017825126648,
        "learning_rate": 0.00017144887605976918,
        "epoch": 1.4990977055942254,
        "step": 11630
    },
    {
        "loss": 2.2816,
        "grad_norm": 2.0666182041168213,
        "learning_rate": 0.00017140629607837932,
        "epoch": 1.4992266047950502,
        "step": 11631
    },
    {
        "loss": 1.7174,
        "grad_norm": 3.7518599033355713,
        "learning_rate": 0.00017136368966560935,
        "epoch": 1.4993555039958752,
        "step": 11632
    },
    {
        "loss": 1.2963,
        "grad_norm": 3.4355838298797607,
        "learning_rate": 0.00017132105683723018,
        "epoch": 1.4994844031967003,
        "step": 11633
    },
    {
        "loss": 2.2931,
        "grad_norm": 1.4400653839111328,
        "learning_rate": 0.00017127839760902277,
        "epoch": 1.499613302397525,
        "step": 11634
    },
    {
        "loss": 2.144,
        "grad_norm": 2.0715231895446777,
        "learning_rate": 0.00017123571199677756,
        "epoch": 1.4997422015983501,
        "step": 11635
    },
    {
        "loss": 1.5748,
        "grad_norm": 2.6911866664886475,
        "learning_rate": 0.0001711930000162946,
        "epoch": 1.499871100799175,
        "step": 11636
    },
    {
        "loss": 2.1026,
        "grad_norm": 1.4899941682815552,
        "learning_rate": 0.00017115026168338418,
        "epoch": 1.5,
        "step": 11637
    },
    {
        "loss": 1.1868,
        "grad_norm": 3.13037371635437,
        "learning_rate": 0.00017110749701386606,
        "epoch": 1.500128899200825,
        "step": 11638
    },
    {
        "loss": 0.8646,
        "grad_norm": 3.3507802486419678,
        "learning_rate": 0.00017106470602356973,
        "epoch": 1.5002577984016499,
        "step": 11639
    },
    {
        "loss": 1.5816,
        "grad_norm": 1.5978364944458008,
        "learning_rate": 0.0001710218887283345,
        "epoch": 1.500386697602475,
        "step": 11640
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.7437140941619873,
        "learning_rate": 0.0001709790451440094,
        "epoch": 1.5005155968032997,
        "step": 11641
    },
    {
        "loss": 1.8197,
        "grad_norm": 3.292646884918213,
        "learning_rate": 0.00017093617528645322,
        "epoch": 1.5006444960041248,
        "step": 11642
    },
    {
        "loss": 1.6496,
        "grad_norm": 3.8923096656799316,
        "learning_rate": 0.00017089327917153434,
        "epoch": 1.5007733952049498,
        "step": 11643
    },
    {
        "loss": 2.5653,
        "grad_norm": 2.724904775619507,
        "learning_rate": 0.00017085035681513097,
        "epoch": 1.5009022944057748,
        "step": 11644
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.3631906509399414,
        "learning_rate": 0.00017080740823313117,
        "epoch": 1.5010311936065996,
        "step": 11645
    },
    {
        "loss": 0.8235,
        "grad_norm": 2.763672351837158,
        "learning_rate": 0.00017076443344143257,
        "epoch": 1.5011600928074245,
        "step": 11646
    },
    {
        "loss": 1.7556,
        "grad_norm": 1.3811852931976318,
        "learning_rate": 0.0001707214324559422,
        "epoch": 1.5012889920082495,
        "step": 11647
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.319136619567871,
        "learning_rate": 0.00017067840529257737,
        "epoch": 1.5014178912090745,
        "step": 11648
    },
    {
        "loss": 1.39,
        "grad_norm": 2.998138427734375,
        "learning_rate": 0.0001706353519672647,
        "epoch": 1.5015467904098996,
        "step": 11649
    },
    {
        "loss": 2.1592,
        "grad_norm": 2.4691665172576904,
        "learning_rate": 0.00017059227249594048,
        "epoch": 1.5016756896107244,
        "step": 11650
    },
    {
        "loss": 0.9713,
        "grad_norm": 1.7454599142074585,
        "learning_rate": 0.00017054916689455095,
        "epoch": 1.5018045888115492,
        "step": 11651
    },
    {
        "loss": 1.4449,
        "grad_norm": 2.3323974609375,
        "learning_rate": 0.0001705060351790519,
        "epoch": 1.5019334880123743,
        "step": 11652
    },
    {
        "loss": 1.4489,
        "grad_norm": 2.291325092315674,
        "learning_rate": 0.00017046287736540843,
        "epoch": 1.5020623872131993,
        "step": 11653
    },
    {
        "loss": 1.4442,
        "grad_norm": 2.6379568576812744,
        "learning_rate": 0.00017041969346959587,
        "epoch": 1.5021912864140243,
        "step": 11654
    },
    {
        "loss": 2.1418,
        "grad_norm": 1.7171101570129395,
        "learning_rate": 0.0001703764835075989,
        "epoch": 1.5023201856148491,
        "step": 11655
    },
    {
        "loss": 1.4782,
        "grad_norm": 3.1244924068450928,
        "learning_rate": 0.00017033324749541188,
        "epoch": 1.5024490848156742,
        "step": 11656
    },
    {
        "loss": 1.8862,
        "grad_norm": 1.5669676065444946,
        "learning_rate": 0.00017028998544903879,
        "epoch": 1.502577984016499,
        "step": 11657
    },
    {
        "loss": 1.21,
        "grad_norm": 2.519946813583374,
        "learning_rate": 0.00017024669738449332,
        "epoch": 1.502706883217324,
        "step": 11658
    },
    {
        "loss": 1.5158,
        "grad_norm": 2.8106727600097656,
        "learning_rate": 0.00017020338331779874,
        "epoch": 1.502835782418149,
        "step": 11659
    },
    {
        "loss": 2.0866,
        "grad_norm": 2.9649930000305176,
        "learning_rate": 0.0001701600432649879,
        "epoch": 1.5029646816189741,
        "step": 11660
    },
    {
        "loss": 0.6848,
        "grad_norm": 2.8353543281555176,
        "learning_rate": 0.00017011667724210335,
        "epoch": 1.503093580819799,
        "step": 11661
    },
    {
        "loss": 2.0455,
        "grad_norm": 3.506223440170288,
        "learning_rate": 0.00017007328526519734,
        "epoch": 1.5032224800206238,
        "step": 11662
    },
    {
        "loss": 2.2631,
        "grad_norm": 7.006194591522217,
        "learning_rate": 0.00017002986735033165,
        "epoch": 1.5033513792214488,
        "step": 11663
    },
    {
        "loss": 1.3417,
        "grad_norm": 2.5482797622680664,
        "learning_rate": 0.00016998642351357733,
        "epoch": 1.5034802784222738,
        "step": 11664
    },
    {
        "loss": 1.9675,
        "grad_norm": 2.8371315002441406,
        "learning_rate": 0.00016994295377101558,
        "epoch": 1.5036091776230989,
        "step": 11665
    },
    {
        "loss": 2.4323,
        "grad_norm": 1.7731285095214844,
        "learning_rate": 0.0001698994581387369,
        "epoch": 1.5037380768239237,
        "step": 11666
    },
    {
        "loss": 0.9825,
        "grad_norm": 3.328725814819336,
        "learning_rate": 0.0001698559366328412,
        "epoch": 1.5038669760247485,
        "step": 11667
    },
    {
        "loss": 1.5155,
        "grad_norm": 3.1051244735717773,
        "learning_rate": 0.0001698123892694385,
        "epoch": 1.5039958752255735,
        "step": 11668
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.714789867401123,
        "learning_rate": 0.00016976881606464795,
        "epoch": 1.5041247744263986,
        "step": 11669
    },
    {
        "loss": 0.7769,
        "grad_norm": 2.5179049968719482,
        "learning_rate": 0.00016972521703459818,
        "epoch": 1.5042536736272236,
        "step": 11670
    },
    {
        "loss": 1.7765,
        "grad_norm": 2.2775399684906006,
        "learning_rate": 0.00016968159219542782,
        "epoch": 1.5043825728280484,
        "step": 11671
    },
    {
        "loss": 2.173,
        "grad_norm": 1.3836168050765991,
        "learning_rate": 0.0001696379415632847,
        "epoch": 1.5045114720288735,
        "step": 11672
    },
    {
        "loss": 1.0794,
        "grad_norm": 2.2614011764526367,
        "learning_rate": 0.00016959426515432635,
        "epoch": 1.5046403712296983,
        "step": 11673
    },
    {
        "loss": 1.8597,
        "grad_norm": 2.2782154083251953,
        "learning_rate": 0.0001695505629847198,
        "epoch": 1.5047692704305233,
        "step": 11674
    },
    {
        "loss": 1.1836,
        "grad_norm": 2.502030849456787,
        "learning_rate": 0.00016950683507064156,
        "epoch": 1.5048981696313484,
        "step": 11675
    },
    {
        "loss": 1.9732,
        "grad_norm": 1.4706687927246094,
        "learning_rate": 0.0001694630814282778,
        "epoch": 1.5050270688321732,
        "step": 11676
    },
    {
        "loss": 2.0152,
        "grad_norm": 2.0485949516296387,
        "learning_rate": 0.00016941930207382407,
        "epoch": 1.5051559680329982,
        "step": 11677
    },
    {
        "loss": 1.9227,
        "grad_norm": 1.7645663022994995,
        "learning_rate": 0.00016937549702348545,
        "epoch": 1.505284867233823,
        "step": 11678
    },
    {
        "loss": 1.5923,
        "grad_norm": 2.202885389328003,
        "learning_rate": 0.0001693316662934768,
        "epoch": 1.505413766434648,
        "step": 11679
    },
    {
        "loss": 1.9858,
        "grad_norm": 1.5579187870025635,
        "learning_rate": 0.00016928780990002208,
        "epoch": 1.5055426656354731,
        "step": 11680
    },
    {
        "loss": 1.6634,
        "grad_norm": 2.857529401779175,
        "learning_rate": 0.00016924392785935507,
        "epoch": 1.5056715648362982,
        "step": 11681
    },
    {
        "loss": 1.6707,
        "grad_norm": 2.312967300415039,
        "learning_rate": 0.00016920002018771887,
        "epoch": 1.505800464037123,
        "step": 11682
    },
    {
        "loss": 0.4959,
        "grad_norm": 2.0427889823913574,
        "learning_rate": 0.00016915608690136604,
        "epoch": 1.5059293632379478,
        "step": 11683
    },
    {
        "loss": 0.9923,
        "grad_norm": 1.985127329826355,
        "learning_rate": 0.0001691121280165587,
        "epoch": 1.5060582624387728,
        "step": 11684
    },
    {
        "loss": 1.3135,
        "grad_norm": 3.7756881713867188,
        "learning_rate": 0.00016906814354956858,
        "epoch": 1.5061871616395979,
        "step": 11685
    },
    {
        "loss": 0.9452,
        "grad_norm": 5.8061442375183105,
        "learning_rate": 0.00016902413351667676,
        "epoch": 1.506316060840423,
        "step": 11686
    },
    {
        "loss": 0.6202,
        "grad_norm": 3.7242796421051025,
        "learning_rate": 0.00016898009793417346,
        "epoch": 1.5064449600412477,
        "step": 11687
    },
    {
        "loss": 0.7204,
        "grad_norm": 2.2316274642944336,
        "learning_rate": 0.00016893603681835897,
        "epoch": 1.5065738592420725,
        "step": 11688
    },
    {
        "loss": 0.9557,
        "grad_norm": 3.36557674407959,
        "learning_rate": 0.0001688919501855426,
        "epoch": 1.5067027584428976,
        "step": 11689
    },
    {
        "loss": 1.5468,
        "grad_norm": 2.6221039295196533,
        "learning_rate": 0.00016884783805204323,
        "epoch": 1.5068316576437226,
        "step": 11690
    },
    {
        "loss": 2.1894,
        "grad_norm": 1.877902865409851,
        "learning_rate": 0.0001688037004341892,
        "epoch": 1.5069605568445477,
        "step": 11691
    },
    {
        "loss": 1.5754,
        "grad_norm": 2.8502233028411865,
        "learning_rate": 0.00016875953734831823,
        "epoch": 1.5070894560453725,
        "step": 11692
    },
    {
        "loss": 2.1249,
        "grad_norm": 1.4741220474243164,
        "learning_rate": 0.00016871534881077752,
        "epoch": 1.5072183552461975,
        "step": 11693
    },
    {
        "loss": 1.9832,
        "grad_norm": 2.0232958793640137,
        "learning_rate": 0.0001686711348379236,
        "epoch": 1.5073472544470223,
        "step": 11694
    },
    {
        "loss": 2.1955,
        "grad_norm": 2.3679685592651367,
        "learning_rate": 0.00016862689544612248,
        "epoch": 1.5074761536478474,
        "step": 11695
    },
    {
        "loss": 1.8981,
        "grad_norm": 3.5926568508148193,
        "learning_rate": 0.00016858263065174978,
        "epoch": 1.5076050528486724,
        "step": 11696
    },
    {
        "loss": 1.838,
        "grad_norm": 2.2190301418304443,
        "learning_rate": 0.00016853834047119012,
        "epoch": 1.5077339520494975,
        "step": 11697
    },
    {
        "loss": 0.5178,
        "grad_norm": 2.8436310291290283,
        "learning_rate": 0.00016849402492083784,
        "epoch": 1.5078628512503223,
        "step": 11698
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.4060865640640259,
        "learning_rate": 0.00016844968401709648,
        "epoch": 1.507991750451147,
        "step": 11699
    },
    {
        "loss": 2.2859,
        "grad_norm": 2.1350889205932617,
        "learning_rate": 0.00016840531777637902,
        "epoch": 1.5081206496519721,
        "step": 11700
    },
    {
        "loss": 1.9284,
        "grad_norm": 3.620081663131714,
        "learning_rate": 0.0001683609262151078,
        "epoch": 1.5082495488527972,
        "step": 11701
    },
    {
        "loss": 1.539,
        "grad_norm": 2.274160146713257,
        "learning_rate": 0.00016831650934971473,
        "epoch": 1.5083784480536222,
        "step": 11702
    },
    {
        "loss": 1.9281,
        "grad_norm": 2.5687689781188965,
        "learning_rate": 0.00016827206719664082,
        "epoch": 1.508507347254447,
        "step": 11703
    },
    {
        "loss": 1.7718,
        "grad_norm": 2.1681742668151855,
        "learning_rate": 0.00016822759977233655,
        "epoch": 1.5086362464552718,
        "step": 11704
    },
    {
        "loss": 1.9763,
        "grad_norm": 2.7366538047790527,
        "learning_rate": 0.00016818310709326175,
        "epoch": 1.5087651456560969,
        "step": 11705
    },
    {
        "loss": 2.3563,
        "grad_norm": 1.7747336626052856,
        "learning_rate": 0.00016813858917588562,
        "epoch": 1.508894044856922,
        "step": 11706
    },
    {
        "loss": 1.8641,
        "grad_norm": 2.6469881534576416,
        "learning_rate": 0.00016809404603668665,
        "epoch": 1.509022944057747,
        "step": 11707
    },
    {
        "loss": 2.0148,
        "grad_norm": 2.120140314102173,
        "learning_rate": 0.0001680494776921527,
        "epoch": 1.5091518432585718,
        "step": 11708
    },
    {
        "loss": 2.1538,
        "grad_norm": 1.5641624927520752,
        "learning_rate": 0.00016800488415878094,
        "epoch": 1.5092807424593968,
        "step": 11709
    },
    {
        "loss": 1.9771,
        "grad_norm": 2.1065361499786377,
        "learning_rate": 0.00016796026545307796,
        "epoch": 1.5094096416602216,
        "step": 11710
    },
    {
        "loss": 1.7654,
        "grad_norm": 3.8626372814178467,
        "learning_rate": 0.00016791562159155945,
        "epoch": 1.5095385408610467,
        "step": 11711
    },
    {
        "loss": 2.411,
        "grad_norm": 2.479257345199585,
        "learning_rate": 0.00016787095259075053,
        "epoch": 1.5096674400618717,
        "step": 11712
    },
    {
        "loss": 1.8669,
        "grad_norm": 2.623457670211792,
        "learning_rate": 0.00016782625846718588,
        "epoch": 1.5097963392626965,
        "step": 11713
    },
    {
        "loss": 2.0812,
        "grad_norm": 2.7052948474884033,
        "learning_rate": 0.00016778153923740905,
        "epoch": 1.5099252384635216,
        "step": 11714
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.6400654315948486,
        "learning_rate": 0.00016773679491797312,
        "epoch": 1.5100541376643464,
        "step": 11715
    },
    {
        "loss": 1.8361,
        "grad_norm": 1.4148657321929932,
        "learning_rate": 0.00016769202552544042,
        "epoch": 1.5101830368651714,
        "step": 11716
    },
    {
        "loss": 0.5173,
        "grad_norm": 3.334613800048828,
        "learning_rate": 0.00016764723107638257,
        "epoch": 1.5103119360659965,
        "step": 11717
    },
    {
        "loss": 1.4793,
        "grad_norm": 1.9917607307434082,
        "learning_rate": 0.0001676024115873803,
        "epoch": 1.5104408352668215,
        "step": 11718
    },
    {
        "loss": 1.9856,
        "grad_norm": 1.4423964023590088,
        "learning_rate": 0.000167557567075024,
        "epoch": 1.5105697344676463,
        "step": 11719
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.220777750015259,
        "learning_rate": 0.0001675126975559129,
        "epoch": 1.5106986336684711,
        "step": 11720
    },
    {
        "loss": 2.0242,
        "grad_norm": 1.6450932025909424,
        "learning_rate": 0.0001674678030466558,
        "epoch": 1.5108275328692962,
        "step": 11721
    },
    {
        "loss": 1.9793,
        "grad_norm": 2.5054616928100586,
        "learning_rate": 0.00016742288356387056,
        "epoch": 1.5109564320701212,
        "step": 11722
    },
    {
        "loss": 1.957,
        "grad_norm": 2.9929022789001465,
        "learning_rate": 0.00016737793912418428,
        "epoch": 1.5110853312709462,
        "step": 11723
    },
    {
        "loss": 1.6007,
        "grad_norm": 1.876449704170227,
        "learning_rate": 0.0001673329697442334,
        "epoch": 1.511214230471771,
        "step": 11724
    },
    {
        "loss": 2.0888,
        "grad_norm": 1.8341466188430786,
        "learning_rate": 0.00016728797544066356,
        "epoch": 1.5113431296725959,
        "step": 11725
    },
    {
        "loss": 1.9654,
        "grad_norm": 1.9722703695297241,
        "learning_rate": 0.0001672429562301296,
        "epoch": 1.511472028873421,
        "step": 11726
    },
    {
        "loss": 1.6396,
        "grad_norm": 2.5120091438293457,
        "learning_rate": 0.0001671979121292956,
        "epoch": 1.511600928074246,
        "step": 11727
    },
    {
        "loss": 1.2732,
        "grad_norm": 4.917544841766357,
        "learning_rate": 0.00016715284315483485,
        "epoch": 1.511729827275071,
        "step": 11728
    },
    {
        "loss": 2.0001,
        "grad_norm": 2.7089853286743164,
        "learning_rate": 0.00016710774932342977,
        "epoch": 1.5118587264758958,
        "step": 11729
    },
    {
        "loss": 0.4185,
        "grad_norm": 2.0312204360961914,
        "learning_rate": 0.00016706263065177223,
        "epoch": 1.5119876256767208,
        "step": 11730
    },
    {
        "loss": 1.7856,
        "grad_norm": 3.284177780151367,
        "learning_rate": 0.000167017487156563,
        "epoch": 1.5121165248775457,
        "step": 11731
    },
    {
        "loss": 0.7078,
        "grad_norm": 3.9377059936523438,
        "learning_rate": 0.00016697231885451222,
        "epoch": 1.5122454240783707,
        "step": 11732
    },
    {
        "loss": 1.6659,
        "grad_norm": 2.8179585933685303,
        "learning_rate": 0.00016692712576233906,
        "epoch": 1.5123743232791957,
        "step": 11733
    },
    {
        "loss": 1.8659,
        "grad_norm": 2.494229316711426,
        "learning_rate": 0.00016688190789677202,
        "epoch": 1.5125032224800208,
        "step": 11734
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.882251262664795,
        "learning_rate": 0.00016683666527454865,
        "epoch": 1.5126321216808456,
        "step": 11735
    },
    {
        "loss": 0.9826,
        "grad_norm": 3.1649601459503174,
        "learning_rate": 0.00016679139791241584,
        "epoch": 1.5127610208816704,
        "step": 11736
    },
    {
        "loss": 1.5883,
        "grad_norm": 2.3823678493499756,
        "learning_rate": 0.00016674610582712943,
        "epoch": 1.5128899200824955,
        "step": 11737
    },
    {
        "loss": 1.1542,
        "grad_norm": 3.6708033084869385,
        "learning_rate": 0.00016670078903545453,
        "epoch": 1.5130188192833205,
        "step": 11738
    },
    {
        "loss": 1.2441,
        "grad_norm": 3.047354221343994,
        "learning_rate": 0.00016665544755416537,
        "epoch": 1.5131477184841455,
        "step": 11739
    },
    {
        "loss": 1.8648,
        "grad_norm": 2.714179277420044,
        "learning_rate": 0.0001666100814000453,
        "epoch": 1.5132766176849703,
        "step": 11740
    },
    {
        "loss": 1.6526,
        "grad_norm": 2.5328381061553955,
        "learning_rate": 0.00016656469058988683,
        "epoch": 1.5134055168857952,
        "step": 11741
    },
    {
        "loss": 1.645,
        "grad_norm": 1.5339263677597046,
        "learning_rate": 0.00016651927514049154,
        "epoch": 1.5135344160866202,
        "step": 11742
    },
    {
        "loss": 1.3847,
        "grad_norm": 2.401984930038452,
        "learning_rate": 0.00016647383506867013,
        "epoch": 1.5136633152874452,
        "step": 11743
    },
    {
        "loss": 2.2601,
        "grad_norm": 2.868967294692993,
        "learning_rate": 0.00016642837039124278,
        "epoch": 1.5137922144882703,
        "step": 11744
    },
    {
        "loss": 1.5825,
        "grad_norm": 3.3190462589263916,
        "learning_rate": 0.00016638288112503805,
        "epoch": 1.513921113689095,
        "step": 11745
    },
    {
        "loss": 2.0699,
        "grad_norm": 1.6091639995574951,
        "learning_rate": 0.0001663373672868941,
        "epoch": 1.51405001288992,
        "step": 11746
    },
    {
        "loss": 2.2318,
        "grad_norm": 2.0939698219299316,
        "learning_rate": 0.00016629182889365827,
        "epoch": 1.514178912090745,
        "step": 11747
    },
    {
        "loss": 1.7671,
        "grad_norm": 2.3857810497283936,
        "learning_rate": 0.0001662462659621867,
        "epoch": 1.51430781129157,
        "step": 11748
    },
    {
        "loss": 2.0179,
        "grad_norm": 1.7299801111221313,
        "learning_rate": 0.00016620067850934477,
        "epoch": 1.514436710492395,
        "step": 11749
    },
    {
        "loss": 1.8927,
        "grad_norm": 1.9820177555084229,
        "learning_rate": 0.00016615506655200682,
        "epoch": 1.5145656096932198,
        "step": 11750
    },
    {
        "loss": 0.7359,
        "grad_norm": 2.5407543182373047,
        "learning_rate": 0.00016610943010705634,
        "epoch": 1.5146945088940449,
        "step": 11751
    },
    {
        "loss": 1.3589,
        "grad_norm": 2.9336419105529785,
        "learning_rate": 0.00016606376919138586,
        "epoch": 1.5148234080948697,
        "step": 11752
    },
    {
        "loss": 1.6122,
        "grad_norm": 2.6715409755706787,
        "learning_rate": 0.00016601808382189713,
        "epoch": 1.5149523072956947,
        "step": 11753
    },
    {
        "loss": 1.4443,
        "grad_norm": 2.617802619934082,
        "learning_rate": 0.0001659723740155007,
        "epoch": 1.5150812064965198,
        "step": 11754
    },
    {
        "loss": 1.5884,
        "grad_norm": 1.4495421648025513,
        "learning_rate": 0.00016592663978911624,
        "epoch": 1.5152101056973448,
        "step": 11755
    },
    {
        "loss": 1.2531,
        "grad_norm": 2.1502325534820557,
        "learning_rate": 0.00016588088115967258,
        "epoch": 1.5153390048981696,
        "step": 11756
    },
    {
        "loss": 1.7229,
        "grad_norm": 1.6063618659973145,
        "learning_rate": 0.00016583509814410742,
        "epoch": 1.5154679040989945,
        "step": 11757
    },
    {
        "loss": 1.9246,
        "grad_norm": 3.2532596588134766,
        "learning_rate": 0.00016578929075936763,
        "epoch": 1.5155968032998195,
        "step": 11758
    },
    {
        "loss": 1.0441,
        "grad_norm": 3.5817151069641113,
        "learning_rate": 0.00016574345902240897,
        "epoch": 1.5157257025006445,
        "step": 11759
    },
    {
        "loss": 1.6777,
        "grad_norm": 2.4351418018341064,
        "learning_rate": 0.00016569760295019617,
        "epoch": 1.5158546017014696,
        "step": 11760
    },
    {
        "loss": 1.5517,
        "grad_norm": 3.1181485652923584,
        "learning_rate": 0.0001656517225597034,
        "epoch": 1.5159835009022944,
        "step": 11761
    },
    {
        "loss": 1.5422,
        "grad_norm": 2.4362947940826416,
        "learning_rate": 0.00016560581786791322,
        "epoch": 1.5161124001031192,
        "step": 11762
    },
    {
        "loss": 2.1611,
        "grad_norm": 3.301452159881592,
        "learning_rate": 0.00016555988889181746,
        "epoch": 1.5162412993039442,
        "step": 11763
    },
    {
        "loss": 2.0553,
        "grad_norm": 2.352059841156006,
        "learning_rate": 0.00016551393564841714,
        "epoch": 1.5163701985047693,
        "step": 11764
    },
    {
        "loss": 1.5471,
        "grad_norm": 2.811296224594116,
        "learning_rate": 0.00016546795815472197,
        "epoch": 1.5164990977055943,
        "step": 11765
    },
    {
        "loss": 1.3319,
        "grad_norm": 1.7950297594070435,
        "learning_rate": 0.0001654219564277506,
        "epoch": 1.5166279969064191,
        "step": 11766
    },
    {
        "loss": 1.6282,
        "grad_norm": 2.210289239883423,
        "learning_rate": 0.0001653759304845312,
        "epoch": 1.5167568961072442,
        "step": 11767
    },
    {
        "loss": 1.4058,
        "grad_norm": 2.1976447105407715,
        "learning_rate": 0.00016532988034210008,
        "epoch": 1.516885795308069,
        "step": 11768
    },
    {
        "loss": 1.7044,
        "grad_norm": 1.8989239931106567,
        "learning_rate": 0.00016528380601750294,
        "epoch": 1.517014694508894,
        "step": 11769
    },
    {
        "loss": 1.1626,
        "grad_norm": 2.829239845275879,
        "learning_rate": 0.00016523770752779467,
        "epoch": 1.517143593709719,
        "step": 11770
    },
    {
        "loss": 1.6901,
        "grad_norm": 2.9652323722839355,
        "learning_rate": 0.0001651915848900387,
        "epoch": 1.5172724929105441,
        "step": 11771
    },
    {
        "loss": 2.4731,
        "grad_norm": 1.659400463104248,
        "learning_rate": 0.00016514543812130755,
        "epoch": 1.517401392111369,
        "step": 11772
    },
    {
        "loss": 1.8289,
        "grad_norm": 2.1923770904541016,
        "learning_rate": 0.00016509926723868267,
        "epoch": 1.5175302913121937,
        "step": 11773
    },
    {
        "loss": 1.7726,
        "grad_norm": 3.8237195014953613,
        "learning_rate": 0.0001650530722592544,
        "epoch": 1.5176591905130188,
        "step": 11774
    },
    {
        "loss": 1.2552,
        "grad_norm": 3.001779317855835,
        "learning_rate": 0.00016500685320012206,
        "epoch": 1.5177880897138438,
        "step": 11775
    },
    {
        "loss": 1.5869,
        "grad_norm": 2.0006237030029297,
        "learning_rate": 0.00016496061007839388,
        "epoch": 1.5179169889146689,
        "step": 11776
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.0765833854675293,
        "learning_rate": 0.0001649143429111868,
        "epoch": 1.5180458881154937,
        "step": 11777
    },
    {
        "loss": 1.763,
        "grad_norm": 1.4280356168746948,
        "learning_rate": 0.00016486805171562723,
        "epoch": 1.5181747873163185,
        "step": 11778
    },
    {
        "loss": 1.9855,
        "grad_norm": 2.433465003967285,
        "learning_rate": 0.0001648217365088497,
        "epoch": 1.5183036865171435,
        "step": 11779
    },
    {
        "loss": 1.419,
        "grad_norm": 2.4992237091064453,
        "learning_rate": 0.000164775397307998,
        "epoch": 1.5184325857179686,
        "step": 11780
    },
    {
        "loss": 2.0741,
        "grad_norm": 3.4935405254364014,
        "learning_rate": 0.00016472903413022508,
        "epoch": 1.5185614849187936,
        "step": 11781
    },
    {
        "loss": 0.9636,
        "grad_norm": 3.2326676845550537,
        "learning_rate": 0.0001646826469926923,
        "epoch": 1.5186903841196184,
        "step": 11782
    },
    {
        "loss": 1.961,
        "grad_norm": 2.754979372024536,
        "learning_rate": 0.00016463623591257007,
        "epoch": 1.5188192833204432,
        "step": 11783
    },
    {
        "loss": 1.2326,
        "grad_norm": 2.691257953643799,
        "learning_rate": 0.0001645898009070379,
        "epoch": 1.5189481825212683,
        "step": 11784
    },
    {
        "loss": 0.7911,
        "grad_norm": 4.425718784332275,
        "learning_rate": 0.00016454334199328364,
        "epoch": 1.5190770817220933,
        "step": 11785
    },
    {
        "loss": 1.709,
        "grad_norm": 2.4067370891571045,
        "learning_rate": 0.0001644968591885043,
        "epoch": 1.5192059809229184,
        "step": 11786
    },
    {
        "loss": 1.7379,
        "grad_norm": 2.8278257846832275,
        "learning_rate": 0.0001644503525099059,
        "epoch": 1.5193348801237432,
        "step": 11787
    },
    {
        "loss": 2.6145,
        "grad_norm": 1.8799530267715454,
        "learning_rate": 0.00016440382197470302,
        "epoch": 1.5194637793245682,
        "step": 11788
    },
    {
        "loss": 1.5362,
        "grad_norm": 3.2574551105499268,
        "learning_rate": 0.00016435726760011913,
        "epoch": 1.519592678525393,
        "step": 11789
    },
    {
        "loss": 0.9033,
        "grad_norm": 2.5641915798187256,
        "learning_rate": 0.0001643106894033866,
        "epoch": 1.519721577726218,
        "step": 11790
    },
    {
        "loss": 1.8103,
        "grad_norm": 2.366515874862671,
        "learning_rate": 0.00016426408740174652,
        "epoch": 1.5198504769270431,
        "step": 11791
    },
    {
        "loss": 1.9281,
        "grad_norm": 1.6545641422271729,
        "learning_rate": 0.00016421746161244887,
        "epoch": 1.5199793761278682,
        "step": 11792
    },
    {
        "loss": 2.5241,
        "grad_norm": 2.028709888458252,
        "learning_rate": 0.0001641708120527524,
        "epoch": 1.520108275328693,
        "step": 11793
    },
    {
        "loss": 1.9621,
        "grad_norm": 3.8186981678009033,
        "learning_rate": 0.00016412413873992455,
        "epoch": 1.5202371745295178,
        "step": 11794
    },
    {
        "loss": 0.9586,
        "grad_norm": 3.2044179439544678,
        "learning_rate": 0.00016407744169124203,
        "epoch": 1.5203660737303428,
        "step": 11795
    },
    {
        "loss": 1.8098,
        "grad_norm": 2.2300145626068115,
        "learning_rate": 0.0001640307209239896,
        "epoch": 1.5204949729311679,
        "step": 11796
    },
    {
        "loss": 2.0275,
        "grad_norm": 2.9200258255004883,
        "learning_rate": 0.00016398397645546125,
        "epoch": 1.520623872131993,
        "step": 11797
    },
    {
        "loss": 1.7074,
        "grad_norm": 2.3713343143463135,
        "learning_rate": 0.0001639372083029598,
        "epoch": 1.5207527713328177,
        "step": 11798
    },
    {
        "loss": 2.0109,
        "grad_norm": 2.9879307746887207,
        "learning_rate": 0.00016389041648379662,
        "epoch": 1.5208816705336425,
        "step": 11799
    },
    {
        "loss": 1.9529,
        "grad_norm": 1.9428707361221313,
        "learning_rate": 0.00016384360101529184,
        "epoch": 1.5210105697344676,
        "step": 11800
    },
    {
        "loss": 2.276,
        "grad_norm": 1.847367763519287,
        "learning_rate": 0.00016379676191477472,
        "epoch": 1.5211394689352926,
        "step": 11801
    },
    {
        "loss": 1.4986,
        "grad_norm": 1.5877363681793213,
        "learning_rate": 0.00016374989919958267,
        "epoch": 1.5212683681361177,
        "step": 11802
    },
    {
        "loss": 1.818,
        "grad_norm": 2.5377843379974365,
        "learning_rate": 0.00016370301288706215,
        "epoch": 1.5213972673369425,
        "step": 11803
    },
    {
        "loss": 1.9957,
        "grad_norm": 2.266106605529785,
        "learning_rate": 0.00016365610299456858,
        "epoch": 1.5215261665377675,
        "step": 11804
    },
    {
        "loss": 1.1963,
        "grad_norm": 1.705552577972412,
        "learning_rate": 0.00016360916953946574,
        "epoch": 1.5216550657385923,
        "step": 11805
    },
    {
        "loss": 1.2235,
        "grad_norm": 2.0932693481445312,
        "learning_rate": 0.0001635622125391263,
        "epoch": 1.5217839649394174,
        "step": 11806
    },
    {
        "loss": 1.921,
        "grad_norm": 2.4931745529174805,
        "learning_rate": 0.00016351523201093158,
        "epoch": 1.5219128641402424,
        "step": 11807
    },
    {
        "loss": 1.416,
        "grad_norm": 2.936877489089966,
        "learning_rate": 0.00016346822797227168,
        "epoch": 1.5220417633410674,
        "step": 11808
    },
    {
        "loss": 1.3584,
        "grad_norm": 2.7347917556762695,
        "learning_rate": 0.00016342120044054537,
        "epoch": 1.5221706625418923,
        "step": 11809
    },
    {
        "loss": 1.2995,
        "grad_norm": 2.240186929702759,
        "learning_rate": 0.00016337414943316009,
        "epoch": 1.522299561742717,
        "step": 11810
    },
    {
        "loss": 1.4701,
        "grad_norm": 3.853646755218506,
        "learning_rate": 0.00016332707496753191,
        "epoch": 1.5224284609435421,
        "step": 11811
    },
    {
        "loss": 1.3467,
        "grad_norm": 3.079822301864624,
        "learning_rate": 0.000163279977061086,
        "epoch": 1.5225573601443672,
        "step": 11812
    },
    {
        "loss": 1.9766,
        "grad_norm": 1.6998424530029297,
        "learning_rate": 0.0001632328557312555,
        "epoch": 1.5226862593451922,
        "step": 11813
    },
    {
        "loss": 2.1817,
        "grad_norm": 2.7568559646606445,
        "learning_rate": 0.0001631857109954826,
        "epoch": 1.522815158546017,
        "step": 11814
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.322756290435791,
        "learning_rate": 0.00016313854287121845,
        "epoch": 1.5229440577468418,
        "step": 11815
    },
    {
        "loss": 2.4644,
        "grad_norm": 2.220412254333496,
        "learning_rate": 0.00016309135137592234,
        "epoch": 1.5230729569476669,
        "step": 11816
    },
    {
        "loss": 1.8516,
        "grad_norm": 2.334829330444336,
        "learning_rate": 0.00016304413652706235,
        "epoch": 1.523201856148492,
        "step": 11817
    },
    {
        "loss": 0.8948,
        "grad_norm": 2.4995641708374023,
        "learning_rate": 0.00016299689834211567,
        "epoch": 1.523330755349317,
        "step": 11818
    },
    {
        "loss": 1.9347,
        "grad_norm": 1.710857629776001,
        "learning_rate": 0.00016294963683856736,
        "epoch": 1.5234596545501418,
        "step": 11819
    },
    {
        "loss": 1.4257,
        "grad_norm": 1.8566107749938965,
        "learning_rate": 0.00016290235203391147,
        "epoch": 1.5235885537509666,
        "step": 11820
    },
    {
        "loss": 1.7945,
        "grad_norm": 2.214585065841675,
        "learning_rate": 0.00016285504394565094,
        "epoch": 1.5237174529517916,
        "step": 11821
    },
    {
        "loss": 2.1048,
        "grad_norm": 2.156576633453369,
        "learning_rate": 0.00016280771259129698,
        "epoch": 1.5238463521526167,
        "step": 11822
    },
    {
        "loss": 1.3084,
        "grad_norm": 2.8811867237091064,
        "learning_rate": 0.0001627603579883695,
        "epoch": 1.5239752513534417,
        "step": 11823
    },
    {
        "loss": 1.8191,
        "grad_norm": 1.767898440361023,
        "learning_rate": 0.00016271298015439704,
        "epoch": 1.5241041505542665,
        "step": 11824
    },
    {
        "loss": 1.4921,
        "grad_norm": 3.1884419918060303,
        "learning_rate": 0.00016266557910691673,
        "epoch": 1.5242330497550916,
        "step": 11825
    },
    {
        "loss": 2.2395,
        "grad_norm": 3.3631339073181152,
        "learning_rate": 0.0001626181548634743,
        "epoch": 1.5243619489559164,
        "step": 11826
    },
    {
        "loss": 1.7689,
        "grad_norm": 2.5014188289642334,
        "learning_rate": 0.00016257070744162406,
        "epoch": 1.5244908481567414,
        "step": 11827
    },
    {
        "loss": 1.494,
        "grad_norm": 3.3334197998046875,
        "learning_rate": 0.00016252323685892877,
        "epoch": 1.5246197473575664,
        "step": 11828
    },
    {
        "loss": 1.0646,
        "grad_norm": 2.4162561893463135,
        "learning_rate": 0.0001624757431329601,
        "epoch": 1.5247486465583915,
        "step": 11829
    },
    {
        "loss": 0.6182,
        "grad_norm": 2.5298588275909424,
        "learning_rate": 0.0001624282262812981,
        "epoch": 1.5248775457592163,
        "step": 11830
    },
    {
        "loss": 2.1264,
        "grad_norm": 1.4763797521591187,
        "learning_rate": 0.0001623806863215311,
        "epoch": 1.5250064449600411,
        "step": 11831
    },
    {
        "loss": 1.5731,
        "grad_norm": 3.989778995513916,
        "learning_rate": 0.00016233312327125642,
        "epoch": 1.5251353441608662,
        "step": 11832
    },
    {
        "loss": 1.8573,
        "grad_norm": 2.490424871444702,
        "learning_rate": 0.00016228553714807978,
        "epoch": 1.5252642433616912,
        "step": 11833
    },
    {
        "loss": 2.2169,
        "grad_norm": 2.6386830806732178,
        "learning_rate": 0.0001622379279696152,
        "epoch": 1.5253931425625162,
        "step": 11834
    },
    {
        "loss": 1.5597,
        "grad_norm": 2.4751102924346924,
        "learning_rate": 0.00016219029575348585,
        "epoch": 1.525522041763341,
        "step": 11835
    },
    {
        "loss": 0.8073,
        "grad_norm": 2.6221466064453125,
        "learning_rate": 0.00016214264051732263,
        "epoch": 1.5256509409641659,
        "step": 11836
    },
    {
        "loss": 2.1157,
        "grad_norm": 1.8108549118041992,
        "learning_rate": 0.0001620949622787654,
        "epoch": 1.525779840164991,
        "step": 11837
    },
    {
        "loss": 1.915,
        "grad_norm": 2.5622026920318604,
        "learning_rate": 0.0001620472610554627,
        "epoch": 1.525908739365816,
        "step": 11838
    },
    {
        "loss": 1.8436,
        "grad_norm": 1.8733851909637451,
        "learning_rate": 0.00016199953686507122,
        "epoch": 1.526037638566641,
        "step": 11839
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.8836337327957153,
        "learning_rate": 0.0001619517897252563,
        "epoch": 1.5261665377674658,
        "step": 11840
    },
    {
        "loss": 1.1367,
        "grad_norm": 1.6224132776260376,
        "learning_rate": 0.0001619040196536918,
        "epoch": 1.5262954369682908,
        "step": 11841
    },
    {
        "loss": 2.0744,
        "grad_norm": 4.917925834655762,
        "learning_rate": 0.00016185622666806002,
        "epoch": 1.5264243361691157,
        "step": 11842
    },
    {
        "loss": 1.4843,
        "grad_norm": 2.920274496078491,
        "learning_rate": 0.00016180841078605176,
        "epoch": 1.5265532353699407,
        "step": 11843
    },
    {
        "loss": 0.9446,
        "grad_norm": 2.9692373275756836,
        "learning_rate": 0.0001617605720253663,
        "epoch": 1.5266821345707657,
        "step": 11844
    },
    {
        "loss": 1.0835,
        "grad_norm": 3.1567904949188232,
        "learning_rate": 0.00016171271040371133,
        "epoch": 1.5268110337715908,
        "step": 11845
    },
    {
        "loss": 0.719,
        "grad_norm": 3.0740983486175537,
        "learning_rate": 0.00016166482593880317,
        "epoch": 1.5269399329724156,
        "step": 11846
    },
    {
        "loss": 2.2581,
        "grad_norm": 2.7101359367370605,
        "learning_rate": 0.00016161691864836656,
        "epoch": 1.5270688321732404,
        "step": 11847
    },
    {
        "loss": 1.7539,
        "grad_norm": 3.036119222640991,
        "learning_rate": 0.00016156898855013428,
        "epoch": 1.5271977313740654,
        "step": 11848
    },
    {
        "loss": 0.7202,
        "grad_norm": 2.6376821994781494,
        "learning_rate": 0.00016152103566184815,
        "epoch": 1.5273266305748905,
        "step": 11849
    },
    {
        "loss": 1.5625,
        "grad_norm": 1.612210988998413,
        "learning_rate": 0.00016147306000125812,
        "epoch": 1.5274555297757155,
        "step": 11850
    },
    {
        "loss": 0.8668,
        "grad_norm": 2.2262966632843018,
        "learning_rate": 0.00016142506158612244,
        "epoch": 1.5275844289765403,
        "step": 11851
    },
    {
        "loss": 1.6797,
        "grad_norm": 2.7601561546325684,
        "learning_rate": 0.00016137704043420835,
        "epoch": 1.5277133281773652,
        "step": 11852
    },
    {
        "loss": 2.3791,
        "grad_norm": 1.7160948514938354,
        "learning_rate": 0.00016132899656329074,
        "epoch": 1.5278422273781902,
        "step": 11853
    },
    {
        "loss": 1.8584,
        "grad_norm": 2.5978915691375732,
        "learning_rate": 0.00016128092999115326,
        "epoch": 1.5279711265790152,
        "step": 11854
    },
    {
        "loss": 2.1666,
        "grad_norm": 1.9089124202728271,
        "learning_rate": 0.00016123284073558822,
        "epoch": 1.5281000257798403,
        "step": 11855
    },
    {
        "loss": 1.8343,
        "grad_norm": 2.163041830062866,
        "learning_rate": 0.00016118472881439594,
        "epoch": 1.528228924980665,
        "step": 11856
    },
    {
        "loss": 1.0664,
        "grad_norm": 2.5275075435638428,
        "learning_rate": 0.00016113659424538539,
        "epoch": 1.52835782418149,
        "step": 11857
    },
    {
        "loss": 1.3565,
        "grad_norm": 3.74219012260437,
        "learning_rate": 0.00016108843704637366,
        "epoch": 1.528486723382315,
        "step": 11858
    },
    {
        "loss": 2.0436,
        "grad_norm": 3.1490254402160645,
        "learning_rate": 0.00016104025723518648,
        "epoch": 1.52861562258314,
        "step": 11859
    },
    {
        "loss": 2.0827,
        "grad_norm": 1.226168155670166,
        "learning_rate": 0.0001609920548296578,
        "epoch": 1.528744521783965,
        "step": 11860
    },
    {
        "loss": 1.6524,
        "grad_norm": 2.9945170879364014,
        "learning_rate": 0.0001609438298476299,
        "epoch": 1.5288734209847898,
        "step": 11861
    },
    {
        "loss": 1.5766,
        "grad_norm": 1.680129051208496,
        "learning_rate": 0.0001608955823069535,
        "epoch": 1.5290023201856149,
        "step": 11862
    },
    {
        "loss": 2.077,
        "grad_norm": 1.6996310949325562,
        "learning_rate": 0.0001608473122254878,
        "epoch": 1.5291312193864397,
        "step": 11863
    },
    {
        "loss": 2.1087,
        "grad_norm": 2.5334692001342773,
        "learning_rate": 0.00016079901962110023,
        "epoch": 1.5292601185872647,
        "step": 11864
    },
    {
        "loss": 1.8892,
        "grad_norm": 1.948599100112915,
        "learning_rate": 0.00016075070451166616,
        "epoch": 1.5293890177880898,
        "step": 11865
    },
    {
        "loss": 2.0502,
        "grad_norm": 1.4577139616012573,
        "learning_rate": 0.00016070236691507,
        "epoch": 1.5295179169889148,
        "step": 11866
    },
    {
        "loss": 2.0689,
        "grad_norm": 2.3389081954956055,
        "learning_rate": 0.00016065400684920408,
        "epoch": 1.5296468161897396,
        "step": 11867
    },
    {
        "loss": 1.4276,
        "grad_norm": 2.5832600593566895,
        "learning_rate": 0.00016060562433196896,
        "epoch": 1.5297757153905645,
        "step": 11868
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.3093161582946777,
        "learning_rate": 0.00016055721938127384,
        "epoch": 1.5299046145913895,
        "step": 11869
    },
    {
        "loss": 2.1603,
        "grad_norm": 1.9229679107666016,
        "learning_rate": 0.0001605087920150361,
        "epoch": 1.5300335137922145,
        "step": 11870
    },
    {
        "loss": 0.4956,
        "grad_norm": 3.327082872390747,
        "learning_rate": 0.000160460342251181,
        "epoch": 1.5301624129930396,
        "step": 11871
    },
    {
        "loss": 1.5884,
        "grad_norm": 2.2350058555603027,
        "learning_rate": 0.00016041187010764282,
        "epoch": 1.5302913121938644,
        "step": 11872
    },
    {
        "loss": 1.7113,
        "grad_norm": 2.235546350479126,
        "learning_rate": 0.00016036337560236362,
        "epoch": 1.5304202113946892,
        "step": 11873
    },
    {
        "loss": 2.5241,
        "grad_norm": 1.1325483322143555,
        "learning_rate": 0.00016031485875329384,
        "epoch": 1.5305491105955142,
        "step": 11874
    },
    {
        "loss": 1.809,
        "grad_norm": 3.4250564575195312,
        "learning_rate": 0.0001602663195783923,
        "epoch": 1.5306780097963393,
        "step": 11875
    },
    {
        "loss": 2.2148,
        "grad_norm": 2.1137454509735107,
        "learning_rate": 0.0001602177580956259,
        "epoch": 1.5308069089971643,
        "step": 11876
    },
    {
        "loss": 1.7069,
        "grad_norm": 2.9565789699554443,
        "learning_rate": 0.00016016917432296998,
        "epoch": 1.5309358081979891,
        "step": 11877
    },
    {
        "loss": 2.0853,
        "grad_norm": 1.6204880475997925,
        "learning_rate": 0.00016012056827840806,
        "epoch": 1.5310647073988142,
        "step": 11878
    },
    {
        "loss": 1.3772,
        "grad_norm": 2.7844996452331543,
        "learning_rate": 0.00016007193997993172,
        "epoch": 1.531193606599639,
        "step": 11879
    },
    {
        "loss": 1.2472,
        "grad_norm": 2.3985040187835693,
        "learning_rate": 0.00016002328944554126,
        "epoch": 1.531322505800464,
        "step": 11880
    },
    {
        "loss": 1.9641,
        "grad_norm": 1.8167853355407715,
        "learning_rate": 0.00015997461669324485,
        "epoch": 1.531451405001289,
        "step": 11881
    },
    {
        "loss": 2.2188,
        "grad_norm": 2.6777541637420654,
        "learning_rate": 0.00015992592174105864,
        "epoch": 1.531580304202114,
        "step": 11882
    },
    {
        "loss": 0.8089,
        "grad_norm": 3.462526559829712,
        "learning_rate": 0.0001598772046070076,
        "epoch": 1.531709203402939,
        "step": 11883
    },
    {
        "loss": 1.2089,
        "grad_norm": 3.140352964401245,
        "learning_rate": 0.00015982846530912453,
        "epoch": 1.5318381026037637,
        "step": 11884
    },
    {
        "loss": 1.0277,
        "grad_norm": 4.130871772766113,
        "learning_rate": 0.00015977970386545038,
        "epoch": 1.5319670018045888,
        "step": 11885
    },
    {
        "loss": 1.0605,
        "grad_norm": 2.959010362625122,
        "learning_rate": 0.00015973092029403465,
        "epoch": 1.5320959010054138,
        "step": 11886
    },
    {
        "loss": 2.0891,
        "grad_norm": 1.7735545635223389,
        "learning_rate": 0.00015968211461293483,
        "epoch": 1.5322248002062389,
        "step": 11887
    },
    {
        "loss": 1.4442,
        "grad_norm": 1.8547875881195068,
        "learning_rate": 0.00015963328684021625,
        "epoch": 1.5323536994070637,
        "step": 11888
    },
    {
        "loss": 2.3756,
        "grad_norm": 1.9376565217971802,
        "learning_rate": 0.00015958443699395304,
        "epoch": 1.5324825986078885,
        "step": 11889
    },
    {
        "loss": 1.1953,
        "grad_norm": 2.322995662689209,
        "learning_rate": 0.0001595355650922271,
        "epoch": 1.5326114978087135,
        "step": 11890
    },
    {
        "loss": 1.541,
        "grad_norm": 2.4907524585723877,
        "learning_rate": 0.00015948667115312862,
        "epoch": 1.5327403970095386,
        "step": 11891
    },
    {
        "loss": 1.6956,
        "grad_norm": 2.0083305835723877,
        "learning_rate": 0.0001594377551947559,
        "epoch": 1.5328692962103636,
        "step": 11892
    },
    {
        "loss": 0.9128,
        "grad_norm": 2.1620519161224365,
        "learning_rate": 0.00015938881723521543,
        "epoch": 1.5329981954111884,
        "step": 11893
    },
    {
        "loss": 2.2734,
        "grad_norm": 1.9850172996520996,
        "learning_rate": 0.0001593398572926218,
        "epoch": 1.5331270946120132,
        "step": 11894
    },
    {
        "loss": 1.4849,
        "grad_norm": 2.1388447284698486,
        "learning_rate": 0.00015929087538509783,
        "epoch": 1.5332559938128383,
        "step": 11895
    },
    {
        "loss": 2.4172,
        "grad_norm": 3.3400163650512695,
        "learning_rate": 0.00015924187153077421,
        "epoch": 1.5333848930136633,
        "step": 11896
    },
    {
        "loss": 1.3061,
        "grad_norm": 2.5250067710876465,
        "learning_rate": 0.00015919284574779025,
        "epoch": 1.5335137922144884,
        "step": 11897
    },
    {
        "loss": 1.9272,
        "grad_norm": 1.7872891426086426,
        "learning_rate": 0.00015914379805429302,
        "epoch": 1.5336426914153132,
        "step": 11898
    },
    {
        "loss": 1.9946,
        "grad_norm": 2.2075393199920654,
        "learning_rate": 0.00015909472846843746,
        "epoch": 1.5337715906161382,
        "step": 11899
    },
    {
        "loss": 1.0268,
        "grad_norm": 1.9059269428253174,
        "learning_rate": 0.00015904563700838725,
        "epoch": 1.533900489816963,
        "step": 11900
    },
    {
        "loss": 1.0059,
        "grad_norm": 3.601304054260254,
        "learning_rate": 0.00015899652369231367,
        "epoch": 1.534029389017788,
        "step": 11901
    },
    {
        "loss": 1.9831,
        "grad_norm": 2.311589002609253,
        "learning_rate": 0.00015894738853839617,
        "epoch": 1.534158288218613,
        "step": 11902
    },
    {
        "loss": 1.6897,
        "grad_norm": 2.7851147651672363,
        "learning_rate": 0.00015889823156482256,
        "epoch": 1.5342871874194381,
        "step": 11903
    },
    {
        "loss": 1.6814,
        "grad_norm": 2.2082464694976807,
        "learning_rate": 0.00015884905278978857,
        "epoch": 1.534416086620263,
        "step": 11904
    },
    {
        "loss": 2.1421,
        "grad_norm": 2.791337251663208,
        "learning_rate": 0.00015879985223149755,
        "epoch": 1.5345449858210878,
        "step": 11905
    },
    {
        "loss": 1.8966,
        "grad_norm": 2.5696890354156494,
        "learning_rate": 0.00015875062990816177,
        "epoch": 1.5346738850219128,
        "step": 11906
    },
    {
        "loss": 1.8594,
        "grad_norm": 1.6965476274490356,
        "learning_rate": 0.00015870138583800087,
        "epoch": 1.5348027842227379,
        "step": 11907
    },
    {
        "loss": 0.8971,
        "grad_norm": 2.389479637145996,
        "learning_rate": 0.00015865212003924285,
        "epoch": 1.534931683423563,
        "step": 11908
    },
    {
        "loss": 1.0287,
        "grad_norm": 2.301622152328491,
        "learning_rate": 0.00015860283253012368,
        "epoch": 1.5350605826243877,
        "step": 11909
    },
    {
        "loss": 0.4383,
        "grad_norm": 3.2391533851623535,
        "learning_rate": 0.00015855352332888733,
        "epoch": 1.5351894818252125,
        "step": 11910
    },
    {
        "loss": 1.7711,
        "grad_norm": 2.143697500228882,
        "learning_rate": 0.00015850419245378588,
        "epoch": 1.5353183810260376,
        "step": 11911
    },
    {
        "loss": 2.0273,
        "grad_norm": 2.5121147632598877,
        "learning_rate": 0.00015845483992307931,
        "epoch": 1.5354472802268626,
        "step": 11912
    },
    {
        "loss": 2.0039,
        "grad_norm": 2.1975619792938232,
        "learning_rate": 0.0001584054657550357,
        "epoch": 1.5355761794276876,
        "step": 11913
    },
    {
        "loss": 2.0552,
        "grad_norm": 2.6969475746154785,
        "learning_rate": 0.00015835606996793126,
        "epoch": 1.5357050786285125,
        "step": 11914
    },
    {
        "loss": 1.1075,
        "grad_norm": 2.101290464401245,
        "learning_rate": 0.0001583066525800501,
        "epoch": 1.5358339778293375,
        "step": 11915
    },
    {
        "loss": 2.6783,
        "grad_norm": 2.5063252449035645,
        "learning_rate": 0.000158257213609684,
        "epoch": 1.5359628770301623,
        "step": 11916
    },
    {
        "loss": 1.6259,
        "grad_norm": 2.223184585571289,
        "learning_rate": 0.0001582077530751333,
        "epoch": 1.5360917762309874,
        "step": 11917
    },
    {
        "loss": 2.3734,
        "grad_norm": 3.978976249694824,
        "learning_rate": 0.00015815827099470604,
        "epoch": 1.5362206754318124,
        "step": 11918
    },
    {
        "loss": 1.7106,
        "grad_norm": 2.077956199645996,
        "learning_rate": 0.00015810876738671804,
        "epoch": 1.5363495746326374,
        "step": 11919
    },
    {
        "loss": 1.7056,
        "grad_norm": 2.0736231803894043,
        "learning_rate": 0.00015805924226949353,
        "epoch": 1.5364784738334623,
        "step": 11920
    },
    {
        "loss": 2.1309,
        "grad_norm": 2.191103219985962,
        "learning_rate": 0.00015800969566136453,
        "epoch": 1.536607373034287,
        "step": 11921
    },
    {
        "loss": 1.3012,
        "grad_norm": 2.4137892723083496,
        "learning_rate": 0.00015796012758067054,
        "epoch": 1.536736272235112,
        "step": 11922
    },
    {
        "loss": 1.8839,
        "grad_norm": 2.075385570526123,
        "learning_rate": 0.00015791053804575984,
        "epoch": 1.5368651714359371,
        "step": 11923
    },
    {
        "loss": 2.2282,
        "grad_norm": 1.768675446510315,
        "learning_rate": 0.00015786092707498805,
        "epoch": 1.5369940706367622,
        "step": 11924
    },
    {
        "loss": 1.3999,
        "grad_norm": 2.4506382942199707,
        "learning_rate": 0.00015781129468671893,
        "epoch": 1.537122969837587,
        "step": 11925
    },
    {
        "loss": 1.5201,
        "grad_norm": 2.5647060871124268,
        "learning_rate": 0.00015776164089932414,
        "epoch": 1.5372518690384118,
        "step": 11926
    },
    {
        "loss": 1.2694,
        "grad_norm": 2.633841037750244,
        "learning_rate": 0.00015771196573118328,
        "epoch": 1.5373807682392369,
        "step": 11927
    },
    {
        "loss": 1.5517,
        "grad_norm": 2.3792967796325684,
        "learning_rate": 0.00015766226920068386,
        "epoch": 1.537509667440062,
        "step": 11928
    },
    {
        "loss": 1.5389,
        "grad_norm": 1.730072021484375,
        "learning_rate": 0.00015761255132622126,
        "epoch": 1.537638566640887,
        "step": 11929
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.5460339784622192,
        "learning_rate": 0.00015756281212619868,
        "epoch": 1.5377674658417118,
        "step": 11930
    },
    {
        "loss": 1.2419,
        "grad_norm": 4.045501708984375,
        "learning_rate": 0.00015751305161902754,
        "epoch": 1.5378963650425366,
        "step": 11931
    },
    {
        "loss": 1.1726,
        "grad_norm": 4.119664669036865,
        "learning_rate": 0.00015746326982312683,
        "epoch": 1.5380252642433616,
        "step": 11932
    },
    {
        "loss": 1.5223,
        "grad_norm": 3.716870069503784,
        "learning_rate": 0.00015741346675692355,
        "epoch": 1.5381541634441867,
        "step": 11933
    },
    {
        "loss": 1.7515,
        "grad_norm": 1.8079942464828491,
        "learning_rate": 0.00015736364243885246,
        "epoch": 1.5382830626450117,
        "step": 11934
    },
    {
        "loss": 2.0478,
        "grad_norm": 2.075657844543457,
        "learning_rate": 0.00015731379688735633,
        "epoch": 1.5384119618458365,
        "step": 11935
    },
    {
        "loss": 1.9842,
        "grad_norm": 2.283764123916626,
        "learning_rate": 0.0001572639301208856,
        "epoch": 1.5385408610466615,
        "step": 11936
    },
    {
        "loss": 1.665,
        "grad_norm": 1.9909123182296753,
        "learning_rate": 0.00015721404215789892,
        "epoch": 1.5386697602474864,
        "step": 11937
    },
    {
        "loss": 1.338,
        "grad_norm": 4.134830474853516,
        "learning_rate": 0.00015716413301686258,
        "epoch": 1.5387986594483114,
        "step": 11938
    },
    {
        "loss": 1.7656,
        "grad_norm": 2.2538058757781982,
        "learning_rate": 0.00015711420271625028,
        "epoch": 1.5389275586491364,
        "step": 11939
    },
    {
        "loss": 1.8003,
        "grad_norm": 4.555150508880615,
        "learning_rate": 0.00015706425127454433,
        "epoch": 1.5390564578499615,
        "step": 11940
    },
    {
        "loss": 1.4422,
        "grad_norm": 3.101010799407959,
        "learning_rate": 0.00015701427871023444,
        "epoch": 1.5391853570507863,
        "step": 11941
    },
    {
        "loss": 1.2721,
        "grad_norm": 3.2249321937561035,
        "learning_rate": 0.00015696428504181813,
        "epoch": 1.5393142562516111,
        "step": 11942
    },
    {
        "loss": 1.8359,
        "grad_norm": 2.9000067710876465,
        "learning_rate": 0.00015691427028780076,
        "epoch": 1.5394431554524362,
        "step": 11943
    },
    {
        "loss": 1.2594,
        "grad_norm": 3.2871158123016357,
        "learning_rate": 0.0001568642344666956,
        "epoch": 1.5395720546532612,
        "step": 11944
    },
    {
        "loss": 0.6608,
        "grad_norm": 2.4858906269073486,
        "learning_rate": 0.00015681417759702364,
        "epoch": 1.5397009538540862,
        "step": 11945
    },
    {
        "loss": 1.5843,
        "grad_norm": 3.1638433933258057,
        "learning_rate": 0.00015676409969731363,
        "epoch": 1.539829853054911,
        "step": 11946
    },
    {
        "loss": 1.4849,
        "grad_norm": 1.5421068668365479,
        "learning_rate": 0.00015671400078610205,
        "epoch": 1.5399587522557359,
        "step": 11947
    },
    {
        "loss": 1.9247,
        "grad_norm": 1.9301609992980957,
        "learning_rate": 0.00015666388088193348,
        "epoch": 1.540087651456561,
        "step": 11948
    },
    {
        "loss": 1.6269,
        "grad_norm": 2.672971248626709,
        "learning_rate": 0.0001566137400033599,
        "epoch": 1.540216550657386,
        "step": 11949
    },
    {
        "loss": 1.1176,
        "grad_norm": 4.445578098297119,
        "learning_rate": 0.00015656357816894127,
        "epoch": 1.540345449858211,
        "step": 11950
    },
    {
        "loss": 0.6006,
        "grad_norm": 2.903398275375366,
        "learning_rate": 0.00015651339539724513,
        "epoch": 1.5404743490590358,
        "step": 11951
    },
    {
        "loss": 2.4592,
        "grad_norm": 1.9727251529693604,
        "learning_rate": 0.00015646319170684695,
        "epoch": 1.5406032482598608,
        "step": 11952
    },
    {
        "loss": 1.5665,
        "grad_norm": 2.346345901489258,
        "learning_rate": 0.0001564129671163297,
        "epoch": 1.5407321474606857,
        "step": 11953
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.6863269805908203,
        "learning_rate": 0.00015636272164428454,
        "epoch": 1.5408610466615107,
        "step": 11954
    },
    {
        "loss": 2.0382,
        "grad_norm": 1.7459096908569336,
        "learning_rate": 0.00015631245530930987,
        "epoch": 1.5409899458623357,
        "step": 11955
    },
    {
        "loss": 1.7263,
        "grad_norm": 2.7078330516815186,
        "learning_rate": 0.00015626216813001209,
        "epoch": 1.5411188450631608,
        "step": 11956
    },
    {
        "loss": 2.1883,
        "grad_norm": 2.478549003601074,
        "learning_rate": 0.0001562118601250052,
        "epoch": 1.5412477442639856,
        "step": 11957
    },
    {
        "loss": 1.6884,
        "grad_norm": 2.932708978652954,
        "learning_rate": 0.00015616153131291097,
        "epoch": 1.5413766434648104,
        "step": 11958
    },
    {
        "loss": 1.7561,
        "grad_norm": 3.610349178314209,
        "learning_rate": 0.00015611118171235882,
        "epoch": 1.5415055426656354,
        "step": 11959
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.680903673171997,
        "learning_rate": 0.00015606081134198588,
        "epoch": 1.5416344418664605,
        "step": 11960
    },
    {
        "loss": 2.0718,
        "grad_norm": 1.8342161178588867,
        "learning_rate": 0.00015601042022043705,
        "epoch": 1.5417633410672855,
        "step": 11961
    },
    {
        "loss": 1.887,
        "grad_norm": 2.4207448959350586,
        "learning_rate": 0.00015596000836636483,
        "epoch": 1.5418922402681103,
        "step": 11962
    },
    {
        "loss": 0.8801,
        "grad_norm": 2.855746269226074,
        "learning_rate": 0.00015590957579842934,
        "epoch": 1.5420211394689352,
        "step": 11963
    },
    {
        "loss": 2.354,
        "grad_norm": 2.0031657218933105,
        "learning_rate": 0.00015585912253529837,
        "epoch": 1.5421500386697602,
        "step": 11964
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.461552619934082,
        "learning_rate": 0.00015580864859564766,
        "epoch": 1.5422789378705852,
        "step": 11965
    },
    {
        "loss": 1.7274,
        "grad_norm": 1.883270502090454,
        "learning_rate": 0.00015575815399816026,
        "epoch": 1.5424078370714103,
        "step": 11966
    },
    {
        "loss": 0.9263,
        "grad_norm": 2.0365781784057617,
        "learning_rate": 0.00015570763876152698,
        "epoch": 1.542536736272235,
        "step": 11967
    },
    {
        "loss": 1.6122,
        "grad_norm": 1.672174334526062,
        "learning_rate": 0.0001556571029044463,
        "epoch": 1.54266563547306,
        "step": 11968
    },
    {
        "loss": 1.6742,
        "grad_norm": 2.336965560913086,
        "learning_rate": 0.00015560654644562426,
        "epoch": 1.542794534673885,
        "step": 11969
    },
    {
        "loss": 1.2104,
        "grad_norm": 2.748328685760498,
        "learning_rate": 0.00015555596940377452,
        "epoch": 1.54292343387471,
        "step": 11970
    },
    {
        "loss": 2.0382,
        "grad_norm": 1.9995709657669067,
        "learning_rate": 0.0001555053717976186,
        "epoch": 1.543052333075535,
        "step": 11971
    },
    {
        "loss": 2.0727,
        "grad_norm": 1.2068769931793213,
        "learning_rate": 0.00015545475364588533,
        "epoch": 1.5431812322763598,
        "step": 11972
    },
    {
        "loss": 2.2293,
        "grad_norm": 2.1832807064056396,
        "learning_rate": 0.0001554041149673113,
        "epoch": 1.5433101314771849,
        "step": 11973
    },
    {
        "loss": 1.6583,
        "grad_norm": 3.0894925594329834,
        "learning_rate": 0.0001553534557806406,
        "epoch": 1.5434390306780097,
        "step": 11974
    },
    {
        "loss": 1.9987,
        "grad_norm": 2.0620474815368652,
        "learning_rate": 0.00015530277610462502,
        "epoch": 1.5435679298788347,
        "step": 11975
    },
    {
        "loss": 1.0213,
        "grad_norm": 4.038215160369873,
        "learning_rate": 0.00015525207595802382,
        "epoch": 1.5436968290796598,
        "step": 11976
    },
    {
        "loss": 2.2418,
        "grad_norm": 2.383538246154785,
        "learning_rate": 0.000155201355359604,
        "epoch": 1.5438257282804848,
        "step": 11977
    },
    {
        "loss": 1.3402,
        "grad_norm": 2.315869092941284,
        "learning_rate": 0.0001551506143281399,
        "epoch": 1.5439546274813096,
        "step": 11978
    },
    {
        "loss": 1.5051,
        "grad_norm": 2.5929245948791504,
        "learning_rate": 0.0001550998528824137,
        "epoch": 1.5440835266821344,
        "step": 11979
    },
    {
        "loss": 1.7602,
        "grad_norm": 2.181464672088623,
        "learning_rate": 0.00015504907104121486,
        "epoch": 1.5442124258829595,
        "step": 11980
    },
    {
        "loss": 1.8932,
        "grad_norm": 1.4211995601654053,
        "learning_rate": 0.00015499826882334048,
        "epoch": 1.5443413250837845,
        "step": 11981
    },
    {
        "loss": 1.479,
        "grad_norm": 2.597167491912842,
        "learning_rate": 0.00015494744624759544,
        "epoch": 1.5444702242846096,
        "step": 11982
    },
    {
        "loss": 1.7424,
        "grad_norm": 1.8848934173583984,
        "learning_rate": 0.0001548966033327919,
        "epoch": 1.5445991234854344,
        "step": 11983
    },
    {
        "loss": 1.7613,
        "grad_norm": 2.3525803089141846,
        "learning_rate": 0.0001548457400977495,
        "epoch": 1.5447280226862592,
        "step": 11984
    },
    {
        "loss": 1.6266,
        "grad_norm": 2.3106958866119385,
        "learning_rate": 0.0001547948565612956,
        "epoch": 1.5448569218870842,
        "step": 11985
    },
    {
        "loss": 1.2605,
        "grad_norm": 2.6741628646850586,
        "learning_rate": 0.00015474395274226493,
        "epoch": 1.5449858210879093,
        "step": 11986
    },
    {
        "loss": 1.4891,
        "grad_norm": 3.136896848678589,
        "learning_rate": 0.00015469302865949973,
        "epoch": 1.5451147202887343,
        "step": 11987
    },
    {
        "loss": 1.9583,
        "grad_norm": 2.4139785766601562,
        "learning_rate": 0.00015464208433185,
        "epoch": 1.5452436194895591,
        "step": 11988
    },
    {
        "loss": 1.7697,
        "grad_norm": 2.8270771503448486,
        "learning_rate": 0.00015459111977817285,
        "epoch": 1.5453725186903842,
        "step": 11989
    },
    {
        "loss": 1.5684,
        "grad_norm": 3.1463589668273926,
        "learning_rate": 0.00015454013501733308,
        "epoch": 1.545501417891209,
        "step": 11990
    },
    {
        "loss": 1.8642,
        "grad_norm": 1.9132038354873657,
        "learning_rate": 0.00015448913006820298,
        "epoch": 1.545630317092034,
        "step": 11991
    },
    {
        "loss": 1.7827,
        "grad_norm": 2.4146461486816406,
        "learning_rate": 0.00015443810494966226,
        "epoch": 1.545759216292859,
        "step": 11992
    },
    {
        "loss": 1.2108,
        "grad_norm": 2.0695760250091553,
        "learning_rate": 0.00015438705968059808,
        "epoch": 1.545888115493684,
        "step": 11993
    },
    {
        "loss": 2.3013,
        "grad_norm": 2.1702637672424316,
        "learning_rate": 0.00015433599427990512,
        "epoch": 1.546017014694509,
        "step": 11994
    },
    {
        "loss": 1.1695,
        "grad_norm": 3.0219178199768066,
        "learning_rate": 0.00015428490876648535,
        "epoch": 1.5461459138953337,
        "step": 11995
    },
    {
        "loss": 2.0685,
        "grad_norm": 1.7076220512390137,
        "learning_rate": 0.00015423380315924865,
        "epoch": 1.5462748130961588,
        "step": 11996
    },
    {
        "loss": 1.6091,
        "grad_norm": 2.806814193725586,
        "learning_rate": 0.0001541826774771117,
        "epoch": 1.5464037122969838,
        "step": 11997
    },
    {
        "loss": 1.982,
        "grad_norm": 3.840015411376953,
        "learning_rate": 0.00015413153173899887,
        "epoch": 1.5465326114978089,
        "step": 11998
    },
    {
        "loss": 1.5213,
        "grad_norm": 2.3046154975891113,
        "learning_rate": 0.00015408036596384225,
        "epoch": 1.5466615106986337,
        "step": 11999
    },
    {
        "loss": 0.8597,
        "grad_norm": 2.63822865486145,
        "learning_rate": 0.000154029180170581,
        "epoch": 1.5467904098994585,
        "step": 12000
    },
    {
        "loss": 0.8507,
        "grad_norm": 3.439345359802246,
        "learning_rate": 0.00015397797437816175,
        "epoch": 1.5469193091002835,
        "step": 12001
    },
    {
        "loss": 2.196,
        "grad_norm": 3.118758201599121,
        "learning_rate": 0.00015392674860553856,
        "epoch": 1.5470482083011086,
        "step": 12002
    },
    {
        "loss": 1.6828,
        "grad_norm": 3.1197805404663086,
        "learning_rate": 0.00015387550287167295,
        "epoch": 1.5471771075019336,
        "step": 12003
    },
    {
        "loss": 1.2685,
        "grad_norm": 3.698662042617798,
        "learning_rate": 0.00015382423719553358,
        "epoch": 1.5473060067027584,
        "step": 12004
    },
    {
        "loss": 1.7028,
        "grad_norm": 1.5815813541412354,
        "learning_rate": 0.00015377295159609697,
        "epoch": 1.5474349059035832,
        "step": 12005
    },
    {
        "loss": 1.8512,
        "grad_norm": 2.6629130840301514,
        "learning_rate": 0.00015372164609234662,
        "epoch": 1.5475638051044083,
        "step": 12006
    },
    {
        "loss": 1.2045,
        "grad_norm": 2.3509562015533447,
        "learning_rate": 0.00015367032070327344,
        "epoch": 1.5476927043052333,
        "step": 12007
    },
    {
        "loss": 1.6649,
        "grad_norm": 2.236518383026123,
        "learning_rate": 0.00015361897544787583,
        "epoch": 1.5478216035060584,
        "step": 12008
    },
    {
        "loss": 1.5894,
        "grad_norm": 2.0611190795898438,
        "learning_rate": 0.00015356761034515944,
        "epoch": 1.5479505027068832,
        "step": 12009
    },
    {
        "loss": 0.8277,
        "grad_norm": 2.614800453186035,
        "learning_rate": 0.00015351622541413736,
        "epoch": 1.5480794019077082,
        "step": 12010
    },
    {
        "loss": 1.687,
        "grad_norm": 2.8386130332946777,
        "learning_rate": 0.0001534648206738299,
        "epoch": 1.548208301108533,
        "step": 12011
    },
    {
        "loss": 1.7611,
        "grad_norm": 2.231393575668335,
        "learning_rate": 0.00015341339614326473,
        "epoch": 1.548337200309358,
        "step": 12012
    },
    {
        "loss": 1.7944,
        "grad_norm": 3.015349864959717,
        "learning_rate": 0.00015336195184147716,
        "epoch": 1.548466099510183,
        "step": 12013
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.5279688835144043,
        "learning_rate": 0.00015331048778750926,
        "epoch": 1.5485949987110081,
        "step": 12014
    },
    {
        "loss": 1.7956,
        "grad_norm": 2.182260036468506,
        "learning_rate": 0.00015325900400041062,
        "epoch": 1.548723897911833,
        "step": 12015
    },
    {
        "loss": 1.0307,
        "grad_norm": 2.7769885063171387,
        "learning_rate": 0.0001532075004992385,
        "epoch": 1.5488527971126578,
        "step": 12016
    },
    {
        "loss": 1.6131,
        "grad_norm": 2.592289686203003,
        "learning_rate": 0.00015315597730305704,
        "epoch": 1.5489816963134828,
        "step": 12017
    },
    {
        "loss": 1.8256,
        "grad_norm": 2.3966593742370605,
        "learning_rate": 0.00015310443443093764,
        "epoch": 1.5491105955143079,
        "step": 12018
    },
    {
        "loss": 1.5858,
        "grad_norm": 2.460068941116333,
        "learning_rate": 0.00015305287190195955,
        "epoch": 1.549239494715133,
        "step": 12019
    },
    {
        "loss": 0.8031,
        "grad_norm": 2.802560806274414,
        "learning_rate": 0.00015300128973520848,
        "epoch": 1.5493683939159577,
        "step": 12020
    },
    {
        "loss": 0.5577,
        "grad_norm": 2.383655071258545,
        "learning_rate": 0.0001529496879497778,
        "epoch": 1.5494972931167825,
        "step": 12021
    },
    {
        "loss": 1.3743,
        "grad_norm": 1.9412132501602173,
        "learning_rate": 0.00015289806656476848,
        "epoch": 1.5496261923176076,
        "step": 12022
    },
    {
        "loss": 1.737,
        "grad_norm": 1.907106876373291,
        "learning_rate": 0.00015284642559928825,
        "epoch": 1.5497550915184326,
        "step": 12023
    },
    {
        "loss": 1.5627,
        "grad_norm": 3.436546564102173,
        "learning_rate": 0.00015279476507245224,
        "epoch": 1.5498839907192576,
        "step": 12024
    },
    {
        "loss": 0.9397,
        "grad_norm": 3.3846065998077393,
        "learning_rate": 0.0001527430850033829,
        "epoch": 1.5500128899200825,
        "step": 12025
    },
    {
        "loss": 1.9661,
        "grad_norm": 3.348562240600586,
        "learning_rate": 0.0001526913854112098,
        "epoch": 1.5501417891209075,
        "step": 12026
    },
    {
        "loss": 1.4901,
        "grad_norm": 2.5885331630706787,
        "learning_rate": 0.00015263966631506976,
        "epoch": 1.5502706883217323,
        "step": 12027
    },
    {
        "loss": 1.9167,
        "grad_norm": 2.0594000816345215,
        "learning_rate": 0.00015258792773410694,
        "epoch": 1.5503995875225574,
        "step": 12028
    },
    {
        "loss": 2.2984,
        "grad_norm": 1.8798081874847412,
        "learning_rate": 0.00015253616968747245,
        "epoch": 1.5505284867233824,
        "step": 12029
    },
    {
        "loss": 2.0424,
        "grad_norm": 1.9937857389450073,
        "learning_rate": 0.00015248439219432512,
        "epoch": 1.5506573859242074,
        "step": 12030
    },
    {
        "loss": 2.0566,
        "grad_norm": 1.8752633333206177,
        "learning_rate": 0.00015243259527383032,
        "epoch": 1.5507862851250322,
        "step": 12031
    },
    {
        "loss": 2.1627,
        "grad_norm": 2.611274242401123,
        "learning_rate": 0.00015238077894516087,
        "epoch": 1.550915184325857,
        "step": 12032
    },
    {
        "loss": 1.502,
        "grad_norm": 2.6197471618652344,
        "learning_rate": 0.00015232894322749716,
        "epoch": 1.551044083526682,
        "step": 12033
    },
    {
        "loss": 2.0917,
        "grad_norm": 2.044559955596924,
        "learning_rate": 0.0001522770881400262,
        "epoch": 1.5511729827275071,
        "step": 12034
    },
    {
        "loss": 1.643,
        "grad_norm": 2.0999550819396973,
        "learning_rate": 0.00015222521370194237,
        "epoch": 1.5513018819283322,
        "step": 12035
    },
    {
        "loss": 2.0019,
        "grad_norm": 2.2486493587493896,
        "learning_rate": 0.00015217331993244755,
        "epoch": 1.551430781129157,
        "step": 12036
    },
    {
        "loss": 0.9098,
        "grad_norm": 2.4193646907806396,
        "learning_rate": 0.00015212140685075013,
        "epoch": 1.5515596803299818,
        "step": 12037
    },
    {
        "loss": 2.2596,
        "grad_norm": 2.7697689533233643,
        "learning_rate": 0.00015206947447606592,
        "epoch": 1.5516885795308069,
        "step": 12038
    },
    {
        "loss": 1.3004,
        "grad_norm": 2.4803755283355713,
        "learning_rate": 0.00015201752282761825,
        "epoch": 1.551817478731632,
        "step": 12039
    },
    {
        "loss": 0.9888,
        "grad_norm": 3.0878047943115234,
        "learning_rate": 0.0001519655519246371,
        "epoch": 1.551946377932457,
        "step": 12040
    },
    {
        "loss": 0.9653,
        "grad_norm": 2.829245090484619,
        "learning_rate": 0.0001519135617863598,
        "epoch": 1.5520752771332817,
        "step": 12041
    },
    {
        "loss": 1.4475,
        "grad_norm": 3.382096290588379,
        "learning_rate": 0.00015186155243203072,
        "epoch": 1.5522041763341066,
        "step": 12042
    },
    {
        "loss": 0.3918,
        "grad_norm": 2.5013420581817627,
        "learning_rate": 0.00015180952388090134,
        "epoch": 1.5523330755349316,
        "step": 12043
    },
    {
        "loss": 1.2797,
        "grad_norm": 3.6528940200805664,
        "learning_rate": 0.00015175747615223033,
        "epoch": 1.5524619747357566,
        "step": 12044
    },
    {
        "loss": 1.3788,
        "grad_norm": 2.3408455848693848,
        "learning_rate": 0.0001517054092652834,
        "epoch": 1.5525908739365817,
        "step": 12045
    },
    {
        "loss": 1.5204,
        "grad_norm": 2.327221632003784,
        "learning_rate": 0.00015165332323933316,
        "epoch": 1.5527197731374065,
        "step": 12046
    },
    {
        "loss": 2.2049,
        "grad_norm": 2.9161603450775146,
        "learning_rate": 0.00015160121809365995,
        "epoch": 1.5528486723382315,
        "step": 12047
    },
    {
        "loss": 2.0379,
        "grad_norm": 2.6082444190979004,
        "learning_rate": 0.00015154909384755032,
        "epoch": 1.5529775715390564,
        "step": 12048
    },
    {
        "loss": 0.67,
        "grad_norm": 3.758962631225586,
        "learning_rate": 0.00015149695052029834,
        "epoch": 1.5531064707398814,
        "step": 12049
    },
    {
        "loss": 1.7828,
        "grad_norm": 1.96279776096344,
        "learning_rate": 0.00015144478813120533,
        "epoch": 1.5532353699407064,
        "step": 12050
    },
    {
        "loss": 1.6936,
        "grad_norm": 3.137324810028076,
        "learning_rate": 0.00015139260669957933,
        "epoch": 1.5533642691415315,
        "step": 12051
    },
    {
        "loss": 2.0714,
        "grad_norm": 1.5819350481033325,
        "learning_rate": 0.00015134040624473544,
        "epoch": 1.5534931683423563,
        "step": 12052
    },
    {
        "loss": 2.1318,
        "grad_norm": 1.6290457248687744,
        "learning_rate": 0.00015128818678599628,
        "epoch": 1.553622067543181,
        "step": 12053
    },
    {
        "loss": 1.6674,
        "grad_norm": 3.7611706256866455,
        "learning_rate": 0.0001512359483426907,
        "epoch": 1.5537509667440061,
        "step": 12054
    },
    {
        "loss": 1.3372,
        "grad_norm": 3.2165987491607666,
        "learning_rate": 0.00015118369093415512,
        "epoch": 1.5538798659448312,
        "step": 12055
    },
    {
        "loss": 2.2092,
        "grad_norm": 2.214658260345459,
        "learning_rate": 0.000151131414579733,
        "epoch": 1.5540087651456562,
        "step": 12056
    },
    {
        "loss": 1.4387,
        "grad_norm": 2.771636962890625,
        "learning_rate": 0.00015107911929877462,
        "epoch": 1.554137664346481,
        "step": 12057
    },
    {
        "loss": 1.7968,
        "grad_norm": 2.3909640312194824,
        "learning_rate": 0.00015102680511063733,
        "epoch": 1.5542665635473059,
        "step": 12058
    },
    {
        "loss": 1.8905,
        "grad_norm": 1.8982412815093994,
        "learning_rate": 0.0001509744720346854,
        "epoch": 1.554395462748131,
        "step": 12059
    },
    {
        "loss": 1.5834,
        "grad_norm": 2.4278550148010254,
        "learning_rate": 0.00015092212009029028,
        "epoch": 1.554524361948956,
        "step": 12060
    },
    {
        "loss": 1.6348,
        "grad_norm": 2.654877185821533,
        "learning_rate": 0.0001508697492968302,
        "epoch": 1.554653261149781,
        "step": 12061
    },
    {
        "loss": 1.7412,
        "grad_norm": 2.011103391647339,
        "learning_rate": 0.00015081735967369055,
        "epoch": 1.5547821603506058,
        "step": 12062
    },
    {
        "loss": 1.8492,
        "grad_norm": 3.542041778564453,
        "learning_rate": 0.00015076495124026338,
        "epoch": 1.5549110595514308,
        "step": 12063
    },
    {
        "loss": 0.9746,
        "grad_norm": 2.9961538314819336,
        "learning_rate": 0.00015071252401594834,
        "epoch": 1.5550399587522556,
        "step": 12064
    },
    {
        "loss": 1.4551,
        "grad_norm": 2.665360689163208,
        "learning_rate": 0.00015066007802015124,
        "epoch": 1.5551688579530807,
        "step": 12065
    },
    {
        "loss": 1.2059,
        "grad_norm": 2.4919304847717285,
        "learning_rate": 0.0001506076132722852,
        "epoch": 1.5552977571539057,
        "step": 12066
    },
    {
        "loss": 2.1836,
        "grad_norm": 2.213087320327759,
        "learning_rate": 0.00015055512979177056,
        "epoch": 1.5554266563547308,
        "step": 12067
    },
    {
        "loss": 1.3008,
        "grad_norm": 1.8484630584716797,
        "learning_rate": 0.00015050262759803417,
        "epoch": 1.5555555555555556,
        "step": 12068
    },
    {
        "loss": 1.1641,
        "grad_norm": 2.1379494667053223,
        "learning_rate": 0.00015045010671050992,
        "epoch": 1.5556844547563804,
        "step": 12069
    },
    {
        "loss": 1.5532,
        "grad_norm": 2.359128952026367,
        "learning_rate": 0.00015039756714863898,
        "epoch": 1.5558133539572054,
        "step": 12070
    },
    {
        "loss": 2.0458,
        "grad_norm": 2.0858302116394043,
        "learning_rate": 0.00015034500893186875,
        "epoch": 1.5559422531580305,
        "step": 12071
    },
    {
        "loss": 1.4407,
        "grad_norm": 1.9414070844650269,
        "learning_rate": 0.00015029243207965393,
        "epoch": 1.5560711523588555,
        "step": 12072
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.8493510484695435,
        "learning_rate": 0.0001502398366114563,
        "epoch": 1.5562000515596803,
        "step": 12073
    },
    {
        "loss": 1.7324,
        "grad_norm": 2.1100831031799316,
        "learning_rate": 0.00015018722254674422,
        "epoch": 1.5563289507605051,
        "step": 12074
    },
    {
        "loss": 0.827,
        "grad_norm": 1.9660526514053345,
        "learning_rate": 0.00015013458990499308,
        "epoch": 1.5564578499613302,
        "step": 12075
    },
    {
        "loss": 1.9091,
        "grad_norm": 2.7035653591156006,
        "learning_rate": 0.000150081938705685,
        "epoch": 1.5565867491621552,
        "step": 12076
    },
    {
        "loss": 1.7826,
        "grad_norm": 2.512307643890381,
        "learning_rate": 0.00015002926896830917,
        "epoch": 1.5567156483629803,
        "step": 12077
    },
    {
        "loss": 1.8326,
        "grad_norm": 1.459963083267212,
        "learning_rate": 0.00014997658071236151,
        "epoch": 1.556844547563805,
        "step": 12078
    },
    {
        "loss": 2.2653,
        "grad_norm": 3.003312110900879,
        "learning_rate": 0.0001499238739573448,
        "epoch": 1.55697344676463,
        "step": 12079
    },
    {
        "loss": 1.8148,
        "grad_norm": 2.018022298812866,
        "learning_rate": 0.00014987114872276863,
        "epoch": 1.557102345965455,
        "step": 12080
    },
    {
        "loss": 1.6675,
        "grad_norm": 3.1166164875030518,
        "learning_rate": 0.00014981840502814966,
        "epoch": 1.55723124516628,
        "step": 12081
    },
    {
        "loss": 1.8386,
        "grad_norm": 2.900306224822998,
        "learning_rate": 0.0001497656428930113,
        "epoch": 1.557360144367105,
        "step": 12082
    },
    {
        "loss": 2.3268,
        "grad_norm": 1.9930369853973389,
        "learning_rate": 0.0001497128623368833,
        "epoch": 1.5574890435679298,
        "step": 12083
    },
    {
        "loss": 2.1355,
        "grad_norm": 1.853164792060852,
        "learning_rate": 0.000149660063379303,
        "epoch": 1.5576179427687549,
        "step": 12084
    },
    {
        "loss": 1.8204,
        "grad_norm": 2.7251250743865967,
        "learning_rate": 0.0001496072460398141,
        "epoch": 1.5577468419695797,
        "step": 12085
    },
    {
        "loss": 1.1805,
        "grad_norm": 1.8350536823272705,
        "learning_rate": 0.00014955441033796699,
        "epoch": 1.5578757411704047,
        "step": 12086
    },
    {
        "loss": 1.7094,
        "grad_norm": 1.883552074432373,
        "learning_rate": 0.0001495015562933195,
        "epoch": 1.5580046403712298,
        "step": 12087
    },
    {
        "loss": 1.2394,
        "grad_norm": 2.7570698261260986,
        "learning_rate": 0.00014944868392543543,
        "epoch": 1.5581335395720548,
        "step": 12088
    },
    {
        "loss": 1.8179,
        "grad_norm": 1.8878213167190552,
        "learning_rate": 0.00014939579325388568,
        "epoch": 1.5582624387728796,
        "step": 12089
    },
    {
        "loss": 1.8125,
        "grad_norm": 2.1185367107391357,
        "learning_rate": 0.00014934288429824827,
        "epoch": 1.5583913379737044,
        "step": 12090
    },
    {
        "loss": 1.9824,
        "grad_norm": 1.787459373474121,
        "learning_rate": 0.00014928995707810762,
        "epoch": 1.5585202371745295,
        "step": 12091
    },
    {
        "loss": 1.597,
        "grad_norm": 2.880178689956665,
        "learning_rate": 0.0001492370116130549,
        "epoch": 1.5586491363753545,
        "step": 12092
    },
    {
        "loss": 1.0782,
        "grad_norm": 3.573355197906494,
        "learning_rate": 0.00014918404792268818,
        "epoch": 1.5587780355761796,
        "step": 12093
    },
    {
        "loss": 1.7939,
        "grad_norm": 2.094238519668579,
        "learning_rate": 0.00014913106602661223,
        "epoch": 1.5589069347770044,
        "step": 12094
    },
    {
        "loss": 2.0012,
        "grad_norm": 2.667482852935791,
        "learning_rate": 0.00014907806594443855,
        "epoch": 1.5590358339778292,
        "step": 12095
    },
    {
        "loss": 1.5528,
        "grad_norm": 2.4690895080566406,
        "learning_rate": 0.0001490250476957853,
        "epoch": 1.5591647331786542,
        "step": 12096
    },
    {
        "loss": 1.1326,
        "grad_norm": 3.715141773223877,
        "learning_rate": 0.00014897201130027746,
        "epoch": 1.5592936323794793,
        "step": 12097
    },
    {
        "loss": 1.9682,
        "grad_norm": 2.627103090286255,
        "learning_rate": 0.00014891895677754678,
        "epoch": 1.5594225315803043,
        "step": 12098
    },
    {
        "loss": 0.6258,
        "grad_norm": 4.702762603759766,
        "learning_rate": 0.00014886588414723178,
        "epoch": 1.5595514307811291,
        "step": 12099
    },
    {
        "loss": 1.2064,
        "grad_norm": 2.85546875,
        "learning_rate": 0.0001488127934289771,
        "epoch": 1.5596803299819542,
        "step": 12100
    },
    {
        "loss": 1.5319,
        "grad_norm": 3.212381601333618,
        "learning_rate": 0.0001487596846424349,
        "epoch": 1.559809229182779,
        "step": 12101
    },
    {
        "loss": 2.0607,
        "grad_norm": 3.1817190647125244,
        "learning_rate": 0.0001487065578072636,
        "epoch": 1.559938128383604,
        "step": 12102
    },
    {
        "loss": 2.163,
        "grad_norm": 1.7102993726730347,
        "learning_rate": 0.00014865341294312813,
        "epoch": 1.560067027584429,
        "step": 12103
    },
    {
        "loss": 1.831,
        "grad_norm": 1.866898536682129,
        "learning_rate": 0.00014860025006970069,
        "epoch": 1.560195926785254,
        "step": 12104
    },
    {
        "loss": 2.0556,
        "grad_norm": 3.171304225921631,
        "learning_rate": 0.00014854706920665949,
        "epoch": 1.560324825986079,
        "step": 12105
    },
    {
        "loss": 2.3323,
        "grad_norm": 2.342797040939331,
        "learning_rate": 0.00014849387037368957,
        "epoch": 1.5604537251869037,
        "step": 12106
    },
    {
        "loss": 1.2435,
        "grad_norm": 2.3102047443389893,
        "learning_rate": 0.00014844065359048307,
        "epoch": 1.5605826243877288,
        "step": 12107
    },
    {
        "loss": 1.8205,
        "grad_norm": 2.809753179550171,
        "learning_rate": 0.0001483874188767383,
        "epoch": 1.5607115235885538,
        "step": 12108
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.8063253164291382,
        "learning_rate": 0.00014833416625216022,
        "epoch": 1.5608404227893788,
        "step": 12109
    },
    {
        "loss": 1.9695,
        "grad_norm": 1.6864233016967773,
        "learning_rate": 0.0001482808957364607,
        "epoch": 1.5609693219902037,
        "step": 12110
    },
    {
        "loss": 1.3199,
        "grad_norm": 2.69281005859375,
        "learning_rate": 0.00014822760734935802,
        "epoch": 1.5610982211910285,
        "step": 12111
    },
    {
        "loss": 2.097,
        "grad_norm": 1.5076169967651367,
        "learning_rate": 0.00014817430111057714,
        "epoch": 1.5612271203918535,
        "step": 12112
    },
    {
        "loss": 1.1938,
        "grad_norm": 2.661242961883545,
        "learning_rate": 0.00014812097703984956,
        "epoch": 1.5613560195926786,
        "step": 12113
    },
    {
        "loss": 1.4099,
        "grad_norm": 2.5355772972106934,
        "learning_rate": 0.00014806763515691336,
        "epoch": 1.5614849187935036,
        "step": 12114
    },
    {
        "loss": 1.3459,
        "grad_norm": 1.4830312728881836,
        "learning_rate": 0.00014801427548151355,
        "epoch": 1.5616138179943284,
        "step": 12115
    },
    {
        "loss": 1.6841,
        "grad_norm": 2.2707529067993164,
        "learning_rate": 0.00014796089803340143,
        "epoch": 1.5617427171951532,
        "step": 12116
    },
    {
        "loss": 2.4415,
        "grad_norm": 2.1705610752105713,
        "learning_rate": 0.00014790750283233457,
        "epoch": 1.5618716163959783,
        "step": 12117
    },
    {
        "loss": 1.7706,
        "grad_norm": 2.848466157913208,
        "learning_rate": 0.00014785408989807783,
        "epoch": 1.5620005155968033,
        "step": 12118
    },
    {
        "loss": 1.7657,
        "grad_norm": 1.9501827955245972,
        "learning_rate": 0.00014780065925040217,
        "epoch": 1.5621294147976283,
        "step": 12119
    },
    {
        "loss": 2.22,
        "grad_norm": 1.395011067390442,
        "learning_rate": 0.00014774721090908498,
        "epoch": 1.5622583139984532,
        "step": 12120
    },
    {
        "loss": 1.0707,
        "grad_norm": 3.2098069190979004,
        "learning_rate": 0.00014769374489391075,
        "epoch": 1.5623872131992782,
        "step": 12121
    },
    {
        "loss": 1.227,
        "grad_norm": 2.184915542602539,
        "learning_rate": 0.00014764026122467014,
        "epoch": 1.562516112400103,
        "step": 12122
    },
    {
        "loss": 1.8302,
        "grad_norm": 2.105881690979004,
        "learning_rate": 0.00014758675992116005,
        "epoch": 1.562645011600928,
        "step": 12123
    },
    {
        "loss": 1.7533,
        "grad_norm": 2.603205919265747,
        "learning_rate": 0.00014753324100318457,
        "epoch": 1.562773910801753,
        "step": 12124
    },
    {
        "loss": 1.6287,
        "grad_norm": 1.7170261144638062,
        "learning_rate": 0.0001474797044905539,
        "epoch": 1.5629028100025781,
        "step": 12125
    },
    {
        "loss": 1.6018,
        "grad_norm": 2.9475014209747314,
        "learning_rate": 0.00014742615040308477,
        "epoch": 1.563031709203403,
        "step": 12126
    },
    {
        "loss": 2.1412,
        "grad_norm": 2.145562171936035,
        "learning_rate": 0.00014737257876060053,
        "epoch": 1.5631606084042278,
        "step": 12127
    },
    {
        "loss": 1.7974,
        "grad_norm": 2.8418679237365723,
        "learning_rate": 0.000147318989582931,
        "epoch": 1.5632895076050528,
        "step": 12128
    },
    {
        "loss": 2.2564,
        "grad_norm": 1.7322617769241333,
        "learning_rate": 0.00014726538288991242,
        "epoch": 1.5634184068058778,
        "step": 12129
    },
    {
        "loss": 2.3985,
        "grad_norm": 2.203585386276245,
        "learning_rate": 0.0001472117587013876,
        "epoch": 1.5635473060067029,
        "step": 12130
    },
    {
        "loss": 1.4345,
        "grad_norm": 2.3233373165130615,
        "learning_rate": 0.0001471581170372056,
        "epoch": 1.5636762052075277,
        "step": 12131
    },
    {
        "loss": 1.8823,
        "grad_norm": 2.165496826171875,
        "learning_rate": 0.00014710445791722248,
        "epoch": 1.5638051044083525,
        "step": 12132
    },
    {
        "loss": 1.2852,
        "grad_norm": 2.5551064014434814,
        "learning_rate": 0.00014705078136130037,
        "epoch": 1.5639340036091776,
        "step": 12133
    },
    {
        "loss": 2.0584,
        "grad_norm": 2.419828414916992,
        "learning_rate": 0.00014699708738930752,
        "epoch": 1.5640629028100026,
        "step": 12134
    },
    {
        "loss": 2.0812,
        "grad_norm": 1.4509661197662354,
        "learning_rate": 0.00014694337602111937,
        "epoch": 1.5641918020108276,
        "step": 12135
    },
    {
        "loss": 1.9564,
        "grad_norm": 1.7222751379013062,
        "learning_rate": 0.00014688964727661735,
        "epoch": 1.5643207012116525,
        "step": 12136
    },
    {
        "loss": 1.0403,
        "grad_norm": 2.4919400215148926,
        "learning_rate": 0.00014683590117568927,
        "epoch": 1.5644496004124775,
        "step": 12137
    },
    {
        "loss": 1.5106,
        "grad_norm": 1.7235007286071777,
        "learning_rate": 0.00014678213773822976,
        "epoch": 1.5645784996133023,
        "step": 12138
    },
    {
        "loss": 1.6166,
        "grad_norm": 2.738959312438965,
        "learning_rate": 0.00014672835698413962,
        "epoch": 1.5647073988141273,
        "step": 12139
    },
    {
        "loss": 1.2514,
        "grad_norm": 2.937403678894043,
        "learning_rate": 0.00014667455893332563,
        "epoch": 1.5648362980149524,
        "step": 12140
    },
    {
        "loss": 1.4152,
        "grad_norm": 3.557934522628784,
        "learning_rate": 0.00014662074360570184,
        "epoch": 1.5649651972157774,
        "step": 12141
    },
    {
        "loss": 2.097,
        "grad_norm": 2.8928704261779785,
        "learning_rate": 0.00014656691102118806,
        "epoch": 1.5650940964166022,
        "step": 12142
    },
    {
        "loss": 1.8502,
        "grad_norm": 2.639110565185547,
        "learning_rate": 0.00014651306119971074,
        "epoch": 1.565222995617427,
        "step": 12143
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.489670515060425,
        "learning_rate": 0.0001464591941612026,
        "epoch": 1.565351894818252,
        "step": 12144
    },
    {
        "loss": 1.7935,
        "grad_norm": 3.8579623699188232,
        "learning_rate": 0.00014640530992560276,
        "epoch": 1.5654807940190771,
        "step": 12145
    },
    {
        "loss": 2.198,
        "grad_norm": 2.7442140579223633,
        "learning_rate": 0.0001463514085128568,
        "epoch": 1.5656096932199022,
        "step": 12146
    },
    {
        "loss": 1.3374,
        "grad_norm": 2.872303009033203,
        "learning_rate": 0.00014629748994291654,
        "epoch": 1.565738592420727,
        "step": 12147
    },
    {
        "loss": 1.5785,
        "grad_norm": 2.479386806488037,
        "learning_rate": 0.00014624355423574007,
        "epoch": 1.5658674916215518,
        "step": 12148
    },
    {
        "loss": 1.7966,
        "grad_norm": 1.79329514503479,
        "learning_rate": 0.0001461896014112922,
        "epoch": 1.5659963908223768,
        "step": 12149
    },
    {
        "loss": 1.7276,
        "grad_norm": 2.279449701309204,
        "learning_rate": 0.00014613563148954383,
        "epoch": 1.5661252900232019,
        "step": 12150
    },
    {
        "loss": 1.3948,
        "grad_norm": 2.774973154067993,
        "learning_rate": 0.0001460816444904718,
        "epoch": 1.566254189224027,
        "step": 12151
    },
    {
        "loss": 1.4856,
        "grad_norm": 2.9641129970550537,
        "learning_rate": 0.00014602764043406005,
        "epoch": 1.5663830884248517,
        "step": 12152
    },
    {
        "loss": 1.8283,
        "grad_norm": 2.7895684242248535,
        "learning_rate": 0.00014597361934029824,
        "epoch": 1.5665119876256766,
        "step": 12153
    },
    {
        "loss": 1.2515,
        "grad_norm": 3.4772555828094482,
        "learning_rate": 0.00014591958122918254,
        "epoch": 1.5666408868265016,
        "step": 12154
    },
    {
        "loss": 1.7438,
        "grad_norm": 2.6085333824157715,
        "learning_rate": 0.00014586552612071558,
        "epoch": 1.5667697860273266,
        "step": 12155
    },
    {
        "loss": 2.1422,
        "grad_norm": 1.480851173400879,
        "learning_rate": 0.00014581145403490608,
        "epoch": 1.5668986852281517,
        "step": 12156
    },
    {
        "loss": 1.5604,
        "grad_norm": 2.073775053024292,
        "learning_rate": 0.00014575736499176882,
        "epoch": 1.5670275844289765,
        "step": 12157
    },
    {
        "loss": 1.8611,
        "grad_norm": 1.9362571239471436,
        "learning_rate": 0.00014570325901132537,
        "epoch": 1.5671564836298015,
        "step": 12158
    },
    {
        "loss": 2.0228,
        "grad_norm": 1.3602007627487183,
        "learning_rate": 0.00014564913611360327,
        "epoch": 1.5672853828306264,
        "step": 12159
    },
    {
        "loss": 1.3526,
        "grad_norm": 3.219740390777588,
        "learning_rate": 0.00014559499631863635,
        "epoch": 1.5674142820314514,
        "step": 12160
    },
    {
        "loss": 1.943,
        "grad_norm": 1.9348994493484497,
        "learning_rate": 0.00014554083964646468,
        "epoch": 1.5675431812322764,
        "step": 12161
    },
    {
        "loss": 1.6159,
        "grad_norm": 4.10063362121582,
        "learning_rate": 0.00014548666611713463,
        "epoch": 1.5676720804331015,
        "step": 12162
    },
    {
        "loss": 1.2547,
        "grad_norm": 3.7768800258636475,
        "learning_rate": 0.00014543247575069882,
        "epoch": 1.5678009796339263,
        "step": 12163
    },
    {
        "loss": 0.6598,
        "grad_norm": 3.851895570755005,
        "learning_rate": 0.00014537826856721604,
        "epoch": 1.567929878834751,
        "step": 12164
    },
    {
        "loss": 2.0151,
        "grad_norm": 2.4819414615631104,
        "learning_rate": 0.0001453240445867512,
        "epoch": 1.5680587780355761,
        "step": 12165
    },
    {
        "loss": 1.3886,
        "grad_norm": 3.141500949859619,
        "learning_rate": 0.00014526980382937587,
        "epoch": 1.5681876772364012,
        "step": 12166
    },
    {
        "loss": 1.3196,
        "grad_norm": 3.4231183528900146,
        "learning_rate": 0.00014521554631516747,
        "epoch": 1.5683165764372262,
        "step": 12167
    },
    {
        "loss": 2.4211,
        "grad_norm": 2.216942071914673,
        "learning_rate": 0.0001451612720642093,
        "epoch": 1.568445475638051,
        "step": 12168
    },
    {
        "loss": 1.5372,
        "grad_norm": 1.4145804643630981,
        "learning_rate": 0.00014510698109659165,
        "epoch": 1.5685743748388759,
        "step": 12169
    },
    {
        "loss": 2.1614,
        "grad_norm": 3.4171299934387207,
        "learning_rate": 0.0001450526734324104,
        "epoch": 1.568703274039701,
        "step": 12170
    },
    {
        "loss": 2.0919,
        "grad_norm": 2.7603254318237305,
        "learning_rate": 0.00014499834909176772,
        "epoch": 1.568832173240526,
        "step": 12171
    },
    {
        "loss": 2.1084,
        "grad_norm": 1.6564877033233643,
        "learning_rate": 0.0001449440080947722,
        "epoch": 1.568961072441351,
        "step": 12172
    },
    {
        "loss": 1.8095,
        "grad_norm": 1.6705518960952759,
        "learning_rate": 0.00014488965046153854,
        "epoch": 1.5690899716421758,
        "step": 12173
    },
    {
        "loss": 1.2035,
        "grad_norm": 2.341851234436035,
        "learning_rate": 0.000144835276212187,
        "epoch": 1.5692188708430008,
        "step": 12174
    },
    {
        "loss": 1.1759,
        "grad_norm": 2.8648808002471924,
        "learning_rate": 0.00014478088536684487,
        "epoch": 1.5693477700438256,
        "step": 12175
    },
    {
        "loss": 1.2076,
        "grad_norm": 2.0547053813934326,
        "learning_rate": 0.00014472647794564508,
        "epoch": 1.5694766692446507,
        "step": 12176
    },
    {
        "loss": 1.3987,
        "grad_norm": 2.7392585277557373,
        "learning_rate": 0.00014467205396872674,
        "epoch": 1.5696055684454757,
        "step": 12177
    },
    {
        "loss": 0.8928,
        "grad_norm": 3.222700595855713,
        "learning_rate": 0.00014461761345623515,
        "epoch": 1.5697344676463008,
        "step": 12178
    },
    {
        "loss": 1.3952,
        "grad_norm": 3.0851383209228516,
        "learning_rate": 0.0001445631564283218,
        "epoch": 1.5698633668471256,
        "step": 12179
    },
    {
        "loss": 1.7413,
        "grad_norm": 2.0577199459075928,
        "learning_rate": 0.00014450868290514414,
        "epoch": 1.5699922660479504,
        "step": 12180
    },
    {
        "loss": 1.7541,
        "grad_norm": 2.2603063583374023,
        "learning_rate": 0.00014445419290686582,
        "epoch": 1.5701211652487754,
        "step": 12181
    },
    {
        "loss": 1.2827,
        "grad_norm": 3.240309238433838,
        "learning_rate": 0.0001443996864536565,
        "epoch": 1.5702500644496005,
        "step": 12182
    },
    {
        "loss": 1.9741,
        "grad_norm": 2.1611592769622803,
        "learning_rate": 0.00014434516356569217,
        "epoch": 1.5703789636504255,
        "step": 12183
    },
    {
        "loss": 1.678,
        "grad_norm": 2.023834228515625,
        "learning_rate": 0.0001442906242631547,
        "epoch": 1.5705078628512503,
        "step": 12184
    },
    {
        "loss": 2.1179,
        "grad_norm": 2.4566738605499268,
        "learning_rate": 0.000144236068566232,
        "epoch": 1.5706367620520751,
        "step": 12185
    },
    {
        "loss": 0.9349,
        "grad_norm": 3.6155831813812256,
        "learning_rate": 0.0001441814964951181,
        "epoch": 1.5707656612529002,
        "step": 12186
    },
    {
        "loss": 1.7755,
        "grad_norm": 1.7858028411865234,
        "learning_rate": 0.00014412690807001324,
        "epoch": 1.5708945604537252,
        "step": 12187
    },
    {
        "loss": 1.6199,
        "grad_norm": 2.140303134918213,
        "learning_rate": 0.00014407230331112334,
        "epoch": 1.5710234596545503,
        "step": 12188
    },
    {
        "loss": 1.618,
        "grad_norm": 2.3719778060913086,
        "learning_rate": 0.00014401768223866088,
        "epoch": 1.571152358855375,
        "step": 12189
    },
    {
        "loss": 1.8568,
        "grad_norm": 1.9451340436935425,
        "learning_rate": 0.00014396304487284415,
        "epoch": 1.5712812580562,
        "step": 12190
    },
    {
        "loss": 1.1257,
        "grad_norm": 2.458796739578247,
        "learning_rate": 0.00014390839123389702,
        "epoch": 1.571410157257025,
        "step": 12191
    },
    {
        "loss": 1.6634,
        "grad_norm": 2.2141215801239014,
        "learning_rate": 0.00014385372134205019,
        "epoch": 1.57153905645785,
        "step": 12192
    },
    {
        "loss": 0.9618,
        "grad_norm": 3.3281195163726807,
        "learning_rate": 0.00014379903521753986,
        "epoch": 1.571667955658675,
        "step": 12193
    },
    {
        "loss": 1.6827,
        "grad_norm": 3.2628417015075684,
        "learning_rate": 0.00014374433288060834,
        "epoch": 1.5717968548594998,
        "step": 12194
    },
    {
        "loss": 1.651,
        "grad_norm": 1.4888527393341064,
        "learning_rate": 0.00014368961435150395,
        "epoch": 1.5719257540603249,
        "step": 12195
    },
    {
        "loss": 2.524,
        "grad_norm": 2.146436929702759,
        "learning_rate": 0.00014363487965048105,
        "epoch": 1.5720546532611497,
        "step": 12196
    },
    {
        "loss": 1.4307,
        "grad_norm": 3.9070522785186768,
        "learning_rate": 0.00014358012879779993,
        "epoch": 1.5721835524619747,
        "step": 12197
    },
    {
        "loss": 1.7369,
        "grad_norm": 3.137232542037964,
        "learning_rate": 0.00014352536181372688,
        "epoch": 1.5723124516627998,
        "step": 12198
    },
    {
        "loss": 1.6444,
        "grad_norm": 2.677137613296509,
        "learning_rate": 0.00014347057871853404,
        "epoch": 1.5724413508636248,
        "step": 12199
    },
    {
        "loss": 1.8958,
        "grad_norm": 2.068751096725464,
        "learning_rate": 0.00014341577953249984,
        "epoch": 1.5725702500644496,
        "step": 12200
    },
    {
        "loss": 1.9368,
        "grad_norm": 1.9299408197402954,
        "learning_rate": 0.00014336096427590843,
        "epoch": 1.5726991492652744,
        "step": 12201
    },
    {
        "loss": 1.6418,
        "grad_norm": 1.6847742795944214,
        "learning_rate": 0.00014330613296904983,
        "epoch": 1.5728280484660995,
        "step": 12202
    },
    {
        "loss": 0.3808,
        "grad_norm": 2.5693676471710205,
        "learning_rate": 0.0001432512856322202,
        "epoch": 1.5729569476669245,
        "step": 12203
    },
    {
        "loss": 1.7971,
        "grad_norm": 3.494668483734131,
        "learning_rate": 0.00014319642228572148,
        "epoch": 1.5730858468677495,
        "step": 12204
    },
    {
        "loss": 2.1724,
        "grad_norm": 1.8720066547393799,
        "learning_rate": 0.00014314154294986148,
        "epoch": 1.5732147460685744,
        "step": 12205
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.8242132663726807,
        "learning_rate": 0.00014308664764495435,
        "epoch": 1.5733436452693992,
        "step": 12206
    },
    {
        "loss": 2.095,
        "grad_norm": 1.8213022947311401,
        "learning_rate": 0.00014303173639131967,
        "epoch": 1.5734725444702242,
        "step": 12207
    },
    {
        "loss": 1.7704,
        "grad_norm": 2.1651062965393066,
        "learning_rate": 0.0001429768092092831,
        "epoch": 1.5736014436710493,
        "step": 12208
    },
    {
        "loss": 1.3651,
        "grad_norm": 1.762024164199829,
        "learning_rate": 0.00014292186611917625,
        "epoch": 1.5737303428718743,
        "step": 12209
    },
    {
        "loss": 1.0664,
        "grad_norm": 2.947474956512451,
        "learning_rate": 0.00014286690714133653,
        "epoch": 1.5738592420726991,
        "step": 12210
    },
    {
        "loss": 1.8569,
        "grad_norm": 2.86297607421875,
        "learning_rate": 0.00014281193229610724,
        "epoch": 1.5739881412735242,
        "step": 12211
    },
    {
        "loss": 1.0006,
        "grad_norm": 2.9189321994781494,
        "learning_rate": 0.0001427569416038376,
        "epoch": 1.574117040474349,
        "step": 12212
    },
    {
        "loss": 1.5075,
        "grad_norm": 3.3986852169036865,
        "learning_rate": 0.00014270193508488272,
        "epoch": 1.574245939675174,
        "step": 12213
    },
    {
        "loss": 1.3795,
        "grad_norm": 2.83613657951355,
        "learning_rate": 0.00014264691275960346,
        "epoch": 1.574374838875999,
        "step": 12214
    },
    {
        "loss": 0.6872,
        "grad_norm": 2.472886800765991,
        "learning_rate": 0.00014259187464836666,
        "epoch": 1.574503738076824,
        "step": 12215
    },
    {
        "loss": 2.8324,
        "grad_norm": 1.3645384311676025,
        "learning_rate": 0.00014253682077154474,
        "epoch": 1.574632637277649,
        "step": 12216
    },
    {
        "loss": 1.4797,
        "grad_norm": 2.6673085689544678,
        "learning_rate": 0.00014248175114951644,
        "epoch": 1.5747615364784737,
        "step": 12217
    },
    {
        "loss": 1.7525,
        "grad_norm": 2.1850996017456055,
        "learning_rate": 0.00014242666580266598,
        "epoch": 1.5748904356792988,
        "step": 12218
    },
    {
        "loss": 1.5946,
        "grad_norm": 1.680591344833374,
        "learning_rate": 0.00014237156475138332,
        "epoch": 1.5750193348801238,
        "step": 12219
    },
    {
        "loss": 1.6413,
        "grad_norm": 2.005800247192383,
        "learning_rate": 0.00014231644801606454,
        "epoch": 1.5751482340809488,
        "step": 12220
    },
    {
        "loss": 1.5591,
        "grad_norm": 2.027001142501831,
        "learning_rate": 0.00014226131561711125,
        "epoch": 1.5752771332817737,
        "step": 12221
    },
    {
        "loss": 2.2258,
        "grad_norm": 2.5328431129455566,
        "learning_rate": 0.00014220616757493085,
        "epoch": 1.5754060324825985,
        "step": 12222
    },
    {
        "loss": 1.3858,
        "grad_norm": 2.819079637527466,
        "learning_rate": 0.00014215100390993696,
        "epoch": 1.5755349316834235,
        "step": 12223
    },
    {
        "loss": 1.8455,
        "grad_norm": 3.318060874938965,
        "learning_rate": 0.0001420958246425485,
        "epoch": 1.5756638308842486,
        "step": 12224
    },
    {
        "loss": 1.9322,
        "grad_norm": 2.3364851474761963,
        "learning_rate": 0.00014204062979319035,
        "epoch": 1.5757927300850736,
        "step": 12225
    },
    {
        "loss": 1.0985,
        "grad_norm": 2.7090330123901367,
        "learning_rate": 0.00014198541938229315,
        "epoch": 1.5759216292858984,
        "step": 12226
    },
    {
        "loss": 1.8541,
        "grad_norm": 2.4178569316864014,
        "learning_rate": 0.0001419301934302933,
        "epoch": 1.5760505284867232,
        "step": 12227
    },
    {
        "loss": 1.577,
        "grad_norm": 2.8003809452056885,
        "learning_rate": 0.00014187495195763292,
        "epoch": 1.5761794276875483,
        "step": 12228
    },
    {
        "loss": 1.4807,
        "grad_norm": 2.263761043548584,
        "learning_rate": 0.00014181969498475996,
        "epoch": 1.5763083268883733,
        "step": 12229
    },
    {
        "loss": 2.3088,
        "grad_norm": 2.5632989406585693,
        "learning_rate": 0.000141764422532128,
        "epoch": 1.5764372260891983,
        "step": 12230
    },
    {
        "loss": 1.4897,
        "grad_norm": 1.6093147993087769,
        "learning_rate": 0.00014170913462019643,
        "epoch": 1.5765661252900232,
        "step": 12231
    },
    {
        "loss": 1.381,
        "grad_norm": 2.859123945236206,
        "learning_rate": 0.0001416538312694303,
        "epoch": 1.5766950244908482,
        "step": 12232
    },
    {
        "loss": 1.1543,
        "grad_norm": 2.9379498958587646,
        "learning_rate": 0.0001415985125003003,
        "epoch": 1.576823923691673,
        "step": 12233
    },
    {
        "loss": 1.2483,
        "grad_norm": 3.201887845993042,
        "learning_rate": 0.00014154317833328322,
        "epoch": 1.576952822892498,
        "step": 12234
    },
    {
        "loss": 0.9547,
        "grad_norm": 2.657742738723755,
        "learning_rate": 0.00014148782878886112,
        "epoch": 1.577081722093323,
        "step": 12235
    },
    {
        "loss": 1.6035,
        "grad_norm": 2.604524612426758,
        "learning_rate": 0.00014143246388752186,
        "epoch": 1.5772106212941481,
        "step": 12236
    },
    {
        "loss": 1.4833,
        "grad_norm": 2.588714599609375,
        "learning_rate": 0.0001413770836497591,
        "epoch": 1.577339520494973,
        "step": 12237
    },
    {
        "loss": 1.1707,
        "grad_norm": 2.694352149963379,
        "learning_rate": 0.00014132168809607198,
        "epoch": 1.5774684196957978,
        "step": 12238
    },
    {
        "loss": 1.3738,
        "grad_norm": 2.028862714767456,
        "learning_rate": 0.00014126627724696543,
        "epoch": 1.5775973188966228,
        "step": 12239
    },
    {
        "loss": 1.2392,
        "grad_norm": 2.3507134914398193,
        "learning_rate": 0.00014121085112295018,
        "epoch": 1.5777262180974478,
        "step": 12240
    },
    {
        "loss": 1.2213,
        "grad_norm": 2.6982674598693848,
        "learning_rate": 0.00014115540974454243,
        "epoch": 1.5778551172982729,
        "step": 12241
    },
    {
        "loss": 1.093,
        "grad_norm": 3.4214203357696533,
        "learning_rate": 0.00014109995313226403,
        "epoch": 1.5779840164990977,
        "step": 12242
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.8822215795516968,
        "learning_rate": 0.00014104448130664254,
        "epoch": 1.5781129156999225,
        "step": 12243
    },
    {
        "loss": 2.1746,
        "grad_norm": 2.57637357711792,
        "learning_rate": 0.00014098899428821105,
        "epoch": 1.5782418149007476,
        "step": 12244
    },
    {
        "loss": 1.0179,
        "grad_norm": 3.3757309913635254,
        "learning_rate": 0.00014093349209750843,
        "epoch": 1.5783707141015726,
        "step": 12245
    },
    {
        "loss": 1.6509,
        "grad_norm": 2.3613266944885254,
        "learning_rate": 0.00014087797475507903,
        "epoch": 1.5784996133023976,
        "step": 12246
    },
    {
        "loss": 2.2894,
        "grad_norm": 2.344589948654175,
        "learning_rate": 0.00014082244228147273,
        "epoch": 1.5786285125032224,
        "step": 12247
    },
    {
        "loss": 1.5949,
        "grad_norm": 3.053321123123169,
        "learning_rate": 0.00014076689469724558,
        "epoch": 1.5787574117040475,
        "step": 12248
    },
    {
        "loss": 1.3685,
        "grad_norm": 1.849270224571228,
        "learning_rate": 0.00014071133202295832,
        "epoch": 1.5788863109048723,
        "step": 12249
    },
    {
        "loss": 2.1914,
        "grad_norm": 2.4916720390319824,
        "learning_rate": 0.00014065575427917777,
        "epoch": 1.5790152101056973,
        "step": 12250
    },
    {
        "loss": 0.9774,
        "grad_norm": 2.410489082336426,
        "learning_rate": 0.00014060016148647656,
        "epoch": 1.5791441093065224,
        "step": 12251
    },
    {
        "loss": 2.5176,
        "grad_norm": 1.3350192308425903,
        "learning_rate": 0.00014054455366543247,
        "epoch": 1.5792730085073474,
        "step": 12252
    },
    {
        "loss": 1.9572,
        "grad_norm": 1.9339404106140137,
        "learning_rate": 0.00014048893083662905,
        "epoch": 1.5794019077081722,
        "step": 12253
    },
    {
        "loss": 1.8361,
        "grad_norm": 1.7979357242584229,
        "learning_rate": 0.00014043329302065532,
        "epoch": 1.579530806908997,
        "step": 12254
    },
    {
        "loss": 0.2993,
        "grad_norm": 1.9379634857177734,
        "learning_rate": 0.0001403776402381059,
        "epoch": 1.579659706109822,
        "step": 12255
    },
    {
        "loss": 1.1069,
        "grad_norm": 3.554494619369507,
        "learning_rate": 0.00014032197250958076,
        "epoch": 1.5797886053106471,
        "step": 12256
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.5358927249908447,
        "learning_rate": 0.0001402662898556859,
        "epoch": 1.5799175045114722,
        "step": 12257
    },
    {
        "loss": 1.7821,
        "grad_norm": 3.098024845123291,
        "learning_rate": 0.00014021059229703232,
        "epoch": 1.580046403712297,
        "step": 12258
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.404449462890625,
        "learning_rate": 0.00014015487985423678,
        "epoch": 1.5801753029131218,
        "step": 12259
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.4358551502227783,
        "learning_rate": 0.0001400991525479215,
        "epoch": 1.5803042021139468,
        "step": 12260
    },
    {
        "loss": 1.5207,
        "grad_norm": 1.7017502784729004,
        "learning_rate": 0.0001400434103987142,
        "epoch": 1.5804331013147719,
        "step": 12261
    },
    {
        "loss": 2.1033,
        "grad_norm": 2.8477561473846436,
        "learning_rate": 0.00013998765342724813,
        "epoch": 1.580562000515597,
        "step": 12262
    },
    {
        "loss": 1.8634,
        "grad_norm": 2.603442430496216,
        "learning_rate": 0.000139931881654162,
        "epoch": 1.5806908997164217,
        "step": 12263
    },
    {
        "loss": 1.4417,
        "grad_norm": 2.815243721008301,
        "learning_rate": 0.0001398760951000998,
        "epoch": 1.5808197989172466,
        "step": 12264
    },
    {
        "loss": 2.3067,
        "grad_norm": 1.6233285665512085,
        "learning_rate": 0.00013982029378571158,
        "epoch": 1.5809486981180716,
        "step": 12265
    },
    {
        "loss": 2.3941,
        "grad_norm": 2.67173433303833,
        "learning_rate": 0.00013976447773165216,
        "epoch": 1.5810775973188966,
        "step": 12266
    },
    {
        "loss": 0.998,
        "grad_norm": 1.5871062278747559,
        "learning_rate": 0.00013970864695858203,
        "epoch": 1.5812064965197217,
        "step": 12267
    },
    {
        "loss": 1.8296,
        "grad_norm": 3.5480127334594727,
        "learning_rate": 0.00013965280148716755,
        "epoch": 1.5813353957205465,
        "step": 12268
    },
    {
        "loss": 2.0898,
        "grad_norm": 1.5826653242111206,
        "learning_rate": 0.00013959694133808002,
        "epoch": 1.5814642949213715,
        "step": 12269
    },
    {
        "loss": 2.2636,
        "grad_norm": 1.8197145462036133,
        "learning_rate": 0.0001395410665319962,
        "epoch": 1.5815931941221963,
        "step": 12270
    },
    {
        "loss": 1.7738,
        "grad_norm": 3.0873734951019287,
        "learning_rate": 0.0001394851770895988,
        "epoch": 1.5817220933230214,
        "step": 12271
    },
    {
        "loss": 1.3466,
        "grad_norm": 3.146195411682129,
        "learning_rate": 0.0001394292730315752,
        "epoch": 1.5818509925238464,
        "step": 12272
    },
    {
        "loss": 1.9843,
        "grad_norm": 1.679843783378601,
        "learning_rate": 0.00013937335437861858,
        "epoch": 1.5819798917246715,
        "step": 12273
    },
    {
        "loss": 1.7194,
        "grad_norm": 2.595508098602295,
        "learning_rate": 0.00013931742115142766,
        "epoch": 1.5821087909254963,
        "step": 12274
    },
    {
        "loss": 1.7725,
        "grad_norm": 1.9178106784820557,
        "learning_rate": 0.00013926147337070636,
        "epoch": 1.582237690126321,
        "step": 12275
    },
    {
        "loss": 1.4008,
        "grad_norm": 3.268570899963379,
        "learning_rate": 0.00013920551105716397,
        "epoch": 1.5823665893271461,
        "step": 12276
    },
    {
        "loss": 1.8295,
        "grad_norm": 2.114485263824463,
        "learning_rate": 0.0001391495342315152,
        "epoch": 1.5824954885279712,
        "step": 12277
    },
    {
        "loss": 1.5783,
        "grad_norm": 2.9203314781188965,
        "learning_rate": 0.00013909354291448016,
        "epoch": 1.5826243877287962,
        "step": 12278
    },
    {
        "loss": 2.0804,
        "grad_norm": 2.7945468425750732,
        "learning_rate": 0.0001390375371267843,
        "epoch": 1.582753286929621,
        "step": 12279
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.7551605701446533,
        "learning_rate": 0.00013898151688915836,
        "epoch": 1.5828821861304458,
        "step": 12280
    },
    {
        "loss": 1.3425,
        "grad_norm": 2.5981383323669434,
        "learning_rate": 0.0001389254822223385,
        "epoch": 1.5830110853312709,
        "step": 12281
    },
    {
        "loss": 1.6083,
        "grad_norm": 1.560319423675537,
        "learning_rate": 0.00013886943314706651,
        "epoch": 1.583139984532096,
        "step": 12282
    },
    {
        "loss": 1.4839,
        "grad_norm": 2.5695624351501465,
        "learning_rate": 0.00013881336968408884,
        "epoch": 1.583268883732921,
        "step": 12283
    },
    {
        "loss": 1.7752,
        "grad_norm": 2.164799690246582,
        "learning_rate": 0.0001387572918541577,
        "epoch": 1.5833977829337458,
        "step": 12284
    },
    {
        "loss": 1.6203,
        "grad_norm": 3.5059118270874023,
        "learning_rate": 0.00013870119967803076,
        "epoch": 1.5835266821345708,
        "step": 12285
    },
    {
        "loss": 1.8084,
        "grad_norm": 2.9841506481170654,
        "learning_rate": 0.00013864509317647073,
        "epoch": 1.5836555813353956,
        "step": 12286
    },
    {
        "loss": 1.9268,
        "grad_norm": 3.2326154708862305,
        "learning_rate": 0.0001385889723702455,
        "epoch": 1.5837844805362207,
        "step": 12287
    },
    {
        "loss": 2.006,
        "grad_norm": 3.112863540649414,
        "learning_rate": 0.00013853283728012893,
        "epoch": 1.5839133797370457,
        "step": 12288
    },
    {
        "loss": 1.517,
        "grad_norm": 2.5936474800109863,
        "learning_rate": 0.00013847668792689926,
        "epoch": 1.5840422789378708,
        "step": 12289
    },
    {
        "loss": 1.6275,
        "grad_norm": 2.447443962097168,
        "learning_rate": 0.00013842052433134046,
        "epoch": 1.5841711781386956,
        "step": 12290
    },
    {
        "loss": 0.6761,
        "grad_norm": 3.1532928943634033,
        "learning_rate": 0.00013836434651424195,
        "epoch": 1.5843000773395204,
        "step": 12291
    },
    {
        "loss": 1.713,
        "grad_norm": 2.2356410026550293,
        "learning_rate": 0.00013830815449639814,
        "epoch": 1.5844289765403454,
        "step": 12292
    },
    {
        "loss": 0.6239,
        "grad_norm": 1.845705270767212,
        "learning_rate": 0.00013825194829860876,
        "epoch": 1.5845578757411705,
        "step": 12293
    },
    {
        "loss": 2.4588,
        "grad_norm": 2.3022964000701904,
        "learning_rate": 0.00013819572794167878,
        "epoch": 1.5846867749419955,
        "step": 12294
    },
    {
        "loss": 2.1118,
        "grad_norm": 1.6305440664291382,
        "learning_rate": 0.0001381394934464185,
        "epoch": 1.5848156741428203,
        "step": 12295
    },
    {
        "loss": 1.9826,
        "grad_norm": 1.9693334102630615,
        "learning_rate": 0.00013808324483364332,
        "epoch": 1.5849445733436451,
        "step": 12296
    },
    {
        "loss": 0.9992,
        "grad_norm": 4.28881311416626,
        "learning_rate": 0.00013802698212417392,
        "epoch": 1.5850734725444702,
        "step": 12297
    },
    {
        "loss": 1.3809,
        "grad_norm": 2.0103812217712402,
        "learning_rate": 0.00013797070533883615,
        "epoch": 1.5852023717452952,
        "step": 12298
    },
    {
        "loss": 1.7025,
        "grad_norm": 2.5943470001220703,
        "learning_rate": 0.00013791441449846148,
        "epoch": 1.5853312709461203,
        "step": 12299
    },
    {
        "loss": 2.2168,
        "grad_norm": 2.8157389163970947,
        "learning_rate": 0.00013785810962388582,
        "epoch": 1.585460170146945,
        "step": 12300
    },
    {
        "loss": 1.7631,
        "grad_norm": 2.2287545204162598,
        "learning_rate": 0.0001378017907359507,
        "epoch": 1.5855890693477699,
        "step": 12301
    },
    {
        "loss": 1.8034,
        "grad_norm": 2.6115047931671143,
        "learning_rate": 0.00013774545785550312,
        "epoch": 1.585717968548595,
        "step": 12302
    },
    {
        "loss": 1.9507,
        "grad_norm": 1.930246114730835,
        "learning_rate": 0.00013768911100339482,
        "epoch": 1.58584686774942,
        "step": 12303
    },
    {
        "loss": 2.0229,
        "grad_norm": 2.6090495586395264,
        "learning_rate": 0.00013763275020048268,
        "epoch": 1.585975766950245,
        "step": 12304
    },
    {
        "loss": 1.811,
        "grad_norm": 2.2204411029815674,
        "learning_rate": 0.00013757637546762936,
        "epoch": 1.5861046661510698,
        "step": 12305
    },
    {
        "loss": 2.179,
        "grad_norm": 2.9426114559173584,
        "learning_rate": 0.0001375199868257018,
        "epoch": 1.5862335653518949,
        "step": 12306
    },
    {
        "loss": 1.9566,
        "grad_norm": 1.8412504196166992,
        "learning_rate": 0.0001374635842955726,
        "epoch": 1.5863624645527197,
        "step": 12307
    },
    {
        "loss": 1.5144,
        "grad_norm": 1.374126672744751,
        "learning_rate": 0.0001374071678981196,
        "epoch": 1.5864913637535447,
        "step": 12308
    },
    {
        "loss": 1.3815,
        "grad_norm": 2.5483951568603516,
        "learning_rate": 0.00013735073765422552,
        "epoch": 1.5866202629543698,
        "step": 12309
    },
    {
        "loss": 1.2001,
        "grad_norm": 3.7778592109680176,
        "learning_rate": 0.00013729429358477825,
        "epoch": 1.5867491621551948,
        "step": 12310
    },
    {
        "loss": 1.375,
        "grad_norm": 2.963771104812622,
        "learning_rate": 0.00013723783571067087,
        "epoch": 1.5868780613560196,
        "step": 12311
    },
    {
        "loss": 2.1062,
        "grad_norm": 1.73024320602417,
        "learning_rate": 0.0001371813640528015,
        "epoch": 1.5870069605568444,
        "step": 12312
    },
    {
        "loss": 2.1482,
        "grad_norm": 2.4736199378967285,
        "learning_rate": 0.0001371248786320734,
        "epoch": 1.5871358597576695,
        "step": 12313
    },
    {
        "loss": 2.1658,
        "grad_norm": 1.678450345993042,
        "learning_rate": 0.00013706837946939494,
        "epoch": 1.5872647589584945,
        "step": 12314
    },
    {
        "loss": 1.3757,
        "grad_norm": 2.5784080028533936,
        "learning_rate": 0.00013701186658567936,
        "epoch": 1.5873936581593195,
        "step": 12315
    },
    {
        "loss": 1.8787,
        "grad_norm": 1.381519079208374,
        "learning_rate": 0.00013695534000184563,
        "epoch": 1.5875225573601444,
        "step": 12316
    },
    {
        "loss": 1.5921,
        "grad_norm": 3.0964863300323486,
        "learning_rate": 0.0001368987997388169,
        "epoch": 1.5876514565609692,
        "step": 12317
    },
    {
        "loss": 1.2366,
        "grad_norm": 2.2650201320648193,
        "learning_rate": 0.00013684224581752186,
        "epoch": 1.5877803557617942,
        "step": 12318
    },
    {
        "loss": 0.5509,
        "grad_norm": 2.4798803329467773,
        "learning_rate": 0.00013678567825889447,
        "epoch": 1.5879092549626193,
        "step": 12319
    },
    {
        "loss": 1.855,
        "grad_norm": 2.0132532119750977,
        "learning_rate": 0.00013672909708387326,
        "epoch": 1.5880381541634443,
        "step": 12320
    },
    {
        "loss": 1.6914,
        "grad_norm": 2.3347229957580566,
        "learning_rate": 0.000136672502313402,
        "epoch": 1.588167053364269,
        "step": 12321
    },
    {
        "loss": 1.6419,
        "grad_norm": 2.433468818664551,
        "learning_rate": 0.0001366158939684299,
        "epoch": 1.5882959525650941,
        "step": 12322
    },
    {
        "loss": 2.4307,
        "grad_norm": 1.6697614192962646,
        "learning_rate": 0.00013655927206991032,
        "epoch": 1.588424851765919,
        "step": 12323
    },
    {
        "loss": 1.3448,
        "grad_norm": 4.03175163269043,
        "learning_rate": 0.0001365026366388022,
        "epoch": 1.588553750966744,
        "step": 12324
    },
    {
        "loss": 1.5209,
        "grad_norm": 2.7452704906463623,
        "learning_rate": 0.00013644598769606972,
        "epoch": 1.588682650167569,
        "step": 12325
    },
    {
        "loss": 1.8428,
        "grad_norm": 1.7217798233032227,
        "learning_rate": 0.0001363893252626815,
        "epoch": 1.5888115493683939,
        "step": 12326
    },
    {
        "loss": 1.3204,
        "grad_norm": 3.1880669593811035,
        "learning_rate": 0.0001363326493596115,
        "epoch": 1.588940448569219,
        "step": 12327
    },
    {
        "loss": 1.5832,
        "grad_norm": 2.023937940597534,
        "learning_rate": 0.00013627596000783854,
        "epoch": 1.5890693477700437,
        "step": 12328
    },
    {
        "loss": 2.1415,
        "grad_norm": 2.1602554321289062,
        "learning_rate": 0.0001362192572283465,
        "epoch": 1.5891982469708688,
        "step": 12329
    },
    {
        "loss": 1.5355,
        "grad_norm": 3.115814685821533,
        "learning_rate": 0.00013616254104212408,
        "epoch": 1.5893271461716938,
        "step": 12330
    },
    {
        "loss": 2.4647,
        "grad_norm": 3.338550090789795,
        "learning_rate": 0.00013610581147016518,
        "epoch": 1.5894560453725188,
        "step": 12331
    },
    {
        "loss": 1.4682,
        "grad_norm": 1.73166024684906,
        "learning_rate": 0.00013604906853346827,
        "epoch": 1.5895849445733437,
        "step": 12332
    },
    {
        "loss": 2.1645,
        "grad_norm": 2.3947906494140625,
        "learning_rate": 0.0001359923122530374,
        "epoch": 1.5897138437741685,
        "step": 12333
    },
    {
        "loss": 1.6644,
        "grad_norm": 3.9184494018554688,
        "learning_rate": 0.00013593554264988106,
        "epoch": 1.5898427429749935,
        "step": 12334
    },
    {
        "loss": 1.2594,
        "grad_norm": 3.252805233001709,
        "learning_rate": 0.00013587875974501246,
        "epoch": 1.5899716421758185,
        "step": 12335
    },
    {
        "loss": 1.2354,
        "grad_norm": 4.1684112548828125,
        "learning_rate": 0.00013582196355945044,
        "epoch": 1.5901005413766436,
        "step": 12336
    },
    {
        "loss": 1.2313,
        "grad_norm": 1.5380537509918213,
        "learning_rate": 0.00013576515411421825,
        "epoch": 1.5902294405774684,
        "step": 12337
    },
    {
        "loss": 1.9197,
        "grad_norm": 3.0891904830932617,
        "learning_rate": 0.00013570833143034406,
        "epoch": 1.5903583397782932,
        "step": 12338
    },
    {
        "loss": 2.3086,
        "grad_norm": 1.7670644521713257,
        "learning_rate": 0.00013565149552886144,
        "epoch": 1.5904872389791183,
        "step": 12339
    },
    {
        "loss": 1.4805,
        "grad_norm": 3.0440797805786133,
        "learning_rate": 0.00013559464643080803,
        "epoch": 1.5906161381799433,
        "step": 12340
    },
    {
        "loss": 1.5649,
        "grad_norm": 2.271031618118286,
        "learning_rate": 0.00013553778415722684,
        "epoch": 1.5907450373807683,
        "step": 12341
    },
    {
        "loss": 1.7496,
        "grad_norm": 2.34971284866333,
        "learning_rate": 0.000135480908729166,
        "epoch": 1.5908739365815932,
        "step": 12342
    },
    {
        "loss": 1.531,
        "grad_norm": 2.6726925373077393,
        "learning_rate": 0.000135424020167678,
        "epoch": 1.5910028357824182,
        "step": 12343
    },
    {
        "loss": 1.1426,
        "grad_norm": 2.8339085578918457,
        "learning_rate": 0.00013536711849382053,
        "epoch": 1.591131734983243,
        "step": 12344
    },
    {
        "loss": 2.3136,
        "grad_norm": 1.4008667469024658,
        "learning_rate": 0.00013531020372865594,
        "epoch": 1.591260634184068,
        "step": 12345
    },
    {
        "loss": 1.7472,
        "grad_norm": 2.086106300354004,
        "learning_rate": 0.00013525327589325147,
        "epoch": 1.591389533384893,
        "step": 12346
    },
    {
        "loss": 1.2931,
        "grad_norm": 3.3524227142333984,
        "learning_rate": 0.00013519633500867932,
        "epoch": 1.5915184325857181,
        "step": 12347
    },
    {
        "loss": 2.421,
        "grad_norm": 1.9630179405212402,
        "learning_rate": 0.0001351393810960164,
        "epoch": 1.591647331786543,
        "step": 12348
    },
    {
        "loss": 2.1718,
        "grad_norm": 2.2398841381073,
        "learning_rate": 0.00013508241417634425,
        "epoch": 1.5917762309873678,
        "step": 12349
    },
    {
        "loss": 0.9036,
        "grad_norm": 3.273557424545288,
        "learning_rate": 0.00013502543427074986,
        "epoch": 1.5919051301881928,
        "step": 12350
    },
    {
        "loss": 1.4722,
        "grad_norm": 2.732806921005249,
        "learning_rate": 0.00013496844140032448,
        "epoch": 1.5920340293890178,
        "step": 12351
    },
    {
        "loss": 1.2208,
        "grad_norm": 3.0898213386535645,
        "learning_rate": 0.00013491143558616394,
        "epoch": 1.5921629285898429,
        "step": 12352
    },
    {
        "loss": 1.4544,
        "grad_norm": 2.681095838546753,
        "learning_rate": 0.00013485441684936967,
        "epoch": 1.5922918277906677,
        "step": 12353
    },
    {
        "loss": 2.068,
        "grad_norm": 2.68585205078125,
        "learning_rate": 0.0001347973852110472,
        "epoch": 1.5924207269914925,
        "step": 12354
    },
    {
        "loss": 1.2109,
        "grad_norm": 2.708051919937134,
        "learning_rate": 0.00013474034069230702,
        "epoch": 1.5925496261923175,
        "step": 12355
    },
    {
        "loss": 1.9537,
        "grad_norm": 2.7116539478302,
        "learning_rate": 0.00013468328331426476,
        "epoch": 1.5926785253931426,
        "step": 12356
    },
    {
        "loss": 1.6122,
        "grad_norm": 2.2272884845733643,
        "learning_rate": 0.00013462621309804008,
        "epoch": 1.5928074245939676,
        "step": 12357
    },
    {
        "loss": 1.1559,
        "grad_norm": 1.7103509902954102,
        "learning_rate": 0.00013456913006475787,
        "epoch": 1.5929363237947924,
        "step": 12358
    },
    {
        "loss": 1.6356,
        "grad_norm": 2.9675705432891846,
        "learning_rate": 0.00013451203423554787,
        "epoch": 1.5930652229956175,
        "step": 12359
    },
    {
        "loss": 2.0644,
        "grad_norm": 2.3079915046691895,
        "learning_rate": 0.00013445492563154426,
        "epoch": 1.5931941221964423,
        "step": 12360
    },
    {
        "loss": 1.2046,
        "grad_norm": 1.9941781759262085,
        "learning_rate": 0.0001343978042738861,
        "epoch": 1.5933230213972673,
        "step": 12361
    },
    {
        "loss": 1.6213,
        "grad_norm": 1.9827642440795898,
        "learning_rate": 0.00013434067018371705,
        "epoch": 1.5934519205980924,
        "step": 12362
    },
    {
        "loss": 0.9592,
        "grad_norm": 3.2121384143829346,
        "learning_rate": 0.00013428352338218564,
        "epoch": 1.5935808197989172,
        "step": 12363
    },
    {
        "loss": 1.7777,
        "grad_norm": 1.4200897216796875,
        "learning_rate": 0.000134226363890445,
        "epoch": 1.5937097189997422,
        "step": 12364
    },
    {
        "loss": 1.3807,
        "grad_norm": 2.1009485721588135,
        "learning_rate": 0.00013416919172965302,
        "epoch": 1.593838618200567,
        "step": 12365
    },
    {
        "loss": 2.079,
        "grad_norm": 3.311788320541382,
        "learning_rate": 0.00013411200692097208,
        "epoch": 1.593967517401392,
        "step": 12366
    },
    {
        "loss": 1.7532,
        "grad_norm": 2.9366767406463623,
        "learning_rate": 0.00013405480948556968,
        "epoch": 1.5940964166022171,
        "step": 12367
    },
    {
        "loss": 1.9245,
        "grad_norm": 3.0388786792755127,
        "learning_rate": 0.0001339975994446177,
        "epoch": 1.5942253158030422,
        "step": 12368
    },
    {
        "loss": 1.7838,
        "grad_norm": 2.621732473373413,
        "learning_rate": 0.00013394037681929236,
        "epoch": 1.594354215003867,
        "step": 12369
    },
    {
        "loss": 0.4515,
        "grad_norm": 1.5998762845993042,
        "learning_rate": 0.00013388314163077526,
        "epoch": 1.5944831142046918,
        "step": 12370
    },
    {
        "loss": 1.7495,
        "grad_norm": 3.140781879425049,
        "learning_rate": 0.00013382589390025215,
        "epoch": 1.5946120134055168,
        "step": 12371
    },
    {
        "loss": 1.6388,
        "grad_norm": 3.9830760955810547,
        "learning_rate": 0.00013376863364891343,
        "epoch": 1.5947409126063419,
        "step": 12372
    },
    {
        "loss": 0.7122,
        "grad_norm": 4.359744071960449,
        "learning_rate": 0.00013371136089795446,
        "epoch": 1.594869811807167,
        "step": 12373
    },
    {
        "loss": 1.2335,
        "grad_norm": 2.1127591133117676,
        "learning_rate": 0.0001336540756685751,
        "epoch": 1.5949987110079917,
        "step": 12374
    },
    {
        "loss": 2.2564,
        "grad_norm": 2.161041736602783,
        "learning_rate": 0.0001335967779819793,
        "epoch": 1.5951276102088165,
        "step": 12375
    },
    {
        "loss": 0.7403,
        "grad_norm": 3.693417549133301,
        "learning_rate": 0.0001335394678593765,
        "epoch": 1.5952565094096416,
        "step": 12376
    },
    {
        "loss": 0.8663,
        "grad_norm": 4.399055004119873,
        "learning_rate": 0.00013348214532198015,
        "epoch": 1.5953854086104666,
        "step": 12377
    },
    {
        "loss": 1.5162,
        "grad_norm": 2.489135503768921,
        "learning_rate": 0.0001334248103910085,
        "epoch": 1.5955143078112917,
        "step": 12378
    },
    {
        "loss": 1.8396,
        "grad_norm": 2.0377635955810547,
        "learning_rate": 0.0001333674630876843,
        "epoch": 1.5956432070121165,
        "step": 12379
    },
    {
        "loss": 1.4142,
        "grad_norm": 2.575653553009033,
        "learning_rate": 0.00013331010343323497,
        "epoch": 1.5957721062129415,
        "step": 12380
    },
    {
        "loss": 1.799,
        "grad_norm": 2.030233383178711,
        "learning_rate": 0.00013325273144889244,
        "epoch": 1.5959010054137663,
        "step": 12381
    },
    {
        "loss": 1.6133,
        "grad_norm": 1.8153181076049805,
        "learning_rate": 0.00013319534715589319,
        "epoch": 1.5960299046145914,
        "step": 12382
    },
    {
        "loss": 1.8516,
        "grad_norm": 2.509159803390503,
        "learning_rate": 0.00013313795057547822,
        "epoch": 1.5961588038154164,
        "step": 12383
    },
    {
        "loss": 2.4192,
        "grad_norm": 1.704958438873291,
        "learning_rate": 0.00013308054172889337,
        "epoch": 1.5962877030162415,
        "step": 12384
    },
    {
        "loss": 1.5624,
        "grad_norm": 2.1101930141448975,
        "learning_rate": 0.00013302312063738882,
        "epoch": 1.5964166022170663,
        "step": 12385
    },
    {
        "loss": 1.3117,
        "grad_norm": 3.064371347427368,
        "learning_rate": 0.00013296568732221886,
        "epoch": 1.596545501417891,
        "step": 12386
    },
    {
        "loss": 1.82,
        "grad_norm": 2.1123268604278564,
        "learning_rate": 0.00013290824180464306,
        "epoch": 1.5966744006187161,
        "step": 12387
    },
    {
        "loss": 2.3287,
        "grad_norm": 1.6802260875701904,
        "learning_rate": 0.00013285078410592508,
        "epoch": 1.5968032998195412,
        "step": 12388
    },
    {
        "loss": 0.9215,
        "grad_norm": 2.9085848331451416,
        "learning_rate": 0.00013279331424733302,
        "epoch": 1.5969321990203662,
        "step": 12389
    },
    {
        "loss": 0.3818,
        "grad_norm": 1.7475191354751587,
        "learning_rate": 0.00013273583225013983,
        "epoch": 1.597061098221191,
        "step": 12390
    },
    {
        "loss": 1.7215,
        "grad_norm": 2.5728676319122314,
        "learning_rate": 0.00013267833813562278,
        "epoch": 1.5971899974220158,
        "step": 12391
    },
    {
        "loss": 1.4978,
        "grad_norm": 3.9094057083129883,
        "learning_rate": 0.00013262083192506322,
        "epoch": 1.5973188966228409,
        "step": 12392
    },
    {
        "loss": 2.0001,
        "grad_norm": 2.8862881660461426,
        "learning_rate": 0.0001325633136397477,
        "epoch": 1.597447795823666,
        "step": 12393
    },
    {
        "loss": 2.4533,
        "grad_norm": 3.4328083992004395,
        "learning_rate": 0.00013250578330096678,
        "epoch": 1.597576695024491,
        "step": 12394
    },
    {
        "loss": 1.5174,
        "grad_norm": 2.8959579467773438,
        "learning_rate": 0.0001324482409300156,
        "epoch": 1.5977055942253158,
        "step": 12395
    },
    {
        "loss": 0.7993,
        "grad_norm": 3.616703510284424,
        "learning_rate": 0.0001323906865481937,
        "epoch": 1.5978344934261408,
        "step": 12396
    },
    {
        "loss": 2.2572,
        "grad_norm": 2.395280122756958,
        "learning_rate": 0.00013233312017680513,
        "epoch": 1.5979633926269656,
        "step": 12397
    },
    {
        "loss": 1.6465,
        "grad_norm": 2.8873653411865234,
        "learning_rate": 0.0001322755418371584,
        "epoch": 1.5980922918277907,
        "step": 12398
    },
    {
        "loss": 1.1714,
        "grad_norm": 2.5984785556793213,
        "learning_rate": 0.00013221795155056633,
        "epoch": 1.5982211910286157,
        "step": 12399
    },
    {
        "loss": 1.8697,
        "grad_norm": 1.8585271835327148,
        "learning_rate": 0.00013216034933834616,
        "epoch": 1.5983500902294405,
        "step": 12400
    },
    {
        "loss": 2.0495,
        "grad_norm": 2.259127616882324,
        "learning_rate": 0.00013210273522181983,
        "epoch": 1.5984789894302656,
        "step": 12401
    },
    {
        "loss": 1.7255,
        "grad_norm": 2.7302520275115967,
        "learning_rate": 0.00013204510922231358,
        "epoch": 1.5986078886310904,
        "step": 12402
    },
    {
        "loss": 1.3791,
        "grad_norm": 2.0865139961242676,
        "learning_rate": 0.00013198747136115743,
        "epoch": 1.5987367878319154,
        "step": 12403
    },
    {
        "loss": 1.5925,
        "grad_norm": 2.506415367126465,
        "learning_rate": 0.0001319298216596868,
        "epoch": 1.5988656870327405,
        "step": 12404
    },
    {
        "loss": 1.194,
        "grad_norm": 1.9200899600982666,
        "learning_rate": 0.00013187216013924082,
        "epoch": 1.5989945862335655,
        "step": 12405
    },
    {
        "loss": 1.4272,
        "grad_norm": 1.6927145719528198,
        "learning_rate": 0.00013181448682116303,
        "epoch": 1.5991234854343903,
        "step": 12406
    },
    {
        "loss": 1.6384,
        "grad_norm": 3.446126937866211,
        "learning_rate": 0.00013175680172680183,
        "epoch": 1.5992523846352151,
        "step": 12407
    },
    {
        "loss": 1.9921,
        "grad_norm": 2.4510254859924316,
        "learning_rate": 0.00013169910487750955,
        "epoch": 1.5993812838360402,
        "step": 12408
    },
    {
        "loss": 1.8988,
        "grad_norm": 1.7892447710037231,
        "learning_rate": 0.00013164139629464258,
        "epoch": 1.5995101830368652,
        "step": 12409
    },
    {
        "loss": 0.5537,
        "grad_norm": 2.269805431365967,
        "learning_rate": 0.00013158367599956243,
        "epoch": 1.5996390822376902,
        "step": 12410
    },
    {
        "loss": 1.2407,
        "grad_norm": 2.4086992740631104,
        "learning_rate": 0.0001315259440136344,
        "epoch": 1.599767981438515,
        "step": 12411
    },
    {
        "loss": 1.955,
        "grad_norm": 1.831710696220398,
        "learning_rate": 0.00013146820035822825,
        "epoch": 1.5998968806393399,
        "step": 12412
    },
    {
        "loss": 1.8579,
        "grad_norm": 2.278261423110962,
        "learning_rate": 0.00013141044505471814,
        "epoch": 1.600025779840165,
        "step": 12413
    },
    {
        "loss": 1.0503,
        "grad_norm": 3.104079246520996,
        "learning_rate": 0.00013135267812448235,
        "epoch": 1.60015467904099,
        "step": 12414
    },
    {
        "loss": 1.5647,
        "grad_norm": 1.6928192377090454,
        "learning_rate": 0.00013129489958890367,
        "epoch": 1.600283578241815,
        "step": 12415
    },
    {
        "loss": 1.7132,
        "grad_norm": 2.705476760864258,
        "learning_rate": 0.0001312371094693691,
        "epoch": 1.6004124774426398,
        "step": 12416
    },
    {
        "loss": 1.7889,
        "grad_norm": 2.1474010944366455,
        "learning_rate": 0.00013117930778726972,
        "epoch": 1.6005413766434649,
        "step": 12417
    },
    {
        "loss": 0.6792,
        "grad_norm": 2.2129011154174805,
        "learning_rate": 0.00013112149456400148,
        "epoch": 1.6006702758442897,
        "step": 12418
    },
    {
        "loss": 1.6514,
        "grad_norm": 3.4554765224456787,
        "learning_rate": 0.0001310636698209641,
        "epoch": 1.6007991750451147,
        "step": 12419
    },
    {
        "loss": 0.7042,
        "grad_norm": 3.082461357116699,
        "learning_rate": 0.00013100583357956134,
        "epoch": 1.6009280742459397,
        "step": 12420
    },
    {
        "loss": 1.279,
        "grad_norm": 2.660924196243286,
        "learning_rate": 0.000130947985861202,
        "epoch": 1.6010569734467648,
        "step": 12421
    },
    {
        "loss": 1.8488,
        "grad_norm": 1.5846771001815796,
        "learning_rate": 0.0001308901266872985,
        "epoch": 1.6011858726475896,
        "step": 12422
    },
    {
        "loss": 1.1661,
        "grad_norm": 2.0014071464538574,
        "learning_rate": 0.00013083225607926754,
        "epoch": 1.6013147718484144,
        "step": 12423
    },
    {
        "loss": 2.1545,
        "grad_norm": 2.3272242546081543,
        "learning_rate": 0.00013077437405853052,
        "epoch": 1.6014436710492395,
        "step": 12424
    },
    {
        "loss": 1.1934,
        "grad_norm": 1.4057995080947876,
        "learning_rate": 0.00013071648064651273,
        "epoch": 1.6015725702500645,
        "step": 12425
    },
    {
        "loss": 1.269,
        "grad_norm": 2.5279977321624756,
        "learning_rate": 0.00013065857586464324,
        "epoch": 1.6017014694508895,
        "step": 12426
    },
    {
        "loss": 0.9498,
        "grad_norm": 2.156705379486084,
        "learning_rate": 0.00013060065973435626,
        "epoch": 1.6018303686517144,
        "step": 12427
    },
    {
        "loss": 1.6615,
        "grad_norm": 2.664916515350342,
        "learning_rate": 0.00013054273227708956,
        "epoch": 1.6019592678525392,
        "step": 12428
    },
    {
        "loss": 2.1077,
        "grad_norm": 2.392991304397583,
        "learning_rate": 0.00013048479351428522,
        "epoch": 1.6020881670533642,
        "step": 12429
    },
    {
        "loss": 1.6567,
        "grad_norm": 3.833847761154175,
        "learning_rate": 0.00013042684346738963,
        "epoch": 1.6022170662541892,
        "step": 12430
    },
    {
        "loss": 1.8438,
        "grad_norm": 2.318072557449341,
        "learning_rate": 0.00013036888215785318,
        "epoch": 1.6023459654550143,
        "step": 12431
    },
    {
        "loss": 0.9187,
        "grad_norm": 2.634565591812134,
        "learning_rate": 0.0001303109096071306,
        "epoch": 1.602474864655839,
        "step": 12432
    },
    {
        "loss": 1.8772,
        "grad_norm": 1.4579747915267944,
        "learning_rate": 0.00013025292583668068,
        "epoch": 1.6026037638566641,
        "step": 12433
    },
    {
        "loss": 0.7881,
        "grad_norm": 3.017113208770752,
        "learning_rate": 0.00013019493086796624,
        "epoch": 1.602732663057489,
        "step": 12434
    },
    {
        "loss": 1.2544,
        "grad_norm": 2.770920991897583,
        "learning_rate": 0.00013013692472245466,
        "epoch": 1.602861562258314,
        "step": 12435
    },
    {
        "loss": 1.7037,
        "grad_norm": 1.933538794517517,
        "learning_rate": 0.0001300789074216171,
        "epoch": 1.602990461459139,
        "step": 12436
    },
    {
        "loss": 1.8949,
        "grad_norm": 2.124155044555664,
        "learning_rate": 0.0001300208789869289,
        "epoch": 1.6031193606599639,
        "step": 12437
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.4967474937438965,
        "learning_rate": 0.00012996283943986957,
        "epoch": 1.603248259860789,
        "step": 12438
    },
    {
        "loss": 1.4365,
        "grad_norm": 2.678593635559082,
        "learning_rate": 0.00012990478880192273,
        "epoch": 1.6033771590616137,
        "step": 12439
    },
    {
        "loss": 1.4704,
        "grad_norm": 2.2932016849517822,
        "learning_rate": 0.00012984672709457598,
        "epoch": 1.6035060582624387,
        "step": 12440
    },
    {
        "loss": 1.5207,
        "grad_norm": 2.9850881099700928,
        "learning_rate": 0.00012978865433932134,
        "epoch": 1.6036349574632638,
        "step": 12441
    },
    {
        "loss": 0.9305,
        "grad_norm": 3.5784904956817627,
        "learning_rate": 0.0001297305705576548,
        "epoch": 1.6037638566640888,
        "step": 12442
    },
    {
        "loss": 1.2477,
        "grad_norm": 2.8883988857269287,
        "learning_rate": 0.00012967247577107588,
        "epoch": 1.6038927558649136,
        "step": 12443
    },
    {
        "loss": 1.6833,
        "grad_norm": 1.8575025796890259,
        "learning_rate": 0.0001296143700010891,
        "epoch": 1.6040216550657385,
        "step": 12444
    },
    {
        "loss": 1.2229,
        "grad_norm": 2.4741194248199463,
        "learning_rate": 0.00012955625326920242,
        "epoch": 1.6041505542665635,
        "step": 12445
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.219881772994995,
        "learning_rate": 0.00012949812559692804,
        "epoch": 1.6042794534673885,
        "step": 12446
    },
    {
        "loss": 1.3243,
        "grad_norm": 1.4873861074447632,
        "learning_rate": 0.00012943998700578216,
        "epoch": 1.6044083526682136,
        "step": 12447
    },
    {
        "loss": 1.4996,
        "grad_norm": 3.233704090118408,
        "learning_rate": 0.0001293818375172851,
        "epoch": 1.6045372518690384,
        "step": 12448
    },
    {
        "loss": 1.5148,
        "grad_norm": 1.598410964012146,
        "learning_rate": 0.00012932367715296115,
        "epoch": 1.6046661510698632,
        "step": 12449
    },
    {
        "loss": 1.5664,
        "grad_norm": 2.6277143955230713,
        "learning_rate": 0.00012926550593433866,
        "epoch": 1.6047950502706883,
        "step": 12450
    },
    {
        "loss": 1.7264,
        "grad_norm": 3.2350614070892334,
        "learning_rate": 0.0001292073238829498,
        "epoch": 1.6049239494715133,
        "step": 12451
    },
    {
        "loss": 1.0818,
        "grad_norm": 2.8957321643829346,
        "learning_rate": 0.00012914913102033126,
        "epoch": 1.6050528486723383,
        "step": 12452
    },
    {
        "loss": 1.4036,
        "grad_norm": 2.729163885116577,
        "learning_rate": 0.00012909092736802325,
        "epoch": 1.6051817478731631,
        "step": 12453
    },
    {
        "loss": 1.171,
        "grad_norm": 2.887028455734253,
        "learning_rate": 0.0001290327129475701,
        "epoch": 1.6053106470739882,
        "step": 12454
    },
    {
        "loss": 1.5389,
        "grad_norm": 2.290233612060547,
        "learning_rate": 0.0001289744877805202,
        "epoch": 1.605439546274813,
        "step": 12455
    },
    {
        "loss": 1.7807,
        "grad_norm": 2.1420981884002686,
        "learning_rate": 0.00012891625188842583,
        "epoch": 1.605568445475638,
        "step": 12456
    },
    {
        "loss": 1.1818,
        "grad_norm": 3.627163887023926,
        "learning_rate": 0.0001288580052928432,
        "epoch": 1.605697344676463,
        "step": 12457
    },
    {
        "loss": 1.8144,
        "grad_norm": 2.8024566173553467,
        "learning_rate": 0.00012879974801533283,
        "epoch": 1.6058262438772881,
        "step": 12458
    },
    {
        "loss": 1.9046,
        "grad_norm": 2.058272361755371,
        "learning_rate": 0.00012874148007745872,
        "epoch": 1.605955143078113,
        "step": 12459
    },
    {
        "loss": 1.8034,
        "grad_norm": 2.736557960510254,
        "learning_rate": 0.0001286832015007891,
        "epoch": 1.6060840422789378,
        "step": 12460
    },
    {
        "loss": 2.1419,
        "grad_norm": 3.1275832653045654,
        "learning_rate": 0.00012862491230689605,
        "epoch": 1.6062129414797628,
        "step": 12461
    },
    {
        "loss": 1.8699,
        "grad_norm": 3.1213574409484863,
        "learning_rate": 0.0001285666125173555,
        "epoch": 1.6063418406805878,
        "step": 12462
    },
    {
        "loss": 1.6701,
        "grad_norm": 2.933156728744507,
        "learning_rate": 0.00012850830215374755,
        "epoch": 1.6064707398814129,
        "step": 12463
    },
    {
        "loss": 2.302,
        "grad_norm": 3.1687893867492676,
        "learning_rate": 0.00012844998123765598,
        "epoch": 1.6065996390822377,
        "step": 12464
    },
    {
        "loss": 1.2108,
        "grad_norm": 2.5798416137695312,
        "learning_rate": 0.00012839164979066848,
        "epoch": 1.6067285382830625,
        "step": 12465
    },
    {
        "loss": 2.3742,
        "grad_norm": 2.293769359588623,
        "learning_rate": 0.00012833330783437684,
        "epoch": 1.6068574374838875,
        "step": 12466
    },
    {
        "loss": 2.3538,
        "grad_norm": 2.2472083568573,
        "learning_rate": 0.00012827495539037656,
        "epoch": 1.6069863366847126,
        "step": 12467
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.0143280029296875,
        "learning_rate": 0.00012821659248026695,
        "epoch": 1.6071152358855376,
        "step": 12468
    },
    {
        "loss": 0.8862,
        "grad_norm": 2.8869497776031494,
        "learning_rate": 0.00012815821912565156,
        "epoch": 1.6072441350863624,
        "step": 12469
    },
    {
        "loss": 0.944,
        "grad_norm": 2.858757972717285,
        "learning_rate": 0.00012809983534813743,
        "epoch": 1.6073730342871875,
        "step": 12470
    },
    {
        "loss": 1.8685,
        "grad_norm": 2.5370941162109375,
        "learning_rate": 0.00012804144116933562,
        "epoch": 1.6075019334880123,
        "step": 12471
    },
    {
        "loss": 1.8506,
        "grad_norm": 1.9375901222229004,
        "learning_rate": 0.00012798303661086104,
        "epoch": 1.6076308326888373,
        "step": 12472
    },
    {
        "loss": 0.9274,
        "grad_norm": 2.359043836593628,
        "learning_rate": 0.00012792462169433234,
        "epoch": 1.6077597318896624,
        "step": 12473
    },
    {
        "loss": 1.5212,
        "grad_norm": 2.9202117919921875,
        "learning_rate": 0.000127866196441372,
        "epoch": 1.6078886310904872,
        "step": 12474
    },
    {
        "loss": 1.7101,
        "grad_norm": 2.722010374069214,
        "learning_rate": 0.00012780776087360665,
        "epoch": 1.6080175302913122,
        "step": 12475
    },
    {
        "loss": 1.3132,
        "grad_norm": 2.6556687355041504,
        "learning_rate": 0.00012774931501266637,
        "epoch": 1.608146429492137,
        "step": 12476
    },
    {
        "loss": 2.147,
        "grad_norm": 2.331343173980713,
        "learning_rate": 0.00012769085888018517,
        "epoch": 1.608275328692962,
        "step": 12477
    },
    {
        "loss": 0.8286,
        "grad_norm": 2.3676681518554688,
        "learning_rate": 0.00012763239249780086,
        "epoch": 1.6084042278937871,
        "step": 12478
    },
    {
        "loss": 0.9533,
        "grad_norm": 2.98236083984375,
        "learning_rate": 0.00012757391588715506,
        "epoch": 1.6085331270946122,
        "step": 12479
    },
    {
        "loss": 0.7444,
        "grad_norm": 2.504042625427246,
        "learning_rate": 0.00012751542906989317,
        "epoch": 1.608662026295437,
        "step": 12480
    },
    {
        "loss": 1.3494,
        "grad_norm": 2.8121840953826904,
        "learning_rate": 0.0001274569320676643,
        "epoch": 1.6087909254962618,
        "step": 12481
    },
    {
        "loss": 1.1873,
        "grad_norm": 3.3092081546783447,
        "learning_rate": 0.00012739842490212152,
        "epoch": 1.6089198246970868,
        "step": 12482
    },
    {
        "loss": 2.1507,
        "grad_norm": 2.8106741905212402,
        "learning_rate": 0.00012733990759492144,
        "epoch": 1.6090487238979119,
        "step": 12483
    },
    {
        "loss": 1.0631,
        "grad_norm": 2.7793149948120117,
        "learning_rate": 0.00012728138016772453,
        "epoch": 1.609177623098737,
        "step": 12484
    },
    {
        "loss": 2.3049,
        "grad_norm": 2.155153751373291,
        "learning_rate": 0.0001272228426421949,
        "epoch": 1.6093065222995617,
        "step": 12485
    },
    {
        "loss": 0.6381,
        "grad_norm": 2.554332733154297,
        "learning_rate": 0.00012716429504000068,
        "epoch": 1.6094354215003865,
        "step": 12486
    },
    {
        "loss": 1.3129,
        "grad_norm": 4.185836315155029,
        "learning_rate": 0.0001271057373828135,
        "epoch": 1.6095643207012116,
        "step": 12487
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.41800856590271,
        "learning_rate": 0.00012704716969230874,
        "epoch": 1.6096932199020366,
        "step": 12488
    },
    {
        "loss": 1.5439,
        "grad_norm": 1.5648366212844849,
        "learning_rate": 0.00012698859199016546,
        "epoch": 1.6098221191028617,
        "step": 12489
    },
    {
        "loss": 1.4473,
        "grad_norm": 5.412414073944092,
        "learning_rate": 0.00012693000429806652,
        "epoch": 1.6099510183036865,
        "step": 12490
    },
    {
        "loss": 1.7526,
        "grad_norm": 1.8241263628005981,
        "learning_rate": 0.00012687140663769828,
        "epoch": 1.6100799175045115,
        "step": 12491
    },
    {
        "loss": 2.2949,
        "grad_norm": 1.6745476722717285,
        "learning_rate": 0.0001268127990307512,
        "epoch": 1.6102088167053363,
        "step": 12492
    },
    {
        "loss": 1.5085,
        "grad_norm": 1.675004243850708,
        "learning_rate": 0.00012675418149891906,
        "epoch": 1.6103377159061614,
        "step": 12493
    },
    {
        "loss": 1.5706,
        "grad_norm": 2.9177896976470947,
        "learning_rate": 0.0001266955540638994,
        "epoch": 1.6104666151069864,
        "step": 12494
    },
    {
        "loss": 1.7391,
        "grad_norm": 2.8023931980133057,
        "learning_rate": 0.00012663691674739345,
        "epoch": 1.6105955143078114,
        "step": 12495
    },
    {
        "loss": 1.2565,
        "grad_norm": 2.119403839111328,
        "learning_rate": 0.00012657826957110607,
        "epoch": 1.6107244135086363,
        "step": 12496
    },
    {
        "loss": 1.457,
        "grad_norm": 2.8723790645599365,
        "learning_rate": 0.00012651961255674585,
        "epoch": 1.610853312709461,
        "step": 12497
    },
    {
        "loss": 1.8814,
        "grad_norm": 2.9793238639831543,
        "learning_rate": 0.00012646094572602487,
        "epoch": 1.6109822119102861,
        "step": 12498
    },
    {
        "loss": 1.8727,
        "grad_norm": 2.694451332092285,
        "learning_rate": 0.0001264022691006589,
        "epoch": 1.6111111111111112,
        "step": 12499
    },
    {
        "loss": 1.945,
        "grad_norm": 1.7788145542144775,
        "learning_rate": 0.0001263435827023677,
        "epoch": 1.6112400103119362,
        "step": 12500
    },
    {
        "loss": 1.4064,
        "grad_norm": 2.2192718982696533,
        "learning_rate": 0.00012628488655287395,
        "epoch": 1.611368909512761,
        "step": 12501
    },
    {
        "loss": 1.9757,
        "grad_norm": 2.0187063217163086,
        "learning_rate": 0.0001262261806739043,
        "epoch": 1.6114978087135858,
        "step": 12502
    },
    {
        "loss": 1.2173,
        "grad_norm": 2.9753243923187256,
        "learning_rate": 0.00012616746508718927,
        "epoch": 1.6116267079144109,
        "step": 12503
    },
    {
        "loss": 0.996,
        "grad_norm": 3.3222250938415527,
        "learning_rate": 0.00012610873981446264,
        "epoch": 1.611755607115236,
        "step": 12504
    },
    {
        "loss": 1.3557,
        "grad_norm": 2.9359147548675537,
        "learning_rate": 0.00012605000487746177,
        "epoch": 1.611884506316061,
        "step": 12505
    },
    {
        "loss": 1.0899,
        "grad_norm": 2.3220105171203613,
        "learning_rate": 0.0001259912602979277,
        "epoch": 1.6120134055168858,
        "step": 12506
    },
    {
        "loss": 0.7442,
        "grad_norm": 3.406810998916626,
        "learning_rate": 0.00012593250609760505,
        "epoch": 1.6121423047177108,
        "step": 12507
    },
    {
        "loss": 1.7301,
        "grad_norm": 2.6638715267181396,
        "learning_rate": 0.0001258737422982418,
        "epoch": 1.6122712039185356,
        "step": 12508
    },
    {
        "loss": 2.0339,
        "grad_norm": 2.481877326965332,
        "learning_rate": 0.0001258149689215899,
        "epoch": 1.6124001031193607,
        "step": 12509
    },
    {
        "loss": 2.0382,
        "grad_norm": 1.5179136991500854,
        "learning_rate": 0.0001257561859894045,
        "epoch": 1.6125290023201857,
        "step": 12510
    },
    {
        "loss": 2.0929,
        "grad_norm": 1.9376522302627563,
        "learning_rate": 0.00012569739352344435,
        "epoch": 1.6126579015210105,
        "step": 12511
    },
    {
        "loss": 1.3199,
        "grad_norm": 3.128781795501709,
        "learning_rate": 0.0001256385915454718,
        "epoch": 1.6127868007218356,
        "step": 12512
    },
    {
        "loss": 1.7323,
        "grad_norm": 3.2437920570373535,
        "learning_rate": 0.00012557978007725267,
        "epoch": 1.6129156999226604,
        "step": 12513
    },
    {
        "loss": 1.4857,
        "grad_norm": 1.531691074371338,
        "learning_rate": 0.00012552095914055626,
        "epoch": 1.6130445991234854,
        "step": 12514
    },
    {
        "loss": 1.7651,
        "grad_norm": 1.6171938180923462,
        "learning_rate": 0.00012546212875715548,
        "epoch": 1.6131734983243105,
        "step": 12515
    },
    {
        "loss": 0.8373,
        "grad_norm": 4.140722751617432,
        "learning_rate": 0.0001254032889488265,
        "epoch": 1.6133023975251355,
        "step": 12516
    },
    {
        "loss": 0.8443,
        "grad_norm": 1.7371653318405151,
        "learning_rate": 0.00012534443973734959,
        "epoch": 1.6134312967259603,
        "step": 12517
    },
    {
        "loss": 1.7701,
        "grad_norm": 1.7106759548187256,
        "learning_rate": 0.00012528558114450764,
        "epoch": 1.6135601959267851,
        "step": 12518
    },
    {
        "loss": 1.7588,
        "grad_norm": 1.795532464981079,
        "learning_rate": 0.0001252267131920874,
        "epoch": 1.6136890951276102,
        "step": 12519
    },
    {
        "loss": 2.1135,
        "grad_norm": 2.994800567626953,
        "learning_rate": 0.00012516783590187948,
        "epoch": 1.6138179943284352,
        "step": 12520
    },
    {
        "loss": 2.374,
        "grad_norm": 2.1642730236053467,
        "learning_rate": 0.00012510894929567743,
        "epoch": 1.6139468935292602,
        "step": 12521
    },
    {
        "loss": 1.5454,
        "grad_norm": 2.2767181396484375,
        "learning_rate": 0.0001250500533952782,
        "epoch": 1.614075792730085,
        "step": 12522
    },
    {
        "loss": 1.1362,
        "grad_norm": 1.9022783041000366,
        "learning_rate": 0.00012499114822248287,
        "epoch": 1.6142046919309099,
        "step": 12523
    },
    {
        "loss": 2.1002,
        "grad_norm": 1.6667176485061646,
        "learning_rate": 0.00012493223379909504,
        "epoch": 1.614333591131735,
        "step": 12524
    },
    {
        "loss": 1.9853,
        "grad_norm": 2.2973227500915527,
        "learning_rate": 0.00012487331014692215,
        "epoch": 1.61446249033256,
        "step": 12525
    },
    {
        "loss": 1.24,
        "grad_norm": 2.950927734375,
        "learning_rate": 0.00012481437728777538,
        "epoch": 1.614591389533385,
        "step": 12526
    },
    {
        "loss": 1.5394,
        "grad_norm": 3.2103891372680664,
        "learning_rate": 0.00012475543524346884,
        "epoch": 1.6147202887342098,
        "step": 12527
    },
    {
        "loss": 1.3054,
        "grad_norm": 1.9114242792129517,
        "learning_rate": 0.0001246964840358202,
        "epoch": 1.6148491879350348,
        "step": 12528
    },
    {
        "loss": 1.9398,
        "grad_norm": 1.4469817876815796,
        "learning_rate": 0.00012463752368665056,
        "epoch": 1.6149780871358597,
        "step": 12529
    },
    {
        "loss": 1.3158,
        "grad_norm": 3.7141308784484863,
        "learning_rate": 0.00012457855421778435,
        "epoch": 1.6151069863366847,
        "step": 12530
    },
    {
        "loss": 1.6641,
        "grad_norm": 2.8335022926330566,
        "learning_rate": 0.00012451957565104936,
        "epoch": 1.6152358855375097,
        "step": 12531
    },
    {
        "loss": 1.5948,
        "grad_norm": 2.114476442337036,
        "learning_rate": 0.0001244605880082769,
        "epoch": 1.6153647847383348,
        "step": 12532
    },
    {
        "loss": 2.1945,
        "grad_norm": 1.2658754587173462,
        "learning_rate": 0.00012440159131130128,
        "epoch": 1.6154936839391596,
        "step": 12533
    },
    {
        "loss": 1.9977,
        "grad_norm": 2.6283977031707764,
        "learning_rate": 0.0001243425855819609,
        "epoch": 1.6156225831399844,
        "step": 12534
    },
    {
        "loss": 1.3837,
        "grad_norm": 1.8738536834716797,
        "learning_rate": 0.00012428357084209656,
        "epoch": 1.6157514823408095,
        "step": 12535
    },
    {
        "loss": 1.5896,
        "grad_norm": 2.5077435970306396,
        "learning_rate": 0.00012422454711355285,
        "epoch": 1.6158803815416345,
        "step": 12536
    },
    {
        "loss": 1.7636,
        "grad_norm": 1.7732735872268677,
        "learning_rate": 0.00012416551441817795,
        "epoch": 1.6160092807424595,
        "step": 12537
    },
    {
        "loss": 2.4583,
        "grad_norm": 3.6426877975463867,
        "learning_rate": 0.00012410647277782293,
        "epoch": 1.6161381799432843,
        "step": 12538
    },
    {
        "loss": 1.2475,
        "grad_norm": 2.6050570011138916,
        "learning_rate": 0.00012404742221434228,
        "epoch": 1.6162670791441092,
        "step": 12539
    },
    {
        "loss": 0.9671,
        "grad_norm": 2.2853987216949463,
        "learning_rate": 0.00012398836274959418,
        "epoch": 1.6163959783449342,
        "step": 12540
    },
    {
        "loss": 1.8282,
        "grad_norm": 2.7101612091064453,
        "learning_rate": 0.0001239292944054394,
        "epoch": 1.6165248775457592,
        "step": 12541
    },
    {
        "loss": 1.303,
        "grad_norm": 2.271933078765869,
        "learning_rate": 0.00012387021720374234,
        "epoch": 1.6166537767465843,
        "step": 12542
    },
    {
        "loss": 1.38,
        "grad_norm": 1.8784188032150269,
        "learning_rate": 0.00012381113116637104,
        "epoch": 1.616782675947409,
        "step": 12543
    },
    {
        "loss": 1.0425,
        "grad_norm": 3.5297939777374268,
        "learning_rate": 0.00012375203631519624,
        "epoch": 1.616911575148234,
        "step": 12544
    },
    {
        "loss": 0.6341,
        "grad_norm": 2.5475616455078125,
        "learning_rate": 0.0001236929326720923,
        "epoch": 1.617040474349059,
        "step": 12545
    },
    {
        "loss": 2.2605,
        "grad_norm": 3.510355234146118,
        "learning_rate": 0.00012363382025893662,
        "epoch": 1.617169373549884,
        "step": 12546
    },
    {
        "loss": 1.8984,
        "grad_norm": 2.353328227996826,
        "learning_rate": 0.00012357469909760998,
        "epoch": 1.617298272750709,
        "step": 12547
    },
    {
        "loss": 1.6394,
        "grad_norm": 1.8374651670455933,
        "learning_rate": 0.00012351556920999637,
        "epoch": 1.6174271719515338,
        "step": 12548
    },
    {
        "loss": 1.856,
        "grad_norm": 1.986886978149414,
        "learning_rate": 0.00012345643061798295,
        "epoch": 1.6175560711523589,
        "step": 12549
    },
    {
        "loss": 1.4973,
        "grad_norm": 2.3566744327545166,
        "learning_rate": 0.00012339728334346007,
        "epoch": 1.6176849703531837,
        "step": 12550
    },
    {
        "loss": 1.4057,
        "grad_norm": 2.658358573913574,
        "learning_rate": 0.00012333812740832176,
        "epoch": 1.6178138695540087,
        "step": 12551
    },
    {
        "loss": 1.4723,
        "grad_norm": 2.6788177490234375,
        "learning_rate": 0.00012327896283446447,
        "epoch": 1.6179427687548338,
        "step": 12552
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.7978968620300293,
        "learning_rate": 0.00012321978964378822,
        "epoch": 1.6180716679556588,
        "step": 12553
    },
    {
        "loss": 0.3168,
        "grad_norm": 1.285096287727356,
        "learning_rate": 0.00012316060785819656,
        "epoch": 1.6182005671564836,
        "step": 12554
    },
    {
        "loss": 1.771,
        "grad_norm": 2.25583553314209,
        "learning_rate": 0.00012310141749959574,
        "epoch": 1.6183294663573085,
        "step": 12555
    },
    {
        "loss": 1.4373,
        "grad_norm": 1.8574351072311401,
        "learning_rate": 0.00012304221858989525,
        "epoch": 1.6184583655581335,
        "step": 12556
    },
    {
        "loss": 1.7572,
        "grad_norm": 2.3423891067504883,
        "learning_rate": 0.00012298301115100824,
        "epoch": 1.6185872647589585,
        "step": 12557
    },
    {
        "loss": 1.7753,
        "grad_norm": 1.914049744606018,
        "learning_rate": 0.0001229237952048502,
        "epoch": 1.6187161639597836,
        "step": 12558
    },
    {
        "loss": 1.7683,
        "grad_norm": 3.1877238750457764,
        "learning_rate": 0.00012286457077334025,
        "epoch": 1.6188450631606084,
        "step": 12559
    },
    {
        "loss": 1.2727,
        "grad_norm": 1.62028169631958,
        "learning_rate": 0.0001228053378784008,
        "epoch": 1.6189739623614332,
        "step": 12560
    },
    {
        "loss": 1.8522,
        "grad_norm": 2.2949299812316895,
        "learning_rate": 0.00012274609654195707,
        "epoch": 1.6191028615622582,
        "step": 12561
    },
    {
        "loss": 2.2923,
        "grad_norm": 2.7802939414978027,
        "learning_rate": 0.0001226868467859375,
        "epoch": 1.6192317607630833,
        "step": 12562
    },
    {
        "loss": 1.654,
        "grad_norm": 3.6819546222686768,
        "learning_rate": 0.00012262758863227377,
        "epoch": 1.6193606599639083,
        "step": 12563
    },
    {
        "loss": 1.3015,
        "grad_norm": 2.700133800506592,
        "learning_rate": 0.00012256832210290043,
        "epoch": 1.6194895591647331,
        "step": 12564
    },
    {
        "loss": 1.4181,
        "grad_norm": 2.9090192317962646,
        "learning_rate": 0.00012250904721975537,
        "epoch": 1.6196184583655582,
        "step": 12565
    },
    {
        "loss": 1.6408,
        "grad_norm": 3.4173574447631836,
        "learning_rate": 0.00012244976400477945,
        "epoch": 1.619747357566383,
        "step": 12566
    },
    {
        "loss": 2.2192,
        "grad_norm": 2.4368932247161865,
        "learning_rate": 0.00012239047247991644,
        "epoch": 1.619876256767208,
        "step": 12567
    },
    {
        "loss": 1.9537,
        "grad_norm": 2.571930170059204,
        "learning_rate": 0.00012233117266711386,
        "epoch": 1.620005155968033,
        "step": 12568
    },
    {
        "loss": 1.9267,
        "grad_norm": 1.9322364330291748,
        "learning_rate": 0.0001222718645883214,
        "epoch": 1.6201340551688581,
        "step": 12569
    },
    {
        "loss": 1.5854,
        "grad_norm": 2.435668706893921,
        "learning_rate": 0.00012221254826549216,
        "epoch": 1.620262954369683,
        "step": 12570
    },
    {
        "loss": 1.7858,
        "grad_norm": 2.834414005279541,
        "learning_rate": 0.0001221532237205827,
        "epoch": 1.6203918535705077,
        "step": 12571
    },
    {
        "loss": 1.1422,
        "grad_norm": 3.164799690246582,
        "learning_rate": 0.0001220938909755521,
        "epoch": 1.6205207527713328,
        "step": 12572
    },
    {
        "loss": 1.5973,
        "grad_norm": 1.6306599378585815,
        "learning_rate": 0.00012203455005236259,
        "epoch": 1.6206496519721578,
        "step": 12573
    },
    {
        "loss": 1.9579,
        "grad_norm": 1.8278987407684326,
        "learning_rate": 0.00012197520097297986,
        "epoch": 1.6207785511729829,
        "step": 12574
    },
    {
        "loss": 1.494,
        "grad_norm": 1.7003040313720703,
        "learning_rate": 0.00012191584375937182,
        "epoch": 1.6209074503738077,
        "step": 12575
    },
    {
        "loss": 2.1278,
        "grad_norm": 2.4200820922851562,
        "learning_rate": 0.00012185647843350987,
        "epoch": 1.6210363495746325,
        "step": 12576
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.8211342096328735,
        "learning_rate": 0.00012179710501736858,
        "epoch": 1.6211652487754575,
        "step": 12577
    },
    {
        "loss": 0.895,
        "grad_norm": 2.1461048126220703,
        "learning_rate": 0.00012173772353292525,
        "epoch": 1.6212941479762826,
        "step": 12578
    },
    {
        "loss": 1.3739,
        "grad_norm": 2.069572687149048,
        "learning_rate": 0.00012167833400216015,
        "epoch": 1.6214230471771076,
        "step": 12579
    },
    {
        "loss": 1.1959,
        "grad_norm": 2.3912599086761475,
        "learning_rate": 0.00012161893644705662,
        "epoch": 1.6215519463779324,
        "step": 12580
    },
    {
        "loss": 1.9851,
        "grad_norm": 2.501021146774292,
        "learning_rate": 0.00012155953088960096,
        "epoch": 1.6216808455787572,
        "step": 12581
    },
    {
        "loss": 1.9054,
        "grad_norm": 2.9922988414764404,
        "learning_rate": 0.00012150011735178244,
        "epoch": 1.6218097447795823,
        "step": 12582
    },
    {
        "loss": 1.7142,
        "grad_norm": 2.5285677909851074,
        "learning_rate": 0.00012144069585559319,
        "epoch": 1.6219386439804073,
        "step": 12583
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.3000521659851074,
        "learning_rate": 0.0001213812664230283,
        "epoch": 1.6220675431812324,
        "step": 12584
    },
    {
        "loss": 0.7315,
        "grad_norm": 2.430558204650879,
        "learning_rate": 0.00012132182907608606,
        "epoch": 1.6221964423820572,
        "step": 12585
    },
    {
        "loss": 1.2393,
        "grad_norm": 3.7815141677856445,
        "learning_rate": 0.00012126238383676755,
        "epoch": 1.6223253415828822,
        "step": 12586
    },
    {
        "loss": 1.4943,
        "grad_norm": 2.6401076316833496,
        "learning_rate": 0.00012120293072707627,
        "epoch": 1.622454240783707,
        "step": 12587
    },
    {
        "loss": 1.66,
        "grad_norm": 1.9908515214920044,
        "learning_rate": 0.00012114346976901946,
        "epoch": 1.622583139984532,
        "step": 12588
    },
    {
        "loss": 0.7274,
        "grad_norm": 2.3601179122924805,
        "learning_rate": 0.0001210840009846068,
        "epoch": 1.6227120391853571,
        "step": 12589
    },
    {
        "loss": 1.236,
        "grad_norm": 2.861392021179199,
        "learning_rate": 0.00012102452439585077,
        "epoch": 1.6228409383861822,
        "step": 12590
    },
    {
        "loss": 1.8265,
        "grad_norm": 1.5074831247329712,
        "learning_rate": 0.00012096504002476728,
        "epoch": 1.622969837587007,
        "step": 12591
    },
    {
        "loss": 0.8827,
        "grad_norm": 2.3519704341888428,
        "learning_rate": 0.00012090554789337441,
        "epoch": 1.6230987367878318,
        "step": 12592
    },
    {
        "loss": 0.7289,
        "grad_norm": 4.366932392120361,
        "learning_rate": 0.00012084604802369344,
        "epoch": 1.6232276359886568,
        "step": 12593
    },
    {
        "loss": 2.4545,
        "grad_norm": 2.158059597015381,
        "learning_rate": 0.00012078654043774876,
        "epoch": 1.6233565351894819,
        "step": 12594
    },
    {
        "loss": 1.6479,
        "grad_norm": 2.4659578800201416,
        "learning_rate": 0.00012072702515756728,
        "epoch": 1.623485434390307,
        "step": 12595
    },
    {
        "loss": 2.4183,
        "grad_norm": 4.491206645965576,
        "learning_rate": 0.00012066750220517891,
        "epoch": 1.6236143335911317,
        "step": 12596
    },
    {
        "loss": 2.2801,
        "grad_norm": 1.9649550914764404,
        "learning_rate": 0.00012060797160261625,
        "epoch": 1.6237432327919565,
        "step": 12597
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.8640393018722534,
        "learning_rate": 0.00012054843337191494,
        "epoch": 1.6238721319927816,
        "step": 12598
    },
    {
        "loss": 1.4751,
        "grad_norm": 2.523862838745117,
        "learning_rate": 0.00012048888753511329,
        "epoch": 1.6240010311936066,
        "step": 12599
    },
    {
        "loss": 0.691,
        "grad_norm": 2.7533116340637207,
        "learning_rate": 0.00012042933411425245,
        "epoch": 1.6241299303944317,
        "step": 12600
    },
    {
        "loss": 1.4961,
        "grad_norm": 1.517259955406189,
        "learning_rate": 0.00012036977313137634,
        "epoch": 1.6242588295952565,
        "step": 12601
    },
    {
        "loss": 1.8672,
        "grad_norm": 2.38934326171875,
        "learning_rate": 0.00012031020460853193,
        "epoch": 1.6243877287960815,
        "step": 12602
    },
    {
        "loss": 1.4065,
        "grad_norm": 3.8809876441955566,
        "learning_rate": 0.00012025062856776885,
        "epoch": 1.6245166279969063,
        "step": 12603
    },
    {
        "loss": 1.3825,
        "grad_norm": 4.875489234924316,
        "learning_rate": 0.00012019104503113904,
        "epoch": 1.6246455271977314,
        "step": 12604
    },
    {
        "loss": 1.9754,
        "grad_norm": 2.7817018032073975,
        "learning_rate": 0.00012013145402069798,
        "epoch": 1.6247744263985564,
        "step": 12605
    },
    {
        "loss": 1.2912,
        "grad_norm": 2.505922555923462,
        "learning_rate": 0.00012007185555850353,
        "epoch": 1.6249033255993814,
        "step": 12606
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.3711671829223633,
        "learning_rate": 0.0001200122496666161,
        "epoch": 1.6250322248002063,
        "step": 12607
    },
    {
        "loss": 1.6824,
        "grad_norm": 2.065215587615967,
        "learning_rate": 0.00011995263636709959,
        "epoch": 1.625161124001031,
        "step": 12608
    },
    {
        "loss": 1.6044,
        "grad_norm": 2.1896965503692627,
        "learning_rate": 0.00011989301568201969,
        "epoch": 1.6252900232018561,
        "step": 12609
    },
    {
        "loss": 1.2284,
        "grad_norm": 1.3201541900634766,
        "learning_rate": 0.0001198333876334453,
        "epoch": 1.6254189224026812,
        "step": 12610
    },
    {
        "loss": 1.1563,
        "grad_norm": 1.3361917734146118,
        "learning_rate": 0.00011977375224344828,
        "epoch": 1.6255478216035062,
        "step": 12611
    },
    {
        "loss": 1.6148,
        "grad_norm": 1.8293265104293823,
        "learning_rate": 0.00011971410953410287,
        "epoch": 1.625676720804331,
        "step": 12612
    },
    {
        "loss": 1.6876,
        "grad_norm": 2.4132590293884277,
        "learning_rate": 0.000119654459527486,
        "epoch": 1.6258056200051558,
        "step": 12613
    },
    {
        "loss": 1.7241,
        "grad_norm": 2.8663525581359863,
        "learning_rate": 0.00011959480224567749,
        "epoch": 1.6259345192059809,
        "step": 12614
    },
    {
        "loss": 1.5415,
        "grad_norm": 2.3671889305114746,
        "learning_rate": 0.00011953513771075977,
        "epoch": 1.626063418406806,
        "step": 12615
    },
    {
        "loss": 2.3997,
        "grad_norm": 1.5965722799301147,
        "learning_rate": 0.00011947546594481786,
        "epoch": 1.626192317607631,
        "step": 12616
    },
    {
        "loss": 2.3382,
        "grad_norm": 1.798944354057312,
        "learning_rate": 0.00011941578696993968,
        "epoch": 1.6263212168084558,
        "step": 12617
    },
    {
        "loss": 1.9497,
        "grad_norm": 2.892051935195923,
        "learning_rate": 0.00011935610080821543,
        "epoch": 1.6264501160092806,
        "step": 12618
    },
    {
        "loss": 1.0307,
        "grad_norm": 3.418438196182251,
        "learning_rate": 0.00011929640748173855,
        "epoch": 1.6265790152101056,
        "step": 12619
    },
    {
        "loss": 1.7975,
        "grad_norm": 2.296219825744629,
        "learning_rate": 0.00011923670701260486,
        "epoch": 1.6267079144109307,
        "step": 12620
    },
    {
        "loss": 2.0347,
        "grad_norm": 2.319521427154541,
        "learning_rate": 0.00011917699942291225,
        "epoch": 1.6268368136117557,
        "step": 12621
    },
    {
        "loss": 1.5359,
        "grad_norm": 2.568236827850342,
        "learning_rate": 0.00011911728473476226,
        "epoch": 1.6269657128125805,
        "step": 12622
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.6109322309494019,
        "learning_rate": 0.00011905756297025843,
        "epoch": 1.6270946120134056,
        "step": 12623
    },
    {
        "loss": 1.8008,
        "grad_norm": 1.9534786939620972,
        "learning_rate": 0.00011899783415150687,
        "epoch": 1.6272235112142304,
        "step": 12624
    },
    {
        "loss": 0.9805,
        "grad_norm": 1.6690829992294312,
        "learning_rate": 0.00011893809830061682,
        "epoch": 1.6273524104150554,
        "step": 12625
    },
    {
        "loss": 1.2355,
        "grad_norm": 1.980247974395752,
        "learning_rate": 0.00011887835543969974,
        "epoch": 1.6274813096158804,
        "step": 12626
    },
    {
        "loss": 1.3696,
        "grad_norm": 1.833867073059082,
        "learning_rate": 0.00011881860559086932,
        "epoch": 1.6276102088167055,
        "step": 12627
    },
    {
        "loss": 1.2281,
        "grad_norm": 2.188662528991699,
        "learning_rate": 0.00011875884877624271,
        "epoch": 1.6277391080175303,
        "step": 12628
    },
    {
        "loss": 2.0823,
        "grad_norm": 1.7989771366119385,
        "learning_rate": 0.000118699085017939,
        "epoch": 1.6278680072183551,
        "step": 12629
    },
    {
        "loss": 1.7322,
        "grad_norm": 2.5107815265655518,
        "learning_rate": 0.00011863931433808002,
        "epoch": 1.6279969064191802,
        "step": 12630
    },
    {
        "loss": 1.9776,
        "grad_norm": 1.5772963762283325,
        "learning_rate": 0.00011857953675879024,
        "epoch": 1.6281258056200052,
        "step": 12631
    },
    {
        "loss": 1.7188,
        "grad_norm": 3.2213854789733887,
        "learning_rate": 0.00011851975230219654,
        "epoch": 1.6282547048208302,
        "step": 12632
    },
    {
        "loss": 0.9779,
        "grad_norm": 2.2055437564849854,
        "learning_rate": 0.00011845996099042844,
        "epoch": 1.628383604021655,
        "step": 12633
    },
    {
        "loss": 0.9917,
        "grad_norm": 3.3701725006103516,
        "learning_rate": 0.00011840016284561803,
        "epoch": 1.6285125032224799,
        "step": 12634
    },
    {
        "loss": 1.5659,
        "grad_norm": 2.216804027557373,
        "learning_rate": 0.00011834035788989965,
        "epoch": 1.628641402423305,
        "step": 12635
    },
    {
        "loss": 2.1042,
        "grad_norm": 1.777896761894226,
        "learning_rate": 0.00011828054614541076,
        "epoch": 1.62877030162413,
        "step": 12636
    },
    {
        "loss": 1.1978,
        "grad_norm": 4.3132004737854,
        "learning_rate": 0.00011822072763429089,
        "epoch": 1.628899200824955,
        "step": 12637
    },
    {
        "loss": 1.961,
        "grad_norm": 1.4616645574569702,
        "learning_rate": 0.00011816090237868175,
        "epoch": 1.6290281000257798,
        "step": 12638
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.8524144887924194,
        "learning_rate": 0.00011810107040072838,
        "epoch": 1.6291569992266048,
        "step": 12639
    },
    {
        "loss": 1.4623,
        "grad_norm": 2.9974777698516846,
        "learning_rate": 0.00011804123172257772,
        "epoch": 1.6292858984274297,
        "step": 12640
    },
    {
        "loss": 1.7526,
        "grad_norm": 1.6613484621047974,
        "learning_rate": 0.00011798138636637919,
        "epoch": 1.6294147976282547,
        "step": 12641
    },
    {
        "loss": 1.3525,
        "grad_norm": 2.917957067489624,
        "learning_rate": 0.00011792153435428513,
        "epoch": 1.6295436968290797,
        "step": 12642
    },
    {
        "loss": 1.3741,
        "grad_norm": 3.7328684329986572,
        "learning_rate": 0.00011786167570845004,
        "epoch": 1.6296725960299048,
        "step": 12643
    },
    {
        "loss": 1.5501,
        "grad_norm": 3.6163010597229004,
        "learning_rate": 0.00011780181045103043,
        "epoch": 1.6298014952307296,
        "step": 12644
    },
    {
        "loss": 1.8141,
        "grad_norm": 2.475945472717285,
        "learning_rate": 0.00011774193860418619,
        "epoch": 1.6299303944315544,
        "step": 12645
    },
    {
        "loss": 1.6014,
        "grad_norm": 4.4069342613220215,
        "learning_rate": 0.000117682060190079,
        "epoch": 1.6300592936323794,
        "step": 12646
    },
    {
        "loss": 1.1759,
        "grad_norm": 3.39068865776062,
        "learning_rate": 0.00011762217523087311,
        "epoch": 1.6301881928332045,
        "step": 12647
    },
    {
        "loss": 2.4457,
        "grad_norm": 1.944743037223816,
        "learning_rate": 0.00011756228374873531,
        "epoch": 1.6303170920340295,
        "step": 12648
    },
    {
        "loss": 2.2175,
        "grad_norm": 1.805019736289978,
        "learning_rate": 0.00011750238576583459,
        "epoch": 1.6304459912348543,
        "step": 12649
    },
    {
        "loss": 1.256,
        "grad_norm": 3.800353765487671,
        "learning_rate": 0.00011744248130434259,
        "epoch": 1.6305748904356792,
        "step": 12650
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.6826157569885254,
        "learning_rate": 0.00011738257038643316,
        "epoch": 1.6307037896365042,
        "step": 12651
    },
    {
        "loss": 1.5583,
        "grad_norm": 2.5248241424560547,
        "learning_rate": 0.00011732265303428252,
        "epoch": 1.6308326888373292,
        "step": 12652
    },
    {
        "loss": 1.8596,
        "grad_norm": 2.368962526321411,
        "learning_rate": 0.0001172627292700696,
        "epoch": 1.6309615880381543,
        "step": 12653
    },
    {
        "loss": 1.6134,
        "grad_norm": 2.114650011062622,
        "learning_rate": 0.00011720279911597547,
        "epoch": 1.631090487238979,
        "step": 12654
    },
    {
        "loss": 2.4966,
        "grad_norm": 2.145545721054077,
        "learning_rate": 0.00011714286259418317,
        "epoch": 1.631219386439804,
        "step": 12655
    },
    {
        "loss": 0.8153,
        "grad_norm": 3.905813455581665,
        "learning_rate": 0.0001170829197268789,
        "epoch": 1.631348285640629,
        "step": 12656
    },
    {
        "loss": 0.8089,
        "grad_norm": 2.090655565261841,
        "learning_rate": 0.00011702297053625064,
        "epoch": 1.631477184841454,
        "step": 12657
    },
    {
        "loss": 1.154,
        "grad_norm": 3.44061279296875,
        "learning_rate": 0.00011696301504448874,
        "epoch": 1.631606084042279,
        "step": 12658
    },
    {
        "loss": 1.7761,
        "grad_norm": 2.141101360321045,
        "learning_rate": 0.00011690305327378632,
        "epoch": 1.6317349832431038,
        "step": 12659
    },
    {
        "loss": 1.6945,
        "grad_norm": 3.573263168334961,
        "learning_rate": 0.00011684308524633842,
        "epoch": 1.6318638824439289,
        "step": 12660
    },
    {
        "loss": 2.0508,
        "grad_norm": 2.6699090003967285,
        "learning_rate": 0.00011678311098434221,
        "epoch": 1.6319927816447537,
        "step": 12661
    },
    {
        "loss": 2.0557,
        "grad_norm": 2.3125219345092773,
        "learning_rate": 0.00011672313050999775,
        "epoch": 1.6321216808455787,
        "step": 12662
    },
    {
        "loss": 1.6915,
        "grad_norm": 2.4784512519836426,
        "learning_rate": 0.000116663143845507,
        "epoch": 1.6322505800464038,
        "step": 12663
    },
    {
        "loss": 2.0716,
        "grad_norm": 3.1997525691986084,
        "learning_rate": 0.00011660315101307433,
        "epoch": 1.6323794792472288,
        "step": 12664
    },
    {
        "loss": 1.356,
        "grad_norm": 2.160844087600708,
        "learning_rate": 0.00011654315203490634,
        "epoch": 1.6325083784480536,
        "step": 12665
    },
    {
        "loss": 2.0356,
        "grad_norm": 2.0084757804870605,
        "learning_rate": 0.00011648314693321194,
        "epoch": 1.6326372776488784,
        "step": 12666
    },
    {
        "loss": 1.9124,
        "grad_norm": 2.392850637435913,
        "learning_rate": 0.00011642313573020234,
        "epoch": 1.6327661768497035,
        "step": 12667
    },
    {
        "loss": 1.5777,
        "grad_norm": 1.8669759035110474,
        "learning_rate": 0.00011636311844809093,
        "epoch": 1.6328950760505285,
        "step": 12668
    },
    {
        "loss": 1.4977,
        "grad_norm": 2.244372844696045,
        "learning_rate": 0.00011630309510909325,
        "epoch": 1.6330239752513536,
        "step": 12669
    },
    {
        "loss": 1.97,
        "grad_norm": 3.4321987628936768,
        "learning_rate": 0.00011624306573542756,
        "epoch": 1.6331528744521784,
        "step": 12670
    },
    {
        "loss": 0.8967,
        "grad_norm": 2.1382009983062744,
        "learning_rate": 0.00011618303034931395,
        "epoch": 1.6332817736530032,
        "step": 12671
    },
    {
        "loss": 0.7849,
        "grad_norm": 2.7499094009399414,
        "learning_rate": 0.00011612298897297442,
        "epoch": 1.6334106728538282,
        "step": 12672
    },
    {
        "loss": 1.377,
        "grad_norm": 3.6164286136627197,
        "learning_rate": 0.00011606294162863398,
        "epoch": 1.6335395720546533,
        "step": 12673
    },
    {
        "loss": 2.4051,
        "grad_norm": 1.89369535446167,
        "learning_rate": 0.00011600288833851931,
        "epoch": 1.6336684712554783,
        "step": 12674
    },
    {
        "loss": 2.5476,
        "grad_norm": 2.4395997524261475,
        "learning_rate": 0.0001159428291248593,
        "epoch": 1.6337973704563031,
        "step": 12675
    },
    {
        "loss": 1.3452,
        "grad_norm": 3.017071485519409,
        "learning_rate": 0.00011588276400988539,
        "epoch": 1.6339262696571282,
        "step": 12676
    },
    {
        "loss": 1.4915,
        "grad_norm": 2.4253387451171875,
        "learning_rate": 0.00011582269301583098,
        "epoch": 1.634055168857953,
        "step": 12677
    },
    {
        "loss": 1.9441,
        "grad_norm": 2.752382278442383,
        "learning_rate": 0.00011576261616493123,
        "epoch": 1.634184068058778,
        "step": 12678
    },
    {
        "loss": 1.6973,
        "grad_norm": 1.4266247749328613,
        "learning_rate": 0.00011570253347942426,
        "epoch": 1.634312967259603,
        "step": 12679
    },
    {
        "loss": 0.8305,
        "grad_norm": 3.442556858062744,
        "learning_rate": 0.00011564244498154989,
        "epoch": 1.634441866460428,
        "step": 12680
    },
    {
        "loss": 2.0198,
        "grad_norm": 1.9492361545562744,
        "learning_rate": 0.0001155823506935501,
        "epoch": 1.634570765661253,
        "step": 12681
    },
    {
        "loss": 0.9894,
        "grad_norm": 2.2434885501861572,
        "learning_rate": 0.00011552225063766908,
        "epoch": 1.6346996648620777,
        "step": 12682
    },
    {
        "loss": 2.5586,
        "grad_norm": 1.8082544803619385,
        "learning_rate": 0.00011546214483615319,
        "epoch": 1.6348285640629028,
        "step": 12683
    },
    {
        "loss": 1.9692,
        "grad_norm": 2.6192514896392822,
        "learning_rate": 0.00011540203331125083,
        "epoch": 1.6349574632637278,
        "step": 12684
    },
    {
        "loss": 1.8013,
        "grad_norm": 3.1346611976623535,
        "learning_rate": 0.0001153419160852126,
        "epoch": 1.6350863624645529,
        "step": 12685
    },
    {
        "loss": 2.0082,
        "grad_norm": 3.0017313957214355,
        "learning_rate": 0.00011528179318029107,
        "epoch": 1.6352152616653777,
        "step": 12686
    },
    {
        "loss": 0.9397,
        "grad_norm": 3.5995664596557617,
        "learning_rate": 0.0001152216646187412,
        "epoch": 1.6353441608662025,
        "step": 12687
    },
    {
        "loss": 2.1166,
        "grad_norm": 3.025941848754883,
        "learning_rate": 0.0001151615304228198,
        "epoch": 1.6354730600670275,
        "step": 12688
    },
    {
        "loss": 2.0806,
        "grad_norm": 1.670426607131958,
        "learning_rate": 0.00011510139061478582,
        "epoch": 1.6356019592678526,
        "step": 12689
    },
    {
        "loss": 0.8403,
        "grad_norm": 3.4119873046875,
        "learning_rate": 0.00011504124521690028,
        "epoch": 1.6357308584686776,
        "step": 12690
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.7813565731048584,
        "learning_rate": 0.00011498109425142629,
        "epoch": 1.6358597576695024,
        "step": 12691
    },
    {
        "loss": 1.7694,
        "grad_norm": 3.5711910724639893,
        "learning_rate": 0.00011492093774062889,
        "epoch": 1.6359886568703272,
        "step": 12692
    },
    {
        "loss": 1.4812,
        "grad_norm": 2.8017160892486572,
        "learning_rate": 0.00011486077570677554,
        "epoch": 1.6361175560711523,
        "step": 12693
    },
    {
        "loss": 1.294,
        "grad_norm": 4.346752166748047,
        "learning_rate": 0.00011480060817213553,
        "epoch": 1.6362464552719773,
        "step": 12694
    },
    {
        "loss": 1.2695,
        "grad_norm": 3.0482802391052246,
        "learning_rate": 0.00011474043515897979,
        "epoch": 1.6363753544728024,
        "step": 12695
    },
    {
        "loss": 0.8736,
        "grad_norm": 1.9096746444702148,
        "learning_rate": 0.00011468025668958195,
        "epoch": 1.6365042536736272,
        "step": 12696
    },
    {
        "loss": 2.075,
        "grad_norm": 2.4499831199645996,
        "learning_rate": 0.00011462007278621732,
        "epoch": 1.6366331528744522,
        "step": 12697
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.2532052993774414,
        "learning_rate": 0.0001145598834711632,
        "epoch": 1.636762052075277,
        "step": 12698
    },
    {
        "loss": 1.0264,
        "grad_norm": 3.257708787918091,
        "learning_rate": 0.00011449968876669896,
        "epoch": 1.636890951276102,
        "step": 12699
    },
    {
        "loss": 0.7887,
        "grad_norm": 1.1639952659606934,
        "learning_rate": 0.000114439488695106,
        "epoch": 1.637019850476927,
        "step": 12700
    },
    {
        "loss": 0.541,
        "grad_norm": 1.9943270683288574,
        "learning_rate": 0.00011437928327866761,
        "epoch": 1.6371487496777521,
        "step": 12701
    },
    {
        "loss": 1.6084,
        "grad_norm": 3.371154546737671,
        "learning_rate": 0.00011431907253966911,
        "epoch": 1.637277648878577,
        "step": 12702
    },
    {
        "loss": 1.9011,
        "grad_norm": 2.5805375576019287,
        "learning_rate": 0.00011425885650039772,
        "epoch": 1.6374065480794018,
        "step": 12703
    },
    {
        "loss": 1.9855,
        "grad_norm": 2.0379114151000977,
        "learning_rate": 0.00011419863518314287,
        "epoch": 1.6375354472802268,
        "step": 12704
    },
    {
        "loss": 1.9099,
        "grad_norm": 1.3136181831359863,
        "learning_rate": 0.00011413840861019571,
        "epoch": 1.6376643464810519,
        "step": 12705
    },
    {
        "loss": 1.8834,
        "grad_norm": 2.3977692127227783,
        "learning_rate": 0.00011407817680384938,
        "epoch": 1.637793245681877,
        "step": 12706
    },
    {
        "loss": 1.3257,
        "grad_norm": 3.2459499835968018,
        "learning_rate": 0.00011401793978639892,
        "epoch": 1.6379221448827017,
        "step": 12707
    },
    {
        "loss": 2.1288,
        "grad_norm": 2.632305145263672,
        "learning_rate": 0.0001139576975801414,
        "epoch": 1.6380510440835265,
        "step": 12708
    },
    {
        "loss": 1.4429,
        "grad_norm": 1.293234944343567,
        "learning_rate": 0.00011389745020737561,
        "epoch": 1.6381799432843516,
        "step": 12709
    },
    {
        "loss": 1.5101,
        "grad_norm": 2.8777377605438232,
        "learning_rate": 0.00011383719769040266,
        "epoch": 1.6383088424851766,
        "step": 12710
    },
    {
        "loss": 2.5752,
        "grad_norm": 2.5519492626190186,
        "learning_rate": 0.00011377694005152519,
        "epoch": 1.6384377416860016,
        "step": 12711
    },
    {
        "loss": 0.5574,
        "grad_norm": 4.973344326019287,
        "learning_rate": 0.00011371667731304783,
        "epoch": 1.6385666408868265,
        "step": 12712
    },
    {
        "loss": 1.1964,
        "grad_norm": 4.022456169128418,
        "learning_rate": 0.00011365640949727714,
        "epoch": 1.6386955400876515,
        "step": 12713
    },
    {
        "loss": 1.6827,
        "grad_norm": 5.527038097381592,
        "learning_rate": 0.00011359613662652153,
        "epoch": 1.6388244392884763,
        "step": 12714
    },
    {
        "loss": 1.6727,
        "grad_norm": 2.863651990890503,
        "learning_rate": 0.00011353585872309125,
        "epoch": 1.6389533384893014,
        "step": 12715
    },
    {
        "loss": 2.2952,
        "grad_norm": 3.7268402576446533,
        "learning_rate": 0.00011347557580929852,
        "epoch": 1.6390822376901264,
        "step": 12716
    },
    {
        "loss": 1.3455,
        "grad_norm": 3.0804364681243896,
        "learning_rate": 0.00011341528790745734,
        "epoch": 1.6392111368909514,
        "step": 12717
    },
    {
        "loss": 0.6198,
        "grad_norm": 2.2984273433685303,
        "learning_rate": 0.00011335499503988351,
        "epoch": 1.6393400360917763,
        "step": 12718
    },
    {
        "loss": 1.5919,
        "grad_norm": 3.0306735038757324,
        "learning_rate": 0.00011329469722889477,
        "epoch": 1.639468935292601,
        "step": 12719
    },
    {
        "loss": 1.7454,
        "grad_norm": 2.3461623191833496,
        "learning_rate": 0.00011323439449681048,
        "epoch": 1.639597834493426,
        "step": 12720
    },
    {
        "loss": 1.9412,
        "grad_norm": 1.8502763509750366,
        "learning_rate": 0.00011317408686595228,
        "epoch": 1.6397267336942511,
        "step": 12721
    },
    {
        "loss": 1.5771,
        "grad_norm": 2.546178102493286,
        "learning_rate": 0.00011311377435864314,
        "epoch": 1.6398556328950762,
        "step": 12722
    },
    {
        "loss": 0.6783,
        "grad_norm": 3.323420286178589,
        "learning_rate": 0.00011305345699720809,
        "epoch": 1.639984532095901,
        "step": 12723
    },
    {
        "loss": 1.2989,
        "grad_norm": 3.325183153152466,
        "learning_rate": 0.00011299313480397384,
        "epoch": 1.6401134312967258,
        "step": 12724
    },
    {
        "loss": 1.5841,
        "grad_norm": 2.468606472015381,
        "learning_rate": 0.00011293280780126896,
        "epoch": 1.6402423304975509,
        "step": 12725
    },
    {
        "loss": 1.6706,
        "grad_norm": 3.455010414123535,
        "learning_rate": 0.00011287247601142361,
        "epoch": 1.640371229698376,
        "step": 12726
    },
    {
        "loss": 1.3762,
        "grad_norm": 2.608901023864746,
        "learning_rate": 0.00011281213945677018,
        "epoch": 1.640500128899201,
        "step": 12727
    },
    {
        "loss": 1.586,
        "grad_norm": 1.8034595251083374,
        "learning_rate": 0.00011275179815964243,
        "epoch": 1.6406290281000258,
        "step": 12728
    },
    {
        "loss": 1.5823,
        "grad_norm": 3.0547614097595215,
        "learning_rate": 0.0001126914521423759,
        "epoch": 1.6407579273008506,
        "step": 12729
    },
    {
        "loss": 2.0054,
        "grad_norm": 1.9754619598388672,
        "learning_rate": 0.00011263110142730807,
        "epoch": 1.6408868265016756,
        "step": 12730
    },
    {
        "loss": 1.4351,
        "grad_norm": 3.0266191959381104,
        "learning_rate": 0.00011257074603677793,
        "epoch": 1.6410157257025006,
        "step": 12731
    },
    {
        "loss": 1.7075,
        "grad_norm": 2.5473618507385254,
        "learning_rate": 0.00011251038599312636,
        "epoch": 1.6411446249033257,
        "step": 12732
    },
    {
        "loss": 0.8877,
        "grad_norm": 2.9223859310150146,
        "learning_rate": 0.00011245002131869594,
        "epoch": 1.6412735241041505,
        "step": 12733
    },
    {
        "loss": 2.0138,
        "grad_norm": 2.895098924636841,
        "learning_rate": 0.00011238965203583091,
        "epoch": 1.6414024233049755,
        "step": 12734
    },
    {
        "loss": 1.5132,
        "grad_norm": 2.795989751815796,
        "learning_rate": 0.00011232927816687724,
        "epoch": 1.6415313225058004,
        "step": 12735
    },
    {
        "loss": 1.8389,
        "grad_norm": 3.0251331329345703,
        "learning_rate": 0.00011226889973418264,
        "epoch": 1.6416602217066254,
        "step": 12736
    },
    {
        "loss": 1.4857,
        "grad_norm": 1.403791069984436,
        "learning_rate": 0.00011220851676009631,
        "epoch": 1.6417891209074504,
        "step": 12737
    },
    {
        "loss": 2.3143,
        "grad_norm": 1.7621381282806396,
        "learning_rate": 0.0001121481292669696,
        "epoch": 1.6419180201082755,
        "step": 12738
    },
    {
        "loss": 1.0316,
        "grad_norm": 2.6025753021240234,
        "learning_rate": 0.00011208773727715507,
        "epoch": 1.6420469193091003,
        "step": 12739
    },
    {
        "loss": 1.7542,
        "grad_norm": 2.4422807693481445,
        "learning_rate": 0.00011202734081300705,
        "epoch": 1.6421758185099251,
        "step": 12740
    },
    {
        "loss": 1.4125,
        "grad_norm": 2.1851155757904053,
        "learning_rate": 0.00011196693989688167,
        "epoch": 1.6423047177107502,
        "step": 12741
    },
    {
        "loss": 1.0675,
        "grad_norm": 3.2368600368499756,
        "learning_rate": 0.0001119065345511366,
        "epoch": 1.6424336169115752,
        "step": 12742
    },
    {
        "loss": 1.5212,
        "grad_norm": 2.854858636856079,
        "learning_rate": 0.00011184612479813095,
        "epoch": 1.6425625161124002,
        "step": 12743
    },
    {
        "loss": 1.0996,
        "grad_norm": 2.17722749710083,
        "learning_rate": 0.00011178571066022605,
        "epoch": 1.642691415313225,
        "step": 12744
    },
    {
        "loss": 1.1449,
        "grad_norm": 3.021334648132324,
        "learning_rate": 0.00011172529215978424,
        "epoch": 1.6428203145140499,
        "step": 12745
    },
    {
        "loss": 1.9839,
        "grad_norm": 1.7563328742980957,
        "learning_rate": 0.00011166486931916984,
        "epoch": 1.642949213714875,
        "step": 12746
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.6944186687469482,
        "learning_rate": 0.00011160444216074856,
        "epoch": 1.6430781129157,
        "step": 12747
    },
    {
        "loss": 0.7654,
        "grad_norm": 3.190357208251953,
        "learning_rate": 0.00011154401070688782,
        "epoch": 1.643207012116525,
        "step": 12748
    },
    {
        "loss": 0.9084,
        "grad_norm": 2.5832784175872803,
        "learning_rate": 0.00011148357497995663,
        "epoch": 1.6433359113173498,
        "step": 12749
    },
    {
        "loss": 0.8655,
        "grad_norm": 4.312405586242676,
        "learning_rate": 0.00011142313500232556,
        "epoch": 1.6434648105181748,
        "step": 12750
    },
    {
        "loss": 2.5633,
        "grad_norm": 1.67423677444458,
        "learning_rate": 0.00011136269079636657,
        "epoch": 1.6435937097189997,
        "step": 12751
    },
    {
        "loss": 1.8858,
        "grad_norm": 1.8451780080795288,
        "learning_rate": 0.00011130224238445384,
        "epoch": 1.6437226089198247,
        "step": 12752
    },
    {
        "loss": 1.212,
        "grad_norm": 1.8533207178115845,
        "learning_rate": 0.0001112417897889622,
        "epoch": 1.6438515081206497,
        "step": 12753
    },
    {
        "loss": 1.5628,
        "grad_norm": 3.1545932292938232,
        "learning_rate": 0.00011118133303226845,
        "epoch": 1.6439804073214748,
        "step": 12754
    },
    {
        "loss": 2.0187,
        "grad_norm": 2.206475257873535,
        "learning_rate": 0.00011112087213675129,
        "epoch": 1.6441093065222996,
        "step": 12755
    },
    {
        "loss": 0.673,
        "grad_norm": 3.150824785232544,
        "learning_rate": 0.00011106040712479035,
        "epoch": 1.6442382057231244,
        "step": 12756
    },
    {
        "loss": 1.818,
        "grad_norm": 2.1301958560943604,
        "learning_rate": 0.00011099993801876718,
        "epoch": 1.6443671049239494,
        "step": 12757
    },
    {
        "loss": 1.9229,
        "grad_norm": 2.68357253074646,
        "learning_rate": 0.00011093946484106461,
        "epoch": 1.6444960041247745,
        "step": 12758
    },
    {
        "loss": 2.0631,
        "grad_norm": 3.272129535675049,
        "learning_rate": 0.0001108789876140671,
        "epoch": 1.6446249033255995,
        "step": 12759
    },
    {
        "loss": 1.4007,
        "grad_norm": 2.026278257369995,
        "learning_rate": 0.00011081850636016046,
        "epoch": 1.6447538025264243,
        "step": 12760
    },
    {
        "loss": 1.5461,
        "grad_norm": 1.9578160047531128,
        "learning_rate": 0.0001107580211017324,
        "epoch": 1.6448827017272492,
        "step": 12761
    },
    {
        "loss": 1.3977,
        "grad_norm": 2.3461430072784424,
        "learning_rate": 0.00011069753186117163,
        "epoch": 1.6450116009280742,
        "step": 12762
    },
    {
        "loss": 0.5747,
        "grad_norm": 1.1726640462875366,
        "learning_rate": 0.00011063703866086857,
        "epoch": 1.6451405001288992,
        "step": 12763
    },
    {
        "loss": 1.7565,
        "grad_norm": 2.6025123596191406,
        "learning_rate": 0.00011057654152321508,
        "epoch": 1.6452693993297243,
        "step": 12764
    },
    {
        "loss": 1.547,
        "grad_norm": 3.850046157836914,
        "learning_rate": 0.00011051604047060442,
        "epoch": 1.645398298530549,
        "step": 12765
    },
    {
        "loss": 1.6946,
        "grad_norm": 5.386803150177002,
        "learning_rate": 0.0001104555355254314,
        "epoch": 1.645527197731374,
        "step": 12766
    },
    {
        "loss": 1.194,
        "grad_norm": 3.9600613117218018,
        "learning_rate": 0.00011039502671009216,
        "epoch": 1.645656096932199,
        "step": 12767
    },
    {
        "loss": 1.7982,
        "grad_norm": 3.641026020050049,
        "learning_rate": 0.00011033451404698422,
        "epoch": 1.645784996133024,
        "step": 12768
    },
    {
        "loss": 1.0937,
        "grad_norm": 2.246284008026123,
        "learning_rate": 0.00011027399755850702,
        "epoch": 1.645913895333849,
        "step": 12769
    },
    {
        "loss": 1.4688,
        "grad_norm": 3.4995386600494385,
        "learning_rate": 0.00011021347726706067,
        "epoch": 1.6460427945346738,
        "step": 12770
    },
    {
        "loss": 1.4026,
        "grad_norm": 2.0007429122924805,
        "learning_rate": 0.00011015295319504695,
        "epoch": 1.6461716937354989,
        "step": 12771
    },
    {
        "loss": 1.8502,
        "grad_norm": 2.681877851486206,
        "learning_rate": 0.00011009242536486948,
        "epoch": 1.6463005929363237,
        "step": 12772
    },
    {
        "loss": 2.2243,
        "grad_norm": 2.379004716873169,
        "learning_rate": 0.00011003189379893272,
        "epoch": 1.6464294921371487,
        "step": 12773
    },
    {
        "loss": 2.2166,
        "grad_norm": 2.6476669311523438,
        "learning_rate": 0.00010997135851964281,
        "epoch": 1.6465583913379738,
        "step": 12774
    },
    {
        "loss": 1.7508,
        "grad_norm": 2.128542900085449,
        "learning_rate": 0.00010991081954940712,
        "epoch": 1.6466872905387988,
        "step": 12775
    },
    {
        "loss": 1.7865,
        "grad_norm": 1.9764518737792969,
        "learning_rate": 0.00010985027691063444,
        "epoch": 1.6468161897396236,
        "step": 12776
    },
    {
        "loss": 1.2332,
        "grad_norm": 1.9285203218460083,
        "learning_rate": 0.00010978973062573476,
        "epoch": 1.6469450889404484,
        "step": 12777
    },
    {
        "loss": 1.7171,
        "grad_norm": 3.4102323055267334,
        "learning_rate": 0.00010972918071711986,
        "epoch": 1.6470739881412735,
        "step": 12778
    },
    {
        "loss": 1.0377,
        "grad_norm": 2.463679313659668,
        "learning_rate": 0.00010966862720720248,
        "epoch": 1.6472028873420985,
        "step": 12779
    },
    {
        "loss": 2.3499,
        "grad_norm": 2.2156121730804443,
        "learning_rate": 0.00010960807011839675,
        "epoch": 1.6473317865429236,
        "step": 12780
    },
    {
        "loss": 1.7892,
        "grad_norm": 1.539803385734558,
        "learning_rate": 0.00010954750947311815,
        "epoch": 1.6474606857437484,
        "step": 12781
    },
    {
        "loss": 0.9193,
        "grad_norm": 2.9346961975097656,
        "learning_rate": 0.00010948694529378356,
        "epoch": 1.6475895849445732,
        "step": 12782
    },
    {
        "loss": 1.175,
        "grad_norm": 2.5553784370422363,
        "learning_rate": 0.00010942637760281103,
        "epoch": 1.6477184841453982,
        "step": 12783
    },
    {
        "loss": 0.9823,
        "grad_norm": 1.7217907905578613,
        "learning_rate": 0.00010936580642261997,
        "epoch": 1.6478473833462233,
        "step": 12784
    },
    {
        "loss": 2.3022,
        "grad_norm": 3.1176092624664307,
        "learning_rate": 0.00010930523177563101,
        "epoch": 1.6479762825470483,
        "step": 12785
    },
    {
        "loss": 2.4149,
        "grad_norm": 2.3337645530700684,
        "learning_rate": 0.00010924465368426655,
        "epoch": 1.6481051817478731,
        "step": 12786
    },
    {
        "loss": 1.47,
        "grad_norm": 2.4266867637634277,
        "learning_rate": 0.0001091840721709494,
        "epoch": 1.6482340809486982,
        "step": 12787
    },
    {
        "loss": 1.0838,
        "grad_norm": 2.6107800006866455,
        "learning_rate": 0.00010912348725810412,
        "epoch": 1.648362980149523,
        "step": 12788
    },
    {
        "loss": 1.799,
        "grad_norm": 2.63653302192688,
        "learning_rate": 0.00010906289896815679,
        "epoch": 1.648491879350348,
        "step": 12789
    },
    {
        "loss": 1.5761,
        "grad_norm": 2.6270713806152344,
        "learning_rate": 0.0001090023073235343,
        "epoch": 1.648620778551173,
        "step": 12790
    },
    {
        "loss": 1.5076,
        "grad_norm": 3.51693058013916,
        "learning_rate": 0.00010894171234666481,
        "epoch": 1.648749677751998,
        "step": 12791
    },
    {
        "loss": 1.7223,
        "grad_norm": 2.349656343460083,
        "learning_rate": 0.00010888111405997824,
        "epoch": 1.648878576952823,
        "step": 12792
    },
    {
        "loss": 1.5059,
        "grad_norm": 2.3857874870300293,
        "learning_rate": 0.00010882051248590493,
        "epoch": 1.6490074761536477,
        "step": 12793
    },
    {
        "loss": 1.4093,
        "grad_norm": 2.5767557621002197,
        "learning_rate": 0.00010875990764687684,
        "epoch": 1.6491363753544728,
        "step": 12794
    },
    {
        "loss": 1.2993,
        "grad_norm": 3.240779399871826,
        "learning_rate": 0.00010869929956532739,
        "epoch": 1.6492652745552978,
        "step": 12795
    },
    {
        "loss": 2.0789,
        "grad_norm": 2.2757999897003174,
        "learning_rate": 0.00010863868826369086,
        "epoch": 1.6493941737561228,
        "step": 12796
    },
    {
        "loss": 1.8476,
        "grad_norm": 2.0772244930267334,
        "learning_rate": 0.00010857807376440279,
        "epoch": 1.6495230729569477,
        "step": 12797
    },
    {
        "loss": 1.3671,
        "grad_norm": 3.0971996784210205,
        "learning_rate": 0.00010851745608989989,
        "epoch": 1.6496519721577725,
        "step": 12798
    },
    {
        "loss": 2.1935,
        "grad_norm": 3.536129951477051,
        "learning_rate": 0.00010845683526262014,
        "epoch": 1.6497808713585975,
        "step": 12799
    },
    {
        "loss": 1.5766,
        "grad_norm": 2.026966094970703,
        "learning_rate": 0.0001083962113050026,
        "epoch": 1.6499097705594226,
        "step": 12800
    },
    {
        "loss": 0.4392,
        "grad_norm": 1.5928353071212769,
        "learning_rate": 0.00010833558423948749,
        "epoch": 1.6500386697602476,
        "step": 12801
    },
    {
        "loss": 1.6263,
        "grad_norm": 2.215153217315674,
        "learning_rate": 0.00010827495408851613,
        "epoch": 1.6501675689610724,
        "step": 12802
    },
    {
        "loss": 1.8098,
        "grad_norm": 1.5690133571624756,
        "learning_rate": 0.00010821432087453143,
        "epoch": 1.6502964681618972,
        "step": 12803
    },
    {
        "loss": 1.688,
        "grad_norm": 2.6960268020629883,
        "learning_rate": 0.00010815368461997661,
        "epoch": 1.6504253673627223,
        "step": 12804
    },
    {
        "loss": 0.7009,
        "grad_norm": 2.9143805503845215,
        "learning_rate": 0.00010809304534729657,
        "epoch": 1.6505542665635473,
        "step": 12805
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.6243913173675537,
        "learning_rate": 0.0001080324030789374,
        "epoch": 1.6506831657643724,
        "step": 12806
    },
    {
        "loss": 0.6644,
        "grad_norm": 2.7986814975738525,
        "learning_rate": 0.00010797175783734606,
        "epoch": 1.6508120649651972,
        "step": 12807
    },
    {
        "loss": 2.0233,
        "grad_norm": 2.447793960571289,
        "learning_rate": 0.00010791110964497049,
        "epoch": 1.6509409641660222,
        "step": 12808
    },
    {
        "loss": 2.0897,
        "grad_norm": 2.526948928833008,
        "learning_rate": 0.00010785045852426034,
        "epoch": 1.651069863366847,
        "step": 12809
    },
    {
        "loss": 1.8318,
        "grad_norm": 1.5756882429122925,
        "learning_rate": 0.00010778980449766551,
        "epoch": 1.651198762567672,
        "step": 12810
    },
    {
        "loss": 1.3578,
        "grad_norm": 2.913985013961792,
        "learning_rate": 0.00010772914758763734,
        "epoch": 1.651327661768497,
        "step": 12811
    },
    {
        "loss": 1.6887,
        "grad_norm": 3.72944974899292,
        "learning_rate": 0.00010766848781662859,
        "epoch": 1.6514565609693221,
        "step": 12812
    },
    {
        "loss": 0.7494,
        "grad_norm": 2.858961343765259,
        "learning_rate": 0.0001076078252070926,
        "epoch": 1.651585460170147,
        "step": 12813
    },
    {
        "loss": 2.4296,
        "grad_norm": 2.0637478828430176,
        "learning_rate": 0.00010754715978148391,
        "epoch": 1.6517143593709718,
        "step": 12814
    },
    {
        "loss": 1.3362,
        "grad_norm": 2.5182716846466064,
        "learning_rate": 0.00010748649156225816,
        "epoch": 1.6518432585717968,
        "step": 12815
    },
    {
        "loss": 1.0055,
        "grad_norm": 2.7280943393707275,
        "learning_rate": 0.00010742582057187196,
        "epoch": 1.6519721577726219,
        "step": 12816
    },
    {
        "loss": 1.3821,
        "grad_norm": 2.7119975090026855,
        "learning_rate": 0.00010736514683278291,
        "epoch": 1.652101056973447,
        "step": 12817
    },
    {
        "loss": 2.1827,
        "grad_norm": 2.3908989429473877,
        "learning_rate": 0.0001073044703674498,
        "epoch": 1.6522299561742717,
        "step": 12818
    },
    {
        "loss": 2.0488,
        "grad_norm": 2.5674893856048584,
        "learning_rate": 0.00010724379119833206,
        "epoch": 1.6523588553750965,
        "step": 12819
    },
    {
        "loss": 1.3186,
        "grad_norm": 2.9796993732452393,
        "learning_rate": 0.00010718310934789084,
        "epoch": 1.6524877545759216,
        "step": 12820
    },
    {
        "loss": 2.1508,
        "grad_norm": 1.9479042291641235,
        "learning_rate": 0.00010712242483858741,
        "epoch": 1.6526166537767466,
        "step": 12821
    },
    {
        "loss": 1.5355,
        "grad_norm": 2.1471333503723145,
        "learning_rate": 0.00010706173769288441,
        "epoch": 1.6527455529775716,
        "step": 12822
    },
    {
        "loss": 1.3465,
        "grad_norm": 2.660637140274048,
        "learning_rate": 0.00010700104793324577,
        "epoch": 1.6528744521783965,
        "step": 12823
    },
    {
        "loss": 1.8167,
        "grad_norm": 1.4876965284347534,
        "learning_rate": 0.00010694035558213594,
        "epoch": 1.6530033513792215,
        "step": 12824
    },
    {
        "loss": 0.26,
        "grad_norm": 2.0939056873321533,
        "learning_rate": 0.00010687966066202033,
        "epoch": 1.6531322505800463,
        "step": 12825
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.7565003633499146,
        "learning_rate": 0.00010681896319536588,
        "epoch": 1.6532611497808714,
        "step": 12826
    },
    {
        "loss": 1.9008,
        "grad_norm": 2.2852256298065186,
        "learning_rate": 0.00010675826320463967,
        "epoch": 1.6533900489816964,
        "step": 12827
    },
    {
        "loss": 1.5985,
        "grad_norm": 2.7643284797668457,
        "learning_rate": 0.00010669756071231001,
        "epoch": 1.6535189481825214,
        "step": 12828
    },
    {
        "loss": 0.907,
        "grad_norm": 3.918627977371216,
        "learning_rate": 0.00010663685574084659,
        "epoch": 1.6536478473833462,
        "step": 12829
    },
    {
        "loss": 1.9495,
        "grad_norm": 2.1090664863586426,
        "learning_rate": 0.00010657614831271942,
        "epoch": 1.653776746584171,
        "step": 12830
    },
    {
        "loss": 2.0418,
        "grad_norm": 1.7331719398498535,
        "learning_rate": 0.00010651543845039969,
        "epoch": 1.653905645784996,
        "step": 12831
    },
    {
        "loss": 2.1269,
        "grad_norm": 2.002744674682617,
        "learning_rate": 0.00010645472617635941,
        "epoch": 1.6540345449858211,
        "step": 12832
    },
    {
        "loss": 1.5982,
        "grad_norm": 3.503509759902954,
        "learning_rate": 0.00010639401151307158,
        "epoch": 1.6541634441866462,
        "step": 12833
    },
    {
        "loss": 1.1242,
        "grad_norm": 2.819648504257202,
        "learning_rate": 0.00010633329448300993,
        "epoch": 1.654292343387471,
        "step": 12834
    },
    {
        "loss": 1.8654,
        "grad_norm": 2.3525302410125732,
        "learning_rate": 0.00010627257510864925,
        "epoch": 1.6544212425882958,
        "step": 12835
    },
    {
        "loss": 1.258,
        "grad_norm": 2.7717721462249756,
        "learning_rate": 0.00010621185341246489,
        "epoch": 1.6545501417891209,
        "step": 12836
    },
    {
        "loss": 2.2935,
        "grad_norm": 2.255481719970703,
        "learning_rate": 0.00010615112941693375,
        "epoch": 1.654679040989946,
        "step": 12837
    },
    {
        "loss": 2.1213,
        "grad_norm": 1.8655829429626465,
        "learning_rate": 0.00010609040314453267,
        "epoch": 1.654807940190771,
        "step": 12838
    },
    {
        "loss": 1.4265,
        "grad_norm": 3.2228591442108154,
        "learning_rate": 0.00010602967461773973,
        "epoch": 1.6549368393915957,
        "step": 12839
    },
    {
        "loss": 0.951,
        "grad_norm": 2.353867292404175,
        "learning_rate": 0.00010596894385903422,
        "epoch": 1.6550657385924206,
        "step": 12840
    },
    {
        "loss": 1.8033,
        "grad_norm": 2.1561965942382812,
        "learning_rate": 0.00010590821089089576,
        "epoch": 1.6551946377932456,
        "step": 12841
    },
    {
        "loss": 2.1216,
        "grad_norm": 2.3779525756835938,
        "learning_rate": 0.00010584747573580479,
        "epoch": 1.6553235369940706,
        "step": 12842
    },
    {
        "loss": 1.1128,
        "grad_norm": 2.420264720916748,
        "learning_rate": 0.00010578673841624314,
        "epoch": 1.6554524361948957,
        "step": 12843
    },
    {
        "loss": 1.9039,
        "grad_norm": 2.3289244174957275,
        "learning_rate": 0.00010572599895469262,
        "epoch": 1.6555813353957205,
        "step": 12844
    },
    {
        "loss": 2.2487,
        "grad_norm": 2.2337887287139893,
        "learning_rate": 0.0001056652573736362,
        "epoch": 1.6557102345965455,
        "step": 12845
    },
    {
        "loss": 1.8582,
        "grad_norm": 2.506556272506714,
        "learning_rate": 0.00010560451369555797,
        "epoch": 1.6558391337973704,
        "step": 12846
    },
    {
        "loss": 1.7572,
        "grad_norm": 3.188990831375122,
        "learning_rate": 0.00010554376794294231,
        "epoch": 1.6559680329981954,
        "step": 12847
    },
    {
        "loss": 1.2113,
        "grad_norm": 2.306436777114868,
        "learning_rate": 0.00010548302013827463,
        "epoch": 1.6560969321990204,
        "step": 12848
    },
    {
        "loss": 2.0609,
        "grad_norm": 1.6514900922775269,
        "learning_rate": 0.00010542227030404097,
        "epoch": 1.6562258313998455,
        "step": 12849
    },
    {
        "loss": 1.7198,
        "grad_norm": 1.871501088142395,
        "learning_rate": 0.00010536151846272812,
        "epoch": 1.6563547306006703,
        "step": 12850
    },
    {
        "loss": 2.0147,
        "grad_norm": 1.5426193475723267,
        "learning_rate": 0.00010530076463682377,
        "epoch": 1.656483629801495,
        "step": 12851
    },
    {
        "loss": 0.2057,
        "grad_norm": 1.5126152038574219,
        "learning_rate": 0.00010524000884881615,
        "epoch": 1.6566125290023201,
        "step": 12852
    },
    {
        "loss": 2.0151,
        "grad_norm": 1.6639968156814575,
        "learning_rate": 0.0001051792511211942,
        "epoch": 1.6567414282031452,
        "step": 12853
    },
    {
        "loss": 1.6651,
        "grad_norm": 2.432913064956665,
        "learning_rate": 0.00010511849147644795,
        "epoch": 1.6568703274039702,
        "step": 12854
    },
    {
        "loss": 1.9714,
        "grad_norm": 1.8500484228134155,
        "learning_rate": 0.00010505772993706788,
        "epoch": 1.656999226604795,
        "step": 12855
    },
    {
        "loss": 2.0802,
        "grad_norm": 1.7816380262374878,
        "learning_rate": 0.00010499696652554473,
        "epoch": 1.6571281258056199,
        "step": 12856
    },
    {
        "loss": 1.0857,
        "grad_norm": 3.7702174186706543,
        "learning_rate": 0.00010493620126437082,
        "epoch": 1.657257025006445,
        "step": 12857
    },
    {
        "loss": 1.9901,
        "grad_norm": 2.625345468521118,
        "learning_rate": 0.00010487543417603853,
        "epoch": 1.65738592420727,
        "step": 12858
    },
    {
        "loss": 1.6245,
        "grad_norm": 4.360247611999512,
        "learning_rate": 0.00010481466528304095,
        "epoch": 1.657514823408095,
        "step": 12859
    },
    {
        "loss": 2.2452,
        "grad_norm": 3.188727855682373,
        "learning_rate": 0.00010475389460787242,
        "epoch": 1.6576437226089198,
        "step": 12860
    },
    {
        "loss": 2.2963,
        "grad_norm": 1.9223533868789673,
        "learning_rate": 0.00010469312217302701,
        "epoch": 1.6577726218097448,
        "step": 12861
    },
    {
        "loss": 1.0913,
        "grad_norm": 1.9378950595855713,
        "learning_rate": 0.00010463234800099999,
        "epoch": 1.6579015210105696,
        "step": 12862
    },
    {
        "loss": 2.1448,
        "grad_norm": 3.624591588973999,
        "learning_rate": 0.00010457157211428748,
        "epoch": 1.6580304202113947,
        "step": 12863
    },
    {
        "loss": 1.8838,
        "grad_norm": 2.115086317062378,
        "learning_rate": 0.0001045107945353858,
        "epoch": 1.6581593194122197,
        "step": 12864
    },
    {
        "loss": 0.9361,
        "grad_norm": 3.4027841091156006,
        "learning_rate": 0.00010445001528679206,
        "epoch": 1.6582882186130448,
        "step": 12865
    },
    {
        "loss": 1.6683,
        "grad_norm": 2.2026526927948,
        "learning_rate": 0.00010438923439100401,
        "epoch": 1.6584171178138696,
        "step": 12866
    },
    {
        "loss": 1.6354,
        "grad_norm": 2.666532039642334,
        "learning_rate": 0.00010432845187051996,
        "epoch": 1.6585460170146944,
        "step": 12867
    },
    {
        "loss": 2.2478,
        "grad_norm": 3.6584713459014893,
        "learning_rate": 0.0001042676677478389,
        "epoch": 1.6586749162155194,
        "step": 12868
    },
    {
        "loss": 2.1988,
        "grad_norm": 4.369727611541748,
        "learning_rate": 0.0001042068820454603,
        "epoch": 1.6588038154163445,
        "step": 12869
    },
    {
        "loss": 0.6142,
        "grad_norm": 3.2059271335601807,
        "learning_rate": 0.00010414609478588417,
        "epoch": 1.6589327146171695,
        "step": 12870
    },
    {
        "loss": 1.1132,
        "grad_norm": 3.8059356212615967,
        "learning_rate": 0.00010408530599161149,
        "epoch": 1.6590616138179943,
        "step": 12871
    },
    {
        "loss": 1.4351,
        "grad_norm": 3.0670671463012695,
        "learning_rate": 0.00010402451568514349,
        "epoch": 1.6591905130188191,
        "step": 12872
    },
    {
        "loss": 1.9142,
        "grad_norm": 1.7093994617462158,
        "learning_rate": 0.00010396372388898159,
        "epoch": 1.6593194122196442,
        "step": 12873
    },
    {
        "loss": 1.7248,
        "grad_norm": 2.4268181324005127,
        "learning_rate": 0.0001039029306256286,
        "epoch": 1.6594483114204692,
        "step": 12874
    },
    {
        "loss": 2.1802,
        "grad_norm": 2.4166696071624756,
        "learning_rate": 0.00010384213591758724,
        "epoch": 1.6595772106212943,
        "step": 12875
    },
    {
        "loss": 0.6459,
        "grad_norm": 3.330872058868408,
        "learning_rate": 0.00010378133978736089,
        "epoch": 1.659706109822119,
        "step": 12876
    },
    {
        "loss": 1.5574,
        "grad_norm": 2.6528565883636475,
        "learning_rate": 0.00010372054225745376,
        "epoch": 1.659835009022944,
        "step": 12877
    },
    {
        "loss": 1.3111,
        "grad_norm": 1.4921194314956665,
        "learning_rate": 0.00010365974335037036,
        "epoch": 1.659963908223769,
        "step": 12878
    },
    {
        "loss": 1.3041,
        "grad_norm": 2.9818544387817383,
        "learning_rate": 0.0001035989430886153,
        "epoch": 1.660092807424594,
        "step": 12879
    },
    {
        "loss": 1.6087,
        "grad_norm": 2.2012414932250977,
        "learning_rate": 0.00010353814149469446,
        "epoch": 1.660221706625419,
        "step": 12880
    },
    {
        "loss": 0.8182,
        "grad_norm": 2.577357769012451,
        "learning_rate": 0.00010347733859111372,
        "epoch": 1.6603506058262438,
        "step": 12881
    },
    {
        "loss": 0.8572,
        "grad_norm": 2.711678981781006,
        "learning_rate": 0.00010341653440037957,
        "epoch": 1.6604795050270689,
        "step": 12882
    },
    {
        "loss": 1.4712,
        "grad_norm": 3.3397719860076904,
        "learning_rate": 0.00010335572894499894,
        "epoch": 1.6606084042278937,
        "step": 12883
    },
    {
        "loss": 1.9493,
        "grad_norm": 2.027460813522339,
        "learning_rate": 0.00010329492224747932,
        "epoch": 1.6607373034287187,
        "step": 12884
    },
    {
        "loss": 1.7525,
        "grad_norm": 2.8706247806549072,
        "learning_rate": 0.00010323411433032855,
        "epoch": 1.6608662026295438,
        "step": 12885
    },
    {
        "loss": 2.1751,
        "grad_norm": 2.3370649814605713,
        "learning_rate": 0.00010317330521605495,
        "epoch": 1.6609951018303688,
        "step": 12886
    },
    {
        "loss": 2.1338,
        "grad_norm": 3.411979913711548,
        "learning_rate": 0.00010311249492716721,
        "epoch": 1.6611240010311936,
        "step": 12887
    },
    {
        "loss": 1.5717,
        "grad_norm": 3.6423349380493164,
        "learning_rate": 0.0001030516834861748,
        "epoch": 1.6612529002320184,
        "step": 12888
    },
    {
        "loss": 1.4451,
        "grad_norm": 2.8422763347625732,
        "learning_rate": 0.00010299087091558738,
        "epoch": 1.6613817994328435,
        "step": 12889
    },
    {
        "loss": 1.7797,
        "grad_norm": 3.408158540725708,
        "learning_rate": 0.00010293005723791462,
        "epoch": 1.6615106986336685,
        "step": 12890
    },
    {
        "loss": 1.1937,
        "grad_norm": 2.540107011795044,
        "learning_rate": 0.00010286924247566733,
        "epoch": 1.6616395978344936,
        "step": 12891
    },
    {
        "loss": 1.6511,
        "grad_norm": 1.5144942998886108,
        "learning_rate": 0.00010280842665135633,
        "epoch": 1.6617684970353184,
        "step": 12892
    },
    {
        "loss": 1.0512,
        "grad_norm": 3.0078911781311035,
        "learning_rate": 0.00010274760978749271,
        "epoch": 1.6618973962361432,
        "step": 12893
    },
    {
        "loss": 1.4926,
        "grad_norm": 3.5372612476348877,
        "learning_rate": 0.00010268679190658836,
        "epoch": 1.6620262954369682,
        "step": 12894
    },
    {
        "loss": 1.5327,
        "grad_norm": 1.9575320482254028,
        "learning_rate": 0.00010262597303115539,
        "epoch": 1.6621551946377933,
        "step": 12895
    },
    {
        "loss": 1.2843,
        "grad_norm": 1.9282342195510864,
        "learning_rate": 0.00010256515318370572,
        "epoch": 1.6622840938386183,
        "step": 12896
    },
    {
        "loss": 2.3639,
        "grad_norm": 2.8098928928375244,
        "learning_rate": 0.00010250433238675254,
        "epoch": 1.6624129930394431,
        "step": 12897
    },
    {
        "loss": 1.4,
        "grad_norm": 1.6908228397369385,
        "learning_rate": 0.00010244351066280881,
        "epoch": 1.6625418922402682,
        "step": 12898
    },
    {
        "loss": 1.2965,
        "grad_norm": 2.6772301197052,
        "learning_rate": 0.00010238268803438799,
        "epoch": 1.662670791441093,
        "step": 12899
    },
    {
        "loss": 1.7784,
        "grad_norm": 1.63527250289917,
        "learning_rate": 0.00010232186452400387,
        "epoch": 1.662799690641918,
        "step": 12900
    },
    {
        "loss": 2.089,
        "grad_norm": 1.884592056274414,
        "learning_rate": 0.00010226104015417055,
        "epoch": 1.662928589842743,
        "step": 12901
    },
    {
        "loss": 0.9189,
        "grad_norm": 2.476442813873291,
        "learning_rate": 0.00010220021494740246,
        "epoch": 1.663057489043568,
        "step": 12902
    },
    {
        "loss": 1.3492,
        "grad_norm": 2.38029146194458,
        "learning_rate": 0.00010213938892621433,
        "epoch": 1.663186388244393,
        "step": 12903
    },
    {
        "loss": 1.8959,
        "grad_norm": 2.4862546920776367,
        "learning_rate": 0.00010207856211312113,
        "epoch": 1.6633152874452177,
        "step": 12904
    },
    {
        "loss": 2.4272,
        "grad_norm": 2.929448127746582,
        "learning_rate": 0.00010201773453063835,
        "epoch": 1.6634441866460428,
        "step": 12905
    },
    {
        "loss": 2.0645,
        "grad_norm": 1.795357346534729,
        "learning_rate": 0.00010195690620128171,
        "epoch": 1.6635730858468678,
        "step": 12906
    },
    {
        "loss": 2.292,
        "grad_norm": 1.8368819952011108,
        "learning_rate": 0.00010189607714756666,
        "epoch": 1.6637019850476928,
        "step": 12907
    },
    {
        "loss": 0.3541,
        "grad_norm": 1.5860780477523804,
        "learning_rate": 0.00010183524739200975,
        "epoch": 1.6638308842485177,
        "step": 12908
    },
    {
        "loss": 1.5426,
        "grad_norm": 2.3987720012664795,
        "learning_rate": 0.00010177441695712733,
        "epoch": 1.6639597834493425,
        "step": 12909
    },
    {
        "loss": 0.6975,
        "grad_norm": 2.21000599861145,
        "learning_rate": 0.00010171358586543584,
        "epoch": 1.6640886826501675,
        "step": 12910
    },
    {
        "loss": 1.8118,
        "grad_norm": 3.9447128772735596,
        "learning_rate": 0.0001016527541394526,
        "epoch": 1.6642175818509926,
        "step": 12911
    },
    {
        "loss": 1.3535,
        "grad_norm": 1.8967113494873047,
        "learning_rate": 0.00010159192180169464,
        "epoch": 1.6643464810518176,
        "step": 12912
    },
    {
        "loss": 1.8266,
        "grad_norm": 2.5192413330078125,
        "learning_rate": 0.00010153108887467899,
        "epoch": 1.6644753802526424,
        "step": 12913
    },
    {
        "loss": 1.2823,
        "grad_norm": 3.879136085510254,
        "learning_rate": 0.00010147025538092367,
        "epoch": 1.6646042794534672,
        "step": 12914
    },
    {
        "loss": 1.3534,
        "grad_norm": 2.1854965686798096,
        "learning_rate": 0.00010140942134294631,
        "epoch": 1.6647331786542923,
        "step": 12915
    },
    {
        "loss": 1.7352,
        "grad_norm": 2.9803600311279297,
        "learning_rate": 0.00010134858678326493,
        "epoch": 1.6648620778551173,
        "step": 12916
    },
    {
        "loss": 1.2661,
        "grad_norm": 2.986461877822876,
        "learning_rate": 0.00010128775172439774,
        "epoch": 1.6649909770559423,
        "step": 12917
    },
    {
        "loss": 1.5413,
        "grad_norm": 2.1859400272369385,
        "learning_rate": 0.00010122691618886315,
        "epoch": 1.6651198762567672,
        "step": 12918
    },
    {
        "loss": 0.9782,
        "grad_norm": 3.4477927684783936,
        "learning_rate": 0.00010116608019917968,
        "epoch": 1.6652487754575922,
        "step": 12919
    },
    {
        "loss": 1.5251,
        "grad_norm": 3.6980843544006348,
        "learning_rate": 0.00010110524377786607,
        "epoch": 1.665377674658417,
        "step": 12920
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.359734296798706,
        "learning_rate": 0.00010104440694744111,
        "epoch": 1.665506573859242,
        "step": 12921
    },
    {
        "loss": 1.1212,
        "grad_norm": 1.8076260089874268,
        "learning_rate": 0.00010098356973042407,
        "epoch": 1.665635473060067,
        "step": 12922
    },
    {
        "loss": 1.5113,
        "grad_norm": 2.110816717147827,
        "learning_rate": 0.00010092273214933417,
        "epoch": 1.6657643722608921,
        "step": 12923
    },
    {
        "loss": 1.7703,
        "grad_norm": 1.7790125608444214,
        "learning_rate": 0.00010086189422669026,
        "epoch": 1.665893271461717,
        "step": 12924
    },
    {
        "loss": 1.0654,
        "grad_norm": 1.8206110000610352,
        "learning_rate": 0.00010080105598501228,
        "epoch": 1.6660221706625418,
        "step": 12925
    },
    {
        "loss": 1.9306,
        "grad_norm": 2.56303334236145,
        "learning_rate": 0.00010074021744681961,
        "epoch": 1.6661510698633668,
        "step": 12926
    },
    {
        "loss": 2.5867,
        "grad_norm": 2.442403793334961,
        "learning_rate": 0.00010067937863463179,
        "epoch": 1.6662799690641918,
        "step": 12927
    },
    {
        "loss": 2.0271,
        "grad_norm": 2.442035436630249,
        "learning_rate": 0.00010061853957096887,
        "epoch": 1.6664088682650169,
        "step": 12928
    },
    {
        "loss": 2.1096,
        "grad_norm": 2.95915150642395,
        "learning_rate": 0.00010055770027835074,
        "epoch": 1.6665377674658417,
        "step": 12929
    },
    {
        "loss": 0.8064,
        "grad_norm": 2.8133797645568848,
        "learning_rate": 0.0001004968607792969,
        "epoch": 1.6666666666666665,
        "step": 12930
    },
    {
        "loss": 1.3755,
        "grad_norm": 2.264014482498169,
        "learning_rate": 0.00010043602109632784,
        "epoch": 1.6667955658674916,
        "step": 12931
    },
    {
        "loss": 1.8228,
        "grad_norm": 3.0422561168670654,
        "learning_rate": 0.0001003751812519635,
        "epoch": 1.6669244650683166,
        "step": 12932
    },
    {
        "loss": 1.4059,
        "grad_norm": 1.9697595834732056,
        "learning_rate": 0.00010031434126872402,
        "epoch": 1.6670533642691416,
        "step": 12933
    },
    {
        "loss": 1.913,
        "grad_norm": 3.0837957859039307,
        "learning_rate": 0.00010025350116912961,
        "epoch": 1.6671822634699665,
        "step": 12934
    },
    {
        "loss": 1.6649,
        "grad_norm": 2.1626291275024414,
        "learning_rate": 0.00010019266097570057,
        "epoch": 1.6673111626707915,
        "step": 12935
    },
    {
        "loss": 0.9207,
        "grad_norm": 3.272526264190674,
        "learning_rate": 0.00010013182071095713,
        "epoch": 1.6674400618716163,
        "step": 12936
    },
    {
        "loss": 2.1749,
        "grad_norm": 2.525930643081665,
        "learning_rate": 0.00010007098039741964,
        "epoch": 1.6675689610724413,
        "step": 12937
    },
    {
        "loss": 1.8646,
        "grad_norm": 2.6896421909332275,
        "learning_rate": 0.00010001014005760827,
        "epoch": 1.6676978602732664,
        "step": 12938
    },
    {
        "loss": 1.2232,
        "grad_norm": 4.911395072937012,
        "learning_rate": 9.994929971404365e-05,
        "epoch": 1.6678267594740914,
        "step": 12939
    },
    {
        "loss": 1.6804,
        "grad_norm": 1.3189517259597778,
        "learning_rate": 9.9888459389246e-05,
        "epoch": 1.6679556586749162,
        "step": 12940
    },
    {
        "loss": 2.0782,
        "grad_norm": 2.1062071323394775,
        "learning_rate": 9.982761910573561e-05,
        "epoch": 1.668084557875741,
        "step": 12941
    },
    {
        "loss": 0.6684,
        "grad_norm": 2.4655117988586426,
        "learning_rate": 9.976677888603284e-05,
        "epoch": 1.668213457076566,
        "step": 12942
    },
    {
        "loss": 1.7034,
        "grad_norm": 2.6898276805877686,
        "learning_rate": 9.970593875265797e-05,
        "epoch": 1.6683423562773911,
        "step": 12943
    },
    {
        "loss": 1.3952,
        "grad_norm": 1.8411099910736084,
        "learning_rate": 9.964509872813114e-05,
        "epoch": 1.6684712554782162,
        "step": 12944
    },
    {
        "loss": 1.0064,
        "grad_norm": 2.0958359241485596,
        "learning_rate": 9.958425883497284e-05,
        "epoch": 1.668600154679041,
        "step": 12945
    },
    {
        "loss": 0.831,
        "grad_norm": 2.775442361831665,
        "learning_rate": 9.952341909570325e-05,
        "epoch": 1.6687290538798658,
        "step": 12946
    },
    {
        "loss": 0.799,
        "grad_norm": 1.3575764894485474,
        "learning_rate": 9.946257953284206e-05,
        "epoch": 1.6688579530806908,
        "step": 12947
    },
    {
        "loss": 1.3574,
        "grad_norm": 2.9792349338531494,
        "learning_rate": 9.940174016890976e-05,
        "epoch": 1.6689868522815159,
        "step": 12948
    },
    {
        "loss": 1.2875,
        "grad_norm": 2.4056079387664795,
        "learning_rate": 9.934090102642617e-05,
        "epoch": 1.669115751482341,
        "step": 12949
    },
    {
        "loss": 1.7271,
        "grad_norm": 1.8397341966629028,
        "learning_rate": 9.928006212791121e-05,
        "epoch": 1.6692446506831657,
        "step": 12950
    },
    {
        "loss": 1.8027,
        "grad_norm": 2.698096990585327,
        "learning_rate": 9.921922349588468e-05,
        "epoch": 1.6693735498839906,
        "step": 12951
    },
    {
        "loss": 1.4486,
        "grad_norm": 2.0825278759002686,
        "learning_rate": 9.915838515286627e-05,
        "epoch": 1.6695024490848156,
        "step": 12952
    },
    {
        "loss": 1.9604,
        "grad_norm": 2.249589681625366,
        "learning_rate": 9.909754712137562e-05,
        "epoch": 1.6696313482856406,
        "step": 12953
    },
    {
        "loss": 1.4114,
        "grad_norm": 2.678851366043091,
        "learning_rate": 9.903670942393219e-05,
        "epoch": 1.6697602474864657,
        "step": 12954
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.7867258787155151,
        "learning_rate": 9.89758720830552e-05,
        "epoch": 1.6698891466872905,
        "step": 12955
    },
    {
        "loss": 2.0019,
        "grad_norm": 1.8589972257614136,
        "learning_rate": 9.891503512126417e-05,
        "epoch": 1.6700180458881155,
        "step": 12956
    },
    {
        "loss": 2.4449,
        "grad_norm": 2.417389392852783,
        "learning_rate": 9.885419856107803e-05,
        "epoch": 1.6701469450889403,
        "step": 12957
    },
    {
        "loss": 1.3774,
        "grad_norm": 1.8035192489624023,
        "learning_rate": 9.879336242501578e-05,
        "epoch": 1.6702758442897654,
        "step": 12958
    },
    {
        "loss": 1.5264,
        "grad_norm": 2.651498317718506,
        "learning_rate": 9.873252673559617e-05,
        "epoch": 1.6704047434905904,
        "step": 12959
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.3052207231521606,
        "learning_rate": 9.867169151533777e-05,
        "epoch": 1.6705336426914155,
        "step": 12960
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.810303807258606,
        "learning_rate": 9.861085678675898e-05,
        "epoch": 1.6706625418922403,
        "step": 12961
    },
    {
        "loss": 0.8682,
        "grad_norm": 3.617588758468628,
        "learning_rate": 9.855002257237828e-05,
        "epoch": 1.670791441093065,
        "step": 12962
    },
    {
        "loss": 1.5827,
        "grad_norm": 2.533320665359497,
        "learning_rate": 9.848918889471374e-05,
        "epoch": 1.6709203402938901,
        "step": 12963
    },
    {
        "loss": 1.312,
        "grad_norm": 3.6172378063201904,
        "learning_rate": 9.842835577628285e-05,
        "epoch": 1.6710492394947152,
        "step": 12964
    },
    {
        "loss": 1.0594,
        "grad_norm": 2.660640001296997,
        "learning_rate": 9.836752323960369e-05,
        "epoch": 1.6711781386955402,
        "step": 12965
    },
    {
        "loss": 1.1601,
        "grad_norm": 2.1766750812530518,
        "learning_rate": 9.830669130719355e-05,
        "epoch": 1.671307037896365,
        "step": 12966
    },
    {
        "loss": 2.6506,
        "grad_norm": 1.5465879440307617,
        "learning_rate": 9.82458600015697e-05,
        "epoch": 1.6714359370971898,
        "step": 12967
    },
    {
        "loss": 1.9088,
        "grad_norm": 2.9476051330566406,
        "learning_rate": 9.818502934524911e-05,
        "epoch": 1.6715648362980149,
        "step": 12968
    },
    {
        "loss": 2.2165,
        "grad_norm": 2.1421449184417725,
        "learning_rate": 9.812419936074852e-05,
        "epoch": 1.67169373549884,
        "step": 12969
    },
    {
        "loss": 1.8386,
        "grad_norm": 1.8752166032791138,
        "learning_rate": 9.806337007058447e-05,
        "epoch": 1.671822634699665,
        "step": 12970
    },
    {
        "loss": 0.9361,
        "grad_norm": 4.651752471923828,
        "learning_rate": 9.800254149727321e-05,
        "epoch": 1.6719515339004898,
        "step": 12971
    },
    {
        "loss": 1.341,
        "grad_norm": 1.6581404209136963,
        "learning_rate": 9.794171366333054e-05,
        "epoch": 1.6720804331013148,
        "step": 12972
    },
    {
        "loss": 1.5821,
        "grad_norm": 1.8960411548614502,
        "learning_rate": 9.788088659127252e-05,
        "epoch": 1.6722093323021396,
        "step": 12973
    },
    {
        "loss": 1.1,
        "grad_norm": 2.1697440147399902,
        "learning_rate": 9.782006030361437e-05,
        "epoch": 1.6723382315029647,
        "step": 12974
    },
    {
        "loss": 2.3102,
        "grad_norm": 1.6005446910858154,
        "learning_rate": 9.775923482287131e-05,
        "epoch": 1.6724671307037897,
        "step": 12975
    },
    {
        "loss": 2.0617,
        "grad_norm": 1.9474518299102783,
        "learning_rate": 9.769841017155811e-05,
        "epoch": 1.6725960299046148,
        "step": 12976
    },
    {
        "loss": 1.386,
        "grad_norm": 3.655658721923828,
        "learning_rate": 9.763758637218932e-05,
        "epoch": 1.6727249291054396,
        "step": 12977
    },
    {
        "loss": 1.5987,
        "grad_norm": 1.9637210369110107,
        "learning_rate": 9.757676344727906e-05,
        "epoch": 1.6728538283062644,
        "step": 12978
    },
    {
        "loss": 2.3262,
        "grad_norm": 2.544800281524658,
        "learning_rate": 9.751594141934144e-05,
        "epoch": 1.6729827275070894,
        "step": 12979
    },
    {
        "loss": 1.0178,
        "grad_norm": 2.569265365600586,
        "learning_rate": 9.745512031088995e-05,
        "epoch": 1.6731116267079145,
        "step": 12980
    },
    {
        "loss": 1.2589,
        "grad_norm": 4.227445602416992,
        "learning_rate": 9.739430014443777e-05,
        "epoch": 1.6732405259087395,
        "step": 12981
    },
    {
        "loss": 0.9024,
        "grad_norm": 3.1625843048095703,
        "learning_rate": 9.733348094249777e-05,
        "epoch": 1.6733694251095643,
        "step": 12982
    },
    {
        "loss": 1.3056,
        "grad_norm": 1.8575859069824219,
        "learning_rate": 9.727266272758249e-05,
        "epoch": 1.6734983243103891,
        "step": 12983
    },
    {
        "loss": 1.7899,
        "grad_norm": 2.0587329864501953,
        "learning_rate": 9.721184552220408e-05,
        "epoch": 1.6736272235112142,
        "step": 12984
    },
    {
        "loss": 1.4853,
        "grad_norm": 3.3291735649108887,
        "learning_rate": 9.715102934887429e-05,
        "epoch": 1.6737561227120392,
        "step": 12985
    },
    {
        "loss": 2.6791,
        "grad_norm": 1.713833212852478,
        "learning_rate": 9.709021423010454e-05,
        "epoch": 1.6738850219128643,
        "step": 12986
    },
    {
        "loss": 1.8888,
        "grad_norm": 3.9053292274475098,
        "learning_rate": 9.702940018840586e-05,
        "epoch": 1.674013921113689,
        "step": 12987
    },
    {
        "loss": 0.9561,
        "grad_norm": 2.970287322998047,
        "learning_rate": 9.696858724628878e-05,
        "epoch": 1.674142820314514,
        "step": 12988
    },
    {
        "loss": 0.9787,
        "grad_norm": 3.285601854324341,
        "learning_rate": 9.690777542626345e-05,
        "epoch": 1.674271719515339,
        "step": 12989
    },
    {
        "loss": 1.3207,
        "grad_norm": 4.5065412521362305,
        "learning_rate": 9.684696475083986e-05,
        "epoch": 1.674400618716164,
        "step": 12990
    },
    {
        "loss": 1.5073,
        "grad_norm": 2.8238344192504883,
        "learning_rate": 9.67861552425273e-05,
        "epoch": 1.674529517916989,
        "step": 12991
    },
    {
        "loss": 0.9355,
        "grad_norm": 1.8554191589355469,
        "learning_rate": 9.672534692383464e-05,
        "epoch": 1.6746584171178138,
        "step": 12992
    },
    {
        "loss": 0.9492,
        "grad_norm": 3.4070677757263184,
        "learning_rate": 9.666453981727036e-05,
        "epoch": 1.6747873163186389,
        "step": 12993
    },
    {
        "loss": 1.3199,
        "grad_norm": 4.040163040161133,
        "learning_rate": 9.660373394534257e-05,
        "epoch": 1.6749162155194637,
        "step": 12994
    },
    {
        "loss": 2.1758,
        "grad_norm": 2.078411817550659,
        "learning_rate": 9.654292933055867e-05,
        "epoch": 1.6750451147202887,
        "step": 12995
    },
    {
        "loss": 0.945,
        "grad_norm": 3.72055721282959,
        "learning_rate": 9.648212599542604e-05,
        "epoch": 1.6751740139211138,
        "step": 12996
    },
    {
        "loss": 1.5105,
        "grad_norm": 1.8496407270431519,
        "learning_rate": 9.642132396245122e-05,
        "epoch": 1.6753029131219388,
        "step": 12997
    },
    {
        "loss": 1.7085,
        "grad_norm": 2.8599259853363037,
        "learning_rate": 9.636052325414034e-05,
        "epoch": 1.6754318123227636,
        "step": 12998
    },
    {
        "loss": 1.6461,
        "grad_norm": 3.4539010524749756,
        "learning_rate": 9.629972389299909e-05,
        "epoch": 1.6755607115235884,
        "step": 12999
    },
    {
        "loss": 1.7133,
        "grad_norm": 2.545856475830078,
        "learning_rate": 9.623892590153265e-05,
        "epoch": 1.6756896107244135,
        "step": 13000
    },
    {
        "loss": 0.258,
        "grad_norm": 0.9280271530151367,
        "learning_rate": 9.617812930224565e-05,
        "epoch": 1.6758185099252385,
        "step": 13001
    },
    {
        "loss": 1.0679,
        "grad_norm": 2.7630598545074463,
        "learning_rate": 9.611733411764226e-05,
        "epoch": 1.6759474091260635,
        "step": 13002
    },
    {
        "loss": 1.9636,
        "grad_norm": 2.3029980659484863,
        "learning_rate": 9.605654037022601e-05,
        "epoch": 1.6760763083268884,
        "step": 13003
    },
    {
        "loss": 1.7415,
        "grad_norm": 2.3292574882507324,
        "learning_rate": 9.599574808250036e-05,
        "epoch": 1.6762052075277132,
        "step": 13004
    },
    {
        "loss": 2.4256,
        "grad_norm": 3.1414687633514404,
        "learning_rate": 9.593495727696745e-05,
        "epoch": 1.6763341067285382,
        "step": 13005
    },
    {
        "loss": 1.009,
        "grad_norm": 2.3914473056793213,
        "learning_rate": 9.58741679761293e-05,
        "epoch": 1.6764630059293633,
        "step": 13006
    },
    {
        "loss": 0.9947,
        "grad_norm": 2.0499370098114014,
        "learning_rate": 9.581338020248763e-05,
        "epoch": 1.6765919051301883,
        "step": 13007
    },
    {
        "loss": 2.5292,
        "grad_norm": 2.8020904064178467,
        "learning_rate": 9.57525939785432e-05,
        "epoch": 1.6767208043310131,
        "step": 13008
    },
    {
        "loss": 1.8188,
        "grad_norm": 2.337512969970703,
        "learning_rate": 9.569180932679631e-05,
        "epoch": 1.6768497035318382,
        "step": 13009
    },
    {
        "loss": 2.2416,
        "grad_norm": 2.2324273586273193,
        "learning_rate": 9.563102626974667e-05,
        "epoch": 1.676978602732663,
        "step": 13010
    },
    {
        "loss": 2.0763,
        "grad_norm": 1.7268191576004028,
        "learning_rate": 9.557024482989345e-05,
        "epoch": 1.677107501933488,
        "step": 13011
    },
    {
        "loss": 1.6424,
        "grad_norm": 2.7421908378601074,
        "learning_rate": 9.550946502973503e-05,
        "epoch": 1.677236401134313,
        "step": 13012
    },
    {
        "loss": 1.2958,
        "grad_norm": 1.7619285583496094,
        "learning_rate": 9.544868689176962e-05,
        "epoch": 1.677365300335138,
        "step": 13013
    },
    {
        "loss": 1.9415,
        "grad_norm": 1.5244550704956055,
        "learning_rate": 9.538791043849441e-05,
        "epoch": 1.677494199535963,
        "step": 13014
    },
    {
        "loss": 1.4025,
        "grad_norm": 1.66802179813385,
        "learning_rate": 9.532713569240607e-05,
        "epoch": 1.6776230987367877,
        "step": 13015
    },
    {
        "loss": 0.8381,
        "grad_norm": 2.4004838466644287,
        "learning_rate": 9.526636267600069e-05,
        "epoch": 1.6777519979376128,
        "step": 13016
    },
    {
        "loss": 1.8936,
        "grad_norm": 2.5629172325134277,
        "learning_rate": 9.520559141177367e-05,
        "epoch": 1.6778808971384378,
        "step": 13017
    },
    {
        "loss": 1.6618,
        "grad_norm": 2.3786699771881104,
        "learning_rate": 9.514482192221979e-05,
        "epoch": 1.6780097963392628,
        "step": 13018
    },
    {
        "loss": 2.2614,
        "grad_norm": 1.7799756526947021,
        "learning_rate": 9.508405422983316e-05,
        "epoch": 1.6781386955400877,
        "step": 13019
    },
    {
        "loss": 0.935,
        "grad_norm": 4.043932914733887,
        "learning_rate": 9.50232883571071e-05,
        "epoch": 1.6782675947409125,
        "step": 13020
    },
    {
        "loss": 1.5995,
        "grad_norm": 2.349005937576294,
        "learning_rate": 9.49625243265348e-05,
        "epoch": 1.6783964939417375,
        "step": 13021
    },
    {
        "loss": 1.0556,
        "grad_norm": 2.7139127254486084,
        "learning_rate": 9.490176216060796e-05,
        "epoch": 1.6785253931425625,
        "step": 13022
    },
    {
        "loss": 1.892,
        "grad_norm": 1.9627479314804077,
        "learning_rate": 9.484100188181794e-05,
        "epoch": 1.6786542923433876,
        "step": 13023
    },
    {
        "loss": 0.7483,
        "grad_norm": 3.087730646133423,
        "learning_rate": 9.478024351265577e-05,
        "epoch": 1.6787831915442124,
        "step": 13024
    },
    {
        "loss": 2.121,
        "grad_norm": 2.3983914852142334,
        "learning_rate": 9.471948707561127e-05,
        "epoch": 1.6789120907450372,
        "step": 13025
    },
    {
        "loss": 1.7218,
        "grad_norm": 1.8916176557540894,
        "learning_rate": 9.465873259317372e-05,
        "epoch": 1.6790409899458623,
        "step": 13026
    },
    {
        "loss": 2.2214,
        "grad_norm": 2.0028977394104004,
        "learning_rate": 9.459798008783174e-05,
        "epoch": 1.6791698891466873,
        "step": 13027
    },
    {
        "loss": 1.4827,
        "grad_norm": 2.9020042419433594,
        "learning_rate": 9.453722958207311e-05,
        "epoch": 1.6792987883475123,
        "step": 13028
    },
    {
        "loss": 1.2096,
        "grad_norm": 2.7075014114379883,
        "learning_rate": 9.447648109838478e-05,
        "epoch": 1.6794276875483372,
        "step": 13029
    },
    {
        "loss": 1.7905,
        "grad_norm": 2.986938953399658,
        "learning_rate": 9.441573465925339e-05,
        "epoch": 1.6795565867491622,
        "step": 13030
    },
    {
        "loss": 1.9305,
        "grad_norm": 2.833853006362915,
        "learning_rate": 9.435499028716438e-05,
        "epoch": 1.679685485949987,
        "step": 13031
    },
    {
        "loss": 1.2495,
        "grad_norm": 3.0552847385406494,
        "learning_rate": 9.429424800460253e-05,
        "epoch": 1.679814385150812,
        "step": 13032
    },
    {
        "loss": 1.1342,
        "grad_norm": 2.8322911262512207,
        "learning_rate": 9.423350783405195e-05,
        "epoch": 1.679943284351637,
        "step": 13033
    },
    {
        "loss": 1.9661,
        "grad_norm": 2.730886697769165,
        "learning_rate": 9.417276979799586e-05,
        "epoch": 1.6800721835524621,
        "step": 13034
    },
    {
        "loss": 1.2479,
        "grad_norm": 2.6495132446289062,
        "learning_rate": 9.411203391891672e-05,
        "epoch": 1.680201082753287,
        "step": 13035
    },
    {
        "loss": 1.088,
        "grad_norm": 2.2598023414611816,
        "learning_rate": 9.405130021929625e-05,
        "epoch": 1.6803299819541118,
        "step": 13036
    },
    {
        "loss": 0.6708,
        "grad_norm": 3.67844557762146,
        "learning_rate": 9.399056872161514e-05,
        "epoch": 1.6804588811549368,
        "step": 13037
    },
    {
        "loss": 1.5808,
        "grad_norm": 2.873781681060791,
        "learning_rate": 9.39298394483539e-05,
        "epoch": 1.6805877803557618,
        "step": 13038
    },
    {
        "loss": 2.062,
        "grad_norm": 3.095461845397949,
        "learning_rate": 9.38691124219913e-05,
        "epoch": 1.6807166795565869,
        "step": 13039
    },
    {
        "loss": 2.0355,
        "grad_norm": 1.610009789466858,
        "learning_rate": 9.380838766500575e-05,
        "epoch": 1.6808455787574117,
        "step": 13040
    },
    {
        "loss": 1.8733,
        "grad_norm": 2.319450616836548,
        "learning_rate": 9.37476651998751e-05,
        "epoch": 1.6809744779582365,
        "step": 13041
    },
    {
        "loss": 1.4684,
        "grad_norm": 2.800379991531372,
        "learning_rate": 9.368694504907591e-05,
        "epoch": 1.6811033771590616,
        "step": 13042
    },
    {
        "loss": 2.1885,
        "grad_norm": 2.7353971004486084,
        "learning_rate": 9.362622723508389e-05,
        "epoch": 1.6812322763598866,
        "step": 13043
    },
    {
        "loss": 1.5013,
        "grad_norm": 2.5975520610809326,
        "learning_rate": 9.356551178037444e-05,
        "epoch": 1.6813611755607116,
        "step": 13044
    },
    {
        "loss": 2.063,
        "grad_norm": 3.0365443229675293,
        "learning_rate": 9.350479870742129e-05,
        "epoch": 1.6814900747615364,
        "step": 13045
    },
    {
        "loss": 1.2799,
        "grad_norm": 2.942009449005127,
        "learning_rate": 9.344408803869762e-05,
        "epoch": 1.6816189739623615,
        "step": 13046
    },
    {
        "loss": 0.8586,
        "grad_norm": 1.7481287717819214,
        "learning_rate": 9.338337979667611e-05,
        "epoch": 1.6817478731631863,
        "step": 13047
    },
    {
        "loss": 1.4718,
        "grad_norm": 3.2301530838012695,
        "learning_rate": 9.332267400382801e-05,
        "epoch": 1.6818767723640113,
        "step": 13048
    },
    {
        "loss": 2.2276,
        "grad_norm": 1.8933310508728027,
        "learning_rate": 9.326197068262391e-05,
        "epoch": 1.6820056715648364,
        "step": 13049
    },
    {
        "loss": 1.6851,
        "grad_norm": 2.196470022201538,
        "learning_rate": 9.320126985553339e-05,
        "epoch": 1.6821345707656614,
        "step": 13050
    },
    {
        "loss": 1.3791,
        "grad_norm": 4.190445423126221,
        "learning_rate": 9.314057154502514e-05,
        "epoch": 1.6822634699664862,
        "step": 13051
    },
    {
        "loss": 1.2588,
        "grad_norm": 2.074350357055664,
        "learning_rate": 9.3079875773567e-05,
        "epoch": 1.682392369167311,
        "step": 13052
    },
    {
        "loss": 1.1715,
        "grad_norm": 3.87921142578125,
        "learning_rate": 9.301918256362573e-05,
        "epoch": 1.682521268368136,
        "step": 13053
    },
    {
        "loss": 1.4406,
        "grad_norm": 2.7305891513824463,
        "learning_rate": 9.295849193766709e-05,
        "epoch": 1.6826501675689611,
        "step": 13054
    },
    {
        "loss": 1.0958,
        "grad_norm": 3.9466371536254883,
        "learning_rate": 9.289780391815644e-05,
        "epoch": 1.6827790667697862,
        "step": 13055
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.148772716522217,
        "learning_rate": 9.28371185275573e-05,
        "epoch": 1.682907965970611,
        "step": 13056
    },
    {
        "loss": 1.5179,
        "grad_norm": 2.0357980728149414,
        "learning_rate": 9.277643578833265e-05,
        "epoch": 1.6830368651714358,
        "step": 13057
    },
    {
        "loss": 1.62,
        "grad_norm": 2.678222417831421,
        "learning_rate": 9.27157557229448e-05,
        "epoch": 1.6831657643722608,
        "step": 13058
    },
    {
        "loss": 1.5812,
        "grad_norm": 2.0321900844573975,
        "learning_rate": 9.26550783538546e-05,
        "epoch": 1.6832946635730859,
        "step": 13059
    },
    {
        "loss": 2.4983,
        "grad_norm": 2.1186888217926025,
        "learning_rate": 9.259440370352195e-05,
        "epoch": 1.683423562773911,
        "step": 13060
    },
    {
        "loss": 1.362,
        "grad_norm": 2.567288637161255,
        "learning_rate": 9.253373179440626e-05,
        "epoch": 1.6835524619747357,
        "step": 13061
    },
    {
        "loss": 0.8495,
        "grad_norm": 3.83976411819458,
        "learning_rate": 9.24730626489651e-05,
        "epoch": 1.6836813611755606,
        "step": 13062
    },
    {
        "loss": 1.808,
        "grad_norm": 2.596632957458496,
        "learning_rate": 9.241239628965549e-05,
        "epoch": 1.6838102603763856,
        "step": 13063
    },
    {
        "loss": 1.7241,
        "grad_norm": 2.086920738220215,
        "learning_rate": 9.235173273893361e-05,
        "epoch": 1.6839391595772106,
        "step": 13064
    },
    {
        "loss": 1.7188,
        "grad_norm": 2.3882198333740234,
        "learning_rate": 9.229107201925421e-05,
        "epoch": 1.6840680587780357,
        "step": 13065
    },
    {
        "loss": 1.9281,
        "grad_norm": 2.1280007362365723,
        "learning_rate": 9.223041415307117e-05,
        "epoch": 1.6841969579788605,
        "step": 13066
    },
    {
        "loss": 2.6212,
        "grad_norm": 2.2360241413116455,
        "learning_rate": 9.21697591628373e-05,
        "epoch": 1.6843258571796855,
        "step": 13067
    },
    {
        "loss": 1.1467,
        "grad_norm": 3.0623319149017334,
        "learning_rate": 9.210910707100428e-05,
        "epoch": 1.6844547563805103,
        "step": 13068
    },
    {
        "loss": 1.4865,
        "grad_norm": 2.479187488555908,
        "learning_rate": 9.204845790002284e-05,
        "epoch": 1.6845836555813354,
        "step": 13069
    },
    {
        "loss": 1.3164,
        "grad_norm": 2.283987283706665,
        "learning_rate": 9.198781167234252e-05,
        "epoch": 1.6847125547821604,
        "step": 13070
    },
    {
        "loss": 0.4241,
        "grad_norm": 2.082650899887085,
        "learning_rate": 9.192716841041165e-05,
        "epoch": 1.6848414539829855,
        "step": 13071
    },
    {
        "loss": 1.2357,
        "grad_norm": 2.773324489593506,
        "learning_rate": 9.186652813667807e-05,
        "epoch": 1.6849703531838103,
        "step": 13072
    },
    {
        "loss": 0.5087,
        "grad_norm": 3.102864980697632,
        "learning_rate": 9.180589087358766e-05,
        "epoch": 1.685099252384635,
        "step": 13073
    },
    {
        "loss": 0.7888,
        "grad_norm": 4.371271133422852,
        "learning_rate": 9.174525664358556e-05,
        "epoch": 1.6852281515854601,
        "step": 13074
    },
    {
        "loss": 1.3951,
        "grad_norm": 2.9342265129089355,
        "learning_rate": 9.168462546911611e-05,
        "epoch": 1.6853570507862852,
        "step": 13075
    },
    {
        "loss": 0.9867,
        "grad_norm": 1.4956135749816895,
        "learning_rate": 9.162399737262209e-05,
        "epoch": 1.6854859499871102,
        "step": 13076
    },
    {
        "loss": 2.641,
        "grad_norm": 1.723669171333313,
        "learning_rate": 9.156337237654513e-05,
        "epoch": 1.685614849187935,
        "step": 13077
    },
    {
        "loss": 2.0171,
        "grad_norm": 1.6038626432418823,
        "learning_rate": 9.15027505033263e-05,
        "epoch": 1.6857437483887598,
        "step": 13078
    },
    {
        "loss": 1.334,
        "grad_norm": 1.6790902614593506,
        "learning_rate": 9.144213177540462e-05,
        "epoch": 1.6858726475895849,
        "step": 13079
    },
    {
        "loss": 1.6024,
        "grad_norm": 2.5528016090393066,
        "learning_rate": 9.138151621521845e-05,
        "epoch": 1.68600154679041,
        "step": 13080
    },
    {
        "loss": 1.8472,
        "grad_norm": 2.5952823162078857,
        "learning_rate": 9.132090384520516e-05,
        "epoch": 1.686130445991235,
        "step": 13081
    },
    {
        "loss": 1.7855,
        "grad_norm": 2.6045451164245605,
        "learning_rate": 9.12602946878006e-05,
        "epoch": 1.6862593451920598,
        "step": 13082
    },
    {
        "loss": 1.7791,
        "grad_norm": 1.934012770652771,
        "learning_rate": 9.119968876543952e-05,
        "epoch": 1.6863882443928848,
        "step": 13083
    },
    {
        "loss": 1.6847,
        "grad_norm": 2.342334508895874,
        "learning_rate": 9.113908610055549e-05,
        "epoch": 1.6865171435937096,
        "step": 13084
    },
    {
        "loss": 1.8858,
        "grad_norm": 1.8123635053634644,
        "learning_rate": 9.107848671558088e-05,
        "epoch": 1.6866460427945347,
        "step": 13085
    },
    {
        "loss": 2.1553,
        "grad_norm": 1.9334403276443481,
        "learning_rate": 9.101789063294685e-05,
        "epoch": 1.6867749419953597,
        "step": 13086
    },
    {
        "loss": 1.5205,
        "grad_norm": 3.2587785720825195,
        "learning_rate": 9.09572978750833e-05,
        "epoch": 1.6869038411961848,
        "step": 13087
    },
    {
        "loss": 1.0716,
        "grad_norm": 2.846161365509033,
        "learning_rate": 9.089670846441884e-05,
        "epoch": 1.6870327403970096,
        "step": 13088
    },
    {
        "loss": 2.0012,
        "grad_norm": 2.221827507019043,
        "learning_rate": 9.083612242338128e-05,
        "epoch": 1.6871616395978344,
        "step": 13089
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.806799292564392,
        "learning_rate": 9.077553977439646e-05,
        "epoch": 1.6872905387986594,
        "step": 13090
    },
    {
        "loss": 1.3278,
        "grad_norm": 2.777146339416504,
        "learning_rate": 9.071496053988933e-05,
        "epoch": 1.6874194379994845,
        "step": 13091
    },
    {
        "loss": 1.2136,
        "grad_norm": 3.384687662124634,
        "learning_rate": 9.065438474228383e-05,
        "epoch": 1.6875483372003095,
        "step": 13092
    },
    {
        "loss": 2.056,
        "grad_norm": 1.4143558740615845,
        "learning_rate": 9.059381240400228e-05,
        "epoch": 1.6876772364011343,
        "step": 13093
    },
    {
        "loss": 1.6746,
        "grad_norm": 1.8382149934768677,
        "learning_rate": 9.053324354746568e-05,
        "epoch": 1.6878061356019591,
        "step": 13094
    },
    {
        "loss": 1.3976,
        "grad_norm": 3.323129177093506,
        "learning_rate": 9.047267819509424e-05,
        "epoch": 1.6879350348027842,
        "step": 13095
    },
    {
        "loss": 2.3542,
        "grad_norm": 1.6115505695343018,
        "learning_rate": 9.041211636930616e-05,
        "epoch": 1.6880639340036092,
        "step": 13096
    },
    {
        "loss": 1.4755,
        "grad_norm": 3.133965253829956,
        "learning_rate": 9.035155809251868e-05,
        "epoch": 1.6881928332044343,
        "step": 13097
    },
    {
        "loss": 1.5592,
        "grad_norm": 1.970853328704834,
        "learning_rate": 9.029100338714796e-05,
        "epoch": 1.688321732405259,
        "step": 13098
    },
    {
        "loss": 1.6717,
        "grad_norm": 2.4878387451171875,
        "learning_rate": 9.023045227560853e-05,
        "epoch": 1.6884506316060839,
        "step": 13099
    },
    {
        "loss": 1.9461,
        "grad_norm": 2.2993955612182617,
        "learning_rate": 9.016990478031367e-05,
        "epoch": 1.688579530806909,
        "step": 13100
    },
    {
        "loss": 1.251,
        "grad_norm": 2.013131618499756,
        "learning_rate": 9.010936092367527e-05,
        "epoch": 1.688708430007734,
        "step": 13101
    },
    {
        "loss": 1.4335,
        "grad_norm": 2.649808883666992,
        "learning_rate": 9.004882072810398e-05,
        "epoch": 1.688837329208559,
        "step": 13102
    },
    {
        "loss": 1.1377,
        "grad_norm": 2.9553418159484863,
        "learning_rate": 8.998828421600902e-05,
        "epoch": 1.6889662284093838,
        "step": 13103
    },
    {
        "loss": 1.102,
        "grad_norm": 3.426058530807495,
        "learning_rate": 8.992775140979826e-05,
        "epoch": 1.6890951276102089,
        "step": 13104
    },
    {
        "loss": 1.9903,
        "grad_norm": 2.338998556137085,
        "learning_rate": 8.98672223318781e-05,
        "epoch": 1.6892240268110337,
        "step": 13105
    },
    {
        "loss": 1.7399,
        "grad_norm": 2.4110312461853027,
        "learning_rate": 8.980669700465389e-05,
        "epoch": 1.6893529260118587,
        "step": 13106
    },
    {
        "loss": 0.7266,
        "grad_norm": 2.0234031677246094,
        "learning_rate": 8.974617545052942e-05,
        "epoch": 1.6894818252126838,
        "step": 13107
    },
    {
        "loss": 1.2617,
        "grad_norm": 2.6407110691070557,
        "learning_rate": 8.96856576919066e-05,
        "epoch": 1.6896107244135088,
        "step": 13108
    },
    {
        "loss": 1.1104,
        "grad_norm": 3.1478350162506104,
        "learning_rate": 8.962514375118676e-05,
        "epoch": 1.6897396236143336,
        "step": 13109
    },
    {
        "loss": 1.4049,
        "grad_norm": 2.255647659301758,
        "learning_rate": 8.956463365076932e-05,
        "epoch": 1.6898685228151584,
        "step": 13110
    },
    {
        "loss": 1.5582,
        "grad_norm": 1.964780569076538,
        "learning_rate": 8.95041274130522e-05,
        "epoch": 1.6899974220159835,
        "step": 13111
    },
    {
        "loss": 1.9265,
        "grad_norm": 1.693792462348938,
        "learning_rate": 8.944362506043251e-05,
        "epoch": 1.6901263212168085,
        "step": 13112
    },
    {
        "loss": 1.7475,
        "grad_norm": 2.289767026901245,
        "learning_rate": 8.93831266153051e-05,
        "epoch": 1.6902552204176335,
        "step": 13113
    },
    {
        "loss": 2.0633,
        "grad_norm": 1.5065046548843384,
        "learning_rate": 8.932263210006368e-05,
        "epoch": 1.6903841196184584,
        "step": 13114
    },
    {
        "loss": 1.6837,
        "grad_norm": 1.6443523168563843,
        "learning_rate": 8.926214153710096e-05,
        "epoch": 1.6905130188192832,
        "step": 13115
    },
    {
        "loss": 1.1741,
        "grad_norm": 2.3286139965057373,
        "learning_rate": 8.920165494880759e-05,
        "epoch": 1.6906419180201082,
        "step": 13116
    },
    {
        "loss": 1.1228,
        "grad_norm": 3.4511024951934814,
        "learning_rate": 8.914117235757298e-05,
        "epoch": 1.6907708172209333,
        "step": 13117
    },
    {
        "loss": 0.9868,
        "grad_norm": 2.5761821269989014,
        "learning_rate": 8.908069378578507e-05,
        "epoch": 1.6908997164217583,
        "step": 13118
    },
    {
        "loss": 1.9485,
        "grad_norm": 1.8536362648010254,
        "learning_rate": 8.90202192558303e-05,
        "epoch": 1.691028615622583,
        "step": 13119
    },
    {
        "loss": 1.743,
        "grad_norm": 2.829789400100708,
        "learning_rate": 8.895974879009357e-05,
        "epoch": 1.6911575148234081,
        "step": 13120
    },
    {
        "loss": 1.3753,
        "grad_norm": 1.9193781614303589,
        "learning_rate": 8.889928241095835e-05,
        "epoch": 1.691286414024233,
        "step": 13121
    },
    {
        "loss": 2.0072,
        "grad_norm": 2.658433675765991,
        "learning_rate": 8.883882014080637e-05,
        "epoch": 1.691415313225058,
        "step": 13122
    },
    {
        "loss": 1.4286,
        "grad_norm": 2.894035577774048,
        "learning_rate": 8.877836200201836e-05,
        "epoch": 1.691544212425883,
        "step": 13123
    },
    {
        "loss": 1.4501,
        "grad_norm": 2.3316545486450195,
        "learning_rate": 8.871790801697316e-05,
        "epoch": 1.6916731116267079,
        "step": 13124
    },
    {
        "loss": 1.2143,
        "grad_norm": 2.9828040599823,
        "learning_rate": 8.865745820804768e-05,
        "epoch": 1.691802010827533,
        "step": 13125
    },
    {
        "loss": 1.6754,
        "grad_norm": 2.922785520553589,
        "learning_rate": 8.859701259761812e-05,
        "epoch": 1.6919309100283577,
        "step": 13126
    },
    {
        "loss": 1.7885,
        "grad_norm": 2.694148540496826,
        "learning_rate": 8.853657120805859e-05,
        "epoch": 1.6920598092291828,
        "step": 13127
    },
    {
        "loss": 2.1333,
        "grad_norm": 2.327676296234131,
        "learning_rate": 8.847613406174158e-05,
        "epoch": 1.6921887084300078,
        "step": 13128
    },
    {
        "loss": 1.7875,
        "grad_norm": 2.9696741104125977,
        "learning_rate": 8.841570118103851e-05,
        "epoch": 1.6923176076308328,
        "step": 13129
    },
    {
        "loss": 2.3976,
        "grad_norm": 2.598393678665161,
        "learning_rate": 8.835527258831888e-05,
        "epoch": 1.6924465068316576,
        "step": 13130
    },
    {
        "loss": 1.8651,
        "grad_norm": 2.312845468521118,
        "learning_rate": 8.829484830595019e-05,
        "epoch": 1.6925754060324825,
        "step": 13131
    },
    {
        "loss": 2.1198,
        "grad_norm": 2.5471301078796387,
        "learning_rate": 8.82344283562992e-05,
        "epoch": 1.6927043052333075,
        "step": 13132
    },
    {
        "loss": 1.0246,
        "grad_norm": 3.051961660385132,
        "learning_rate": 8.817401276173054e-05,
        "epoch": 1.6928332044341325,
        "step": 13133
    },
    {
        "loss": 1.799,
        "grad_norm": 1.9935712814331055,
        "learning_rate": 8.811360154460726e-05,
        "epoch": 1.6929621036349576,
        "step": 13134
    },
    {
        "loss": 1.4446,
        "grad_norm": 2.0149483680725098,
        "learning_rate": 8.805319472729092e-05,
        "epoch": 1.6930910028357824,
        "step": 13135
    },
    {
        "loss": 1.4373,
        "grad_norm": 2.8840150833129883,
        "learning_rate": 8.799279233214136e-05,
        "epoch": 1.6932199020366072,
        "step": 13136
    },
    {
        "loss": 1.5747,
        "grad_norm": 1.5692367553710938,
        "learning_rate": 8.793239438151681e-05,
        "epoch": 1.6933488012374323,
        "step": 13137
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.0118255615234375,
        "learning_rate": 8.787200089777388e-05,
        "epoch": 1.6934777004382573,
        "step": 13138
    },
    {
        "loss": 0.4004,
        "grad_norm": 2.168980836868286,
        "learning_rate": 8.781161190326737e-05,
        "epoch": 1.6936065996390823,
        "step": 13139
    },
    {
        "loss": 1.9621,
        "grad_norm": 2.1405458450317383,
        "learning_rate": 8.77512274203508e-05,
        "epoch": 1.6937354988399071,
        "step": 13140
    },
    {
        "loss": 2.1234,
        "grad_norm": 1.6111955642700195,
        "learning_rate": 8.76908474713758e-05,
        "epoch": 1.6938643980407322,
        "step": 13141
    },
    {
        "loss": 2.2055,
        "grad_norm": 1.8348888158798218,
        "learning_rate": 8.763047207869187e-05,
        "epoch": 1.693993297241557,
        "step": 13142
    },
    {
        "loss": 1.6961,
        "grad_norm": 3.436002016067505,
        "learning_rate": 8.757010126464766e-05,
        "epoch": 1.694122196442382,
        "step": 13143
    },
    {
        "loss": 1.5077,
        "grad_norm": 2.935192584991455,
        "learning_rate": 8.750973505158957e-05,
        "epoch": 1.694251095643207,
        "step": 13144
    },
    {
        "loss": 1.257,
        "grad_norm": 3.0713653564453125,
        "learning_rate": 8.744937346186233e-05,
        "epoch": 1.6943799948440321,
        "step": 13145
    },
    {
        "loss": 1.245,
        "grad_norm": 1.4556224346160889,
        "learning_rate": 8.738901651780934e-05,
        "epoch": 1.694508894044857,
        "step": 13146
    },
    {
        "loss": 1.8271,
        "grad_norm": 2.8187344074249268,
        "learning_rate": 8.732866424177198e-05,
        "epoch": 1.6946377932456818,
        "step": 13147
    },
    {
        "loss": 2.135,
        "grad_norm": 1.8901231288909912,
        "learning_rate": 8.726831665608953e-05,
        "epoch": 1.6947666924465068,
        "step": 13148
    },
    {
        "loss": 2.1161,
        "grad_norm": 2.5014169216156006,
        "learning_rate": 8.720797378310036e-05,
        "epoch": 1.6948955916473318,
        "step": 13149
    },
    {
        "loss": 1.2255,
        "grad_norm": 3.3855082988739014,
        "learning_rate": 8.714763564514052e-05,
        "epoch": 1.6950244908481569,
        "step": 13150
    },
    {
        "loss": 0.3127,
        "grad_norm": 1.7233819961547852,
        "learning_rate": 8.708730226454449e-05,
        "epoch": 1.6951533900489817,
        "step": 13151
    },
    {
        "loss": 0.8712,
        "grad_norm": 3.1081018447875977,
        "learning_rate": 8.702697366364494e-05,
        "epoch": 1.6952822892498065,
        "step": 13152
    },
    {
        "loss": 1.7076,
        "grad_norm": 1.697729229927063,
        "learning_rate": 8.696664986477275e-05,
        "epoch": 1.6954111884506315,
        "step": 13153
    },
    {
        "loss": 1.6075,
        "grad_norm": 5.612988471984863,
        "learning_rate": 8.690633089025713e-05,
        "epoch": 1.6955400876514566,
        "step": 13154
    },
    {
        "loss": 1.9876,
        "grad_norm": 2.6643664836883545,
        "learning_rate": 8.684601676242539e-05,
        "epoch": 1.6956689868522816,
        "step": 13155
    },
    {
        "loss": 1.6302,
        "grad_norm": 2.90362811088562,
        "learning_rate": 8.678570750360296e-05,
        "epoch": 1.6957978860531064,
        "step": 13156
    },
    {
        "loss": 0.8051,
        "grad_norm": 2.6340579986572266,
        "learning_rate": 8.67254031361139e-05,
        "epoch": 1.6959267852539315,
        "step": 13157
    },
    {
        "loss": 1.9913,
        "grad_norm": 2.8269968032836914,
        "learning_rate": 8.666510368228011e-05,
        "epoch": 1.6960556844547563,
        "step": 13158
    },
    {
        "loss": 1.6767,
        "grad_norm": 2.3844361305236816,
        "learning_rate": 8.660480916442133e-05,
        "epoch": 1.6961845836555813,
        "step": 13159
    },
    {
        "loss": 1.3863,
        "grad_norm": 2.5949697494506836,
        "learning_rate": 8.65445196048563e-05,
        "epoch": 1.6963134828564064,
        "step": 13160
    },
    {
        "loss": 1.7205,
        "grad_norm": 2.0011281967163086,
        "learning_rate": 8.648423502590132e-05,
        "epoch": 1.6964423820572312,
        "step": 13161
    },
    {
        "loss": 1.7133,
        "grad_norm": 1.4873428344726562,
        "learning_rate": 8.64239554498709e-05,
        "epoch": 1.6965712812580562,
        "step": 13162
    },
    {
        "loss": 1.9474,
        "grad_norm": 2.5431320667266846,
        "learning_rate": 8.636368089907806e-05,
        "epoch": 1.696700180458881,
        "step": 13163
    },
    {
        "loss": 1.7826,
        "grad_norm": 1.4368748664855957,
        "learning_rate": 8.630341139583372e-05,
        "epoch": 1.696829079659706,
        "step": 13164
    },
    {
        "loss": 1.862,
        "grad_norm": 3.820326805114746,
        "learning_rate": 8.62431469624465e-05,
        "epoch": 1.6969579788605311,
        "step": 13165
    },
    {
        "loss": 1.536,
        "grad_norm": 2.907599925994873,
        "learning_rate": 8.618288762122402e-05,
        "epoch": 1.6970868780613562,
        "step": 13166
    },
    {
        "loss": 1.7372,
        "grad_norm": 2.5203256607055664,
        "learning_rate": 8.612263339447138e-05,
        "epoch": 1.697215777262181,
        "step": 13167
    },
    {
        "loss": 1.0252,
        "grad_norm": 2.2739315032958984,
        "learning_rate": 8.606238430449199e-05,
        "epoch": 1.6973446764630058,
        "step": 13168
    },
    {
        "loss": 1.1664,
        "grad_norm": 2.10833477973938,
        "learning_rate": 8.600214037358734e-05,
        "epoch": 1.6974735756638308,
        "step": 13169
    },
    {
        "loss": 0.5434,
        "grad_norm": 3.1203677654266357,
        "learning_rate": 8.5941901624057e-05,
        "epoch": 1.6976024748646559,
        "step": 13170
    },
    {
        "loss": 1.7789,
        "grad_norm": 1.626961350440979,
        "learning_rate": 8.588166807819862e-05,
        "epoch": 1.697731374065481,
        "step": 13171
    },
    {
        "loss": 1.5462,
        "grad_norm": 2.7284319400787354,
        "learning_rate": 8.582143975830797e-05,
        "epoch": 1.6978602732663057,
        "step": 13172
    },
    {
        "loss": 2.2937,
        "grad_norm": 2.102611780166626,
        "learning_rate": 8.576121668667864e-05,
        "epoch": 1.6979891724671305,
        "step": 13173
    },
    {
        "loss": 0.8572,
        "grad_norm": 2.066115140914917,
        "learning_rate": 8.570099888560286e-05,
        "epoch": 1.6981180716679556,
        "step": 13174
    },
    {
        "loss": 2.4161,
        "grad_norm": 2.032923936843872,
        "learning_rate": 8.564078637737045e-05,
        "epoch": 1.6982469708687806,
        "step": 13175
    },
    {
        "loss": 1.84,
        "grad_norm": 1.7778476476669312,
        "learning_rate": 8.558057918426898e-05,
        "epoch": 1.6983758700696057,
        "step": 13176
    },
    {
        "loss": 1.5894,
        "grad_norm": 1.8244407176971436,
        "learning_rate": 8.552037732858483e-05,
        "epoch": 1.6985047692704305,
        "step": 13177
    },
    {
        "loss": 1.6953,
        "grad_norm": 2.3542962074279785,
        "learning_rate": 8.546018083260187e-05,
        "epoch": 1.6986336684712555,
        "step": 13178
    },
    {
        "loss": 1.6664,
        "grad_norm": 2.8193652629852295,
        "learning_rate": 8.539998971860201e-05,
        "epoch": 1.6987625676720803,
        "step": 13179
    },
    {
        "loss": 1.7409,
        "grad_norm": 4.036488056182861,
        "learning_rate": 8.53398040088655e-05,
        "epoch": 1.6988914668729054,
        "step": 13180
    },
    {
        "loss": 1.5282,
        "grad_norm": 1.9536970853805542,
        "learning_rate": 8.52796237256704e-05,
        "epoch": 1.6990203660737304,
        "step": 13181
    },
    {
        "loss": 1.6366,
        "grad_norm": 3.0704619884490967,
        "learning_rate": 8.521944889129234e-05,
        "epoch": 1.6991492652745555,
        "step": 13182
    },
    {
        "loss": 1.575,
        "grad_norm": 2.52205491065979,
        "learning_rate": 8.515927952800567e-05,
        "epoch": 1.6992781644753803,
        "step": 13183
    },
    {
        "loss": 2.1399,
        "grad_norm": 2.1325583457946777,
        "learning_rate": 8.509911565808227e-05,
        "epoch": 1.699407063676205,
        "step": 13184
    },
    {
        "loss": 1.8106,
        "grad_norm": 2.66191029548645,
        "learning_rate": 8.503895730379207e-05,
        "epoch": 1.6995359628770301,
        "step": 13185
    },
    {
        "loss": 1.5575,
        "grad_norm": 3.11031436920166,
        "learning_rate": 8.497880448740298e-05,
        "epoch": 1.6996648620778552,
        "step": 13186
    },
    {
        "loss": 1.6833,
        "grad_norm": 3.181560516357422,
        "learning_rate": 8.491865723118084e-05,
        "epoch": 1.6997937612786802,
        "step": 13187
    },
    {
        "loss": 2.029,
        "grad_norm": 1.9942446947097778,
        "learning_rate": 8.485851555738946e-05,
        "epoch": 1.699922660479505,
        "step": 13188
    },
    {
        "loss": 1.2263,
        "grad_norm": 4.474679946899414,
        "learning_rate": 8.479837948829053e-05,
        "epoch": 1.7000515596803298,
        "step": 13189
    },
    {
        "loss": 1.4944,
        "grad_norm": 4.681922912597656,
        "learning_rate": 8.473824904614363e-05,
        "epoch": 1.7001804588811549,
        "step": 13190
    },
    {
        "loss": 1.9716,
        "grad_norm": 3.4439008235931396,
        "learning_rate": 8.467812425320651e-05,
        "epoch": 1.70030935808198,
        "step": 13191
    },
    {
        "loss": 1.1627,
        "grad_norm": 3.495654582977295,
        "learning_rate": 8.461800513173462e-05,
        "epoch": 1.700438257282805,
        "step": 13192
    },
    {
        "loss": 1.4258,
        "grad_norm": 2.577955484390259,
        "learning_rate": 8.455789170398127e-05,
        "epoch": 1.7005671564836298,
        "step": 13193
    },
    {
        "loss": 1.8104,
        "grad_norm": 2.2580835819244385,
        "learning_rate": 8.449778399219771e-05,
        "epoch": 1.7006960556844548,
        "step": 13194
    },
    {
        "loss": 1.6315,
        "grad_norm": 1.7005228996276855,
        "learning_rate": 8.443768201863316e-05,
        "epoch": 1.7008249548852796,
        "step": 13195
    },
    {
        "loss": 2.0481,
        "grad_norm": 2.4701664447784424,
        "learning_rate": 8.437758580553449e-05,
        "epoch": 1.7009538540861047,
        "step": 13196
    },
    {
        "loss": 1.883,
        "grad_norm": 2.91601824760437,
        "learning_rate": 8.431749537514684e-05,
        "epoch": 1.7010827532869297,
        "step": 13197
    },
    {
        "loss": 1.5145,
        "grad_norm": 3.207698106765747,
        "learning_rate": 8.4257410749713e-05,
        "epoch": 1.7012116524877545,
        "step": 13198
    },
    {
        "loss": 1.7304,
        "grad_norm": 1.905444622039795,
        "learning_rate": 8.419733195147319e-05,
        "epoch": 1.7013405516885796,
        "step": 13199
    },
    {
        "loss": 1.5879,
        "grad_norm": 2.126659393310547,
        "learning_rate": 8.413725900266622e-05,
        "epoch": 1.7014694508894044,
        "step": 13200
    },
    {
        "loss": 2.1193,
        "grad_norm": 3.058075428009033,
        "learning_rate": 8.407719192552829e-05,
        "epoch": 1.7015983500902294,
        "step": 13201
    },
    {
        "loss": 1.5679,
        "grad_norm": 2.0369107723236084,
        "learning_rate": 8.401713074229349e-05,
        "epoch": 1.7017272492910545,
        "step": 13202
    },
    {
        "loss": 1.7352,
        "grad_norm": 3.263822078704834,
        "learning_rate": 8.395707547519373e-05,
        "epoch": 1.7018561484918795,
        "step": 13203
    },
    {
        "loss": 1.7419,
        "grad_norm": 3.152402877807617,
        "learning_rate": 8.389702614645882e-05,
        "epoch": 1.7019850476927043,
        "step": 13204
    },
    {
        "loss": 1.6754,
        "grad_norm": 1.9488341808319092,
        "learning_rate": 8.383698277831625e-05,
        "epoch": 1.7021139468935291,
        "step": 13205
    },
    {
        "loss": 1.4805,
        "grad_norm": 2.858884334564209,
        "learning_rate": 8.377694539299134e-05,
        "epoch": 1.7022428460943542,
        "step": 13206
    },
    {
        "loss": 1.2474,
        "grad_norm": 4.03919792175293,
        "learning_rate": 8.371691401270715e-05,
        "epoch": 1.7023717452951792,
        "step": 13207
    },
    {
        "loss": 1.5089,
        "grad_norm": 1.7350326776504517,
        "learning_rate": 8.365688865968478e-05,
        "epoch": 1.7025006444960042,
        "step": 13208
    },
    {
        "loss": 1.9608,
        "grad_norm": 2.494615077972412,
        "learning_rate": 8.359686935614281e-05,
        "epoch": 1.702629543696829,
        "step": 13209
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.8849347829818726,
        "learning_rate": 8.353685612429767e-05,
        "epoch": 1.7027584428976539,
        "step": 13210
    },
    {
        "loss": 1.7743,
        "grad_norm": 2.944770097732544,
        "learning_rate": 8.34768489863635e-05,
        "epoch": 1.702887342098479,
        "step": 13211
    },
    {
        "loss": 2.03,
        "grad_norm": 1.9328944683074951,
        "learning_rate": 8.34168479645523e-05,
        "epoch": 1.703016241299304,
        "step": 13212
    },
    {
        "loss": 1.7605,
        "grad_norm": 1.8945103883743286,
        "learning_rate": 8.335685308107351e-05,
        "epoch": 1.703145140500129,
        "step": 13213
    },
    {
        "loss": 1.0916,
        "grad_norm": 2.860529899597168,
        "learning_rate": 8.329686435813487e-05,
        "epoch": 1.7032740397009538,
        "step": 13214
    },
    {
        "loss": 1.5531,
        "grad_norm": 1.8673560619354248,
        "learning_rate": 8.323688181794145e-05,
        "epoch": 1.7034029389017789,
        "step": 13215
    },
    {
        "loss": 1.1913,
        "grad_norm": 1.4866610765457153,
        "learning_rate": 8.317690548269565e-05,
        "epoch": 1.7035318381026037,
        "step": 13216
    },
    {
        "loss": 1.9302,
        "grad_norm": 2.386127233505249,
        "learning_rate": 8.311693537459846e-05,
        "epoch": 1.7036607373034287,
        "step": 13217
    },
    {
        "loss": 1.8025,
        "grad_norm": 2.560654401779175,
        "learning_rate": 8.305697151584793e-05,
        "epoch": 1.7037896365042537,
        "step": 13218
    },
    {
        "loss": 1.3666,
        "grad_norm": 2.1930606365203857,
        "learning_rate": 8.299701392863994e-05,
        "epoch": 1.7039185357050788,
        "step": 13219
    },
    {
        "loss": 1.6823,
        "grad_norm": 3.050774335861206,
        "learning_rate": 8.293706263516814e-05,
        "epoch": 1.7040474349059036,
        "step": 13220
    },
    {
        "loss": 0.7655,
        "grad_norm": 2.5740628242492676,
        "learning_rate": 8.287711765762374e-05,
        "epoch": 1.7041763341067284,
        "step": 13221
    },
    {
        "loss": 1.5948,
        "grad_norm": 2.249349594116211,
        "learning_rate": 8.281717901819567e-05,
        "epoch": 1.7043052333075535,
        "step": 13222
    },
    {
        "loss": 0.9269,
        "grad_norm": 1.2965672016143799,
        "learning_rate": 8.275724673907051e-05,
        "epoch": 1.7044341325083785,
        "step": 13223
    },
    {
        "loss": 1.5643,
        "grad_norm": 2.363422393798828,
        "learning_rate": 8.269732084243232e-05,
        "epoch": 1.7045630317092035,
        "step": 13224
    },
    {
        "loss": 1.652,
        "grad_norm": 2.1433920860290527,
        "learning_rate": 8.263740135046326e-05,
        "epoch": 1.7046919309100284,
        "step": 13225
    },
    {
        "loss": 2.0042,
        "grad_norm": 3.140111207962036,
        "learning_rate": 8.257748828534267e-05,
        "epoch": 1.7048208301108532,
        "step": 13226
    },
    {
        "loss": 1.8505,
        "grad_norm": 2.3493027687072754,
        "learning_rate": 8.251758166924761e-05,
        "epoch": 1.7049497293116782,
        "step": 13227
    },
    {
        "loss": 1.426,
        "grad_norm": 2.372457504272461,
        "learning_rate": 8.245768152435288e-05,
        "epoch": 1.7050786285125032,
        "step": 13228
    },
    {
        "loss": 1.7067,
        "grad_norm": 3.5942940711975098,
        "learning_rate": 8.239778787283073e-05,
        "epoch": 1.7052075277133283,
        "step": 13229
    },
    {
        "loss": 1.4093,
        "grad_norm": 2.7699599266052246,
        "learning_rate": 8.233790073685097e-05,
        "epoch": 1.705336426914153,
        "step": 13230
    },
    {
        "loss": 1.2253,
        "grad_norm": 3.5595264434814453,
        "learning_rate": 8.227802013858141e-05,
        "epoch": 1.7054653261149781,
        "step": 13231
    },
    {
        "loss": 1.655,
        "grad_norm": 3.020437002182007,
        "learning_rate": 8.221814610018698e-05,
        "epoch": 1.705594225315803,
        "step": 13232
    },
    {
        "loss": 1.1512,
        "grad_norm": 2.6757993698120117,
        "learning_rate": 8.21582786438303e-05,
        "epoch": 1.705723124516628,
        "step": 13233
    },
    {
        "loss": 1.367,
        "grad_norm": 2.989222288131714,
        "learning_rate": 8.209841779167167e-05,
        "epoch": 1.705852023717453,
        "step": 13234
    },
    {
        "loss": 1.5439,
        "grad_norm": 3.090768575668335,
        "learning_rate": 8.20385635658688e-05,
        "epoch": 1.7059809229182779,
        "step": 13235
    },
    {
        "loss": 1.7188,
        "grad_norm": 2.044424295425415,
        "learning_rate": 8.197871598857705e-05,
        "epoch": 1.706109822119103,
        "step": 13236
    },
    {
        "loss": 1.7711,
        "grad_norm": 2.6964733600616455,
        "learning_rate": 8.19188750819493e-05,
        "epoch": 1.7062387213199277,
        "step": 13237
    },
    {
        "loss": 2.3024,
        "grad_norm": 1.6072001457214355,
        "learning_rate": 8.185904086813592e-05,
        "epoch": 1.7063676205207527,
        "step": 13238
    },
    {
        "loss": 1.1244,
        "grad_norm": 2.1437103748321533,
        "learning_rate": 8.179921336928483e-05,
        "epoch": 1.7064965197215778,
        "step": 13239
    },
    {
        "loss": 1.6511,
        "grad_norm": 2.267965316772461,
        "learning_rate": 8.173939260754146e-05,
        "epoch": 1.7066254189224028,
        "step": 13240
    },
    {
        "loss": 1.6283,
        "grad_norm": 4.21755313873291,
        "learning_rate": 8.167957860504864e-05,
        "epoch": 1.7067543181232276,
        "step": 13241
    },
    {
        "loss": 2.3314,
        "grad_norm": 4.709007263183594,
        "learning_rate": 8.161977138394705e-05,
        "epoch": 1.7068832173240525,
        "step": 13242
    },
    {
        "loss": 1.5408,
        "grad_norm": 2.8956401348114014,
        "learning_rate": 8.15599709663745e-05,
        "epoch": 1.7070121165248775,
        "step": 13243
    },
    {
        "loss": 1.6913,
        "grad_norm": 3.3277833461761475,
        "learning_rate": 8.15001773744664e-05,
        "epoch": 1.7071410157257025,
        "step": 13244
    },
    {
        "loss": 2.3294,
        "grad_norm": 2.2354824542999268,
        "learning_rate": 8.144039063035562e-05,
        "epoch": 1.7072699149265276,
        "step": 13245
    },
    {
        "loss": 0.8033,
        "grad_norm": 2.0902481079101562,
        "learning_rate": 8.138061075617252e-05,
        "epoch": 1.7073988141273524,
        "step": 13246
    },
    {
        "loss": 2.0647,
        "grad_norm": 1.4556039571762085,
        "learning_rate": 8.132083777404473e-05,
        "epoch": 1.7075277133281772,
        "step": 13247
    },
    {
        "loss": 1.1725,
        "grad_norm": 2.355928897857666,
        "learning_rate": 8.126107170609784e-05,
        "epoch": 1.7076566125290022,
        "step": 13248
    },
    {
        "loss": 1.8469,
        "grad_norm": 2.5112464427948,
        "learning_rate": 8.120131257445435e-05,
        "epoch": 1.7077855117298273,
        "step": 13249
    },
    {
        "loss": 0.9885,
        "grad_norm": 3.4769575595855713,
        "learning_rate": 8.114156040123444e-05,
        "epoch": 1.7079144109306523,
        "step": 13250
    },
    {
        "loss": 2.0679,
        "grad_norm": 3.1073153018951416,
        "learning_rate": 8.108181520855563e-05,
        "epoch": 1.7080433101314771,
        "step": 13251
    },
    {
        "loss": 1.3195,
        "grad_norm": 3.19862961769104,
        "learning_rate": 8.102207701853287e-05,
        "epoch": 1.7081722093323022,
        "step": 13252
    },
    {
        "loss": 1.4992,
        "grad_norm": 2.778110980987549,
        "learning_rate": 8.096234585327857e-05,
        "epoch": 1.708301108533127,
        "step": 13253
    },
    {
        "loss": 1.3031,
        "grad_norm": 3.693014144897461,
        "learning_rate": 8.090262173490249e-05,
        "epoch": 1.708430007733952,
        "step": 13254
    },
    {
        "loss": 1.1517,
        "grad_norm": 2.4499120712280273,
        "learning_rate": 8.084290468551164e-05,
        "epoch": 1.708558906934777,
        "step": 13255
    },
    {
        "loss": 1.6936,
        "grad_norm": 2.2164688110351562,
        "learning_rate": 8.078319472721102e-05,
        "epoch": 1.7086878061356021,
        "step": 13256
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.7356767654418945,
        "learning_rate": 8.072349188210213e-05,
        "epoch": 1.708816705336427,
        "step": 13257
    },
    {
        "loss": 0.8747,
        "grad_norm": 4.238648414611816,
        "learning_rate": 8.066379617228424e-05,
        "epoch": 1.7089456045372518,
        "step": 13258
    },
    {
        "loss": 1.8669,
        "grad_norm": 2.6170570850372314,
        "learning_rate": 8.06041076198543e-05,
        "epoch": 1.7090745037380768,
        "step": 13259
    },
    {
        "loss": 1.028,
        "grad_norm": 1.989941120147705,
        "learning_rate": 8.054442624690616e-05,
        "epoch": 1.7092034029389018,
        "step": 13260
    },
    {
        "loss": 1.3635,
        "grad_norm": 2.1431937217712402,
        "learning_rate": 8.048475207553118e-05,
        "epoch": 1.7093323021397269,
        "step": 13261
    },
    {
        "loss": 1.9803,
        "grad_norm": 2.3934988975524902,
        "learning_rate": 8.042508512781803e-05,
        "epoch": 1.7094612013405517,
        "step": 13262
    },
    {
        "loss": 1.3838,
        "grad_norm": 2.777407169342041,
        "learning_rate": 8.036542542585269e-05,
        "epoch": 1.7095901005413765,
        "step": 13263
    },
    {
        "loss": 1.6638,
        "grad_norm": 1.890167474746704,
        "learning_rate": 8.03057729917184e-05,
        "epoch": 1.7097189997422015,
        "step": 13264
    },
    {
        "loss": 1.6501,
        "grad_norm": 2.401832342147827,
        "learning_rate": 8.024612784749602e-05,
        "epoch": 1.7098478989430266,
        "step": 13265
    },
    {
        "loss": 1.9582,
        "grad_norm": 1.7808170318603516,
        "learning_rate": 8.018649001526335e-05,
        "epoch": 1.7099767981438516,
        "step": 13266
    },
    {
        "loss": 1.9192,
        "grad_norm": 1.6158311367034912,
        "learning_rate": 8.012685951709564e-05,
        "epoch": 1.7101056973446764,
        "step": 13267
    },
    {
        "loss": 1.4042,
        "grad_norm": 2.1756272315979004,
        "learning_rate": 8.00672363750654e-05,
        "epoch": 1.7102345965455015,
        "step": 13268
    },
    {
        "loss": 2.0383,
        "grad_norm": 2.764326810836792,
        "learning_rate": 8.000762061124239e-05,
        "epoch": 1.7103634957463263,
        "step": 13269
    },
    {
        "loss": 2.0834,
        "grad_norm": 1.478011131286621,
        "learning_rate": 7.99480122476937e-05,
        "epoch": 1.7104923949471513,
        "step": 13270
    },
    {
        "loss": 1.9854,
        "grad_norm": 2.4189071655273438,
        "learning_rate": 7.988841130648365e-05,
        "epoch": 1.7106212941479764,
        "step": 13271
    },
    {
        "loss": 0.6736,
        "grad_norm": 1.9391400814056396,
        "learning_rate": 7.982881780967365e-05,
        "epoch": 1.7107501933488012,
        "step": 13272
    },
    {
        "loss": 1.8084,
        "grad_norm": 1.879441499710083,
        "learning_rate": 7.976923177932294e-05,
        "epoch": 1.7108790925496262,
        "step": 13273
    },
    {
        "loss": 1.6802,
        "grad_norm": 1.7514957189559937,
        "learning_rate": 7.970965323748718e-05,
        "epoch": 1.711007991750451,
        "step": 13274
    },
    {
        "loss": 2.3235,
        "grad_norm": 1.603568196296692,
        "learning_rate": 7.96500822062196e-05,
        "epoch": 1.711136890951276,
        "step": 13275
    },
    {
        "loss": 1.7258,
        "grad_norm": 3.071470260620117,
        "learning_rate": 7.959051870757104e-05,
        "epoch": 1.7112657901521011,
        "step": 13276
    },
    {
        "loss": 0.6001,
        "grad_norm": 2.999803304672241,
        "learning_rate": 7.9530962763589e-05,
        "epoch": 1.7113946893529262,
        "step": 13277
    },
    {
        "loss": 0.7237,
        "grad_norm": 3.0406174659729004,
        "learning_rate": 7.947141439631847e-05,
        "epoch": 1.711523588553751,
        "step": 13278
    },
    {
        "loss": 1.0876,
        "grad_norm": 4.023169994354248,
        "learning_rate": 7.941187362780153e-05,
        "epoch": 1.7116524877545758,
        "step": 13279
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.2447266578674316,
        "learning_rate": 7.935234048007746e-05,
        "epoch": 1.7117813869554008,
        "step": 13280
    },
    {
        "loss": 0.9512,
        "grad_norm": 3.354060411453247,
        "learning_rate": 7.929281497518265e-05,
        "epoch": 1.7119102861562259,
        "step": 13281
    },
    {
        "loss": 0.9409,
        "grad_norm": 3.078707695007324,
        "learning_rate": 7.923329713515096e-05,
        "epoch": 1.712039185357051,
        "step": 13282
    },
    {
        "loss": 1.5502,
        "grad_norm": 2.379061698913574,
        "learning_rate": 7.917378698201312e-05,
        "epoch": 1.7121680845578757,
        "step": 13283
    },
    {
        "loss": 0.7383,
        "grad_norm": 2.1182916164398193,
        "learning_rate": 7.911428453779707e-05,
        "epoch": 1.7122969837587005,
        "step": 13284
    },
    {
        "loss": 1.4662,
        "grad_norm": 2.428586483001709,
        "learning_rate": 7.90547898245279e-05,
        "epoch": 1.7124258829595256,
        "step": 13285
    },
    {
        "loss": 1.8129,
        "grad_norm": 1.5658342838287354,
        "learning_rate": 7.89953028642279e-05,
        "epoch": 1.7125547821603506,
        "step": 13286
    },
    {
        "loss": 2.3734,
        "grad_norm": 1.4903820753097534,
        "learning_rate": 7.893582367891646e-05,
        "epoch": 1.7126836813611757,
        "step": 13287
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.251563787460327,
        "learning_rate": 7.887635229061002e-05,
        "epoch": 1.7128125805620005,
        "step": 13288
    },
    {
        "loss": 1.483,
        "grad_norm": 3.1689584255218506,
        "learning_rate": 7.881688872132213e-05,
        "epoch": 1.7129414797628255,
        "step": 13289
    },
    {
        "loss": 1.1892,
        "grad_norm": 2.3337936401367188,
        "learning_rate": 7.87574329930639e-05,
        "epoch": 1.7130703789636503,
        "step": 13290
    },
    {
        "loss": 1.1109,
        "grad_norm": 3.0575175285339355,
        "learning_rate": 7.86979851278427e-05,
        "epoch": 1.7131992781644754,
        "step": 13291
    },
    {
        "loss": 2.3085,
        "grad_norm": 2.1198463439941406,
        "learning_rate": 7.863854514766352e-05,
        "epoch": 1.7133281773653004,
        "step": 13292
    },
    {
        "loss": 1.4123,
        "grad_norm": 2.6067283153533936,
        "learning_rate": 7.857911307452855e-05,
        "epoch": 1.7134570765661254,
        "step": 13293
    },
    {
        "loss": 0.9453,
        "grad_norm": 3.2393558025360107,
        "learning_rate": 7.851968893043676e-05,
        "epoch": 1.7135859757669503,
        "step": 13294
    },
    {
        "loss": 0.6123,
        "grad_norm": 1.2135109901428223,
        "learning_rate": 7.846027273738413e-05,
        "epoch": 1.713714874967775,
        "step": 13295
    },
    {
        "loss": 1.5879,
        "grad_norm": 3.1965246200561523,
        "learning_rate": 7.840086451736423e-05,
        "epoch": 1.7138437741686001,
        "step": 13296
    },
    {
        "loss": 1.3305,
        "grad_norm": 4.296572685241699,
        "learning_rate": 7.834146429236694e-05,
        "epoch": 1.7139726733694252,
        "step": 13297
    },
    {
        "loss": 1.3007,
        "grad_norm": 2.846837043762207,
        "learning_rate": 7.828207208437952e-05,
        "epoch": 1.7141015725702502,
        "step": 13298
    },
    {
        "loss": 1.9351,
        "grad_norm": 2.387598991394043,
        "learning_rate": 7.822268791538649e-05,
        "epoch": 1.714230471771075,
        "step": 13299
    },
    {
        "loss": 2.0518,
        "grad_norm": 1.8556468486785889,
        "learning_rate": 7.816331180736913e-05,
        "epoch": 1.7143593709718998,
        "step": 13300
    },
    {
        "loss": 1.4992,
        "grad_norm": 3.023725986480713,
        "learning_rate": 7.810394378230577e-05,
        "epoch": 1.7144882701727249,
        "step": 13301
    },
    {
        "loss": 2.4669,
        "grad_norm": 1.5089421272277832,
        "learning_rate": 7.80445838621717e-05,
        "epoch": 1.71461716937355,
        "step": 13302
    },
    {
        "loss": 1.0284,
        "grad_norm": 2.6662778854370117,
        "learning_rate": 7.798523206893938e-05,
        "epoch": 1.714746068574375,
        "step": 13303
    },
    {
        "loss": 1.6033,
        "grad_norm": 2.7319328784942627,
        "learning_rate": 7.792588842457808e-05,
        "epoch": 1.7148749677751998,
        "step": 13304
    },
    {
        "loss": 2.0824,
        "grad_norm": 2.470317840576172,
        "learning_rate": 7.786655295105416e-05,
        "epoch": 1.7150038669760246,
        "step": 13305
    },
    {
        "loss": 2.2625,
        "grad_norm": 2.5779786109924316,
        "learning_rate": 7.780722567033078e-05,
        "epoch": 1.7151327661768496,
        "step": 13306
    },
    {
        "loss": 2.3271,
        "grad_norm": 1.6400904655456543,
        "learning_rate": 7.774790660436866e-05,
        "epoch": 1.7152616653776747,
        "step": 13307
    },
    {
        "loss": 1.2882,
        "grad_norm": 3.311276435852051,
        "learning_rate": 7.768859577512457e-05,
        "epoch": 1.7153905645784997,
        "step": 13308
    },
    {
        "loss": 1.6595,
        "grad_norm": 2.128749370574951,
        "learning_rate": 7.762929320455274e-05,
        "epoch": 1.7155194637793245,
        "step": 13309
    },
    {
        "loss": 1.7794,
        "grad_norm": 2.3503270149230957,
        "learning_rate": 7.756999891460455e-05,
        "epoch": 1.7156483629801496,
        "step": 13310
    },
    {
        "loss": 2.2435,
        "grad_norm": 1.7470405101776123,
        "learning_rate": 7.751071292722794e-05,
        "epoch": 1.7157772621809744,
        "step": 13311
    },
    {
        "loss": 1.1418,
        "grad_norm": 3.0734941959381104,
        "learning_rate": 7.745143526436775e-05,
        "epoch": 1.7159061613817994,
        "step": 13312
    },
    {
        "loss": 0.5064,
        "grad_norm": 1.2797400951385498,
        "learning_rate": 7.73921659479663e-05,
        "epoch": 1.7160350605826244,
        "step": 13313
    },
    {
        "loss": 1.7128,
        "grad_norm": 2.2548348903656006,
        "learning_rate": 7.733290499996203e-05,
        "epoch": 1.7161639597834495,
        "step": 13314
    },
    {
        "loss": 1.2474,
        "grad_norm": 2.4410910606384277,
        "learning_rate": 7.72736524422906e-05,
        "epoch": 1.7162928589842743,
        "step": 13315
    },
    {
        "loss": 2.0376,
        "grad_norm": 3.8471591472625732,
        "learning_rate": 7.721440829688497e-05,
        "epoch": 1.7164217581850991,
        "step": 13316
    },
    {
        "loss": 2.4246,
        "grad_norm": 1.4196901321411133,
        "learning_rate": 7.715517258567445e-05,
        "epoch": 1.7165506573859242,
        "step": 13317
    },
    {
        "loss": 1.5797,
        "grad_norm": 2.174818992614746,
        "learning_rate": 7.709594533058544e-05,
        "epoch": 1.7166795565867492,
        "step": 13318
    },
    {
        "loss": 1.0072,
        "grad_norm": 2.371090888977051,
        "learning_rate": 7.703672655354119e-05,
        "epoch": 1.7168084557875742,
        "step": 13319
    },
    {
        "loss": 2.3718,
        "grad_norm": 2.7515175342559814,
        "learning_rate": 7.697751627646184e-05,
        "epoch": 1.716937354988399,
        "step": 13320
    },
    {
        "loss": 1.8378,
        "grad_norm": 2.635125160217285,
        "learning_rate": 7.69183145212643e-05,
        "epoch": 1.7170662541892239,
        "step": 13321
    },
    {
        "loss": 1.3322,
        "grad_norm": 2.3269753456115723,
        "learning_rate": 7.685912130986241e-05,
        "epoch": 1.717195153390049,
        "step": 13322
    },
    {
        "loss": 2.2589,
        "grad_norm": 2.4332265853881836,
        "learning_rate": 7.679993666416669e-05,
        "epoch": 1.717324052590874,
        "step": 13323
    },
    {
        "loss": 1.7196,
        "grad_norm": 2.4650964736938477,
        "learning_rate": 7.674076060608503e-05,
        "epoch": 1.717452951791699,
        "step": 13324
    },
    {
        "loss": 2.0678,
        "grad_norm": 3.719332456588745,
        "learning_rate": 7.66815931575213e-05,
        "epoch": 1.7175818509925238,
        "step": 13325
    },
    {
        "loss": 1.7605,
        "grad_norm": 2.8630268573760986,
        "learning_rate": 7.662243434037663e-05,
        "epoch": 1.7177107501933488,
        "step": 13326
    },
    {
        "loss": 0.9827,
        "grad_norm": 3.5238091945648193,
        "learning_rate": 7.65632841765492e-05,
        "epoch": 1.7178396493941737,
        "step": 13327
    },
    {
        "loss": 2.2991,
        "grad_norm": 1.931848168373108,
        "learning_rate": 7.650414268793358e-05,
        "epoch": 1.7179685485949987,
        "step": 13328
    },
    {
        "loss": 2.4204,
        "grad_norm": 2.6339526176452637,
        "learning_rate": 7.644500989642116e-05,
        "epoch": 1.7180974477958237,
        "step": 13329
    },
    {
        "loss": 1.6107,
        "grad_norm": 3.6003966331481934,
        "learning_rate": 7.638588582390062e-05,
        "epoch": 1.7182263469966488,
        "step": 13330
    },
    {
        "loss": 1.4952,
        "grad_norm": 2.4447617530822754,
        "learning_rate": 7.63267704922566e-05,
        "epoch": 1.7183552461974736,
        "step": 13331
    },
    {
        "loss": 1.1681,
        "grad_norm": 2.2149524688720703,
        "learning_rate": 7.626766392337093e-05,
        "epoch": 1.7184841453982984,
        "step": 13332
    },
    {
        "loss": 1.0135,
        "grad_norm": 2.1490843296051025,
        "learning_rate": 7.620856613912248e-05,
        "epoch": 1.7186130445991235,
        "step": 13333
    },
    {
        "loss": 1.6195,
        "grad_norm": 2.702371597290039,
        "learning_rate": 7.61494771613864e-05,
        "epoch": 1.7187419437999485,
        "step": 13334
    },
    {
        "loss": 2.0266,
        "grad_norm": 1.4173814058303833,
        "learning_rate": 7.609039701203478e-05,
        "epoch": 1.7188708430007735,
        "step": 13335
    },
    {
        "loss": 1.3039,
        "grad_norm": 3.0483100414276123,
        "learning_rate": 7.60313257129364e-05,
        "epoch": 1.7189997422015983,
        "step": 13336
    },
    {
        "loss": 2.2163,
        "grad_norm": 1.4626750946044922,
        "learning_rate": 7.597226328595682e-05,
        "epoch": 1.7191286414024232,
        "step": 13337
    },
    {
        "loss": 1.6878,
        "grad_norm": 1.84638512134552,
        "learning_rate": 7.591320975295824e-05,
        "epoch": 1.7192575406032482,
        "step": 13338
    },
    {
        "loss": 1.8437,
        "grad_norm": 3.951859474182129,
        "learning_rate": 7.585416513579962e-05,
        "epoch": 1.7193864398040732,
        "step": 13339
    },
    {
        "loss": 1.456,
        "grad_norm": 2.075223207473755,
        "learning_rate": 7.579512945633647e-05,
        "epoch": 1.7195153390048983,
        "step": 13340
    },
    {
        "loss": 2.108,
        "grad_norm": 2.4174535274505615,
        "learning_rate": 7.573610273642154e-05,
        "epoch": 1.719644238205723,
        "step": 13341
    },
    {
        "loss": 1.8875,
        "grad_norm": 2.8950350284576416,
        "learning_rate": 7.567708499790344e-05,
        "epoch": 1.719773137406548,
        "step": 13342
    },
    {
        "loss": 1.2137,
        "grad_norm": 3.0042715072631836,
        "learning_rate": 7.561807626262785e-05,
        "epoch": 1.719902036607373,
        "step": 13343
    },
    {
        "loss": 1.4343,
        "grad_norm": 3.0864861011505127,
        "learning_rate": 7.555907655243743e-05,
        "epoch": 1.720030935808198,
        "step": 13344
    },
    {
        "loss": 1.9381,
        "grad_norm": 2.5927000045776367,
        "learning_rate": 7.550008588917108e-05,
        "epoch": 1.720159835009023,
        "step": 13345
    },
    {
        "loss": 1.5343,
        "grad_norm": 2.5545430183410645,
        "learning_rate": 7.544110429466429e-05,
        "epoch": 1.7202887342098478,
        "step": 13346
    },
    {
        "loss": 1.6786,
        "grad_norm": 2.9194204807281494,
        "learning_rate": 7.538213179074983e-05,
        "epoch": 1.7204176334106729,
        "step": 13347
    },
    {
        "loss": 2.0724,
        "grad_norm": 2.431766986846924,
        "learning_rate": 7.532316839925622e-05,
        "epoch": 1.7205465326114977,
        "step": 13348
    },
    {
        "loss": 1.0169,
        "grad_norm": 3.305408477783203,
        "learning_rate": 7.526421414200903e-05,
        "epoch": 1.7206754318123227,
        "step": 13349
    },
    {
        "loss": 2.3425,
        "grad_norm": 2.085278034210205,
        "learning_rate": 7.520526904083079e-05,
        "epoch": 1.7208043310131478,
        "step": 13350
    },
    {
        "loss": 1.8123,
        "grad_norm": 2.5994396209716797,
        "learning_rate": 7.514633311754008e-05,
        "epoch": 1.7209332302139728,
        "step": 13351
    },
    {
        "loss": 2.0173,
        "grad_norm": 2.5061750411987305,
        "learning_rate": 7.508740639395239e-05,
        "epoch": 1.7210621294147976,
        "step": 13352
    },
    {
        "loss": 1.9606,
        "grad_norm": 1.690442681312561,
        "learning_rate": 7.502848889187973e-05,
        "epoch": 1.7211910286156225,
        "step": 13353
    },
    {
        "loss": 1.2352,
        "grad_norm": 2.7529966831207275,
        "learning_rate": 7.496958063313061e-05,
        "epoch": 1.7213199278164475,
        "step": 13354
    },
    {
        "loss": 1.6057,
        "grad_norm": 2.2585549354553223,
        "learning_rate": 7.491068163951028e-05,
        "epoch": 1.7214488270172725,
        "step": 13355
    },
    {
        "loss": 1.6121,
        "grad_norm": 1.892806887626648,
        "learning_rate": 7.485179193282049e-05,
        "epoch": 1.7215777262180976,
        "step": 13356
    },
    {
        "loss": 1.0154,
        "grad_norm": 3.8214292526245117,
        "learning_rate": 7.479291153485932e-05,
        "epoch": 1.7217066254189224,
        "step": 13357
    },
    {
        "loss": 2.0587,
        "grad_norm": 2.107659101486206,
        "learning_rate": 7.473404046742201e-05,
        "epoch": 1.7218355246197472,
        "step": 13358
    },
    {
        "loss": 1.4053,
        "grad_norm": 1.8306612968444824,
        "learning_rate": 7.467517875229988e-05,
        "epoch": 1.7219644238205722,
        "step": 13359
    },
    {
        "loss": 1.9961,
        "grad_norm": 1.7043895721435547,
        "learning_rate": 7.461632641128051e-05,
        "epoch": 1.7220933230213973,
        "step": 13360
    },
    {
        "loss": 1.5811,
        "grad_norm": 3.5472702980041504,
        "learning_rate": 7.455748346614879e-05,
        "epoch": 1.7222222222222223,
        "step": 13361
    },
    {
        "loss": 0.4726,
        "grad_norm": 1.8455026149749756,
        "learning_rate": 7.449864993868554e-05,
        "epoch": 1.7223511214230471,
        "step": 13362
    },
    {
        "loss": 2.1142,
        "grad_norm": 2.8024559020996094,
        "learning_rate": 7.443982585066818e-05,
        "epoch": 1.7224800206238722,
        "step": 13363
    },
    {
        "loss": 2.4958,
        "grad_norm": 2.6329073905944824,
        "learning_rate": 7.43810112238711e-05,
        "epoch": 1.722608919824697,
        "step": 13364
    },
    {
        "loss": 0.5561,
        "grad_norm": 2.472416639328003,
        "learning_rate": 7.432220608006441e-05,
        "epoch": 1.722737819025522,
        "step": 13365
    },
    {
        "loss": 2.036,
        "grad_norm": 2.7399580478668213,
        "learning_rate": 7.426341044101512e-05,
        "epoch": 1.722866718226347,
        "step": 13366
    },
    {
        "loss": 0.9415,
        "grad_norm": 2.187727212905884,
        "learning_rate": 7.420462432848699e-05,
        "epoch": 1.722995617427172,
        "step": 13367
    },
    {
        "loss": 2.0751,
        "grad_norm": 1.9566422700881958,
        "learning_rate": 7.414584776423983e-05,
        "epoch": 1.723124516627997,
        "step": 13368
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.1450252532958984,
        "learning_rate": 7.40870807700301e-05,
        "epoch": 1.7232534158288217,
        "step": 13369
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.1769418716430664,
        "learning_rate": 7.402832336761065e-05,
        "epoch": 1.7233823150296468,
        "step": 13370
    },
    {
        "loss": 1.7147,
        "grad_norm": 1.8294789791107178,
        "learning_rate": 7.39695755787308e-05,
        "epoch": 1.7235112142304718,
        "step": 13371
    },
    {
        "loss": 1.7883,
        "grad_norm": 2.123138666152954,
        "learning_rate": 7.391083742513636e-05,
        "epoch": 1.7236401134312969,
        "step": 13372
    },
    {
        "loss": 1.4921,
        "grad_norm": 1.4688355922698975,
        "learning_rate": 7.385210892856953e-05,
        "epoch": 1.7237690126321217,
        "step": 13373
    },
    {
        "loss": 1.6968,
        "grad_norm": 2.645925998687744,
        "learning_rate": 7.379339011076878e-05,
        "epoch": 1.7238979118329465,
        "step": 13374
    },
    {
        "loss": 0.5444,
        "grad_norm": 3.8334317207336426,
        "learning_rate": 7.373468099346946e-05,
        "epoch": 1.7240268110337715,
        "step": 13375
    },
    {
        "loss": 1.8865,
        "grad_norm": 2.0287272930145264,
        "learning_rate": 7.367598159840301e-05,
        "epoch": 1.7241557102345966,
        "step": 13376
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.7705096006393433,
        "learning_rate": 7.361729194729687e-05,
        "epoch": 1.7242846094354216,
        "step": 13377
    },
    {
        "loss": 1.3496,
        "grad_norm": 2.623716115951538,
        "learning_rate": 7.355861206187575e-05,
        "epoch": 1.7244135086362464,
        "step": 13378
    },
    {
        "loss": 2.2836,
        "grad_norm": 2.5434319972991943,
        "learning_rate": 7.349994196386011e-05,
        "epoch": 1.7245424078370712,
        "step": 13379
    },
    {
        "loss": 1.2221,
        "grad_norm": 2.759211540222168,
        "learning_rate": 7.344128167496687e-05,
        "epoch": 1.7246713070378963,
        "step": 13380
    },
    {
        "loss": 1.3306,
        "grad_norm": 2.966862678527832,
        "learning_rate": 7.33826312169096e-05,
        "epoch": 1.7248002062387213,
        "step": 13381
    },
    {
        "loss": 2.3289,
        "grad_norm": 2.04341197013855,
        "learning_rate": 7.332399061139816e-05,
        "epoch": 1.7249291054395464,
        "step": 13382
    },
    {
        "loss": 1.0725,
        "grad_norm": 3.3469948768615723,
        "learning_rate": 7.326535988013818e-05,
        "epoch": 1.7250580046403712,
        "step": 13383
    },
    {
        "loss": 0.5069,
        "grad_norm": 2.71889591217041,
        "learning_rate": 7.32067390448325e-05,
        "epoch": 1.7251869038411962,
        "step": 13384
    },
    {
        "loss": 1.9121,
        "grad_norm": 3.0400846004486084,
        "learning_rate": 7.314812812717979e-05,
        "epoch": 1.725315803042021,
        "step": 13385
    },
    {
        "loss": 2.1989,
        "grad_norm": 2.159477949142456,
        "learning_rate": 7.308952714887516e-05,
        "epoch": 1.725444702242846,
        "step": 13386
    },
    {
        "loss": 1.3798,
        "grad_norm": 3.009005069732666,
        "learning_rate": 7.303093613161003e-05,
        "epoch": 1.7255736014436711,
        "step": 13387
    },
    {
        "loss": 0.924,
        "grad_norm": 2.306170701980591,
        "learning_rate": 7.297235509707215e-05,
        "epoch": 1.7257025006444962,
        "step": 13388
    },
    {
        "loss": 0.7269,
        "grad_norm": 2.161196231842041,
        "learning_rate": 7.291378406694558e-05,
        "epoch": 1.725831399845321,
        "step": 13389
    },
    {
        "loss": 0.4783,
        "grad_norm": 3.4701297283172607,
        "learning_rate": 7.285522306291061e-05,
        "epoch": 1.7259602990461458,
        "step": 13390
    },
    {
        "loss": 1.98,
        "grad_norm": 2.099745273590088,
        "learning_rate": 7.279667210664384e-05,
        "epoch": 1.7260891982469708,
        "step": 13391
    },
    {
        "loss": 2.11,
        "grad_norm": 5.95221471786499,
        "learning_rate": 7.273813121981836e-05,
        "epoch": 1.7262180974477959,
        "step": 13392
    },
    {
        "loss": 1.844,
        "grad_norm": 1.9825323820114136,
        "learning_rate": 7.26796004241034e-05,
        "epoch": 1.726346996648621,
        "step": 13393
    },
    {
        "loss": 0.5725,
        "grad_norm": 2.309941530227661,
        "learning_rate": 7.262107974116401e-05,
        "epoch": 1.7264758958494457,
        "step": 13394
    },
    {
        "loss": 1.8653,
        "grad_norm": 1.9256367683410645,
        "learning_rate": 7.256256919266227e-05,
        "epoch": 1.7266047950502705,
        "step": 13395
    },
    {
        "loss": 0.9043,
        "grad_norm": 2.3556249141693115,
        "learning_rate": 7.250406880025606e-05,
        "epoch": 1.7267336942510956,
        "step": 13396
    },
    {
        "loss": 0.2458,
        "grad_norm": 0.9730314612388611,
        "learning_rate": 7.244557858559942e-05,
        "epoch": 1.7268625934519206,
        "step": 13397
    },
    {
        "loss": 1.5798,
        "grad_norm": 2.441260814666748,
        "learning_rate": 7.2387098570343e-05,
        "epoch": 1.7269914926527457,
        "step": 13398
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.9401576519012451,
        "learning_rate": 7.232862877613351e-05,
        "epoch": 1.7271203918535705,
        "step": 13399
    },
    {
        "loss": 1.19,
        "grad_norm": 5.213468551635742,
        "learning_rate": 7.227016922461343e-05,
        "epoch": 1.7272492910543955,
        "step": 13400
    },
    {
        "loss": 1.2021,
        "grad_norm": 2.496394395828247,
        "learning_rate": 7.22117199374222e-05,
        "epoch": 1.7273781902552203,
        "step": 13401
    },
    {
        "loss": 1.691,
        "grad_norm": 2.566206693649292,
        "learning_rate": 7.2153280936195e-05,
        "epoch": 1.7275070894560454,
        "step": 13402
    },
    {
        "loss": 1.3945,
        "grad_norm": 2.0665574073791504,
        "learning_rate": 7.209485224256331e-05,
        "epoch": 1.7276359886568704,
        "step": 13403
    },
    {
        "loss": 1.4574,
        "grad_norm": 3.115429401397705,
        "learning_rate": 7.203643387815473e-05,
        "epoch": 1.7277648878576954,
        "step": 13404
    },
    {
        "loss": 1.0073,
        "grad_norm": 2.7989354133605957,
        "learning_rate": 7.197802586459316e-05,
        "epoch": 1.7278937870585203,
        "step": 13405
    },
    {
        "loss": 0.5456,
        "grad_norm": 1.9262746572494507,
        "learning_rate": 7.191962822349859e-05,
        "epoch": 1.728022686259345,
        "step": 13406
    },
    {
        "loss": 1.2868,
        "grad_norm": 2.675692319869995,
        "learning_rate": 7.186124097648716e-05,
        "epoch": 1.7281515854601701,
        "step": 13407
    },
    {
        "loss": 1.4878,
        "grad_norm": 3.235321044921875,
        "learning_rate": 7.180286414517108e-05,
        "epoch": 1.7282804846609952,
        "step": 13408
    },
    {
        "loss": 1.2944,
        "grad_norm": 1.9280189275741577,
        "learning_rate": 7.174449775115905e-05,
        "epoch": 1.7284093838618202,
        "step": 13409
    },
    {
        "loss": 1.7649,
        "grad_norm": 1.920350432395935,
        "learning_rate": 7.16861418160557e-05,
        "epoch": 1.728538283062645,
        "step": 13410
    },
    {
        "loss": 1.4003,
        "grad_norm": 3.234846830368042,
        "learning_rate": 7.162779636146135e-05,
        "epoch": 1.7286671822634698,
        "step": 13411
    },
    {
        "loss": 1.5621,
        "grad_norm": 3.57440447807312,
        "learning_rate": 7.156946140897322e-05,
        "epoch": 1.7287960814642949,
        "step": 13412
    },
    {
        "loss": 1.4287,
        "grad_norm": 3.518184185028076,
        "learning_rate": 7.15111369801842e-05,
        "epoch": 1.72892498066512,
        "step": 13413
    },
    {
        "loss": 1.1023,
        "grad_norm": 2.6625328063964844,
        "learning_rate": 7.145282309668313e-05,
        "epoch": 1.729053879865945,
        "step": 13414
    },
    {
        "loss": 1.6939,
        "grad_norm": 2.333195209503174,
        "learning_rate": 7.139451978005552e-05,
        "epoch": 1.7291827790667698,
        "step": 13415
    },
    {
        "loss": 1.9089,
        "grad_norm": 2.616499423980713,
        "learning_rate": 7.133622705188257e-05,
        "epoch": 1.7293116782675946,
        "step": 13416
    },
    {
        "loss": 1.2879,
        "grad_norm": 3.7434329986572266,
        "learning_rate": 7.127794493374123e-05,
        "epoch": 1.7294405774684196,
        "step": 13417
    },
    {
        "loss": 1.5055,
        "grad_norm": 1.870346188545227,
        "learning_rate": 7.12196734472053e-05,
        "epoch": 1.7295694766692447,
        "step": 13418
    },
    {
        "loss": 0.6093,
        "grad_norm": 3.121123790740967,
        "learning_rate": 7.116141261384413e-05,
        "epoch": 1.7296983758700697,
        "step": 13419
    },
    {
        "loss": 2.3182,
        "grad_norm": 2.5328903198242188,
        "learning_rate": 7.11031624552232e-05,
        "epoch": 1.7298272750708945,
        "step": 13420
    },
    {
        "loss": 1.7813,
        "grad_norm": 3.0506839752197266,
        "learning_rate": 7.104492299290413e-05,
        "epoch": 1.7299561742717195,
        "step": 13421
    },
    {
        "loss": 1.7866,
        "grad_norm": 3.349076986312866,
        "learning_rate": 7.098669424844451e-05,
        "epoch": 1.7300850734725444,
        "step": 13422
    },
    {
        "loss": 1.7511,
        "grad_norm": 3.5161850452423096,
        "learning_rate": 7.0928476243398e-05,
        "epoch": 1.7302139726733694,
        "step": 13423
    },
    {
        "loss": 1.9519,
        "grad_norm": 1.8360798358917236,
        "learning_rate": 7.087026899931426e-05,
        "epoch": 1.7303428718741944,
        "step": 13424
    },
    {
        "loss": 0.8268,
        "grad_norm": 3.634000301361084,
        "learning_rate": 7.081207253773885e-05,
        "epoch": 1.7304717710750195,
        "step": 13425
    },
    {
        "loss": 1.8407,
        "grad_norm": 3.0211293697357178,
        "learning_rate": 7.075388688021374e-05,
        "epoch": 1.7306006702758443,
        "step": 13426
    },
    {
        "loss": 1.3401,
        "grad_norm": 3.989670753479004,
        "learning_rate": 7.069571204827662e-05,
        "epoch": 1.7307295694766691,
        "step": 13427
    },
    {
        "loss": 1.1417,
        "grad_norm": 4.6777849197387695,
        "learning_rate": 7.063754806346077e-05,
        "epoch": 1.7308584686774942,
        "step": 13428
    },
    {
        "loss": 1.744,
        "grad_norm": 1.5424338579177856,
        "learning_rate": 7.057939494729629e-05,
        "epoch": 1.7309873678783192,
        "step": 13429
    },
    {
        "loss": 1.6042,
        "grad_norm": 2.394242286682129,
        "learning_rate": 7.05212527213087e-05,
        "epoch": 1.7311162670791442,
        "step": 13430
    },
    {
        "loss": 1.6263,
        "grad_norm": 2.8534488677978516,
        "learning_rate": 7.046312140701945e-05,
        "epoch": 1.731245166279969,
        "step": 13431
    },
    {
        "loss": 1.2794,
        "grad_norm": 2.4437572956085205,
        "learning_rate": 7.040500102594643e-05,
        "epoch": 1.7313740654807939,
        "step": 13432
    },
    {
        "loss": 2.0301,
        "grad_norm": 2.2310867309570312,
        "learning_rate": 7.034689159960314e-05,
        "epoch": 1.731502964681619,
        "step": 13433
    },
    {
        "loss": 2.3969,
        "grad_norm": 2.0716731548309326,
        "learning_rate": 7.02887931494987e-05,
        "epoch": 1.731631863882444,
        "step": 13434
    },
    {
        "loss": 1.9804,
        "grad_norm": 2.123840808868408,
        "learning_rate": 7.02307056971389e-05,
        "epoch": 1.731760763083269,
        "step": 13435
    },
    {
        "loss": 2.0461,
        "grad_norm": 2.658400297164917,
        "learning_rate": 7.017262926402497e-05,
        "epoch": 1.7318896622840938,
        "step": 13436
    },
    {
        "loss": 1.9681,
        "grad_norm": 1.5264540910720825,
        "learning_rate": 7.011456387165414e-05,
        "epoch": 1.7320185614849188,
        "step": 13437
    },
    {
        "loss": 2.3687,
        "grad_norm": 2.4712982177734375,
        "learning_rate": 7.005650954151965e-05,
        "epoch": 1.7321474606857437,
        "step": 13438
    },
    {
        "loss": 1.9329,
        "grad_norm": 2.39902925491333,
        "learning_rate": 6.999846629511056e-05,
        "epoch": 1.7322763598865687,
        "step": 13439
    },
    {
        "loss": 1.9334,
        "grad_norm": 2.2574150562286377,
        "learning_rate": 6.994043415391181e-05,
        "epoch": 1.7324052590873937,
        "step": 13440
    },
    {
        "loss": 1.1957,
        "grad_norm": 2.8254001140594482,
        "learning_rate": 6.988241313940434e-05,
        "epoch": 1.7325341582882188,
        "step": 13441
    },
    {
        "loss": 1.9256,
        "grad_norm": 1.6382077932357788,
        "learning_rate": 6.982440327306474e-05,
        "epoch": 1.7326630574890436,
        "step": 13442
    },
    {
        "loss": 0.7011,
        "grad_norm": 2.5796091556549072,
        "learning_rate": 6.976640457636592e-05,
        "epoch": 1.7327919566898684,
        "step": 13443
    },
    {
        "loss": 1.0541,
        "grad_norm": 3.5160274505615234,
        "learning_rate": 6.970841707077622e-05,
        "epoch": 1.7329208558906934,
        "step": 13444
    },
    {
        "loss": 1.014,
        "grad_norm": 2.3071560859680176,
        "learning_rate": 6.965044077776e-05,
        "epoch": 1.7330497550915185,
        "step": 13445
    },
    {
        "loss": 2.3527,
        "grad_norm": 2.4363274574279785,
        "learning_rate": 6.959247571877746e-05,
        "epoch": 1.7331786542923435,
        "step": 13446
    },
    {
        "loss": 1.4203,
        "grad_norm": 2.8841474056243896,
        "learning_rate": 6.953452191528465e-05,
        "epoch": 1.7333075534931683,
        "step": 13447
    },
    {
        "loss": 1.7821,
        "grad_norm": 3.520998954772949,
        "learning_rate": 6.947657938873329e-05,
        "epoch": 1.7334364526939932,
        "step": 13448
    },
    {
        "loss": 1.2879,
        "grad_norm": 2.3356752395629883,
        "learning_rate": 6.941864816057138e-05,
        "epoch": 1.7335653518948182,
        "step": 13449
    },
    {
        "loss": 1.5662,
        "grad_norm": 1.9248493909835815,
        "learning_rate": 6.936072825224242e-05,
        "epoch": 1.7336942510956432,
        "step": 13450
    },
    {
        "loss": 1.8434,
        "grad_norm": 2.358713150024414,
        "learning_rate": 6.930281968518532e-05,
        "epoch": 1.7338231502964683,
        "step": 13451
    },
    {
        "loss": 0.4127,
        "grad_norm": 1.98308527469635,
        "learning_rate": 6.924492248083567e-05,
        "epoch": 1.733952049497293,
        "step": 13452
    },
    {
        "loss": 1.385,
        "grad_norm": 1.742488980293274,
        "learning_rate": 6.918703666062419e-05,
        "epoch": 1.734080948698118,
        "step": 13453
    },
    {
        "loss": 2.0421,
        "grad_norm": 1.846944808959961,
        "learning_rate": 6.912916224597766e-05,
        "epoch": 1.734209847898943,
        "step": 13454
    },
    {
        "loss": 1.2574,
        "grad_norm": 3.0965025424957275,
        "learning_rate": 6.907129925831853e-05,
        "epoch": 1.734338747099768,
        "step": 13455
    },
    {
        "loss": 1.8876,
        "grad_norm": 2.0853466987609863,
        "learning_rate": 6.901344771906507e-05,
        "epoch": 1.734467646300593,
        "step": 13456
    },
    {
        "loss": 1.353,
        "grad_norm": 2.3251426219940186,
        "learning_rate": 6.895560764963129e-05,
        "epoch": 1.7345965455014178,
        "step": 13457
    },
    {
        "loss": 1.5322,
        "grad_norm": 2.019801139831543,
        "learning_rate": 6.889777907142699e-05,
        "epoch": 1.7347254447022429,
        "step": 13458
    },
    {
        "loss": 1.2404,
        "grad_norm": 2.9232563972473145,
        "learning_rate": 6.883996200585753e-05,
        "epoch": 1.7348543439030677,
        "step": 13459
    },
    {
        "loss": 1.5306,
        "grad_norm": 2.5253517627716064,
        "learning_rate": 6.878215647432447e-05,
        "epoch": 1.7349832431038927,
        "step": 13460
    },
    {
        "loss": 1.1813,
        "grad_norm": 2.5408599376678467,
        "learning_rate": 6.872436249822462e-05,
        "epoch": 1.7351121423047178,
        "step": 13461
    },
    {
        "loss": 0.9458,
        "grad_norm": 3.4096410274505615,
        "learning_rate": 6.866658009895078e-05,
        "epoch": 1.7352410415055428,
        "step": 13462
    },
    {
        "loss": 1.4724,
        "grad_norm": 2.7245872020721436,
        "learning_rate": 6.860880929789131e-05,
        "epoch": 1.7353699407063676,
        "step": 13463
    },
    {
        "loss": 1.9128,
        "grad_norm": 4.388653755187988,
        "learning_rate": 6.855105011643038e-05,
        "epoch": 1.7354988399071924,
        "step": 13464
    },
    {
        "loss": 0.9894,
        "grad_norm": 2.614579439163208,
        "learning_rate": 6.849330257594765e-05,
        "epoch": 1.7356277391080175,
        "step": 13465
    },
    {
        "loss": 1.1486,
        "grad_norm": 3.999549388885498,
        "learning_rate": 6.843556669781897e-05,
        "epoch": 1.7357566383088425,
        "step": 13466
    },
    {
        "loss": 1.1526,
        "grad_norm": 3.5404574871063232,
        "learning_rate": 6.83778425034155e-05,
        "epoch": 1.7358855375096676,
        "step": 13467
    },
    {
        "loss": 1.1337,
        "grad_norm": 4.204044342041016,
        "learning_rate": 6.832013001410375e-05,
        "epoch": 1.7360144367104924,
        "step": 13468
    },
    {
        "loss": 2.1662,
        "grad_norm": 2.5521743297576904,
        "learning_rate": 6.826242925124668e-05,
        "epoch": 1.7361433359113172,
        "step": 13469
    },
    {
        "loss": 0.682,
        "grad_norm": 2.5243072509765625,
        "learning_rate": 6.820474023620233e-05,
        "epoch": 1.7362722351121422,
        "step": 13470
    },
    {
        "loss": 1.5418,
        "grad_norm": 1.835694432258606,
        "learning_rate": 6.814706299032462e-05,
        "epoch": 1.7364011343129673,
        "step": 13471
    },
    {
        "loss": 1.4613,
        "grad_norm": 3.7014434337615967,
        "learning_rate": 6.808939753496304e-05,
        "epoch": 1.7365300335137923,
        "step": 13472
    },
    {
        "loss": 1.6611,
        "grad_norm": 1.968436598777771,
        "learning_rate": 6.80317438914627e-05,
        "epoch": 1.7366589327146171,
        "step": 13473
    },
    {
        "loss": 1.8855,
        "grad_norm": 2.2457611560821533,
        "learning_rate": 6.797410208116439e-05,
        "epoch": 1.7367878319154422,
        "step": 13474
    },
    {
        "loss": 1.2824,
        "grad_norm": 1.617889642715454,
        "learning_rate": 6.79164721254045e-05,
        "epoch": 1.736916731116267,
        "step": 13475
    },
    {
        "loss": 0.9827,
        "grad_norm": 1.5215476751327515,
        "learning_rate": 6.78588540455149e-05,
        "epoch": 1.737045630317092,
        "step": 13476
    },
    {
        "loss": 1.3863,
        "grad_norm": 2.414968967437744,
        "learning_rate": 6.780124786282348e-05,
        "epoch": 1.737174529517917,
        "step": 13477
    },
    {
        "loss": 2.0595,
        "grad_norm": 2.3154587745666504,
        "learning_rate": 6.774365359865327e-05,
        "epoch": 1.737303428718742,
        "step": 13478
    },
    {
        "loss": 2.0536,
        "grad_norm": 2.443990707397461,
        "learning_rate": 6.768607127432309e-05,
        "epoch": 1.737432327919567,
        "step": 13479
    },
    {
        "loss": 1.2338,
        "grad_norm": 3.268440008163452,
        "learning_rate": 6.762850091114732e-05,
        "epoch": 1.7375612271203917,
        "step": 13480
    },
    {
        "loss": 1.3992,
        "grad_norm": 2.218369960784912,
        "learning_rate": 6.757094253043587e-05,
        "epoch": 1.7376901263212168,
        "step": 13481
    },
    {
        "loss": 1.882,
        "grad_norm": 1.9035733938217163,
        "learning_rate": 6.751339615349415e-05,
        "epoch": 1.7378190255220418,
        "step": 13482
    },
    {
        "loss": 1.7643,
        "grad_norm": 2.6133358478546143,
        "learning_rate": 6.745586180162344e-05,
        "epoch": 1.7379479247228669,
        "step": 13483
    },
    {
        "loss": 1.2436,
        "grad_norm": 2.275836706161499,
        "learning_rate": 6.739833949612028e-05,
        "epoch": 1.7380768239236917,
        "step": 13484
    },
    {
        "loss": 1.4952,
        "grad_norm": 2.627086639404297,
        "learning_rate": 6.734082925827681e-05,
        "epoch": 1.7382057231245165,
        "step": 13485
    },
    {
        "loss": 2.085,
        "grad_norm": 1.4367954730987549,
        "learning_rate": 6.728333110938069e-05,
        "epoch": 1.7383346223253415,
        "step": 13486
    },
    {
        "loss": 1.9592,
        "grad_norm": 1.450913429260254,
        "learning_rate": 6.722584507071517e-05,
        "epoch": 1.7384635215261666,
        "step": 13487
    },
    {
        "loss": 1.8097,
        "grad_norm": 2.5742034912109375,
        "learning_rate": 6.716837116355894e-05,
        "epoch": 1.7385924207269916,
        "step": 13488
    },
    {
        "loss": 2.0799,
        "grad_norm": 2.38134765625,
        "learning_rate": 6.711090940918626e-05,
        "epoch": 1.7387213199278164,
        "step": 13489
    },
    {
        "loss": 1.29,
        "grad_norm": 2.2538483142852783,
        "learning_rate": 6.70534598288669e-05,
        "epoch": 1.7388502191286412,
        "step": 13490
    },
    {
        "loss": 0.6293,
        "grad_norm": 2.1184427738189697,
        "learning_rate": 6.699602244386605e-05,
        "epoch": 1.7389791183294663,
        "step": 13491
    },
    {
        "loss": 1.5214,
        "grad_norm": 2.0085606575012207,
        "learning_rate": 6.693859727544445e-05,
        "epoch": 1.7391080175302913,
        "step": 13492
    },
    {
        "loss": 2.22,
        "grad_norm": 2.8805856704711914,
        "learning_rate": 6.688118434485818e-05,
        "epoch": 1.7392369167311164,
        "step": 13493
    },
    {
        "loss": 2.2453,
        "grad_norm": 2.0284650325775146,
        "learning_rate": 6.682378367335915e-05,
        "epoch": 1.7393658159319412,
        "step": 13494
    },
    {
        "loss": 1.8103,
        "grad_norm": 2.0553462505340576,
        "learning_rate": 6.676639528219438e-05,
        "epoch": 1.7394947151327662,
        "step": 13495
    },
    {
        "loss": 1.7943,
        "grad_norm": 2.5531771183013916,
        "learning_rate": 6.670901919260644e-05,
        "epoch": 1.739623614333591,
        "step": 13496
    },
    {
        "loss": 1.8289,
        "grad_norm": 1.91328763961792,
        "learning_rate": 6.665165542583338e-05,
        "epoch": 1.739752513534416,
        "step": 13497
    },
    {
        "loss": 0.9606,
        "grad_norm": 2.125723123550415,
        "learning_rate": 6.659430400310865e-05,
        "epoch": 1.739881412735241,
        "step": 13498
    },
    {
        "loss": 1.2197,
        "grad_norm": 2.144056797027588,
        "learning_rate": 6.653696494566103e-05,
        "epoch": 1.7400103119360661,
        "step": 13499
    },
    {
        "loss": 1.1329,
        "grad_norm": 2.536679983139038,
        "learning_rate": 6.647963827471509e-05,
        "epoch": 1.740139211136891,
        "step": 13500
    },
    {
        "loss": 0.9706,
        "grad_norm": 1.2320371866226196,
        "learning_rate": 6.642232401149048e-05,
        "epoch": 1.7402681103377158,
        "step": 13501
    },
    {
        "loss": 0.9215,
        "grad_norm": 2.610562801361084,
        "learning_rate": 6.636502217720233e-05,
        "epoch": 1.7403970095385408,
        "step": 13502
    },
    {
        "loss": 1.2636,
        "grad_norm": 2.68424916267395,
        "learning_rate": 6.630773279306115e-05,
        "epoch": 1.7405259087393659,
        "step": 13503
    },
    {
        "loss": 2.1142,
        "grad_norm": 1.2718603610992432,
        "learning_rate": 6.625045588027293e-05,
        "epoch": 1.740654807940191,
        "step": 13504
    },
    {
        "loss": 2.1277,
        "grad_norm": 2.337395429611206,
        "learning_rate": 6.619319146003897e-05,
        "epoch": 1.7407837071410157,
        "step": 13505
    },
    {
        "loss": 1.0498,
        "grad_norm": 2.411566972732544,
        "learning_rate": 6.613593955355594e-05,
        "epoch": 1.7409126063418405,
        "step": 13506
    },
    {
        "loss": 1.5016,
        "grad_norm": 4.868345260620117,
        "learning_rate": 6.607870018201586e-05,
        "epoch": 1.7410415055426656,
        "step": 13507
    },
    {
        "loss": 1.5568,
        "grad_norm": 2.470269203186035,
        "learning_rate": 6.602147336660645e-05,
        "epoch": 1.7411704047434906,
        "step": 13508
    },
    {
        "loss": 1.4699,
        "grad_norm": 3.1700806617736816,
        "learning_rate": 6.596425912851012e-05,
        "epoch": 1.7412993039443156,
        "step": 13509
    },
    {
        "loss": 0.5384,
        "grad_norm": 1.7011911869049072,
        "learning_rate": 6.590705748890504e-05,
        "epoch": 1.7414282031451405,
        "step": 13510
    },
    {
        "loss": 1.6618,
        "grad_norm": 2.23179030418396,
        "learning_rate": 6.584986846896486e-05,
        "epoch": 1.7415571023459655,
        "step": 13511
    },
    {
        "loss": 1.4827,
        "grad_norm": 3.378011465072632,
        "learning_rate": 6.579269208985832e-05,
        "epoch": 1.7416860015467903,
        "step": 13512
    },
    {
        "loss": 1.5029,
        "grad_norm": 3.4293265342712402,
        "learning_rate": 6.573552837274945e-05,
        "epoch": 1.7418149007476154,
        "step": 13513
    },
    {
        "loss": 0.9172,
        "grad_norm": 2.9600203037261963,
        "learning_rate": 6.567837733879772e-05,
        "epoch": 1.7419437999484404,
        "step": 13514
    },
    {
        "loss": 1.4408,
        "grad_norm": 2.239872932434082,
        "learning_rate": 6.562123900915784e-05,
        "epoch": 1.7420726991492654,
        "step": 13515
    },
    {
        "loss": 1.3155,
        "grad_norm": 2.805870532989502,
        "learning_rate": 6.55641134049797e-05,
        "epoch": 1.7422015983500903,
        "step": 13516
    },
    {
        "loss": 1.5682,
        "grad_norm": 1.7984939813613892,
        "learning_rate": 6.55070005474089e-05,
        "epoch": 1.742330497550915,
        "step": 13517
    },
    {
        "loss": 1.8553,
        "grad_norm": 2.2157034873962402,
        "learning_rate": 6.544990045758587e-05,
        "epoch": 1.74245939675174,
        "step": 13518
    },
    {
        "loss": 1.9422,
        "grad_norm": 1.7690021991729736,
        "learning_rate": 6.53928131566465e-05,
        "epoch": 1.7425882959525651,
        "step": 13519
    },
    {
        "loss": 1.7845,
        "grad_norm": 1.684959888458252,
        "learning_rate": 6.533573866572193e-05,
        "epoch": 1.7427171951533902,
        "step": 13520
    },
    {
        "loss": 1.6368,
        "grad_norm": 2.309849739074707,
        "learning_rate": 6.527867700593855e-05,
        "epoch": 1.742846094354215,
        "step": 13521
    },
    {
        "loss": 1.6422,
        "grad_norm": 1.4848847389221191,
        "learning_rate": 6.522162819841797e-05,
        "epoch": 1.7429749935550398,
        "step": 13522
    },
    {
        "loss": 1.6856,
        "grad_norm": 1.8616453409194946,
        "learning_rate": 6.516459226427715e-05,
        "epoch": 1.7431038927558649,
        "step": 13523
    },
    {
        "loss": 2.0486,
        "grad_norm": 2.6057024002075195,
        "learning_rate": 6.510756922462802e-05,
        "epoch": 1.74323279195669,
        "step": 13524
    },
    {
        "loss": 1.988,
        "grad_norm": 2.191566228866577,
        "learning_rate": 6.505055910057833e-05,
        "epoch": 1.743361691157515,
        "step": 13525
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.5893528461456299,
        "learning_rate": 6.49935619132303e-05,
        "epoch": 1.7434905903583398,
        "step": 13526
    },
    {
        "loss": 2.1146,
        "grad_norm": 5.717356204986572,
        "learning_rate": 6.493657768368164e-05,
        "epoch": 1.7436194895591646,
        "step": 13527
    },
    {
        "loss": 1.3345,
        "grad_norm": 2.6302759647369385,
        "learning_rate": 6.487960643302566e-05,
        "epoch": 1.7437483887599896,
        "step": 13528
    },
    {
        "loss": 2.0177,
        "grad_norm": 1.7603527307510376,
        "learning_rate": 6.48226481823504e-05,
        "epoch": 1.7438772879608146,
        "step": 13529
    },
    {
        "loss": 1.0009,
        "grad_norm": 4.675081253051758,
        "learning_rate": 6.476570295273918e-05,
        "epoch": 1.7440061871616397,
        "step": 13530
    },
    {
        "loss": 0.9469,
        "grad_norm": 1.8092514276504517,
        "learning_rate": 6.470877076527058e-05,
        "epoch": 1.7441350863624645,
        "step": 13531
    },
    {
        "loss": 1.5602,
        "grad_norm": 2.644240140914917,
        "learning_rate": 6.465185164101833e-05,
        "epoch": 1.7442639855632895,
        "step": 13532
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.2744373083114624,
        "learning_rate": 6.459494560105115e-05,
        "epoch": 1.7443928847641144,
        "step": 13533
    },
    {
        "loss": 1.495,
        "grad_norm": 2.2361197471618652,
        "learning_rate": 6.453805266643337e-05,
        "epoch": 1.7445217839649394,
        "step": 13534
    },
    {
        "loss": 1.5828,
        "grad_norm": 2.60898756980896,
        "learning_rate": 6.448117285822405e-05,
        "epoch": 1.7446506831657644,
        "step": 13535
    },
    {
        "loss": 1.1455,
        "grad_norm": 2.113938093185425,
        "learning_rate": 6.442430619747751e-05,
        "epoch": 1.7447795823665895,
        "step": 13536
    },
    {
        "loss": 1.3205,
        "grad_norm": 1.7038453817367554,
        "learning_rate": 6.436745270524321e-05,
        "epoch": 1.7449084815674143,
        "step": 13537
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.7162113189697266,
        "learning_rate": 6.431061240256578e-05,
        "epoch": 1.745037380768239,
        "step": 13538
    },
    {
        "loss": 1.5188,
        "grad_norm": 2.5846056938171387,
        "learning_rate": 6.425378531048485e-05,
        "epoch": 1.7451662799690641,
        "step": 13539
    },
    {
        "loss": 1.5177,
        "grad_norm": 3.353692054748535,
        "learning_rate": 6.419697145003533e-05,
        "epoch": 1.7452951791698892,
        "step": 13540
    },
    {
        "loss": 0.8076,
        "grad_norm": 2.3432228565216064,
        "learning_rate": 6.414017084224694e-05,
        "epoch": 1.7454240783707142,
        "step": 13541
    },
    {
        "loss": 0.8968,
        "grad_norm": 2.691492795944214,
        "learning_rate": 6.408338350814514e-05,
        "epoch": 1.745552977571539,
        "step": 13542
    },
    {
        "loss": 1.1829,
        "grad_norm": 2.91615891456604,
        "learning_rate": 6.402660946874958e-05,
        "epoch": 1.7456818767723639,
        "step": 13543
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.5510834455490112,
        "learning_rate": 6.39698487450755e-05,
        "epoch": 1.745810775973189,
        "step": 13544
    },
    {
        "loss": 2.1199,
        "grad_norm": 1.3783221244812012,
        "learning_rate": 6.391310135813339e-05,
        "epoch": 1.745939675174014,
        "step": 13545
    },
    {
        "loss": 1.4357,
        "grad_norm": 2.1952691078186035,
        "learning_rate": 6.385636732892843e-05,
        "epoch": 1.746068574374839,
        "step": 13546
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.8915704488754272,
        "learning_rate": 6.379964667846084e-05,
        "epoch": 1.7461974735756638,
        "step": 13547
    },
    {
        "loss": 0.9128,
        "grad_norm": 2.629260540008545,
        "learning_rate": 6.374293942772646e-05,
        "epoch": 1.7463263727764888,
        "step": 13548
    },
    {
        "loss": 1.8384,
        "grad_norm": 3.0771396160125732,
        "learning_rate": 6.368624559771537e-05,
        "epoch": 1.7464552719773137,
        "step": 13549
    },
    {
        "loss": 2.0408,
        "grad_norm": 2.088721990585327,
        "learning_rate": 6.362956520941302e-05,
        "epoch": 1.7465841711781387,
        "step": 13550
    },
    {
        "loss": 1.5945,
        "grad_norm": 1.5868703126907349,
        "learning_rate": 6.357289828380022e-05,
        "epoch": 1.7467130703789637,
        "step": 13551
    },
    {
        "loss": 1.4613,
        "grad_norm": 1.9985027313232422,
        "learning_rate": 6.351624484185232e-05,
        "epoch": 1.7468419695797888,
        "step": 13552
    },
    {
        "loss": 1.2375,
        "grad_norm": 3.483752489089966,
        "learning_rate": 6.345960490453995e-05,
        "epoch": 1.7469708687806136,
        "step": 13553
    },
    {
        "loss": 1.3864,
        "grad_norm": 1.688634991645813,
        "learning_rate": 6.340297849282858e-05,
        "epoch": 1.7470997679814384,
        "step": 13554
    },
    {
        "loss": 1.3071,
        "grad_norm": 3.0819661617279053,
        "learning_rate": 6.334636562767875e-05,
        "epoch": 1.7472286671822634,
        "step": 13555
    },
    {
        "loss": 2.0247,
        "grad_norm": 2.4612369537353516,
        "learning_rate": 6.328976633004602e-05,
        "epoch": 1.7473575663830885,
        "step": 13556
    },
    {
        "loss": 1.6826,
        "grad_norm": 2.3398373126983643,
        "learning_rate": 6.323318062088086e-05,
        "epoch": 1.7474864655839135,
        "step": 13557
    },
    {
        "loss": 1.1805,
        "grad_norm": 2.43135929107666,
        "learning_rate": 6.317660852112861e-05,
        "epoch": 1.7476153647847383,
        "step": 13558
    },
    {
        "loss": 1.6174,
        "grad_norm": 2.9283382892608643,
        "learning_rate": 6.31200500517301e-05,
        "epoch": 1.7477442639855632,
        "step": 13559
    },
    {
        "loss": 1.8575,
        "grad_norm": 2.90889310836792,
        "learning_rate": 6.306350523362035e-05,
        "epoch": 1.7478731631863882,
        "step": 13560
    },
    {
        "loss": 1.9674,
        "grad_norm": 2.0640969276428223,
        "learning_rate": 6.300697408772963e-05,
        "epoch": 1.7480020623872132,
        "step": 13561
    },
    {
        "loss": 1.4039,
        "grad_norm": 3.104787826538086,
        "learning_rate": 6.295045663498354e-05,
        "epoch": 1.7481309615880383,
        "step": 13562
    },
    {
        "loss": 1.9596,
        "grad_norm": 1.5700478553771973,
        "learning_rate": 6.289395289630212e-05,
        "epoch": 1.748259860788863,
        "step": 13563
    },
    {
        "loss": 2.1085,
        "grad_norm": 2.4490628242492676,
        "learning_rate": 6.283746289260036e-05,
        "epoch": 1.748388759989688,
        "step": 13564
    },
    {
        "loss": 1.8136,
        "grad_norm": 1.5718039274215698,
        "learning_rate": 6.278098664478869e-05,
        "epoch": 1.748517659190513,
        "step": 13565
    },
    {
        "loss": 1.6335,
        "grad_norm": 3.246663808822632,
        "learning_rate": 6.272452417377172e-05,
        "epoch": 1.748646558391338,
        "step": 13566
    },
    {
        "loss": 1.2619,
        "grad_norm": 2.8787972927093506,
        "learning_rate": 6.266807550044924e-05,
        "epoch": 1.748775457592163,
        "step": 13567
    },
    {
        "loss": 1.2466,
        "grad_norm": 2.0610201358795166,
        "learning_rate": 6.261164064571631e-05,
        "epoch": 1.7489043567929878,
        "step": 13568
    },
    {
        "loss": 1.8598,
        "grad_norm": 2.3548569679260254,
        "learning_rate": 6.255521963046238e-05,
        "epoch": 1.7490332559938129,
        "step": 13569
    },
    {
        "loss": 1.8865,
        "grad_norm": 1.7474435567855835,
        "learning_rate": 6.249881247557199e-05,
        "epoch": 1.7491621551946377,
        "step": 13570
    },
    {
        "loss": 1.9747,
        "grad_norm": 1.7220288515090942,
        "learning_rate": 6.244241920192451e-05,
        "epoch": 1.7492910543954627,
        "step": 13571
    },
    {
        "loss": 1.4004,
        "grad_norm": 3.6996641159057617,
        "learning_rate": 6.238603983039417e-05,
        "epoch": 1.7494199535962878,
        "step": 13572
    },
    {
        "loss": 1.5483,
        "grad_norm": 4.2220377922058105,
        "learning_rate": 6.232967438185009e-05,
        "epoch": 1.7495488527971128,
        "step": 13573
    },
    {
        "loss": 1.9357,
        "grad_norm": 1.5889992713928223,
        "learning_rate": 6.227332287715617e-05,
        "epoch": 1.7496777519979376,
        "step": 13574
    },
    {
        "loss": 2.2005,
        "grad_norm": 1.9625540971755981,
        "learning_rate": 6.221698533717107e-05,
        "epoch": 1.7498066511987624,
        "step": 13575
    },
    {
        "loss": 1.0951,
        "grad_norm": 2.6207878589630127,
        "learning_rate": 6.216066178274881e-05,
        "epoch": 1.7499355503995875,
        "step": 13576
    },
    {
        "loss": 0.3107,
        "grad_norm": 0.9390692710876465,
        "learning_rate": 6.21043522347374e-05,
        "epoch": 1.7500644496004125,
        "step": 13577
    },
    {
        "loss": 1.9899,
        "grad_norm": 1.7014336585998535,
        "learning_rate": 6.204805671398011e-05,
        "epoch": 1.7501933488012376,
        "step": 13578
    },
    {
        "loss": 2.2169,
        "grad_norm": 3.3952419757843018,
        "learning_rate": 6.199177524131526e-05,
        "epoch": 1.7503222480020624,
        "step": 13579
    },
    {
        "loss": 2.0148,
        "grad_norm": 2.1883206367492676,
        "learning_rate": 6.193550783757551e-05,
        "epoch": 1.7504511472028872,
        "step": 13580
    },
    {
        "loss": 1.4634,
        "grad_norm": 3.1715962886810303,
        "learning_rate": 6.187925452358849e-05,
        "epoch": 1.7505800464037122,
        "step": 13581
    },
    {
        "loss": 1.9015,
        "grad_norm": 2.885807514190674,
        "learning_rate": 6.182301532017694e-05,
        "epoch": 1.7507089456045373,
        "step": 13582
    },
    {
        "loss": 1.6486,
        "grad_norm": 1.8407796621322632,
        "learning_rate": 6.176679024815767e-05,
        "epoch": 1.7508378448053623,
        "step": 13583
    },
    {
        "loss": 1.5107,
        "grad_norm": 1.4374693632125854,
        "learning_rate": 6.171057932834272e-05,
        "epoch": 1.7509667440061871,
        "step": 13584
    },
    {
        "loss": 1.3235,
        "grad_norm": 1.6098167896270752,
        "learning_rate": 6.165438258153904e-05,
        "epoch": 1.7510956432070122,
        "step": 13585
    },
    {
        "loss": 1.6899,
        "grad_norm": 2.290231466293335,
        "learning_rate": 6.159820002854801e-05,
        "epoch": 1.751224542407837,
        "step": 13586
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.7463109493255615,
        "learning_rate": 6.154203169016587e-05,
        "epoch": 1.751353441608662,
        "step": 13587
    },
    {
        "loss": 1.7423,
        "grad_norm": 2.940704107284546,
        "learning_rate": 6.14858775871836e-05,
        "epoch": 1.751482340809487,
        "step": 13588
    },
    {
        "loss": 0.9383,
        "grad_norm": 3.5840375423431396,
        "learning_rate": 6.14297377403869e-05,
        "epoch": 1.751611240010312,
        "step": 13589
    },
    {
        "loss": 1.6686,
        "grad_norm": 3.7378597259521484,
        "learning_rate": 6.137361217055621e-05,
        "epoch": 1.751740139211137,
        "step": 13590
    },
    {
        "loss": 1.1392,
        "grad_norm": 2.8551902770996094,
        "learning_rate": 6.131750089846666e-05,
        "epoch": 1.7518690384119617,
        "step": 13591
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.0217888355255127,
        "learning_rate": 6.1261403944888e-05,
        "epoch": 1.7519979376127868,
        "step": 13592
    },
    {
        "loss": 1.4045,
        "grad_norm": 3.414372682571411,
        "learning_rate": 6.120532133058517e-05,
        "epoch": 1.7521268368136118,
        "step": 13593
    },
    {
        "loss": 1.0771,
        "grad_norm": 2.7936134338378906,
        "learning_rate": 6.114925307631698e-05,
        "epoch": 1.7522557360144368,
        "step": 13594
    },
    {
        "loss": 1.9328,
        "grad_norm": 2.2791383266448975,
        "learning_rate": 6.109319920283743e-05,
        "epoch": 1.7523846352152617,
        "step": 13595
    },
    {
        "loss": 1.3714,
        "grad_norm": 2.9951303005218506,
        "learning_rate": 6.103715973089532e-05,
        "epoch": 1.7525135344160865,
        "step": 13596
    },
    {
        "loss": 1.3013,
        "grad_norm": 2.7754669189453125,
        "learning_rate": 6.0981134681233876e-05,
        "epoch": 1.7526424336169115,
        "step": 13597
    },
    {
        "loss": 1.4068,
        "grad_norm": 3.3027429580688477,
        "learning_rate": 6.0925124074590856e-05,
        "epoch": 1.7527713328177366,
        "step": 13598
    },
    {
        "loss": 1.3907,
        "grad_norm": 2.7602691650390625,
        "learning_rate": 6.0869127931699234e-05,
        "epoch": 1.7529002320185616,
        "step": 13599
    },
    {
        "loss": 1.6495,
        "grad_norm": 1.8062744140625,
        "learning_rate": 6.081314627328585e-05,
        "epoch": 1.7530291312193864,
        "step": 13600
    },
    {
        "loss": 0.7214,
        "grad_norm": 1.6980708837509155,
        "learning_rate": 6.075717912007265e-05,
        "epoch": 1.7531580304202112,
        "step": 13601
    },
    {
        "loss": 1.9805,
        "grad_norm": 1.6256297826766968,
        "learning_rate": 6.070122649277632e-05,
        "epoch": 1.7532869296210363,
        "step": 13602
    },
    {
        "loss": 2.1468,
        "grad_norm": 2.370490550994873,
        "learning_rate": 6.064528841210792e-05,
        "epoch": 1.7534158288218613,
        "step": 13603
    },
    {
        "loss": 2.1202,
        "grad_norm": 3.311166524887085,
        "learning_rate": 6.0589364898773184e-05,
        "epoch": 1.7535447280226863,
        "step": 13604
    },
    {
        "loss": 1.8382,
        "grad_norm": 1.947601556777954,
        "learning_rate": 6.053345597347243e-05,
        "epoch": 1.7536736272235112,
        "step": 13605
    },
    {
        "loss": 2.0833,
        "grad_norm": 1.6086817979812622,
        "learning_rate": 6.047756165690066e-05,
        "epoch": 1.7538025264243362,
        "step": 13606
    },
    {
        "loss": 2.2153,
        "grad_norm": 4.388792037963867,
        "learning_rate": 6.04216819697474e-05,
        "epoch": 1.753931425625161,
        "step": 13607
    },
    {
        "loss": 1.7003,
        "grad_norm": 2.9908714294433594,
        "learning_rate": 6.0365816932696765e-05,
        "epoch": 1.754060324825986,
        "step": 13608
    },
    {
        "loss": 1.2668,
        "grad_norm": 2.680654764175415,
        "learning_rate": 6.0309966566427375e-05,
        "epoch": 1.754189224026811,
        "step": 13609
    },
    {
        "loss": 1.3274,
        "grad_norm": 1.8536672592163086,
        "learning_rate": 6.0254130891612717e-05,
        "epoch": 1.7543181232276361,
        "step": 13610
    },
    {
        "loss": 1.5036,
        "grad_norm": 1.8143068552017212,
        "learning_rate": 6.0198309928920685e-05,
        "epoch": 1.754447022428461,
        "step": 13611
    },
    {
        "loss": 2.217,
        "grad_norm": 2.6654012203216553,
        "learning_rate": 6.0142503699013266e-05,
        "epoch": 1.7545759216292858,
        "step": 13612
    },
    {
        "loss": 1.1906,
        "grad_norm": 2.0326783657073975,
        "learning_rate": 6.008671222254775e-05,
        "epoch": 1.7547048208301108,
        "step": 13613
    },
    {
        "loss": 1.5268,
        "grad_norm": 3.3376352787017822,
        "learning_rate": 6.003093552017555e-05,
        "epoch": 1.7548337200309359,
        "step": 13614
    },
    {
        "loss": 1.4815,
        "grad_norm": 2.2060422897338867,
        "learning_rate": 5.99751736125425e-05,
        "epoch": 1.754962619231761,
        "step": 13615
    },
    {
        "loss": 1.404,
        "grad_norm": 3.462639093399048,
        "learning_rate": 5.991942652028956e-05,
        "epoch": 1.7550915184325857,
        "step": 13616
    },
    {
        "loss": 1.601,
        "grad_norm": 2.2856380939483643,
        "learning_rate": 5.986369426405137e-05,
        "epoch": 1.7552204176334105,
        "step": 13617
    },
    {
        "loss": 0.175,
        "grad_norm": 0.49249064922332764,
        "learning_rate": 5.9807976864457494e-05,
        "epoch": 1.7553493168342356,
        "step": 13618
    },
    {
        "loss": 1.0475,
        "grad_norm": 2.436274290084839,
        "learning_rate": 5.975227434213226e-05,
        "epoch": 1.7554782160350606,
        "step": 13619
    },
    {
        "loss": 1.9987,
        "grad_norm": 3.0880773067474365,
        "learning_rate": 5.9696586717694095e-05,
        "epoch": 1.7556071152358856,
        "step": 13620
    },
    {
        "loss": 1.6502,
        "grad_norm": 2.6206250190734863,
        "learning_rate": 5.9640914011756e-05,
        "epoch": 1.7557360144367105,
        "step": 13621
    },
    {
        "loss": 0.9894,
        "grad_norm": 1.9648197889328003,
        "learning_rate": 5.9585256244925546e-05,
        "epoch": 1.7558649136375355,
        "step": 13622
    },
    {
        "loss": 1.7051,
        "grad_norm": 2.0199966430664062,
        "learning_rate": 5.9529613437804675e-05,
        "epoch": 1.7559938128383603,
        "step": 13623
    },
    {
        "loss": 1.4385,
        "grad_norm": 2.501444101333618,
        "learning_rate": 5.947398561098987e-05,
        "epoch": 1.7561227120391854,
        "step": 13624
    },
    {
        "loss": 2.2725,
        "grad_norm": 1.7849147319793701,
        "learning_rate": 5.941837278507201e-05,
        "epoch": 1.7562516112400104,
        "step": 13625
    },
    {
        "loss": 1.3324,
        "grad_norm": 4.940504550933838,
        "learning_rate": 5.936277498063634e-05,
        "epoch": 1.7563805104408354,
        "step": 13626
    },
    {
        "loss": 0.5044,
        "grad_norm": 1.91063392162323,
        "learning_rate": 5.9307192218262906e-05,
        "epoch": 1.7565094096416602,
        "step": 13627
    },
    {
        "loss": 1.6747,
        "grad_norm": 2.1146469116210938,
        "learning_rate": 5.925162451852591e-05,
        "epoch": 1.756638308842485,
        "step": 13628
    },
    {
        "loss": 0.8449,
        "grad_norm": 2.3894400596618652,
        "learning_rate": 5.919607190199363e-05,
        "epoch": 1.75676720804331,
        "step": 13629
    },
    {
        "loss": 1.1158,
        "grad_norm": 2.919049024581909,
        "learning_rate": 5.914053438922953e-05,
        "epoch": 1.7568961072441351,
        "step": 13630
    },
    {
        "loss": 1.8836,
        "grad_norm": 2.1775805950164795,
        "learning_rate": 5.908501200079094e-05,
        "epoch": 1.7570250064449602,
        "step": 13631
    },
    {
        "loss": 1.5517,
        "grad_norm": 1.9287842512130737,
        "learning_rate": 5.902950475722959e-05,
        "epoch": 1.757153905645785,
        "step": 13632
    },
    {
        "loss": 0.7195,
        "grad_norm": 3.5333969593048096,
        "learning_rate": 5.897401267909206e-05,
        "epoch": 1.7572828048466098,
        "step": 13633
    },
    {
        "loss": 1.9904,
        "grad_norm": 1.6328513622283936,
        "learning_rate": 5.891853578691895e-05,
        "epoch": 1.7574117040474349,
        "step": 13634
    },
    {
        "loss": 1.7423,
        "grad_norm": 1.2095998525619507,
        "learning_rate": 5.886307410124493e-05,
        "epoch": 1.75754060324826,
        "step": 13635
    },
    {
        "loss": 1.3417,
        "grad_norm": 2.1506924629211426,
        "learning_rate": 5.8807627642599814e-05,
        "epoch": 1.757669502449085,
        "step": 13636
    },
    {
        "loss": 1.7811,
        "grad_norm": 2.8150851726531982,
        "learning_rate": 5.8752196431507225e-05,
        "epoch": 1.7577984016499097,
        "step": 13637
    },
    {
        "loss": 1.262,
        "grad_norm": 2.5049002170562744,
        "learning_rate": 5.869678048848527e-05,
        "epoch": 1.7579273008507346,
        "step": 13638
    },
    {
        "loss": 1.8288,
        "grad_norm": 1.8818354606628418,
        "learning_rate": 5.864137983404646e-05,
        "epoch": 1.7580562000515596,
        "step": 13639
    },
    {
        "loss": 1.7685,
        "grad_norm": 1.536993384361267,
        "learning_rate": 5.858599448869759e-05,
        "epoch": 1.7581850992523846,
        "step": 13640
    },
    {
        "loss": 1.9532,
        "grad_norm": 1.8736616373062134,
        "learning_rate": 5.853062447293981e-05,
        "epoch": 1.7583139984532097,
        "step": 13641
    },
    {
        "loss": 1.0125,
        "grad_norm": 3.092884063720703,
        "learning_rate": 5.847526980726861e-05,
        "epoch": 1.7584428976540345,
        "step": 13642
    },
    {
        "loss": 0.3348,
        "grad_norm": 1.3837019205093384,
        "learning_rate": 5.8419930512173625e-05,
        "epoch": 1.7585717968548595,
        "step": 13643
    },
    {
        "loss": 1.3106,
        "grad_norm": 1.200943946838379,
        "learning_rate": 5.836460660813924e-05,
        "epoch": 1.7587006960556844,
        "step": 13644
    },
    {
        "loss": 1.6986,
        "grad_norm": 2.9945719242095947,
        "learning_rate": 5.830929811564385e-05,
        "epoch": 1.7588295952565094,
        "step": 13645
    },
    {
        "loss": 1.9715,
        "grad_norm": 2.4518046379089355,
        "learning_rate": 5.825400505515981e-05,
        "epoch": 1.7589584944573344,
        "step": 13646
    },
    {
        "loss": 1.359,
        "grad_norm": 3.3376893997192383,
        "learning_rate": 5.819872744715444e-05,
        "epoch": 1.7590873936581595,
        "step": 13647
    },
    {
        "loss": 1.9796,
        "grad_norm": 5.084715843200684,
        "learning_rate": 5.81434653120889e-05,
        "epoch": 1.7592162928589843,
        "step": 13648
    },
    {
        "loss": 1.6307,
        "grad_norm": 3.0450992584228516,
        "learning_rate": 5.80882186704186e-05,
        "epoch": 1.759345192059809,
        "step": 13649
    },
    {
        "loss": 2.2428,
        "grad_norm": 2.1992616653442383,
        "learning_rate": 5.803298754259359e-05,
        "epoch": 1.7594740912606341,
        "step": 13650
    },
    {
        "loss": 0.7977,
        "grad_norm": 2.528743028640747,
        "learning_rate": 5.797777194905797e-05,
        "epoch": 1.7596029904614592,
        "step": 13651
    },
    {
        "loss": 1.281,
        "grad_norm": 2.1097424030303955,
        "learning_rate": 5.7922571910249666e-05,
        "epoch": 1.7597318896622842,
        "step": 13652
    },
    {
        "loss": 1.9636,
        "grad_norm": 1.9999217987060547,
        "learning_rate": 5.78673874466016e-05,
        "epoch": 1.759860788863109,
        "step": 13653
    },
    {
        "loss": 1.607,
        "grad_norm": 2.557865619659424,
        "learning_rate": 5.781221857854044e-05,
        "epoch": 1.7599896880639339,
        "step": 13654
    },
    {
        "loss": 1.7223,
        "grad_norm": 2.2013823986053467,
        "learning_rate": 5.775706532648722e-05,
        "epoch": 1.760118587264759,
        "step": 13655
    },
    {
        "loss": 2.1197,
        "grad_norm": 2.852386474609375,
        "learning_rate": 5.770192771085712e-05,
        "epoch": 1.760247486465584,
        "step": 13656
    },
    {
        "loss": 1.3324,
        "grad_norm": 2.2718985080718994,
        "learning_rate": 5.764680575205966e-05,
        "epoch": 1.760376385666409,
        "step": 13657
    },
    {
        "loss": 2.1869,
        "grad_norm": 2.191412925720215,
        "learning_rate": 5.7591699470498494e-05,
        "epoch": 1.7605052848672338,
        "step": 13658
    },
    {
        "loss": 1.8469,
        "grad_norm": 3.596043348312378,
        "learning_rate": 5.753660888657144e-05,
        "epoch": 1.7606341840680588,
        "step": 13659
    },
    {
        "loss": 1.184,
        "grad_norm": 2.408682107925415,
        "learning_rate": 5.7481534020670424e-05,
        "epoch": 1.7607630832688836,
        "step": 13660
    },
    {
        "loss": 1.4838,
        "grad_norm": 4.001369476318359,
        "learning_rate": 5.742647489318189e-05,
        "epoch": 1.7608919824697087,
        "step": 13661
    },
    {
        "loss": 2.4885,
        "grad_norm": 3.3094122409820557,
        "learning_rate": 5.737143152448629e-05,
        "epoch": 1.7610208816705337,
        "step": 13662
    },
    {
        "loss": 0.8626,
        "grad_norm": 2.6336748600006104,
        "learning_rate": 5.731640393495781e-05,
        "epoch": 1.7611497808713588,
        "step": 13663
    },
    {
        "loss": 0.7591,
        "grad_norm": 2.3893747329711914,
        "learning_rate": 5.726139214496552e-05,
        "epoch": 1.7612786800721836,
        "step": 13664
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.6172269582748413,
        "learning_rate": 5.720639617487218e-05,
        "epoch": 1.7614075792730084,
        "step": 13665
    },
    {
        "loss": 1.5137,
        "grad_norm": 2.9308903217315674,
        "learning_rate": 5.715141604503467e-05,
        "epoch": 1.7615364784738334,
        "step": 13666
    },
    {
        "loss": 1.9217,
        "grad_norm": 2.327240467071533,
        "learning_rate": 5.7096451775804426e-05,
        "epoch": 1.7616653776746585,
        "step": 13667
    },
    {
        "loss": 1.5991,
        "grad_norm": 2.5533783435821533,
        "learning_rate": 5.7041503387526715e-05,
        "epoch": 1.7617942768754835,
        "step": 13668
    },
    {
        "loss": 1.7267,
        "grad_norm": 1.8103724718093872,
        "learning_rate": 5.698657090054059e-05,
        "epoch": 1.7619231760763083,
        "step": 13669
    },
    {
        "loss": 1.8658,
        "grad_norm": 3.52880597114563,
        "learning_rate": 5.6931654335179926e-05,
        "epoch": 1.7620520752771331,
        "step": 13670
    },
    {
        "loss": 1.0668,
        "grad_norm": 3.585271120071411,
        "learning_rate": 5.687675371177224e-05,
        "epoch": 1.7621809744779582,
        "step": 13671
    },
    {
        "loss": 1.7499,
        "grad_norm": 2.2281956672668457,
        "learning_rate": 5.68218690506392e-05,
        "epoch": 1.7623098736787832,
        "step": 13672
    },
    {
        "loss": 0.9545,
        "grad_norm": 2.407205820083618,
        "learning_rate": 5.6767000372096716e-05,
        "epoch": 1.7624387728796083,
        "step": 13673
    },
    {
        "loss": 2.206,
        "grad_norm": 1.4805946350097656,
        "learning_rate": 5.6712147696454644e-05,
        "epoch": 1.762567672080433,
        "step": 13674
    },
    {
        "loss": 1.5108,
        "grad_norm": 2.934114694595337,
        "learning_rate": 5.665731104401697e-05,
        "epoch": 1.762696571281258,
        "step": 13675
    },
    {
        "loss": 1.2073,
        "grad_norm": 2.37906813621521,
        "learning_rate": 5.6602490435081744e-05,
        "epoch": 1.762825470482083,
        "step": 13676
    },
    {
        "loss": 1.6319,
        "grad_norm": 1.9128594398498535,
        "learning_rate": 5.6547685889940905e-05,
        "epoch": 1.762954369682908,
        "step": 13677
    },
    {
        "loss": 1.6173,
        "grad_norm": 1.9370101690292358,
        "learning_rate": 5.649289742888095e-05,
        "epoch": 1.763083268883733,
        "step": 13678
    },
    {
        "loss": 2.5441,
        "grad_norm": 1.4766076803207397,
        "learning_rate": 5.6438125072182026e-05,
        "epoch": 1.7632121680845578,
        "step": 13679
    },
    {
        "loss": 1.2012,
        "grad_norm": 3.220186710357666,
        "learning_rate": 5.638336884011805e-05,
        "epoch": 1.7633410672853829,
        "step": 13680
    },
    {
        "loss": 1.5993,
        "grad_norm": 2.030447483062744,
        "learning_rate": 5.632862875295764e-05,
        "epoch": 1.7634699664862077,
        "step": 13681
    },
    {
        "loss": 1.9079,
        "grad_norm": 2.024888277053833,
        "learning_rate": 5.627390483096298e-05,
        "epoch": 1.7635988656870327,
        "step": 13682
    },
    {
        "loss": 2.4411,
        "grad_norm": 1.8109831809997559,
        "learning_rate": 5.621919709439024e-05,
        "epoch": 1.7637277648878578,
        "step": 13683
    },
    {
        "loss": 2.021,
        "grad_norm": 1.9995383024215698,
        "learning_rate": 5.6164505563490065e-05,
        "epoch": 1.7638566640886828,
        "step": 13684
    },
    {
        "loss": 1.1888,
        "grad_norm": 3.1959328651428223,
        "learning_rate": 5.6109830258506756e-05,
        "epoch": 1.7639855632895076,
        "step": 13685
    },
    {
        "loss": 1.2018,
        "grad_norm": 2.426985502243042,
        "learning_rate": 5.605517119967826e-05,
        "epoch": 1.7641144624903324,
        "step": 13686
    },
    {
        "loss": 2.0516,
        "grad_norm": 1.6808446645736694,
        "learning_rate": 5.600052840723725e-05,
        "epoch": 1.7642433616911575,
        "step": 13687
    },
    {
        "loss": 1.0212,
        "grad_norm": 2.2369790077209473,
        "learning_rate": 5.59459019014099e-05,
        "epoch": 1.7643722608919825,
        "step": 13688
    },
    {
        "loss": 1.733,
        "grad_norm": 2.5952036380767822,
        "learning_rate": 5.5891291702416424e-05,
        "epoch": 1.7645011600928076,
        "step": 13689
    },
    {
        "loss": 1.7741,
        "grad_norm": 2.4913039207458496,
        "learning_rate": 5.5836697830471155e-05,
        "epoch": 1.7646300592936324,
        "step": 13690
    },
    {
        "loss": 1.9226,
        "grad_norm": 1.7305423021316528,
        "learning_rate": 5.5782120305782205e-05,
        "epoch": 1.7647589584944572,
        "step": 13691
    },
    {
        "loss": 1.3442,
        "grad_norm": 3.2719907760620117,
        "learning_rate": 5.57275591485517e-05,
        "epoch": 1.7648878576952822,
        "step": 13692
    },
    {
        "loss": 2.3156,
        "grad_norm": 2.3337602615356445,
        "learning_rate": 5.567301437897573e-05,
        "epoch": 1.7650167568961073,
        "step": 13693
    },
    {
        "loss": 0.4911,
        "grad_norm": 1.8654292821884155,
        "learning_rate": 5.561848601724413e-05,
        "epoch": 1.7651456560969323,
        "step": 13694
    },
    {
        "loss": 1.2529,
        "grad_norm": 2.7681124210357666,
        "learning_rate": 5.556397408354117e-05,
        "epoch": 1.7652745552977571,
        "step": 13695
    },
    {
        "loss": 1.5912,
        "grad_norm": 2.0092248916625977,
        "learning_rate": 5.5509478598044517e-05,
        "epoch": 1.7654034544985822,
        "step": 13696
    },
    {
        "loss": 1.7904,
        "grad_norm": 2.8250105381011963,
        "learning_rate": 5.545499958092596e-05,
        "epoch": 1.765532353699407,
        "step": 13697
    },
    {
        "loss": 1.7667,
        "grad_norm": 2.048410177230835,
        "learning_rate": 5.540053705235113e-05,
        "epoch": 1.765661252900232,
        "step": 13698
    },
    {
        "loss": 1.5326,
        "grad_norm": 2.729759693145752,
        "learning_rate": 5.534609103247963e-05,
        "epoch": 1.765790152101057,
        "step": 13699
    },
    {
        "loss": 1.5526,
        "grad_norm": 2.058274269104004,
        "learning_rate": 5.5291661541464745e-05,
        "epoch": 1.765919051301882,
        "step": 13700
    },
    {
        "loss": 1.4236,
        "grad_norm": 2.847099781036377,
        "learning_rate": 5.523724859945409e-05,
        "epoch": 1.766047950502707,
        "step": 13701
    },
    {
        "loss": 1.1805,
        "grad_norm": 3.1019480228424072,
        "learning_rate": 5.51828522265889e-05,
        "epoch": 1.7661768497035317,
        "step": 13702
    },
    {
        "loss": 1.6657,
        "grad_norm": 3.1328485012054443,
        "learning_rate": 5.512847244300383e-05,
        "epoch": 1.7663057489043568,
        "step": 13703
    },
    {
        "loss": 1.3075,
        "grad_norm": 2.403503179550171,
        "learning_rate": 5.507410926882821e-05,
        "epoch": 1.7664346481051818,
        "step": 13704
    },
    {
        "loss": 0.9349,
        "grad_norm": 2.5760622024536133,
        "learning_rate": 5.5019762724184676e-05,
        "epoch": 1.7665635473060068,
        "step": 13705
    },
    {
        "loss": 0.9805,
        "grad_norm": 1.9479453563690186,
        "learning_rate": 5.496543282918991e-05,
        "epoch": 1.7666924465068317,
        "step": 13706
    },
    {
        "loss": 2.064,
        "grad_norm": 2.361161947250366,
        "learning_rate": 5.491111960395434e-05,
        "epoch": 1.7668213457076565,
        "step": 13707
    },
    {
        "loss": 1.5364,
        "grad_norm": 3.2554731369018555,
        "learning_rate": 5.485682306858231e-05,
        "epoch": 1.7669502449084815,
        "step": 13708
    },
    {
        "loss": 1.5181,
        "grad_norm": 2.7162976264953613,
        "learning_rate": 5.480254324317189e-05,
        "epoch": 1.7670791441093066,
        "step": 13709
    },
    {
        "loss": 1.9563,
        "grad_norm": 2.093841791152954,
        "learning_rate": 5.474828014781503e-05,
        "epoch": 1.7672080433101316,
        "step": 13710
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.7992949485778809,
        "learning_rate": 5.469403380259733e-05,
        "epoch": 1.7673369425109564,
        "step": 13711
    },
    {
        "loss": 1.9512,
        "grad_norm": 1.7961523532867432,
        "learning_rate": 5.463980422759864e-05,
        "epoch": 1.7674658417117812,
        "step": 13712
    },
    {
        "loss": 1.7382,
        "grad_norm": 1.9172719717025757,
        "learning_rate": 5.4585591442892125e-05,
        "epoch": 1.7675947409126063,
        "step": 13713
    },
    {
        "loss": 1.4562,
        "grad_norm": 2.8412554264068604,
        "learning_rate": 5.453139546854493e-05,
        "epoch": 1.7677236401134313,
        "step": 13714
    },
    {
        "loss": 1.9118,
        "grad_norm": 2.351752996444702,
        "learning_rate": 5.447721632461793e-05,
        "epoch": 1.7678525393142563,
        "step": 13715
    },
    {
        "loss": 0.9973,
        "grad_norm": 2.585231304168701,
        "learning_rate": 5.442305403116581e-05,
        "epoch": 1.7679814385150812,
        "step": 13716
    },
    {
        "loss": 1.2263,
        "grad_norm": 3.5177788734436035,
        "learning_rate": 5.4368908608236825e-05,
        "epoch": 1.7681103377159062,
        "step": 13717
    },
    {
        "loss": 1.1729,
        "grad_norm": 1.8923102617263794,
        "learning_rate": 5.4314780075873494e-05,
        "epoch": 1.768239236916731,
        "step": 13718
    },
    {
        "loss": 1.1734,
        "grad_norm": 2.9345011711120605,
        "learning_rate": 5.426066845411171e-05,
        "epoch": 1.768368136117556,
        "step": 13719
    },
    {
        "loss": 2.0918,
        "grad_norm": 2.6534738540649414,
        "learning_rate": 5.420657376298077e-05,
        "epoch": 1.768497035318381,
        "step": 13720
    },
    {
        "loss": 0.79,
        "grad_norm": 3.9480834007263184,
        "learning_rate": 5.415249602250445e-05,
        "epoch": 1.7686259345192061,
        "step": 13721
    },
    {
        "loss": 1.5547,
        "grad_norm": 2.5934994220733643,
        "learning_rate": 5.409843525269973e-05,
        "epoch": 1.768754833720031,
        "step": 13722
    },
    {
        "loss": 2.1812,
        "grad_norm": 2.0853471755981445,
        "learning_rate": 5.404439147357754e-05,
        "epoch": 1.7688837329208558,
        "step": 13723
    },
    {
        "loss": 0.6952,
        "grad_norm": 2.7776412963867188,
        "learning_rate": 5.399036470514239e-05,
        "epoch": 1.7690126321216808,
        "step": 13724
    },
    {
        "loss": 0.6785,
        "grad_norm": 2.240535020828247,
        "learning_rate": 5.3936354967392556e-05,
        "epoch": 1.7691415313225058,
        "step": 13725
    },
    {
        "loss": 1.3439,
        "grad_norm": 2.015397310256958,
        "learning_rate": 5.388236228032e-05,
        "epoch": 1.7692704305233309,
        "step": 13726
    },
    {
        "loss": 1.4994,
        "grad_norm": 2.1871540546417236,
        "learning_rate": 5.382838666391037e-05,
        "epoch": 1.7693993297241557,
        "step": 13727
    },
    {
        "loss": 1.7641,
        "grad_norm": 2.836167573928833,
        "learning_rate": 5.377442813814284e-05,
        "epoch": 1.7695282289249805,
        "step": 13728
    },
    {
        "loss": 1.5256,
        "grad_norm": 2.650320291519165,
        "learning_rate": 5.372048672299073e-05,
        "epoch": 1.7696571281258056,
        "step": 13729
    },
    {
        "loss": 1.5048,
        "grad_norm": 1.4815011024475098,
        "learning_rate": 5.366656243842054e-05,
        "epoch": 1.7697860273266306,
        "step": 13730
    },
    {
        "loss": 1.2586,
        "grad_norm": 2.6564271450042725,
        "learning_rate": 5.361265530439263e-05,
        "epoch": 1.7699149265274556,
        "step": 13731
    },
    {
        "loss": 1.2109,
        "grad_norm": 2.2036209106445312,
        "learning_rate": 5.355876534086095e-05,
        "epoch": 1.7700438257282805,
        "step": 13732
    },
    {
        "loss": 1.6133,
        "grad_norm": 2.6319785118103027,
        "learning_rate": 5.350489256777311e-05,
        "epoch": 1.7701727249291055,
        "step": 13733
    },
    {
        "loss": 1.7505,
        "grad_norm": 2.5904977321624756,
        "learning_rate": 5.345103700507033e-05,
        "epoch": 1.7703016241299303,
        "step": 13734
    },
    {
        "loss": 1.2797,
        "grad_norm": 2.7450103759765625,
        "learning_rate": 5.339719867268769e-05,
        "epoch": 1.7704305233307553,
        "step": 13735
    },
    {
        "loss": 1.3202,
        "grad_norm": 3.5937092304229736,
        "learning_rate": 5.334337759055359e-05,
        "epoch": 1.7705594225315804,
        "step": 13736
    },
    {
        "loss": 1.4636,
        "grad_norm": 2.409646511077881,
        "learning_rate": 5.328957377859016e-05,
        "epoch": 1.7706883217324054,
        "step": 13737
    },
    {
        "loss": 1.7336,
        "grad_norm": 2.7933733463287354,
        "learning_rate": 5.32357872567131e-05,
        "epoch": 1.7708172209332302,
        "step": 13738
    },
    {
        "loss": 1.5764,
        "grad_norm": 3.201744318008423,
        "learning_rate": 5.318201804483183e-05,
        "epoch": 1.770946120134055,
        "step": 13739
    },
    {
        "loss": 1.3827,
        "grad_norm": 2.1229145526885986,
        "learning_rate": 5.3128266162849224e-05,
        "epoch": 1.77107501933488,
        "step": 13740
    },
    {
        "loss": 2.1046,
        "grad_norm": 2.795975685119629,
        "learning_rate": 5.30745316306618e-05,
        "epoch": 1.7712039185357051,
        "step": 13741
    },
    {
        "loss": 1.7756,
        "grad_norm": 2.6060688495635986,
        "learning_rate": 5.3020814468159674e-05,
        "epoch": 1.7713328177365302,
        "step": 13742
    },
    {
        "loss": 0.3586,
        "grad_norm": 1.600756049156189,
        "learning_rate": 5.296711469522649e-05,
        "epoch": 1.771461716937355,
        "step": 13743
    },
    {
        "loss": 1.9266,
        "grad_norm": 1.1636161804199219,
        "learning_rate": 5.2913432331739446e-05,
        "epoch": 1.7715906161381798,
        "step": 13744
    },
    {
        "loss": 1.9987,
        "grad_norm": 2.576521873474121,
        "learning_rate": 5.285976739756926e-05,
        "epoch": 1.7717195153390048,
        "step": 13745
    },
    {
        "loss": 2.265,
        "grad_norm": 1.7176268100738525,
        "learning_rate": 5.2806119912580485e-05,
        "epoch": 1.7718484145398299,
        "step": 13746
    },
    {
        "loss": 1.5793,
        "grad_norm": 3.548884868621826,
        "learning_rate": 5.275248989663086e-05,
        "epoch": 1.771977313740655,
        "step": 13747
    },
    {
        "loss": 2.4111,
        "grad_norm": 1.6284241676330566,
        "learning_rate": 5.26988773695718e-05,
        "epoch": 1.7721062129414797,
        "step": 13748
    },
    {
        "loss": 0.5368,
        "grad_norm": 2.6948087215423584,
        "learning_rate": 5.264528235124824e-05,
        "epoch": 1.7722351121423046,
        "step": 13749
    },
    {
        "loss": 1.5655,
        "grad_norm": 2.364276170730591,
        "learning_rate": 5.259170486149858e-05,
        "epoch": 1.7723640113431296,
        "step": 13750
    },
    {
        "loss": 1.2869,
        "grad_norm": 2.5717477798461914,
        "learning_rate": 5.253814492015474e-05,
        "epoch": 1.7724929105439546,
        "step": 13751
    },
    {
        "loss": 0.9057,
        "grad_norm": 1.2624188661575317,
        "learning_rate": 5.248460254704239e-05,
        "epoch": 1.7726218097447797,
        "step": 13752
    },
    {
        "loss": 1.1609,
        "grad_norm": 3.0921390056610107,
        "learning_rate": 5.2431077761980394e-05,
        "epoch": 1.7727507089456045,
        "step": 13753
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.1097021102905273,
        "learning_rate": 5.2377570584781175e-05,
        "epoch": 1.7728796081464295,
        "step": 13754
    },
    {
        "loss": 1.2997,
        "grad_norm": 2.6233558654785156,
        "learning_rate": 5.232408103525065e-05,
        "epoch": 1.7730085073472543,
        "step": 13755
    },
    {
        "loss": 1.9531,
        "grad_norm": 2.149693489074707,
        "learning_rate": 5.2270609133188306e-05,
        "epoch": 1.7731374065480794,
        "step": 13756
    },
    {
        "loss": 1.3321,
        "grad_norm": 2.44578218460083,
        "learning_rate": 5.221715489838697e-05,
        "epoch": 1.7732663057489044,
        "step": 13757
    },
    {
        "loss": 0.6834,
        "grad_norm": 2.437026262283325,
        "learning_rate": 5.216371835063301e-05,
        "epoch": 1.7733952049497295,
        "step": 13758
    },
    {
        "loss": 1.749,
        "grad_norm": 4.350154876708984,
        "learning_rate": 5.211029950970605e-05,
        "epoch": 1.7735241041505543,
        "step": 13759
    },
    {
        "loss": 1.6665,
        "grad_norm": 1.673437237739563,
        "learning_rate": 5.205689839537974e-05,
        "epoch": 1.773653003351379,
        "step": 13760
    },
    {
        "loss": 1.6878,
        "grad_norm": 3.940114736557007,
        "learning_rate": 5.2003515027420325e-05,
        "epoch": 1.7737819025522041,
        "step": 13761
    },
    {
        "loss": 1.5342,
        "grad_norm": 2.5264289379119873,
        "learning_rate": 5.1950149425588e-05,
        "epoch": 1.7739108017530292,
        "step": 13762
    },
    {
        "loss": 0.5167,
        "grad_norm": 2.0701470375061035,
        "learning_rate": 5.1896801609636504e-05,
        "epoch": 1.7740397009538542,
        "step": 13763
    },
    {
        "loss": 1.4225,
        "grad_norm": 2.9062840938568115,
        "learning_rate": 5.1843471599312646e-05,
        "epoch": 1.774168600154679,
        "step": 13764
    },
    {
        "loss": 1.2353,
        "grad_norm": 4.796840667724609,
        "learning_rate": 5.179015941435681e-05,
        "epoch": 1.7742974993555038,
        "step": 13765
    },
    {
        "loss": 0.7962,
        "grad_norm": 4.292494773864746,
        "learning_rate": 5.173686507450268e-05,
        "epoch": 1.7744263985563289,
        "step": 13766
    },
    {
        "loss": 1.1273,
        "grad_norm": 2.3326785564422607,
        "learning_rate": 5.168358859947753e-05,
        "epoch": 1.774555297757154,
        "step": 13767
    },
    {
        "loss": 0.6721,
        "grad_norm": 2.2667651176452637,
        "learning_rate": 5.1630330009001705e-05,
        "epoch": 1.774684196957979,
        "step": 13768
    },
    {
        "loss": 1.393,
        "grad_norm": 3.069302797317505,
        "learning_rate": 5.15770893227894e-05,
        "epoch": 1.7748130961588038,
        "step": 13769
    },
    {
        "loss": 2.1815,
        "grad_norm": 1.4176909923553467,
        "learning_rate": 5.152386656054775e-05,
        "epoch": 1.7749419953596288,
        "step": 13770
    },
    {
        "loss": 1.2958,
        "grad_norm": 1.6600279808044434,
        "learning_rate": 5.147066174197739e-05,
        "epoch": 1.7750708945604536,
        "step": 13771
    },
    {
        "loss": 1.5002,
        "grad_norm": 3.2997260093688965,
        "learning_rate": 5.141747488677242e-05,
        "epoch": 1.7751997937612787,
        "step": 13772
    },
    {
        "loss": 1.6737,
        "grad_norm": 2.191585063934326,
        "learning_rate": 5.136430601462018e-05,
        "epoch": 1.7753286929621037,
        "step": 13773
    },
    {
        "loss": 1.1073,
        "grad_norm": 3.367229700088501,
        "learning_rate": 5.131115514520135e-05,
        "epoch": 1.7754575921629288,
        "step": 13774
    },
    {
        "loss": 1.6194,
        "grad_norm": 3.1434192657470703,
        "learning_rate": 5.125802229819001e-05,
        "epoch": 1.7755864913637536,
        "step": 13775
    },
    {
        "loss": 1.4315,
        "grad_norm": 3.929373025894165,
        "learning_rate": 5.1204907493253354e-05,
        "epoch": 1.7757153905645784,
        "step": 13776
    },
    {
        "loss": 1.3516,
        "grad_norm": 2.2849795818328857,
        "learning_rate": 5.115181075005252e-05,
        "epoch": 1.7758442897654034,
        "step": 13777
    },
    {
        "loss": 1.2363,
        "grad_norm": 2.960026502609253,
        "learning_rate": 5.109873208824113e-05,
        "epoch": 1.7759731889662285,
        "step": 13778
    },
    {
        "loss": 1.6082,
        "grad_norm": 1.4741660356521606,
        "learning_rate": 5.10456715274665e-05,
        "epoch": 1.7761020881670535,
        "step": 13779
    },
    {
        "loss": 2.199,
        "grad_norm": 2.681427001953125,
        "learning_rate": 5.099262908736948e-05,
        "epoch": 1.7762309873678783,
        "step": 13780
    },
    {
        "loss": 1.3532,
        "grad_norm": 4.745290756225586,
        "learning_rate": 5.0939604787583886e-05,
        "epoch": 1.7763598865687031,
        "step": 13781
    },
    {
        "loss": 1.7191,
        "grad_norm": 2.5934672355651855,
        "learning_rate": 5.0886598647736885e-05,
        "epoch": 1.7764887857695282,
        "step": 13782
    },
    {
        "loss": 0.7744,
        "grad_norm": 1.9410749673843384,
        "learning_rate": 5.083361068744892e-05,
        "epoch": 1.7766176849703532,
        "step": 13783
    },
    {
        "loss": 1.4081,
        "grad_norm": 2.3569884300231934,
        "learning_rate": 5.0780640926333834e-05,
        "epoch": 1.7767465841711783,
        "step": 13784
    },
    {
        "loss": 1.0593,
        "grad_norm": 2.913571357727051,
        "learning_rate": 5.072768938399846e-05,
        "epoch": 1.776875483372003,
        "step": 13785
    },
    {
        "loss": 1.7769,
        "grad_norm": 2.1831259727478027,
        "learning_rate": 5.0674756080043297e-05,
        "epoch": 1.777004382572828,
        "step": 13786
    },
    {
        "loss": 1.3327,
        "grad_norm": 2.154956579208374,
        "learning_rate": 5.062184103406176e-05,
        "epoch": 1.777133281773653,
        "step": 13787
    },
    {
        "loss": 1.9314,
        "grad_norm": 2.3998448848724365,
        "learning_rate": 5.056894426564054e-05,
        "epoch": 1.777262180974478,
        "step": 13788
    },
    {
        "loss": 1.59,
        "grad_norm": 1.4777964353561401,
        "learning_rate": 5.051606579435972e-05,
        "epoch": 1.777391080175303,
        "step": 13789
    },
    {
        "loss": 2.029,
        "grad_norm": 1.6936622858047485,
        "learning_rate": 5.04632056397925e-05,
        "epoch": 1.7775199793761278,
        "step": 13790
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.5324126482009888,
        "learning_rate": 5.041036382150528e-05,
        "epoch": 1.7776488785769529,
        "step": 13791
    },
    {
        "loss": 2.0435,
        "grad_norm": 2.5269367694854736,
        "learning_rate": 5.035754035905773e-05,
        "epoch": 1.7777777777777777,
        "step": 13792
    },
    {
        "loss": 2.045,
        "grad_norm": 1.8168636560440063,
        "learning_rate": 5.030473527200254e-05,
        "epoch": 1.7779066769786027,
        "step": 13793
    },
    {
        "loss": 1.4745,
        "grad_norm": 2.455759048461914,
        "learning_rate": 5.0251948579886207e-05,
        "epoch": 1.7780355761794278,
        "step": 13794
    },
    {
        "loss": 1.6315,
        "grad_norm": 2.705995559692383,
        "learning_rate": 5.019918030224757e-05,
        "epoch": 1.7781644753802528,
        "step": 13795
    },
    {
        "loss": 1.1525,
        "grad_norm": 4.471742630004883,
        "learning_rate": 5.0146430458619085e-05,
        "epoch": 1.7782933745810776,
        "step": 13796
    },
    {
        "loss": 1.6522,
        "grad_norm": 2.4611830711364746,
        "learning_rate": 5.009369906852655e-05,
        "epoch": 1.7784222737819024,
        "step": 13797
    },
    {
        "loss": 1.2725,
        "grad_norm": 2.6780643463134766,
        "learning_rate": 5.0040986151488664e-05,
        "epoch": 1.7785511729827275,
        "step": 13798
    },
    {
        "loss": 0.3401,
        "grad_norm": 3.5191187858581543,
        "learning_rate": 4.998829172701718e-05,
        "epoch": 1.7786800721835525,
        "step": 13799
    },
    {
        "loss": 1.4765,
        "grad_norm": 6.379824161529541,
        "learning_rate": 4.993561581461763e-05,
        "epoch": 1.7788089713843775,
        "step": 13800
    },
    {
        "loss": 2.0085,
        "grad_norm": 1.4654266834259033,
        "learning_rate": 4.988295843378786e-05,
        "epoch": 1.7789378705852024,
        "step": 13801
    },
    {
        "loss": 1.8257,
        "grad_norm": 2.858088493347168,
        "learning_rate": 4.983031960401926e-05,
        "epoch": 1.7790667697860272,
        "step": 13802
    },
    {
        "loss": 1.7689,
        "grad_norm": 2.3037259578704834,
        "learning_rate": 4.977769934479656e-05,
        "epoch": 1.7791956689868522,
        "step": 13803
    },
    {
        "loss": 0.9859,
        "grad_norm": 3.2266335487365723,
        "learning_rate": 4.9725097675597254e-05,
        "epoch": 1.7793245681876773,
        "step": 13804
    },
    {
        "loss": 1.4051,
        "grad_norm": 2.7046611309051514,
        "learning_rate": 4.9672514615892185e-05,
        "epoch": 1.7794534673885023,
        "step": 13805
    },
    {
        "loss": 2.1521,
        "grad_norm": 1.90495765209198,
        "learning_rate": 4.961995018514516e-05,
        "epoch": 1.7795823665893271,
        "step": 13806
    },
    {
        "loss": 1.7225,
        "grad_norm": 1.4924955368041992,
        "learning_rate": 4.956740440281318e-05,
        "epoch": 1.7797112657901522,
        "step": 13807
    },
    {
        "loss": 1.1983,
        "grad_norm": 3.7395853996276855,
        "learning_rate": 4.95148772883463e-05,
        "epoch": 1.779840164990977,
        "step": 13808
    },
    {
        "loss": 1.4607,
        "grad_norm": 2.4860715866088867,
        "learning_rate": 4.946236886118768e-05,
        "epoch": 1.779969064191802,
        "step": 13809
    },
    {
        "loss": 1.2139,
        "grad_norm": 2.6952812671661377,
        "learning_rate": 4.940987914077342e-05,
        "epoch": 1.780097963392627,
        "step": 13810
    },
    {
        "loss": 1.1874,
        "grad_norm": 2.8422160148620605,
        "learning_rate": 4.9357408146533255e-05,
        "epoch": 1.780226862593452,
        "step": 13811
    },
    {
        "loss": 1.2426,
        "grad_norm": 2.4282848834991455,
        "learning_rate": 4.9304955897889205e-05,
        "epoch": 1.780355761794277,
        "step": 13812
    },
    {
        "loss": 1.1936,
        "grad_norm": 3.2141263484954834,
        "learning_rate": 4.9252522414256686e-05,
        "epoch": 1.7804846609951017,
        "step": 13813
    },
    {
        "loss": 0.2815,
        "grad_norm": 1.4953504800796509,
        "learning_rate": 4.920010771504446e-05,
        "epoch": 1.7806135601959268,
        "step": 13814
    },
    {
        "loss": 1.435,
        "grad_norm": 2.1269564628601074,
        "learning_rate": 4.914771181965396e-05,
        "epoch": 1.7807424593967518,
        "step": 13815
    },
    {
        "loss": 1.0226,
        "grad_norm": 2.89394211769104,
        "learning_rate": 4.909533474747962e-05,
        "epoch": 1.7808713585975768,
        "step": 13816
    },
    {
        "loss": 0.7539,
        "grad_norm": 2.8049845695495605,
        "learning_rate": 4.9042976517909465e-05,
        "epoch": 1.7810002577984017,
        "step": 13817
    },
    {
        "loss": 0.404,
        "grad_norm": 1.6824439764022827,
        "learning_rate": 4.8990637150323784e-05,
        "epoch": 1.7811291569992265,
        "step": 13818
    },
    {
        "loss": 1.9384,
        "grad_norm": 1.4687379598617554,
        "learning_rate": 4.893831666409623e-05,
        "epoch": 1.7812580562000515,
        "step": 13819
    },
    {
        "loss": 2.0051,
        "grad_norm": 2.5106518268585205,
        "learning_rate": 4.8886015078593714e-05,
        "epoch": 1.7813869554008765,
        "step": 13820
    },
    {
        "loss": 1.7994,
        "grad_norm": 2.017960548400879,
        "learning_rate": 4.883373241317577e-05,
        "epoch": 1.7815158546017016,
        "step": 13821
    },
    {
        "loss": 0.9599,
        "grad_norm": 3.511000156402588,
        "learning_rate": 4.878146868719513e-05,
        "epoch": 1.7816447538025264,
        "step": 13822
    },
    {
        "loss": 2.0861,
        "grad_norm": 2.6570143699645996,
        "learning_rate": 4.872922391999745e-05,
        "epoch": 1.7817736530033512,
        "step": 13823
    },
    {
        "loss": 1.7616,
        "grad_norm": 1.855509638786316,
        "learning_rate": 4.867699813092136e-05,
        "epoch": 1.7819025522041763,
        "step": 13824
    },
    {
        "loss": 0.9261,
        "grad_norm": 2.247885227203369,
        "learning_rate": 4.862479133929849e-05,
        "epoch": 1.7820314514050013,
        "step": 13825
    },
    {
        "loss": 1.932,
        "grad_norm": 2.5524582862854004,
        "learning_rate": 4.857260356445344e-05,
        "epoch": 1.7821603506058263,
        "step": 13826
    },
    {
        "loss": 0.6481,
        "grad_norm": 3.56494140625,
        "learning_rate": 4.85204348257036e-05,
        "epoch": 1.7822892498066512,
        "step": 13827
    },
    {
        "loss": 1.7816,
        "grad_norm": 2.050699472427368,
        "learning_rate": 4.846828514235989e-05,
        "epoch": 1.7824181490074762,
        "step": 13828
    },
    {
        "loss": 1.7496,
        "grad_norm": 2.443129539489746,
        "learning_rate": 4.84161545337254e-05,
        "epoch": 1.782547048208301,
        "step": 13829
    },
    {
        "loss": 1.3991,
        "grad_norm": 1.4892714023590088,
        "learning_rate": 4.8364043019096504e-05,
        "epoch": 1.782675947409126,
        "step": 13830
    },
    {
        "loss": 1.9632,
        "grad_norm": 2.2034614086151123,
        "learning_rate": 4.831195061776278e-05,
        "epoch": 1.782804846609951,
        "step": 13831
    },
    {
        "loss": 1.9345,
        "grad_norm": 2.1987271308898926,
        "learning_rate": 4.82598773490063e-05,
        "epoch": 1.7829337458107761,
        "step": 13832
    },
    {
        "loss": 1.8612,
        "grad_norm": 2.916747808456421,
        "learning_rate": 4.820782323210221e-05,
        "epoch": 1.783062645011601,
        "step": 13833
    },
    {
        "loss": 0.4642,
        "grad_norm": 2.836315631866455,
        "learning_rate": 4.815578828631888e-05,
        "epoch": 1.7831915442124258,
        "step": 13834
    },
    {
        "loss": 1.5281,
        "grad_norm": 2.416123151779175,
        "learning_rate": 4.810377253091697e-05,
        "epoch": 1.7833204434132508,
        "step": 13835
    },
    {
        "loss": 1.4354,
        "grad_norm": 3.0874574184417725,
        "learning_rate": 4.805177598515035e-05,
        "epoch": 1.7834493426140758,
        "step": 13836
    },
    {
        "loss": 1.5575,
        "grad_norm": 2.766958713531494,
        "learning_rate": 4.799979866826596e-05,
        "epoch": 1.7835782418149009,
        "step": 13837
    },
    {
        "loss": 0.8007,
        "grad_norm": 1.8251805305480957,
        "learning_rate": 4.794784059950344e-05,
        "epoch": 1.7837071410157257,
        "step": 13838
    },
    {
        "loss": 2.1619,
        "grad_norm": 3.0707504749298096,
        "learning_rate": 4.789590179809528e-05,
        "epoch": 1.7838360402165505,
        "step": 13839
    },
    {
        "loss": 1.465,
        "grad_norm": 4.036446571350098,
        "learning_rate": 4.784398228326684e-05,
        "epoch": 1.7839649394173756,
        "step": 13840
    },
    {
        "loss": 2.3127,
        "grad_norm": 1.6652799844741821,
        "learning_rate": 4.779208207423641e-05,
        "epoch": 1.7840938386182006,
        "step": 13841
    },
    {
        "loss": 1.9115,
        "grad_norm": 2.0244529247283936,
        "learning_rate": 4.7740201190215096e-05,
        "epoch": 1.7842227378190256,
        "step": 13842
    },
    {
        "loss": 1.7094,
        "grad_norm": 2.8280885219573975,
        "learning_rate": 4.768833965040679e-05,
        "epoch": 1.7843516370198504,
        "step": 13843
    },
    {
        "loss": 2.0149,
        "grad_norm": 1.7305134534835815,
        "learning_rate": 4.763649747400828e-05,
        "epoch": 1.7844805362206755,
        "step": 13844
    },
    {
        "loss": 1.895,
        "grad_norm": 2.542762517929077,
        "learning_rate": 4.758467468020947e-05,
        "epoch": 1.7846094354215003,
        "step": 13845
    },
    {
        "loss": 1.8277,
        "grad_norm": 2.49841046333313,
        "learning_rate": 4.753287128819249e-05,
        "epoch": 1.7847383346223253,
        "step": 13846
    },
    {
        "loss": 1.4797,
        "grad_norm": 2.6330432891845703,
        "learning_rate": 4.7481087317132625e-05,
        "epoch": 1.7848672338231504,
        "step": 13847
    },
    {
        "loss": 2.2018,
        "grad_norm": 1.9448291063308716,
        "learning_rate": 4.742932278619815e-05,
        "epoch": 1.7849961330239754,
        "step": 13848
    },
    {
        "loss": 2.2357,
        "grad_norm": 2.5951530933380127,
        "learning_rate": 4.737757771454983e-05,
        "epoch": 1.7851250322248002,
        "step": 13849
    },
    {
        "loss": 1.6127,
        "grad_norm": 2.9889750480651855,
        "learning_rate": 4.7325852121341294e-05,
        "epoch": 1.785253931425625,
        "step": 13850
    },
    {
        "loss": 1.5564,
        "grad_norm": 2.8016979694366455,
        "learning_rate": 4.727414602571934e-05,
        "epoch": 1.78538283062645,
        "step": 13851
    },
    {
        "loss": 1.7805,
        "grad_norm": 2.763152837753296,
        "learning_rate": 4.7222459446822855e-05,
        "epoch": 1.7855117298272751,
        "step": 13852
    },
    {
        "loss": 1.5489,
        "grad_norm": 1.5978689193725586,
        "learning_rate": 4.717079240378389e-05,
        "epoch": 1.7856406290281002,
        "step": 13853
    },
    {
        "loss": 1.7488,
        "grad_norm": 1.6597685813903809,
        "learning_rate": 4.711914491572744e-05,
        "epoch": 1.785769528228925,
        "step": 13854
    },
    {
        "loss": 1.3,
        "grad_norm": 3.421170949935913,
        "learning_rate": 4.706751700177102e-05,
        "epoch": 1.7858984274297498,
        "step": 13855
    },
    {
        "loss": 1.8248,
        "grad_norm": 2.200525999069214,
        "learning_rate": 4.7015908681024925e-05,
        "epoch": 1.7860273266305748,
        "step": 13856
    },
    {
        "loss": 1.9831,
        "grad_norm": 2.687534809112549,
        "learning_rate": 4.69643199725922e-05,
        "epoch": 1.7861562258313999,
        "step": 13857
    },
    {
        "loss": 1.9391,
        "grad_norm": 2.460562229156494,
        "learning_rate": 4.691275089556866e-05,
        "epoch": 1.786285125032225,
        "step": 13858
    },
    {
        "loss": 0.957,
        "grad_norm": 2.6167428493499756,
        "learning_rate": 4.686120146904285e-05,
        "epoch": 1.7864140242330497,
        "step": 13859
    },
    {
        "loss": 1.0948,
        "grad_norm": 3.102170467376709,
        "learning_rate": 4.6809671712095985e-05,
        "epoch": 1.7865429234338746,
        "step": 13860
    },
    {
        "loss": 1.6757,
        "grad_norm": 1.5027503967285156,
        "learning_rate": 4.6758161643802024e-05,
        "epoch": 1.7866718226346996,
        "step": 13861
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.6457208395004272,
        "learning_rate": 4.670667128322784e-05,
        "epoch": 1.7868007218355246,
        "step": 13862
    },
    {
        "loss": 1.1282,
        "grad_norm": 2.259906768798828,
        "learning_rate": 4.665520064943286e-05,
        "epoch": 1.7869296210363497,
        "step": 13863
    },
    {
        "loss": 1.5181,
        "grad_norm": 2.9600014686584473,
        "learning_rate": 4.660374976146882e-05,
        "epoch": 1.7870585202371745,
        "step": 13864
    },
    {
        "loss": 0.4418,
        "grad_norm": 2.6695423126220703,
        "learning_rate": 4.655231863838081e-05,
        "epoch": 1.7871874194379995,
        "step": 13865
    },
    {
        "loss": 1.1393,
        "grad_norm": 2.858534336090088,
        "learning_rate": 4.650090729920628e-05,
        "epoch": 1.7873163186388243,
        "step": 13866
    },
    {
        "loss": 2.5784,
        "grad_norm": 1.826276421546936,
        "learning_rate": 4.6449515762975225e-05,
        "epoch": 1.7874452178396494,
        "step": 13867
    },
    {
        "loss": 1.6656,
        "grad_norm": 1.8510643243789673,
        "learning_rate": 4.639814404871081e-05,
        "epoch": 1.7875741170404744,
        "step": 13868
    },
    {
        "loss": 1.6096,
        "grad_norm": 1.765659213066101,
        "learning_rate": 4.634679217542819e-05,
        "epoch": 1.7877030162412995,
        "step": 13869
    },
    {
        "loss": 1.7129,
        "grad_norm": 2.365983247756958,
        "learning_rate": 4.629546016213544e-05,
        "epoch": 1.7878319154421243,
        "step": 13870
    },
    {
        "loss": 1.0551,
        "grad_norm": 3.5494673252105713,
        "learning_rate": 4.624414802783368e-05,
        "epoch": 1.787960814642949,
        "step": 13871
    },
    {
        "loss": 1.1629,
        "grad_norm": 3.077777624130249,
        "learning_rate": 4.6192855791516165e-05,
        "epoch": 1.7880897138437741,
        "step": 13872
    },
    {
        "loss": 1.5705,
        "grad_norm": 2.394357681274414,
        "learning_rate": 4.614158347216897e-05,
        "epoch": 1.7882186130445992,
        "step": 13873
    },
    {
        "loss": 1.6388,
        "grad_norm": 1.9718185663223267,
        "learning_rate": 4.60903310887708e-05,
        "epoch": 1.7883475122454242,
        "step": 13874
    },
    {
        "loss": 1.6219,
        "grad_norm": 2.629960060119629,
        "learning_rate": 4.6039098660292946e-05,
        "epoch": 1.788476411446249,
        "step": 13875
    },
    {
        "loss": 1.2316,
        "grad_norm": 2.8508119583129883,
        "learning_rate": 4.598788620569932e-05,
        "epoch": 1.7886053106470738,
        "step": 13876
    },
    {
        "loss": 1.2925,
        "grad_norm": 2.0723283290863037,
        "learning_rate": 4.59366937439465e-05,
        "epoch": 1.7887342098478989,
        "step": 13877
    },
    {
        "loss": 0.2448,
        "grad_norm": 0.9478484392166138,
        "learning_rate": 4.58855212939835e-05,
        "epoch": 1.788863109048724,
        "step": 13878
    },
    {
        "loss": 1.5756,
        "grad_norm": 2.3532114028930664,
        "learning_rate": 4.5834368874752265e-05,
        "epoch": 1.788992008249549,
        "step": 13879
    },
    {
        "loss": 2.0215,
        "grad_norm": 1.9958381652832031,
        "learning_rate": 4.5783236505187096e-05,
        "epoch": 1.7891209074503738,
        "step": 13880
    },
    {
        "loss": 1.8894,
        "grad_norm": 3.8847389221191406,
        "learning_rate": 4.5732124204214556e-05,
        "epoch": 1.7892498066511988,
        "step": 13881
    },
    {
        "loss": 1.121,
        "grad_norm": 2.9223101139068604,
        "learning_rate": 4.5681031990754384e-05,
        "epoch": 1.7893787058520236,
        "step": 13882
    },
    {
        "loss": 1.6403,
        "grad_norm": 2.5787057876586914,
        "learning_rate": 4.56299598837186e-05,
        "epoch": 1.7895076050528487,
        "step": 13883
    },
    {
        "loss": 1.5364,
        "grad_norm": 3.952188491821289,
        "learning_rate": 4.557890790201162e-05,
        "epoch": 1.7896365042536737,
        "step": 13884
    },
    {
        "loss": 1.5642,
        "grad_norm": 2.8955652713775635,
        "learning_rate": 4.5527876064530916e-05,
        "epoch": 1.7897654034544985,
        "step": 13885
    },
    {
        "loss": 1.9865,
        "grad_norm": 2.367588758468628,
        "learning_rate": 4.547686439016582e-05,
        "epoch": 1.7898943026553236,
        "step": 13886
    },
    {
        "loss": 1.4761,
        "grad_norm": 3.032188892364502,
        "learning_rate": 4.542587289779853e-05,
        "epoch": 1.7900232018561484,
        "step": 13887
    },
    {
        "loss": 1.4444,
        "grad_norm": 2.518807888031006,
        "learning_rate": 4.5374901606304064e-05,
        "epoch": 1.7901521010569734,
        "step": 13888
    },
    {
        "loss": 1.6452,
        "grad_norm": 2.4838855266571045,
        "learning_rate": 4.532395053454956e-05,
        "epoch": 1.7902810002577985,
        "step": 13889
    },
    {
        "loss": 1.1334,
        "grad_norm": 1.6106717586517334,
        "learning_rate": 4.527301970139479e-05,
        "epoch": 1.7904098994586235,
        "step": 13890
    },
    {
        "loss": 2.1253,
        "grad_norm": 1.4726117849349976,
        "learning_rate": 4.522210912569204e-05,
        "epoch": 1.7905387986594483,
        "step": 13891
    },
    {
        "loss": 1.9238,
        "grad_norm": 2.8734309673309326,
        "learning_rate": 4.517121882628611e-05,
        "epoch": 1.7906676978602731,
        "step": 13892
    },
    {
        "loss": 2.0365,
        "grad_norm": 1.9554674625396729,
        "learning_rate": 4.512034882201424e-05,
        "epoch": 1.7907965970610982,
        "step": 13893
    },
    {
        "loss": 1.1416,
        "grad_norm": 2.24855637550354,
        "learning_rate": 4.506949913170628e-05,
        "epoch": 1.7909254962619232,
        "step": 13894
    },
    {
        "loss": 2.1494,
        "grad_norm": 1.7721409797668457,
        "learning_rate": 4.501866977418435e-05,
        "epoch": 1.7910543954627482,
        "step": 13895
    },
    {
        "loss": 1.2744,
        "grad_norm": 2.528926372528076,
        "learning_rate": 4.4967860768263394e-05,
        "epoch": 1.791183294663573,
        "step": 13896
    },
    {
        "loss": 1.8299,
        "grad_norm": 2.205650568008423,
        "learning_rate": 4.4917072132750596e-05,
        "epoch": 1.7913121938643979,
        "step": 13897
    },
    {
        "loss": 1.5214,
        "grad_norm": 2.1281960010528564,
        "learning_rate": 4.486630388644524e-05,
        "epoch": 1.791441093065223,
        "step": 13898
    },
    {
        "loss": 1.4973,
        "grad_norm": 3.547213315963745,
        "learning_rate": 4.481555604813986e-05,
        "epoch": 1.791569992266048,
        "step": 13899
    },
    {
        "loss": 2.0197,
        "grad_norm": 3.5345773696899414,
        "learning_rate": 4.476482863661885e-05,
        "epoch": 1.791698891466873,
        "step": 13900
    },
    {
        "loss": 1.2871,
        "grad_norm": 2.358865737915039,
        "learning_rate": 4.4714121670659095e-05,
        "epoch": 1.7918277906676978,
        "step": 13901
    },
    {
        "loss": 1.1016,
        "grad_norm": 2.637498617172241,
        "learning_rate": 4.4663435169030235e-05,
        "epoch": 1.7919566898685229,
        "step": 13902
    },
    {
        "loss": 1.3656,
        "grad_norm": 2.4155561923980713,
        "learning_rate": 4.461276915049413e-05,
        "epoch": 1.7920855890693477,
        "step": 13903
    },
    {
        "loss": 1.0921,
        "grad_norm": 1.5578956604003906,
        "learning_rate": 4.456212363380466e-05,
        "epoch": 1.7922144882701727,
        "step": 13904
    },
    {
        "loss": 1.6856,
        "grad_norm": 3.3528151512145996,
        "learning_rate": 4.4511498637708924e-05,
        "epoch": 1.7923433874709978,
        "step": 13905
    },
    {
        "loss": 1.2637,
        "grad_norm": 1.7170239686965942,
        "learning_rate": 4.4460894180945865e-05,
        "epoch": 1.7924722866718228,
        "step": 13906
    },
    {
        "loss": 2.6108,
        "grad_norm": 3.2119834423065186,
        "learning_rate": 4.441031028224696e-05,
        "epoch": 1.7926011858726476,
        "step": 13907
    },
    {
        "loss": 1.8486,
        "grad_norm": 2.3745920658111572,
        "learning_rate": 4.435974696033606e-05,
        "epoch": 1.7927300850734724,
        "step": 13908
    },
    {
        "loss": 1.7557,
        "grad_norm": 2.7610414028167725,
        "learning_rate": 4.430920423392939e-05,
        "epoch": 1.7928589842742975,
        "step": 13909
    },
    {
        "loss": 1.9587,
        "grad_norm": 2.2609269618988037,
        "learning_rate": 4.425868212173567e-05,
        "epoch": 1.7929878834751225,
        "step": 13910
    },
    {
        "loss": 1.4976,
        "grad_norm": 2.43565034866333,
        "learning_rate": 4.4208180642455856e-05,
        "epoch": 1.7931167826759475,
        "step": 13911
    },
    {
        "loss": 1.627,
        "grad_norm": 1.5846762657165527,
        "learning_rate": 4.4157699814783194e-05,
        "epoch": 1.7932456818767724,
        "step": 13912
    },
    {
        "loss": 1.8656,
        "grad_norm": 2.383479595184326,
        "learning_rate": 4.410723965740363e-05,
        "epoch": 1.7933745810775972,
        "step": 13913
    },
    {
        "loss": 1.3402,
        "grad_norm": 2.066345453262329,
        "learning_rate": 4.405680018899523e-05,
        "epoch": 1.7935034802784222,
        "step": 13914
    },
    {
        "loss": 1.5029,
        "grad_norm": 2.0762157440185547,
        "learning_rate": 4.4006381428228024e-05,
        "epoch": 1.7936323794792473,
        "step": 13915
    },
    {
        "loss": 2.3279,
        "grad_norm": 2.563629150390625,
        "learning_rate": 4.395598339376518e-05,
        "epoch": 1.7937612786800723,
        "step": 13916
    },
    {
        "loss": 0.9382,
        "grad_norm": 3.8475611209869385,
        "learning_rate": 4.3905606104261595e-05,
        "epoch": 1.793890177880897,
        "step": 13917
    },
    {
        "loss": 2.3326,
        "grad_norm": 1.2668039798736572,
        "learning_rate": 4.3855249578364587e-05,
        "epoch": 1.7940190770817221,
        "step": 13918
    },
    {
        "loss": 1.3844,
        "grad_norm": 1.4623385667800903,
        "learning_rate": 4.3804913834714066e-05,
        "epoch": 1.794147976282547,
        "step": 13919
    },
    {
        "loss": 1.1042,
        "grad_norm": 2.495919942855835,
        "learning_rate": 4.375459889194204e-05,
        "epoch": 1.794276875483372,
        "step": 13920
    },
    {
        "loss": 1.8034,
        "grad_norm": 1.9179420471191406,
        "learning_rate": 4.370430476867248e-05,
        "epoch": 1.794405774684197,
        "step": 13921
    },
    {
        "loss": 1.1479,
        "grad_norm": 2.8671562671661377,
        "learning_rate": 4.3654031483522365e-05,
        "epoch": 1.7945346738850219,
        "step": 13922
    },
    {
        "loss": 1.7055,
        "grad_norm": 1.6405627727508545,
        "learning_rate": 4.360377905510047e-05,
        "epoch": 1.794663573085847,
        "step": 13923
    },
    {
        "loss": 1.4216,
        "grad_norm": 3.093899726867676,
        "learning_rate": 4.3553547502007965e-05,
        "epoch": 1.7947924722866717,
        "step": 13924
    },
    {
        "loss": 1.7982,
        "grad_norm": 2.7403435707092285,
        "learning_rate": 4.35033368428383e-05,
        "epoch": 1.7949213714874968,
        "step": 13925
    },
    {
        "loss": 0.9529,
        "grad_norm": 3.182788848876953,
        "learning_rate": 4.3453147096177136e-05,
        "epoch": 1.7950502706883218,
        "step": 13926
    },
    {
        "loss": 1.4664,
        "grad_norm": 2.2703444957733154,
        "learning_rate": 4.3402978280602545e-05,
        "epoch": 1.7951791698891468,
        "step": 13927
    },
    {
        "loss": 1.4243,
        "grad_norm": 1.6820900440216064,
        "learning_rate": 4.3352830414684696e-05,
        "epoch": 1.7953080690899716,
        "step": 13928
    },
    {
        "loss": 1.3601,
        "grad_norm": 3.666049003601074,
        "learning_rate": 4.3302703516985966e-05,
        "epoch": 1.7954369682907965,
        "step": 13929
    },
    {
        "loss": 1.7337,
        "grad_norm": 1.7801107168197632,
        "learning_rate": 4.3252597606061274e-05,
        "epoch": 1.7955658674916215,
        "step": 13930
    },
    {
        "loss": 1.631,
        "grad_norm": 3.254488468170166,
        "learning_rate": 4.320251270045755e-05,
        "epoch": 1.7956947666924465,
        "step": 13931
    },
    {
        "loss": 1.4411,
        "grad_norm": 2.1619489192962646,
        "learning_rate": 4.315244881871362e-05,
        "epoch": 1.7958236658932716,
        "step": 13932
    },
    {
        "loss": 2.6921,
        "grad_norm": 1.5726966857910156,
        "learning_rate": 4.310240597936126e-05,
        "epoch": 1.7959525650940964,
        "step": 13933
    },
    {
        "loss": 1.7819,
        "grad_norm": 2.035316228866577,
        "learning_rate": 4.305238420092391e-05,
        "epoch": 1.7960814642949212,
        "step": 13934
    },
    {
        "loss": 0.3171,
        "grad_norm": 1.136781096458435,
        "learning_rate": 4.300238350191727e-05,
        "epoch": 1.7962103634957463,
        "step": 13935
    },
    {
        "loss": 1.1666,
        "grad_norm": 2.669825553894043,
        "learning_rate": 4.295240390084951e-05,
        "epoch": 1.7963392626965713,
        "step": 13936
    },
    {
        "loss": 2.161,
        "grad_norm": 2.8712263107299805,
        "learning_rate": 4.2902445416220905e-05,
        "epoch": 1.7964681618973963,
        "step": 13937
    },
    {
        "loss": 1.5596,
        "grad_norm": 3.8204619884490967,
        "learning_rate": 4.285250806652343e-05,
        "epoch": 1.7965970610982211,
        "step": 13938
    },
    {
        "loss": 0.9454,
        "grad_norm": 3.2941136360168457,
        "learning_rate": 4.280259187024201e-05,
        "epoch": 1.7967259602990462,
        "step": 13939
    },
    {
        "loss": 1.5325,
        "grad_norm": 1.6841164827346802,
        "learning_rate": 4.2752696845853204e-05,
        "epoch": 1.796854859499871,
        "step": 13940
    },
    {
        "loss": 0.9571,
        "grad_norm": 3.863847494125366,
        "learning_rate": 4.270282301182592e-05,
        "epoch": 1.796983758700696,
        "step": 13941
    },
    {
        "loss": 1.493,
        "grad_norm": 2.438197374343872,
        "learning_rate": 4.265297038662115e-05,
        "epoch": 1.797112657901521,
        "step": 13942
    },
    {
        "loss": 2.187,
        "grad_norm": 3.0696918964385986,
        "learning_rate": 4.2603138988692156e-05,
        "epoch": 1.7972415571023461,
        "step": 13943
    },
    {
        "loss": 1.5538,
        "grad_norm": 4.045281410217285,
        "learning_rate": 4.255332883648423e-05,
        "epoch": 1.797370456303171,
        "step": 13944
    },
    {
        "loss": 1.3513,
        "grad_norm": 2.058544635772705,
        "learning_rate": 4.2503539948434836e-05,
        "epoch": 1.7974993555039958,
        "step": 13945
    },
    {
        "loss": 1.2673,
        "grad_norm": 2.294724702835083,
        "learning_rate": 4.245377234297345e-05,
        "epoch": 1.7976282547048208,
        "step": 13946
    },
    {
        "loss": 1.9699,
        "grad_norm": 3.5879862308502197,
        "learning_rate": 4.240402603852203e-05,
        "epoch": 1.7977571539056458,
        "step": 13947
    },
    {
        "loss": 1.7196,
        "grad_norm": 1.6428096294403076,
        "learning_rate": 4.235430105349434e-05,
        "epoch": 1.7978860531064709,
        "step": 13948
    },
    {
        "loss": 1.5451,
        "grad_norm": 4.282599925994873,
        "learning_rate": 4.2304597406296084e-05,
        "epoch": 1.7980149523072957,
        "step": 13949
    },
    {
        "loss": 1.8613,
        "grad_norm": 2.2990121841430664,
        "learning_rate": 4.22549151153256e-05,
        "epoch": 1.7981438515081205,
        "step": 13950
    },
    {
        "loss": 1.2615,
        "grad_norm": 2.3617289066314697,
        "learning_rate": 4.22052541989729e-05,
        "epoch": 1.7982727507089455,
        "step": 13951
    },
    {
        "loss": 0.7023,
        "grad_norm": 4.0424346923828125,
        "learning_rate": 4.2155614675620115e-05,
        "epoch": 1.7984016499097706,
        "step": 13952
    },
    {
        "loss": 2.0947,
        "grad_norm": 2.096426486968994,
        "learning_rate": 4.210599656364171e-05,
        "epoch": 1.7985305491105956,
        "step": 13953
    },
    {
        "loss": 1.2787,
        "grad_norm": 3.1945013999938965,
        "learning_rate": 4.205639988140415e-05,
        "epoch": 1.7986594483114204,
        "step": 13954
    },
    {
        "loss": 1.3532,
        "grad_norm": 2.819394588470459,
        "learning_rate": 4.200682464726553e-05,
        "epoch": 1.7987883475122455,
        "step": 13955
    },
    {
        "loss": 1.5392,
        "grad_norm": 1.9056205749511719,
        "learning_rate": 4.195727087957667e-05,
        "epoch": 1.7989172467130703,
        "step": 13956
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.4765074253082275,
        "learning_rate": 4.1907738596680036e-05,
        "epoch": 1.7990461459138953,
        "step": 13957
    },
    {
        "loss": 1.805,
        "grad_norm": 2.356796979904175,
        "learning_rate": 4.1858227816910225e-05,
        "epoch": 1.7991750451147204,
        "step": 13958
    },
    {
        "loss": 1.9494,
        "grad_norm": 3.6506500244140625,
        "learning_rate": 4.180873855859385e-05,
        "epoch": 1.7993039443155452,
        "step": 13959
    },
    {
        "loss": 1.6418,
        "grad_norm": 1.5469450950622559,
        "learning_rate": 4.175927084004969e-05,
        "epoch": 1.7994328435163702,
        "step": 13960
    },
    {
        "loss": 1.1229,
        "grad_norm": 1.8505241870880127,
        "learning_rate": 4.17098246795884e-05,
        "epoch": 1.799561742717195,
        "step": 13961
    },
    {
        "loss": 1.9486,
        "grad_norm": 2.4914958477020264,
        "learning_rate": 4.166040009551272e-05,
        "epoch": 1.79969064191802,
        "step": 13962
    },
    {
        "loss": 1.7011,
        "grad_norm": 2.8801591396331787,
        "learning_rate": 4.161099710611728e-05,
        "epoch": 1.7998195411188451,
        "step": 13963
    },
    {
        "loss": 2.2239,
        "grad_norm": 2.3403491973876953,
        "learning_rate": 4.156161572968902e-05,
        "epoch": 1.7999484403196702,
        "step": 13964
    },
    {
        "loss": 1.404,
        "grad_norm": 1.369705319404602,
        "learning_rate": 4.151225598450664e-05,
        "epoch": 1.800077339520495,
        "step": 13965
    },
    {
        "loss": 1.1299,
        "grad_norm": 2.360283613204956,
        "learning_rate": 4.1462917888840866e-05,
        "epoch": 1.8002062387213198,
        "step": 13966
    },
    {
        "loss": 1.4597,
        "grad_norm": 1.7060701847076416,
        "learning_rate": 4.141360146095443e-05,
        "epoch": 1.8003351379221448,
        "step": 13967
    },
    {
        "loss": 1.9372,
        "grad_norm": 2.0406575202941895,
        "learning_rate": 4.136430671910202e-05,
        "epoch": 1.8004640371229699,
        "step": 13968
    },
    {
        "loss": 1.8731,
        "grad_norm": 1.9852269887924194,
        "learning_rate": 4.13150336815302e-05,
        "epoch": 1.800592936323795,
        "step": 13969
    },
    {
        "loss": 1.9927,
        "grad_norm": 2.6057424545288086,
        "learning_rate": 4.126578236647789e-05,
        "epoch": 1.8007218355246197,
        "step": 13970
    },
    {
        "loss": 1.1028,
        "grad_norm": 3.246412992477417,
        "learning_rate": 4.1216552792175675e-05,
        "epoch": 1.8008507347254445,
        "step": 13971
    },
    {
        "loss": 1.7106,
        "grad_norm": 1.694471001625061,
        "learning_rate": 4.116734497684579e-05,
        "epoch": 1.8009796339262696,
        "step": 13972
    },
    {
        "loss": 1.6897,
        "grad_norm": 1.9265886545181274,
        "learning_rate": 4.1118158938703055e-05,
        "epoch": 1.8011085331270946,
        "step": 13973
    },
    {
        "loss": 0.8386,
        "grad_norm": 2.972301721572876,
        "learning_rate": 4.106899469595381e-05,
        "epoch": 1.8012374323279197,
        "step": 13974
    },
    {
        "loss": 1.3133,
        "grad_norm": 3.0551798343658447,
        "learning_rate": 4.101985226679641e-05,
        "epoch": 1.8013663315287445,
        "step": 13975
    },
    {
        "loss": 2.4374,
        "grad_norm": 1.5588362216949463,
        "learning_rate": 4.0970731669421226e-05,
        "epoch": 1.8014952307295695,
        "step": 13976
    },
    {
        "loss": 1.6082,
        "grad_norm": 2.020707607269287,
        "learning_rate": 4.092163292201043e-05,
        "epoch": 1.8016241299303943,
        "step": 13977
    },
    {
        "loss": 0.9225,
        "grad_norm": 2.2817420959472656,
        "learning_rate": 4.087255604273818e-05,
        "epoch": 1.8017530291312194,
        "step": 13978
    },
    {
        "loss": 0.8518,
        "grad_norm": 2.540269374847412,
        "learning_rate": 4.082350104977049e-05,
        "epoch": 1.8018819283320444,
        "step": 13979
    },
    {
        "loss": 2.1664,
        "grad_norm": 1.2490086555480957,
        "learning_rate": 4.077446796126523e-05,
        "epoch": 1.8020108275328695,
        "step": 13980
    },
    {
        "loss": 1.021,
        "grad_norm": 2.9720828533172607,
        "learning_rate": 4.072545679537236e-05,
        "epoch": 1.8021397267336943,
        "step": 13981
    },
    {
        "loss": 2.026,
        "grad_norm": 1.7774003744125366,
        "learning_rate": 4.067646757023361e-05,
        "epoch": 1.802268625934519,
        "step": 13982
    },
    {
        "loss": 1.1977,
        "grad_norm": 2.2313387393951416,
        "learning_rate": 4.06275003039825e-05,
        "epoch": 1.8023975251353441,
        "step": 13983
    },
    {
        "loss": 1.2629,
        "grad_norm": 2.0582380294799805,
        "learning_rate": 4.057855501474452e-05,
        "epoch": 1.8025264243361692,
        "step": 13984
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.264493227005005,
        "learning_rate": 4.0529631720636994e-05,
        "epoch": 1.8026553235369942,
        "step": 13985
    },
    {
        "loss": 1.3923,
        "grad_norm": 1.8845572471618652,
        "learning_rate": 4.0480730439768965e-05,
        "epoch": 1.802784222737819,
        "step": 13986
    },
    {
        "loss": 1.9348,
        "grad_norm": 1.7301844358444214,
        "learning_rate": 4.0431851190241764e-05,
        "epoch": 1.8029131219386438,
        "step": 13987
    },
    {
        "loss": 1.9435,
        "grad_norm": 1.9622553586959839,
        "learning_rate": 4.038299399014813e-05,
        "epoch": 1.8030420211394689,
        "step": 13988
    },
    {
        "loss": 1.732,
        "grad_norm": 3.264735460281372,
        "learning_rate": 4.0334158857572804e-05,
        "epoch": 1.803170920340294,
        "step": 13989
    },
    {
        "loss": 2.0087,
        "grad_norm": 2.7662429809570312,
        "learning_rate": 4.0285345810592325e-05,
        "epoch": 1.803299819541119,
        "step": 13990
    },
    {
        "loss": 1.5256,
        "grad_norm": 3.007223606109619,
        "learning_rate": 4.023655486727509e-05,
        "epoch": 1.8034287187419438,
        "step": 13991
    },
    {
        "loss": 2.0085,
        "grad_norm": 2.351076364517212,
        "learning_rate": 4.018778604568124e-05,
        "epoch": 1.8035576179427688,
        "step": 13992
    },
    {
        "loss": 1.5806,
        "grad_norm": 3.4235117435455322,
        "learning_rate": 4.013903936386289e-05,
        "epoch": 1.8036865171435936,
        "step": 13993
    },
    {
        "loss": 1.9586,
        "grad_norm": 2.188938617706299,
        "learning_rate": 4.009031483986377e-05,
        "epoch": 1.8038154163444187,
        "step": 13994
    },
    {
        "loss": 1.1561,
        "grad_norm": 1.9494267702102661,
        "learning_rate": 4.004161249171956e-05,
        "epoch": 1.8039443155452437,
        "step": 13995
    },
    {
        "loss": 2.2748,
        "grad_norm": 2.360215663909912,
        "learning_rate": 3.999293233745761e-05,
        "epoch": 1.8040732147460685,
        "step": 13996
    },
    {
        "loss": 0.9662,
        "grad_norm": 5.996734619140625,
        "learning_rate": 3.9944274395097006e-05,
        "epoch": 1.8042021139468936,
        "step": 13997
    },
    {
        "loss": 1.7655,
        "grad_norm": 1.8991873264312744,
        "learning_rate": 3.9895638682648974e-05,
        "epoch": 1.8043310131477184,
        "step": 13998
    },
    {
        "loss": 1.2674,
        "grad_norm": 2.9480745792388916,
        "learning_rate": 3.984702521811613e-05,
        "epoch": 1.8044599123485434,
        "step": 13999
    },
    {
        "loss": 1.3904,
        "grad_norm": 2.232416868209839,
        "learning_rate": 3.979843401949297e-05,
        "epoch": 1.8045888115493685,
        "step": 14000
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.356910228729248,
        "learning_rate": 3.974986510476576e-05,
        "epoch": 1.8047177107501935,
        "step": 14001
    },
    {
        "loss": 1.5633,
        "grad_norm": 1.7266325950622559,
        "learning_rate": 3.970131849191252e-05,
        "epoch": 1.8048466099510183,
        "step": 14002
    },
    {
        "loss": 1.6842,
        "grad_norm": 1.4395400285720825,
        "learning_rate": 3.9652794198902856e-05,
        "epoch": 1.8049755091518431,
        "step": 14003
    },
    {
        "loss": 2.0858,
        "grad_norm": 1.3127577304840088,
        "learning_rate": 3.960429224369856e-05,
        "epoch": 1.8051044083526682,
        "step": 14004
    },
    {
        "loss": 1.277,
        "grad_norm": 1.9453729391098022,
        "learning_rate": 3.955581264425272e-05,
        "epoch": 1.8052333075534932,
        "step": 14005
    },
    {
        "loss": 2.2056,
        "grad_norm": 2.941887617111206,
        "learning_rate": 3.950735541851027e-05,
        "epoch": 1.8053622067543182,
        "step": 14006
    },
    {
        "loss": 1.4406,
        "grad_norm": 2.4551503658294678,
        "learning_rate": 3.945892058440792e-05,
        "epoch": 1.805491105955143,
        "step": 14007
    },
    {
        "loss": 0.9559,
        "grad_norm": 1.9437249898910522,
        "learning_rate": 3.941050815987398e-05,
        "epoch": 1.8056200051559679,
        "step": 14008
    },
    {
        "loss": 0.9942,
        "grad_norm": 2.4075164794921875,
        "learning_rate": 3.936211816282863e-05,
        "epoch": 1.805748904356793,
        "step": 14009
    },
    {
        "loss": 1.0164,
        "grad_norm": 2.0523345470428467,
        "learning_rate": 3.931375061118363e-05,
        "epoch": 1.805877803557618,
        "step": 14010
    },
    {
        "loss": 1.9699,
        "grad_norm": 2.257427930831909,
        "learning_rate": 3.9265405522842436e-05,
        "epoch": 1.806006702758443,
        "step": 14011
    },
    {
        "loss": 1.394,
        "grad_norm": 3.5501773357391357,
        "learning_rate": 3.921708291570023e-05,
        "epoch": 1.8061356019592678,
        "step": 14012
    },
    {
        "loss": 2.2856,
        "grad_norm": 1.95267653465271,
        "learning_rate": 3.9168782807643856e-05,
        "epoch": 1.8062645011600929,
        "step": 14013
    },
    {
        "loss": 1.9542,
        "grad_norm": 1.796981692314148,
        "learning_rate": 3.912050521655167e-05,
        "epoch": 1.8063934003609177,
        "step": 14014
    },
    {
        "loss": 1.8358,
        "grad_norm": 2.577127456665039,
        "learning_rate": 3.907225016029415e-05,
        "epoch": 1.8065222995617427,
        "step": 14015
    },
    {
        "loss": 2.124,
        "grad_norm": 2.2121706008911133,
        "learning_rate": 3.9024017656733e-05,
        "epoch": 1.8066511987625677,
        "step": 14016
    },
    {
        "loss": 1.1505,
        "grad_norm": 3.228065252304077,
        "learning_rate": 3.8975807723721694e-05,
        "epoch": 1.8067800979633928,
        "step": 14017
    },
    {
        "loss": 1.7993,
        "grad_norm": 3.0844309329986572,
        "learning_rate": 3.892762037910537e-05,
        "epoch": 1.8069089971642176,
        "step": 14018
    },
    {
        "loss": 1.5137,
        "grad_norm": 3.2284998893737793,
        "learning_rate": 3.887945564072081e-05,
        "epoch": 1.8070378963650424,
        "step": 14019
    },
    {
        "loss": 1.4049,
        "grad_norm": 3.265279769897461,
        "learning_rate": 3.883131352639626e-05,
        "epoch": 1.8071667955658675,
        "step": 14020
    },
    {
        "loss": 0.9058,
        "grad_norm": 3.2863945960998535,
        "learning_rate": 3.8783194053952064e-05,
        "epoch": 1.8072956947666925,
        "step": 14021
    },
    {
        "loss": 0.7401,
        "grad_norm": 3.004246473312378,
        "learning_rate": 3.8735097241199726e-05,
        "epoch": 1.8074245939675175,
        "step": 14022
    },
    {
        "loss": 1.2111,
        "grad_norm": 3.249652862548828,
        "learning_rate": 3.86870231059425e-05,
        "epoch": 1.8075534931683424,
        "step": 14023
    },
    {
        "loss": 1.2733,
        "grad_norm": 2.246088981628418,
        "learning_rate": 3.863897166597525e-05,
        "epoch": 1.8076823923691672,
        "step": 14024
    },
    {
        "loss": 1.2426,
        "grad_norm": 2.816913366317749,
        "learning_rate": 3.859094293908443e-05,
        "epoch": 1.8078112915699922,
        "step": 14025
    },
    {
        "loss": 1.0549,
        "grad_norm": 2.2480478286743164,
        "learning_rate": 3.8542936943048155e-05,
        "epoch": 1.8079401907708172,
        "step": 14026
    },
    {
        "loss": 1.3365,
        "grad_norm": 1.8674901723861694,
        "learning_rate": 3.849495369563605e-05,
        "epoch": 1.8080690899716423,
        "step": 14027
    },
    {
        "loss": 2.0167,
        "grad_norm": 1.393742322921753,
        "learning_rate": 3.8446993214609215e-05,
        "epoch": 1.808197989172467,
        "step": 14028
    },
    {
        "loss": 1.5286,
        "grad_norm": 4.591433525085449,
        "learning_rate": 3.8399055517720795e-05,
        "epoch": 1.8083268883732921,
        "step": 14029
    },
    {
        "loss": 1.3285,
        "grad_norm": 2.8148539066314697,
        "learning_rate": 3.8351140622714834e-05,
        "epoch": 1.808455787574117,
        "step": 14030
    },
    {
        "loss": 1.3444,
        "grad_norm": 1.4810969829559326,
        "learning_rate": 3.8303248547327206e-05,
        "epoch": 1.808584686774942,
        "step": 14031
    },
    {
        "loss": 0.6615,
        "grad_norm": 2.5501697063446045,
        "learning_rate": 3.825537930928568e-05,
        "epoch": 1.808713585975767,
        "step": 14032
    },
    {
        "loss": 1.3803,
        "grad_norm": 2.754652261734009,
        "learning_rate": 3.820753292630914e-05,
        "epoch": 1.8088424851765919,
        "step": 14033
    },
    {
        "loss": 0.6746,
        "grad_norm": 2.206855297088623,
        "learning_rate": 3.815970941610817e-05,
        "epoch": 1.808971384377417,
        "step": 14034
    },
    {
        "loss": 0.7946,
        "grad_norm": 2.659407377243042,
        "learning_rate": 3.811190879638484e-05,
        "epoch": 1.8091002835782417,
        "step": 14035
    },
    {
        "loss": 1.6741,
        "grad_norm": 3.1134257316589355,
        "learning_rate": 3.8064131084832774e-05,
        "epoch": 1.8092291827790667,
        "step": 14036
    },
    {
        "loss": 1.4412,
        "grad_norm": 2.0565571784973145,
        "learning_rate": 3.8016376299137056e-05,
        "epoch": 1.8093580819798918,
        "step": 14037
    },
    {
        "loss": 1.7522,
        "grad_norm": 2.9951272010803223,
        "learning_rate": 3.7968644456974534e-05,
        "epoch": 1.8094869811807168,
        "step": 14038
    },
    {
        "loss": 1.9664,
        "grad_norm": 1.8716075420379639,
        "learning_rate": 3.792093557601327e-05,
        "epoch": 1.8096158803815416,
        "step": 14039
    },
    {
        "loss": 0.8072,
        "grad_norm": 2.274068593978882,
        "learning_rate": 3.7873249673912934e-05,
        "epoch": 1.8097447795823665,
        "step": 14040
    },
    {
        "loss": 1.0746,
        "grad_norm": 1.7467007637023926,
        "learning_rate": 3.782558676832466e-05,
        "epoch": 1.8098736787831915,
        "step": 14041
    },
    {
        "loss": 1.9471,
        "grad_norm": 2.462852954864502,
        "learning_rate": 3.777794687689116e-05,
        "epoch": 1.8100025779840165,
        "step": 14042
    },
    {
        "loss": 1.3425,
        "grad_norm": 4.604709148406982,
        "learning_rate": 3.773033001724653e-05,
        "epoch": 1.8101314771848416,
        "step": 14043
    },
    {
        "loss": 0.9531,
        "grad_norm": 2.7751386165618896,
        "learning_rate": 3.7682736207016386e-05,
        "epoch": 1.8102603763856664,
        "step": 14044
    },
    {
        "loss": 1.2809,
        "grad_norm": 2.891113519668579,
        "learning_rate": 3.7635165463817704e-05,
        "epoch": 1.8103892755864912,
        "step": 14045
    },
    {
        "loss": 0.7464,
        "grad_norm": 2.431171417236328,
        "learning_rate": 3.7587617805259314e-05,
        "epoch": 1.8105181747873162,
        "step": 14046
    },
    {
        "loss": 0.662,
        "grad_norm": 1.9381102323532104,
        "learning_rate": 3.7540093248940866e-05,
        "epoch": 1.8106470739881413,
        "step": 14047
    },
    {
        "loss": 2.3728,
        "grad_norm": 1.8064461946487427,
        "learning_rate": 3.749259181245388e-05,
        "epoch": 1.8107759731889663,
        "step": 14048
    },
    {
        "loss": 1.896,
        "grad_norm": 1.8097178936004639,
        "learning_rate": 3.744511351338141e-05,
        "epoch": 1.8109048723897911,
        "step": 14049
    },
    {
        "loss": 1.5998,
        "grad_norm": 2.496112585067749,
        "learning_rate": 3.739765836929766e-05,
        "epoch": 1.8110337715906162,
        "step": 14050
    },
    {
        "loss": 1.8738,
        "grad_norm": 3.4717438220977783,
        "learning_rate": 3.735022639776828e-05,
        "epoch": 1.811162670791441,
        "step": 14051
    },
    {
        "loss": 1.4746,
        "grad_norm": 5.568687915802002,
        "learning_rate": 3.730281761635077e-05,
        "epoch": 1.811291569992266,
        "step": 14052
    },
    {
        "loss": 1.5892,
        "grad_norm": 2.1681365966796875,
        "learning_rate": 3.725543204259334e-05,
        "epoch": 1.811420469193091,
        "step": 14053
    },
    {
        "loss": 1.9495,
        "grad_norm": 2.1034650802612305,
        "learning_rate": 3.7208069694036065e-05,
        "epoch": 1.8115493683939161,
        "step": 14054
    },
    {
        "loss": 1.8631,
        "grad_norm": 2.3193371295928955,
        "learning_rate": 3.716073058821051e-05,
        "epoch": 1.811678267594741,
        "step": 14055
    },
    {
        "loss": 1.9654,
        "grad_norm": 2.0562548637390137,
        "learning_rate": 3.7113414742639383e-05,
        "epoch": 1.8118071667955657,
        "step": 14056
    },
    {
        "loss": 2.1709,
        "grad_norm": 5.520936012268066,
        "learning_rate": 3.706612217483687e-05,
        "epoch": 1.8119360659963908,
        "step": 14057
    },
    {
        "loss": 1.3407,
        "grad_norm": 2.128570795059204,
        "learning_rate": 3.701885290230849e-05,
        "epoch": 1.8120649651972158,
        "step": 14058
    },
    {
        "loss": 0.8906,
        "grad_norm": 1.703075885772705,
        "learning_rate": 3.6971606942551253e-05,
        "epoch": 1.8121938643980409,
        "step": 14059
    },
    {
        "loss": 1.7511,
        "grad_norm": 2.548515796661377,
        "learning_rate": 3.692438431305346e-05,
        "epoch": 1.8123227635988657,
        "step": 14060
    },
    {
        "loss": 1.8058,
        "grad_norm": 2.0186805725097656,
        "learning_rate": 3.687718503129478e-05,
        "epoch": 1.8124516627996905,
        "step": 14061
    },
    {
        "loss": 1.4574,
        "grad_norm": 2.169539213180542,
        "learning_rate": 3.683000911474617e-05,
        "epoch": 1.8125805620005155,
        "step": 14062
    },
    {
        "loss": 2.1835,
        "grad_norm": 2.692986011505127,
        "learning_rate": 3.6782856580870295e-05,
        "epoch": 1.8127094612013406,
        "step": 14063
    },
    {
        "loss": 2.2255,
        "grad_norm": 2.4873902797698975,
        "learning_rate": 3.673572744712056e-05,
        "epoch": 1.8128383604021656,
        "step": 14064
    },
    {
        "loss": 1.7055,
        "grad_norm": 3.0213685035705566,
        "learning_rate": 3.668862173094212e-05,
        "epoch": 1.8129672596029904,
        "step": 14065
    },
    {
        "loss": 0.4051,
        "grad_norm": 2.836747407913208,
        "learning_rate": 3.664153944977153e-05,
        "epoch": 1.8130961588038152,
        "step": 14066
    },
    {
        "loss": 1.7961,
        "grad_norm": 3.0959928035736084,
        "learning_rate": 3.659448062103644e-05,
        "epoch": 1.8132250580046403,
        "step": 14067
    },
    {
        "loss": 1.2645,
        "grad_norm": 3.361753225326538,
        "learning_rate": 3.654744526215575e-05,
        "epoch": 1.8133539572054653,
        "step": 14068
    },
    {
        "loss": 1.8756,
        "grad_norm": 2.1842219829559326,
        "learning_rate": 3.650043339054019e-05,
        "epoch": 1.8134828564062904,
        "step": 14069
    },
    {
        "loss": 2.5462,
        "grad_norm": 2.280458927154541,
        "learning_rate": 3.64534450235911e-05,
        "epoch": 1.8136117556071152,
        "step": 14070
    },
    {
        "loss": 1.4252,
        "grad_norm": 1.5899639129638672,
        "learning_rate": 3.6406480178701444e-05,
        "epoch": 1.8137406548079402,
        "step": 14071
    },
    {
        "loss": 1.5767,
        "grad_norm": 2.878077983856201,
        "learning_rate": 3.6359538873255685e-05,
        "epoch": 1.813869554008765,
        "step": 14072
    },
    {
        "loss": 1.3906,
        "grad_norm": 1.7630746364593506,
        "learning_rate": 3.631262112462926e-05,
        "epoch": 1.81399845320959,
        "step": 14073
    },
    {
        "loss": 1.153,
        "grad_norm": 2.698375940322876,
        "learning_rate": 3.626572695018898e-05,
        "epoch": 1.8141273524104151,
        "step": 14074
    },
    {
        "loss": 1.3498,
        "grad_norm": 2.5364296436309814,
        "learning_rate": 3.621885636729301e-05,
        "epoch": 1.8142562516112402,
        "step": 14075
    },
    {
        "loss": 1.8911,
        "grad_norm": 3.279850482940674,
        "learning_rate": 3.617200939329068e-05,
        "epoch": 1.814385150812065,
        "step": 14076
    },
    {
        "loss": 1.9235,
        "grad_norm": 2.6075706481933594,
        "learning_rate": 3.612518604552262e-05,
        "epoch": 1.8145140500128898,
        "step": 14077
    },
    {
        "loss": 1.5085,
        "grad_norm": 2.327892780303955,
        "learning_rate": 3.607838634132071e-05,
        "epoch": 1.8146429492137148,
        "step": 14078
    },
    {
        "loss": 1.492,
        "grad_norm": 2.514150857925415,
        "learning_rate": 3.6031610298008e-05,
        "epoch": 1.8147718484145399,
        "step": 14079
    },
    {
        "loss": 1.8191,
        "grad_norm": 3.846773624420166,
        "learning_rate": 3.598485793289912e-05,
        "epoch": 1.814900747615365,
        "step": 14080
    },
    {
        "loss": 2.1225,
        "grad_norm": 1.7851316928863525,
        "learning_rate": 3.5938129263299446e-05,
        "epoch": 1.8150296468161897,
        "step": 14081
    },
    {
        "loss": 1.541,
        "grad_norm": 1.780144214630127,
        "learning_rate": 3.589142430650578e-05,
        "epoch": 1.8151585460170145,
        "step": 14082
    },
    {
        "loss": 0.8036,
        "grad_norm": 2.5715653896331787,
        "learning_rate": 3.58447430798064e-05,
        "epoch": 1.8152874452178396,
        "step": 14083
    },
    {
        "loss": 1.0161,
        "grad_norm": 2.837712526321411,
        "learning_rate": 3.579808560048047e-05,
        "epoch": 1.8154163444186646,
        "step": 14084
    },
    {
        "loss": 1.007,
        "grad_norm": 2.3702006340026855,
        "learning_rate": 3.5751451885798336e-05,
        "epoch": 1.8155452436194897,
        "step": 14085
    },
    {
        "loss": 1.4975,
        "grad_norm": 1.76934015750885,
        "learning_rate": 3.570484195302208e-05,
        "epoch": 1.8156741428203145,
        "step": 14086
    },
    {
        "loss": 2.0172,
        "grad_norm": 2.6138916015625,
        "learning_rate": 3.565825581940425e-05,
        "epoch": 1.8158030420211395,
        "step": 14087
    },
    {
        "loss": 1.712,
        "grad_norm": 1.9899662733078003,
        "learning_rate": 3.561169350218891e-05,
        "epoch": 1.8159319412219643,
        "step": 14088
    },
    {
        "loss": 1.6265,
        "grad_norm": 2.718508243560791,
        "learning_rate": 3.556515501861155e-05,
        "epoch": 1.8160608404227894,
        "step": 14089
    },
    {
        "loss": 1.0944,
        "grad_norm": 2.8354294300079346,
        "learning_rate": 3.5518640385898485e-05,
        "epoch": 1.8161897396236144,
        "step": 14090
    },
    {
        "loss": 1.4361,
        "grad_norm": 2.9498367309570312,
        "learning_rate": 3.54721496212673e-05,
        "epoch": 1.8163186388244394,
        "step": 14091
    },
    {
        "loss": 1.7997,
        "grad_norm": 1.7581937313079834,
        "learning_rate": 3.5425682741926844e-05,
        "epoch": 1.8164475380252643,
        "step": 14092
    },
    {
        "loss": 1.4015,
        "grad_norm": 1.8787659406661987,
        "learning_rate": 3.537923976507702e-05,
        "epoch": 1.816576437226089,
        "step": 14093
    },
    {
        "loss": 1.4179,
        "grad_norm": 2.420682191848755,
        "learning_rate": 3.533282070790892e-05,
        "epoch": 1.8167053364269141,
        "step": 14094
    },
    {
        "loss": 1.0816,
        "grad_norm": 3.5341086387634277,
        "learning_rate": 3.528642558760478e-05,
        "epoch": 1.8168342356277392,
        "step": 14095
    },
    {
        "loss": 1.2415,
        "grad_norm": 1.885421633720398,
        "learning_rate": 3.524005442133782e-05,
        "epoch": 1.8169631348285642,
        "step": 14096
    },
    {
        "loss": 1.3677,
        "grad_norm": 2.376207113265991,
        "learning_rate": 3.519370722627294e-05,
        "epoch": 1.817092034029389,
        "step": 14097
    },
    {
        "loss": 1.2104,
        "grad_norm": 3.851562976837158,
        "learning_rate": 3.5147384019565436e-05,
        "epoch": 1.8172209332302138,
        "step": 14098
    },
    {
        "loss": 0.9847,
        "grad_norm": 3.075554847717285,
        "learning_rate": 3.510108481836204e-05,
        "epoch": 1.8173498324310389,
        "step": 14099
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.018785238265991,
        "learning_rate": 3.5054809639800855e-05,
        "epoch": 1.817478731631864,
        "step": 14100
    },
    {
        "loss": 1.4921,
        "grad_norm": 2.2417924404144287,
        "learning_rate": 3.500855850101073e-05,
        "epoch": 1.817607630832689,
        "step": 14101
    },
    {
        "loss": 1.77,
        "grad_norm": 2.4643948078155518,
        "learning_rate": 3.496233141911164e-05,
        "epoch": 1.8177365300335138,
        "step": 14102
    },
    {
        "loss": 2.0785,
        "grad_norm": 3.266235113143921,
        "learning_rate": 3.491612841121511e-05,
        "epoch": 1.8178654292343386,
        "step": 14103
    },
    {
        "loss": 1.7434,
        "grad_norm": 2.343126058578491,
        "learning_rate": 3.486994949442306e-05,
        "epoch": 1.8179943284351636,
        "step": 14104
    },
    {
        "loss": 2.1301,
        "grad_norm": 2.2577412128448486,
        "learning_rate": 3.4823794685828856e-05,
        "epoch": 1.8181232276359887,
        "step": 14105
    },
    {
        "loss": 1.7729,
        "grad_norm": 2.945521354675293,
        "learning_rate": 3.4777664002517096e-05,
        "epoch": 1.8182521268368137,
        "step": 14106
    },
    {
        "loss": 2.1693,
        "grad_norm": 2.0200092792510986,
        "learning_rate": 3.473155746156317e-05,
        "epoch": 1.8183810260376385,
        "step": 14107
    },
    {
        "loss": 1.7102,
        "grad_norm": 2.247818946838379,
        "learning_rate": 3.4685475080033667e-05,
        "epoch": 1.8185099252384636,
        "step": 14108
    },
    {
        "loss": 1.7233,
        "grad_norm": 2.0601344108581543,
        "learning_rate": 3.463941687498618e-05,
        "epoch": 1.8186388244392884,
        "step": 14109
    },
    {
        "loss": 1.2124,
        "grad_norm": 1.7469396591186523,
        "learning_rate": 3.4593382863469395e-05,
        "epoch": 1.8187677236401134,
        "step": 14110
    },
    {
        "loss": 1.0716,
        "grad_norm": 1.5126272439956665,
        "learning_rate": 3.454737306252298e-05,
        "epoch": 1.8188966228409384,
        "step": 14111
    },
    {
        "loss": 1.4542,
        "grad_norm": 2.1387853622436523,
        "learning_rate": 3.450138748917771e-05,
        "epoch": 1.8190255220417635,
        "step": 14112
    },
    {
        "loss": 0.952,
        "grad_norm": 3.148005723953247,
        "learning_rate": 3.4455426160455206e-05,
        "epoch": 1.8191544212425883,
        "step": 14113
    },
    {
        "loss": 1.262,
        "grad_norm": 3.2905843257904053,
        "learning_rate": 3.4409489093368563e-05,
        "epoch": 1.8192833204434131,
        "step": 14114
    },
    {
        "loss": 0.59,
        "grad_norm": 0.9496402144432068,
        "learning_rate": 3.436357630492157e-05,
        "epoch": 1.8194122196442382,
        "step": 14115
    },
    {
        "loss": 1.4524,
        "grad_norm": 2.268376111984253,
        "learning_rate": 3.431768781210876e-05,
        "epoch": 1.8195411188450632,
        "step": 14116
    },
    {
        "loss": 2.0935,
        "grad_norm": 2.0365984439849854,
        "learning_rate": 3.4271823631916254e-05,
        "epoch": 1.8196700180458882,
        "step": 14117
    },
    {
        "loss": 1.6186,
        "grad_norm": 3.3086154460906982,
        "learning_rate": 3.422598378132084e-05,
        "epoch": 1.819798917246713,
        "step": 14118
    },
    {
        "loss": 1.0301,
        "grad_norm": 1.9919273853302002,
        "learning_rate": 3.418016827729018e-05,
        "epoch": 1.8199278164475379,
        "step": 14119
    },
    {
        "loss": 2.0534,
        "grad_norm": 2.2939963340759277,
        "learning_rate": 3.413437713678349e-05,
        "epoch": 1.820056715648363,
        "step": 14120
    },
    {
        "loss": 2.3613,
        "grad_norm": 2.161529779434204,
        "learning_rate": 3.408861037675025e-05,
        "epoch": 1.820185614849188,
        "step": 14121
    },
    {
        "loss": 2.0785,
        "grad_norm": 2.41593861579895,
        "learning_rate": 3.4042868014131225e-05,
        "epoch": 1.820314514050013,
        "step": 14122
    },
    {
        "loss": 1.5999,
        "grad_norm": 3.605717182159424,
        "learning_rate": 3.399715006585839e-05,
        "epoch": 1.8204434132508378,
        "step": 14123
    },
    {
        "loss": 0.4983,
        "grad_norm": 2.041869878768921,
        "learning_rate": 3.395145654885431e-05,
        "epoch": 1.8205723124516628,
        "step": 14124
    },
    {
        "loss": 2.0082,
        "grad_norm": 2.3045969009399414,
        "learning_rate": 3.3905787480032735e-05,
        "epoch": 1.8207012116524877,
        "step": 14125
    },
    {
        "loss": 1.0455,
        "grad_norm": 2.257477045059204,
        "learning_rate": 3.3860142876298254e-05,
        "epoch": 1.8208301108533127,
        "step": 14126
    },
    {
        "loss": 1.6692,
        "grad_norm": 3.0684585571289062,
        "learning_rate": 3.3814522754546416e-05,
        "epoch": 1.8209590100541377,
        "step": 14127
    },
    {
        "loss": 1.9783,
        "grad_norm": 3.2434065341949463,
        "learning_rate": 3.376892713166375e-05,
        "epoch": 1.8210879092549628,
        "step": 14128
    },
    {
        "loss": 1.4159,
        "grad_norm": 1.6879695653915405,
        "learning_rate": 3.3723356024527685e-05,
        "epoch": 1.8212168084557876,
        "step": 14129
    },
    {
        "loss": 1.6915,
        "grad_norm": 2.6035783290863037,
        "learning_rate": 3.367780945000645e-05,
        "epoch": 1.8213457076566124,
        "step": 14130
    },
    {
        "loss": 1.8182,
        "grad_norm": 1.7340096235275269,
        "learning_rate": 3.3632287424959585e-05,
        "epoch": 1.8214746068574375,
        "step": 14131
    },
    {
        "loss": 1.6254,
        "grad_norm": 2.5614516735076904,
        "learning_rate": 3.3586789966237254e-05,
        "epoch": 1.8216035060582625,
        "step": 14132
    },
    {
        "loss": 1.765,
        "grad_norm": 2.544649600982666,
        "learning_rate": 3.354131709068027e-05,
        "epoch": 1.8217324052590875,
        "step": 14133
    },
    {
        "loss": 2.3119,
        "grad_norm": 2.9836337566375732,
        "learning_rate": 3.3495868815120925e-05,
        "epoch": 1.8218613044599123,
        "step": 14134
    },
    {
        "loss": 2.2364,
        "grad_norm": 2.2559773921966553,
        "learning_rate": 3.345044515638197e-05,
        "epoch": 1.8219902036607372,
        "step": 14135
    },
    {
        "loss": 1.7438,
        "grad_norm": 1.9175728559494019,
        "learning_rate": 3.340504613127717e-05,
        "epoch": 1.8221191028615622,
        "step": 14136
    },
    {
        "loss": 0.8014,
        "grad_norm": 2.0817580223083496,
        "learning_rate": 3.3359671756611454e-05,
        "epoch": 1.8222480020623872,
        "step": 14137
    },
    {
        "loss": 0.7128,
        "grad_norm": 2.107389450073242,
        "learning_rate": 3.3314322049180055e-05,
        "epoch": 1.8223769012632123,
        "step": 14138
    },
    {
        "loss": 1.0263,
        "grad_norm": 2.6852219104766846,
        "learning_rate": 3.3268997025769386e-05,
        "epoch": 1.822505800464037,
        "step": 14139
    },
    {
        "loss": 1.7299,
        "grad_norm": 2.589597702026367,
        "learning_rate": 3.32236967031569e-05,
        "epoch": 1.822634699664862,
        "step": 14140
    },
    {
        "loss": 1.9208,
        "grad_norm": 1.9289532899856567,
        "learning_rate": 3.317842109811068e-05,
        "epoch": 1.822763598865687,
        "step": 14141
    },
    {
        "loss": 1.124,
        "grad_norm": 1.89970862865448,
        "learning_rate": 3.3133170227389675e-05,
        "epoch": 1.822892498066512,
        "step": 14142
    },
    {
        "loss": 2.251,
        "grad_norm": 1.6165778636932373,
        "learning_rate": 3.308794410774371e-05,
        "epoch": 1.823021397267337,
        "step": 14143
    },
    {
        "loss": 1.4901,
        "grad_norm": 3.3674380779266357,
        "learning_rate": 3.304274275591345e-05,
        "epoch": 1.8231502964681618,
        "step": 14144
    },
    {
        "loss": 1.2397,
        "grad_norm": 3.0019662380218506,
        "learning_rate": 3.299756618863039e-05,
        "epoch": 1.8232791956689869,
        "step": 14145
    },
    {
        "loss": 1.1039,
        "grad_norm": 2.1163105964660645,
        "learning_rate": 3.2952414422616814e-05,
        "epoch": 1.8234080948698117,
        "step": 14146
    },
    {
        "loss": 1.3713,
        "grad_norm": 2.970834493637085,
        "learning_rate": 3.290728747458583e-05,
        "epoch": 1.8235369940706367,
        "step": 14147
    },
    {
        "loss": 1.5684,
        "grad_norm": 3.1927456855773926,
        "learning_rate": 3.286218536124157e-05,
        "epoch": 1.8236658932714618,
        "step": 14148
    },
    {
        "loss": 1.8442,
        "grad_norm": 2.826789379119873,
        "learning_rate": 3.281710809927879e-05,
        "epoch": 1.8237947924722868,
        "step": 14149
    },
    {
        "loss": 1.3959,
        "grad_norm": 2.2314507961273193,
        "learning_rate": 3.277205570538274e-05,
        "epoch": 1.8239236916731116,
        "step": 14150
    },
    {
        "loss": 1.4945,
        "grad_norm": 1.9955511093139648,
        "learning_rate": 3.27270281962301e-05,
        "epoch": 1.8240525908739365,
        "step": 14151
    },
    {
        "loss": 1.0157,
        "grad_norm": 3.1653494834899902,
        "learning_rate": 3.2682025588487875e-05,
        "epoch": 1.8241814900747615,
        "step": 14152
    },
    {
        "loss": 1.3199,
        "grad_norm": 2.513784885406494,
        "learning_rate": 3.263704789881394e-05,
        "epoch": 1.8243103892755865,
        "step": 14153
    },
    {
        "loss": 0.9031,
        "grad_norm": 3.5033915042877197,
        "learning_rate": 3.2592095143857205e-05,
        "epoch": 1.8244392884764116,
        "step": 14154
    },
    {
        "loss": 1.8812,
        "grad_norm": 1.8197705745697021,
        "learning_rate": 3.2547167340257115e-05,
        "epoch": 1.8245681876772364,
        "step": 14155
    },
    {
        "loss": 2.0796,
        "grad_norm": 1.8406589031219482,
        "learning_rate": 3.250226450464362e-05,
        "epoch": 1.8246970868780612,
        "step": 14156
    },
    {
        "loss": 1.7931,
        "grad_norm": 3.284324884414673,
        "learning_rate": 3.245738665363799e-05,
        "epoch": 1.8248259860788862,
        "step": 14157
    },
    {
        "loss": 1.7495,
        "grad_norm": 3.4628114700317383,
        "learning_rate": 3.241253380385193e-05,
        "epoch": 1.8249548852797113,
        "step": 14158
    },
    {
        "loss": 1.8315,
        "grad_norm": 3.311941385269165,
        "learning_rate": 3.236770597188792e-05,
        "epoch": 1.8250837844805363,
        "step": 14159
    },
    {
        "loss": 0.9833,
        "grad_norm": 1.8187787532806396,
        "learning_rate": 3.232290317433918e-05,
        "epoch": 1.8252126836813611,
        "step": 14160
    },
    {
        "loss": 1.5177,
        "grad_norm": 1.8871920108795166,
        "learning_rate": 3.227812542778969e-05,
        "epoch": 1.8253415828821862,
        "step": 14161
    },
    {
        "loss": 1.9956,
        "grad_norm": 3.4392011165618896,
        "learning_rate": 3.2233372748814136e-05,
        "epoch": 1.825470482083011,
        "step": 14162
    },
    {
        "loss": 1.5425,
        "grad_norm": 2.169898748397827,
        "learning_rate": 3.2188645153977907e-05,
        "epoch": 1.825599381283836,
        "step": 14163
    },
    {
        "loss": 0.9428,
        "grad_norm": 3.5823802947998047,
        "learning_rate": 3.21439426598371e-05,
        "epoch": 1.825728280484661,
        "step": 14164
    },
    {
        "loss": 2.1274,
        "grad_norm": 2.0114903450012207,
        "learning_rate": 3.2099265282938715e-05,
        "epoch": 1.825857179685486,
        "step": 14165
    },
    {
        "loss": 0.9543,
        "grad_norm": 2.3993940353393555,
        "learning_rate": 3.205461303982031e-05,
        "epoch": 1.825986078886311,
        "step": 14166
    },
    {
        "loss": 1.7981,
        "grad_norm": 3.1942927837371826,
        "learning_rate": 3.2009985947009824e-05,
        "epoch": 1.8261149780871357,
        "step": 14167
    },
    {
        "loss": 1.2662,
        "grad_norm": 2.475118637084961,
        "learning_rate": 3.196538402102645e-05,
        "epoch": 1.8262438772879608,
        "step": 14168
    },
    {
        "loss": 1.8598,
        "grad_norm": 1.5941498279571533,
        "learning_rate": 3.192080727837975e-05,
        "epoch": 1.8263727764887858,
        "step": 14169
    },
    {
        "loss": 1.8095,
        "grad_norm": 2.548675537109375,
        "learning_rate": 3.187625573556992e-05,
        "epoch": 1.8265016756896109,
        "step": 14170
    },
    {
        "loss": 0.9387,
        "grad_norm": 1.7596373558044434,
        "learning_rate": 3.1831729409088096e-05,
        "epoch": 1.8266305748904357,
        "step": 14171
    },
    {
        "loss": 1.8254,
        "grad_norm": 2.393763780593872,
        "learning_rate": 3.178722831541593e-05,
        "epoch": 1.8267594740912605,
        "step": 14172
    },
    {
        "loss": 0.9256,
        "grad_norm": 2.9045815467834473,
        "learning_rate": 3.174275247102536e-05,
        "epoch": 1.8268883732920855,
        "step": 14173
    },
    {
        "loss": 0.5254,
        "grad_norm": 3.275156021118164,
        "learning_rate": 3.169830189237972e-05,
        "epoch": 1.8270172724929106,
        "step": 14174
    },
    {
        "loss": 1.788,
        "grad_norm": 3.3190953731536865,
        "learning_rate": 3.1653876595932456e-05,
        "epoch": 1.8271461716937356,
        "step": 14175
    },
    {
        "loss": 1.0703,
        "grad_norm": 3.275803804397583,
        "learning_rate": 3.160947659812781e-05,
        "epoch": 1.8272750708945604,
        "step": 14176
    },
    {
        "loss": 2.0482,
        "grad_norm": 2.059156656265259,
        "learning_rate": 3.156510191540064e-05,
        "epoch": 1.8274039700953852,
        "step": 14177
    },
    {
        "loss": 1.7486,
        "grad_norm": 3.3801655769348145,
        "learning_rate": 3.1520752564176446e-05,
        "epoch": 1.8275328692962103,
        "step": 14178
    },
    {
        "loss": 0.3394,
        "grad_norm": 1.4558320045471191,
        "learning_rate": 3.147642856087134e-05,
        "epoch": 1.8276617684970353,
        "step": 14179
    },
    {
        "loss": 1.5543,
        "grad_norm": 2.6531805992126465,
        "learning_rate": 3.143212992189212e-05,
        "epoch": 1.8277906676978604,
        "step": 14180
    },
    {
        "loss": 1.6881,
        "grad_norm": 1.6570254564285278,
        "learning_rate": 3.138785666363599e-05,
        "epoch": 1.8279195668986852,
        "step": 14181
    },
    {
        "loss": 0.9101,
        "grad_norm": 2.2499165534973145,
        "learning_rate": 3.134360880249112e-05,
        "epoch": 1.8280484660995102,
        "step": 14182
    },
    {
        "loss": 1.6371,
        "grad_norm": 3.3829545974731445,
        "learning_rate": 3.1299386354836056e-05,
        "epoch": 1.828177365300335,
        "step": 14183
    },
    {
        "loss": 1.0372,
        "grad_norm": 2.3296170234680176,
        "learning_rate": 3.1255189337039656e-05,
        "epoch": 1.82830626450116,
        "step": 14184
    },
    {
        "loss": 2.2236,
        "grad_norm": 1.8772464990615845,
        "learning_rate": 3.121101776546192e-05,
        "epoch": 1.8284351637019851,
        "step": 14185
    },
    {
        "loss": 1.562,
        "grad_norm": 2.9671552181243896,
        "learning_rate": 3.1166871656453145e-05,
        "epoch": 1.8285640629028102,
        "step": 14186
    },
    {
        "loss": 1.8194,
        "grad_norm": 2.5004124641418457,
        "learning_rate": 3.112275102635408e-05,
        "epoch": 1.828692962103635,
        "step": 14187
    },
    {
        "loss": 0.63,
        "grad_norm": 2.3417227268218994,
        "learning_rate": 3.107865589149638e-05,
        "epoch": 1.8288218613044598,
        "step": 14188
    },
    {
        "loss": 1.9204,
        "grad_norm": 2.4338955879211426,
        "learning_rate": 3.103458626820206e-05,
        "epoch": 1.8289507605052848,
        "step": 14189
    },
    {
        "loss": 0.4025,
        "grad_norm": 1.6061441898345947,
        "learning_rate": 3.099054217278339e-05,
        "epoch": 1.8290796597061099,
        "step": 14190
    },
    {
        "loss": 1.3293,
        "grad_norm": 2.573666572570801,
        "learning_rate": 3.0946523621543855e-05,
        "epoch": 1.829208558906935,
        "step": 14191
    },
    {
        "loss": 1.3367,
        "grad_norm": 2.714228868484497,
        "learning_rate": 3.0902530630777006e-05,
        "epoch": 1.8293374581077597,
        "step": 14192
    },
    {
        "loss": 0.798,
        "grad_norm": 2.8095123767852783,
        "learning_rate": 3.085856321676704e-05,
        "epoch": 1.8294663573085845,
        "step": 14193
    },
    {
        "loss": 1.7295,
        "grad_norm": 2.1844370365142822,
        "learning_rate": 3.0814621395788734e-05,
        "epoch": 1.8295952565094096,
        "step": 14194
    },
    {
        "loss": 1.9397,
        "grad_norm": 1.9650318622589111,
        "learning_rate": 3.077070518410734e-05,
        "epoch": 1.8297241557102346,
        "step": 14195
    },
    {
        "loss": 1.2856,
        "grad_norm": 2.454735279083252,
        "learning_rate": 3.072681459797863e-05,
        "epoch": 1.8298530549110597,
        "step": 14196
    },
    {
        "loss": 1.3945,
        "grad_norm": 2.4311776161193848,
        "learning_rate": 3.068294965364896e-05,
        "epoch": 1.8299819541118845,
        "step": 14197
    },
    {
        "loss": 1.7289,
        "grad_norm": 2.804346799850464,
        "learning_rate": 3.063911036735504e-05,
        "epoch": 1.8301108533127095,
        "step": 14198
    },
    {
        "loss": 1.6738,
        "grad_norm": 2.0790534019470215,
        "learning_rate": 3.0595296755324376e-05,
        "epoch": 1.8302397525135343,
        "step": 14199
    },
    {
        "loss": 1.7002,
        "grad_norm": 2.775512218475342,
        "learning_rate": 3.055150883377479e-05,
        "epoch": 1.8303686517143594,
        "step": 14200
    },
    {
        "loss": 1.0481,
        "grad_norm": 1.8525819778442383,
        "learning_rate": 3.0507746618914257e-05,
        "epoch": 1.8304975509151844,
        "step": 14201
    },
    {
        "loss": 1.9726,
        "grad_norm": 2.0458078384399414,
        "learning_rate": 3.0464010126941923e-05,
        "epoch": 1.8306264501160094,
        "step": 14202
    },
    {
        "loss": 1.3123,
        "grad_norm": 2.91426944732666,
        "learning_rate": 3.0420299374046946e-05,
        "epoch": 1.8307553493168343,
        "step": 14203
    },
    {
        "loss": 1.8204,
        "grad_norm": 2.460446357727051,
        "learning_rate": 3.037661437640895e-05,
        "epoch": 1.830884248517659,
        "step": 14204
    },
    {
        "loss": 1.7504,
        "grad_norm": 3.9101433753967285,
        "learning_rate": 3.033295515019836e-05,
        "epoch": 1.8310131477184841,
        "step": 14205
    },
    {
        "loss": 1.6656,
        "grad_norm": 2.8152029514312744,
        "learning_rate": 3.0289321711575824e-05,
        "epoch": 1.8311420469193092,
        "step": 14206
    },
    {
        "loss": 0.4224,
        "grad_norm": 2.0901267528533936,
        "learning_rate": 3.0245714076692168e-05,
        "epoch": 1.8312709461201342,
        "step": 14207
    },
    {
        "loss": 1.3285,
        "grad_norm": 3.152801752090454,
        "learning_rate": 3.02021322616893e-05,
        "epoch": 1.831399845320959,
        "step": 14208
    },
    {
        "loss": 1.2083,
        "grad_norm": 6.219654560089111,
        "learning_rate": 3.0158576282699114e-05,
        "epoch": 1.8315287445217838,
        "step": 14209
    },
    {
        "loss": 0.9449,
        "grad_norm": 3.772110939025879,
        "learning_rate": 3.0115046155844062e-05,
        "epoch": 1.8316576437226089,
        "step": 14210
    },
    {
        "loss": 2.0111,
        "grad_norm": 2.116885185241699,
        "learning_rate": 3.0071541897237045e-05,
        "epoch": 1.831786542923434,
        "step": 14211
    },
    {
        "loss": 2.4698,
        "grad_norm": 2.856682777404785,
        "learning_rate": 3.002806352298131e-05,
        "epoch": 1.831915442124259,
        "step": 14212
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.4492247104644775,
        "learning_rate": 2.9984611049170663e-05,
        "epoch": 1.8320443413250838,
        "step": 14213
    },
    {
        "loss": 2.3491,
        "grad_norm": 3.306647777557373,
        "learning_rate": 2.9941184491889207e-05,
        "epoch": 1.8321732405259086,
        "step": 14214
    },
    {
        "loss": 1.749,
        "grad_norm": 2.895301342010498,
        "learning_rate": 2.9897783867211404e-05,
        "epoch": 1.8323021397267336,
        "step": 14215
    },
    {
        "loss": 1.6359,
        "grad_norm": 2.782200336456299,
        "learning_rate": 2.985440919120238e-05,
        "epoch": 1.8324310389275587,
        "step": 14216
    },
    {
        "loss": 1.701,
        "grad_norm": 2.0710091590881348,
        "learning_rate": 2.9811060479917396e-05,
        "epoch": 1.8325599381283837,
        "step": 14217
    },
    {
        "loss": 2.0878,
        "grad_norm": 1.936712384223938,
        "learning_rate": 2.9767737749402135e-05,
        "epoch": 1.8326888373292085,
        "step": 14218
    },
    {
        "loss": 1.9365,
        "grad_norm": 2.490180253982544,
        "learning_rate": 2.9724441015692785e-05,
        "epoch": 1.8328177365300335,
        "step": 14219
    },
    {
        "loss": 1.6571,
        "grad_norm": 2.746744394302368,
        "learning_rate": 2.968117029481581e-05,
        "epoch": 1.8329466357308584,
        "step": 14220
    },
    {
        "loss": 1.3395,
        "grad_norm": 2.3878047466278076,
        "learning_rate": 2.9637925602787962e-05,
        "epoch": 1.8330755349316834,
        "step": 14221
    },
    {
        "loss": 1.5357,
        "grad_norm": 2.3222930431365967,
        "learning_rate": 2.959470695561668e-05,
        "epoch": 1.8332044341325084,
        "step": 14222
    },
    {
        "loss": 0.8245,
        "grad_norm": 2.98056960105896,
        "learning_rate": 2.9551514369299536e-05,
        "epoch": 1.8333333333333335,
        "step": 14223
    },
    {
        "loss": 1.9009,
        "grad_norm": 2.6037309169769287,
        "learning_rate": 2.950834785982416e-05,
        "epoch": 1.8334622325341583,
        "step": 14224
    },
    {
        "loss": 1.6213,
        "grad_norm": 2.4759392738342285,
        "learning_rate": 2.9465207443169196e-05,
        "epoch": 1.8335911317349831,
        "step": 14225
    },
    {
        "loss": 1.4873,
        "grad_norm": 1.5915013551712036,
        "learning_rate": 2.942209313530314e-05,
        "epoch": 1.8337200309358082,
        "step": 14226
    },
    {
        "loss": 1.9115,
        "grad_norm": 2.393035888671875,
        "learning_rate": 2.937900495218495e-05,
        "epoch": 1.8338489301366332,
        "step": 14227
    },
    {
        "loss": 1.4652,
        "grad_norm": 1.6326384544372559,
        "learning_rate": 2.9335942909763946e-05,
        "epoch": 1.8339778293374582,
        "step": 14228
    },
    {
        "loss": 1.3262,
        "grad_norm": 1.6254874467849731,
        "learning_rate": 2.9292907023979687e-05,
        "epoch": 1.834106728538283,
        "step": 14229
    },
    {
        "loss": 1.98,
        "grad_norm": 3.5895609855651855,
        "learning_rate": 2.9249897310762197e-05,
        "epoch": 1.8342356277391079,
        "step": 14230
    },
    {
        "loss": 1.4652,
        "grad_norm": 2.9143731594085693,
        "learning_rate": 2.9206913786031686e-05,
        "epoch": 1.834364526939933,
        "step": 14231
    },
    {
        "loss": 1.1486,
        "grad_norm": 3.5397422313690186,
        "learning_rate": 2.9163956465698616e-05,
        "epoch": 1.834493426140758,
        "step": 14232
    },
    {
        "loss": 2.1348,
        "grad_norm": 1.8850911855697632,
        "learning_rate": 2.9121025365664025e-05,
        "epoch": 1.834622325341583,
        "step": 14233
    },
    {
        "loss": 1.784,
        "grad_norm": 3.320117235183716,
        "learning_rate": 2.907812050181895e-05,
        "epoch": 1.8347512245424078,
        "step": 14234
    },
    {
        "loss": 1.7025,
        "grad_norm": 2.9542486667633057,
        "learning_rate": 2.9035241890044886e-05,
        "epoch": 1.8348801237432328,
        "step": 14235
    },
    {
        "loss": 1.3515,
        "grad_norm": 2.4436557292938232,
        "learning_rate": 2.8992389546213526e-05,
        "epoch": 1.8350090229440577,
        "step": 14236
    },
    {
        "loss": 1.9124,
        "grad_norm": 2.27441143989563,
        "learning_rate": 2.894956348618687e-05,
        "epoch": 1.8351379221448827,
        "step": 14237
    },
    {
        "loss": 2.3044,
        "grad_norm": 1.81438410282135,
        "learning_rate": 2.89067637258171e-05,
        "epoch": 1.8352668213457077,
        "step": 14238
    },
    {
        "loss": 1.663,
        "grad_norm": 2.73643159866333,
        "learning_rate": 2.8863990280946886e-05,
        "epoch": 1.8353957205465328,
        "step": 14239
    },
    {
        "loss": 2.0575,
        "grad_norm": 2.703918695449829,
        "learning_rate": 2.8821243167409007e-05,
        "epoch": 1.8355246197473576,
        "step": 14240
    },
    {
        "loss": 2.1775,
        "grad_norm": 2.78511381149292,
        "learning_rate": 2.8778522401026488e-05,
        "epoch": 1.8356535189481824,
        "step": 14241
    },
    {
        "loss": 2.4388,
        "grad_norm": 1.9626268148422241,
        "learning_rate": 2.8735827997612597e-05,
        "epoch": 1.8357824181490074,
        "step": 14242
    },
    {
        "loss": 1.7873,
        "grad_norm": 1.4738950729370117,
        "learning_rate": 2.8693159972970896e-05,
        "epoch": 1.8359113173498325,
        "step": 14243
    },
    {
        "loss": 2.152,
        "grad_norm": 2.5443227291107178,
        "learning_rate": 2.8650518342895138e-05,
        "epoch": 1.8360402165506575,
        "step": 14244
    },
    {
        "loss": 1.5123,
        "grad_norm": 1.9356080293655396,
        "learning_rate": 2.860790312316931e-05,
        "epoch": 1.8361691157514823,
        "step": 14245
    },
    {
        "loss": 1.308,
        "grad_norm": 2.1216635704040527,
        "learning_rate": 2.8565314329567693e-05,
        "epoch": 1.8362980149523072,
        "step": 14246
    },
    {
        "loss": 1.4002,
        "grad_norm": 2.4355521202087402,
        "learning_rate": 2.85227519778547e-05,
        "epoch": 1.8364269141531322,
        "step": 14247
    },
    {
        "loss": 1.5713,
        "grad_norm": 2.361584186553955,
        "learning_rate": 2.8480216083784995e-05,
        "epoch": 1.8365558133539572,
        "step": 14248
    },
    {
        "loss": 1.5606,
        "grad_norm": 3.240253448486328,
        "learning_rate": 2.8437706663103338e-05,
        "epoch": 1.8366847125547823,
        "step": 14249
    },
    {
        "loss": 0.6252,
        "grad_norm": 1.902566909790039,
        "learning_rate": 2.8395223731544974e-05,
        "epoch": 1.836813611755607,
        "step": 14250
    },
    {
        "loss": 2.0176,
        "grad_norm": 3.891390323638916,
        "learning_rate": 2.835276730483506e-05,
        "epoch": 1.836942510956432,
        "step": 14251
    },
    {
        "loss": 2.0623,
        "grad_norm": 3.048252582550049,
        "learning_rate": 2.831033739868909e-05,
        "epoch": 1.837071410157257,
        "step": 14252
    },
    {
        "loss": 0.7962,
        "grad_norm": 3.761957883834839,
        "learning_rate": 2.826793402881267e-05,
        "epoch": 1.837200309358082,
        "step": 14253
    },
    {
        "loss": 0.6984,
        "grad_norm": 2.0945510864257812,
        "learning_rate": 2.8225557210901597e-05,
        "epoch": 1.837329208558907,
        "step": 14254
    },
    {
        "loss": 2.2028,
        "grad_norm": 2.901656150817871,
        "learning_rate": 2.8183206960641785e-05,
        "epoch": 1.8374581077597318,
        "step": 14255
    },
    {
        "loss": 1.5115,
        "grad_norm": 2.1467528343200684,
        "learning_rate": 2.814088329370951e-05,
        "epoch": 1.8375870069605569,
        "step": 14256
    },
    {
        "loss": 1.5311,
        "grad_norm": 2.3591501712799072,
        "learning_rate": 2.8098586225771063e-05,
        "epoch": 1.8377159061613817,
        "step": 14257
    },
    {
        "loss": 1.5297,
        "grad_norm": 1.7155587673187256,
        "learning_rate": 2.8056315772482866e-05,
        "epoch": 1.8378448053622067,
        "step": 14258
    },
    {
        "loss": 1.1107,
        "grad_norm": 2.7334072589874268,
        "learning_rate": 2.8014071949491527e-05,
        "epoch": 1.8379737045630318,
        "step": 14259
    },
    {
        "loss": 1.5622,
        "grad_norm": 2.675524950027466,
        "learning_rate": 2.7971854772433803e-05,
        "epoch": 1.8381026037638568,
        "step": 14260
    },
    {
        "loss": 2.1544,
        "grad_norm": 1.7420916557312012,
        "learning_rate": 2.792966425693658e-05,
        "epoch": 1.8382315029646816,
        "step": 14261
    },
    {
        "loss": 1.2329,
        "grad_norm": 2.1594929695129395,
        "learning_rate": 2.7887500418616863e-05,
        "epoch": 1.8383604021655064,
        "step": 14262
    },
    {
        "loss": 2.0484,
        "grad_norm": 2.805203437805176,
        "learning_rate": 2.7845363273081825e-05,
        "epoch": 1.8384893013663315,
        "step": 14263
    },
    {
        "loss": 0.8316,
        "grad_norm": 2.046485185623169,
        "learning_rate": 2.780325283592873e-05,
        "epoch": 1.8386182005671565,
        "step": 14264
    },
    {
        "loss": 1.7639,
        "grad_norm": 1.389776587486267,
        "learning_rate": 2.7761169122744947e-05,
        "epoch": 1.8387470997679816,
        "step": 14265
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.4708406925201416,
        "learning_rate": 2.771911214910786e-05,
        "epoch": 1.8388759989688064,
        "step": 14266
    },
    {
        "loss": 1.6242,
        "grad_norm": 2.9341654777526855,
        "learning_rate": 2.7677081930585237e-05,
        "epoch": 1.8390048981696312,
        "step": 14267
    },
    {
        "loss": 1.184,
        "grad_norm": 3.15929913520813,
        "learning_rate": 2.76350784827347e-05,
        "epoch": 1.8391337973704562,
        "step": 14268
    },
    {
        "loss": 1.8881,
        "grad_norm": 1.805866003036499,
        "learning_rate": 2.7593101821104016e-05,
        "epoch": 1.8392626965712813,
        "step": 14269
    },
    {
        "loss": 0.7104,
        "grad_norm": 3.504683017730713,
        "learning_rate": 2.7551151961231048e-05,
        "epoch": 1.8393915957721063,
        "step": 14270
    },
    {
        "loss": 1.4513,
        "grad_norm": 2.9948065280914307,
        "learning_rate": 2.7509228918643715e-05,
        "epoch": 1.8395204949729311,
        "step": 14271
    },
    {
        "loss": 2.1863,
        "grad_norm": 2.5509138107299805,
        "learning_rate": 2.7467332708859927e-05,
        "epoch": 1.8396493941737562,
        "step": 14272
    },
    {
        "loss": 1.1386,
        "grad_norm": 1.3760881423950195,
        "learning_rate": 2.7425463347388014e-05,
        "epoch": 1.839778293374581,
        "step": 14273
    },
    {
        "loss": 1.0644,
        "grad_norm": 1.9953075647354126,
        "learning_rate": 2.7383620849725978e-05,
        "epoch": 1.839907192575406,
        "step": 14274
    },
    {
        "loss": 1.5385,
        "grad_norm": 2.944125175476074,
        "learning_rate": 2.734180523136203e-05,
        "epoch": 1.840036091776231,
        "step": 14275
    },
    {
        "loss": 2.0669,
        "grad_norm": 1.3392270803451538,
        "learning_rate": 2.730001650777442e-05,
        "epoch": 1.840164990977056,
        "step": 14276
    },
    {
        "loss": 1.5606,
        "grad_norm": 2.6340346336364746,
        "learning_rate": 2.725825469443144e-05,
        "epoch": 1.840293890177881,
        "step": 14277
    },
    {
        "loss": 0.647,
        "grad_norm": 2.7508418560028076,
        "learning_rate": 2.7216519806791396e-05,
        "epoch": 1.8404227893787057,
        "step": 14278
    },
    {
        "loss": 1.9591,
        "grad_norm": 1.7553831338882446,
        "learning_rate": 2.7174811860302717e-05,
        "epoch": 1.8405516885795308,
        "step": 14279
    },
    {
        "loss": 1.2522,
        "grad_norm": 1.8225497007369995,
        "learning_rate": 2.7133130870403665e-05,
        "epoch": 1.8406805877803558,
        "step": 14280
    },
    {
        "loss": 1.5866,
        "grad_norm": 2.346832036972046,
        "learning_rate": 2.7091476852522945e-05,
        "epoch": 1.8408094869811809,
        "step": 14281
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.2118778228759766,
        "learning_rate": 2.704984982207871e-05,
        "epoch": 1.8409383861820057,
        "step": 14282
    },
    {
        "loss": 1.7604,
        "grad_norm": 1.8704025745391846,
        "learning_rate": 2.7008249794479402e-05,
        "epoch": 1.8410672853828305,
        "step": 14283
    },
    {
        "loss": 1.6886,
        "grad_norm": 2.9548332691192627,
        "learning_rate": 2.6966676785123622e-05,
        "epoch": 1.8411961845836555,
        "step": 14284
    },
    {
        "loss": 1.1479,
        "grad_norm": 2.606353998184204,
        "learning_rate": 2.692513080939979e-05,
        "epoch": 1.8413250837844806,
        "step": 14285
    },
    {
        "loss": 1.3529,
        "grad_norm": 1.886664628982544,
        "learning_rate": 2.688361188268631e-05,
        "epoch": 1.8414539829853056,
        "step": 14286
    },
    {
        "loss": 1.3898,
        "grad_norm": 2.779465436935425,
        "learning_rate": 2.6842120020351623e-05,
        "epoch": 1.8415828821861304,
        "step": 14287
    },
    {
        "loss": 1.582,
        "grad_norm": 3.667851448059082,
        "learning_rate": 2.6800655237754124e-05,
        "epoch": 1.8417117813869552,
        "step": 14288
    },
    {
        "loss": 1.3728,
        "grad_norm": 3.4173009395599365,
        "learning_rate": 2.675921755024208e-05,
        "epoch": 1.8418406805877803,
        "step": 14289
    },
    {
        "loss": 0.9291,
        "grad_norm": 3.0620059967041016,
        "learning_rate": 2.6717806973154093e-05,
        "epoch": 1.8419695797886053,
        "step": 14290
    },
    {
        "loss": 1.4334,
        "grad_norm": 2.651740074157715,
        "learning_rate": 2.667642352181837e-05,
        "epoch": 1.8420984789894304,
        "step": 14291
    },
    {
        "loss": 0.6648,
        "grad_norm": 2.49625825881958,
        "learning_rate": 2.663506721155319e-05,
        "epoch": 1.8422273781902552,
        "step": 14292
    },
    {
        "loss": 2.0232,
        "grad_norm": 1.8827097415924072,
        "learning_rate": 2.6593738057666772e-05,
        "epoch": 1.8423562773910802,
        "step": 14293
    },
    {
        "loss": 1.0243,
        "grad_norm": 2.27396821975708,
        "learning_rate": 2.6552436075457322e-05,
        "epoch": 1.842485176591905,
        "step": 14294
    },
    {
        "loss": 0.6046,
        "grad_norm": 1.2387301921844482,
        "learning_rate": 2.6511161280212927e-05,
        "epoch": 1.84261407579273,
        "step": 14295
    },
    {
        "loss": 2.1815,
        "grad_norm": 1.6092418432235718,
        "learning_rate": 2.64699136872117e-05,
        "epoch": 1.842742974993555,
        "step": 14296
    },
    {
        "loss": 1.2339,
        "grad_norm": 1.5982239246368408,
        "learning_rate": 2.6428693311721532e-05,
        "epoch": 1.8428718741943801,
        "step": 14297
    },
    {
        "loss": 1.3324,
        "grad_norm": 2.19461989402771,
        "learning_rate": 2.638750016900059e-05,
        "epoch": 1.843000773395205,
        "step": 14298
    },
    {
        "loss": 2.2714,
        "grad_norm": 2.1501777172088623,
        "learning_rate": 2.6346334274296457e-05,
        "epoch": 1.8431296725960298,
        "step": 14299
    },
    {
        "loss": 1.9743,
        "grad_norm": 1.643486738204956,
        "learning_rate": 2.6305195642846846e-05,
        "epoch": 1.8432585717968548,
        "step": 14300
    },
    {
        "loss": 0.5609,
        "grad_norm": 1.9448747634887695,
        "learning_rate": 2.6264084289879663e-05,
        "epoch": 1.8433874709976799,
        "step": 14301
    },
    {
        "loss": 1.3694,
        "grad_norm": 2.971181631088257,
        "learning_rate": 2.622300023061235e-05,
        "epoch": 1.843516370198505,
        "step": 14302
    },
    {
        "loss": 1.8681,
        "grad_norm": 1.856537103652954,
        "learning_rate": 2.618194348025227e-05,
        "epoch": 1.8436452693993297,
        "step": 14303
    },
    {
        "loss": 1.4108,
        "grad_norm": 2.9745776653289795,
        "learning_rate": 2.6140914053997057e-05,
        "epoch": 1.8437741686001545,
        "step": 14304
    },
    {
        "loss": 0.9554,
        "grad_norm": 2.826589584350586,
        "learning_rate": 2.6099911967033664e-05,
        "epoch": 1.8439030678009796,
        "step": 14305
    },
    {
        "loss": 1.5018,
        "grad_norm": 2.254730224609375,
        "learning_rate": 2.605893723453918e-05,
        "epoch": 1.8440319670018046,
        "step": 14306
    },
    {
        "loss": 2.1364,
        "grad_norm": 1.630790114402771,
        "learning_rate": 2.6017989871680858e-05,
        "epoch": 1.8441608662026296,
        "step": 14307
    },
    {
        "loss": 0.6525,
        "grad_norm": 2.2709755897521973,
        "learning_rate": 2.5977069893615413e-05,
        "epoch": 1.8442897654034545,
        "step": 14308
    },
    {
        "loss": 1.9939,
        "grad_norm": 2.542022943496704,
        "learning_rate": 2.5936177315489586e-05,
        "epoch": 1.8444186646042795,
        "step": 14309
    },
    {
        "loss": 1.5516,
        "grad_norm": 1.8474617004394531,
        "learning_rate": 2.5895312152439942e-05,
        "epoch": 1.8445475638051043,
        "step": 14310
    },
    {
        "loss": 1.922,
        "grad_norm": 1.6556705236434937,
        "learning_rate": 2.5854474419592912e-05,
        "epoch": 1.8446764630059294,
        "step": 14311
    },
    {
        "loss": 1.7425,
        "grad_norm": 2.6856682300567627,
        "learning_rate": 2.5813664132064808e-05,
        "epoch": 1.8448053622067544,
        "step": 14312
    },
    {
        "loss": 1.6794,
        "grad_norm": 3.1860594749450684,
        "learning_rate": 2.5772881304961737e-05,
        "epoch": 1.8449342614075794,
        "step": 14313
    },
    {
        "loss": 1.3045,
        "grad_norm": 2.072587251663208,
        "learning_rate": 2.5732125953379573e-05,
        "epoch": 1.8450631606084043,
        "step": 14314
    },
    {
        "loss": 1.2393,
        "grad_norm": 1.9991003274917603,
        "learning_rate": 2.5691398092404363e-05,
        "epoch": 1.845192059809229,
        "step": 14315
    },
    {
        "loss": 2.195,
        "grad_norm": 1.842750906944275,
        "learning_rate": 2.5650697737111452e-05,
        "epoch": 1.845320959010054,
        "step": 14316
    },
    {
        "loss": 1.2381,
        "grad_norm": 1.967689037322998,
        "learning_rate": 2.5610024902566233e-05,
        "epoch": 1.8454498582108791,
        "step": 14317
    },
    {
        "loss": 2.0703,
        "grad_norm": 2.5221879482269287,
        "learning_rate": 2.5569379603824196e-05,
        "epoch": 1.8455787574117042,
        "step": 14318
    },
    {
        "loss": 2.1902,
        "grad_norm": 2.6709794998168945,
        "learning_rate": 2.552876185593027e-05,
        "epoch": 1.845707656612529,
        "step": 14319
    },
    {
        "loss": 2.0933,
        "grad_norm": 2.974510431289673,
        "learning_rate": 2.5488171673919225e-05,
        "epoch": 1.8458365558133538,
        "step": 14320
    },
    {
        "loss": 1.529,
        "grad_norm": 2.7267863750457764,
        "learning_rate": 2.544760907281597e-05,
        "epoch": 1.8459654550141789,
        "step": 14321
    },
    {
        "loss": 1.3758,
        "grad_norm": 3.590075731277466,
        "learning_rate": 2.5407074067634674e-05,
        "epoch": 1.846094354215004,
        "step": 14322
    },
    {
        "loss": 2.139,
        "grad_norm": 2.5061638355255127,
        "learning_rate": 2.5366566673379544e-05,
        "epoch": 1.846223253415829,
        "step": 14323
    },
    {
        "loss": 1.642,
        "grad_norm": 2.6843786239624023,
        "learning_rate": 2.532608690504481e-05,
        "epoch": 1.8463521526166538,
        "step": 14324
    },
    {
        "loss": 1.6079,
        "grad_norm": 3.0708959102630615,
        "learning_rate": 2.5285634777614132e-05,
        "epoch": 1.8464810518174786,
        "step": 14325
    },
    {
        "loss": 1.6984,
        "grad_norm": 2.760404586791992,
        "learning_rate": 2.5245210306061072e-05,
        "epoch": 1.8466099510183036,
        "step": 14326
    },
    {
        "loss": 1.5351,
        "grad_norm": 3.0846261978149414,
        "learning_rate": 2.520481350534892e-05,
        "epoch": 1.8467388502191286,
        "step": 14327
    },
    {
        "loss": 0.9053,
        "grad_norm": 3.191714286804199,
        "learning_rate": 2.5164444390430743e-05,
        "epoch": 1.8468677494199537,
        "step": 14328
    },
    {
        "loss": 1.7046,
        "grad_norm": 7.568283557891846,
        "learning_rate": 2.5124102976249386e-05,
        "epoch": 1.8469966486207785,
        "step": 14329
    },
    {
        "loss": 2.0082,
        "grad_norm": 2.9926679134368896,
        "learning_rate": 2.5083789277737403e-05,
        "epoch": 1.8471255478216035,
        "step": 14330
    },
    {
        "loss": 1.1913,
        "grad_norm": 2.75791597366333,
        "learning_rate": 2.504350330981702e-05,
        "epoch": 1.8472544470224284,
        "step": 14331
    },
    {
        "loss": 1.3014,
        "grad_norm": 3.164973258972168,
        "learning_rate": 2.500324508740053e-05,
        "epoch": 1.8473833462232534,
        "step": 14332
    },
    {
        "loss": 2.0851,
        "grad_norm": 1.9397659301757812,
        "learning_rate": 2.4963014625389446e-05,
        "epoch": 1.8475122454240784,
        "step": 14333
    },
    {
        "loss": 1.695,
        "grad_norm": 2.3964977264404297,
        "learning_rate": 2.492281193867525e-05,
        "epoch": 1.8476411446249035,
        "step": 14334
    },
    {
        "loss": 0.8252,
        "grad_norm": 2.746978759765625,
        "learning_rate": 2.4882637042139366e-05,
        "epoch": 1.8477700438257283,
        "step": 14335
    },
    {
        "loss": 0.9634,
        "grad_norm": 3.1037659645080566,
        "learning_rate": 2.4842489950652613e-05,
        "epoch": 1.847898943026553,
        "step": 14336
    },
    {
        "loss": 1.7541,
        "grad_norm": 2.2724156379699707,
        "learning_rate": 2.480237067907556e-05,
        "epoch": 1.8480278422273781,
        "step": 14337
    },
    {
        "loss": 1.3109,
        "grad_norm": 2.2240610122680664,
        "learning_rate": 2.476227924225879e-05,
        "epoch": 1.8481567414282032,
        "step": 14338
    },
    {
        "loss": 2.0186,
        "grad_norm": 2.266256332397461,
        "learning_rate": 2.4722215655042047e-05,
        "epoch": 1.8482856406290282,
        "step": 14339
    },
    {
        "loss": 1.7192,
        "grad_norm": 3.8738036155700684,
        "learning_rate": 2.4682179932255132e-05,
        "epoch": 1.848414539829853,
        "step": 14340
    },
    {
        "loss": 0.6029,
        "grad_norm": 2.423269748687744,
        "learning_rate": 2.4642172088717587e-05,
        "epoch": 1.8485434390306779,
        "step": 14341
    },
    {
        "loss": 1.805,
        "grad_norm": 2.183180809020996,
        "learning_rate": 2.460219213923842e-05,
        "epoch": 1.848672338231503,
        "step": 14342
    },
    {
        "loss": 2.1726,
        "grad_norm": 2.6974828243255615,
        "learning_rate": 2.4562240098616417e-05,
        "epoch": 1.848801237432328,
        "step": 14343
    },
    {
        "loss": 1.2859,
        "grad_norm": 2.2189347743988037,
        "learning_rate": 2.4522315981639966e-05,
        "epoch": 1.848930136633153,
        "step": 14344
    },
    {
        "loss": 1.5788,
        "grad_norm": 1.815394639968872,
        "learning_rate": 2.448241980308723e-05,
        "epoch": 1.8490590358339778,
        "step": 14345
    },
    {
        "loss": 1.1645,
        "grad_norm": 2.459836959838867,
        "learning_rate": 2.444255157772596e-05,
        "epoch": 1.8491879350348028,
        "step": 14346
    },
    {
        "loss": 1.7156,
        "grad_norm": 2.1710517406463623,
        "learning_rate": 2.440271132031354e-05,
        "epoch": 1.8493168342356276,
        "step": 14347
    },
    {
        "loss": 0.9126,
        "grad_norm": 2.752819061279297,
        "learning_rate": 2.4362899045596967e-05,
        "epoch": 1.8494457334364527,
        "step": 14348
    },
    {
        "loss": 2.1601,
        "grad_norm": 1.8241989612579346,
        "learning_rate": 2.4323114768313194e-05,
        "epoch": 1.8495746326372777,
        "step": 14349
    },
    {
        "loss": 0.9946,
        "grad_norm": 3.353196859359741,
        "learning_rate": 2.4283358503188268e-05,
        "epoch": 1.8497035318381028,
        "step": 14350
    },
    {
        "loss": 2.4179,
        "grad_norm": 1.3899648189544678,
        "learning_rate": 2.4243630264938223e-05,
        "epoch": 1.8498324310389276,
        "step": 14351
    },
    {
        "loss": 1.0095,
        "grad_norm": 2.3136770725250244,
        "learning_rate": 2.4203930068268798e-05,
        "epoch": 1.8499613302397524,
        "step": 14352
    },
    {
        "loss": 0.5309,
        "grad_norm": 2.403564214706421,
        "learning_rate": 2.4164257927875135e-05,
        "epoch": 1.8500902294405774,
        "step": 14353
    },
    {
        "loss": 0.9146,
        "grad_norm": 1.957707405090332,
        "learning_rate": 2.4124613858441958e-05,
        "epoch": 1.8502191286414025,
        "step": 14354
    },
    {
        "loss": 1.3728,
        "grad_norm": 2.474912643432617,
        "learning_rate": 2.4084997874643977e-05,
        "epoch": 1.8503480278422275,
        "step": 14355
    },
    {
        "loss": 1.6381,
        "grad_norm": 2.418086290359497,
        "learning_rate": 2.404540999114495e-05,
        "epoch": 1.8504769270430523,
        "step": 14356
    },
    {
        "loss": 1.9757,
        "grad_norm": 2.0750348567962646,
        "learning_rate": 2.400585022259858e-05,
        "epoch": 1.8506058262438772,
        "step": 14357
    },
    {
        "loss": 2.1643,
        "grad_norm": 1.909576416015625,
        "learning_rate": 2.396631858364824e-05,
        "epoch": 1.8507347254447022,
        "step": 14358
    },
    {
        "loss": 1.8441,
        "grad_norm": 2.3358280658721924,
        "learning_rate": 2.392681508892667e-05,
        "epoch": 1.8508636246455272,
        "step": 14359
    },
    {
        "loss": 0.8118,
        "grad_norm": 3.821519136428833,
        "learning_rate": 2.3887339753056282e-05,
        "epoch": 1.8509925238463523,
        "step": 14360
    },
    {
        "loss": 0.8732,
        "grad_norm": 2.4596478939056396,
        "learning_rate": 2.3847892590649024e-05,
        "epoch": 1.851121423047177,
        "step": 14361
    },
    {
        "loss": 1.6813,
        "grad_norm": 2.0066606998443604,
        "learning_rate": 2.3808473616306516e-05,
        "epoch": 1.851250322248002,
        "step": 14362
    },
    {
        "loss": 2.3385,
        "grad_norm": 2.346074104309082,
        "learning_rate": 2.376908284461985e-05,
        "epoch": 1.851379221448827,
        "step": 14363
    },
    {
        "loss": 1.2988,
        "grad_norm": 1.8350236415863037,
        "learning_rate": 2.372972029016969e-05,
        "epoch": 1.851508120649652,
        "step": 14364
    },
    {
        "loss": 1.5363,
        "grad_norm": 2.7343852519989014,
        "learning_rate": 2.3690385967526208e-05,
        "epoch": 1.851637019850477,
        "step": 14365
    },
    {
        "loss": 1.5808,
        "grad_norm": 1.594384789466858,
        "learning_rate": 2.3651079891249318e-05,
        "epoch": 1.8517659190513018,
        "step": 14366
    },
    {
        "loss": 0.5901,
        "grad_norm": 3.951815605163574,
        "learning_rate": 2.361180207588842e-05,
        "epoch": 1.8518948182521269,
        "step": 14367
    },
    {
        "loss": 1.0567,
        "grad_norm": 1.784127950668335,
        "learning_rate": 2.3572552535982094e-05,
        "epoch": 1.8520237174529517,
        "step": 14368
    },
    {
        "loss": 1.3733,
        "grad_norm": 1.8003917932510376,
        "learning_rate": 2.3533331286059003e-05,
        "epoch": 1.8521526166537767,
        "step": 14369
    },
    {
        "loss": 1.4809,
        "grad_norm": 2.6616673469543457,
        "learning_rate": 2.3494138340636983e-05,
        "epoch": 1.8522815158546018,
        "step": 14370
    },
    {
        "loss": 1.598,
        "grad_norm": 3.1202409267425537,
        "learning_rate": 2.3454973714223393e-05,
        "epoch": 1.8524104150554268,
        "step": 14371
    },
    {
        "loss": 1.549,
        "grad_norm": 2.4744646549224854,
        "learning_rate": 2.341583742131551e-05,
        "epoch": 1.8525393142562516,
        "step": 14372
    },
    {
        "loss": 2.288,
        "grad_norm": 2.3317573070526123,
        "learning_rate": 2.3376729476399528e-05,
        "epoch": 1.8526682134570764,
        "step": 14373
    },
    {
        "loss": 2.2541,
        "grad_norm": 2.9332363605499268,
        "learning_rate": 2.333764989395144e-05,
        "epoch": 1.8527971126579015,
        "step": 14374
    },
    {
        "loss": 0.4507,
        "grad_norm": 4.175892353057861,
        "learning_rate": 2.3298598688436923e-05,
        "epoch": 1.8529260118587265,
        "step": 14375
    },
    {
        "loss": 1.0375,
        "grad_norm": 1.8471782207489014,
        "learning_rate": 2.325957587431088e-05,
        "epoch": 1.8530549110595516,
        "step": 14376
    },
    {
        "loss": 1.5885,
        "grad_norm": 1.8957761526107788,
        "learning_rate": 2.3220581466017745e-05,
        "epoch": 1.8531838102603764,
        "step": 14377
    },
    {
        "loss": 0.5841,
        "grad_norm": 1.9181321859359741,
        "learning_rate": 2.3181615477991547e-05,
        "epoch": 1.8533127094612012,
        "step": 14378
    },
    {
        "loss": 1.1704,
        "grad_norm": 3.3350908756256104,
        "learning_rate": 2.314267792465571e-05,
        "epoch": 1.8534416086620262,
        "step": 14379
    },
    {
        "loss": 1.9018,
        "grad_norm": 1.6238813400268555,
        "learning_rate": 2.3103768820423156e-05,
        "epoch": 1.8535705078628513,
        "step": 14380
    },
    {
        "loss": 0.8377,
        "grad_norm": 2.136296272277832,
        "learning_rate": 2.3064888179696265e-05,
        "epoch": 1.8536994070636763,
        "step": 14381
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.4811060428619385,
        "learning_rate": 2.3026036016866816e-05,
        "epoch": 1.8538283062645011,
        "step": 14382
    },
    {
        "loss": 1.5572,
        "grad_norm": 2.485130786895752,
        "learning_rate": 2.2987212346316266e-05,
        "epoch": 1.8539572054653262,
        "step": 14383
    },
    {
        "loss": 1.654,
        "grad_norm": 2.5545847415924072,
        "learning_rate": 2.2948417182415415e-05,
        "epoch": 1.854086104666151,
        "step": 14384
    },
    {
        "loss": 2.0268,
        "grad_norm": 1.9512521028518677,
        "learning_rate": 2.290965053952423e-05,
        "epoch": 1.854215003866976,
        "step": 14385
    },
    {
        "loss": 1.2514,
        "grad_norm": 1.9317964315414429,
        "learning_rate": 2.2870912431992588e-05,
        "epoch": 1.854343903067801,
        "step": 14386
    },
    {
        "loss": 1.8288,
        "grad_norm": 2.646373748779297,
        "learning_rate": 2.2832202874159527e-05,
        "epoch": 1.854472802268626,
        "step": 14387
    },
    {
        "loss": 1.9239,
        "grad_norm": 2.4711334705352783,
        "learning_rate": 2.279352188035344e-05,
        "epoch": 1.854601701469451,
        "step": 14388
    },
    {
        "loss": 0.9679,
        "grad_norm": 3.1475343704223633,
        "learning_rate": 2.2754869464892593e-05,
        "epoch": 1.8547306006702757,
        "step": 14389
    },
    {
        "loss": 1.2006,
        "grad_norm": 2.127204418182373,
        "learning_rate": 2.27162456420841e-05,
        "epoch": 1.8548594998711008,
        "step": 14390
    },
    {
        "loss": 0.9822,
        "grad_norm": 2.2175447940826416,
        "learning_rate": 2.2677650426224727e-05,
        "epoch": 1.8549883990719258,
        "step": 14391
    },
    {
        "loss": 1.9913,
        "grad_norm": 2.58949613571167,
        "learning_rate": 2.2639083831600873e-05,
        "epoch": 1.8551172982727508,
        "step": 14392
    },
    {
        "loss": 2.2715,
        "grad_norm": 1.993971347808838,
        "learning_rate": 2.2600545872488034e-05,
        "epoch": 1.8552461974735757,
        "step": 14393
    },
    {
        "loss": 1.5647,
        "grad_norm": 2.260704278945923,
        "learning_rate": 2.25620365631512e-05,
        "epoch": 1.8553750966744005,
        "step": 14394
    },
    {
        "loss": 1.4798,
        "grad_norm": 1.8796495199203491,
        "learning_rate": 2.252355591784484e-05,
        "epoch": 1.8555039958752255,
        "step": 14395
    },
    {
        "loss": 0.6409,
        "grad_norm": 1.6126798391342163,
        "learning_rate": 2.2485103950812712e-05,
        "epoch": 1.8556328950760506,
        "step": 14396
    },
    {
        "loss": 1.7501,
        "grad_norm": 2.0353891849517822,
        "learning_rate": 2.2446680676288e-05,
        "epoch": 1.8557617942768756,
        "step": 14397
    },
    {
        "loss": 1.7352,
        "grad_norm": 2.537665605545044,
        "learning_rate": 2.240828610849326e-05,
        "epoch": 1.8558906934777004,
        "step": 14398
    },
    {
        "loss": 1.2739,
        "grad_norm": 2.452430009841919,
        "learning_rate": 2.236992026164031e-05,
        "epoch": 1.8560195926785252,
        "step": 14399
    },
    {
        "loss": 1.1642,
        "grad_norm": 2.6320180892944336,
        "learning_rate": 2.2331583149930668e-05,
        "epoch": 1.8561484918793503,
        "step": 14400
    },
    {
        "loss": 1.6356,
        "grad_norm": 1.255914568901062,
        "learning_rate": 2.2293274787554986e-05,
        "epoch": 1.8562773910801753,
        "step": 14401
    },
    {
        "loss": 1.8944,
        "grad_norm": 2.903409242630005,
        "learning_rate": 2.2254995188693017e-05,
        "epoch": 1.8564062902810003,
        "step": 14402
    },
    {
        "loss": 1.4166,
        "grad_norm": 2.2197036743164062,
        "learning_rate": 2.2216744367514396e-05,
        "epoch": 1.8565351894818252,
        "step": 14403
    },
    {
        "loss": 1.1563,
        "grad_norm": 3.2186315059661865,
        "learning_rate": 2.2178522338177777e-05,
        "epoch": 1.8566640886826502,
        "step": 14404
    },
    {
        "loss": 1.7558,
        "grad_norm": 1.8882427215576172,
        "learning_rate": 2.2140329114831082e-05,
        "epoch": 1.856792987883475,
        "step": 14405
    },
    {
        "loss": 1.9668,
        "grad_norm": 1.8481439352035522,
        "learning_rate": 2.2102164711611974e-05,
        "epoch": 1.8569218870843,
        "step": 14406
    },
    {
        "loss": 1.5719,
        "grad_norm": 2.326934337615967,
        "learning_rate": 2.2064029142647135e-05,
        "epoch": 1.857050786285125,
        "step": 14407
    },
    {
        "loss": 1.9378,
        "grad_norm": 2.7158000469207764,
        "learning_rate": 2.202592242205238e-05,
        "epoch": 1.8571796854859501,
        "step": 14408
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.7175853252410889,
        "learning_rate": 2.1987844563933357e-05,
        "epoch": 1.857308584686775,
        "step": 14409
    },
    {
        "loss": 1.1619,
        "grad_norm": 2.912964105606079,
        "learning_rate": 2.1949795582384636e-05,
        "epoch": 1.8574374838875998,
        "step": 14410
    },
    {
        "loss": 0.9567,
        "grad_norm": 3.5600321292877197,
        "learning_rate": 2.1911775491490292e-05,
        "epoch": 1.8575663830884248,
        "step": 14411
    },
    {
        "loss": 1.8218,
        "grad_norm": 1.826940894126892,
        "learning_rate": 2.187378430532363e-05,
        "epoch": 1.8576952822892498,
        "step": 14412
    },
    {
        "loss": 0.8616,
        "grad_norm": 2.38043212890625,
        "learning_rate": 2.183582203794725e-05,
        "epoch": 1.8578241814900749,
        "step": 14413
    },
    {
        "loss": 0.8705,
        "grad_norm": 2.0724120140075684,
        "learning_rate": 2.179788870341306e-05,
        "epoch": 1.8579530806908997,
        "step": 14414
    },
    {
        "loss": 0.7838,
        "grad_norm": 2.1082866191864014,
        "learning_rate": 2.1759984315762283e-05,
        "epoch": 1.8580819798917245,
        "step": 14415
    },
    {
        "loss": 1.4889,
        "grad_norm": 1.7542980909347534,
        "learning_rate": 2.1722108889025295e-05,
        "epoch": 1.8582108790925496,
        "step": 14416
    },
    {
        "loss": 1.3466,
        "grad_norm": 1.6965395212173462,
        "learning_rate": 2.168426243722207e-05,
        "epoch": 1.8583397782933746,
        "step": 14417
    },
    {
        "loss": 1.4716,
        "grad_norm": 4.193633556365967,
        "learning_rate": 2.1646444974361634e-05,
        "epoch": 1.8584686774941996,
        "step": 14418
    },
    {
        "loss": 1.9681,
        "grad_norm": 2.0750060081481934,
        "learning_rate": 2.160865651444206e-05,
        "epoch": 1.8585975766950245,
        "step": 14419
    },
    {
        "loss": 1.0031,
        "grad_norm": 2.747117280960083,
        "learning_rate": 2.157089707145117e-05,
        "epoch": 1.8587264758958495,
        "step": 14420
    },
    {
        "loss": 1.158,
        "grad_norm": 3.3779284954071045,
        "learning_rate": 2.1533166659365713e-05,
        "epoch": 1.8588553750966743,
        "step": 14421
    },
    {
        "loss": 0.4585,
        "grad_norm": 2.520643949508667,
        "learning_rate": 2.1495465292151674e-05,
        "epoch": 1.8589842742974994,
        "step": 14422
    },
    {
        "loss": 1.4136,
        "grad_norm": 3.3860926628112793,
        "learning_rate": 2.1457792983764612e-05,
        "epoch": 1.8591131734983244,
        "step": 14423
    },
    {
        "loss": 1.3401,
        "grad_norm": 2.996033191680908,
        "learning_rate": 2.1420149748149087e-05,
        "epoch": 1.8592420726991494,
        "step": 14424
    },
    {
        "loss": 0.9289,
        "grad_norm": 2.5899291038513184,
        "learning_rate": 2.1382535599238673e-05,
        "epoch": 1.8593709718999742,
        "step": 14425
    },
    {
        "loss": 2.0807,
        "grad_norm": 1.2138793468475342,
        "learning_rate": 2.1344950550956676e-05,
        "epoch": 1.859499871100799,
        "step": 14426
    },
    {
        "loss": 1.4712,
        "grad_norm": 2.4702091217041016,
        "learning_rate": 2.130739461721526e-05,
        "epoch": 1.859628770301624,
        "step": 14427
    },
    {
        "loss": 0.6694,
        "grad_norm": 1.772807240486145,
        "learning_rate": 2.1269867811916e-05,
        "epoch": 1.8597576695024491,
        "step": 14428
    },
    {
        "loss": 1.3898,
        "grad_norm": 3.017998456954956,
        "learning_rate": 2.1232370148949587e-05,
        "epoch": 1.8598865687032742,
        "step": 14429
    },
    {
        "loss": 1.4884,
        "grad_norm": 2.8835034370422363,
        "learning_rate": 2.1194901642195964e-05,
        "epoch": 1.860015467904099,
        "step": 14430
    },
    {
        "loss": 1.6164,
        "grad_norm": 2.2463481426239014,
        "learning_rate": 2.1157462305524267e-05,
        "epoch": 1.8601443671049238,
        "step": 14431
    },
    {
        "loss": 0.7094,
        "grad_norm": 2.427415370941162,
        "learning_rate": 2.1120052152792858e-05,
        "epoch": 1.8602732663057489,
        "step": 14432
    },
    {
        "loss": 1.3384,
        "grad_norm": 3.366455316543579,
        "learning_rate": 2.1082671197849168e-05,
        "epoch": 1.860402165506574,
        "step": 14433
    },
    {
        "loss": 0.9222,
        "grad_norm": 2.425971031188965,
        "learning_rate": 2.1045319454530143e-05,
        "epoch": 1.860531064707399,
        "step": 14434
    },
    {
        "loss": 1.5142,
        "grad_norm": 4.953242778778076,
        "learning_rate": 2.10079969366617e-05,
        "epoch": 1.8606599639082237,
        "step": 14435
    },
    {
        "loss": 1.8183,
        "grad_norm": 2.2607650756835938,
        "learning_rate": 2.0970703658058688e-05,
        "epoch": 1.8607888631090486,
        "step": 14436
    },
    {
        "loss": 1.7015,
        "grad_norm": 2.1769421100616455,
        "learning_rate": 2.0933439632525643e-05,
        "epoch": 1.8609177623098736,
        "step": 14437
    },
    {
        "loss": 2.3262,
        "grad_norm": 2.1533613204956055,
        "learning_rate": 2.0896204873855908e-05,
        "epoch": 1.8610466615106986,
        "step": 14438
    },
    {
        "loss": 1.8267,
        "grad_norm": 1.6178187131881714,
        "learning_rate": 2.0858999395832078e-05,
        "epoch": 1.8611755607115237,
        "step": 14439
    },
    {
        "loss": 1.8761,
        "grad_norm": 1.8971195220947266,
        "learning_rate": 2.0821823212226088e-05,
        "epoch": 1.8613044599123485,
        "step": 14440
    },
    {
        "loss": 0.8046,
        "grad_norm": 1.8360052108764648,
        "learning_rate": 2.078467633679886e-05,
        "epoch": 1.8614333591131735,
        "step": 14441
    },
    {
        "loss": 1.5986,
        "grad_norm": 2.464946985244751,
        "learning_rate": 2.0747558783300246e-05,
        "epoch": 1.8615622583139984,
        "step": 14442
    },
    {
        "loss": 1.232,
        "grad_norm": 3.1912598609924316,
        "learning_rate": 2.0710470565469708e-05,
        "epoch": 1.8616911575148234,
        "step": 14443
    },
    {
        "loss": 1.9206,
        "grad_norm": 2.5625972747802734,
        "learning_rate": 2.0673411697035594e-05,
        "epoch": 1.8618200567156484,
        "step": 14444
    },
    {
        "loss": 0.6605,
        "grad_norm": 2.0672571659088135,
        "learning_rate": 2.063638219171541e-05,
        "epoch": 1.8619489559164735,
        "step": 14445
    },
    {
        "loss": 2.0119,
        "grad_norm": 2.333589553833008,
        "learning_rate": 2.059938206321578e-05,
        "epoch": 1.8620778551172983,
        "step": 14446
    },
    {
        "loss": 1.852,
        "grad_norm": 1.5403565168380737,
        "learning_rate": 2.0562411325232478e-05,
        "epoch": 1.862206754318123,
        "step": 14447
    },
    {
        "loss": 2.1626,
        "grad_norm": 1.7695322036743164,
        "learning_rate": 2.052546999145042e-05,
        "epoch": 1.8623356535189481,
        "step": 14448
    },
    {
        "loss": 1.5472,
        "grad_norm": 2.2225117683410645,
        "learning_rate": 2.0488558075543573e-05,
        "epoch": 1.8624645527197732,
        "step": 14449
    },
    {
        "loss": 2.0455,
        "grad_norm": 1.454932451248169,
        "learning_rate": 2.0451675591175035e-05,
        "epoch": 1.8625934519205982,
        "step": 14450
    },
    {
        "loss": 1.6899,
        "grad_norm": 3.272547960281372,
        "learning_rate": 2.041482255199717e-05,
        "epoch": 1.862722351121423,
        "step": 14451
    },
    {
        "loss": 2.0215,
        "grad_norm": 1.7261654138565063,
        "learning_rate": 2.0377998971651302e-05,
        "epoch": 1.8628512503222479,
        "step": 14452
    },
    {
        "loss": 0.8235,
        "grad_norm": 4.00214147567749,
        "learning_rate": 2.0341204863767617e-05,
        "epoch": 1.862980149523073,
        "step": 14453
    },
    {
        "loss": 1.7179,
        "grad_norm": 2.992699384689331,
        "learning_rate": 2.0304440241965862e-05,
        "epoch": 1.863109048723898,
        "step": 14454
    },
    {
        "loss": 1.7566,
        "grad_norm": 2.1260569095611572,
        "learning_rate": 2.026770511985454e-05,
        "epoch": 1.863237947924723,
        "step": 14455
    },
    {
        "loss": 1.6529,
        "grad_norm": 3.614130973815918,
        "learning_rate": 2.02309995110313e-05,
        "epoch": 1.8633668471255478,
        "step": 14456
    },
    {
        "loss": 1.3521,
        "grad_norm": 2.693666458129883,
        "learning_rate": 2.019432342908303e-05,
        "epoch": 1.8634957463263728,
        "step": 14457
    },
    {
        "loss": 1.757,
        "grad_norm": 2.1590843200683594,
        "learning_rate": 2.0157676887585564e-05,
        "epoch": 1.8636246455271976,
        "step": 14458
    },
    {
        "loss": 2.1565,
        "grad_norm": 3.6951496601104736,
        "learning_rate": 2.0121059900103533e-05,
        "epoch": 1.8637535447280227,
        "step": 14459
    },
    {
        "loss": 1.1514,
        "grad_norm": 3.840697765350342,
        "learning_rate": 2.008447248019112e-05,
        "epoch": 1.8638824439288477,
        "step": 14460
    },
    {
        "loss": 0.9407,
        "grad_norm": 2.290395736694336,
        "learning_rate": 2.0047914641391308e-05,
        "epoch": 1.8640113431296728,
        "step": 14461
    },
    {
        "loss": 1.1511,
        "grad_norm": 3.3121697902679443,
        "learning_rate": 2.001138639723612e-05,
        "epoch": 1.8641402423304976,
        "step": 14462
    },
    {
        "loss": 1.7171,
        "grad_norm": 2.4568846225738525,
        "learning_rate": 1.9974887761246662e-05,
        "epoch": 1.8642691415313224,
        "step": 14463
    },
    {
        "loss": 1.8909,
        "grad_norm": 2.309394598007202,
        "learning_rate": 1.9938418746933075e-05,
        "epoch": 1.8643980407321474,
        "step": 14464
    },
    {
        "loss": 1.2875,
        "grad_norm": 3.76129150390625,
        "learning_rate": 1.9901979367794533e-05,
        "epoch": 1.8645269399329725,
        "step": 14465
    },
    {
        "loss": 1.7397,
        "grad_norm": 2.8551998138427734,
        "learning_rate": 1.9865569637319238e-05,
        "epoch": 1.8646558391337975,
        "step": 14466
    },
    {
        "loss": 1.7097,
        "grad_norm": 2.5474658012390137,
        "learning_rate": 1.9829189568984374e-05,
        "epoch": 1.8647847383346223,
        "step": 14467
    },
    {
        "loss": 2.0383,
        "grad_norm": 2.2450177669525146,
        "learning_rate": 1.979283917625635e-05,
        "epoch": 1.8649136375354471,
        "step": 14468
    },
    {
        "loss": 1.4081,
        "grad_norm": 3.2897017002105713,
        "learning_rate": 1.9756518472590346e-05,
        "epoch": 1.8650425367362722,
        "step": 14469
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.1157379150390625,
        "learning_rate": 1.9720227471430653e-05,
        "epoch": 1.8651714359370972,
        "step": 14470
    },
    {
        "loss": 1.4714,
        "grad_norm": 2.427725315093994,
        "learning_rate": 1.968396618621051e-05,
        "epoch": 1.8653003351379223,
        "step": 14471
    },
    {
        "loss": 1.1892,
        "grad_norm": 3.5670766830444336,
        "learning_rate": 1.9647734630352288e-05,
        "epoch": 1.865429234338747,
        "step": 14472
    },
    {
        "loss": 0.7577,
        "grad_norm": 3.4481709003448486,
        "learning_rate": 1.961153281726713e-05,
        "epoch": 1.865558133539572,
        "step": 14473
    },
    {
        "loss": 1.5104,
        "grad_norm": 2.4949989318847656,
        "learning_rate": 1.95753607603555e-05,
        "epoch": 1.865687032740397,
        "step": 14474
    },
    {
        "loss": 1.7888,
        "grad_norm": 2.4042069911956787,
        "learning_rate": 1.9539218473006637e-05,
        "epoch": 1.865815931941222,
        "step": 14475
    },
    {
        "loss": 1.35,
        "grad_norm": 2.58210825920105,
        "learning_rate": 1.9503105968598513e-05,
        "epoch": 1.865944831142047,
        "step": 14476
    },
    {
        "loss": 0.2598,
        "grad_norm": 0.7536559104919434,
        "learning_rate": 1.9467023260498652e-05,
        "epoch": 1.8660737303428718,
        "step": 14477
    },
    {
        "loss": 1.9072,
        "grad_norm": 1.7692060470581055,
        "learning_rate": 1.9430970362063117e-05,
        "epoch": 1.8662026295436969,
        "step": 14478
    },
    {
        "loss": 1.036,
        "grad_norm": 2.735130786895752,
        "learning_rate": 1.9394947286637066e-05,
        "epoch": 1.8663315287445217,
        "step": 14479
    },
    {
        "loss": 0.6674,
        "grad_norm": 3.0882997512817383,
        "learning_rate": 1.935895404755462e-05,
        "epoch": 1.8664604279453467,
        "step": 14480
    },
    {
        "loss": 1.6423,
        "grad_norm": 2.1519248485565186,
        "learning_rate": 1.9322990658138827e-05,
        "epoch": 1.8665893271461718,
        "step": 14481
    },
    {
        "loss": 0.4698,
        "grad_norm": 3.33764386177063,
        "learning_rate": 1.9287057131701713e-05,
        "epoch": 1.8667182263469968,
        "step": 14482
    },
    {
        "loss": 0.8115,
        "grad_norm": 2.934833526611328,
        "learning_rate": 1.925115348154426e-05,
        "epoch": 1.8668471255478216,
        "step": 14483
    },
    {
        "loss": 1.0027,
        "grad_norm": 1.916815161705017,
        "learning_rate": 1.9215279720956303e-05,
        "epoch": 1.8669760247486464,
        "step": 14484
    },
    {
        "loss": 1.6829,
        "grad_norm": 2.2009778022766113,
        "learning_rate": 1.917943586321681e-05,
        "epoch": 1.8671049239494715,
        "step": 14485
    },
    {
        "loss": 1.551,
        "grad_norm": 2.4204978942871094,
        "learning_rate": 1.9143621921593502e-05,
        "epoch": 1.8672338231502965,
        "step": 14486
    },
    {
        "loss": 1.8559,
        "grad_norm": 2.2917537689208984,
        "learning_rate": 1.9107837909343074e-05,
        "epoch": 1.8673627223511216,
        "step": 14487
    },
    {
        "loss": 1.5216,
        "grad_norm": 2.1647913455963135,
        "learning_rate": 1.9072083839711106e-05,
        "epoch": 1.8674916215519464,
        "step": 14488
    },
    {
        "loss": 1.7405,
        "grad_norm": 2.6274890899658203,
        "learning_rate": 1.9036359725932217e-05,
        "epoch": 1.8676205207527712,
        "step": 14489
    },
    {
        "loss": 1.8202,
        "grad_norm": 1.5118821859359741,
        "learning_rate": 1.9000665581229726e-05,
        "epoch": 1.8677494199535962,
        "step": 14490
    },
    {
        "loss": 1.5516,
        "grad_norm": 2.8532536029815674,
        "learning_rate": 1.8965001418816164e-05,
        "epoch": 1.8678783191544213,
        "step": 14491
    },
    {
        "loss": 1.3671,
        "grad_norm": 3.4502179622650146,
        "learning_rate": 1.8929367251892716e-05,
        "epoch": 1.8680072183552463,
        "step": 14492
    },
    {
        "loss": 1.8986,
        "grad_norm": 4.215429782867432,
        "learning_rate": 1.8893763093649485e-05,
        "epoch": 1.8681361175560711,
        "step": 14493
    },
    {
        "loss": 0.6312,
        "grad_norm": 2.970200777053833,
        "learning_rate": 1.885818895726561e-05,
        "epoch": 1.8682650167568962,
        "step": 14494
    },
    {
        "loss": 1.9678,
        "grad_norm": 4.697319507598877,
        "learning_rate": 1.8822644855908967e-05,
        "epoch": 1.868393915957721,
        "step": 14495
    },
    {
        "loss": 1.9425,
        "grad_norm": 1.6031407117843628,
        "learning_rate": 1.8787130802736398e-05,
        "epoch": 1.868522815158546,
        "step": 14496
    },
    {
        "loss": 1.4255,
        "grad_norm": 1.786829948425293,
        "learning_rate": 1.8751646810893586e-05,
        "epoch": 1.868651714359371,
        "step": 14497
    },
    {
        "loss": 1.1186,
        "grad_norm": 1.9224815368652344,
        "learning_rate": 1.8716192893515093e-05,
        "epoch": 1.868780613560196,
        "step": 14498
    },
    {
        "loss": 0.5273,
        "grad_norm": 1.8728455305099487,
        "learning_rate": 1.8680769063724346e-05,
        "epoch": 1.868909512761021,
        "step": 14499
    },
    {
        "loss": 1.3413,
        "grad_norm": 2.3600006103515625,
        "learning_rate": 1.8645375334633696e-05,
        "epoch": 1.8690384119618457,
        "step": 14500
    },
    {
        "loss": 2.09,
        "grad_norm": 2.3306617736816406,
        "learning_rate": 1.8610011719344177e-05,
        "epoch": 1.8691673111626708,
        "step": 14501
    },
    {
        "loss": 1.0386,
        "grad_norm": 1.7854526042938232,
        "learning_rate": 1.8574678230945976e-05,
        "epoch": 1.8692962103634958,
        "step": 14502
    },
    {
        "loss": 1.7235,
        "grad_norm": 1.629982829093933,
        "learning_rate": 1.8539374882517857e-05,
        "epoch": 1.8694251095643208,
        "step": 14503
    },
    {
        "loss": 1.3579,
        "grad_norm": 1.952246069908142,
        "learning_rate": 1.8504101687127507e-05,
        "epoch": 1.8695540087651457,
        "step": 14504
    },
    {
        "loss": 1.7311,
        "grad_norm": 3.573640823364258,
        "learning_rate": 1.846885865783151e-05,
        "epoch": 1.8696829079659705,
        "step": 14505
    },
    {
        "loss": 1.5541,
        "grad_norm": 2.0708682537078857,
        "learning_rate": 1.8433645807675225e-05,
        "epoch": 1.8698118071667955,
        "step": 14506
    },
    {
        "loss": 1.7168,
        "grad_norm": 1.9122343063354492,
        "learning_rate": 1.839846314969278e-05,
        "epoch": 1.8699407063676206,
        "step": 14507
    },
    {
        "loss": 1.6894,
        "grad_norm": 2.592001438140869,
        "learning_rate": 1.8363310696907343e-05,
        "epoch": 1.8700696055684456,
        "step": 14508
    },
    {
        "loss": 1.5735,
        "grad_norm": 2.9774696826934814,
        "learning_rate": 1.8328188462330677e-05,
        "epoch": 1.8701985047692704,
        "step": 14509
    },
    {
        "loss": 1.4288,
        "grad_norm": 2.286712408065796,
        "learning_rate": 1.8293096458963476e-05,
        "epoch": 1.8703274039700952,
        "step": 14510
    },
    {
        "loss": 1.3953,
        "grad_norm": 2.26884126663208,
        "learning_rate": 1.8258034699795212e-05,
        "epoch": 1.8704563031709203,
        "step": 14511
    },
    {
        "loss": 1.3064,
        "grad_norm": 1.8626066446304321,
        "learning_rate": 1.8223003197804134e-05,
        "epoch": 1.8705852023717453,
        "step": 14512
    },
    {
        "loss": 1.6463,
        "grad_norm": 2.208686113357544,
        "learning_rate": 1.8188001965957334e-05,
        "epoch": 1.8707141015725703,
        "step": 14513
    },
    {
        "loss": 1.872,
        "grad_norm": 1.6183075904846191,
        "learning_rate": 1.8153031017210685e-05,
        "epoch": 1.8708430007733952,
        "step": 14514
    },
    {
        "loss": 2.1754,
        "grad_norm": 2.080094575881958,
        "learning_rate": 1.8118090364508822e-05,
        "epoch": 1.8709718999742202,
        "step": 14515
    },
    {
        "loss": 1.9614,
        "grad_norm": 2.4974172115325928,
        "learning_rate": 1.808318002078524e-05,
        "epoch": 1.871100799175045,
        "step": 14516
    },
    {
        "loss": 1.7399,
        "grad_norm": 2.322472095489502,
        "learning_rate": 1.804829999896215e-05,
        "epoch": 1.87122969837587,
        "step": 14517
    },
    {
        "loss": 1.0945,
        "grad_norm": 1.8111753463745117,
        "learning_rate": 1.8013450311950474e-05,
        "epoch": 1.871358597576695,
        "step": 14518
    },
    {
        "loss": 1.404,
        "grad_norm": 2.5104634761810303,
        "learning_rate": 1.7978630972650156e-05,
        "epoch": 1.8714874967775201,
        "step": 14519
    },
    {
        "loss": 1.9688,
        "grad_norm": 2.2361509799957275,
        "learning_rate": 1.794384199394965e-05,
        "epoch": 1.871616395978345,
        "step": 14520
    },
    {
        "loss": 1.8069,
        "grad_norm": 1.9361670017242432,
        "learning_rate": 1.7909083388726243e-05,
        "epoch": 1.8717452951791698,
        "step": 14521
    },
    {
        "loss": 1.817,
        "grad_norm": 5.629615783691406,
        "learning_rate": 1.7874355169846047e-05,
        "epoch": 1.8718741943799948,
        "step": 14522
    },
    {
        "loss": 1.2453,
        "grad_norm": 3.3773415088653564,
        "learning_rate": 1.783965735016385e-05,
        "epoch": 1.8720030935808198,
        "step": 14523
    },
    {
        "loss": 1.6314,
        "grad_norm": 2.18701434135437,
        "learning_rate": 1.7804989942523142e-05,
        "epoch": 1.8721319927816449,
        "step": 14524
    },
    {
        "loss": 1.4317,
        "grad_norm": 3.109252691268921,
        "learning_rate": 1.7770352959756387e-05,
        "epoch": 1.8722608919824697,
        "step": 14525
    },
    {
        "loss": 1.7033,
        "grad_norm": 2.402838706970215,
        "learning_rate": 1.7735746414684518e-05,
        "epoch": 1.8723897911832945,
        "step": 14526
    },
    {
        "loss": 0.7459,
        "grad_norm": 2.55531644821167,
        "learning_rate": 1.7701170320117356e-05,
        "epoch": 1.8725186903841196,
        "step": 14527
    },
    {
        "loss": 1.7993,
        "grad_norm": 2.716777801513672,
        "learning_rate": 1.7666624688853393e-05,
        "epoch": 1.8726475895849446,
        "step": 14528
    },
    {
        "loss": 1.6475,
        "grad_norm": 3.1254708766937256,
        "learning_rate": 1.7632109533679862e-05,
        "epoch": 1.8727764887857696,
        "step": 14529
    },
    {
        "loss": 1.1501,
        "grad_norm": 2.7541141510009766,
        "learning_rate": 1.7597624867372707e-05,
        "epoch": 1.8729053879865944,
        "step": 14530
    },
    {
        "loss": 1.1201,
        "grad_norm": 3.088712453842163,
        "learning_rate": 1.7563170702696598e-05,
        "epoch": 1.8730342871874195,
        "step": 14531
    },
    {
        "loss": 0.8225,
        "grad_norm": 1.7537658214569092,
        "learning_rate": 1.7528747052404792e-05,
        "epoch": 1.8731631863882443,
        "step": 14532
    },
    {
        "loss": 0.9641,
        "grad_norm": 2.830808162689209,
        "learning_rate": 1.7494353929239647e-05,
        "epoch": 1.8732920855890693,
        "step": 14533
    },
    {
        "loss": 1.5493,
        "grad_norm": 1.983078122138977,
        "learning_rate": 1.7459991345931702e-05,
        "epoch": 1.8734209847898944,
        "step": 14534
    },
    {
        "loss": 1.9016,
        "grad_norm": 3.349461078643799,
        "learning_rate": 1.7425659315200417e-05,
        "epoch": 1.8735498839907194,
        "step": 14535
    },
    {
        "loss": 0.7925,
        "grad_norm": 2.7765538692474365,
        "learning_rate": 1.739135784975412e-05,
        "epoch": 1.8736787831915442,
        "step": 14536
    },
    {
        "loss": 2.092,
        "grad_norm": 2.122575283050537,
        "learning_rate": 1.735708696228955e-05,
        "epoch": 1.873807682392369,
        "step": 14537
    },
    {
        "loss": 1.5086,
        "grad_norm": 1.5295708179473877,
        "learning_rate": 1.7322846665492288e-05,
        "epoch": 1.873936581593194,
        "step": 14538
    },
    {
        "loss": 2.1386,
        "grad_norm": Infinity,
        "learning_rate": 1.7322846665492288e-05,
        "epoch": 1.8740654807940191,
        "step": 14539
    },
    {
        "loss": 1.7258,
        "grad_norm": 2.256312370300293,
        "learning_rate": 1.7288636972036532e-05,
        "epoch": 1.8741943799948442,
        "step": 14540
    },
    {
        "loss": 1.8598,
        "grad_norm": 3.106546401977539,
        "learning_rate": 1.725445789458515e-05,
        "epoch": 1.874323279195669,
        "step": 14541
    },
    {
        "loss": 2.213,
        "grad_norm": 1.9095112085342407,
        "learning_rate": 1.7220309445789618e-05,
        "epoch": 1.8744521783964938,
        "step": 14542
    },
    {
        "loss": 1.8394,
        "grad_norm": 2.9410014152526855,
        "learning_rate": 1.7186191638290273e-05,
        "epoch": 1.8745810775973188,
        "step": 14543
    },
    {
        "loss": 1.8716,
        "grad_norm": 1.7669999599456787,
        "learning_rate": 1.7152104484715958e-05,
        "epoch": 1.8747099767981439,
        "step": 14544
    },
    {
        "loss": 0.8004,
        "grad_norm": 2.3075997829437256,
        "learning_rate": 1.7118047997684174e-05,
        "epoch": 1.874838875998969,
        "step": 14545
    },
    {
        "loss": 1.4508,
        "grad_norm": 1.523746371269226,
        "learning_rate": 1.7084022189801074e-05,
        "epoch": 1.8749677751997937,
        "step": 14546
    },
    {
        "loss": 1.7823,
        "grad_norm": 2.281620502471924,
        "learning_rate": 1.70500270736615e-05,
        "epoch": 1.8750966744006186,
        "step": 14547
    },
    {
        "loss": 2.1428,
        "grad_norm": 1.5862308740615845,
        "learning_rate": 1.7016062661848886e-05,
        "epoch": 1.8752255736014436,
        "step": 14548
    },
    {
        "loss": 1.9999,
        "grad_norm": 2.2451257705688477,
        "learning_rate": 1.6982128966935307e-05,
        "epoch": 1.8753544728022686,
        "step": 14549
    },
    {
        "loss": 1.7741,
        "grad_norm": 3.4330942630767822,
        "learning_rate": 1.694822600148145e-05,
        "epoch": 1.8754833720030937,
        "step": 14550
    },
    {
        "loss": 2.0772,
        "grad_norm": 2.1406137943267822,
        "learning_rate": 1.6914353778036862e-05,
        "epoch": 1.8756122712039185,
        "step": 14551
    },
    {
        "loss": 1.0397,
        "grad_norm": 1.164281964302063,
        "learning_rate": 1.6880512309139285e-05,
        "epoch": 1.8757411704047435,
        "step": 14552
    },
    {
        "loss": 1.5882,
        "grad_norm": 4.081875801086426,
        "learning_rate": 1.6846701607315286e-05,
        "epoch": 1.8758700696055683,
        "step": 14553
    },
    {
        "loss": 0.8817,
        "grad_norm": 1.5961315631866455,
        "learning_rate": 1.6812921685080218e-05,
        "epoch": 1.8759989688063934,
        "step": 14554
    },
    {
        "loss": 0.9798,
        "grad_norm": 1.942277193069458,
        "learning_rate": 1.6779172554937783e-05,
        "epoch": 1.8761278680072184,
        "step": 14555
    },
    {
        "loss": 1.1468,
        "grad_norm": 2.6029741764068604,
        "learning_rate": 1.6745454229380352e-05,
        "epoch": 1.8762567672080435,
        "step": 14556
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.8436717987060547,
        "learning_rate": 1.6711766720889123e-05,
        "epoch": 1.8763856664088683,
        "step": 14557
    },
    {
        "loss": 0.9273,
        "grad_norm": 1.4699060916900635,
        "learning_rate": 1.667811004193346e-05,
        "epoch": 1.876514565609693,
        "step": 14558
    },
    {
        "loss": 1.352,
        "grad_norm": 2.654083728790283,
        "learning_rate": 1.6644484204971554e-05,
        "epoch": 1.8766434648105181,
        "step": 14559
    },
    {
        "loss": 1.9268,
        "grad_norm": 1.59280526638031,
        "learning_rate": 1.6610889222450277e-05,
        "epoch": 1.8767723640113432,
        "step": 14560
    },
    {
        "loss": 2.0257,
        "grad_norm": 2.312009811401367,
        "learning_rate": 1.6577325106804962e-05,
        "epoch": 1.8769012632121682,
        "step": 14561
    },
    {
        "loss": 0.8883,
        "grad_norm": 2.32070255279541,
        "learning_rate": 1.654379187045949e-05,
        "epoch": 1.877030162412993,
        "step": 14562
    },
    {
        "loss": 1.8332,
        "grad_norm": 1.6551612615585327,
        "learning_rate": 1.651028952582636e-05,
        "epoch": 1.8771590616138178,
        "step": 14563
    },
    {
        "loss": 1.7976,
        "grad_norm": 4.4539475440979,
        "learning_rate": 1.647681808530661e-05,
        "epoch": 1.8772879608146429,
        "step": 14564
    },
    {
        "loss": 1.6541,
        "grad_norm": 2.6019387245178223,
        "learning_rate": 1.644337756128985e-05,
        "epoch": 1.877416860015468,
        "step": 14565
    },
    {
        "loss": 1.8207,
        "grad_norm": 2.7196383476257324,
        "learning_rate": 1.640996796615425e-05,
        "epoch": 1.877545759216293,
        "step": 14566
    },
    {
        "loss": 0.7611,
        "grad_norm": 2.5910675525665283,
        "learning_rate": 1.6376589312266487e-05,
        "epoch": 1.8776746584171178,
        "step": 14567
    },
    {
        "loss": 0.857,
        "grad_norm": 2.13305401802063,
        "learning_rate": 1.6343241611982018e-05,
        "epoch": 1.8778035576179428,
        "step": 14568
    },
    {
        "loss": 1.3099,
        "grad_norm": 2.8538453578948975,
        "learning_rate": 1.6309924877644457e-05,
        "epoch": 1.8779324568187676,
        "step": 14569
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.4308717250823975,
        "learning_rate": 1.6276639121586124e-05,
        "epoch": 1.8780613560195927,
        "step": 14570
    },
    {
        "loss": 1.0707,
        "grad_norm": 4.0272111892700195,
        "learning_rate": 1.6243384356128044e-05,
        "epoch": 1.8781902552204177,
        "step": 14571
    },
    {
        "loss": 1.6784,
        "grad_norm": 2.400705099105835,
        "learning_rate": 1.621016059357958e-05,
        "epoch": 1.8783191544212428,
        "step": 14572
    },
    {
        "loss": 2.0891,
        "grad_norm": 2.4175875186920166,
        "learning_rate": 1.6176967846238587e-05,
        "epoch": 1.8784480536220676,
        "step": 14573
    },
    {
        "loss": 1.1886,
        "grad_norm": 1.6347814798355103,
        "learning_rate": 1.6143806126391735e-05,
        "epoch": 1.8785769528228924,
        "step": 14574
    },
    {
        "loss": 0.7559,
        "grad_norm": 3.162648916244507,
        "learning_rate": 1.6110675446313772e-05,
        "epoch": 1.8787058520237174,
        "step": 14575
    },
    {
        "loss": 1.3805,
        "grad_norm": 2.5675268173217773,
        "learning_rate": 1.6077575818268144e-05,
        "epoch": 1.8788347512245425,
        "step": 14576
    },
    {
        "loss": 1.8337,
        "grad_norm": 2.2283263206481934,
        "learning_rate": 1.6044507254507045e-05,
        "epoch": 1.8789636504253675,
        "step": 14577
    },
    {
        "loss": 1.0212,
        "grad_norm": 2.0634655952453613,
        "learning_rate": 1.601146976727085e-05,
        "epoch": 1.8790925496261923,
        "step": 14578
    },
    {
        "loss": 1.7827,
        "grad_norm": 2.4993879795074463,
        "learning_rate": 1.597846336878854e-05,
        "epoch": 1.8792214488270171,
        "step": 14579
    },
    {
        "loss": 1.8753,
        "grad_norm": 2.1971821784973145,
        "learning_rate": 1.5945488071277605e-05,
        "epoch": 1.8793503480278422,
        "step": 14580
    },
    {
        "loss": 1.8049,
        "grad_norm": 2.7010676860809326,
        "learning_rate": 1.5912543886944e-05,
        "epoch": 1.8794792472286672,
        "step": 14581
    },
    {
        "loss": 1.0246,
        "grad_norm": 3.716827869415283,
        "learning_rate": 1.587963082798215e-05,
        "epoch": 1.8796081464294923,
        "step": 14582
    },
    {
        "loss": 0.7317,
        "grad_norm": 2.0264675617218018,
        "learning_rate": 1.5846748906575027e-05,
        "epoch": 1.879737045630317,
        "step": 14583
    },
    {
        "loss": 1.6421,
        "grad_norm": 3.0746381282806396,
        "learning_rate": 1.5813898134893933e-05,
        "epoch": 1.8798659448311419,
        "step": 14584
    },
    {
        "loss": 1.8453,
        "grad_norm": 2.5139646530151367,
        "learning_rate": 1.5781078525098957e-05,
        "epoch": 1.879994844031967,
        "step": 14585
    },
    {
        "loss": 1.3876,
        "grad_norm": 1.5171198844909668,
        "learning_rate": 1.5748290089338214e-05,
        "epoch": 1.880123743232792,
        "step": 14586
    },
    {
        "loss": 1.874,
        "grad_norm": 2.3122470378875732,
        "learning_rate": 1.571553283974849e-05,
        "epoch": 1.880252642433617,
        "step": 14587
    },
    {
        "loss": 1.6288,
        "grad_norm": 3.599454402923584,
        "learning_rate": 1.5682806788455186e-05,
        "epoch": 1.8803815416344418,
        "step": 14588
    },
    {
        "loss": 2.1472,
        "grad_norm": 2.012815475463867,
        "learning_rate": 1.5650111947571954e-05,
        "epoch": 1.8805104408352669,
        "step": 14589
    },
    {
        "loss": 1.1443,
        "grad_norm": 2.7244856357574463,
        "learning_rate": 1.5617448329200858e-05,
        "epoch": 1.8806393400360917,
        "step": 14590
    },
    {
        "loss": 1.8066,
        "grad_norm": 2.8732805252075195,
        "learning_rate": 1.558481594543271e-05,
        "epoch": 1.8807682392369167,
        "step": 14591
    },
    {
        "loss": 1.5874,
        "grad_norm": 2.0527594089508057,
        "learning_rate": 1.555221480834632e-05,
        "epoch": 1.8808971384377418,
        "step": 14592
    },
    {
        "loss": 1.1739,
        "grad_norm": 1.9738317728042603,
        "learning_rate": 1.5519644930009135e-05,
        "epoch": 1.8810260376385668,
        "step": 14593
    },
    {
        "loss": 1.9637,
        "grad_norm": 2.922971725463867,
        "learning_rate": 1.548710632247724e-05,
        "epoch": 1.8811549368393916,
        "step": 14594
    },
    {
        "loss": 1.859,
        "grad_norm": 3.374438762664795,
        "learning_rate": 1.5454598997794868e-05,
        "epoch": 1.8812838360402164,
        "step": 14595
    },
    {
        "loss": 1.7334,
        "grad_norm": 4.110619068145752,
        "learning_rate": 1.5422122967994734e-05,
        "epoch": 1.8814127352410415,
        "step": 14596
    },
    {
        "loss": 1.6915,
        "grad_norm": 2.0632357597351074,
        "learning_rate": 1.5389678245098025e-05,
        "epoch": 1.8815416344418665,
        "step": 14597
    },
    {
        "loss": 1.7848,
        "grad_norm": 2.4449210166931152,
        "learning_rate": 1.5357264841114306e-05,
        "epoch": 1.8816705336426915,
        "step": 14598
    },
    {
        "loss": 2.1489,
        "grad_norm": 3.50109601020813,
        "learning_rate": 1.5324882768041504e-05,
        "epoch": 1.8817994328435164,
        "step": 14599
    },
    {
        "loss": 1.7994,
        "grad_norm": 2.962372303009033,
        "learning_rate": 1.529253203786607e-05,
        "epoch": 1.8819283320443412,
        "step": 14600
    },
    {
        "loss": 1.8089,
        "grad_norm": 2.0782346725463867,
        "learning_rate": 1.526021266256268e-05,
        "epoch": 1.8820572312451662,
        "step": 14601
    },
    {
        "loss": 1.2954,
        "grad_norm": 3.0774736404418945,
        "learning_rate": 1.5227924654094704e-05,
        "epoch": 1.8821861304459913,
        "step": 14602
    },
    {
        "loss": 1.2667,
        "grad_norm": 3.3525164127349854,
        "learning_rate": 1.51956680244135e-05,
        "epoch": 1.8823150296468163,
        "step": 14603
    },
    {
        "loss": 1.155,
        "grad_norm": 1.9230279922485352,
        "learning_rate": 1.5163442785459003e-05,
        "epoch": 1.8824439288476411,
        "step": 14604
    },
    {
        "loss": 1.3415,
        "grad_norm": 2.428274393081665,
        "learning_rate": 1.5131248949159704e-05,
        "epoch": 1.8825728280484662,
        "step": 14605
    },
    {
        "loss": 1.864,
        "grad_norm": 2.3703856468200684,
        "learning_rate": 1.5099086527432216e-05,
        "epoch": 1.882701727249291,
        "step": 14606
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.4972965717315674,
        "learning_rate": 1.5066955532181548e-05,
        "epoch": 1.882830626450116,
        "step": 14607
    },
    {
        "loss": 1.701,
        "grad_norm": 1.6461684703826904,
        "learning_rate": 1.5034855975301355e-05,
        "epoch": 1.882959525650941,
        "step": 14608
    },
    {
        "loss": 1.8319,
        "grad_norm": 2.052616834640503,
        "learning_rate": 1.5002787868673219e-05,
        "epoch": 1.883088424851766,
        "step": 14609
    },
    {
        "loss": 1.2081,
        "grad_norm": 3.049532413482666,
        "learning_rate": 1.4970751224167278e-05,
        "epoch": 1.883217324052591,
        "step": 14610
    },
    {
        "loss": 1.4548,
        "grad_norm": 2.4457788467407227,
        "learning_rate": 1.493874605364225e-05,
        "epoch": 1.8833462232534157,
        "step": 14611
    },
    {
        "loss": 1.5659,
        "grad_norm": 1.9503833055496216,
        "learning_rate": 1.4906772368944888e-05,
        "epoch": 1.8834751224542408,
        "step": 14612
    },
    {
        "loss": 1.3088,
        "grad_norm": 1.2773391008377075,
        "learning_rate": 1.4874830181910426e-05,
        "epoch": 1.8836040216550658,
        "step": 14613
    },
    {
        "loss": 1.8329,
        "grad_norm": 2.178499698638916,
        "learning_rate": 1.4842919504362396e-05,
        "epoch": 1.8837329208558908,
        "step": 14614
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.8847466707229614,
        "learning_rate": 1.4811040348112681e-05,
        "epoch": 1.8838618200567157,
        "step": 14615
    },
    {
        "loss": 0.9497,
        "grad_norm": 3.0115199089050293,
        "learning_rate": 1.4779192724961532e-05,
        "epoch": 1.8839907192575405,
        "step": 14616
    },
    {
        "loss": 1.7596,
        "grad_norm": 2.2084879875183105,
        "learning_rate": 1.4747376646697487e-05,
        "epoch": 1.8841196184583655,
        "step": 14617
    },
    {
        "loss": 1.0818,
        "grad_norm": 2.787158489227295,
        "learning_rate": 1.4715592125097344e-05,
        "epoch": 1.8842485176591905,
        "step": 14618
    },
    {
        "loss": 1.3771,
        "grad_norm": 4.1851301193237305,
        "learning_rate": 1.4683839171926428e-05,
        "epoch": 1.8843774168600156,
        "step": 14619
    },
    {
        "loss": 1.6774,
        "grad_norm": 2.856123924255371,
        "learning_rate": 1.4652117798938259e-05,
        "epoch": 1.8845063160608404,
        "step": 14620
    },
    {
        "loss": 1.4617,
        "grad_norm": 1.9774237871170044,
        "learning_rate": 1.4620428017874421e-05,
        "epoch": 1.8846352152616652,
        "step": 14621
    },
    {
        "loss": 1.2919,
        "grad_norm": 1.951390266418457,
        "learning_rate": 1.4588769840465278e-05,
        "epoch": 1.8847641144624903,
        "step": 14622
    },
    {
        "loss": 1.818,
        "grad_norm": 2.8989222049713135,
        "learning_rate": 1.4557143278429175e-05,
        "epoch": 1.8848930136633153,
        "step": 14623
    },
    {
        "loss": 1.8713,
        "grad_norm": 2.200615882873535,
        "learning_rate": 1.4525548343472773e-05,
        "epoch": 1.8850219128641403,
        "step": 14624
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.1988422870635986,
        "learning_rate": 1.449398504729127e-05,
        "epoch": 1.8851508120649652,
        "step": 14625
    },
    {
        "loss": 1.4234,
        "grad_norm": 2.5417497158050537,
        "learning_rate": 1.4462453401567777e-05,
        "epoch": 1.8852797112657902,
        "step": 14626
    },
    {
        "loss": 0.9144,
        "grad_norm": 2.4889118671417236,
        "learning_rate": 1.4430953417973859e-05,
        "epoch": 1.885408610466615,
        "step": 14627
    },
    {
        "loss": 2.0615,
        "grad_norm": 2.218597412109375,
        "learning_rate": 1.4399485108169574e-05,
        "epoch": 1.88553750966744,
        "step": 14628
    },
    {
        "loss": 1.1691,
        "grad_norm": 3.0051016807556152,
        "learning_rate": 1.4368048483802964e-05,
        "epoch": 1.885666408868265,
        "step": 14629
    },
    {
        "loss": 2.0305,
        "grad_norm": 1.7042595148086548,
        "learning_rate": 1.4336643556510454e-05,
        "epoch": 1.8857953080690901,
        "step": 14630
    },
    {
        "loss": 1.4104,
        "grad_norm": 1.681386947631836,
        "learning_rate": 1.4305270337916721e-05,
        "epoch": 1.885924207269915,
        "step": 14631
    },
    {
        "loss": 1.8364,
        "grad_norm": 1.6855943202972412,
        "learning_rate": 1.4273928839634688e-05,
        "epoch": 1.8860531064707398,
        "step": 14632
    },
    {
        "loss": 1.3431,
        "grad_norm": 2.6299514770507812,
        "learning_rate": 1.4242619073265606e-05,
        "epoch": 1.8861820056715648,
        "step": 14633
    },
    {
        "loss": 0.541,
        "grad_norm": 1.4248027801513672,
        "learning_rate": 1.4211341050398919e-05,
        "epoch": 1.8863109048723898,
        "step": 14634
    },
    {
        "loss": 2.1691,
        "grad_norm": 1.7601120471954346,
        "learning_rate": 1.4180094782612263e-05,
        "epoch": 1.8864398040732149,
        "step": 14635
    },
    {
        "loss": 1.0675,
        "grad_norm": 2.0974466800689697,
        "learning_rate": 1.4148880281471721e-05,
        "epoch": 1.8865687032740397,
        "step": 14636
    },
    {
        "loss": 1.5845,
        "grad_norm": 1.7668293714523315,
        "learning_rate": 1.4117697558531507e-05,
        "epoch": 1.8866976024748645,
        "step": 14637
    },
    {
        "loss": 1.4043,
        "grad_norm": 2.0753912925720215,
        "learning_rate": 1.4086546625333807e-05,
        "epoch": 1.8868265016756895,
        "step": 14638
    },
    {
        "loss": 0.9267,
        "grad_norm": 3.5688438415527344,
        "learning_rate": 1.4055427493409546e-05,
        "epoch": 1.8869554008765146,
        "step": 14639
    },
    {
        "loss": 1.2339,
        "grad_norm": 2.739880323410034,
        "learning_rate": 1.4024340174277529e-05,
        "epoch": 1.8870843000773396,
        "step": 14640
    },
    {
        "loss": 1.8288,
        "grad_norm": 2.634763240814209,
        "learning_rate": 1.3993284679444795e-05,
        "epoch": 1.8872131992781644,
        "step": 14641
    },
    {
        "loss": 0.6101,
        "grad_norm": 2.2115650177001953,
        "learning_rate": 1.396226102040692e-05,
        "epoch": 1.8873420984789895,
        "step": 14642
    },
    {
        "loss": 1.8829,
        "grad_norm": 2.789951801300049,
        "learning_rate": 1.393126920864719e-05,
        "epoch": 1.8874709976798143,
        "step": 14643
    },
    {
        "loss": 2.1299,
        "grad_norm": 2.32741641998291,
        "learning_rate": 1.3900309255637445e-05,
        "epoch": 1.8875998968806393,
        "step": 14644
    },
    {
        "loss": 0.4082,
        "grad_norm": 3.607084035873413,
        "learning_rate": 1.3869381172837758e-05,
        "epoch": 1.8877287960814644,
        "step": 14645
    },
    {
        "loss": 1.1542,
        "grad_norm": 3.928676128387451,
        "learning_rate": 1.3838484971696252e-05,
        "epoch": 1.8878576952822892,
        "step": 14646
    },
    {
        "loss": 2.0993,
        "grad_norm": 1.5040591955184937,
        "learning_rate": 1.380762066364929e-05,
        "epoch": 1.8879865944831142,
        "step": 14647
    },
    {
        "loss": 1.8326,
        "grad_norm": 2.62308669090271,
        "learning_rate": 1.3776788260121432e-05,
        "epoch": 1.888115493683939,
        "step": 14648
    },
    {
        "loss": 1.2977,
        "grad_norm": 2.375065326690674,
        "learning_rate": 1.374598777252547e-05,
        "epoch": 1.888244392884764,
        "step": 14649
    },
    {
        "loss": 0.9092,
        "grad_norm": 2.673238754272461,
        "learning_rate": 1.371521921226233e-05,
        "epoch": 1.8883732920855891,
        "step": 14650
    },
    {
        "loss": 1.575,
        "grad_norm": 2.834561347961426,
        "learning_rate": 1.3684482590721159e-05,
        "epoch": 1.8885021912864142,
        "step": 14651
    },
    {
        "loss": 0.8762,
        "grad_norm": 3.09135103225708,
        "learning_rate": 1.3653777919279176e-05,
        "epoch": 1.888631090487239,
        "step": 14652
    },
    {
        "loss": 1.1455,
        "grad_norm": 2.151052713394165,
        "learning_rate": 1.3623105209302001e-05,
        "epoch": 1.8887599896880638,
        "step": 14653
    },
    {
        "loss": 0.708,
        "grad_norm": 1.7717199325561523,
        "learning_rate": 1.3592464472143262e-05,
        "epoch": 1.8888888888888888,
        "step": 14654
    },
    {
        "loss": 2.447,
        "grad_norm": 2.748029947280884,
        "learning_rate": 1.3561855719144611e-05,
        "epoch": 1.8890177880897139,
        "step": 14655
    },
    {
        "loss": 1.4722,
        "grad_norm": 2.132931709289551,
        "learning_rate": 1.3531278961636217e-05,
        "epoch": 1.889146687290539,
        "step": 14656
    },
    {
        "loss": 1.2752,
        "grad_norm": 3.4084532260894775,
        "learning_rate": 1.350073421093614e-05,
        "epoch": 1.8892755864913637,
        "step": 14657
    },
    {
        "loss": 1.2414,
        "grad_norm": 3.302548885345459,
        "learning_rate": 1.3470221478350598e-05,
        "epoch": 1.8894044856921886,
        "step": 14658
    },
    {
        "loss": 1.2148,
        "grad_norm": 2.5583178997039795,
        "learning_rate": 1.3439740775174125e-05,
        "epoch": 1.8895333848930136,
        "step": 14659
    },
    {
        "loss": 1.9003,
        "grad_norm": 3.5761992931365967,
        "learning_rate": 1.3409292112689353e-05,
        "epoch": 1.8896622840938386,
        "step": 14660
    },
    {
        "loss": 1.4148,
        "grad_norm": 2.459975481033325,
        "learning_rate": 1.3378875502166776e-05,
        "epoch": 1.8897911832946637,
        "step": 14661
    },
    {
        "loss": 1.473,
        "grad_norm": 3.6236767768859863,
        "learning_rate": 1.3348490954865444e-05,
        "epoch": 1.8899200824954885,
        "step": 14662
    },
    {
        "loss": 2.1198,
        "grad_norm": 1.9732486009597778,
        "learning_rate": 1.3318138482032272e-05,
        "epoch": 1.8900489816963135,
        "step": 14663
    },
    {
        "loss": 0.812,
        "grad_norm": 1.4165561199188232,
        "learning_rate": 1.3287818094902382e-05,
        "epoch": 1.8901778808971383,
        "step": 14664
    },
    {
        "loss": 1.5278,
        "grad_norm": 3.657109498977661,
        "learning_rate": 1.3257529804698976e-05,
        "epoch": 1.8903067800979634,
        "step": 14665
    },
    {
        "loss": 1.2712,
        "grad_norm": 1.2388845682144165,
        "learning_rate": 1.3227273622633463e-05,
        "epoch": 1.8904356792987884,
        "step": 14666
    },
    {
        "loss": 0.9657,
        "grad_norm": 3.574557304382324,
        "learning_rate": 1.3197049559905283e-05,
        "epoch": 1.8905645784996135,
        "step": 14667
    },
    {
        "loss": 1.5146,
        "grad_norm": 2.994896411895752,
        "learning_rate": 1.3166857627702022e-05,
        "epoch": 1.8906934777004383,
        "step": 14668
    },
    {
        "loss": 0.9088,
        "grad_norm": 3.7491328716278076,
        "learning_rate": 1.3136697837199297e-05,
        "epoch": 1.890822376901263,
        "step": 14669
    },
    {
        "loss": 1.332,
        "grad_norm": 1.813970923423767,
        "learning_rate": 1.3106570199561019e-05,
        "epoch": 1.8909512761020881,
        "step": 14670
    },
    {
        "loss": 1.1203,
        "grad_norm": 3.6118266582489014,
        "learning_rate": 1.3076474725939103e-05,
        "epoch": 1.8910801753029132,
        "step": 14671
    },
    {
        "loss": 1.8693,
        "grad_norm": 2.628570079803467,
        "learning_rate": 1.3046411427473315e-05,
        "epoch": 1.8912090745037382,
        "step": 14672
    },
    {
        "loss": 1.3995,
        "grad_norm": 2.8426215648651123,
        "learning_rate": 1.3016380315291943e-05,
        "epoch": 1.891337973704563,
        "step": 14673
    },
    {
        "loss": 1.6988,
        "grad_norm": 1.3979179859161377,
        "learning_rate": 1.298638140051106e-05,
        "epoch": 1.8914668729053878,
        "step": 14674
    },
    {
        "loss": 1.3561,
        "grad_norm": 3.4823038578033447,
        "learning_rate": 1.2956414694234831e-05,
        "epoch": 1.8915957721062129,
        "step": 14675
    },
    {
        "loss": 1.1064,
        "grad_norm": 3.2587203979492188,
        "learning_rate": 1.2926480207555746e-05,
        "epoch": 1.891724671307038,
        "step": 14676
    },
    {
        "loss": 1.2777,
        "grad_norm": 2.4525063037872314,
        "learning_rate": 1.2896577951554167e-05,
        "epoch": 1.891853570507863,
        "step": 14677
    },
    {
        "loss": 1.6695,
        "grad_norm": 1.5667575597763062,
        "learning_rate": 1.2866707937298383e-05,
        "epoch": 1.8919824697086878,
        "step": 14678
    },
    {
        "loss": 1.0057,
        "grad_norm": 2.1168718338012695,
        "learning_rate": 1.2836870175845096e-05,
        "epoch": 1.8921113689095128,
        "step": 14679
    },
    {
        "loss": 1.6105,
        "grad_norm": 3.280726909637451,
        "learning_rate": 1.2807064678238833e-05,
        "epoch": 1.8922402681103376,
        "step": 14680
    },
    {
        "loss": 1.7981,
        "grad_norm": 2.339292526245117,
        "learning_rate": 1.2777291455512231e-05,
        "epoch": 1.8923691673111627,
        "step": 14681
    },
    {
        "loss": 1.6736,
        "grad_norm": 2.5189778804779053,
        "learning_rate": 1.2747550518686024e-05,
        "epoch": 1.8924980665119877,
        "step": 14682
    },
    {
        "loss": 2.7624,
        "grad_norm": 1.954452633857727,
        "learning_rate": 1.2717841878768943e-05,
        "epoch": 1.8926269657128125,
        "step": 14683
    },
    {
        "loss": 1.523,
        "grad_norm": 2.7748053073883057,
        "learning_rate": 1.268816554675778e-05,
        "epoch": 1.8927558649136376,
        "step": 14684
    },
    {
        "loss": 1.6868,
        "grad_norm": 2.0994391441345215,
        "learning_rate": 1.265852153363738e-05,
        "epoch": 1.8928847641144624,
        "step": 14685
    },
    {
        "loss": 1.5164,
        "grad_norm": 2.743121862411499,
        "learning_rate": 1.2628909850380533e-05,
        "epoch": 1.8930136633152874,
        "step": 14686
    },
    {
        "loss": 1.588,
        "grad_norm": 2.383892059326172,
        "learning_rate": 1.2599330507948259e-05,
        "epoch": 1.8931425625161125,
        "step": 14687
    },
    {
        "loss": 2.2018,
        "grad_norm": 2.00941801071167,
        "learning_rate": 1.2569783517289534e-05,
        "epoch": 1.8932714617169375,
        "step": 14688
    },
    {
        "loss": 1.9824,
        "grad_norm": 1.6001478433609009,
        "learning_rate": 1.2540268889341111e-05,
        "epoch": 1.8934003609177623,
        "step": 14689
    },
    {
        "loss": 1.1279,
        "grad_norm": 2.909970760345459,
        "learning_rate": 1.251078663502816e-05,
        "epoch": 1.8935292601185871,
        "step": 14690
    },
    {
        "loss": 1.3164,
        "grad_norm": 3.0007901191711426,
        "learning_rate": 1.2481336765263618e-05,
        "epoch": 1.8936581593194122,
        "step": 14691
    },
    {
        "loss": 2.3145,
        "grad_norm": 2.0909535884857178,
        "learning_rate": 1.2451919290948388e-05,
        "epoch": 1.8937870585202372,
        "step": 14692
    },
    {
        "loss": 1.9409,
        "grad_norm": 2.128805160522461,
        "learning_rate": 1.2422534222971682e-05,
        "epoch": 1.8939159577210622,
        "step": 14693
    },
    {
        "loss": 2.0632,
        "grad_norm": 2.5103461742401123,
        "learning_rate": 1.2393181572210488e-05,
        "epoch": 1.894044856921887,
        "step": 14694
    },
    {
        "loss": 2.0244,
        "grad_norm": 2.5468409061431885,
        "learning_rate": 1.2363861349529649e-05,
        "epoch": 1.8941737561227119,
        "step": 14695
    },
    {
        "loss": 2.229,
        "grad_norm": 2.186889886856079,
        "learning_rate": 1.2334573565782359e-05,
        "epoch": 1.894302655323537,
        "step": 14696
    },
    {
        "loss": 1.1688,
        "grad_norm": 3.4691991806030273,
        "learning_rate": 1.2305318231809582e-05,
        "epoch": 1.894431554524362,
        "step": 14697
    },
    {
        "loss": 1.3854,
        "grad_norm": 2.5558176040649414,
        "learning_rate": 1.2276095358440286e-05,
        "epoch": 1.894560453725187,
        "step": 14698
    },
    {
        "loss": 1.6101,
        "grad_norm": 2.600863218307495,
        "learning_rate": 1.2246904956491512e-05,
        "epoch": 1.8946893529260118,
        "step": 14699
    },
    {
        "loss": 1.3171,
        "grad_norm": 2.871330738067627,
        "learning_rate": 1.2217747036768201e-05,
        "epoch": 1.8948182521268369,
        "step": 14700
    },
    {
        "loss": 1.42,
        "grad_norm": 2.7990341186523438,
        "learning_rate": 1.218862161006329e-05,
        "epoch": 1.8949471513276617,
        "step": 14701
    },
    {
        "loss": 1.4135,
        "grad_norm": 1.871008276939392,
        "learning_rate": 1.2159528687157695e-05,
        "epoch": 1.8950760505284867,
        "step": 14702
    },
    {
        "loss": 1.503,
        "grad_norm": 2.5473921298980713,
        "learning_rate": 1.2130468278820218e-05,
        "epoch": 1.8952049497293117,
        "step": 14703
    },
    {
        "loss": 1.8965,
        "grad_norm": 2.6033124923706055,
        "learning_rate": 1.2101440395807873e-05,
        "epoch": 1.8953338489301368,
        "step": 14704
    },
    {
        "loss": 1.7991,
        "grad_norm": 2.14886474609375,
        "learning_rate": 1.2072445048865456e-05,
        "epoch": 1.8954627481309616,
        "step": 14705
    },
    {
        "loss": 1.0389,
        "grad_norm": 2.3364405632019043,
        "learning_rate": 1.2043482248725534e-05,
        "epoch": 1.8955916473317864,
        "step": 14706
    },
    {
        "loss": 1.7144,
        "grad_norm": 2.3856029510498047,
        "learning_rate": 1.2014552006109026e-05,
        "epoch": 1.8957205465326115,
        "step": 14707
    },
    {
        "loss": 1.6663,
        "grad_norm": 2.3590919971466064,
        "learning_rate": 1.1985654331724516e-05,
        "epoch": 1.8958494457334365,
        "step": 14708
    },
    {
        "loss": 2.3509,
        "grad_norm": 1.5800262689590454,
        "learning_rate": 1.195678923626855e-05,
        "epoch": 1.8959783449342615,
        "step": 14709
    },
    {
        "loss": 1.1429,
        "grad_norm": 2.8796825408935547,
        "learning_rate": 1.1927956730425838e-05,
        "epoch": 1.8961072441350864,
        "step": 14710
    },
    {
        "loss": 1.8768,
        "grad_norm": 2.9187076091766357,
        "learning_rate": 1.1899156824868862e-05,
        "epoch": 1.8962361433359112,
        "step": 14711
    },
    {
        "loss": 1.4116,
        "grad_norm": 4.359245300292969,
        "learning_rate": 1.187038953025783e-05,
        "epoch": 1.8963650425367362,
        "step": 14712
    },
    {
        "loss": 1.7044,
        "grad_norm": 2.618398904800415,
        "learning_rate": 1.1841654857241313e-05,
        "epoch": 1.8964939417375613,
        "step": 14713
    },
    {
        "loss": 1.6625,
        "grad_norm": 3.8846275806427,
        "learning_rate": 1.181295281645548e-05,
        "epoch": 1.8966228409383863,
        "step": 14714
    },
    {
        "loss": 1.9967,
        "grad_norm": 1.6072889566421509,
        "learning_rate": 1.1784283418524578e-05,
        "epoch": 1.896751740139211,
        "step": 14715
    },
    {
        "loss": 1.6139,
        "grad_norm": 2.159501791000366,
        "learning_rate": 1.1755646674060694e-05,
        "epoch": 1.8968806393400361,
        "step": 14716
    },
    {
        "loss": 1.65,
        "grad_norm": 2.50620436668396,
        "learning_rate": 1.1727042593663862e-05,
        "epoch": 1.897009538540861,
        "step": 14717
    },
    {
        "loss": 1.7269,
        "grad_norm": 2.2120509147644043,
        "learning_rate": 1.169847118792201e-05,
        "epoch": 1.897138437741686,
        "step": 14718
    },
    {
        "loss": 1.267,
        "grad_norm": 1.962225079536438,
        "learning_rate": 1.1669932467411004e-05,
        "epoch": 1.897267336942511,
        "step": 14719
    },
    {
        "loss": 1.9523,
        "grad_norm": 1.45888352394104,
        "learning_rate": 1.164142644269447e-05,
        "epoch": 1.8973962361433359,
        "step": 14720
    },
    {
        "loss": 1.8739,
        "grad_norm": 2.636229991912842,
        "learning_rate": 1.1612953124324233e-05,
        "epoch": 1.897525135344161,
        "step": 14721
    },
    {
        "loss": 1.8328,
        "grad_norm": 2.3703904151916504,
        "learning_rate": 1.1584512522839752e-05,
        "epoch": 1.8976540345449857,
        "step": 14722
    },
    {
        "loss": 2.4821,
        "grad_norm": 2.525498867034912,
        "learning_rate": 1.1556104648768428e-05,
        "epoch": 1.8977829337458108,
        "step": 14723
    },
    {
        "loss": 2.1281,
        "grad_norm": 1.7639193534851074,
        "learning_rate": 1.1527729512625596e-05,
        "epoch": 1.8979118329466358,
        "step": 14724
    },
    {
        "loss": 1.5861,
        "grad_norm": 2.9994773864746094,
        "learning_rate": 1.1499387124914429e-05,
        "epoch": 1.8980407321474608,
        "step": 14725
    },
    {
        "loss": 0.7189,
        "grad_norm": 1.5247092247009277,
        "learning_rate": 1.1471077496125915e-05,
        "epoch": 1.8981696313482856,
        "step": 14726
    },
    {
        "loss": 1.6474,
        "grad_norm": 1.2599223852157593,
        "learning_rate": 1.1442800636739171e-05,
        "epoch": 1.8982985305491105,
        "step": 14727
    },
    {
        "loss": 0.8985,
        "grad_norm": 3.3228719234466553,
        "learning_rate": 1.141455655722099e-05,
        "epoch": 1.8984274297499355,
        "step": 14728
    },
    {
        "loss": 1.8601,
        "grad_norm": 1.7693991661071777,
        "learning_rate": 1.1386345268025866e-05,
        "epoch": 1.8985563289507605,
        "step": 14729
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.25350022315979,
        "learning_rate": 1.1358166779596524e-05,
        "epoch": 1.8986852281515856,
        "step": 14730
    },
    {
        "loss": 1.5823,
        "grad_norm": 2.3465373516082764,
        "learning_rate": 1.1330021102363275e-05,
        "epoch": 1.8988141273524104,
        "step": 14731
    },
    {
        "loss": 1.8819,
        "grad_norm": 2.1931352615356445,
        "learning_rate": 1.1301908246744441e-05,
        "epoch": 1.8989430265532352,
        "step": 14732
    },
    {
        "loss": 1.4019,
        "grad_norm": 1.4053877592086792,
        "learning_rate": 1.1273828223146089e-05,
        "epoch": 1.8990719257540603,
        "step": 14733
    },
    {
        "loss": 0.6751,
        "grad_norm": 2.3623368740081787,
        "learning_rate": 1.1245781041962166e-05,
        "epoch": 1.8992008249548853,
        "step": 14734
    },
    {
        "loss": 1.5258,
        "grad_norm": 2.640543222427368,
        "learning_rate": 1.1217766713574495e-05,
        "epoch": 1.8993297241557103,
        "step": 14735
    },
    {
        "loss": 1.8866,
        "grad_norm": 2.451942205429077,
        "learning_rate": 1.1189785248352703e-05,
        "epoch": 1.8994586233565351,
        "step": 14736
    },
    {
        "loss": 1.7119,
        "grad_norm": 2.7176406383514404,
        "learning_rate": 1.1161836656654168e-05,
        "epoch": 1.8995875225573602,
        "step": 14737
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.3799619674682617,
        "learning_rate": 1.113392094882436e-05,
        "epoch": 1.899716421758185,
        "step": 14738
    },
    {
        "loss": 0.6865,
        "grad_norm": 1.9692999124526978,
        "learning_rate": 1.110603813519635e-05,
        "epoch": 1.89984532095901,
        "step": 14739
    },
    {
        "loss": 1.7973,
        "grad_norm": 1.947681188583374,
        "learning_rate": 1.1078188226091068e-05,
        "epoch": 1.899974220159835,
        "step": 14740
    },
    {
        "loss": 1.447,
        "grad_norm": 2.014584541320801,
        "learning_rate": 1.1050371231817314e-05,
        "epoch": 1.9001031193606601,
        "step": 14741
    },
    {
        "loss": 0.961,
        "grad_norm": 2.5865304470062256,
        "learning_rate": 1.1022587162671639e-05,
        "epoch": 1.900232018561485,
        "step": 14742
    },
    {
        "loss": 0.5526,
        "grad_norm": 2.6521623134613037,
        "learning_rate": 1.0994836028938449e-05,
        "epoch": 1.9003609177623098,
        "step": 14743
    },
    {
        "loss": 1.5793,
        "grad_norm": 2.6462619304656982,
        "learning_rate": 1.096711784089004e-05,
        "epoch": 1.9004898169631348,
        "step": 14744
    },
    {
        "loss": 1.4321,
        "grad_norm": 2.1873157024383545,
        "learning_rate": 1.093943260878637e-05,
        "epoch": 1.9006187161639598,
        "step": 14745
    },
    {
        "loss": 1.8081,
        "grad_norm": 2.814232110977173,
        "learning_rate": 1.0911780342875278e-05,
        "epoch": 1.9007476153647849,
        "step": 14746
    },
    {
        "loss": 1.5448,
        "grad_norm": 2.6287434101104736,
        "learning_rate": 1.088416105339235e-05,
        "epoch": 1.9008765145656097,
        "step": 14747
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.3180158138275146,
        "learning_rate": 1.0856574750561044e-05,
        "epoch": 1.9010054137664345,
        "step": 14748
    },
    {
        "loss": 1.4922,
        "grad_norm": 2.828660726547241,
        "learning_rate": 1.0829021444592535e-05,
        "epoch": 1.9011343129672595,
        "step": 14749
    },
    {
        "loss": 1.5113,
        "grad_norm": 2.9687764644622803,
        "learning_rate": 1.080150114568581e-05,
        "epoch": 1.9012632121680846,
        "step": 14750
    },
    {
        "loss": 1.5734,
        "grad_norm": 2.4210712909698486,
        "learning_rate": 1.0774013864027633e-05,
        "epoch": 1.9013921113689096,
        "step": 14751
    },
    {
        "loss": 1.6722,
        "grad_norm": 2.3866896629333496,
        "learning_rate": 1.0746559609792562e-05,
        "epoch": 1.9015210105697344,
        "step": 14752
    },
    {
        "loss": 1.6108,
        "grad_norm": 4.001289367675781,
        "learning_rate": 1.0719138393142903e-05,
        "epoch": 1.9016499097705595,
        "step": 14753
    },
    {
        "loss": 1.4053,
        "grad_norm": 2.348738670349121,
        "learning_rate": 1.0691750224228713e-05,
        "epoch": 1.9017788089713843,
        "step": 14754
    },
    {
        "loss": 1.7973,
        "grad_norm": 3.632842540740967,
        "learning_rate": 1.0664395113187975e-05,
        "epoch": 1.9019077081722093,
        "step": 14755
    },
    {
        "loss": 1.7648,
        "grad_norm": 3.119987726211548,
        "learning_rate": 1.063707307014623e-05,
        "epoch": 1.9020366073730344,
        "step": 14756
    },
    {
        "loss": 2.0438,
        "grad_norm": 2.451209545135498,
        "learning_rate": 1.0609784105216892e-05,
        "epoch": 1.9021655065738592,
        "step": 14757
    },
    {
        "loss": 1.7228,
        "grad_norm": 1.8731178045272827,
        "learning_rate": 1.0582528228501065e-05,
        "epoch": 1.9022944057746842,
        "step": 14758
    },
    {
        "loss": 2.0716,
        "grad_norm": 2.3215248584747314,
        "learning_rate": 1.0555305450087649e-05,
        "epoch": 1.902423304975509,
        "step": 14759
    },
    {
        "loss": 1.866,
        "grad_norm": 2.1335017681121826,
        "learning_rate": 1.0528115780053249e-05,
        "epoch": 1.902552204176334,
        "step": 14760
    },
    {
        "loss": 1.0573,
        "grad_norm": 3.7083566188812256,
        "learning_rate": 1.050095922846236e-05,
        "epoch": 1.9026811033771591,
        "step": 14761
    },
    {
        "loss": 1.9077,
        "grad_norm": 2.0901694297790527,
        "learning_rate": 1.047383580536705e-05,
        "epoch": 1.9028100025779842,
        "step": 14762
    },
    {
        "loss": 0.9601,
        "grad_norm": 3.2615792751312256,
        "learning_rate": 1.0446745520807177e-05,
        "epoch": 1.902938901778809,
        "step": 14763
    },
    {
        "loss": 1.3844,
        "grad_norm": 1.719972848892212,
        "learning_rate": 1.0419688384810322e-05,
        "epoch": 1.9030678009796338,
        "step": 14764
    },
    {
        "loss": 2.0767,
        "grad_norm": 2.2947804927825928,
        "learning_rate": 1.0392664407391839e-05,
        "epoch": 1.9031967001804588,
        "step": 14765
    },
    {
        "loss": 1.1796,
        "grad_norm": 3.2955172061920166,
        "learning_rate": 1.0365673598554793e-05,
        "epoch": 1.9033255993812839,
        "step": 14766
    },
    {
        "loss": 1.9623,
        "grad_norm": 1.5359669923782349,
        "learning_rate": 1.0338715968289924e-05,
        "epoch": 1.903454498582109,
        "step": 14767
    },
    {
        "loss": 1.1689,
        "grad_norm": 3.069453477859497,
        "learning_rate": 1.0311791526575754e-05,
        "epoch": 1.9035833977829337,
        "step": 14768
    },
    {
        "loss": 1.4515,
        "grad_norm": 2.271244525909424,
        "learning_rate": 1.0284900283378485e-05,
        "epoch": 1.9037122969837585,
        "step": 14769
    },
    {
        "loss": 2.5872,
        "grad_norm": 1.7469007968902588,
        "learning_rate": 1.0258042248652011e-05,
        "epoch": 1.9038411961845836,
        "step": 14770
    },
    {
        "loss": 1.1161,
        "grad_norm": 2.47232723236084,
        "learning_rate": 1.0231217432337947e-05,
        "epoch": 1.9039700953854086,
        "step": 14771
    },
    {
        "loss": 1.5045,
        "grad_norm": 2.9919421672821045,
        "learning_rate": 1.0204425844365729e-05,
        "epoch": 1.9040989945862337,
        "step": 14772
    },
    {
        "loss": 1.0414,
        "grad_norm": 1.8581818342208862,
        "learning_rate": 1.0177667494652332e-05,
        "epoch": 1.9042278937870585,
        "step": 14773
    },
    {
        "loss": 1.8083,
        "grad_norm": 3.7348222732543945,
        "learning_rate": 1.0150942393102481e-05,
        "epoch": 1.9043567929878835,
        "step": 14774
    },
    {
        "loss": 2.1371,
        "grad_norm": 1.896618366241455,
        "learning_rate": 1.0124250549608572e-05,
        "epoch": 1.9044856921887083,
        "step": 14775
    },
    {
        "loss": 2.284,
        "grad_norm": 2.1252992153167725,
        "learning_rate": 1.009759197405078e-05,
        "epoch": 1.9046145913895334,
        "step": 14776
    },
    {
        "loss": 1.5602,
        "grad_norm": 2.5322959423065186,
        "learning_rate": 1.00709666762968e-05,
        "epoch": 1.9047434905903584,
        "step": 14777
    },
    {
        "loss": 0.4044,
        "grad_norm": 2.019376039505005,
        "learning_rate": 1.004437466620225e-05,
        "epoch": 1.9048723897911835,
        "step": 14778
    },
    {
        "loss": 2.0726,
        "grad_norm": 2.238023281097412,
        "learning_rate": 1.001781595361021e-05,
        "epoch": 1.9050012889920083,
        "step": 14779
    },
    {
        "loss": 1.9862,
        "grad_norm": 2.2691473960876465,
        "learning_rate": 9.991290548351517e-06,
        "epoch": 1.905130188192833,
        "step": 14780
    },
    {
        "loss": 1.5245,
        "grad_norm": 2.935110569000244,
        "learning_rate": 9.964798460244706e-06,
        "epoch": 1.9052590873936581,
        "step": 14781
    },
    {
        "loss": 2.3323,
        "grad_norm": 1.9831169843673706,
        "learning_rate": 9.938339699095922e-06,
        "epoch": 1.9053879865944832,
        "step": 14782
    },
    {
        "loss": 1.4988,
        "grad_norm": 2.7728779315948486,
        "learning_rate": 9.911914274699008e-06,
        "epoch": 1.9055168857953082,
        "step": 14783
    },
    {
        "loss": 1.4329,
        "grad_norm": 2.7680587768554688,
        "learning_rate": 9.885522196835462e-06,
        "epoch": 1.905645784996133,
        "step": 14784
    },
    {
        "loss": 1.6019,
        "grad_norm": 2.153151035308838,
        "learning_rate": 9.85916347527438e-06,
        "epoch": 1.9057746841969578,
        "step": 14785
    },
    {
        "loss": 1.8973,
        "grad_norm": 1.9591684341430664,
        "learning_rate": 9.832838119772714e-06,
        "epoch": 1.9059035833977829,
        "step": 14786
    },
    {
        "loss": 1.4432,
        "grad_norm": 2.4180784225463867,
        "learning_rate": 9.80654614007479e-06,
        "epoch": 1.906032482598608,
        "step": 14787
    },
    {
        "loss": 2.0369,
        "grad_norm": 2.2528676986694336,
        "learning_rate": 9.780287545912692e-06,
        "epoch": 1.906161381799433,
        "step": 14788
    },
    {
        "loss": 1.9357,
        "grad_norm": 2.591057300567627,
        "learning_rate": 9.754062347006276e-06,
        "epoch": 1.9062902810002578,
        "step": 14789
    },
    {
        "loss": 1.0824,
        "grad_norm": 3.303546190261841,
        "learning_rate": 9.727870553062868e-06,
        "epoch": 1.9064191802010828,
        "step": 14790
    },
    {
        "loss": 1.6413,
        "grad_norm": 2.9945809841156006,
        "learning_rate": 9.701712173777488e-06,
        "epoch": 1.9065480794019076,
        "step": 14791
    },
    {
        "loss": 1.6345,
        "grad_norm": 2.647671699523926,
        "learning_rate": 9.67558721883276e-06,
        "epoch": 1.9066769786027327,
        "step": 14792
    },
    {
        "loss": 1.9076,
        "grad_norm": 3.0759363174438477,
        "learning_rate": 9.649495697899003e-06,
        "epoch": 1.9068058778035577,
        "step": 14793
    },
    {
        "loss": 2.3604,
        "grad_norm": 1.734073281288147,
        "learning_rate": 9.623437620634035e-06,
        "epoch": 1.9069347770043825,
        "step": 14794
    },
    {
        "loss": 1.3745,
        "grad_norm": 2.7852041721343994,
        "learning_rate": 9.597412996683496e-06,
        "epoch": 1.9070636762052076,
        "step": 14795
    },
    {
        "loss": 1.673,
        "grad_norm": 2.3077783584594727,
        "learning_rate": 9.571421835680461e-06,
        "epoch": 1.9071925754060324,
        "step": 14796
    },
    {
        "loss": 1.152,
        "grad_norm": 2.5200469493865967,
        "learning_rate": 9.545464147245675e-06,
        "epoch": 1.9073214746068574,
        "step": 14797
    },
    {
        "loss": 0.4207,
        "grad_norm": 3.131596088409424,
        "learning_rate": 9.519539940987532e-06,
        "epoch": 1.9074503738076825,
        "step": 14798
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.7992881536483765,
        "learning_rate": 9.493649226501978e-06,
        "epoch": 1.9075792730085075,
        "step": 14799
    },
    {
        "loss": 1.4936,
        "grad_norm": 2.830904006958008,
        "learning_rate": 9.46779201337259e-06,
        "epoch": 1.9077081722093323,
        "step": 14800
    },
    {
        "loss": 1.9439,
        "grad_norm": 1.5070728063583374,
        "learning_rate": 9.441968311170534e-06,
        "epoch": 1.9078370714101571,
        "step": 14801
    },
    {
        "loss": 0.6845,
        "grad_norm": 2.4568867683410645,
        "learning_rate": 9.4161781294545e-06,
        "epoch": 1.9079659706109822,
        "step": 14802
    },
    {
        "loss": 1.1736,
        "grad_norm": 3.315718650817871,
        "learning_rate": 9.39042147777105e-06,
        "epoch": 1.9080948698118072,
        "step": 14803
    },
    {
        "loss": 1.1476,
        "grad_norm": 1.5652803182601929,
        "learning_rate": 9.364698365653957e-06,
        "epoch": 1.9082237690126322,
        "step": 14804
    },
    {
        "loss": 1.5277,
        "grad_norm": 2.888798236846924,
        "learning_rate": 9.33900880262475e-06,
        "epoch": 1.908352668213457,
        "step": 14805
    },
    {
        "loss": 1.8816,
        "grad_norm": 2.4459095001220703,
        "learning_rate": 9.313352798192631e-06,
        "epoch": 1.9084815674142819,
        "step": 14806
    },
    {
        "loss": 1.8918,
        "grad_norm": 3.10801100730896,
        "learning_rate": 9.287730361854274e-06,
        "epoch": 1.908610466615107,
        "step": 14807
    },
    {
        "loss": 0.5307,
        "grad_norm": 1.2715718746185303,
        "learning_rate": 9.262141503093834e-06,
        "epoch": 1.908739365815932,
        "step": 14808
    },
    {
        "loss": 1.6007,
        "grad_norm": 2.6227006912231445,
        "learning_rate": 9.236586231383393e-06,
        "epoch": 1.908868265016757,
        "step": 14809
    },
    {
        "loss": 1.2034,
        "grad_norm": 2.5520830154418945,
        "learning_rate": 9.211064556182137e-06,
        "epoch": 1.9089971642175818,
        "step": 14810
    },
    {
        "loss": 1.6974,
        "grad_norm": 2.573975086212158,
        "learning_rate": 9.185576486937042e-06,
        "epoch": 1.9091260634184068,
        "step": 14811
    },
    {
        "loss": 1.4431,
        "grad_norm": 2.4839577674865723,
        "learning_rate": 9.160122033082763e-06,
        "epoch": 1.9092549626192317,
        "step": 14812
    },
    {
        "loss": 1.3879,
        "grad_norm": 3.3494555950164795,
        "learning_rate": 9.134701204041308e-06,
        "epoch": 1.9093838618200567,
        "step": 14813
    },
    {
        "loss": 1.5447,
        "grad_norm": 2.906411647796631,
        "learning_rate": 9.109314009222336e-06,
        "epoch": 1.9095127610208817,
        "step": 14814
    },
    {
        "loss": 1.8368,
        "grad_norm": 2.3615646362304688,
        "learning_rate": 9.083960458023055e-06,
        "epoch": 1.9096416602217068,
        "step": 14815
    },
    {
        "loss": 1.7967,
        "grad_norm": 1.5162230730056763,
        "learning_rate": 9.058640559828169e-06,
        "epoch": 1.9097705594225316,
        "step": 14816
    },
    {
        "loss": 0.9045,
        "grad_norm": 2.4793107509613037,
        "learning_rate": 9.033354324009979e-06,
        "epoch": 1.9098994586233564,
        "step": 14817
    },
    {
        "loss": 1.7551,
        "grad_norm": 3.552773952484131,
        "learning_rate": 9.00810175992829e-06,
        "epoch": 1.9100283578241815,
        "step": 14818
    },
    {
        "loss": 0.6528,
        "grad_norm": 3.2671682834625244,
        "learning_rate": 8.982882876930399e-06,
        "epoch": 1.9101572570250065,
        "step": 14819
    },
    {
        "loss": 0.3312,
        "grad_norm": 1.3187460899353027,
        "learning_rate": 8.957697684351395e-06,
        "epoch": 1.9102861562258315,
        "step": 14820
    },
    {
        "loss": 1.8566,
        "grad_norm": 3.57932448387146,
        "learning_rate": 8.932546191513513e-06,
        "epoch": 1.9104150554266563,
        "step": 14821
    },
    {
        "loss": 1.4836,
        "grad_norm": 3.0934793949127197,
        "learning_rate": 8.907428407726692e-06,
        "epoch": 1.9105439546274812,
        "step": 14822
    },
    {
        "loss": 1.476,
        "grad_norm": 2.3815553188323975,
        "learning_rate": 8.882344342288518e-06,
        "epoch": 1.9106728538283062,
        "step": 14823
    },
    {
        "loss": 0.6026,
        "grad_norm": 1.9071885347366333,
        "learning_rate": 8.857294004483907e-06,
        "epoch": 1.9108017530291312,
        "step": 14824
    },
    {
        "loss": 1.6317,
        "grad_norm": 2.0249898433685303,
        "learning_rate": 8.832277403585298e-06,
        "epoch": 1.9109306522299563,
        "step": 14825
    },
    {
        "loss": 1.6572,
        "grad_norm": 2.607736825942993,
        "learning_rate": 8.807294548852908e-06,
        "epoch": 1.911059551430781,
        "step": 14826
    },
    {
        "loss": 1.8103,
        "grad_norm": 2.9035449028015137,
        "learning_rate": 8.78234544953408e-06,
        "epoch": 1.9111884506316061,
        "step": 14827
    },
    {
        "loss": 1.0163,
        "grad_norm": 1.6764953136444092,
        "learning_rate": 8.75743011486383e-06,
        "epoch": 1.911317349832431,
        "step": 14828
    },
    {
        "loss": 1.8695,
        "grad_norm": 2.0596883296966553,
        "learning_rate": 8.732548554064801e-06,
        "epoch": 1.911446249033256,
        "step": 14829
    },
    {
        "loss": 0.9994,
        "grad_norm": 2.4657883644104004,
        "learning_rate": 8.70770077634695e-06,
        "epoch": 1.911575148234081,
        "step": 14830
    },
    {
        "loss": 0.866,
        "grad_norm": 1.9449350833892822,
        "learning_rate": 8.68288679090784e-06,
        "epoch": 1.9117040474349059,
        "step": 14831
    },
    {
        "loss": 0.9898,
        "grad_norm": 3.497431993484497,
        "learning_rate": 8.658106606932459e-06,
        "epoch": 1.911832946635731,
        "step": 14832
    },
    {
        "loss": 1.1636,
        "grad_norm": 2.8191535472869873,
        "learning_rate": 8.633360233593313e-06,
        "epoch": 1.9119618458365557,
        "step": 14833
    },
    {
        "loss": 1.7903,
        "grad_norm": 2.140974998474121,
        "learning_rate": 8.608647680050396e-06,
        "epoch": 1.9120907450373807,
        "step": 14834
    },
    {
        "loss": 1.2873,
        "grad_norm": 2.2995235919952393,
        "learning_rate": 8.583968955451171e-06,
        "epoch": 1.9122196442382058,
        "step": 14835
    },
    {
        "loss": 0.8265,
        "grad_norm": 2.3538968563079834,
        "learning_rate": 8.559324068930518e-06,
        "epoch": 1.9123485434390308,
        "step": 14836
    },
    {
        "loss": 1.717,
        "grad_norm": 2.918724536895752,
        "learning_rate": 8.534713029611063e-06,
        "epoch": 1.9124774426398556,
        "step": 14837
    },
    {
        "loss": 1.8289,
        "grad_norm": 1.5763694047927856,
        "learning_rate": 8.510135846602496e-06,
        "epoch": 1.9126063418406805,
        "step": 14838
    },
    {
        "loss": 1.9908,
        "grad_norm": 1.9126932621002197,
        "learning_rate": 8.485592529002195e-06,
        "epoch": 1.9127352410415055,
        "step": 14839
    },
    {
        "loss": 0.7247,
        "grad_norm": 2.117230176925659,
        "learning_rate": 8.461083085895093e-06,
        "epoch": 1.9128641402423305,
        "step": 14840
    },
    {
        "loss": 1.117,
        "grad_norm": 2.489459276199341,
        "learning_rate": 8.436607526353401e-06,
        "epoch": 1.9129930394431556,
        "step": 14841
    },
    {
        "loss": 2.0271,
        "grad_norm": 10.233466148376465,
        "learning_rate": 8.412165859436848e-06,
        "epoch": 1.9131219386439804,
        "step": 14842
    },
    {
        "loss": 1.636,
        "grad_norm": 1.60981023311615,
        "learning_rate": 8.387758094192787e-06,
        "epoch": 1.9132508378448052,
        "step": 14843
    },
    {
        "loss": 1.2975,
        "grad_norm": 2.519284248352051,
        "learning_rate": 8.363384239655692e-06,
        "epoch": 1.9133797370456302,
        "step": 14844
    },
    {
        "loss": 2.3051,
        "grad_norm": 2.222076654434204,
        "learning_rate": 8.339044304847654e-06,
        "epoch": 1.9135086362464553,
        "step": 14845
    },
    {
        "loss": 1.1009,
        "grad_norm": 1.9868278503417969,
        "learning_rate": 8.314738298778323e-06,
        "epoch": 1.9136375354472803,
        "step": 14846
    },
    {
        "loss": 1.8959,
        "grad_norm": 2.451570749282837,
        "learning_rate": 8.290466230444671e-06,
        "epoch": 1.9137664346481051,
        "step": 14847
    },
    {
        "loss": 0.7769,
        "grad_norm": 2.2975988388061523,
        "learning_rate": 8.266228108831076e-06,
        "epoch": 1.9138953338489302,
        "step": 14848
    },
    {
        "loss": 1.0711,
        "grad_norm": 1.860059142112732,
        "learning_rate": 8.242023942909416e-06,
        "epoch": 1.914024233049755,
        "step": 14849
    },
    {
        "loss": 1.2943,
        "grad_norm": 3.3401060104370117,
        "learning_rate": 8.21785374163896e-06,
        "epoch": 1.91415313225058,
        "step": 14850
    },
    {
        "loss": 1.5068,
        "grad_norm": 2.3248729705810547,
        "learning_rate": 8.193717513966437e-06,
        "epoch": 1.914282031451405,
        "step": 14851
    },
    {
        "loss": 1.798,
        "grad_norm": 1.940399169921875,
        "learning_rate": 8.16961526882597e-06,
        "epoch": 1.9144109306522301,
        "step": 14852
    },
    {
        "loss": 1.3998,
        "grad_norm": 1.7607940435409546,
        "learning_rate": 8.145547015139088e-06,
        "epoch": 1.914539829853055,
        "step": 14853
    },
    {
        "loss": 1.6646,
        "grad_norm": 3.672665596008301,
        "learning_rate": 8.12151276181493e-06,
        "epoch": 1.9146687290538797,
        "step": 14854
    },
    {
        "loss": 1.8127,
        "grad_norm": 1.9261279106140137,
        "learning_rate": 8.097512517749705e-06,
        "epoch": 1.9147976282547048,
        "step": 14855
    },
    {
        "loss": 0.9774,
        "grad_norm": 2.2814884185791016,
        "learning_rate": 8.073546291827249e-06,
        "epoch": 1.9149265274555298,
        "step": 14856
    },
    {
        "loss": 1.666,
        "grad_norm": 2.3003528118133545,
        "learning_rate": 8.049614092918855e-06,
        "epoch": 1.9150554266563549,
        "step": 14857
    },
    {
        "loss": 1.7743,
        "grad_norm": 2.273271322250366,
        "learning_rate": 8.02571592988307e-06,
        "epoch": 1.9151843258571797,
        "step": 14858
    },
    {
        "loss": 1.8199,
        "grad_norm": 2.804886817932129,
        "learning_rate": 8.001851811565919e-06,
        "epoch": 1.9153132250580045,
        "step": 14859
    },
    {
        "loss": 1.7344,
        "grad_norm": 2.5004465579986572,
        "learning_rate": 7.978021746800934e-06,
        "epoch": 1.9154421242588295,
        "step": 14860
    },
    {
        "loss": 1.9229,
        "grad_norm": 1.412502646446228,
        "learning_rate": 7.954225744408772e-06,
        "epoch": 1.9155710234596546,
        "step": 14861
    },
    {
        "loss": 2.2904,
        "grad_norm": 2.5057942867279053,
        "learning_rate": 7.930463813197652e-06,
        "epoch": 1.9156999226604796,
        "step": 14862
    },
    {
        "loss": 1.2613,
        "grad_norm": 2.447699785232544,
        "learning_rate": 7.906735961963252e-06,
        "epoch": 1.9158288218613044,
        "step": 14863
    },
    {
        "loss": 1.4853,
        "grad_norm": 1.681166410446167,
        "learning_rate": 7.883042199488511e-06,
        "epoch": 1.9159577210621292,
        "step": 14864
    },
    {
        "loss": 1.6374,
        "grad_norm": 2.0640668869018555,
        "learning_rate": 7.859382534543791e-06,
        "epoch": 1.9160866202629543,
        "step": 14865
    },
    {
        "loss": 2.1685,
        "grad_norm": 1.5398389101028442,
        "learning_rate": 7.83575697588681e-06,
        "epoch": 1.9162155194637793,
        "step": 14866
    },
    {
        "loss": 1.0561,
        "grad_norm": 3.2021148204803467,
        "learning_rate": 7.812165532262706e-06,
        "epoch": 1.9163444186646044,
        "step": 14867
    },
    {
        "loss": 1.0044,
        "grad_norm": 3.4037132263183594,
        "learning_rate": 7.788608212403947e-06,
        "epoch": 1.9164733178654292,
        "step": 14868
    },
    {
        "loss": 1.6739,
        "grad_norm": 3.0056910514831543,
        "learning_rate": 7.765085025030373e-06,
        "epoch": 1.9166022170662542,
        "step": 14869
    },
    {
        "loss": 1.7014,
        "grad_norm": 2.5741024017333984,
        "learning_rate": 7.74159597884918e-06,
        "epoch": 1.916731116267079,
        "step": 14870
    },
    {
        "loss": 1.6606,
        "grad_norm": 2.705669641494751,
        "learning_rate": 7.718141082555052e-06,
        "epoch": 1.916860015467904,
        "step": 14871
    },
    {
        "loss": 1.9213,
        "grad_norm": 2.603793144226074,
        "learning_rate": 7.69472034482992e-06,
        "epoch": 1.9169889146687291,
        "step": 14872
    },
    {
        "loss": 1.944,
        "grad_norm": 3.9545974731445312,
        "learning_rate": 7.671333774342925e-06,
        "epoch": 1.9171178138695542,
        "step": 14873
    },
    {
        "loss": 1.0307,
        "grad_norm": 3.1123104095458984,
        "learning_rate": 7.647981379750857e-06,
        "epoch": 1.917246713070379,
        "step": 14874
    },
    {
        "loss": 1.919,
        "grad_norm": 2.581275463104248,
        "learning_rate": 7.624663169697699e-06,
        "epoch": 1.9173756122712038,
        "step": 14875
    },
    {
        "loss": 1.7416,
        "grad_norm": 2.4236605167388916,
        "learning_rate": 7.601379152814736e-06,
        "epoch": 1.9175045114720288,
        "step": 14876
    },
    {
        "loss": 1.4206,
        "grad_norm": 2.818671703338623,
        "learning_rate": 7.578129337720808e-06,
        "epoch": 1.9176334106728539,
        "step": 14877
    },
    {
        "loss": 1.3196,
        "grad_norm": 1.9468755722045898,
        "learning_rate": 7.554913733021773e-06,
        "epoch": 1.917762309873679,
        "step": 14878
    },
    {
        "loss": 1.9148,
        "grad_norm": 2.4112303256988525,
        "learning_rate": 7.531732347311005e-06,
        "epoch": 1.9178912090745037,
        "step": 14879
    },
    {
        "loss": 1.0235,
        "grad_norm": 2.7624688148498535,
        "learning_rate": 7.508585189169337e-06,
        "epoch": 1.9180201082753285,
        "step": 14880
    },
    {
        "loss": 1.3609,
        "grad_norm": 2.401461601257324,
        "learning_rate": 7.4854722671647175e-06,
        "epoch": 1.9181490074761536,
        "step": 14881
    },
    {
        "loss": 0.7181,
        "grad_norm": 2.60554838180542,
        "learning_rate": 7.462393589852512e-06,
        "epoch": 1.9182779066769786,
        "step": 14882
    },
    {
        "loss": 1.7294,
        "grad_norm": 2.597123146057129,
        "learning_rate": 7.4393491657754e-06,
        "epoch": 1.9184068058778037,
        "step": 14883
    },
    {
        "loss": 2.2268,
        "grad_norm": 2.8091886043548584,
        "learning_rate": 7.41633900346339e-06,
        "epoch": 1.9185357050786285,
        "step": 14884
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.134303331375122,
        "learning_rate": 7.3933631114337935e-06,
        "epoch": 1.9186646042794535,
        "step": 14885
    },
    {
        "loss": 1.0692,
        "grad_norm": 4.549428939819336,
        "learning_rate": 7.37042149819126e-06,
        "epoch": 1.9187935034802783,
        "step": 14886
    },
    {
        "loss": 1.4686,
        "grad_norm": 1.575565218925476,
        "learning_rate": 7.347514172227687e-06,
        "epoch": 1.9189224026811034,
        "step": 14887
    },
    {
        "loss": 0.9902,
        "grad_norm": 2.2492687702178955,
        "learning_rate": 7.324641142022415e-06,
        "epoch": 1.9190513018819284,
        "step": 14888
    },
    {
        "loss": 1.2821,
        "grad_norm": 1.6010475158691406,
        "learning_rate": 7.301802416042014e-06,
        "epoch": 1.9191802010827534,
        "step": 14889
    },
    {
        "loss": 1.6336,
        "grad_norm": 2.8094990253448486,
        "learning_rate": 7.2789980027402e-06,
        "epoch": 1.9193091002835783,
        "step": 14890
    },
    {
        "loss": 1.7911,
        "grad_norm": 3.2430036067962646,
        "learning_rate": 7.256227910558267e-06,
        "epoch": 1.919437999484403,
        "step": 14891
    },
    {
        "loss": 1.6926,
        "grad_norm": 1.6837406158447266,
        "learning_rate": 7.233492147924658e-06,
        "epoch": 1.9195668986852281,
        "step": 14892
    },
    {
        "loss": 1.3024,
        "grad_norm": 2.2274367809295654,
        "learning_rate": 7.210790723255045e-06,
        "epoch": 1.9196957978860532,
        "step": 14893
    },
    {
        "loss": 1.5405,
        "grad_norm": 2.5438132286071777,
        "learning_rate": 7.188123644952627e-06,
        "epoch": 1.9198246970868782,
        "step": 14894
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.933060884475708,
        "learning_rate": 7.16549092140758e-06,
        "epoch": 1.919953596287703,
        "step": 14895
    },
    {
        "loss": 1.4196,
        "grad_norm": 2.576747179031372,
        "learning_rate": 7.142892560997494e-06,
        "epoch": 1.9200824954885278,
        "step": 14896
    },
    {
        "loss": 1.2418,
        "grad_norm": 3.366640329360962,
        "learning_rate": 7.120328572087409e-06,
        "epoch": 1.9202113946893529,
        "step": 14897
    },
    {
        "loss": 1.7818,
        "grad_norm": 2.5114407539367676,
        "learning_rate": 7.0977989630294e-06,
        "epoch": 1.920340293890178,
        "step": 14898
    },
    {
        "loss": 1.6654,
        "grad_norm": 2.453885555267334,
        "learning_rate": 7.075303742162931e-06,
        "epoch": 1.920469193091003,
        "step": 14899
    },
    {
        "loss": 1.4149,
        "grad_norm": 3.2507033348083496,
        "learning_rate": 7.052842917814706e-06,
        "epoch": 1.9205980922918278,
        "step": 14900
    },
    {
        "loss": 1.5127,
        "grad_norm": 2.2092761993408203,
        "learning_rate": 7.030416498298698e-06,
        "epoch": 1.9207269914926526,
        "step": 14901
    },
    {
        "loss": 1.6132,
        "grad_norm": 2.7976362705230713,
        "learning_rate": 7.008024491916154e-06,
        "epoch": 1.9208558906934776,
        "step": 14902
    },
    {
        "loss": 1.2338,
        "grad_norm": 2.6630513668060303,
        "learning_rate": 6.985666906955601e-06,
        "epoch": 1.9209847898943027,
        "step": 14903
    },
    {
        "loss": 0.4255,
        "grad_norm": 2.827991008758545,
        "learning_rate": 6.9633437516927526e-06,
        "epoch": 1.9211136890951277,
        "step": 14904
    },
    {
        "loss": 0.829,
        "grad_norm": 2.374366521835327,
        "learning_rate": 6.941055034390709e-06,
        "epoch": 1.9212425882959525,
        "step": 14905
    },
    {
        "loss": 0.7954,
        "grad_norm": 2.9485437870025635,
        "learning_rate": 6.91880076329976e-06,
        "epoch": 1.9213714874967776,
        "step": 14906
    },
    {
        "loss": 1.4893,
        "grad_norm": 2.086526870727539,
        "learning_rate": 6.896580946657272e-06,
        "epoch": 1.9215003866976024,
        "step": 14907
    },
    {
        "loss": 1.9506,
        "grad_norm": 2.4869771003723145,
        "learning_rate": 6.874395592688176e-06,
        "epoch": 1.9216292858984274,
        "step": 14908
    },
    {
        "loss": 1.368,
        "grad_norm": 3.0089213848114014,
        "learning_rate": 6.852244709604438e-06,
        "epoch": 1.9217581850992524,
        "step": 14909
    },
    {
        "loss": 1.4911,
        "grad_norm": 2.172161817550659,
        "learning_rate": 6.830128305605255e-06,
        "epoch": 1.9218870843000775,
        "step": 14910
    },
    {
        "loss": 1.7764,
        "grad_norm": 3.3051390647888184,
        "learning_rate": 6.80804638887722e-06,
        "epoch": 1.9220159835009023,
        "step": 14911
    },
    {
        "loss": 1.7038,
        "grad_norm": 3.353020668029785,
        "learning_rate": 6.785998967594054e-06,
        "epoch": 1.9221448827017271,
        "step": 14912
    },
    {
        "loss": 1.4953,
        "grad_norm": 2.915515661239624,
        "learning_rate": 6.76398604991656e-06,
        "epoch": 1.9222737819025522,
        "step": 14913
    },
    {
        "loss": 1.1172,
        "grad_norm": 2.980710506439209,
        "learning_rate": 6.742007643993109e-06,
        "epoch": 1.9224026811033772,
        "step": 14914
    },
    {
        "loss": 0.7905,
        "grad_norm": 4.524786472320557,
        "learning_rate": 6.720063757959028e-06,
        "epoch": 1.9225315803042022,
        "step": 14915
    },
    {
        "loss": 1.4635,
        "grad_norm": 3.240935802459717,
        "learning_rate": 6.698154399936962e-06,
        "epoch": 1.922660479505027,
        "step": 14916
    },
    {
        "loss": 1.5768,
        "grad_norm": 2.581146240234375,
        "learning_rate": 6.676279578036771e-06,
        "epoch": 1.9227893787058519,
        "step": 14917
    },
    {
        "loss": 0.8874,
        "grad_norm": 3.0777649879455566,
        "learning_rate": 6.654439300355486e-06,
        "epoch": 1.922918277906677,
        "step": 14918
    },
    {
        "loss": 1.7085,
        "grad_norm": 2.258704423904419,
        "learning_rate": 6.632633574977443e-06,
        "epoch": 1.923047177107502,
        "step": 14919
    },
    {
        "loss": 1.9759,
        "grad_norm": 1.5413564443588257,
        "learning_rate": 6.610862409974117e-06,
        "epoch": 1.923176076308327,
        "step": 14920
    },
    {
        "loss": 1.7971,
        "grad_norm": 2.969792604446411,
        "learning_rate": 6.589125813404151e-06,
        "epoch": 1.9233049755091518,
        "step": 14921
    },
    {
        "loss": 1.6235,
        "grad_norm": 1.530999779701233,
        "learning_rate": 6.567423793313554e-06,
        "epoch": 1.9234338747099768,
        "step": 14922
    },
    {
        "loss": 1.679,
        "grad_norm": 2.8568522930145264,
        "learning_rate": 6.545756357735422e-06,
        "epoch": 1.9235627739108017,
        "step": 14923
    },
    {
        "loss": 0.666,
        "grad_norm": 2.437378406524658,
        "learning_rate": 6.524123514689917e-06,
        "epoch": 1.9236916731116267,
        "step": 14924
    },
    {
        "loss": 1.5452,
        "grad_norm": 2.745434284210205,
        "learning_rate": 6.50252527218469e-06,
        "epoch": 1.9238205723124517,
        "step": 14925
    },
    {
        "loss": 1.4456,
        "grad_norm": 2.3880438804626465,
        "learning_rate": 6.4809616382143885e-06,
        "epoch": 1.9239494715132768,
        "step": 14926
    },
    {
        "loss": 0.8296,
        "grad_norm": 3.7759809494018555,
        "learning_rate": 6.459432620760841e-06,
        "epoch": 1.9240783707141016,
        "step": 14927
    },
    {
        "loss": 1.8795,
        "grad_norm": 3.427391529083252,
        "learning_rate": 6.437938227793205e-06,
        "epoch": 1.9242072699149264,
        "step": 14928
    },
    {
        "loss": 1.7239,
        "grad_norm": 3.0822038650512695,
        "learning_rate": 6.416478467267739e-06,
        "epoch": 1.9243361691157514,
        "step": 14929
    },
    {
        "loss": 1.8263,
        "grad_norm": 1.6050969362258911,
        "learning_rate": 6.395053347127722e-06,
        "epoch": 1.9244650683165765,
        "step": 14930
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.5728031396865845,
        "learning_rate": 6.373662875303921e-06,
        "epoch": 1.9245939675174015,
        "step": 14931
    },
    {
        "loss": 1.4689,
        "grad_norm": 2.0286312103271484,
        "learning_rate": 6.35230705971408e-06,
        "epoch": 1.9247228667182263,
        "step": 14932
    },
    {
        "loss": 1.2366,
        "grad_norm": 1.7616604566574097,
        "learning_rate": 6.330985908263132e-06,
        "epoch": 1.9248517659190512,
        "step": 14933
    },
    {
        "loss": 0.5687,
        "grad_norm": 3.391761064529419,
        "learning_rate": 6.309699428843219e-06,
        "epoch": 1.9249806651198762,
        "step": 14934
    },
    {
        "loss": 0.8514,
        "grad_norm": 3.138833999633789,
        "learning_rate": 6.2884476293336155e-06,
        "epoch": 1.9251095643207012,
        "step": 14935
    },
    {
        "loss": 1.3644,
        "grad_norm": 3.7973742485046387,
        "learning_rate": 6.2672305176008085e-06,
        "epoch": 1.9252384635215263,
        "step": 14936
    },
    {
        "loss": 0.5562,
        "grad_norm": 2.1002941131591797,
        "learning_rate": 6.2460481014983805e-06,
        "epoch": 1.925367362722351,
        "step": 14937
    },
    {
        "loss": 0.9502,
        "grad_norm": 2.873004674911499,
        "learning_rate": 6.2249003888670835e-06,
        "epoch": 1.925496261923176,
        "step": 14938
    },
    {
        "loss": 2.3832,
        "grad_norm": 1.437626838684082,
        "learning_rate": 6.20378738753491e-06,
        "epoch": 1.925625161124001,
        "step": 14939
    },
    {
        "loss": 1.7601,
        "grad_norm": 1.7979108095169067,
        "learning_rate": 6.1827091053169635e-06,
        "epoch": 1.925754060324826,
        "step": 14940
    },
    {
        "loss": 1.7557,
        "grad_norm": 1.8060221672058105,
        "learning_rate": 6.1616655500152944e-06,
        "epoch": 1.925882959525651,
        "step": 14941
    },
    {
        "loss": 1.276,
        "grad_norm": 1.9823973178863525,
        "learning_rate": 6.140656729419459e-06,
        "epoch": 1.9260118587264758,
        "step": 14942
    },
    {
        "loss": 1.263,
        "grad_norm": 3.6274333000183105,
        "learning_rate": 6.1196826513058914e-06,
        "epoch": 1.9261407579273009,
        "step": 14943
    },
    {
        "loss": 1.9875,
        "grad_norm": 1.6910576820373535,
        "learning_rate": 6.098743323438227e-06,
        "epoch": 1.9262696571281257,
        "step": 14944
    },
    {
        "loss": 1.94,
        "grad_norm": 2.9195172786712646,
        "learning_rate": 6.077838753567322e-06,
        "epoch": 1.9263985563289507,
        "step": 14945
    },
    {
        "loss": 1.9172,
        "grad_norm": 2.5117006301879883,
        "learning_rate": 6.05696894943113e-06,
        "epoch": 1.9265274555297758,
        "step": 14946
    },
    {
        "loss": 0.7999,
        "grad_norm": 1.7574350833892822,
        "learning_rate": 6.03613391875455e-06,
        "epoch": 1.9266563547306008,
        "step": 14947
    },
    {
        "loss": 1.3201,
        "grad_norm": 1.9498964548110962,
        "learning_rate": 6.015333669249923e-06,
        "epoch": 1.9267852539314256,
        "step": 14948
    },
    {
        "loss": 1.8127,
        "grad_norm": 2.808391571044922,
        "learning_rate": 5.994568208616502e-06,
        "epoch": 1.9269141531322505,
        "step": 14949
    },
    {
        "loss": 1.9398,
        "grad_norm": 2.764859914779663,
        "learning_rate": 5.973837544540717e-06,
        "epoch": 1.9270430523330755,
        "step": 14950
    },
    {
        "loss": 2.205,
        "grad_norm": 3.757614850997925,
        "learning_rate": 5.953141684696118e-06,
        "epoch": 1.9271719515339005,
        "step": 14951
    },
    {
        "loss": 1.2307,
        "grad_norm": 1.9013254642486572,
        "learning_rate": 5.9324806367433985e-06,
        "epoch": 1.9273008507347256,
        "step": 14952
    },
    {
        "loss": 1.5648,
        "grad_norm": 2.7989392280578613,
        "learning_rate": 5.911854408330331e-06,
        "epoch": 1.9274297499355504,
        "step": 14953
    },
    {
        "loss": 1.2897,
        "grad_norm": 2.5039572715759277,
        "learning_rate": 5.891263007091807e-06,
        "epoch": 1.9275586491363752,
        "step": 14954
    },
    {
        "loss": 0.4201,
        "grad_norm": 1.1364015340805054,
        "learning_rate": 5.870706440649798e-06,
        "epoch": 1.9276875483372002,
        "step": 14955
    },
    {
        "loss": 1.3412,
        "grad_norm": 4.070107936859131,
        "learning_rate": 5.850184716613505e-06,
        "epoch": 1.9278164475380253,
        "step": 14956
    },
    {
        "loss": 1.5591,
        "grad_norm": 2.715564489364624,
        "learning_rate": 5.829697842579129e-06,
        "epoch": 1.9279453467388503,
        "step": 14957
    },
    {
        "loss": 0.5453,
        "grad_norm": 1.0329132080078125,
        "learning_rate": 5.8092458261298385e-06,
        "epoch": 1.9280742459396751,
        "step": 14958
    },
    {
        "loss": 1.9416,
        "grad_norm": 2.9107797145843506,
        "learning_rate": 5.788828674836211e-06,
        "epoch": 1.9282031451405002,
        "step": 14959
    },
    {
        "loss": 2.0027,
        "grad_norm": 1.3305771350860596,
        "learning_rate": 5.768446396255689e-06,
        "epoch": 1.928332044341325,
        "step": 14960
    },
    {
        "loss": 1.518,
        "grad_norm": 3.244408369064331,
        "learning_rate": 5.748098997932838e-06,
        "epoch": 1.92846094354215,
        "step": 14961
    },
    {
        "loss": 1.9266,
        "grad_norm": 1.727202296257019,
        "learning_rate": 5.7277864873994e-06,
        "epoch": 1.928589842742975,
        "step": 14962
    },
    {
        "loss": 1.7435,
        "grad_norm": 3.5504233837127686,
        "learning_rate": 5.707508872174194e-06,
        "epoch": 1.9287187419438,
        "step": 14963
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.858816385269165,
        "learning_rate": 5.687266159762916e-06,
        "epoch": 1.928847641144625,
        "step": 14964
    },
    {
        "loss": 1.9426,
        "grad_norm": 1.6832664012908936,
        "learning_rate": 5.667058357658639e-06,
        "epoch": 1.9289765403454497,
        "step": 14965
    },
    {
        "loss": 1.5271,
        "grad_norm": 2.7906649112701416,
        "learning_rate": 5.646885473341334e-06,
        "epoch": 1.9291054395462748,
        "step": 14966
    },
    {
        "loss": 1.7332,
        "grad_norm": 1.6109710931777954,
        "learning_rate": 5.626747514278086e-06,
        "epoch": 1.9292343387470998,
        "step": 14967
    },
    {
        "loss": 0.5349,
        "grad_norm": 3.6379754543304443,
        "learning_rate": 5.6066444879230516e-06,
        "epoch": 1.9293632379479249,
        "step": 14968
    },
    {
        "loss": 2.2262,
        "grad_norm": 2.33176851272583,
        "learning_rate": 5.58657640171748e-06,
        "epoch": 1.9294921371487497,
        "step": 14969
    },
    {
        "loss": 1.0512,
        "grad_norm": 2.089824676513672,
        "learning_rate": 5.5665432630896516e-06,
        "epoch": 1.9296210363495745,
        "step": 14970
    },
    {
        "loss": 1.3179,
        "grad_norm": 2.527843952178955,
        "learning_rate": 5.5465450794549455e-06,
        "epoch": 1.9297499355503995,
        "step": 14971
    },
    {
        "loss": 2.2088,
        "grad_norm": 1.7396860122680664,
        "learning_rate": 5.526581858215718e-06,
        "epoch": 1.9298788347512246,
        "step": 14972
    },
    {
        "loss": 1.6615,
        "grad_norm": 1.693771481513977,
        "learning_rate": 5.506653606761547e-06,
        "epoch": 1.9300077339520496,
        "step": 14973
    },
    {
        "loss": 1.3996,
        "grad_norm": 2.267533540725708,
        "learning_rate": 5.486760332468932e-06,
        "epoch": 1.9301366331528744,
        "step": 14974
    },
    {
        "loss": 1.4994,
        "grad_norm": 2.2089176177978516,
        "learning_rate": 5.466902042701472e-06,
        "epoch": 1.9302655323536992,
        "step": 14975
    },
    {
        "loss": 0.8804,
        "grad_norm": 2.6000895500183105,
        "learning_rate": 5.447078744809786e-06,
        "epoch": 1.9303944315545243,
        "step": 14976
    },
    {
        "loss": 1.4179,
        "grad_norm": 2.625589370727539,
        "learning_rate": 5.427290446131572e-06,
        "epoch": 1.9305233307553493,
        "step": 14977
    },
    {
        "loss": 2.0294,
        "grad_norm": 1.9789320230484009,
        "learning_rate": 5.4075371539915045e-06,
        "epoch": 1.9306522299561744,
        "step": 14978
    },
    {
        "loss": 0.972,
        "grad_norm": 3.4958856105804443,
        "learning_rate": 5.387818875701467e-06,
        "epoch": 1.9307811291569992,
        "step": 14979
    },
    {
        "loss": 1.243,
        "grad_norm": 2.9532697200775146,
        "learning_rate": 5.368135618560277e-06,
        "epoch": 1.9309100283578242,
        "step": 14980
    },
    {
        "loss": 1.4914,
        "grad_norm": 3.532916307449341,
        "learning_rate": 5.34848738985364e-06,
        "epoch": 1.931038927558649,
        "step": 14981
    },
    {
        "loss": 1.5811,
        "grad_norm": 1.804964542388916,
        "learning_rate": 5.328874196854572e-06,
        "epoch": 1.931167826759474,
        "step": 14982
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.0648040771484375,
        "learning_rate": 5.309296046822954e-06,
        "epoch": 1.931296725960299,
        "step": 14983
    },
    {
        "loss": 1.6346,
        "grad_norm": 2.717834949493408,
        "learning_rate": 5.289752947005722e-06,
        "epoch": 1.9314256251611241,
        "step": 14984
    },
    {
        "loss": 1.0722,
        "grad_norm": 1.3855684995651245,
        "learning_rate": 5.270244904636856e-06,
        "epoch": 1.931554524361949,
        "step": 14985
    },
    {
        "loss": 1.6575,
        "grad_norm": 2.178497314453125,
        "learning_rate": 5.250771926937348e-06,
        "epoch": 1.9316834235627738,
        "step": 14986
    },
    {
        "loss": 1.6761,
        "grad_norm": 1.596494197845459,
        "learning_rate": 5.231334021115219e-06,
        "epoch": 1.9318123227635988,
        "step": 14987
    },
    {
        "loss": 1.9612,
        "grad_norm": 2.292811155319214,
        "learning_rate": 5.211931194365493e-06,
        "epoch": 1.9319412219644239,
        "step": 14988
    },
    {
        "loss": 2.0153,
        "grad_norm": 2.9200470447540283,
        "learning_rate": 5.192563453870169e-06,
        "epoch": 1.932070121165249,
        "step": 14989
    },
    {
        "loss": 1.4216,
        "grad_norm": 2.0039987564086914,
        "learning_rate": 5.173230806798401e-06,
        "epoch": 1.9321990203660737,
        "step": 14990
    },
    {
        "loss": 1.8796,
        "grad_norm": 2.2597620487213135,
        "learning_rate": 5.153933260306232e-06,
        "epoch": 1.9323279195668985,
        "step": 14991
    },
    {
        "loss": 1.6717,
        "grad_norm": 2.121692419052124,
        "learning_rate": 5.134670821536735e-06,
        "epoch": 1.9324568187677236,
        "step": 14992
    },
    {
        "loss": 1.4766,
        "grad_norm": 1.6493817567825317,
        "learning_rate": 5.115443497619987e-06,
        "epoch": 1.9325857179685486,
        "step": 14993
    },
    {
        "loss": 2.0081,
        "grad_norm": 1.552993655204773,
        "learning_rate": 5.09625129567306e-06,
        "epoch": 1.9327146171693736,
        "step": 14994
    },
    {
        "loss": 2.3076,
        "grad_norm": 2.8816349506378174,
        "learning_rate": 5.077094222800005e-06,
        "epoch": 1.9328435163701985,
        "step": 14995
    },
    {
        "loss": 0.9286,
        "grad_norm": 3.3535430431365967,
        "learning_rate": 5.057972286092017e-06,
        "epoch": 1.9329724155710235,
        "step": 14996
    },
    {
        "loss": 1.7064,
        "grad_norm": 3.3215713500976562,
        "learning_rate": 5.038885492627133e-06,
        "epoch": 1.9331013147718483,
        "step": 14997
    },
    {
        "loss": 0.7981,
        "grad_norm": 2.0004968643188477,
        "learning_rate": 5.0198338494703126e-06,
        "epoch": 1.9332302139726734,
        "step": 14998
    },
    {
        "loss": 1.4871,
        "grad_norm": 2.715967893600464,
        "learning_rate": 5.000817363673738e-06,
        "epoch": 1.9333591131734984,
        "step": 14999
    },
    {
        "loss": 2.0923,
        "grad_norm": 1.2791727781295776,
        "learning_rate": 4.981836042276411e-06,
        "epoch": 1.9334880123743234,
        "step": 15000
    },
    {
        "loss": 1.7694,
        "grad_norm": 1.775246262550354,
        "learning_rate": 4.962889892304334e-06,
        "epoch": 1.9336169115751483,
        "step": 15001
    },
    {
        "loss": 1.2723,
        "grad_norm": 1.5556176900863647,
        "learning_rate": 4.943978920770542e-06,
        "epoch": 1.933745810775973,
        "step": 15002
    },
    {
        "loss": 1.2923,
        "grad_norm": 2.893017530441284,
        "learning_rate": 4.9251031346750245e-06,
        "epoch": 1.9338747099767981,
        "step": 15003
    },
    {
        "loss": 1.4955,
        "grad_norm": 2.2269716262817383,
        "learning_rate": 4.906262541004725e-06,
        "epoch": 1.9340036091776232,
        "step": 15004
    },
    {
        "loss": 1.0855,
        "grad_norm": 3.0070853233337402,
        "learning_rate": 4.887457146733587e-06,
        "epoch": 1.9341325083784482,
        "step": 15005
    },
    {
        "loss": 2.0014,
        "grad_norm": 2.2622056007385254,
        "learning_rate": 4.868686958822466e-06,
        "epoch": 1.934261407579273,
        "step": 15006
    },
    {
        "loss": 2.0829,
        "grad_norm": 1.8678218126296997,
        "learning_rate": 4.849951984219336e-06,
        "epoch": 1.9343903067800978,
        "step": 15007
    },
    {
        "loss": 1.6276,
        "grad_norm": 2.438650369644165,
        "learning_rate": 4.831252229858996e-06,
        "epoch": 1.9345192059809229,
        "step": 15008
    },
    {
        "loss": 1.9471,
        "grad_norm": 3.095438241958618,
        "learning_rate": 4.812587702663229e-06,
        "epoch": 1.934648105181748,
        "step": 15009
    },
    {
        "loss": 1.2571,
        "grad_norm": 2.001835584640503,
        "learning_rate": 4.793958409540811e-06,
        "epoch": 1.934777004382573,
        "step": 15010
    },
    {
        "loss": 1.3972,
        "grad_norm": 1.6308550834655762,
        "learning_rate": 4.775364357387469e-06,
        "epoch": 1.9349059035833978,
        "step": 15011
    },
    {
        "loss": 1.7141,
        "grad_norm": 1.8754165172576904,
        "learning_rate": 4.756805553085808e-06,
        "epoch": 1.9350348027842226,
        "step": 15012
    },
    {
        "loss": 1.8926,
        "grad_norm": 1.7975705862045288,
        "learning_rate": 4.738282003505579e-06,
        "epoch": 1.9351637019850476,
        "step": 15013
    },
    {
        "loss": 1.5313,
        "grad_norm": 3.3124940395355225,
        "learning_rate": 4.719793715503306e-06,
        "epoch": 1.9352926011858727,
        "step": 15014
    },
    {
        "loss": 0.4035,
        "grad_norm": 3.4120914936065674,
        "learning_rate": 4.701340695922529e-06,
        "epoch": 1.9354215003866977,
        "step": 15015
    },
    {
        "loss": 1.9123,
        "grad_norm": 1.6219680309295654,
        "learning_rate": 4.6829229515936935e-06,
        "epoch": 1.9355503995875225,
        "step": 15016
    },
    {
        "loss": 1.3934,
        "grad_norm": 2.8363256454467773,
        "learning_rate": 4.664540489334224e-06,
        "epoch": 1.9356792987883475,
        "step": 15017
    },
    {
        "loss": 0.6662,
        "grad_norm": 3.5846269130706787,
        "learning_rate": 4.646193315948488e-06,
        "epoch": 1.9358081979891724,
        "step": 15018
    },
    {
        "loss": 0.9319,
        "grad_norm": 3.35622501373291,
        "learning_rate": 4.6278814382277765e-06,
        "epoch": 1.9359370971899974,
        "step": 15019
    },
    {
        "loss": 1.9015,
        "grad_norm": 2.3804545402526855,
        "learning_rate": 4.609604862950312e-06,
        "epoch": 1.9360659963908224,
        "step": 15020
    },
    {
        "loss": 1.8566,
        "grad_norm": 2.0594029426574707,
        "learning_rate": 4.59136359688127e-06,
        "epoch": 1.9361948955916475,
        "step": 15021
    },
    {
        "loss": 1.1555,
        "grad_norm": 2.0919058322906494,
        "learning_rate": 4.573157646772719e-06,
        "epoch": 1.9363237947924723,
        "step": 15022
    },
    {
        "loss": 1.9162,
        "grad_norm": 2.4094836711883545,
        "learning_rate": 4.554987019363643e-06,
        "epoch": 1.9364526939932971,
        "step": 15023
    },
    {
        "loss": 1.8894,
        "grad_norm": 2.0188863277435303,
        "learning_rate": 4.536851721380076e-06,
        "epoch": 1.9365815931941222,
        "step": 15024
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.1574547290802,
        "learning_rate": 4.518751759534845e-06,
        "epoch": 1.9367104923949472,
        "step": 15025
    },
    {
        "loss": 1.2723,
        "grad_norm": 3.2110939025878906,
        "learning_rate": 4.500687140527737e-06,
        "epoch": 1.9368393915957722,
        "step": 15026
    },
    {
        "loss": 2.0384,
        "grad_norm": 2.1770596504211426,
        "learning_rate": 4.482657871045448e-06,
        "epoch": 1.936968290796597,
        "step": 15027
    },
    {
        "loss": 1.7114,
        "grad_norm": 2.7311723232269287,
        "learning_rate": 4.464663957761606e-06,
        "epoch": 1.9370971899974219,
        "step": 15028
    },
    {
        "loss": 1.924,
        "grad_norm": 1.8206498622894287,
        "learning_rate": 4.446705407336693e-06,
        "epoch": 1.937226089198247,
        "step": 15029
    },
    {
        "loss": 1.5662,
        "grad_norm": 2.4063656330108643,
        "learning_rate": 4.428782226418249e-06,
        "epoch": 1.937354988399072,
        "step": 15030
    },
    {
        "loss": 2.0138,
        "grad_norm": 3.3539557456970215,
        "learning_rate": 4.410894421640566e-06,
        "epoch": 1.937483887599897,
        "step": 15031
    },
    {
        "loss": 1.4201,
        "grad_norm": 1.6445086002349854,
        "learning_rate": 4.393041999624914e-06,
        "epoch": 1.9376127868007218,
        "step": 15032
    },
    {
        "loss": 0.6521,
        "grad_norm": 4.731836795806885,
        "learning_rate": 4.375224966979452e-06,
        "epoch": 1.9377416860015468,
        "step": 15033
    },
    {
        "loss": 1.9185,
        "grad_norm": 2.0838496685028076,
        "learning_rate": 4.3574433302992155e-06,
        "epoch": 1.9378705852023717,
        "step": 15034
    },
    {
        "loss": 2.0728,
        "grad_norm": 2.580878734588623,
        "learning_rate": 4.339697096166195e-06,
        "epoch": 1.9379994844031967,
        "step": 15035
    },
    {
        "loss": 0.9112,
        "grad_norm": 2.072256326675415,
        "learning_rate": 4.321986271149225e-06,
        "epoch": 1.9381283836040217,
        "step": 15036
    },
    {
        "loss": 1.2121,
        "grad_norm": 1.6046137809753418,
        "learning_rate": 4.304310861804028e-06,
        "epoch": 1.9382572828048468,
        "step": 15037
    },
    {
        "loss": 1.6812,
        "grad_norm": 2.7172257900238037,
        "learning_rate": 4.286670874673338e-06,
        "epoch": 1.9383861820056716,
        "step": 15038
    },
    {
        "loss": 1.6474,
        "grad_norm": 2.716979503631592,
        "learning_rate": 4.269066316286574e-06,
        "epoch": 1.9385150812064964,
        "step": 15039
    },
    {
        "loss": 1.8712,
        "grad_norm": 1.9541481733322144,
        "learning_rate": 4.251497193160126e-06,
        "epoch": 1.9386439804073214,
        "step": 15040
    },
    {
        "loss": 1.637,
        "grad_norm": 2.7811005115509033,
        "learning_rate": 4.233963511797401e-06,
        "epoch": 1.9387728796081465,
        "step": 15041
    },
    {
        "loss": 1.6854,
        "grad_norm": 2.246260166168213,
        "learning_rate": 4.21646527868852e-06,
        "epoch": 1.9389017788089715,
        "step": 15042
    },
    {
        "loss": 1.569,
        "grad_norm": 1.7010852098464966,
        "learning_rate": 4.199002500310533e-06,
        "epoch": 1.9390306780097963,
        "step": 15043
    },
    {
        "loss": 1.6381,
        "grad_norm": 2.805842638015747,
        "learning_rate": 4.181575183127384e-06,
        "epoch": 1.9391595772106212,
        "step": 15044
    },
    {
        "loss": 2.037,
        "grad_norm": 1.858243703842163,
        "learning_rate": 4.1641833335898435e-06,
        "epoch": 1.9392884764114462,
        "step": 15045
    },
    {
        "loss": 1.1622,
        "grad_norm": 2.820781707763672,
        "learning_rate": 4.146826958135597e-06,
        "epoch": 1.9394173756122712,
        "step": 15046
    },
    {
        "loss": 0.655,
        "grad_norm": 2.2770957946777344,
        "learning_rate": 4.129506063189237e-06,
        "epoch": 1.9395462748130963,
        "step": 15047
    },
    {
        "loss": 0.9347,
        "grad_norm": 3.9011287689208984,
        "learning_rate": 4.112220655162136e-06,
        "epoch": 1.939675174013921,
        "step": 15048
    },
    {
        "loss": 2.4372,
        "grad_norm": 3.182349681854248,
        "learning_rate": 4.094970740452586e-06,
        "epoch": 1.939804073214746,
        "step": 15049
    },
    {
        "loss": 1.1671,
        "grad_norm": 3.5889904499053955,
        "learning_rate": 4.077756325445692e-06,
        "epoch": 1.939932972415571,
        "step": 15050
    },
    {
        "loss": 1.43,
        "grad_norm": 2.8091535568237305,
        "learning_rate": 4.060577416513489e-06,
        "epoch": 1.940061871616396,
        "step": 15051
    },
    {
        "loss": 1.3509,
        "grad_norm": 1.9967141151428223,
        "learning_rate": 4.043434020014825e-06,
        "epoch": 1.940190770817221,
        "step": 15052
    },
    {
        "loss": 1.1747,
        "grad_norm": 2.576357126235962,
        "learning_rate": 4.0263261422954e-06,
        "epoch": 1.9403196700180458,
        "step": 15053
    },
    {
        "loss": 2.4065,
        "grad_norm": 1.652870774269104,
        "learning_rate": 4.009253789687739e-06,
        "epoch": 1.9404485692188709,
        "step": 15054
    },
    {
        "loss": 1.3752,
        "grad_norm": 2.786893606185913,
        "learning_rate": 3.992216968511364e-06,
        "epoch": 1.9405774684196957,
        "step": 15055
    },
    {
        "loss": 1.4333,
        "grad_norm": 3.173210620880127,
        "learning_rate": 3.975215685072431e-06,
        "epoch": 1.9407063676205207,
        "step": 15056
    },
    {
        "loss": 1.6856,
        "grad_norm": 2.2722549438476562,
        "learning_rate": 3.958249945664028e-06,
        "epoch": 1.9408352668213458,
        "step": 15057
    },
    {
        "loss": 2.345,
        "grad_norm": 2.3454344272613525,
        "learning_rate": 3.9413197565662084e-06,
        "epoch": 1.9409641660221708,
        "step": 15058
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.960278034210205,
        "learning_rate": 3.924425124045716e-06,
        "epoch": 1.9410930652229956,
        "step": 15059
    },
    {
        "loss": 1.9891,
        "grad_norm": 1.731268048286438,
        "learning_rate": 3.907566054356171e-06,
        "epoch": 1.9412219644238204,
        "step": 15060
    },
    {
        "loss": 2.1288,
        "grad_norm": 1.631973147392273,
        "learning_rate": 3.890742553738036e-06,
        "epoch": 1.9413508636246455,
        "step": 15061
    },
    {
        "loss": 1.8381,
        "grad_norm": 2.3525524139404297,
        "learning_rate": 3.873954628418597e-06,
        "epoch": 1.9414797628254705,
        "step": 15062
    },
    {
        "loss": 0.8073,
        "grad_norm": 2.2870445251464844,
        "learning_rate": 3.8572022846119935e-06,
        "epoch": 1.9416086620262956,
        "step": 15063
    },
    {
        "loss": 1.4597,
        "grad_norm": 3.1591174602508545,
        "learning_rate": 3.840485528519211e-06,
        "epoch": 1.9417375612271204,
        "step": 15064
    },
    {
        "loss": 1.1932,
        "grad_norm": 2.656985282897949,
        "learning_rate": 3.823804366328032e-06,
        "epoch": 1.9418664604279452,
        "step": 15065
    },
    {
        "loss": 1.7397,
        "grad_norm": 2.2625668048858643,
        "learning_rate": 3.8071588042130515e-06,
        "epoch": 1.9419953596287702,
        "step": 15066
    },
    {
        "loss": 0.3409,
        "grad_norm": 1.3326387405395508,
        "learning_rate": 3.7905488483356954e-06,
        "epoch": 1.9421242588295953,
        "step": 15067
    },
    {
        "loss": 1.6777,
        "grad_norm": 3.2391281127929688,
        "learning_rate": 3.7739745048442467e-06,
        "epoch": 1.9422531580304203,
        "step": 15068
    },
    {
        "loss": 1.4581,
        "grad_norm": 2.95461368560791,
        "learning_rate": 3.7574357798737527e-06,
        "epoch": 1.9423820572312451,
        "step": 15069
    },
    {
        "loss": 1.1964,
        "grad_norm": 3.067229986190796,
        "learning_rate": 3.740932679546105e-06,
        "epoch": 1.9425109564320702,
        "step": 15070
    },
    {
        "loss": 1.2808,
        "grad_norm": 3.3521640300750732,
        "learning_rate": 3.724465209969974e-06,
        "epoch": 1.942639855632895,
        "step": 15071
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.817673921585083,
        "learning_rate": 3.7080333772409827e-06,
        "epoch": 1.94276875483372,
        "step": 15072
    },
    {
        "loss": 1.7454,
        "grad_norm": 2.4161319732666016,
        "learning_rate": 3.6916371874413324e-06,
        "epoch": 1.942897654034545,
        "step": 15073
    },
    {
        "loss": 2.049,
        "grad_norm": 2.6423773765563965,
        "learning_rate": 3.675276646640158e-06,
        "epoch": 1.94302655323537,
        "step": 15074
    },
    {
        "loss": 1.2657,
        "grad_norm": 2.3098196983337402,
        "learning_rate": 3.65895176089347e-06,
        "epoch": 1.943155452436195,
        "step": 15075
    },
    {
        "loss": 1.9178,
        "grad_norm": 2.6543097496032715,
        "learning_rate": 3.642662536243957e-06,
        "epoch": 1.9432843516370197,
        "step": 15076
    },
    {
        "loss": 1.0238,
        "grad_norm": 2.555091619491577,
        "learning_rate": 3.6264089787211074e-06,
        "epoch": 1.9434132508378448,
        "step": 15077
    },
    {
        "loss": 1.4442,
        "grad_norm": 2.1407363414764404,
        "learning_rate": 3.6101910943413863e-06,
        "epoch": 1.9435421500386698,
        "step": 15078
    },
    {
        "loss": 0.9401,
        "grad_norm": 3.0221776962280273,
        "learning_rate": 3.594008889107803e-06,
        "epoch": 1.9436710492394949,
        "step": 15079
    },
    {
        "loss": 1.8012,
        "grad_norm": 3.0047762393951416,
        "learning_rate": 3.5778623690102655e-06,
        "epoch": 1.9437999484403197,
        "step": 15080
    },
    {
        "loss": 1.9091,
        "grad_norm": 1.6395609378814697,
        "learning_rate": 3.5617515400255706e-06,
        "epoch": 1.9439288476411445,
        "step": 15081
    },
    {
        "loss": 0.6062,
        "grad_norm": 2.481170892715454,
        "learning_rate": 3.545676408117182e-06,
        "epoch": 1.9440577468419695,
        "step": 15082
    },
    {
        "loss": 1.8483,
        "grad_norm": 2.2047083377838135,
        "learning_rate": 3.529636979235362e-06,
        "epoch": 1.9441866460427946,
        "step": 15083
    },
    {
        "loss": 1.106,
        "grad_norm": 3.3923287391662598,
        "learning_rate": 3.5136332593172174e-06,
        "epoch": 1.9443155452436196,
        "step": 15084
    },
    {
        "loss": 1.658,
        "grad_norm": 1.8509130477905273,
        "learning_rate": 3.4976652542865863e-06,
        "epoch": 1.9444444444444444,
        "step": 15085
    },
    {
        "loss": 0.8961,
        "grad_norm": 4.015568733215332,
        "learning_rate": 3.4817329700540966e-06,
        "epoch": 1.9445733436452692,
        "step": 15086
    },
    {
        "loss": 0.7243,
        "grad_norm": 1.9005143642425537,
        "learning_rate": 3.465836412517165e-06,
        "epoch": 1.9447022428460943,
        "step": 15087
    },
    {
        "loss": 1.5921,
        "grad_norm": 2.269676685333252,
        "learning_rate": 3.4499755875599392e-06,
        "epoch": 1.9448311420469193,
        "step": 15088
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.4278175830841064,
        "learning_rate": 3.43415050105349e-06,
        "epoch": 1.9449600412477444,
        "step": 15089
    },
    {
        "loss": 1.5003,
        "grad_norm": 2.7325708866119385,
        "learning_rate": 3.4183611588554208e-06,
        "epoch": 1.9450889404485692,
        "step": 15090
    },
    {
        "loss": 1.3304,
        "grad_norm": 3.588512897491455,
        "learning_rate": 3.4026075668102674e-06,
        "epoch": 1.9452178396493942,
        "step": 15091
    },
    {
        "loss": 1.7952,
        "grad_norm": 1.6964669227600098,
        "learning_rate": 3.386889730749343e-06,
        "epoch": 1.945346738850219,
        "step": 15092
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.5600318908691406,
        "learning_rate": 3.37120765649066e-06,
        "epoch": 1.945475638051044,
        "step": 15093
    },
    {
        "loss": 1.3904,
        "grad_norm": 2.1400392055511475,
        "learning_rate": 3.355561349838954e-06,
        "epoch": 1.945604537251869,
        "step": 15094
    },
    {
        "loss": 1.3267,
        "grad_norm": 2.8874874114990234,
        "learning_rate": 3.3399508165859017e-06,
        "epoch": 1.9457334364526941,
        "step": 15095
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.783447742462158,
        "learning_rate": 3.3243760625097153e-06,
        "epoch": 1.945862335653519,
        "step": 15096
    },
    {
        "loss": 1.879,
        "grad_norm": 2.7224276065826416,
        "learning_rate": 3.308837093375461e-06,
        "epoch": 1.9459912348543438,
        "step": 15097
    },
    {
        "loss": 0.898,
        "grad_norm": 1.9274218082427979,
        "learning_rate": 3.293333914935015e-06,
        "epoch": 1.9461201340551688,
        "step": 15098
    },
    {
        "loss": 1.5886,
        "grad_norm": 1.9782339334487915,
        "learning_rate": 3.277866532926943e-06,
        "epoch": 1.9462490332559939,
        "step": 15099
    },
    {
        "loss": 1.9819,
        "grad_norm": 4.259166240692139,
        "learning_rate": 3.262434953076543e-06,
        "epoch": 1.946377932456819,
        "step": 15100
    },
    {
        "loss": 1.6658,
        "grad_norm": 1.613893985748291,
        "learning_rate": 3.2470391810959232e-06,
        "epoch": 1.9465068316576437,
        "step": 15101
    },
    {
        "loss": 2.7083,
        "grad_norm": 2.0960443019866943,
        "learning_rate": 3.23167922268387e-06,
        "epoch": 1.9466357308584685,
        "step": 15102
    },
    {
        "loss": 1.4206,
        "grad_norm": 3.5385758876800537,
        "learning_rate": 3.216355083525957e-06,
        "epoch": 1.9467646300592936,
        "step": 15103
    },
    {
        "loss": 1.3739,
        "grad_norm": 2.2965734004974365,
        "learning_rate": 3.201066769294492e-06,
        "epoch": 1.9468935292601186,
        "step": 15104
    },
    {
        "loss": 1.462,
        "grad_norm": 2.308364152908325,
        "learning_rate": 3.1858142856484695e-06,
        "epoch": 1.9470224284609436,
        "step": 15105
    },
    {
        "loss": 0.1374,
        "grad_norm": 0.44989025592803955,
        "learning_rate": 3.170597638233785e-06,
        "epoch": 1.9471513276617685,
        "step": 15106
    },
    {
        "loss": 2.0593,
        "grad_norm": 1.9674078226089478,
        "learning_rate": 3.155416832682845e-06,
        "epoch": 1.9472802268625935,
        "step": 15107
    },
    {
        "loss": 0.3587,
        "grad_norm": 1.1358369588851929,
        "learning_rate": 3.1402718746148973e-06,
        "epoch": 1.9474091260634183,
        "step": 15108
    },
    {
        "loss": 2.1694,
        "grad_norm": 1.898349404335022,
        "learning_rate": 3.125162769635992e-06,
        "epoch": 1.9475380252642434,
        "step": 15109
    },
    {
        "loss": 1.108,
        "grad_norm": 1.938166618347168,
        "learning_rate": 3.1100895233387885e-06,
        "epoch": 1.9476669244650684,
        "step": 15110
    },
    {
        "loss": 1.9068,
        "grad_norm": 2.8312370777130127,
        "learning_rate": 3.0950521413027014e-06,
        "epoch": 1.9477958236658934,
        "step": 15111
    },
    {
        "loss": 1.9497,
        "grad_norm": 2.0418052673339844,
        "learning_rate": 3.0800506290939778e-06,
        "epoch": 1.9479247228667183,
        "step": 15112
    },
    {
        "loss": 1.4392,
        "grad_norm": 3.138899087905884,
        "learning_rate": 3.065084992265421e-06,
        "epoch": 1.948053622067543,
        "step": 15113
    },
    {
        "loss": 1.0845,
        "grad_norm": 3.4367895126342773,
        "learning_rate": 3.050155236356611e-06,
        "epoch": 1.948182521268368,
        "step": 15114
    },
    {
        "loss": 1.3317,
        "grad_norm": 3.3913567066192627,
        "learning_rate": 3.0352613668939376e-06,
        "epoch": 1.9483114204691931,
        "step": 15115
    },
    {
        "loss": 1.1196,
        "grad_norm": 1.821452021598816,
        "learning_rate": 3.020403389390403e-06,
        "epoch": 1.9484403196700182,
        "step": 15116
    },
    {
        "loss": 1.5505,
        "grad_norm": 2.3519821166992188,
        "learning_rate": 3.0055813093457617e-06,
        "epoch": 1.948569218870843,
        "step": 15117
    },
    {
        "loss": 2.0666,
        "grad_norm": 2.9228053092956543,
        "learning_rate": 2.9907951322464933e-06,
        "epoch": 1.9486981180716678,
        "step": 15118
    },
    {
        "loss": 1.6066,
        "grad_norm": 2.2057433128356934,
        "learning_rate": 2.976044863565752e-06,
        "epoch": 1.9488270172724929,
        "step": 15119
    },
    {
        "loss": 0.9582,
        "grad_norm": 2.661057710647583,
        "learning_rate": 2.9613305087634156e-06,
        "epoch": 1.948955916473318,
        "step": 15120
    },
    {
        "loss": 0.827,
        "grad_norm": 1.056876301765442,
        "learning_rate": 2.9466520732860826e-06,
        "epoch": 1.949084815674143,
        "step": 15121
    },
    {
        "loss": 1.123,
        "grad_norm": 2.213724374771118,
        "learning_rate": 2.932009562567006e-06,
        "epoch": 1.9492137148749678,
        "step": 15122
    },
    {
        "loss": 1.3444,
        "grad_norm": 2.7531638145446777,
        "learning_rate": 2.917402982026296e-06,
        "epoch": 1.9493426140757926,
        "step": 15123
    },
    {
        "loss": 1.2337,
        "grad_norm": 2.7089757919311523,
        "learning_rate": 2.902832337070538e-06,
        "epoch": 1.9494715132766176,
        "step": 15124
    },
    {
        "loss": 1.2759,
        "grad_norm": 2.4240024089813232,
        "learning_rate": 2.888297633093129e-06,
        "epoch": 1.9496004124774426,
        "step": 15125
    },
    {
        "loss": 0.7355,
        "grad_norm": 2.2438876628875732,
        "learning_rate": 2.8737988754742095e-06,
        "epoch": 1.9497293116782677,
        "step": 15126
    },
    {
        "loss": 1.0328,
        "grad_norm": 3.1094398498535156,
        "learning_rate": 2.8593360695805536e-06,
        "epoch": 1.9498582108790925,
        "step": 15127
    },
    {
        "loss": 1.9076,
        "grad_norm": 2.3597967624664307,
        "learning_rate": 2.8449092207655794e-06,
        "epoch": 1.9499871100799175,
        "step": 15128
    },
    {
        "loss": 1.892,
        "grad_norm": 3.375249147415161,
        "learning_rate": 2.8305183343695694e-06,
        "epoch": 1.9501160092807424,
        "step": 15129
    },
    {
        "loss": 1.0424,
        "grad_norm": 2.274876117706299,
        "learning_rate": 2.816163415719286e-06,
        "epoch": 1.9502449084815674,
        "step": 15130
    },
    {
        "loss": 1.6861,
        "grad_norm": 3.047036647796631,
        "learning_rate": 2.8018444701282566e-06,
        "epoch": 1.9503738076823924,
        "step": 15131
    },
    {
        "loss": 0.4346,
        "grad_norm": 1.9114999771118164,
        "learning_rate": 2.787561502896774e-06,
        "epoch": 1.9505027068832175,
        "step": 15132
    },
    {
        "loss": 1.8861,
        "grad_norm": 2.2166059017181396,
        "learning_rate": 2.7733145193117204e-06,
        "epoch": 1.9506316060840423,
        "step": 15133
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.3870571851730347,
        "learning_rate": 2.759103524646678e-06,
        "epoch": 1.950760505284867,
        "step": 15134
    },
    {
        "loss": 2.5868,
        "grad_norm": 1.9993724822998047,
        "learning_rate": 2.744928524161927e-06,
        "epoch": 1.9508894044856921,
        "step": 15135
    },
    {
        "loss": 1.4732,
        "grad_norm": 3.461073875427246,
        "learning_rate": 2.7307895231043934e-06,
        "epoch": 1.9510183036865172,
        "step": 15136
    },
    {
        "loss": 2.1363,
        "grad_norm": 2.089358329772949,
        "learning_rate": 2.7166865267077123e-06,
        "epoch": 1.9511472028873422,
        "step": 15137
    },
    {
        "loss": 1.4243,
        "grad_norm": 2.4564549922943115,
        "learning_rate": 2.7026195401921527e-06,
        "epoch": 1.951276102088167,
        "step": 15138
    },
    {
        "loss": 1.3932,
        "grad_norm": 3.5725128650665283,
        "learning_rate": 2.6885885687646607e-06,
        "epoch": 1.9514050012889919,
        "step": 15139
    },
    {
        "loss": 1.5819,
        "grad_norm": 2.559819221496582,
        "learning_rate": 2.674593617618937e-06,
        "epoch": 1.951533900489817,
        "step": 15140
    },
    {
        "loss": 0.6801,
        "grad_norm": 3.5562756061553955,
        "learning_rate": 2.660634691935271e-06,
        "epoch": 1.951662799690642,
        "step": 15141
    },
    {
        "loss": 1.8199,
        "grad_norm": 1.830682396888733,
        "learning_rate": 2.64671179688053e-06,
        "epoch": 1.951791698891467,
        "step": 15142
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.4197242259979248,
        "learning_rate": 2.6328249376084358e-06,
        "epoch": 1.9519205980922918,
        "step": 15143
    },
    {
        "loss": 1.3935,
        "grad_norm": 2.9932022094726562,
        "learning_rate": 2.618974119259221e-06,
        "epoch": 1.9520494972931168,
        "step": 15144
    },
    {
        "loss": 0.9143,
        "grad_norm": 2.4835615158081055,
        "learning_rate": 2.605159346959851e-06,
        "epoch": 1.9521783964939416,
        "step": 15145
    },
    {
        "loss": 1.5604,
        "grad_norm": 2.425689458847046,
        "learning_rate": 2.5913806258239803e-06,
        "epoch": 1.9523072956947667,
        "step": 15146
    },
    {
        "loss": 1.3568,
        "grad_norm": 4.06959342956543,
        "learning_rate": 2.5776379609517954e-06,
        "epoch": 1.9524361948955917,
        "step": 15147
    },
    {
        "loss": 1.4314,
        "grad_norm": 2.079580545425415,
        "learning_rate": 2.563931357430205e-06,
        "epoch": 1.9525650940964168,
        "step": 15148
    },
    {
        "loss": 1.7757,
        "grad_norm": 2.3070366382598877,
        "learning_rate": 2.550260820332817e-06,
        "epoch": 1.9526939932972416,
        "step": 15149
    },
    {
        "loss": 1.5285,
        "grad_norm": 3.080030679702759,
        "learning_rate": 2.536626354719851e-06,
        "epoch": 1.9528228924980664,
        "step": 15150
    },
    {
        "loss": 1.9938,
        "grad_norm": 2.5408318042755127,
        "learning_rate": 2.5230279656381363e-06,
        "epoch": 1.9529517916988914,
        "step": 15151
    },
    {
        "loss": 1.8535,
        "grad_norm": 2.528150796890259,
        "learning_rate": 2.5094656581212015e-06,
        "epoch": 1.9530806908997165,
        "step": 15152
    },
    {
        "loss": 0.4141,
        "grad_norm": 1.8001562356948853,
        "learning_rate": 2.4959394371891877e-06,
        "epoch": 1.9532095901005415,
        "step": 15153
    },
    {
        "loss": 2.1061,
        "grad_norm": 2.857943534851074,
        "learning_rate": 2.4824493078488998e-06,
        "epoch": 1.9533384893013663,
        "step": 15154
    },
    {
        "loss": 0.7344,
        "grad_norm": 2.3314719200134277,
        "learning_rate": 2.468995275093755e-06,
        "epoch": 1.9534673885021911,
        "step": 15155
    },
    {
        "loss": 1.8543,
        "grad_norm": 1.718470573425293,
        "learning_rate": 2.4555773439038256e-06,
        "epoch": 1.9535962877030162,
        "step": 15156
    },
    {
        "loss": 1.7689,
        "grad_norm": 2.515810251235962,
        "learning_rate": 2.4421955192458602e-06,
        "epoch": 1.9537251869038412,
        "step": 15157
    },
    {
        "loss": 0.5473,
        "grad_norm": 2.5224101543426514,
        "learning_rate": 2.4288498060732078e-06,
        "epoch": 1.9538540861046663,
        "step": 15158
    },
    {
        "loss": 2.1122,
        "grad_norm": 2.2139508724212646,
        "learning_rate": 2.4155402093257727e-06,
        "epoch": 1.953982985305491,
        "step": 15159
    },
    {
        "loss": 2.4181,
        "grad_norm": 1.8239439725875854,
        "learning_rate": 2.4022667339302363e-06,
        "epoch": 1.954111884506316,
        "step": 15160
    },
    {
        "loss": 1.3992,
        "grad_norm": 2.8055331707000732,
        "learning_rate": 2.3890293847998123e-06,
        "epoch": 1.954240783707141,
        "step": 15161
    },
    {
        "loss": 1.4527,
        "grad_norm": 1.7038049697875977,
        "learning_rate": 2.3758281668343597e-06,
        "epoch": 1.954369682907966,
        "step": 15162
    },
    {
        "loss": 1.6699,
        "grad_norm": 2.9300577640533447,
        "learning_rate": 2.3626630849204023e-06,
        "epoch": 1.954498582108791,
        "step": 15163
    },
    {
        "loss": 2.2019,
        "grad_norm": 1.6083003282546997,
        "learning_rate": 2.349534143931076e-06,
        "epoch": 1.9546274813096158,
        "step": 15164
    },
    {
        "loss": 1.3492,
        "grad_norm": 2.343492269515991,
        "learning_rate": 2.336441348726026e-06,
        "epoch": 1.9547563805104409,
        "step": 15165
    },
    {
        "loss": 1.8986,
        "grad_norm": 2.1660561561584473,
        "learning_rate": 2.3233847041516876e-06,
        "epoch": 1.9548852797112657,
        "step": 15166
    },
    {
        "loss": 1.1735,
        "grad_norm": 2.399611473083496,
        "learning_rate": 2.3103642150410386e-06,
        "epoch": 1.9550141789120907,
        "step": 15167
    },
    {
        "loss": 0.769,
        "grad_norm": 3.0938940048217773,
        "learning_rate": 2.2973798862136576e-06,
        "epoch": 1.9551430781129158,
        "step": 15168
    },
    {
        "loss": 0.9837,
        "grad_norm": 2.3882713317871094,
        "learning_rate": 2.284431722475766e-06,
        "epoch": 1.9552719773137408,
        "step": 15169
    },
    {
        "loss": 1.3839,
        "grad_norm": 1.9151771068572998,
        "learning_rate": 2.271519728620175e-06,
        "epoch": 1.9554008765145656,
        "step": 15170
    },
    {
        "loss": 1.2227,
        "grad_norm": 2.176417589187622,
        "learning_rate": 2.258643909426328e-06,
        "epoch": 1.9555297757153904,
        "step": 15171
    },
    {
        "loss": 1.9925,
        "grad_norm": 2.1211462020874023,
        "learning_rate": 2.2458042696602566e-06,
        "epoch": 1.9556586749162155,
        "step": 15172
    },
    {
        "loss": 1.9871,
        "grad_norm": 2.435391426086426,
        "learning_rate": 2.233000814074615e-06,
        "epoch": 1.9557875741170405,
        "step": 15173
    },
    {
        "loss": 1.4657,
        "grad_norm": 3.25046706199646,
        "learning_rate": 2.220233547408701e-06,
        "epoch": 1.9559164733178656,
        "step": 15174
    },
    {
        "loss": 1.6162,
        "grad_norm": 4.110962390899658,
        "learning_rate": 2.207502474388379e-06,
        "epoch": 1.9560453725186904,
        "step": 15175
    },
    {
        "loss": 0.9963,
        "grad_norm": 3.5427815914154053,
        "learning_rate": 2.194807599726045e-06,
        "epoch": 1.9561742717195152,
        "step": 15176
    },
    {
        "loss": 1.7421,
        "grad_norm": 1.755289912223816,
        "learning_rate": 2.18214892812082e-06,
        "epoch": 1.9563031709203402,
        "step": 15177
    },
    {
        "loss": 2.1855,
        "grad_norm": 3.2222113609313965,
        "learning_rate": 2.1695264642583756e-06,
        "epoch": 1.9564320701211653,
        "step": 15178
    },
    {
        "loss": 1.1447,
        "grad_norm": 2.5529863834381104,
        "learning_rate": 2.156940212810943e-06,
        "epoch": 1.9565609693219903,
        "step": 15179
    },
    {
        "loss": 0.9072,
        "grad_norm": 2.5916411876678467,
        "learning_rate": 2.1443901784374276e-06,
        "epoch": 1.9566898685228151,
        "step": 15180
    },
    {
        "loss": 1.1483,
        "grad_norm": 2.0567214488983154,
        "learning_rate": 2.131876365783292e-06,
        "epoch": 1.9568187677236402,
        "step": 15181
    },
    {
        "loss": 0.7875,
        "grad_norm": 2.2027533054351807,
        "learning_rate": 2.119398779480497e-06,
        "epoch": 1.956947666924465,
        "step": 15182
    },
    {
        "loss": 1.1277,
        "grad_norm": 2.373549222946167,
        "learning_rate": 2.106957424147771e-06,
        "epoch": 1.95707656612529,
        "step": 15183
    },
    {
        "loss": 1.0369,
        "grad_norm": 2.120464563369751,
        "learning_rate": 2.094552304390307e-06,
        "epoch": 1.957205465326115,
        "step": 15184
    },
    {
        "loss": 1.2976,
        "grad_norm": 2.8268988132476807,
        "learning_rate": 2.0821834247999106e-06,
        "epoch": 1.95733436452694,
        "step": 15185
    },
    {
        "loss": 1.6687,
        "grad_norm": 3.037590980529785,
        "learning_rate": 2.069850789954997e-06,
        "epoch": 1.957463263727765,
        "step": 15186
    },
    {
        "loss": 1.5634,
        "grad_norm": 2.823509454727173,
        "learning_rate": 2.0575544044205474e-06,
        "epoch": 1.9575921629285897,
        "step": 15187
    },
    {
        "loss": 2.3308,
        "grad_norm": 2.4911580085754395,
        "learning_rate": 2.0452942727481106e-06,
        "epoch": 1.9577210621294148,
        "step": 15188
    },
    {
        "loss": 1.3149,
        "grad_norm": 3.777982711791992,
        "learning_rate": 2.033070399475845e-06,
        "epoch": 1.9578499613302398,
        "step": 15189
    },
    {
        "loss": 1.2741,
        "grad_norm": 2.312046766281128,
        "learning_rate": 2.020882789128453e-06,
        "epoch": 1.9579788605310648,
        "step": 15190
    },
    {
        "loss": 1.4962,
        "grad_norm": 2.4752233028411865,
        "learning_rate": 2.0087314462172935e-06,
        "epoch": 1.9581077597318897,
        "step": 15191
    },
    {
        "loss": 1.2809,
        "grad_norm": 2.5474283695220947,
        "learning_rate": 1.9966163752402345e-06,
        "epoch": 1.9582366589327145,
        "step": 15192
    },
    {
        "loss": 1.2596,
        "grad_norm": 3.340020179748535,
        "learning_rate": 1.9845375806816444e-06,
        "epoch": 1.9583655581335395,
        "step": 15193
    },
    {
        "loss": 1.7725,
        "grad_norm": 1.6223372220993042,
        "learning_rate": 1.972495067012614e-06,
        "epoch": 1.9584944573343646,
        "step": 15194
    },
    {
        "loss": 1.9998,
        "grad_norm": 2.116018533706665,
        "learning_rate": 1.960488838690744e-06,
        "epoch": 1.9586233565351896,
        "step": 15195
    },
    {
        "loss": 1.0683,
        "grad_norm": 3.0625834465026855,
        "learning_rate": 1.9485189001601566e-06,
        "epoch": 1.9587522557360144,
        "step": 15196
    },
    {
        "loss": 1.13,
        "grad_norm": 2.151263475418091,
        "learning_rate": 1.936585255851631e-06,
        "epoch": 1.9588811549368392,
        "step": 15197
    },
    {
        "loss": 1.1779,
        "grad_norm": 3.568187952041626,
        "learning_rate": 1.9246879101824542e-06,
        "epoch": 1.9590100541376643,
        "step": 15198
    },
    {
        "loss": 1.1736,
        "grad_norm": 2.2263529300689697,
        "learning_rate": 1.9128268675564167e-06,
        "epoch": 1.9591389533384893,
        "step": 15199
    },
    {
        "loss": 1.3157,
        "grad_norm": 2.5697381496429443,
        "learning_rate": 1.901002132364016e-06,
        "epoch": 1.9592678525393143,
        "step": 15200
    },
    {
        "loss": 1.9317,
        "grad_norm": 3.4965696334838867,
        "learning_rate": 1.8892137089822071e-06,
        "epoch": 1.9593967517401392,
        "step": 15201
    },
    {
        "loss": 1.0179,
        "grad_norm": 2.8274574279785156,
        "learning_rate": 1.8774616017745438e-06,
        "epoch": 1.9595256509409642,
        "step": 15202
    },
    {
        "loss": 0.8207,
        "grad_norm": 2.5254037380218506,
        "learning_rate": 1.8657458150911023e-06,
        "epoch": 1.959654550141789,
        "step": 15203
    },
    {
        "loss": 1.3332,
        "grad_norm": 3.558465003967285,
        "learning_rate": 1.8540663532685465e-06,
        "epoch": 1.959783449342614,
        "step": 15204
    },
    {
        "loss": 1.861,
        "grad_norm": 2.694295883178711,
        "learning_rate": 1.842423220630085e-06,
        "epoch": 1.959912348543439,
        "step": 15205
    },
    {
        "loss": 2.2706,
        "grad_norm": 2.3452155590057373,
        "learning_rate": 1.8308164214854817e-06,
        "epoch": 1.9600412477442641,
        "step": 15206
    },
    {
        "loss": 1.8814,
        "grad_norm": 3.2329530715942383,
        "learning_rate": 1.819245960131033e-06,
        "epoch": 1.960170146945089,
        "step": 15207
    },
    {
        "loss": 1.5856,
        "grad_norm": 1.927529215812683,
        "learning_rate": 1.807711840849624e-06,
        "epoch": 1.9602990461459138,
        "step": 15208
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.792554259300232,
        "learning_rate": 1.7962140679106842e-06,
        "epoch": 1.9604279453467388,
        "step": 15209
    },
    {
        "loss": 1.5692,
        "grad_norm": 1.8519097566604614,
        "learning_rate": 1.7847526455700757e-06,
        "epoch": 1.9605568445475638,
        "step": 15210
    },
    {
        "loss": 1.7379,
        "grad_norm": 1.7025806903839111,
        "learning_rate": 1.773327578070394e-06,
        "epoch": 1.9606857437483889,
        "step": 15211
    },
    {
        "loss": 1.891,
        "grad_norm": 1.4063410758972168,
        "learning_rate": 1.7619388696406558e-06,
        "epoch": 1.9608146429492137,
        "step": 15212
    },
    {
        "loss": 1.1966,
        "grad_norm": 1.7873611450195312,
        "learning_rate": 1.7505865244964115e-06,
        "epoch": 1.9609435421500385,
        "step": 15213
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.889725923538208,
        "learning_rate": 1.7392705468398329e-06,
        "epoch": 1.9610724413508636,
        "step": 15214
    },
    {
        "loss": 1.8922,
        "grad_norm": 1.813727617263794,
        "learning_rate": 1.7279909408595919e-06,
        "epoch": 1.9612013405516886,
        "step": 15215
    },
    {
        "loss": 1.4039,
        "grad_norm": 2.5766208171844482,
        "learning_rate": 1.7167477107308038e-06,
        "epoch": 1.9613302397525136,
        "step": 15216
    },
    {
        "loss": 1.1048,
        "grad_norm": 2.6352338790893555,
        "learning_rate": 1.7055408606152845e-06,
        "epoch": 1.9614591389533385,
        "step": 15217
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.7752964496612549,
        "learning_rate": 1.694370394661282e-06,
        "epoch": 1.9615880381541635,
        "step": 15218
    },
    {
        "loss": 1.9025,
        "grad_norm": 3.1302926540374756,
        "learning_rate": 1.6832363170035893e-06,
        "epoch": 1.9617169373549883,
        "step": 15219
    },
    {
        "loss": 0.9636,
        "grad_norm": 3.038158655166626,
        "learning_rate": 1.6721386317635423e-06,
        "epoch": 1.9618458365558133,
        "step": 15220
    },
    {
        "loss": 1.8161,
        "grad_norm": 1.5658469200134277,
        "learning_rate": 1.6610773430490002e-06,
        "epoch": 1.9619747357566384,
        "step": 15221
    },
    {
        "loss": 1.4994,
        "grad_norm": 2.388000965118408,
        "learning_rate": 1.650052454954354e-06,
        "epoch": 1.9621036349574634,
        "step": 15222
    },
    {
        "loss": 1.0224,
        "grad_norm": 2.9903268814086914,
        "learning_rate": 1.6390639715605283e-06,
        "epoch": 1.9622325341582882,
        "step": 15223
    },
    {
        "loss": 1.7251,
        "grad_norm": 2.6940219402313232,
        "learning_rate": 1.628111896934914e-06,
        "epoch": 1.962361433359113,
        "step": 15224
    },
    {
        "loss": 1.425,
        "grad_norm": 2.371480703353882,
        "learning_rate": 1.617196235131546e-06,
        "epoch": 1.962490332559938,
        "step": 15225
    },
    {
        "loss": 2.0572,
        "grad_norm": 3.65228271484375,
        "learning_rate": 1.6063169901908592e-06,
        "epoch": 1.9626192317607631,
        "step": 15226
    },
    {
        "loss": 1.5728,
        "grad_norm": 2.0508193969726562,
        "learning_rate": 1.5954741661398764e-06,
        "epoch": 1.9627481309615882,
        "step": 15227
    },
    {
        "loss": 0.7531,
        "grad_norm": 1.9086098670959473,
        "learning_rate": 1.5846677669921206e-06,
        "epoch": 1.962877030162413,
        "step": 15228
    },
    {
        "loss": 2.0721,
        "grad_norm": 2.347794532775879,
        "learning_rate": 1.5738977967476365e-06,
        "epoch": 1.9630059293632378,
        "step": 15229
    },
    {
        "loss": 0.8172,
        "grad_norm": 3.2705161571502686,
        "learning_rate": 1.5631642593929352e-06,
        "epoch": 1.9631348285640629,
        "step": 15230
    },
    {
        "loss": 1.1069,
        "grad_norm": 3.617340087890625,
        "learning_rate": 1.5524671589011497e-06,
        "epoch": 1.963263727764888,
        "step": 15231
    },
    {
        "loss": 1.1513,
        "grad_norm": 3.0006980895996094,
        "learning_rate": 1.541806499231857e-06,
        "epoch": 1.963392626965713,
        "step": 15232
    },
    {
        "loss": 1.5764,
        "grad_norm": 2.635579824447632,
        "learning_rate": 1.5311822843310787e-06,
        "epoch": 1.9635215261665377,
        "step": 15233
    },
    {
        "loss": 1.0521,
        "grad_norm": 6.650941371917725,
        "learning_rate": 1.5205945181315018e-06,
        "epoch": 1.9636504253673626,
        "step": 15234
    },
    {
        "loss": 1.6683,
        "grad_norm": 2.465996265411377,
        "learning_rate": 1.5100432045521918e-06,
        "epoch": 1.9637793245681876,
        "step": 15235
    },
    {
        "loss": 1.8554,
        "grad_norm": 2.410325050354004,
        "learning_rate": 1.4995283474987908e-06,
        "epoch": 1.9639082237690126,
        "step": 15236
    },
    {
        "loss": 0.6694,
        "grad_norm": 3.029134511947632,
        "learning_rate": 1.4890499508634192e-06,
        "epoch": 1.9640371229698377,
        "step": 15237
    },
    {
        "loss": 2.0744,
        "grad_norm": 3.534999132156372,
        "learning_rate": 1.4786080185246852e-06,
        "epoch": 1.9641660221706625,
        "step": 15238
    },
    {
        "loss": 1.6345,
        "grad_norm": 2.573702812194824,
        "learning_rate": 1.4682025543477419e-06,
        "epoch": 1.9642949213714875,
        "step": 15239
    },
    {
        "loss": 1.6845,
        "grad_norm": 5.030900478363037,
        "learning_rate": 1.4578335621842187e-06,
        "epoch": 1.9644238205723124,
        "step": 15240
    },
    {
        "loss": 0.6952,
        "grad_norm": 2.4257924556732178,
        "learning_rate": 1.4475010458722127e-06,
        "epoch": 1.9645527197731374,
        "step": 15241
    },
    {
        "loss": 1.7546,
        "grad_norm": 2.8122060298919678,
        "learning_rate": 1.4372050092364086e-06,
        "epoch": 1.9646816189739624,
        "step": 15242
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.243682861328125,
        "learning_rate": 1.4269454560879025e-06,
        "epoch": 1.9648105181747875,
        "step": 15243
    },
    {
        "loss": 0.9676,
        "grad_norm": 2.956028461456299,
        "learning_rate": 1.4167223902243344e-06,
        "epoch": 1.9649394173756123,
        "step": 15244
    },
    {
        "loss": 0.3622,
        "grad_norm": 1.5604702234268188,
        "learning_rate": 1.4065358154297991e-06,
        "epoch": 1.965068316576437,
        "step": 15245
    },
    {
        "loss": 1.5577,
        "grad_norm": 3.089176893234253,
        "learning_rate": 1.3963857354749256e-06,
        "epoch": 1.9651972157772621,
        "step": 15246
    },
    {
        "loss": 1.4648,
        "grad_norm": 4.10972261428833,
        "learning_rate": 1.3862721541167745e-06,
        "epoch": 1.9653261149780872,
        "step": 15247
    },
    {
        "loss": 1.1635,
        "grad_norm": 2.97853422164917,
        "learning_rate": 1.376195075098985e-06,
        "epoch": 1.9654550141789122,
        "step": 15248
    },
    {
        "loss": 1.7512,
        "grad_norm": 2.728935480117798,
        "learning_rate": 1.366154502151651e-06,
        "epoch": 1.965583913379737,
        "step": 15249
    },
    {
        "loss": 1.6538,
        "grad_norm": 2.547175884246826,
        "learning_rate": 1.356150438991255e-06,
        "epoch": 1.9657128125805619,
        "step": 15250
    },
    {
        "loss": 1.7151,
        "grad_norm": 2.2597954273223877,
        "learning_rate": 1.3461828893209128e-06,
        "epoch": 1.965841711781387,
        "step": 15251
    },
    {
        "loss": 1.1148,
        "grad_norm": 2.1962573528289795,
        "learning_rate": 1.3362518568301508e-06,
        "epoch": 1.965970610982212,
        "step": 15252
    },
    {
        "loss": 1.6819,
        "grad_norm": 2.0461649894714355,
        "learning_rate": 1.326357345194984e-06,
        "epoch": 1.966099510183037,
        "step": 15253
    },
    {
        "loss": 0.6017,
        "grad_norm": 2.0497889518737793,
        "learning_rate": 1.316499358077894e-06,
        "epoch": 1.9662284093838618,
        "step": 15254
    },
    {
        "loss": 1.8289,
        "grad_norm": 2.651309013366699,
        "learning_rate": 1.3066778991278949e-06,
        "epoch": 1.9663573085846868,
        "step": 15255
    },
    {
        "loss": 1.7807,
        "grad_norm": 2.7267048358917236,
        "learning_rate": 1.2968929719804345e-06,
        "epoch": 1.9664862077855116,
        "step": 15256
    },
    {
        "loss": 0.9266,
        "grad_norm": 1.365939736366272,
        "learning_rate": 1.2871445802574377e-06,
        "epoch": 1.9666151069863367,
        "step": 15257
    },
    {
        "loss": 1.6227,
        "grad_norm": 2.3058583736419678,
        "learning_rate": 1.277432727567307e-06,
        "epoch": 1.9667440061871617,
        "step": 15258
    },
    {
        "loss": 0.8696,
        "grad_norm": 2.445491313934326,
        "learning_rate": 1.2677574175049667e-06,
        "epoch": 1.9668729053879868,
        "step": 15259
    },
    {
        "loss": 1.1577,
        "grad_norm": 2.589952230453491,
        "learning_rate": 1.2581186536517631e-06,
        "epoch": 1.9670018045888116,
        "step": 15260
    },
    {
        "loss": 1.2246,
        "grad_norm": 1.5131056308746338,
        "learning_rate": 1.2485164395755312e-06,
        "epoch": 1.9671307037896364,
        "step": 15261
    },
    {
        "loss": 1.938,
        "grad_norm": 3.1788055896759033,
        "learning_rate": 1.2389507788305832e-06,
        "epoch": 1.9672596029904614,
        "step": 15262
    },
    {
        "loss": 1.5848,
        "grad_norm": 2.4927072525024414,
        "learning_rate": 1.2294216749576759e-06,
        "epoch": 1.9673885021912865,
        "step": 15263
    },
    {
        "loss": 1.4499,
        "grad_norm": 2.2666852474212646,
        "learning_rate": 1.2199291314840434e-06,
        "epoch": 1.9675174013921115,
        "step": 15264
    },
    {
        "loss": 1.5586,
        "grad_norm": 2.4099786281585693,
        "learning_rate": 1.2104731519234302e-06,
        "epoch": 1.9676463005929363,
        "step": 15265
    },
    {
        "loss": 2.0367,
        "grad_norm": 2.275083303451538,
        "learning_rate": 1.2010537397760036e-06,
        "epoch": 1.9677751997937611,
        "step": 15266
    },
    {
        "loss": 0.5103,
        "grad_norm": 1.6651852130889893,
        "learning_rate": 1.1916708985283964e-06,
        "epoch": 1.9679040989945862,
        "step": 15267
    },
    {
        "loss": 0.6394,
        "grad_norm": 3.2580597400665283,
        "learning_rate": 1.1823246316537085e-06,
        "epoch": 1.9680329981954112,
        "step": 15268
    },
    {
        "loss": 1.4159,
        "grad_norm": 1.9787715673446655,
        "learning_rate": 1.1730149426115055e-06,
        "epoch": 1.9681618973962363,
        "step": 15269
    },
    {
        "loss": 1.264,
        "grad_norm": 2.4532341957092285,
        "learning_rate": 1.1637418348478202e-06,
        "epoch": 1.968290796597061,
        "step": 15270
    },
    {
        "loss": 0.3411,
        "grad_norm": 2.1961886882781982,
        "learning_rate": 1.1545053117951287e-06,
        "epoch": 1.968419695797886,
        "step": 15271
    },
    {
        "loss": 1.6324,
        "grad_norm": 2.6263065338134766,
        "learning_rate": 1.1453053768723854e-06,
        "epoch": 1.968548594998711,
        "step": 15272
    },
    {
        "loss": 1.3076,
        "grad_norm": 3.758460521697998,
        "learning_rate": 1.136142033484977e-06,
        "epoch": 1.968677494199536,
        "step": 15273
    },
    {
        "loss": 2.0872,
        "grad_norm": 1.595116376876831,
        "learning_rate": 1.1270152850247683e-06,
        "epoch": 1.968806393400361,
        "step": 15274
    },
    {
        "loss": 1.5568,
        "grad_norm": 1.8666770458221436,
        "learning_rate": 1.1179251348700458e-06,
        "epoch": 1.9689352926011858,
        "step": 15275
    },
    {
        "loss": 2.1429,
        "grad_norm": 2.042773485183716,
        "learning_rate": 1.108871586385607e-06,
        "epoch": 1.9690641918020109,
        "step": 15276
    },
    {
        "loss": 1.3868,
        "grad_norm": 2.532318592071533,
        "learning_rate": 1.0998546429226487e-06,
        "epoch": 1.9691930910028357,
        "step": 15277
    },
    {
        "loss": 1.148,
        "grad_norm": 3.4102559089660645,
        "learning_rate": 1.0908743078188455e-06,
        "epoch": 1.9693219902036607,
        "step": 15278
    },
    {
        "loss": 1.4414,
        "grad_norm": 2.116417407989502,
        "learning_rate": 1.0819305843982941e-06,
        "epoch": 1.9694508894044858,
        "step": 15279
    },
    {
        "loss": 2.0118,
        "grad_norm": 2.4487102031707764,
        "learning_rate": 1.0730234759715685e-06,
        "epoch": 1.9695797886053108,
        "step": 15280
    },
    {
        "loss": 1.6293,
        "grad_norm": 1.6181533336639404,
        "learning_rate": 1.0641529858356536e-06,
        "epoch": 1.9697086878061356,
        "step": 15281
    },
    {
        "loss": 1.3244,
        "grad_norm": 1.701456904411316,
        "learning_rate": 1.055319117274045e-06,
        "epoch": 1.9698375870069604,
        "step": 15282
    },
    {
        "loss": 1.7253,
        "grad_norm": 2.6348695755004883,
        "learning_rate": 1.0465218735566162e-06,
        "epoch": 1.9699664862077855,
        "step": 15283
    },
    {
        "loss": 1.8737,
        "grad_norm": 2.221818447113037,
        "learning_rate": 1.0377612579397178e-06,
        "epoch": 1.9700953854086105,
        "step": 15284
    },
    {
        "loss": 1.5903,
        "grad_norm": 1.4750407934188843,
        "learning_rate": 1.0290372736661224e-06,
        "epoch": 1.9702242846094355,
        "step": 15285
    },
    {
        "loss": 1.5481,
        "grad_norm": 2.1900036334991455,
        "learning_rate": 1.0203499239650582e-06,
        "epoch": 1.9703531838102604,
        "step": 15286
    },
    {
        "loss": 1.2703,
        "grad_norm": 3.614377498626709,
        "learning_rate": 1.0116992120521862e-06,
        "epoch": 1.9704820830110852,
        "step": 15287
    },
    {
        "loss": 1.1316,
        "grad_norm": 2.7664389610290527,
        "learning_rate": 1.0030851411296117e-06,
        "epoch": 1.9706109822119102,
        "step": 15288
    },
    {
        "loss": 1.0842,
        "grad_norm": 2.3829689025878906,
        "learning_rate": 9.945077143858616e-07,
        "epoch": 1.9707398814127353,
        "step": 15289
    },
    {
        "loss": 1.5626,
        "grad_norm": 3.2163732051849365,
        "learning_rate": 9.859669349959632e-07,
        "epoch": 1.9708687806135603,
        "step": 15290
    },
    {
        "loss": 0.7578,
        "grad_norm": 1.584086537361145,
        "learning_rate": 9.774628061212654e-07,
        "epoch": 1.9709976798143851,
        "step": 15291
    },
    {
        "loss": 1.6865,
        "grad_norm": 2.3162598609924316,
        "learning_rate": 9.689953309096278e-07,
        "epoch": 1.9711265790152102,
        "step": 15292
    },
    {
        "loss": 1.7793,
        "grad_norm": 3.2312161922454834,
        "learning_rate": 9.605645124953321e-07,
        "epoch": 1.971255478216035,
        "step": 15293
    },
    {
        "loss": 0.9055,
        "grad_norm": 2.93412184715271,
        "learning_rate": 9.521703539991045e-07,
        "epoch": 1.97138437741686,
        "step": 15294
    },
    {
        "loss": 1.7978,
        "grad_norm": 1.980243444442749,
        "learning_rate": 9.438128585280481e-07,
        "epoch": 1.971513276617685,
        "step": 15295
    },
    {
        "loss": 1.5129,
        "grad_norm": 5.207828998565674,
        "learning_rate": 9.354920291757663e-07,
        "epoch": 1.97164217581851,
        "step": 15296
    },
    {
        "loss": 1.4125,
        "grad_norm": 2.1299948692321777,
        "learning_rate": 9.272078690222174e-07,
        "epoch": 1.971771075019335,
        "step": 15297
    },
    {
        "loss": 1.3333,
        "grad_norm": 2.6136350631713867,
        "learning_rate": 9.189603811338265e-07,
        "epoch": 1.9718999742201597,
        "step": 15298
    },
    {
        "loss": 0.9584,
        "grad_norm": 3.965655565261841,
        "learning_rate": 9.107495685634737e-07,
        "epoch": 1.9720288734209848,
        "step": 15299
    },
    {
        "loss": 1.7714,
        "grad_norm": 2.167840003967285,
        "learning_rate": 9.025754343504055e-07,
        "epoch": 1.9721577726218098,
        "step": 15300
    },
    {
        "loss": 1.81,
        "grad_norm": 2.052541494369507,
        "learning_rate": 8.944379815203241e-07,
        "epoch": 1.9722866718226348,
        "step": 15301
    },
    {
        "loss": 1.4017,
        "grad_norm": 2.686919689178467,
        "learning_rate": 8.86337213085342e-07,
        "epoch": 1.9724155710234597,
        "step": 15302
    },
    {
        "loss": 0.7969,
        "grad_norm": 3.3238284587860107,
        "learning_rate": 8.782731320440052e-07,
        "epoch": 1.9725444702242845,
        "step": 15303
    },
    {
        "loss": 2.1362,
        "grad_norm": 2.6439390182495117,
        "learning_rate": 8.702457413812481e-07,
        "epoch": 1.9726733694251095,
        "step": 15304
    },
    {
        "loss": 1.356,
        "grad_norm": 2.1781976222991943,
        "learning_rate": 8.622550440684829e-07,
        "epoch": 1.9728022686259346,
        "step": 15305
    },
    {
        "loss": 2.3837,
        "grad_norm": 3.2073395252227783,
        "learning_rate": 8.543010430634546e-07,
        "epoch": 1.9729311678267596,
        "step": 15306
    },
    {
        "loss": 1.5118,
        "grad_norm": 1.8835124969482422,
        "learning_rate": 8.463837413104525e-07,
        "epoch": 1.9730600670275844,
        "step": 15307
    },
    {
        "loss": 1.8973,
        "grad_norm": 3.9165713787078857,
        "learning_rate": 8.385031417400324e-07,
        "epoch": 1.9731889662284092,
        "step": 15308
    },
    {
        "loss": 0.855,
        "grad_norm": 3.9511799812316895,
        "learning_rate": 8.306592472692276e-07,
        "epoch": 1.9733178654292343,
        "step": 15309
    },
    {
        "loss": 1.1207,
        "grad_norm": 3.0717015266418457,
        "learning_rate": 8.228520608015599e-07,
        "epoch": 1.9734467646300593,
        "step": 15310
    },
    {
        "loss": 1.69,
        "grad_norm": 2.3900296688079834,
        "learning_rate": 8.150815852268623e-07,
        "epoch": 1.9735756638308843,
        "step": 15311
    },
    {
        "loss": 1.58,
        "grad_norm": 2.3086812496185303,
        "learning_rate": 8.073478234214116e-07,
        "epoch": 1.9737045630317092,
        "step": 15312
    },
    {
        "loss": 1.7765,
        "grad_norm": 2.140950918197632,
        "learning_rate": 7.996507782478957e-07,
        "epoch": 1.9738334622325342,
        "step": 15313
    },
    {
        "loss": 1.4234,
        "grad_norm": 1.5581821203231812,
        "learning_rate": 7.919904525554134e-07,
        "epoch": 1.973962361433359,
        "step": 15314
    },
    {
        "loss": 0.8088,
        "grad_norm": 2.465716600418091,
        "learning_rate": 7.843668491794409e-07,
        "epoch": 1.974091260634184,
        "step": 15315
    },
    {
        "loss": 1.841,
        "grad_norm": 2.2368907928466797,
        "learning_rate": 7.767799709419543e-07,
        "epoch": 1.974220159835009,
        "step": 15316
    },
    {
        "loss": 1.4639,
        "grad_norm": 2.1623802185058594,
        "learning_rate": 7.692298206512405e-07,
        "epoch": 1.9743490590358341,
        "step": 15317
    },
    {
        "loss": 1.9551,
        "grad_norm": 2.168083429336548,
        "learning_rate": 7.617164011020194e-07,
        "epoch": 1.974477958236659,
        "step": 15318
    },
    {
        "loss": 1.7664,
        "grad_norm": 5.2217278480529785,
        "learning_rate": 7.542397150754332e-07,
        "epoch": 1.9746068574374838,
        "step": 15319
    },
    {
        "loss": 2.2025,
        "grad_norm": 1.786312222480774,
        "learning_rate": 7.467997653389902e-07,
        "epoch": 1.9747357566383088,
        "step": 15320
    },
    {
        "loss": 1.0222,
        "grad_norm": 3.5205698013305664,
        "learning_rate": 7.393965546466319e-07,
        "epoch": 1.9748646558391338,
        "step": 15321
    },
    {
        "loss": 1.2324,
        "grad_norm": 2.844602108001709,
        "learning_rate": 7.320300857387108e-07,
        "epoch": 1.9749935550399589,
        "step": 15322
    },
    {
        "loss": 2.0024,
        "grad_norm": 3.457303285598755,
        "learning_rate": 7.247003613419234e-07,
        "epoch": 1.9751224542407837,
        "step": 15323
    },
    {
        "loss": 1.7622,
        "grad_norm": 2.688570499420166,
        "learning_rate": 7.174073841694551e-07,
        "epoch": 1.9752513534416085,
        "step": 15324
    },
    {
        "loss": 0.8449,
        "grad_norm": 2.7153544425964355,
        "learning_rate": 7.101511569207797e-07,
        "epoch": 1.9753802526424336,
        "step": 15325
    },
    {
        "loss": 1.8252,
        "grad_norm": 1.8638763427734375,
        "learning_rate": 7.029316822818377e-07,
        "epoch": 1.9755091518432586,
        "step": 15326
    },
    {
        "loss": 0.7708,
        "grad_norm": 2.770430088043213,
        "learning_rate": 6.957489629249802e-07,
        "epoch": 1.9756380510440836,
        "step": 15327
    },
    {
        "loss": 1.8262,
        "grad_norm": 2.893082618713379,
        "learning_rate": 6.88603001508914e-07,
        "epoch": 1.9757669502449084,
        "step": 15328
    },
    {
        "loss": 1.2482,
        "grad_norm": 1.7276297807693481,
        "learning_rate": 6.814938006787342e-07,
        "epoch": 1.9758958494457335,
        "step": 15329
    },
    {
        "loss": 1.5546,
        "grad_norm": 1.8078320026397705,
        "learning_rate": 6.744213630659801e-07,
        "epoch": 1.9760247486465583,
        "step": 15330
    },
    {
        "loss": 1.8108,
        "grad_norm": 2.4999144077301025,
        "learning_rate": 6.673856912885023e-07,
        "epoch": 1.9761536478473833,
        "step": 15331
    },
    {
        "loss": 1.2005,
        "grad_norm": 2.209963321685791,
        "learning_rate": 6.603867879505954e-07,
        "epoch": 1.9762825470482084,
        "step": 15332
    },
    {
        "loss": 1.3037,
        "grad_norm": 1.9440069198608398,
        "learning_rate": 6.534246556429758e-07,
        "epoch": 1.9764114462490334,
        "step": 15333
    },
    {
        "loss": 1.4417,
        "grad_norm": 2.4584314823150635,
        "learning_rate": 6.46499296942682e-07,
        "epoch": 1.9765403454498582,
        "step": 15334
    },
    {
        "loss": 2.1844,
        "grad_norm": 2.4903717041015625,
        "learning_rate": 6.396107144131636e-07,
        "epoch": 1.976669244650683,
        "step": 15335
    },
    {
        "loss": 1.3064,
        "grad_norm": 2.871917724609375,
        "learning_rate": 6.327589106042808e-07,
        "epoch": 1.976798143851508,
        "step": 15336
    },
    {
        "loss": 1.6919,
        "grad_norm": 2.7261970043182373,
        "learning_rate": 6.259438880522494e-07,
        "epoch": 1.9769270430523331,
        "step": 15337
    },
    {
        "loss": 1.9868,
        "grad_norm": 2.2080490589141846,
        "learning_rate": 6.191656492796849e-07,
        "epoch": 1.9770559422531582,
        "step": 15338
    },
    {
        "loss": 0.8207,
        "grad_norm": 3.4381191730499268,
        "learning_rate": 6.124241967955802e-07,
        "epoch": 1.977184841453983,
        "step": 15339
    },
    {
        "loss": 1.24,
        "grad_norm": 2.679922103881836,
        "learning_rate": 6.057195330953058e-07,
        "epoch": 1.9773137406548078,
        "step": 15340
    },
    {
        "loss": 1.8896,
        "grad_norm": 2.104971170425415,
        "learning_rate": 5.990516606606656e-07,
        "epoch": 1.9774426398556328,
        "step": 15341
    },
    {
        "loss": 0.689,
        "grad_norm": 2.3084709644317627,
        "learning_rate": 5.924205819597628e-07,
        "epoch": 1.9775715390564579,
        "step": 15342
    },
    {
        "loss": 0.5856,
        "grad_norm": 1.9524277448654175,
        "learning_rate": 5.85826299447123e-07,
        "epoch": 1.977700438257283,
        "step": 15343
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.6151061058044434,
        "learning_rate": 5.792688155636827e-07,
        "epoch": 1.9778293374581077,
        "step": 15344
    },
    {
        "loss": 2.2441,
        "grad_norm": 2.3381266593933105,
        "learning_rate": 5.727481327367113e-07,
        "epoch": 1.9779582366589326,
        "step": 15345
    },
    {
        "loss": 1.8955,
        "grad_norm": 1.5546844005584717,
        "learning_rate": 5.662642533798445e-07,
        "epoch": 1.9780871358597576,
        "step": 15346
    },
    {
        "loss": 1.6434,
        "grad_norm": 2.791949510574341,
        "learning_rate": 5.598171798931851e-07,
        "epoch": 1.9782160350605826,
        "step": 15347
    },
    {
        "loss": 1.7928,
        "grad_norm": 2.012071132659912,
        "learning_rate": 5.534069146630794e-07,
        "epoch": 1.9783449342614077,
        "step": 15348
    },
    {
        "loss": 1.271,
        "grad_norm": 3.7872538566589355,
        "learning_rate": 5.470334600623406e-07,
        "epoch": 1.9784738334622325,
        "step": 15349
    },
    {
        "loss": 2.1178,
        "grad_norm": 1.430401086807251,
        "learning_rate": 5.406968184501482e-07,
        "epoch": 1.9786027326630575,
        "step": 15350
    },
    {
        "loss": 2.2534,
        "grad_norm": 2.8356971740722656,
        "learning_rate": 5.34396992172026e-07,
        "epoch": 1.9787316318638823,
        "step": 15351
    },
    {
        "loss": 2.1106,
        "grad_norm": 1.844411015510559,
        "learning_rate": 5.281339835598976e-07,
        "epoch": 1.9788605310647074,
        "step": 15352
    },
    {
        "loss": 2.102,
        "grad_norm": 1.9313807487487793,
        "learning_rate": 5.219077949320305e-07,
        "epoch": 1.9789894302655324,
        "step": 15353
    },
    {
        "loss": 1.1811,
        "grad_norm": 2.3616390228271484,
        "learning_rate": 5.157184285930816e-07,
        "epoch": 1.9791183294663575,
        "step": 15354
    },
    {
        "loss": 1.4763,
        "grad_norm": 1.615984559059143,
        "learning_rate": 5.095658868340847e-07,
        "epoch": 1.9792472286671823,
        "step": 15355
    },
    {
        "loss": 1.6843,
        "grad_norm": 2.1202075481414795,
        "learning_rate": 5.034501719324291e-07,
        "epoch": 1.979376127868007,
        "step": 15356
    },
    {
        "loss": 0.9909,
        "grad_norm": 1.4114127159118652,
        "learning_rate": 4.973712861518487e-07,
        "epoch": 1.9795050270688321,
        "step": 15357
    },
    {
        "loss": 0.5846,
        "grad_norm": 3.8520407676696777,
        "learning_rate": 4.913292317425211e-07,
        "epoch": 1.9796339262696572,
        "step": 15358
    },
    {
        "loss": 0.7053,
        "grad_norm": 2.7678439617156982,
        "learning_rate": 4.853240109409019e-07,
        "epoch": 1.9797628254704822,
        "step": 15359
    },
    {
        "loss": 1.3248,
        "grad_norm": 1.6828678846359253,
        "learning_rate": 4.793556259698461e-07,
        "epoch": 1.979891724671307,
        "step": 15360
    },
    {
        "loss": 0.6647,
        "grad_norm": 4.097873210906982,
        "learning_rate": 4.734240790386202e-07,
        "epoch": 1.9800206238721318,
        "step": 15361
    },
    {
        "loss": 0.6736,
        "grad_norm": 2.628636360168457,
        "learning_rate": 4.6752937234278984e-07,
        "epoch": 1.9801495230729569,
        "step": 15362
    },
    {
        "loss": 1.0034,
        "grad_norm": 1.4959813356399536,
        "learning_rate": 4.616715080642986e-07,
        "epoch": 1.980278422273782,
        "step": 15363
    },
    {
        "loss": 1.9924,
        "grad_norm": 1.9549813270568848,
        "learning_rate": 4.55850488371512e-07,
        "epoch": 1.980407321474607,
        "step": 15364
    },
    {
        "loss": 1.4347,
        "grad_norm": 5.0653510093688965,
        "learning_rate": 4.500663154190399e-07,
        "epoch": 1.9805362206754318,
        "step": 15365
    },
    {
        "loss": 0.9403,
        "grad_norm": 2.9133400917053223,
        "learning_rate": 4.443189913479584e-07,
        "epoch": 1.9806651198762568,
        "step": 15366
    },
    {
        "loss": 1.1375,
        "grad_norm": 3.7197773456573486,
        "learning_rate": 4.3860851828566584e-07,
        "epoch": 1.9807940190770816,
        "step": 15367
    },
    {
        "loss": 0.9404,
        "grad_norm": 2.277679920196533,
        "learning_rate": 4.3293489834591585e-07,
        "epoch": 1.9809229182779067,
        "step": 15368
    },
    {
        "loss": 1.2066,
        "grad_norm": 2.1516032218933105,
        "learning_rate": 4.272981336288395e-07,
        "epoch": 1.9810518174787317,
        "step": 15369
    },
    {
        "loss": 1.3303,
        "grad_norm": 2.3850152492523193,
        "learning_rate": 4.2169822622089015e-07,
        "epoch": 1.9811807166795568,
        "step": 15370
    },
    {
        "loss": 1.1985,
        "grad_norm": 2.4060189723968506,
        "learning_rate": 4.1613517819490964e-07,
        "epoch": 1.9813096158803816,
        "step": 15371
    },
    {
        "loss": 1.5841,
        "grad_norm": 2.057628870010376,
        "learning_rate": 4.1060899161008415e-07,
        "epoch": 1.9814385150812064,
        "step": 15372
    },
    {
        "loss": 2.0389,
        "grad_norm": 2.6008260250091553,
        "learning_rate": 4.051196685119663e-07,
        "epoch": 1.9815674142820314,
        "step": 15373
    },
    {
        "loss": 1.821,
        "grad_norm": 1.906731367111206,
        "learning_rate": 3.9966721093244174e-07,
        "epoch": 1.9816963134828565,
        "step": 15374
    },
    {
        "loss": 1.4241,
        "grad_norm": 2.932072401046753,
        "learning_rate": 3.9425162088978506e-07,
        "epoch": 1.9818252126836815,
        "step": 15375
    },
    {
        "loss": 1.8298,
        "grad_norm": 2.7135348320007324,
        "learning_rate": 3.888729003885816e-07,
        "epoch": 1.9819541118845063,
        "step": 15376
    },
    {
        "loss": 1.1633,
        "grad_norm": 2.395339250564575,
        "learning_rate": 3.8353105141978317e-07,
        "epoch": 1.9820830110853311,
        "step": 15377
    },
    {
        "loss": 2.0564,
        "grad_norm": 1.6221190690994263,
        "learning_rate": 3.7822607596073036e-07,
        "epoch": 1.9822119102861562,
        "step": 15378
    },
    {
        "loss": 1.4788,
        "grad_norm": 3.07875919342041,
        "learning_rate": 3.7295797597507456e-07,
        "epoch": 1.9823408094869812,
        "step": 15379
    },
    {
        "loss": 1.4283,
        "grad_norm": 1.1366440057754517,
        "learning_rate": 3.677267534128004e-07,
        "epoch": 1.9824697086878063,
        "step": 15380
    },
    {
        "loss": 1.3258,
        "grad_norm": 3.0453712940216064,
        "learning_rate": 3.625324102103256e-07,
        "epoch": 1.982598607888631,
        "step": 15381
    },
    {
        "loss": 2.0042,
        "grad_norm": 2.335083484649658,
        "learning_rate": 3.5737494829031216e-07,
        "epoch": 1.9827275070894559,
        "step": 15382
    },
    {
        "loss": 1.2357,
        "grad_norm": 2.1892919540405273,
        "learning_rate": 3.522543695618219e-07,
        "epoch": 1.982856406290281,
        "step": 15383
    },
    {
        "loss": 1.0837,
        "grad_norm": 2.385310411453247,
        "learning_rate": 3.4717067592029417e-07,
        "epoch": 1.982985305491106,
        "step": 15384
    },
    {
        "loss": 1.1891,
        "grad_norm": 2.589960813522339,
        "learning_rate": 3.421238692474571e-07,
        "epoch": 1.983114204691931,
        "step": 15385
    },
    {
        "loss": 1.0815,
        "grad_norm": 2.627354621887207,
        "learning_rate": 3.3711395141140524e-07,
        "epoch": 1.9832431038927558,
        "step": 15386
    },
    {
        "loss": 1.6562,
        "grad_norm": 2.499891996383667,
        "learning_rate": 3.321409242666107e-07,
        "epoch": 1.9833720030935809,
        "step": 15387
    },
    {
        "loss": 1.2376,
        "grad_norm": 2.2031002044677734,
        "learning_rate": 3.272047896538344e-07,
        "epoch": 1.9835009022944057,
        "step": 15388
    },
    {
        "loss": 1.9781,
        "grad_norm": 3.1083736419677734,
        "learning_rate": 3.2230554940022585e-07,
        "epoch": 1.9836298014952307,
        "step": 15389
    },
    {
        "loss": 0.7154,
        "grad_norm": 3.027968645095825,
        "learning_rate": 3.1744320531926773e-07,
        "epoch": 1.9837587006960558,
        "step": 15390
    },
    {
        "loss": 1.1475,
        "grad_norm": 2.674685478210449,
        "learning_rate": 3.1261775921075373e-07,
        "epoch": 1.9838875998968808,
        "step": 15391
    },
    {
        "loss": 0.9574,
        "grad_norm": 3.7194957733154297,
        "learning_rate": 3.078292128608662e-07,
        "epoch": 1.9840164990977056,
        "step": 15392
    },
    {
        "loss": 2.0755,
        "grad_norm": 2.0164690017700195,
        "learning_rate": 3.030775680421205e-07,
        "epoch": 1.9841453982985304,
        "step": 15393
    },
    {
        "loss": 1.2571,
        "grad_norm": 3.070915699005127,
        "learning_rate": 2.9836282651332093e-07,
        "epoch": 1.9842742974993555,
        "step": 15394
    },
    {
        "loss": 1.5716,
        "grad_norm": 3.4096720218658447,
        "learning_rate": 2.9368499001969364e-07,
        "epoch": 1.9844031967001805,
        "step": 15395
    },
    {
        "loss": 2.2601,
        "grad_norm": 2.460120677947998,
        "learning_rate": 2.8904406029274243e-07,
        "epoch": 1.9845320959010055,
        "step": 15396
    },
    {
        "loss": 1.3694,
        "grad_norm": 3.200124740600586,
        "learning_rate": 2.8444003905030434e-07,
        "epoch": 1.9846609951018304,
        "step": 15397
    },
    {
        "loss": 1.9644,
        "grad_norm": 3.30314040184021,
        "learning_rate": 2.7987292799663835e-07,
        "epoch": 1.9847898943026552,
        "step": 15398
    },
    {
        "loss": 1.3191,
        "grad_norm": 4.094122409820557,
        "learning_rate": 2.753427288222477e-07,
        "epoch": 1.9849187935034802,
        "step": 15399
    },
    {
        "loss": 1.9143,
        "grad_norm": 2.7509708404541016,
        "learning_rate": 2.7084944320399096e-07,
        "epoch": 1.9850476927043053,
        "step": 15400
    },
    {
        "loss": 1.5586,
        "grad_norm": 2.5578560829162598,
        "learning_rate": 2.663930728051045e-07,
        "epoch": 1.9851765919051303,
        "step": 15401
    },
    {
        "loss": 1.6181,
        "grad_norm": 1.9267637729644775,
        "learning_rate": 2.619736192751354e-07,
        "epoch": 1.9853054911059551,
        "step": 15402
    },
    {
        "loss": 1.1595,
        "grad_norm": 3.0757951736450195,
        "learning_rate": 2.5759108424995294e-07,
        "epoch": 1.9854343903067802,
        "step": 15403
    },
    {
        "loss": 1.6201,
        "grad_norm": 2.58512020111084,
        "learning_rate": 2.532454693517816e-07,
        "epoch": 1.985563289507605,
        "step": 15404
    },
    {
        "loss": 0.7101,
        "grad_norm": 3.72379469871521,
        "learning_rate": 2.489367761891681e-07,
        "epoch": 1.98569218870843,
        "step": 15405
    },
    {
        "loss": 0.7559,
        "grad_norm": 2.764343023300171,
        "learning_rate": 2.4466500635699217e-07,
        "epoch": 1.985821087909255,
        "step": 15406
    },
    {
        "loss": 0.4647,
        "grad_norm": 2.593498706817627,
        "learning_rate": 2.404301614364779e-07,
        "epoch": 1.98594998711008,
        "step": 15407
    },
    {
        "loss": 2.5095,
        "grad_norm": 2.2951579093933105,
        "learning_rate": 2.3623224299516021e-07,
        "epoch": 1.986078886310905,
        "step": 15408
    },
    {
        "loss": 1.8866,
        "grad_norm": 2.682889461517334,
        "learning_rate": 2.3207125258694063e-07,
        "epoch": 1.9862077855117297,
        "step": 15409
    },
    {
        "loss": 2.05,
        "grad_norm": 1.6921318769454956,
        "learning_rate": 2.279471917520204e-07,
        "epoch": 1.9863366847125548,
        "step": 15410
    },
    {
        "loss": 1.4117,
        "grad_norm": 1.72711980342865,
        "learning_rate": 2.238600620169118e-07,
        "epoch": 1.9864655839133798,
        "step": 15411
    },
    {
        "loss": 1.4062,
        "grad_norm": 2.8907065391540527,
        "learning_rate": 2.1980986489452682e-07,
        "epoch": 1.9865944831142048,
        "step": 15412
    },
    {
        "loss": 1.6271,
        "grad_norm": 2.6915838718414307,
        "learning_rate": 2.1579660188404404e-07,
        "epoch": 1.9867233823150297,
        "step": 15413
    },
    {
        "loss": 1.1428,
        "grad_norm": 2.0701651573181152,
        "learning_rate": 2.1182027447098628e-07,
        "epoch": 1.9868522815158545,
        "step": 15414
    },
    {
        "loss": 1.8546,
        "grad_norm": 2.5878067016601562,
        "learning_rate": 2.0788088412723172e-07,
        "epoch": 1.9869811807166795,
        "step": 15415
    },
    {
        "loss": 1.7154,
        "grad_norm": 2.216012954711914,
        "learning_rate": 2.0397843231095838e-07,
        "epoch": 1.9871100799175045,
        "step": 15416
    },
    {
        "loss": 1.3785,
        "grad_norm": 2.620253324508667,
        "learning_rate": 2.001129204666552e-07,
        "epoch": 1.9872389791183296,
        "step": 15417
    },
    {
        "loss": 1.4597,
        "grad_norm": 1.7344640493392944,
        "learning_rate": 1.9628435002518873e-07,
        "epoch": 1.9873678783191544,
        "step": 15418
    },
    {
        "loss": 0.869,
        "grad_norm": 2.4822843074798584,
        "learning_rate": 1.924927224037032e-07,
        "epoch": 1.9874967775199792,
        "step": 15419
    },
    {
        "loss": 0.8815,
        "grad_norm": 3.0306313037872314,
        "learning_rate": 1.8873803900569808e-07,
        "epoch": 1.9876256767208043,
        "step": 15420
    },
    {
        "loss": 0.7577,
        "grad_norm": 2.368255376815796,
        "learning_rate": 1.850203012209839e-07,
        "epoch": 1.9877545759216293,
        "step": 15421
    },
    {
        "loss": 2.1416,
        "grad_norm": 2.1093902587890625,
        "learning_rate": 1.813395104256932e-07,
        "epoch": 1.9878834751224543,
        "step": 15422
    },
    {
        "loss": 1.4009,
        "grad_norm": 3.1726903915405273,
        "learning_rate": 1.7769566798229166e-07,
        "epoch": 1.9880123743232792,
        "step": 15423
    },
    {
        "loss": 1.3312,
        "grad_norm": 2.3362278938293457,
        "learning_rate": 1.7408877523957813e-07,
        "epoch": 1.9881412735241042,
        "step": 15424
    },
    {
        "loss": 1.903,
        "grad_norm": 2.4433200359344482,
        "learning_rate": 1.7051883353262908e-07,
        "epoch": 1.988270172724929,
        "step": 15425
    },
    {
        "loss": 0.7168,
        "grad_norm": 2.0476884841918945,
        "learning_rate": 1.6698584418290975e-07,
        "epoch": 1.988399071925754,
        "step": 15426
    },
    {
        "loss": 0.4759,
        "grad_norm": 1.380273461341858,
        "learning_rate": 1.6348980849816288e-07,
        "epoch": 1.988527971126579,
        "step": 15427
    },
    {
        "loss": 1.715,
        "grad_norm": 1.5217291116714478,
        "learning_rate": 1.600307277724533e-07,
        "epoch": 1.9886568703274041,
        "step": 15428
    },
    {
        "loss": 1.724,
        "grad_norm": 2.387026786804199,
        "learning_rate": 1.566086032861791e-07,
        "epoch": 1.988785769528229,
        "step": 15429
    },
    {
        "loss": 1.769,
        "grad_norm": 3.744640827178955,
        "learning_rate": 1.5322343630606028e-07,
        "epoch": 1.9889146687290538,
        "step": 15430
    },
    {
        "loss": 0.7436,
        "grad_norm": 1.6784534454345703,
        "learning_rate": 1.498752280851279e-07,
        "epoch": 1.9890435679298788,
        "step": 15431
    },
    {
        "loss": 0.9379,
        "grad_norm": 5.177656650543213,
        "learning_rate": 1.4656397986274606e-07,
        "epoch": 1.9891724671307038,
        "step": 15432
    },
    {
        "loss": 1.7792,
        "grad_norm": 3.0731563568115234,
        "learning_rate": 1.4328969286458993e-07,
        "epoch": 1.9893013663315289,
        "step": 15433
    },
    {
        "loss": 1.806,
        "grad_norm": 2.624926805496216,
        "learning_rate": 1.4005236830262335e-07,
        "epoch": 1.9894302655323537,
        "step": 15434
    },
    {
        "loss": 1.4265,
        "grad_norm": 2.4632227420806885,
        "learning_rate": 1.3685200737519887e-07,
        "epoch": 1.9895591647331785,
        "step": 15435
    },
    {
        "loss": 0.5669,
        "grad_norm": 2.467773675918579,
        "learning_rate": 1.3368861126692445e-07,
        "epoch": 1.9896880639340035,
        "step": 15436
    },
    {
        "loss": 1.7881,
        "grad_norm": 1.9914990663528442,
        "learning_rate": 1.3056218114874118e-07,
        "epoch": 1.9898169631348286,
        "step": 15437
    },
    {
        "loss": 0.5827,
        "grad_norm": 1.5825093984603882,
        "learning_rate": 1.274727181779234e-07,
        "epoch": 1.9899458623356536,
        "step": 15438
    },
    {
        "loss": 0.74,
        "grad_norm": 3.044370651245117,
        "learning_rate": 1.2442022349804517e-07,
        "epoch": 1.9900747615364784,
        "step": 15439
    },
    {
        "loss": 1.6803,
        "grad_norm": 2.4951822757720947,
        "learning_rate": 1.214046982390027e-07,
        "epoch": 1.9902036607373035,
        "step": 15440
    },
    {
        "loss": 1.8935,
        "grad_norm": 1.9199522733688354,
        "learning_rate": 1.184261435170031e-07,
        "epoch": 1.9903325599381283,
        "step": 15441
    },
    {
        "loss": 1.5686,
        "grad_norm": 2.370110511779785,
        "learning_rate": 1.1548456043456445e-07,
        "epoch": 1.9904614591389533,
        "step": 15442
    },
    {
        "loss": 1.9783,
        "grad_norm": 2.9550552368164062,
        "learning_rate": 1.1257995008056022e-07,
        "epoch": 1.9905903583397784,
        "step": 15443
    },
    {
        "loss": 2.2077,
        "grad_norm": 1.3790391683578491,
        "learning_rate": 1.0971231353011924e-07,
        "epoch": 1.9907192575406032,
        "step": 15444
    },
    {
        "loss": 2.0881,
        "grad_norm": 2.486827850341797,
        "learning_rate": 1.0688165184471465e-07,
        "epoch": 1.9908481567414282,
        "step": 15445
    },
    {
        "loss": 1.743,
        "grad_norm": 2.5649073123931885,
        "learning_rate": 1.0408796607213055e-07,
        "epoch": 1.990977055942253,
        "step": 15446
    },
    {
        "loss": 2.0297,
        "grad_norm": 2.6908774375915527,
        "learning_rate": 1.0133125724646197e-07,
        "epoch": 1.991105955143078,
        "step": 15447
    },
    {
        "loss": 1.3281,
        "grad_norm": 2.9239838123321533,
        "learning_rate": 9.861152638812598e-08,
        "epoch": 1.9912348543439031,
        "step": 15448
    },
    {
        "loss": 1.9391,
        "grad_norm": 2.2202229499816895,
        "learning_rate": 9.592877450382842e-08,
        "epoch": 1.9913637535447282,
        "step": 15449
    },
    {
        "loss": 1.4271,
        "grad_norm": 2.284919261932373,
        "learning_rate": 9.328300258663048e-08,
        "epoch": 1.991492652745553,
        "step": 15450
    },
    {
        "loss": 1.4318,
        "grad_norm": 1.8240244388580322,
        "learning_rate": 9.067421161583767e-08,
        "epoch": 1.9916215519463778,
        "step": 15451
    },
    {
        "loss": 2.223,
        "grad_norm": 2.941396951675415,
        "learning_rate": 8.81024025571331e-08,
        "epoch": 1.9917504511472028,
        "step": 15452
    },
    {
        "loss": 1.8249,
        "grad_norm": 1.7498310804367065,
        "learning_rate": 8.556757636248858e-08,
        "epoch": 1.9918793503480279,
        "step": 15453
    },
    {
        "loss": 2.1202,
        "grad_norm": 2.7991182804107666,
        "learning_rate": 8.306973397016471e-08,
        "epoch": 1.992008249548853,
        "step": 15454
    },
    {
        "loss": 1.9306,
        "grad_norm": 1.6075718402862549,
        "learning_rate": 8.060887630475522e-08,
        "epoch": 1.9921371487496777,
        "step": 15455
    },
    {
        "loss": 2.0719,
        "grad_norm": 2.5086841583251953,
        "learning_rate": 7.81850042771648e-08,
        "epoch": 1.9922660479505025,
        "step": 15456
    },
    {
        "loss": 1.2186,
        "grad_norm": 2.2070436477661133,
        "learning_rate": 7.579811878458687e-08,
        "epoch": 1.9923949471513276,
        "step": 15457
    },
    {
        "loss": 2.0375,
        "grad_norm": 2.51906156539917,
        "learning_rate": 7.344822071055912e-08,
        "epoch": 1.9925238463521526,
        "step": 15458
    },
    {
        "loss": 2.1918,
        "grad_norm": 2.0772385597229004,
        "learning_rate": 7.113531092487469e-08,
        "epoch": 1.9926527455529777,
        "step": 15459
    },
    {
        "loss": 0.8998,
        "grad_norm": 2.7021901607513428,
        "learning_rate": 6.885939028371535e-08,
        "epoch": 1.9927816447538025,
        "step": 15460
    },
    {
        "loss": 1.7448,
        "grad_norm": 2.7882745265960693,
        "learning_rate": 6.662045962948505e-08,
        "epoch": 1.9929105439546275,
        "step": 15461
    },
    {
        "loss": 1.6753,
        "grad_norm": 4.335976600646973,
        "learning_rate": 6.441851979095414e-08,
        "epoch": 1.9930394431554523,
        "step": 15462
    },
    {
        "loss": 1.6269,
        "grad_norm": 2.8284575939178467,
        "learning_rate": 6.225357158317068e-08,
        "epoch": 1.9931683423562774,
        "step": 15463
    },
    {
        "loss": 1.386,
        "grad_norm": 2.5177602767944336,
        "learning_rate": 6.012561580750475e-08,
        "epoch": 1.9932972415571024,
        "step": 15464
    },
    {
        "loss": 1.8763,
        "grad_norm": 2.4754457473754883,
        "learning_rate": 5.803465325162627e-08,
        "epoch": 1.9934261407579275,
        "step": 15465
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.0096609592437744,
        "learning_rate": 5.5980684689516115e-08,
        "epoch": 1.9935550399587523,
        "step": 15466
    },
    {
        "loss": 0.9424,
        "grad_norm": 1.362487554550171,
        "learning_rate": 5.3963710881477224e-08,
        "epoch": 1.993683939159577,
        "step": 15467
    },
    {
        "loss": 2.2693,
        "grad_norm": 2.711146354675293,
        "learning_rate": 5.198373257406797e-08,
        "epoch": 1.9938128383604021,
        "step": 15468
    },
    {
        "loss": 2.0825,
        "grad_norm": 2.269907236099243,
        "learning_rate": 5.0040750500213174e-08,
        "epoch": 1.9939417375612272,
        "step": 15469
    },
    {
        "loss": 2.0308,
        "grad_norm": 3.9069910049438477,
        "learning_rate": 4.813476537911532e-08,
        "epoch": 1.9940706367620522,
        "step": 15470
    },
    {
        "loss": 2.2423,
        "grad_norm": 2.1151459217071533,
        "learning_rate": 4.626577791627673e-08,
        "epoch": 1.994199535962877,
        "step": 15471
    },
    {
        "loss": 1.276,
        "grad_norm": 1.962985873222351,
        "learning_rate": 4.443378880351068e-08,
        "epoch": 1.9943284351637018,
        "step": 15472
    },
    {
        "loss": 1.4862,
        "grad_norm": 1.900702953338623,
        "learning_rate": 4.2638798718941386e-08,
        "epoch": 1.9944573343645269,
        "step": 15473
    },
    {
        "loss": 1.3645,
        "grad_norm": 2.8195340633392334,
        "learning_rate": 4.0880808326992926e-08,
        "epoch": 1.994586233565352,
        "step": 15474
    },
    {
        "loss": 1.4082,
        "grad_norm": 2.329432487487793,
        "learning_rate": 3.915981827838921e-08,
        "epoch": 1.994715132766177,
        "step": 15475
    },
    {
        "loss": 1.2406,
        "grad_norm": 3.9562594890594482,
        "learning_rate": 3.747582921015402e-08,
        "epoch": 1.9948440319670018,
        "step": 15476
    },
    {
        "loss": 1.792,
        "grad_norm": 4.179759502410889,
        "learning_rate": 3.582884174564427e-08,
        "epoch": 1.9949729311678268,
        "step": 15477
    },
    {
        "loss": 2.3541,
        "grad_norm": 2.785163640975952,
        "learning_rate": 3.421885649449452e-08,
        "epoch": 1.9951018303686516,
        "step": 15478
    },
    {
        "loss": 1.3957,
        "grad_norm": 1.8768452405929565,
        "learning_rate": 3.26458740526503e-08,
        "epoch": 1.9952307295694767,
        "step": 15479
    },
    {
        "loss": 0.9324,
        "grad_norm": 2.118657112121582,
        "learning_rate": 3.1109895002345846e-08,
        "epoch": 1.9953596287703017,
        "step": 15480
    },
    {
        "loss": 1.4432,
        "grad_norm": 5.466736316680908,
        "learning_rate": 2.961091991213749e-08,
        "epoch": 1.9954885279711265,
        "step": 15481
    },
    {
        "loss": 1.4517,
        "grad_norm": 2.3103933334350586,
        "learning_rate": 2.814894933688139e-08,
        "epoch": 1.9956174271719516,
        "step": 15482
    },
    {
        "loss": 1.58,
        "grad_norm": 1.6638226509094238,
        "learning_rate": 2.6723983817733554e-08,
        "epoch": 1.9957463263727764,
        "step": 15483
    },
    {
        "loss": 2.0715,
        "grad_norm": 2.4771900177001953,
        "learning_rate": 2.5336023882149838e-08,
        "epoch": 1.9958752255736014,
        "step": 15484
    },
    {
        "loss": 2.1805,
        "grad_norm": 2.4785008430480957,
        "learning_rate": 2.398507004388595e-08,
        "epoch": 1.9960041247744265,
        "step": 15485
    },
    {
        "loss": 1.2588,
        "grad_norm": 2.9381821155548096,
        "learning_rate": 2.2671122803008536e-08,
        "epoch": 1.9961330239752515,
        "step": 15486
    },
    {
        "loss": 1.5392,
        "grad_norm": 2.3301069736480713,
        "learning_rate": 2.1394182645873006e-08,
        "epoch": 1.9962619231760763,
        "step": 15487
    },
    {
        "loss": 1.1405,
        "grad_norm": 2.7765848636627197,
        "learning_rate": 2.015425004516791e-08,
        "epoch": 1.9963908223769011,
        "step": 15488
    },
    {
        "loss": 1.6765,
        "grad_norm": 3.085878849029541,
        "learning_rate": 1.8951325459826142e-08,
        "epoch": 1.9965197215777262,
        "step": 15489
    },
    {
        "loss": 1.7562,
        "grad_norm": 2.1305534839630127,
        "learning_rate": 1.7785409335147053e-08,
        "epoch": 1.9966486207785512,
        "step": 15490
    },
    {
        "loss": 1.1536,
        "grad_norm": 1.9811036586761475,
        "learning_rate": 1.6656502102685435e-08,
        "epoch": 1.9967775199793762,
        "step": 15491
    },
    {
        "loss": 1.8707,
        "grad_norm": 2.394503593444824,
        "learning_rate": 1.556460418030703e-08,
        "epoch": 1.996906419180201,
        "step": 15492
    },
    {
        "loss": 1.4991,
        "grad_norm": 2.2550814151763916,
        "learning_rate": 1.4509715972188531e-08,
        "epoch": 1.9970353183810259,
        "step": 15493
    },
    {
        "loss": 1.5216,
        "grad_norm": 2.225829839706421,
        "learning_rate": 1.3491837868817581e-08,
        "epoch": 1.997164217581851,
        "step": 15494
    },
    {
        "loss": 1.8085,
        "grad_norm": 2.3285017013549805,
        "learning_rate": 1.251097024693726e-08,
        "epoch": 1.997293116782676,
        "step": 15495
    },
    {
        "loss": 0.9321,
        "grad_norm": 1.7321089506149292,
        "learning_rate": 1.1567113469634905e-08,
        "epoch": 1.997422015983501,
        "step": 15496
    },
    {
        "loss": 1.018,
        "grad_norm": 2.5095396041870117,
        "learning_rate": 1.0660267886286601e-08,
        "epoch": 1.9975509151843258,
        "step": 15497
    },
    {
        "loss": 1.8498,
        "grad_norm": 3.3564517498016357,
        "learning_rate": 9.790433832568279e-09,
        "epoch": 1.9976798143851509,
        "step": 15498
    },
    {
        "loss": 1.393,
        "grad_norm": 2.0677318572998047,
        "learning_rate": 8.957611630444618e-09,
        "epoch": 1.9978087135859757,
        "step": 15499
    },
    {
        "loss": 1.102,
        "grad_norm": 3.134072780609131,
        "learning_rate": 8.161801588191243e-09,
        "epoch": 1.9979376127868007,
        "step": 15500
    },
    {
        "loss": 1.2386,
        "grad_norm": 1.7263426780700684,
        "learning_rate": 7.403004000372526e-09,
        "epoch": 1.9980665119876257,
        "step": 15501
    },
    {
        "loss": 0.922,
        "grad_norm": 2.6258747577667236,
        "learning_rate": 6.681219147874895e-09,
        "epoch": 1.9981954111884508,
        "step": 15502
    },
    {
        "loss": 0.8418,
        "grad_norm": 3.288539409637451,
        "learning_rate": 5.9964472978735195e-09,
        "epoch": 1.9983243103892756,
        "step": 15503
    },
    {
        "loss": 1.0721,
        "grad_norm": 2.2850937843322754,
        "learning_rate": 5.348688703821214e-09,
        "epoch": 1.9984532095901004,
        "step": 15504
    },
    {
        "loss": 1.312,
        "grad_norm": 1.9735386371612549,
        "learning_rate": 4.7379436055039475e-09,
        "epoch": 1.9985821087909255,
        "step": 15505
    },
    {
        "loss": 1.6123,
        "grad_norm": 2.684389591217041,
        "learning_rate": 4.164212228985332e-09,
        "epoch": 1.9987110079917505,
        "step": 15506
    },
    {
        "loss": 1.6337,
        "grad_norm": 2.629655599594116,
        "learning_rate": 3.627494786639929e-09,
        "epoch": 1.9988399071925755,
        "step": 15507
    },
    {
        "loss": 1.8964,
        "grad_norm": 2.813758134841919,
        "learning_rate": 3.127791477131048e-09,
        "epoch": 1.9989688063934004,
        "step": 15508
    },
    {
        "loss": 1.2828,
        "grad_norm": 3.3389744758605957,
        "learning_rate": 2.6651024854329464e-09,
        "epoch": 1.9990977055942252,
        "step": 15509
    },
    {
        "loss": 0.7321,
        "grad_norm": 1.2710504531860352,
        "learning_rate": 2.239427982797526e-09,
        "epoch": 1.9992266047950502,
        "step": 15510
    },
    {
        "loss": 1.218,
        "grad_norm": 1.5243319272994995,
        "learning_rate": 1.850768126809843e-09,
        "epoch": 1.9993555039958752,
        "step": 15511
    },
    {
        "loss": 0.8725,
        "grad_norm": 2.54935359954834,
        "learning_rate": 1.4991230613214945e-09,
        "epoch": 1.9994844031967003,
        "step": 15512
    },
    {
        "loss": 0.407,
        "grad_norm": 1.8898271322250366,
        "learning_rate": 1.184492916495028e-09,
        "epoch": 1.999613302397525,
        "step": 15513
    },
    {
        "loss": 1.7886,
        "grad_norm": 1.8107421398162842,
        "learning_rate": 9.068778087928387e-10,
        "epoch": 1.9997422015983501,
        "step": 15514
    },
    {
        "loss": 0.429,
        "grad_norm": 3.152364730834961,
        "learning_rate": 6.66277840988272e-10,
        "epoch": 1.999871100799175,
        "step": 15515
    },
    {
        "loss": 2.3944,
        "grad_norm": 1.7613261938095093,
        "learning_rate": 4.626931021212144e-10,
        "epoch": 2.0,
        "step": 15516
    },
    {
        "train_runtime": 42957.1564,
        "train_samples_per_second": 0.722,
        "train_steps_per_second": 0.361,
        "total_flos": 6.324153645264077e+17,
        "train_loss": 1.7567044096002722,
        "epoch": 2.0,
        "step": 15516
    }
]