[
    {
        "loss": 5.8554,
        "grad_norm": 5.165621757507324,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 7.452120128176467e-05,
        "step": 1
    },
    {
        "loss": 4.6342,
        "grad_norm": 3.690275192260742,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.00014904240256352933,
        "step": 2
    },
    {
        "loss": 5.0888,
        "grad_norm": 5.063528537750244,
        "learning_rate": 2.4e-05,
        "epoch": 0.000223563603845294,
        "step": 3
    },
    {
        "loss": 3.8742,
        "grad_norm": 2.6930196285247803,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.00029808480512705867,
        "step": 4
    },
    {
        "loss": 5.5528,
        "grad_norm": 5.387547969818115,
        "learning_rate": 4e-05,
        "epoch": 0.00037260600640882333,
        "step": 5
    },
    {
        "loss": 5.7968,
        "grad_norm": 5.0558576583862305,
        "learning_rate": 4.8e-05,
        "epoch": 0.000447127207690588,
        "step": 6
    },
    {
        "loss": 2.9227,
        "grad_norm": 2.5083084106445312,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0005216484089723526,
        "step": 7
    },
    {
        "loss": 3.6286,
        "grad_norm": 2.4683103561401367,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0005961696102541173,
        "step": 8
    },
    {
        "loss": 3.5255,
        "grad_norm": 2.2311911582946777,
        "learning_rate": 7.2e-05,
        "epoch": 0.0006706908115358819,
        "step": 9
    },
    {
        "loss": 3.8378,
        "grad_norm": 3.0918009281158447,
        "learning_rate": 8e-05,
        "epoch": 0.0007452120128176467,
        "step": 10
    },
    {
        "loss": 3.9501,
        "grad_norm": 4.743215084075928,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0008197332140994113,
        "step": 11
    },
    {
        "loss": 3.982,
        "grad_norm": 4.763010025024414,
        "learning_rate": 9.6e-05,
        "epoch": 0.000894254415381176,
        "step": 12
    },
    {
        "loss": 3.133,
        "grad_norm": 2.179236888885498,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0009687756166629406,
        "step": 13
    },
    {
        "loss": 3.9426,
        "grad_norm": 3.114621877670288,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0010432968179447052,
        "step": 14
    },
    {
        "loss": 3.5091,
        "grad_norm": Infinity,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0011178180192264698,
        "step": 15
    },
    {
        "loss": 4.1926,
        "grad_norm": 4.223032474517822,
        "learning_rate": 0.00012,
        "epoch": 0.0011923392205082347,
        "step": 16
    },
    {
        "loss": 4.2693,
        "grad_norm": 5.651657581329346,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0012668604217899993,
        "step": 17
    },
    {
        "loss": 3.3272,
        "grad_norm": 3.298103094100952,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.0013413816230717639,
        "step": 18
    },
    {
        "loss": 3.4887,
        "grad_norm": 5.606564044952393,
        "learning_rate": 0.000144,
        "epoch": 0.0014159028243535285,
        "step": 19
    },
    {
        "loss": 3.3695,
        "grad_norm": 5.481271266937256,
        "learning_rate": 0.000152,
        "epoch": 0.0014904240256352933,
        "step": 20
    },
    {
        "loss": 3.8919,
        "grad_norm": 5.824317455291748,
        "learning_rate": 0.00016,
        "epoch": 0.001564945226917058,
        "step": 21
    },
    {
        "loss": 3.4599,
        "grad_norm": 4.479311466217041,
        "learning_rate": 0.000168,
        "epoch": 0.0016394664281988226,
        "step": 22
    },
    {
        "loss": 3.8435,
        "grad_norm": 6.868091583251953,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.0017139876294805872,
        "step": 23
    },
    {
        "loss": 3.6805,
        "grad_norm": 4.352238655090332,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.001788508830762352,
        "step": 24
    },
    {
        "loss": 2.8325,
        "grad_norm": 6.878488063812256,
        "learning_rate": 0.000192,
        "epoch": 0.0018630300320441166,
        "step": 25
    },
    {
        "loss": 3.3211,
        "grad_norm": 3.020634412765503,
        "learning_rate": 0.0002,
        "epoch": 0.0019375512333258812,
        "step": 26
    },
    {
        "loss": 3.0998,
        "grad_norm": 4.87092399597168,
        "learning_rate": 0.0001999999752433628,
        "epoch": 0.002012072434607646,
        "step": 27
    },
    {
        "loss": 3.1862,
        "grad_norm": 5.73323917388916,
        "learning_rate": 0.00019999990097346353,
        "epoch": 0.0020865936358894104,
        "step": 28
    },
    {
        "loss": 2.806,
        "grad_norm": 4.116093158721924,
        "learning_rate": 0.00019999977719033888,
        "epoch": 0.0021611148371711753,
        "step": 29
    },
    {
        "loss": 2.9858,
        "grad_norm": 3.2577857971191406,
        "learning_rate": 0.00019999960389405018,
        "epoch": 0.0022356360384529397,
        "step": 30
    },
    {
        "loss": 3.2839,
        "grad_norm": 2.145172119140625,
        "learning_rate": 0.00019999938108468323,
        "epoch": 0.0023101572397347045,
        "step": 31
    },
    {
        "loss": 3.3082,
        "grad_norm": 4.286689281463623,
        "learning_rate": 0.00019999910876234835,
        "epoch": 0.0023846784410164693,
        "step": 32
    },
    {
        "loss": 3.2371,
        "grad_norm": 3.6814262866973877,
        "learning_rate": 0.0001999987869271804,
        "epoch": 0.0024591996422982337,
        "step": 33
    },
    {
        "loss": 3.3588,
        "grad_norm": 2.7952845096588135,
        "learning_rate": 0.00019999841557933866,
        "epoch": 0.0025337208435799985,
        "step": 34
    },
    {
        "loss": 3.1181,
        "grad_norm": 1.8077725172042847,
        "learning_rate": 0.0001999979947190071,
        "epoch": 0.0026082420448617634,
        "step": 35
    },
    {
        "loss": 2.5267,
        "grad_norm": 2.8344340324401855,
        "learning_rate": 0.00019999752434639404,
        "epoch": 0.0026827632461435278,
        "step": 36
    },
    {
        "loss": 2.9035,
        "grad_norm": 1.479363203048706,
        "learning_rate": 0.00019999700446173235,
        "epoch": 0.0027572844474252926,
        "step": 37
    },
    {
        "loss": 2.4039,
        "grad_norm": 3.4870247840881348,
        "learning_rate": 0.00019999643506527948,
        "epoch": 0.002831805648707057,
        "step": 38
    },
    {
        "loss": 3.2208,
        "grad_norm": 2.7665467262268066,
        "learning_rate": 0.00019999581615731736,
        "epoch": 0.002906326849988822,
        "step": 39
    },
    {
        "loss": 2.7775,
        "grad_norm": 3.9629170894622803,
        "learning_rate": 0.00019999514773815245,
        "epoch": 0.0029808480512705867,
        "step": 40
    },
    {
        "loss": 2.6937,
        "grad_norm": 3.1547393798828125,
        "learning_rate": 0.00019999442980811564,
        "epoch": 0.003055369252552351,
        "step": 41
    },
    {
        "loss": 3.2793,
        "grad_norm": 2.562375068664551,
        "learning_rate": 0.00019999366236756247,
        "epoch": 0.003129890453834116,
        "step": 42
    },
    {
        "loss": 2.4314,
        "grad_norm": 4.0912933349609375,
        "learning_rate": 0.0001999928454168729,
        "epoch": 0.0032044116551158803,
        "step": 43
    },
    {
        "loss": 2.5628,
        "grad_norm": 2.800647020339966,
        "learning_rate": 0.00019999197895645142,
        "epoch": 0.003278932856397645,
        "step": 44
    },
    {
        "loss": 2.1874,
        "grad_norm": 1.68551504611969,
        "learning_rate": 0.000199991062986727,
        "epoch": 0.00335345405767941,
        "step": 45
    },
    {
        "loss": 2.5279,
        "grad_norm": 2.5815043449401855,
        "learning_rate": 0.00019999009750815329,
        "epoch": 0.0034279752589611743,
        "step": 46
    },
    {
        "loss": 2.8305,
        "grad_norm": 2.330211639404297,
        "learning_rate": 0.0001999890825212082,
        "epoch": 0.003502496460242939,
        "step": 47
    },
    {
        "loss": 3.2803,
        "grad_norm": 4.310317039489746,
        "learning_rate": 0.00019998801802639435,
        "epoch": 0.003577017661524704,
        "step": 48
    },
    {
        "loss": 2.3999,
        "grad_norm": 3.7724833488464355,
        "learning_rate": 0.0001999869040242388,
        "epoch": 0.0036515388628064684,
        "step": 49
    },
    {
        "loss": 3.0162,
        "grad_norm": 1.999874234199524,
        "learning_rate": 0.00019998574051529312,
        "epoch": 0.003726060064088233,
        "step": 50
    },
    {
        "loss": 2.5388,
        "grad_norm": 2.3263556957244873,
        "learning_rate": 0.0001999845275001334,
        "epoch": 0.0038005812653699976,
        "step": 51
    },
    {
        "loss": 2.1851,
        "grad_norm": 4.897803783416748,
        "learning_rate": 0.00019998326497936027,
        "epoch": 0.0038751024666517624,
        "step": 52
    },
    {
        "loss": 2.4981,
        "grad_norm": 2.243694543838501,
        "learning_rate": 0.00019998195295359877,
        "epoch": 0.003949623667933527,
        "step": 53
    },
    {
        "loss": 2.5892,
        "grad_norm": 4.173165798187256,
        "learning_rate": 0.00019998059142349863,
        "epoch": 0.004024144869215292,
        "step": 54
    },
    {
        "loss": 2.811,
        "grad_norm": 1.8633149862289429,
        "learning_rate": 0.00019997918038973392,
        "epoch": 0.004098666070497056,
        "step": 55
    },
    {
        "loss": 2.8672,
        "grad_norm": 2.0564143657684326,
        "learning_rate": 0.00019997771985300332,
        "epoch": 0.004173187271778821,
        "step": 56
    },
    {
        "loss": 2.9154,
        "grad_norm": 1.889208436012268,
        "learning_rate": 0.00019997620981402996,
        "epoch": 0.004247708473060586,
        "step": 57
    },
    {
        "loss": 2.7031,
        "grad_norm": 2.3206515312194824,
        "learning_rate": 0.00019997465027356154,
        "epoch": 0.0043222296743423505,
        "step": 58
    },
    {
        "loss": 2.6294,
        "grad_norm": 2.110126256942749,
        "learning_rate": 0.0001999730412323702,
        "epoch": 0.004396750875624115,
        "step": 59
    },
    {
        "loss": 2.8672,
        "grad_norm": 3.159449577331543,
        "learning_rate": 0.0001999713826912527,
        "epoch": 0.004471272076905879,
        "step": 60
    },
    {
        "loss": 2.6268,
        "grad_norm": 3.0232439041137695,
        "learning_rate": 0.00019996967465103017,
        "epoch": 0.004545793278187644,
        "step": 61
    },
    {
        "loss": 2.7477,
        "grad_norm": 1.450063943862915,
        "learning_rate": 0.00019996791711254832,
        "epoch": 0.004620314479469409,
        "step": 62
    },
    {
        "loss": 2.9596,
        "grad_norm": 3.1872992515563965,
        "learning_rate": 0.00019996611007667742,
        "epoch": 0.004694835680751174,
        "step": 63
    },
    {
        "loss": 2.4822,
        "grad_norm": 2.6032657623291016,
        "learning_rate": 0.00019996425354431213,
        "epoch": 0.004769356882032939,
        "step": 64
    },
    {
        "loss": 3.1319,
        "grad_norm": 3.0404157638549805,
        "learning_rate": 0.00019996234751637173,
        "epoch": 0.004843878083314703,
        "step": 65
    },
    {
        "loss": 2.4384,
        "grad_norm": 3.2309916019439697,
        "learning_rate": 0.00019996039199379992,
        "epoch": 0.0049183992845964674,
        "step": 66
    },
    {
        "loss": 2.9672,
        "grad_norm": 3.8583927154541016,
        "learning_rate": 0.00019995838697756498,
        "epoch": 0.004992920485878232,
        "step": 67
    },
    {
        "loss": 2.2036,
        "grad_norm": 2.527509927749634,
        "learning_rate": 0.00019995633246865959,
        "epoch": 0.005067441687159997,
        "step": 68
    },
    {
        "loss": 2.181,
        "grad_norm": 3.422990560531616,
        "learning_rate": 0.0001999542284681011,
        "epoch": 0.005141962888441762,
        "step": 69
    },
    {
        "loss": 3.6873,
        "grad_norm": 5.12086296081543,
        "learning_rate": 0.00019995207497693117,
        "epoch": 0.005216484089723527,
        "step": 70
    },
    {
        "loss": 2.5088,
        "grad_norm": 2.4677228927612305,
        "learning_rate": 0.00019994987199621617,
        "epoch": 0.005291005291005291,
        "step": 71
    },
    {
        "loss": 2.9715,
        "grad_norm": 3.466505289077759,
        "learning_rate": 0.00019994761952704676,
        "epoch": 0.0053655264922870555,
        "step": 72
    },
    {
        "loss": 2.8628,
        "grad_norm": 1.9640272855758667,
        "learning_rate": 0.00019994531757053826,
        "epoch": 0.00544004769356882,
        "step": 73
    },
    {
        "loss": 2.0219,
        "grad_norm": 4.678342819213867,
        "learning_rate": 0.0001999429661278305,
        "epoch": 0.005514568894850585,
        "step": 74
    },
    {
        "loss": 1.8936,
        "grad_norm": 4.0941925048828125,
        "learning_rate": 0.00019994056520008767,
        "epoch": 0.00558909009613235,
        "step": 75
    },
    {
        "loss": 2.8598,
        "grad_norm": 3.2886147499084473,
        "learning_rate": 0.0001999381147884986,
        "epoch": 0.005663611297414114,
        "step": 76
    },
    {
        "loss": 3.0123,
        "grad_norm": 2.852648973464966,
        "learning_rate": 0.00019993561489427654,
        "epoch": 0.005738132498695879,
        "step": 77
    },
    {
        "loss": 1.9138,
        "grad_norm": 3.9141266345977783,
        "learning_rate": 0.0001999330655186593,
        "epoch": 0.005812653699977644,
        "step": 78
    },
    {
        "loss": 2.6202,
        "grad_norm": 2.357208251953125,
        "learning_rate": 0.0001999304666629091,
        "epoch": 0.0058871749012594085,
        "step": 79
    },
    {
        "loss": 3.6422,
        "grad_norm": 1.9507923126220703,
        "learning_rate": 0.00019992781832831277,
        "epoch": 0.005961696102541173,
        "step": 80
    },
    {
        "loss": 2.8958,
        "grad_norm": 2.502276659011841,
        "learning_rate": 0.0001999251205161816,
        "epoch": 0.006036217303822937,
        "step": 81
    },
    {
        "loss": 2.8122,
        "grad_norm": 2.410005569458008,
        "learning_rate": 0.00019992237322785132,
        "epoch": 0.006110738505104702,
        "step": 82
    },
    {
        "loss": 2.6354,
        "grad_norm": 1.8346360921859741,
        "learning_rate": 0.00019991957646468226,
        "epoch": 0.006185259706386467,
        "step": 83
    },
    {
        "loss": 3.0441,
        "grad_norm": 2.5124130249023438,
        "learning_rate": 0.00019991673022805913,
        "epoch": 0.006259780907668232,
        "step": 84
    },
    {
        "loss": 2.6756,
        "grad_norm": 2.8164172172546387,
        "learning_rate": 0.0001999138345193912,
        "epoch": 0.006334302108949997,
        "step": 85
    },
    {
        "loss": 2.0132,
        "grad_norm": 4.2282562255859375,
        "learning_rate": 0.00019991088934011227,
        "epoch": 0.0064088233102317605,
        "step": 86
    },
    {
        "loss": 2.3341,
        "grad_norm": 2.14048171043396,
        "learning_rate": 0.00019990789469168053,
        "epoch": 0.006483344511513525,
        "step": 87
    },
    {
        "loss": 2.5724,
        "grad_norm": 2.7170915603637695,
        "learning_rate": 0.00019990485057557882,
        "epoch": 0.00655786571279529,
        "step": 88
    },
    {
        "loss": 2.3889,
        "grad_norm": 2.7773799896240234,
        "learning_rate": 0.0001999017569933143,
        "epoch": 0.006632386914077055,
        "step": 89
    },
    {
        "loss": 3.0973,
        "grad_norm": 3.5397064685821533,
        "learning_rate": 0.00019989861394641874,
        "epoch": 0.00670690811535882,
        "step": 90
    },
    {
        "loss": 3.2988,
        "grad_norm": 2.9224753379821777,
        "learning_rate": 0.00019989542143644838,
        "epoch": 0.006781429316640584,
        "step": 91
    },
    {
        "loss": 2.1846,
        "grad_norm": 3.3446593284606934,
        "learning_rate": 0.0001998921794649839,
        "epoch": 0.006855950517922349,
        "step": 92
    },
    {
        "loss": 2.6509,
        "grad_norm": 3.3090853691101074,
        "learning_rate": 0.00019988888803363053,
        "epoch": 0.0069304717192041135,
        "step": 93
    },
    {
        "loss": 2.6431,
        "grad_norm": 3.196514129638672,
        "learning_rate": 0.00019988554714401796,
        "epoch": 0.007004992920485878,
        "step": 94
    },
    {
        "loss": 2.4568,
        "grad_norm": 2.9485628604888916,
        "learning_rate": 0.00019988215679780038,
        "epoch": 0.007079514121767643,
        "step": 95
    },
    {
        "loss": 2.4713,
        "grad_norm": 3.048536777496338,
        "learning_rate": 0.00019987871699665645,
        "epoch": 0.007154035323049408,
        "step": 96
    },
    {
        "loss": 2.8438,
        "grad_norm": 3.2346835136413574,
        "learning_rate": 0.00019987522774228932,
        "epoch": 0.007228556524331172,
        "step": 97
    },
    {
        "loss": 2.9342,
        "grad_norm": 2.8785619735717773,
        "learning_rate": 0.00019987168903642667,
        "epoch": 0.007303077725612937,
        "step": 98
    },
    {
        "loss": 2.9215,
        "grad_norm": 2.7519521713256836,
        "learning_rate": 0.00019986810088082056,
        "epoch": 0.007377598926894702,
        "step": 99
    },
    {
        "loss": 2.628,
        "grad_norm": 3.3609726428985596,
        "learning_rate": 0.0001998644632772477,
        "epoch": 0.007452120128176466,
        "step": 100
    },
    {
        "loss": 1.9901,
        "grad_norm": 2.6927661895751953,
        "learning_rate": 0.0001998607762275091,
        "epoch": 0.007526641329458231,
        "step": 101
    },
    {
        "loss": 1.4004,
        "grad_norm": 4.601839065551758,
        "learning_rate": 0.00019985703973343042,
        "epoch": 0.007601162530739995,
        "step": 102
    },
    {
        "loss": 3.1793,
        "grad_norm": 2.7589895725250244,
        "learning_rate": 0.00019985325379686165,
        "epoch": 0.00767568373202176,
        "step": 103
    },
    {
        "loss": 1.8294,
        "grad_norm": 6.161064147949219,
        "learning_rate": 0.00019984941841967734,
        "epoch": 0.007750204933303525,
        "step": 104
    },
    {
        "loss": 2.3264,
        "grad_norm": 2.5897936820983887,
        "learning_rate": 0.00019984553360377657,
        "epoch": 0.00782472613458529,
        "step": 105
    },
    {
        "loss": 2.6031,
        "grad_norm": 2.092285633087158,
        "learning_rate": 0.00019984159935108276,
        "epoch": 0.007899247335867055,
        "step": 106
    },
    {
        "loss": 2.5173,
        "grad_norm": 2.61429762840271,
        "learning_rate": 0.00019983761566354393,
        "epoch": 0.00797376853714882,
        "step": 107
    },
    {
        "loss": 2.4944,
        "grad_norm": 2.8753418922424316,
        "learning_rate": 0.0001998335825431325,
        "epoch": 0.008048289738430584,
        "step": 108
    },
    {
        "loss": 2.4216,
        "grad_norm": 2.4138290882110596,
        "learning_rate": 0.00019982949999184548,
        "epoch": 0.008122810939712349,
        "step": 109
    },
    {
        "loss": 2.6844,
        "grad_norm": 3.006235361099243,
        "learning_rate": 0.0001998253680117042,
        "epoch": 0.008197332140994112,
        "step": 110
    },
    {
        "loss": 2.71,
        "grad_norm": 2.1795294284820557,
        "learning_rate": 0.00019982118660475458,
        "epoch": 0.008271853342275877,
        "step": 111
    },
    {
        "loss": 2.0899,
        "grad_norm": 3.6732370853424072,
        "learning_rate": 0.0001998169557730669,
        "epoch": 0.008346374543557642,
        "step": 112
    },
    {
        "loss": 2.6662,
        "grad_norm": 2.483614444732666,
        "learning_rate": 0.0001998126755187361,
        "epoch": 0.008420895744839407,
        "step": 113
    },
    {
        "loss": 2.075,
        "grad_norm": 3.986807107925415,
        "learning_rate": 0.0001998083458438814,
        "epoch": 0.008495416946121171,
        "step": 114
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.8978047370910645,
        "learning_rate": 0.00019980396675064657,
        "epoch": 0.008569938147402936,
        "step": 115
    },
    {
        "loss": 2.2101,
        "grad_norm": 3.27756404876709,
        "learning_rate": 0.0001997995382411998,
        "epoch": 0.008644459348684701,
        "step": 116
    },
    {
        "loss": 2.1118,
        "grad_norm": 3.181647777557373,
        "learning_rate": 0.0001997950603177339,
        "epoch": 0.008718980549966466,
        "step": 117
    },
    {
        "loss": 2.5478,
        "grad_norm": 3.516711950302124,
        "learning_rate": 0.00019979053298246595,
        "epoch": 0.00879350175124823,
        "step": 118
    },
    {
        "loss": 2.7739,
        "grad_norm": 3.638066291809082,
        "learning_rate": 0.00019978595623763761,
        "epoch": 0.008868022952529996,
        "step": 119
    },
    {
        "loss": 2.2443,
        "grad_norm": 3.554185152053833,
        "learning_rate": 0.00019978133008551497,
        "epoch": 0.008942544153811759,
        "step": 120
    },
    {
        "loss": 2.938,
        "grad_norm": 2.802473545074463,
        "learning_rate": 0.00019977665452838855,
        "epoch": 0.009017065355093523,
        "step": 121
    },
    {
        "loss": 2.5716,
        "grad_norm": 2.157296895980835,
        "learning_rate": 0.00019977192956857346,
        "epoch": 0.009091586556375288,
        "step": 122
    },
    {
        "loss": 2.923,
        "grad_norm": 2.4182472229003906,
        "learning_rate": 0.00019976715520840914,
        "epoch": 0.009166107757657053,
        "step": 123
    },
    {
        "loss": 2.9728,
        "grad_norm": 2.611222267150879,
        "learning_rate": 0.00019976233145025952,
        "epoch": 0.009240628958938818,
        "step": 124
    },
    {
        "loss": 3.0636,
        "grad_norm": 2.3772597312927246,
        "learning_rate": 0.00019975745829651297,
        "epoch": 0.009315150160220583,
        "step": 125
    },
    {
        "loss": 3.0808,
        "grad_norm": 1.7471210956573486,
        "learning_rate": 0.00019975253574958243,
        "epoch": 0.009389671361502348,
        "step": 126
    },
    {
        "loss": 2.8751,
        "grad_norm": 2.341752290725708,
        "learning_rate": 0.00019974756381190516,
        "epoch": 0.009464192562784112,
        "step": 127
    },
    {
        "loss": 3.2341,
        "grad_norm": 2.6172895431518555,
        "learning_rate": 0.00019974254248594293,
        "epoch": 0.009538713764065877,
        "step": 128
    },
    {
        "loss": 2.5762,
        "grad_norm": 3.2972662448883057,
        "learning_rate": 0.000199737471774182,
        "epoch": 0.009613234965347642,
        "step": 129
    },
    {
        "loss": 2.4694,
        "grad_norm": 2.697244644165039,
        "learning_rate": 0.000199732351679133,
        "epoch": 0.009687756166629405,
        "step": 130
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.763864755630493,
        "learning_rate": 0.00019972718220333103,
        "epoch": 0.00976227736791117,
        "step": 131
    },
    {
        "loss": 2.2424,
        "grad_norm": 2.988482713699341,
        "learning_rate": 0.00019972196334933578,
        "epoch": 0.009836798569192935,
        "step": 132
    },
    {
        "loss": 2.9127,
        "grad_norm": 1.965844988822937,
        "learning_rate": 0.00019971669511973115,
        "epoch": 0.0099113197704747,
        "step": 133
    },
    {
        "loss": 1.9531,
        "grad_norm": 3.1701059341430664,
        "learning_rate": 0.0001997113775171257,
        "epoch": 0.009985840971756465,
        "step": 134
    },
    {
        "loss": 2.2374,
        "grad_norm": 3.195579767227173,
        "learning_rate": 0.0001997060105441523,
        "epoch": 0.01006036217303823,
        "step": 135
    },
    {
        "loss": 2.3991,
        "grad_norm": 2.204901695251465,
        "learning_rate": 0.00019970059420346833,
        "epoch": 0.010134883374319994,
        "step": 136
    },
    {
        "loss": 2.6567,
        "grad_norm": 2.656548261642456,
        "learning_rate": 0.00019969512849775565,
        "epoch": 0.010209404575601759,
        "step": 137
    },
    {
        "loss": 2.7746,
        "grad_norm": 2.261226177215576,
        "learning_rate": 0.00019968961342972042,
        "epoch": 0.010283925776883524,
        "step": 138
    },
    {
        "loss": 1.8252,
        "grad_norm": 2.6737425327301025,
        "learning_rate": 0.00019968404900209337,
        "epoch": 0.010358446978165289,
        "step": 139
    },
    {
        "loss": 2.7831,
        "grad_norm": 2.126857280731201,
        "learning_rate": 0.0001996784352176296,
        "epoch": 0.010432968179447054,
        "step": 140
    },
    {
        "loss": 3.1112,
        "grad_norm": 2.0964531898498535,
        "learning_rate": 0.00019967277207910878,
        "epoch": 0.010507489380728817,
        "step": 141
    },
    {
        "loss": 2.9996,
        "grad_norm": 2.225374698638916,
        "learning_rate": 0.0001996670595893348,
        "epoch": 0.010582010582010581,
        "step": 142
    },
    {
        "loss": 3.1651,
        "grad_norm": 2.504026174545288,
        "learning_rate": 0.00019966129775113615,
        "epoch": 0.010656531783292346,
        "step": 143
    },
    {
        "loss": 2.8153,
        "grad_norm": 3.5836095809936523,
        "learning_rate": 0.0001996554865673657,
        "epoch": 0.010731052984574111,
        "step": 144
    },
    {
        "loss": 2.8673,
        "grad_norm": 2.750792980194092,
        "learning_rate": 0.00019964962604090077,
        "epoch": 0.010805574185855876,
        "step": 145
    },
    {
        "loss": 2.5988,
        "grad_norm": 3.2085447311401367,
        "learning_rate": 0.00019964371617464308,
        "epoch": 0.01088009538713764,
        "step": 146
    },
    {
        "loss": 2.9755,
        "grad_norm": 3.2078652381896973,
        "learning_rate": 0.0001996377569715188,
        "epoch": 0.010954616588419406,
        "step": 147
    },
    {
        "loss": 2.5625,
        "grad_norm": 2.8336126804351807,
        "learning_rate": 0.00019963174843447854,
        "epoch": 0.01102913778970117,
        "step": 148
    },
    {
        "loss": 2.3236,
        "grad_norm": 2.076235294342041,
        "learning_rate": 0.0001996256905664973,
        "epoch": 0.011103658990982935,
        "step": 149
    },
    {
        "loss": 2.7237,
        "grad_norm": 2.2501914501190186,
        "learning_rate": 0.00019961958337057456,
        "epoch": 0.0111781801922647,
        "step": 150
    },
    {
        "loss": 1.7284,
        "grad_norm": 4.18800687789917,
        "learning_rate": 0.00019961342684973418,
        "epoch": 0.011252701393546463,
        "step": 151
    },
    {
        "loss": 2.6622,
        "grad_norm": 2.8631627559661865,
        "learning_rate": 0.00019960722100702442,
        "epoch": 0.011327222594828228,
        "step": 152
    },
    {
        "loss": 2.1663,
        "grad_norm": 3.0235869884490967,
        "learning_rate": 0.00019960096584551807,
        "epoch": 0.011401743796109993,
        "step": 153
    },
    {
        "loss": 2.5713,
        "grad_norm": 4.0992350578308105,
        "learning_rate": 0.00019959466136831218,
        "epoch": 0.011476264997391758,
        "step": 154
    },
    {
        "loss": 2.7469,
        "grad_norm": 3.8359780311584473,
        "learning_rate": 0.00019958830757852835,
        "epoch": 0.011550786198673522,
        "step": 155
    },
    {
        "loss": 2.1892,
        "grad_norm": 3.128485679626465,
        "learning_rate": 0.00019958190447931257,
        "epoch": 0.011625307399955287,
        "step": 156
    },
    {
        "loss": 2.4653,
        "grad_norm": 2.2594680786132812,
        "learning_rate": 0.0001995754520738352,
        "epoch": 0.011699828601237052,
        "step": 157
    },
    {
        "loss": 2.2837,
        "grad_norm": 3.7327840328216553,
        "learning_rate": 0.00019956895036529103,
        "epoch": 0.011774349802518817,
        "step": 158
    },
    {
        "loss": 2.8861,
        "grad_norm": 2.7804393768310547,
        "learning_rate": 0.00019956239935689925,
        "epoch": 0.011848871003800582,
        "step": 159
    },
    {
        "loss": 2.026,
        "grad_norm": 2.1713414192199707,
        "learning_rate": 0.00019955579905190352,
        "epoch": 0.011923392205082347,
        "step": 160
    },
    {
        "loss": 2.6002,
        "grad_norm": 2.39854097366333,
        "learning_rate": 0.00019954914945357187,
        "epoch": 0.011997913406364111,
        "step": 161
    },
    {
        "loss": 2.4797,
        "grad_norm": 2.8900585174560547,
        "learning_rate": 0.00019954245056519668,
        "epoch": 0.012072434607645875,
        "step": 162
    },
    {
        "loss": 2.2666,
        "grad_norm": 2.4987053871154785,
        "learning_rate": 0.00019953570239009482,
        "epoch": 0.01214695580892764,
        "step": 163
    },
    {
        "loss": 2.6126,
        "grad_norm": 2.6741647720336914,
        "learning_rate": 0.00019952890493160757,
        "epoch": 0.012221477010209404,
        "step": 164
    },
    {
        "loss": 3.1653,
        "grad_norm": 3.315638303756714,
        "learning_rate": 0.00019952205819310053,
        "epoch": 0.012295998211491169,
        "step": 165
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.6156530380249023,
        "learning_rate": 0.00019951516217796376,
        "epoch": 0.012370519412772934,
        "step": 166
    },
    {
        "loss": 2.864,
        "grad_norm": 3.5935921669006348,
        "learning_rate": 0.00019950821688961167,
        "epoch": 0.012445040614054699,
        "step": 167
    },
    {
        "loss": 2.1649,
        "grad_norm": 2.4047296047210693,
        "learning_rate": 0.00019950122233148315,
        "epoch": 0.012519561815336464,
        "step": 168
    },
    {
        "loss": 2.4027,
        "grad_norm": 3.916794538497925,
        "learning_rate": 0.0001994941785070414,
        "epoch": 0.012594083016618228,
        "step": 169
    },
    {
        "loss": 2.6592,
        "grad_norm": 2.2930519580841064,
        "learning_rate": 0.00019948708541977407,
        "epoch": 0.012668604217899993,
        "step": 170
    },
    {
        "loss": 3.0253,
        "grad_norm": 1.7043051719665527,
        "learning_rate": 0.00019947994307319315,
        "epoch": 0.012743125419181758,
        "step": 171
    },
    {
        "loss": 1.6805,
        "grad_norm": 4.051605701446533,
        "learning_rate": 0.0001994727514708351,
        "epoch": 0.012817646620463521,
        "step": 172
    },
    {
        "loss": 3.2902,
        "grad_norm": 2.1270594596862793,
        "learning_rate": 0.00019946551061626066,
        "epoch": 0.012892167821745286,
        "step": 173
    },
    {
        "loss": 2.6776,
        "grad_norm": 1.9066288471221924,
        "learning_rate": 0.00019945822051305507,
        "epoch": 0.01296668902302705,
        "step": 174
    },
    {
        "loss": 2.478,
        "grad_norm": 3.806946039199829,
        "learning_rate": 0.00019945088116482787,
        "epoch": 0.013041210224308816,
        "step": 175
    },
    {
        "loss": 2.8721,
        "grad_norm": 2.9675023555755615,
        "learning_rate": 0.00019944349257521298,
        "epoch": 0.01311573142559058,
        "step": 176
    },
    {
        "loss": 3.225,
        "grad_norm": 1.598202109336853,
        "learning_rate": 0.00019943605474786877,
        "epoch": 0.013190252626872345,
        "step": 177
    },
    {
        "loss": 2.5405,
        "grad_norm": 2.654750347137451,
        "learning_rate": 0.000199428567686478,
        "epoch": 0.01326477382815411,
        "step": 178
    },
    {
        "loss": 2.9542,
        "grad_norm": 1.8927297592163086,
        "learning_rate": 0.00019942103139474764,
        "epoch": 0.013339295029435875,
        "step": 179
    },
    {
        "loss": 3.2238,
        "grad_norm": 2.504149913787842,
        "learning_rate": 0.00019941344587640924,
        "epoch": 0.01341381623071764,
        "step": 180
    },
    {
        "loss": 2.7444,
        "grad_norm": 1.6999701261520386,
        "learning_rate": 0.0001994058111352186,
        "epoch": 0.013488337431999405,
        "step": 181
    },
    {
        "loss": 2.3456,
        "grad_norm": 2.3652050495147705,
        "learning_rate": 0.000199398127174956,
        "epoch": 0.013562858633281168,
        "step": 182
    },
    {
        "loss": 2.7121,
        "grad_norm": 2.1367297172546387,
        "learning_rate": 0.00019939039399942594,
        "epoch": 0.013637379834562932,
        "step": 183
    },
    {
        "loss": 2.0619,
        "grad_norm": 3.067194700241089,
        "learning_rate": 0.00019938261161245739,
        "epoch": 0.013711901035844697,
        "step": 184
    },
    {
        "loss": 2.0083,
        "grad_norm": 2.5429584980010986,
        "learning_rate": 0.0001993747800179037,
        "epoch": 0.013786422237126462,
        "step": 185
    },
    {
        "loss": 2.5972,
        "grad_norm": 3.4304988384246826,
        "learning_rate": 0.00019936689921964252,
        "epoch": 0.013860943438408227,
        "step": 186
    },
    {
        "loss": 2.3572,
        "grad_norm": 3.6743850708007812,
        "learning_rate": 0.00019935896922157587,
        "epoch": 0.013935464639689992,
        "step": 187
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.3986916542053223,
        "learning_rate": 0.00019935099002763016,
        "epoch": 0.014009985840971757,
        "step": 188
    },
    {
        "loss": 2.3543,
        "grad_norm": 2.9627997875213623,
        "learning_rate": 0.0001993429616417562,
        "epoch": 0.014084507042253521,
        "step": 189
    },
    {
        "loss": 2.5791,
        "grad_norm": 2.356072425842285,
        "learning_rate": 0.00019933488406792907,
        "epoch": 0.014159028243535286,
        "step": 190
    },
    {
        "loss": 1.6322,
        "grad_norm": 2.8147194385528564,
        "learning_rate": 0.00019932675731014824,
        "epoch": 0.014233549444817051,
        "step": 191
    },
    {
        "loss": 2.3531,
        "grad_norm": 2.841278076171875,
        "learning_rate": 0.0001993185813724375,
        "epoch": 0.014308070646098816,
        "step": 192
    },
    {
        "loss": 2.9697,
        "grad_norm": 2.210848093032837,
        "learning_rate": 0.0001993103562588451,
        "epoch": 0.014382591847380579,
        "step": 193
    },
    {
        "loss": 1.7335,
        "grad_norm": 3.734687089920044,
        "learning_rate": 0.00019930208197344352,
        "epoch": 0.014457113048662344,
        "step": 194
    },
    {
        "loss": 2.5432,
        "grad_norm": 2.0653188228607178,
        "learning_rate": 0.0001992937585203296,
        "epoch": 0.014531634249944109,
        "step": 195
    },
    {
        "loss": 2.4477,
        "grad_norm": 3.6742494106292725,
        "learning_rate": 0.00019928538590362465,
        "epoch": 0.014606155451225874,
        "step": 196
    },
    {
        "loss": 1.9177,
        "grad_norm": 2.3871142864227295,
        "learning_rate": 0.0001992769641274741,
        "epoch": 0.014680676652507638,
        "step": 197
    },
    {
        "loss": 1.9202,
        "grad_norm": 4.1197052001953125,
        "learning_rate": 0.00019926849319604797,
        "epoch": 0.014755197853789403,
        "step": 198
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.1302831172943115,
        "learning_rate": 0.0001992599731135404,
        "epoch": 0.014829719055071168,
        "step": 199
    },
    {
        "loss": 2.3874,
        "grad_norm": 3.2110230922698975,
        "learning_rate": 0.00019925140388417,
        "epoch": 0.014904240256352933,
        "step": 200
    },
    {
        "loss": 3.1396,
        "grad_norm": 4.298569679260254,
        "learning_rate": 0.0001992427855121797,
        "epoch": 0.014978761457634698,
        "step": 201
    },
    {
        "loss": 2.6583,
        "grad_norm": 2.746190309524536,
        "learning_rate": 0.0001992341180018367,
        "epoch": 0.015053282658916462,
        "step": 202
    },
    {
        "loss": 1.924,
        "grad_norm": 3.3172643184661865,
        "learning_rate": 0.00019922540135743256,
        "epoch": 0.015127803860198226,
        "step": 203
    },
    {
        "loss": 2.5815,
        "grad_norm": 1.8903546333312988,
        "learning_rate": 0.00019921663558328324,
        "epoch": 0.01520232506147999,
        "step": 204
    },
    {
        "loss": 2.3506,
        "grad_norm": 2.8747975826263428,
        "learning_rate": 0.0001992078206837289,
        "epoch": 0.015276846262761755,
        "step": 205
    },
    {
        "loss": 2.82,
        "grad_norm": 2.6219825744628906,
        "learning_rate": 0.00019919895666313409,
        "epoch": 0.01535136746404352,
        "step": 206
    },
    {
        "loss": 2.6111,
        "grad_norm": 3.1335511207580566,
        "learning_rate": 0.00019919004352588767,
        "epoch": 0.015425888665325285,
        "step": 207
    },
    {
        "loss": 2.5301,
        "grad_norm": 3.149826765060425,
        "learning_rate": 0.00019918108127640291,
        "epoch": 0.01550040986660705,
        "step": 208
    },
    {
        "loss": 2.9006,
        "grad_norm": 2.7168800830841064,
        "learning_rate": 0.00019917206991911722,
        "epoch": 0.015574931067888815,
        "step": 209
    },
    {
        "loss": 2.2668,
        "grad_norm": 3.796434164047241,
        "learning_rate": 0.00019916300945849248,
        "epoch": 0.01564945226917058,
        "step": 210
    },
    {
        "loss": 2.8287,
        "grad_norm": 2.77836537361145,
        "learning_rate": 0.00019915389989901474,
        "epoch": 0.015723973470452342,
        "step": 211
    },
    {
        "loss": 2.3267,
        "grad_norm": 2.4728689193725586,
        "learning_rate": 0.0001991447412451945,
        "epoch": 0.01579849467173411,
        "step": 212
    },
    {
        "loss": 2.6843,
        "grad_norm": 1.9313899278640747,
        "learning_rate": 0.00019913553350156652,
        "epoch": 0.015873015873015872,
        "step": 213
    },
    {
        "loss": 2.3002,
        "grad_norm": 3.3642544746398926,
        "learning_rate": 0.00019912627667268982,
        "epoch": 0.01594753707429764,
        "step": 214
    },
    {
        "loss": 2.8088,
        "grad_norm": 2.9049859046936035,
        "learning_rate": 0.00019911697076314776,
        "epoch": 0.016022058275579402,
        "step": 215
    },
    {
        "loss": 1.755,
        "grad_norm": 1.5536061525344849,
        "learning_rate": 0.00019910761577754805,
        "epoch": 0.01609657947686117,
        "step": 216
    },
    {
        "loss": 1.7261,
        "grad_norm": 3.2542471885681152,
        "learning_rate": 0.0001990982117205226,
        "epoch": 0.01617110067814293,
        "step": 217
    },
    {
        "loss": 2.7586,
        "grad_norm": 2.838357448577881,
        "learning_rate": 0.0001990887585967277,
        "epoch": 0.016245621879424698,
        "step": 218
    },
    {
        "loss": 3.0414,
        "grad_norm": 2.3867578506469727,
        "learning_rate": 0.00019907925641084387,
        "epoch": 0.01632014308070646,
        "step": 219
    },
    {
        "loss": 2.7244,
        "grad_norm": 2.497694492340088,
        "learning_rate": 0.00019906970516757596,
        "epoch": 0.016394664281988224,
        "step": 220
    },
    {
        "loss": 2.7411,
        "grad_norm": 2.0542540550231934,
        "learning_rate": 0.00019906010487165312,
        "epoch": 0.01646918548326999,
        "step": 221
    },
    {
        "loss": 2.6122,
        "grad_norm": 2.1560239791870117,
        "learning_rate": 0.00019905045552782874,
        "epoch": 0.016543706684551754,
        "step": 222
    },
    {
        "loss": 2.1508,
        "grad_norm": 2.7408015727996826,
        "learning_rate": 0.00019904075714088056,
        "epoch": 0.01661822788583352,
        "step": 223
    },
    {
        "loss": 2.2303,
        "grad_norm": 4.401638031005859,
        "learning_rate": 0.00019903100971561054,
        "epoch": 0.016692749087115284,
        "step": 224
    },
    {
        "loss": 2.5474,
        "grad_norm": 3.9099831581115723,
        "learning_rate": 0.00019902121325684498,
        "epoch": 0.01676727028839705,
        "step": 225
    },
    {
        "loss": 1.1805,
        "grad_norm": 3.066164970397949,
        "learning_rate": 0.00019901136776943442,
        "epoch": 0.016841791489678813,
        "step": 226
    },
    {
        "loss": 2.5609,
        "grad_norm": 2.6868910789489746,
        "learning_rate": 0.00019900147325825366,
        "epoch": 0.01691631269096058,
        "step": 227
    },
    {
        "loss": 3.0033,
        "grad_norm": 2.356229543685913,
        "learning_rate": 0.00019899152972820182,
        "epoch": 0.016990833892242343,
        "step": 228
    },
    {
        "loss": 2.6338,
        "grad_norm": 2.1313657760620117,
        "learning_rate": 0.00019898153718420227,
        "epoch": 0.017065355093524106,
        "step": 229
    },
    {
        "loss": 2.705,
        "grad_norm": 3.115950345993042,
        "learning_rate": 0.0001989714956312026,
        "epoch": 0.017139876294805872,
        "step": 230
    },
    {
        "loss": 2.394,
        "grad_norm": 2.5375711917877197,
        "learning_rate": 0.00019896140507417477,
        "epoch": 0.017214397496087636,
        "step": 231
    },
    {
        "loss": 2.5539,
        "grad_norm": 3.2954764366149902,
        "learning_rate": 0.00019895126551811493,
        "epoch": 0.017288918697369402,
        "step": 232
    },
    {
        "loss": 1.7216,
        "grad_norm": 4.529842853546143,
        "learning_rate": 0.0001989410769680435,
        "epoch": 0.017363439898651165,
        "step": 233
    },
    {
        "loss": 2.5845,
        "grad_norm": 2.8039402961730957,
        "learning_rate": 0.00019893083942900511,
        "epoch": 0.017437961099932932,
        "step": 234
    },
    {
        "loss": 3.124,
        "grad_norm": 2.352071523666382,
        "learning_rate": 0.0001989205529060688,
        "epoch": 0.017512482301214695,
        "step": 235
    },
    {
        "loss": 2.9386,
        "grad_norm": 2.684513807296753,
        "learning_rate": 0.00019891021740432773,
        "epoch": 0.01758700350249646,
        "step": 236
    },
    {
        "loss": 1.8265,
        "grad_norm": 2.2370028495788574,
        "learning_rate": 0.00019889983292889932,
        "epoch": 0.017661524703778225,
        "step": 237
    },
    {
        "loss": 2.5045,
        "grad_norm": 3.4477109909057617,
        "learning_rate": 0.00019888939948492526,
        "epoch": 0.01773604590505999,
        "step": 238
    },
    {
        "loss": 2.6903,
        "grad_norm": 2.169445037841797,
        "learning_rate": 0.0001988789170775715,
        "epoch": 0.017810567106341754,
        "step": 239
    },
    {
        "loss": 2.789,
        "grad_norm": 2.3535523414611816,
        "learning_rate": 0.00019886838571202828,
        "epoch": 0.017885088307623517,
        "step": 240
    },
    {
        "loss": 2.6862,
        "grad_norm": 2.384664535522461,
        "learning_rate": 0.00019885780539350992,
        "epoch": 0.017959609508905284,
        "step": 241
    },
    {
        "loss": 2.5519,
        "grad_norm": 4.00353479385376,
        "learning_rate": 0.00019884717612725516,
        "epoch": 0.018034130710187047,
        "step": 242
    },
    {
        "loss": 2.911,
        "grad_norm": 2.232844352722168,
        "learning_rate": 0.00019883649791852686,
        "epoch": 0.018108651911468814,
        "step": 243
    },
    {
        "loss": 2.1102,
        "grad_norm": 2.701688766479492,
        "learning_rate": 0.0001988257707726122,
        "epoch": 0.018183173112750577,
        "step": 244
    },
    {
        "loss": 2.4175,
        "grad_norm": 1.995369791984558,
        "learning_rate": 0.00019881499469482248,
        "epoch": 0.018257694314032343,
        "step": 245
    },
    {
        "loss": 2.2047,
        "grad_norm": 2.090271234512329,
        "learning_rate": 0.0001988041696904933,
        "epoch": 0.018332215515314106,
        "step": 246
    },
    {
        "loss": 2.9274,
        "grad_norm": 3.112405300140381,
        "learning_rate": 0.0001987932957649845,
        "epoch": 0.018406736716595873,
        "step": 247
    },
    {
        "loss": 3.0061,
        "grad_norm": 2.9306797981262207,
        "learning_rate": 0.00019878237292368013,
        "epoch": 0.018481257917877636,
        "step": 248
    },
    {
        "loss": 3.2371,
        "grad_norm": 1.8907252550125122,
        "learning_rate": 0.00019877140117198838,
        "epoch": 0.018555779119159403,
        "step": 249
    },
    {
        "loss": 2.0477,
        "grad_norm": 3.603347063064575,
        "learning_rate": 0.0001987603805153418,
        "epoch": 0.018630300320441166,
        "step": 250
    },
    {
        "loss": 2.8627,
        "grad_norm": 1.9533859491348267,
        "learning_rate": 0.00019874931095919705,
        "epoch": 0.01870482152172293,
        "step": 251
    },
    {
        "loss": 2.2675,
        "grad_norm": 2.353933572769165,
        "learning_rate": 0.00019873819250903498,
        "epoch": 0.018779342723004695,
        "step": 252
    },
    {
        "loss": 2.5789,
        "grad_norm": 3.557101249694824,
        "learning_rate": 0.00019872702517036075,
        "epoch": 0.01885386392428646,
        "step": 253
    },
    {
        "loss": 2.4567,
        "grad_norm": 3.5745275020599365,
        "learning_rate": 0.0001987158089487037,
        "epoch": 0.018928385125568225,
        "step": 254
    },
    {
        "loss": 2.7066,
        "grad_norm": 2.251016616821289,
        "learning_rate": 0.0001987045438496173,
        "epoch": 0.019002906326849988,
        "step": 255
    },
    {
        "loss": 3.1024,
        "grad_norm": 2.3827412128448486,
        "learning_rate": 0.00019869322987867927,
        "epoch": 0.019077427528131755,
        "step": 256
    },
    {
        "loss": 2.3289,
        "grad_norm": 3.25012469291687,
        "learning_rate": 0.00019868186704149156,
        "epoch": 0.019151948729413518,
        "step": 257
    },
    {
        "loss": 2.5838,
        "grad_norm": 2.0880887508392334,
        "learning_rate": 0.00019867045534368026,
        "epoch": 0.019226469930695284,
        "step": 258
    },
    {
        "loss": 3.0549,
        "grad_norm": 3.185614824295044,
        "learning_rate": 0.00019865899479089568,
        "epoch": 0.019300991131977047,
        "step": 259
    },
    {
        "loss": 2.1379,
        "grad_norm": 3.9637629985809326,
        "learning_rate": 0.00019864748538881227,
        "epoch": 0.01937551233325881,
        "step": 260
    },
    {
        "loss": 2.6192,
        "grad_norm": 2.926030158996582,
        "learning_rate": 0.00019863592714312883,
        "epoch": 0.019450033534540577,
        "step": 261
    },
    {
        "loss": 2.7918,
        "grad_norm": 2.569870948791504,
        "learning_rate": 0.0001986243200595681,
        "epoch": 0.01952455473582234,
        "step": 262
    },
    {
        "loss": 2.6087,
        "grad_norm": 2.6565473079681396,
        "learning_rate": 0.0001986126641438772,
        "epoch": 0.019599075937104107,
        "step": 263
    },
    {
        "loss": 2.7772,
        "grad_norm": 2.607250213623047,
        "learning_rate": 0.00019860095940182735,
        "epoch": 0.01967359713838587,
        "step": 264
    },
    {
        "loss": 2.2202,
        "grad_norm": 3.2382190227508545,
        "learning_rate": 0.0001985892058392139,
        "epoch": 0.019748118339667636,
        "step": 265
    },
    {
        "loss": 2.7004,
        "grad_norm": 3.813842535018921,
        "learning_rate": 0.00019857740346185653,
        "epoch": 0.0198226395409494,
        "step": 266
    },
    {
        "loss": 2.6876,
        "grad_norm": 2.84843111038208,
        "learning_rate": 0.00019856555227559884,
        "epoch": 0.019897160742231166,
        "step": 267
    },
    {
        "loss": 2.8111,
        "grad_norm": 2.3223440647125244,
        "learning_rate": 0.00019855365228630884,
        "epoch": 0.01997168194351293,
        "step": 268
    },
    {
        "loss": 2.5771,
        "grad_norm": 2.5540101528167725,
        "learning_rate": 0.00019854170349987857,
        "epoch": 0.020046203144794696,
        "step": 269
    },
    {
        "loss": 2.0702,
        "grad_norm": 3.1822404861450195,
        "learning_rate": 0.0001985297059222243,
        "epoch": 0.02012072434607646,
        "step": 270
    },
    {
        "loss": 2.5155,
        "grad_norm": 2.2488021850585938,
        "learning_rate": 0.00019851765955928636,
        "epoch": 0.020195245547358222,
        "step": 271
    },
    {
        "loss": 2.9896,
        "grad_norm": 1.9045172929763794,
        "learning_rate": 0.00019850556441702936,
        "epoch": 0.02026976674863999,
        "step": 272
    },
    {
        "loss": 2.6469,
        "grad_norm": 2.3791913986206055,
        "learning_rate": 0.00019849342050144196,
        "epoch": 0.02034428794992175,
        "step": 273
    },
    {
        "loss": 2.021,
        "grad_norm": 3.038337230682373,
        "learning_rate": 0.00019848122781853707,
        "epoch": 0.020418809151203518,
        "step": 274
    },
    {
        "loss": 2.4954,
        "grad_norm": 3.450655460357666,
        "learning_rate": 0.0001984689863743516,
        "epoch": 0.02049333035248528,
        "step": 275
    },
    {
        "loss": 2.8931,
        "grad_norm": 3.8694686889648438,
        "learning_rate": 0.00019845669617494675,
        "epoch": 0.020567851553767048,
        "step": 276
    },
    {
        "loss": 2.3573,
        "grad_norm": 3.9327595233917236,
        "learning_rate": 0.00019844435722640778,
        "epoch": 0.02064237275504881,
        "step": 277
    },
    {
        "loss": 2.5812,
        "grad_norm": 2.7666420936584473,
        "learning_rate": 0.00019843196953484414,
        "epoch": 0.020716893956330577,
        "step": 278
    },
    {
        "loss": 2.2501,
        "grad_norm": 2.2977616786956787,
        "learning_rate": 0.0001984195331063893,
        "epoch": 0.02079141515761234,
        "step": 279
    },
    {
        "loss": 2.5011,
        "grad_norm": 2.4951069355010986,
        "learning_rate": 0.00019840704794720103,
        "epoch": 0.020865936358894107,
        "step": 280
    },
    {
        "loss": 2.6917,
        "grad_norm": 2.5916688442230225,
        "learning_rate": 0.0001983945140634611,
        "epoch": 0.02094045756017587,
        "step": 281
    },
    {
        "loss": 2.3853,
        "grad_norm": 1.7749310731887817,
        "learning_rate": 0.00019838193146137545,
        "epoch": 0.021014978761457633,
        "step": 282
    },
    {
        "loss": 2.2778,
        "grad_norm": 2.7833383083343506,
        "learning_rate": 0.00019836930014717415,
        "epoch": 0.0210894999627394,
        "step": 283
    },
    {
        "loss": 1.5324,
        "grad_norm": 3.697528123855591,
        "learning_rate": 0.00019835662012711135,
        "epoch": 0.021164021164021163,
        "step": 284
    },
    {
        "loss": 1.9514,
        "grad_norm": 1.6894513368606567,
        "learning_rate": 0.0001983438914074654,
        "epoch": 0.02123854236530293,
        "step": 285
    },
    {
        "loss": 2.8486,
        "grad_norm": 3.5991718769073486,
        "learning_rate": 0.0001983311139945386,
        "epoch": 0.021313063566584693,
        "step": 286
    },
    {
        "loss": 2.3173,
        "grad_norm": 2.5190024375915527,
        "learning_rate": 0.00019831828789465758,
        "epoch": 0.02138758476786646,
        "step": 287
    },
    {
        "loss": 2.7457,
        "grad_norm": 3.761650323867798,
        "learning_rate": 0.0001983054131141729,
        "epoch": 0.021462105969148222,
        "step": 288
    },
    {
        "loss": 2.3353,
        "grad_norm": 3.2806036472320557,
        "learning_rate": 0.00019829248965945933,
        "epoch": 0.02153662717042999,
        "step": 289
    },
    {
        "loss": 2.4264,
        "grad_norm": 2.5467007160186768,
        "learning_rate": 0.00019827951753691566,
        "epoch": 0.021611148371711752,
        "step": 290
    },
    {
        "loss": 2.4009,
        "grad_norm": 3.3787782192230225,
        "learning_rate": 0.00019826649675296477,
        "epoch": 0.02168566957299352,
        "step": 291
    },
    {
        "loss": 2.6103,
        "grad_norm": 3.2640933990478516,
        "learning_rate": 0.00019825342731405378,
        "epoch": 0.02176019077427528,
        "step": 292
    },
    {
        "loss": 2.1702,
        "grad_norm": 3.300914764404297,
        "learning_rate": 0.00019824030922665372,
        "epoch": 0.021834711975557045,
        "step": 293
    },
    {
        "loss": 3.2222,
        "grad_norm": 2.447990894317627,
        "learning_rate": 0.00019822714249725983,
        "epoch": 0.02190923317683881,
        "step": 294
    },
    {
        "loss": 2.6855,
        "grad_norm": 3.319150686264038,
        "learning_rate": 0.00019821392713239132,
        "epoch": 0.021983754378120574,
        "step": 295
    },
    {
        "loss": 1.8779,
        "grad_norm": 5.117676734924316,
        "learning_rate": 0.0001982006631385916,
        "epoch": 0.02205827557940234,
        "step": 296
    },
    {
        "loss": 2.8648,
        "grad_norm": 3.9883830547332764,
        "learning_rate": 0.00019818735052242817,
        "epoch": 0.022132796780684104,
        "step": 297
    },
    {
        "loss": 2.853,
        "grad_norm": 2.817744493484497,
        "learning_rate": 0.00019817398929049244,
        "epoch": 0.02220731798196587,
        "step": 298
    },
    {
        "loss": 2.9393,
        "grad_norm": 1.513471245765686,
        "learning_rate": 0.00019816057944940002,
        "epoch": 0.022281839183247634,
        "step": 299
    },
    {
        "loss": 1.578,
        "grad_norm": 3.618741035461426,
        "learning_rate": 0.00019814712100579056,
        "epoch": 0.0223563603845294,
        "step": 300
    },
    {
        "loss": 2.1447,
        "grad_norm": 2.5623221397399902,
        "learning_rate": 0.0001981336139663278,
        "epoch": 0.022430881585811163,
        "step": 301
    },
    {
        "loss": 2.7538,
        "grad_norm": 2.918339729309082,
        "learning_rate": 0.0001981200583376995,
        "epoch": 0.022505402787092926,
        "step": 302
    },
    {
        "loss": 1.6302,
        "grad_norm": 3.3257482051849365,
        "learning_rate": 0.0001981064541266175,
        "epoch": 0.022579923988374693,
        "step": 303
    },
    {
        "loss": 2.4134,
        "grad_norm": 2.5172648429870605,
        "learning_rate": 0.0001980928013398177,
        "epoch": 0.022654445189656456,
        "step": 304
    },
    {
        "loss": 2.6886,
        "grad_norm": 2.7415685653686523,
        "learning_rate": 0.00019807909998406002,
        "epoch": 0.022728966390938223,
        "step": 305
    },
    {
        "loss": 2.3665,
        "grad_norm": 2.643371343612671,
        "learning_rate": 0.00019806535006612846,
        "epoch": 0.022803487592219986,
        "step": 306
    },
    {
        "loss": 2.2505,
        "grad_norm": 2.534212589263916,
        "learning_rate": 0.0001980515515928311,
        "epoch": 0.022878008793501752,
        "step": 307
    },
    {
        "loss": 2.628,
        "grad_norm": 1.8643081188201904,
        "learning_rate": 0.00019803770457099993,
        "epoch": 0.022952529994783515,
        "step": 308
    },
    {
        "loss": 2.6026,
        "grad_norm": 3.6017205715179443,
        "learning_rate": 0.0001980238090074911,
        "epoch": 0.023027051196065282,
        "step": 309
    },
    {
        "loss": 2.5115,
        "grad_norm": 2.385310649871826,
        "learning_rate": 0.0001980098649091848,
        "epoch": 0.023101572397347045,
        "step": 310
    },
    {
        "loss": 2.6689,
        "grad_norm": 2.7888846397399902,
        "learning_rate": 0.00019799587228298512,
        "epoch": 0.02317609359862881,
        "step": 311
    },
    {
        "loss": 2.5501,
        "grad_norm": 3.3927230834960938,
        "learning_rate": 0.00019798183113582036,
        "epoch": 0.023250614799910575,
        "step": 312
    },
    {
        "loss": 1.9598,
        "grad_norm": 3.8867416381835938,
        "learning_rate": 0.0001979677414746427,
        "epoch": 0.023325136001192338,
        "step": 313
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.5181422233581543,
        "learning_rate": 0.00019795360330642842,
        "epoch": 0.023399657202474104,
        "step": 314
    },
    {
        "loss": 2.3984,
        "grad_norm": 4.039994716644287,
        "learning_rate": 0.00019793941663817774,
        "epoch": 0.023474178403755867,
        "step": 315
    },
    {
        "loss": 2.2622,
        "grad_norm": 2.9529740810394287,
        "learning_rate": 0.000197925181476915,
        "epoch": 0.023548699605037634,
        "step": 316
    },
    {
        "loss": 1.556,
        "grad_norm": 1.6791937351226807,
        "learning_rate": 0.0001979108978296885,
        "epoch": 0.023623220806319397,
        "step": 317
    },
    {
        "loss": 2.4297,
        "grad_norm": 3.368837356567383,
        "learning_rate": 0.00019789656570357053,
        "epoch": 0.023697742007601164,
        "step": 318
    },
    {
        "loss": 2.4512,
        "grad_norm": 2.9393675327301025,
        "learning_rate": 0.00019788218510565734,
        "epoch": 0.023772263208882927,
        "step": 319
    },
    {
        "loss": 2.5346,
        "grad_norm": 2.664785385131836,
        "learning_rate": 0.00019786775604306928,
        "epoch": 0.023846784410164693,
        "step": 320
    },
    {
        "loss": 2.3806,
        "grad_norm": 3.431647777557373,
        "learning_rate": 0.00019785327852295064,
        "epoch": 0.023921305611446456,
        "step": 321
    },
    {
        "loss": 2.3504,
        "grad_norm": 2.783097743988037,
        "learning_rate": 0.00019783875255246973,
        "epoch": 0.023995826812728223,
        "step": 322
    },
    {
        "loss": 2.8127,
        "grad_norm": 2.531644821166992,
        "learning_rate": 0.00019782417813881884,
        "epoch": 0.024070348014009986,
        "step": 323
    },
    {
        "loss": 2.305,
        "grad_norm": 3.844564199447632,
        "learning_rate": 0.0001978095552892142,
        "epoch": 0.02414486921529175,
        "step": 324
    },
    {
        "loss": 2.9648,
        "grad_norm": 2.9873859882354736,
        "learning_rate": 0.0001977948840108961,
        "epoch": 0.024219390416573516,
        "step": 325
    },
    {
        "loss": 2.6679,
        "grad_norm": 2.7106759548187256,
        "learning_rate": 0.00019778016431112877,
        "epoch": 0.02429391161785528,
        "step": 326
    },
    {
        "loss": 2.5971,
        "grad_norm": 1.9861822128295898,
        "learning_rate": 0.00019776539619720037,
        "epoch": 0.024368432819137045,
        "step": 327
    },
    {
        "loss": 2.2515,
        "grad_norm": 3.209710121154785,
        "learning_rate": 0.00019775057967642314,
        "epoch": 0.02444295402041881,
        "step": 328
    },
    {
        "loss": 2.6715,
        "grad_norm": 2.767139196395874,
        "learning_rate": 0.00019773571475613318,
        "epoch": 0.024517475221700575,
        "step": 329
    },
    {
        "loss": 2.3376,
        "grad_norm": 2.735809564590454,
        "learning_rate": 0.0001977208014436906,
        "epoch": 0.024591996422982338,
        "step": 330
    },
    {
        "loss": 3.0438,
        "grad_norm": 2.01533579826355,
        "learning_rate": 0.0001977058397464795,
        "epoch": 0.024666517624264105,
        "step": 331
    },
    {
        "loss": 3.1575,
        "grad_norm": 3.9268741607666016,
        "learning_rate": 0.00019769082967190788,
        "epoch": 0.024741038825545868,
        "step": 332
    },
    {
        "loss": 2.4826,
        "grad_norm": 3.894817352294922,
        "learning_rate": 0.00019767577122740772,
        "epoch": 0.02481556002682763,
        "step": 333
    },
    {
        "loss": 2.1539,
        "grad_norm": 3.9027552604675293,
        "learning_rate": 0.00019766066442043499,
        "epoch": 0.024890081228109397,
        "step": 334
    },
    {
        "loss": 2.2349,
        "grad_norm": 3.6276450157165527,
        "learning_rate": 0.00019764550925846947,
        "epoch": 0.02496460242939116,
        "step": 335
    },
    {
        "loss": 2.8889,
        "grad_norm": 1.9116514921188354,
        "learning_rate": 0.0001976303057490151,
        "epoch": 0.025039123630672927,
        "step": 336
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.4336838722229004,
        "learning_rate": 0.00019761505389959952,
        "epoch": 0.02511364483195469,
        "step": 337
    },
    {
        "loss": 2.7479,
        "grad_norm": 2.208204507827759,
        "learning_rate": 0.00019759975371777452,
        "epoch": 0.025188166033236457,
        "step": 338
    },
    {
        "loss": 3.2353,
        "grad_norm": 3.053144931793213,
        "learning_rate": 0.00019758440521111564,
        "epoch": 0.02526268723451822,
        "step": 339
    },
    {
        "loss": 1.7456,
        "grad_norm": 3.80619215965271,
        "learning_rate": 0.00019756900838722245,
        "epoch": 0.025337208435799986,
        "step": 340
    },
    {
        "loss": 2.5169,
        "grad_norm": 3.825978994369507,
        "learning_rate": 0.00019755356325371848,
        "epoch": 0.02541172963708175,
        "step": 341
    },
    {
        "loss": 2.8012,
        "grad_norm": 2.2813405990600586,
        "learning_rate": 0.00019753806981825104,
        "epoch": 0.025486250838363516,
        "step": 342
    },
    {
        "loss": 2.7604,
        "grad_norm": 2.0147876739501953,
        "learning_rate": 0.00019752252808849147,
        "epoch": 0.02556077203964528,
        "step": 343
    },
    {
        "loss": 2.585,
        "grad_norm": 3.4863781929016113,
        "learning_rate": 0.00019750693807213501,
        "epoch": 0.025635293240927042,
        "step": 344
    },
    {
        "loss": 2.4685,
        "grad_norm": 2.203209161758423,
        "learning_rate": 0.00019749129977690078,
        "epoch": 0.02570981444220881,
        "step": 345
    },
    {
        "loss": 2.4949,
        "grad_norm": 2.785825490951538,
        "learning_rate": 0.00019747561321053177,
        "epoch": 0.025784335643490572,
        "step": 346
    },
    {
        "loss": 2.2176,
        "grad_norm": 3.1851837635040283,
        "learning_rate": 0.00019745987838079495,
        "epoch": 0.02585885684477234,
        "step": 347
    },
    {
        "loss": 2.4213,
        "grad_norm": 2.2771711349487305,
        "learning_rate": 0.00019744409529548116,
        "epoch": 0.0259333780460541,
        "step": 348
    },
    {
        "loss": 1.3455,
        "grad_norm": 3.2509102821350098,
        "learning_rate": 0.00019742826396240508,
        "epoch": 0.026007899247335868,
        "step": 349
    },
    {
        "loss": 2.9507,
        "grad_norm": 3.699028968811035,
        "learning_rate": 0.0001974123843894054,
        "epoch": 0.02608242044861763,
        "step": 350
    },
    {
        "loss": 1.9595,
        "grad_norm": 4.146139144897461,
        "learning_rate": 0.00019739645658434452,
        "epoch": 0.026156941649899398,
        "step": 351
    },
    {
        "loss": 2.695,
        "grad_norm": 1.8635461330413818,
        "learning_rate": 0.00019738048055510885,
        "epoch": 0.02623146285118116,
        "step": 352
    },
    {
        "loss": 2.3408,
        "grad_norm": 2.817873001098633,
        "learning_rate": 0.0001973644563096087,
        "epoch": 0.026305984052462927,
        "step": 353
    },
    {
        "loss": 2.4361,
        "grad_norm": 3.9793286323547363,
        "learning_rate": 0.0001973483838557781,
        "epoch": 0.02638050525374469,
        "step": 354
    },
    {
        "loss": 2.7607,
        "grad_norm": 3.0150039196014404,
        "learning_rate": 0.00019733226320157513,
        "epoch": 0.026455026455026454,
        "step": 355
    },
    {
        "loss": 3.1093,
        "grad_norm": 2.18601393699646,
        "learning_rate": 0.00019731609435498165,
        "epoch": 0.02652954765630822,
        "step": 356
    },
    {
        "loss": 2.3926,
        "grad_norm": 2.5455234050750732,
        "learning_rate": 0.00019729987732400336,
        "epoch": 0.026604068857589983,
        "step": 357
    },
    {
        "loss": 2.3065,
        "grad_norm": 3.9455151557922363,
        "learning_rate": 0.00019728361211666983,
        "epoch": 0.02667859005887175,
        "step": 358
    },
    {
        "loss": 3.0429,
        "grad_norm": 2.349169969558716,
        "learning_rate": 0.00019726729874103448,
        "epoch": 0.026753111260153513,
        "step": 359
    },
    {
        "loss": 2.7858,
        "grad_norm": 1.9422328472137451,
        "learning_rate": 0.00019725093720517466,
        "epoch": 0.02682763246143528,
        "step": 360
    },
    {
        "loss": 2.5164,
        "grad_norm": 3.1400246620178223,
        "learning_rate": 0.00019723452751719146,
        "epoch": 0.026902153662717043,
        "step": 361
    },
    {
        "loss": 2.9198,
        "grad_norm": 2.2317981719970703,
        "learning_rate": 0.0001972180696852099,
        "epoch": 0.02697667486399881,
        "step": 362
    },
    {
        "loss": 2.7093,
        "grad_norm": 2.465810537338257,
        "learning_rate": 0.0001972015637173787,
        "epoch": 0.027051196065280572,
        "step": 363
    },
    {
        "loss": 2.6345,
        "grad_norm": 2.063539981842041,
        "learning_rate": 0.0001971850096218706,
        "epoch": 0.027125717266562335,
        "step": 364
    },
    {
        "loss": 2.8023,
        "grad_norm": 2.6110517978668213,
        "learning_rate": 0.00019716840740688203,
        "epoch": 0.027200238467844102,
        "step": 365
    },
    {
        "loss": 2.7933,
        "grad_norm": 1.425405502319336,
        "learning_rate": 0.0001971517570806333,
        "epoch": 0.027274759669125865,
        "step": 366
    },
    {
        "loss": 2.2568,
        "grad_norm": 2.8313848972320557,
        "learning_rate": 0.0001971350586513685,
        "epoch": 0.02734928087040763,
        "step": 367
    },
    {
        "loss": 2.3741,
        "grad_norm": 2.1614561080932617,
        "learning_rate": 0.00019711831212735562,
        "epoch": 0.027423802071689395,
        "step": 368
    },
    {
        "loss": 2.8196,
        "grad_norm": 2.4436848163604736,
        "learning_rate": 0.0001971015175168864,
        "epoch": 0.02749832327297116,
        "step": 369
    },
    {
        "loss": 2.5873,
        "grad_norm": 2.7680416107177734,
        "learning_rate": 0.00019708467482827636,
        "epoch": 0.027572844474252924,
        "step": 370
    },
    {
        "loss": 2.5188,
        "grad_norm": 2.8050153255462646,
        "learning_rate": 0.00019706778406986493,
        "epoch": 0.02764736567553469,
        "step": 371
    },
    {
        "loss": 2.6491,
        "grad_norm": 2.431798219680786,
        "learning_rate": 0.00019705084525001524,
        "epoch": 0.027721886876816454,
        "step": 372
    },
    {
        "loss": 2.1684,
        "grad_norm": 1.8830678462982178,
        "learning_rate": 0.00019703385837711426,
        "epoch": 0.02779640807809822,
        "step": 373
    },
    {
        "loss": 2.423,
        "grad_norm": 2.374234676361084,
        "learning_rate": 0.00019701682345957272,
        "epoch": 0.027870929279379984,
        "step": 374
    },
    {
        "loss": 2.4791,
        "grad_norm": 2.256455898284912,
        "learning_rate": 0.00019699974050582523,
        "epoch": 0.027945450480661747,
        "step": 375
    },
    {
        "loss": 1.6988,
        "grad_norm": 2.7720348834991455,
        "learning_rate": 0.00019698260952433005,
        "epoch": 0.028019971681943513,
        "step": 376
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.293661594390869,
        "learning_rate": 0.00019696543052356936,
        "epoch": 0.028094492883225276,
        "step": 377
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.650040864944458,
        "learning_rate": 0.00019694820351204894,
        "epoch": 0.028169014084507043,
        "step": 378
    },
    {
        "loss": 1.926,
        "grad_norm": 3.9069504737854004,
        "learning_rate": 0.00019693092849829856,
        "epoch": 0.028243535285788806,
        "step": 379
    },
    {
        "loss": 2.5385,
        "grad_norm": 2.8292088508605957,
        "learning_rate": 0.0001969136054908716,
        "epoch": 0.028318056487070573,
        "step": 380
    },
    {
        "loss": 2.2656,
        "grad_norm": 3.6241683959960938,
        "learning_rate": 0.00019689623449834525,
        "epoch": 0.028392577688352336,
        "step": 381
    },
    {
        "loss": 2.7393,
        "grad_norm": 1.5834217071533203,
        "learning_rate": 0.00019687881552932045,
        "epoch": 0.028467098889634102,
        "step": 382
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.004674196243286,
        "learning_rate": 0.0001968613485924219,
        "epoch": 0.028541620090915865,
        "step": 383
    },
    {
        "loss": 2.8719,
        "grad_norm": 1.5817562341690063,
        "learning_rate": 0.00019684383369629807,
        "epoch": 0.028616141292197632,
        "step": 384
    },
    {
        "loss": 2.5189,
        "grad_norm": 2.415659189224243,
        "learning_rate": 0.00019682627084962112,
        "epoch": 0.028690662493479395,
        "step": 385
    },
    {
        "loss": 2.514,
        "grad_norm": 2.973515033721924,
        "learning_rate": 0.00019680866006108704,
        "epoch": 0.028765183694761158,
        "step": 386
    },
    {
        "loss": 2.4301,
        "grad_norm": 3.647023916244507,
        "learning_rate": 0.0001967910013394155,
        "epoch": 0.028839704896042925,
        "step": 387
    },
    {
        "loss": 2.1147,
        "grad_norm": 2.817622423171997,
        "learning_rate": 0.0001967732946933499,
        "epoch": 0.028914226097324688,
        "step": 388
    },
    {
        "loss": 2.9436,
        "grad_norm": 3.6359548568725586,
        "learning_rate": 0.00019675554013165735,
        "epoch": 0.028988747298606454,
        "step": 389
    },
    {
        "loss": 2.2228,
        "grad_norm": 2.6798603534698486,
        "learning_rate": 0.00019673773766312874,
        "epoch": 0.029063268499888217,
        "step": 390
    },
    {
        "loss": 2.3393,
        "grad_norm": 2.4459781646728516,
        "learning_rate": 0.0001967198872965787,
        "epoch": 0.029137789701169984,
        "step": 391
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.4847545623779297,
        "learning_rate": 0.00019670198904084548,
        "epoch": 0.029212310902451747,
        "step": 392
    },
    {
        "loss": 2.3991,
        "grad_norm": 2.936270236968994,
        "learning_rate": 0.00019668404290479105,
        "epoch": 0.029286832103733514,
        "step": 393
    },
    {
        "loss": 2.3091,
        "grad_norm": 1.897149920463562,
        "learning_rate": 0.00019666604889730124,
        "epoch": 0.029361353305015277,
        "step": 394
    },
    {
        "loss": 2.1217,
        "grad_norm": 3.0314767360687256,
        "learning_rate": 0.0001966480070272854,
        "epoch": 0.02943587450629704,
        "step": 395
    },
    {
        "loss": 1.7785,
        "grad_norm": 3.194655179977417,
        "learning_rate": 0.00019662991730367663,
        "epoch": 0.029510395707578806,
        "step": 396
    },
    {
        "loss": 2.2427,
        "grad_norm": 2.776545286178589,
        "learning_rate": 0.0001966117797354318,
        "epoch": 0.02958491690886057,
        "step": 397
    },
    {
        "loss": 2.826,
        "grad_norm": 2.161247968673706,
        "learning_rate": 0.0001965935943315314,
        "epoch": 0.029659438110142336,
        "step": 398
    },
    {
        "loss": 2.7278,
        "grad_norm": 3.5852158069610596,
        "learning_rate": 0.0001965753611009796,
        "epoch": 0.0297339593114241,
        "step": 399
    },
    {
        "loss": 2.8948,
        "grad_norm": 3.376673460006714,
        "learning_rate": 0.00019655708005280432,
        "epoch": 0.029808480512705866,
        "step": 400
    },
    {
        "loss": 2.483,
        "grad_norm": 3.355689287185669,
        "learning_rate": 0.00019653875119605703,
        "epoch": 0.02988300171398763,
        "step": 401
    },
    {
        "loss": 2.0118,
        "grad_norm": 4.513538837432861,
        "learning_rate": 0.00019652037453981298,
        "epoch": 0.029957522915269395,
        "step": 402
    },
    {
        "loss": 2.2827,
        "grad_norm": 3.1670353412628174,
        "learning_rate": 0.00019650195009317107,
        "epoch": 0.03003204411655116,
        "step": 403
    },
    {
        "loss": 2.7282,
        "grad_norm": 2.6605148315429688,
        "learning_rate": 0.00019648347786525383,
        "epoch": 0.030106565317832925,
        "step": 404
    },
    {
        "loss": 1.8943,
        "grad_norm": 3.263603448867798,
        "learning_rate": 0.00019646495786520743,
        "epoch": 0.030181086519114688,
        "step": 405
    },
    {
        "loss": 2.0245,
        "grad_norm": 3.1065075397491455,
        "learning_rate": 0.00019644639010220184,
        "epoch": 0.03025560772039645,
        "step": 406
    },
    {
        "loss": 1.9421,
        "grad_norm": 3.1848671436309814,
        "learning_rate": 0.00019642777458543046,
        "epoch": 0.030330128921678218,
        "step": 407
    },
    {
        "loss": 2.0518,
        "grad_norm": 3.355881929397583,
        "learning_rate": 0.00019640911132411048,
        "epoch": 0.03040465012295998,
        "step": 408
    },
    {
        "loss": 2.8031,
        "grad_norm": 2.3316023349761963,
        "learning_rate": 0.00019639040032748268,
        "epoch": 0.030479171324241747,
        "step": 409
    },
    {
        "loss": 2.9648,
        "grad_norm": 4.79382848739624,
        "learning_rate": 0.0001963716416048115,
        "epoch": 0.03055369252552351,
        "step": 410
    },
    {
        "loss": 1.7084,
        "grad_norm": 3.892564296722412,
        "learning_rate": 0.00019635283516538502,
        "epoch": 0.030628213726805277,
        "step": 411
    },
    {
        "loss": 2.7418,
        "grad_norm": 2.0598039627075195,
        "learning_rate": 0.00019633398101851488,
        "epoch": 0.03070273492808704,
        "step": 412
    },
    {
        "loss": 1.9187,
        "grad_norm": 3.277419328689575,
        "learning_rate": 0.00019631507917353637,
        "epoch": 0.030777256129368807,
        "step": 413
    },
    {
        "loss": 2.908,
        "grad_norm": 3.4034488201141357,
        "learning_rate": 0.00019629612963980848,
        "epoch": 0.03085177733065057,
        "step": 414
    },
    {
        "loss": 2.3789,
        "grad_norm": 1.6551899909973145,
        "learning_rate": 0.00019627713242671374,
        "epoch": 0.030926298531932336,
        "step": 415
    },
    {
        "loss": 2.6516,
        "grad_norm": 2.631202459335327,
        "learning_rate": 0.00019625808754365819,
        "epoch": 0.0310008197332141,
        "step": 416
    },
    {
        "loss": 2.7861,
        "grad_norm": 2.886664867401123,
        "learning_rate": 0.0001962389950000717,
        "epoch": 0.031075340934495863,
        "step": 417
    },
    {
        "loss": 2.4876,
        "grad_norm": 2.0784120559692383,
        "learning_rate": 0.00019621985480540756,
        "epoch": 0.03114986213577763,
        "step": 418
    },
    {
        "loss": 2.1663,
        "grad_norm": 2.9655983448028564,
        "learning_rate": 0.00019620066696914267,
        "epoch": 0.031224383337059392,
        "step": 419
    },
    {
        "loss": 2.5015,
        "grad_norm": 3.368152618408203,
        "learning_rate": 0.00019618143150077762,
        "epoch": 0.03129890453834116,
        "step": 420
    },
    {
        "loss": 2.4812,
        "grad_norm": 2.789292573928833,
        "learning_rate": 0.0001961621484098365,
        "epoch": 0.03137342573962292,
        "step": 421
    },
    {
        "loss": 2.6831,
        "grad_norm": 2.2517600059509277,
        "learning_rate": 0.000196142817705867,
        "epoch": 0.031447946940904685,
        "step": 422
    },
    {
        "loss": 1.8574,
        "grad_norm": 2.6617238521575928,
        "learning_rate": 0.00019612343939844035,
        "epoch": 0.031522468142186455,
        "step": 423
    },
    {
        "loss": 2.7048,
        "grad_norm": 3.712235927581787,
        "learning_rate": 0.00019610401349715143,
        "epoch": 0.03159698934346822,
        "step": 424
    },
    {
        "loss": 2.8153,
        "grad_norm": 3.6288082599639893,
        "learning_rate": 0.00019608454001161863,
        "epoch": 0.03167151054474998,
        "step": 425
    },
    {
        "loss": 2.7987,
        "grad_norm": 3.0376992225646973,
        "learning_rate": 0.0001960650189514839,
        "epoch": 0.031746031746031744,
        "step": 426
    },
    {
        "loss": 1.7842,
        "grad_norm": 3.8662912845611572,
        "learning_rate": 0.00019604545032641277,
        "epoch": 0.03182055294731351,
        "step": 427
    },
    {
        "loss": 2.6117,
        "grad_norm": 2.8809831142425537,
        "learning_rate": 0.00019602583414609427,
        "epoch": 0.03189507414859528,
        "step": 428
    },
    {
        "loss": 1.9459,
        "grad_norm": 3.330197334289551,
        "learning_rate": 0.00019600617042024108,
        "epoch": 0.03196959534987704,
        "step": 429
    },
    {
        "loss": 2.5891,
        "grad_norm": 3.1908645629882812,
        "learning_rate": 0.00019598645915858926,
        "epoch": 0.032044116551158804,
        "step": 430
    },
    {
        "loss": 1.7504,
        "grad_norm": 1.4034419059753418,
        "learning_rate": 0.0001959667003708986,
        "epoch": 0.03211863775244057,
        "step": 431
    },
    {
        "loss": 2.0426,
        "grad_norm": 3.7088229656219482,
        "learning_rate": 0.00019594689406695227,
        "epoch": 0.03219315895372234,
        "step": 432
    },
    {
        "loss": 2.5526,
        "grad_norm": 3.9769933223724365,
        "learning_rate": 0.000195927040256557,
        "epoch": 0.0322676801550041,
        "step": 433
    },
    {
        "loss": 2.7214,
        "grad_norm": 2.701554536819458,
        "learning_rate": 0.0001959071389495431,
        "epoch": 0.03234220135628586,
        "step": 434
    },
    {
        "loss": 2.1132,
        "grad_norm": 3.5575239658355713,
        "learning_rate": 0.00019588719015576435,
        "epoch": 0.032416722557567626,
        "step": 435
    },
    {
        "loss": 2.8962,
        "grad_norm": 2.923630952835083,
        "learning_rate": 0.0001958671938850981,
        "epoch": 0.032491243758849396,
        "step": 436
    },
    {
        "loss": 2.4625,
        "grad_norm": 1.998190999031067,
        "learning_rate": 0.00019584715014744504,
        "epoch": 0.03256576496013116,
        "step": 437
    },
    {
        "loss": 2.73,
        "grad_norm": 2.386162519454956,
        "learning_rate": 0.00019582705895272956,
        "epoch": 0.03264028616141292,
        "step": 438
    },
    {
        "loss": 2.2499,
        "grad_norm": 3.401031732559204,
        "learning_rate": 0.00019580692031089946,
        "epoch": 0.032714807362694685,
        "step": 439
    },
    {
        "loss": 2.4081,
        "grad_norm": 3.1994967460632324,
        "learning_rate": 0.00019578673423192606,
        "epoch": 0.03278932856397645,
        "step": 440
    },
    {
        "loss": 2.6062,
        "grad_norm": 2.292774200439453,
        "learning_rate": 0.0001957665007258041,
        "epoch": 0.03286384976525822,
        "step": 441
    },
    {
        "loss": 2.5236,
        "grad_norm": 2.1416664123535156,
        "learning_rate": 0.0001957462198025519,
        "epoch": 0.03293837096653998,
        "step": 442
    },
    {
        "loss": 1.7947,
        "grad_norm": 2.8168928623199463,
        "learning_rate": 0.00019572589147221118,
        "epoch": 0.033012892167821745,
        "step": 443
    },
    {
        "loss": 1.7437,
        "grad_norm": 3.3484456539154053,
        "learning_rate": 0.00019570551574484718,
        "epoch": 0.03308741336910351,
        "step": 444
    },
    {
        "loss": 2.1459,
        "grad_norm": 3.6415011882781982,
        "learning_rate": 0.00019568509263054854,
        "epoch": 0.03316193457038528,
        "step": 445
    },
    {
        "loss": 2.0945,
        "grad_norm": 2.5113744735717773,
        "learning_rate": 0.0001956646221394275,
        "epoch": 0.03323645577166704,
        "step": 446
    },
    {
        "loss": 2.9002,
        "grad_norm": 2.461120843887329,
        "learning_rate": 0.00019564410428161958,
        "epoch": 0.033310976972948804,
        "step": 447
    },
    {
        "loss": 2.7076,
        "grad_norm": 2.48109769821167,
        "learning_rate": 0.0001956235390672839,
        "epoch": 0.03338549817423057,
        "step": 448
    },
    {
        "loss": 2.9661,
        "grad_norm": 1.2240256071090698,
        "learning_rate": 0.00019560292650660298,
        "epoch": 0.03346001937551233,
        "step": 449
    },
    {
        "loss": 2.2683,
        "grad_norm": 2.7858388423919678,
        "learning_rate": 0.00019558226660978273,
        "epoch": 0.0335345405767941,
        "step": 450
    },
    {
        "loss": 2.7691,
        "grad_norm": 2.0640313625335693,
        "learning_rate": 0.00019556155938705254,
        "epoch": 0.03360906177807586,
        "step": 451
    },
    {
        "loss": 1.8187,
        "grad_norm": 2.7419772148132324,
        "learning_rate": 0.00019554080484866527,
        "epoch": 0.033683582979357626,
        "step": 452
    },
    {
        "loss": 3.0592,
        "grad_norm": 2.179154634475708,
        "learning_rate": 0.00019552000300489714,
        "epoch": 0.03375810418063939,
        "step": 453
    },
    {
        "loss": 2.596,
        "grad_norm": 2.8640804290771484,
        "learning_rate": 0.00019549915386604788,
        "epoch": 0.03383262538192116,
        "step": 454
    },
    {
        "loss": 2.6969,
        "grad_norm": 2.7134156227111816,
        "learning_rate": 0.00019547825744244054,
        "epoch": 0.03390714658320292,
        "step": 455
    },
    {
        "loss": 2.6901,
        "grad_norm": 1.3362646102905273,
        "learning_rate": 0.0001954573137444216,
        "epoch": 0.033981667784484686,
        "step": 456
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.0695343017578125,
        "learning_rate": 0.00019543632278236098,
        "epoch": 0.03405618898576645,
        "step": 457
    },
    {
        "loss": 3.1268,
        "grad_norm": 2.6794395446777344,
        "learning_rate": 0.00019541528456665206,
        "epoch": 0.03413071018704821,
        "step": 458
    },
    {
        "loss": 2.4397,
        "grad_norm": 2.4017727375030518,
        "learning_rate": 0.00019539419910771146,
        "epoch": 0.03420523138832998,
        "step": 459
    },
    {
        "loss": 2.6691,
        "grad_norm": 3.732722282409668,
        "learning_rate": 0.0001953730664159793,
        "epoch": 0.034279752589611745,
        "step": 460
    },
    {
        "loss": 2.4672,
        "grad_norm": 2.1907713413238525,
        "learning_rate": 0.00019535188650191912,
        "epoch": 0.03435427379089351,
        "step": 461
    },
    {
        "loss": 2.1128,
        "grad_norm": 2.0963611602783203,
        "learning_rate": 0.00019533065937601778,
        "epoch": 0.03442879499217527,
        "step": 462
    },
    {
        "loss": 2.5259,
        "grad_norm": 2.8602123260498047,
        "learning_rate": 0.00019530938504878548,
        "epoch": 0.03450331619345704,
        "step": 463
    },
    {
        "loss": 2.6194,
        "grad_norm": 5.716311454772949,
        "learning_rate": 0.00019528806353075586,
        "epoch": 0.034577837394738804,
        "step": 464
    },
    {
        "loss": 2.511,
        "grad_norm": 2.6707000732421875,
        "learning_rate": 0.00019526669483248588,
        "epoch": 0.03465235859602057,
        "step": 465
    },
    {
        "loss": 2.6318,
        "grad_norm": 1.8635209798812866,
        "learning_rate": 0.00019524527896455591,
        "epoch": 0.03472687979730233,
        "step": 466
    },
    {
        "loss": 2.7709,
        "grad_norm": 1.2976101636886597,
        "learning_rate": 0.0001952238159375696,
        "epoch": 0.0348014009985841,
        "step": 467
    },
    {
        "loss": 2.2847,
        "grad_norm": 2.279232978820801,
        "learning_rate": 0.0001952023057621541,
        "epoch": 0.034875922199865864,
        "step": 468
    },
    {
        "loss": 2.7275,
        "grad_norm": 2.338265895843506,
        "learning_rate": 0.00019518074844895972,
        "epoch": 0.03495044340114763,
        "step": 469
    },
    {
        "loss": 2.3302,
        "grad_norm": 2.4376213550567627,
        "learning_rate": 0.0001951591440086602,
        "epoch": 0.03502496460242939,
        "step": 470
    },
    {
        "loss": 1.7141,
        "grad_norm": 3.8623952865600586,
        "learning_rate": 0.00019513749245195263,
        "epoch": 0.03509948580371115,
        "step": 471
    },
    {
        "loss": 1.719,
        "grad_norm": 3.9178590774536133,
        "learning_rate": 0.00019511579378955738,
        "epoch": 0.03517400700499292,
        "step": 472
    },
    {
        "loss": 2.8612,
        "grad_norm": 3.497797966003418,
        "learning_rate": 0.0001950940480322182,
        "epoch": 0.035248528206274686,
        "step": 473
    },
    {
        "loss": 2.5442,
        "grad_norm": 2.1477808952331543,
        "learning_rate": 0.00019507225519070206,
        "epoch": 0.03532304940755645,
        "step": 474
    },
    {
        "loss": 2.4061,
        "grad_norm": 2.3097803592681885,
        "learning_rate": 0.00019505041527579937,
        "epoch": 0.03539757060883821,
        "step": 475
    },
    {
        "loss": 2.5354,
        "grad_norm": 2.919313907623291,
        "learning_rate": 0.0001950285282983238,
        "epoch": 0.03547209181011998,
        "step": 476
    },
    {
        "loss": 2.1008,
        "grad_norm": 2.9250662326812744,
        "learning_rate": 0.00019500659426911225,
        "epoch": 0.035546613011401745,
        "step": 477
    },
    {
        "loss": 1.884,
        "grad_norm": 2.604625701904297,
        "learning_rate": 0.000194984613199025,
        "epoch": 0.03562113421268351,
        "step": 478
    },
    {
        "loss": 2.7391,
        "grad_norm": 2.378948211669922,
        "learning_rate": 0.0001949625850989456,
        "epoch": 0.03569565541396527,
        "step": 479
    },
    {
        "loss": 2.3748,
        "grad_norm": 2.7726991176605225,
        "learning_rate": 0.00019494050997978093,
        "epoch": 0.035770176615247035,
        "step": 480
    },
    {
        "loss": 3.145,
        "grad_norm": 1.6837475299835205,
        "learning_rate": 0.00019491838785246104,
        "epoch": 0.035844697816528805,
        "step": 481
    },
    {
        "loss": 2.7211,
        "grad_norm": 2.615619421005249,
        "learning_rate": 0.00019489621872793934,
        "epoch": 0.03591921901781057,
        "step": 482
    },
    {
        "loss": 2.8851,
        "grad_norm": 3.4602315425872803,
        "learning_rate": 0.00019487400261719248,
        "epoch": 0.03599374021909233,
        "step": 483
    },
    {
        "loss": 2.2211,
        "grad_norm": 2.6049721240997314,
        "learning_rate": 0.00019485173953122042,
        "epoch": 0.036068261420374094,
        "step": 484
    },
    {
        "loss": 2.3555,
        "grad_norm": 1.7758924961090088,
        "learning_rate": 0.0001948294294810463,
        "epoch": 0.036142782621655864,
        "step": 485
    },
    {
        "loss": 2.297,
        "grad_norm": 3.084555149078369,
        "learning_rate": 0.00019480707247771658,
        "epoch": 0.03621730382293763,
        "step": 486
    },
    {
        "loss": 2.7307,
        "grad_norm": 2.895108461380005,
        "learning_rate": 0.00019478466853230093,
        "epoch": 0.03629182502421939,
        "step": 487
    },
    {
        "loss": 2.7169,
        "grad_norm": 1.9583687782287598,
        "learning_rate": 0.00019476221765589232,
        "epoch": 0.03636634622550115,
        "step": 488
    },
    {
        "loss": 2.9917,
        "grad_norm": 3.716407060623169,
        "learning_rate": 0.00019473971985960683,
        "epoch": 0.036440867426782916,
        "step": 489
    },
    {
        "loss": 2.2433,
        "grad_norm": 2.9603822231292725,
        "learning_rate": 0.00019471717515458394,
        "epoch": 0.036515388628064686,
        "step": 490
    },
    {
        "loss": 2.0007,
        "grad_norm": 2.354806900024414,
        "learning_rate": 0.00019469458355198622,
        "epoch": 0.03658990982934645,
        "step": 491
    },
    {
        "loss": 1.3614,
        "grad_norm": 3.26987624168396,
        "learning_rate": 0.00019467194506299953,
        "epoch": 0.03666443103062821,
        "step": 492
    },
    {
        "loss": 2.1825,
        "grad_norm": 3.433535099029541,
        "learning_rate": 0.0001946492596988329,
        "epoch": 0.036738952231909976,
        "step": 493
    },
    {
        "loss": 2.1707,
        "grad_norm": 2.7656826972961426,
        "learning_rate": 0.00019462652747071868,
        "epoch": 0.036813473433191746,
        "step": 494
    },
    {
        "loss": 2.2703,
        "grad_norm": 2.2463901042938232,
        "learning_rate": 0.00019460374838991225,
        "epoch": 0.03688799463447351,
        "step": 495
    },
    {
        "loss": 2.5062,
        "grad_norm": 2.475101947784424,
        "learning_rate": 0.00019458092246769232,
        "epoch": 0.03696251583575527,
        "step": 496
    },
    {
        "loss": 2.9637,
        "grad_norm": 5.044809818267822,
        "learning_rate": 0.00019455804971536074,
        "epoch": 0.037037037037037035,
        "step": 497
    },
    {
        "loss": 2.3672,
        "grad_norm": 1.765366792678833,
        "learning_rate": 0.00019453513014424257,
        "epoch": 0.037111558238318805,
        "step": 498
    },
    {
        "loss": 2.4346,
        "grad_norm": 3.820345401763916,
        "learning_rate": 0.00019451216376568603,
        "epoch": 0.03718607943960057,
        "step": 499
    },
    {
        "loss": 2.7724,
        "grad_norm": 3.3292758464813232,
        "learning_rate": 0.00019448915059106252,
        "epoch": 0.03726060064088233,
        "step": 500
    },
    {
        "loss": 2.9834,
        "grad_norm": 2.813218832015991,
        "learning_rate": 0.00019446609063176662,
        "epoch": 0.037335121842164094,
        "step": 501
    },
    {
        "loss": 2.5076,
        "grad_norm": 2.4350974559783936,
        "learning_rate": 0.00019444298389921608,
        "epoch": 0.03740964304344586,
        "step": 502
    },
    {
        "loss": 2.5784,
        "grad_norm": 2.8610846996307373,
        "learning_rate": 0.0001944198304048518,
        "epoch": 0.03748416424472763,
        "step": 503
    },
    {
        "loss": 2.0427,
        "grad_norm": 3.0000345706939697,
        "learning_rate": 0.00019439663016013783,
        "epoch": 0.03755868544600939,
        "step": 504
    },
    {
        "loss": 2.6371,
        "grad_norm": 2.745971202850342,
        "learning_rate": 0.00019437338317656138,
        "epoch": 0.037633206647291154,
        "step": 505
    },
    {
        "loss": 2.658,
        "grad_norm": 1.8865790367126465,
        "learning_rate": 0.00019435008946563275,
        "epoch": 0.03770772784857292,
        "step": 506
    },
    {
        "loss": 2.3787,
        "grad_norm": 3.585581064224243,
        "learning_rate": 0.00019432674903888548,
        "epoch": 0.03778224904985469,
        "step": 507
    },
    {
        "loss": 2.2349,
        "grad_norm": 3.875722646713257,
        "learning_rate": 0.00019430336190787614,
        "epoch": 0.03785677025113645,
        "step": 508
    },
    {
        "loss": 2.983,
        "grad_norm": 2.500782012939453,
        "learning_rate": 0.00019427992808418448,
        "epoch": 0.03793129145241821,
        "step": 509
    },
    {
        "loss": 2.7467,
        "grad_norm": 3.761770486831665,
        "learning_rate": 0.00019425644757941336,
        "epoch": 0.038005812653699976,
        "step": 510
    },
    {
        "loss": 1.9045,
        "grad_norm": 2.8818788528442383,
        "learning_rate": 0.0001942329204051887,
        "epoch": 0.03808033385498174,
        "step": 511
    },
    {
        "loss": 2.635,
        "grad_norm": 2.163330316543579,
        "learning_rate": 0.00019420934657315964,
        "epoch": 0.03815485505626351,
        "step": 512
    },
    {
        "loss": 2.7242,
        "grad_norm": 3.1548714637756348,
        "learning_rate": 0.00019418572609499832,
        "epoch": 0.03822937625754527,
        "step": 513
    },
    {
        "loss": 2.7562,
        "grad_norm": 2.6878511905670166,
        "learning_rate": 0.0001941620589824,
        "epoch": 0.038303897458827035,
        "step": 514
    },
    {
        "loss": 2.9757,
        "grad_norm": 2.4715523719787598,
        "learning_rate": 0.00019413834524708307,
        "epoch": 0.0383784186601088,
        "step": 515
    },
    {
        "loss": 2.3736,
        "grad_norm": 2.950516700744629,
        "learning_rate": 0.00019411458490078897,
        "epoch": 0.03845293986139057,
        "step": 516
    },
    {
        "loss": 1.9835,
        "grad_norm": 3.2007429599761963,
        "learning_rate": 0.0001940907779552822,
        "epoch": 0.03852746106267233,
        "step": 517
    },
    {
        "loss": 2.3705,
        "grad_norm": 1.7026211023330688,
        "learning_rate": 0.00019406692442235044,
        "epoch": 0.038601982263954095,
        "step": 518
    },
    {
        "loss": 2.8766,
        "grad_norm": 2.0280299186706543,
        "learning_rate": 0.00019404302431380422,
        "epoch": 0.03867650346523586,
        "step": 519
    },
    {
        "loss": 2.68,
        "grad_norm": 1.8479076623916626,
        "learning_rate": 0.00019401907764147742,
        "epoch": 0.03875102466651762,
        "step": 520
    },
    {
        "loss": 1.2398,
        "grad_norm": 3.7524383068084717,
        "learning_rate": 0.00019399508441722668,
        "epoch": 0.03882554586779939,
        "step": 521
    },
    {
        "loss": 2.671,
        "grad_norm": 1.692493200302124,
        "learning_rate": 0.00019397104465293194,
        "epoch": 0.038900067069081154,
        "step": 522
    },
    {
        "loss": 2.4242,
        "grad_norm": 1.429410696029663,
        "learning_rate": 0.000193946958360496,
        "epoch": 0.03897458827036292,
        "step": 523
    },
    {
        "loss": 2.7293,
        "grad_norm": 0.9418175220489502,
        "learning_rate": 0.0001939228255518448,
        "epoch": 0.03904910947164468,
        "step": 524
    },
    {
        "loss": 2.379,
        "grad_norm": 2.966787338256836,
        "learning_rate": 0.0001938986462389273,
        "epoch": 0.03912363067292645,
        "step": 525
    },
    {
        "loss": 2.8611,
        "grad_norm": 2.15598201751709,
        "learning_rate": 0.00019387442043371545,
        "epoch": 0.03919815187420821,
        "step": 526
    },
    {
        "loss": 2.5772,
        "grad_norm": 3.1004891395568848,
        "learning_rate": 0.00019385014814820428,
        "epoch": 0.039272673075489976,
        "step": 527
    },
    {
        "loss": 2.18,
        "grad_norm": 4.993495464324951,
        "learning_rate": 0.0001938258293944117,
        "epoch": 0.03934719427677174,
        "step": 528
    },
    {
        "loss": 2.053,
        "grad_norm": 3.197523593902588,
        "learning_rate": 0.00019380146418437882,
        "epoch": 0.03942171547805351,
        "step": 529
    },
    {
        "loss": 2.074,
        "grad_norm": 3.230523109436035,
        "learning_rate": 0.0001937770525301696,
        "epoch": 0.03949623667933527,
        "step": 530
    },
    {
        "loss": 1.9789,
        "grad_norm": 3.390925645828247,
        "learning_rate": 0.00019375259444387105,
        "epoch": 0.039570757880617036,
        "step": 531
    },
    {
        "loss": 2.7346,
        "grad_norm": 2.4761126041412354,
        "learning_rate": 0.00019372808993759318,
        "epoch": 0.0396452790818988,
        "step": 532
    },
    {
        "loss": 2.4736,
        "grad_norm": 2.017866373062134,
        "learning_rate": 0.000193703539023469,
        "epoch": 0.03971980028318056,
        "step": 533
    },
    {
        "loss": 2.5654,
        "grad_norm": 2.220566511154175,
        "learning_rate": 0.00019367894171365443,
        "epoch": 0.03979432148446233,
        "step": 534
    },
    {
        "loss": 2.6567,
        "grad_norm": 1.9343278408050537,
        "learning_rate": 0.00019365429802032839,
        "epoch": 0.039868842685744095,
        "step": 535
    },
    {
        "loss": 2.2537,
        "grad_norm": 6.121317386627197,
        "learning_rate": 0.00019362960795569283,
        "epoch": 0.03994336388702586,
        "step": 536
    },
    {
        "loss": 2.3359,
        "grad_norm": 4.288340091705322,
        "learning_rate": 0.00019360487153197257,
        "epoch": 0.04001788508830762,
        "step": 537
    },
    {
        "loss": 2.3311,
        "grad_norm": 3.0159640312194824,
        "learning_rate": 0.00019358008876141548,
        "epoch": 0.04009240628958939,
        "step": 538
    },
    {
        "loss": 2.633,
        "grad_norm": 3.4576375484466553,
        "learning_rate": 0.00019355525965629228,
        "epoch": 0.040166927490871154,
        "step": 539
    },
    {
        "loss": 2.5393,
        "grad_norm": 3.572695016860962,
        "learning_rate": 0.00019353038422889664,
        "epoch": 0.04024144869215292,
        "step": 540
    },
    {
        "loss": 2.8709,
        "grad_norm": 2.274613857269287,
        "learning_rate": 0.00019350546249154526,
        "epoch": 0.04031596989343468,
        "step": 541
    },
    {
        "loss": 2.1986,
        "grad_norm": 2.990856409072876,
        "learning_rate": 0.00019348049445657768,
        "epoch": 0.040390491094716444,
        "step": 542
    },
    {
        "loss": 2.2261,
        "grad_norm": 3.6579151153564453,
        "learning_rate": 0.00019345548013635638,
        "epoch": 0.040465012295998214,
        "step": 543
    },
    {
        "loss": 2.0851,
        "grad_norm": 2.443260431289673,
        "learning_rate": 0.0001934304195432668,
        "epoch": 0.04053953349727998,
        "step": 544
    },
    {
        "loss": 1.726,
        "grad_norm": 4.482163429260254,
        "learning_rate": 0.00019340531268971725,
        "epoch": 0.04061405469856174,
        "step": 545
    },
    {
        "loss": 2.2348,
        "grad_norm": 3.1493070125579834,
        "learning_rate": 0.00019338015958813892,
        "epoch": 0.0406885758998435,
        "step": 546
    },
    {
        "loss": 1.9944,
        "grad_norm": 2.7369191646575928,
        "learning_rate": 0.00019335496025098598,
        "epoch": 0.04076309710112527,
        "step": 547
    },
    {
        "loss": 2.4735,
        "grad_norm": 1.4753025770187378,
        "learning_rate": 0.00019332971469073544,
        "epoch": 0.040837618302407036,
        "step": 548
    },
    {
        "loss": 2.776,
        "grad_norm": 1.9247076511383057,
        "learning_rate": 0.00019330442291988717,
        "epoch": 0.0409121395036888,
        "step": 549
    },
    {
        "loss": 2.8207,
        "grad_norm": 2.303074359893799,
        "learning_rate": 0.000193279084950964,
        "epoch": 0.04098666070497056,
        "step": 550
    },
    {
        "loss": 2.5158,
        "grad_norm": 2.674471139907837,
        "learning_rate": 0.00019325370079651153,
        "epoch": 0.041061181906252325,
        "step": 551
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.8371378183364868,
        "learning_rate": 0.00019322827046909836,
        "epoch": 0.041135703107534095,
        "step": 552
    },
    {
        "loss": 3.0979,
        "grad_norm": 3.726863145828247,
        "learning_rate": 0.00019320279398131582,
        "epoch": 0.04121022430881586,
        "step": 553
    },
    {
        "loss": 2.6174,
        "grad_norm": 2.9117627143859863,
        "learning_rate": 0.0001931772713457782,
        "epoch": 0.04128474551009762,
        "step": 554
    },
    {
        "loss": 2.6467,
        "grad_norm": 3.4417927265167236,
        "learning_rate": 0.0001931517025751225,
        "epoch": 0.041359266711379385,
        "step": 555
    },
    {
        "loss": 2.8387,
        "grad_norm": 2.2159230709075928,
        "learning_rate": 0.00019312608768200877,
        "epoch": 0.041433787912661155,
        "step": 556
    },
    {
        "loss": 2.4665,
        "grad_norm": 2.3827619552612305,
        "learning_rate": 0.00019310042667911975,
        "epoch": 0.04150830911394292,
        "step": 557
    },
    {
        "loss": 2.7512,
        "grad_norm": 2.324162721633911,
        "learning_rate": 0.000193074719579161,
        "epoch": 0.04158283031522468,
        "step": 558
    },
    {
        "loss": 2.6506,
        "grad_norm": 3.437952756881714,
        "learning_rate": 0.00019304896639486094,
        "epoch": 0.041657351516506444,
        "step": 559
    },
    {
        "loss": 2.5818,
        "grad_norm": 2.1977689266204834,
        "learning_rate": 0.0001930231671389709,
        "epoch": 0.041731872717788214,
        "step": 560
    },
    {
        "loss": 1.9629,
        "grad_norm": 2.7416458129882812,
        "learning_rate": 0.00019299732182426484,
        "epoch": 0.04180639391906998,
        "step": 561
    },
    {
        "loss": 2.7967,
        "grad_norm": 2.339334487915039,
        "learning_rate": 0.00019297143046353968,
        "epoch": 0.04188091512035174,
        "step": 562
    },
    {
        "loss": 2.2482,
        "grad_norm": 2.3600025177001953,
        "learning_rate": 0.0001929454930696151,
        "epoch": 0.0419554363216335,
        "step": 563
    },
    {
        "loss": 2.3463,
        "grad_norm": 3.168267011642456,
        "learning_rate": 0.00019291950965533348,
        "epoch": 0.042029957522915266,
        "step": 564
    },
    {
        "loss": 2.5732,
        "grad_norm": 3.9380698204040527,
        "learning_rate": 0.0001928934802335601,
        "epoch": 0.042104478724197036,
        "step": 565
    },
    {
        "loss": 1.3881,
        "grad_norm": 3.829078197479248,
        "learning_rate": 0.000192867404817183,
        "epoch": 0.0421789999254788,
        "step": 566
    },
    {
        "loss": 2.615,
        "grad_norm": 2.246131420135498,
        "learning_rate": 0.00019284128341911293,
        "epoch": 0.04225352112676056,
        "step": 567
    },
    {
        "loss": 2.4628,
        "grad_norm": 2.96824049949646,
        "learning_rate": 0.0001928151160522835,
        "epoch": 0.042328042328042326,
        "step": 568
    },
    {
        "loss": 2.7394,
        "grad_norm": 2.714994192123413,
        "learning_rate": 0.00019278890272965096,
        "epoch": 0.042402563529324096,
        "step": 569
    },
    {
        "loss": 2.5649,
        "grad_norm": 2.6580047607421875,
        "learning_rate": 0.0001927626434641944,
        "epoch": 0.04247708473060586,
        "step": 570
    },
    {
        "loss": 2.9451,
        "grad_norm": 2.1876416206359863,
        "learning_rate": 0.00019273633826891572,
        "epoch": 0.04255160593188762,
        "step": 571
    },
    {
        "loss": 2.5781,
        "grad_norm": 2.4082515239715576,
        "learning_rate": 0.0001927099871568394,
        "epoch": 0.042626127133169385,
        "step": 572
    },
    {
        "loss": 3.1593,
        "grad_norm": 3.043159008026123,
        "learning_rate": 0.00019268359014101277,
        "epoch": 0.04270064833445115,
        "step": 573
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.8888471126556396,
        "learning_rate": 0.0001926571472345059,
        "epoch": 0.04277516953573292,
        "step": 574
    },
    {
        "loss": 2.6485,
        "grad_norm": 4.035520076751709,
        "learning_rate": 0.00019263065845041144,
        "epoch": 0.04284969073701468,
        "step": 575
    },
    {
        "loss": 2.7199,
        "grad_norm": 2.074985980987549,
        "learning_rate": 0.00019260412380184492,
        "epoch": 0.042924211938296444,
        "step": 576
    },
    {
        "loss": 1.974,
        "grad_norm": 3.3924012184143066,
        "learning_rate": 0.0001925775433019445,
        "epoch": 0.04299873313957821,
        "step": 577
    },
    {
        "loss": 2.3301,
        "grad_norm": 2.5119922161102295,
        "learning_rate": 0.00019255091696387106,
        "epoch": 0.04307325434085998,
        "step": 578
    },
    {
        "loss": 2.5868,
        "grad_norm": 2.4706990718841553,
        "learning_rate": 0.00019252424480080817,
        "epoch": 0.04314777554214174,
        "step": 579
    },
    {
        "loss": 2.0885,
        "grad_norm": 2.269355297088623,
        "learning_rate": 0.00019249752682596208,
        "epoch": 0.043222296743423504,
        "step": 580
    },
    {
        "loss": 3.4443,
        "grad_norm": 2.8595340251922607,
        "learning_rate": 0.00019247076305256176,
        "epoch": 0.04329681794470527,
        "step": 581
    },
    {
        "loss": 1.718,
        "grad_norm": 3.0289907455444336,
        "learning_rate": 0.0001924439534938588,
        "epoch": 0.04337133914598704,
        "step": 582
    },
    {
        "loss": 2.5631,
        "grad_norm": 2.6878271102905273,
        "learning_rate": 0.0001924170981631275,
        "epoch": 0.0434458603472688,
        "step": 583
    },
    {
        "loss": 2.2471,
        "grad_norm": 1.9804171323776245,
        "learning_rate": 0.00019239019707366483,
        "epoch": 0.04352038154855056,
        "step": 584
    },
    {
        "loss": 1.8079,
        "grad_norm": 2.837925672531128,
        "learning_rate": 0.0001923632502387904,
        "epoch": 0.043594902749832326,
        "step": 585
    },
    {
        "loss": 2.3641,
        "grad_norm": 2.419811248779297,
        "learning_rate": 0.00019233625767184643,
        "epoch": 0.04366942395111409,
        "step": 586
    },
    {
        "loss": 2.8776,
        "grad_norm": 1.7715750932693481,
        "learning_rate": 0.0001923092193861979,
        "epoch": 0.04374394515239586,
        "step": 587
    },
    {
        "loss": 2.5106,
        "grad_norm": 3.101853132247925,
        "learning_rate": 0.00019228213539523224,
        "epoch": 0.04381846635367762,
        "step": 588
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.391103744506836,
        "learning_rate": 0.00019225500571235974,
        "epoch": 0.043892987554959385,
        "step": 589
    },
    {
        "loss": 2.7357,
        "grad_norm": 1.8580659627914429,
        "learning_rate": 0.0001922278303510131,
        "epoch": 0.04396750875624115,
        "step": 590
    },
    {
        "loss": 2.6721,
        "grad_norm": 2.3432695865631104,
        "learning_rate": 0.0001922006093246478,
        "epoch": 0.04404202995752292,
        "step": 591
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.6090140342712402,
        "learning_rate": 0.0001921733426467418,
        "epoch": 0.04411655115880468,
        "step": 592
    },
    {
        "loss": 2.6133,
        "grad_norm": 2.0773096084594727,
        "learning_rate": 0.00019214603033079576,
        "epoch": 0.044191072360086445,
        "step": 593
    },
    {
        "loss": 2.1473,
        "grad_norm": 2.783141851425171,
        "learning_rate": 0.00019211867239033294,
        "epoch": 0.04426559356136821,
        "step": 594
    },
    {
        "loss": 2.3952,
        "grad_norm": 3.5392229557037354,
        "learning_rate": 0.0001920912688388991,
        "epoch": 0.04434011476264997,
        "step": 595
    },
    {
        "loss": 1.9271,
        "grad_norm": 2.967398166656494,
        "learning_rate": 0.0001920638196900626,
        "epoch": 0.04441463596393174,
        "step": 596
    },
    {
        "loss": 2.059,
        "grad_norm": 3.445831537246704,
        "learning_rate": 0.0001920363249574145,
        "epoch": 0.044489157165213504,
        "step": 597
    },
    {
        "loss": 2.2232,
        "grad_norm": 2.5624420642852783,
        "learning_rate": 0.00019200878465456826,
        "epoch": 0.04456367836649527,
        "step": 598
    },
    {
        "loss": 2.1164,
        "grad_norm": 2.847309112548828,
        "learning_rate": 0.00019198119879516005,
        "epoch": 0.04463819956777703,
        "step": 599
    },
    {
        "loss": 2.8713,
        "grad_norm": 3.247896194458008,
        "learning_rate": 0.00019195356739284852,
        "epoch": 0.0447127207690588,
        "step": 600
    },
    {
        "loss": 2.8685,
        "grad_norm": 2.077850103378296,
        "learning_rate": 0.00019192589046131486,
        "epoch": 0.04478724197034056,
        "step": 601
    },
    {
        "loss": 1.9199,
        "grad_norm": 3.5896213054656982,
        "learning_rate": 0.00019189816801426282,
        "epoch": 0.044861763171622326,
        "step": 602
    },
    {
        "loss": 2.5955,
        "grad_norm": 2.7993197441101074,
        "learning_rate": 0.00019187040006541872,
        "epoch": 0.04493628437290409,
        "step": 603
    },
    {
        "loss": 3.0913,
        "grad_norm": 1.952676773071289,
        "learning_rate": 0.00019184258662853138,
        "epoch": 0.04501080557418585,
        "step": 604
    },
    {
        "loss": 1.8199,
        "grad_norm": 3.4528443813323975,
        "learning_rate": 0.00019181472771737212,
        "epoch": 0.04508532677546762,
        "step": 605
    },
    {
        "loss": 2.9522,
        "grad_norm": 3.3731579780578613,
        "learning_rate": 0.00019178682334573482,
        "epoch": 0.045159847976749386,
        "step": 606
    },
    {
        "loss": 2.5564,
        "grad_norm": 3.4539759159088135,
        "learning_rate": 0.0001917588735274358,
        "epoch": 0.04523436917803115,
        "step": 607
    },
    {
        "loss": 3.0208,
        "grad_norm": 2.395451784133911,
        "learning_rate": 0.000191730878276314,
        "epoch": 0.04530889037931291,
        "step": 608
    },
    {
        "loss": 1.9795,
        "grad_norm": 2.5558547973632812,
        "learning_rate": 0.00019170283760623077,
        "epoch": 0.04538341158059468,
        "step": 609
    },
    {
        "loss": 2.0154,
        "grad_norm": 3.4896769523620605,
        "learning_rate": 0.0001916747515310699,
        "epoch": 0.045457932781876445,
        "step": 610
    },
    {
        "loss": 2.6648,
        "grad_norm": 2.4330339431762695,
        "learning_rate": 0.00019164662006473782,
        "epoch": 0.04553245398315821,
        "step": 611
    },
    {
        "loss": 2.3743,
        "grad_norm": 2.5142414569854736,
        "learning_rate": 0.00019161844322116326,
        "epoch": 0.04560697518443997,
        "step": 612
    },
    {
        "loss": 2.5444,
        "grad_norm": 2.301731824874878,
        "learning_rate": 0.00019159022101429755,
        "epoch": 0.04568149638572174,
        "step": 613
    },
    {
        "loss": 2.6774,
        "grad_norm": 2.3665411472320557,
        "learning_rate": 0.00019156195345811444,
        "epoch": 0.045756017587003504,
        "step": 614
    },
    {
        "loss": 2.4244,
        "grad_norm": 2.5244598388671875,
        "learning_rate": 0.00019153364056661003,
        "epoch": 0.04583053878828527,
        "step": 615
    },
    {
        "loss": 2.8083,
        "grad_norm": 2.0830066204071045,
        "learning_rate": 0.00019150528235380306,
        "epoch": 0.04590505998956703,
        "step": 616
    },
    {
        "loss": 2.1801,
        "grad_norm": 3.0461935997009277,
        "learning_rate": 0.00019147687883373454,
        "epoch": 0.045979581190848794,
        "step": 617
    },
    {
        "loss": 2.706,
        "grad_norm": 2.3798768520355225,
        "learning_rate": 0.00019144843002046806,
        "epoch": 0.046054102392130564,
        "step": 618
    },
    {
        "loss": 2.7913,
        "grad_norm": 2.986006498336792,
        "learning_rate": 0.0001914199359280895,
        "epoch": 0.04612862359341233,
        "step": 619
    },
    {
        "loss": 2.5199,
        "grad_norm": 2.9359970092773438,
        "learning_rate": 0.00019139139657070722,
        "epoch": 0.04620314479469409,
        "step": 620
    },
    {
        "loss": 2.1156,
        "grad_norm": 2.852924108505249,
        "learning_rate": 0.000191362811962452,
        "epoch": 0.04627766599597585,
        "step": 621
    },
    {
        "loss": 2.4311,
        "grad_norm": 2.68292236328125,
        "learning_rate": 0.000191334182117477,
        "epoch": 0.04635218719725762,
        "step": 622
    },
    {
        "loss": 2.1917,
        "grad_norm": 4.452510833740234,
        "learning_rate": 0.00019130550704995783,
        "epoch": 0.046426708398539386,
        "step": 623
    },
    {
        "loss": 2.0412,
        "grad_norm": 3.6487314701080322,
        "learning_rate": 0.00019127678677409245,
        "epoch": 0.04650122959982115,
        "step": 624
    },
    {
        "loss": 1.0385,
        "grad_norm": 3.168741464614868,
        "learning_rate": 0.00019124802130410118,
        "epoch": 0.04657575080110291,
        "step": 625
    },
    {
        "loss": 3.2386,
        "grad_norm": 3.257389545440674,
        "learning_rate": 0.00019121921065422677,
        "epoch": 0.046650272002384675,
        "step": 626
    },
    {
        "loss": 1.6607,
        "grad_norm": 2.4693453311920166,
        "learning_rate": 0.0001911903548387343,
        "epoch": 0.046724793203666445,
        "step": 627
    },
    {
        "loss": 1.7898,
        "grad_norm": 3.8511266708374023,
        "learning_rate": 0.00019116145387191124,
        "epoch": 0.04679931440494821,
        "step": 628
    },
    {
        "loss": 2.6021,
        "grad_norm": 2.4214706420898438,
        "learning_rate": 0.00019113250776806742,
        "epoch": 0.04687383560622997,
        "step": 629
    },
    {
        "loss": 2.3762,
        "grad_norm": 3.372847318649292,
        "learning_rate": 0.00019110351654153494,
        "epoch": 0.046948356807511735,
        "step": 630
    },
    {
        "loss": 1.9114,
        "grad_norm": 2.6052961349487305,
        "learning_rate": 0.00019107448020666838,
        "epoch": 0.047022878008793505,
        "step": 631
    },
    {
        "loss": 2.48,
        "grad_norm": 1.629142165184021,
        "learning_rate": 0.00019104539877784455,
        "epoch": 0.04709739921007527,
        "step": 632
    },
    {
        "loss": 1.9592,
        "grad_norm": 3.7358527183532715,
        "learning_rate": 0.0001910162722694626,
        "epoch": 0.04717192041135703,
        "step": 633
    },
    {
        "loss": 2.5417,
        "grad_norm": 2.6813466548919678,
        "learning_rate": 0.00019098710069594406,
        "epoch": 0.047246441612638794,
        "step": 634
    },
    {
        "loss": 2.7938,
        "grad_norm": 2.232626438140869,
        "learning_rate": 0.0001909578840717327,
        "epoch": 0.04732096281392056,
        "step": 635
    },
    {
        "loss": 1.6966,
        "grad_norm": 3.3030149936676025,
        "learning_rate": 0.00019092862241129464,
        "epoch": 0.04739548401520233,
        "step": 636
    },
    {
        "loss": 2.5362,
        "grad_norm": 3.443840980529785,
        "learning_rate": 0.0001908993157291183,
        "epoch": 0.04747000521648409,
        "step": 637
    },
    {
        "loss": 2.8714,
        "grad_norm": 1.7221183776855469,
        "learning_rate": 0.00019086996403971433,
        "epoch": 0.04754452641776585,
        "step": 638
    },
    {
        "loss": 2.65,
        "grad_norm": 1.9209966659545898,
        "learning_rate": 0.00019084056735761573,
        "epoch": 0.047619047619047616,
        "step": 639
    },
    {
        "loss": 3.0658,
        "grad_norm": 1.7187975645065308,
        "learning_rate": 0.0001908111256973778,
        "epoch": 0.047693568820329386,
        "step": 640
    },
    {
        "loss": 1.7769,
        "grad_norm": 3.0434091091156006,
        "learning_rate": 0.00019078163907357802,
        "epoch": 0.04776809002161115,
        "step": 641
    },
    {
        "loss": 1.7461,
        "grad_norm": 4.357077598571777,
        "learning_rate": 0.00019075210750081625,
        "epoch": 0.04784261122289291,
        "step": 642
    },
    {
        "loss": 2.3556,
        "grad_norm": 3.073944091796875,
        "learning_rate": 0.00019072253099371443,
        "epoch": 0.047917132424174676,
        "step": 643
    },
    {
        "loss": 2.307,
        "grad_norm": 4.003132343292236,
        "learning_rate": 0.00019069290956691697,
        "epoch": 0.047991653625456446,
        "step": 644
    },
    {
        "loss": 2.3943,
        "grad_norm": 3.052321434020996,
        "learning_rate": 0.00019066324323509034,
        "epoch": 0.04806617482673821,
        "step": 645
    },
    {
        "loss": 2.1664,
        "grad_norm": 2.9173617362976074,
        "learning_rate": 0.00019063353201292333,
        "epoch": 0.04814069602801997,
        "step": 646
    },
    {
        "loss": 1.6379,
        "grad_norm": 4.995972633361816,
        "learning_rate": 0.00019060377591512695,
        "epoch": 0.048215217229301735,
        "step": 647
    },
    {
        "loss": 2.6909,
        "grad_norm": 1.9604713916778564,
        "learning_rate": 0.00019057397495643439,
        "epoch": 0.0482897384305835,
        "step": 648
    },
    {
        "loss": 1.8346,
        "grad_norm": 4.521107196807861,
        "learning_rate": 0.00019054412915160113,
        "epoch": 0.04836425963186527,
        "step": 649
    },
    {
        "loss": 2.5906,
        "grad_norm": 2.349958896636963,
        "learning_rate": 0.00019051423851540472,
        "epoch": 0.04843878083314703,
        "step": 650
    },
    {
        "loss": 2.3396,
        "grad_norm": 3.4695777893066406,
        "learning_rate": 0.0001904843030626451,
        "epoch": 0.048513302034428794,
        "step": 651
    },
    {
        "loss": 2.4823,
        "grad_norm": 2.4996891021728516,
        "learning_rate": 0.0001904543228081442,
        "epoch": 0.04858782323571056,
        "step": 652
    },
    {
        "loss": 2.3315,
        "grad_norm": 2.6364691257476807,
        "learning_rate": 0.00019042429776674628,
        "epoch": 0.04866234443699233,
        "step": 653
    },
    {
        "loss": 2.1454,
        "grad_norm": 3.0776190757751465,
        "learning_rate": 0.00019039422795331771,
        "epoch": 0.04873686563827409,
        "step": 654
    },
    {
        "loss": 2.8108,
        "grad_norm": 2.3266940116882324,
        "learning_rate": 0.00019036411338274703,
        "epoch": 0.048811386839555854,
        "step": 655
    },
    {
        "loss": 2.0459,
        "grad_norm": 3.3539748191833496,
        "learning_rate": 0.00019033395406994498,
        "epoch": 0.04888590804083762,
        "step": 656
    },
    {
        "loss": 2.9294,
        "grad_norm": 3.411525011062622,
        "learning_rate": 0.00019030375002984437,
        "epoch": 0.04896042924211938,
        "step": 657
    },
    {
        "loss": 3.1327,
        "grad_norm": 2.794583797454834,
        "learning_rate": 0.00019027350127740026,
        "epoch": 0.04903495044340115,
        "step": 658
    },
    {
        "loss": 2.2451,
        "grad_norm": 2.148322582244873,
        "learning_rate": 0.00019024320782758976,
        "epoch": 0.04910947164468291,
        "step": 659
    },
    {
        "loss": 2.6554,
        "grad_norm": 3.1716160774230957,
        "learning_rate": 0.00019021286969541216,
        "epoch": 0.049183992845964676,
        "step": 660
    },
    {
        "loss": 1.8305,
        "grad_norm": 5.511232852935791,
        "learning_rate": 0.0001901824868958889,
        "epoch": 0.04925851404724644,
        "step": 661
    },
    {
        "loss": 2.2351,
        "grad_norm": 3.3441684246063232,
        "learning_rate": 0.00019015205944406343,
        "epoch": 0.04933303524852821,
        "step": 662
    },
    {
        "loss": 1.9504,
        "grad_norm": 3.0925865173339844,
        "learning_rate": 0.00019012158735500143,
        "epoch": 0.04940755644980997,
        "step": 663
    },
    {
        "loss": 1.6208,
        "grad_norm": 3.412278175354004,
        "learning_rate": 0.0001900910706437906,
        "epoch": 0.049482077651091735,
        "step": 664
    },
    {
        "loss": 2.8939,
        "grad_norm": 2.0285091400146484,
        "learning_rate": 0.00019006050932554082,
        "epoch": 0.0495565988523735,
        "step": 665
    },
    {
        "loss": 2.2767,
        "grad_norm": 3.328993558883667,
        "learning_rate": 0.0001900299034153839,
        "epoch": 0.04963112005365526,
        "step": 666
    },
    {
        "loss": 2.252,
        "grad_norm": 3.0760157108306885,
        "learning_rate": 0.0001899992529284739,
        "epoch": 0.04970564125493703,
        "step": 667
    },
    {
        "loss": 2.3483,
        "grad_norm": 2.716825008392334,
        "learning_rate": 0.00018996855787998686,
        "epoch": 0.049780162456218795,
        "step": 668
    },
    {
        "loss": 2.389,
        "grad_norm": 4.477259159088135,
        "learning_rate": 0.00018993781828512092,
        "epoch": 0.04985468365750056,
        "step": 669
    },
    {
        "loss": 2.627,
        "grad_norm": 2.7748496532440186,
        "learning_rate": 0.00018990703415909624,
        "epoch": 0.04992920485878232,
        "step": 670
    },
    {
        "loss": 2.3768,
        "grad_norm": 2.761319160461426,
        "learning_rate": 0.00018987620551715506,
        "epoch": 0.05000372606006409,
        "step": 671
    },
    {
        "loss": 2.7324,
        "grad_norm": 3.5008983612060547,
        "learning_rate": 0.00018984533237456167,
        "epoch": 0.050078247261345854,
        "step": 672
    },
    {
        "loss": 1.3839,
        "grad_norm": 4.438934326171875,
        "learning_rate": 0.00018981441474660228,
        "epoch": 0.05015276846262762,
        "step": 673
    },
    {
        "loss": 2.6013,
        "grad_norm": 2.2630553245544434,
        "learning_rate": 0.00018978345264858534,
        "epoch": 0.05022728966390938,
        "step": 674
    },
    {
        "loss": 2.7724,
        "grad_norm": 3.347623586654663,
        "learning_rate": 0.00018975244609584116,
        "epoch": 0.05030181086519115,
        "step": 675
    },
    {
        "loss": 3.0559,
        "grad_norm": 3.882988929748535,
        "learning_rate": 0.00018972139510372205,
        "epoch": 0.05037633206647291,
        "step": 676
    },
    {
        "loss": 2.4139,
        "grad_norm": 1.5870767831802368,
        "learning_rate": 0.00018969029968760243,
        "epoch": 0.050450853267754676,
        "step": 677
    },
    {
        "loss": 2.1393,
        "grad_norm": 2.0878872871398926,
        "learning_rate": 0.00018965915986287867,
        "epoch": 0.05052537446903644,
        "step": 678
    },
    {
        "loss": 2.4869,
        "grad_norm": 3.2722220420837402,
        "learning_rate": 0.00018962797564496905,
        "epoch": 0.0505998956703182,
        "step": 679
    },
    {
        "loss": 2.7519,
        "grad_norm": 3.171849250793457,
        "learning_rate": 0.00018959674704931392,
        "epoch": 0.05067441687159997,
        "step": 680
    },
    {
        "loss": 2.788,
        "grad_norm": 3.0003483295440674,
        "learning_rate": 0.0001895654740913756,
        "epoch": 0.050748938072881736,
        "step": 681
    },
    {
        "loss": 2.5304,
        "grad_norm": 3.4965124130249023,
        "learning_rate": 0.00018953415678663838,
        "epoch": 0.0508234592741635,
        "step": 682
    },
    {
        "loss": 2.7486,
        "grad_norm": 1.4084163904190063,
        "learning_rate": 0.00018950279515060842,
        "epoch": 0.05089798047544526,
        "step": 683
    },
    {
        "loss": 2.2874,
        "grad_norm": 2.2271621227264404,
        "learning_rate": 0.00018947138919881394,
        "epoch": 0.05097250167672703,
        "step": 684
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.629605531692505,
        "learning_rate": 0.00018943993894680503,
        "epoch": 0.051047022878008795,
        "step": 685
    },
    {
        "loss": 2.5225,
        "grad_norm": 2.314326286315918,
        "learning_rate": 0.00018940844441015376,
        "epoch": 0.05112154407929056,
        "step": 686
    },
    {
        "loss": 2.0032,
        "grad_norm": 3.2119336128234863,
        "learning_rate": 0.00018937690560445408,
        "epoch": 0.05119606528057232,
        "step": 687
    },
    {
        "loss": 2.5107,
        "grad_norm": 2.604419469833374,
        "learning_rate": 0.00018934532254532194,
        "epoch": 0.051270586481854084,
        "step": 688
    },
    {
        "loss": 1.621,
        "grad_norm": 5.0423054695129395,
        "learning_rate": 0.00018931369524839508,
        "epoch": 0.051345107683135854,
        "step": 689
    },
    {
        "loss": 3.1024,
        "grad_norm": 3.192375898361206,
        "learning_rate": 0.00018928202372933326,
        "epoch": 0.05141962888441762,
        "step": 690
    },
    {
        "loss": 2.6646,
        "grad_norm": 3.286630868911743,
        "learning_rate": 0.00018925030800381803,
        "epoch": 0.05149415008569938,
        "step": 691
    },
    {
        "loss": 2.3605,
        "grad_norm": 2.0430591106414795,
        "learning_rate": 0.00018921854808755294,
        "epoch": 0.051568671286981144,
        "step": 692
    },
    {
        "loss": 2.522,
        "grad_norm": 2.736621618270874,
        "learning_rate": 0.00018918674399626332,
        "epoch": 0.051643192488262914,
        "step": 693
    },
    {
        "loss": 3.0126,
        "grad_norm": 2.812807559967041,
        "learning_rate": 0.00018915489574569647,
        "epoch": 0.05171771368954468,
        "step": 694
    },
    {
        "loss": 2.6803,
        "grad_norm": 2.572401762008667,
        "learning_rate": 0.00018912300335162144,
        "epoch": 0.05179223489082644,
        "step": 695
    },
    {
        "loss": 1.8885,
        "grad_norm": 2.630031108856201,
        "learning_rate": 0.00018909106682982928,
        "epoch": 0.0518667560921082,
        "step": 696
    },
    {
        "loss": 2.5207,
        "grad_norm": 2.2527542114257812,
        "learning_rate": 0.00018905908619613272,
        "epoch": 0.051941277293389966,
        "step": 697
    },
    {
        "loss": 1.5671,
        "grad_norm": 5.084735870361328,
        "learning_rate": 0.00018902706146636645,
        "epoch": 0.052015798494671736,
        "step": 698
    },
    {
        "loss": 2.6068,
        "grad_norm": 2.3563950061798096,
        "learning_rate": 0.00018899499265638702,
        "epoch": 0.0520903196959535,
        "step": 699
    },
    {
        "loss": 2.2324,
        "grad_norm": 2.4353978633880615,
        "learning_rate": 0.00018896287978207264,
        "epoch": 0.05216484089723526,
        "step": 700
    },
    {
        "loss": 2.0852,
        "grad_norm": 2.3313257694244385,
        "learning_rate": 0.00018893072285932353,
        "epoch": 0.052239362098517025,
        "step": 701
    },
    {
        "loss": 2.2747,
        "grad_norm": 2.8497185707092285,
        "learning_rate": 0.00018889852190406164,
        "epoch": 0.052313883299798795,
        "step": 702
    },
    {
        "loss": 1.6313,
        "grad_norm": 3.4354171752929688,
        "learning_rate": 0.00018886627693223064,
        "epoch": 0.05238840450108056,
        "step": 703
    },
    {
        "loss": 2.1867,
        "grad_norm": 2.914384603500366,
        "learning_rate": 0.00018883398795979613,
        "epoch": 0.05246292570236232,
        "step": 704
    },
    {
        "loss": 2.1489,
        "grad_norm": 3.196564197540283,
        "learning_rate": 0.0001888016550027454,
        "epoch": 0.052537446903644085,
        "step": 705
    },
    {
        "loss": 2.5097,
        "grad_norm": 4.086974620819092,
        "learning_rate": 0.0001887692780770876,
        "epoch": 0.052611968104925855,
        "step": 706
    },
    {
        "loss": 3.2416,
        "grad_norm": 3.1125106811523438,
        "learning_rate": 0.0001887368571988536,
        "epoch": 0.05268648930620762,
        "step": 707
    },
    {
        "loss": 2.111,
        "grad_norm": 4.0439300537109375,
        "learning_rate": 0.00018870439238409601,
        "epoch": 0.05276101050748938,
        "step": 708
    },
    {
        "loss": 2.8153,
        "grad_norm": 2.7549710273742676,
        "learning_rate": 0.00018867188364888923,
        "epoch": 0.052835531708771144,
        "step": 709
    },
    {
        "loss": 2.602,
        "grad_norm": 2.6487178802490234,
        "learning_rate": 0.00018863933100932942,
        "epoch": 0.05291005291005291,
        "step": 710
    },
    {
        "loss": 2.6032,
        "grad_norm": 3.383706569671631,
        "learning_rate": 0.00018860673448153445,
        "epoch": 0.05298457411133468,
        "step": 711
    },
    {
        "loss": 2.5279,
        "grad_norm": 2.6805434226989746,
        "learning_rate": 0.00018857409408164392,
        "epoch": 0.05305909531261644,
        "step": 712
    },
    {
        "loss": 2.7109,
        "grad_norm": 2.7428412437438965,
        "learning_rate": 0.00018854140982581914,
        "epoch": 0.0531336165138982,
        "step": 713
    },
    {
        "loss": 2.643,
        "grad_norm": 2.29689359664917,
        "learning_rate": 0.0001885086817302432,
        "epoch": 0.053208137715179966,
        "step": 714
    },
    {
        "loss": 2.3631,
        "grad_norm": 2.750502347946167,
        "learning_rate": 0.00018847590981112085,
        "epoch": 0.053282658916461736,
        "step": 715
    },
    {
        "loss": 3.027,
        "grad_norm": 1.966687798500061,
        "learning_rate": 0.0001884430940846785,
        "epoch": 0.0533571801177435,
        "step": 716
    },
    {
        "loss": 1.714,
        "grad_norm": 3.600165605545044,
        "learning_rate": 0.0001884102345671643,
        "epoch": 0.05343170131902526,
        "step": 717
    },
    {
        "loss": 2.6613,
        "grad_norm": 2.5396125316619873,
        "learning_rate": 0.0001883773312748481,
        "epoch": 0.053506222520307026,
        "step": 718
    },
    {
        "loss": 2.3115,
        "grad_norm": 3.5353140830993652,
        "learning_rate": 0.00018834438422402137,
        "epoch": 0.05358074372158879,
        "step": 719
    },
    {
        "loss": 2.7547,
        "grad_norm": 1.9080028533935547,
        "learning_rate": 0.0001883113934309973,
        "epoch": 0.05365526492287056,
        "step": 720
    },
    {
        "loss": 2.4356,
        "grad_norm": 2.395599126815796,
        "learning_rate": 0.00018827835891211067,
        "epoch": 0.05372978612415232,
        "step": 721
    },
    {
        "loss": 2.2556,
        "grad_norm": 2.669354200363159,
        "learning_rate": 0.00018824528068371802,
        "epoch": 0.053804307325434085,
        "step": 722
    },
    {
        "loss": 2.5877,
        "grad_norm": 2.112480878829956,
        "learning_rate": 0.0001882121587621974,
        "epoch": 0.05387882852671585,
        "step": 723
    },
    {
        "loss": 3.0029,
        "grad_norm": 1.6040393114089966,
        "learning_rate": 0.00018817899316394857,
        "epoch": 0.05395334972799762,
        "step": 724
    },
    {
        "loss": 3.0239,
        "grad_norm": 1.9035133123397827,
        "learning_rate": 0.00018814578390539293,
        "epoch": 0.05402787092927938,
        "step": 725
    },
    {
        "loss": 2.0909,
        "grad_norm": 3.681213855743408,
        "learning_rate": 0.00018811253100297343,
        "epoch": 0.054102392130561144,
        "step": 726
    },
    {
        "loss": 2.5964,
        "grad_norm": 3.2343525886535645,
        "learning_rate": 0.00018807923447315475,
        "epoch": 0.05417691333184291,
        "step": 727
    },
    {
        "loss": 2.4292,
        "grad_norm": 2.3002243041992188,
        "learning_rate": 0.000188045894332423,
        "epoch": 0.05425143453312467,
        "step": 728
    },
    {
        "loss": 2.5806,
        "grad_norm": 2.7966818809509277,
        "learning_rate": 0.00018801251059728604,
        "epoch": 0.05432595573440644,
        "step": 729
    },
    {
        "loss": 3.011,
        "grad_norm": 2.86787486076355,
        "learning_rate": 0.0001879790832842732,
        "epoch": 0.054400476935688204,
        "step": 730
    },
    {
        "loss": 1.9676,
        "grad_norm": 4.947239398956299,
        "learning_rate": 0.00018794561240993547,
        "epoch": 0.05447499813696997,
        "step": 731
    },
    {
        "loss": 2.3569,
        "grad_norm": 2.9189534187316895,
        "learning_rate": 0.00018791209799084538,
        "epoch": 0.05454951933825173,
        "step": 732
    },
    {
        "loss": 2.1494,
        "grad_norm": 2.9471352100372314,
        "learning_rate": 0.00018787854004359697,
        "epoch": 0.0546240405395335,
        "step": 733
    },
    {
        "loss": 2.5315,
        "grad_norm": 5.9750657081604,
        "learning_rate": 0.00018784493858480596,
        "epoch": 0.05469856174081526,
        "step": 734
    },
    {
        "loss": 2.5766,
        "grad_norm": 3.4577534198760986,
        "learning_rate": 0.00018781129363110947,
        "epoch": 0.054773082942097026,
        "step": 735
    },
    {
        "loss": 2.5466,
        "grad_norm": 2.010512590408325,
        "learning_rate": 0.0001877776051991662,
        "epoch": 0.05484760414337879,
        "step": 736
    },
    {
        "loss": 1.5411,
        "grad_norm": 3.511547565460205,
        "learning_rate": 0.00018774387330565645,
        "epoch": 0.05492212534466056,
        "step": 737
    },
    {
        "loss": 2.6651,
        "grad_norm": 1.6867848634719849,
        "learning_rate": 0.000187710097967282,
        "epoch": 0.05499664654594232,
        "step": 738
    },
    {
        "loss": 2.468,
        "grad_norm": 3.174180269241333,
        "learning_rate": 0.00018767627920076603,
        "epoch": 0.055071167747224085,
        "step": 739
    },
    {
        "loss": 2.7302,
        "grad_norm": 2.858346939086914,
        "learning_rate": 0.00018764241702285341,
        "epoch": 0.05514568894850585,
        "step": 740
    },
    {
        "loss": 2.8744,
        "grad_norm": 1.370592713356018,
        "learning_rate": 0.00018760851145031038,
        "epoch": 0.05522021014978761,
        "step": 741
    },
    {
        "loss": 2.3932,
        "grad_norm": 1.9655592441558838,
        "learning_rate": 0.00018757456249992468,
        "epoch": 0.05529473135106938,
        "step": 742
    },
    {
        "loss": 2.6262,
        "grad_norm": 1.9881610870361328,
        "learning_rate": 0.0001875405701885056,
        "epoch": 0.055369252552351145,
        "step": 743
    },
    {
        "loss": 2.3056,
        "grad_norm": 2.539868116378784,
        "learning_rate": 0.00018750653453288382,
        "epoch": 0.05544377375363291,
        "step": 744
    },
    {
        "loss": 2.5949,
        "grad_norm": 1.9599827527999878,
        "learning_rate": 0.00018747245554991148,
        "epoch": 0.05551829495491467,
        "step": 745
    },
    {
        "loss": 2.3746,
        "grad_norm": 3.366689920425415,
        "learning_rate": 0.00018743833325646222,
        "epoch": 0.05559281615619644,
        "step": 746
    },
    {
        "loss": 2.7205,
        "grad_norm": 2.4641361236572266,
        "learning_rate": 0.0001874041676694311,
        "epoch": 0.055667337357478204,
        "step": 747
    },
    {
        "loss": 2.6231,
        "grad_norm": 3.9222753047943115,
        "learning_rate": 0.00018736995880573468,
        "epoch": 0.05574185855875997,
        "step": 748
    },
    {
        "loss": 2.5939,
        "grad_norm": 2.4134204387664795,
        "learning_rate": 0.0001873357066823108,
        "epoch": 0.05581637976004173,
        "step": 749
    },
    {
        "loss": 3.2101,
        "grad_norm": 2.6108174324035645,
        "learning_rate": 0.00018730141131611882,
        "epoch": 0.05589090096132349,
        "step": 750
    },
    {
        "loss": 2.2527,
        "grad_norm": 3.2830584049224854,
        "learning_rate": 0.00018726707272413956,
        "epoch": 0.05596542216260526,
        "step": 751
    },
    {
        "loss": 2.6388,
        "grad_norm": 2.366410255432129,
        "learning_rate": 0.0001872326909233751,
        "epoch": 0.056039943363887026,
        "step": 752
    },
    {
        "loss": 2.6878,
        "grad_norm": 2.0482964515686035,
        "learning_rate": 0.0001871982659308491,
        "epoch": 0.05611446456516879,
        "step": 753
    },
    {
        "loss": 2.6947,
        "grad_norm": 1.9849662780761719,
        "learning_rate": 0.0001871637977636064,
        "epoch": 0.05618898576645055,
        "step": 754
    },
    {
        "loss": 2.8344,
        "grad_norm": 2.322608470916748,
        "learning_rate": 0.0001871292864387134,
        "epoch": 0.05626350696773232,
        "step": 755
    },
    {
        "loss": 2.541,
        "grad_norm": 2.0965640544891357,
        "learning_rate": 0.00018709473197325768,
        "epoch": 0.056338028169014086,
        "step": 756
    },
    {
        "loss": 2.6067,
        "grad_norm": 2.3038012981414795,
        "learning_rate": 0.0001870601343843484,
        "epoch": 0.05641254937029585,
        "step": 757
    },
    {
        "loss": 2.1227,
        "grad_norm": 3.17830753326416,
        "learning_rate": 0.0001870254936891159,
        "epoch": 0.05648707057157761,
        "step": 758
    },
    {
        "loss": 2.5482,
        "grad_norm": 2.0519518852233887,
        "learning_rate": 0.00018699080990471192,
        "epoch": 0.056561591772859375,
        "step": 759
    },
    {
        "loss": 2.6084,
        "grad_norm": 3.001574754714966,
        "learning_rate": 0.00018695608304830958,
        "epoch": 0.056636112974141145,
        "step": 760
    },
    {
        "loss": 1.9442,
        "grad_norm": 3.660414218902588,
        "learning_rate": 0.00018692131313710326,
        "epoch": 0.05671063417542291,
        "step": 761
    },
    {
        "loss": 2.6981,
        "grad_norm": 2.106217861175537,
        "learning_rate": 0.00018688650018830865,
        "epoch": 0.05678515537670467,
        "step": 762
    },
    {
        "loss": 2.1526,
        "grad_norm": 2.2734458446502686,
        "learning_rate": 0.0001868516442191628,
        "epoch": 0.056859676577986434,
        "step": 763
    },
    {
        "loss": 2.6157,
        "grad_norm": 3.146232843399048,
        "learning_rate": 0.0001868167452469241,
        "epoch": 0.056934197779268204,
        "step": 764
    },
    {
        "loss": 2.5099,
        "grad_norm": 2.598602056503296,
        "learning_rate": 0.0001867818032888721,
        "epoch": 0.05700871898054997,
        "step": 765
    },
    {
        "loss": 2.8236,
        "grad_norm": 2.0030148029327393,
        "learning_rate": 0.0001867468183623077,
        "epoch": 0.05708324018183173,
        "step": 766
    },
    {
        "loss": 2.045,
        "grad_norm": 2.356858730316162,
        "learning_rate": 0.00018671179048455314,
        "epoch": 0.057157761383113494,
        "step": 767
    },
    {
        "loss": 2.7484,
        "grad_norm": 1.88496732711792,
        "learning_rate": 0.00018667671967295183,
        "epoch": 0.057232282584395264,
        "step": 768
    },
    {
        "loss": 2.9476,
        "grad_norm": 3.2237749099731445,
        "learning_rate": 0.00018664160594486853,
        "epoch": 0.05730680378567703,
        "step": 769
    },
    {
        "loss": 2.4773,
        "grad_norm": 3.0072035789489746,
        "learning_rate": 0.00018660644931768912,
        "epoch": 0.05738132498695879,
        "step": 770
    },
    {
        "loss": 2.589,
        "grad_norm": 3.2996511459350586,
        "learning_rate": 0.00018657124980882083,
        "epoch": 0.05745584618824055,
        "step": 771
    },
    {
        "loss": 3.0256,
        "grad_norm": 2.375279664993286,
        "learning_rate": 0.0001865360074356921,
        "epoch": 0.057530367389522316,
        "step": 772
    },
    {
        "loss": 2.0284,
        "grad_norm": 3.468709945678711,
        "learning_rate": 0.0001865007222157526,
        "epoch": 0.057604888590804086,
        "step": 773
    },
    {
        "loss": 2.9811,
        "grad_norm": 1.9893146753311157,
        "learning_rate": 0.00018646539416647314,
        "epoch": 0.05767940979208585,
        "step": 774
    },
    {
        "loss": 2.5256,
        "grad_norm": 1.6397147178649902,
        "learning_rate": 0.00018643002330534584,
        "epoch": 0.05775393099336761,
        "step": 775
    },
    {
        "loss": 1.691,
        "grad_norm": 3.3267831802368164,
        "learning_rate": 0.00018639460964988398,
        "epoch": 0.057828452194649375,
        "step": 776
    },
    {
        "loss": 3.4901,
        "grad_norm": 2.3168904781341553,
        "learning_rate": 0.00018635915321762198,
        "epoch": 0.057902973395931145,
        "step": 777
    },
    {
        "loss": 2.704,
        "grad_norm": 2.9897570610046387,
        "learning_rate": 0.0001863236540261155,
        "epoch": 0.05797749459721291,
        "step": 778
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.7856653928756714,
        "learning_rate": 0.00018628811209294138,
        "epoch": 0.05805201579849467,
        "step": 779
    },
    {
        "loss": 2.7229,
        "grad_norm": 2.452610492706299,
        "learning_rate": 0.00018625252743569753,
        "epoch": 0.058126536999776435,
        "step": 780
    },
    {
        "loss": 2.0579,
        "grad_norm": 2.712528944015503,
        "learning_rate": 0.00018621690007200313,
        "epoch": 0.0582010582010582,
        "step": 781
    },
    {
        "loss": 2.7946,
        "grad_norm": 3.155545234680176,
        "learning_rate": 0.00018618123001949846,
        "epoch": 0.05827557940233997,
        "step": 782
    },
    {
        "loss": 2.9558,
        "grad_norm": 3.3696863651275635,
        "learning_rate": 0.0001861455172958449,
        "epoch": 0.05835010060362173,
        "step": 783
    },
    {
        "loss": 2.5134,
        "grad_norm": 2.3021562099456787,
        "learning_rate": 0.00018610976191872497,
        "epoch": 0.058424621804903494,
        "step": 784
    },
    {
        "loss": 2.3997,
        "grad_norm": 2.117804527282715,
        "learning_rate": 0.00018607396390584238,
        "epoch": 0.05849914300618526,
        "step": 785
    },
    {
        "loss": 2.9612,
        "grad_norm": 2.5196523666381836,
        "learning_rate": 0.0001860381232749219,
        "epoch": 0.05857366420746703,
        "step": 786
    },
    {
        "loss": 2.217,
        "grad_norm": 3.0405452251434326,
        "learning_rate": 0.00018600224004370936,
        "epoch": 0.05864818540874879,
        "step": 787
    },
    {
        "loss": 3.0034,
        "grad_norm": 2.512240171432495,
        "learning_rate": 0.00018596631422997172,
        "epoch": 0.05872270661003055,
        "step": 788
    },
    {
        "loss": 3.392,
        "grad_norm": 3.11975359916687,
        "learning_rate": 0.00018593034585149708,
        "epoch": 0.058797227811312316,
        "step": 789
    },
    {
        "loss": 2.8694,
        "grad_norm": 2.1249070167541504,
        "learning_rate": 0.00018589433492609451,
        "epoch": 0.05887174901259408,
        "step": 790
    },
    {
        "loss": 2.4185,
        "grad_norm": 3.442230224609375,
        "learning_rate": 0.0001858582814715942,
        "epoch": 0.05894627021387585,
        "step": 791
    },
    {
        "loss": 2.7057,
        "grad_norm": 1.956094741821289,
        "learning_rate": 0.00018582218550584746,
        "epoch": 0.05902079141515761,
        "step": 792
    },
    {
        "loss": 2.3686,
        "grad_norm": 3.6634981632232666,
        "learning_rate": 0.0001857860470467265,
        "epoch": 0.059095312616439376,
        "step": 793
    },
    {
        "loss": 2.1008,
        "grad_norm": 3.6967902183532715,
        "learning_rate": 0.0001857498661121247,
        "epoch": 0.05916983381772114,
        "step": 794
    },
    {
        "loss": 2.1851,
        "grad_norm": 2.888362169265747,
        "learning_rate": 0.00018571364271995644,
        "epoch": 0.05924435501900291,
        "step": 795
    },
    {
        "loss": 2.8884,
        "grad_norm": 2.4446208477020264,
        "learning_rate": 0.00018567737688815705,
        "epoch": 0.05931887622028467,
        "step": 796
    },
    {
        "loss": 2.4401,
        "grad_norm": 1.8997734785079956,
        "learning_rate": 0.00018564106863468296,
        "epoch": 0.059393397421566435,
        "step": 797
    },
    {
        "loss": 2.6402,
        "grad_norm": 2.9326953887939453,
        "learning_rate": 0.0001856047179775116,
        "epoch": 0.0594679186228482,
        "step": 798
    },
    {
        "loss": 2.9928,
        "grad_norm": 2.3620288372039795,
        "learning_rate": 0.0001855683249346414,
        "epoch": 0.05954243982412997,
        "step": 799
    },
    {
        "loss": 2.9381,
        "grad_norm": 2.416018486022949,
        "learning_rate": 0.00018553188952409163,
        "epoch": 0.05961696102541173,
        "step": 800
    },
    {
        "loss": 2.7479,
        "grad_norm": 2.1450600624084473,
        "learning_rate": 0.00018549541176390272,
        "epoch": 0.059691482226693494,
        "step": 801
    },
    {
        "loss": 2.8955,
        "grad_norm": 3.2619645595550537,
        "learning_rate": 0.00018545889167213603,
        "epoch": 0.05976600342797526,
        "step": 802
    },
    {
        "loss": 2.7565,
        "grad_norm": 1.3969019651412964,
        "learning_rate": 0.00018542232926687383,
        "epoch": 0.05984052462925702,
        "step": 803
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.5813541412353516,
        "learning_rate": 0.00018538572456621935,
        "epoch": 0.05991504583053879,
        "step": 804
    },
    {
        "loss": 2.567,
        "grad_norm": 1.318678617477417,
        "learning_rate": 0.0001853490775882968,
        "epoch": 0.059989567031820554,
        "step": 805
    },
    {
        "loss": 2.7343,
        "grad_norm": 1.628915548324585,
        "learning_rate": 0.0001853123883512513,
        "epoch": 0.06006408823310232,
        "step": 806
    },
    {
        "loss": 2.2822,
        "grad_norm": 2.6438815593719482,
        "learning_rate": 0.00018527565687324887,
        "epoch": 0.06013860943438408,
        "step": 807
    },
    {
        "loss": 2.5874,
        "grad_norm": 3.0632035732269287,
        "learning_rate": 0.00018523888317247645,
        "epoch": 0.06021313063566585,
        "step": 808
    },
    {
        "loss": 2.4854,
        "grad_norm": 2.068392038345337,
        "learning_rate": 0.000185202067267142,
        "epoch": 0.06028765183694761,
        "step": 809
    },
    {
        "loss": 2.5426,
        "grad_norm": 2.4416983127593994,
        "learning_rate": 0.00018516520917547416,
        "epoch": 0.060362173038229376,
        "step": 810
    },
    {
        "loss": 2.2003,
        "grad_norm": 2.043020486831665,
        "learning_rate": 0.00018512830891572265,
        "epoch": 0.06043669423951114,
        "step": 811
    },
    {
        "loss": 1.9569,
        "grad_norm": 3.1146280765533447,
        "learning_rate": 0.00018509136650615796,
        "epoch": 0.0605112154407929,
        "step": 812
    },
    {
        "loss": 2.5352,
        "grad_norm": 3.6291916370391846,
        "learning_rate": 0.00018505438196507153,
        "epoch": 0.06058573664207467,
        "step": 813
    },
    {
        "loss": 2.8803,
        "grad_norm": 3.7681727409362793,
        "learning_rate": 0.0001850173553107756,
        "epoch": 0.060660257843356435,
        "step": 814
    },
    {
        "loss": 2.4689,
        "grad_norm": 2.443258285522461,
        "learning_rate": 0.00018498028656160323,
        "epoch": 0.0607347790446382,
        "step": 815
    },
    {
        "loss": 2.4475,
        "grad_norm": 2.4258646965026855,
        "learning_rate": 0.0001849431757359084,
        "epoch": 0.06080930024591996,
        "step": 816
    },
    {
        "loss": 2.1572,
        "grad_norm": 2.6734695434570312,
        "learning_rate": 0.00018490602285206595,
        "epoch": 0.06088382144720173,
        "step": 817
    },
    {
        "loss": 2.8179,
        "grad_norm": 2.0444419384002686,
        "learning_rate": 0.00018486882792847142,
        "epoch": 0.060958342648483495,
        "step": 818
    },
    {
        "loss": 2.9702,
        "grad_norm": 1.9381530284881592,
        "learning_rate": 0.00018483159098354126,
        "epoch": 0.06103286384976526,
        "step": 819
    },
    {
        "loss": 2.4736,
        "grad_norm": 2.656740188598633,
        "learning_rate": 0.0001847943120357127,
        "epoch": 0.06110738505104702,
        "step": 820
    },
    {
        "loss": 2.5544,
        "grad_norm": 1.9598431587219238,
        "learning_rate": 0.00018475699110344379,
        "epoch": 0.06118190625232879,
        "step": 821
    },
    {
        "loss": 1.8924,
        "grad_norm": 2.991513252258301,
        "learning_rate": 0.00018471962820521332,
        "epoch": 0.061256427453610554,
        "step": 822
    },
    {
        "loss": 2.9781,
        "grad_norm": 2.7659099102020264,
        "learning_rate": 0.00018468222335952088,
        "epoch": 0.06133094865489232,
        "step": 823
    },
    {
        "loss": 2.1578,
        "grad_norm": 1.6996341943740845,
        "learning_rate": 0.00018464477658488684,
        "epoch": 0.06140546985617408,
        "step": 824
    },
    {
        "loss": 1.8164,
        "grad_norm": 3.3784608840942383,
        "learning_rate": 0.00018460728789985233,
        "epoch": 0.06147999105745584,
        "step": 825
    },
    {
        "loss": 2.1055,
        "grad_norm": 2.6776397228240967,
        "learning_rate": 0.00018456975732297922,
        "epoch": 0.06155451225873761,
        "step": 826
    },
    {
        "loss": 2.6488,
        "grad_norm": 2.6534290313720703,
        "learning_rate": 0.00018453218487285013,
        "epoch": 0.061629033460019376,
        "step": 827
    },
    {
        "loss": 2.2358,
        "grad_norm": 3.3719089031219482,
        "learning_rate": 0.0001844945705680684,
        "epoch": 0.06170355466130114,
        "step": 828
    },
    {
        "loss": 2.5912,
        "grad_norm": 1.825371503829956,
        "learning_rate": 0.00018445691442725814,
        "epoch": 0.0617780758625829,
        "step": 829
    },
    {
        "loss": 2.1261,
        "grad_norm": 3.4743030071258545,
        "learning_rate": 0.0001844192164690641,
        "epoch": 0.06185259706386467,
        "step": 830
    },
    {
        "loss": 2.2766,
        "grad_norm": 3.0839455127716064,
        "learning_rate": 0.00018438147671215175,
        "epoch": 0.061927118265146436,
        "step": 831
    },
    {
        "loss": 2.5539,
        "grad_norm": 2.398038387298584,
        "learning_rate": 0.00018434369517520736,
        "epoch": 0.0620016394664282,
        "step": 832
    },
    {
        "loss": 2.185,
        "grad_norm": 2.927436351776123,
        "learning_rate": 0.00018430587187693776,
        "epoch": 0.06207616066770996,
        "step": 833
    },
    {
        "loss": 3.0054,
        "grad_norm": 2.7538363933563232,
        "learning_rate": 0.0001842680068360705,
        "epoch": 0.062150681868991725,
        "step": 834
    },
    {
        "loss": 2.5039,
        "grad_norm": 2.1180951595306396,
        "learning_rate": 0.00018423010007135378,
        "epoch": 0.062225203070273495,
        "step": 835
    },
    {
        "loss": 2.7756,
        "grad_norm": 2.679382801055908,
        "learning_rate": 0.00018419215160155652,
        "epoch": 0.06229972427155526,
        "step": 836
    },
    {
        "loss": 2.6287,
        "grad_norm": 1.9663925170898438,
        "learning_rate": 0.00018415416144546825,
        "epoch": 0.06237424547283702,
        "step": 837
    },
    {
        "loss": 2.4437,
        "grad_norm": 1.864439845085144,
        "learning_rate": 0.00018411612962189914,
        "epoch": 0.062448766674118784,
        "step": 838
    },
    {
        "loss": 2.0292,
        "grad_norm": 2.963121175765991,
        "learning_rate": 0.00018407805614967994,
        "epoch": 0.06252328787540055,
        "step": 839
    },
    {
        "loss": 2.234,
        "grad_norm": 2.230893611907959,
        "learning_rate": 0.00018403994104766212,
        "epoch": 0.06259780907668232,
        "step": 840
    },
    {
        "loss": 1.6975,
        "grad_norm": 2.7843942642211914,
        "learning_rate": 0.00018400178433471771,
        "epoch": 0.06267233027796408,
        "step": 841
    },
    {
        "loss": 3.153,
        "grad_norm": 1.7697865962982178,
        "learning_rate": 0.00018396358602973934,
        "epoch": 0.06274685147924584,
        "step": 842
    },
    {
        "loss": 2.6478,
        "grad_norm": 2.5344152450561523,
        "learning_rate": 0.00018392534615164027,
        "epoch": 0.0628213726805276,
        "step": 843
    },
    {
        "loss": 1.6867,
        "grad_norm": 5.312929153442383,
        "learning_rate": 0.0001838870647193543,
        "epoch": 0.06289589388180937,
        "step": 844
    },
    {
        "loss": 2.0815,
        "grad_norm": 4.299020290374756,
        "learning_rate": 0.00018384874175183575,
        "epoch": 0.06297041508309113,
        "step": 845
    },
    {
        "loss": 2.8382,
        "grad_norm": 3.2630455493927,
        "learning_rate": 0.00018381037726805966,
        "epoch": 0.06304493628437291,
        "step": 846
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.8869892358779907,
        "learning_rate": 0.00018377197128702157,
        "epoch": 0.06311945748565467,
        "step": 847
    },
    {
        "loss": 2.1596,
        "grad_norm": 3.101982593536377,
        "learning_rate": 0.00018373352382773742,
        "epoch": 0.06319397868693644,
        "step": 848
    },
    {
        "loss": 2.7658,
        "grad_norm": 2.2390530109405518,
        "learning_rate": 0.00018369503490924395,
        "epoch": 0.0632684998882182,
        "step": 849
    },
    {
        "loss": 2.4887,
        "grad_norm": 2.6829769611358643,
        "learning_rate": 0.00018365650455059817,
        "epoch": 0.06334302108949996,
        "step": 850
    },
    {
        "loss": 2.558,
        "grad_norm": 2.121109962463379,
        "learning_rate": 0.00018361793277087772,
        "epoch": 0.06341754229078173,
        "step": 851
    },
    {
        "loss": 2.2061,
        "grad_norm": 2.54978609085083,
        "learning_rate": 0.00018357931958918082,
        "epoch": 0.06349206349206349,
        "step": 852
    },
    {
        "loss": 2.5718,
        "grad_norm": 1.6912744045257568,
        "learning_rate": 0.0001835406650246261,
        "epoch": 0.06356658469334525,
        "step": 853
    },
    {
        "loss": 1.6521,
        "grad_norm": 3.0982179641723633,
        "learning_rate": 0.00018350196909635268,
        "epoch": 0.06364110589462701,
        "step": 854
    },
    {
        "loss": 2.4913,
        "grad_norm": 2.364340305328369,
        "learning_rate": 0.00018346323182352023,
        "epoch": 0.06371562709590879,
        "step": 855
    },
    {
        "loss": 2.6007,
        "grad_norm": 2.492661476135254,
        "learning_rate": 0.00018342445322530878,
        "epoch": 0.06379014829719055,
        "step": 856
    },
    {
        "loss": 2.9744,
        "grad_norm": 4.202767848968506,
        "learning_rate": 0.00018338563332091892,
        "epoch": 0.06386466949847232,
        "step": 857
    },
    {
        "loss": 2.6849,
        "grad_norm": 1.9934194087982178,
        "learning_rate": 0.00018334677212957163,
        "epoch": 0.06393919069975408,
        "step": 858
    },
    {
        "loss": 2.8468,
        "grad_norm": 1.8366949558258057,
        "learning_rate": 0.00018330786967050836,
        "epoch": 0.06401371190103584,
        "step": 859
    },
    {
        "loss": 2.5141,
        "grad_norm": 2.3753578662872314,
        "learning_rate": 0.00018326892596299104,
        "epoch": 0.06408823310231761,
        "step": 860
    },
    {
        "loss": 3.033,
        "grad_norm": 2.8207576274871826,
        "learning_rate": 0.00018322994102630192,
        "epoch": 0.06416275430359937,
        "step": 861
    },
    {
        "loss": 3.09,
        "grad_norm": 3.6661038398742676,
        "learning_rate": 0.00018319091487974375,
        "epoch": 0.06423727550488113,
        "step": 862
    },
    {
        "loss": 2.8543,
        "grad_norm": 1.9417062997817993,
        "learning_rate": 0.00018315184754263964,
        "epoch": 0.0643117967061629,
        "step": 863
    },
    {
        "loss": 2.6824,
        "grad_norm": 2.9122185707092285,
        "learning_rate": 0.0001831127390343331,
        "epoch": 0.06438631790744467,
        "step": 864
    },
    {
        "loss": 1.9142,
        "grad_norm": 3.2265024185180664,
        "learning_rate": 0.00018307358937418803,
        "epoch": 0.06446083910872644,
        "step": 865
    },
    {
        "loss": 2.5628,
        "grad_norm": 2.640301465988159,
        "learning_rate": 0.00018303439858158876,
        "epoch": 0.0645353603100082,
        "step": 866
    },
    {
        "loss": 2.4042,
        "grad_norm": 2.765883445739746,
        "learning_rate": 0.00018299516667593987,
        "epoch": 0.06460988151128996,
        "step": 867
    },
    {
        "loss": 2.4611,
        "grad_norm": 2.509875535964966,
        "learning_rate": 0.0001829558936766664,
        "epoch": 0.06468440271257173,
        "step": 868
    },
    {
        "loss": 2.6884,
        "grad_norm": 1.7493253946304321,
        "learning_rate": 0.00018291657960321369,
        "epoch": 0.06475892391385349,
        "step": 869
    },
    {
        "loss": 2.7966,
        "grad_norm": 2.0951666831970215,
        "learning_rate": 0.00018287722447504737,
        "epoch": 0.06483344511513525,
        "step": 870
    },
    {
        "loss": 2.3586,
        "grad_norm": 2.971557378768921,
        "learning_rate": 0.00018283782831165354,
        "epoch": 0.06490796631641702,
        "step": 871
    },
    {
        "loss": 1.9283,
        "grad_norm": 2.314375162124634,
        "learning_rate": 0.00018279839113253851,
        "epoch": 0.06498248751769879,
        "step": 872
    },
    {
        "loss": 2.8151,
        "grad_norm": 2.0968940258026123,
        "learning_rate": 0.0001827589129572289,
        "epoch": 0.06505700871898056,
        "step": 873
    },
    {
        "loss": 2.7545,
        "grad_norm": 2.01250958442688,
        "learning_rate": 0.00018271939380527162,
        "epoch": 0.06513152992026232,
        "step": 874
    },
    {
        "loss": 2.4175,
        "grad_norm": 3.856560230255127,
        "learning_rate": 0.00018267983369623395,
        "epoch": 0.06520605112154408,
        "step": 875
    },
    {
        "loss": 3.0701,
        "grad_norm": 2.5277998447418213,
        "learning_rate": 0.00018264023264970335,
        "epoch": 0.06528057232282584,
        "step": 876
    },
    {
        "loss": 2.1317,
        "grad_norm": 3.904057741165161,
        "learning_rate": 0.00018260059068528762,
        "epoch": 0.06535509352410761,
        "step": 877
    },
    {
        "loss": 2.2026,
        "grad_norm": 3.1138010025024414,
        "learning_rate": 0.00018256090782261478,
        "epoch": 0.06542961472538937,
        "step": 878
    },
    {
        "loss": 2.9666,
        "grad_norm": 3.308342218399048,
        "learning_rate": 0.00018252118408133315,
        "epoch": 0.06550413592667113,
        "step": 879
    },
    {
        "loss": 1.8497,
        "grad_norm": 3.2436041831970215,
        "learning_rate": 0.0001824814194811112,
        "epoch": 0.0655786571279529,
        "step": 880
    },
    {
        "loss": 2.8808,
        "grad_norm": 1.8705841302871704,
        "learning_rate": 0.00018244161404163775,
        "epoch": 0.06565317832923467,
        "step": 881
    },
    {
        "loss": 2.361,
        "grad_norm": 3.2912230491638184,
        "learning_rate": 0.00018240176778262175,
        "epoch": 0.06572769953051644,
        "step": 882
    },
    {
        "loss": 2.2415,
        "grad_norm": 2.890225887298584,
        "learning_rate": 0.00018236188072379234,
        "epoch": 0.0658022207317982,
        "step": 883
    },
    {
        "loss": 1.9123,
        "grad_norm": 2.710749626159668,
        "learning_rate": 0.00018232195288489896,
        "epoch": 0.06587674193307996,
        "step": 884
    },
    {
        "loss": 2.5823,
        "grad_norm": 1.9311808347702026,
        "learning_rate": 0.00018228198428571116,
        "epoch": 0.06595126313436173,
        "step": 885
    },
    {
        "loss": 2.128,
        "grad_norm": 2.3060240745544434,
        "learning_rate": 0.00018224197494601875,
        "epoch": 0.06602578433564349,
        "step": 886
    },
    {
        "loss": 2.0388,
        "grad_norm": 3.5253026485443115,
        "learning_rate": 0.00018220192488563165,
        "epoch": 0.06610030553692525,
        "step": 887
    },
    {
        "loss": 2.687,
        "grad_norm": 2.4282307624816895,
        "learning_rate": 0.0001821618341243799,
        "epoch": 0.06617482673820702,
        "step": 888
    },
    {
        "loss": 2.5876,
        "grad_norm": 2.641751289367676,
        "learning_rate": 0.00018212170268211382,
        "epoch": 0.06624934793948878,
        "step": 889
    },
    {
        "loss": 2.1061,
        "grad_norm": 2.4688236713409424,
        "learning_rate": 0.00018208153057870373,
        "epoch": 0.06632386914077056,
        "step": 890
    },
    {
        "loss": 2.3482,
        "grad_norm": 2.8882830142974854,
        "learning_rate": 0.00018204131783404023,
        "epoch": 0.06639839034205232,
        "step": 891
    },
    {
        "loss": 2.4846,
        "grad_norm": 2.426542282104492,
        "learning_rate": 0.00018200106446803393,
        "epoch": 0.06647291154333408,
        "step": 892
    },
    {
        "loss": 2.3283,
        "grad_norm": 4.390859127044678,
        "learning_rate": 0.00018196077050061557,
        "epoch": 0.06654743274461584,
        "step": 893
    },
    {
        "loss": 2.5576,
        "grad_norm": 3.0318145751953125,
        "learning_rate": 0.00018192043595173605,
        "epoch": 0.06662195394589761,
        "step": 894
    },
    {
        "loss": 1.6365,
        "grad_norm": 2.0301923751831055,
        "learning_rate": 0.0001818800608413663,
        "epoch": 0.06669647514717937,
        "step": 895
    },
    {
        "loss": 2.1196,
        "grad_norm": 2.577279806137085,
        "learning_rate": 0.0001818396451894974,
        "epoch": 0.06677099634846113,
        "step": 896
    },
    {
        "loss": 2.6806,
        "grad_norm": 3.171748399734497,
        "learning_rate": 0.0001817991890161404,
        "epoch": 0.0668455175497429,
        "step": 897
    },
    {
        "loss": 2.386,
        "grad_norm": 3.4300997257232666,
        "learning_rate": 0.00018175869234132648,
        "epoch": 0.06692003875102466,
        "step": 898
    },
    {
        "loss": 2.6915,
        "grad_norm": 3.426682710647583,
        "learning_rate": 0.00018171815518510696,
        "epoch": 0.06699455995230644,
        "step": 899
    },
    {
        "loss": 2.2737,
        "grad_norm": 2.943094491958618,
        "learning_rate": 0.00018167757756755304,
        "epoch": 0.0670690811535882,
        "step": 900
    },
    {
        "loss": 2.2754,
        "grad_norm": 2.962388038635254,
        "learning_rate": 0.000181636959508756,
        "epoch": 0.06714360235486996,
        "step": 901
    },
    {
        "loss": 2.111,
        "grad_norm": 4.820944309234619,
        "learning_rate": 0.0001815963010288272,
        "epoch": 0.06721812355615173,
        "step": 902
    },
    {
        "loss": 2.9395,
        "grad_norm": 2.9885754585266113,
        "learning_rate": 0.00018155560214789805,
        "epoch": 0.06729264475743349,
        "step": 903
    },
    {
        "loss": 2.5737,
        "grad_norm": 3.2245635986328125,
        "learning_rate": 0.0001815148628861198,
        "epoch": 0.06736716595871525,
        "step": 904
    },
    {
        "loss": 2.4389,
        "grad_norm": 2.6043167114257812,
        "learning_rate": 0.00018147408326366383,
        "epoch": 0.06744168715999702,
        "step": 905
    },
    {
        "loss": 2.7355,
        "grad_norm": 2.0474376678466797,
        "learning_rate": 0.0001814332633007215,
        "epoch": 0.06751620836127878,
        "step": 906
    },
    {
        "loss": 1.9618,
        "grad_norm": 3.0246076583862305,
        "learning_rate": 0.00018139240301750405,
        "epoch": 0.06759072956256054,
        "step": 907
    },
    {
        "loss": 2.1422,
        "grad_norm": 2.8998589515686035,
        "learning_rate": 0.0001813515024342428,
        "epoch": 0.06766525076384232,
        "step": 908
    },
    {
        "loss": 2.8674,
        "grad_norm": 2.0481507778167725,
        "learning_rate": 0.00018131056157118892,
        "epoch": 0.06773977196512408,
        "step": 909
    },
    {
        "loss": 2.6048,
        "grad_norm": 2.9031789302825928,
        "learning_rate": 0.00018126958044861361,
        "epoch": 0.06781429316640585,
        "step": 910
    },
    {
        "loss": 2.5682,
        "grad_norm": 2.0868234634399414,
        "learning_rate": 0.00018122855908680796,
        "epoch": 0.06788881436768761,
        "step": 911
    },
    {
        "loss": 2.2939,
        "grad_norm": 2.000343084335327,
        "learning_rate": 0.00018118749750608296,
        "epoch": 0.06796333556896937,
        "step": 912
    },
    {
        "loss": 3.0558,
        "grad_norm": 2.933159828186035,
        "learning_rate": 0.00018114639572676958,
        "epoch": 0.06803785677025113,
        "step": 913
    },
    {
        "loss": 2.4625,
        "grad_norm": 3.7298989295959473,
        "learning_rate": 0.00018110525376921862,
        "epoch": 0.0681123779715329,
        "step": 914
    },
    {
        "loss": 2.9701,
        "grad_norm": 3.7275924682617188,
        "learning_rate": 0.00018106407165380083,
        "epoch": 0.06818689917281466,
        "step": 915
    },
    {
        "loss": 1.7958,
        "grad_norm": 3.699202537536621,
        "learning_rate": 0.00018102284940090685,
        "epoch": 0.06826142037409642,
        "step": 916
    },
    {
        "loss": 2.3878,
        "grad_norm": 2.366013526916504,
        "learning_rate": 0.0001809815870309471,
        "epoch": 0.0683359415753782,
        "step": 917
    },
    {
        "loss": 2.8851,
        "grad_norm": 4.015712261199951,
        "learning_rate": 0.00018094028456435202,
        "epoch": 0.06841046277665996,
        "step": 918
    },
    {
        "loss": 2.3934,
        "grad_norm": 2.2858753204345703,
        "learning_rate": 0.00018089894202157173,
        "epoch": 0.06848498397794173,
        "step": 919
    },
    {
        "loss": 2.3522,
        "grad_norm": 3.476969003677368,
        "learning_rate": 0.0001808575594230763,
        "epoch": 0.06855950517922349,
        "step": 920
    },
    {
        "loss": 1.9268,
        "grad_norm": 2.6832363605499268,
        "learning_rate": 0.00018081613678935565,
        "epoch": 0.06863402638050525,
        "step": 921
    },
    {
        "loss": 2.6884,
        "grad_norm": 2.393131732940674,
        "learning_rate": 0.00018077467414091944,
        "epoch": 0.06870854758178702,
        "step": 922
    },
    {
        "loss": 2.4656,
        "grad_norm": 2.066380500793457,
        "learning_rate": 0.0001807331714982972,
        "epoch": 0.06878306878306878,
        "step": 923
    },
    {
        "loss": 2.3296,
        "grad_norm": 2.3169519901275635,
        "learning_rate": 0.00018069162888203826,
        "epoch": 0.06885758998435054,
        "step": 924
    },
    {
        "loss": 1.7529,
        "grad_norm": 3.281447172164917,
        "learning_rate": 0.0001806500463127117,
        "epoch": 0.06893211118563232,
        "step": 925
    },
    {
        "loss": 1.5948,
        "grad_norm": 3.7711024284362793,
        "learning_rate": 0.00018060842381090644,
        "epoch": 0.06900663238691408,
        "step": 926
    },
    {
        "loss": 2.7246,
        "grad_norm": 3.069161891937256,
        "learning_rate": 0.00018056676139723113,
        "epoch": 0.06908115358819585,
        "step": 927
    },
    {
        "loss": 2.2815,
        "grad_norm": 3.007061243057251,
        "learning_rate": 0.00018052505909231417,
        "epoch": 0.06915567478947761,
        "step": 928
    },
    {
        "loss": 2.5175,
        "grad_norm": 1.9940392971038818,
        "learning_rate": 0.00018048331691680378,
        "epoch": 0.06923019599075937,
        "step": 929
    },
    {
        "loss": 2.6737,
        "grad_norm": 2.728374719619751,
        "learning_rate": 0.00018044153489136787,
        "epoch": 0.06930471719204113,
        "step": 930
    },
    {
        "loss": 2.5258,
        "grad_norm": 3.7025914192199707,
        "learning_rate": 0.00018039971303669407,
        "epoch": 0.0693792383933229,
        "step": 931
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.6959266662597656,
        "learning_rate": 0.00018035785137348974,
        "epoch": 0.06945375959460466,
        "step": 932
    },
    {
        "loss": 3.1794,
        "grad_norm": 4.262856960296631,
        "learning_rate": 0.00018031594992248202,
        "epoch": 0.06952828079588642,
        "step": 933
    },
    {
        "loss": 2.3994,
        "grad_norm": 3.1247916221618652,
        "learning_rate": 0.00018027400870441763,
        "epoch": 0.0696028019971682,
        "step": 934
    },
    {
        "loss": 2.6391,
        "grad_norm": 2.085514783859253,
        "learning_rate": 0.00018023202774006305,
        "epoch": 0.06967732319844996,
        "step": 935
    },
    {
        "loss": 2.8361,
        "grad_norm": 2.271756887435913,
        "learning_rate": 0.00018019000705020444,
        "epoch": 0.06975184439973173,
        "step": 936
    },
    {
        "loss": 3.2432,
        "grad_norm": 2.0191423892974854,
        "learning_rate": 0.0001801479466556476,
        "epoch": 0.06982636560101349,
        "step": 937
    },
    {
        "loss": 2.6564,
        "grad_norm": 2.748185873031616,
        "learning_rate": 0.00018010584657721806,
        "epoch": 0.06990088680229525,
        "step": 938
    },
    {
        "loss": 2.1391,
        "grad_norm": 3.0609824657440186,
        "learning_rate": 0.0001800637068357609,
        "epoch": 0.06997540800357702,
        "step": 939
    },
    {
        "loss": 1.2803,
        "grad_norm": 2.530740737915039,
        "learning_rate": 0.00018002152745214093,
        "epoch": 0.07004992920485878,
        "step": 940
    },
    {
        "loss": 2.7874,
        "grad_norm": 2.964468479156494,
        "learning_rate": 0.0001799793084472425,
        "epoch": 0.07012445040614054,
        "step": 941
    },
    {
        "loss": 2.5554,
        "grad_norm": 2.0403177738189697,
        "learning_rate": 0.0001799370498419696,
        "epoch": 0.0701989716074223,
        "step": 942
    },
    {
        "loss": 2.8096,
        "grad_norm": 3.1829442977905273,
        "learning_rate": 0.0001798947516572459,
        "epoch": 0.07027349280870408,
        "step": 943
    },
    {
        "loss": 2.2327,
        "grad_norm": 2.7165017127990723,
        "learning_rate": 0.00017985241391401462,
        "epoch": 0.07034801400998585,
        "step": 944
    },
    {
        "loss": 2.3277,
        "grad_norm": 1.9339346885681152,
        "learning_rate": 0.00017981003663323857,
        "epoch": 0.07042253521126761,
        "step": 945
    },
    {
        "loss": 2.6703,
        "grad_norm": 2.8586924076080322,
        "learning_rate": 0.00017976761983590006,
        "epoch": 0.07049705641254937,
        "step": 946
    },
    {
        "loss": 2.4788,
        "grad_norm": 3.7102532386779785,
        "learning_rate": 0.00017972516354300107,
        "epoch": 0.07057157761383114,
        "step": 947
    },
    {
        "loss": 2.8364,
        "grad_norm": 1.7574405670166016,
        "learning_rate": 0.00017968266777556316,
        "epoch": 0.0706460988151129,
        "step": 948
    },
    {
        "loss": 3.0662,
        "grad_norm": 2.4373044967651367,
        "learning_rate": 0.0001796401325546273,
        "epoch": 0.07072062001639466,
        "step": 949
    },
    {
        "loss": 1.9653,
        "grad_norm": 2.7878401279449463,
        "learning_rate": 0.0001795975579012541,
        "epoch": 0.07079514121767642,
        "step": 950
    },
    {
        "loss": 2.0108,
        "grad_norm": 3.1459457874298096,
        "learning_rate": 0.00017955494383652365,
        "epoch": 0.07086966241895819,
        "step": 951
    },
    {
        "loss": 2.3872,
        "grad_norm": 2.9883615970611572,
        "learning_rate": 0.00017951229038153557,
        "epoch": 0.07094418362023996,
        "step": 952
    },
    {
        "loss": 2.8481,
        "grad_norm": 2.4990429878234863,
        "learning_rate": 0.00017946959755740904,
        "epoch": 0.07101870482152173,
        "step": 953
    },
    {
        "loss": 2.6423,
        "grad_norm": 2.3948309421539307,
        "learning_rate": 0.00017942686538528258,
        "epoch": 0.07109322602280349,
        "step": 954
    },
    {
        "loss": 2.3338,
        "grad_norm": 2.0824227333068848,
        "learning_rate": 0.00017938409388631437,
        "epoch": 0.07116774722408525,
        "step": 955
    },
    {
        "loss": 1.9124,
        "grad_norm": 3.6930270195007324,
        "learning_rate": 0.00017934128308168192,
        "epoch": 0.07124226842536702,
        "step": 956
    },
    {
        "loss": 2.2608,
        "grad_norm": 2.635761260986328,
        "learning_rate": 0.0001792984329925823,
        "epoch": 0.07131678962664878,
        "step": 957
    },
    {
        "loss": 2.0346,
        "grad_norm": 2.5356950759887695,
        "learning_rate": 0.00017925554364023197,
        "epoch": 0.07139131082793054,
        "step": 958
    },
    {
        "loss": 2.4392,
        "grad_norm": 2.9939770698547363,
        "learning_rate": 0.00017921261504586685,
        "epoch": 0.0714658320292123,
        "step": 959
    },
    {
        "loss": 1.464,
        "grad_norm": 3.8317129611968994,
        "learning_rate": 0.00017916964723074235,
        "epoch": 0.07154035323049407,
        "step": 960
    },
    {
        "loss": 2.4021,
        "grad_norm": 2.1649720668792725,
        "learning_rate": 0.00017912664021613317,
        "epoch": 0.07161487443177585,
        "step": 961
    },
    {
        "loss": 2.7298,
        "grad_norm": 3.8073670864105225,
        "learning_rate": 0.0001790835940233335,
        "epoch": 0.07168939563305761,
        "step": 962
    },
    {
        "loss": 2.6901,
        "grad_norm": 2.5654191970825195,
        "learning_rate": 0.00017904050867365693,
        "epoch": 0.07176391683433937,
        "step": 963
    },
    {
        "loss": 1.5109,
        "grad_norm": 3.094486713409424,
        "learning_rate": 0.00017899738418843646,
        "epoch": 0.07183843803562114,
        "step": 964
    },
    {
        "loss": 2.847,
        "grad_norm": 1.4803500175476074,
        "learning_rate": 0.0001789542205890244,
        "epoch": 0.0719129592369029,
        "step": 965
    },
    {
        "loss": 2.5439,
        "grad_norm": 1.6257023811340332,
        "learning_rate": 0.00017891101789679247,
        "epoch": 0.07198748043818466,
        "step": 966
    },
    {
        "loss": 2.5686,
        "grad_norm": 2.712292194366455,
        "learning_rate": 0.00017886777613313176,
        "epoch": 0.07206200163946642,
        "step": 967
    },
    {
        "loss": 2.4777,
        "grad_norm": 2.793912172317505,
        "learning_rate": 0.00017882449531945262,
        "epoch": 0.07213652284074819,
        "step": 968
    },
    {
        "loss": 2.243,
        "grad_norm": 2.5456480979919434,
        "learning_rate": 0.00017878117547718487,
        "epoch": 0.07221104404202995,
        "step": 969
    },
    {
        "loss": 1.9542,
        "grad_norm": 2.7857117652893066,
        "learning_rate": 0.00017873781662777755,
        "epoch": 0.07228556524331173,
        "step": 970
    },
    {
        "loss": 2.9463,
        "grad_norm": 2.3138785362243652,
        "learning_rate": 0.00017869441879269903,
        "epoch": 0.07236008644459349,
        "step": 971
    },
    {
        "loss": 2.275,
        "grad_norm": 3.282135248184204,
        "learning_rate": 0.00017865098199343704,
        "epoch": 0.07243460764587525,
        "step": 972
    },
    {
        "loss": 2.3673,
        "grad_norm": 1.7808257341384888,
        "learning_rate": 0.00017860750625149852,
        "epoch": 0.07250912884715702,
        "step": 973
    },
    {
        "loss": 2.4459,
        "grad_norm": 3.2425358295440674,
        "learning_rate": 0.00017856399158840973,
        "epoch": 0.07258365004843878,
        "step": 974
    },
    {
        "loss": 2.5987,
        "grad_norm": 2.0152502059936523,
        "learning_rate": 0.00017852043802571628,
        "epoch": 0.07265817124972054,
        "step": 975
    },
    {
        "loss": 2.8347,
        "grad_norm": 4.6550679206848145,
        "learning_rate": 0.00017847684558498287,
        "epoch": 0.0727326924510023,
        "step": 976
    },
    {
        "loss": 1.2579,
        "grad_norm": 3.714905023574829,
        "learning_rate": 0.00017843321428779357,
        "epoch": 0.07280721365228407,
        "step": 977
    },
    {
        "loss": 3.0917,
        "grad_norm": 2.9242711067199707,
        "learning_rate": 0.00017838954415575172,
        "epoch": 0.07288173485356583,
        "step": 978
    },
    {
        "loss": 2.1265,
        "grad_norm": 3.724465847015381,
        "learning_rate": 0.00017834583521047977,
        "epoch": 0.07295625605484761,
        "step": 979
    },
    {
        "loss": 2.064,
        "grad_norm": 4.134457588195801,
        "learning_rate": 0.00017830208747361952,
        "epoch": 0.07303077725612937,
        "step": 980
    },
    {
        "loss": 2.3638,
        "grad_norm": 2.9524619579315186,
        "learning_rate": 0.0001782583009668318,
        "epoch": 0.07310529845741114,
        "step": 981
    },
    {
        "loss": 2.9167,
        "grad_norm": 2.692584276199341,
        "learning_rate": 0.00017821447571179683,
        "epoch": 0.0731798196586929,
        "step": 982
    },
    {
        "loss": 2.7646,
        "grad_norm": 2.6114261150360107,
        "learning_rate": 0.0001781706117302139,
        "epoch": 0.07325434085997466,
        "step": 983
    },
    {
        "loss": 2.3779,
        "grad_norm": 2.8181774616241455,
        "learning_rate": 0.00017812670904380148,
        "epoch": 0.07332886206125643,
        "step": 984
    },
    {
        "loss": 2.4149,
        "grad_norm": 3.6505651473999023,
        "learning_rate": 0.0001780827676742973,
        "epoch": 0.07340338326253819,
        "step": 985
    },
    {
        "loss": 2.3248,
        "grad_norm": 2.751537322998047,
        "learning_rate": 0.00017803878764345807,
        "epoch": 0.07347790446381995,
        "step": 986
    },
    {
        "loss": 2.54,
        "grad_norm": 2.045137643814087,
        "learning_rate": 0.00017799476897305982,
        "epoch": 0.07355242566510173,
        "step": 987
    },
    {
        "loss": 2.8062,
        "grad_norm": 3.1439766883850098,
        "learning_rate": 0.0001779507116848976,
        "epoch": 0.07362694686638349,
        "step": 988
    },
    {
        "loss": 2.4493,
        "grad_norm": 2.0291218757629395,
        "learning_rate": 0.00017790661580078564,
        "epoch": 0.07370146806766525,
        "step": 989
    },
    {
        "loss": 2.6456,
        "grad_norm": 2.590541124343872,
        "learning_rate": 0.00017786248134255724,
        "epoch": 0.07377598926894702,
        "step": 990
    },
    {
        "loss": 1.8617,
        "grad_norm": 2.9457809925079346,
        "learning_rate": 0.00017781830833206485,
        "epoch": 0.07385051047022878,
        "step": 991
    },
    {
        "loss": 2.5141,
        "grad_norm": 2.729297161102295,
        "learning_rate": 0.0001777740967911799,
        "epoch": 0.07392503167151054,
        "step": 992
    },
    {
        "loss": 1.5506,
        "grad_norm": 3.0955817699432373,
        "learning_rate": 0.00017772984674179303,
        "epoch": 0.0739995528727923,
        "step": 993
    },
    {
        "loss": 2.2033,
        "grad_norm": 2.3024444580078125,
        "learning_rate": 0.00017768555820581386,
        "epoch": 0.07407407407407407,
        "step": 994
    },
    {
        "loss": 2.7274,
        "grad_norm": 3.077704668045044,
        "learning_rate": 0.00017764123120517114,
        "epoch": 0.07414859527535583,
        "step": 995
    },
    {
        "loss": 1.7114,
        "grad_norm": 3.7248082160949707,
        "learning_rate": 0.00017759686576181255,
        "epoch": 0.07422311647663761,
        "step": 996
    },
    {
        "loss": 2.6425,
        "grad_norm": 1.7948182821273804,
        "learning_rate": 0.0001775524618977049,
        "epoch": 0.07429763767791937,
        "step": 997
    },
    {
        "loss": 2.8001,
        "grad_norm": 2.4297635555267334,
        "learning_rate": 0.00017750801963483404,
        "epoch": 0.07437215887920114,
        "step": 998
    },
    {
        "loss": 1.642,
        "grad_norm": 3.1939656734466553,
        "learning_rate": 0.00017746353899520477,
        "epoch": 0.0744466800804829,
        "step": 999
    },
    {
        "loss": 2.3045,
        "grad_norm": 3.300156831741333,
        "learning_rate": 0.00017741902000084086,
        "epoch": 0.07452120128176466,
        "step": 1000
    },
    {
        "loss": 3.1018,
        "grad_norm": 2.2754013538360596,
        "learning_rate": 0.00017737446267378516,
        "epoch": 0.07459572248304643,
        "step": 1001
    },
    {
        "loss": 1.9588,
        "grad_norm": 3.0887975692749023,
        "learning_rate": 0.0001773298670360995,
        "epoch": 0.07467024368432819,
        "step": 1002
    },
    {
        "loss": 2.2287,
        "grad_norm": 2.7453250885009766,
        "learning_rate": 0.0001772852331098646,
        "epoch": 0.07474476488560995,
        "step": 1003
    },
    {
        "loss": 2.5078,
        "grad_norm": 2.6522748470306396,
        "learning_rate": 0.00017724056091718014,
        "epoch": 0.07481928608689171,
        "step": 1004
    },
    {
        "loss": 2.6245,
        "grad_norm": 2.4732635021209717,
        "learning_rate": 0.00017719585048016484,
        "epoch": 0.07489380728817349,
        "step": 1005
    },
    {
        "loss": 2.6215,
        "grad_norm": 1.6215488910675049,
        "learning_rate": 0.0001771511018209563,
        "epoch": 0.07496832848945525,
        "step": 1006
    },
    {
        "loss": 2.6864,
        "grad_norm": 2.479780912399292,
        "learning_rate": 0.00017710631496171102,
        "epoch": 0.07504284969073702,
        "step": 1007
    },
    {
        "loss": 2.6064,
        "grad_norm": 1.8832982778549194,
        "learning_rate": 0.00017706148992460444,
        "epoch": 0.07511737089201878,
        "step": 1008
    },
    {
        "loss": 3.2433,
        "grad_norm": 3.2509353160858154,
        "learning_rate": 0.00017701662673183096,
        "epoch": 0.07519189209330054,
        "step": 1009
    },
    {
        "loss": 2.5729,
        "grad_norm": 2.552741765975952,
        "learning_rate": 0.00017697172540560375,
        "epoch": 0.07526641329458231,
        "step": 1010
    },
    {
        "loss": 2.7228,
        "grad_norm": 1.866345763206482,
        "learning_rate": 0.00017692678596815492,
        "epoch": 0.07534093449586407,
        "step": 1011
    },
    {
        "loss": 2.4835,
        "grad_norm": 3.2513844966888428,
        "learning_rate": 0.00017688180844173553,
        "epoch": 0.07541545569714583,
        "step": 1012
    },
    {
        "loss": 2.5932,
        "grad_norm": 1.9032871723175049,
        "learning_rate": 0.00017683679284861536,
        "epoch": 0.0754899768984276,
        "step": 1013
    },
    {
        "loss": 2.5201,
        "grad_norm": 3.336336374282837,
        "learning_rate": 0.00017679173921108311,
        "epoch": 0.07556449809970937,
        "step": 1014
    },
    {
        "loss": 2.1658,
        "grad_norm": 3.0539002418518066,
        "learning_rate": 0.00017674664755144636,
        "epoch": 0.07563901930099114,
        "step": 1015
    },
    {
        "loss": 2.5891,
        "grad_norm": 2.9570631980895996,
        "learning_rate": 0.00017670151789203145,
        "epoch": 0.0757135405022729,
        "step": 1016
    },
    {
        "loss": 2.5381,
        "grad_norm": 2.765256643295288,
        "learning_rate": 0.0001766563502551835,
        "epoch": 0.07578806170355466,
        "step": 1017
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.9136320352554321,
        "learning_rate": 0.00017661114466326655,
        "epoch": 0.07586258290483643,
        "step": 1018
    },
    {
        "loss": 1.9462,
        "grad_norm": 2.951174259185791,
        "learning_rate": 0.00017656590113866334,
        "epoch": 0.07593710410611819,
        "step": 1019
    },
    {
        "loss": 2.257,
        "grad_norm": 2.121206283569336,
        "learning_rate": 0.0001765206197037754,
        "epoch": 0.07601162530739995,
        "step": 1020
    },
    {
        "loss": 2.5049,
        "grad_norm": 2.880242347717285,
        "learning_rate": 0.00017647530038102308,
        "epoch": 0.07608614650868172,
        "step": 1021
    },
    {
        "loss": 2.2803,
        "grad_norm": 3.317934513092041,
        "learning_rate": 0.00017642994319284547,
        "epoch": 0.07616066770996348,
        "step": 1022
    },
    {
        "loss": 2.412,
        "grad_norm": 3.0531671047210693,
        "learning_rate": 0.00017638454816170038,
        "epoch": 0.07623518891124526,
        "step": 1023
    },
    {
        "loss": 2.6666,
        "grad_norm": 1.825298547744751,
        "learning_rate": 0.00017633911531006436,
        "epoch": 0.07630971011252702,
        "step": 1024
    },
    {
        "loss": 2.6953,
        "grad_norm": 2.2371389865875244,
        "learning_rate": 0.00017629364466043273,
        "epoch": 0.07638423131380878,
        "step": 1025
    },
    {
        "loss": 2.6269,
        "grad_norm": 2.3658554553985596,
        "learning_rate": 0.00017624813623531952,
        "epoch": 0.07645875251509054,
        "step": 1026
    },
    {
        "loss": 2.7683,
        "grad_norm": 1.3519014120101929,
        "learning_rate": 0.0001762025900572574,
        "epoch": 0.07653327371637231,
        "step": 1027
    },
    {
        "loss": 2.0392,
        "grad_norm": 3.02003812789917,
        "learning_rate": 0.00017615700614879775,
        "epoch": 0.07660779491765407,
        "step": 1028
    },
    {
        "loss": 2.4267,
        "grad_norm": 4.508542537689209,
        "learning_rate": 0.00017611138453251071,
        "epoch": 0.07668231611893583,
        "step": 1029
    },
    {
        "loss": 2.0032,
        "grad_norm": 3.3346431255340576,
        "learning_rate": 0.000176065725230985,
        "epoch": 0.0767568373202176,
        "step": 1030
    },
    {
        "loss": 2.6035,
        "grad_norm": 1.6990060806274414,
        "learning_rate": 0.00017602002826682807,
        "epoch": 0.07683135852149936,
        "step": 1031
    },
    {
        "loss": 2.7657,
        "grad_norm": 2.733729124069214,
        "learning_rate": 0.000175974293662666,
        "epoch": 0.07690587972278114,
        "step": 1032
    },
    {
        "loss": 2.5338,
        "grad_norm": 2.7771482467651367,
        "learning_rate": 0.0001759285214411434,
        "epoch": 0.0769804009240629,
        "step": 1033
    },
    {
        "loss": 2.7523,
        "grad_norm": 3.2621519565582275,
        "learning_rate": 0.00017588271162492368,
        "epoch": 0.07705492212534466,
        "step": 1034
    },
    {
        "loss": 1.7408,
        "grad_norm": 2.718573570251465,
        "learning_rate": 0.00017583686423668874,
        "epoch": 0.07712944332662643,
        "step": 1035
    },
    {
        "loss": 2.7713,
        "grad_norm": 3.696732759475708,
        "learning_rate": 0.00017579097929913915,
        "epoch": 0.07720396452790819,
        "step": 1036
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.809775948524475,
        "learning_rate": 0.000175745056834994,
        "epoch": 0.07727848572918995,
        "step": 1037
    },
    {
        "loss": 2.1647,
        "grad_norm": 3.0773351192474365,
        "learning_rate": 0.00017569909686699107,
        "epoch": 0.07735300693047172,
        "step": 1038
    },
    {
        "loss": 2.7883,
        "grad_norm": 2.6008670330047607,
        "learning_rate": 0.00017565309941788658,
        "epoch": 0.07742752813175348,
        "step": 1039
    },
    {
        "loss": 2.7885,
        "grad_norm": 2.7369518280029297,
        "learning_rate": 0.00017560706451045542,
        "epoch": 0.07750204933303524,
        "step": 1040
    },
    {
        "loss": 2.7185,
        "grad_norm": 1.8831413984298706,
        "learning_rate": 0.00017556099216749097,
        "epoch": 0.07757657053431702,
        "step": 1041
    },
    {
        "loss": 2.8847,
        "grad_norm": 2.066021680831909,
        "learning_rate": 0.00017551488241180512,
        "epoch": 0.07765109173559878,
        "step": 1042
    },
    {
        "loss": 1.26,
        "grad_norm": 4.178749084472656,
        "learning_rate": 0.0001754687352662284,
        "epoch": 0.07772561293688054,
        "step": 1043
    },
    {
        "loss": 2.8674,
        "grad_norm": 2.7444334030151367,
        "learning_rate": 0.00017542255075360966,
        "epoch": 0.07780013413816231,
        "step": 1044
    },
    {
        "loss": 3.0473,
        "grad_norm": 2.0485002994537354,
        "learning_rate": 0.00017537632889681648,
        "epoch": 0.07787465533944407,
        "step": 1045
    },
    {
        "loss": 2.8984,
        "grad_norm": 2.2322003841400146,
        "learning_rate": 0.00017533006971873474,
        "epoch": 0.07794917654072583,
        "step": 1046
    },
    {
        "loss": 1.6343,
        "grad_norm": 1.542547345161438,
        "learning_rate": 0.0001752837732422689,
        "epoch": 0.0780236977420076,
        "step": 1047
    },
    {
        "loss": 2.0463,
        "grad_norm": 3.9175479412078857,
        "learning_rate": 0.00017523743949034181,
        "epoch": 0.07809821894328936,
        "step": 1048
    },
    {
        "loss": 2.448,
        "grad_norm": 2.44123911857605,
        "learning_rate": 0.00017519106848589493,
        "epoch": 0.07817274014457114,
        "step": 1049
    },
    {
        "loss": 2.4541,
        "grad_norm": 2.2572741508483887,
        "learning_rate": 0.00017514466025188798,
        "epoch": 0.0782472613458529,
        "step": 1050
    },
    {
        "loss": 2.7809,
        "grad_norm": 3.568103551864624,
        "learning_rate": 0.00017509821481129922,
        "epoch": 0.07832178254713466,
        "step": 1051
    },
    {
        "loss": 1.4809,
        "grad_norm": 2.945272445678711,
        "learning_rate": 0.00017505173218712532,
        "epoch": 0.07839630374841643,
        "step": 1052
    },
    {
        "loss": 2.7434,
        "grad_norm": 2.923661708831787,
        "learning_rate": 0.00017500521240238132,
        "epoch": 0.07847082494969819,
        "step": 1053
    },
    {
        "loss": 2.5457,
        "grad_norm": 2.4715182781219482,
        "learning_rate": 0.00017495865548010074,
        "epoch": 0.07854534615097995,
        "step": 1054
    },
    {
        "loss": 2.1522,
        "grad_norm": 2.8811399936676025,
        "learning_rate": 0.0001749120614433354,
        "epoch": 0.07861986735226172,
        "step": 1055
    },
    {
        "loss": 2.0004,
        "grad_norm": 2.82344913482666,
        "learning_rate": 0.0001748654303151555,
        "epoch": 0.07869438855354348,
        "step": 1056
    },
    {
        "loss": 1.8741,
        "grad_norm": 3.291429042816162,
        "learning_rate": 0.00017481876211864977,
        "epoch": 0.07876890975482524,
        "step": 1057
    },
    {
        "loss": 2.8734,
        "grad_norm": 1.9934040307998657,
        "learning_rate": 0.00017477205687692498,
        "epoch": 0.07884343095610702,
        "step": 1058
    },
    {
        "loss": 2.1414,
        "grad_norm": 2.6507863998413086,
        "learning_rate": 0.00017472531461310654,
        "epoch": 0.07891795215738878,
        "step": 1059
    },
    {
        "loss": 2.0167,
        "grad_norm": 3.4457948207855225,
        "learning_rate": 0.00017467853535033806,
        "epoch": 0.07899247335867055,
        "step": 1060
    },
    {
        "loss": 2.7158,
        "grad_norm": 1.918440580368042,
        "learning_rate": 0.00017463171911178145,
        "epoch": 0.07906699455995231,
        "step": 1061
    },
    {
        "loss": 3.0029,
        "grad_norm": 2.1189677715301514,
        "learning_rate": 0.00017458486592061704,
        "epoch": 0.07914151576123407,
        "step": 1062
    },
    {
        "loss": 3.1308,
        "grad_norm": 3.1099395751953125,
        "learning_rate": 0.00017453797580004325,
        "epoch": 0.07921603696251583,
        "step": 1063
    },
    {
        "loss": 2.4907,
        "grad_norm": 2.486933469772339,
        "learning_rate": 0.000174491048773277,
        "epoch": 0.0792905581637976,
        "step": 1064
    },
    {
        "loss": 2.5188,
        "grad_norm": 2.476346492767334,
        "learning_rate": 0.00017444408486355344,
        "epoch": 0.07936507936507936,
        "step": 1065
    },
    {
        "loss": 2.6348,
        "grad_norm": 1.8983768224716187,
        "learning_rate": 0.00017439708409412586,
        "epoch": 0.07943960056636112,
        "step": 1066
    },
    {
        "loss": 2.0401,
        "grad_norm": 3.1740870475769043,
        "learning_rate": 0.0001743500464882659,
        "epoch": 0.0795141217676429,
        "step": 1067
    },
    {
        "loss": 2.4246,
        "grad_norm": 4.007361888885498,
        "learning_rate": 0.00017430297206926345,
        "epoch": 0.07958864296892466,
        "step": 1068
    },
    {
        "loss": 2.0209,
        "grad_norm": 2.9426543712615967,
        "learning_rate": 0.00017425586086042654,
        "epoch": 0.07966316417020643,
        "step": 1069
    },
    {
        "loss": 1.7271,
        "grad_norm": 2.2259361743927,
        "learning_rate": 0.0001742087128850815,
        "epoch": 0.07973768537148819,
        "step": 1070
    },
    {
        "loss": 2.7791,
        "grad_norm": 2.595996618270874,
        "learning_rate": 0.00017416152816657288,
        "epoch": 0.07981220657276995,
        "step": 1071
    },
    {
        "loss": 2.5574,
        "grad_norm": 3.6765308380126953,
        "learning_rate": 0.0001741143067282633,
        "epoch": 0.07988672777405172,
        "step": 1072
    },
    {
        "loss": 2.6637,
        "grad_norm": 3.418034315109253,
        "learning_rate": 0.0001740670485935337,
        "epoch": 0.07996124897533348,
        "step": 1073
    },
    {
        "loss": 2.9057,
        "grad_norm": 1.6884561777114868,
        "learning_rate": 0.0001740197537857831,
        "epoch": 0.08003577017661524,
        "step": 1074
    },
    {
        "loss": 2.4993,
        "grad_norm": 2.9257359504699707,
        "learning_rate": 0.00017397242232842873,
        "epoch": 0.080110291377897,
        "step": 1075
    },
    {
        "loss": 1.9371,
        "grad_norm": 3.7910938262939453,
        "learning_rate": 0.00017392505424490592,
        "epoch": 0.08018481257917878,
        "step": 1076
    },
    {
        "loss": 2.5077,
        "grad_norm": 2.166872262954712,
        "learning_rate": 0.00017387764955866818,
        "epoch": 0.08025933378046055,
        "step": 1077
    },
    {
        "loss": 2.9227,
        "grad_norm": 2.3084943294525146,
        "learning_rate": 0.0001738302082931871,
        "epoch": 0.08033385498174231,
        "step": 1078
    },
    {
        "loss": 2.2079,
        "grad_norm": 3.024099826812744,
        "learning_rate": 0.00017378273047195244,
        "epoch": 0.08040837618302407,
        "step": 1079
    },
    {
        "loss": 2.472,
        "grad_norm": 3.0634078979492188,
        "learning_rate": 0.000173735216118472,
        "epoch": 0.08048289738430583,
        "step": 1080
    },
    {
        "loss": 2.4533,
        "grad_norm": 2.400482416152954,
        "learning_rate": 0.00017368766525627168,
        "epoch": 0.0805574185855876,
        "step": 1081
    },
    {
        "loss": 2.4209,
        "grad_norm": 3.1856441497802734,
        "learning_rate": 0.00017364007790889548,
        "epoch": 0.08063193978686936,
        "step": 1082
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.1484506130218506,
        "learning_rate": 0.00017359245409990544,
        "epoch": 0.08070646098815112,
        "step": 1083
    },
    {
        "loss": 2.6376,
        "grad_norm": 2.1917145252227783,
        "learning_rate": 0.0001735447938528817,
        "epoch": 0.08078098218943289,
        "step": 1084
    },
    {
        "loss": 1.9435,
        "grad_norm": 3.976179361343384,
        "learning_rate": 0.0001734970971914224,
        "epoch": 0.08085550339071466,
        "step": 1085
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.8819313049316406,
        "learning_rate": 0.0001734493641391437,
        "epoch": 0.08093002459199643,
        "step": 1086
    },
    {
        "loss": 2.5044,
        "grad_norm": 2.5119001865386963,
        "learning_rate": 0.00017340159471967984,
        "epoch": 0.08100454579327819,
        "step": 1087
    },
    {
        "loss": 2.4257,
        "grad_norm": 1.7705585956573486,
        "learning_rate": 0.00017335378895668295,
        "epoch": 0.08107906699455995,
        "step": 1088
    },
    {
        "loss": 2.8235,
        "grad_norm": 2.250663995742798,
        "learning_rate": 0.00017330594687382328,
        "epoch": 0.08115358819584172,
        "step": 1089
    },
    {
        "loss": 2.4808,
        "grad_norm": 2.881995677947998,
        "learning_rate": 0.000173258068494789,
        "epoch": 0.08122810939712348,
        "step": 1090
    },
    {
        "loss": 2.7494,
        "grad_norm": 2.6890854835510254,
        "learning_rate": 0.00017321015384328629,
        "epoch": 0.08130263059840524,
        "step": 1091
    },
    {
        "loss": 2.9001,
        "grad_norm": 2.977543592453003,
        "learning_rate": 0.00017316220294303922,
        "epoch": 0.081377151799687,
        "step": 1092
    },
    {
        "loss": 2.6575,
        "grad_norm": 1.7988040447235107,
        "learning_rate": 0.00017311421581778986,
        "epoch": 0.08145167300096877,
        "step": 1093
    },
    {
        "loss": 2.6148,
        "grad_norm": 3.513079881668091,
        "learning_rate": 0.00017306619249129822,
        "epoch": 0.08152619420225055,
        "step": 1094
    },
    {
        "loss": 2.5985,
        "grad_norm": 1.8455923795700073,
        "learning_rate": 0.00017301813298734224,
        "epoch": 0.08160071540353231,
        "step": 1095
    },
    {
        "loss": 1.71,
        "grad_norm": 3.2681632041931152,
        "learning_rate": 0.0001729700373297177,
        "epoch": 0.08167523660481407,
        "step": 1096
    },
    {
        "loss": 2.0926,
        "grad_norm": 3.3979012966156006,
        "learning_rate": 0.00017292190554223842,
        "epoch": 0.08174975780609584,
        "step": 1097
    },
    {
        "loss": 2.1049,
        "grad_norm": 2.960895538330078,
        "learning_rate": 0.00017287373764873589,
        "epoch": 0.0818242790073776,
        "step": 1098
    },
    {
        "loss": 2.4566,
        "grad_norm": 2.9423258304595947,
        "learning_rate": 0.00017282553367305975,
        "epoch": 0.08189880020865936,
        "step": 1099
    },
    {
        "loss": 2.4962,
        "grad_norm": 2.81508207321167,
        "learning_rate": 0.0001727772936390773,
        "epoch": 0.08197332140994112,
        "step": 1100
    },
    {
        "loss": 2.5607,
        "grad_norm": 2.3686811923980713,
        "learning_rate": 0.00017272901757067378,
        "epoch": 0.08204784261122289,
        "step": 1101
    },
    {
        "loss": 2.3828,
        "grad_norm": 2.104402542114258,
        "learning_rate": 0.0001726807054917522,
        "epoch": 0.08212236381250465,
        "step": 1102
    },
    {
        "loss": 2.9714,
        "grad_norm": 2.3534796237945557,
        "learning_rate": 0.00017263235742623353,
        "epoch": 0.08219688501378643,
        "step": 1103
    },
    {
        "loss": 1.092,
        "grad_norm": 3.0817666053771973,
        "learning_rate": 0.0001725839733980564,
        "epoch": 0.08227140621506819,
        "step": 1104
    },
    {
        "loss": 2.2919,
        "grad_norm": 2.613128185272217,
        "learning_rate": 0.0001725355534311774,
        "epoch": 0.08234592741634995,
        "step": 1105
    },
    {
        "loss": 2.9872,
        "grad_norm": 2.9308247566223145,
        "learning_rate": 0.00017248709754957083,
        "epoch": 0.08242044861763172,
        "step": 1106
    },
    {
        "loss": 2.1905,
        "grad_norm": 3.1781725883483887,
        "learning_rate": 0.00017243860577722874,
        "epoch": 0.08249496981891348,
        "step": 1107
    },
    {
        "loss": 2.5968,
        "grad_norm": 3.2867608070373535,
        "learning_rate": 0.00017239007813816103,
        "epoch": 0.08256949102019524,
        "step": 1108
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.4108455181121826,
        "learning_rate": 0.0001723415146563953,
        "epoch": 0.082644012221477,
        "step": 1109
    },
    {
        "loss": 1.693,
        "grad_norm": 3.6151130199432373,
        "learning_rate": 0.00017229291535597697,
        "epoch": 0.08271853342275877,
        "step": 1110
    },
    {
        "loss": 1.8666,
        "grad_norm": 3.435410499572754,
        "learning_rate": 0.00017224428026096903,
        "epoch": 0.08279305462404055,
        "step": 1111
    },
    {
        "loss": 2.5555,
        "grad_norm": 2.7984001636505127,
        "learning_rate": 0.00017219560939545246,
        "epoch": 0.08286757582532231,
        "step": 1112
    },
    {
        "loss": 2.3918,
        "grad_norm": 3.119225263595581,
        "learning_rate": 0.0001721469027835257,
        "epoch": 0.08294209702660407,
        "step": 1113
    },
    {
        "loss": 2.8764,
        "grad_norm": 2.0885746479034424,
        "learning_rate": 0.000172098160449305,
        "epoch": 0.08301661822788584,
        "step": 1114
    },
    {
        "loss": 3.1802,
        "grad_norm": 2.6962475776672363,
        "learning_rate": 0.0001720493824169243,
        "epoch": 0.0830911394291676,
        "step": 1115
    },
    {
        "loss": 2.205,
        "grad_norm": 3.1962568759918213,
        "learning_rate": 0.00017200056871053521,
        "epoch": 0.08316566063044936,
        "step": 1116
    },
    {
        "loss": 2.5547,
        "grad_norm": 2.4759395122528076,
        "learning_rate": 0.000171951719354307,
        "epoch": 0.08324018183173112,
        "step": 1117
    },
    {
        "loss": 2.5641,
        "grad_norm": 2.5288538932800293,
        "learning_rate": 0.00017190283437242655,
        "epoch": 0.08331470303301289,
        "step": 1118
    },
    {
        "loss": 2.1888,
        "grad_norm": 2.433945417404175,
        "learning_rate": 0.00017185391378909842,
        "epoch": 0.08338922423429465,
        "step": 1119
    },
    {
        "loss": 2.2183,
        "grad_norm": 5.518693923950195,
        "learning_rate": 0.00017180495762854483,
        "epoch": 0.08346374543557643,
        "step": 1120
    },
    {
        "loss": 2.7702,
        "grad_norm": 2.4387006759643555,
        "learning_rate": 0.00017175596591500556,
        "epoch": 0.08353826663685819,
        "step": 1121
    },
    {
        "loss": 2.7404,
        "grad_norm": 7.683805465698242,
        "learning_rate": 0.000171706938672738,
        "epoch": 0.08361278783813995,
        "step": 1122
    },
    {
        "loss": 2.6632,
        "grad_norm": 1.9058303833007812,
        "learning_rate": 0.00017165787592601713,
        "epoch": 0.08368730903942172,
        "step": 1123
    },
    {
        "loss": 2.8964,
        "grad_norm": 2.720714807510376,
        "learning_rate": 0.00017160877769913557,
        "epoch": 0.08376183024070348,
        "step": 1124
    },
    {
        "loss": 2.598,
        "grad_norm": 1.4216954708099365,
        "learning_rate": 0.00017155964401640339,
        "epoch": 0.08383635144198524,
        "step": 1125
    },
    {
        "loss": 2.7203,
        "grad_norm": 3.223198413848877,
        "learning_rate": 0.00017151047490214838,
        "epoch": 0.083910872643267,
        "step": 1126
    },
    {
        "loss": 2.9278,
        "grad_norm": 2.681457757949829,
        "learning_rate": 0.00017146127038071568,
        "epoch": 0.08398539384454877,
        "step": 1127
    },
    {
        "loss": 2.8665,
        "grad_norm": 2.501492738723755,
        "learning_rate": 0.00017141203047646816,
        "epoch": 0.08405991504583053,
        "step": 1128
    },
    {
        "loss": 2.7657,
        "grad_norm": 2.4311206340789795,
        "learning_rate": 0.00017136275521378603,
        "epoch": 0.08413443624711231,
        "step": 1129
    },
    {
        "loss": 2.2351,
        "grad_norm": 2.586174726486206,
        "learning_rate": 0.0001713134446170671,
        "epoch": 0.08420895744839407,
        "step": 1130
    },
    {
        "loss": 2.8788,
        "grad_norm": 2.323831558227539,
        "learning_rate": 0.00017126409871072665,
        "epoch": 0.08428347864967584,
        "step": 1131
    },
    {
        "loss": 2.4245,
        "grad_norm": 2.892518997192383,
        "learning_rate": 0.0001712147175191975,
        "epoch": 0.0843579998509576,
        "step": 1132
    },
    {
        "loss": 2.8878,
        "grad_norm": 2.923586845397949,
        "learning_rate": 0.00017116530106692988,
        "epoch": 0.08443252105223936,
        "step": 1133
    },
    {
        "loss": 2.3896,
        "grad_norm": 2.9136197566986084,
        "learning_rate": 0.00017111584937839146,
        "epoch": 0.08450704225352113,
        "step": 1134
    },
    {
        "loss": 2.0651,
        "grad_norm": 3.584677219390869,
        "learning_rate": 0.00017106636247806744,
        "epoch": 0.08458156345480289,
        "step": 1135
    },
    {
        "loss": 2.9653,
        "grad_norm": 1.6771440505981445,
        "learning_rate": 0.00017101684039046036,
        "epoch": 0.08465608465608465,
        "step": 1136
    },
    {
        "loss": 2.3624,
        "grad_norm": 2.801804304122925,
        "learning_rate": 0.00017096728314009026,
        "epoch": 0.08473060585736641,
        "step": 1137
    },
    {
        "loss": 2.4472,
        "grad_norm": 2.7112059593200684,
        "learning_rate": 0.00017091769075149453,
        "epoch": 0.08480512705864819,
        "step": 1138
    },
    {
        "loss": 1.7736,
        "grad_norm": 3.2749438285827637,
        "learning_rate": 0.000170868063249228,
        "epoch": 0.08487964825992995,
        "step": 1139
    },
    {
        "loss": 2.8132,
        "grad_norm": 3.10776686668396,
        "learning_rate": 0.0001708184006578629,
        "epoch": 0.08495416946121172,
        "step": 1140
    },
    {
        "loss": 2.1855,
        "grad_norm": 1.922481894493103,
        "learning_rate": 0.00017076870300198875,
        "epoch": 0.08502869066249348,
        "step": 1141
    },
    {
        "loss": 2.0865,
        "grad_norm": 2.6342787742614746,
        "learning_rate": 0.0001707189703062125,
        "epoch": 0.08510321186377524,
        "step": 1142
    },
    {
        "loss": 2.294,
        "grad_norm": 3.6715633869171143,
        "learning_rate": 0.00017066920259515848,
        "epoch": 0.085177733065057,
        "step": 1143
    },
    {
        "loss": 2.1679,
        "grad_norm": 3.115724802017212,
        "learning_rate": 0.00017061939989346827,
        "epoch": 0.08525225426633877,
        "step": 1144
    },
    {
        "loss": 2.5166,
        "grad_norm": 3.0046703815460205,
        "learning_rate": 0.00017056956222580083,
        "epoch": 0.08532677546762053,
        "step": 1145
    },
    {
        "loss": 2.7988,
        "grad_norm": 1.973031997680664,
        "learning_rate": 0.00017051968961683241,
        "epoch": 0.0854012966689023,
        "step": 1146
    },
    {
        "loss": 2.6315,
        "grad_norm": 2.500255584716797,
        "learning_rate": 0.00017046978209125658,
        "epoch": 0.08547581787018407,
        "step": 1147
    },
    {
        "loss": 1.7264,
        "grad_norm": 3.877814769744873,
        "learning_rate": 0.00017041983967378422,
        "epoch": 0.08555033907146584,
        "step": 1148
    },
    {
        "loss": 2.785,
        "grad_norm": 1.2834277153015137,
        "learning_rate": 0.00017036986238914338,
        "epoch": 0.0856248602727476,
        "step": 1149
    },
    {
        "loss": 2.6708,
        "grad_norm": 2.2870473861694336,
        "learning_rate": 0.00017031985026207954,
        "epoch": 0.08569938147402936,
        "step": 1150
    },
    {
        "loss": 1.9541,
        "grad_norm": 2.9476630687713623,
        "learning_rate": 0.00017026980331735528,
        "epoch": 0.08577390267531113,
        "step": 1151
    },
    {
        "loss": 2.6117,
        "grad_norm": 2.209228038787842,
        "learning_rate": 0.0001702197215797505,
        "epoch": 0.08584842387659289,
        "step": 1152
    },
    {
        "loss": 2.4386,
        "grad_norm": 2.4291863441467285,
        "learning_rate": 0.00017016960507406232,
        "epoch": 0.08592294507787465,
        "step": 1153
    },
    {
        "loss": 2.4117,
        "grad_norm": 2.564258098602295,
        "learning_rate": 0.00017011945382510506,
        "epoch": 0.08599746627915641,
        "step": 1154
    },
    {
        "loss": 2.4535,
        "grad_norm": 2.2569289207458496,
        "learning_rate": 0.00017006926785771023,
        "epoch": 0.08607198748043818,
        "step": 1155
    },
    {
        "loss": 2.574,
        "grad_norm": 4.053842544555664,
        "learning_rate": 0.0001700190471967266,
        "epoch": 0.08614650868171995,
        "step": 1156
    },
    {
        "loss": 2.406,
        "grad_norm": 1.8913127183914185,
        "learning_rate": 0.00016996879186701995,
        "epoch": 0.08622102988300172,
        "step": 1157
    },
    {
        "loss": 2.4875,
        "grad_norm": 2.3537440299987793,
        "learning_rate": 0.00016991850189347342,
        "epoch": 0.08629555108428348,
        "step": 1158
    },
    {
        "loss": 3.2144,
        "grad_norm": 4.512746334075928,
        "learning_rate": 0.00016986817730098723,
        "epoch": 0.08637007228556524,
        "step": 1159
    },
    {
        "loss": 2.7886,
        "grad_norm": 2.715789556503296,
        "learning_rate": 0.00016981781811447867,
        "epoch": 0.08644459348684701,
        "step": 1160
    },
    {
        "loss": 2.8201,
        "grad_norm": 3.630253553390503,
        "learning_rate": 0.00016976742435888232,
        "epoch": 0.08651911468812877,
        "step": 1161
    },
    {
        "loss": 2.0129,
        "grad_norm": 4.475266456604004,
        "learning_rate": 0.0001697169960591497,
        "epoch": 0.08659363588941053,
        "step": 1162
    },
    {
        "loss": 2.8188,
        "grad_norm": 2.4495460987091064,
        "learning_rate": 0.00016966653324024952,
        "epoch": 0.0866681570906923,
        "step": 1163
    },
    {
        "loss": 1.8662,
        "grad_norm": 3.6691746711730957,
        "learning_rate": 0.00016961603592716764,
        "epoch": 0.08674267829197407,
        "step": 1164
    },
    {
        "loss": 2.0991,
        "grad_norm": 2.4551403522491455,
        "learning_rate": 0.00016956550414490683,
        "epoch": 0.08681719949325584,
        "step": 1165
    },
    {
        "loss": 2.9343,
        "grad_norm": 1.9553011655807495,
        "learning_rate": 0.00016951493791848713,
        "epoch": 0.0868917206945376,
        "step": 1166
    },
    {
        "loss": 2.4235,
        "grad_norm": 2.9510724544525146,
        "learning_rate": 0.00016946433727294544,
        "epoch": 0.08696624189581936,
        "step": 1167
    },
    {
        "loss": 1.8716,
        "grad_norm": 2.70015287399292,
        "learning_rate": 0.0001694137022333359,
        "epoch": 0.08704076309710113,
        "step": 1168
    },
    {
        "loss": 2.2794,
        "grad_norm": 2.513500690460205,
        "learning_rate": 0.00016936303282472946,
        "epoch": 0.08711528429838289,
        "step": 1169
    },
    {
        "loss": 2.727,
        "grad_norm": 1.597851276397705,
        "learning_rate": 0.00016931232907221427,
        "epoch": 0.08718980549966465,
        "step": 1170
    },
    {
        "loss": 2.5053,
        "grad_norm": 2.6245455741882324,
        "learning_rate": 0.00016926159100089544,
        "epoch": 0.08726432670094642,
        "step": 1171
    },
    {
        "loss": 2.1605,
        "grad_norm": 3.126235008239746,
        "learning_rate": 0.00016921081863589504,
        "epoch": 0.08733884790222818,
        "step": 1172
    },
    {
        "loss": 2.6808,
        "grad_norm": 2.8180246353149414,
        "learning_rate": 0.0001691600120023521,
        "epoch": 0.08741336910350996,
        "step": 1173
    },
    {
        "loss": 2.9967,
        "grad_norm": 8.99250316619873,
        "learning_rate": 0.00016910917112542264,
        "epoch": 0.08748789030479172,
        "step": 1174
    },
    {
        "loss": 3.0735,
        "grad_norm": 2.836792469024658,
        "learning_rate": 0.00016905829603027964,
        "epoch": 0.08756241150607348,
        "step": 1175
    },
    {
        "loss": 2.7713,
        "grad_norm": 1.1582856178283691,
        "learning_rate": 0.0001690073867421131,
        "epoch": 0.08763693270735524,
        "step": 1176
    },
    {
        "loss": 2.6856,
        "grad_norm": 2.1876864433288574,
        "learning_rate": 0.00016895644328612984,
        "epoch": 0.08771145390863701,
        "step": 1177
    },
    {
        "loss": 2.3895,
        "grad_norm": 2.1869215965270996,
        "learning_rate": 0.00016890546568755355,
        "epoch": 0.08778597510991877,
        "step": 1178
    },
    {
        "loss": 2.8281,
        "grad_norm": 1.1675809621810913,
        "learning_rate": 0.00016885445397162501,
        "epoch": 0.08786049631120053,
        "step": 1179
    },
    {
        "loss": 1.7362,
        "grad_norm": 3.04335618019104,
        "learning_rate": 0.00016880340816360177,
        "epoch": 0.0879350175124823,
        "step": 1180
    },
    {
        "loss": 2.6351,
        "grad_norm": 2.370957136154175,
        "learning_rate": 0.00016875232828875825,
        "epoch": 0.08800953871376406,
        "step": 1181
    },
    {
        "loss": 2.6566,
        "grad_norm": 2.7852511405944824,
        "learning_rate": 0.0001687012143723858,
        "epoch": 0.08808405991504584,
        "step": 1182
    },
    {
        "loss": 2.3784,
        "grad_norm": 3.8101353645324707,
        "learning_rate": 0.00016865006643979256,
        "epoch": 0.0881585811163276,
        "step": 1183
    },
    {
        "loss": 2.7237,
        "grad_norm": 2.963143825531006,
        "learning_rate": 0.00016859888451630357,
        "epoch": 0.08823310231760936,
        "step": 1184
    },
    {
        "loss": 2.3489,
        "grad_norm": 4.048808574676514,
        "learning_rate": 0.00016854766862726067,
        "epoch": 0.08830762351889113,
        "step": 1185
    },
    {
        "loss": 1.7432,
        "grad_norm": 1.9959603548049927,
        "learning_rate": 0.00016849641879802251,
        "epoch": 0.08838214472017289,
        "step": 1186
    },
    {
        "loss": 2.6024,
        "grad_norm": 2.048021078109741,
        "learning_rate": 0.00016844513505396463,
        "epoch": 0.08845666592145465,
        "step": 1187
    },
    {
        "loss": 2.6316,
        "grad_norm": 2.2307395935058594,
        "learning_rate": 0.0001683938174204792,
        "epoch": 0.08853118712273642,
        "step": 1188
    },
    {
        "loss": 2.5426,
        "grad_norm": 5.147169589996338,
        "learning_rate": 0.00016834246592297529,
        "epoch": 0.08860570832401818,
        "step": 1189
    },
    {
        "loss": 2.4166,
        "grad_norm": 3.7350614070892334,
        "learning_rate": 0.00016829108058687871,
        "epoch": 0.08868022952529994,
        "step": 1190
    },
    {
        "loss": 2.4974,
        "grad_norm": 1.9163380861282349,
        "learning_rate": 0.00016823966143763206,
        "epoch": 0.08875475072658172,
        "step": 1191
    },
    {
        "loss": 1.9576,
        "grad_norm": 2.7904558181762695,
        "learning_rate": 0.0001681882085006946,
        "epoch": 0.08882927192786348,
        "step": 1192
    },
    {
        "loss": 2.4077,
        "grad_norm": 3.1502742767333984,
        "learning_rate": 0.00016813672180154233,
        "epoch": 0.08890379312914524,
        "step": 1193
    },
    {
        "loss": 1.8364,
        "grad_norm": 3.399967908859253,
        "learning_rate": 0.0001680852013656681,
        "epoch": 0.08897831433042701,
        "step": 1194
    },
    {
        "loss": 1.8068,
        "grad_norm": 3.4094061851501465,
        "learning_rate": 0.00016803364721858127,
        "epoch": 0.08905283553170877,
        "step": 1195
    },
    {
        "loss": 2.5708,
        "grad_norm": 4.4410719871521,
        "learning_rate": 0.00016798205938580807,
        "epoch": 0.08912735673299053,
        "step": 1196
    },
    {
        "loss": 2.8853,
        "grad_norm": 2.7567858695983887,
        "learning_rate": 0.00016793043789289125,
        "epoch": 0.0892018779342723,
        "step": 1197
    },
    {
        "loss": 2.1695,
        "grad_norm": 5.035507678985596,
        "learning_rate": 0.00016787878276539034,
        "epoch": 0.08927639913555406,
        "step": 1198
    },
    {
        "loss": 2.9508,
        "grad_norm": 2.2065327167510986,
        "learning_rate": 0.00016782709402888147,
        "epoch": 0.08935092033683582,
        "step": 1199
    },
    {
        "loss": 2.7632,
        "grad_norm": 2.1082098484039307,
        "learning_rate": 0.00016777537170895745,
        "epoch": 0.0894254415381176,
        "step": 1200
    },
    {
        "loss": 2.2554,
        "grad_norm": 3.3635013103485107,
        "learning_rate": 0.0001677236158312277,
        "epoch": 0.08949996273939936,
        "step": 1201
    },
    {
        "loss": 2.733,
        "grad_norm": 2.0946567058563232,
        "learning_rate": 0.00016767182642131817,
        "epoch": 0.08957448394068113,
        "step": 1202
    },
    {
        "loss": 3.0945,
        "grad_norm": 2.060514450073242,
        "learning_rate": 0.00016762000350487158,
        "epoch": 0.08964900514196289,
        "step": 1203
    },
    {
        "loss": 2.6514,
        "grad_norm": 2.567161798477173,
        "learning_rate": 0.0001675681471075471,
        "epoch": 0.08972352634324465,
        "step": 1204
    },
    {
        "loss": 2.5459,
        "grad_norm": 2.312019109725952,
        "learning_rate": 0.00016751625725502058,
        "epoch": 0.08979804754452642,
        "step": 1205
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.4924027919769287,
        "learning_rate": 0.00016746433397298437,
        "epoch": 0.08987256874580818,
        "step": 1206
    },
    {
        "loss": 2.7159,
        "grad_norm": 1.8775500059127808,
        "learning_rate": 0.00016741237728714735,
        "epoch": 0.08994708994708994,
        "step": 1207
    },
    {
        "loss": 2.2678,
        "grad_norm": 3.148393392562866,
        "learning_rate": 0.00016736038722323503,
        "epoch": 0.0900216111483717,
        "step": 1208
    },
    {
        "loss": 1.8426,
        "grad_norm": 2.743072748184204,
        "learning_rate": 0.00016730836380698935,
        "epoch": 0.09009613234965348,
        "step": 1209
    },
    {
        "loss": 2.8328,
        "grad_norm": 2.3861312866210938,
        "learning_rate": 0.0001672563070641688,
        "epoch": 0.09017065355093525,
        "step": 1210
    },
    {
        "loss": 2.4713,
        "grad_norm": 2.285381317138672,
        "learning_rate": 0.00016720421702054843,
        "epoch": 0.09024517475221701,
        "step": 1211
    },
    {
        "loss": 2.3085,
        "grad_norm": 2.627835988998413,
        "learning_rate": 0.00016715209370191968,
        "epoch": 0.09031969595349877,
        "step": 1212
    },
    {
        "loss": 1.6785,
        "grad_norm": 1.4717929363250732,
        "learning_rate": 0.00016709993713409052,
        "epoch": 0.09039421715478053,
        "step": 1213
    },
    {
        "loss": 2.4671,
        "grad_norm": 2.9951846599578857,
        "learning_rate": 0.00016704774734288539,
        "epoch": 0.0904687383560623,
        "step": 1214
    },
    {
        "loss": 2.2694,
        "grad_norm": 2.9542298316955566,
        "learning_rate": 0.0001669955243541452,
        "epoch": 0.09054325955734406,
        "step": 1215
    },
    {
        "loss": 2.242,
        "grad_norm": 2.415008783340454,
        "learning_rate": 0.00016694326819372717,
        "epoch": 0.09061778075862582,
        "step": 1216
    },
    {
        "loss": 2.6262,
        "grad_norm": 1.9808236360549927,
        "learning_rate": 0.0001668909788875051,
        "epoch": 0.09069230195990759,
        "step": 1217
    },
    {
        "loss": 1.9508,
        "grad_norm": 3.2994296550750732,
        "learning_rate": 0.0001668386564613691,
        "epoch": 0.09076682316118936,
        "step": 1218
    },
    {
        "loss": 2.0646,
        "grad_norm": 3.4999003410339355,
        "learning_rate": 0.00016678630094122573,
        "epoch": 0.09084134436247113,
        "step": 1219
    },
    {
        "loss": 2.3291,
        "grad_norm": 2.3403241634368896,
        "learning_rate": 0.000166733912352998,
        "epoch": 0.09091586556375289,
        "step": 1220
    },
    {
        "loss": 2.7223,
        "grad_norm": 2.3574512004852295,
        "learning_rate": 0.0001666814907226251,
        "epoch": 0.09099038676503465,
        "step": 1221
    },
    {
        "loss": 1.6204,
        "grad_norm": 2.4847185611724854,
        "learning_rate": 0.00016662903607606272,
        "epoch": 0.09106490796631642,
        "step": 1222
    },
    {
        "loss": 2.5976,
        "grad_norm": 2.7537167072296143,
        "learning_rate": 0.00016657654843928293,
        "epoch": 0.09113942916759818,
        "step": 1223
    },
    {
        "loss": 2.6412,
        "grad_norm": 2.2194807529449463,
        "learning_rate": 0.00016652402783827404,
        "epoch": 0.09121395036887994,
        "step": 1224
    },
    {
        "loss": 2.3913,
        "grad_norm": 2.738844156265259,
        "learning_rate": 0.0001664714742990407,
        "epoch": 0.0912884715701617,
        "step": 1225
    },
    {
        "loss": 2.0433,
        "grad_norm": 3.489086627960205,
        "learning_rate": 0.00016641888784760394,
        "epoch": 0.09136299277144348,
        "step": 1226
    },
    {
        "loss": 2.5522,
        "grad_norm": 2.9805679321289062,
        "learning_rate": 0.000166366268510001,
        "epoch": 0.09143751397272525,
        "step": 1227
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.906695604324341,
        "learning_rate": 0.0001663136163122854,
        "epoch": 0.09151203517400701,
        "step": 1228
    },
    {
        "loss": 2.2768,
        "grad_norm": 4.379935264587402,
        "learning_rate": 0.00016626093128052706,
        "epoch": 0.09158655637528877,
        "step": 1229
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.2420451641082764,
        "learning_rate": 0.000166208213440812,
        "epoch": 0.09166107757657053,
        "step": 1230
    },
    {
        "loss": 2.4965,
        "grad_norm": 3.626988172531128,
        "learning_rate": 0.00016615546281924254,
        "epoch": 0.0917355987778523,
        "step": 1231
    },
    {
        "loss": 2.3009,
        "grad_norm": 3.7615296840667725,
        "learning_rate": 0.00016610267944193726,
        "epoch": 0.09181011997913406,
        "step": 1232
    },
    {
        "loss": 2.7298,
        "grad_norm": 2.206914186477661,
        "learning_rate": 0.00016604986333503097,
        "epoch": 0.09188464118041582,
        "step": 1233
    },
    {
        "loss": 2.6754,
        "grad_norm": 3.4794819355010986,
        "learning_rate": 0.00016599701452467462,
        "epoch": 0.09195916238169759,
        "step": 1234
    },
    {
        "loss": 1.7825,
        "grad_norm": 3.243220806121826,
        "learning_rate": 0.00016594413303703536,
        "epoch": 0.09203368358297936,
        "step": 1235
    },
    {
        "loss": 2.875,
        "grad_norm": 3.235445737838745,
        "learning_rate": 0.00016589121889829658,
        "epoch": 0.09210820478426113,
        "step": 1236
    },
    {
        "loss": 2.522,
        "grad_norm": 2.496183156967163,
        "learning_rate": 0.00016583827213465785,
        "epoch": 0.09218272598554289,
        "step": 1237
    },
    {
        "loss": 2.7008,
        "grad_norm": 4.370755195617676,
        "learning_rate": 0.00016578529277233475,
        "epoch": 0.09225724718682465,
        "step": 1238
    },
    {
        "loss": 1.9123,
        "grad_norm": 3.615387201309204,
        "learning_rate": 0.00016573228083755913,
        "epoch": 0.09233176838810642,
        "step": 1239
    },
    {
        "loss": 2.7955,
        "grad_norm": 3.4821159839630127,
        "learning_rate": 0.00016567923635657899,
        "epoch": 0.09240628958938818,
        "step": 1240
    },
    {
        "loss": 2.7652,
        "grad_norm": 2.136033296585083,
        "learning_rate": 0.0001656261593556583,
        "epoch": 0.09248081079066994,
        "step": 1241
    },
    {
        "loss": 2.3134,
        "grad_norm": 3.499835252761841,
        "learning_rate": 0.00016557304986107728,
        "epoch": 0.0925553319919517,
        "step": 1242
    },
    {
        "loss": 2.8909,
        "grad_norm": 2.4851322174072266,
        "learning_rate": 0.00016551990789913222,
        "epoch": 0.09262985319323347,
        "step": 1243
    },
    {
        "loss": 3.0704,
        "grad_norm": 1.9774391651153564,
        "learning_rate": 0.00016546673349613532,
        "epoch": 0.09270437439451525,
        "step": 1244
    },
    {
        "loss": 2.6556,
        "grad_norm": 2.4327311515808105,
        "learning_rate": 0.0001654135266784151,
        "epoch": 0.09277889559579701,
        "step": 1245
    },
    {
        "loss": 2.6449,
        "grad_norm": 3.57694149017334,
        "learning_rate": 0.00016536028747231587,
        "epoch": 0.09285341679707877,
        "step": 1246
    },
    {
        "loss": 2.1536,
        "grad_norm": 3.970017671585083,
        "learning_rate": 0.00016530701590419824,
        "epoch": 0.09292793799836054,
        "step": 1247
    },
    {
        "loss": 2.8794,
        "grad_norm": 3.3919548988342285,
        "learning_rate": 0.0001652537120004386,
        "epoch": 0.0930024591996423,
        "step": 1248
    },
    {
        "loss": 2.1518,
        "grad_norm": 4.810261249542236,
        "learning_rate": 0.0001652003757874295,
        "epoch": 0.09307698040092406,
        "step": 1249
    },
    {
        "loss": 2.8237,
        "grad_norm": 2.3289496898651123,
        "learning_rate": 0.00016514700729157949,
        "epoch": 0.09315150160220582,
        "step": 1250
    },
    {
        "loss": 3.0124,
        "grad_norm": 2.1813912391662598,
        "learning_rate": 0.00016509360653931297,
        "epoch": 0.09322602280348759,
        "step": 1251
    },
    {
        "loss": 1.9151,
        "grad_norm": 2.747079372406006,
        "learning_rate": 0.00016504017355707044,
        "epoch": 0.09330054400476935,
        "step": 1252
    },
    {
        "loss": 2.4869,
        "grad_norm": 3.1254584789276123,
        "learning_rate": 0.00016498670837130833,
        "epoch": 0.09337506520605113,
        "step": 1253
    },
    {
        "loss": 3.0481,
        "grad_norm": 2.26583194732666,
        "learning_rate": 0.000164933211008499,
        "epoch": 0.09344958640733289,
        "step": 1254
    },
    {
        "loss": 2.4654,
        "grad_norm": 2.646056890487671,
        "learning_rate": 0.00016487968149513075,
        "epoch": 0.09352410760861465,
        "step": 1255
    },
    {
        "loss": 2.3753,
        "grad_norm": 2.757303476333618,
        "learning_rate": 0.00016482611985770777,
        "epoch": 0.09359862880989642,
        "step": 1256
    },
    {
        "loss": 3.1099,
        "grad_norm": 3.369955539703369,
        "learning_rate": 0.00016477252612275023,
        "epoch": 0.09367315001117818,
        "step": 1257
    },
    {
        "loss": 2.8003,
        "grad_norm": 1.8971699476242065,
        "learning_rate": 0.0001647189003167941,
        "epoch": 0.09374767121245994,
        "step": 1258
    },
    {
        "loss": 2.5826,
        "grad_norm": 5.044479846954346,
        "learning_rate": 0.00016466524246639127,
        "epoch": 0.0938221924137417,
        "step": 1259
    },
    {
        "loss": 2.3704,
        "grad_norm": 3.092158555984497,
        "learning_rate": 0.00016461155259810955,
        "epoch": 0.09389671361502347,
        "step": 1260
    },
    {
        "loss": 2.7242,
        "grad_norm": 2.6682002544403076,
        "learning_rate": 0.00016455783073853248,
        "epoch": 0.09397123481630523,
        "step": 1261
    },
    {
        "loss": 2.6336,
        "grad_norm": 3.3763315677642822,
        "learning_rate": 0.00016450407691425957,
        "epoch": 0.09404575601758701,
        "step": 1262
    },
    {
        "loss": 1.9108,
        "grad_norm": 4.2871856689453125,
        "learning_rate": 0.00016445029115190604,
        "epoch": 0.09412027721886877,
        "step": 1263
    },
    {
        "loss": 2.8893,
        "grad_norm": 2.3716421127319336,
        "learning_rate": 0.00016439647347810303,
        "epoch": 0.09419479842015054,
        "step": 1264
    },
    {
        "loss": 2.3363,
        "grad_norm": 2.5795485973358154,
        "learning_rate": 0.00016434262391949744,
        "epoch": 0.0942693196214323,
        "step": 1265
    },
    {
        "loss": 2.8305,
        "grad_norm": 2.137463331222534,
        "learning_rate": 0.00016428874250275192,
        "epoch": 0.09434384082271406,
        "step": 1266
    },
    {
        "loss": 2.3334,
        "grad_norm": 2.7980048656463623,
        "learning_rate": 0.00016423482925454494,
        "epoch": 0.09441836202399582,
        "step": 1267
    },
    {
        "loss": 2.4349,
        "grad_norm": 2.757240056991577,
        "learning_rate": 0.00016418088420157073,
        "epoch": 0.09449288322527759,
        "step": 1268
    },
    {
        "loss": 2.2352,
        "grad_norm": 3.922445297241211,
        "learning_rate": 0.00016412690737053914,
        "epoch": 0.09456740442655935,
        "step": 1269
    },
    {
        "loss": 2.415,
        "grad_norm": 4.696018695831299,
        "learning_rate": 0.00016407289878817604,
        "epoch": 0.09464192562784111,
        "step": 1270
    },
    {
        "loss": 2.561,
        "grad_norm": 2.3458306789398193,
        "learning_rate": 0.00016401885848122273,
        "epoch": 0.09471644682912289,
        "step": 1271
    },
    {
        "loss": 2.2481,
        "grad_norm": 4.202385425567627,
        "learning_rate": 0.00016396478647643642,
        "epoch": 0.09479096803040465,
        "step": 1272
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.8006820678710938,
        "learning_rate": 0.0001639106828005898,
        "epoch": 0.09486548923168642,
        "step": 1273
    },
    {
        "loss": 2.1902,
        "grad_norm": 3.458014726638794,
        "learning_rate": 0.0001638565474804715,
        "epoch": 0.09494001043296818,
        "step": 1274
    },
    {
        "loss": 2.7881,
        "grad_norm": 3.766277551651001,
        "learning_rate": 0.00016380238054288564,
        "epoch": 0.09501453163424994,
        "step": 1275
    },
    {
        "loss": 2.2516,
        "grad_norm": 1.97324800491333,
        "learning_rate": 0.00016374818201465204,
        "epoch": 0.0950890528355317,
        "step": 1276
    },
    {
        "loss": 2.8806,
        "grad_norm": 2.765547752380371,
        "learning_rate": 0.00016369395192260616,
        "epoch": 0.09516357403681347,
        "step": 1277
    },
    {
        "loss": 1.6322,
        "grad_norm": 2.9167370796203613,
        "learning_rate": 0.00016363969029359915,
        "epoch": 0.09523809523809523,
        "step": 1278
    },
    {
        "loss": 1.8703,
        "grad_norm": 3.0774593353271484,
        "learning_rate": 0.00016358539715449758,
        "epoch": 0.095312616439377,
        "step": 1279
    },
    {
        "loss": 2.3349,
        "grad_norm": 3.10996413230896,
        "learning_rate": 0.00016353107253218392,
        "epoch": 0.09538713764065877,
        "step": 1280
    },
    {
        "loss": 2.3495,
        "grad_norm": 2.7385101318359375,
        "learning_rate": 0.000163476716453556,
        "epoch": 0.09546165884194054,
        "step": 1281
    },
    {
        "loss": 2.4535,
        "grad_norm": 2.6102747917175293,
        "learning_rate": 0.00016342232894552732,
        "epoch": 0.0955361800432223,
        "step": 1282
    },
    {
        "loss": 2.3896,
        "grad_norm": 3.5039443969726562,
        "learning_rate": 0.00016336791003502684,
        "epoch": 0.09561070124450406,
        "step": 1283
    },
    {
        "loss": 2.1358,
        "grad_norm": 3.2915563583374023,
        "learning_rate": 0.00016331345974899923,
        "epoch": 0.09568522244578583,
        "step": 1284
    },
    {
        "loss": 2.4871,
        "grad_norm": 2.0549333095550537,
        "learning_rate": 0.00016325897811440458,
        "epoch": 0.09575974364706759,
        "step": 1285
    },
    {
        "loss": 2.0333,
        "grad_norm": 3.5276103019714355,
        "learning_rate": 0.00016320446515821852,
        "epoch": 0.09583426484834935,
        "step": 1286
    },
    {
        "loss": 2.5424,
        "grad_norm": 2.2625091075897217,
        "learning_rate": 0.00016314992090743225,
        "epoch": 0.09590878604963111,
        "step": 1287
    },
    {
        "loss": 2.2376,
        "grad_norm": 3.1293582916259766,
        "learning_rate": 0.00016309534538905233,
        "epoch": 0.09598330725091289,
        "step": 1288
    },
    {
        "loss": 2.8022,
        "grad_norm": 2.485692024230957,
        "learning_rate": 0.00016304073863010094,
        "epoch": 0.09605782845219465,
        "step": 1289
    },
    {
        "loss": 1.9984,
        "grad_norm": 2.77173113822937,
        "learning_rate": 0.00016298610065761568,
        "epoch": 0.09613234965347642,
        "step": 1290
    },
    {
        "loss": 2.5234,
        "grad_norm": 3.3650426864624023,
        "learning_rate": 0.00016293143149864958,
        "epoch": 0.09620687085475818,
        "step": 1291
    },
    {
        "loss": 2.4039,
        "grad_norm": 2.249854803085327,
        "learning_rate": 0.00016287673118027112,
        "epoch": 0.09628139205603994,
        "step": 1292
    },
    {
        "loss": 2.4366,
        "grad_norm": 1.85558021068573,
        "learning_rate": 0.00016282199972956425,
        "epoch": 0.09635591325732171,
        "step": 1293
    },
    {
        "loss": 2.4343,
        "grad_norm": 2.260554552078247,
        "learning_rate": 0.00016276723717362828,
        "epoch": 0.09643043445860347,
        "step": 1294
    },
    {
        "loss": 2.7815,
        "grad_norm": 3.203650951385498,
        "learning_rate": 0.00016271244353957792,
        "epoch": 0.09650495565988523,
        "step": 1295
    },
    {
        "loss": 3.093,
        "grad_norm": 2.4022891521453857,
        "learning_rate": 0.00016265761885454337,
        "epoch": 0.096579476861167,
        "step": 1296
    },
    {
        "loss": 1.8778,
        "grad_norm": 2.2737512588500977,
        "learning_rate": 0.00016260276314567006,
        "epoch": 0.09665399806244877,
        "step": 1297
    },
    {
        "loss": 2.696,
        "grad_norm": 2.128143310546875,
        "learning_rate": 0.00016254787644011889,
        "epoch": 0.09672851926373054,
        "step": 1298
    },
    {
        "loss": 2.3564,
        "grad_norm": 2.420743465423584,
        "learning_rate": 0.000162492958765066,
        "epoch": 0.0968030404650123,
        "step": 1299
    },
    {
        "loss": 2.357,
        "grad_norm": 2.5175681114196777,
        "learning_rate": 0.00016243801014770304,
        "epoch": 0.09687756166629406,
        "step": 1300
    },
    {
        "loss": 1.7577,
        "grad_norm": 4.636664867401123,
        "learning_rate": 0.00016238303061523674,
        "epoch": 0.09695208286757583,
        "step": 1301
    },
    {
        "loss": 2.1075,
        "grad_norm": 3.180330276489258,
        "learning_rate": 0.00016232802019488935,
        "epoch": 0.09702660406885759,
        "step": 1302
    },
    {
        "loss": 1.8832,
        "grad_norm": 2.2915892601013184,
        "learning_rate": 0.00016227297891389833,
        "epoch": 0.09710112527013935,
        "step": 1303
    },
    {
        "loss": 2.6809,
        "grad_norm": 2.7085320949554443,
        "learning_rate": 0.00016221790679951637,
        "epoch": 0.09717564647142111,
        "step": 1304
    },
    {
        "loss": 2.1682,
        "grad_norm": 2.8797008991241455,
        "learning_rate": 0.00016216280387901152,
        "epoch": 0.09725016767270288,
        "step": 1305
    },
    {
        "loss": 2.5373,
        "grad_norm": 2.6311638355255127,
        "learning_rate": 0.000162107670179667,
        "epoch": 0.09732468887398465,
        "step": 1306
    },
    {
        "loss": 2.9943,
        "grad_norm": 2.248898506164551,
        "learning_rate": 0.00016205250572878138,
        "epoch": 0.09739921007526642,
        "step": 1307
    },
    {
        "loss": 2.5117,
        "grad_norm": 3.5828189849853516,
        "learning_rate": 0.0001619973105536683,
        "epoch": 0.09747373127654818,
        "step": 1308
    },
    {
        "loss": 2.4248,
        "grad_norm": 3.378228187561035,
        "learning_rate": 0.0001619420846816568,
        "epoch": 0.09754825247782994,
        "step": 1309
    },
    {
        "loss": 2.5363,
        "grad_norm": 3.2823524475097656,
        "learning_rate": 0.0001618868281400909,
        "epoch": 0.09762277367911171,
        "step": 1310
    },
    {
        "loss": 2.2923,
        "grad_norm": 3.990311861038208,
        "learning_rate": 0.00016183154095633003,
        "epoch": 0.09769729488039347,
        "step": 1311
    },
    {
        "loss": 2.3475,
        "grad_norm": 4.999581336975098,
        "learning_rate": 0.00016177622315774858,
        "epoch": 0.09777181608167523,
        "step": 1312
    },
    {
        "loss": 3.0146,
        "grad_norm": 4.65812349319458,
        "learning_rate": 0.0001617208747717363,
        "epoch": 0.097846337282957,
        "step": 1313
    },
    {
        "loss": 2.348,
        "grad_norm": 2.603248119354248,
        "learning_rate": 0.00016166549582569795,
        "epoch": 0.09792085848423876,
        "step": 1314
    },
    {
        "loss": 2.7834,
        "grad_norm": 2.8840200901031494,
        "learning_rate": 0.00016161008634705347,
        "epoch": 0.09799537968552054,
        "step": 1315
    },
    {
        "loss": 2.7854,
        "grad_norm": 2.4011237621307373,
        "learning_rate": 0.00016155464636323784,
        "epoch": 0.0980699008868023,
        "step": 1316
    },
    {
        "loss": 2.0414,
        "grad_norm": 2.672954797744751,
        "learning_rate": 0.0001614991759017013,
        "epoch": 0.09814442208808406,
        "step": 1317
    },
    {
        "loss": 2.6644,
        "grad_norm": 2.74139404296875,
        "learning_rate": 0.00016144367498990906,
        "epoch": 0.09821894328936583,
        "step": 1318
    },
    {
        "loss": 2.5445,
        "grad_norm": 2.852130651473999,
        "learning_rate": 0.00016138814365534144,
        "epoch": 0.09829346449064759,
        "step": 1319
    },
    {
        "loss": 1.7394,
        "grad_norm": 4.006320476531982,
        "learning_rate": 0.00016133258192549383,
        "epoch": 0.09836798569192935,
        "step": 1320
    },
    {
        "loss": 2.8357,
        "grad_norm": 1.7375117540359497,
        "learning_rate": 0.0001612769898278766,
        "epoch": 0.09844250689321112,
        "step": 1321
    },
    {
        "loss": 2.6525,
        "grad_norm": 2.8437418937683105,
        "learning_rate": 0.00016122136739001532,
        "epoch": 0.09851702809449288,
        "step": 1322
    },
    {
        "loss": 2.6748,
        "grad_norm": 3.3946540355682373,
        "learning_rate": 0.00016116571463945043,
        "epoch": 0.09859154929577464,
        "step": 1323
    },
    {
        "loss": 2.2121,
        "grad_norm": 3.5089118480682373,
        "learning_rate": 0.0001611100316037374,
        "epoch": 0.09866607049705642,
        "step": 1324
    },
    {
        "loss": 2.0209,
        "grad_norm": 3.3566811084747314,
        "learning_rate": 0.00016105431831044676,
        "epoch": 0.09874059169833818,
        "step": 1325
    },
    {
        "loss": 2.4101,
        "grad_norm": 2.318925142288208,
        "learning_rate": 0.00016099857478716397,
        "epoch": 0.09881511289961994,
        "step": 1326
    },
    {
        "loss": 2.8181,
        "grad_norm": 2.197169542312622,
        "learning_rate": 0.0001609428010614895,
        "epoch": 0.09888963410090171,
        "step": 1327
    },
    {
        "loss": 1.4281,
        "grad_norm": 3.078453302383423,
        "learning_rate": 0.00016088699716103872,
        "epoch": 0.09896415530218347,
        "step": 1328
    },
    {
        "loss": 2.4076,
        "grad_norm": 3.562404155731201,
        "learning_rate": 0.000160831163113442,
        "epoch": 0.09903867650346523,
        "step": 1329
    },
    {
        "loss": 2.9375,
        "grad_norm": 2.6318182945251465,
        "learning_rate": 0.00016077529894634456,
        "epoch": 0.099113197704747,
        "step": 1330
    },
    {
        "loss": 2.8266,
        "grad_norm": 2.2002105712890625,
        "learning_rate": 0.0001607194046874066,
        "epoch": 0.09918771890602876,
        "step": 1331
    },
    {
        "loss": 1.99,
        "grad_norm": 2.6288840770721436,
        "learning_rate": 0.0001606634803643032,
        "epoch": 0.09926224010731052,
        "step": 1332
    },
    {
        "loss": 3.4128,
        "grad_norm": 3.2614729404449463,
        "learning_rate": 0.00016060752600472432,
        "epoch": 0.0993367613085923,
        "step": 1333
    },
    {
        "loss": 1.9188,
        "grad_norm": 3.4773404598236084,
        "learning_rate": 0.00016055154163637482,
        "epoch": 0.09941128250987406,
        "step": 1334
    },
    {
        "loss": 2.0786,
        "grad_norm": 2.5595157146453857,
        "learning_rate": 0.00016049552728697437,
        "epoch": 0.09948580371115583,
        "step": 1335
    },
    {
        "loss": 1.2605,
        "grad_norm": 3.175452470779419,
        "learning_rate": 0.00016043948298425747,
        "epoch": 0.09956032491243759,
        "step": 1336
    },
    {
        "loss": 2.2508,
        "grad_norm": 2.226191997528076,
        "learning_rate": 0.0001603834087559736,
        "epoch": 0.09963484611371935,
        "step": 1337
    },
    {
        "loss": 2.3278,
        "grad_norm": 6.360077381134033,
        "learning_rate": 0.0001603273046298868,
        "epoch": 0.09970936731500112,
        "step": 1338
    },
    {
        "loss": 2.4545,
        "grad_norm": 2.082786798477173,
        "learning_rate": 0.0001602711706337762,
        "epoch": 0.09978388851628288,
        "step": 1339
    },
    {
        "loss": 2.4245,
        "grad_norm": 4.239309787750244,
        "learning_rate": 0.00016021500679543545,
        "epoch": 0.09985840971756464,
        "step": 1340
    },
    {
        "loss": 2.6922,
        "grad_norm": 3.6271214485168457,
        "learning_rate": 0.00016015881314267324,
        "epoch": 0.0999329309188464,
        "step": 1341
    },
    {
        "loss": 2.6529,
        "grad_norm": 2.6373209953308105,
        "learning_rate": 0.00016010258970331279,
        "epoch": 0.10000745212012818,
        "step": 1342
    },
    {
        "loss": 1.8961,
        "grad_norm": 3.0611958503723145,
        "learning_rate": 0.00016004633650519218,
        "epoch": 0.10008197332140994,
        "step": 1343
    },
    {
        "loss": 1.664,
        "grad_norm": 3.562368392944336,
        "learning_rate": 0.00015999005357616425,
        "epoch": 0.10015649452269171,
        "step": 1344
    },
    {
        "loss": 2.8874,
        "grad_norm": 2.6790695190429688,
        "learning_rate": 0.00015993374094409648,
        "epoch": 0.10023101572397347,
        "step": 1345
    },
    {
        "loss": 2.3576,
        "grad_norm": 1.6707009077072144,
        "learning_rate": 0.00015987739863687107,
        "epoch": 0.10030553692525523,
        "step": 1346
    },
    {
        "loss": 2.6261,
        "grad_norm": 2.245619297027588,
        "learning_rate": 0.00015982102668238505,
        "epoch": 0.100380058126537,
        "step": 1347
    },
    {
        "loss": 2.998,
        "grad_norm": 2.8314766883850098,
        "learning_rate": 0.00015976462510854998,
        "epoch": 0.10045457932781876,
        "step": 1348
    },
    {
        "loss": 2.2134,
        "grad_norm": 2.611135721206665,
        "learning_rate": 0.00015970819394329203,
        "epoch": 0.10052910052910052,
        "step": 1349
    },
    {
        "loss": 2.4793,
        "grad_norm": 2.0060067176818848,
        "learning_rate": 0.00015965173321455222,
        "epoch": 0.1006036217303823,
        "step": 1350
    },
    {
        "loss": 2.0156,
        "grad_norm": 3.2176201343536377,
        "learning_rate": 0.0001595952429502861,
        "epoch": 0.10067814293166406,
        "step": 1351
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.4792275428771973,
        "learning_rate": 0.00015953872317846378,
        "epoch": 0.10075266413294583,
        "step": 1352
    },
    {
        "loss": 2.6273,
        "grad_norm": 3.7289748191833496,
        "learning_rate": 0.0001594821739270701,
        "epoch": 0.10082718533422759,
        "step": 1353
    },
    {
        "loss": 2.7869,
        "grad_norm": 2.7502994537353516,
        "learning_rate": 0.00015942559522410447,
        "epoch": 0.10090170653550935,
        "step": 1354
    },
    {
        "loss": 3.0518,
        "grad_norm": 2.311960220336914,
        "learning_rate": 0.0001593689870975808,
        "epoch": 0.10097622773679112,
        "step": 1355
    },
    {
        "loss": 2.6305,
        "grad_norm": 2.911104202270508,
        "learning_rate": 0.00015931234957552767,
        "epoch": 0.10105074893807288,
        "step": 1356
    },
    {
        "loss": 2.3466,
        "grad_norm": 2.345797300338745,
        "learning_rate": 0.00015925568268598816,
        "epoch": 0.10112527013935464,
        "step": 1357
    },
    {
        "loss": 2.2948,
        "grad_norm": 3.7708792686462402,
        "learning_rate": 0.0001591989864570199,
        "epoch": 0.1011997913406364,
        "step": 1358
    },
    {
        "loss": 2.3252,
        "grad_norm": 2.5248429775238037,
        "learning_rate": 0.00015914226091669504,
        "epoch": 0.10127431254191818,
        "step": 1359
    },
    {
        "loss": 2.8834,
        "grad_norm": 2.378194570541382,
        "learning_rate": 0.00015908550609310023,
        "epoch": 0.10134883374319995,
        "step": 1360
    },
    {
        "loss": 2.2268,
        "grad_norm": 3.2344369888305664,
        "learning_rate": 0.00015902872201433671,
        "epoch": 0.10142335494448171,
        "step": 1361
    },
    {
        "loss": 3.0406,
        "grad_norm": 2.545065402984619,
        "learning_rate": 0.00015897190870852013,
        "epoch": 0.10149787614576347,
        "step": 1362
    },
    {
        "loss": 1.7976,
        "grad_norm": 2.474123239517212,
        "learning_rate": 0.00015891506620378053,
        "epoch": 0.10157239734704523,
        "step": 1363
    },
    {
        "loss": 2.1327,
        "grad_norm": 2.5481252670288086,
        "learning_rate": 0.00015885819452826261,
        "epoch": 0.101646918548327,
        "step": 1364
    },
    {
        "loss": 2.3921,
        "grad_norm": 3.3253743648529053,
        "learning_rate": 0.0001588012937101253,
        "epoch": 0.10172143974960876,
        "step": 1365
    },
    {
        "loss": 1.7669,
        "grad_norm": 3.022152900695801,
        "learning_rate": 0.0001587443637775421,
        "epoch": 0.10179596095089052,
        "step": 1366
    },
    {
        "loss": 2.6567,
        "grad_norm": 2.8257501125335693,
        "learning_rate": 0.0001586874047587009,
        "epoch": 0.10187048215217229,
        "step": 1367
    },
    {
        "loss": 2.5426,
        "grad_norm": 3.7393736839294434,
        "learning_rate": 0.00015863041668180393,
        "epoch": 0.10194500335345406,
        "step": 1368
    },
    {
        "loss": 2.6392,
        "grad_norm": 2.318375825881958,
        "learning_rate": 0.00015857339957506793,
        "epoch": 0.10201952455473583,
        "step": 1369
    },
    {
        "loss": 2.782,
        "grad_norm": 2.8290183544158936,
        "learning_rate": 0.00015851635346672387,
        "epoch": 0.10209404575601759,
        "step": 1370
    },
    {
        "loss": 1.563,
        "grad_norm": 2.696573257446289,
        "learning_rate": 0.00015845927838501714,
        "epoch": 0.10216856695729935,
        "step": 1371
    },
    {
        "loss": 2.3327,
        "grad_norm": 2.68400502204895,
        "learning_rate": 0.00015840217435820754,
        "epoch": 0.10224308815858112,
        "step": 1372
    },
    {
        "loss": 2.0564,
        "grad_norm": 2.4651551246643066,
        "learning_rate": 0.00015834504141456907,
        "epoch": 0.10231760935986288,
        "step": 1373
    },
    {
        "loss": 2.3102,
        "grad_norm": 5.440133571624756,
        "learning_rate": 0.0001582878795823902,
        "epoch": 0.10239213056114464,
        "step": 1374
    },
    {
        "loss": 2.8562,
        "grad_norm": 1.894728183746338,
        "learning_rate": 0.00015823068888997356,
        "epoch": 0.1024666517624264,
        "step": 1375
    },
    {
        "loss": 2.1515,
        "grad_norm": 3.3670010566711426,
        "learning_rate": 0.00015817346936563618,
        "epoch": 0.10254117296370817,
        "step": 1376
    },
    {
        "loss": 2.0703,
        "grad_norm": 3.4756875038146973,
        "learning_rate": 0.00015811622103770932,
        "epoch": 0.10261569416498995,
        "step": 1377
    },
    {
        "loss": 2.2956,
        "grad_norm": 1.7587718963623047,
        "learning_rate": 0.00015805894393453843,
        "epoch": 0.10269021536627171,
        "step": 1378
    },
    {
        "loss": 2.2581,
        "grad_norm": 1.4061115980148315,
        "learning_rate": 0.00015800163808448337,
        "epoch": 0.10276473656755347,
        "step": 1379
    },
    {
        "loss": 2.5025,
        "grad_norm": 2.979609966278076,
        "learning_rate": 0.00015794430351591808,
        "epoch": 0.10283925776883523,
        "step": 1380
    },
    {
        "loss": 2.8869,
        "grad_norm": 1.884117603302002,
        "learning_rate": 0.0001578869402572308,
        "epoch": 0.102913778970117,
        "step": 1381
    },
    {
        "loss": 1.8238,
        "grad_norm": 2.9046645164489746,
        "learning_rate": 0.000157829548336824,
        "epoch": 0.10298830017139876,
        "step": 1382
    },
    {
        "loss": 1.772,
        "grad_norm": 3.2671825885772705,
        "learning_rate": 0.00015777212778311423,
        "epoch": 0.10306282137268052,
        "step": 1383
    },
    {
        "loss": 2.1624,
        "grad_norm": 3.3089230060577393,
        "learning_rate": 0.00015771467862453234,
        "epoch": 0.10313734257396229,
        "step": 1384
    },
    {
        "loss": 2.4698,
        "grad_norm": 2.483875036239624,
        "learning_rate": 0.00015765720088952326,
        "epoch": 0.10321186377524405,
        "step": 1385
    },
    {
        "loss": 2.6383,
        "grad_norm": 2.6646714210510254,
        "learning_rate": 0.0001575996946065461,
        "epoch": 0.10328638497652583,
        "step": 1386
    },
    {
        "loss": 2.752,
        "grad_norm": 1.8593087196350098,
        "learning_rate": 0.00015754215980407415,
        "epoch": 0.10336090617780759,
        "step": 1387
    },
    {
        "loss": 2.3688,
        "grad_norm": 2.7858221530914307,
        "learning_rate": 0.00015748459651059466,
        "epoch": 0.10343542737908935,
        "step": 1388
    },
    {
        "loss": 2.4144,
        "grad_norm": 2.067983388900757,
        "learning_rate": 0.00015742700475460925,
        "epoch": 0.10350994858037112,
        "step": 1389
    },
    {
        "loss": 2.7786,
        "grad_norm": 5.643821716308594,
        "learning_rate": 0.0001573693845646334,
        "epoch": 0.10358446978165288,
        "step": 1390
    },
    {
        "loss": 1.8357,
        "grad_norm": 1.940446138381958,
        "learning_rate": 0.00015731173596919673,
        "epoch": 0.10365899098293464,
        "step": 1391
    },
    {
        "loss": 2.4159,
        "grad_norm": 5.686247825622559,
        "learning_rate": 0.000157254058996843,
        "epoch": 0.1037335121842164,
        "step": 1392
    },
    {
        "loss": 2.4781,
        "grad_norm": 2.1431081295013428,
        "learning_rate": 0.00015719635367612993,
        "epoch": 0.10380803338549817,
        "step": 1393
    },
    {
        "loss": 2.7647,
        "grad_norm": 2.014979362487793,
        "learning_rate": 0.00015713862003562937,
        "epoch": 0.10388255458677993,
        "step": 1394
    },
    {
        "loss": 1.9411,
        "grad_norm": 3.362865686416626,
        "learning_rate": 0.0001570808581039271,
        "epoch": 0.10395707578806171,
        "step": 1395
    },
    {
        "loss": 2.4875,
        "grad_norm": 2.2789974212646484,
        "learning_rate": 0.0001570230679096229,
        "epoch": 0.10403159698934347,
        "step": 1396
    },
    {
        "loss": 2.5953,
        "grad_norm": 2.3192813396453857,
        "learning_rate": 0.00015696524948133065,
        "epoch": 0.10410611819062524,
        "step": 1397
    },
    {
        "loss": 2.376,
        "grad_norm": 4.0021514892578125,
        "learning_rate": 0.00015690740284767814,
        "epoch": 0.104180639391907,
        "step": 1398
    },
    {
        "loss": 2.8318,
        "grad_norm": 2.1268575191497803,
        "learning_rate": 0.0001568495280373071,
        "epoch": 0.10425516059318876,
        "step": 1399
    },
    {
        "loss": 2.9669,
        "grad_norm": 2.225484609603882,
        "learning_rate": 0.00015679162507887328,
        "epoch": 0.10432968179447052,
        "step": 1400
    },
    {
        "loss": 2.2001,
        "grad_norm": 3.362215280532837,
        "learning_rate": 0.0001567336940010463,
        "epoch": 0.10440420299575229,
        "step": 1401
    },
    {
        "loss": 2.8414,
        "grad_norm": 2.064004898071289,
        "learning_rate": 0.00015667573483250973,
        "epoch": 0.10447872419703405,
        "step": 1402
    },
    {
        "loss": 2.6142,
        "grad_norm": 4.307216167449951,
        "learning_rate": 0.0001566177476019611,
        "epoch": 0.10455324539831583,
        "step": 1403
    },
    {
        "loss": 2.7883,
        "grad_norm": 2.4289638996124268,
        "learning_rate": 0.00015655973233811175,
        "epoch": 0.10462776659959759,
        "step": 1404
    },
    {
        "loss": 2.2091,
        "grad_norm": 3.2484467029571533,
        "learning_rate": 0.00015650168906968694,
        "epoch": 0.10470228780087935,
        "step": 1405
    },
    {
        "loss": 2.1944,
        "grad_norm": 5.505086421966553,
        "learning_rate": 0.00015644361782542578,
        "epoch": 0.10477680900216112,
        "step": 1406
    },
    {
        "loss": 1.8251,
        "grad_norm": 3.363474130630493,
        "learning_rate": 0.00015638551863408128,
        "epoch": 0.10485133020344288,
        "step": 1407
    },
    {
        "loss": 2.3045,
        "grad_norm": 3.4555160999298096,
        "learning_rate": 0.00015632739152442022,
        "epoch": 0.10492585140472464,
        "step": 1408
    },
    {
        "loss": 2.3556,
        "grad_norm": 2.0528242588043213,
        "learning_rate": 0.00015626923652522327,
        "epoch": 0.1050003726060064,
        "step": 1409
    },
    {
        "loss": 2.3985,
        "grad_norm": 3.0295474529266357,
        "learning_rate": 0.00015621105366528482,
        "epoch": 0.10507489380728817,
        "step": 1410
    },
    {
        "loss": 2.1942,
        "grad_norm": 2.36655330657959,
        "learning_rate": 0.00015615284297341314,
        "epoch": 0.10514941500856993,
        "step": 1411
    },
    {
        "loss": 2.1763,
        "grad_norm": 2.609652519226074,
        "learning_rate": 0.0001560946044784303,
        "epoch": 0.10522393620985171,
        "step": 1412
    },
    {
        "loss": 2.6922,
        "grad_norm": 2.8348238468170166,
        "learning_rate": 0.00015603633820917198,
        "epoch": 0.10529845741113347,
        "step": 1413
    },
    {
        "loss": 2.2685,
        "grad_norm": 2.1748666763305664,
        "learning_rate": 0.0001559780441944878,
        "epoch": 0.10537297861241524,
        "step": 1414
    },
    {
        "loss": 2.8374,
        "grad_norm": 3.605498790740967,
        "learning_rate": 0.00015591972246324097,
        "epoch": 0.105447499813697,
        "step": 1415
    },
    {
        "loss": 2.794,
        "grad_norm": 2.2804081439971924,
        "learning_rate": 0.00015586137304430858,
        "epoch": 0.10552202101497876,
        "step": 1416
    },
    {
        "loss": 2.1891,
        "grad_norm": 2.5973243713378906,
        "learning_rate": 0.00015580299596658129,
        "epoch": 0.10559654221626052,
        "step": 1417
    },
    {
        "loss": 2.5056,
        "grad_norm": 2.372558832168579,
        "learning_rate": 0.00015574459125896345,
        "epoch": 0.10567106341754229,
        "step": 1418
    },
    {
        "loss": 2.9629,
        "grad_norm": 2.0844860076904297,
        "learning_rate": 0.00015568615895037322,
        "epoch": 0.10574558461882405,
        "step": 1419
    },
    {
        "loss": 2.2595,
        "grad_norm": 2.068077802658081,
        "learning_rate": 0.00015562769906974234,
        "epoch": 0.10582010582010581,
        "step": 1420
    },
    {
        "loss": 2.2849,
        "grad_norm": 2.508237361907959,
        "learning_rate": 0.00015556921164601613,
        "epoch": 0.10589462702138759,
        "step": 1421
    },
    {
        "loss": 3.0285,
        "grad_norm": 2.5498504638671875,
        "learning_rate": 0.00015551069670815374,
        "epoch": 0.10596914822266935,
        "step": 1422
    },
    {
        "loss": 2.9417,
        "grad_norm": 2.936641216278076,
        "learning_rate": 0.0001554521542851278,
        "epoch": 0.10604366942395112,
        "step": 1423
    },
    {
        "loss": 2.5139,
        "grad_norm": 2.130225419998169,
        "learning_rate": 0.00015539358440592449,
        "epoch": 0.10611819062523288,
        "step": 1424
    },
    {
        "loss": 2.7986,
        "grad_norm": 2.5822627544403076,
        "learning_rate": 0.0001553349870995438,
        "epoch": 0.10619271182651464,
        "step": 1425
    },
    {
        "loss": 2.9085,
        "grad_norm": 2.40462327003479,
        "learning_rate": 0.00015527636239499914,
        "epoch": 0.1062672330277964,
        "step": 1426
    },
    {
        "loss": 2.2637,
        "grad_norm": 2.741459846496582,
        "learning_rate": 0.00015521771032131745,
        "epoch": 0.10634175422907817,
        "step": 1427
    },
    {
        "loss": 2.0024,
        "grad_norm": 2.438527822494507,
        "learning_rate": 0.0001551590309075394,
        "epoch": 0.10641627543035993,
        "step": 1428
    },
    {
        "loss": 1.6935,
        "grad_norm": 4.086498260498047,
        "learning_rate": 0.00015510032418271897,
        "epoch": 0.1064907966316417,
        "step": 1429
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.5459322929382324,
        "learning_rate": 0.00015504159017592391,
        "epoch": 0.10656531783292347,
        "step": 1430
    },
    {
        "loss": 2.7624,
        "grad_norm": 3.115643262863159,
        "learning_rate": 0.00015498282891623525,
        "epoch": 0.10663983903420524,
        "step": 1431
    },
    {
        "loss": 2.7257,
        "grad_norm": 3.1173927783966064,
        "learning_rate": 0.0001549240404327477,
        "epoch": 0.106714360235487,
        "step": 1432
    },
    {
        "loss": 2.3107,
        "grad_norm": 2.3388352394104004,
        "learning_rate": 0.00015486522475456927,
        "epoch": 0.10678888143676876,
        "step": 1433
    },
    {
        "loss": 0.794,
        "grad_norm": 3.3742642402648926,
        "learning_rate": 0.00015480638191082155,
        "epoch": 0.10686340263805053,
        "step": 1434
    },
    {
        "loss": 2.5869,
        "grad_norm": 2.7884058952331543,
        "learning_rate": 0.00015474751193063962,
        "epoch": 0.10693792383933229,
        "step": 1435
    },
    {
        "loss": 2.2806,
        "grad_norm": 3.016162872314453,
        "learning_rate": 0.00015468861484317187,
        "epoch": 0.10701244504061405,
        "step": 1436
    },
    {
        "loss": 2.1834,
        "grad_norm": 2.521033763885498,
        "learning_rate": 0.00015462969067758025,
        "epoch": 0.10708696624189581,
        "step": 1437
    },
    {
        "loss": 2.6719,
        "grad_norm": 3.387962579727173,
        "learning_rate": 0.00015457073946303993,
        "epoch": 0.10716148744317758,
        "step": 1438
    },
    {
        "loss": 2.0161,
        "grad_norm": 3.021188735961914,
        "learning_rate": 0.00015451176122873968,
        "epoch": 0.10723600864445935,
        "step": 1439
    },
    {
        "loss": 2.5129,
        "grad_norm": 2.5835161209106445,
        "learning_rate": 0.00015445275600388153,
        "epoch": 0.10731052984574112,
        "step": 1440
    },
    {
        "loss": 2.2625,
        "grad_norm": 1.8967866897583008,
        "learning_rate": 0.0001543937238176809,
        "epoch": 0.10738505104702288,
        "step": 1441
    },
    {
        "loss": 2.1445,
        "grad_norm": 2.0551226139068604,
        "learning_rate": 0.00015433466469936654,
        "epoch": 0.10745957224830464,
        "step": 1442
    },
    {
        "loss": 2.7224,
        "grad_norm": 2.2840068340301514,
        "learning_rate": 0.0001542755786781806,
        "epoch": 0.10753409344958641,
        "step": 1443
    },
    {
        "loss": 2.8157,
        "grad_norm": 2.3348028659820557,
        "learning_rate": 0.00015421646578337844,
        "epoch": 0.10760861465086817,
        "step": 1444
    },
    {
        "loss": 2.7172,
        "grad_norm": 2.220025062561035,
        "learning_rate": 0.00015415732604422884,
        "epoch": 0.10768313585214993,
        "step": 1445
    },
    {
        "loss": 2.4746,
        "grad_norm": 1.7683008909225464,
        "learning_rate": 0.00015409815949001378,
        "epoch": 0.1077576570534317,
        "step": 1446
    },
    {
        "loss": 2.0136,
        "grad_norm": 4.871327877044678,
        "learning_rate": 0.00015403896615002858,
        "epoch": 0.10783217825471346,
        "step": 1447
    },
    {
        "loss": 2.329,
        "grad_norm": 3.4480483531951904,
        "learning_rate": 0.00015397974605358184,
        "epoch": 0.10790669945599524,
        "step": 1448
    },
    {
        "loss": 2.5901,
        "grad_norm": 2.1167376041412354,
        "learning_rate": 0.00015392049922999528,
        "epoch": 0.107981220657277,
        "step": 1449
    },
    {
        "loss": 2.5678,
        "grad_norm": 2.5485358238220215,
        "learning_rate": 0.00015386122570860402,
        "epoch": 0.10805574185855876,
        "step": 1450
    },
    {
        "loss": 2.4852,
        "grad_norm": 3.3873937129974365,
        "learning_rate": 0.0001538019255187563,
        "epoch": 0.10813026305984053,
        "step": 1451
    },
    {
        "loss": 2.4918,
        "grad_norm": 4.103485107421875,
        "learning_rate": 0.00015374259868981355,
        "epoch": 0.10820478426112229,
        "step": 1452
    },
    {
        "loss": 2.2542,
        "grad_norm": 3.1849730014801025,
        "learning_rate": 0.00015368324525115047,
        "epoch": 0.10827930546240405,
        "step": 1453
    },
    {
        "loss": 2.6737,
        "grad_norm": 4.130378246307373,
        "learning_rate": 0.0001536238652321549,
        "epoch": 0.10835382666368581,
        "step": 1454
    },
    {
        "loss": 2.3309,
        "grad_norm": 2.697575092315674,
        "learning_rate": 0.0001535644586622278,
        "epoch": 0.10842834786496758,
        "step": 1455
    },
    {
        "loss": 1.7738,
        "grad_norm": 3.449428081512451,
        "learning_rate": 0.0001535050255707833,
        "epoch": 0.10850286906624934,
        "step": 1456
    },
    {
        "loss": 2.1565,
        "grad_norm": 3.1043503284454346,
        "learning_rate": 0.00015344556598724868,
        "epoch": 0.10857739026753112,
        "step": 1457
    },
    {
        "loss": 2.5856,
        "grad_norm": 3.163480281829834,
        "learning_rate": 0.00015338607994106433,
        "epoch": 0.10865191146881288,
        "step": 1458
    },
    {
        "loss": 2.5863,
        "grad_norm": 2.0396482944488525,
        "learning_rate": 0.00015332656746168378,
        "epoch": 0.10872643267009464,
        "step": 1459
    },
    {
        "loss": 2.7311,
        "grad_norm": 2.0516436100006104,
        "learning_rate": 0.00015326702857857354,
        "epoch": 0.10880095387137641,
        "step": 1460
    },
    {
        "loss": 2.2044,
        "grad_norm": 2.6881394386291504,
        "learning_rate": 0.00015320746332121328,
        "epoch": 0.10887547507265817,
        "step": 1461
    },
    {
        "loss": 2.8904,
        "grad_norm": 1.780775785446167,
        "learning_rate": 0.00015314787171909571,
        "epoch": 0.10894999627393993,
        "step": 1462
    },
    {
        "loss": 2.4138,
        "grad_norm": 2.826244354248047,
        "learning_rate": 0.00015308825380172663,
        "epoch": 0.1090245174752217,
        "step": 1463
    },
    {
        "loss": 2.8492,
        "grad_norm": 2.153153419494629,
        "learning_rate": 0.00015302860959862477,
        "epoch": 0.10909903867650346,
        "step": 1464
    },
    {
        "loss": 2.1977,
        "grad_norm": 2.6402485370635986,
        "learning_rate": 0.00015296893913932196,
        "epoch": 0.10917355987778524,
        "step": 1465
    },
    {
        "loss": 2.987,
        "grad_norm": 1.8608317375183105,
        "learning_rate": 0.00015290924245336295,
        "epoch": 0.109248081079067,
        "step": 1466
    },
    {
        "loss": 1.5074,
        "grad_norm": 3.7217559814453125,
        "learning_rate": 0.00015284951957030555,
        "epoch": 0.10932260228034876,
        "step": 1467
    },
    {
        "loss": 2.671,
        "grad_norm": 2.1379077434539795,
        "learning_rate": 0.00015278977051972055,
        "epoch": 0.10939712348163053,
        "step": 1468
    },
    {
        "loss": 2.699,
        "grad_norm": 2.668543815612793,
        "learning_rate": 0.00015272999533119162,
        "epoch": 0.10947164468291229,
        "step": 1469
    },
    {
        "loss": 2.2412,
        "grad_norm": 2.9179341793060303,
        "learning_rate": 0.00015267019403431543,
        "epoch": 0.10954616588419405,
        "step": 1470
    },
    {
        "loss": 2.4507,
        "grad_norm": 2.627570390701294,
        "learning_rate": 0.00015261036665870155,
        "epoch": 0.10962068708547582,
        "step": 1471
    },
    {
        "loss": 2.5743,
        "grad_norm": 1.882132649421692,
        "learning_rate": 0.00015255051323397246,
        "epoch": 0.10969520828675758,
        "step": 1472
    },
    {
        "loss": 2.5093,
        "grad_norm": 2.2473840713500977,
        "learning_rate": 0.0001524906337897636,
        "epoch": 0.10976972948803934,
        "step": 1473
    },
    {
        "loss": 2.2851,
        "grad_norm": 3.550959825515747,
        "learning_rate": 0.00015243072835572318,
        "epoch": 0.10984425068932112,
        "step": 1474
    },
    {
        "loss": 2.2052,
        "grad_norm": 1.3178577423095703,
        "learning_rate": 0.00015237079696151236,
        "epoch": 0.10991877189060288,
        "step": 1475
    },
    {
        "loss": 2.9384,
        "grad_norm": 2.4815404415130615,
        "learning_rate": 0.00015231083963680522,
        "epoch": 0.10999329309188464,
        "step": 1476
    },
    {
        "loss": 2.529,
        "grad_norm": 3.5153627395629883,
        "learning_rate": 0.00015225085641128847,
        "epoch": 0.11006781429316641,
        "step": 1477
    },
    {
        "loss": 3.0096,
        "grad_norm": 2.9894044399261475,
        "learning_rate": 0.0001521908473146618,
        "epoch": 0.11014233549444817,
        "step": 1478
    },
    {
        "loss": 2.5666,
        "grad_norm": 3.7512013912200928,
        "learning_rate": 0.00015213081237663774,
        "epoch": 0.11021685669572993,
        "step": 1479
    },
    {
        "loss": 2.4359,
        "grad_norm": 2.653073787689209,
        "learning_rate": 0.0001520707516269415,
        "epoch": 0.1102913778970117,
        "step": 1480
    },
    {
        "loss": 1.5427,
        "grad_norm": 2.4161102771759033,
        "learning_rate": 0.00015201066509531116,
        "epoch": 0.11036589909829346,
        "step": 1481
    },
    {
        "loss": 2.4831,
        "grad_norm": 4.106245517730713,
        "learning_rate": 0.0001519505528114975,
        "epoch": 0.11044042029957522,
        "step": 1482
    },
    {
        "loss": 1.4278,
        "grad_norm": 3.9237701892852783,
        "learning_rate": 0.0001518904148052641,
        "epoch": 0.110514941500857,
        "step": 1483
    },
    {
        "loss": 1.4613,
        "grad_norm": 3.286691904067993,
        "learning_rate": 0.00015183025110638724,
        "epoch": 0.11058946270213876,
        "step": 1484
    },
    {
        "loss": 2.6816,
        "grad_norm": 1.8267364501953125,
        "learning_rate": 0.0001517700617446559,
        "epoch": 0.11066398390342053,
        "step": 1485
    },
    {
        "loss": 2.1055,
        "grad_norm": 2.410656690597534,
        "learning_rate": 0.00015170984674987189,
        "epoch": 0.11073850510470229,
        "step": 1486
    },
    {
        "loss": 2.4763,
        "grad_norm": 2.2589335441589355,
        "learning_rate": 0.00015164960615184954,
        "epoch": 0.11081302630598405,
        "step": 1487
    },
    {
        "loss": 2.5031,
        "grad_norm": 2.8344528675079346,
        "learning_rate": 0.00015158933998041601,
        "epoch": 0.11088754750726582,
        "step": 1488
    },
    {
        "loss": 2.7944,
        "grad_norm": 3.4974730014801025,
        "learning_rate": 0.00015152904826541103,
        "epoch": 0.11096206870854758,
        "step": 1489
    },
    {
        "loss": 1.9468,
        "grad_norm": 3.240133762359619,
        "learning_rate": 0.000151468731036687,
        "epoch": 0.11103658990982934,
        "step": 1490
    },
    {
        "loss": 1.7967,
        "grad_norm": 3.051666498184204,
        "learning_rate": 0.00015140838832410896,
        "epoch": 0.1111111111111111,
        "step": 1491
    },
    {
        "loss": 3.1127,
        "grad_norm": 2.553645610809326,
        "learning_rate": 0.00015134802015755451,
        "epoch": 0.11118563231239288,
        "step": 1492
    },
    {
        "loss": 3.1623,
        "grad_norm": 1.8078131675720215,
        "learning_rate": 0.00015128762656691402,
        "epoch": 0.11126015351367465,
        "step": 1493
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.6361255645751953,
        "learning_rate": 0.00015122720758209022,
        "epoch": 0.11133467471495641,
        "step": 1494
    },
    {
        "loss": 2.7822,
        "grad_norm": 2.6751983165740967,
        "learning_rate": 0.00015116676323299858,
        "epoch": 0.11140919591623817,
        "step": 1495
    },
    {
        "loss": 1.9932,
        "grad_norm": 2.731684923171997,
        "learning_rate": 0.00015110629354956711,
        "epoch": 0.11148371711751993,
        "step": 1496
    },
    {
        "loss": 1.8824,
        "grad_norm": 1.4856969118118286,
        "learning_rate": 0.00015104579856173628,
        "epoch": 0.1115582383188017,
        "step": 1497
    },
    {
        "loss": 2.2996,
        "grad_norm": 3.284881830215454,
        "learning_rate": 0.00015098527829945915,
        "epoch": 0.11163275952008346,
        "step": 1498
    },
    {
        "loss": 2.6671,
        "grad_norm": 2.1516098976135254,
        "learning_rate": 0.00015092473279270128,
        "epoch": 0.11170728072136522,
        "step": 1499
    },
    {
        "loss": 2.5613,
        "grad_norm": 2.1370060443878174,
        "learning_rate": 0.00015086416207144075,
        "epoch": 0.11178180192264699,
        "step": 1500
    },
    {
        "loss": 2.8201,
        "grad_norm": 2.6171412467956543,
        "learning_rate": 0.00015080356616566806,
        "epoch": 0.11185632312392876,
        "step": 1501
    },
    {
        "loss": 2.8344,
        "grad_norm": 2.375994920730591,
        "learning_rate": 0.0001507429451053863,
        "epoch": 0.11193084432521053,
        "step": 1502
    },
    {
        "loss": 2.4996,
        "grad_norm": 2.586972236633301,
        "learning_rate": 0.00015068229892061088,
        "epoch": 0.11200536552649229,
        "step": 1503
    },
    {
        "loss": 3.1362,
        "grad_norm": 2.068894386291504,
        "learning_rate": 0.00015062162764136976,
        "epoch": 0.11207988672777405,
        "step": 1504
    },
    {
        "loss": 2.2426,
        "grad_norm": 2.4148480892181396,
        "learning_rate": 0.00015056093129770323,
        "epoch": 0.11215440792905582,
        "step": 1505
    },
    {
        "loss": 1.7783,
        "grad_norm": 2.498591899871826,
        "learning_rate": 0.00015050020991966406,
        "epoch": 0.11222892913033758,
        "step": 1506
    },
    {
        "loss": 1.8143,
        "grad_norm": 2.8437724113464355,
        "learning_rate": 0.0001504394635373174,
        "epoch": 0.11230345033161934,
        "step": 1507
    },
    {
        "loss": 2.4979,
        "grad_norm": 1.6190317869186401,
        "learning_rate": 0.00015037869218074076,
        "epoch": 0.1123779715329011,
        "step": 1508
    },
    {
        "loss": 2.4361,
        "grad_norm": 1.9756436347961426,
        "learning_rate": 0.00015031789588002407,
        "epoch": 0.11245249273418287,
        "step": 1509
    },
    {
        "loss": 2.5649,
        "grad_norm": 3.186159372329712,
        "learning_rate": 0.0001502570746652695,
        "epoch": 0.11252701393546465,
        "step": 1510
    },
    {
        "loss": 2.1872,
        "grad_norm": 3.269125461578369,
        "learning_rate": 0.0001501962285665917,
        "epoch": 0.11260153513674641,
        "step": 1511
    },
    {
        "loss": 2.5144,
        "grad_norm": 2.0349087715148926,
        "learning_rate": 0.0001501353576141175,
        "epoch": 0.11267605633802817,
        "step": 1512
    },
    {
        "loss": 2.2496,
        "grad_norm": 2.480778217315674,
        "learning_rate": 0.00015007446183798608,
        "epoch": 0.11275057753930993,
        "step": 1513
    },
    {
        "loss": 2.432,
        "grad_norm": 2.3194286823272705,
        "learning_rate": 0.00015001354126834905,
        "epoch": 0.1128250987405917,
        "step": 1514
    },
    {
        "loss": 1.6343,
        "grad_norm": 2.9231512546539307,
        "learning_rate": 0.00014995259593537002,
        "epoch": 0.11289961994187346,
        "step": 1515
    },
    {
        "loss": 1.5212,
        "grad_norm": 3.33008074760437,
        "learning_rate": 0.0001498916258692252,
        "epoch": 0.11297414114315522,
        "step": 1516
    },
    {
        "loss": 2.4858,
        "grad_norm": 3.8023438453674316,
        "learning_rate": 0.00014983063110010273,
        "epoch": 0.11304866234443699,
        "step": 1517
    },
    {
        "loss": 1.774,
        "grad_norm": 3.298661231994629,
        "learning_rate": 0.00014976961165820318,
        "epoch": 0.11312318354571875,
        "step": 1518
    },
    {
        "loss": 1.6117,
        "grad_norm": 2.9809443950653076,
        "learning_rate": 0.00014970856757373925,
        "epoch": 0.11319770474700053,
        "step": 1519
    },
    {
        "loss": 2.0117,
        "grad_norm": 2.604842185974121,
        "learning_rate": 0.00014964749887693587,
        "epoch": 0.11327222594828229,
        "step": 1520
    },
    {
        "loss": 1.846,
        "grad_norm": 2.924830913543701,
        "learning_rate": 0.00014958640559803012,
        "epoch": 0.11334674714956405,
        "step": 1521
    },
    {
        "loss": 2.5876,
        "grad_norm": 3.31667160987854,
        "learning_rate": 0.00014952528776727135,
        "epoch": 0.11342126835084582,
        "step": 1522
    },
    {
        "loss": 2.6221,
        "grad_norm": 2.7680931091308594,
        "learning_rate": 0.00014946414541492094,
        "epoch": 0.11349578955212758,
        "step": 1523
    },
    {
        "loss": 2.4748,
        "grad_norm": 3.7574079036712646,
        "learning_rate": 0.00014940297857125252,
        "epoch": 0.11357031075340934,
        "step": 1524
    },
    {
        "loss": 2.7047,
        "grad_norm": 2.8881852626800537,
        "learning_rate": 0.00014934178726655177,
        "epoch": 0.1136448319546911,
        "step": 1525
    },
    {
        "loss": 2.5248,
        "grad_norm": 2.074880838394165,
        "learning_rate": 0.00014928057153111647,
        "epoch": 0.11371935315597287,
        "step": 1526
    },
    {
        "loss": 2.5773,
        "grad_norm": 2.452993631362915,
        "learning_rate": 0.00014921933139525664,
        "epoch": 0.11379387435725465,
        "step": 1527
    },
    {
        "loss": 3.0915,
        "grad_norm": 2.1716527938842773,
        "learning_rate": 0.00014915806688929414,
        "epoch": 0.11386839555853641,
        "step": 1528
    },
    {
        "loss": 2.4431,
        "grad_norm": 2.4001455307006836,
        "learning_rate": 0.00014909677804356315,
        "epoch": 0.11394291675981817,
        "step": 1529
    },
    {
        "loss": 2.294,
        "grad_norm": 4.315352439880371,
        "learning_rate": 0.00014903546488840975,
        "epoch": 0.11401743796109994,
        "step": 1530
    },
    {
        "loss": 2.1134,
        "grad_norm": 4.053248405456543,
        "learning_rate": 0.00014897412745419207,
        "epoch": 0.1140919591623817,
        "step": 1531
    },
    {
        "loss": 1.959,
        "grad_norm": 2.215249538421631,
        "learning_rate": 0.00014891276577128028,
        "epoch": 0.11416648036366346,
        "step": 1532
    },
    {
        "loss": 1.2281,
        "grad_norm": 3.070335626602173,
        "learning_rate": 0.00014885137987005653,
        "epoch": 0.11424100156494522,
        "step": 1533
    },
    {
        "loss": 2.6957,
        "grad_norm": 2.2817347049713135,
        "learning_rate": 0.00014878996978091508,
        "epoch": 0.11431552276622699,
        "step": 1534
    },
    {
        "loss": 1.8388,
        "grad_norm": 2.838045597076416,
        "learning_rate": 0.000148728535534262,
        "epoch": 0.11439004396750875,
        "step": 1535
    },
    {
        "loss": 2.6401,
        "grad_norm": 2.384836196899414,
        "learning_rate": 0.00014866707716051548,
        "epoch": 0.11446456516879053,
        "step": 1536
    },
    {
        "loss": 1.9302,
        "grad_norm": 3.6092169284820557,
        "learning_rate": 0.00014860559469010543,
        "epoch": 0.11453908637007229,
        "step": 1537
    },
    {
        "loss": 2.6985,
        "grad_norm": 1.8790068626403809,
        "learning_rate": 0.000148544088153474,
        "epoch": 0.11461360757135405,
        "step": 1538
    },
    {
        "loss": 2.2767,
        "grad_norm": 4.014743328094482,
        "learning_rate": 0.00014848255758107497,
        "epoch": 0.11468812877263582,
        "step": 1539
    },
    {
        "loss": 1.6147,
        "grad_norm": 4.773458957672119,
        "learning_rate": 0.00014842100300337417,
        "epoch": 0.11476264997391758,
        "step": 1540
    },
    {
        "loss": 1.8434,
        "grad_norm": 2.6065242290496826,
        "learning_rate": 0.0001483594244508493,
        "epoch": 0.11483717117519934,
        "step": 1541
    },
    {
        "loss": 2.39,
        "grad_norm": 3.7707111835479736,
        "learning_rate": 0.00014829782195398993,
        "epoch": 0.1149116923764811,
        "step": 1542
    },
    {
        "loss": 2.616,
        "grad_norm": 2.1693131923675537,
        "learning_rate": 0.00014823619554329745,
        "epoch": 0.11498621357776287,
        "step": 1543
    },
    {
        "loss": 2.0078,
        "grad_norm": 4.38721227645874,
        "learning_rate": 0.00014817454524928517,
        "epoch": 0.11506073477904463,
        "step": 1544
    },
    {
        "loss": 2.9612,
        "grad_norm": 2.2916789054870605,
        "learning_rate": 0.0001481128711024781,
        "epoch": 0.11513525598032641,
        "step": 1545
    },
    {
        "loss": 2.4086,
        "grad_norm": 2.499795436859131,
        "learning_rate": 0.0001480511731334131,
        "epoch": 0.11520977718160817,
        "step": 1546
    },
    {
        "loss": 2.5705,
        "grad_norm": 2.7226827144622803,
        "learning_rate": 0.00014798945137263895,
        "epoch": 0.11528429838288994,
        "step": 1547
    },
    {
        "loss": 2.2123,
        "grad_norm": 4.361526966094971,
        "learning_rate": 0.00014792770585071605,
        "epoch": 0.1153588195841717,
        "step": 1548
    },
    {
        "loss": 2.5428,
        "grad_norm": 2.3678138256073,
        "learning_rate": 0.0001478659365982167,
        "epoch": 0.11543334078545346,
        "step": 1549
    },
    {
        "loss": 2.2179,
        "grad_norm": 3.473735809326172,
        "learning_rate": 0.0001478041436457248,
        "epoch": 0.11550786198673522,
        "step": 1550
    },
    {
        "loss": 2.8206,
        "grad_norm": 3.0251283645629883,
        "learning_rate": 0.00014774232702383607,
        "epoch": 0.11558238318801699,
        "step": 1551
    },
    {
        "loss": 2.6088,
        "grad_norm": 2.8184592723846436,
        "learning_rate": 0.000147680486763158,
        "epoch": 0.11565690438929875,
        "step": 1552
    },
    {
        "loss": 2.9908,
        "grad_norm": 2.2840769290924072,
        "learning_rate": 0.0001476186228943097,
        "epoch": 0.11573142559058051,
        "step": 1553
    },
    {
        "loss": 2.4868,
        "grad_norm": 3.2831335067749023,
        "learning_rate": 0.00014755673544792196,
        "epoch": 0.11580594679186229,
        "step": 1554
    },
    {
        "loss": 1.6135,
        "grad_norm": 3.2594470977783203,
        "learning_rate": 0.00014749482445463734,
        "epoch": 0.11588046799314405,
        "step": 1555
    },
    {
        "loss": 2.7686,
        "grad_norm": 2.2908437252044678,
        "learning_rate": 0.00014743288994510995,
        "epoch": 0.11595498919442582,
        "step": 1556
    },
    {
        "loss": 2.3493,
        "grad_norm": 3.4880588054656982,
        "learning_rate": 0.00014737093195000566,
        "epoch": 0.11602951039570758,
        "step": 1557
    },
    {
        "loss": 3.0975,
        "grad_norm": 2.8076419830322266,
        "learning_rate": 0.00014730895050000183,
        "epoch": 0.11610403159698934,
        "step": 1558
    },
    {
        "loss": 2.5099,
        "grad_norm": 2.344449996948242,
        "learning_rate": 0.00014724694562578755,
        "epoch": 0.1161785527982711,
        "step": 1559
    },
    {
        "loss": 2.7669,
        "grad_norm": 1.8885215520858765,
        "learning_rate": 0.00014718491735806345,
        "epoch": 0.11625307399955287,
        "step": 1560
    },
    {
        "loss": 1.9972,
        "grad_norm": 3.9655466079711914,
        "learning_rate": 0.00014712286572754173,
        "epoch": 0.11632759520083463,
        "step": 1561
    },
    {
        "loss": 2.0064,
        "grad_norm": 3.4943344593048096,
        "learning_rate": 0.00014706079076494622,
        "epoch": 0.1164021164021164,
        "step": 1562
    },
    {
        "loss": 2.6013,
        "grad_norm": 2.698716163635254,
        "learning_rate": 0.00014699869250101228,
        "epoch": 0.11647663760339817,
        "step": 1563
    },
    {
        "loss": 2.3291,
        "grad_norm": 3.827758550643921,
        "learning_rate": 0.00014693657096648675,
        "epoch": 0.11655115880467994,
        "step": 1564
    },
    {
        "loss": 2.7157,
        "grad_norm": 2.842322826385498,
        "learning_rate": 0.00014687442619212807,
        "epoch": 0.1166256800059617,
        "step": 1565
    },
    {
        "loss": 2.1856,
        "grad_norm": 2.51820707321167,
        "learning_rate": 0.00014681225820870614,
        "epoch": 0.11670020120724346,
        "step": 1566
    },
    {
        "loss": 2.6686,
        "grad_norm": 2.4366378784179688,
        "learning_rate": 0.00014675006704700235,
        "epoch": 0.11677472240852523,
        "step": 1567
    },
    {
        "loss": 1.5929,
        "grad_norm": 2.960108757019043,
        "learning_rate": 0.00014668785273780956,
        "epoch": 0.11684924360980699,
        "step": 1568
    },
    {
        "loss": 2.8637,
        "grad_norm": 3.496567487716675,
        "learning_rate": 0.00014662561531193218,
        "epoch": 0.11692376481108875,
        "step": 1569
    },
    {
        "loss": 2.5639,
        "grad_norm": 2.338862419128418,
        "learning_rate": 0.000146563354800186,
        "epoch": 0.11699828601237051,
        "step": 1570
    },
    {
        "loss": 1.7062,
        "grad_norm": 2.2658047676086426,
        "learning_rate": 0.00014650107123339814,
        "epoch": 0.11707280721365228,
        "step": 1571
    },
    {
        "loss": 2.6724,
        "grad_norm": 3.740013360977173,
        "learning_rate": 0.00014643876464240734,
        "epoch": 0.11714732841493405,
        "step": 1572
    },
    {
        "loss": 2.7218,
        "grad_norm": 2.409212827682495,
        "learning_rate": 0.00014637643505806356,
        "epoch": 0.11722184961621582,
        "step": 1573
    },
    {
        "loss": 2.7141,
        "grad_norm": 2.851297378540039,
        "learning_rate": 0.00014631408251122822,
        "epoch": 0.11729637081749758,
        "step": 1574
    },
    {
        "loss": 2.3685,
        "grad_norm": 2.242497205734253,
        "learning_rate": 0.00014625170703277414,
        "epoch": 0.11737089201877934,
        "step": 1575
    },
    {
        "loss": 2.5793,
        "grad_norm": 1.9755001068115234,
        "learning_rate": 0.00014618930865358548,
        "epoch": 0.1174454132200611,
        "step": 1576
    },
    {
        "loss": 2.5975,
        "grad_norm": 3.5001282691955566,
        "learning_rate": 0.00014612688740455772,
        "epoch": 0.11751993442134287,
        "step": 1577
    },
    {
        "loss": 2.4187,
        "grad_norm": 2.963479518890381,
        "learning_rate": 0.0001460644433165976,
        "epoch": 0.11759445562262463,
        "step": 1578
    },
    {
        "loss": 2.0843,
        "grad_norm": 3.1121697425842285,
        "learning_rate": 0.00014600197642062327,
        "epoch": 0.1176689768239064,
        "step": 1579
    },
    {
        "loss": 2.009,
        "grad_norm": 3.5615737438201904,
        "learning_rate": 0.00014593948674756417,
        "epoch": 0.11774349802518816,
        "step": 1580
    },
    {
        "loss": 2.7092,
        "grad_norm": 2.194835662841797,
        "learning_rate": 0.0001458769743283609,
        "epoch": 0.11781801922646994,
        "step": 1581
    },
    {
        "loss": 3.0157,
        "grad_norm": 1.9917054176330566,
        "learning_rate": 0.00014581443919396548,
        "epoch": 0.1178925404277517,
        "step": 1582
    },
    {
        "loss": 2.4318,
        "grad_norm": 1.7130913734436035,
        "learning_rate": 0.0001457518813753411,
        "epoch": 0.11796706162903346,
        "step": 1583
    },
    {
        "loss": 1.7416,
        "grad_norm": 3.727907657623291,
        "learning_rate": 0.00014568930090346215,
        "epoch": 0.11804158283031523,
        "step": 1584
    },
    {
        "loss": 2.8471,
        "grad_norm": 2.6677024364471436,
        "learning_rate": 0.00014562669780931428,
        "epoch": 0.11811610403159699,
        "step": 1585
    },
    {
        "loss": 2.1478,
        "grad_norm": 1.826324701309204,
        "learning_rate": 0.00014556407212389436,
        "epoch": 0.11819062523287875,
        "step": 1586
    },
    {
        "loss": 2.7291,
        "grad_norm": 2.1946964263916016,
        "learning_rate": 0.00014550142387821037,
        "epoch": 0.11826514643416051,
        "step": 1587
    },
    {
        "loss": 1.7391,
        "grad_norm": 3.7029361724853516,
        "learning_rate": 0.00014543875310328155,
        "epoch": 0.11833966763544228,
        "step": 1588
    },
    {
        "loss": 1.847,
        "grad_norm": 3.4369473457336426,
        "learning_rate": 0.0001453760598301382,
        "epoch": 0.11841418883672405,
        "step": 1589
    },
    {
        "loss": 3.1157,
        "grad_norm": 2.116121530532837,
        "learning_rate": 0.00014531334408982184,
        "epoch": 0.11848871003800582,
        "step": 1590
    },
    {
        "loss": 2.1356,
        "grad_norm": 3.0618765354156494,
        "learning_rate": 0.00014525060591338512,
        "epoch": 0.11856323123928758,
        "step": 1591
    },
    {
        "loss": 2.7415,
        "grad_norm": 2.494633674621582,
        "learning_rate": 0.00014518784533189177,
        "epoch": 0.11863775244056934,
        "step": 1592
    },
    {
        "loss": 2.7428,
        "grad_norm": 2.6000664234161377,
        "learning_rate": 0.00014512506237641655,
        "epoch": 0.11871227364185111,
        "step": 1593
    },
    {
        "loss": 2.6004,
        "grad_norm": 2.994640588760376,
        "learning_rate": 0.00014506225707804538,
        "epoch": 0.11878679484313287,
        "step": 1594
    },
    {
        "loss": 2.6391,
        "grad_norm": 2.3595478534698486,
        "learning_rate": 0.0001449994294678752,
        "epoch": 0.11886131604441463,
        "step": 1595
    },
    {
        "loss": 2.4543,
        "grad_norm": 2.447404623031616,
        "learning_rate": 0.00014493657957701406,
        "epoch": 0.1189358372456964,
        "step": 1596
    },
    {
        "loss": 2.8018,
        "grad_norm": 3.9286816120147705,
        "learning_rate": 0.00014487370743658098,
        "epoch": 0.11901035844697816,
        "step": 1597
    },
    {
        "loss": 2.1981,
        "grad_norm": 2.1834776401519775,
        "learning_rate": 0.00014481081307770604,
        "epoch": 0.11908487964825994,
        "step": 1598
    },
    {
        "loss": 1.9092,
        "grad_norm": 2.4304006099700928,
        "learning_rate": 0.00014474789653153022,
        "epoch": 0.1191594008495417,
        "step": 1599
    },
    {
        "loss": 2.2771,
        "grad_norm": 2.6147091388702393,
        "learning_rate": 0.00014468495782920567,
        "epoch": 0.11923392205082346,
        "step": 1600
    },
    {
        "loss": 2.5868,
        "grad_norm": 3.1923375129699707,
        "learning_rate": 0.00014462199700189532,
        "epoch": 0.11930844325210523,
        "step": 1601
    },
    {
        "loss": 2.9327,
        "grad_norm": 3.716933488845825,
        "learning_rate": 0.00014455901408077314,
        "epoch": 0.11938296445338699,
        "step": 1602
    },
    {
        "loss": 2.4044,
        "grad_norm": 3.054429054260254,
        "learning_rate": 0.00014449600909702409,
        "epoch": 0.11945748565466875,
        "step": 1603
    },
    {
        "loss": 2.0703,
        "grad_norm": 3.738633871078491,
        "learning_rate": 0.00014443298208184394,
        "epoch": 0.11953200685595052,
        "step": 1604
    },
    {
        "loss": 3.0473,
        "grad_norm": 1.2264106273651123,
        "learning_rate": 0.0001443699330664395,
        "epoch": 0.11960652805723228,
        "step": 1605
    },
    {
        "loss": 2.3116,
        "grad_norm": 3.259909152984619,
        "learning_rate": 0.00014430686208202835,
        "epoch": 0.11968104925851404,
        "step": 1606
    },
    {
        "loss": 2.2334,
        "grad_norm": 2.7060861587524414,
        "learning_rate": 0.00014424376915983902,
        "epoch": 0.11975557045979582,
        "step": 1607
    },
    {
        "loss": 3.1936,
        "grad_norm": 2.7847750186920166,
        "learning_rate": 0.00014418065433111085,
        "epoch": 0.11983009166107758,
        "step": 1608
    },
    {
        "loss": 2.6904,
        "grad_norm": 4.360015392303467,
        "learning_rate": 0.00014411751762709408,
        "epoch": 0.11990461286235934,
        "step": 1609
    },
    {
        "loss": 2.6021,
        "grad_norm": 2.2168328762054443,
        "learning_rate": 0.00014405435907904977,
        "epoch": 0.11997913406364111,
        "step": 1610
    },
    {
        "loss": 2.6611,
        "grad_norm": 1.7865924835205078,
        "learning_rate": 0.00014399117871824978,
        "epoch": 0.12005365526492287,
        "step": 1611
    },
    {
        "loss": 2.0875,
        "grad_norm": 2.4797017574310303,
        "learning_rate": 0.00014392797657597676,
        "epoch": 0.12012817646620463,
        "step": 1612
    },
    {
        "loss": 2.7889,
        "grad_norm": 2.7751731872558594,
        "learning_rate": 0.0001438647526835242,
        "epoch": 0.1202026976674864,
        "step": 1613
    },
    {
        "loss": 1.7057,
        "grad_norm": 3.894299268722534,
        "learning_rate": 0.00014380150707219626,
        "epoch": 0.12027721886876816,
        "step": 1614
    },
    {
        "loss": 2.2859,
        "grad_norm": 1.9855769872665405,
        "learning_rate": 0.00014373823977330792,
        "epoch": 0.12035174007004992,
        "step": 1615
    },
    {
        "loss": 2.2221,
        "grad_norm": 2.4175679683685303,
        "learning_rate": 0.00014367495081818495,
        "epoch": 0.1204262612713317,
        "step": 1616
    },
    {
        "loss": 1.3692,
        "grad_norm": 3.52238392829895,
        "learning_rate": 0.00014361164023816376,
        "epoch": 0.12050078247261346,
        "step": 1617
    },
    {
        "loss": 2.7059,
        "grad_norm": 1.831876516342163,
        "learning_rate": 0.00014354830806459148,
        "epoch": 0.12057530367389523,
        "step": 1618
    },
    {
        "loss": 2.5792,
        "grad_norm": 2.2624292373657227,
        "learning_rate": 0.0001434849543288259,
        "epoch": 0.12064982487517699,
        "step": 1619
    },
    {
        "loss": 3.0059,
        "grad_norm": 2.3818180561065674,
        "learning_rate": 0.00014342157906223563,
        "epoch": 0.12072434607645875,
        "step": 1620
    },
    {
        "loss": 1.5363,
        "grad_norm": 3.9576451778411865,
        "learning_rate": 0.00014335818229619976,
        "epoch": 0.12079886727774052,
        "step": 1621
    },
    {
        "loss": 2.0335,
        "grad_norm": 2.473778009414673,
        "learning_rate": 0.0001432947640621081,
        "epoch": 0.12087338847902228,
        "step": 1622
    },
    {
        "loss": 2.7214,
        "grad_norm": 2.7743725776672363,
        "learning_rate": 0.00014323132439136114,
        "epoch": 0.12094790968030404,
        "step": 1623
    },
    {
        "loss": 2.4085,
        "grad_norm": 2.9411463737487793,
        "learning_rate": 0.0001431678633153699,
        "epoch": 0.1210224308815858,
        "step": 1624
    },
    {
        "loss": 2.2591,
        "grad_norm": 2.6286208629608154,
        "learning_rate": 0.0001431043808655561,
        "epoch": 0.12109695208286758,
        "step": 1625
    },
    {
        "loss": 1.3877,
        "grad_norm": 2.1992859840393066,
        "learning_rate": 0.00014304087707335192,
        "epoch": 0.12117147328414934,
        "step": 1626
    },
    {
        "loss": 2.8456,
        "grad_norm": 2.296673536300659,
        "learning_rate": 0.00014297735197020013,
        "epoch": 0.12124599448543111,
        "step": 1627
    },
    {
        "loss": 2.817,
        "grad_norm": 1.7462236881256104,
        "learning_rate": 0.00014291380558755423,
        "epoch": 0.12132051568671287,
        "step": 1628
    },
    {
        "loss": 3.1696,
        "grad_norm": 2.2154057025909424,
        "learning_rate": 0.00014285023795687795,
        "epoch": 0.12139503688799463,
        "step": 1629
    },
    {
        "loss": 2.8535,
        "grad_norm": 2.936635732650757,
        "learning_rate": 0.0001427866491096458,
        "epoch": 0.1214695580892764,
        "step": 1630
    },
    {
        "loss": 3.1953,
        "grad_norm": 6.705221652984619,
        "learning_rate": 0.0001427230390773427,
        "epoch": 0.12154407929055816,
        "step": 1631
    },
    {
        "loss": 1.693,
        "grad_norm": 4.83030891418457,
        "learning_rate": 0.00014265940789146402,
        "epoch": 0.12161860049183992,
        "step": 1632
    },
    {
        "loss": 2.2372,
        "grad_norm": 3.2168116569519043,
        "learning_rate": 0.00014259575558351568,
        "epoch": 0.12169312169312169,
        "step": 1633
    },
    {
        "loss": 2.7671,
        "grad_norm": 2.2155539989471436,
        "learning_rate": 0.000142532082185014,
        "epoch": 0.12176764289440346,
        "step": 1634
    },
    {
        "loss": 1.8158,
        "grad_norm": 3.8479573726654053,
        "learning_rate": 0.00014246838772748576,
        "epoch": 0.12184216409568523,
        "step": 1635
    },
    {
        "loss": 1.0749,
        "grad_norm": 3.4983065128326416,
        "learning_rate": 0.0001424046722424682,
        "epoch": 0.12191668529696699,
        "step": 1636
    },
    {
        "loss": 2.6655,
        "grad_norm": 3.4526491165161133,
        "learning_rate": 0.00014234093576150894,
        "epoch": 0.12199120649824875,
        "step": 1637
    },
    {
        "loss": 1.6577,
        "grad_norm": 4.400582313537598,
        "learning_rate": 0.00014227717831616593,
        "epoch": 0.12206572769953052,
        "step": 1638
    },
    {
        "loss": 1.8341,
        "grad_norm": 3.134706974029541,
        "learning_rate": 0.00014221339993800765,
        "epoch": 0.12214024890081228,
        "step": 1639
    },
    {
        "loss": 2.5975,
        "grad_norm": 3.8422634601593018,
        "learning_rate": 0.00014214960065861285,
        "epoch": 0.12221477010209404,
        "step": 1640
    },
    {
        "loss": 1.8511,
        "grad_norm": 3.117493152618408,
        "learning_rate": 0.00014208578050957065,
        "epoch": 0.1222892913033758,
        "step": 1641
    },
    {
        "loss": 2.36,
        "grad_norm": 2.382979154586792,
        "learning_rate": 0.00014202193952248042,
        "epoch": 0.12236381250465758,
        "step": 1642
    },
    {
        "loss": 2.791,
        "grad_norm": 2.284799814224243,
        "learning_rate": 0.00014195807772895202,
        "epoch": 0.12243833370593935,
        "step": 1643
    },
    {
        "loss": 2.3059,
        "grad_norm": 2.6052186489105225,
        "learning_rate": 0.00014189419516060544,
        "epoch": 0.12251285490722111,
        "step": 1644
    },
    {
        "loss": 2.7003,
        "grad_norm": 1.9040511846542358,
        "learning_rate": 0.00014183029184907105,
        "epoch": 0.12258737610850287,
        "step": 1645
    },
    {
        "loss": 1.6282,
        "grad_norm": 2.892019510269165,
        "learning_rate": 0.00014176636782598957,
        "epoch": 0.12266189730978463,
        "step": 1646
    },
    {
        "loss": 2.6265,
        "grad_norm": 3.9023778438568115,
        "learning_rate": 0.0001417024231230117,
        "epoch": 0.1227364185110664,
        "step": 1647
    },
    {
        "loss": 2.367,
        "grad_norm": 2.273503541946411,
        "learning_rate": 0.0001416384577717987,
        "epoch": 0.12281093971234816,
        "step": 1648
    },
    {
        "loss": 2.9544,
        "grad_norm": 2.7735848426818848,
        "learning_rate": 0.00014157447180402185,
        "epoch": 0.12288546091362992,
        "step": 1649
    },
    {
        "loss": 2.2778,
        "grad_norm": 2.3195128440856934,
        "learning_rate": 0.00014151046525136268,
        "epoch": 0.12295998211491169,
        "step": 1650
    },
    {
        "loss": 1.2539,
        "grad_norm": 2.6269683837890625,
        "learning_rate": 0.00014144643814551297,
        "epoch": 0.12303450331619346,
        "step": 1651
    },
    {
        "loss": 2.5942,
        "grad_norm": 3.128788948059082,
        "learning_rate": 0.00014138239051817464,
        "epoch": 0.12310902451747523,
        "step": 1652
    },
    {
        "loss": 2.6541,
        "grad_norm": 1.358940839767456,
        "learning_rate": 0.00014131832240105974,
        "epoch": 0.12318354571875699,
        "step": 1653
    },
    {
        "loss": 2.5478,
        "grad_norm": 2.969353437423706,
        "learning_rate": 0.0001412542338258905,
        "epoch": 0.12325806692003875,
        "step": 1654
    },
    {
        "loss": 2.0641,
        "grad_norm": 3.2008297443389893,
        "learning_rate": 0.00014119012482439927,
        "epoch": 0.12333258812132052,
        "step": 1655
    },
    {
        "loss": 2.6849,
        "grad_norm": 3.023683547973633,
        "learning_rate": 0.00014112599542832855,
        "epoch": 0.12340710932260228,
        "step": 1656
    },
    {
        "loss": 2.4425,
        "grad_norm": 1.341539978981018,
        "learning_rate": 0.00014106184566943085,
        "epoch": 0.12348163052388404,
        "step": 1657
    },
    {
        "loss": 2.2952,
        "grad_norm": 3.32137131690979,
        "learning_rate": 0.00014099767557946882,
        "epoch": 0.1235561517251658,
        "step": 1658
    },
    {
        "loss": 2.6232,
        "grad_norm": 2.7126293182373047,
        "learning_rate": 0.00014093348519021522,
        "epoch": 0.12363067292644757,
        "step": 1659
    },
    {
        "loss": 1.7801,
        "grad_norm": 1.485321283340454,
        "learning_rate": 0.00014086927453345278,
        "epoch": 0.12370519412772935,
        "step": 1660
    },
    {
        "loss": 2.3878,
        "grad_norm": 3.164393424987793,
        "learning_rate": 0.00014080504364097433,
        "epoch": 0.12377971532901111,
        "step": 1661
    },
    {
        "loss": 1.7938,
        "grad_norm": 2.4051902294158936,
        "learning_rate": 0.00014074079254458263,
        "epoch": 0.12385423653029287,
        "step": 1662
    },
    {
        "loss": 2.5332,
        "grad_norm": 2.3567168712615967,
        "learning_rate": 0.00014067652127609058,
        "epoch": 0.12392875773157463,
        "step": 1663
    },
    {
        "loss": 2.3853,
        "grad_norm": 2.1782383918762207,
        "learning_rate": 0.00014061222986732087,
        "epoch": 0.1240032789328564,
        "step": 1664
    },
    {
        "loss": 3.179,
        "grad_norm": 2.749436616897583,
        "learning_rate": 0.00014054791835010642,
        "epoch": 0.12407780013413816,
        "step": 1665
    },
    {
        "loss": 1.8542,
        "grad_norm": 2.4074597358703613,
        "learning_rate": 0.00014048358675628988,
        "epoch": 0.12415232133541992,
        "step": 1666
    },
    {
        "loss": 2.031,
        "grad_norm": 2.915884494781494,
        "learning_rate": 0.00014041923511772398,
        "epoch": 0.12422684253670169,
        "step": 1667
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.8916537761688232,
        "learning_rate": 0.00014035486346627125,
        "epoch": 0.12430136373798345,
        "step": 1668
    },
    {
        "loss": 2.6348,
        "grad_norm": 2.5166633129119873,
        "learning_rate": 0.0001402904718338043,
        "epoch": 0.12437588493926523,
        "step": 1669
    },
    {
        "loss": 2.0903,
        "grad_norm": 3.39632248878479,
        "learning_rate": 0.00014022606025220543,
        "epoch": 0.12445040614054699,
        "step": 1670
    },
    {
        "loss": 3.0419,
        "grad_norm": 2.9970927238464355,
        "learning_rate": 0.00014016162875336703,
        "epoch": 0.12452492734182875,
        "step": 1671
    },
    {
        "loss": 2.4252,
        "grad_norm": 3.0784201622009277,
        "learning_rate": 0.00014009717736919114,
        "epoch": 0.12459944854311052,
        "step": 1672
    },
    {
        "loss": 2.7044,
        "grad_norm": 2.4827239513397217,
        "learning_rate": 0.00014003270613158984,
        "epoch": 0.12467396974439228,
        "step": 1673
    },
    {
        "loss": 1.6911,
        "grad_norm": 2.700869560241699,
        "learning_rate": 0.00013996821507248489,
        "epoch": 0.12474849094567404,
        "step": 1674
    },
    {
        "loss": 2.424,
        "grad_norm": 1.857944369316101,
        "learning_rate": 0.000139903704223808,
        "epoch": 0.1248230121469558,
        "step": 1675
    },
    {
        "loss": 2.4575,
        "grad_norm": 3.8685622215270996,
        "learning_rate": 0.0001398391736175005,
        "epoch": 0.12489753334823757,
        "step": 1676
    },
    {
        "loss": 2.5768,
        "grad_norm": 2.5363271236419678,
        "learning_rate": 0.00013977462328551368,
        "epoch": 0.12497205454951933,
        "step": 1677
    },
    {
        "loss": 2.2394,
        "grad_norm": 2.7402660846710205,
        "learning_rate": 0.00013971005325980849,
        "epoch": 0.1250465757508011,
        "step": 1678
    },
    {
        "loss": 3.077,
        "grad_norm": 3.174971580505371,
        "learning_rate": 0.0001396454635723557,
        "epoch": 0.12512109695208287,
        "step": 1679
    },
    {
        "loss": 3.1208,
        "grad_norm": 2.619752883911133,
        "learning_rate": 0.00013958085425513572,
        "epoch": 0.12519561815336464,
        "step": 1680
    },
    {
        "loss": 2.107,
        "grad_norm": 3.048218250274658,
        "learning_rate": 0.00013951622534013878,
        "epoch": 0.1252701393546464,
        "step": 1681
    },
    {
        "loss": 1.9354,
        "grad_norm": 2.1748569011688232,
        "learning_rate": 0.0001394515768593648,
        "epoch": 0.12534466055592816,
        "step": 1682
    },
    {
        "loss": 2.0376,
        "grad_norm": 2.995426893234253,
        "learning_rate": 0.0001393869088448233,
        "epoch": 0.12541918175720992,
        "step": 1683
    },
    {
        "loss": 1.6541,
        "grad_norm": 3.915696620941162,
        "learning_rate": 0.00013932222132853354,
        "epoch": 0.1254937029584917,
        "step": 1684
    },
    {
        "loss": 2.3341,
        "grad_norm": 1.817598581314087,
        "learning_rate": 0.00013925751434252446,
        "epoch": 0.12556822415977345,
        "step": 1685
    },
    {
        "loss": 2.0648,
        "grad_norm": 3.0504941940307617,
        "learning_rate": 0.00013919278791883462,
        "epoch": 0.1256427453610552,
        "step": 1686
    },
    {
        "loss": 2.5921,
        "grad_norm": 2.1148767471313477,
        "learning_rate": 0.0001391280420895121,
        "epoch": 0.12571726656233698,
        "step": 1687
    },
    {
        "loss": 1.6745,
        "grad_norm": 3.4859139919281006,
        "learning_rate": 0.00013906327688661477,
        "epoch": 0.12579178776361874,
        "step": 1688
    },
    {
        "loss": 2.6503,
        "grad_norm": 2.648406505584717,
        "learning_rate": 0.00013899849234220998,
        "epoch": 0.1258663089649005,
        "step": 1689
    },
    {
        "loss": 2.2181,
        "grad_norm": 3.392454147338867,
        "learning_rate": 0.0001389336884883747,
        "epoch": 0.12594083016618227,
        "step": 1690
    },
    {
        "loss": 2.3442,
        "grad_norm": 2.090634346008301,
        "learning_rate": 0.0001388688653571954,
        "epoch": 0.12601535136746406,
        "step": 1691
    },
    {
        "loss": 2.3645,
        "grad_norm": 3.4871654510498047,
        "learning_rate": 0.00013880402298076816,
        "epoch": 0.12608987256874582,
        "step": 1692
    },
    {
        "loss": 2.7057,
        "grad_norm": 3.4750466346740723,
        "learning_rate": 0.0001387391613911985,
        "epoch": 0.12616439377002758,
        "step": 1693
    },
    {
        "loss": 2.6771,
        "grad_norm": 2.7466931343078613,
        "learning_rate": 0.00013867428062060163,
        "epoch": 0.12623891497130935,
        "step": 1694
    },
    {
        "loss": 2.3818,
        "grad_norm": 2.3978586196899414,
        "learning_rate": 0.00013860938070110209,
        "epoch": 0.1263134361725911,
        "step": 1695
    },
    {
        "loss": 3.1494,
        "grad_norm": 2.7734804153442383,
        "learning_rate": 0.0001385444616648339,
        "epoch": 0.12638795737387287,
        "step": 1696
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.9514386653900146,
        "learning_rate": 0.00013847952354394068,
        "epoch": 0.12646247857515464,
        "step": 1697
    },
    {
        "loss": 2.6289,
        "grad_norm": 3.6393423080444336,
        "learning_rate": 0.00013841456637057536,
        "epoch": 0.1265369997764364,
        "step": 1698
    },
    {
        "loss": 2.1777,
        "grad_norm": 3.0682027339935303,
        "learning_rate": 0.00013834959017690042,
        "epoch": 0.12661152097771816,
        "step": 1699
    },
    {
        "loss": 2.295,
        "grad_norm": 2.0121848583221436,
        "learning_rate": 0.00013828459499508767,
        "epoch": 0.12668604217899992,
        "step": 1700
    },
    {
        "loss": 2.3046,
        "grad_norm": 3.193424701690674,
        "learning_rate": 0.00013821958085731835,
        "epoch": 0.1267605633802817,
        "step": 1701
    },
    {
        "loss": 2.7484,
        "grad_norm": 2.427454948425293,
        "learning_rate": 0.0001381545477957831,
        "epoch": 0.12683508458156345,
        "step": 1702
    },
    {
        "loss": 1.7441,
        "grad_norm": 3.3369100093841553,
        "learning_rate": 0.00013808949584268195,
        "epoch": 0.12690960578284521,
        "step": 1703
    },
    {
        "loss": 2.3601,
        "grad_norm": 3.0908827781677246,
        "learning_rate": 0.00013802442503022417,
        "epoch": 0.12698412698412698,
        "step": 1704
    },
    {
        "loss": 2.7553,
        "grad_norm": 2.663522481918335,
        "learning_rate": 0.00013795933539062852,
        "epoch": 0.12705864818540874,
        "step": 1705
    },
    {
        "loss": 2.4623,
        "grad_norm": 1.845503330230713,
        "learning_rate": 0.00013789422695612298,
        "epoch": 0.1271331693866905,
        "step": 1706
    },
    {
        "loss": 2.5237,
        "grad_norm": 2.7504444122314453,
        "learning_rate": 0.00013782909975894485,
        "epoch": 0.12720769058797227,
        "step": 1707
    },
    {
        "loss": 2.7547,
        "grad_norm": 2.574281930923462,
        "learning_rate": 0.0001377639538313408,
        "epoch": 0.12728221178925403,
        "step": 1708
    },
    {
        "loss": 2.0752,
        "grad_norm": 3.0243444442749023,
        "learning_rate": 0.00013769878920556666,
        "epoch": 0.12735673299053582,
        "step": 1709
    },
    {
        "loss": 1.9028,
        "grad_norm": 4.1442975997924805,
        "learning_rate": 0.00013763360591388757,
        "epoch": 0.12743125419181758,
        "step": 1710
    },
    {
        "loss": 2.028,
        "grad_norm": 3.2742292881011963,
        "learning_rate": 0.00013756840398857794,
        "epoch": 0.12750577539309935,
        "step": 1711
    },
    {
        "loss": 2.6445,
        "grad_norm": 1.9347760677337646,
        "learning_rate": 0.00013750318346192138,
        "epoch": 0.1275802965943811,
        "step": 1712
    },
    {
        "loss": 1.9482,
        "grad_norm": 3.472012996673584,
        "learning_rate": 0.00013743794436621066,
        "epoch": 0.12765481779566287,
        "step": 1713
    },
    {
        "loss": 1.6693,
        "grad_norm": 2.920567750930786,
        "learning_rate": 0.00013737268673374783,
        "epoch": 0.12772933899694464,
        "step": 1714
    },
    {
        "loss": 2.0124,
        "grad_norm": 2.094161033630371,
        "learning_rate": 0.00013730741059684415,
        "epoch": 0.1278038601982264,
        "step": 1715
    },
    {
        "loss": 2.6673,
        "grad_norm": 1.9469510316848755,
        "learning_rate": 0.00013724211598781978,
        "epoch": 0.12787838139950816,
        "step": 1716
    },
    {
        "loss": 1.7015,
        "grad_norm": 1.5806870460510254,
        "learning_rate": 0.00013717680293900446,
        "epoch": 0.12795290260078993,
        "step": 1717
    },
    {
        "loss": 2.7891,
        "grad_norm": 2.303704023361206,
        "learning_rate": 0.00013711147148273662,
        "epoch": 0.1280274238020717,
        "step": 1718
    },
    {
        "loss": 2.1969,
        "grad_norm": 3.092092990875244,
        "learning_rate": 0.0001370461216513641,
        "epoch": 0.12810194500335345,
        "step": 1719
    },
    {
        "loss": 1.9262,
        "grad_norm": 3.383136034011841,
        "learning_rate": 0.00013698075347724374,
        "epoch": 0.12817646620463521,
        "step": 1720
    },
    {
        "loss": 2.5435,
        "grad_norm": 3.2121424674987793,
        "learning_rate": 0.00013691536699274139,
        "epoch": 0.12825098740591698,
        "step": 1721
    },
    {
        "loss": 2.306,
        "grad_norm": 3.950420379638672,
        "learning_rate": 0.00013684996223023216,
        "epoch": 0.12832550860719874,
        "step": 1722
    },
    {
        "loss": 2.3284,
        "grad_norm": 3.308757781982422,
        "learning_rate": 0.0001367845392221,
        "epoch": 0.1284000298084805,
        "step": 1723
    },
    {
        "loss": 2.3336,
        "grad_norm": 3.961226463317871,
        "learning_rate": 0.00013671909800073802,
        "epoch": 0.12847455100976227,
        "step": 1724
    },
    {
        "loss": 2.1921,
        "grad_norm": 3.0160117149353027,
        "learning_rate": 0.0001366536385985483,
        "epoch": 0.12854907221104403,
        "step": 1725
    },
    {
        "loss": 2.8572,
        "grad_norm": 2.2902352809906006,
        "learning_rate": 0.00013658816104794187,
        "epoch": 0.1286235934123258,
        "step": 1726
    },
    {
        "loss": 2.9709,
        "grad_norm": 2.3335211277008057,
        "learning_rate": 0.00013652266538133893,
        "epoch": 0.12869811461360758,
        "step": 1727
    },
    {
        "loss": 2.7321,
        "grad_norm": 1.9688451290130615,
        "learning_rate": 0.00013645715163116846,
        "epoch": 0.12877263581488935,
        "step": 1728
    },
    {
        "loss": 2.2231,
        "grad_norm": 1.9962449073791504,
        "learning_rate": 0.00013639161982986848,
        "epoch": 0.1288471570161711,
        "step": 1729
    },
    {
        "loss": 2.2144,
        "grad_norm": 2.8113510608673096,
        "learning_rate": 0.00013632607000988592,
        "epoch": 0.12892167821745287,
        "step": 1730
    },
    {
        "loss": 1.4108,
        "grad_norm": 4.5058207511901855,
        "learning_rate": 0.00013626050220367665,
        "epoch": 0.12899619941873464,
        "step": 1731
    },
    {
        "loss": 2.3614,
        "grad_norm": 1.913863182067871,
        "learning_rate": 0.00013619491644370545,
        "epoch": 0.1290707206200164,
        "step": 1732
    },
    {
        "loss": 2.0964,
        "grad_norm": 4.648575782775879,
        "learning_rate": 0.0001361293127624459,
        "epoch": 0.12914524182129816,
        "step": 1733
    },
    {
        "loss": 1.4651,
        "grad_norm": 3.68583345413208,
        "learning_rate": 0.00013606369119238066,
        "epoch": 0.12921976302257993,
        "step": 1734
    },
    {
        "loss": 2.2857,
        "grad_norm": 1.1059478521347046,
        "learning_rate": 0.000135998051766001,
        "epoch": 0.1292942842238617,
        "step": 1735
    },
    {
        "loss": 1.9642,
        "grad_norm": 2.9372925758361816,
        "learning_rate": 0.00013593239451580727,
        "epoch": 0.12936880542514345,
        "step": 1736
    },
    {
        "loss": 2.2905,
        "grad_norm": 1.977499008178711,
        "learning_rate": 0.0001358667194743084,
        "epoch": 0.12944332662642521,
        "step": 1737
    },
    {
        "loss": 2.7074,
        "grad_norm": 2.460597038269043,
        "learning_rate": 0.00013580102667402235,
        "epoch": 0.12951784782770698,
        "step": 1738
    },
    {
        "loss": 2.2804,
        "grad_norm": 3.3111414909362793,
        "learning_rate": 0.00013573531614747566,
        "epoch": 0.12959236902898874,
        "step": 1739
    },
    {
        "loss": 1.7973,
        "grad_norm": 3.1943166255950928,
        "learning_rate": 0.0001356695879272039,
        "epoch": 0.1296668902302705,
        "step": 1740
    },
    {
        "loss": 2.0517,
        "grad_norm": 2.944061279296875,
        "learning_rate": 0.00013560384204575117,
        "epoch": 0.12974141143155227,
        "step": 1741
    },
    {
        "loss": 2.5016,
        "grad_norm": 1.723807692527771,
        "learning_rate": 0.00013553807853567044,
        "epoch": 0.12981593263283403,
        "step": 1742
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.767317295074463,
        "learning_rate": 0.0001354722974295234,
        "epoch": 0.1298904538341158,
        "step": 1743
    },
    {
        "loss": 2.5899,
        "grad_norm": 3.445436477661133,
        "learning_rate": 0.00013540649875988037,
        "epoch": 0.12996497503539758,
        "step": 1744
    },
    {
        "loss": 2.317,
        "grad_norm": 1.4365246295928955,
        "learning_rate": 0.00013534068255932055,
        "epoch": 0.13003949623667935,
        "step": 1745
    },
    {
        "loss": 3.0159,
        "grad_norm": 2.0605673789978027,
        "learning_rate": 0.00013527484886043152,
        "epoch": 0.1301140174379611,
        "step": 1746
    },
    {
        "loss": 2.8909,
        "grad_norm": 2.056990146636963,
        "learning_rate": 0.0001352089976958098,
        "epoch": 0.13018853863924287,
        "step": 1747
    },
    {
        "loss": 2.6588,
        "grad_norm": 2.0582878589630127,
        "learning_rate": 0.00013514312909806048,
        "epoch": 0.13026305984052464,
        "step": 1748
    },
    {
        "loss": 2.368,
        "grad_norm": 2.977599859237671,
        "learning_rate": 0.00013507724309979718,
        "epoch": 0.1303375810418064,
        "step": 1749
    },
    {
        "loss": 2.5714,
        "grad_norm": 3.971072196960449,
        "learning_rate": 0.00013501133973364233,
        "epoch": 0.13041210224308816,
        "step": 1750
    },
    {
        "loss": 1.7859,
        "grad_norm": 2.0046541690826416,
        "learning_rate": 0.00013494541903222675,
        "epoch": 0.13048662344436993,
        "step": 1751
    },
    {
        "loss": 2.1265,
        "grad_norm": 2.916328191757202,
        "learning_rate": 0.00013487948102818996,
        "epoch": 0.1305611446456517,
        "step": 1752
    },
    {
        "loss": 2.3754,
        "grad_norm": 3.6319363117218018,
        "learning_rate": 0.00013481352575418,
        "epoch": 0.13063566584693345,
        "step": 1753
    },
    {
        "loss": 2.1017,
        "grad_norm": 2.8678994178771973,
        "learning_rate": 0.00013474755324285352,
        "epoch": 0.13071018704821522,
        "step": 1754
    },
    {
        "loss": 2.6138,
        "grad_norm": 1.6421605348587036,
        "learning_rate": 0.00013468156352687568,
        "epoch": 0.13078470824949698,
        "step": 1755
    },
    {
        "loss": 2.8655,
        "grad_norm": 2.324387550354004,
        "learning_rate": 0.00013461555663892014,
        "epoch": 0.13085922945077874,
        "step": 1756
    },
    {
        "loss": 2.5758,
        "grad_norm": 1.7969828844070435,
        "learning_rate": 0.00013454953261166907,
        "epoch": 0.1309337506520605,
        "step": 1757
    },
    {
        "loss": 2.7174,
        "grad_norm": 3.005237340927124,
        "learning_rate": 0.0001344834914778131,
        "epoch": 0.13100827185334227,
        "step": 1758
    },
    {
        "loss": 1.9972,
        "grad_norm": 5.081474781036377,
        "learning_rate": 0.00013441743327005144,
        "epoch": 0.13108279305462403,
        "step": 1759
    },
    {
        "loss": 2.8604,
        "grad_norm": 2.889770984649658,
        "learning_rate": 0.00013435135802109153,
        "epoch": 0.1311573142559058,
        "step": 1760
    },
    {
        "loss": 2.6291,
        "grad_norm": 2.539891004562378,
        "learning_rate": 0.0001342852657636495,
        "epoch": 0.13123183545718756,
        "step": 1761
    },
    {
        "loss": 2.0095,
        "grad_norm": 3.025144338607788,
        "learning_rate": 0.00013421915653044977,
        "epoch": 0.13130635665846935,
        "step": 1762
    },
    {
        "loss": 2.4774,
        "grad_norm": 2.4934475421905518,
        "learning_rate": 0.00013415303035422518,
        "epoch": 0.1313808778597511,
        "step": 1763
    },
    {
        "loss": 2.61,
        "grad_norm": 2.1633763313293457,
        "learning_rate": 0.00013408688726771696,
        "epoch": 0.13145539906103287,
        "step": 1764
    },
    {
        "loss": 2.8186,
        "grad_norm": 2.9901294708251953,
        "learning_rate": 0.00013402072730367475,
        "epoch": 0.13152992026231464,
        "step": 1765
    },
    {
        "loss": 2.228,
        "grad_norm": 2.5796735286712646,
        "learning_rate": 0.00013395455049485644,
        "epoch": 0.1316044414635964,
        "step": 1766
    },
    {
        "loss": 2.5268,
        "grad_norm": 3.9011545181274414,
        "learning_rate": 0.0001338883568740284,
        "epoch": 0.13167896266487816,
        "step": 1767
    },
    {
        "loss": 2.6559,
        "grad_norm": 2.449716329574585,
        "learning_rate": 0.00013382214647396525,
        "epoch": 0.13175348386615993,
        "step": 1768
    },
    {
        "loss": 2.5726,
        "grad_norm": 3.1588497161865234,
        "learning_rate": 0.0001337559193274499,
        "epoch": 0.1318280050674417,
        "step": 1769
    },
    {
        "loss": 2.4618,
        "grad_norm": 4.057288646697998,
        "learning_rate": 0.0001336896754672736,
        "epoch": 0.13190252626872345,
        "step": 1770
    },
    {
        "loss": 2.4386,
        "grad_norm": 3.0858490467071533,
        "learning_rate": 0.0001336234149262359,
        "epoch": 0.13197704747000522,
        "step": 1771
    },
    {
        "loss": 2.0507,
        "grad_norm": 2.8983373641967773,
        "learning_rate": 0.0001335571377371444,
        "epoch": 0.13205156867128698,
        "step": 1772
    },
    {
        "loss": 2.0668,
        "grad_norm": 4.104763031005859,
        "learning_rate": 0.0001334908439328153,
        "epoch": 0.13212608987256874,
        "step": 1773
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.8978828191757202,
        "learning_rate": 0.0001334245335460728,
        "epoch": 0.1322006110738505,
        "step": 1774
    },
    {
        "loss": 2.9893,
        "grad_norm": 2.552722215652466,
        "learning_rate": 0.00013335820660974923,
        "epoch": 0.13227513227513227,
        "step": 1775
    },
    {
        "loss": 2.2932,
        "grad_norm": 3.3012194633483887,
        "learning_rate": 0.0001332918631566853,
        "epoch": 0.13234965347641403,
        "step": 1776
    },
    {
        "loss": 2.8092,
        "grad_norm": 2.5335495471954346,
        "learning_rate": 0.0001332255032197298,
        "epoch": 0.1324241746776958,
        "step": 1777
    },
    {
        "loss": 2.6445,
        "grad_norm": 1.3212109804153442,
        "learning_rate": 0.0001331591268317398,
        "epoch": 0.13249869587897756,
        "step": 1778
    },
    {
        "loss": 2.6997,
        "grad_norm": 2.081876754760742,
        "learning_rate": 0.0001330927340255803,
        "epoch": 0.13257321708025932,
        "step": 1779
    },
    {
        "loss": 2.2263,
        "grad_norm": 2.6838366985321045,
        "learning_rate": 0.0001330263248341246,
        "epoch": 0.1326477382815411,
        "step": 1780
    },
    {
        "loss": 2.5411,
        "grad_norm": 3.0580759048461914,
        "learning_rate": 0.00013295989929025407,
        "epoch": 0.13272225948282287,
        "step": 1781
    },
    {
        "loss": 2.1887,
        "grad_norm": 2.059495449066162,
        "learning_rate": 0.0001328934574268582,
        "epoch": 0.13279678068410464,
        "step": 1782
    },
    {
        "loss": 2.2059,
        "grad_norm": 3.921330213546753,
        "learning_rate": 0.0001328269992768345,
        "epoch": 0.1328713018853864,
        "step": 1783
    },
    {
        "loss": 2.7065,
        "grad_norm": 2.52394962310791,
        "learning_rate": 0.00013276052487308854,
        "epoch": 0.13294582308666816,
        "step": 1784
    },
    {
        "loss": 2.0831,
        "grad_norm": 2.7722582817077637,
        "learning_rate": 0.00013269403424853404,
        "epoch": 0.13302034428794993,
        "step": 1785
    },
    {
        "loss": 1.8276,
        "grad_norm": 1.7939773797988892,
        "learning_rate": 0.00013262752743609267,
        "epoch": 0.1330948654892317,
        "step": 1786
    },
    {
        "loss": 2.541,
        "grad_norm": 3.0902774333953857,
        "learning_rate": 0.00013256100446869408,
        "epoch": 0.13316938669051345,
        "step": 1787
    },
    {
        "loss": 2.5917,
        "grad_norm": 3.203768730163574,
        "learning_rate": 0.00013249446537927603,
        "epoch": 0.13324390789179522,
        "step": 1788
    },
    {
        "loss": 2.8778,
        "grad_norm": 2.856919288635254,
        "learning_rate": 0.0001324279102007842,
        "epoch": 0.13331842909307698,
        "step": 1789
    },
    {
        "loss": 1.1607,
        "grad_norm": 4.437078952789307,
        "learning_rate": 0.0001323613389661722,
        "epoch": 0.13339295029435874,
        "step": 1790
    },
    {
        "loss": 2.588,
        "grad_norm": 2.5515029430389404,
        "learning_rate": 0.00013229475170840167,
        "epoch": 0.1334674714956405,
        "step": 1791
    },
    {
        "loss": 2.0128,
        "grad_norm": 3.5241708755493164,
        "learning_rate": 0.00013222814846044212,
        "epoch": 0.13354199269692227,
        "step": 1792
    },
    {
        "loss": 2.7025,
        "grad_norm": 2.76750111579895,
        "learning_rate": 0.00013216152925527095,
        "epoch": 0.13361651389820403,
        "step": 1793
    },
    {
        "loss": 1.884,
        "grad_norm": 3.049952507019043,
        "learning_rate": 0.0001320948941258736,
        "epoch": 0.1336910350994858,
        "step": 1794
    },
    {
        "loss": 2.54,
        "grad_norm": 3.504793405532837,
        "learning_rate": 0.00013202824310524323,
        "epoch": 0.13376555630076756,
        "step": 1795
    },
    {
        "loss": 1.5663,
        "grad_norm": 3.980483055114746,
        "learning_rate": 0.000131961576226381,
        "epoch": 0.13384007750204932,
        "step": 1796
    },
    {
        "loss": 2.7889,
        "grad_norm": 1.7114628553390503,
        "learning_rate": 0.00013189489352229583,
        "epoch": 0.1339145987033311,
        "step": 1797
    },
    {
        "loss": 2.3883,
        "grad_norm": 2.6668546199798584,
        "learning_rate": 0.0001318281950260045,
        "epoch": 0.13398911990461287,
        "step": 1798
    },
    {
        "loss": 2.5481,
        "grad_norm": 1.919527292251587,
        "learning_rate": 0.0001317614807705317,
        "epoch": 0.13406364110589464,
        "step": 1799
    },
    {
        "loss": 2.0577,
        "grad_norm": 2.6929664611816406,
        "learning_rate": 0.00013169475078890968,
        "epoch": 0.1341381623071764,
        "step": 1800
    },
    {
        "loss": 2.1648,
        "grad_norm": 2.6977734565734863,
        "learning_rate": 0.00013162800511417884,
        "epoch": 0.13421268350845816,
        "step": 1801
    },
    {
        "loss": 2.7885,
        "grad_norm": 3.1016385555267334,
        "learning_rate": 0.00013156124377938699,
        "epoch": 0.13428720470973993,
        "step": 1802
    },
    {
        "loss": 1.8984,
        "grad_norm": 2.805459976196289,
        "learning_rate": 0.0001314944668175899,
        "epoch": 0.1343617259110217,
        "step": 1803
    },
    {
        "loss": 2.5767,
        "grad_norm": 2.449019432067871,
        "learning_rate": 0.00013142767426185112,
        "epoch": 0.13443624711230345,
        "step": 1804
    },
    {
        "loss": 2.0997,
        "grad_norm": 3.416076898574829,
        "learning_rate": 0.00013136086614524167,
        "epoch": 0.13451076831358522,
        "step": 1805
    },
    {
        "loss": 2.4608,
        "grad_norm": 2.2181596755981445,
        "learning_rate": 0.00013129404250084058,
        "epoch": 0.13458528951486698,
        "step": 1806
    },
    {
        "loss": 2.0814,
        "grad_norm": 2.1687817573547363,
        "learning_rate": 0.00013122720336173434,
        "epoch": 0.13465981071614874,
        "step": 1807
    },
    {
        "loss": 2.5434,
        "grad_norm": 1.7496594190597534,
        "learning_rate": 0.0001311603487610172,
        "epoch": 0.1347343319174305,
        "step": 1808
    },
    {
        "loss": 2.3418,
        "grad_norm": 3.2311253547668457,
        "learning_rate": 0.0001310934787317911,
        "epoch": 0.13480885311871227,
        "step": 1809
    },
    {
        "loss": 2.9665,
        "grad_norm": 1.87363600730896,
        "learning_rate": 0.00013102659330716557,
        "epoch": 0.13488337431999403,
        "step": 1810
    },
    {
        "loss": 2.4328,
        "grad_norm": 2.638516426086426,
        "learning_rate": 0.00013095969252025776,
        "epoch": 0.1349578955212758,
        "step": 1811
    },
    {
        "loss": 1.473,
        "grad_norm": 3.8301422595977783,
        "learning_rate": 0.00013089277640419245,
        "epoch": 0.13503241672255756,
        "step": 1812
    },
    {
        "loss": 1.5687,
        "grad_norm": 3.1588234901428223,
        "learning_rate": 0.00013082584499210197,
        "epoch": 0.13510693792383932,
        "step": 1813
    },
    {
        "loss": 1.4642,
        "grad_norm": 3.8888118267059326,
        "learning_rate": 0.00013075889831712633,
        "epoch": 0.13518145912512108,
        "step": 1814
    },
    {
        "loss": 2.7406,
        "grad_norm": 1.9739208221435547,
        "learning_rate": 0.00013069193641241294,
        "epoch": 0.13525598032640287,
        "step": 1815
    },
    {
        "loss": 2.3972,
        "grad_norm": 2.192362070083618,
        "learning_rate": 0.00013062495931111687,
        "epoch": 0.13533050152768464,
        "step": 1816
    },
    {
        "loss": 2.6987,
        "grad_norm": 2.188835620880127,
        "learning_rate": 0.00013055796704640067,
        "epoch": 0.1354050227289664,
        "step": 1817
    },
    {
        "loss": 2.0869,
        "grad_norm": 3.4771482944488525,
        "learning_rate": 0.00013049095965143441,
        "epoch": 0.13547954393024816,
        "step": 1818
    },
    {
        "loss": 2.373,
        "grad_norm": 4.109935760498047,
        "learning_rate": 0.00013042393715939564,
        "epoch": 0.13555406513152993,
        "step": 1819
    },
    {
        "loss": 2.5739,
        "grad_norm": 2.292724132537842,
        "learning_rate": 0.00013035689960346936,
        "epoch": 0.1356285863328117,
        "step": 1820
    },
    {
        "loss": 2.3075,
        "grad_norm": 3.401261329650879,
        "learning_rate": 0.00013028984701684814,
        "epoch": 0.13570310753409345,
        "step": 1821
    },
    {
        "loss": 2.5985,
        "grad_norm": 2.010763168334961,
        "learning_rate": 0.0001302227794327318,
        "epoch": 0.13577762873537522,
        "step": 1822
    },
    {
        "loss": 2.6993,
        "grad_norm": 2.8244099617004395,
        "learning_rate": 0.0001301556968843278,
        "epoch": 0.13585214993665698,
        "step": 1823
    },
    {
        "loss": 2.5389,
        "grad_norm": 2.5222413539886475,
        "learning_rate": 0.00013008859940485086,
        "epoch": 0.13592667113793874,
        "step": 1824
    },
    {
        "loss": 2.105,
        "grad_norm": 4.355231761932373,
        "learning_rate": 0.00013002148702752312,
        "epoch": 0.1360011923392205,
        "step": 1825
    },
    {
        "loss": 2.1822,
        "grad_norm": 3.3286352157592773,
        "learning_rate": 0.00012995435978557413,
        "epoch": 0.13607571354050227,
        "step": 1826
    },
    {
        "loss": 2.174,
        "grad_norm": 2.857609510421753,
        "learning_rate": 0.0001298872177122408,
        "epoch": 0.13615023474178403,
        "step": 1827
    },
    {
        "loss": 2.3192,
        "grad_norm": 2.28269624710083,
        "learning_rate": 0.00012982006084076737,
        "epoch": 0.1362247559430658,
        "step": 1828
    },
    {
        "loss": 2.5106,
        "grad_norm": 2.5003652572631836,
        "learning_rate": 0.00012975288920440543,
        "epoch": 0.13629927714434756,
        "step": 1829
    },
    {
        "loss": 2.5123,
        "grad_norm": 2.0963313579559326,
        "learning_rate": 0.00012968570283641379,
        "epoch": 0.13637379834562932,
        "step": 1830
    },
    {
        "loss": 2.1514,
        "grad_norm": 2.7516989707946777,
        "learning_rate": 0.00012961850177005863,
        "epoch": 0.13644831954691108,
        "step": 1831
    },
    {
        "loss": 2.7796,
        "grad_norm": 1.819578766822815,
        "learning_rate": 0.0001295512860386135,
        "epoch": 0.13652284074819285,
        "step": 1832
    },
    {
        "loss": 2.9879,
        "grad_norm": 2.931290864944458,
        "learning_rate": 0.00012948405567535896,
        "epoch": 0.13659736194947464,
        "step": 1833
    },
    {
        "loss": 2.9051,
        "grad_norm": 1.2863441705703735,
        "learning_rate": 0.00012941681071358305,
        "epoch": 0.1366718831507564,
        "step": 1834
    },
    {
        "loss": 1.1129,
        "grad_norm": 2.347325086593628,
        "learning_rate": 0.00012934955118658095,
        "epoch": 0.13674640435203816,
        "step": 1835
    },
    {
        "loss": 2.721,
        "grad_norm": 3.731278419494629,
        "learning_rate": 0.00012928227712765504,
        "epoch": 0.13682092555331993,
        "step": 1836
    },
    {
        "loss": 2.9482,
        "grad_norm": 1.9943058490753174,
        "learning_rate": 0.00012921498857011493,
        "epoch": 0.1368954467546017,
        "step": 1837
    },
    {
        "loss": 1.6007,
        "grad_norm": 3.4757516384124756,
        "learning_rate": 0.00012914768554727734,
        "epoch": 0.13696996795588345,
        "step": 1838
    },
    {
        "loss": 2.3727,
        "grad_norm": 3.3813016414642334,
        "learning_rate": 0.0001290803680924663,
        "epoch": 0.13704448915716522,
        "step": 1839
    },
    {
        "loss": 2.9963,
        "grad_norm": 4.896125793457031,
        "learning_rate": 0.00012901303623901272,
        "epoch": 0.13711901035844698,
        "step": 1840
    },
    {
        "loss": 1.4422,
        "grad_norm": 1.2139735221862793,
        "learning_rate": 0.00012894569002025493,
        "epoch": 0.13719353155972874,
        "step": 1841
    },
    {
        "loss": 2.3029,
        "grad_norm": 2.8409531116485596,
        "learning_rate": 0.0001288783294695383,
        "epoch": 0.1372680527610105,
        "step": 1842
    },
    {
        "loss": 2.7515,
        "grad_norm": 3.680576801300049,
        "learning_rate": 0.00012881095462021505,
        "epoch": 0.13734257396229227,
        "step": 1843
    },
    {
        "loss": 2.4279,
        "grad_norm": 1.9195317029953003,
        "learning_rate": 0.00012874356550564485,
        "epoch": 0.13741709516357403,
        "step": 1844
    },
    {
        "loss": 1.8203,
        "grad_norm": 4.230845928192139,
        "learning_rate": 0.0001286761621591942,
        "epoch": 0.1374916163648558,
        "step": 1845
    },
    {
        "loss": 2.2554,
        "grad_norm": 2.247968912124634,
        "learning_rate": 0.0001286087446142367,
        "epoch": 0.13756613756613756,
        "step": 1846
    },
    {
        "loss": 1.8379,
        "grad_norm": 3.507176399230957,
        "learning_rate": 0.000128541312904153,
        "epoch": 0.13764065876741932,
        "step": 1847
    },
    {
        "loss": 2.1877,
        "grad_norm": 4.140087604522705,
        "learning_rate": 0.00012847386706233067,
        "epoch": 0.13771517996870108,
        "step": 1848
    },
    {
        "loss": 2.9027,
        "grad_norm": 3.038508176803589,
        "learning_rate": 0.00012840640712216448,
        "epoch": 0.13778970116998285,
        "step": 1849
    },
    {
        "loss": 1.6024,
        "grad_norm": 2.475250720977783,
        "learning_rate": 0.00012833893311705596,
        "epoch": 0.13786422237126464,
        "step": 1850
    },
    {
        "loss": 1.8314,
        "grad_norm": 3.1817684173583984,
        "learning_rate": 0.00012827144508041372,
        "epoch": 0.1379387435725464,
        "step": 1851
    },
    {
        "loss": 1.9726,
        "grad_norm": 4.38327693939209,
        "learning_rate": 0.00012820394304565333,
        "epoch": 0.13801326477382816,
        "step": 1852
    },
    {
        "loss": 1.9843,
        "grad_norm": 3.4271552562713623,
        "learning_rate": 0.00012813642704619725,
        "epoch": 0.13808778597510993,
        "step": 1853
    },
    {
        "loss": 2.7807,
        "grad_norm": 2.3640363216400146,
        "learning_rate": 0.00012806889711547482,
        "epoch": 0.1381623071763917,
        "step": 1854
    },
    {
        "loss": 2.6437,
        "grad_norm": 3.1758196353912354,
        "learning_rate": 0.00012800135328692235,
        "epoch": 0.13823682837767345,
        "step": 1855
    },
    {
        "loss": 2.6815,
        "grad_norm": 2.365475654602051,
        "learning_rate": 0.00012793379559398302,
        "epoch": 0.13831134957895522,
        "step": 1856
    },
    {
        "loss": 2.7682,
        "grad_norm": 2.1752562522888184,
        "learning_rate": 0.0001278662240701068,
        "epoch": 0.13838587078023698,
        "step": 1857
    },
    {
        "loss": 1.2455,
        "grad_norm": 3.7276322841644287,
        "learning_rate": 0.00012779863874875065,
        "epoch": 0.13846039198151874,
        "step": 1858
    },
    {
        "loss": 1.9631,
        "grad_norm": 4.111079216003418,
        "learning_rate": 0.00012773103966337817,
        "epoch": 0.1385349131828005,
        "step": 1859
    },
    {
        "loss": 2.2167,
        "grad_norm": 2.8001327514648438,
        "learning_rate": 0.00012766342684745997,
        "epoch": 0.13860943438408227,
        "step": 1860
    },
    {
        "loss": 2.3647,
        "grad_norm": 2.3728044033050537,
        "learning_rate": 0.00012759580033447332,
        "epoch": 0.13868395558536403,
        "step": 1861
    },
    {
        "loss": 2.5098,
        "grad_norm": 2.241422653198242,
        "learning_rate": 0.0001275281601579023,
        "epoch": 0.1387584767866458,
        "step": 1862
    },
    {
        "loss": 3.2902,
        "grad_norm": 2.559926748275757,
        "learning_rate": 0.0001274605063512379,
        "epoch": 0.13883299798792756,
        "step": 1863
    },
    {
        "loss": 1.8789,
        "grad_norm": 2.917512893676758,
        "learning_rate": 0.00012739283894797757,
        "epoch": 0.13890751918920932,
        "step": 1864
    },
    {
        "loss": 2.5052,
        "grad_norm": 2.1183035373687744,
        "learning_rate": 0.00012732515798162577,
        "epoch": 0.13898204039049109,
        "step": 1865
    },
    {
        "loss": 1.6247,
        "grad_norm": 2.861933469772339,
        "learning_rate": 0.00012725746348569346,
        "epoch": 0.13905656159177285,
        "step": 1866
    },
    {
        "loss": 2.4853,
        "grad_norm": 1.6241604089736938,
        "learning_rate": 0.00012718975549369854,
        "epoch": 0.1391310827930546,
        "step": 1867
    },
    {
        "loss": 2.2633,
        "grad_norm": 2.9863221645355225,
        "learning_rate": 0.0001271220340391654,
        "epoch": 0.1392056039943364,
        "step": 1868
    },
    {
        "loss": 2.6301,
        "grad_norm": 2.0484702587127686,
        "learning_rate": 0.00012705429915562506,
        "epoch": 0.13928012519561817,
        "step": 1869
    },
    {
        "loss": 2.1411,
        "grad_norm": 3.4167211055755615,
        "learning_rate": 0.0001269865508766154,
        "epoch": 0.13935464639689993,
        "step": 1870
    },
    {
        "loss": 2.6842,
        "grad_norm": 3.2222564220428467,
        "learning_rate": 0.00012691878923568072,
        "epoch": 0.1394291675981817,
        "step": 1871
    },
    {
        "loss": 2.4358,
        "grad_norm": 2.590763568878174,
        "learning_rate": 0.00012685101426637208,
        "epoch": 0.13950368879946345,
        "step": 1872
    },
    {
        "loss": 1.9559,
        "grad_norm": 3.2952585220336914,
        "learning_rate": 0.00012678322600224708,
        "epoch": 0.13957821000074522,
        "step": 1873
    },
    {
        "loss": 2.474,
        "grad_norm": 3.2573728561401367,
        "learning_rate": 0.0001267154244768699,
        "epoch": 0.13965273120202698,
        "step": 1874
    },
    {
        "loss": 2.05,
        "grad_norm": 2.755782127380371,
        "learning_rate": 0.00012664760972381134,
        "epoch": 0.13972725240330874,
        "step": 1875
    },
    {
        "loss": 3.0233,
        "grad_norm": 2.285818576812744,
        "learning_rate": 0.0001265797817766486,
        "epoch": 0.1398017736045905,
        "step": 1876
    },
    {
        "loss": 2.2872,
        "grad_norm": 2.1270532608032227,
        "learning_rate": 0.00012651194066896562,
        "epoch": 0.13987629480587227,
        "step": 1877
    },
    {
        "loss": 2.6545,
        "grad_norm": 2.5941872596740723,
        "learning_rate": 0.0001264440864343527,
        "epoch": 0.13995081600715403,
        "step": 1878
    },
    {
        "loss": 1.9476,
        "grad_norm": 3.6566059589385986,
        "learning_rate": 0.00012637621910640672,
        "epoch": 0.1400253372084358,
        "step": 1879
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.1623551845550537,
        "learning_rate": 0.000126308338718731,
        "epoch": 0.14009985840971756,
        "step": 1880
    },
    {
        "loss": 2.0588,
        "grad_norm": 3.406266689300537,
        "learning_rate": 0.0001262404453049353,
        "epoch": 0.14017437961099932,
        "step": 1881
    },
    {
        "loss": 2.959,
        "grad_norm": 2.4796299934387207,
        "learning_rate": 0.000126172538898636,
        "epoch": 0.14024890081228109,
        "step": 1882
    },
    {
        "loss": 2.3289,
        "grad_norm": 1.9637196063995361,
        "learning_rate": 0.00012610461953345564,
        "epoch": 0.14032342201356285,
        "step": 1883
    },
    {
        "loss": 2.6828,
        "grad_norm": 1.6130543947219849,
        "learning_rate": 0.00012603668724302345,
        "epoch": 0.1403979432148446,
        "step": 1884
    },
    {
        "loss": 1.9642,
        "grad_norm": 3.7025017738342285,
        "learning_rate": 0.0001259687420609748,
        "epoch": 0.14047246441612637,
        "step": 1885
    },
    {
        "loss": 2.8645,
        "grad_norm": 2.8649227619171143,
        "learning_rate": 0.0001259007840209517,
        "epoch": 0.14054698561740817,
        "step": 1886
    },
    {
        "loss": 2.6939,
        "grad_norm": 2.471994638442993,
        "learning_rate": 0.00012583281315660229,
        "epoch": 0.14062150681868993,
        "step": 1887
    },
    {
        "loss": 2.3051,
        "grad_norm": 3.4307918548583984,
        "learning_rate": 0.00012576482950158124,
        "epoch": 0.1406960280199717,
        "step": 1888
    },
    {
        "loss": 1.9527,
        "grad_norm": 3.0566635131835938,
        "learning_rate": 0.0001256968330895495,
        "epoch": 0.14077054922125345,
        "step": 1889
    },
    {
        "loss": 1.9706,
        "grad_norm": 3.691131830215454,
        "learning_rate": 0.00012562882395417426,
        "epoch": 0.14084507042253522,
        "step": 1890
    },
    {
        "loss": 2.576,
        "grad_norm": 3.739335536956787,
        "learning_rate": 0.0001255608021291291,
        "epoch": 0.14091959162381698,
        "step": 1891
    },
    {
        "loss": 2.6292,
        "grad_norm": 2.7098915576934814,
        "learning_rate": 0.00012549276764809382,
        "epoch": 0.14099411282509874,
        "step": 1892
    },
    {
        "loss": 2.8659,
        "grad_norm": 3.55340838432312,
        "learning_rate": 0.00012542472054475462,
        "epoch": 0.1410686340263805,
        "step": 1893
    },
    {
        "loss": 2.8367,
        "grad_norm": 1.9737215042114258,
        "learning_rate": 0.0001253566608528037,
        "epoch": 0.14114315522766227,
        "step": 1894
    },
    {
        "loss": 2.1868,
        "grad_norm": 3.0059494972229004,
        "learning_rate": 0.00012528858860593977,
        "epoch": 0.14121767642894403,
        "step": 1895
    },
    {
        "loss": 2.3718,
        "grad_norm": 2.448540210723877,
        "learning_rate": 0.00012522050383786758,
        "epoch": 0.1412921976302258,
        "step": 1896
    },
    {
        "loss": 2.3598,
        "grad_norm": 2.3900136947631836,
        "learning_rate": 0.00012515240658229809,
        "epoch": 0.14136671883150756,
        "step": 1897
    },
    {
        "loss": 2.2974,
        "grad_norm": 3.1170177459716797,
        "learning_rate": 0.00012508429687294856,
        "epoch": 0.14144124003278932,
        "step": 1898
    },
    {
        "loss": 1.2147,
        "grad_norm": 2.8940939903259277,
        "learning_rate": 0.0001250161747435423,
        "epoch": 0.14151576123407109,
        "step": 1899
    },
    {
        "loss": 2.4187,
        "grad_norm": 2.8450074195861816,
        "learning_rate": 0.00012494804022780874,
        "epoch": 0.14159028243535285,
        "step": 1900
    },
    {
        "loss": 2.7887,
        "grad_norm": 1.9818304777145386,
        "learning_rate": 0.0001248798933594836,
        "epoch": 0.1416648036366346,
        "step": 1901
    },
    {
        "loss": 2.2426,
        "grad_norm": 3.5878372192382812,
        "learning_rate": 0.0001248117341723086,
        "epoch": 0.14173932483791638,
        "step": 1902
    },
    {
        "loss": 2.8611,
        "grad_norm": 1.6087415218353271,
        "learning_rate": 0.0001247435627000316,
        "epoch": 0.14181384603919814,
        "step": 1903
    },
    {
        "loss": 2.3374,
        "grad_norm": 3.115178346633911,
        "learning_rate": 0.0001246753789764065,
        "epoch": 0.14188836724047993,
        "step": 1904
    },
    {
        "loss": 2.356,
        "grad_norm": 2.52925181388855,
        "learning_rate": 0.0001246071830351933,
        "epoch": 0.1419628884417617,
        "step": 1905
    },
    {
        "loss": 2.1865,
        "grad_norm": 2.549722671508789,
        "learning_rate": 0.00012453897491015802,
        "epoch": 0.14203740964304346,
        "step": 1906
    },
    {
        "loss": 2.2247,
        "grad_norm": 3.457887887954712,
        "learning_rate": 0.00012447075463507278,
        "epoch": 0.14211193084432522,
        "step": 1907
    },
    {
        "loss": 1.9309,
        "grad_norm": 3.425210475921631,
        "learning_rate": 0.0001244025222437157,
        "epoch": 0.14218645204560698,
        "step": 1908
    },
    {
        "loss": 2.4572,
        "grad_norm": 2.4138360023498535,
        "learning_rate": 0.00012433427776987078,
        "epoch": 0.14226097324688874,
        "step": 1909
    },
    {
        "loss": 2.6864,
        "grad_norm": 2.697648286819458,
        "learning_rate": 0.0001242660212473282,
        "epoch": 0.1423354944481705,
        "step": 1910
    },
    {
        "loss": 2.2845,
        "grad_norm": 3.9995739459991455,
        "learning_rate": 0.0001241977527098839,
        "epoch": 0.14241001564945227,
        "step": 1911
    },
    {
        "loss": 2.9267,
        "grad_norm": 3.622361898422241,
        "learning_rate": 0.0001241294721913399,
        "epoch": 0.14248453685073403,
        "step": 1912
    },
    {
        "loss": 2.482,
        "grad_norm": 3.1109437942504883,
        "learning_rate": 0.00012406117972550414,
        "epoch": 0.1425590580520158,
        "step": 1913
    },
    {
        "loss": 2.0376,
        "grad_norm": 4.408568859100342,
        "learning_rate": 0.00012399287534619043,
        "epoch": 0.14263357925329756,
        "step": 1914
    },
    {
        "loss": 2.0092,
        "grad_norm": 2.4561996459960938,
        "learning_rate": 0.00012392455908721853,
        "epoch": 0.14270810045457932,
        "step": 1915
    },
    {
        "loss": 2.3091,
        "grad_norm": 4.524236679077148,
        "learning_rate": 0.00012385623098241404,
        "epoch": 0.14278262165586109,
        "step": 1916
    },
    {
        "loss": 2.2983,
        "grad_norm": 2.554466485977173,
        "learning_rate": 0.00012378789106560846,
        "epoch": 0.14285714285714285,
        "step": 1917
    },
    {
        "loss": 2.4898,
        "grad_norm": 2.595076560974121,
        "learning_rate": 0.0001237195393706391,
        "epoch": 0.1429316640584246,
        "step": 1918
    },
    {
        "loss": 1.4661,
        "grad_norm": 4.314127445220947,
        "learning_rate": 0.00012365117593134915,
        "epoch": 0.14300618525970638,
        "step": 1919
    },
    {
        "loss": 1.8883,
        "grad_norm": 3.0348124504089355,
        "learning_rate": 0.00012358280078158753,
        "epoch": 0.14308070646098814,
        "step": 1920
    },
    {
        "loss": 2.5379,
        "grad_norm": 2.5534725189208984,
        "learning_rate": 0.00012351441395520906,
        "epoch": 0.14315522766226993,
        "step": 1921
    },
    {
        "loss": 2.3655,
        "grad_norm": 3.1449971199035645,
        "learning_rate": 0.00012344601548607432,
        "epoch": 0.1432297488635517,
        "step": 1922
    },
    {
        "loss": 2.585,
        "grad_norm": 2.2667009830474854,
        "learning_rate": 0.0001233776054080496,
        "epoch": 0.14330427006483346,
        "step": 1923
    },
    {
        "loss": 2.0651,
        "grad_norm": 3.0431041717529297,
        "learning_rate": 0.00012330918375500697,
        "epoch": 0.14337879126611522,
        "step": 1924
    },
    {
        "loss": 2.4505,
        "grad_norm": 2.9991276264190674,
        "learning_rate": 0.00012324075056082423,
        "epoch": 0.14345331246739698,
        "step": 1925
    },
    {
        "loss": 2.3544,
        "grad_norm": 3.7362513542175293,
        "learning_rate": 0.00012317230585938492,
        "epoch": 0.14352783366867874,
        "step": 1926
    },
    {
        "loss": 2.207,
        "grad_norm": 4.023782730102539,
        "learning_rate": 0.00012310384968457815,
        "epoch": 0.1436023548699605,
        "step": 1927
    },
    {
        "loss": 2.0353,
        "grad_norm": 3.541503667831421,
        "learning_rate": 0.00012303538207029897,
        "epoch": 0.14367687607124227,
        "step": 1928
    },
    {
        "loss": 2.3928,
        "grad_norm": 3.925264596939087,
        "learning_rate": 0.00012296690305044786,
        "epoch": 0.14375139727252403,
        "step": 1929
    },
    {
        "loss": 2.413,
        "grad_norm": 2.881671905517578,
        "learning_rate": 0.000122898412658931,
        "epoch": 0.1438259184738058,
        "step": 1930
    },
    {
        "loss": 2.174,
        "grad_norm": 2.3757247924804688,
        "learning_rate": 0.0001228299109296603,
        "epoch": 0.14390043967508756,
        "step": 1931
    },
    {
        "loss": 2.187,
        "grad_norm": 2.0625858306884766,
        "learning_rate": 0.0001227613978965531,
        "epoch": 0.14397496087636932,
        "step": 1932
    },
    {
        "loss": 2.4666,
        "grad_norm": 3.2434003353118896,
        "learning_rate": 0.00012269287359353254,
        "epoch": 0.1440494820776511,
        "step": 1933
    },
    {
        "loss": 2.354,
        "grad_norm": 2.0549604892730713,
        "learning_rate": 0.0001226243380545272,
        "epoch": 0.14412400327893285,
        "step": 1934
    },
    {
        "loss": 2.9481,
        "grad_norm": 2.839878559112549,
        "learning_rate": 0.0001225557913134713,
        "epoch": 0.1441985244802146,
        "step": 1935
    },
    {
        "loss": 2.5642,
        "grad_norm": 2.3334250450134277,
        "learning_rate": 0.00012248723340430457,
        "epoch": 0.14427304568149638,
        "step": 1936
    },
    {
        "loss": 1.7285,
        "grad_norm": 2.284431219100952,
        "learning_rate": 0.0001224186643609722,
        "epoch": 0.14434756688277814,
        "step": 1937
    },
    {
        "loss": 2.1411,
        "grad_norm": 2.803898811340332,
        "learning_rate": 0.00012235008421742513,
        "epoch": 0.1444220880840599,
        "step": 1938
    },
    {
        "loss": 2.1432,
        "grad_norm": 3.87007999420166,
        "learning_rate": 0.00012228149300761947,
        "epoch": 0.1444966092853417,
        "step": 1939
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.8250584602355957,
        "learning_rate": 0.00012221289076551705,
        "epoch": 0.14457113048662346,
        "step": 1940
    },
    {
        "loss": 2.2856,
        "grad_norm": 2.539456844329834,
        "learning_rate": 0.0001221442775250851,
        "epoch": 0.14464565168790522,
        "step": 1941
    },
    {
        "loss": 1.8395,
        "grad_norm": 4.606256008148193,
        "learning_rate": 0.00012207565332029624,
        "epoch": 0.14472017288918698,
        "step": 1942
    },
    {
        "loss": 1.7064,
        "grad_norm": 4.254705905914307,
        "learning_rate": 0.00012200701818512863,
        "epoch": 0.14479469409046875,
        "step": 1943
    },
    {
        "loss": 2.5796,
        "grad_norm": 2.8477072715759277,
        "learning_rate": 0.00012193837215356567,
        "epoch": 0.1448692152917505,
        "step": 1944
    },
    {
        "loss": 2.6165,
        "grad_norm": 2.1371073722839355,
        "learning_rate": 0.00012186971525959634,
        "epoch": 0.14494373649303227,
        "step": 1945
    },
    {
        "loss": 2.5879,
        "grad_norm": 2.876363754272461,
        "learning_rate": 0.00012180104753721491,
        "epoch": 0.14501825769431403,
        "step": 1946
    },
    {
        "loss": 1.6156,
        "grad_norm": 2.153111696243286,
        "learning_rate": 0.00012173236902042096,
        "epoch": 0.1450927788955958,
        "step": 1947
    },
    {
        "loss": 3.0806,
        "grad_norm": 3.905993700027466,
        "learning_rate": 0.00012166367974321948,
        "epoch": 0.14516730009687756,
        "step": 1948
    },
    {
        "loss": 2.2512,
        "grad_norm": 3.2991127967834473,
        "learning_rate": 0.00012159497973962084,
        "epoch": 0.14524182129815932,
        "step": 1949
    },
    {
        "loss": 2.7376,
        "grad_norm": 2.7170639038085938,
        "learning_rate": 0.00012152626904364067,
        "epoch": 0.1453163424994411,
        "step": 1950
    },
    {
        "loss": 1.8677,
        "grad_norm": 3.0152997970581055,
        "learning_rate": 0.0001214575476892998,
        "epoch": 0.14539086370072285,
        "step": 1951
    },
    {
        "loss": 2.6605,
        "grad_norm": 2.625308036804199,
        "learning_rate": 0.0001213888157106245,
        "epoch": 0.1454653849020046,
        "step": 1952
    },
    {
        "loss": 2.3573,
        "grad_norm": 3.0097453594207764,
        "learning_rate": 0.00012132007314164618,
        "epoch": 0.14553990610328638,
        "step": 1953
    },
    {
        "loss": 2.5421,
        "grad_norm": 2.3482749462127686,
        "learning_rate": 0.00012125132001640152,
        "epoch": 0.14561442730456814,
        "step": 1954
    },
    {
        "loss": 2.3919,
        "grad_norm": 3.159940242767334,
        "learning_rate": 0.00012118255636893247,
        "epoch": 0.1456889485058499,
        "step": 1955
    },
    {
        "loss": 1.981,
        "grad_norm": 3.2733309268951416,
        "learning_rate": 0.00012111378223328622,
        "epoch": 0.14576346970713167,
        "step": 1956
    },
    {
        "loss": 2.6408,
        "grad_norm": 3.0897445678710938,
        "learning_rate": 0.00012104499764351503,
        "epoch": 0.14583799090841346,
        "step": 1957
    },
    {
        "loss": 2.6345,
        "grad_norm": 3.385209083557129,
        "learning_rate": 0.00012097620263367637,
        "epoch": 0.14591251210969522,
        "step": 1958
    },
    {
        "loss": 2.5953,
        "grad_norm": 1.9492815732955933,
        "learning_rate": 0.00012090739723783301,
        "epoch": 0.14598703331097698,
        "step": 1959
    },
    {
        "loss": 2.8963,
        "grad_norm": 2.249070882797241,
        "learning_rate": 0.00012083858149005263,
        "epoch": 0.14606155451225875,
        "step": 1960
    },
    {
        "loss": 1.3698,
        "grad_norm": 4.521503925323486,
        "learning_rate": 0.00012076975542440823,
        "epoch": 0.1461360757135405,
        "step": 1961
    },
    {
        "loss": 2.7584,
        "grad_norm": 2.3920950889587402,
        "learning_rate": 0.00012070091907497787,
        "epoch": 0.14621059691482227,
        "step": 1962
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.5301791429519653,
        "learning_rate": 0.00012063207247584463,
        "epoch": 0.14628511811610403,
        "step": 1963
    },
    {
        "loss": 2.4442,
        "grad_norm": 3.1380183696746826,
        "learning_rate": 0.00012056321566109679,
        "epoch": 0.1463596393173858,
        "step": 1964
    },
    {
        "loss": 1.9208,
        "grad_norm": 3.669804334640503,
        "learning_rate": 0.00012049434866482747,
        "epoch": 0.14643416051866756,
        "step": 1965
    },
    {
        "loss": 2.3622,
        "grad_norm": 2.7461166381835938,
        "learning_rate": 0.00012042547152113515,
        "epoch": 0.14650868171994932,
        "step": 1966
    },
    {
        "loss": 2.2009,
        "grad_norm": 2.1584689617156982,
        "learning_rate": 0.00012035658426412301,
        "epoch": 0.1465832029212311,
        "step": 1967
    },
    {
        "loss": 2.2054,
        "grad_norm": 4.1048264503479,
        "learning_rate": 0.00012028768692789947,
        "epoch": 0.14665772412251285,
        "step": 1968
    },
    {
        "loss": 2.543,
        "grad_norm": 2.5605733394622803,
        "learning_rate": 0.00012021877954657784,
        "epoch": 0.1467322453237946,
        "step": 1969
    },
    {
        "loss": 2.5606,
        "grad_norm": 1.8328890800476074,
        "learning_rate": 0.00012014986215427639,
        "epoch": 0.14680676652507638,
        "step": 1970
    },
    {
        "loss": 2.3531,
        "grad_norm": 2.6119744777679443,
        "learning_rate": 0.00012008093478511845,
        "epoch": 0.14688128772635814,
        "step": 1971
    },
    {
        "loss": 2.3865,
        "grad_norm": 2.0604357719421387,
        "learning_rate": 0.00012001199747323216,
        "epoch": 0.1469558089276399,
        "step": 1972
    },
    {
        "loss": 1.654,
        "grad_norm": 5.134774684906006,
        "learning_rate": 0.00011994305025275065,
        "epoch": 0.14703033012892167,
        "step": 1973
    },
    {
        "loss": 2.3633,
        "grad_norm": 2.2768824100494385,
        "learning_rate": 0.00011987409315781195,
        "epoch": 0.14710485133020346,
        "step": 1974
    },
    {
        "loss": 2.1489,
        "grad_norm": 3.943128824234009,
        "learning_rate": 0.00011980512622255894,
        "epoch": 0.14717937253148522,
        "step": 1975
    },
    {
        "loss": 3.047,
        "grad_norm": 2.037893295288086,
        "learning_rate": 0.00011973614948113951,
        "epoch": 0.14725389373276698,
        "step": 1976
    },
    {
        "loss": 2.3869,
        "grad_norm": 3.059023141860962,
        "learning_rate": 0.00011966716296770619,
        "epoch": 0.14732841493404875,
        "step": 1977
    },
    {
        "loss": 2.9429,
        "grad_norm": 2.1451685428619385,
        "learning_rate": 0.00011959816671641655,
        "epoch": 0.1474029361353305,
        "step": 1978
    },
    {
        "loss": 2.1154,
        "grad_norm": 4.49058198928833,
        "learning_rate": 0.00011952916076143284,
        "epoch": 0.14747745733661227,
        "step": 1979
    },
    {
        "loss": 2.4637,
        "grad_norm": 2.6374666690826416,
        "learning_rate": 0.00011946014513692216,
        "epoch": 0.14755197853789404,
        "step": 1980
    },
    {
        "loss": 2.4871,
        "grad_norm": 2.8867905139923096,
        "learning_rate": 0.00011939111987705642,
        "epoch": 0.1476264997391758,
        "step": 1981
    },
    {
        "loss": 2.2326,
        "grad_norm": 3.0162265300750732,
        "learning_rate": 0.0001193220850160123,
        "epoch": 0.14770102094045756,
        "step": 1982
    },
    {
        "loss": 2.413,
        "grad_norm": 2.49950909614563,
        "learning_rate": 0.00011925304058797118,
        "epoch": 0.14777554214173932,
        "step": 1983
    },
    {
        "loss": 2.0431,
        "grad_norm": 2.702665328979492,
        "learning_rate": 0.00011918398662711929,
        "epoch": 0.1478500633430211,
        "step": 1984
    },
    {
        "loss": 2.5059,
        "grad_norm": 2.0351102352142334,
        "learning_rate": 0.00011911492316764747,
        "epoch": 0.14792458454430285,
        "step": 1985
    },
    {
        "loss": 2.3322,
        "grad_norm": 2.7658307552337646,
        "learning_rate": 0.00011904585024375128,
        "epoch": 0.1479991057455846,
        "step": 1986
    },
    {
        "loss": 1.8458,
        "grad_norm": 3.5065107345581055,
        "learning_rate": 0.00011897676788963101,
        "epoch": 0.14807362694686638,
        "step": 1987
    },
    {
        "loss": 2.3963,
        "grad_norm": 2.0383453369140625,
        "learning_rate": 0.00011890767613949157,
        "epoch": 0.14814814814814814,
        "step": 1988
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.6389243602752686,
        "learning_rate": 0.00011883857502754257,
        "epoch": 0.1482226693494299,
        "step": 1989
    },
    {
        "loss": 2.2046,
        "grad_norm": 3.575590133666992,
        "learning_rate": 0.00011876946458799824,
        "epoch": 0.14829719055071167,
        "step": 1990
    },
    {
        "loss": 2.2701,
        "grad_norm": 3.219820261001587,
        "learning_rate": 0.0001187003448550774,
        "epoch": 0.14837171175199343,
        "step": 1991
    },
    {
        "loss": 1.8512,
        "grad_norm": 1.6704436540603638,
        "learning_rate": 0.00011863121586300355,
        "epoch": 0.14844623295327522,
        "step": 1992
    },
    {
        "loss": 1.6701,
        "grad_norm": 2.80711030960083,
        "learning_rate": 0.00011856207764600458,
        "epoch": 0.14852075415455698,
        "step": 1993
    },
    {
        "loss": 2.0209,
        "grad_norm": 1.6907951831817627,
        "learning_rate": 0.00011849293023831325,
        "epoch": 0.14859527535583875,
        "step": 1994
    },
    {
        "loss": 2.5757,
        "grad_norm": 2.298938751220703,
        "learning_rate": 0.00011842377367416658,
        "epoch": 0.1486697965571205,
        "step": 1995
    },
    {
        "loss": 2.1266,
        "grad_norm": 4.013073444366455,
        "learning_rate": 0.00011835460798780631,
        "epoch": 0.14874431775840227,
        "step": 1996
    },
    {
        "loss": 2.4284,
        "grad_norm": 3.5895402431488037,
        "learning_rate": 0.00011828543321347866,
        "epoch": 0.14881883895968404,
        "step": 1997
    },
    {
        "loss": 2.6396,
        "grad_norm": 1.9295979738235474,
        "learning_rate": 0.00011821624938543428,
        "epoch": 0.1488933601609658,
        "step": 1998
    },
    {
        "loss": 2.6003,
        "grad_norm": 2.6357085704803467,
        "learning_rate": 0.00011814705653792837,
        "epoch": 0.14896788136224756,
        "step": 1999
    },
    {
        "loss": 3.0887,
        "grad_norm": 1.8136026859283447,
        "learning_rate": 0.00011807785470522055,
        "epoch": 0.14904240256352932,
        "step": 2000
    },
    {
        "loss": 2.3946,
        "grad_norm": 2.1046602725982666,
        "learning_rate": 0.00011800864392157491,
        "epoch": 0.1491169237648111,
        "step": 2001
    },
    {
        "loss": 1.6778,
        "grad_norm": 5.63043212890625,
        "learning_rate": 0.00011793942422126003,
        "epoch": 0.14919144496609285,
        "step": 2002
    },
    {
        "loss": 2.5431,
        "grad_norm": 2.4560556411743164,
        "learning_rate": 0.0001178701956385488,
        "epoch": 0.14926596616737461,
        "step": 2003
    },
    {
        "loss": 1.3251,
        "grad_norm": 4.005588531494141,
        "learning_rate": 0.00011780095820771856,
        "epoch": 0.14934048736865638,
        "step": 2004
    },
    {
        "loss": 1.98,
        "grad_norm": 2.5016705989837646,
        "learning_rate": 0.00011773171196305106,
        "epoch": 0.14941500856993814,
        "step": 2005
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.47890043258667,
        "learning_rate": 0.00011766245693883238,
        "epoch": 0.1494895297712199,
        "step": 2006
    },
    {
        "loss": 2.5203,
        "grad_norm": 2.3422276973724365,
        "learning_rate": 0.0001175931931693529,
        "epoch": 0.14956405097250167,
        "step": 2007
    },
    {
        "loss": 2.2927,
        "grad_norm": 3.118453025817871,
        "learning_rate": 0.0001175239206889074,
        "epoch": 0.14963857217378343,
        "step": 2008
    },
    {
        "loss": 2.5357,
        "grad_norm": 3.300002336502075,
        "learning_rate": 0.00011745463953179501,
        "epoch": 0.1497130933750652,
        "step": 2009
    },
    {
        "loss": 1.7568,
        "grad_norm": 3.0373191833496094,
        "learning_rate": 0.00011738534973231905,
        "epoch": 0.14978761457634698,
        "step": 2010
    },
    {
        "loss": 2.1273,
        "grad_norm": 4.897765636444092,
        "learning_rate": 0.00011731605132478718,
        "epoch": 0.14986213577762875,
        "step": 2011
    },
    {
        "loss": 2.4263,
        "grad_norm": 3.456085681915283,
        "learning_rate": 0.0001172467443435113,
        "epoch": 0.1499366569789105,
        "step": 2012
    },
    {
        "loss": 2.8929,
        "grad_norm": 2.5005040168762207,
        "learning_rate": 0.00011717742882280758,
        "epoch": 0.15001117818019227,
        "step": 2013
    },
    {
        "loss": 2.6001,
        "grad_norm": 2.110516309738159,
        "learning_rate": 0.00011710810479699639,
        "epoch": 0.15008569938147404,
        "step": 2014
    },
    {
        "loss": 2.5936,
        "grad_norm": 2.462419271469116,
        "learning_rate": 0.0001170387723004023,
        "epoch": 0.1501602205827558,
        "step": 2015
    },
    {
        "loss": 2.1219,
        "grad_norm": 2.631619930267334,
        "learning_rate": 0.00011696943136735416,
        "epoch": 0.15023474178403756,
        "step": 2016
    },
    {
        "loss": 2.6266,
        "grad_norm": 2.0307257175445557,
        "learning_rate": 0.00011690008203218493,
        "epoch": 0.15030926298531933,
        "step": 2017
    },
    {
        "loss": 2.1027,
        "grad_norm": 2.2683608531951904,
        "learning_rate": 0.00011683072432923168,
        "epoch": 0.1503837841866011,
        "step": 2018
    },
    {
        "loss": 2.3007,
        "grad_norm": 2.6411983966827393,
        "learning_rate": 0.00011676135829283572,
        "epoch": 0.15045830538788285,
        "step": 2019
    },
    {
        "loss": 2.8838,
        "grad_norm": 2.071202039718628,
        "learning_rate": 0.00011669198395734247,
        "epoch": 0.15053282658916461,
        "step": 2020
    },
    {
        "loss": 2.4683,
        "grad_norm": 3.8892464637756348,
        "learning_rate": 0.00011662260135710135,
        "epoch": 0.15060734779044638,
        "step": 2021
    },
    {
        "loss": 3.0036,
        "grad_norm": 2.7321722507476807,
        "learning_rate": 0.00011655321052646606,
        "epoch": 0.15068186899172814,
        "step": 2022
    },
    {
        "loss": 2.6701,
        "grad_norm": 2.180588722229004,
        "learning_rate": 0.00011648381149979419,
        "epoch": 0.1507563901930099,
        "step": 2023
    },
    {
        "loss": 2.2317,
        "grad_norm": 1.4317599534988403,
        "learning_rate": 0.0001164144043114475,
        "epoch": 0.15083091139429167,
        "step": 2024
    },
    {
        "loss": 2.5935,
        "grad_norm": 2.0661027431488037,
        "learning_rate": 0.00011634498899579182,
        "epoch": 0.15090543259557343,
        "step": 2025
    },
    {
        "loss": 2.741,
        "grad_norm": 3.478121757507324,
        "learning_rate": 0.00011627556558719684,
        "epoch": 0.1509799537968552,
        "step": 2026
    },
    {
        "loss": 1.5823,
        "grad_norm": 4.401633262634277,
        "learning_rate": 0.00011620613412003647,
        "epoch": 0.15105447499813696,
        "step": 2027
    },
    {
        "loss": 1.7342,
        "grad_norm": 2.3242671489715576,
        "learning_rate": 0.00011613669462868839,
        "epoch": 0.15112899619941875,
        "step": 2028
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.1521332263946533,
        "learning_rate": 0.00011606724714753445,
        "epoch": 0.1512035174007005,
        "step": 2029
    },
    {
        "loss": 1.9958,
        "grad_norm": 1.8911916017532349,
        "learning_rate": 0.00011599779171096035,
        "epoch": 0.15127803860198227,
        "step": 2030
    },
    {
        "loss": 2.0361,
        "grad_norm": 4.640079498291016,
        "learning_rate": 0.0001159283283533557,
        "epoch": 0.15135255980326404,
        "step": 2031
    },
    {
        "loss": 2.3023,
        "grad_norm": 2.4194140434265137,
        "learning_rate": 0.0001158588571091142,
        "epoch": 0.1514270810045458,
        "step": 2032
    },
    {
        "loss": 2.7565,
        "grad_norm": 3.6095266342163086,
        "learning_rate": 0.00011578937801263324,
        "epoch": 0.15150160220582756,
        "step": 2033
    },
    {
        "loss": 2.8688,
        "grad_norm": 2.323316812515259,
        "learning_rate": 0.0001157198910983142,
        "epoch": 0.15157612340710933,
        "step": 2034
    },
    {
        "loss": 1.5316,
        "grad_norm": 2.8116071224212646,
        "learning_rate": 0.00011565039640056239,
        "epoch": 0.1516506446083911,
        "step": 2035
    },
    {
        "loss": 2.0627,
        "grad_norm": 2.6667251586914062,
        "learning_rate": 0.00011558089395378681,
        "epoch": 0.15172516580967285,
        "step": 2036
    },
    {
        "loss": 2.5942,
        "grad_norm": 2.2980761528015137,
        "learning_rate": 0.0001155113837924005,
        "epoch": 0.15179968701095461,
        "step": 2037
    },
    {
        "loss": 2.3835,
        "grad_norm": 3.295483350753784,
        "learning_rate": 0.00011544186595082016,
        "epoch": 0.15187420821223638,
        "step": 2038
    },
    {
        "loss": 1.7974,
        "grad_norm": 2.8150928020477295,
        "learning_rate": 0.00011537234046346639,
        "epoch": 0.15194872941351814,
        "step": 2039
    },
    {
        "loss": 2.1838,
        "grad_norm": 4.391683578491211,
        "learning_rate": 0.00011530280736476349,
        "epoch": 0.1520232506147999,
        "step": 2040
    },
    {
        "loss": 2.0057,
        "grad_norm": 3.1891164779663086,
        "learning_rate": 0.0001152332666891396,
        "epoch": 0.15209777181608167,
        "step": 2041
    },
    {
        "loss": 1.4637,
        "grad_norm": 3.5316736698150635,
        "learning_rate": 0.00011516371847102656,
        "epoch": 0.15217229301736343,
        "step": 2042
    },
    {
        "loss": 1.8271,
        "grad_norm": 3.4360036849975586,
        "learning_rate": 0.00011509416274485998,
        "epoch": 0.1522468142186452,
        "step": 2043
    },
    {
        "loss": 2.5001,
        "grad_norm": 2.064358949661255,
        "learning_rate": 0.00011502459954507921,
        "epoch": 0.15232133541992696,
        "step": 2044
    },
    {
        "loss": 3.1544,
        "grad_norm": 4.112706184387207,
        "learning_rate": 0.00011495502890612723,
        "epoch": 0.15239585662120875,
        "step": 2045
    },
    {
        "loss": 2.7525,
        "grad_norm": 1.9121402502059937,
        "learning_rate": 0.00011488545086245078,
        "epoch": 0.1524703778224905,
        "step": 2046
    },
    {
        "loss": 1.9607,
        "grad_norm": 3.1051676273345947,
        "learning_rate": 0.00011481586544850018,
        "epoch": 0.15254489902377227,
        "step": 2047
    },
    {
        "loss": 2.5059,
        "grad_norm": 2.0886054039001465,
        "learning_rate": 0.00011474627269872946,
        "epoch": 0.15261942022505404,
        "step": 2048
    },
    {
        "loss": 2.7524,
        "grad_norm": 1.9943053722381592,
        "learning_rate": 0.0001146766726475963,
        "epoch": 0.1526939414263358,
        "step": 2049
    },
    {
        "loss": 2.235,
        "grad_norm": 2.0001842975616455,
        "learning_rate": 0.00011460706532956196,
        "epoch": 0.15276846262761756,
        "step": 2050
    },
    {
        "loss": 2.692,
        "grad_norm": 3.7479991912841797,
        "learning_rate": 0.00011453745077909124,
        "epoch": 0.15284298382889933,
        "step": 2051
    },
    {
        "loss": 2.5631,
        "grad_norm": 1.962811827659607,
        "learning_rate": 0.00011446782903065263,
        "epoch": 0.1529175050301811,
        "step": 2052
    },
    {
        "loss": 1.8158,
        "grad_norm": 3.205634117126465,
        "learning_rate": 0.0001143982001187182,
        "epoch": 0.15299202623146285,
        "step": 2053
    },
    {
        "loss": 2.3828,
        "grad_norm": 2.6224870681762695,
        "learning_rate": 0.00011432856407776336,
        "epoch": 0.15306654743274462,
        "step": 2054
    },
    {
        "loss": 1.9417,
        "grad_norm": 3.4529576301574707,
        "learning_rate": 0.00011425892094226733,
        "epoch": 0.15314106863402638,
        "step": 2055
    },
    {
        "loss": 2.1979,
        "grad_norm": 4.115844249725342,
        "learning_rate": 0.00011418927074671263,
        "epoch": 0.15321558983530814,
        "step": 2056
    },
    {
        "loss": 2.5046,
        "grad_norm": 2.5439090728759766,
        "learning_rate": 0.00011411961352558538,
        "epoch": 0.1532901110365899,
        "step": 2057
    },
    {
        "loss": 2.7776,
        "grad_norm": 2.354156255722046,
        "learning_rate": 0.00011404994931337513,
        "epoch": 0.15336463223787167,
        "step": 2058
    },
    {
        "loss": 2.7821,
        "grad_norm": 1.6962132453918457,
        "learning_rate": 0.00011398027814457492,
        "epoch": 0.15343915343915343,
        "step": 2059
    },
    {
        "loss": 2.8323,
        "grad_norm": 2.6833255290985107,
        "learning_rate": 0.00011391060005368129,
        "epoch": 0.1535136746404352,
        "step": 2060
    },
    {
        "loss": 2.9198,
        "grad_norm": 3.8980040550231934,
        "learning_rate": 0.00011384091507519403,
        "epoch": 0.15358819584171696,
        "step": 2061
    },
    {
        "loss": 1.4775,
        "grad_norm": 2.1079330444335938,
        "learning_rate": 0.00011377122324361654,
        "epoch": 0.15366271704299872,
        "step": 2062
    },
    {
        "loss": 1.9574,
        "grad_norm": 2.876465082168579,
        "learning_rate": 0.00011370152459345551,
        "epoch": 0.1537372382442805,
        "step": 2063
    },
    {
        "loss": 2.1225,
        "grad_norm": 3.4965250492095947,
        "learning_rate": 0.00011363181915922096,
        "epoch": 0.15381175944556227,
        "step": 2064
    },
    {
        "loss": 3.0064,
        "grad_norm": 2.056354522705078,
        "learning_rate": 0.00011356210697542646,
        "epoch": 0.15388628064684404,
        "step": 2065
    },
    {
        "loss": 2.9955,
        "grad_norm": 3.365739107131958,
        "learning_rate": 0.00011349238807658866,
        "epoch": 0.1539608018481258,
        "step": 2066
    },
    {
        "loss": 2.2681,
        "grad_norm": 2.194981575012207,
        "learning_rate": 0.00011342266249722777,
        "epoch": 0.15403532304940756,
        "step": 2067
    },
    {
        "loss": 2.3939,
        "grad_norm": 2.1795899868011475,
        "learning_rate": 0.00011335293027186717,
        "epoch": 0.15410984425068933,
        "step": 2068
    },
    {
        "loss": 1.0246,
        "grad_norm": 3.9001855850219727,
        "learning_rate": 0.00011328319143503355,
        "epoch": 0.1541843654519711,
        "step": 2069
    },
    {
        "loss": 2.5257,
        "grad_norm": 1.8477771282196045,
        "learning_rate": 0.00011321344602125694,
        "epoch": 0.15425888665325285,
        "step": 2070
    },
    {
        "loss": 2.8907,
        "grad_norm": 2.714468479156494,
        "learning_rate": 0.00011314369406507051,
        "epoch": 0.15433340785453462,
        "step": 2071
    },
    {
        "loss": 1.7704,
        "grad_norm": 2.8340229988098145,
        "learning_rate": 0.00011307393560101079,
        "epoch": 0.15440792905581638,
        "step": 2072
    },
    {
        "loss": 2.5523,
        "grad_norm": 2.240332841873169,
        "learning_rate": 0.00011300417066361747,
        "epoch": 0.15448245025709814,
        "step": 2073
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.567643642425537,
        "learning_rate": 0.00011293439928743345,
        "epoch": 0.1545569714583799,
        "step": 2074
    },
    {
        "loss": 1.8208,
        "grad_norm": 3.640719413757324,
        "learning_rate": 0.0001128646215070048,
        "epoch": 0.15463149265966167,
        "step": 2075
    },
    {
        "loss": 2.039,
        "grad_norm": 3.4734160900115967,
        "learning_rate": 0.0001127948373568808,
        "epoch": 0.15470601386094343,
        "step": 2076
    },
    {
        "loss": 3.3408,
        "grad_norm": 2.5964205265045166,
        "learning_rate": 0.00011272504687161392,
        "epoch": 0.1547805350622252,
        "step": 2077
    },
    {
        "loss": 2.6562,
        "grad_norm": 2.773016929626465,
        "learning_rate": 0.00011265525008575963,
        "epoch": 0.15485505626350696,
        "step": 2078
    },
    {
        "loss": 3.3019,
        "grad_norm": 2.5202934741973877,
        "learning_rate": 0.00011258544703387668,
        "epoch": 0.15492957746478872,
        "step": 2079
    },
    {
        "loss": 2.4793,
        "grad_norm": 3.007685422897339,
        "learning_rate": 0.00011251563775052676,
        "epoch": 0.15500409866607048,
        "step": 2080
    },
    {
        "loss": 2.2537,
        "grad_norm": 2.1124086380004883,
        "learning_rate": 0.00011244582227027484,
        "epoch": 0.15507861986735227,
        "step": 2081
    },
    {
        "loss": 1.9026,
        "grad_norm": 3.576324462890625,
        "learning_rate": 0.00011237600062768873,
        "epoch": 0.15515314106863404,
        "step": 2082
    },
    {
        "loss": 2.3056,
        "grad_norm": 2.986698865890503,
        "learning_rate": 0.0001123061728573395,
        "epoch": 0.1552276622699158,
        "step": 2083
    },
    {
        "loss": 2.5474,
        "grad_norm": 2.3477470874786377,
        "learning_rate": 0.00011223633899380114,
        "epoch": 0.15530218347119756,
        "step": 2084
    },
    {
        "loss": 2.6066,
        "grad_norm": 2.8206756114959717,
        "learning_rate": 0.00011216649907165069,
        "epoch": 0.15537670467247933,
        "step": 2085
    },
    {
        "loss": 2.8437,
        "grad_norm": 2.7652974128723145,
        "learning_rate": 0.00011209665312546817,
        "epoch": 0.1554512258737611,
        "step": 2086
    },
    {
        "loss": 3.1435,
        "grad_norm": 2.5508718490600586,
        "learning_rate": 0.00011202680118983655,
        "epoch": 0.15552574707504285,
        "step": 2087
    },
    {
        "loss": 1.8165,
        "grad_norm": 3.8319361209869385,
        "learning_rate": 0.00011195694329934194,
        "epoch": 0.15560026827632462,
        "step": 2088
    },
    {
        "loss": 1.767,
        "grad_norm": 3.485273838043213,
        "learning_rate": 0.00011188707948857313,
        "epoch": 0.15567478947760638,
        "step": 2089
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.4389612674713135,
        "learning_rate": 0.00011181720979212205,
        "epoch": 0.15574931067888814,
        "step": 2090
    },
    {
        "loss": 2.2269,
        "grad_norm": 2.125986099243164,
        "learning_rate": 0.00011174733424458348,
        "epoch": 0.1558238318801699,
        "step": 2091
    },
    {
        "loss": 3.1639,
        "grad_norm": 2.4773266315460205,
        "learning_rate": 0.00011167745288055505,
        "epoch": 0.15589835308145167,
        "step": 2092
    },
    {
        "loss": 2.2261,
        "grad_norm": 3.3580048084259033,
        "learning_rate": 0.00011160756573463735,
        "epoch": 0.15597287428273343,
        "step": 2093
    },
    {
        "loss": 2.2033,
        "grad_norm": 2.7589809894561768,
        "learning_rate": 0.00011153767284143377,
        "epoch": 0.1560473954840152,
        "step": 2094
    },
    {
        "loss": 1.7812,
        "grad_norm": 3.017723560333252,
        "learning_rate": 0.00011146777423555057,
        "epoch": 0.15612191668529696,
        "step": 2095
    },
    {
        "loss": 2.0441,
        "grad_norm": 4.515239238739014,
        "learning_rate": 0.00011139786995159689,
        "epoch": 0.15619643788657872,
        "step": 2096
    },
    {
        "loss": 2.516,
        "grad_norm": 2.8696911334991455,
        "learning_rate": 0.00011132796002418452,
        "epoch": 0.15627095908786048,
        "step": 2097
    },
    {
        "loss": 2.7158,
        "grad_norm": 2.595921277999878,
        "learning_rate": 0.00011125804448792831,
        "epoch": 0.15634548028914227,
        "step": 2098
    },
    {
        "loss": 2.9132,
        "grad_norm": 3.564545154571533,
        "learning_rate": 0.00011118812337744556,
        "epoch": 0.15642000149042404,
        "step": 2099
    },
    {
        "loss": 2.1362,
        "grad_norm": 2.387540578842163,
        "learning_rate": 0.00011111819672735664,
        "epoch": 0.1564945226917058,
        "step": 2100
    },
    {
        "loss": 2.509,
        "grad_norm": 1.8006494045257568,
        "learning_rate": 0.00011104826457228446,
        "epoch": 0.15656904389298756,
        "step": 2101
    },
    {
        "loss": 2.3583,
        "grad_norm": 3.3736345767974854,
        "learning_rate": 0.00011097832694685469,
        "epoch": 0.15664356509426933,
        "step": 2102
    },
    {
        "loss": 2.2606,
        "grad_norm": 2.8701882362365723,
        "learning_rate": 0.00011090838388569584,
        "epoch": 0.1567180862955511,
        "step": 2103
    },
    {
        "loss": 1.852,
        "grad_norm": 3.5709707736968994,
        "learning_rate": 0.00011083843542343891,
        "epoch": 0.15679260749683285,
        "step": 2104
    },
    {
        "loss": 1.9945,
        "grad_norm": 2.577597141265869,
        "learning_rate": 0.00011076848159471771,
        "epoch": 0.15686712869811462,
        "step": 2105
    },
    {
        "loss": 2.4732,
        "grad_norm": 3.652782917022705,
        "learning_rate": 0.00011069852243416867,
        "epoch": 0.15694164989939638,
        "step": 2106
    },
    {
        "loss": 2.533,
        "grad_norm": 3.4603865146636963,
        "learning_rate": 0.00011062855797643089,
        "epoch": 0.15701617110067814,
        "step": 2107
    },
    {
        "loss": 2.4588,
        "grad_norm": 3.8664331436157227,
        "learning_rate": 0.000110558588256146,
        "epoch": 0.1570906923019599,
        "step": 2108
    },
    {
        "loss": 2.1833,
        "grad_norm": 2.6760292053222656,
        "learning_rate": 0.00011048861330795835,
        "epoch": 0.15716521350324167,
        "step": 2109
    },
    {
        "loss": 2.6877,
        "grad_norm": 2.479027509689331,
        "learning_rate": 0.00011041863316651482,
        "epoch": 0.15723973470452343,
        "step": 2110
    },
    {
        "loss": 1.9528,
        "grad_norm": 5.003004550933838,
        "learning_rate": 0.00011034864786646488,
        "epoch": 0.1573142559058052,
        "step": 2111
    },
    {
        "loss": 2.4936,
        "grad_norm": 2.050096273422241,
        "learning_rate": 0.00011027865744246049,
        "epoch": 0.15738877710708696,
        "step": 2112
    },
    {
        "loss": 2.7836,
        "grad_norm": 1.9603532552719116,
        "learning_rate": 0.00011020866192915625,
        "epoch": 0.15746329830836872,
        "step": 2113
    },
    {
        "loss": 1.8182,
        "grad_norm": 1.8962029218673706,
        "learning_rate": 0.00011013866136120927,
        "epoch": 0.15753781950965048,
        "step": 2114
    },
    {
        "loss": 2.2181,
        "grad_norm": 2.9522271156311035,
        "learning_rate": 0.00011006865577327899,
        "epoch": 0.15761234071093225,
        "step": 2115
    },
    {
        "loss": 2.5085,
        "grad_norm": 2.0590126514434814,
        "learning_rate": 0.0001099986452000276,
        "epoch": 0.15768686191221404,
        "step": 2116
    },
    {
        "loss": 2.6411,
        "grad_norm": 2.0899291038513184,
        "learning_rate": 0.00010992862967611959,
        "epoch": 0.1577613831134958,
        "step": 2117
    },
    {
        "loss": 2.0915,
        "grad_norm": 2.8024158477783203,
        "learning_rate": 0.00010985860923622189,
        "epoch": 0.15783590431477756,
        "step": 2118
    },
    {
        "loss": 2.6534,
        "grad_norm": 2.378561019897461,
        "learning_rate": 0.000109788583915004,
        "epoch": 0.15791042551605933,
        "step": 2119
    },
    {
        "loss": 2.8215,
        "grad_norm": 3.16971755027771,
        "learning_rate": 0.00010971855374713764,
        "epoch": 0.1579849467173411,
        "step": 2120
    },
    {
        "loss": 2.0939,
        "grad_norm": 4.995590686798096,
        "learning_rate": 0.00010964851876729714,
        "epoch": 0.15805946791862285,
        "step": 2121
    },
    {
        "loss": 1.9784,
        "grad_norm": 4.459268093109131,
        "learning_rate": 0.00010957847901015905,
        "epoch": 0.15813398911990462,
        "step": 2122
    },
    {
        "loss": 2.8313,
        "grad_norm": 2.558709144592285,
        "learning_rate": 0.00010950843451040236,
        "epoch": 0.15820851032118638,
        "step": 2123
    },
    {
        "loss": 2.4158,
        "grad_norm": 2.6371114253997803,
        "learning_rate": 0.00010943838530270844,
        "epoch": 0.15828303152246814,
        "step": 2124
    },
    {
        "loss": 2.3835,
        "grad_norm": 3.0064799785614014,
        "learning_rate": 0.00010936833142176084,
        "epoch": 0.1583575527237499,
        "step": 2125
    },
    {
        "loss": 2.013,
        "grad_norm": 2.886138677597046,
        "learning_rate": 0.00010929827290224565,
        "epoch": 0.15843207392503167,
        "step": 2126
    },
    {
        "loss": 2.3348,
        "grad_norm": 2.782754421234131,
        "learning_rate": 0.00010922820977885106,
        "epoch": 0.15850659512631343,
        "step": 2127
    },
    {
        "loss": 2.2582,
        "grad_norm": 2.832468032836914,
        "learning_rate": 0.00010915814208626769,
        "epoch": 0.1585811163275952,
        "step": 2128
    },
    {
        "loss": 2.2463,
        "grad_norm": 2.973802089691162,
        "learning_rate": 0.00010908806985918827,
        "epoch": 0.15865563752887696,
        "step": 2129
    },
    {
        "loss": 2.7282,
        "grad_norm": 2.931328773498535,
        "learning_rate": 0.00010901799313230785,
        "epoch": 0.15873015873015872,
        "step": 2130
    },
    {
        "loss": 2.4813,
        "grad_norm": 2.638162851333618,
        "learning_rate": 0.00010894791194032383,
        "epoch": 0.15880467993144048,
        "step": 2131
    },
    {
        "loss": 2.1998,
        "grad_norm": 3.349579095840454,
        "learning_rate": 0.00010887782631793555,
        "epoch": 0.15887920113272225,
        "step": 2132
    },
    {
        "loss": 1.8626,
        "grad_norm": 3.9521713256835938,
        "learning_rate": 0.00010880773629984482,
        "epoch": 0.158953722334004,
        "step": 2133
    },
    {
        "loss": 1.7917,
        "grad_norm": 2.90962553024292,
        "learning_rate": 0.0001087376419207554,
        "epoch": 0.1590282435352858,
        "step": 2134
    },
    {
        "loss": 2.3633,
        "grad_norm": 2.2866199016571045,
        "learning_rate": 0.00010866754321537341,
        "epoch": 0.15910276473656756,
        "step": 2135
    },
    {
        "loss": 2.4828,
        "grad_norm": 2.9833791255950928,
        "learning_rate": 0.00010859744021840692,
        "epoch": 0.15917728593784933,
        "step": 2136
    },
    {
        "loss": 2.449,
        "grad_norm": 2.39255690574646,
        "learning_rate": 0.00010852733296456628,
        "epoch": 0.1592518071391311,
        "step": 2137
    },
    {
        "loss": 2.7491,
        "grad_norm": 2.1153371334075928,
        "learning_rate": 0.00010845722148856388,
        "epoch": 0.15932632834041285,
        "step": 2138
    },
    {
        "loss": 2.5542,
        "grad_norm": 2.3720970153808594,
        "learning_rate": 0.00010838710582511419,
        "epoch": 0.15940084954169462,
        "step": 2139
    },
    {
        "loss": 1.8458,
        "grad_norm": 2.4119067192077637,
        "learning_rate": 0.00010831698600893379,
        "epoch": 0.15947537074297638,
        "step": 2140
    },
    {
        "loss": 2.5841,
        "grad_norm": 4.469527721405029,
        "learning_rate": 0.00010824686207474126,
        "epoch": 0.15954989194425814,
        "step": 2141
    },
    {
        "loss": 2.4685,
        "grad_norm": 4.285590648651123,
        "learning_rate": 0.0001081767340572573,
        "epoch": 0.1596244131455399,
        "step": 2142
    },
    {
        "loss": 2.2471,
        "grad_norm": 3.1531927585601807,
        "learning_rate": 0.00010810660199120459,
        "epoch": 0.15969893434682167,
        "step": 2143
    },
    {
        "loss": 2.2174,
        "grad_norm": 3.3612911701202393,
        "learning_rate": 0.0001080364659113078,
        "epoch": 0.15977345554810343,
        "step": 2144
    },
    {
        "loss": 3.0268,
        "grad_norm": 2.0154731273651123,
        "learning_rate": 0.00010796632585229361,
        "epoch": 0.1598479767493852,
        "step": 2145
    },
    {
        "loss": 2.4397,
        "grad_norm": 2.526724338531494,
        "learning_rate": 0.00010789618184889059,
        "epoch": 0.15992249795066696,
        "step": 2146
    },
    {
        "loss": 1.8932,
        "grad_norm": 3.890465259552002,
        "learning_rate": 0.00010782603393582941,
        "epoch": 0.15999701915194872,
        "step": 2147
    },
    {
        "loss": 2.6192,
        "grad_norm": 1.8943371772766113,
        "learning_rate": 0.00010775588214784254,
        "epoch": 0.16007154035323048,
        "step": 2148
    },
    {
        "loss": 2.6675,
        "grad_norm": 2.3665902614593506,
        "learning_rate": 0.00010768572651966451,
        "epoch": 0.16014606155451225,
        "step": 2149
    },
    {
        "loss": 2.4674,
        "grad_norm": 2.8125526905059814,
        "learning_rate": 0.0001076155670860316,
        "epoch": 0.160220582755794,
        "step": 2150
    },
    {
        "loss": 2.253,
        "grad_norm": 2.1438591480255127,
        "learning_rate": 0.00010754540388168206,
        "epoch": 0.1602951039570758,
        "step": 2151
    },
    {
        "loss": 2.4499,
        "grad_norm": 2.471928119659424,
        "learning_rate": 0.00010747523694135605,
        "epoch": 0.16036962515835756,
        "step": 2152
    },
    {
        "loss": 2.6756,
        "grad_norm": 2.9134461879730225,
        "learning_rate": 0.00010740506629979537,
        "epoch": 0.16044414635963933,
        "step": 2153
    },
    {
        "loss": 1.4735,
        "grad_norm": 4.0794548988342285,
        "learning_rate": 0.00010733489199174396,
        "epoch": 0.1605186675609211,
        "step": 2154
    },
    {
        "loss": 2.8253,
        "grad_norm": 2.7663044929504395,
        "learning_rate": 0.00010726471405194731,
        "epoch": 0.16059318876220285,
        "step": 2155
    },
    {
        "loss": 2.5438,
        "grad_norm": 2.510618209838867,
        "learning_rate": 0.00010719453251515288,
        "epoch": 0.16066770996348462,
        "step": 2156
    },
    {
        "loss": 2.4714,
        "grad_norm": 2.8721771240234375,
        "learning_rate": 0.00010712434741610986,
        "epoch": 0.16074223116476638,
        "step": 2157
    },
    {
        "loss": 2.9116,
        "grad_norm": 2.1514978408813477,
        "learning_rate": 0.00010705415878956908,
        "epoch": 0.16081675236604814,
        "step": 2158
    },
    {
        "loss": 2.7745,
        "grad_norm": 3.1635985374450684,
        "learning_rate": 0.00010698396667028339,
        "epoch": 0.1608912735673299,
        "step": 2159
    },
    {
        "loss": 2.7125,
        "grad_norm": 2.2537014484405518,
        "learning_rate": 0.00010691377109300707,
        "epoch": 0.16096579476861167,
        "step": 2160
    },
    {
        "loss": 1.5951,
        "grad_norm": 2.219015598297119,
        "learning_rate": 0.0001068435720924963,
        "epoch": 0.16104031596989343,
        "step": 2161
    },
    {
        "loss": 2.0924,
        "grad_norm": 3.3087592124938965,
        "learning_rate": 0.00010677336970350891,
        "epoch": 0.1611148371711752,
        "step": 2162
    },
    {
        "loss": 2.1879,
        "grad_norm": 2.367945909500122,
        "learning_rate": 0.00010670316396080438,
        "epoch": 0.16118935837245696,
        "step": 2163
    },
    {
        "loss": 2.1614,
        "grad_norm": 3.087171792984009,
        "learning_rate": 0.00010663295489914393,
        "epoch": 0.16126387957373872,
        "step": 2164
    },
    {
        "loss": 2.4331,
        "grad_norm": 1.8651812076568604,
        "learning_rate": 0.00010656274255329029,
        "epoch": 0.16133840077502049,
        "step": 2165
    },
    {
        "loss": 2.667,
        "grad_norm": 3.182795524597168,
        "learning_rate": 0.00010649252695800793,
        "epoch": 0.16141292197630225,
        "step": 2166
    },
    {
        "loss": 2.2183,
        "grad_norm": 3.10892391204834,
        "learning_rate": 0.00010642230814806288,
        "epoch": 0.161487443177584,
        "step": 2167
    },
    {
        "loss": 2.8737,
        "grad_norm": 2.91375470161438,
        "learning_rate": 0.00010635208615822277,
        "epoch": 0.16156196437886577,
        "step": 2168
    },
    {
        "loss": 2.4804,
        "grad_norm": 2.5265209674835205,
        "learning_rate": 0.00010628186102325681,
        "epoch": 0.16163648558014757,
        "step": 2169
    },
    {
        "loss": 2.3427,
        "grad_norm": 2.8718433380126953,
        "learning_rate": 0.00010621163277793575,
        "epoch": 0.16171100678142933,
        "step": 2170
    },
    {
        "loss": 2.6633,
        "grad_norm": 2.789128303527832,
        "learning_rate": 0.00010614140145703195,
        "epoch": 0.1617855279827111,
        "step": 2171
    },
    {
        "loss": 2.4964,
        "grad_norm": 2.6266469955444336,
        "learning_rate": 0.00010607116709531918,
        "epoch": 0.16186004918399285,
        "step": 2172
    },
    {
        "loss": 2.4353,
        "grad_norm": 1.6879613399505615,
        "learning_rate": 0.0001060009297275728,
        "epoch": 0.16193457038527462,
        "step": 2173
    },
    {
        "loss": 2.9015,
        "grad_norm": 2.2403452396392822,
        "learning_rate": 0.00010593068938856961,
        "epoch": 0.16200909158655638,
        "step": 2174
    },
    {
        "loss": 2.609,
        "grad_norm": 2.402115821838379,
        "learning_rate": 0.00010586044611308792,
        "epoch": 0.16208361278783814,
        "step": 2175
    },
    {
        "loss": 2.9539,
        "grad_norm": 1.9833731651306152,
        "learning_rate": 0.00010579019993590742,
        "epoch": 0.1621581339891199,
        "step": 2176
    },
    {
        "loss": 2.7754,
        "grad_norm": 3.30411958694458,
        "learning_rate": 0.00010571995089180937,
        "epoch": 0.16223265519040167,
        "step": 2177
    },
    {
        "loss": 2.8839,
        "grad_norm": 1.9503562450408936,
        "learning_rate": 0.00010564969901557637,
        "epoch": 0.16230717639168343,
        "step": 2178
    },
    {
        "loss": 2.4783,
        "grad_norm": 3.7488906383514404,
        "learning_rate": 0.00010557944434199236,
        "epoch": 0.1623816975929652,
        "step": 2179
    },
    {
        "loss": 2.5187,
        "grad_norm": 3.361398220062256,
        "learning_rate": 0.00010550918690584277,
        "epoch": 0.16245621879424696,
        "step": 2180
    },
    {
        "loss": 2.239,
        "grad_norm": 2.469910144805908,
        "learning_rate": 0.00010543892674191436,
        "epoch": 0.16253073999552872,
        "step": 2181
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.0626463890075684,
        "learning_rate": 0.00010536866388499522,
        "epoch": 0.16260526119681049,
        "step": 2182
    },
    {
        "loss": 2.0978,
        "grad_norm": 2.987877368927002,
        "learning_rate": 0.0001052983983698748,
        "epoch": 0.16267978239809225,
        "step": 2183
    },
    {
        "loss": 2.8479,
        "grad_norm": 2.9650988578796387,
        "learning_rate": 0.00010522813023134386,
        "epoch": 0.162754303599374,
        "step": 2184
    },
    {
        "loss": 2.8703,
        "grad_norm": 3.4230921268463135,
        "learning_rate": 0.00010515785950419448,
        "epoch": 0.16282882480065577,
        "step": 2185
    },
    {
        "loss": 3.0714,
        "grad_norm": 1.588976263999939,
        "learning_rate": 0.00010508758622321993,
        "epoch": 0.16290334600193754,
        "step": 2186
    },
    {
        "loss": 2.4946,
        "grad_norm": 3.1306161880493164,
        "learning_rate": 0.0001050173104232149,
        "epoch": 0.16297786720321933,
        "step": 2187
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.0906848907470703,
        "learning_rate": 0.00010494703213897514,
        "epoch": 0.1630523884045011,
        "step": 2188
    },
    {
        "loss": 2.6225,
        "grad_norm": 2.6461539268493652,
        "learning_rate": 0.0001048767514052978,
        "epoch": 0.16312690960578285,
        "step": 2189
    },
    {
        "loss": 2.6949,
        "grad_norm": 2.825085401535034,
        "learning_rate": 0.00010480646825698118,
        "epoch": 0.16320143080706462,
        "step": 2190
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.8698232173919678,
        "learning_rate": 0.0001047361827288247,
        "epoch": 0.16327595200834638,
        "step": 2191
    },
    {
        "loss": 2.7349,
        "grad_norm": 3.6535236835479736,
        "learning_rate": 0.00010466589485562915,
        "epoch": 0.16335047320962814,
        "step": 2192
    },
    {
        "loss": 2.5,
        "grad_norm": 3.675983190536499,
        "learning_rate": 0.0001045956046721962,
        "epoch": 0.1634249944109099,
        "step": 2193
    },
    {
        "loss": 2.5799,
        "grad_norm": 3.1443352699279785,
        "learning_rate": 0.00010452531221332895,
        "epoch": 0.16349951561219167,
        "step": 2194
    },
    {
        "loss": 2.7885,
        "grad_norm": 3.052745819091797,
        "learning_rate": 0.0001044550175138314,
        "epoch": 0.16357403681347343,
        "step": 2195
    },
    {
        "loss": 2.0494,
        "grad_norm": 3.5561506748199463,
        "learning_rate": 0.00010438472060850884,
        "epoch": 0.1636485580147552,
        "step": 2196
    },
    {
        "loss": 1.7842,
        "grad_norm": 2.891211986541748,
        "learning_rate": 0.00010431442153216753,
        "epoch": 0.16372307921603696,
        "step": 2197
    },
    {
        "loss": 2.5645,
        "grad_norm": 2.7592692375183105,
        "learning_rate": 0.00010424412031961484,
        "epoch": 0.16379760041731872,
        "step": 2198
    },
    {
        "loss": 2.1673,
        "grad_norm": 2.6065967082977295,
        "learning_rate": 0.00010417381700565922,
        "epoch": 0.16387212161860049,
        "step": 2199
    },
    {
        "loss": 2.6504,
        "grad_norm": 3.4607629776000977,
        "learning_rate": 0.00010410351162511015,
        "epoch": 0.16394664281988225,
        "step": 2200
    },
    {
        "loss": 2.2159,
        "grad_norm": 2.909057140350342,
        "learning_rate": 0.00010403320421277809,
        "epoch": 0.164021164021164,
        "step": 2201
    },
    {
        "loss": 2.1702,
        "grad_norm": 2.2756078243255615,
        "learning_rate": 0.00010396289480347454,
        "epoch": 0.16409568522244578,
        "step": 2202
    },
    {
        "loss": 2.6105,
        "grad_norm": 2.271670341491699,
        "learning_rate": 0.00010389258343201205,
        "epoch": 0.16417020642372754,
        "step": 2203
    },
    {
        "loss": 2.529,
        "grad_norm": 2.963075637817383,
        "learning_rate": 0.000103822270133204,
        "epoch": 0.1642447276250093,
        "step": 2204
    },
    {
        "loss": 2.2327,
        "grad_norm": 2.955688714981079,
        "learning_rate": 0.0001037519549418649,
        "epoch": 0.1643192488262911,
        "step": 2205
    },
    {
        "loss": 2.4736,
        "grad_norm": 2.989574670791626,
        "learning_rate": 0.00010368163789281005,
        "epoch": 0.16439377002757286,
        "step": 2206
    },
    {
        "loss": 2.4664,
        "grad_norm": 3.9418394565582275,
        "learning_rate": 0.0001036113190208557,
        "epoch": 0.16446829122885462,
        "step": 2207
    },
    {
        "loss": 2.4919,
        "grad_norm": 1.894105315208435,
        "learning_rate": 0.00010354099836081909,
        "epoch": 0.16454281243013638,
        "step": 2208
    },
    {
        "loss": 2.9576,
        "grad_norm": 2.856083869934082,
        "learning_rate": 0.0001034706759475182,
        "epoch": 0.16461733363141814,
        "step": 2209
    },
    {
        "loss": 2.1914,
        "grad_norm": 4.05551290512085,
        "learning_rate": 0.00010340035181577204,
        "epoch": 0.1646918548326999,
        "step": 2210
    },
    {
        "loss": 2.5511,
        "grad_norm": 5.08676290512085,
        "learning_rate": 0.00010333002600040036,
        "epoch": 0.16476637603398167,
        "step": 2211
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.9571412801742554,
        "learning_rate": 0.00010325969853622375,
        "epoch": 0.16484089723526343,
        "step": 2212
    },
    {
        "loss": 2.5824,
        "grad_norm": 3.170474052429199,
        "learning_rate": 0.0001031893694580637,
        "epoch": 0.1649154184365452,
        "step": 2213
    },
    {
        "loss": 2.3686,
        "grad_norm": 2.12166690826416,
        "learning_rate": 0.00010311903880074232,
        "epoch": 0.16498993963782696,
        "step": 2214
    },
    {
        "loss": 1.9785,
        "grad_norm": 2.839387893676758,
        "learning_rate": 0.00010304870659908277,
        "epoch": 0.16506446083910872,
        "step": 2215
    },
    {
        "loss": 2.5888,
        "grad_norm": 2.8691184520721436,
        "learning_rate": 0.00010297837288790873,
        "epoch": 0.1651389820403905,
        "step": 2216
    },
    {
        "loss": 2.4829,
        "grad_norm": 1.671534538269043,
        "learning_rate": 0.00010290803770204473,
        "epoch": 0.16521350324167225,
        "step": 2217
    },
    {
        "loss": 2.5638,
        "grad_norm": 2.3242709636688232,
        "learning_rate": 0.00010283770107631609,
        "epoch": 0.165288024442954,
        "step": 2218
    },
    {
        "loss": 2.3792,
        "grad_norm": 2.527496576309204,
        "learning_rate": 0.00010276736304554868,
        "epoch": 0.16536254564423578,
        "step": 2219
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.771523952484131,
        "learning_rate": 0.00010269702364456928,
        "epoch": 0.16543706684551754,
        "step": 2220
    },
    {
        "loss": 2.0162,
        "grad_norm": 4.0572285652160645,
        "learning_rate": 0.00010262668290820511,
        "epoch": 0.1655115880467993,
        "step": 2221
    },
    {
        "loss": 2.6223,
        "grad_norm": 2.7056925296783447,
        "learning_rate": 0.00010255634087128425,
        "epoch": 0.1655861092480811,
        "step": 2222
    },
    {
        "loss": 2.5118,
        "grad_norm": 2.259841203689575,
        "learning_rate": 0.0001024859975686353,
        "epoch": 0.16566063044936286,
        "step": 2223
    },
    {
        "loss": 2.4386,
        "grad_norm": 1.936010479927063,
        "learning_rate": 0.00010241565303508755,
        "epoch": 0.16573515165064462,
        "step": 2224
    },
    {
        "loss": 2.7348,
        "grad_norm": 4.015944957733154,
        "learning_rate": 0.00010234530730547092,
        "epoch": 0.16580967285192638,
        "step": 2225
    },
    {
        "loss": 2.4691,
        "grad_norm": 3.3349406719207764,
        "learning_rate": 0.00010227496041461581,
        "epoch": 0.16588419405320814,
        "step": 2226
    },
    {
        "loss": 2.6699,
        "grad_norm": 1.9476203918457031,
        "learning_rate": 0.00010220461239735335,
        "epoch": 0.1659587152544899,
        "step": 2227
    },
    {
        "loss": 2.3101,
        "grad_norm": 2.2426350116729736,
        "learning_rate": 0.00010213426328851508,
        "epoch": 0.16603323645577167,
        "step": 2228
    },
    {
        "loss": 2.6166,
        "grad_norm": 2.2095754146575928,
        "learning_rate": 0.00010206391312293316,
        "epoch": 0.16610775765705343,
        "step": 2229
    },
    {
        "loss": 2.491,
        "grad_norm": 2.325066328048706,
        "learning_rate": 0.00010199356193544028,
        "epoch": 0.1661822788583352,
        "step": 2230
    },
    {
        "loss": 2.7482,
        "grad_norm": 3.3668646812438965,
        "learning_rate": 0.0001019232097608696,
        "epoch": 0.16625680005961696,
        "step": 2231
    },
    {
        "loss": 2.0962,
        "grad_norm": 2.653668165206909,
        "learning_rate": 0.00010185285663405482,
        "epoch": 0.16633132126089872,
        "step": 2232
    },
    {
        "loss": 1.5749,
        "grad_norm": 3.330538272857666,
        "learning_rate": 0.00010178250258983005,
        "epoch": 0.1664058424621805,
        "step": 2233
    },
    {
        "loss": 2.5245,
        "grad_norm": 3.0827062129974365,
        "learning_rate": 0.00010171214766302988,
        "epoch": 0.16648036366346225,
        "step": 2234
    },
    {
        "loss": 2.1082,
        "grad_norm": 3.9726874828338623,
        "learning_rate": 0.0001016417918884893,
        "epoch": 0.166554884864744,
        "step": 2235
    },
    {
        "loss": 2.6629,
        "grad_norm": 2.5596811771392822,
        "learning_rate": 0.00010157143530104382,
        "epoch": 0.16662940606602578,
        "step": 2236
    },
    {
        "loss": 2.4159,
        "grad_norm": 2.893895387649536,
        "learning_rate": 0.0001015010779355293,
        "epoch": 0.16670392726730754,
        "step": 2237
    },
    {
        "loss": 2.489,
        "grad_norm": 2.802629232406616,
        "learning_rate": 0.00010143071982678189,
        "epoch": 0.1667784484685893,
        "step": 2238
    },
    {
        "loss": 1.821,
        "grad_norm": 3.022740364074707,
        "learning_rate": 0.0001013603610096383,
        "epoch": 0.16685296966987107,
        "step": 2239
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.7534902095794678,
        "learning_rate": 0.0001012900015189354,
        "epoch": 0.16692749087115286,
        "step": 2240
    },
    {
        "loss": 2.5606,
        "grad_norm": 2.1687560081481934,
        "learning_rate": 0.00010121964138951055,
        "epoch": 0.16700201207243462,
        "step": 2241
    },
    {
        "loss": 2.7372,
        "grad_norm": 2.041273832321167,
        "learning_rate": 0.00010114928065620124,
        "epoch": 0.16707653327371638,
        "step": 2242
    },
    {
        "loss": 2.5906,
        "grad_norm": 2.377697467803955,
        "learning_rate": 0.0001010789193538455,
        "epoch": 0.16715105447499815,
        "step": 2243
    },
    {
        "loss": 2.4056,
        "grad_norm": 2.5860612392425537,
        "learning_rate": 0.00010100855751728142,
        "epoch": 0.1672255756762799,
        "step": 2244
    },
    {
        "loss": 2.3197,
        "grad_norm": 3.412395477294922,
        "learning_rate": 0.0001009381951813475,
        "epoch": 0.16730009687756167,
        "step": 2245
    },
    {
        "loss": 2.113,
        "grad_norm": 3.4579782485961914,
        "learning_rate": 0.00010086783238088246,
        "epoch": 0.16737461807884343,
        "step": 2246
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.0798447132110596,
        "learning_rate": 0.00010079746915072515,
        "epoch": 0.1674491392801252,
        "step": 2247
    },
    {
        "loss": 2.8898,
        "grad_norm": 2.7026455402374268,
        "learning_rate": 0.00010072710552571481,
        "epoch": 0.16752366048140696,
        "step": 2248
    },
    {
        "loss": 3.0317,
        "grad_norm": 1.829735279083252,
        "learning_rate": 0.00010065674154069067,
        "epoch": 0.16759818168268872,
        "step": 2249
    },
    {
        "loss": 1.358,
        "grad_norm": 2.8089563846588135,
        "learning_rate": 0.00010058637723049227,
        "epoch": 0.1676727028839705,
        "step": 2250
    },
    {
        "loss": 2.7608,
        "grad_norm": 2.5975341796875,
        "learning_rate": 0.00010051601262995936,
        "epoch": 0.16774722408525225,
        "step": 2251
    },
    {
        "loss": 1.4434,
        "grad_norm": 4.535867691040039,
        "learning_rate": 0.00010044564777393161,
        "epoch": 0.167821745286534,
        "step": 2252
    },
    {
        "loss": 2.3782,
        "grad_norm": 2.197739601135254,
        "learning_rate": 0.00010037528269724917,
        "epoch": 0.16789626648781578,
        "step": 2253
    },
    {
        "loss": 2.0168,
        "grad_norm": 2.8129775524139404,
        "learning_rate": 0.0001003049174347519,
        "epoch": 0.16797078768909754,
        "step": 2254
    },
    {
        "loss": 2.7388,
        "grad_norm": 2.7429442405700684,
        "learning_rate": 0.00010023455202128008,
        "epoch": 0.1680453088903793,
        "step": 2255
    },
    {
        "loss": 2.6323,
        "grad_norm": 1.880183458328247,
        "learning_rate": 0.00010016418649167383,
        "epoch": 0.16811983009166107,
        "step": 2256
    },
    {
        "loss": 2.633,
        "grad_norm": 2.2205307483673096,
        "learning_rate": 0.00010009382088077346,
        "epoch": 0.16819435129294283,
        "step": 2257
    },
    {
        "loss": 2.419,
        "grad_norm": 2.4413139820098877,
        "learning_rate": 0.00010002345522341933,
        "epoch": 0.16826887249422462,
        "step": 2258
    },
    {
        "loss": 2.1342,
        "grad_norm": 3.9048423767089844,
        "learning_rate": 9.995308955445173e-05,
        "epoch": 0.16834339369550638,
        "step": 2259
    },
    {
        "loss": 2.8535,
        "grad_norm": 3.170104742050171,
        "learning_rate": 9.988272390871106e-05,
        "epoch": 0.16841791489678815,
        "step": 2260
    },
    {
        "loss": 2.0661,
        "grad_norm": 2.857734441757202,
        "learning_rate": 9.981235832103761e-05,
        "epoch": 0.1684924360980699,
        "step": 2261
    },
    {
        "loss": 2.5447,
        "grad_norm": 3.267454147338867,
        "learning_rate": 9.974199282627171e-05,
        "epoch": 0.16856695729935167,
        "step": 2262
    },
    {
        "loss": 3.5139,
        "grad_norm": 3.4880621433258057,
        "learning_rate": 9.967162745925356e-05,
        "epoch": 0.16864147850063343,
        "step": 2263
    },
    {
        "loss": 2.1048,
        "grad_norm": 3.3274292945861816,
        "learning_rate": 9.960126225482345e-05,
        "epoch": 0.1687159997019152,
        "step": 2264
    },
    {
        "loss": 2.545,
        "grad_norm": 2.077777147293091,
        "learning_rate": 9.953089724782143e-05,
        "epoch": 0.16879052090319696,
        "step": 2265
    },
    {
        "loss": 2.7601,
        "grad_norm": 2.5909886360168457,
        "learning_rate": 9.946053247308758e-05,
        "epoch": 0.16886504210447872,
        "step": 2266
    },
    {
        "loss": 2.5203,
        "grad_norm": 3.0091207027435303,
        "learning_rate": 9.939016796546174e-05,
        "epoch": 0.1689395633057605,
        "step": 2267
    },
    {
        "loss": 2.371,
        "grad_norm": 2.9042344093322754,
        "learning_rate": 9.931980375978368e-05,
        "epoch": 0.16901408450704225,
        "step": 2268
    },
    {
        "loss": 2.8656,
        "grad_norm": 3.8938212394714355,
        "learning_rate": 9.924943989089315e-05,
        "epoch": 0.169088605708324,
        "step": 2269
    },
    {
        "loss": 2.2114,
        "grad_norm": 3.15315580368042,
        "learning_rate": 9.91790763936294e-05,
        "epoch": 0.16916312690960578,
        "step": 2270
    },
    {
        "loss": 2.2066,
        "grad_norm": 3.7057790756225586,
        "learning_rate": 9.910871330283185e-05,
        "epoch": 0.16923764811088754,
        "step": 2271
    },
    {
        "loss": 2.0642,
        "grad_norm": 3.476219654083252,
        "learning_rate": 9.903835065333952e-05,
        "epoch": 0.1693121693121693,
        "step": 2272
    },
    {
        "loss": 2.1643,
        "grad_norm": 2.4922127723693848,
        "learning_rate": 9.896798847999125e-05,
        "epoch": 0.16938669051345107,
        "step": 2273
    },
    {
        "loss": 1.461,
        "grad_norm": 2.732670307159424,
        "learning_rate": 9.889762681762576e-05,
        "epoch": 0.16946121171473283,
        "step": 2274
    },
    {
        "loss": 2.9277,
        "grad_norm": 2.434885263442993,
        "learning_rate": 9.882726570108123e-05,
        "epoch": 0.16953573291601462,
        "step": 2275
    },
    {
        "loss": 1.5178,
        "grad_norm": 3.2540931701660156,
        "learning_rate": 9.875690516519592e-05,
        "epoch": 0.16961025411729638,
        "step": 2276
    },
    {
        "loss": 2.4892,
        "grad_norm": 3.8245580196380615,
        "learning_rate": 9.868654524480753e-05,
        "epoch": 0.16968477531857815,
        "step": 2277
    },
    {
        "loss": 1.7955,
        "grad_norm": 4.460704803466797,
        "learning_rate": 9.861618597475356e-05,
        "epoch": 0.1697592965198599,
        "step": 2278
    },
    {
        "loss": 2.2983,
        "grad_norm": 3.0639941692352295,
        "learning_rate": 9.854582738987131e-05,
        "epoch": 0.16983381772114167,
        "step": 2279
    },
    {
        "loss": 2.2279,
        "grad_norm": 1.1126644611358643,
        "learning_rate": 9.847546952499745e-05,
        "epoch": 0.16990833892242344,
        "step": 2280
    },
    {
        "loss": 2.4781,
        "grad_norm": 4.0081939697265625,
        "learning_rate": 9.840511241496864e-05,
        "epoch": 0.1699828601237052,
        "step": 2281
    },
    {
        "loss": 2.7903,
        "grad_norm": 1.8629000186920166,
        "learning_rate": 9.833475609462082e-05,
        "epoch": 0.17005738132498696,
        "step": 2282
    },
    {
        "loss": 2.7522,
        "grad_norm": 2.5199170112609863,
        "learning_rate": 9.826440059878977e-05,
        "epoch": 0.17013190252626872,
        "step": 2283
    },
    {
        "loss": 2.3281,
        "grad_norm": 3.0421531200408936,
        "learning_rate": 9.819404596231089e-05,
        "epoch": 0.1702064237275505,
        "step": 2284
    },
    {
        "loss": 2.4655,
        "grad_norm": 3.30511212348938,
        "learning_rate": 9.812369222001893e-05,
        "epoch": 0.17028094492883225,
        "step": 2285
    },
    {
        "loss": 1.9103,
        "grad_norm": 3.4017696380615234,
        "learning_rate": 9.805333940674845e-05,
        "epoch": 0.170355466130114,
        "step": 2286
    },
    {
        "loss": 2.3891,
        "grad_norm": 2.512397289276123,
        "learning_rate": 9.79829875573333e-05,
        "epoch": 0.17042998733139578,
        "step": 2287
    },
    {
        "loss": 2.8101,
        "grad_norm": 1.9270553588867188,
        "learning_rate": 9.791263670660708e-05,
        "epoch": 0.17050450853267754,
        "step": 2288
    },
    {
        "loss": 2.4886,
        "grad_norm": 3.113288402557373,
        "learning_rate": 9.784228688940281e-05,
        "epoch": 0.1705790297339593,
        "step": 2289
    },
    {
        "loss": 2.5684,
        "grad_norm": 2.17187762260437,
        "learning_rate": 9.777193814055288e-05,
        "epoch": 0.17065355093524107,
        "step": 2290
    },
    {
        "loss": 1.7903,
        "grad_norm": 3.596576452255249,
        "learning_rate": 9.77015904948894e-05,
        "epoch": 0.17072807213652283,
        "step": 2291
    },
    {
        "loss": 2.4804,
        "grad_norm": 2.7457997798919678,
        "learning_rate": 9.76312439872437e-05,
        "epoch": 0.1708025933378046,
        "step": 2292
    },
    {
        "loss": 2.5303,
        "grad_norm": 2.3290679454803467,
        "learning_rate": 9.756089865244664e-05,
        "epoch": 0.17087711453908638,
        "step": 2293
    },
    {
        "loss": 2.7502,
        "grad_norm": 2.829831123352051,
        "learning_rate": 9.749055452532856e-05,
        "epoch": 0.17095163574036815,
        "step": 2294
    },
    {
        "loss": 2.5613,
        "grad_norm": 2.5986173152923584,
        "learning_rate": 9.74202116407191e-05,
        "epoch": 0.1710261569416499,
        "step": 2295
    },
    {
        "loss": 1.8593,
        "grad_norm": 3.330489158630371,
        "learning_rate": 9.734987003344732e-05,
        "epoch": 0.17110067814293167,
        "step": 2296
    },
    {
        "loss": 2.5088,
        "grad_norm": 3.647275447845459,
        "learning_rate": 9.727952973834164e-05,
        "epoch": 0.17117519934421344,
        "step": 2297
    },
    {
        "loss": 2.2484,
        "grad_norm": 2.067462205886841,
        "learning_rate": 9.720919079022988e-05,
        "epoch": 0.1712497205454952,
        "step": 2298
    },
    {
        "loss": 1.7286,
        "grad_norm": 3.7834815979003906,
        "learning_rate": 9.713885322393914e-05,
        "epoch": 0.17132424174677696,
        "step": 2299
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.413973808288574,
        "learning_rate": 9.706851707429586e-05,
        "epoch": 0.17139876294805872,
        "step": 2300
    },
    {
        "loss": 2.3574,
        "grad_norm": 1.9514673948287964,
        "learning_rate": 9.699818237612578e-05,
        "epoch": 0.1714732841493405,
        "step": 2301
    },
    {
        "loss": 1.9792,
        "grad_norm": 2.809114694595337,
        "learning_rate": 9.692784916425389e-05,
        "epoch": 0.17154780535062225,
        "step": 2302
    },
    {
        "loss": 1.8126,
        "grad_norm": 5.284473896026611,
        "learning_rate": 9.685751747350442e-05,
        "epoch": 0.17162232655190401,
        "step": 2303
    },
    {
        "loss": 2.6081,
        "grad_norm": 2.1583616733551025,
        "learning_rate": 9.6787187338701e-05,
        "epoch": 0.17169684775318578,
        "step": 2304
    },
    {
        "loss": 2.9556,
        "grad_norm": 2.515789747238159,
        "learning_rate": 9.671685879466634e-05,
        "epoch": 0.17177136895446754,
        "step": 2305
    },
    {
        "loss": 2.4758,
        "grad_norm": 3.4879302978515625,
        "learning_rate": 9.664653187622234e-05,
        "epoch": 0.1718458901557493,
        "step": 2306
    },
    {
        "loss": 2.7965,
        "grad_norm": 1.5095769166946411,
        "learning_rate": 9.657620661819025e-05,
        "epoch": 0.17192041135703107,
        "step": 2307
    },
    {
        "loss": 2.3788,
        "grad_norm": 1.9116474390029907,
        "learning_rate": 9.650588305539034e-05,
        "epoch": 0.17199493255831283,
        "step": 2308
    },
    {
        "loss": 2.6807,
        "grad_norm": 2.5170934200286865,
        "learning_rate": 9.643556122264217e-05,
        "epoch": 0.1720694537595946,
        "step": 2309
    },
    {
        "loss": 2.701,
        "grad_norm": 1.8699841499328613,
        "learning_rate": 9.636524115476435e-05,
        "epoch": 0.17214397496087636,
        "step": 2310
    },
    {
        "loss": 2.5998,
        "grad_norm": 2.5053510665893555,
        "learning_rate": 9.629492288657465e-05,
        "epoch": 0.17221849616215815,
        "step": 2311
    },
    {
        "loss": 1.6286,
        "grad_norm": 3.3270137310028076,
        "learning_rate": 9.622460645288993e-05,
        "epoch": 0.1722930173634399,
        "step": 2312
    },
    {
        "loss": 2.1681,
        "grad_norm": 3.9507429599761963,
        "learning_rate": 9.615429188852617e-05,
        "epoch": 0.17236753856472167,
        "step": 2313
    },
    {
        "loss": 2.9398,
        "grad_norm": 2.2201850414276123,
        "learning_rate": 9.608397922829843e-05,
        "epoch": 0.17244205976600344,
        "step": 2314
    },
    {
        "loss": 1.7187,
        "grad_norm": 3.6345865726470947,
        "learning_rate": 9.601366850702082e-05,
        "epoch": 0.1725165809672852,
        "step": 2315
    },
    {
        "loss": 2.6555,
        "grad_norm": 2.2896552085876465,
        "learning_rate": 9.594335975950645e-05,
        "epoch": 0.17259110216856696,
        "step": 2316
    },
    {
        "loss": 2.5995,
        "grad_norm": 3.18045711517334,
        "learning_rate": 9.587305302056749e-05,
        "epoch": 0.17266562336984873,
        "step": 2317
    },
    {
        "loss": 2.7267,
        "grad_norm": 2.524420976638794,
        "learning_rate": 9.580274832501507e-05,
        "epoch": 0.1727401445711305,
        "step": 2318
    },
    {
        "loss": 2.4205,
        "grad_norm": 2.193352222442627,
        "learning_rate": 9.573244570765944e-05,
        "epoch": 0.17281466577241225,
        "step": 2319
    },
    {
        "loss": 2.5462,
        "grad_norm": 2.3813886642456055,
        "learning_rate": 9.566214520330966e-05,
        "epoch": 0.17288918697369401,
        "step": 2320
    },
    {
        "loss": 2.6506,
        "grad_norm": 2.717880964279175,
        "learning_rate": 9.559184684677383e-05,
        "epoch": 0.17296370817497578,
        "step": 2321
    },
    {
        "loss": 2.8644,
        "grad_norm": 2.3389337062835693,
        "learning_rate": 9.552155067285898e-05,
        "epoch": 0.17303822937625754,
        "step": 2322
    },
    {
        "loss": 2.6072,
        "grad_norm": 3.2114672660827637,
        "learning_rate": 9.545125671637101e-05,
        "epoch": 0.1731127505775393,
        "step": 2323
    },
    {
        "loss": 3.0426,
        "grad_norm": 2.7985482215881348,
        "learning_rate": 9.538096501211477e-05,
        "epoch": 0.17318727177882107,
        "step": 2324
    },
    {
        "loss": 1.9879,
        "grad_norm": 2.6386611461639404,
        "learning_rate": 9.531067559489403e-05,
        "epoch": 0.17326179298010283,
        "step": 2325
    },
    {
        "loss": 2.5793,
        "grad_norm": 2.7026002407073975,
        "learning_rate": 9.524038849951133e-05,
        "epoch": 0.1733363141813846,
        "step": 2326
    },
    {
        "loss": 2.0132,
        "grad_norm": 2.8072385787963867,
        "learning_rate": 9.517010376076814e-05,
        "epoch": 0.17341083538266636,
        "step": 2327
    },
    {
        "loss": 2.0858,
        "grad_norm": 3.29129695892334,
        "learning_rate": 9.509982141346473e-05,
        "epoch": 0.17348535658394815,
        "step": 2328
    },
    {
        "loss": 1.9369,
        "grad_norm": 3.8961474895477295,
        "learning_rate": 9.502954149240014e-05,
        "epoch": 0.1735598777852299,
        "step": 2329
    },
    {
        "loss": 2.2212,
        "grad_norm": 3.3891420364379883,
        "learning_rate": 9.495926403237236e-05,
        "epoch": 0.17363439898651167,
        "step": 2330
    },
    {
        "loss": 2.7069,
        "grad_norm": 3.264024019241333,
        "learning_rate": 9.488898906817803e-05,
        "epoch": 0.17370892018779344,
        "step": 2331
    },
    {
        "loss": 2.2341,
        "grad_norm": 2.8494389057159424,
        "learning_rate": 9.481871663461253e-05,
        "epoch": 0.1737834413890752,
        "step": 2332
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.2833850383758545,
        "learning_rate": 9.474844676647007e-05,
        "epoch": 0.17385796259035696,
        "step": 2333
    },
    {
        "loss": 2.9959,
        "grad_norm": 4.30295467376709,
        "learning_rate": 9.467817949854358e-05,
        "epoch": 0.17393248379163873,
        "step": 2334
    },
    {
        "loss": 2.1477,
        "grad_norm": 2.6017005443573,
        "learning_rate": 9.460791486562473e-05,
        "epoch": 0.1740070049929205,
        "step": 2335
    },
    {
        "loss": 2.1472,
        "grad_norm": 3.050691843032837,
        "learning_rate": 9.453765290250369e-05,
        "epoch": 0.17408152619420225,
        "step": 2336
    },
    {
        "loss": 2.0647,
        "grad_norm": 1.9052667617797852,
        "learning_rate": 9.446739364396962e-05,
        "epoch": 0.17415604739548401,
        "step": 2337
    },
    {
        "loss": 2.872,
        "grad_norm": 2.086858034133911,
        "learning_rate": 9.439713712481012e-05,
        "epoch": 0.17423056859676578,
        "step": 2338
    },
    {
        "loss": 2.5692,
        "grad_norm": 3.2904369831085205,
        "learning_rate": 9.432688337981144e-05,
        "epoch": 0.17430508979804754,
        "step": 2339
    },
    {
        "loss": 2.2504,
        "grad_norm": 2.7116405963897705,
        "learning_rate": 9.42566324437586e-05,
        "epoch": 0.1743796109993293,
        "step": 2340
    },
    {
        "loss": 2.4435,
        "grad_norm": 3.6096370220184326,
        "learning_rate": 9.418638435143509e-05,
        "epoch": 0.17445413220061107,
        "step": 2341
    },
    {
        "loss": 2.1808,
        "grad_norm": 3.189586877822876,
        "learning_rate": 9.411613913762306e-05,
        "epoch": 0.17452865340189283,
        "step": 2342
    },
    {
        "loss": 2.2286,
        "grad_norm": 2.964738130569458,
        "learning_rate": 9.404589683710316e-05,
        "epoch": 0.1746031746031746,
        "step": 2343
    },
    {
        "loss": 2.3086,
        "grad_norm": 3.272608518600464,
        "learning_rate": 9.397565748465467e-05,
        "epoch": 0.17467769580445636,
        "step": 2344
    },
    {
        "loss": 1.4519,
        "grad_norm": 2.3980469703674316,
        "learning_rate": 9.390542111505554e-05,
        "epoch": 0.17475221700573812,
        "step": 2345
    },
    {
        "loss": 2.0138,
        "grad_norm": 2.6795895099639893,
        "learning_rate": 9.383518776308186e-05,
        "epoch": 0.1748267382070199,
        "step": 2346
    },
    {
        "loss": 2.2283,
        "grad_norm": 2.4442391395568848,
        "learning_rate": 9.376495746350866e-05,
        "epoch": 0.17490125940830167,
        "step": 2347
    },
    {
        "loss": 2.5864,
        "grad_norm": 2.2323338985443115,
        "learning_rate": 9.36947302511091e-05,
        "epoch": 0.17497578060958344,
        "step": 2348
    },
    {
        "loss": 2.6002,
        "grad_norm": 4.390686511993408,
        "learning_rate": 9.362450616065511e-05,
        "epoch": 0.1750503018108652,
        "step": 2349
    },
    {
        "loss": 2.5808,
        "grad_norm": 2.391775608062744,
        "learning_rate": 9.355428522691685e-05,
        "epoch": 0.17512482301214696,
        "step": 2350
    },
    {
        "loss": 2.6004,
        "grad_norm": 2.9761741161346436,
        "learning_rate": 9.3484067484663e-05,
        "epoch": 0.17519934421342873,
        "step": 2351
    },
    {
        "loss": 2.7544,
        "grad_norm": 1.911120057106018,
        "learning_rate": 9.341385296866077e-05,
        "epoch": 0.1752738654147105,
        "step": 2352
    },
    {
        "loss": 2.2286,
        "grad_norm": 4.0255537033081055,
        "learning_rate": 9.334364171367554e-05,
        "epoch": 0.17534838661599225,
        "step": 2353
    },
    {
        "loss": 2.6436,
        "grad_norm": 2.0539493560791016,
        "learning_rate": 9.327343375447125e-05,
        "epoch": 0.17542290781727402,
        "step": 2354
    },
    {
        "loss": 2.1394,
        "grad_norm": 3.057812452316284,
        "learning_rate": 9.320322912581018e-05,
        "epoch": 0.17549742901855578,
        "step": 2355
    },
    {
        "loss": 1.957,
        "grad_norm": 3.858302116394043,
        "learning_rate": 9.313302786245294e-05,
        "epoch": 0.17557195021983754,
        "step": 2356
    },
    {
        "loss": 2.4688,
        "grad_norm": 2.9373891353607178,
        "learning_rate": 9.306282999915844e-05,
        "epoch": 0.1756464714211193,
        "step": 2357
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.9670131206512451,
        "learning_rate": 9.299263557068391e-05,
        "epoch": 0.17572099262240107,
        "step": 2358
    },
    {
        "loss": 2.6058,
        "grad_norm": 2.6622838973999023,
        "learning_rate": 9.292244461178501e-05,
        "epoch": 0.17579551382368283,
        "step": 2359
    },
    {
        "loss": 2.1376,
        "grad_norm": 4.308390140533447,
        "learning_rate": 9.285225715721552e-05,
        "epoch": 0.1758700350249646,
        "step": 2360
    },
    {
        "loss": 2.1714,
        "grad_norm": 3.5680665969848633,
        "learning_rate": 9.278207324172754e-05,
        "epoch": 0.17594455622624636,
        "step": 2361
    },
    {
        "loss": 1.7275,
        "grad_norm": 2.8320446014404297,
        "learning_rate": 9.271189290007145e-05,
        "epoch": 0.17601907742752812,
        "step": 2362
    },
    {
        "loss": 2.6301,
        "grad_norm": 2.643665075302124,
        "learning_rate": 9.264171616699583e-05,
        "epoch": 0.17609359862880988,
        "step": 2363
    },
    {
        "loss": 2.6498,
        "grad_norm": 2.8479366302490234,
        "learning_rate": 9.257154307724743e-05,
        "epoch": 0.17616811983009167,
        "step": 2364
    },
    {
        "loss": 2.6856,
        "grad_norm": 1.9972999095916748,
        "learning_rate": 9.250137366557135e-05,
        "epoch": 0.17624264103137344,
        "step": 2365
    },
    {
        "loss": 2.4993,
        "grad_norm": 3.030330181121826,
        "learning_rate": 9.243120796671068e-05,
        "epoch": 0.1763171622326552,
        "step": 2366
    },
    {
        "loss": 2.2612,
        "grad_norm": 1.7055312395095825,
        "learning_rate": 9.236104601540675e-05,
        "epoch": 0.17639168343393696,
        "step": 2367
    },
    {
        "loss": 2.4653,
        "grad_norm": 2.2429795265197754,
        "learning_rate": 9.229088784639911e-05,
        "epoch": 0.17646620463521873,
        "step": 2368
    },
    {
        "loss": 1.9117,
        "grad_norm": 2.7402737140655518,
        "learning_rate": 9.222073349442525e-05,
        "epoch": 0.1765407258365005,
        "step": 2369
    },
    {
        "loss": 2.4528,
        "grad_norm": 2.3819580078125,
        "learning_rate": 9.215058299422103e-05,
        "epoch": 0.17661524703778225,
        "step": 2370
    },
    {
        "loss": 2.6402,
        "grad_norm": 3.2290821075439453,
        "learning_rate": 9.208043638052018e-05,
        "epoch": 0.17668976823906402,
        "step": 2371
    },
    {
        "loss": 2.7367,
        "grad_norm": 2.0686557292938232,
        "learning_rate": 9.20102936880546e-05,
        "epoch": 0.17676428944034578,
        "step": 2372
    },
    {
        "loss": 2.655,
        "grad_norm": 3.242388963699341,
        "learning_rate": 9.194015495155421e-05,
        "epoch": 0.17683881064162754,
        "step": 2373
    },
    {
        "loss": 2.0177,
        "grad_norm": 3.60325288772583,
        "learning_rate": 9.187002020574697e-05,
        "epoch": 0.1769133318429093,
        "step": 2374
    },
    {
        "loss": 2.421,
        "grad_norm": 1.6264281272888184,
        "learning_rate": 9.179988948535899e-05,
        "epoch": 0.17698785304419107,
        "step": 2375
    },
    {
        "loss": 2.2439,
        "grad_norm": 2.8399109840393066,
        "learning_rate": 9.172976282511421e-05,
        "epoch": 0.17706237424547283,
        "step": 2376
    },
    {
        "loss": 2.7727,
        "grad_norm": 1.9143648147583008,
        "learning_rate": 9.165964025973466e-05,
        "epoch": 0.1771368954467546,
        "step": 2377
    },
    {
        "loss": 2.839,
        "grad_norm": 1.8397274017333984,
        "learning_rate": 9.158952182394033e-05,
        "epoch": 0.17721141664803636,
        "step": 2378
    },
    {
        "loss": 2.5219,
        "grad_norm": 2.8997392654418945,
        "learning_rate": 9.151940755244908e-05,
        "epoch": 0.17728593784931812,
        "step": 2379
    },
    {
        "loss": 2.9921,
        "grad_norm": 2.6778645515441895,
        "learning_rate": 9.144929747997687e-05,
        "epoch": 0.17736045905059988,
        "step": 2380
    },
    {
        "loss": 2.3339,
        "grad_norm": 3.0115602016448975,
        "learning_rate": 9.137919164123747e-05,
        "epoch": 0.17743498025188165,
        "step": 2381
    },
    {
        "loss": 2.6745,
        "grad_norm": 2.430373430252075,
        "learning_rate": 9.130909007094256e-05,
        "epoch": 0.17750950145316344,
        "step": 2382
    },
    {
        "loss": 2.454,
        "grad_norm": 3.2573838233947754,
        "learning_rate": 9.123899280380173e-05,
        "epoch": 0.1775840226544452,
        "step": 2383
    },
    {
        "loss": 2.3483,
        "grad_norm": 3.3238885402679443,
        "learning_rate": 9.116889987452238e-05,
        "epoch": 0.17765854385572696,
        "step": 2384
    },
    {
        "loss": 2.3038,
        "grad_norm": 3.02666974067688,
        "learning_rate": 9.109881131780993e-05,
        "epoch": 0.17773306505700873,
        "step": 2385
    },
    {
        "loss": 2.5942,
        "grad_norm": 2.8169846534729004,
        "learning_rate": 9.102872716836744e-05,
        "epoch": 0.1778075862582905,
        "step": 2386
    },
    {
        "loss": 2.7324,
        "grad_norm": 2.7097630500793457,
        "learning_rate": 9.09586474608959e-05,
        "epoch": 0.17788210745957225,
        "step": 2387
    },
    {
        "loss": 2.6449,
        "grad_norm": 2.074007034301758,
        "learning_rate": 9.0888572230094e-05,
        "epoch": 0.17795662866085402,
        "step": 2388
    },
    {
        "loss": 2.1601,
        "grad_norm": 3.192044734954834,
        "learning_rate": 9.081850151065838e-05,
        "epoch": 0.17803114986213578,
        "step": 2389
    },
    {
        "loss": 1.5723,
        "grad_norm": 2.4125165939331055,
        "learning_rate": 9.074843533728325e-05,
        "epoch": 0.17810567106341754,
        "step": 2390
    },
    {
        "loss": 2.3531,
        "grad_norm": 1.8460723161697388,
        "learning_rate": 9.067837374466075e-05,
        "epoch": 0.1781801922646993,
        "step": 2391
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.994140863418579,
        "learning_rate": 9.060831676748062e-05,
        "epoch": 0.17825471346598107,
        "step": 2392
    },
    {
        "loss": 2.4103,
        "grad_norm": 1.7052644491195679,
        "learning_rate": 9.053826444043042e-05,
        "epoch": 0.17832923466726283,
        "step": 2393
    },
    {
        "loss": 2.5896,
        "grad_norm": 3.4716989994049072,
        "learning_rate": 9.046821679819527e-05,
        "epoch": 0.1784037558685446,
        "step": 2394
    },
    {
        "loss": 2.7922,
        "grad_norm": 2.4108495712280273,
        "learning_rate": 9.039817387545806e-05,
        "epoch": 0.17847827706982636,
        "step": 2395
    },
    {
        "loss": 2.0699,
        "grad_norm": 3.3762052059173584,
        "learning_rate": 9.032813570689942e-05,
        "epoch": 0.17855279827110812,
        "step": 2396
    },
    {
        "loss": 2.3947,
        "grad_norm": 2.3614208698272705,
        "learning_rate": 9.025810232719743e-05,
        "epoch": 0.17862731947238988,
        "step": 2397
    },
    {
        "loss": 2.9491,
        "grad_norm": 2.4169511795043945,
        "learning_rate": 9.018807377102801e-05,
        "epoch": 0.17870184067367165,
        "step": 2398
    },
    {
        "loss": 2.5503,
        "grad_norm": 1.700429081916809,
        "learning_rate": 9.011805007306453e-05,
        "epoch": 0.17877636187495344,
        "step": 2399
    },
    {
        "loss": 2.1364,
        "grad_norm": 2.612065553665161,
        "learning_rate": 9.0048031267978e-05,
        "epoch": 0.1788508830762352,
        "step": 2400
    },
    {
        "loss": 2.6951,
        "grad_norm": 2.3136723041534424,
        "learning_rate": 8.997801739043711e-05,
        "epoch": 0.17892540427751696,
        "step": 2401
    },
    {
        "loss": 1.9999,
        "grad_norm": 2.9195988178253174,
        "learning_rate": 8.990800847510792e-05,
        "epoch": 0.17899992547879873,
        "step": 2402
    },
    {
        "loss": 2.331,
        "grad_norm": 2.353938102722168,
        "learning_rate": 8.983800455665425e-05,
        "epoch": 0.1790744466800805,
        "step": 2403
    },
    {
        "loss": 2.9765,
        "grad_norm": 1.8627893924713135,
        "learning_rate": 8.97680056697372e-05,
        "epoch": 0.17914896788136225,
        "step": 2404
    },
    {
        "loss": 2.1363,
        "grad_norm": 2.8897616863250732,
        "learning_rate": 8.969801184901555e-05,
        "epoch": 0.17922348908264402,
        "step": 2405
    },
    {
        "loss": 2.3671,
        "grad_norm": 1.9002606868743896,
        "learning_rate": 8.962802312914566e-05,
        "epoch": 0.17929801028392578,
        "step": 2406
    },
    {
        "loss": 2.1422,
        "grad_norm": 3.454862356185913,
        "learning_rate": 8.955803954478105e-05,
        "epoch": 0.17937253148520754,
        "step": 2407
    },
    {
        "loss": 2.3401,
        "grad_norm": 6.2479681968688965,
        "learning_rate": 8.948806113057304e-05,
        "epoch": 0.1794470526864893,
        "step": 2408
    },
    {
        "loss": 2.4123,
        "grad_norm": 1.9016231298446655,
        "learning_rate": 8.94180879211701e-05,
        "epoch": 0.17952157388777107,
        "step": 2409
    },
    {
        "loss": 2.2675,
        "grad_norm": 2.2732486724853516,
        "learning_rate": 8.934811995121834e-05,
        "epoch": 0.17959609508905283,
        "step": 2410
    },
    {
        "loss": 2.3225,
        "grad_norm": 2.5330536365509033,
        "learning_rate": 8.92781572553612e-05,
        "epoch": 0.1796706162903346,
        "step": 2411
    },
    {
        "loss": 2.1219,
        "grad_norm": 2.7511281967163086,
        "learning_rate": 8.920819986823942e-05,
        "epoch": 0.17974513749161636,
        "step": 2412
    },
    {
        "loss": 2.2023,
        "grad_norm": 2.4342942237854004,
        "learning_rate": 8.913824782449134e-05,
        "epoch": 0.17981965869289812,
        "step": 2413
    },
    {
        "loss": 1.5785,
        "grad_norm": 2.0331904888153076,
        "learning_rate": 8.906830115875234e-05,
        "epoch": 0.17989417989417988,
        "step": 2414
    },
    {
        "loss": 2.0865,
        "grad_norm": 4.267195224761963,
        "learning_rate": 8.89983599056554e-05,
        "epoch": 0.17996870109546165,
        "step": 2415
    },
    {
        "loss": 2.9235,
        "grad_norm": 1.8700554370880127,
        "learning_rate": 8.892842409983069e-05,
        "epoch": 0.1800432222967434,
        "step": 2416
    },
    {
        "loss": 2.055,
        "grad_norm": 2.1157968044281006,
        "learning_rate": 8.885849377590577e-05,
        "epoch": 0.1801177434980252,
        "step": 2417
    },
    {
        "loss": 1.9084,
        "grad_norm": 3.557973861694336,
        "learning_rate": 8.87885689685054e-05,
        "epoch": 0.18019226469930696,
        "step": 2418
    },
    {
        "loss": 2.3607,
        "grad_norm": 2.2734034061431885,
        "learning_rate": 8.871864971225158e-05,
        "epoch": 0.18026678590058873,
        "step": 2419
    },
    {
        "loss": 1.794,
        "grad_norm": 2.989457130432129,
        "learning_rate": 8.864873604176372e-05,
        "epoch": 0.1803413071018705,
        "step": 2420
    },
    {
        "loss": 2.4458,
        "grad_norm": 2.613431692123413,
        "learning_rate": 8.857882799165834e-05,
        "epoch": 0.18041582830315225,
        "step": 2421
    },
    {
        "loss": 1.5897,
        "grad_norm": 2.8424429893493652,
        "learning_rate": 8.850892559654917e-05,
        "epoch": 0.18049034950443402,
        "step": 2422
    },
    {
        "loss": 2.4546,
        "grad_norm": 2.8210010528564453,
        "learning_rate": 8.843902889104721e-05,
        "epoch": 0.18056487070571578,
        "step": 2423
    },
    {
        "loss": 2.0577,
        "grad_norm": 3.572190046310425,
        "learning_rate": 8.836913790976062e-05,
        "epoch": 0.18063939190699754,
        "step": 2424
    },
    {
        "loss": 2.5154,
        "grad_norm": 2.829624652862549,
        "learning_rate": 8.829925268729462e-05,
        "epoch": 0.1807139131082793,
        "step": 2425
    },
    {
        "loss": 2.7607,
        "grad_norm": 2.4316904544830322,
        "learning_rate": 8.822937325825179e-05,
        "epoch": 0.18078843430956107,
        "step": 2426
    },
    {
        "loss": 2.5482,
        "grad_norm": 1.9899568557739258,
        "learning_rate": 8.815949965723167e-05,
        "epoch": 0.18086295551084283,
        "step": 2427
    },
    {
        "loss": 2.4311,
        "grad_norm": 2.779888153076172,
        "learning_rate": 8.808963191883099e-05,
        "epoch": 0.1809374767121246,
        "step": 2428
    },
    {
        "loss": 2.4533,
        "grad_norm": 2.416378974914551,
        "learning_rate": 8.801977007764351e-05,
        "epoch": 0.18101199791340636,
        "step": 2429
    },
    {
        "loss": 2.6844,
        "grad_norm": 3.030712366104126,
        "learning_rate": 8.794991416826013e-05,
        "epoch": 0.18108651911468812,
        "step": 2430
    },
    {
        "loss": 3.0398,
        "grad_norm": 1.9321171045303345,
        "learning_rate": 8.788006422526881e-05,
        "epoch": 0.18116104031596988,
        "step": 2431
    },
    {
        "loss": 2.4629,
        "grad_norm": 2.054910898208618,
        "learning_rate": 8.781022028325457e-05,
        "epoch": 0.18123556151725165,
        "step": 2432
    },
    {
        "loss": 1.4136,
        "grad_norm": 3.676081895828247,
        "learning_rate": 8.774038237679941e-05,
        "epoch": 0.1813100827185334,
        "step": 2433
    },
    {
        "loss": 2.3486,
        "grad_norm": 3.145169258117676,
        "learning_rate": 8.767055054048235e-05,
        "epoch": 0.18138460391981517,
        "step": 2434
    },
    {
        "loss": 2.1445,
        "grad_norm": 4.669334411621094,
        "learning_rate": 8.760072480887941e-05,
        "epoch": 0.18145912512109696,
        "step": 2435
    },
    {
        "loss": 2.8598,
        "grad_norm": 3.334656000137329,
        "learning_rate": 8.753090521656364e-05,
        "epoch": 0.18153364632237873,
        "step": 2436
    },
    {
        "loss": 2.2693,
        "grad_norm": 3.6579174995422363,
        "learning_rate": 8.746109179810496e-05,
        "epoch": 0.1816081675236605,
        "step": 2437
    },
    {
        "loss": 2.1419,
        "grad_norm": 3.2288918495178223,
        "learning_rate": 8.739128458807033e-05,
        "epoch": 0.18168268872494225,
        "step": 2438
    },
    {
        "loss": 2.3476,
        "grad_norm": 2.371041774749756,
        "learning_rate": 8.732148362102356e-05,
        "epoch": 0.18175720992622402,
        "step": 2439
    },
    {
        "loss": 3.135,
        "grad_norm": 2.0165622234344482,
        "learning_rate": 8.725168893152533e-05,
        "epoch": 0.18183173112750578,
        "step": 2440
    },
    {
        "loss": 2.7812,
        "grad_norm": 1.8808242082595825,
        "learning_rate": 8.71819005541334e-05,
        "epoch": 0.18190625232878754,
        "step": 2441
    },
    {
        "loss": 2.146,
        "grad_norm": 3.726966381072998,
        "learning_rate": 8.711211852340222e-05,
        "epoch": 0.1819807735300693,
        "step": 2442
    },
    {
        "loss": 2.3291,
        "grad_norm": 4.190196990966797,
        "learning_rate": 8.704234287388315e-05,
        "epoch": 0.18205529473135107,
        "step": 2443
    },
    {
        "loss": 2.5734,
        "grad_norm": 1.9223495721817017,
        "learning_rate": 8.697257364012438e-05,
        "epoch": 0.18212981593263283,
        "step": 2444
    },
    {
        "loss": 2.5412,
        "grad_norm": 3.226144313812256,
        "learning_rate": 8.690281085667095e-05,
        "epoch": 0.1822043371339146,
        "step": 2445
    },
    {
        "loss": 2.9188,
        "grad_norm": 2.1256253719329834,
        "learning_rate": 8.683305455806474e-05,
        "epoch": 0.18227885833519636,
        "step": 2446
    },
    {
        "loss": 3.2033,
        "grad_norm": 2.7169225215911865,
        "learning_rate": 8.676330477884438e-05,
        "epoch": 0.18235337953647812,
        "step": 2447
    },
    {
        "loss": 2.2637,
        "grad_norm": 3.6006875038146973,
        "learning_rate": 8.669356155354523e-05,
        "epoch": 0.18242790073775988,
        "step": 2448
    },
    {
        "loss": 2.1745,
        "grad_norm": 3.542032241821289,
        "learning_rate": 8.662382491669946e-05,
        "epoch": 0.18250242193904165,
        "step": 2449
    },
    {
        "loss": 2.7882,
        "grad_norm": 2.784754514694214,
        "learning_rate": 8.655409490283594e-05,
        "epoch": 0.1825769431403234,
        "step": 2450
    },
    {
        "loss": 3.0068,
        "grad_norm": 2.588104724884033,
        "learning_rate": 8.64843715464803e-05,
        "epoch": 0.18265146434160517,
        "step": 2451
    },
    {
        "loss": 1.9785,
        "grad_norm": 3.328021764755249,
        "learning_rate": 8.641465488215486e-05,
        "epoch": 0.18272598554288697,
        "step": 2452
    },
    {
        "loss": 2.1619,
        "grad_norm": 3.2521345615386963,
        "learning_rate": 8.634494494437863e-05,
        "epoch": 0.18280050674416873,
        "step": 2453
    },
    {
        "loss": 3.2396,
        "grad_norm": 3.736178398132324,
        "learning_rate": 8.62752417676673e-05,
        "epoch": 0.1828750279454505,
        "step": 2454
    },
    {
        "loss": 2.0236,
        "grad_norm": 2.6081957817077637,
        "learning_rate": 8.620554538653315e-05,
        "epoch": 0.18294954914673225,
        "step": 2455
    },
    {
        "loss": 2.4234,
        "grad_norm": 3.9381837844848633,
        "learning_rate": 8.613585583548515e-05,
        "epoch": 0.18302407034801402,
        "step": 2456
    },
    {
        "loss": 2.6843,
        "grad_norm": 2.1798794269561768,
        "learning_rate": 8.606617314902893e-05,
        "epoch": 0.18309859154929578,
        "step": 2457
    },
    {
        "loss": 1.5342,
        "grad_norm": 2.0095183849334717,
        "learning_rate": 8.599649736166656e-05,
        "epoch": 0.18317311275057754,
        "step": 2458
    },
    {
        "loss": 2.0762,
        "grad_norm": 3.060701847076416,
        "learning_rate": 8.592682850789692e-05,
        "epoch": 0.1832476339518593,
        "step": 2459
    },
    {
        "loss": 2.4954,
        "grad_norm": 2.347679376602173,
        "learning_rate": 8.585716662221531e-05,
        "epoch": 0.18332215515314107,
        "step": 2460
    },
    {
        "loss": 2.2708,
        "grad_norm": 1.8219817876815796,
        "learning_rate": 8.578751173911354e-05,
        "epoch": 0.18339667635442283,
        "step": 2461
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.6772801876068115,
        "learning_rate": 8.571786389308017e-05,
        "epoch": 0.1834711975557046,
        "step": 2462
    },
    {
        "loss": 1.8148,
        "grad_norm": 3.887687921524048,
        "learning_rate": 8.564822311859993e-05,
        "epoch": 0.18354571875698636,
        "step": 2463
    },
    {
        "loss": 1.8396,
        "grad_norm": 3.3547589778900146,
        "learning_rate": 8.557858945015444e-05,
        "epoch": 0.18362023995826812,
        "step": 2464
    },
    {
        "loss": 1.893,
        "grad_norm": 2.5724546909332275,
        "learning_rate": 8.550896292222146e-05,
        "epoch": 0.18369476115954989,
        "step": 2465
    },
    {
        "loss": 2.9701,
        "grad_norm": 1.9436635971069336,
        "learning_rate": 8.543934356927543e-05,
        "epoch": 0.18376928236083165,
        "step": 2466
    },
    {
        "loss": 2.3942,
        "grad_norm": 2.8349802494049072,
        "learning_rate": 8.536973142578724e-05,
        "epoch": 0.1838438035621134,
        "step": 2467
    },
    {
        "loss": 2.0505,
        "grad_norm": 4.675099849700928,
        "learning_rate": 8.530012652622397e-05,
        "epoch": 0.18391832476339517,
        "step": 2468
    },
    {
        "loss": 2.5747,
        "grad_norm": 2.1023638248443604,
        "learning_rate": 8.523052890504947e-05,
        "epoch": 0.18399284596467694,
        "step": 2469
    },
    {
        "loss": 2.4401,
        "grad_norm": 2.9588401317596436,
        "learning_rate": 8.516093859672369e-05,
        "epoch": 0.18406736716595873,
        "step": 2470
    },
    {
        "loss": 2.5782,
        "grad_norm": 2.104083299636841,
        "learning_rate": 8.509135563570307e-05,
        "epoch": 0.1841418883672405,
        "step": 2471
    },
    {
        "loss": 2.0798,
        "grad_norm": 1.8635562658309937,
        "learning_rate": 8.502178005644049e-05,
        "epoch": 0.18421640956852225,
        "step": 2472
    },
    {
        "loss": 2.2866,
        "grad_norm": 1.4293371438980103,
        "learning_rate": 8.495221189338498e-05,
        "epoch": 0.18429093076980402,
        "step": 2473
    },
    {
        "loss": 1.9005,
        "grad_norm": 2.9726688861846924,
        "learning_rate": 8.488265118098217e-05,
        "epoch": 0.18436545197108578,
        "step": 2474
    },
    {
        "loss": 2.5938,
        "grad_norm": 3.2771079540252686,
        "learning_rate": 8.481309795367369e-05,
        "epoch": 0.18443997317236754,
        "step": 2475
    },
    {
        "loss": 2.6463,
        "grad_norm": 2.099456310272217,
        "learning_rate": 8.474355224589772e-05,
        "epoch": 0.1845144943736493,
        "step": 2476
    },
    {
        "loss": 1.9659,
        "grad_norm": 3.0785300731658936,
        "learning_rate": 8.467401409208858e-05,
        "epoch": 0.18458901557493107,
        "step": 2477
    },
    {
        "loss": 2.1794,
        "grad_norm": 3.287686586380005,
        "learning_rate": 8.460448352667686e-05,
        "epoch": 0.18466353677621283,
        "step": 2478
    },
    {
        "loss": 2.306,
        "grad_norm": 2.4962213039398193,
        "learning_rate": 8.453496058408951e-05,
        "epoch": 0.1847380579774946,
        "step": 2479
    },
    {
        "loss": 1.8195,
        "grad_norm": 2.798861503601074,
        "learning_rate": 8.44654452987495e-05,
        "epoch": 0.18481257917877636,
        "step": 2480
    },
    {
        "loss": 1.7153,
        "grad_norm": 3.1538922786712646,
        "learning_rate": 8.439593770507622e-05,
        "epoch": 0.18488710038005812,
        "step": 2481
    },
    {
        "loss": 2.5723,
        "grad_norm": 2.855213165283203,
        "learning_rate": 8.432643783748512e-05,
        "epoch": 0.18496162158133989,
        "step": 2482
    },
    {
        "loss": 2.4486,
        "grad_norm": 4.2029032707214355,
        "learning_rate": 8.425694573038786e-05,
        "epoch": 0.18503614278262165,
        "step": 2483
    },
    {
        "loss": 2.6713,
        "grad_norm": 1.824139952659607,
        "learning_rate": 8.418746141819223e-05,
        "epoch": 0.1851106639839034,
        "step": 2484
    },
    {
        "loss": 2.2367,
        "grad_norm": 1.9724295139312744,
        "learning_rate": 8.41179849353022e-05,
        "epoch": 0.18518518518518517,
        "step": 2485
    },
    {
        "loss": 2.4693,
        "grad_norm": 3.4915127754211426,
        "learning_rate": 8.40485163161179e-05,
        "epoch": 0.18525970638646694,
        "step": 2486
    },
    {
        "loss": 1.9878,
        "grad_norm": 3.7173383235931396,
        "learning_rate": 8.397905559503547e-05,
        "epoch": 0.1853342275877487,
        "step": 2487
    },
    {
        "loss": 2.9374,
        "grad_norm": 2.3180160522460938,
        "learning_rate": 8.390960280644721e-05,
        "epoch": 0.1854087487890305,
        "step": 2488
    },
    {
        "loss": 2.2773,
        "grad_norm": 2.495408773422241,
        "learning_rate": 8.384015798474147e-05,
        "epoch": 0.18548326999031226,
        "step": 2489
    },
    {
        "loss": 2.1921,
        "grad_norm": 3.590437173843384,
        "learning_rate": 8.377072116430265e-05,
        "epoch": 0.18555779119159402,
        "step": 2490
    },
    {
        "loss": 1.7613,
        "grad_norm": 2.910264015197754,
        "learning_rate": 8.370129237951114e-05,
        "epoch": 0.18563231239287578,
        "step": 2491
    },
    {
        "loss": 2.4331,
        "grad_norm": 2.5302672386169434,
        "learning_rate": 8.363187166474346e-05,
        "epoch": 0.18570683359415754,
        "step": 2492
    },
    {
        "loss": 1.59,
        "grad_norm": 2.915900945663452,
        "learning_rate": 8.356245905437211e-05,
        "epoch": 0.1857813547954393,
        "step": 2493
    },
    {
        "loss": 2.2864,
        "grad_norm": 2.009167432785034,
        "learning_rate": 8.349305458276551e-05,
        "epoch": 0.18585587599672107,
        "step": 2494
    },
    {
        "loss": 2.5951,
        "grad_norm": 1.1896358728408813,
        "learning_rate": 8.342365828428806e-05,
        "epoch": 0.18593039719800283,
        "step": 2495
    },
    {
        "loss": 1.7665,
        "grad_norm": 3.142730474472046,
        "learning_rate": 8.335427019330015e-05,
        "epoch": 0.1860049183992846,
        "step": 2496
    },
    {
        "loss": 2.0663,
        "grad_norm": 2.455347776412964,
        "learning_rate": 8.328489034415814e-05,
        "epoch": 0.18607943960056636,
        "step": 2497
    },
    {
        "loss": 2.4999,
        "grad_norm": 2.212550163269043,
        "learning_rate": 8.321551877121424e-05,
        "epoch": 0.18615396080184812,
        "step": 2498
    },
    {
        "loss": 1.8326,
        "grad_norm": 6.937155723571777,
        "learning_rate": 8.314615550881656e-05,
        "epoch": 0.18622848200312989,
        "step": 2499
    },
    {
        "loss": 2.3973,
        "grad_norm": 3.219921827316284,
        "learning_rate": 8.307680059130918e-05,
        "epoch": 0.18630300320441165,
        "step": 2500
    },
    {
        "loss": 1.8303,
        "grad_norm": 1.2462257146835327,
        "learning_rate": 8.30074540530319e-05,
        "epoch": 0.1863775244056934,
        "step": 2501
    },
    {
        "loss": 2.5351,
        "grad_norm": 2.909489631652832,
        "learning_rate": 8.293811592832056e-05,
        "epoch": 0.18645204560697518,
        "step": 2502
    },
    {
        "loss": 2.8565,
        "grad_norm": 3.0453996658325195,
        "learning_rate": 8.28687862515067e-05,
        "epoch": 0.18652656680825694,
        "step": 2503
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.4794633388519287,
        "learning_rate": 8.279946505691769e-05,
        "epoch": 0.1866010880095387,
        "step": 2504
    },
    {
        "loss": 2.5079,
        "grad_norm": 3.148993968963623,
        "learning_rate": 8.273015237887673e-05,
        "epoch": 0.1866756092108205,
        "step": 2505
    },
    {
        "loss": 2.1816,
        "grad_norm": 3.647827625274658,
        "learning_rate": 8.266084825170282e-05,
        "epoch": 0.18675013041210226,
        "step": 2506
    },
    {
        "loss": 1.2505,
        "grad_norm": 3.7621867656707764,
        "learning_rate": 8.259155270971068e-05,
        "epoch": 0.18682465161338402,
        "step": 2507
    },
    {
        "loss": 1.9795,
        "grad_norm": 3.994025230407715,
        "learning_rate": 8.252226578721082e-05,
        "epoch": 0.18689917281466578,
        "step": 2508
    },
    {
        "loss": 2.9099,
        "grad_norm": 3.2542190551757812,
        "learning_rate": 8.245298751850946e-05,
        "epoch": 0.18697369401594754,
        "step": 2509
    },
    {
        "loss": 2.1753,
        "grad_norm": 3.3390984535217285,
        "learning_rate": 8.238371793790855e-05,
        "epoch": 0.1870482152172293,
        "step": 2510
    },
    {
        "loss": 2.578,
        "grad_norm": 1.9860897064208984,
        "learning_rate": 8.231445707970568e-05,
        "epoch": 0.18712273641851107,
        "step": 2511
    },
    {
        "loss": 2.3528,
        "grad_norm": 4.543534755706787,
        "learning_rate": 8.224520497819419e-05,
        "epoch": 0.18719725761979283,
        "step": 2512
    },
    {
        "loss": 2.0412,
        "grad_norm": 3.1280131340026855,
        "learning_rate": 8.217596166766311e-05,
        "epoch": 0.1872717788210746,
        "step": 2513
    },
    {
        "loss": 2.3383,
        "grad_norm": 3.596574306488037,
        "learning_rate": 8.2106727182397e-05,
        "epoch": 0.18734630002235636,
        "step": 2514
    },
    {
        "loss": 2.3474,
        "grad_norm": 2.2574656009674072,
        "learning_rate": 8.203750155667618e-05,
        "epoch": 0.18742082122363812,
        "step": 2515
    },
    {
        "loss": 1.9136,
        "grad_norm": 3.936093330383301,
        "learning_rate": 8.196828482477651e-05,
        "epoch": 0.1874953424249199,
        "step": 2516
    },
    {
        "loss": 2.064,
        "grad_norm": 2.133523464202881,
        "learning_rate": 8.189907702096939e-05,
        "epoch": 0.18756986362620165,
        "step": 2517
    },
    {
        "loss": 2.1335,
        "grad_norm": 3.241776943206787,
        "learning_rate": 8.182987817952201e-05,
        "epoch": 0.1876443848274834,
        "step": 2518
    },
    {
        "loss": 2.1312,
        "grad_norm": 3.0318989753723145,
        "learning_rate": 8.176068833469685e-05,
        "epoch": 0.18771890602876518,
        "step": 2519
    },
    {
        "loss": 2.6637,
        "grad_norm": 2.0665440559387207,
        "learning_rate": 8.169150752075212e-05,
        "epoch": 0.18779342723004694,
        "step": 2520
    },
    {
        "loss": 2.6185,
        "grad_norm": 2.6896066665649414,
        "learning_rate": 8.162233577194151e-05,
        "epoch": 0.1878679484313287,
        "step": 2521
    },
    {
        "loss": 1.986,
        "grad_norm": 4.602815628051758,
        "learning_rate": 8.155317312251419e-05,
        "epoch": 0.18794246963261046,
        "step": 2522
    },
    {
        "loss": 2.8857,
        "grad_norm": 2.0998587608337402,
        "learning_rate": 8.148401960671495e-05,
        "epoch": 0.18801699083389226,
        "step": 2523
    },
    {
        "loss": 2.5057,
        "grad_norm": 2.7767653465270996,
        "learning_rate": 8.14148752587838e-05,
        "epoch": 0.18809151203517402,
        "step": 2524
    },
    {
        "loss": 2.4132,
        "grad_norm": 3.4825100898742676,
        "learning_rate": 8.134574011295652e-05,
        "epoch": 0.18816603323645578,
        "step": 2525
    },
    {
        "loss": 2.4199,
        "grad_norm": 3.345712423324585,
        "learning_rate": 8.127661420346408e-05,
        "epoch": 0.18824055443773755,
        "step": 2526
    },
    {
        "loss": 2.6083,
        "grad_norm": 1.6384296417236328,
        "learning_rate": 8.120749756453302e-05,
        "epoch": 0.1883150756390193,
        "step": 2527
    },
    {
        "loss": 2.0862,
        "grad_norm": 2.6027398109436035,
        "learning_rate": 8.11383902303853e-05,
        "epoch": 0.18838959684030107,
        "step": 2528
    },
    {
        "loss": 2.9275,
        "grad_norm": 2.187549114227295,
        "learning_rate": 8.10692922352381e-05,
        "epoch": 0.18846411804158283,
        "step": 2529
    },
    {
        "loss": 3.0585,
        "grad_norm": 2.359786033630371,
        "learning_rate": 8.100020361330428e-05,
        "epoch": 0.1885386392428646,
        "step": 2530
    },
    {
        "loss": 2.7627,
        "grad_norm": 2.233032703399658,
        "learning_rate": 8.093112439879168e-05,
        "epoch": 0.18861316044414636,
        "step": 2531
    },
    {
        "loss": 2.4539,
        "grad_norm": 3.6972029209136963,
        "learning_rate": 8.086205462590379e-05,
        "epoch": 0.18868768164542812,
        "step": 2532
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.1215386390686035,
        "learning_rate": 8.079299432883934e-05,
        "epoch": 0.1887622028467099,
        "step": 2533
    },
    {
        "loss": 2.6762,
        "grad_norm": 1.8163378238677979,
        "learning_rate": 8.072394354179228e-05,
        "epoch": 0.18883672404799165,
        "step": 2534
    },
    {
        "loss": 2.3393,
        "grad_norm": 2.4047257900238037,
        "learning_rate": 8.065490229895199e-05,
        "epoch": 0.1889112452492734,
        "step": 2535
    },
    {
        "loss": 2.1739,
        "grad_norm": 4.145453453063965,
        "learning_rate": 8.058587063450293e-05,
        "epoch": 0.18898576645055518,
        "step": 2536
    },
    {
        "loss": 2.7618,
        "grad_norm": 3.1049654483795166,
        "learning_rate": 8.051684858262507e-05,
        "epoch": 0.18906028765183694,
        "step": 2537
    },
    {
        "loss": 2.1439,
        "grad_norm": 2.603566884994507,
        "learning_rate": 8.044783617749341e-05,
        "epoch": 0.1891348088531187,
        "step": 2538
    },
    {
        "loss": 1.5039,
        "grad_norm": 2.5708556175231934,
        "learning_rate": 8.037883345327823e-05,
        "epoch": 0.18920933005440047,
        "step": 2539
    },
    {
        "loss": 2.1098,
        "grad_norm": 2.3617937564849854,
        "learning_rate": 8.030984044414514e-05,
        "epoch": 0.18928385125568223,
        "step": 2540
    },
    {
        "loss": 2.4507,
        "grad_norm": 2.932706117630005,
        "learning_rate": 8.024085718425472e-05,
        "epoch": 0.18935837245696402,
        "step": 2541
    },
    {
        "loss": 2.9191,
        "grad_norm": 2.03460693359375,
        "learning_rate": 8.017188370776292e-05,
        "epoch": 0.18943289365824578,
        "step": 2542
    },
    {
        "loss": 2.8116,
        "grad_norm": 2.3874921798706055,
        "learning_rate": 8.010292004882077e-05,
        "epoch": 0.18950741485952755,
        "step": 2543
    },
    {
        "loss": 2.1411,
        "grad_norm": 4.2924041748046875,
        "learning_rate": 8.003396624157438e-05,
        "epoch": 0.1895819360608093,
        "step": 2544
    },
    {
        "loss": 2.8063,
        "grad_norm": 2.0395805835723877,
        "learning_rate": 7.996502232016508e-05,
        "epoch": 0.18965645726209107,
        "step": 2545
    },
    {
        "loss": 2.0254,
        "grad_norm": 3.2361249923706055,
        "learning_rate": 7.989608831872921e-05,
        "epoch": 0.18973097846337283,
        "step": 2546
    },
    {
        "loss": 2.8919,
        "grad_norm": 3.7289297580718994,
        "learning_rate": 7.982716427139833e-05,
        "epoch": 0.1898054996646546,
        "step": 2547
    },
    {
        "loss": 2.7428,
        "grad_norm": 2.361229419708252,
        "learning_rate": 7.975825021229892e-05,
        "epoch": 0.18988002086593636,
        "step": 2548
    },
    {
        "loss": 1.8084,
        "grad_norm": 3.695998430252075,
        "learning_rate": 7.968934617555266e-05,
        "epoch": 0.18995454206721812,
        "step": 2549
    },
    {
        "loss": 2.3604,
        "grad_norm": 2.7144157886505127,
        "learning_rate": 7.962045219527615e-05,
        "epoch": 0.1900290632684999,
        "step": 2550
    },
    {
        "loss": 2.8444,
        "grad_norm": 3.687974452972412,
        "learning_rate": 7.955156830558106e-05,
        "epoch": 0.19010358446978165,
        "step": 2551
    },
    {
        "loss": 1.4711,
        "grad_norm": 3.4919824600219727,
        "learning_rate": 7.948269454057402e-05,
        "epoch": 0.1901781056710634,
        "step": 2552
    },
    {
        "loss": 2.4985,
        "grad_norm": 2.7578284740448,
        "learning_rate": 7.941383093435677e-05,
        "epoch": 0.19025262687234518,
        "step": 2553
    },
    {
        "loss": 2.5493,
        "grad_norm": 2.4050822257995605,
        "learning_rate": 7.934497752102587e-05,
        "epoch": 0.19032714807362694,
        "step": 2554
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.4409642219543457,
        "learning_rate": 7.927613433467293e-05,
        "epoch": 0.1904016692749087,
        "step": 2555
    },
    {
        "loss": 2.5721,
        "grad_norm": 1.970940351486206,
        "learning_rate": 7.920730140938447e-05,
        "epoch": 0.19047619047619047,
        "step": 2556
    },
    {
        "loss": 2.319,
        "grad_norm": 2.9870898723602295,
        "learning_rate": 7.913847877924187e-05,
        "epoch": 0.19055071167747223,
        "step": 2557
    },
    {
        "loss": 1.4252,
        "grad_norm": 2.7393691539764404,
        "learning_rate": 7.906966647832154e-05,
        "epoch": 0.190625232878754,
        "step": 2558
    },
    {
        "loss": 2.3847,
        "grad_norm": 2.2356812953948975,
        "learning_rate": 7.90008645406947e-05,
        "epoch": 0.19069975408003578,
        "step": 2559
    },
    {
        "loss": 1.648,
        "grad_norm": 3.8791000843048096,
        "learning_rate": 7.89320730004274e-05,
        "epoch": 0.19077427528131755,
        "step": 2560
    },
    {
        "loss": 2.761,
        "grad_norm": 2.188180923461914,
        "learning_rate": 7.886329189158057e-05,
        "epoch": 0.1908487964825993,
        "step": 2561
    },
    {
        "loss": 1.6652,
        "grad_norm": 3.774980306625366,
        "learning_rate": 7.879452124821e-05,
        "epoch": 0.19092331768388107,
        "step": 2562
    },
    {
        "loss": 2.2648,
        "grad_norm": 4.1194963455200195,
        "learning_rate": 7.872576110436637e-05,
        "epoch": 0.19099783888516284,
        "step": 2563
    },
    {
        "loss": 2.1955,
        "grad_norm": 2.8422434329986572,
        "learning_rate": 7.8657011494095e-05,
        "epoch": 0.1910723600864446,
        "step": 2564
    },
    {
        "loss": 2.1758,
        "grad_norm": 1.4582421779632568,
        "learning_rate": 7.85882724514361e-05,
        "epoch": 0.19114688128772636,
        "step": 2565
    },
    {
        "loss": 2.3687,
        "grad_norm": 1.932706594467163,
        "learning_rate": 7.85195440104246e-05,
        "epoch": 0.19122140248900812,
        "step": 2566
    },
    {
        "loss": 1.6703,
        "grad_norm": 2.673431396484375,
        "learning_rate": 7.845082620509017e-05,
        "epoch": 0.1912959236902899,
        "step": 2567
    },
    {
        "loss": 2.1756,
        "grad_norm": 3.113903045654297,
        "learning_rate": 7.838211906945732e-05,
        "epoch": 0.19137044489157165,
        "step": 2568
    },
    {
        "loss": 2.5897,
        "grad_norm": 2.707227945327759,
        "learning_rate": 7.831342263754515e-05,
        "epoch": 0.1914449660928534,
        "step": 2569
    },
    {
        "loss": 2.1524,
        "grad_norm": 3.818690776824951,
        "learning_rate": 7.824473694336755e-05,
        "epoch": 0.19151948729413518,
        "step": 2570
    },
    {
        "loss": 2.439,
        "grad_norm": 2.1242146492004395,
        "learning_rate": 7.817606202093306e-05,
        "epoch": 0.19159400849541694,
        "step": 2571
    },
    {
        "loss": 2.4063,
        "grad_norm": 2.3794281482696533,
        "learning_rate": 7.810739790424481e-05,
        "epoch": 0.1916685296966987,
        "step": 2572
    },
    {
        "loss": 2.1545,
        "grad_norm": 3.2273852825164795,
        "learning_rate": 7.803874462730072e-05,
        "epoch": 0.19174305089798047,
        "step": 2573
    },
    {
        "loss": 2.5792,
        "grad_norm": 3.468585252761841,
        "learning_rate": 7.797010222409329e-05,
        "epoch": 0.19181757209926223,
        "step": 2574
    },
    {
        "loss": 2.4485,
        "grad_norm": 2.7470250129699707,
        "learning_rate": 7.790147072860956e-05,
        "epoch": 0.191892093300544,
        "step": 2575
    },
    {
        "loss": 2.124,
        "grad_norm": 2.8018062114715576,
        "learning_rate": 7.783285017483127e-05,
        "epoch": 0.19196661450182578,
        "step": 2576
    },
    {
        "loss": 2.0013,
        "grad_norm": 2.840832471847534,
        "learning_rate": 7.776424059673471e-05,
        "epoch": 0.19204113570310755,
        "step": 2577
    },
    {
        "loss": 2.1928,
        "grad_norm": 3.072619676589966,
        "learning_rate": 7.769564202829066e-05,
        "epoch": 0.1921156569043893,
        "step": 2578
    },
    {
        "loss": 2.729,
        "grad_norm": 2.3862175941467285,
        "learning_rate": 7.762705450346462e-05,
        "epoch": 0.19219017810567107,
        "step": 2579
    },
    {
        "loss": 2.5589,
        "grad_norm": 1.874408483505249,
        "learning_rate": 7.755847805621646e-05,
        "epoch": 0.19226469930695284,
        "step": 2580
    },
    {
        "loss": 2.482,
        "grad_norm": 1.7559449672698975,
        "learning_rate": 7.748991272050064e-05,
        "epoch": 0.1923392205082346,
        "step": 2581
    },
    {
        "loss": 2.6205,
        "grad_norm": 1.9627100229263306,
        "learning_rate": 7.742135853026607e-05,
        "epoch": 0.19241374170951636,
        "step": 2582
    },
    {
        "loss": 1.5183,
        "grad_norm": 2.869086503982544,
        "learning_rate": 7.73528155194562e-05,
        "epoch": 0.19248826291079812,
        "step": 2583
    },
    {
        "loss": 2.5468,
        "grad_norm": 2.1577985286712646,
        "learning_rate": 7.728428372200897e-05,
        "epoch": 0.1925627841120799,
        "step": 2584
    },
    {
        "loss": 1.8191,
        "grad_norm": 4.043944358825684,
        "learning_rate": 7.721576317185659e-05,
        "epoch": 0.19263730531336165,
        "step": 2585
    },
    {
        "loss": 1.9348,
        "grad_norm": 3.63066029548645,
        "learning_rate": 7.714725390292596e-05,
        "epoch": 0.19271182651464341,
        "step": 2586
    },
    {
        "loss": 1.6437,
        "grad_norm": 4.20973539352417,
        "learning_rate": 7.70787559491382e-05,
        "epoch": 0.19278634771592518,
        "step": 2587
    },
    {
        "loss": 2.2753,
        "grad_norm": 3.7669007778167725,
        "learning_rate": 7.701026934440886e-05,
        "epoch": 0.19286086891720694,
        "step": 2588
    },
    {
        "loss": 2.4115,
        "grad_norm": 4.546830654144287,
        "learning_rate": 7.694179412264799e-05,
        "epoch": 0.1929353901184887,
        "step": 2589
    },
    {
        "loss": 1.9116,
        "grad_norm": 4.227919578552246,
        "learning_rate": 7.687333031775982e-05,
        "epoch": 0.19300991131977047,
        "step": 2590
    },
    {
        "loss": 2.7146,
        "grad_norm": 2.2262227535247803,
        "learning_rate": 7.680487796364313e-05,
        "epoch": 0.19308443252105223,
        "step": 2591
    },
    {
        "loss": 2.021,
        "grad_norm": 3.3342392444610596,
        "learning_rate": 7.673643709419078e-05,
        "epoch": 0.193158953722334,
        "step": 2592
    },
    {
        "loss": 2.198,
        "grad_norm": 3.8293676376342773,
        "learning_rate": 7.666800774329015e-05,
        "epoch": 0.19323347492361576,
        "step": 2593
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.4429872035980225,
        "learning_rate": 7.659958994482295e-05,
        "epoch": 0.19330799612489755,
        "step": 2594
    },
    {
        "loss": 1.9333,
        "grad_norm": 3.318112850189209,
        "learning_rate": 7.65311837326649e-05,
        "epoch": 0.1933825173261793,
        "step": 2595
    },
    {
        "loss": 3.147,
        "grad_norm": 1.564841628074646,
        "learning_rate": 7.646278914068631e-05,
        "epoch": 0.19345703852746107,
        "step": 2596
    },
    {
        "loss": 3.165,
        "grad_norm": 2.919062852859497,
        "learning_rate": 7.639440620275145e-05,
        "epoch": 0.19353155972874284,
        "step": 2597
    },
    {
        "loss": 2.4763,
        "grad_norm": 2.1199800968170166,
        "learning_rate": 7.632603495271905e-05,
        "epoch": 0.1936060809300246,
        "step": 2598
    },
    {
        "loss": 2.3019,
        "grad_norm": 2.815425395965576,
        "learning_rate": 7.625767542444192e-05,
        "epoch": 0.19368060213130636,
        "step": 2599
    },
    {
        "loss": 2.9509,
        "grad_norm": 3.081228494644165,
        "learning_rate": 7.618932765176706e-05,
        "epoch": 0.19375512333258813,
        "step": 2600
    },
    {
        "loss": 2.4807,
        "grad_norm": 3.289799451828003,
        "learning_rate": 7.612099166853579e-05,
        "epoch": 0.1938296445338699,
        "step": 2601
    },
    {
        "loss": 2.1664,
        "grad_norm": 3.2249209880828857,
        "learning_rate": 7.60526675085834e-05,
        "epoch": 0.19390416573515165,
        "step": 2602
    },
    {
        "loss": 2.5572,
        "grad_norm": 2.6451778411865234,
        "learning_rate": 7.598435520573944e-05,
        "epoch": 0.19397868693643341,
        "step": 2603
    },
    {
        "loss": 1.4895,
        "grad_norm": 3.8787460327148438,
        "learning_rate": 7.59160547938276e-05,
        "epoch": 0.19405320813771518,
        "step": 2604
    },
    {
        "loss": 2.3684,
        "grad_norm": 1.7450647354125977,
        "learning_rate": 7.584776630666563e-05,
        "epoch": 0.19412772933899694,
        "step": 2605
    },
    {
        "loss": 2.4693,
        "grad_norm": 2.8169913291931152,
        "learning_rate": 7.577948977806542e-05,
        "epoch": 0.1942022505402787,
        "step": 2606
    },
    {
        "loss": 2.4923,
        "grad_norm": 3.0853078365325928,
        "learning_rate": 7.571122524183283e-05,
        "epoch": 0.19427677174156047,
        "step": 2607
    },
    {
        "loss": 2.344,
        "grad_norm": 2.679560661315918,
        "learning_rate": 7.564297273176796e-05,
        "epoch": 0.19435129294284223,
        "step": 2608
    },
    {
        "loss": 2.5243,
        "grad_norm": 2.89359712600708,
        "learning_rate": 7.557473228166482e-05,
        "epoch": 0.194425814144124,
        "step": 2609
    },
    {
        "loss": 2.8041,
        "grad_norm": 2.3014516830444336,
        "learning_rate": 7.550650392531152e-05,
        "epoch": 0.19450033534540576,
        "step": 2610
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.2411890029907227,
        "learning_rate": 7.543828769649014e-05,
        "epoch": 0.19457485654668752,
        "step": 2611
    },
    {
        "loss": 2.6151,
        "grad_norm": 2.2842864990234375,
        "learning_rate": 7.537008362897677e-05,
        "epoch": 0.1946493777479693,
        "step": 2612
    },
    {
        "loss": 2.7331,
        "grad_norm": 3.096437692642212,
        "learning_rate": 7.530189175654142e-05,
        "epoch": 0.19472389894925107,
        "step": 2613
    },
    {
        "loss": 2.4127,
        "grad_norm": 4.267420768737793,
        "learning_rate": 7.52337121129482e-05,
        "epoch": 0.19479842015053284,
        "step": 2614
    },
    {
        "loss": 1.7158,
        "grad_norm": 4.613407135009766,
        "learning_rate": 7.516554473195508e-05,
        "epoch": 0.1948729413518146,
        "step": 2615
    },
    {
        "loss": 1.9357,
        "grad_norm": 3.269991874694824,
        "learning_rate": 7.509738964731389e-05,
        "epoch": 0.19494746255309636,
        "step": 2616
    },
    {
        "loss": 2.6088,
        "grad_norm": 2.458836793899536,
        "learning_rate": 7.502924689277053e-05,
        "epoch": 0.19502198375437813,
        "step": 2617
    },
    {
        "loss": 2.1786,
        "grad_norm": 3.121788263320923,
        "learning_rate": 7.49611165020646e-05,
        "epoch": 0.1950965049556599,
        "step": 2618
    },
    {
        "loss": 1.9363,
        "grad_norm": 2.9238953590393066,
        "learning_rate": 7.489299850892984e-05,
        "epoch": 0.19517102615694165,
        "step": 2619
    },
    {
        "loss": 2.5933,
        "grad_norm": 2.5905299186706543,
        "learning_rate": 7.482489294709359e-05,
        "epoch": 0.19524554735822341,
        "step": 2620
    },
    {
        "loss": 2.2926,
        "grad_norm": 2.6427555084228516,
        "learning_rate": 7.475679985027716e-05,
        "epoch": 0.19532006855950518,
        "step": 2621
    },
    {
        "loss": 2.1293,
        "grad_norm": 3.6605148315429688,
        "learning_rate": 7.468871925219567e-05,
        "epoch": 0.19539458976078694,
        "step": 2622
    },
    {
        "loss": 2.2537,
        "grad_norm": 3.3173108100891113,
        "learning_rate": 7.462065118655806e-05,
        "epoch": 0.1954691109620687,
        "step": 2623
    },
    {
        "loss": 2.3323,
        "grad_norm": 2.6537623405456543,
        "learning_rate": 7.455259568706707e-05,
        "epoch": 0.19554363216335047,
        "step": 2624
    },
    {
        "loss": 2.4601,
        "grad_norm": 1.9612265825271606,
        "learning_rate": 7.448455278741922e-05,
        "epoch": 0.19561815336463223,
        "step": 2625
    },
    {
        "loss": 2.9923,
        "grad_norm": 2.200852155685425,
        "learning_rate": 7.441652252130475e-05,
        "epoch": 0.195692674565914,
        "step": 2626
    },
    {
        "loss": 2.2594,
        "grad_norm": 2.8810818195343018,
        "learning_rate": 7.434850492240768e-05,
        "epoch": 0.19576719576719576,
        "step": 2627
    },
    {
        "loss": 2.421,
        "grad_norm": 2.3882839679718018,
        "learning_rate": 7.428050002440572e-05,
        "epoch": 0.19584171696847752,
        "step": 2628
    },
    {
        "loss": 1.864,
        "grad_norm": 2.6977946758270264,
        "learning_rate": 7.421250786097039e-05,
        "epoch": 0.1959162381697593,
        "step": 2629
    },
    {
        "loss": 2.7417,
        "grad_norm": 2.271925449371338,
        "learning_rate": 7.414452846576675e-05,
        "epoch": 0.19599075937104107,
        "step": 2630
    },
    {
        "loss": 2.3563,
        "grad_norm": 2.391331911087036,
        "learning_rate": 7.407656187245372e-05,
        "epoch": 0.19606528057232284,
        "step": 2631
    },
    {
        "loss": 2.4594,
        "grad_norm": 2.937797784805298,
        "learning_rate": 7.40086081146837e-05,
        "epoch": 0.1961398017736046,
        "step": 2632
    },
    {
        "loss": 2.4345,
        "grad_norm": 3.1788406372070312,
        "learning_rate": 7.394066722610286e-05,
        "epoch": 0.19621432297488636,
        "step": 2633
    },
    {
        "loss": 2.6366,
        "grad_norm": 3.0486438274383545,
        "learning_rate": 7.387273924035098e-05,
        "epoch": 0.19628884417616813,
        "step": 2634
    },
    {
        "loss": 2.4548,
        "grad_norm": 2.8484158515930176,
        "learning_rate": 7.38048241910614e-05,
        "epoch": 0.1963633653774499,
        "step": 2635
    },
    {
        "loss": 2.6649,
        "grad_norm": 2.3961963653564453,
        "learning_rate": 7.37369221118611e-05,
        "epoch": 0.19643788657873165,
        "step": 2636
    },
    {
        "loss": 2.4183,
        "grad_norm": 3.1627538204193115,
        "learning_rate": 7.366903303637058e-05,
        "epoch": 0.19651240778001342,
        "step": 2637
    },
    {
        "loss": 2.4423,
        "grad_norm": 3.0832839012145996,
        "learning_rate": 7.360115699820402e-05,
        "epoch": 0.19658692898129518,
        "step": 2638
    },
    {
        "loss": 2.233,
        "grad_norm": 2.8352925777435303,
        "learning_rate": 7.353329403096897e-05,
        "epoch": 0.19666145018257694,
        "step": 2639
    },
    {
        "loss": 2.1016,
        "grad_norm": 2.423907995223999,
        "learning_rate": 7.34654441682667e-05,
        "epoch": 0.1967359713838587,
        "step": 2640
    },
    {
        "loss": 1.8423,
        "grad_norm": 2.621011734008789,
        "learning_rate": 7.339760744369188e-05,
        "epoch": 0.19681049258514047,
        "step": 2641
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.0343739986419678,
        "learning_rate": 7.332978389083267e-05,
        "epoch": 0.19688501378642223,
        "step": 2642
    },
    {
        "loss": 2.2503,
        "grad_norm": 5.154616832733154,
        "learning_rate": 7.326197354327071e-05,
        "epoch": 0.196959534987704,
        "step": 2643
    },
    {
        "loss": 2.6602,
        "grad_norm": 3.8906593322753906,
        "learning_rate": 7.319417643458116e-05,
        "epoch": 0.19703405618898576,
        "step": 2644
    },
    {
        "loss": 2.3168,
        "grad_norm": 3.4325692653656006,
        "learning_rate": 7.312639259833263e-05,
        "epoch": 0.19710857739026752,
        "step": 2645
    },
    {
        "loss": 2.5073,
        "grad_norm": 1.9879521131515503,
        "learning_rate": 7.305862206808697e-05,
        "epoch": 0.19718309859154928,
        "step": 2646
    },
    {
        "loss": 2.5601,
        "grad_norm": 2.90388560295105,
        "learning_rate": 7.299086487739976e-05,
        "epoch": 0.19725761979283107,
        "step": 2647
    },
    {
        "loss": 2.581,
        "grad_norm": 1.7550376653671265,
        "learning_rate": 7.292312105981973e-05,
        "epoch": 0.19733214099411284,
        "step": 2648
    },
    {
        "loss": 2.2373,
        "grad_norm": 3.4938507080078125,
        "learning_rate": 7.2855390648889e-05,
        "epoch": 0.1974066621953946,
        "step": 2649
    },
    {
        "loss": 2.6954,
        "grad_norm": 1.9178481101989746,
        "learning_rate": 7.278767367814325e-05,
        "epoch": 0.19748118339667636,
        "step": 2650
    },
    {
        "loss": 2.5453,
        "grad_norm": 1.9144370555877686,
        "learning_rate": 7.271997018111125e-05,
        "epoch": 0.19755570459795813,
        "step": 2651
    },
    {
        "loss": 2.6586,
        "grad_norm": 1.4895434379577637,
        "learning_rate": 7.265228019131531e-05,
        "epoch": 0.1976302257992399,
        "step": 2652
    },
    {
        "loss": 2.5159,
        "grad_norm": 2.6000750064849854,
        "learning_rate": 7.258460374227085e-05,
        "epoch": 0.19770474700052165,
        "step": 2653
    },
    {
        "loss": 2.4974,
        "grad_norm": 2.4758481979370117,
        "learning_rate": 7.251694086748675e-05,
        "epoch": 0.19777926820180342,
        "step": 2654
    },
    {
        "loss": 1.9039,
        "grad_norm": 1.2621196508407593,
        "learning_rate": 7.244929160046521e-05,
        "epoch": 0.19785378940308518,
        "step": 2655
    },
    {
        "loss": 2.1947,
        "grad_norm": 3.2669312953948975,
        "learning_rate": 7.238165597470143e-05,
        "epoch": 0.19792831060436694,
        "step": 2656
    },
    {
        "loss": 3.0457,
        "grad_norm": 2.454686164855957,
        "learning_rate": 7.231403402368418e-05,
        "epoch": 0.1980028318056487,
        "step": 2657
    },
    {
        "loss": 2.6673,
        "grad_norm": 2.018066167831421,
        "learning_rate": 7.224642578089516e-05,
        "epoch": 0.19807735300693047,
        "step": 2658
    },
    {
        "loss": 2.0828,
        "grad_norm": 3.127511739730835,
        "learning_rate": 7.217883127980953e-05,
        "epoch": 0.19815187420821223,
        "step": 2659
    },
    {
        "loss": 2.745,
        "grad_norm": 2.836761236190796,
        "learning_rate": 7.211125055389549e-05,
        "epoch": 0.198226395409494,
        "step": 2660
    },
    {
        "loss": 2.8047,
        "grad_norm": 3.679473876953125,
        "learning_rate": 7.204368363661444e-05,
        "epoch": 0.19830091661077576,
        "step": 2661
    },
    {
        "loss": 2.0769,
        "grad_norm": 5.561671257019043,
        "learning_rate": 7.197613056142112e-05,
        "epoch": 0.19837543781205752,
        "step": 2662
    },
    {
        "loss": 1.9838,
        "grad_norm": 3.828911542892456,
        "learning_rate": 7.190859136176308e-05,
        "epoch": 0.19844995901333928,
        "step": 2663
    },
    {
        "loss": 1.8735,
        "grad_norm": 2.6527559757232666,
        "learning_rate": 7.18410660710813e-05,
        "epoch": 0.19852448021462105,
        "step": 2664
    },
    {
        "loss": 2.1334,
        "grad_norm": 5.253804683685303,
        "learning_rate": 7.177355472280974e-05,
        "epoch": 0.19859900141590284,
        "step": 2665
    },
    {
        "loss": 2.4096,
        "grad_norm": 2.311774492263794,
        "learning_rate": 7.170605735037543e-05,
        "epoch": 0.1986735226171846,
        "step": 2666
    },
    {
        "loss": 1.9388,
        "grad_norm": 2.5481934547424316,
        "learning_rate": 7.163857398719867e-05,
        "epoch": 0.19874804381846636,
        "step": 2667
    },
    {
        "loss": 2.3997,
        "grad_norm": 1.6955899000167847,
        "learning_rate": 7.15711046666925e-05,
        "epoch": 0.19882256501974813,
        "step": 2668
    },
    {
        "loss": 2.8853,
        "grad_norm": 2.18152117729187,
        "learning_rate": 7.150364942226332e-05,
        "epoch": 0.1988970862210299,
        "step": 2669
    },
    {
        "loss": 2.3666,
        "grad_norm": 2.0924549102783203,
        "learning_rate": 7.143620828731038e-05,
        "epoch": 0.19897160742231165,
        "step": 2670
    },
    {
        "loss": 2.7464,
        "grad_norm": 3.022688150405884,
        "learning_rate": 7.1368781295226e-05,
        "epoch": 0.19904612862359342,
        "step": 2671
    },
    {
        "loss": 2.6358,
        "grad_norm": 1.8186308145523071,
        "learning_rate": 7.13013684793955e-05,
        "epoch": 0.19912064982487518,
        "step": 2672
    },
    {
        "loss": 3.0088,
        "grad_norm": 1.9169989824295044,
        "learning_rate": 7.123396987319714e-05,
        "epoch": 0.19919517102615694,
        "step": 2673
    },
    {
        "loss": 2.2124,
        "grad_norm": 3.4089581966400146,
        "learning_rate": 7.116658551000223e-05,
        "epoch": 0.1992696922274387,
        "step": 2674
    },
    {
        "loss": 2.661,
        "grad_norm": 2.4506680965423584,
        "learning_rate": 7.109921542317496e-05,
        "epoch": 0.19934421342872047,
        "step": 2675
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.5576705932617188,
        "learning_rate": 7.103185964607244e-05,
        "epoch": 0.19941873463000223,
        "step": 2676
    },
    {
        "loss": 2.5762,
        "grad_norm": 2.8273885250091553,
        "learning_rate": 7.096451821204473e-05,
        "epoch": 0.199493255831284,
        "step": 2677
    },
    {
        "loss": 2.54,
        "grad_norm": 2.8275535106658936,
        "learning_rate": 7.089719115443478e-05,
        "epoch": 0.19956777703256576,
        "step": 2678
    },
    {
        "loss": 2.1237,
        "grad_norm": 2.876504421234131,
        "learning_rate": 7.08298785065784e-05,
        "epoch": 0.19964229823384752,
        "step": 2679
    },
    {
        "loss": 2.4988,
        "grad_norm": 2.967712163925171,
        "learning_rate": 7.076258030180436e-05,
        "epoch": 0.19971681943512928,
        "step": 2680
    },
    {
        "loss": 2.4239,
        "grad_norm": 1.9198423624038696,
        "learning_rate": 7.069529657343415e-05,
        "epoch": 0.19979134063641105,
        "step": 2681
    },
    {
        "loss": 2.1342,
        "grad_norm": 3.657539129257202,
        "learning_rate": 7.062802735478214e-05,
        "epoch": 0.1998658618376928,
        "step": 2682
    },
    {
        "loss": 2.315,
        "grad_norm": 2.8883209228515625,
        "learning_rate": 7.056077267915552e-05,
        "epoch": 0.1999403830389746,
        "step": 2683
    },
    {
        "loss": 2.1711,
        "grad_norm": 4.05996036529541,
        "learning_rate": 7.04935325798543e-05,
        "epoch": 0.20001490424025636,
        "step": 2684
    },
    {
        "loss": 1.9094,
        "grad_norm": 3.6813807487487793,
        "learning_rate": 7.042630709017126e-05,
        "epoch": 0.20008942544153813,
        "step": 2685
    },
    {
        "loss": 2.4414,
        "grad_norm": 1.7510409355163574,
        "learning_rate": 7.035909624339191e-05,
        "epoch": 0.2001639466428199,
        "step": 2686
    },
    {
        "loss": 2.4556,
        "grad_norm": 1.2294975519180298,
        "learning_rate": 7.02919000727946e-05,
        "epoch": 0.20023846784410165,
        "step": 2687
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.948484420776367,
        "learning_rate": 7.022471861165033e-05,
        "epoch": 0.20031298904538342,
        "step": 2688
    },
    {
        "loss": 2.3515,
        "grad_norm": 3.1991891860961914,
        "learning_rate": 7.015755189322276e-05,
        "epoch": 0.20038751024666518,
        "step": 2689
    },
    {
        "loss": 2.696,
        "grad_norm": 3.046074867248535,
        "learning_rate": 7.009039995076844e-05,
        "epoch": 0.20046203144794694,
        "step": 2690
    },
    {
        "loss": 2.5093,
        "grad_norm": 2.09576678276062,
        "learning_rate": 7.002326281753648e-05,
        "epoch": 0.2005365526492287,
        "step": 2691
    },
    {
        "loss": 1.9482,
        "grad_norm": 2.957073450088501,
        "learning_rate": 6.995614052676863e-05,
        "epoch": 0.20061107385051047,
        "step": 2692
    },
    {
        "loss": 2.781,
        "grad_norm": 2.4439146518707275,
        "learning_rate": 6.988903311169936e-05,
        "epoch": 0.20068559505179223,
        "step": 2693
    },
    {
        "loss": 2.3661,
        "grad_norm": 2.026642322540283,
        "learning_rate": 6.982194060555569e-05,
        "epoch": 0.200760116253074,
        "step": 2694
    },
    {
        "loss": 2.526,
        "grad_norm": 1.4779542684555054,
        "learning_rate": 6.975486304155743e-05,
        "epoch": 0.20083463745435576,
        "step": 2695
    },
    {
        "loss": 2.3921,
        "grad_norm": 3.735546588897705,
        "learning_rate": 6.968780045291681e-05,
        "epoch": 0.20090915865563752,
        "step": 2696
    },
    {
        "loss": 2.5091,
        "grad_norm": 1.7576854228973389,
        "learning_rate": 6.962075287283873e-05,
        "epoch": 0.20098367985691928,
        "step": 2697
    },
    {
        "loss": 2.7757,
        "grad_norm": 2.6449263095855713,
        "learning_rate": 6.95537203345206e-05,
        "epoch": 0.20105820105820105,
        "step": 2698
    },
    {
        "loss": 2.2591,
        "grad_norm": 2.3632569313049316,
        "learning_rate": 6.948670287115248e-05,
        "epoch": 0.2011327222594828,
        "step": 2699
    },
    {
        "loss": 2.8219,
        "grad_norm": 3.665498733520508,
        "learning_rate": 6.941970051591685e-05,
        "epoch": 0.2012072434607646,
        "step": 2700
    },
    {
        "loss": 2.2781,
        "grad_norm": 2.0502734184265137,
        "learning_rate": 6.935271330198884e-05,
        "epoch": 0.20128176466204636,
        "step": 2701
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.987865447998047,
        "learning_rate": 6.928574126253598e-05,
        "epoch": 0.20135628586332813,
        "step": 2702
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.5818839073181152,
        "learning_rate": 6.921878443071834e-05,
        "epoch": 0.2014308070646099,
        "step": 2703
    },
    {
        "loss": 2.7951,
        "grad_norm": 2.5646133422851562,
        "learning_rate": 6.915184283968841e-05,
        "epoch": 0.20150532826589165,
        "step": 2704
    },
    {
        "loss": 2.2601,
        "grad_norm": 2.861922264099121,
        "learning_rate": 6.908491652259113e-05,
        "epoch": 0.20157984946717342,
        "step": 2705
    },
    {
        "loss": 2.3396,
        "grad_norm": 3.1772210597991943,
        "learning_rate": 6.9018005512564e-05,
        "epoch": 0.20165437066845518,
        "step": 2706
    },
    {
        "loss": 2.3471,
        "grad_norm": 2.0916521549224854,
        "learning_rate": 6.895110984273678e-05,
        "epoch": 0.20172889186973694,
        "step": 2707
    },
    {
        "loss": 2.9185,
        "grad_norm": 1.8402496576309204,
        "learning_rate": 6.888422954623175e-05,
        "epoch": 0.2018034130710187,
        "step": 2708
    },
    {
        "loss": 2.6484,
        "grad_norm": 1.8056484460830688,
        "learning_rate": 6.881736465616353e-05,
        "epoch": 0.20187793427230047,
        "step": 2709
    },
    {
        "loss": 1.8532,
        "grad_norm": 3.7473154067993164,
        "learning_rate": 6.875051520563906e-05,
        "epoch": 0.20195245547358223,
        "step": 2710
    },
    {
        "loss": 2.2745,
        "grad_norm": 2.662926435470581,
        "learning_rate": 6.868368122775779e-05,
        "epoch": 0.202026976674864,
        "step": 2711
    },
    {
        "loss": 2.3534,
        "grad_norm": 2.8861849308013916,
        "learning_rate": 6.861686275561132e-05,
        "epoch": 0.20210149787614576,
        "step": 2712
    },
    {
        "loss": 2.1147,
        "grad_norm": 2.890341281890869,
        "learning_rate": 6.855005982228371e-05,
        "epoch": 0.20217601907742752,
        "step": 2713
    },
    {
        "loss": 2.5071,
        "grad_norm": 3.3084750175476074,
        "learning_rate": 6.848327246085129e-05,
        "epoch": 0.20225054027870928,
        "step": 2714
    },
    {
        "loss": 1.7948,
        "grad_norm": 2.0896213054656982,
        "learning_rate": 6.841650070438261e-05,
        "epoch": 0.20232506147999105,
        "step": 2715
    },
    {
        "loss": 1.9821,
        "grad_norm": 3.8465752601623535,
        "learning_rate": 6.834974458593866e-05,
        "epoch": 0.2023995826812728,
        "step": 2716
    },
    {
        "loss": 2.9327,
        "grad_norm": 2.1495654582977295,
        "learning_rate": 6.828300413857244e-05,
        "epoch": 0.20247410388255457,
        "step": 2717
    },
    {
        "loss": 2.5447,
        "grad_norm": 1.6901246309280396,
        "learning_rate": 6.821627939532946e-05,
        "epoch": 0.20254862508383636,
        "step": 2718
    },
    {
        "loss": 2.086,
        "grad_norm": 2.7267444133758545,
        "learning_rate": 6.814957038924724e-05,
        "epoch": 0.20262314628511813,
        "step": 2719
    },
    {
        "loss": 2.1598,
        "grad_norm": 3.022423505783081,
        "learning_rate": 6.80828771533556e-05,
        "epoch": 0.2026976674863999,
        "step": 2720
    },
    {
        "loss": 2.3327,
        "grad_norm": 2.8779468536376953,
        "learning_rate": 6.801619972067662e-05,
        "epoch": 0.20277218868768165,
        "step": 2721
    },
    {
        "loss": 1.9582,
        "grad_norm": 2.303563356399536,
        "learning_rate": 6.794953812422438e-05,
        "epoch": 0.20284670988896342,
        "step": 2722
    },
    {
        "loss": 2.6365,
        "grad_norm": 2.390226125717163,
        "learning_rate": 6.788289239700534e-05,
        "epoch": 0.20292123109024518,
        "step": 2723
    },
    {
        "loss": 2.4206,
        "grad_norm": 2.357430934906006,
        "learning_rate": 6.781626257201781e-05,
        "epoch": 0.20299575229152694,
        "step": 2724
    },
    {
        "loss": 2.2028,
        "grad_norm": 2.3603293895721436,
        "learning_rate": 6.774964868225256e-05,
        "epoch": 0.2030702734928087,
        "step": 2725
    },
    {
        "loss": 2.3616,
        "grad_norm": 2.5905559062957764,
        "learning_rate": 6.768305076069221e-05,
        "epoch": 0.20314479469409047,
        "step": 2726
    },
    {
        "loss": 2.1692,
        "grad_norm": 2.018979072570801,
        "learning_rate": 6.761646884031162e-05,
        "epoch": 0.20321931589537223,
        "step": 2727
    },
    {
        "loss": 2.785,
        "grad_norm": 1.650670051574707,
        "learning_rate": 6.75499029540777e-05,
        "epoch": 0.203293837096654,
        "step": 2728
    },
    {
        "loss": 1.7999,
        "grad_norm": 3.361370086669922,
        "learning_rate": 6.74833531349493e-05,
        "epoch": 0.20336835829793576,
        "step": 2729
    },
    {
        "loss": 1.8673,
        "grad_norm": 3.4591224193573,
        "learning_rate": 6.741681941587752e-05,
        "epoch": 0.20344287949921752,
        "step": 2730
    },
    {
        "loss": 2.1822,
        "grad_norm": 3.4757273197174072,
        "learning_rate": 6.735030182980535e-05,
        "epoch": 0.20351740070049928,
        "step": 2731
    },
    {
        "loss": 2.7212,
        "grad_norm": 1.6364294290542603,
        "learning_rate": 6.728380040966782e-05,
        "epoch": 0.20359192190178105,
        "step": 2732
    },
    {
        "loss": 2.1427,
        "grad_norm": 3.1730053424835205,
        "learning_rate": 6.721731518839191e-05,
        "epoch": 0.2036664431030628,
        "step": 2733
    },
    {
        "loss": 2.1381,
        "grad_norm": 4.185059547424316,
        "learning_rate": 6.715084619889671e-05,
        "epoch": 0.20374096430434457,
        "step": 2734
    },
    {
        "loss": 2.1188,
        "grad_norm": 3.6288557052612305,
        "learning_rate": 6.708439347409315e-05,
        "epoch": 0.20381548550562634,
        "step": 2735
    },
    {
        "loss": 2.7525,
        "grad_norm": 2.2249374389648438,
        "learning_rate": 6.701795704688418e-05,
        "epoch": 0.20389000670690813,
        "step": 2736
    },
    {
        "loss": 2.4139,
        "grad_norm": 3.2696895599365234,
        "learning_rate": 6.695153695016463e-05,
        "epoch": 0.2039645279081899,
        "step": 2737
    },
    {
        "loss": 2.1212,
        "grad_norm": 3.7981278896331787,
        "learning_rate": 6.688513321682126e-05,
        "epoch": 0.20403904910947165,
        "step": 2738
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.892573595046997,
        "learning_rate": 6.681874587973273e-05,
        "epoch": 0.20411357031075342,
        "step": 2739
    },
    {
        "loss": 3.3188,
        "grad_norm": 4.631450176239014,
        "learning_rate": 6.675237497176958e-05,
        "epoch": 0.20418809151203518,
        "step": 2740
    },
    {
        "loss": 2.183,
        "grad_norm": 2.607489585876465,
        "learning_rate": 6.668602052579424e-05,
        "epoch": 0.20426261271331694,
        "step": 2741
    },
    {
        "loss": 2.7073,
        "grad_norm": 2.458759307861328,
        "learning_rate": 6.661968257466098e-05,
        "epoch": 0.2043371339145987,
        "step": 2742
    },
    {
        "loss": 2.2245,
        "grad_norm": 3.3439769744873047,
        "learning_rate": 6.655336115121589e-05,
        "epoch": 0.20441165511588047,
        "step": 2743
    },
    {
        "loss": 2.4896,
        "grad_norm": 3.536799669265747,
        "learning_rate": 6.648705628829685e-05,
        "epoch": 0.20448617631716223,
        "step": 2744
    },
    {
        "loss": 2.5511,
        "grad_norm": 2.5374724864959717,
        "learning_rate": 6.642076801873353e-05,
        "epoch": 0.204560697518444,
        "step": 2745
    },
    {
        "loss": 2.7192,
        "grad_norm": 2.3092339038848877,
        "learning_rate": 6.635449637534752e-05,
        "epoch": 0.20463521871972576,
        "step": 2746
    },
    {
        "loss": 1.9734,
        "grad_norm": 3.073173999786377,
        "learning_rate": 6.628824139095203e-05,
        "epoch": 0.20470973992100752,
        "step": 2747
    },
    {
        "loss": 2.4993,
        "grad_norm": 2.8275630474090576,
        "learning_rate": 6.622200309835206e-05,
        "epoch": 0.20478426112228929,
        "step": 2748
    },
    {
        "loss": 2.5126,
        "grad_norm": 2.0029938220977783,
        "learning_rate": 6.615578153034441e-05,
        "epoch": 0.20485878232357105,
        "step": 2749
    },
    {
        "loss": 2.6597,
        "grad_norm": 2.7886874675750732,
        "learning_rate": 6.608957671971744e-05,
        "epoch": 0.2049333035248528,
        "step": 2750
    },
    {
        "loss": 1.9363,
        "grad_norm": 2.491938829421997,
        "learning_rate": 6.602338869925147e-05,
        "epoch": 0.20500782472613457,
        "step": 2751
    },
    {
        "loss": 2.1262,
        "grad_norm": 2.0902180671691895,
        "learning_rate": 6.595721750171825e-05,
        "epoch": 0.20508234592741634,
        "step": 2752
    },
    {
        "loss": 2.7702,
        "grad_norm": 2.7028350830078125,
        "learning_rate": 6.589106315988133e-05,
        "epoch": 0.20515686712869813,
        "step": 2753
    },
    {
        "loss": 2.4591,
        "grad_norm": 4.181146144866943,
        "learning_rate": 6.582492570649587e-05,
        "epoch": 0.2052313883299799,
        "step": 2754
    },
    {
        "loss": 2.1287,
        "grad_norm": 3.602910280227661,
        "learning_rate": 6.575880517430874e-05,
        "epoch": 0.20530590953126165,
        "step": 2755
    },
    {
        "loss": 1.153,
        "grad_norm": 4.2908124923706055,
        "learning_rate": 6.569270159605834e-05,
        "epoch": 0.20538043073254342,
        "step": 2756
    },
    {
        "loss": 2.5726,
        "grad_norm": 2.156219482421875,
        "learning_rate": 6.562661500447474e-05,
        "epoch": 0.20545495193382518,
        "step": 2757
    },
    {
        "loss": 2.9655,
        "grad_norm": 3.3321542739868164,
        "learning_rate": 6.556054543227956e-05,
        "epoch": 0.20552947313510694,
        "step": 2758
    },
    {
        "loss": 1.744,
        "grad_norm": 3.641449213027954,
        "learning_rate": 6.549449291218602e-05,
        "epoch": 0.2056039943363887,
        "step": 2759
    },
    {
        "loss": 2.1683,
        "grad_norm": 2.7022249698638916,
        "learning_rate": 6.542845747689884e-05,
        "epoch": 0.20567851553767047,
        "step": 2760
    },
    {
        "loss": 1.8358,
        "grad_norm": 3.342364549636841,
        "learning_rate": 6.53624391591144e-05,
        "epoch": 0.20575303673895223,
        "step": 2761
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.701792597770691,
        "learning_rate": 6.529643799152049e-05,
        "epoch": 0.205827557940234,
        "step": 2762
    },
    {
        "loss": 2.1454,
        "grad_norm": 2.4777133464813232,
        "learning_rate": 6.523045400679646e-05,
        "epoch": 0.20590207914151576,
        "step": 2763
    },
    {
        "loss": 1.5952,
        "grad_norm": 2.7256476879119873,
        "learning_rate": 6.516448723761315e-05,
        "epoch": 0.20597660034279752,
        "step": 2764
    },
    {
        "loss": 1.497,
        "grad_norm": 3.0144710540771484,
        "learning_rate": 6.509853771663284e-05,
        "epoch": 0.20605112154407929,
        "step": 2765
    },
    {
        "loss": 2.425,
        "grad_norm": 3.1980154514312744,
        "learning_rate": 6.503260547650927e-05,
        "epoch": 0.20612564274536105,
        "step": 2766
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.4491827487945557,
        "learning_rate": 6.496669054988773e-05,
        "epoch": 0.2062001639466428,
        "step": 2767
    },
    {
        "loss": 2.7516,
        "grad_norm": 3.2682135105133057,
        "learning_rate": 6.490079296940484e-05,
        "epoch": 0.20627468514792457,
        "step": 2768
    },
    {
        "loss": 2.8299,
        "grad_norm": 1.7504364252090454,
        "learning_rate": 6.483491276768858e-05,
        "epoch": 0.20634920634920634,
        "step": 2769
    },
    {
        "loss": 2.7262,
        "grad_norm": 2.4607181549072266,
        "learning_rate": 6.476904997735849e-05,
        "epoch": 0.2064237275504881,
        "step": 2770
    },
    {
        "loss": 2.554,
        "grad_norm": 1.9013499021530151,
        "learning_rate": 6.47032046310253e-05,
        "epoch": 0.2064982487517699,
        "step": 2771
    },
    {
        "loss": 2.1487,
        "grad_norm": 2.8924427032470703,
        "learning_rate": 6.463737676129133e-05,
        "epoch": 0.20657276995305165,
        "step": 2772
    },
    {
        "loss": 2.5128,
        "grad_norm": 1.8422284126281738,
        "learning_rate": 6.457156640074994e-05,
        "epoch": 0.20664729115433342,
        "step": 2773
    },
    {
        "loss": 2.4871,
        "grad_norm": 4.700745105743408,
        "learning_rate": 6.450577358198613e-05,
        "epoch": 0.20672181235561518,
        "step": 2774
    },
    {
        "loss": 2.2566,
        "grad_norm": 2.288418769836426,
        "learning_rate": 6.443999833757602e-05,
        "epoch": 0.20679633355689694,
        "step": 2775
    },
    {
        "loss": 2.5155,
        "grad_norm": 2.9064245223999023,
        "learning_rate": 6.437424070008709e-05,
        "epoch": 0.2068708547581787,
        "step": 2776
    },
    {
        "loss": 1.9852,
        "grad_norm": 4.005183219909668,
        "learning_rate": 6.430850070207814e-05,
        "epoch": 0.20694537595946047,
        "step": 2777
    },
    {
        "loss": 2.4134,
        "grad_norm": 2.5399415493011475,
        "learning_rate": 6.424277837609913e-05,
        "epoch": 0.20701989716074223,
        "step": 2778
    },
    {
        "loss": 2.3422,
        "grad_norm": 1.819348931312561,
        "learning_rate": 6.417707375469142e-05,
        "epoch": 0.207094418362024,
        "step": 2779
    },
    {
        "loss": 1.9493,
        "grad_norm": 3.5980942249298096,
        "learning_rate": 6.411138687038742e-05,
        "epoch": 0.20716893956330576,
        "step": 2780
    },
    {
        "loss": 1.7996,
        "grad_norm": 2.954956531524658,
        "learning_rate": 6.40457177557109e-05,
        "epoch": 0.20724346076458752,
        "step": 2781
    },
    {
        "loss": 3.1777,
        "grad_norm": 3.5600461959838867,
        "learning_rate": 6.398006644317687e-05,
        "epoch": 0.20731798196586929,
        "step": 2782
    },
    {
        "loss": 2.0959,
        "grad_norm": 2.64707350730896,
        "learning_rate": 6.391443296529129e-05,
        "epoch": 0.20739250316715105,
        "step": 2783
    },
    {
        "loss": 2.5156,
        "grad_norm": 1.7136945724487305,
        "learning_rate": 6.384881735455161e-05,
        "epoch": 0.2074670243684328,
        "step": 2784
    },
    {
        "loss": 2.3767,
        "grad_norm": 3.070798873901367,
        "learning_rate": 6.378321964344611e-05,
        "epoch": 0.20754154556971458,
        "step": 2785
    },
    {
        "loss": 1.7877,
        "grad_norm": 5.177518844604492,
        "learning_rate": 6.371763986445447e-05,
        "epoch": 0.20761606677099634,
        "step": 2786
    },
    {
        "loss": 2.0358,
        "grad_norm": 5.209067344665527,
        "learning_rate": 6.365207805004736e-05,
        "epoch": 0.2076905879722781,
        "step": 2787
    },
    {
        "loss": 2.8647,
        "grad_norm": 2.508105754852295,
        "learning_rate": 6.358653423268653e-05,
        "epoch": 0.20776510917355986,
        "step": 2788
    },
    {
        "loss": 2.1537,
        "grad_norm": 3.0740320682525635,
        "learning_rate": 6.352100844482496e-05,
        "epoch": 0.20783963037484166,
        "step": 2789
    },
    {
        "loss": 2.7332,
        "grad_norm": 2.6756367683410645,
        "learning_rate": 6.345550071890652e-05,
        "epoch": 0.20791415157612342,
        "step": 2790
    },
    {
        "loss": 2.097,
        "grad_norm": 3.375061273574829,
        "learning_rate": 6.339001108736632e-05,
        "epoch": 0.20798867277740518,
        "step": 2791
    },
    {
        "loss": 3.0971,
        "grad_norm": 2.263362407684326,
        "learning_rate": 6.332453958263039e-05,
        "epoch": 0.20806319397868694,
        "step": 2792
    },
    {
        "loss": 2.5807,
        "grad_norm": 3.7409019470214844,
        "learning_rate": 6.325908623711578e-05,
        "epoch": 0.2081377151799687,
        "step": 2793
    },
    {
        "loss": 2.4088,
        "grad_norm": 4.005861759185791,
        "learning_rate": 6.319365108323059e-05,
        "epoch": 0.20821223638125047,
        "step": 2794
    },
    {
        "loss": 2.6293,
        "grad_norm": 2.9792513847351074,
        "learning_rate": 6.312823415337395e-05,
        "epoch": 0.20828675758253223,
        "step": 2795
    },
    {
        "loss": 2.2162,
        "grad_norm": 4.154196739196777,
        "learning_rate": 6.306283547993587e-05,
        "epoch": 0.208361278783814,
        "step": 2796
    },
    {
        "loss": 2.0764,
        "grad_norm": 3.034291982650757,
        "learning_rate": 6.299745509529745e-05,
        "epoch": 0.20843579998509576,
        "step": 2797
    },
    {
        "loss": 2.2344,
        "grad_norm": 2.9806032180786133,
        "learning_rate": 6.29320930318306e-05,
        "epoch": 0.20851032118637752,
        "step": 2798
    },
    {
        "loss": 2.4965,
        "grad_norm": 2.80622935295105,
        "learning_rate": 6.286674932189824e-05,
        "epoch": 0.20858484238765929,
        "step": 2799
    },
    {
        "loss": 2.0235,
        "grad_norm": 2.6294469833374023,
        "learning_rate": 6.280142399785416e-05,
        "epoch": 0.20865936358894105,
        "step": 2800
    },
    {
        "loss": 2.2673,
        "grad_norm": 1.983432412147522,
        "learning_rate": 6.273611709204304e-05,
        "epoch": 0.2087338847902228,
        "step": 2801
    },
    {
        "loss": 2.5442,
        "grad_norm": 2.9998881816864014,
        "learning_rate": 6.267082863680056e-05,
        "epoch": 0.20880840599150458,
        "step": 2802
    },
    {
        "loss": 2.3725,
        "grad_norm": 2.1147778034210205,
        "learning_rate": 6.260555866445309e-05,
        "epoch": 0.20888292719278634,
        "step": 2803
    },
    {
        "loss": 2.5562,
        "grad_norm": 4.461204528808594,
        "learning_rate": 6.254030720731798e-05,
        "epoch": 0.2089574483940681,
        "step": 2804
    },
    {
        "loss": 1.9975,
        "grad_norm": 3.3001208305358887,
        "learning_rate": 6.247507429770334e-05,
        "epoch": 0.20903196959534986,
        "step": 2805
    },
    {
        "loss": 2.6171,
        "grad_norm": 2.543825149536133,
        "learning_rate": 6.240985996790809e-05,
        "epoch": 0.20910649079663166,
        "step": 2806
    },
    {
        "loss": 2.8015,
        "grad_norm": 2.1887526512145996,
        "learning_rate": 6.234466425022204e-05,
        "epoch": 0.20918101199791342,
        "step": 2807
    },
    {
        "loss": 1.3736,
        "grad_norm": 2.0473053455352783,
        "learning_rate": 6.22794871769257e-05,
        "epoch": 0.20925553319919518,
        "step": 2808
    },
    {
        "loss": 2.7716,
        "grad_norm": 3.3549132347106934,
        "learning_rate": 6.221432878029036e-05,
        "epoch": 0.20933005440047694,
        "step": 2809
    },
    {
        "loss": 2.4948,
        "grad_norm": 2.9317917823791504,
        "learning_rate": 6.214918909257808e-05,
        "epoch": 0.2094045756017587,
        "step": 2810
    },
    {
        "loss": 2.5094,
        "grad_norm": 2.1703526973724365,
        "learning_rate": 6.208406814604164e-05,
        "epoch": 0.20947909680304047,
        "step": 2811
    },
    {
        "loss": 2.6201,
        "grad_norm": 3.079052686691284,
        "learning_rate": 6.20189659729246e-05,
        "epoch": 0.20955361800432223,
        "step": 2812
    },
    {
        "loss": 2.6581,
        "grad_norm": 2.1538009643554688,
        "learning_rate": 6.195388260546115e-05,
        "epoch": 0.209628139205604,
        "step": 2813
    },
    {
        "loss": 1.7391,
        "grad_norm": 3.615133285522461,
        "learning_rate": 6.18888180758762e-05,
        "epoch": 0.20970266040688576,
        "step": 2814
    },
    {
        "loss": 2.1172,
        "grad_norm": 2.92191481590271,
        "learning_rate": 6.18237724163853e-05,
        "epoch": 0.20977718160816752,
        "step": 2815
    },
    {
        "loss": 2.7728,
        "grad_norm": 1.6491856575012207,
        "learning_rate": 6.175874565919471e-05,
        "epoch": 0.2098517028094493,
        "step": 2816
    },
    {
        "loss": 2.4246,
        "grad_norm": 3.2609968185424805,
        "learning_rate": 6.16937378365013e-05,
        "epoch": 0.20992622401073105,
        "step": 2817
    },
    {
        "loss": 3.408,
        "grad_norm": 2.7641727924346924,
        "learning_rate": 6.162874898049259e-05,
        "epoch": 0.2100007452120128,
        "step": 2818
    },
    {
        "loss": 0.728,
        "grad_norm": 3.5440011024475098,
        "learning_rate": 6.156377912334668e-05,
        "epoch": 0.21007526641329458,
        "step": 2819
    },
    {
        "loss": 2.8442,
        "grad_norm": 2.348492383956909,
        "learning_rate": 6.149882829723228e-05,
        "epoch": 0.21014978761457634,
        "step": 2820
    },
    {
        "loss": 2.132,
        "grad_norm": 3.2770254611968994,
        "learning_rate": 6.143389653430862e-05,
        "epoch": 0.2102243088158581,
        "step": 2821
    },
    {
        "loss": 2.8227,
        "grad_norm": 3.0811259746551514,
        "learning_rate": 6.136898386672562e-05,
        "epoch": 0.21029883001713987,
        "step": 2822
    },
    {
        "loss": 2.857,
        "grad_norm": 1.3579961061477661,
        "learning_rate": 6.130409032662366e-05,
        "epoch": 0.21037335121842163,
        "step": 2823
    },
    {
        "loss": 1.1346,
        "grad_norm": 4.170708179473877,
        "learning_rate": 6.123921594613358e-05,
        "epoch": 0.21044787241970342,
        "step": 2824
    },
    {
        "loss": 2.3962,
        "grad_norm": 1.7515952587127686,
        "learning_rate": 6.117436075737689e-05,
        "epoch": 0.21052239362098518,
        "step": 2825
    },
    {
        "loss": 1.38,
        "grad_norm": 3.170107841491699,
        "learning_rate": 6.110952479246545e-05,
        "epoch": 0.21059691482226695,
        "step": 2826
    },
    {
        "loss": 2.4706,
        "grad_norm": 1.549439787864685,
        "learning_rate": 6.10447080835017e-05,
        "epoch": 0.2106714360235487,
        "step": 2827
    },
    {
        "loss": 2.24,
        "grad_norm": 3.0791139602661133,
        "learning_rate": 6.0979910662578556e-05,
        "epoch": 0.21074595722483047,
        "step": 2828
    },
    {
        "loss": 2.7051,
        "grad_norm": 1.657359004020691,
        "learning_rate": 6.09151325617793e-05,
        "epoch": 0.21082047842611223,
        "step": 2829
    },
    {
        "loss": 2.4371,
        "grad_norm": 2.966625213623047,
        "learning_rate": 6.0850373813177686e-05,
        "epoch": 0.210894999627394,
        "step": 2830
    },
    {
        "loss": 1.7305,
        "grad_norm": 2.8065459728240967,
        "learning_rate": 6.0785634448837916e-05,
        "epoch": 0.21096952082867576,
        "step": 2831
    },
    {
        "loss": 2.4928,
        "grad_norm": 2.700540542602539,
        "learning_rate": 6.0720914500814507e-05,
        "epoch": 0.21104404202995752,
        "step": 2832
    },
    {
        "loss": 2.8221,
        "grad_norm": 2.6303515434265137,
        "learning_rate": 6.0656214001152556e-05,
        "epoch": 0.2111185632312393,
        "step": 2833
    },
    {
        "loss": 2.0846,
        "grad_norm": 2.498424530029297,
        "learning_rate": 6.0591532981887245e-05,
        "epoch": 0.21119308443252105,
        "step": 2834
    },
    {
        "loss": 1.9836,
        "grad_norm": 3.358100414276123,
        "learning_rate": 6.0526871475044367e-05,
        "epoch": 0.2112676056338028,
        "step": 2835
    },
    {
        "loss": 2.834,
        "grad_norm": 2.3967432975769043,
        "learning_rate": 6.0462229512639937e-05,
        "epoch": 0.21134212683508458,
        "step": 2836
    },
    {
        "loss": 2.2026,
        "grad_norm": 1.9766584634780884,
        "learning_rate": 6.039760712668023e-05,
        "epoch": 0.21141664803636634,
        "step": 2837
    },
    {
        "loss": 2.5419,
        "grad_norm": 2.4224395751953125,
        "learning_rate": 6.033300434916203e-05,
        "epoch": 0.2114911692376481,
        "step": 2838
    },
    {
        "loss": 2.4647,
        "grad_norm": 1.7085132598876953,
        "learning_rate": 6.026842121207216e-05,
        "epoch": 0.21156569043892987,
        "step": 2839
    },
    {
        "loss": 2.6425,
        "grad_norm": 2.0059022903442383,
        "learning_rate": 6.0203857747387995e-05,
        "epoch": 0.21164021164021163,
        "step": 2840
    },
    {
        "loss": 2.4579,
        "grad_norm": 2.8893024921417236,
        "learning_rate": 6.013931398707684e-05,
        "epoch": 0.2117147328414934,
        "step": 2841
    },
    {
        "loss": 2.0823,
        "grad_norm": 2.403278112411499,
        "learning_rate": 6.0074789963096525e-05,
        "epoch": 0.21178925404277518,
        "step": 2842
    },
    {
        "loss": 2.1783,
        "grad_norm": 1.9663021564483643,
        "learning_rate": 6.001028570739506e-05,
        "epoch": 0.21186377524405695,
        "step": 2843
    },
    {
        "loss": 1.7011,
        "grad_norm": 2.7834813594818115,
        "learning_rate": 5.994580125191051e-05,
        "epoch": 0.2119382964453387,
        "step": 2844
    },
    {
        "loss": 1.6722,
        "grad_norm": 4.118234157562256,
        "learning_rate": 5.9881336628571336e-05,
        "epoch": 0.21201281764662047,
        "step": 2845
    },
    {
        "loss": 2.5213,
        "grad_norm": 2.7148120403289795,
        "learning_rate": 5.981689186929598e-05,
        "epoch": 0.21208733884790223,
        "step": 2846
    },
    {
        "loss": 3.1137,
        "grad_norm": 3.135082960128784,
        "learning_rate": 5.9752467005993216e-05,
        "epoch": 0.212161860049184,
        "step": 2847
    },
    {
        "loss": 2.4464,
        "grad_norm": 2.889033079147339,
        "learning_rate": 5.9688062070561915e-05,
        "epoch": 0.21223638125046576,
        "step": 2848
    },
    {
        "loss": 2.5888,
        "grad_norm": 3.041095018386841,
        "learning_rate": 5.9623677094890994e-05,
        "epoch": 0.21231090245174752,
        "step": 2849
    },
    {
        "loss": 2.4524,
        "grad_norm": 2.8795480728149414,
        "learning_rate": 5.9559312110859676e-05,
        "epoch": 0.2123854236530293,
        "step": 2850
    },
    {
        "loss": 2.5499,
        "grad_norm": 2.3864619731903076,
        "learning_rate": 5.9494967150337054e-05,
        "epoch": 0.21245994485431105,
        "step": 2851
    },
    {
        "loss": 1.6077,
        "grad_norm": 2.50333833694458,
        "learning_rate": 5.943064224518253e-05,
        "epoch": 0.2125344660555928,
        "step": 2852
    },
    {
        "loss": 2.0658,
        "grad_norm": 2.792572021484375,
        "learning_rate": 5.936633742724541e-05,
        "epoch": 0.21260898725687458,
        "step": 2853
    },
    {
        "loss": 2.4821,
        "grad_norm": 4.9113569259643555,
        "learning_rate": 5.9302052728365086e-05,
        "epoch": 0.21268350845815634,
        "step": 2854
    },
    {
        "loss": 2.3796,
        "grad_norm": 3.0321431159973145,
        "learning_rate": 5.923778818037116e-05,
        "epoch": 0.2127580296594381,
        "step": 2855
    },
    {
        "loss": 2.052,
        "grad_norm": 3.4772536754608154,
        "learning_rate": 5.917354381508291e-05,
        "epoch": 0.21283255086071987,
        "step": 2856
    },
    {
        "loss": 2.3979,
        "grad_norm": 4.252895832061768,
        "learning_rate": 5.910931966430997e-05,
        "epoch": 0.21290707206200163,
        "step": 2857
    },
    {
        "loss": 2.0298,
        "grad_norm": 3.6002421379089355,
        "learning_rate": 5.9045115759851765e-05,
        "epoch": 0.2129815932632834,
        "step": 2858
    },
    {
        "loss": 2.8064,
        "grad_norm": 2.5486655235290527,
        "learning_rate": 5.898093213349778e-05,
        "epoch": 0.21305611446456516,
        "step": 2859
    },
    {
        "loss": 2.4505,
        "grad_norm": 3.525723695755005,
        "learning_rate": 5.8916768817027426e-05,
        "epoch": 0.21313063566584695,
        "step": 2860
    },
    {
        "loss": 2.5527,
        "grad_norm": 3.7243943214416504,
        "learning_rate": 5.8852625842209984e-05,
        "epoch": 0.2132051568671287,
        "step": 2861
    },
    {
        "loss": 1.9239,
        "grad_norm": 3.2045233249664307,
        "learning_rate": 5.878850324080487e-05,
        "epoch": 0.21327967806841047,
        "step": 2862
    },
    {
        "loss": 2.0117,
        "grad_norm": 2.9894440174102783,
        "learning_rate": 5.87244010445612e-05,
        "epoch": 0.21335419926969224,
        "step": 2863
    },
    {
        "loss": 2.3171,
        "grad_norm": 3.2580621242523193,
        "learning_rate": 5.866031928521809e-05,
        "epoch": 0.213428720470974,
        "step": 2864
    },
    {
        "loss": 2.645,
        "grad_norm": 1.7049129009246826,
        "learning_rate": 5.859625799450452e-05,
        "epoch": 0.21350324167225576,
        "step": 2865
    },
    {
        "loss": 1.7648,
        "grad_norm": 3.2104251384735107,
        "learning_rate": 5.853221720413935e-05,
        "epoch": 0.21357776287353752,
        "step": 2866
    },
    {
        "loss": 1.8907,
        "grad_norm": 4.345683574676514,
        "learning_rate": 5.84681969458312e-05,
        "epoch": 0.2136522840748193,
        "step": 2867
    },
    {
        "loss": 2.8495,
        "grad_norm": 3.3088574409484863,
        "learning_rate": 5.840419725127868e-05,
        "epoch": 0.21372680527610105,
        "step": 2868
    },
    {
        "loss": 2.5661,
        "grad_norm": 2.7886881828308105,
        "learning_rate": 5.83402181521701e-05,
        "epoch": 0.2138013264773828,
        "step": 2869
    },
    {
        "loss": 1.6809,
        "grad_norm": 3.2019999027252197,
        "learning_rate": 5.827625968018364e-05,
        "epoch": 0.21387584767866458,
        "step": 2870
    },
    {
        "loss": 2.4843,
        "grad_norm": 1.951587438583374,
        "learning_rate": 5.821232186698716e-05,
        "epoch": 0.21395036887994634,
        "step": 2871
    },
    {
        "loss": 2.3677,
        "grad_norm": 3.043710708618164,
        "learning_rate": 5.8148404744238416e-05,
        "epoch": 0.2140248900812281,
        "step": 2872
    },
    {
        "loss": 2.5773,
        "grad_norm": 3.5718977451324463,
        "learning_rate": 5.808450834358485e-05,
        "epoch": 0.21409941128250987,
        "step": 2873
    },
    {
        "loss": 2.729,
        "grad_norm": 2.2385873794555664,
        "learning_rate": 5.8020632696663734e-05,
        "epoch": 0.21417393248379163,
        "step": 2874
    },
    {
        "loss": 2.3544,
        "grad_norm": 4.017364978790283,
        "learning_rate": 5.795677783510187e-05,
        "epoch": 0.2142484536850734,
        "step": 2875
    },
    {
        "loss": 2.5689,
        "grad_norm": 2.5583572387695312,
        "learning_rate": 5.789294379051596e-05,
        "epoch": 0.21432297488635516,
        "step": 2876
    },
    {
        "loss": 2.4328,
        "grad_norm": 1.8855899572372437,
        "learning_rate": 5.782913059451233e-05,
        "epoch": 0.21439749608763695,
        "step": 2877
    },
    {
        "loss": 3.0871,
        "grad_norm": 3.340639352798462,
        "learning_rate": 5.776533827868696e-05,
        "epoch": 0.2144720172889187,
        "step": 2878
    },
    {
        "loss": 2.1206,
        "grad_norm": 4.219205856323242,
        "learning_rate": 5.770156687462558e-05,
        "epoch": 0.21454653849020047,
        "step": 2879
    },
    {
        "loss": 2.7975,
        "grad_norm": 2.536511182785034,
        "learning_rate": 5.76378164139034e-05,
        "epoch": 0.21462105969148224,
        "step": 2880
    },
    {
        "loss": 2.7731,
        "grad_norm": 2.6430087089538574,
        "learning_rate": 5.757408692808541e-05,
        "epoch": 0.214695580892764,
        "step": 2881
    },
    {
        "loss": 2.6355,
        "grad_norm": 2.9908552169799805,
        "learning_rate": 5.751037844872618e-05,
        "epoch": 0.21477010209404576,
        "step": 2882
    },
    {
        "loss": 2.3098,
        "grad_norm": 4.213485240936279,
        "learning_rate": 5.744669100736983e-05,
        "epoch": 0.21484462329532752,
        "step": 2883
    },
    {
        "loss": 2.5382,
        "grad_norm": 2.2136218547821045,
        "learning_rate": 5.738302463555017e-05,
        "epoch": 0.2149191444966093,
        "step": 2884
    },
    {
        "loss": 1.9703,
        "grad_norm": 2.438108205795288,
        "learning_rate": 5.731937936479039e-05,
        "epoch": 0.21499366569789105,
        "step": 2885
    },
    {
        "loss": 2.2557,
        "grad_norm": 2.724914312362671,
        "learning_rate": 5.725575522660347e-05,
        "epoch": 0.21506818689917281,
        "step": 2886
    },
    {
        "loss": 2.2311,
        "grad_norm": 1.6209356784820557,
        "learning_rate": 5.7192152252491705e-05,
        "epoch": 0.21514270810045458,
        "step": 2887
    },
    {
        "loss": 1.9115,
        "grad_norm": 4.005504131317139,
        "learning_rate": 5.712857047394704e-05,
        "epoch": 0.21521722930173634,
        "step": 2888
    },
    {
        "loss": 2.4808,
        "grad_norm": 2.601078987121582,
        "learning_rate": 5.7065009922450905e-05,
        "epoch": 0.2152917505030181,
        "step": 2889
    },
    {
        "loss": 2.6707,
        "grad_norm": 2.525219678878784,
        "learning_rate": 5.7001470629474206e-05,
        "epoch": 0.21536627170429987,
        "step": 2890
    },
    {
        "loss": 1.9648,
        "grad_norm": 2.7707085609436035,
        "learning_rate": 5.6937952626477384e-05,
        "epoch": 0.21544079290558163,
        "step": 2891
    },
    {
        "loss": 2.5957,
        "grad_norm": 3.0490245819091797,
        "learning_rate": 5.687445594491019e-05,
        "epoch": 0.2155153141068634,
        "step": 2892
    },
    {
        "loss": 2.5833,
        "grad_norm": 2.1564133167266846,
        "learning_rate": 5.681098061621193e-05,
        "epoch": 0.21558983530814516,
        "step": 2893
    },
    {
        "loss": 2.4301,
        "grad_norm": 3.957674980163574,
        "learning_rate": 5.674752667181138e-05,
        "epoch": 0.21566435650942692,
        "step": 2894
    },
    {
        "loss": 1.4521,
        "grad_norm": 3.155478000640869,
        "learning_rate": 5.66840941431266e-05,
        "epoch": 0.2157388777107087,
        "step": 2895
    },
    {
        "loss": 2.6831,
        "grad_norm": 2.284536600112915,
        "learning_rate": 5.662068306156512e-05,
        "epoch": 0.21581339891199047,
        "step": 2896
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.4371598958969116,
        "learning_rate": 5.655729345852385e-05,
        "epoch": 0.21588792011327224,
        "step": 2897
    },
    {
        "loss": 2.0216,
        "grad_norm": 2.7732279300689697,
        "learning_rate": 5.649392536538909e-05,
        "epoch": 0.215962441314554,
        "step": 2898
    },
    {
        "loss": 2.1181,
        "grad_norm": 2.198220729827881,
        "learning_rate": 5.643057881353646e-05,
        "epoch": 0.21603696251583576,
        "step": 2899
    },
    {
        "loss": 2.5566,
        "grad_norm": 2.926593780517578,
        "learning_rate": 5.636725383433084e-05,
        "epoch": 0.21611148371711753,
        "step": 2900
    },
    {
        "loss": 2.1853,
        "grad_norm": 4.926783084869385,
        "learning_rate": 5.630395045912655e-05,
        "epoch": 0.2161860049183993,
        "step": 2901
    },
    {
        "loss": 2.5888,
        "grad_norm": 2.681216239929199,
        "learning_rate": 5.624066871926715e-05,
        "epoch": 0.21626052611968105,
        "step": 2902
    },
    {
        "loss": 2.167,
        "grad_norm": 2.042558193206787,
        "learning_rate": 5.617740864608552e-05,
        "epoch": 0.21633504732096281,
        "step": 2903
    },
    {
        "loss": 2.4027,
        "grad_norm": 3.0714187622070312,
        "learning_rate": 5.611417027090382e-05,
        "epoch": 0.21640956852224458,
        "step": 2904
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.82778799533844,
        "learning_rate": 5.605095362503338e-05,
        "epoch": 0.21648408972352634,
        "step": 2905
    },
    {
        "loss": 1.8168,
        "grad_norm": 1.8309776782989502,
        "learning_rate": 5.5987758739774884e-05,
        "epoch": 0.2165586109248081,
        "step": 2906
    },
    {
        "loss": 2.0511,
        "grad_norm": 2.460374355316162,
        "learning_rate": 5.5924585646418115e-05,
        "epoch": 0.21663313212608987,
        "step": 2907
    },
    {
        "loss": 2.8985,
        "grad_norm": 1.7873529195785522,
        "learning_rate": 5.586143437624216e-05,
        "epoch": 0.21670765332737163,
        "step": 2908
    },
    {
        "loss": 2.103,
        "grad_norm": 3.686129331588745,
        "learning_rate": 5.579830496051538e-05,
        "epoch": 0.2167821745286534,
        "step": 2909
    },
    {
        "loss": 2.2686,
        "grad_norm": 3.6358487606048584,
        "learning_rate": 5.573519743049511e-05,
        "epoch": 0.21685669572993516,
        "step": 2910
    },
    {
        "loss": 3.2173,
        "grad_norm": 6.400631904602051,
        "learning_rate": 5.5672111817428016e-05,
        "epoch": 0.21693121693121692,
        "step": 2911
    },
    {
        "loss": 3.2266,
        "grad_norm": 2.27119779586792,
        "learning_rate": 5.5609048152549794e-05,
        "epoch": 0.21700573813249868,
        "step": 2912
    },
    {
        "loss": 2.6958,
        "grad_norm": 1.9685882329940796,
        "learning_rate": 5.5546006467085344e-05,
        "epoch": 0.21708025933378047,
        "step": 2913
    },
    {
        "loss": 2.3165,
        "grad_norm": 3.1670470237731934,
        "learning_rate": 5.5482986792248725e-05,
        "epoch": 0.21715478053506224,
        "step": 2914
    },
    {
        "loss": 2.2575,
        "grad_norm": 3.399038076400757,
        "learning_rate": 5.541998915924291e-05,
        "epoch": 0.217229301736344,
        "step": 2915
    },
    {
        "loss": 1.8931,
        "grad_norm": 3.4396393299102783,
        "learning_rate": 5.535701359926028e-05,
        "epoch": 0.21730382293762576,
        "step": 2916
    },
    {
        "loss": 2.4104,
        "grad_norm": 2.13325572013855,
        "learning_rate": 5.529406014348194e-05,
        "epoch": 0.21737834413890753,
        "step": 2917
    },
    {
        "loss": 1.9702,
        "grad_norm": 3.432908058166504,
        "learning_rate": 5.523112882307827e-05,
        "epoch": 0.2174528653401893,
        "step": 2918
    },
    {
        "loss": 2.0374,
        "grad_norm": 3.4276070594787598,
        "learning_rate": 5.5168219669208656e-05,
        "epoch": 0.21752738654147105,
        "step": 2919
    },
    {
        "loss": 1.6429,
        "grad_norm": 1.8392534255981445,
        "learning_rate": 5.510533271302141e-05,
        "epoch": 0.21760190774275281,
        "step": 2920
    },
    {
        "loss": 2.7329,
        "grad_norm": 2.4629876613616943,
        "learning_rate": 5.504246798565398e-05,
        "epoch": 0.21767642894403458,
        "step": 2921
    },
    {
        "loss": 2.744,
        "grad_norm": 2.240769386291504,
        "learning_rate": 5.497962551823266e-05,
        "epoch": 0.21775095014531634,
        "step": 2922
    },
    {
        "loss": 2.2301,
        "grad_norm": 2.836367607116699,
        "learning_rate": 5.491680534187297e-05,
        "epoch": 0.2178254713465981,
        "step": 2923
    },
    {
        "loss": 2.9706,
        "grad_norm": 2.8338804244995117,
        "learning_rate": 5.485400748767909e-05,
        "epoch": 0.21789999254787987,
        "step": 2924
    },
    {
        "loss": 1.963,
        "grad_norm": 3.8231892585754395,
        "learning_rate": 5.479123198674436e-05,
        "epoch": 0.21797451374916163,
        "step": 2925
    },
    {
        "loss": 2.3747,
        "grad_norm": 3.261479616165161,
        "learning_rate": 5.472847887015102e-05,
        "epoch": 0.2180490349504434,
        "step": 2926
    },
    {
        "loss": 2.7645,
        "grad_norm": 3.4519898891448975,
        "learning_rate": 5.46657481689701e-05,
        "epoch": 0.21812355615172516,
        "step": 2927
    },
    {
        "loss": 2.7659,
        "grad_norm": 2.647677421569824,
        "learning_rate": 5.460303991426168e-05,
        "epoch": 0.21819807735300692,
        "step": 2928
    },
    {
        "loss": 2.6266,
        "grad_norm": 2.9524149894714355,
        "learning_rate": 5.4540354137074676e-05,
        "epoch": 0.21827259855428868,
        "step": 2929
    },
    {
        "loss": 2.7527,
        "grad_norm": 3.787790298461914,
        "learning_rate": 5.447769086844684e-05,
        "epoch": 0.21834711975557047,
        "step": 2930
    },
    {
        "loss": 2.9538,
        "grad_norm": 1.8649888038635254,
        "learning_rate": 5.441505013940487e-05,
        "epoch": 0.21842164095685224,
        "step": 2931
    },
    {
        "loss": 1.9048,
        "grad_norm": 2.870730400085449,
        "learning_rate": 5.435243198096416e-05,
        "epoch": 0.218496162158134,
        "step": 2932
    },
    {
        "loss": 1.9699,
        "grad_norm": 2.43887996673584,
        "learning_rate": 5.428983642412905e-05,
        "epoch": 0.21857068335941576,
        "step": 2933
    },
    {
        "loss": 2.3701,
        "grad_norm": 2.8336081504821777,
        "learning_rate": 5.422726349989264e-05,
        "epoch": 0.21864520456069753,
        "step": 2934
    },
    {
        "loss": 2.2814,
        "grad_norm": 2.9061505794525146,
        "learning_rate": 5.416471323923689e-05,
        "epoch": 0.2187197257619793,
        "step": 2935
    },
    {
        "loss": 2.6735,
        "grad_norm": 2.380265474319458,
        "learning_rate": 5.410218567313239e-05,
        "epoch": 0.21879424696326105,
        "step": 2936
    },
    {
        "loss": 1.3247,
        "grad_norm": 1.8734794855117798,
        "learning_rate": 5.403968083253863e-05,
        "epoch": 0.21886876816454282,
        "step": 2937
    },
    {
        "loss": 2.4281,
        "grad_norm": 2.861463785171509,
        "learning_rate": 5.397719874840381e-05,
        "epoch": 0.21894328936582458,
        "step": 2938
    },
    {
        "loss": 2.2249,
        "grad_norm": 3.049870729446411,
        "learning_rate": 5.391473945166483e-05,
        "epoch": 0.21901781056710634,
        "step": 2939
    },
    {
        "loss": 1.6644,
        "grad_norm": 3.419429063796997,
        "learning_rate": 5.3852302973247414e-05,
        "epoch": 0.2190923317683881,
        "step": 2940
    },
    {
        "loss": 2.8775,
        "grad_norm": 2.3405473232269287,
        "learning_rate": 5.3789889344065794e-05,
        "epoch": 0.21916685296966987,
        "step": 2941
    },
    {
        "loss": 2.4601,
        "grad_norm": 3.123905897140503,
        "learning_rate": 5.3727498595023086e-05,
        "epoch": 0.21924137417095163,
        "step": 2942
    },
    {
        "loss": 2.2669,
        "grad_norm": 2.790809392929077,
        "learning_rate": 5.366513075701087e-05,
        "epoch": 0.2193158953722334,
        "step": 2943
    },
    {
        "loss": 2.313,
        "grad_norm": 2.769009828567505,
        "learning_rate": 5.3602785860909685e-05,
        "epoch": 0.21939041657351516,
        "step": 2944
    },
    {
        "loss": 2.3753,
        "grad_norm": 2.55281925201416,
        "learning_rate": 5.3540463937588405e-05,
        "epoch": 0.21946493777479692,
        "step": 2945
    },
    {
        "loss": 2.7595,
        "grad_norm": 1.574577808380127,
        "learning_rate": 5.347816501790468e-05,
        "epoch": 0.21953945897607868,
        "step": 2946
    },
    {
        "loss": 2.5817,
        "grad_norm": 2.2606563568115234,
        "learning_rate": 5.341588913270479e-05,
        "epoch": 0.21961398017736045,
        "step": 2947
    },
    {
        "loss": 2.0539,
        "grad_norm": 2.9176812171936035,
        "learning_rate": 5.335363631282345e-05,
        "epoch": 0.21968850137864224,
        "step": 2948
    },
    {
        "loss": 2.5542,
        "grad_norm": 2.05651593208313,
        "learning_rate": 5.329140658908423e-05,
        "epoch": 0.219763022579924,
        "step": 2949
    },
    {
        "loss": 2.5496,
        "grad_norm": 2.298957109451294,
        "learning_rate": 5.322919999229898e-05,
        "epoch": 0.21983754378120576,
        "step": 2950
    },
    {
        "loss": 2.634,
        "grad_norm": 2.828032970428467,
        "learning_rate": 5.316701655326828e-05,
        "epoch": 0.21991206498248753,
        "step": 2951
    },
    {
        "loss": 2.7411,
        "grad_norm": 3.6626102924346924,
        "learning_rate": 5.310485630278119e-05,
        "epoch": 0.2199865861837693,
        "step": 2952
    },
    {
        "loss": 1.8402,
        "grad_norm": 2.616461753845215,
        "learning_rate": 5.3042719271615234e-05,
        "epoch": 0.22006110738505105,
        "step": 2953
    },
    {
        "loss": 2.7563,
        "grad_norm": 2.5014045238494873,
        "learning_rate": 5.298060549053652e-05,
        "epoch": 0.22013562858633282,
        "step": 2954
    },
    {
        "loss": 2.4915,
        "grad_norm": 2.5209012031555176,
        "learning_rate": 5.291851499029963e-05,
        "epoch": 0.22021014978761458,
        "step": 2955
    },
    {
        "loss": 2.4235,
        "grad_norm": 2.5050129890441895,
        "learning_rate": 5.2856447801647614e-05,
        "epoch": 0.22028467098889634,
        "step": 2956
    },
    {
        "loss": 1.5265,
        "grad_norm": 2.9080402851104736,
        "learning_rate": 5.279440395531192e-05,
        "epoch": 0.2203591921901781,
        "step": 2957
    },
    {
        "loss": 2.7325,
        "grad_norm": 3.3693315982818604,
        "learning_rate": 5.2732383482012505e-05,
        "epoch": 0.22043371339145987,
        "step": 2958
    },
    {
        "loss": 2.1341,
        "grad_norm": 3.027123212814331,
        "learning_rate": 5.267038641245776e-05,
        "epoch": 0.22050823459274163,
        "step": 2959
    },
    {
        "loss": 2.2626,
        "grad_norm": 2.9062108993530273,
        "learning_rate": 5.2608412777344486e-05,
        "epoch": 0.2205827557940234,
        "step": 2960
    },
    {
        "loss": 1.4256,
        "grad_norm": 3.690717935562134,
        "learning_rate": 5.2546462607357784e-05,
        "epoch": 0.22065727699530516,
        "step": 2961
    },
    {
        "loss": 1.7688,
        "grad_norm": 2.4774277210235596,
        "learning_rate": 5.248453593317124e-05,
        "epoch": 0.22073179819658692,
        "step": 2962
    },
    {
        "loss": 2.2721,
        "grad_norm": 2.5578529834747314,
        "learning_rate": 5.242263278544684e-05,
        "epoch": 0.22080631939786868,
        "step": 2963
    },
    {
        "loss": 2.5757,
        "grad_norm": 3.281075954437256,
        "learning_rate": 5.2360753194834735e-05,
        "epoch": 0.22088084059915045,
        "step": 2964
    },
    {
        "loss": 2.245,
        "grad_norm": 2.732609748840332,
        "learning_rate": 5.2298897191973675e-05,
        "epoch": 0.2209553618004322,
        "step": 2965
    },
    {
        "loss": 2.638,
        "grad_norm": 1.880459189414978,
        "learning_rate": 5.2237064807490485e-05,
        "epoch": 0.221029883001714,
        "step": 2966
    },
    {
        "loss": 2.0622,
        "grad_norm": 4.09727668762207,
        "learning_rate": 5.217525607200051e-05,
        "epoch": 0.22110440420299576,
        "step": 2967
    },
    {
        "loss": 1.8727,
        "grad_norm": 3.091461181640625,
        "learning_rate": 5.2113471016107154e-05,
        "epoch": 0.22117892540427753,
        "step": 2968
    },
    {
        "loss": 2.4339,
        "grad_norm": 2.390178680419922,
        "learning_rate": 5.205170967040225e-05,
        "epoch": 0.2212534466055593,
        "step": 2969
    },
    {
        "loss": 1.4383,
        "grad_norm": 1.9797204732894897,
        "learning_rate": 5.198997206546596e-05,
        "epoch": 0.22132796780684105,
        "step": 2970
    },
    {
        "loss": 2.1163,
        "grad_norm": 5.09169340133667,
        "learning_rate": 5.192825823186651e-05,
        "epoch": 0.22140248900812282,
        "step": 2971
    },
    {
        "loss": 2.716,
        "grad_norm": 2.213373899459839,
        "learning_rate": 5.1866568200160494e-05,
        "epoch": 0.22147701020940458,
        "step": 2972
    },
    {
        "loss": 2.0864,
        "grad_norm": 2.9959909915924072,
        "learning_rate": 5.180490200089258e-05,
        "epoch": 0.22155153141068634,
        "step": 2973
    },
    {
        "loss": 1.0335,
        "grad_norm": 2.9353790283203125,
        "learning_rate": 5.174325966459577e-05,
        "epoch": 0.2216260526119681,
        "step": 2974
    },
    {
        "loss": 2.1146,
        "grad_norm": 2.337515115737915,
        "learning_rate": 5.1681641221791256e-05,
        "epoch": 0.22170057381324987,
        "step": 2975
    },
    {
        "loss": 2.3036,
        "grad_norm": 3.369683265686035,
        "learning_rate": 5.1620046702988215e-05,
        "epoch": 0.22177509501453163,
        "step": 2976
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.450528860092163,
        "learning_rate": 5.1558476138684276e-05,
        "epoch": 0.2218496162158134,
        "step": 2977
    },
    {
        "loss": 1.8868,
        "grad_norm": 3.803178071975708,
        "learning_rate": 5.14969295593649e-05,
        "epoch": 0.22192413741709516,
        "step": 2978
    },
    {
        "loss": 2.7637,
        "grad_norm": 2.776405096054077,
        "learning_rate": 5.1435406995503886e-05,
        "epoch": 0.22199865861837692,
        "step": 2979
    },
    {
        "loss": 2.6261,
        "grad_norm": 2.1783125400543213,
        "learning_rate": 5.137390847756309e-05,
        "epoch": 0.22207317981965868,
        "step": 2980
    },
    {
        "loss": 1.7411,
        "grad_norm": 2.529639482498169,
        "learning_rate": 5.1312434035992375e-05,
        "epoch": 0.22214770102094045,
        "step": 2981
    },
    {
        "loss": 2.7814,
        "grad_norm": 2.303374767303467,
        "learning_rate": 5.1250983701229806e-05,
        "epoch": 0.2222222222222222,
        "step": 2982
    },
    {
        "loss": 2.8459,
        "grad_norm": 1.9339045286178589,
        "learning_rate": 5.118955750370136e-05,
        "epoch": 0.222296743423504,
        "step": 2983
    },
    {
        "loss": 3.1434,
        "grad_norm": 2.2697887420654297,
        "learning_rate": 5.1128155473821306e-05,
        "epoch": 0.22237126462478576,
        "step": 2984
    },
    {
        "loss": 2.765,
        "grad_norm": 3.7181951999664307,
        "learning_rate": 5.106677764199169e-05,
        "epoch": 0.22244578582606753,
        "step": 2985
    },
    {
        "loss": 2.8772,
        "grad_norm": 5.222378253936768,
        "learning_rate": 5.1005424038602724e-05,
        "epoch": 0.2225203070273493,
        "step": 2986
    },
    {
        "loss": 3.142,
        "grad_norm": 3.058616876602173,
        "learning_rate": 5.094409469403261e-05,
        "epoch": 0.22259482822863105,
        "step": 2987
    },
    {
        "loss": 2.3476,
        "grad_norm": 3.172661781311035,
        "learning_rate": 5.088278963864746e-05,
        "epoch": 0.22266934942991282,
        "step": 2988
    },
    {
        "loss": 2.0184,
        "grad_norm": 2.716282367706299,
        "learning_rate": 5.082150890280143e-05,
        "epoch": 0.22274387063119458,
        "step": 2989
    },
    {
        "loss": 2.3438,
        "grad_norm": 2.403088092803955,
        "learning_rate": 5.0760252516836626e-05,
        "epoch": 0.22281839183247634,
        "step": 2990
    },
    {
        "loss": 2.6985,
        "grad_norm": 2.7305049896240234,
        "learning_rate": 5.069902051108315e-05,
        "epoch": 0.2228929130337581,
        "step": 2991
    },
    {
        "loss": 1.3448,
        "grad_norm": 3.3001487255096436,
        "learning_rate": 5.0637812915858865e-05,
        "epoch": 0.22296743423503987,
        "step": 2992
    },
    {
        "loss": 2.0888,
        "grad_norm": 2.6036860942840576,
        "learning_rate": 5.057662976146971e-05,
        "epoch": 0.22304195543632163,
        "step": 2993
    },
    {
        "loss": 2.4714,
        "grad_norm": 3.0569260120391846,
        "learning_rate": 5.051547107820947e-05,
        "epoch": 0.2231164766376034,
        "step": 2994
    },
    {
        "loss": 2.36,
        "grad_norm": 2.286128520965576,
        "learning_rate": 5.04543368963598e-05,
        "epoch": 0.22319099783888516,
        "step": 2995
    },
    {
        "loss": 2.3009,
        "grad_norm": 3.739271879196167,
        "learning_rate": 5.0393227246190286e-05,
        "epoch": 0.22326551904016692,
        "step": 2996
    },
    {
        "loss": 2.0423,
        "grad_norm": 3.715902805328369,
        "learning_rate": 5.033214215795823e-05,
        "epoch": 0.22334004024144868,
        "step": 2997
    },
    {
        "loss": 2.562,
        "grad_norm": 2.313422441482544,
        "learning_rate": 5.0271081661908904e-05,
        "epoch": 0.22341456144273045,
        "step": 2998
    },
    {
        "loss": 2.8447,
        "grad_norm": 3.389105796813965,
        "learning_rate": 5.0210045788275375e-05,
        "epoch": 0.2234890826440122,
        "step": 2999
    },
    {
        "loss": 2.5485,
        "grad_norm": 2.803271532058716,
        "learning_rate": 5.0149034567278466e-05,
        "epoch": 0.22356360384529397,
        "step": 3000
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.356205701828003,
        "learning_rate": 5.008804802912689e-05,
        "epoch": 0.22363812504657576,
        "step": 3001
    },
    {
        "loss": 2.7618,
        "grad_norm": 2.3258113861083984,
        "learning_rate": 5.0027086204017e-05,
        "epoch": 0.22371264624785753,
        "step": 3002
    },
    {
        "loss": 2.6214,
        "grad_norm": 3.6221346855163574,
        "learning_rate": 4.9966149122133074e-05,
        "epoch": 0.2237871674491393,
        "step": 3003
    },
    {
        "loss": 2.2801,
        "grad_norm": 2.373879909515381,
        "learning_rate": 4.990523681364694e-05,
        "epoch": 0.22386168865042105,
        "step": 3004
    },
    {
        "loss": 1.8388,
        "grad_norm": 3.0866289138793945,
        "learning_rate": 4.984434930871842e-05,
        "epoch": 0.22393620985170282,
        "step": 3005
    },
    {
        "loss": 2.9989,
        "grad_norm": 2.770850896835327,
        "learning_rate": 4.97834866374948e-05,
        "epoch": 0.22401073105298458,
        "step": 3006
    },
    {
        "loss": 1.828,
        "grad_norm": 4.103948593139648,
        "learning_rate": 4.9722648830111216e-05,
        "epoch": 0.22408525225426634,
        "step": 3007
    },
    {
        "loss": 2.3831,
        "grad_norm": 2.3482372760772705,
        "learning_rate": 4.966183591669051e-05,
        "epoch": 0.2241597734555481,
        "step": 3008
    },
    {
        "loss": 1.4144,
        "grad_norm": 2.6764745712280273,
        "learning_rate": 4.960104792734304e-05,
        "epoch": 0.22423429465682987,
        "step": 3009
    },
    {
        "loss": 2.7146,
        "grad_norm": 2.155688762664795,
        "learning_rate": 4.9540284892167044e-05,
        "epoch": 0.22430881585811163,
        "step": 3010
    },
    {
        "loss": 2.6791,
        "grad_norm": 3.201058864593506,
        "learning_rate": 4.947954684124821e-05,
        "epoch": 0.2243833370593934,
        "step": 3011
    },
    {
        "loss": 2.2292,
        "grad_norm": 4.8360090255737305,
        "learning_rate": 4.941883380466e-05,
        "epoch": 0.22445785826067516,
        "step": 3012
    },
    {
        "loss": 2.5621,
        "grad_norm": 2.3467206954956055,
        "learning_rate": 4.935814581246335e-05,
        "epoch": 0.22453237946195692,
        "step": 3013
    },
    {
        "loss": 2.5517,
        "grad_norm": 1.6911214590072632,
        "learning_rate": 4.9297482894706914e-05,
        "epoch": 0.22460690066323868,
        "step": 3014
    },
    {
        "loss": 1.99,
        "grad_norm": 3.321600914001465,
        "learning_rate": 4.923684508142688e-05,
        "epoch": 0.22468142186452045,
        "step": 3015
    },
    {
        "loss": 2.7543,
        "grad_norm": 1.9506564140319824,
        "learning_rate": 4.917623240264704e-05,
        "epoch": 0.2247559430658022,
        "step": 3016
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.2822072505950928,
        "learning_rate": 4.911564488837872e-05,
        "epoch": 0.22483046426708397,
        "step": 3017
    },
    {
        "loss": 2.3171,
        "grad_norm": 4.615066051483154,
        "learning_rate": 4.905508256862073e-05,
        "epoch": 0.22490498546836574,
        "step": 3018
    },
    {
        "loss": 2.2078,
        "grad_norm": 1.8459495306015015,
        "learning_rate": 4.899454547335948e-05,
        "epoch": 0.22497950666964753,
        "step": 3019
    },
    {
        "loss": 1.9705,
        "grad_norm": 2.6497950553894043,
        "learning_rate": 4.8934033632568874e-05,
        "epoch": 0.2250540278709293,
        "step": 3020
    },
    {
        "loss": 2.0471,
        "grad_norm": 2.7037267684936523,
        "learning_rate": 4.887354707621035e-05,
        "epoch": 0.22512854907221105,
        "step": 3021
    },
    {
        "loss": 1.9162,
        "grad_norm": 2.8404829502105713,
        "learning_rate": 4.8813085834232695e-05,
        "epoch": 0.22520307027349282,
        "step": 3022
    },
    {
        "loss": 2.6021,
        "grad_norm": 1.815799355506897,
        "learning_rate": 4.8752649936572304e-05,
        "epoch": 0.22527759147477458,
        "step": 3023
    },
    {
        "loss": 2.3252,
        "grad_norm": 2.5664961338043213,
        "learning_rate": 4.8692239413152986e-05,
        "epoch": 0.22535211267605634,
        "step": 3024
    },
    {
        "loss": 2.502,
        "grad_norm": 2.4952824115753174,
        "learning_rate": 4.863185429388588e-05,
        "epoch": 0.2254266338773381,
        "step": 3025
    },
    {
        "loss": 2.3288,
        "grad_norm": 3.1971194744110107,
        "learning_rate": 4.857149460866976e-05,
        "epoch": 0.22550115507861987,
        "step": 3026
    },
    {
        "loss": 2.3443,
        "grad_norm": 2.133070468902588,
        "learning_rate": 4.851116038739059e-05,
        "epoch": 0.22557567627990163,
        "step": 3027
    },
    {
        "loss": 3.0476,
        "grad_norm": 2.0677757263183594,
        "learning_rate": 4.845085165992189e-05,
        "epoch": 0.2256501974811834,
        "step": 3028
    },
    {
        "loss": 2.6966,
        "grad_norm": 1.8536906242370605,
        "learning_rate": 4.83905684561244e-05,
        "epoch": 0.22572471868246516,
        "step": 3029
    },
    {
        "loss": 2.3418,
        "grad_norm": 3.5218448638916016,
        "learning_rate": 4.833031080584631e-05,
        "epoch": 0.22579923988374692,
        "step": 3030
    },
    {
        "loss": 2.4149,
        "grad_norm": 2.9120943546295166,
        "learning_rate": 4.827007873892333e-05,
        "epoch": 0.22587376108502868,
        "step": 3031
    },
    {
        "loss": 2.0244,
        "grad_norm": 3.7406933307647705,
        "learning_rate": 4.820987228517807e-05,
        "epoch": 0.22594828228631045,
        "step": 3032
    },
    {
        "loss": 2.2937,
        "grad_norm": 3.187744379043579,
        "learning_rate": 4.8149691474420924e-05,
        "epoch": 0.2260228034875922,
        "step": 3033
    },
    {
        "loss": 2.6169,
        "grad_norm": 2.600590229034424,
        "learning_rate": 4.808953633644927e-05,
        "epoch": 0.22609732468887397,
        "step": 3034
    },
    {
        "loss": 2.9434,
        "grad_norm": 2.853023052215576,
        "learning_rate": 4.8029406901047914e-05,
        "epoch": 0.22617184589015574,
        "step": 3035
    },
    {
        "loss": 2.7129,
        "grad_norm": 1.8035322427749634,
        "learning_rate": 4.796930319798895e-05,
        "epoch": 0.2262463670914375,
        "step": 3036
    },
    {
        "loss": 2.5004,
        "grad_norm": 3.961851119995117,
        "learning_rate": 4.7909225257031565e-05,
        "epoch": 0.2263208882927193,
        "step": 3037
    },
    {
        "loss": 1.7034,
        "grad_norm": 2.8593146800994873,
        "learning_rate": 4.784917310792253e-05,
        "epoch": 0.22639540949400105,
        "step": 3038
    },
    {
        "loss": 1.9051,
        "grad_norm": 2.7573835849761963,
        "learning_rate": 4.778914678039538e-05,
        "epoch": 0.22646993069528282,
        "step": 3039
    },
    {
        "loss": 2.6747,
        "grad_norm": 1.8757354021072388,
        "learning_rate": 4.772914630417128e-05,
        "epoch": 0.22654445189656458,
        "step": 3040
    },
    {
        "loss": 2.5375,
        "grad_norm": 2.259547710418701,
        "learning_rate": 4.766917170895843e-05,
        "epoch": 0.22661897309784634,
        "step": 3041
    },
    {
        "loss": 2.0385,
        "grad_norm": 4.393761157989502,
        "learning_rate": 4.760922302445212e-05,
        "epoch": 0.2266934942991281,
        "step": 3042
    },
    {
        "loss": 1.9258,
        "grad_norm": 3.180283784866333,
        "learning_rate": 4.754930028033501e-05,
        "epoch": 0.22676801550040987,
        "step": 3043
    },
    {
        "loss": 2.4588,
        "grad_norm": 1.3931851387023926,
        "learning_rate": 4.748940350627669e-05,
        "epoch": 0.22684253670169163,
        "step": 3044
    },
    {
        "loss": 2.5797,
        "grad_norm": 2.5594515800476074,
        "learning_rate": 4.742953273193416e-05,
        "epoch": 0.2269170579029734,
        "step": 3045
    },
    {
        "loss": 2.658,
        "grad_norm": 1.5145235061645508,
        "learning_rate": 4.736968798695128e-05,
        "epoch": 0.22699157910425516,
        "step": 3046
    },
    {
        "loss": 2.6776,
        "grad_norm": 3.3919875621795654,
        "learning_rate": 4.730986930095921e-05,
        "epoch": 0.22706610030553692,
        "step": 3047
    },
    {
        "loss": 2.0192,
        "grad_norm": 3.718924045562744,
        "learning_rate": 4.725007670357614e-05,
        "epoch": 0.22714062150681869,
        "step": 3048
    },
    {
        "loss": 1.4014,
        "grad_norm": 3.17043137550354,
        "learning_rate": 4.7190310224407266e-05,
        "epoch": 0.22721514270810045,
        "step": 3049
    },
    {
        "loss": 2.0516,
        "grad_norm": 2.8939626216888428,
        "learning_rate": 4.713056989304504e-05,
        "epoch": 0.2272896639093822,
        "step": 3050
    },
    {
        "loss": 1.8044,
        "grad_norm": 3.261481285095215,
        "learning_rate": 4.7070855739068787e-05,
        "epoch": 0.22736418511066397,
        "step": 3051
    },
    {
        "loss": 2.3473,
        "grad_norm": 2.7268946170806885,
        "learning_rate": 4.7011167792044976e-05,
        "epoch": 0.22743870631194574,
        "step": 3052
    },
    {
        "loss": 2.6805,
        "grad_norm": 2.3985791206359863,
        "learning_rate": 4.695150608152702e-05,
        "epoch": 0.2275132275132275,
        "step": 3053
    },
    {
        "loss": 2.0759,
        "grad_norm": 3.046679973602295,
        "learning_rate": 4.68918706370554e-05,
        "epoch": 0.2275877487145093,
        "step": 3054
    },
    {
        "loss": 2.6502,
        "grad_norm": 2.252484083175659,
        "learning_rate": 4.683226148815758e-05,
        "epoch": 0.22766226991579105,
        "step": 3055
    },
    {
        "loss": 2.4396,
        "grad_norm": 2.4054229259490967,
        "learning_rate": 4.6772678664348025e-05,
        "epoch": 0.22773679111707282,
        "step": 3056
    },
    {
        "loss": 2.1943,
        "grad_norm": 4.0222978591918945,
        "learning_rate": 4.6713122195128146e-05,
        "epoch": 0.22781131231835458,
        "step": 3057
    },
    {
        "loss": 2.3436,
        "grad_norm": 3.269190549850464,
        "learning_rate": 4.6653592109986254e-05,
        "epoch": 0.22788583351963634,
        "step": 3058
    },
    {
        "loss": 2.6481,
        "grad_norm": 2.1351940631866455,
        "learning_rate": 4.65940884383977e-05,
        "epoch": 0.2279603547209181,
        "step": 3059
    },
    {
        "loss": 2.4295,
        "grad_norm": 3.5529630184173584,
        "learning_rate": 4.65346112098246e-05,
        "epoch": 0.22803487592219987,
        "step": 3060
    },
    {
        "loss": 2.4689,
        "grad_norm": 3.0297255516052246,
        "learning_rate": 4.64751604537162e-05,
        "epoch": 0.22810939712348163,
        "step": 3061
    },
    {
        "loss": 1.2585,
        "grad_norm": 3.217233180999756,
        "learning_rate": 4.6415736199508455e-05,
        "epoch": 0.2281839183247634,
        "step": 3062
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.9089975357055664,
        "learning_rate": 4.635633847662425e-05,
        "epoch": 0.22825843952604516,
        "step": 3063
    },
    {
        "loss": 2.2822,
        "grad_norm": 3.2391717433929443,
        "learning_rate": 4.6296967314473406e-05,
        "epoch": 0.22833296072732692,
        "step": 3064
    },
    {
        "loss": 2.6567,
        "grad_norm": 3.397944211959839,
        "learning_rate": 4.623762274245239e-05,
        "epoch": 0.22840748192860869,
        "step": 3065
    },
    {
        "loss": 2.645,
        "grad_norm": 2.310681104660034,
        "learning_rate": 4.6178304789944826e-05,
        "epoch": 0.22848200312989045,
        "step": 3066
    },
    {
        "loss": 1.8855,
        "grad_norm": 3.2286274433135986,
        "learning_rate": 4.611901348632085e-05,
        "epoch": 0.2285565243311722,
        "step": 3067
    },
    {
        "loss": 2.6864,
        "grad_norm": 2.0171825885772705,
        "learning_rate": 4.605974886093755e-05,
        "epoch": 0.22863104553245397,
        "step": 3068
    },
    {
        "loss": 2.0791,
        "grad_norm": 2.573716163635254,
        "learning_rate": 4.600051094313883e-05,
        "epoch": 0.22870556673373574,
        "step": 3069
    },
    {
        "loss": 1.6793,
        "grad_norm": 4.902868270874023,
        "learning_rate": 4.594129976225522e-05,
        "epoch": 0.2287800879350175,
        "step": 3070
    },
    {
        "loss": 2.0579,
        "grad_norm": 3.232243299484253,
        "learning_rate": 4.588211534760426e-05,
        "epoch": 0.22885460913629926,
        "step": 3071
    },
    {
        "loss": 2.7428,
        "grad_norm": 2.58703875541687,
        "learning_rate": 4.582295772848997e-05,
        "epoch": 0.22892913033758105,
        "step": 3072
    },
    {
        "loss": 1.1449,
        "grad_norm": 3.8895111083984375,
        "learning_rate": 4.5763826934203305e-05,
        "epoch": 0.22900365153886282,
        "step": 3073
    },
    {
        "loss": 2.2208,
        "grad_norm": 2.8962435722351074,
        "learning_rate": 4.5704722994021776e-05,
        "epoch": 0.22907817274014458,
        "step": 3074
    },
    {
        "loss": 2.4059,
        "grad_norm": 3.680692195892334,
        "learning_rate": 4.5645645937209734e-05,
        "epoch": 0.22915269394142634,
        "step": 3075
    },
    {
        "loss": 2.235,
        "grad_norm": 4.019197940826416,
        "learning_rate": 4.558659579301815e-05,
        "epoch": 0.2292272151427081,
        "step": 3076
    },
    {
        "loss": 2.0788,
        "grad_norm": 2.8565638065338135,
        "learning_rate": 4.552757259068468e-05,
        "epoch": 0.22930173634398987,
        "step": 3077
    },
    {
        "loss": 2.7888,
        "grad_norm": 2.5817275047302246,
        "learning_rate": 4.546857635943369e-05,
        "epoch": 0.22937625754527163,
        "step": 3078
    },
    {
        "loss": 2.2019,
        "grad_norm": 3.8524556159973145,
        "learning_rate": 4.540960712847607e-05,
        "epoch": 0.2294507787465534,
        "step": 3079
    },
    {
        "loss": 1.8846,
        "grad_norm": 2.9397857189178467,
        "learning_rate": 4.53506649270095e-05,
        "epoch": 0.22952529994783516,
        "step": 3080
    },
    {
        "loss": 2.6335,
        "grad_norm": 2.963653802871704,
        "learning_rate": 4.5291749784218074e-05,
        "epoch": 0.22959982114911692,
        "step": 3081
    },
    {
        "loss": 2.2824,
        "grad_norm": 2.6752681732177734,
        "learning_rate": 4.523286172927275e-05,
        "epoch": 0.22967434235039869,
        "step": 3082
    },
    {
        "loss": 2.8448,
        "grad_norm": 2.2221908569335938,
        "learning_rate": 4.5174000791330825e-05,
        "epoch": 0.22974886355168045,
        "step": 3083
    },
    {
        "loss": 2.2904,
        "grad_norm": 2.0455286502838135,
        "learning_rate": 4.5115166999536315e-05,
        "epoch": 0.2298233847529622,
        "step": 3084
    },
    {
        "loss": 2.3423,
        "grad_norm": 2.9423515796661377,
        "learning_rate": 4.50563603830198e-05,
        "epoch": 0.22989790595424398,
        "step": 3085
    },
    {
        "loss": 2.4204,
        "grad_norm": 1.413061261177063,
        "learning_rate": 4.4997580970898244e-05,
        "epoch": 0.22997242715552574,
        "step": 3086
    },
    {
        "loss": 2.343,
        "grad_norm": 3.5439343452453613,
        "learning_rate": 4.493882879227541e-05,
        "epoch": 0.2300469483568075,
        "step": 3087
    },
    {
        "loss": 2.6588,
        "grad_norm": 3.2149806022644043,
        "learning_rate": 4.4880103876241305e-05,
        "epoch": 0.23012146955808926,
        "step": 3088
    },
    {
        "loss": 1.8847,
        "grad_norm": 3.3839828968048096,
        "learning_rate": 4.4821406251872654e-05,
        "epoch": 0.23019599075937103,
        "step": 3089
    },
    {
        "loss": 2.0162,
        "grad_norm": 2.666053295135498,
        "learning_rate": 4.476273594823247e-05,
        "epoch": 0.23027051196065282,
        "step": 3090
    },
    {
        "loss": 2.4571,
        "grad_norm": 2.577069044113159,
        "learning_rate": 4.470409299437036e-05,
        "epoch": 0.23034503316193458,
        "step": 3091
    },
    {
        "loss": 2.2234,
        "grad_norm": 2.0687739849090576,
        "learning_rate": 4.4645477419322524e-05,
        "epoch": 0.23041955436321634,
        "step": 3092
    },
    {
        "loss": 2.2902,
        "grad_norm": 3.804152011871338,
        "learning_rate": 4.4586889252111206e-05,
        "epoch": 0.2304940755644981,
        "step": 3093
    },
    {
        "loss": 2.0885,
        "grad_norm": 1.6600476503372192,
        "learning_rate": 4.452832852174553e-05,
        "epoch": 0.23056859676577987,
        "step": 3094
    },
    {
        "loss": 2.7956,
        "grad_norm": 1.9959213733673096,
        "learning_rate": 4.4469795257220716e-05,
        "epoch": 0.23064311796706163,
        "step": 3095
    },
    {
        "loss": 2.1833,
        "grad_norm": 1.4990904331207275,
        "learning_rate": 4.441128948751854e-05,
        "epoch": 0.2307176391683434,
        "step": 3096
    },
    {
        "loss": 2.0009,
        "grad_norm": 3.050069570541382,
        "learning_rate": 4.435281124160715e-05,
        "epoch": 0.23079216036962516,
        "step": 3097
    },
    {
        "loss": 1.4329,
        "grad_norm": 3.5257155895233154,
        "learning_rate": 4.429436054844095e-05,
        "epoch": 0.23086668157090692,
        "step": 3098
    },
    {
        "loss": 2.5892,
        "grad_norm": 2.0576236248016357,
        "learning_rate": 4.423593743696095e-05,
        "epoch": 0.2309412027721887,
        "step": 3099
    },
    {
        "loss": 1.9637,
        "grad_norm": 3.7352309226989746,
        "learning_rate": 4.4177541936094146e-05,
        "epoch": 0.23101572397347045,
        "step": 3100
    },
    {
        "loss": 2.7974,
        "grad_norm": 2.954876661300659,
        "learning_rate": 4.4119174074754234e-05,
        "epoch": 0.2310902451747522,
        "step": 3101
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.748908758163452,
        "learning_rate": 4.4060833881840947e-05,
        "epoch": 0.23116476637603398,
        "step": 3102
    },
    {
        "loss": 2.8596,
        "grad_norm": 3.4991466999053955,
        "learning_rate": 4.4002521386240466e-05,
        "epoch": 0.23123928757731574,
        "step": 3103
    },
    {
        "loss": 3.1503,
        "grad_norm": 3.1691958904266357,
        "learning_rate": 4.394423661682525e-05,
        "epoch": 0.2313138087785975,
        "step": 3104
    },
    {
        "loss": 2.0378,
        "grad_norm": 2.764157772064209,
        "learning_rate": 4.388597960245389e-05,
        "epoch": 0.23138832997987926,
        "step": 3105
    },
    {
        "loss": 2.7913,
        "grad_norm": 2.686143636703491,
        "learning_rate": 4.3827750371971473e-05,
        "epoch": 0.23146285118116103,
        "step": 3106
    },
    {
        "loss": 2.5539,
        "grad_norm": 2.8707430362701416,
        "learning_rate": 4.376954895420912e-05,
        "epoch": 0.23153737238244282,
        "step": 3107
    },
    {
        "loss": 1.7457,
        "grad_norm": 3.096280336380005,
        "learning_rate": 4.37113753779843e-05,
        "epoch": 0.23161189358372458,
        "step": 3108
    },
    {
        "loss": 2.6121,
        "grad_norm": 2.5099117755889893,
        "learning_rate": 4.365322967210058e-05,
        "epoch": 0.23168641478500634,
        "step": 3109
    },
    {
        "loss": 2.1609,
        "grad_norm": 4.165585041046143,
        "learning_rate": 4.3595111865347835e-05,
        "epoch": 0.2317609359862881,
        "step": 3110
    },
    {
        "loss": 1.6925,
        "grad_norm": 3.744560480117798,
        "learning_rate": 4.353702198650218e-05,
        "epoch": 0.23183545718756987,
        "step": 3111
    },
    {
        "loss": 1.7346,
        "grad_norm": 3.2799742221832275,
        "learning_rate": 4.3478960064325714e-05,
        "epoch": 0.23190997838885163,
        "step": 3112
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.5196480751037598,
        "learning_rate": 4.342092612756686e-05,
        "epoch": 0.2319844995901334,
        "step": 3113
    },
    {
        "loss": 2.4849,
        "grad_norm": 3.086345911026001,
        "learning_rate": 4.336292020496006e-05,
        "epoch": 0.23205902079141516,
        "step": 3114
    },
    {
        "loss": 2.5341,
        "grad_norm": 3.0182998180389404,
        "learning_rate": 4.330494232522597e-05,
        "epoch": 0.23213354199269692,
        "step": 3115
    },
    {
        "loss": 2.7372,
        "grad_norm": 3.3511359691619873,
        "learning_rate": 4.324699251707135e-05,
        "epoch": 0.2322080631939787,
        "step": 3116
    },
    {
        "loss": 1.7429,
        "grad_norm": 2.748410940170288,
        "learning_rate": 4.3189070809189034e-05,
        "epoch": 0.23228258439526045,
        "step": 3117
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.2625465393066406,
        "learning_rate": 4.3131177230258e-05,
        "epoch": 0.2323571055965422,
        "step": 3118
    },
    {
        "loss": 2.0604,
        "grad_norm": 2.6645405292510986,
        "learning_rate": 4.307331180894319e-05,
        "epoch": 0.23243162679782398,
        "step": 3119
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.620288372039795,
        "learning_rate": 4.301547457389572e-05,
        "epoch": 0.23250614799910574,
        "step": 3120
    },
    {
        "loss": 2.5218,
        "grad_norm": 3.0252392292022705,
        "learning_rate": 4.2957665553752615e-05,
        "epoch": 0.2325806692003875,
        "step": 3121
    },
    {
        "loss": 2.0767,
        "grad_norm": 3.0896646976470947,
        "learning_rate": 4.289988477713713e-05,
        "epoch": 0.23265519040166927,
        "step": 3122
    },
    {
        "loss": 2.4595,
        "grad_norm": 1.4463759660720825,
        "learning_rate": 4.284213227265834e-05,
        "epoch": 0.23272971160295103,
        "step": 3123
    },
    {
        "loss": 2.5042,
        "grad_norm": 2.5251805782318115,
        "learning_rate": 4.278440806891142e-05,
        "epoch": 0.2328042328042328,
        "step": 3124
    },
    {
        "loss": 2.2573,
        "grad_norm": 3.2385706901550293,
        "learning_rate": 4.272671219447755e-05,
        "epoch": 0.23287875400551458,
        "step": 3125
    },
    {
        "loss": 2.7726,
        "grad_norm": 3.006082773208618,
        "learning_rate": 4.266904467792374e-05,
        "epoch": 0.23295327520679635,
        "step": 3126
    },
    {
        "loss": 2.2097,
        "grad_norm": 4.110421180725098,
        "learning_rate": 4.261140554780322e-05,
        "epoch": 0.2330277964080781,
        "step": 3127
    },
    {
        "loss": 2.5642,
        "grad_norm": 2.24717116355896,
        "learning_rate": 4.255379483265488e-05,
        "epoch": 0.23310231760935987,
        "step": 3128
    },
    {
        "loss": 2.3346,
        "grad_norm": 2.1255342960357666,
        "learning_rate": 4.2496212561003766e-05,
        "epoch": 0.23317683881064163,
        "step": 3129
    },
    {
        "loss": 2.8067,
        "grad_norm": 2.9012019634246826,
        "learning_rate": 4.243865876136067e-05,
        "epoch": 0.2332513600119234,
        "step": 3130
    },
    {
        "loss": 2.5735,
        "grad_norm": 2.1654837131500244,
        "learning_rate": 4.2381133462222386e-05,
        "epoch": 0.23332588121320516,
        "step": 3131
    },
    {
        "loss": 1.5551,
        "grad_norm": 3.856694221496582,
        "learning_rate": 4.23236366920716e-05,
        "epoch": 0.23340040241448692,
        "step": 3132
    },
    {
        "loss": 1.9222,
        "grad_norm": 3.322843313217163,
        "learning_rate": 4.226616847937681e-05,
        "epoch": 0.2334749236157687,
        "step": 3133
    },
    {
        "loss": 1.5542,
        "grad_norm": 1.892849326133728,
        "learning_rate": 4.220872885259247e-05,
        "epoch": 0.23354944481705045,
        "step": 3134
    },
    {
        "loss": 2.074,
        "grad_norm": 2.8122811317443848,
        "learning_rate": 4.215131784015873e-05,
        "epoch": 0.2336239660183322,
        "step": 3135
    },
    {
        "loss": 2.1855,
        "grad_norm": 3.4777345657348633,
        "learning_rate": 4.209393547050172e-05,
        "epoch": 0.23369848721961398,
        "step": 3136
    },
    {
        "loss": 2.4756,
        "grad_norm": 2.641573667526245,
        "learning_rate": 4.203658177203332e-05,
        "epoch": 0.23377300842089574,
        "step": 3137
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.7777230739593506,
        "learning_rate": 4.197925677315122e-05,
        "epoch": 0.2338475296221775,
        "step": 3138
    },
    {
        "loss": 1.6922,
        "grad_norm": 3.3051810264587402,
        "learning_rate": 4.192196050223894e-05,
        "epoch": 0.23392205082345927,
        "step": 3139
    },
    {
        "loss": 2.5738,
        "grad_norm": 3.0367729663848877,
        "learning_rate": 4.186469298766569e-05,
        "epoch": 0.23399657202474103,
        "step": 3140
    },
    {
        "loss": 2.9097,
        "grad_norm": 1.6106736660003662,
        "learning_rate": 4.1807454257786525e-05,
        "epoch": 0.2340710932260228,
        "step": 3141
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.7737927436828613,
        "learning_rate": 4.175024434094214e-05,
        "epoch": 0.23414561442730455,
        "step": 3142
    },
    {
        "loss": 2.3727,
        "grad_norm": 1.8571746349334717,
        "learning_rate": 4.169306326545915e-05,
        "epoch": 0.23422013562858635,
        "step": 3143
    },
    {
        "loss": 2.3071,
        "grad_norm": 3.33886456489563,
        "learning_rate": 4.16359110596497e-05,
        "epoch": 0.2342946568298681,
        "step": 3144
    },
    {
        "loss": 2.5805,
        "grad_norm": 2.7717297077178955,
        "learning_rate": 4.157878775181174e-05,
        "epoch": 0.23436917803114987,
        "step": 3145
    },
    {
        "loss": 2.1802,
        "grad_norm": 2.1068661212921143,
        "learning_rate": 4.152169337022893e-05,
        "epoch": 0.23444369923243163,
        "step": 3146
    },
    {
        "loss": 2.4309,
        "grad_norm": 2.575613260269165,
        "learning_rate": 4.1464627943170465e-05,
        "epoch": 0.2345182204337134,
        "step": 3147
    },
    {
        "loss": 1.8645,
        "grad_norm": 3.10078763961792,
        "learning_rate": 4.140759149889147e-05,
        "epoch": 0.23459274163499516,
        "step": 3148
    },
    {
        "loss": 1.9474,
        "grad_norm": 2.5958588123321533,
        "learning_rate": 4.135058406563236e-05,
        "epoch": 0.23466726283627692,
        "step": 3149
    },
    {
        "loss": 2.6598,
        "grad_norm": 2.145291566848755,
        "learning_rate": 4.129360567161954e-05,
        "epoch": 0.2347417840375587,
        "step": 3150
    },
    {
        "loss": 2.6242,
        "grad_norm": 3.1228177547454834,
        "learning_rate": 4.1236656345064794e-05,
        "epoch": 0.23481630523884045,
        "step": 3151
    },
    {
        "loss": 2.1049,
        "grad_norm": 3.114147424697876,
        "learning_rate": 4.1179736114165615e-05,
        "epoch": 0.2348908264401222,
        "step": 3152
    },
    {
        "loss": 1.4584,
        "grad_norm": 2.05574631690979,
        "learning_rate": 4.112284500710512e-05,
        "epoch": 0.23496534764140398,
        "step": 3153
    },
    {
        "loss": 2.7076,
        "grad_norm": 2.1391665935516357,
        "learning_rate": 4.106598305205185e-05,
        "epoch": 0.23503986884268574,
        "step": 3154
    },
    {
        "loss": 2.7875,
        "grad_norm": 2.590395450592041,
        "learning_rate": 4.1009150277160155e-05,
        "epoch": 0.2351143900439675,
        "step": 3155
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.3886780738830566,
        "learning_rate": 4.095234671056971e-05,
        "epoch": 0.23518891124524927,
        "step": 3156
    },
    {
        "loss": 2.386,
        "grad_norm": 3.529953718185425,
        "learning_rate": 4.089557238040584e-05,
        "epoch": 0.23526343244653103,
        "step": 3157
    },
    {
        "loss": 2.3374,
        "grad_norm": 3.3941898345947266,
        "learning_rate": 4.083882731477944e-05,
        "epoch": 0.2353379536478128,
        "step": 3158
    },
    {
        "loss": 2.608,
        "grad_norm": 2.0294785499572754,
        "learning_rate": 4.07821115417867e-05,
        "epoch": 0.23541247484909456,
        "step": 3159
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.0256645679473877,
        "learning_rate": 4.0725425089509675e-05,
        "epoch": 0.23548699605037632,
        "step": 3160
    },
    {
        "loss": 2.7815,
        "grad_norm": 1.9129772186279297,
        "learning_rate": 4.0668767986015444e-05,
        "epoch": 0.2355615172516581,
        "step": 3161
    },
    {
        "loss": 2.8653,
        "grad_norm": 6.8990888595581055,
        "learning_rate": 4.061214025935698e-05,
        "epoch": 0.23563603845293987,
        "step": 3162
    },
    {
        "loss": 2.5654,
        "grad_norm": 2.7282795906066895,
        "learning_rate": 4.0555541937572415e-05,
        "epoch": 0.23571055965422164,
        "step": 3163
    },
    {
        "loss": 2.0932,
        "grad_norm": 3.4019970893859863,
        "learning_rate": 4.049897304868549e-05,
        "epoch": 0.2357850808555034,
        "step": 3164
    },
    {
        "loss": 2.6548,
        "grad_norm": 1.920440435409546,
        "learning_rate": 4.044243362070531e-05,
        "epoch": 0.23585960205678516,
        "step": 3165
    },
    {
        "loss": 2.4002,
        "grad_norm": 2.7390074729919434,
        "learning_rate": 4.038592368162632e-05,
        "epoch": 0.23593412325806692,
        "step": 3166
    },
    {
        "loss": 2.2835,
        "grad_norm": 3.030576705932617,
        "learning_rate": 4.032944325942856e-05,
        "epoch": 0.2360086444593487,
        "step": 3167
    },
    {
        "loss": 2.262,
        "grad_norm": 3.100067138671875,
        "learning_rate": 4.0272992382077254e-05,
        "epoch": 0.23608316566063045,
        "step": 3168
    },
    {
        "loss": 2.8271,
        "grad_norm": 2.045557975769043,
        "learning_rate": 4.021657107752314e-05,
        "epoch": 0.2361576868619122,
        "step": 3169
    },
    {
        "loss": 2.2751,
        "grad_norm": 4.101467609405518,
        "learning_rate": 4.016017937370218e-05,
        "epoch": 0.23623220806319398,
        "step": 3170
    },
    {
        "loss": 2.1868,
        "grad_norm": 2.7779812812805176,
        "learning_rate": 4.0103817298535794e-05,
        "epoch": 0.23630672926447574,
        "step": 3171
    },
    {
        "loss": 2.4616,
        "grad_norm": 3.2875421047210693,
        "learning_rate": 4.00474848799307e-05,
        "epoch": 0.2363812504657575,
        "step": 3172
    },
    {
        "loss": 1.9998,
        "grad_norm": 2.7087132930755615,
        "learning_rate": 3.999118214577889e-05,
        "epoch": 0.23645577166703927,
        "step": 3173
    },
    {
        "loss": 2.3974,
        "grad_norm": 3.00529146194458,
        "learning_rate": 3.993490912395775e-05,
        "epoch": 0.23653029286832103,
        "step": 3174
    },
    {
        "loss": 3.0441,
        "grad_norm": 1.9365168809890747,
        "learning_rate": 3.987866584232984e-05,
        "epoch": 0.2366048140696028,
        "step": 3175
    },
    {
        "loss": 1.7638,
        "grad_norm": 3.671926975250244,
        "learning_rate": 3.9822452328743084e-05,
        "epoch": 0.23667933527088456,
        "step": 3176
    },
    {
        "loss": 2.136,
        "grad_norm": 2.923515796661377,
        "learning_rate": 3.9766268611030566e-05,
        "epoch": 0.23675385647216632,
        "step": 3177
    },
    {
        "loss": 2.7981,
        "grad_norm": 2.2746312618255615,
        "learning_rate": 3.971011471701079e-05,
        "epoch": 0.2368283776734481,
        "step": 3178
    },
    {
        "loss": 2.4468,
        "grad_norm": 2.3256187438964844,
        "learning_rate": 3.965399067448732e-05,
        "epoch": 0.23690289887472987,
        "step": 3179
    },
    {
        "loss": 2.1725,
        "grad_norm": 3.807833671569824,
        "learning_rate": 3.9597896511249e-05,
        "epoch": 0.23697742007601164,
        "step": 3180
    },
    {
        "loss": 2.5221,
        "grad_norm": 2.2233073711395264,
        "learning_rate": 3.9541832255069954e-05,
        "epoch": 0.2370519412772934,
        "step": 3181
    },
    {
        "loss": 2.3295,
        "grad_norm": 2.449693202972412,
        "learning_rate": 3.948579793370931e-05,
        "epoch": 0.23712646247857516,
        "step": 3182
    },
    {
        "loss": 2.1369,
        "grad_norm": 3.126406192779541,
        "learning_rate": 3.942979357491163e-05,
        "epoch": 0.23720098367985692,
        "step": 3183
    },
    {
        "loss": 2.5148,
        "grad_norm": 2.6620936393737793,
        "learning_rate": 3.937381920640642e-05,
        "epoch": 0.2372755048811387,
        "step": 3184
    },
    {
        "loss": 1.994,
        "grad_norm": 3.518596887588501,
        "learning_rate": 3.931787485590846e-05,
        "epoch": 0.23735002608242045,
        "step": 3185
    },
    {
        "loss": 2.3574,
        "grad_norm": 2.890709161758423,
        "learning_rate": 3.926196055111764e-05,
        "epoch": 0.23742454728370221,
        "step": 3186
    },
    {
        "loss": 2.365,
        "grad_norm": 2.027148962020874,
        "learning_rate": 3.920607631971888e-05,
        "epoch": 0.23749906848498398,
        "step": 3187
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.5389621257781982,
        "learning_rate": 3.9150222189382415e-05,
        "epoch": 0.23757358968626574,
        "step": 3188
    },
    {
        "loss": 2.5091,
        "grad_norm": 1.774845004081726,
        "learning_rate": 3.9094398187763356e-05,
        "epoch": 0.2376481108875475,
        "step": 3189
    },
    {
        "loss": 1.8026,
        "grad_norm": 2.787827491760254,
        "learning_rate": 3.903860434250208e-05,
        "epoch": 0.23772263208882927,
        "step": 3190
    },
    {
        "loss": 2.4006,
        "grad_norm": 4.547905445098877,
        "learning_rate": 3.8982840681223854e-05,
        "epoch": 0.23779715329011103,
        "step": 3191
    },
    {
        "loss": 2.8656,
        "grad_norm": 1.629029393196106,
        "learning_rate": 3.892710723153914e-05,
        "epoch": 0.2378716744913928,
        "step": 3192
    },
    {
        "loss": 2.2149,
        "grad_norm": 2.3372409343719482,
        "learning_rate": 3.887140402104339e-05,
        "epoch": 0.23794619569267456,
        "step": 3193
    },
    {
        "loss": 2.1382,
        "grad_norm": 3.620647430419922,
        "learning_rate": 3.8815731077317086e-05,
        "epoch": 0.23802071689395632,
        "step": 3194
    },
    {
        "loss": 3.0981,
        "grad_norm": 2.608196496963501,
        "learning_rate": 3.876008842792576e-05,
        "epoch": 0.23809523809523808,
        "step": 3195
    },
    {
        "loss": 2.4333,
        "grad_norm": 2.930084228515625,
        "learning_rate": 3.870447610041984e-05,
        "epoch": 0.23816975929651987,
        "step": 3196
    },
    {
        "loss": 2.0444,
        "grad_norm": 3.607232093811035,
        "learning_rate": 3.864889412233485e-05,
        "epoch": 0.23824428049780164,
        "step": 3197
    },
    {
        "loss": 2.5171,
        "grad_norm": 1.3001494407653809,
        "learning_rate": 3.859334252119125e-05,
        "epoch": 0.2383188016990834,
        "step": 3198
    },
    {
        "loss": 2.8824,
        "grad_norm": 1.9714723825454712,
        "learning_rate": 3.8537821324494486e-05,
        "epoch": 0.23839332290036516,
        "step": 3199
    },
    {
        "loss": 2.3098,
        "grad_norm": 2.881978750228882,
        "learning_rate": 3.848233055973483e-05,
        "epoch": 0.23846784410164693,
        "step": 3200
    },
    {
        "loss": 1.9883,
        "grad_norm": 2.29946231842041,
        "learning_rate": 3.842687025438766e-05,
        "epoch": 0.2385423653029287,
        "step": 3201
    },
    {
        "loss": 2.2401,
        "grad_norm": 2.0215461254119873,
        "learning_rate": 3.837144043591317e-05,
        "epoch": 0.23861688650421045,
        "step": 3202
    },
    {
        "loss": 2.0983,
        "grad_norm": 3.0328242778778076,
        "learning_rate": 3.831604113175642e-05,
        "epoch": 0.23869140770549221,
        "step": 3203
    },
    {
        "loss": 1.6437,
        "grad_norm": Infinity,
        "learning_rate": 3.831604113175642e-05,
        "epoch": 0.23876592890677398,
        "step": 3204
    },
    {
        "loss": 2.7744,
        "grad_norm": 2.2749764919281006,
        "learning_rate": 3.826067236934754e-05,
        "epoch": 0.23884045010805574,
        "step": 3205
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.435096502304077,
        "learning_rate": 3.8205334176101305e-05,
        "epoch": 0.2389149713093375,
        "step": 3206
    },
    {
        "loss": 1.7938,
        "grad_norm": 3.777296543121338,
        "learning_rate": 3.81500265794175e-05,
        "epoch": 0.23898949251061927,
        "step": 3207
    },
    {
        "loss": 2.2637,
        "grad_norm": 3.7983319759368896,
        "learning_rate": 3.809474960668078e-05,
        "epoch": 0.23906401371190103,
        "step": 3208
    },
    {
        "loss": 2.2527,
        "grad_norm": 2.6495494842529297,
        "learning_rate": 3.803950328526046e-05,
        "epoch": 0.2391385349131828,
        "step": 3209
    },
    {
        "loss": 2.6247,
        "grad_norm": 2.647155523300171,
        "learning_rate": 3.7984287642510994e-05,
        "epoch": 0.23921305611446456,
        "step": 3210
    },
    {
        "loss": 2.222,
        "grad_norm": 3.047037124633789,
        "learning_rate": 3.792910270577124e-05,
        "epoch": 0.23928757731574632,
        "step": 3211
    },
    {
        "loss": 1.9197,
        "grad_norm": 3.132084846496582,
        "learning_rate": 3.7873948502365234e-05,
        "epoch": 0.23936209851702808,
        "step": 3212
    },
    {
        "loss": 2.0772,
        "grad_norm": 2.71883487701416,
        "learning_rate": 3.781882505960153e-05,
        "epoch": 0.23943661971830985,
        "step": 3213
    },
    {
        "loss": 1.9602,
        "grad_norm": 4.161149978637695,
        "learning_rate": 3.776373240477358e-05,
        "epoch": 0.23951114091959164,
        "step": 3214
    },
    {
        "loss": 2.806,
        "grad_norm": 2.1420235633850098,
        "learning_rate": 3.77086705651596e-05,
        "epoch": 0.2395856621208734,
        "step": 3215
    },
    {
        "loss": 2.3695,
        "grad_norm": 1.713728666305542,
        "learning_rate": 3.76536395680224e-05,
        "epoch": 0.23966018332215516,
        "step": 3216
    },
    {
        "loss": 2.0441,
        "grad_norm": 3.1916732788085938,
        "learning_rate": 3.759863944060982e-05,
        "epoch": 0.23973470452343693,
        "step": 3217
    },
    {
        "loss": 2.319,
        "grad_norm": 2.6474640369415283,
        "learning_rate": 3.754367021015399e-05,
        "epoch": 0.2398092257247187,
        "step": 3218
    },
    {
        "loss": 2.1668,
        "grad_norm": 3.230609178543091,
        "learning_rate": 3.7488731903872135e-05,
        "epoch": 0.23988374692600045,
        "step": 3219
    },
    {
        "loss": 2.8042,
        "grad_norm": 2.3901519775390625,
        "learning_rate": 3.743382454896599e-05,
        "epoch": 0.23995826812728221,
        "step": 3220
    },
    {
        "loss": 2.1059,
        "grad_norm": 3.01094126701355,
        "learning_rate": 3.737894817262194e-05,
        "epoch": 0.24003278932856398,
        "step": 3221
    },
    {
        "loss": 2.7341,
        "grad_norm": 1.705660104751587,
        "learning_rate": 3.7324102802011116e-05,
        "epoch": 0.24010731052984574,
        "step": 3222
    },
    {
        "loss": 2.2045,
        "grad_norm": 2.97605037689209,
        "learning_rate": 3.726928846428919e-05,
        "epoch": 0.2401818317311275,
        "step": 3223
    },
    {
        "loss": 2.2597,
        "grad_norm": 2.0951035022735596,
        "learning_rate": 3.7214505186596636e-05,
        "epoch": 0.24025635293240927,
        "step": 3224
    },
    {
        "loss": 2.2696,
        "grad_norm": 2.582437753677368,
        "learning_rate": 3.715975299605837e-05,
        "epoch": 0.24033087413369103,
        "step": 3225
    },
    {
        "loss": 2.6082,
        "grad_norm": 1.7769416570663452,
        "learning_rate": 3.7105031919784014e-05,
        "epoch": 0.2404053953349728,
        "step": 3226
    },
    {
        "loss": 2.3563,
        "grad_norm": 2.2975897789001465,
        "learning_rate": 3.705034198486781e-05,
        "epoch": 0.24047991653625456,
        "step": 3227
    },
    {
        "loss": 2.479,
        "grad_norm": 2.818882942199707,
        "learning_rate": 3.699568321838842e-05,
        "epoch": 0.24055443773753632,
        "step": 3228
    },
    {
        "loss": 2.979,
        "grad_norm": 2.5837583541870117,
        "learning_rate": 3.694105564740934e-05,
        "epoch": 0.24062895893881808,
        "step": 3229
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.7565505504608154,
        "learning_rate": 3.688645929897836e-05,
        "epoch": 0.24070348014009985,
        "step": 3230
    },
    {
        "loss": 2.305,
        "grad_norm": 2.9636778831481934,
        "learning_rate": 3.683189420012799e-05,
        "epoch": 0.24077800134138164,
        "step": 3231
    },
    {
        "loss": 2.0856,
        "grad_norm": 3.5104820728302,
        "learning_rate": 3.677736037787513e-05,
        "epoch": 0.2408525225426634,
        "step": 3232
    },
    {
        "loss": 2.0684,
        "grad_norm": 3.2441890239715576,
        "learning_rate": 3.672285785922128e-05,
        "epoch": 0.24092704374394516,
        "step": 3233
    },
    {
        "loss": 2.2875,
        "grad_norm": 2.8870742321014404,
        "learning_rate": 3.6668386671152435e-05,
        "epoch": 0.24100156494522693,
        "step": 3234
    },
    {
        "loss": 2.6291,
        "grad_norm": 2.322051763534546,
        "learning_rate": 3.6613946840639066e-05,
        "epoch": 0.2410760861465087,
        "step": 3235
    },
    {
        "loss": 2.7606,
        "grad_norm": 1.4882583618164062,
        "learning_rate": 3.655953839463615e-05,
        "epoch": 0.24115060734779045,
        "step": 3236
    },
    {
        "loss": 2.4032,
        "grad_norm": 2.949643850326538,
        "learning_rate": 3.650516136008302e-05,
        "epoch": 0.24122512854907222,
        "step": 3237
    },
    {
        "loss": 2.354,
        "grad_norm": 2.5254108905792236,
        "learning_rate": 3.6450815763903556e-05,
        "epoch": 0.24129964975035398,
        "step": 3238
    },
    {
        "loss": 1.7552,
        "grad_norm": 2.8602540493011475,
        "learning_rate": 3.6396501633006043e-05,
        "epoch": 0.24137417095163574,
        "step": 3239
    },
    {
        "loss": 2.2207,
        "grad_norm": 2.740508556365967,
        "learning_rate": 3.634221899428323e-05,
        "epoch": 0.2414486921529175,
        "step": 3240
    },
    {
        "loss": 2.9044,
        "grad_norm": 2.2201731204986572,
        "learning_rate": 3.628796787461214e-05,
        "epoch": 0.24152321335419927,
        "step": 3241
    },
    {
        "loss": 2.5919,
        "grad_norm": 1.7291858196258545,
        "learning_rate": 3.623374830085433e-05,
        "epoch": 0.24159773455548103,
        "step": 3242
    },
    {
        "loss": 2.6991,
        "grad_norm": 3.584869623184204,
        "learning_rate": 3.617956029985572e-05,
        "epoch": 0.2416722557567628,
        "step": 3243
    },
    {
        "loss": 1.6904,
        "grad_norm": 3.158370018005371,
        "learning_rate": 3.612540389844645e-05,
        "epoch": 0.24174677695804456,
        "step": 3244
    },
    {
        "loss": 1.7527,
        "grad_norm": 4.485986709594727,
        "learning_rate": 3.607127912344128e-05,
        "epoch": 0.24182129815932632,
        "step": 3245
    },
    {
        "loss": 1.9743,
        "grad_norm": 2.8287532329559326,
        "learning_rate": 3.6017186001639036e-05,
        "epoch": 0.24189581936060808,
        "step": 3246
    },
    {
        "loss": 2.0603,
        "grad_norm": 3.3677878379821777,
        "learning_rate": 3.596312455982308e-05,
        "epoch": 0.24197034056188985,
        "step": 3247
    },
    {
        "loss": 2.0616,
        "grad_norm": 3.3802433013916016,
        "learning_rate": 3.590909482476092e-05,
        "epoch": 0.2420448617631716,
        "step": 3248
    },
    {
        "loss": 2.9238,
        "grad_norm": 2.7647714614868164,
        "learning_rate": 3.5855096823204505e-05,
        "epoch": 0.2421193829644534,
        "step": 3249
    },
    {
        "loss": 2.6841,
        "grad_norm": 3.248856782913208,
        "learning_rate": 3.5801130581889985e-05,
        "epoch": 0.24219390416573516,
        "step": 3250
    },
    {
        "loss": 2.0651,
        "grad_norm": 3.605548858642578,
        "learning_rate": 3.5747196127537817e-05,
        "epoch": 0.24226842536701693,
        "step": 3251
    },
    {
        "loss": 2.7132,
        "grad_norm": 1.969097375869751,
        "learning_rate": 3.569329348685277e-05,
        "epoch": 0.2423429465682987,
        "step": 3252
    },
    {
        "loss": 2.7562,
        "grad_norm": 3.505382776260376,
        "learning_rate": 3.563942268652372e-05,
        "epoch": 0.24241746776958045,
        "step": 3253
    },
    {
        "loss": 0.8929,
        "grad_norm": 3.2859349250793457,
        "learning_rate": 3.5585583753223906e-05,
        "epoch": 0.24249198897086222,
        "step": 3254
    },
    {
        "loss": 2.4003,
        "grad_norm": 2.046799421310425,
        "learning_rate": 3.553177671361074e-05,
        "epoch": 0.24256651017214398,
        "step": 3255
    },
    {
        "loss": 2.5148,
        "grad_norm": 2.7865333557128906,
        "learning_rate": 3.547800159432586e-05,
        "epoch": 0.24264103137342574,
        "step": 3256
    },
    {
        "loss": 1.4646,
        "grad_norm": 2.61655330657959,
        "learning_rate": 3.5424258421995105e-05,
        "epoch": 0.2427155525747075,
        "step": 3257
    },
    {
        "loss": 2.0427,
        "grad_norm": 2.1390862464904785,
        "learning_rate": 3.537054722322844e-05,
        "epoch": 0.24279007377598927,
        "step": 3258
    },
    {
        "loss": 1.8763,
        "grad_norm": 3.226583480834961,
        "learning_rate": 3.531686802462004e-05,
        "epoch": 0.24286459497727103,
        "step": 3259
    },
    {
        "loss": 2.327,
        "grad_norm": 2.2247469425201416,
        "learning_rate": 3.5263220852748244e-05,
        "epoch": 0.2429391161785528,
        "step": 3260
    },
    {
        "loss": 2.3699,
        "grad_norm": 2.5616037845611572,
        "learning_rate": 3.5209605734175574e-05,
        "epoch": 0.24301363737983456,
        "step": 3261
    },
    {
        "loss": 2.2583,
        "grad_norm": 3.930145502090454,
        "learning_rate": 3.515602269544854e-05,
        "epoch": 0.24308815858111632,
        "step": 3262
    },
    {
        "loss": 2.774,
        "grad_norm": 3.474503755569458,
        "learning_rate": 3.5102471763097924e-05,
        "epoch": 0.24316267978239808,
        "step": 3263
    },
    {
        "loss": 1.9811,
        "grad_norm": 3.511465072631836,
        "learning_rate": 3.5048952963638537e-05,
        "epoch": 0.24323720098367985,
        "step": 3264
    },
    {
        "loss": 2.7133,
        "grad_norm": 1.8830525875091553,
        "learning_rate": 3.4995466323569216e-05,
        "epoch": 0.2433117221849616,
        "step": 3265
    },
    {
        "loss": 2.4362,
        "grad_norm": 3.858222723007202,
        "learning_rate": 3.4942011869373084e-05,
        "epoch": 0.24338624338624337,
        "step": 3266
    },
    {
        "loss": 2.4385,
        "grad_norm": 3.154526710510254,
        "learning_rate": 3.4888589627517074e-05,
        "epoch": 0.24346076458752516,
        "step": 3267
    },
    {
        "loss": 2.298,
        "grad_norm": 3.034242868423462,
        "learning_rate": 3.483519962445237e-05,
        "epoch": 0.24353528578880693,
        "step": 3268
    },
    {
        "loss": 2.2607,
        "grad_norm": 3.1658971309661865,
        "learning_rate": 3.478184188661404e-05,
        "epoch": 0.2436098069900887,
        "step": 3269
    },
    {
        "loss": 1.9985,
        "grad_norm": 3.5177204608917236,
        "learning_rate": 3.472851644042127e-05,
        "epoch": 0.24368432819137045,
        "step": 3270
    },
    {
        "loss": 2.7548,
        "grad_norm": 2.150827407836914,
        "learning_rate": 3.467522331227728e-05,
        "epoch": 0.24375884939265222,
        "step": 3271
    },
    {
        "loss": 2.5442,
        "grad_norm": 2.7569167613983154,
        "learning_rate": 3.462196252856914e-05,
        "epoch": 0.24383337059393398,
        "step": 3272
    },
    {
        "loss": 2.2284,
        "grad_norm": 2.5435256958007812,
        "learning_rate": 3.4568734115668135e-05,
        "epoch": 0.24390789179521574,
        "step": 3273
    },
    {
        "loss": 2.7516,
        "grad_norm": 2.5994582176208496,
        "learning_rate": 3.4515538099929304e-05,
        "epoch": 0.2439824129964975,
        "step": 3274
    },
    {
        "loss": 2.2293,
        "grad_norm": 2.7005715370178223,
        "learning_rate": 3.446237450769176e-05,
        "epoch": 0.24405693419777927,
        "step": 3275
    },
    {
        "loss": 2.6401,
        "grad_norm": 3.670745372772217,
        "learning_rate": 3.440924336527859e-05,
        "epoch": 0.24413145539906103,
        "step": 3276
    },
    {
        "loss": 2.1554,
        "grad_norm": 2.3690378665924072,
        "learning_rate": 3.435614469899664e-05,
        "epoch": 0.2442059766003428,
        "step": 3277
    },
    {
        "loss": 0.7853,
        "grad_norm": 3.5723774433135986,
        "learning_rate": 3.430307853513698e-05,
        "epoch": 0.24428049780162456,
        "step": 3278
    },
    {
        "loss": 2.7522,
        "grad_norm": 2.2651360034942627,
        "learning_rate": 3.4250044899974186e-05,
        "epoch": 0.24435501900290632,
        "step": 3279
    },
    {
        "loss": 2.7129,
        "grad_norm": 2.236825466156006,
        "learning_rate": 3.4197043819767115e-05,
        "epoch": 0.24442954020418808,
        "step": 3280
    },
    {
        "loss": 2.288,
        "grad_norm": 3.2715768814086914,
        "learning_rate": 3.414407532075831e-05,
        "epoch": 0.24450406140546985,
        "step": 3281
    },
    {
        "loss": 3.2479,
        "grad_norm": 2.575026035308838,
        "learning_rate": 3.4091139429174135e-05,
        "epoch": 0.2445785826067516,
        "step": 3282
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.4298360347747803,
        "learning_rate": 3.403823617122498e-05,
        "epoch": 0.24465310380803337,
        "step": 3283
    },
    {
        "loss": 2.5854,
        "grad_norm": 2.1328964233398438,
        "learning_rate": 3.3985365573104856e-05,
        "epoch": 0.24472762500931516,
        "step": 3284
    },
    {
        "loss": 2.0747,
        "grad_norm": 1.6472302675247192,
        "learning_rate": 3.3932527660991845e-05,
        "epoch": 0.24480214621059693,
        "step": 3285
    },
    {
        "loss": 1.7314,
        "grad_norm": 2.5601439476013184,
        "learning_rate": 3.387972246104771e-05,
        "epoch": 0.2448766674118787,
        "step": 3286
    },
    {
        "loss": 2.4484,
        "grad_norm": 3.2430763244628906,
        "learning_rate": 3.3826949999417976e-05,
        "epoch": 0.24495118861316045,
        "step": 3287
    },
    {
        "loss": 2.7914,
        "grad_norm": 1.5118077993392944,
        "learning_rate": 3.377421030223209e-05,
        "epoch": 0.24502570981444222,
        "step": 3288
    },
    {
        "loss": 1.6676,
        "grad_norm": 2.2360246181488037,
        "learning_rate": 3.372150339560313e-05,
        "epoch": 0.24510023101572398,
        "step": 3289
    },
    {
        "loss": 2.3159,
        "grad_norm": 1.9031277894973755,
        "learning_rate": 3.366882930562804e-05,
        "epoch": 0.24517475221700574,
        "step": 3290
    },
    {
        "loss": 1.8754,
        "grad_norm": 3.0145673751831055,
        "learning_rate": 3.361618805838749e-05,
        "epoch": 0.2452492734182875,
        "step": 3291
    },
    {
        "loss": 1.8543,
        "grad_norm": 3.5820724964141846,
        "learning_rate": 3.356357967994588e-05,
        "epoch": 0.24532379461956927,
        "step": 3292
    },
    {
        "loss": 2.4234,
        "grad_norm": 3.0671284198760986,
        "learning_rate": 3.3511004196351394e-05,
        "epoch": 0.24539831582085103,
        "step": 3293
    },
    {
        "loss": 1.328,
        "grad_norm": 4.194438934326172,
        "learning_rate": 3.345846163363579e-05,
        "epoch": 0.2454728370221328,
        "step": 3294
    },
    {
        "loss": 2.4414,
        "grad_norm": 3.2858810424804688,
        "learning_rate": 3.340595201781465e-05,
        "epoch": 0.24554735822341456,
        "step": 3295
    },
    {
        "loss": 2.9731,
        "grad_norm": 3.3953781127929688,
        "learning_rate": 3.33534753748872e-05,
        "epoch": 0.24562187942469632,
        "step": 3296
    },
    {
        "loss": 2.5803,
        "grad_norm": 2.5291085243225098,
        "learning_rate": 3.330103173083639e-05,
        "epoch": 0.24569640062597808,
        "step": 3297
    },
    {
        "loss": 2.4652,
        "grad_norm": 2.784597873687744,
        "learning_rate": 3.32486211116287e-05,
        "epoch": 0.24577092182725985,
        "step": 3298
    },
    {
        "loss": 2.0712,
        "grad_norm": 1.966431736946106,
        "learning_rate": 3.319624354321439e-05,
        "epoch": 0.2458454430285416,
        "step": 3299
    },
    {
        "loss": 2.8003,
        "grad_norm": 1.706082820892334,
        "learning_rate": 3.314389905152731e-05,
        "epoch": 0.24591996422982337,
        "step": 3300
    },
    {
        "loss": 2.6048,
        "grad_norm": 1.8532284498214722,
        "learning_rate": 3.309158766248496e-05,
        "epoch": 0.24599448543110514,
        "step": 3301
    },
    {
        "loss": 2.404,
        "grad_norm": 2.5048468112945557,
        "learning_rate": 3.303930940198835e-05,
        "epoch": 0.24606900663238693,
        "step": 3302
    },
    {
        "loss": 2.3386,
        "grad_norm": 3.9118995666503906,
        "learning_rate": 3.29870642959222e-05,
        "epoch": 0.2461435278336687,
        "step": 3303
    },
    {
        "loss": 2.4085,
        "grad_norm": 2.4218270778656006,
        "learning_rate": 3.293485237015481e-05,
        "epoch": 0.24621804903495045,
        "step": 3304
    },
    {
        "loss": 2.3374,
        "grad_norm": 2.279284715652466,
        "learning_rate": 3.288267365053791e-05,
        "epoch": 0.24629257023623222,
        "step": 3305
    },
    {
        "loss": 2.0145,
        "grad_norm": 2.766446828842163,
        "learning_rate": 3.2830528162907014e-05,
        "epoch": 0.24636709143751398,
        "step": 3306
    },
    {
        "loss": 3.0485,
        "grad_norm": 2.430788993835449,
        "learning_rate": 3.2778415933080983e-05,
        "epoch": 0.24644161263879574,
        "step": 3307
    },
    {
        "loss": 2.4648,
        "grad_norm": 2.8057916164398193,
        "learning_rate": 3.2726336986862336e-05,
        "epoch": 0.2465161338400775,
        "step": 3308
    },
    {
        "loss": 1.8115,
        "grad_norm": 2.810943841934204,
        "learning_rate": 3.2674291350037e-05,
        "epoch": 0.24659065504135927,
        "step": 3309
    },
    {
        "loss": 2.0757,
        "grad_norm": 2.1868937015533447,
        "learning_rate": 3.262227904837453e-05,
        "epoch": 0.24666517624264103,
        "step": 3310
    },
    {
        "loss": 2.1856,
        "grad_norm": 3.5068061351776123,
        "learning_rate": 3.2570300107627885e-05,
        "epoch": 0.2467396974439228,
        "step": 3311
    },
    {
        "loss": 2.5554,
        "grad_norm": 2.3624191284179688,
        "learning_rate": 3.2518354553533555e-05,
        "epoch": 0.24681421864520456,
        "step": 3312
    },
    {
        "loss": 2.3173,
        "grad_norm": 1.7486492395401,
        "learning_rate": 3.246644241181153e-05,
        "epoch": 0.24688873984648632,
        "step": 3313
    },
    {
        "loss": 2.2851,
        "grad_norm": 3.163259506225586,
        "learning_rate": 3.2414563708165126e-05,
        "epoch": 0.24696326104776808,
        "step": 3314
    },
    {
        "loss": 2.0579,
        "grad_norm": 2.5738444328308105,
        "learning_rate": 3.236271846828124e-05,
        "epoch": 0.24703778224904985,
        "step": 3315
    },
    {
        "loss": 2.7279,
        "grad_norm": 1.6441013813018799,
        "learning_rate": 3.2310906717830125e-05,
        "epoch": 0.2471123034503316,
        "step": 3316
    },
    {
        "loss": 2.0661,
        "grad_norm": 3.005671262741089,
        "learning_rate": 3.225912848246553e-05,
        "epoch": 0.24718682465161337,
        "step": 3317
    },
    {
        "loss": 2.8752,
        "grad_norm": 2.5094358921051025,
        "learning_rate": 3.2207383787824476e-05,
        "epoch": 0.24726134585289514,
        "step": 3318
    },
    {
        "loss": 1.7989,
        "grad_norm": 3.8705945014953613,
        "learning_rate": 3.215567265952749e-05,
        "epoch": 0.2473358670541769,
        "step": 3319
    },
    {
        "loss": 2.4674,
        "grad_norm": 1.715511679649353,
        "learning_rate": 3.2103995123178454e-05,
        "epoch": 0.2474103882554587,
        "step": 3320
    },
    {
        "loss": 2.9857,
        "grad_norm": 2.293844223022461,
        "learning_rate": 3.205235120436459e-05,
        "epoch": 0.24748490945674045,
        "step": 3321
    },
    {
        "loss": 2.3203,
        "grad_norm": 3.1305620670318604,
        "learning_rate": 3.200074092865655e-05,
        "epoch": 0.24755943065802222,
        "step": 3322
    },
    {
        "loss": 2.252,
        "grad_norm": 2.4649882316589355,
        "learning_rate": 3.194916432160818e-05,
        "epoch": 0.24763395185930398,
        "step": 3323
    },
    {
        "loss": 2.1446,
        "grad_norm": 2.130251169204712,
        "learning_rate": 3.1897621408756804e-05,
        "epoch": 0.24770847306058574,
        "step": 3324
    },
    {
        "loss": 1.7545,
        "grad_norm": 2.3762946128845215,
        "learning_rate": 3.1846112215623024e-05,
        "epoch": 0.2477829942618675,
        "step": 3325
    },
    {
        "loss": 2.1917,
        "grad_norm": 3.3003575801849365,
        "learning_rate": 3.179463676771064e-05,
        "epoch": 0.24785751546314927,
        "step": 3326
    },
    {
        "loss": 3.3113,
        "grad_norm": 2.910762310028076,
        "learning_rate": 3.174319509050695e-05,
        "epoch": 0.24793203666443103,
        "step": 3327
    },
    {
        "loss": 2.1435,
        "grad_norm": 2.5978877544403076,
        "learning_rate": 3.169178720948232e-05,
        "epoch": 0.2480065578657128,
        "step": 3328
    },
    {
        "loss": 2.8461,
        "grad_norm": 3.008922576904297,
        "learning_rate": 3.1640413150090544e-05,
        "epoch": 0.24808107906699456,
        "step": 3329
    },
    {
        "loss": 2.5616,
        "grad_norm": 2.123020887374878,
        "learning_rate": 3.158907293776853e-05,
        "epoch": 0.24815560026827632,
        "step": 3330
    },
    {
        "loss": 2.8558,
        "grad_norm": 2.383610486984253,
        "learning_rate": 3.153776659793654e-05,
        "epoch": 0.24823012146955808,
        "step": 3331
    },
    {
        "loss": 2.2554,
        "grad_norm": 2.4769208431243896,
        "learning_rate": 3.1486494155998005e-05,
        "epoch": 0.24830464267083985,
        "step": 3332
    },
    {
        "loss": 2.09,
        "grad_norm": 2.5898547172546387,
        "learning_rate": 3.14352556373396e-05,
        "epoch": 0.2483791638721216,
        "step": 3333
    },
    {
        "loss": 2.3535,
        "grad_norm": 3.1753807067871094,
        "learning_rate": 3.138405106733124e-05,
        "epoch": 0.24845368507340337,
        "step": 3334
    },
    {
        "loss": 1.8163,
        "grad_norm": 3.3798069953918457,
        "learning_rate": 3.13328804713259e-05,
        "epoch": 0.24852820627468514,
        "step": 3335
    },
    {
        "loss": 2.4626,
        "grad_norm": 2.755232810974121,
        "learning_rate": 3.128174387465986e-05,
        "epoch": 0.2486027274759669,
        "step": 3336
    },
    {
        "loss": 2.4558,
        "grad_norm": 3.0013391971588135,
        "learning_rate": 3.1230641302652554e-05,
        "epoch": 0.24867724867724866,
        "step": 3337
    },
    {
        "loss": 2.5072,
        "grad_norm": 4.3830132484436035,
        "learning_rate": 3.117957278060647e-05,
        "epoch": 0.24875176987853045,
        "step": 3338
    },
    {
        "loss": 2.7175,
        "grad_norm": 2.242526054382324,
        "learning_rate": 3.112853833380733e-05,
        "epoch": 0.24882629107981222,
        "step": 3339
    },
    {
        "loss": 2.1911,
        "grad_norm": 3.3386757373809814,
        "learning_rate": 3.107753798752398e-05,
        "epoch": 0.24890081228109398,
        "step": 3340
    },
    {
        "loss": 2.1224,
        "grad_norm": 2.902735471725464,
        "learning_rate": 3.102657176700835e-05,
        "epoch": 0.24897533348237574,
        "step": 3341
    },
    {
        "loss": 2.1716,
        "grad_norm": 2.230640411376953,
        "learning_rate": 3.097563969749552e-05,
        "epoch": 0.2490498546836575,
        "step": 3342
    },
    {
        "loss": 2.4448,
        "grad_norm": 2.240013360977173,
        "learning_rate": 3.0924741804203564e-05,
        "epoch": 0.24912437588493927,
        "step": 3343
    },
    {
        "loss": 2.1031,
        "grad_norm": 2.5695505142211914,
        "learning_rate": 3.087387811233374e-05,
        "epoch": 0.24919889708622103,
        "step": 3344
    },
    {
        "loss": 2.5196,
        "grad_norm": 4.146265983581543,
        "learning_rate": 3.0823048647070265e-05,
        "epoch": 0.2492734182875028,
        "step": 3345
    },
    {
        "loss": 2.521,
        "grad_norm": 2.635385513305664,
        "learning_rate": 3.0772253433580534e-05,
        "epoch": 0.24934793948878456,
        "step": 3346
    },
    {
        "loss": 2.6106,
        "grad_norm": 2.0830273628234863,
        "learning_rate": 3.072149249701494e-05,
        "epoch": 0.24942246069006632,
        "step": 3347
    },
    {
        "loss": 2.1476,
        "grad_norm": 3.440897226333618,
        "learning_rate": 3.067076586250681e-05,
        "epoch": 0.24949698189134809,
        "step": 3348
    },
    {
        "loss": 2.0253,
        "grad_norm": 1.5301048755645752,
        "learning_rate": 3.0620073555172635e-05,
        "epoch": 0.24957150309262985,
        "step": 3349
    },
    {
        "loss": 2.8252,
        "grad_norm": 2.5892908573150635,
        "learning_rate": 3.056941560011177e-05,
        "epoch": 0.2496460242939116,
        "step": 3350
    },
    {
        "loss": 2.1091,
        "grad_norm": 3.2523093223571777,
        "learning_rate": 3.051879202240666e-05,
        "epoch": 0.24972054549519337,
        "step": 3351
    },
    {
        "loss": 2.0373,
        "grad_norm": 3.9222187995910645,
        "learning_rate": 3.0468202847122695e-05,
        "epoch": 0.24979506669647514,
        "step": 3352
    },
    {
        "loss": 2.7587,
        "grad_norm": 2.167468786239624,
        "learning_rate": 3.0417648099308226e-05,
        "epoch": 0.2498695878977569,
        "step": 3353
    },
    {
        "loss": 2.3634,
        "grad_norm": 1.8965697288513184,
        "learning_rate": 3.03671278039946e-05,
        "epoch": 0.24994410909903866,
        "step": 3354
    },
    {
        "loss": 2.4732,
        "grad_norm": 2.554725170135498,
        "learning_rate": 3.0316641986196014e-05,
        "epoch": 0.25001863030032045,
        "step": 3355
    },
    {
        "loss": 2.1872,
        "grad_norm": 2.927765369415283,
        "learning_rate": 3.0266190670909665e-05,
        "epoch": 0.2500931515016022,
        "step": 3356
    },
    {
        "loss": 0.9531,
        "grad_norm": 3.18916916847229,
        "learning_rate": 3.0215773883115706e-05,
        "epoch": 0.250167672702884,
        "step": 3357
    },
    {
        "loss": 2.049,
        "grad_norm": 2.8813111782073975,
        "learning_rate": 3.0165391647777052e-05,
        "epoch": 0.25024219390416574,
        "step": 3358
    },
    {
        "loss": 2.8353,
        "grad_norm": 1.6777771711349487,
        "learning_rate": 3.0115043989839642e-05,
        "epoch": 0.2503167151054475,
        "step": 3359
    },
    {
        "loss": 2.6431,
        "grad_norm": 2.4116387367248535,
        "learning_rate": 3.0064730934232244e-05,
        "epoch": 0.25039123630672927,
        "step": 3360
    },
    {
        "loss": 2.1653,
        "grad_norm": 1.631559133529663,
        "learning_rate": 3.0014452505866498e-05,
        "epoch": 0.25046575750801103,
        "step": 3361
    },
    {
        "loss": 2.3958,
        "grad_norm": 4.034046173095703,
        "learning_rate": 2.9964208729636944e-05,
        "epoch": 0.2505402787092928,
        "step": 3362
    },
    {
        "loss": 2.7036,
        "grad_norm": 2.446363925933838,
        "learning_rate": 2.991399963042085e-05,
        "epoch": 0.25061479991057456,
        "step": 3363
    },
    {
        "loss": 2.5632,
        "grad_norm": 3.6338069438934326,
        "learning_rate": 2.9863825233078414e-05,
        "epoch": 0.2506893211118563,
        "step": 3364
    },
    {
        "loss": 2.9523,
        "grad_norm": 2.6710362434387207,
        "learning_rate": 2.9813685562452653e-05,
        "epoch": 0.2507638423131381,
        "step": 3365
    },
    {
        "loss": 2.4982,
        "grad_norm": 3.0061838626861572,
        "learning_rate": 2.9763580643369282e-05,
        "epoch": 0.25083836351441985,
        "step": 3366
    },
    {
        "loss": 2.3827,
        "grad_norm": 1.6777008771896362,
        "learning_rate": 2.9713510500636987e-05,
        "epoch": 0.2509128847157016,
        "step": 3367
    },
    {
        "loss": 2.8034,
        "grad_norm": 2.306105136871338,
        "learning_rate": 2.966347515904706e-05,
        "epoch": 0.2509874059169834,
        "step": 3368
    },
    {
        "loss": 1.6865,
        "grad_norm": 2.501746892929077,
        "learning_rate": 2.9613474643373696e-05,
        "epoch": 0.25106192711826514,
        "step": 3369
    },
    {
        "loss": 1.3935,
        "grad_norm": 4.324262619018555,
        "learning_rate": 2.956350897837371e-05,
        "epoch": 0.2511364483195469,
        "step": 3370
    },
    {
        "loss": 1.6015,
        "grad_norm": 2.3602919578552246,
        "learning_rate": 2.9513578188786796e-05,
        "epoch": 0.25121096952082866,
        "step": 3371
    },
    {
        "loss": 2.4408,
        "grad_norm": 4.348548412322998,
        "learning_rate": 2.9463682299335306e-05,
        "epoch": 0.2512854907221104,
        "step": 3372
    },
    {
        "loss": 2.8863,
        "grad_norm": 2.340559482574463,
        "learning_rate": 2.9413821334724334e-05,
        "epoch": 0.2513600119233922,
        "step": 3373
    },
    {
        "loss": 2.4163,
        "grad_norm": 2.6154720783233643,
        "learning_rate": 2.93639953196417e-05,
        "epoch": 0.25143453312467395,
        "step": 3374
    },
    {
        "loss": 2.4237,
        "grad_norm": 1.923122763633728,
        "learning_rate": 2.931420427875784e-05,
        "epoch": 0.2515090543259557,
        "step": 3375
    },
    {
        "loss": 2.0692,
        "grad_norm": 3.5300917625427246,
        "learning_rate": 2.9264448236725995e-05,
        "epoch": 0.2515835755272375,
        "step": 3376
    },
    {
        "loss": 2.602,
        "grad_norm": 2.5433709621429443,
        "learning_rate": 2.9214727218181905e-05,
        "epoch": 0.25165809672851924,
        "step": 3377
    },
    {
        "loss": 2.4523,
        "grad_norm": 1.6582483053207397,
        "learning_rate": 2.9165041247744206e-05,
        "epoch": 0.251732617929801,
        "step": 3378
    },
    {
        "loss": 2.3368,
        "grad_norm": 3.123109817504883,
        "learning_rate": 2.911539035001396e-05,
        "epoch": 0.25180713913108277,
        "step": 3379
    },
    {
        "loss": 2.1706,
        "grad_norm": 2.905052423477173,
        "learning_rate": 2.9065774549574974e-05,
        "epoch": 0.25188166033236453,
        "step": 3380
    },
    {
        "loss": 2.4875,
        "grad_norm": 2.432596445083618,
        "learning_rate": 2.9016193870993693e-05,
        "epoch": 0.2519561815336463,
        "step": 3381
    },
    {
        "loss": 2.1671,
        "grad_norm": 3.4693984985351562,
        "learning_rate": 2.8966648338819047e-05,
        "epoch": 0.2520307027349281,
        "step": 3382
    },
    {
        "loss": 2.9365,
        "grad_norm": 1.929165005683899,
        "learning_rate": 2.8917137977582753e-05,
        "epoch": 0.2521052239362099,
        "step": 3383
    },
    {
        "loss": 2.4877,
        "grad_norm": 2.3454248905181885,
        "learning_rate": 2.8867662811798946e-05,
        "epoch": 0.25217974513749164,
        "step": 3384
    },
    {
        "loss": 2.8586,
        "grad_norm": 1.7443957328796387,
        "learning_rate": 2.881822286596445e-05,
        "epoch": 0.2522542663387734,
        "step": 3385
    },
    {
        "loss": 1.886,
        "grad_norm": 2.591606378555298,
        "learning_rate": 2.876881816455853e-05,
        "epoch": 0.25232878754005517,
        "step": 3386
    },
    {
        "loss": 2.6275,
        "grad_norm": 1.2742069959640503,
        "learning_rate": 2.8719448732043118e-05,
        "epoch": 0.25240330874133693,
        "step": 3387
    },
    {
        "loss": 2.64,
        "grad_norm": 2.164351463317871,
        "learning_rate": 2.867011459286263e-05,
        "epoch": 0.2524778299426187,
        "step": 3388
    },
    {
        "loss": 2.6837,
        "grad_norm": 1.7239629030227661,
        "learning_rate": 2.8620815771444008e-05,
        "epoch": 0.25255235114390046,
        "step": 3389
    },
    {
        "loss": 2.6742,
        "grad_norm": 2.538011074066162,
        "learning_rate": 2.8571552292196756e-05,
        "epoch": 0.2526268723451822,
        "step": 3390
    },
    {
        "loss": 1.6784,
        "grad_norm": 3.139946222305298,
        "learning_rate": 2.8522324179512772e-05,
        "epoch": 0.252701393546464,
        "step": 3391
    },
    {
        "loss": 2.1854,
        "grad_norm": 4.344720840454102,
        "learning_rate": 2.8473131457766524e-05,
        "epoch": 0.25277591474774574,
        "step": 3392
    },
    {
        "loss": 2.7805,
        "grad_norm": 1.6994606256484985,
        "learning_rate": 2.8423974151314948e-05,
        "epoch": 0.2528504359490275,
        "step": 3393
    },
    {
        "loss": 2.3399,
        "grad_norm": 3.7679543495178223,
        "learning_rate": 2.8374852284497446e-05,
        "epoch": 0.25292495715030927,
        "step": 3394
    },
    {
        "loss": 2.3155,
        "grad_norm": 3.1786422729492188,
        "learning_rate": 2.8325765881635903e-05,
        "epoch": 0.25299947835159103,
        "step": 3395
    },
    {
        "loss": 2.1,
        "grad_norm": 3.0536582469940186,
        "learning_rate": 2.8276714967034468e-05,
        "epoch": 0.2530739995528728,
        "step": 3396
    },
    {
        "loss": 3.09,
        "grad_norm": 2.6764140129089355,
        "learning_rate": 2.822769956497997e-05,
        "epoch": 0.25314852075415456,
        "step": 3397
    },
    {
        "loss": 1.6728,
        "grad_norm": 3.3130054473876953,
        "learning_rate": 2.8178719699741508e-05,
        "epoch": 0.2532230419554363,
        "step": 3398
    },
    {
        "loss": 2.6513,
        "grad_norm": 2.812556266784668,
        "learning_rate": 2.8129775395570656e-05,
        "epoch": 0.2532975631567181,
        "step": 3399
    },
    {
        "loss": 2.4855,
        "grad_norm": 2.790794610977173,
        "learning_rate": 2.80808666767013e-05,
        "epoch": 0.25337208435799985,
        "step": 3400
    },
    {
        "loss": 2.0773,
        "grad_norm": 2.896918535232544,
        "learning_rate": 2.80319935673497e-05,
        "epoch": 0.2534466055592816,
        "step": 3401
    },
    {
        "loss": 3.1143,
        "grad_norm": 4.536897659301758,
        "learning_rate": 2.7983156091714613e-05,
        "epoch": 0.2535211267605634,
        "step": 3402
    },
    {
        "loss": 2.5758,
        "grad_norm": 1.8232595920562744,
        "learning_rate": 2.7934354273977037e-05,
        "epoch": 0.25359564796184514,
        "step": 3403
    },
    {
        "loss": 2.7137,
        "grad_norm": 2.0108118057250977,
        "learning_rate": 2.7885588138300433e-05,
        "epoch": 0.2536701691631269,
        "step": 3404
    },
    {
        "loss": 2.4845,
        "grad_norm": 3.5375735759735107,
        "learning_rate": 2.7836857708830367e-05,
        "epoch": 0.25374469036440866,
        "step": 3405
    },
    {
        "loss": 1.8604,
        "grad_norm": 2.507483720779419,
        "learning_rate": 2.7788163009694944e-05,
        "epoch": 0.25381921156569043,
        "step": 3406
    },
    {
        "loss": 2.3946,
        "grad_norm": 2.454777479171753,
        "learning_rate": 2.773950406500446e-05,
        "epoch": 0.2538937327669722,
        "step": 3407
    },
    {
        "loss": 1.596,
        "grad_norm": 4.114292144775391,
        "learning_rate": 2.7690880898851666e-05,
        "epoch": 0.25396825396825395,
        "step": 3408
    },
    {
        "loss": 1.6736,
        "grad_norm": 2.6798946857452393,
        "learning_rate": 2.7642293535311404e-05,
        "epoch": 0.2540427751695357,
        "step": 3409
    },
    {
        "loss": 1.6949,
        "grad_norm": 2.299647092819214,
        "learning_rate": 2.7593741998440847e-05,
        "epoch": 0.2541172963708175,
        "step": 3410
    },
    {
        "loss": 2.6293,
        "grad_norm": 3.2613062858581543,
        "learning_rate": 2.7545226312279482e-05,
        "epoch": 0.25419181757209924,
        "step": 3411
    },
    {
        "loss": 1.8496,
        "grad_norm": 2.520163059234619,
        "learning_rate": 2.749674650084897e-05,
        "epoch": 0.254266338773381,
        "step": 3412
    },
    {
        "loss": 1.9999,
        "grad_norm": 3.0287656784057617,
        "learning_rate": 2.744830258815335e-05,
        "epoch": 0.25434085997466277,
        "step": 3413
    },
    {
        "loss": 2.0585,
        "grad_norm": 3.853264570236206,
        "learning_rate": 2.7399894598178745e-05,
        "epoch": 0.25441538117594453,
        "step": 3414
    },
    {
        "loss": 1.5666,
        "grad_norm": 3.174983024597168,
        "learning_rate": 2.7351522554893483e-05,
        "epoch": 0.2544899023772263,
        "step": 3415
    },
    {
        "loss": 2.6794,
        "grad_norm": 2.149885654449463,
        "learning_rate": 2.7303186482248188e-05,
        "epoch": 0.25456442357850806,
        "step": 3416
    },
    {
        "loss": 2.1576,
        "grad_norm": 3.528151273727417,
        "learning_rate": 2.7254886404175605e-05,
        "epoch": 0.2546389447797899,
        "step": 3417
    },
    {
        "loss": 2.2871,
        "grad_norm": 2.1691484451293945,
        "learning_rate": 2.7206622344590806e-05,
        "epoch": 0.25471346598107164,
        "step": 3418
    },
    {
        "loss": 2.4862,
        "grad_norm": 3.20816707611084,
        "learning_rate": 2.7158394327390734e-05,
        "epoch": 0.2547879871823534,
        "step": 3419
    },
    {
        "loss": 1.6677,
        "grad_norm": 3.825719118118286,
        "learning_rate": 2.7110202376454753e-05,
        "epoch": 0.25486250838363517,
        "step": 3420
    },
    {
        "loss": 3.1176,
        "grad_norm": 3.182149648666382,
        "learning_rate": 2.7062046515644244e-05,
        "epoch": 0.25493702958491693,
        "step": 3421
    },
    {
        "loss": 2.46,
        "grad_norm": 2.4627110958099365,
        "learning_rate": 2.7013926768802777e-05,
        "epoch": 0.2550115507861987,
        "step": 3422
    },
    {
        "loss": 2.6551,
        "grad_norm": 2.5514941215515137,
        "learning_rate": 2.696584315975603e-05,
        "epoch": 0.25508607198748046,
        "step": 3423
    },
    {
        "loss": 2.1757,
        "grad_norm": 3.172785997390747,
        "learning_rate": 2.691779571231172e-05,
        "epoch": 0.2551605931887622,
        "step": 3424
    },
    {
        "loss": 2.1599,
        "grad_norm": 2.563006639480591,
        "learning_rate": 2.686978445025977e-05,
        "epoch": 0.255235114390044,
        "step": 3425
    },
    {
        "loss": 2.7405,
        "grad_norm": 2.5238826274871826,
        "learning_rate": 2.682180939737202e-05,
        "epoch": 0.25530963559132575,
        "step": 3426
    },
    {
        "loss": 2.4236,
        "grad_norm": 2.184814453125,
        "learning_rate": 2.6773870577402617e-05,
        "epoch": 0.2553841567926075,
        "step": 3427
    },
    {
        "loss": 1.5705,
        "grad_norm": 2.3155064582824707,
        "learning_rate": 2.6725968014087598e-05,
        "epoch": 0.25545867799388927,
        "step": 3428
    },
    {
        "loss": 2.2448,
        "grad_norm": 1.72903311252594,
        "learning_rate": 2.6678101731145054e-05,
        "epoch": 0.25553319919517103,
        "step": 3429
    },
    {
        "loss": 2.1919,
        "grad_norm": 3.2087881565093994,
        "learning_rate": 2.6630271752275214e-05,
        "epoch": 0.2556077203964528,
        "step": 3430
    },
    {
        "loss": 2.8814,
        "grad_norm": 2.737473726272583,
        "learning_rate": 2.6582478101160167e-05,
        "epoch": 0.25568224159773456,
        "step": 3431
    },
    {
        "loss": 2.8409,
        "grad_norm": 1.7648853063583374,
        "learning_rate": 2.6534720801464262e-05,
        "epoch": 0.2557567627990163,
        "step": 3432
    },
    {
        "loss": 2.6225,
        "grad_norm": 1.7965251207351685,
        "learning_rate": 2.6486999876833528e-05,
        "epoch": 0.2558312840002981,
        "step": 3433
    },
    {
        "loss": 2.3769,
        "grad_norm": 2.660521984100342,
        "learning_rate": 2.6439315350896277e-05,
        "epoch": 0.25590580520157985,
        "step": 3434
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.924296498298645,
        "learning_rate": 2.6391667247262676e-05,
        "epoch": 0.2559803264028616,
        "step": 3435
    },
    {
        "loss": 2.8419,
        "grad_norm": 2.2562544345855713,
        "learning_rate": 2.6344055589524764e-05,
        "epoch": 0.2560548476041434,
        "step": 3436
    },
    {
        "loss": 2.0679,
        "grad_norm": 3.8233940601348877,
        "learning_rate": 2.6296480401256807e-05,
        "epoch": 0.25612936880542514,
        "step": 3437
    },
    {
        "loss": 1.7236,
        "grad_norm": 2.819087028503418,
        "learning_rate": 2.624894170601463e-05,
        "epoch": 0.2562038900067069,
        "step": 3438
    },
    {
        "loss": 2.3212,
        "grad_norm": 2.696619749069214,
        "learning_rate": 2.6201439527336356e-05,
        "epoch": 0.25627841120798867,
        "step": 3439
    },
    {
        "loss": 1.7698,
        "grad_norm": 3.3125252723693848,
        "learning_rate": 2.6153973888741788e-05,
        "epoch": 0.25635293240927043,
        "step": 3440
    },
    {
        "loss": 1.8737,
        "grad_norm": 2.960766553878784,
        "learning_rate": 2.6106544813732747e-05,
        "epoch": 0.2564274536105522,
        "step": 3441
    },
    {
        "loss": 1.7768,
        "grad_norm": 2.847884178161621,
        "learning_rate": 2.6059152325792946e-05,
        "epoch": 0.25650197481183395,
        "step": 3442
    },
    {
        "loss": 2.9126,
        "grad_norm": 2.4421560764312744,
        "learning_rate": 2.601179644838786e-05,
        "epoch": 0.2565764960131157,
        "step": 3443
    },
    {
        "loss": 1.7092,
        "grad_norm": 3.618119716644287,
        "learning_rate": 2.5964477204965065e-05,
        "epoch": 0.2566510172143975,
        "step": 3444
    },
    {
        "loss": 2.4303,
        "grad_norm": 1.1819052696228027,
        "learning_rate": 2.5917194618953778e-05,
        "epoch": 0.25672553841567924,
        "step": 3445
    },
    {
        "loss": 2.2927,
        "grad_norm": 2.2905843257904053,
        "learning_rate": 2.5869948713765214e-05,
        "epoch": 0.256800059616961,
        "step": 3446
    },
    {
        "loss": 2.6274,
        "grad_norm": 3.17146897315979,
        "learning_rate": 2.5822739512792306e-05,
        "epoch": 0.25687458081824277,
        "step": 3447
    },
    {
        "loss": 2.322,
        "grad_norm": 2.8415310382843018,
        "learning_rate": 2.577556703940991e-05,
        "epoch": 0.25694910201952453,
        "step": 3448
    },
    {
        "loss": 1.9783,
        "grad_norm": 4.394974708557129,
        "learning_rate": 2.5728431316974654e-05,
        "epoch": 0.2570236232208063,
        "step": 3449
    },
    {
        "loss": 2.7205,
        "grad_norm": 2.5588042736053467,
        "learning_rate": 2.5681332368824974e-05,
        "epoch": 0.25709814442208806,
        "step": 3450
    },
    {
        "loss": 1.9945,
        "grad_norm": 5.370887279510498,
        "learning_rate": 2.5634270218281174e-05,
        "epoch": 0.2571726656233698,
        "step": 3451
    },
    {
        "loss": 1.921,
        "grad_norm": 2.4631564617156982,
        "learning_rate": 2.5587244888645113e-05,
        "epoch": 0.2572471868246516,
        "step": 3452
    },
    {
        "loss": 2.3208,
        "grad_norm": 2.1852409839630127,
        "learning_rate": 2.554025640320069e-05,
        "epoch": 0.2573217080259334,
        "step": 3453
    },
    {
        "loss": 2.6298,
        "grad_norm": 2.6651601791381836,
        "learning_rate": 2.5493304785213413e-05,
        "epoch": 0.25739622922721517,
        "step": 3454
    },
    {
        "loss": 2.0553,
        "grad_norm": 2.8017561435699463,
        "learning_rate": 2.5446390057930593e-05,
        "epoch": 0.25747075042849693,
        "step": 3455
    },
    {
        "loss": 1.8831,
        "grad_norm": 2.606867790222168,
        "learning_rate": 2.5399512244581213e-05,
        "epoch": 0.2575452716297787,
        "step": 3456
    },
    {
        "loss": 2.5079,
        "grad_norm": 2.1107592582702637,
        "learning_rate": 2.535267136837597e-05,
        "epoch": 0.25761979283106046,
        "step": 3457
    },
    {
        "loss": 2.016,
        "grad_norm": 2.8479599952697754,
        "learning_rate": 2.530586745250738e-05,
        "epoch": 0.2576943140323422,
        "step": 3458
    },
    {
        "loss": 1.7364,
        "grad_norm": 1.5237454175949097,
        "learning_rate": 2.5259100520149594e-05,
        "epoch": 0.257768835233624,
        "step": 3459
    },
    {
        "loss": 2.657,
        "grad_norm": 3.814379930496216,
        "learning_rate": 2.5212370594458457e-05,
        "epoch": 0.25784335643490575,
        "step": 3460
    },
    {
        "loss": 1.837,
        "grad_norm": 2.8124802112579346,
        "learning_rate": 2.5165677698571476e-05,
        "epoch": 0.2579178776361875,
        "step": 3461
    },
    {
        "loss": 2.6833,
        "grad_norm": 2.335616111755371,
        "learning_rate": 2.511902185560775e-05,
        "epoch": 0.2579923988374693,
        "step": 3462
    },
    {
        "loss": 2.2771,
        "grad_norm": 3.503305435180664,
        "learning_rate": 2.5072403088668228e-05,
        "epoch": 0.25806692003875104,
        "step": 3463
    },
    {
        "loss": 1.7424,
        "grad_norm": 2.567542552947998,
        "learning_rate": 2.5025821420835337e-05,
        "epoch": 0.2581414412400328,
        "step": 3464
    },
    {
        "loss": 2.6828,
        "grad_norm": 1.2694240808486938,
        "learning_rate": 2.4979276875173253e-05,
        "epoch": 0.25821596244131456,
        "step": 3465
    },
    {
        "loss": 2.5216,
        "grad_norm": 2.8365137577056885,
        "learning_rate": 2.4932769474727592e-05,
        "epoch": 0.2582904836425963,
        "step": 3466
    },
    {
        "loss": 2.1073,
        "grad_norm": 1.6912446022033691,
        "learning_rate": 2.4886299242525746e-05,
        "epoch": 0.2583650048438781,
        "step": 3467
    },
    {
        "loss": 2.0488,
        "grad_norm": 3.978154182434082,
        "learning_rate": 2.4839866201576612e-05,
        "epoch": 0.25843952604515985,
        "step": 3468
    },
    {
        "loss": 2.3282,
        "grad_norm": 5.124364852905273,
        "learning_rate": 2.479347037487081e-05,
        "epoch": 0.2585140472464416,
        "step": 3469
    },
    {
        "loss": 2.2317,
        "grad_norm": 2.8120477199554443,
        "learning_rate": 2.4747111785380372e-05,
        "epoch": 0.2585885684477234,
        "step": 3470
    },
    {
        "loss": 1.2674,
        "grad_norm": 3.5747320652008057,
        "learning_rate": 2.470079045605892e-05,
        "epoch": 0.25866308964900514,
        "step": 3471
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.15620756149292,
        "learning_rate": 2.465450640984168e-05,
        "epoch": 0.2587376108502869,
        "step": 3472
    },
    {
        "loss": 2.6607,
        "grad_norm": 1.9835171699523926,
        "learning_rate": 2.4608259669645394e-05,
        "epoch": 0.25881213205156867,
        "step": 3473
    },
    {
        "loss": 2.8459,
        "grad_norm": 2.456209897994995,
        "learning_rate": 2.4562050258368396e-05,
        "epoch": 0.25888665325285043,
        "step": 3474
    },
    {
        "loss": 2.8484,
        "grad_norm": 2.0650758743286133,
        "learning_rate": 2.451587819889045e-05,
        "epoch": 0.2589611744541322,
        "step": 3475
    },
    {
        "loss": 2.6294,
        "grad_norm": 2.8651621341705322,
        "learning_rate": 2.4469743514072795e-05,
        "epoch": 0.25903569565541396,
        "step": 3476
    },
    {
        "loss": 2.4854,
        "grad_norm": 4.056124210357666,
        "learning_rate": 2.442364622675828e-05,
        "epoch": 0.2591102168566957,
        "step": 3477
    },
    {
        "loss": 1.9569,
        "grad_norm": 2.148231267929077,
        "learning_rate": 2.4377586359771176e-05,
        "epoch": 0.2591847380579775,
        "step": 3478
    },
    {
        "loss": 2.8021,
        "grad_norm": 2.405588150024414,
        "learning_rate": 2.4331563935917245e-05,
        "epoch": 0.25925925925925924,
        "step": 3479
    },
    {
        "loss": 2.2275,
        "grad_norm": 2.3341853618621826,
        "learning_rate": 2.4285578977983648e-05,
        "epoch": 0.259333780460541,
        "step": 3480
    },
    {
        "loss": 2.1996,
        "grad_norm": 1.977524995803833,
        "learning_rate": 2.423963150873907e-05,
        "epoch": 0.25940830166182277,
        "step": 3481
    },
    {
        "loss": 1.873,
        "grad_norm": 3.389878749847412,
        "learning_rate": 2.4193721550933613e-05,
        "epoch": 0.25948282286310453,
        "step": 3482
    },
    {
        "loss": 2.1168,
        "grad_norm": 3.5575644969940186,
        "learning_rate": 2.4147849127298806e-05,
        "epoch": 0.2595573440643863,
        "step": 3483
    },
    {
        "loss": 2.1126,
        "grad_norm": 3.634086847305298,
        "learning_rate": 2.410201426054759e-05,
        "epoch": 0.25963186526566806,
        "step": 3484
    },
    {
        "loss": 1.4611,
        "grad_norm": 3.501220941543579,
        "learning_rate": 2.405621697337428e-05,
        "epoch": 0.2597063864669498,
        "step": 3485
    },
    {
        "loss": 2.2069,
        "grad_norm": 3.475254774093628,
        "learning_rate": 2.4010457288454657e-05,
        "epoch": 0.2597809076682316,
        "step": 3486
    },
    {
        "loss": 2.4105,
        "grad_norm": 3.29313588142395,
        "learning_rate": 2.3964735228445755e-05,
        "epoch": 0.25985542886951335,
        "step": 3487
    },
    {
        "loss": 2.8516,
        "grad_norm": 3.5526771545410156,
        "learning_rate": 2.3919050815986143e-05,
        "epoch": 0.25992995007079517,
        "step": 3488
    },
    {
        "loss": 2.53,
        "grad_norm": 2.311452627182007,
        "learning_rate": 2.3873404073695683e-05,
        "epoch": 0.26000447127207693,
        "step": 3489
    },
    {
        "loss": 2.8207,
        "grad_norm": 2.1795430183410645,
        "learning_rate": 2.382779502417549e-05,
        "epoch": 0.2600789924733587,
        "step": 3490
    },
    {
        "loss": 2.7517,
        "grad_norm": 2.441392660140991,
        "learning_rate": 2.3782223690008188e-05,
        "epoch": 0.26015351367464046,
        "step": 3491
    },
    {
        "loss": 2.6093,
        "grad_norm": 2.9054670333862305,
        "learning_rate": 2.373669009375753e-05,
        "epoch": 0.2602280348759222,
        "step": 3492
    },
    {
        "loss": 2.3527,
        "grad_norm": 3.5978994369506836,
        "learning_rate": 2.3691194257968842e-05,
        "epoch": 0.260302556077204,
        "step": 3493
    },
    {
        "loss": 3.0748,
        "grad_norm": 4.079018592834473,
        "learning_rate": 2.3645736205168424e-05,
        "epoch": 0.26037707727848575,
        "step": 3494
    },
    {
        "loss": 2.0365,
        "grad_norm": 3.2249698638916016,
        "learning_rate": 2.3600315957864196e-05,
        "epoch": 0.2604515984797675,
        "step": 3495
    },
    {
        "loss": 2.5688,
        "grad_norm": 1.904279112815857,
        "learning_rate": 2.355493353854511e-05,
        "epoch": 0.2605261196810493,
        "step": 3496
    },
    {
        "loss": 2.0531,
        "grad_norm": 3.020423412322998,
        "learning_rate": 2.350958896968153e-05,
        "epoch": 0.26060064088233104,
        "step": 3497
    },
    {
        "loss": 1.8633,
        "grad_norm": 2.6597580909729004,
        "learning_rate": 2.346428227372506e-05,
        "epoch": 0.2606751620836128,
        "step": 3498
    },
    {
        "loss": 2.0566,
        "grad_norm": 2.882080554962158,
        "learning_rate": 2.3419013473108443e-05,
        "epoch": 0.26074968328489456,
        "step": 3499
    },
    {
        "loss": 1.8448,
        "grad_norm": 2.1585581302642822,
        "learning_rate": 2.3373782590245863e-05,
        "epoch": 0.2608242044861763,
        "step": 3500
    },
    {
        "loss": 2.9073,
        "grad_norm": 2.185882091522217,
        "learning_rate": 2.332858964753253e-05,
        "epoch": 0.2608987256874581,
        "step": 3501
    },
    {
        "loss": 1.9002,
        "grad_norm": 3.250077962875366,
        "learning_rate": 2.328343466734496e-05,
        "epoch": 0.26097324688873985,
        "step": 3502
    },
    {
        "loss": 2.6232,
        "grad_norm": 4.813682556152344,
        "learning_rate": 2.323831767204091e-05,
        "epoch": 0.2610477680900216,
        "step": 3503
    },
    {
        "loss": 2.3547,
        "grad_norm": 2.3703534603118896,
        "learning_rate": 2.3193238683959183e-05,
        "epoch": 0.2611222892913034,
        "step": 3504
    },
    {
        "loss": 2.0369,
        "grad_norm": 3.368954658508301,
        "learning_rate": 2.3148197725419983e-05,
        "epoch": 0.26119681049258514,
        "step": 3505
    },
    {
        "loss": 2.4731,
        "grad_norm": 3.630093574523926,
        "learning_rate": 2.3103194818724472e-05,
        "epoch": 0.2612713316938669,
        "step": 3506
    },
    {
        "loss": 2.2875,
        "grad_norm": 3.0323188304901123,
        "learning_rate": 2.30582299861551e-05,
        "epoch": 0.26134585289514867,
        "step": 3507
    },
    {
        "loss": 2.7169,
        "grad_norm": 2.253741502761841,
        "learning_rate": 2.301330324997545e-05,
        "epoch": 0.26142037409643043,
        "step": 3508
    },
    {
        "loss": 2.8972,
        "grad_norm": 4.182440757751465,
        "learning_rate": 2.2968414632430157e-05,
        "epoch": 0.2614948952977122,
        "step": 3509
    },
    {
        "loss": 1.9538,
        "grad_norm": 3.2475409507751465,
        "learning_rate": 2.2923564155745093e-05,
        "epoch": 0.26156941649899396,
        "step": 3510
    },
    {
        "loss": 2.6883,
        "grad_norm": 2.7437243461608887,
        "learning_rate": 2.2878751842127178e-05,
        "epoch": 0.2616439377002757,
        "step": 3511
    },
    {
        "loss": 2.8504,
        "grad_norm": 2.5503458976745605,
        "learning_rate": 2.283397771376452e-05,
        "epoch": 0.2617184589015575,
        "step": 3512
    },
    {
        "loss": 2.0123,
        "grad_norm": 4.268808841705322,
        "learning_rate": 2.2789241792826123e-05,
        "epoch": 0.26179298010283925,
        "step": 3513
    },
    {
        "loss": 2.6992,
        "grad_norm": 2.1371865272521973,
        "learning_rate": 2.2744544101462295e-05,
        "epoch": 0.261867501304121,
        "step": 3514
    },
    {
        "loss": 2.5693,
        "grad_norm": 1.968538761138916,
        "learning_rate": 2.2699884661804327e-05,
        "epoch": 0.26194202250540277,
        "step": 3515
    },
    {
        "loss": 2.1655,
        "grad_norm": 3.2217161655426025,
        "learning_rate": 2.2655263495964586e-05,
        "epoch": 0.26201654370668453,
        "step": 3516
    },
    {
        "loss": 2.2528,
        "grad_norm": 3.5836386680603027,
        "learning_rate": 2.261068062603644e-05,
        "epoch": 0.2620910649079663,
        "step": 3517
    },
    {
        "loss": 2.3679,
        "grad_norm": 3.1402602195739746,
        "learning_rate": 2.256613607409428e-05,
        "epoch": 0.26216558610924806,
        "step": 3518
    },
    {
        "loss": 1.7934,
        "grad_norm": 3.0086100101470947,
        "learning_rate": 2.252162986219365e-05,
        "epoch": 0.2622401073105298,
        "step": 3519
    },
    {
        "loss": 2.8234,
        "grad_norm": 2.350904941558838,
        "learning_rate": 2.2477162012371e-05,
        "epoch": 0.2623146285118116,
        "step": 3520
    },
    {
        "loss": 2.016,
        "grad_norm": 2.357848882675171,
        "learning_rate": 2.2432732546643863e-05,
        "epoch": 0.26238914971309335,
        "step": 3521
    },
    {
        "loss": 2.3207,
        "grad_norm": 2.544907808303833,
        "learning_rate": 2.238834148701068e-05,
        "epoch": 0.2624636709143751,
        "step": 3522
    },
    {
        "loss": 2.2136,
        "grad_norm": 2.860478162765503,
        "learning_rate": 2.234398885545089e-05,
        "epoch": 0.26253819211565693,
        "step": 3523
    },
    {
        "loss": 2.2084,
        "grad_norm": 3.4882993698120117,
        "learning_rate": 2.229967467392494e-05,
        "epoch": 0.2626127133169387,
        "step": 3524
    },
    {
        "loss": 2.0402,
        "grad_norm": 2.745976448059082,
        "learning_rate": 2.225539896437432e-05,
        "epoch": 0.26268723451822046,
        "step": 3525
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.2722041606903076,
        "learning_rate": 2.221116174872131e-05,
        "epoch": 0.2627617557195022,
        "step": 3526
    },
    {
        "loss": 2.2892,
        "grad_norm": 2.746701240539551,
        "learning_rate": 2.2166963048869195e-05,
        "epoch": 0.262836276920784,
        "step": 3527
    },
    {
        "loss": 2.3999,
        "grad_norm": 2.6728787422180176,
        "learning_rate": 2.212280288670222e-05,
        "epoch": 0.26291079812206575,
        "step": 3528
    },
    {
        "loss": 2.4614,
        "grad_norm": 3.1422886848449707,
        "learning_rate": 2.20786812840855e-05,
        "epoch": 0.2629853193233475,
        "step": 3529
    },
    {
        "loss": 1.971,
        "grad_norm": 2.659708023071289,
        "learning_rate": 2.2034598262865158e-05,
        "epoch": 0.2630598405246293,
        "step": 3530
    },
    {
        "loss": 2.453,
        "grad_norm": 3.2507236003875732,
        "learning_rate": 2.19905538448681e-05,
        "epoch": 0.26313436172591104,
        "step": 3531
    },
    {
        "loss": 2.1352,
        "grad_norm": 1.8584175109863281,
        "learning_rate": 2.194654805190213e-05,
        "epoch": 0.2632088829271928,
        "step": 3532
    },
    {
        "loss": 2.3791,
        "grad_norm": 5.768873691558838,
        "learning_rate": 2.1902580905755978e-05,
        "epoch": 0.26328340412847456,
        "step": 3533
    },
    {
        "loss": 2.6307,
        "grad_norm": 1.8223375082015991,
        "learning_rate": 2.185865242819919e-05,
        "epoch": 0.2633579253297563,
        "step": 3534
    },
    {
        "loss": 2.5173,
        "grad_norm": 2.221863031387329,
        "learning_rate": 2.181476264098228e-05,
        "epoch": 0.2634324465310381,
        "step": 3535
    },
    {
        "loss": 2.2852,
        "grad_norm": 3.089675188064575,
        "learning_rate": 2.177091156583646e-05,
        "epoch": 0.26350696773231985,
        "step": 3536
    },
    {
        "loss": 2.3533,
        "grad_norm": 2.9966888427734375,
        "learning_rate": 2.1727099224473822e-05,
        "epoch": 0.2635814889336016,
        "step": 3537
    },
    {
        "loss": 2.4382,
        "grad_norm": 2.1565468311309814,
        "learning_rate": 2.168332563858729e-05,
        "epoch": 0.2636560101348834,
        "step": 3538
    },
    {
        "loss": 2.0603,
        "grad_norm": 3.5214650630950928,
        "learning_rate": 2.163959082985062e-05,
        "epoch": 0.26373053133616514,
        "step": 3539
    },
    {
        "loss": 3.7011,
        "grad_norm": 3.266308546066284,
        "learning_rate": 2.1595894819918374e-05,
        "epoch": 0.2638050525374469,
        "step": 3540
    },
    {
        "loss": 2.0404,
        "grad_norm": 4.35176420211792,
        "learning_rate": 2.155223763042582e-05,
        "epoch": 0.26387957373872867,
        "step": 3541
    },
    {
        "loss": 1.7675,
        "grad_norm": 5.8432416915893555,
        "learning_rate": 2.1508619282989108e-05,
        "epoch": 0.26395409494001043,
        "step": 3542
    },
    {
        "loss": 2.3793,
        "grad_norm": 2.5384514331817627,
        "learning_rate": 2.1465039799205044e-05,
        "epoch": 0.2640286161412922,
        "step": 3543
    },
    {
        "loss": 2.0416,
        "grad_norm": 3.250539779663086,
        "learning_rate": 2.142149920065132e-05,
        "epoch": 0.26410313734257396,
        "step": 3544
    },
    {
        "loss": 1.5603,
        "grad_norm": 2.5690460205078125,
        "learning_rate": 2.137799750888633e-05,
        "epoch": 0.2641776585438557,
        "step": 3545
    },
    {
        "loss": 2.1033,
        "grad_norm": 3.964639186859131,
        "learning_rate": 2.133453474544912e-05,
        "epoch": 0.2642521797451375,
        "step": 3546
    },
    {
        "loss": 2.0591,
        "grad_norm": 2.8074355125427246,
        "learning_rate": 2.129111093185959e-05,
        "epoch": 0.26432670094641925,
        "step": 3547
    },
    {
        "loss": 2.3028,
        "grad_norm": 2.4032773971557617,
        "learning_rate": 2.1247726089618204e-05,
        "epoch": 0.264401222147701,
        "step": 3548
    },
    {
        "loss": 2.8128,
        "grad_norm": 2.1984119415283203,
        "learning_rate": 2.12043802402063e-05,
        "epoch": 0.2644757433489828,
        "step": 3549
    },
    {
        "loss": 2.454,
        "grad_norm": 2.9552061557769775,
        "learning_rate": 2.1161073405085842e-05,
        "epoch": 0.26455026455026454,
        "step": 3550
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.370999574661255,
        "learning_rate": 2.1117805605699383e-05,
        "epoch": 0.2646247857515463,
        "step": 3551
    },
    {
        "loss": 1.727,
        "grad_norm": 2.346237897872925,
        "learning_rate": 2.1074576863470297e-05,
        "epoch": 0.26469930695282806,
        "step": 3552
    },
    {
        "loss": 2.5358,
        "grad_norm": 2.322824478149414,
        "learning_rate": 2.1031387199802456e-05,
        "epoch": 0.2647738281541098,
        "step": 3553
    },
    {
        "loss": 2.1433,
        "grad_norm": 4.8653244972229,
        "learning_rate": 2.0988236636080573e-05,
        "epoch": 0.2648483493553916,
        "step": 3554
    },
    {
        "loss": 2.2913,
        "grad_norm": 2.1223766803741455,
        "learning_rate": 2.09451251936699e-05,
        "epoch": 0.26492287055667335,
        "step": 3555
    },
    {
        "loss": 2.5408,
        "grad_norm": 2.2818427085876465,
        "learning_rate": 2.0902052893916302e-05,
        "epoch": 0.2649973917579551,
        "step": 3556
    },
    {
        "loss": 2.743,
        "grad_norm": 3.4571115970611572,
        "learning_rate": 2.0859019758146238e-05,
        "epoch": 0.2650719129592369,
        "step": 3557
    },
    {
        "loss": 2.5492,
        "grad_norm": 2.724346160888672,
        "learning_rate": 2.081602580766687e-05,
        "epoch": 0.26514643416051864,
        "step": 3558
    },
    {
        "loss": 2.6051,
        "grad_norm": 2.9141530990600586,
        "learning_rate": 2.0773071063765938e-05,
        "epoch": 0.26522095536180046,
        "step": 3559
    },
    {
        "loss": 2.6066,
        "grad_norm": 1.0428260564804077,
        "learning_rate": 2.073015554771164e-05,
        "epoch": 0.2652954765630822,
        "step": 3560
    },
    {
        "loss": 1.9439,
        "grad_norm": 3.315687894821167,
        "learning_rate": 2.068727928075298e-05,
        "epoch": 0.265369997764364,
        "step": 3561
    },
    {
        "loss": 1.2379,
        "grad_norm": 3.38262939453125,
        "learning_rate": 2.0644442284119313e-05,
        "epoch": 0.26544451896564575,
        "step": 3562
    },
    {
        "loss": 2.1351,
        "grad_norm": 2.9770500659942627,
        "learning_rate": 2.0601644579020684e-05,
        "epoch": 0.2655190401669275,
        "step": 3563
    },
    {
        "loss": 2.208,
        "grad_norm": 3.0383551120758057,
        "learning_rate": 2.055888618664763e-05,
        "epoch": 0.2655935613682093,
        "step": 3564
    },
    {
        "loss": 2.8225,
        "grad_norm": 2.5283305644989014,
        "learning_rate": 2.0516167128171204e-05,
        "epoch": 0.26566808256949104,
        "step": 3565
    },
    {
        "loss": 2.8752,
        "grad_norm": 2.809741735458374,
        "learning_rate": 2.0473487424743032e-05,
        "epoch": 0.2657426037707728,
        "step": 3566
    },
    {
        "loss": 2.1433,
        "grad_norm": 2.264418840408325,
        "learning_rate": 2.043084709749523e-05,
        "epoch": 0.26581712497205456,
        "step": 3567
    },
    {
        "loss": 2.0774,
        "grad_norm": 2.3871943950653076,
        "learning_rate": 2.0388246167540425e-05,
        "epoch": 0.2658916461733363,
        "step": 3568
    },
    {
        "loss": 2.2596,
        "grad_norm": 1.9240527153015137,
        "learning_rate": 2.0345684655971754e-05,
        "epoch": 0.2659661673746181,
        "step": 3569
    },
    {
        "loss": 2.1023,
        "grad_norm": 2.9216408729553223,
        "learning_rate": 2.0303162583862767e-05,
        "epoch": 0.26604068857589985,
        "step": 3570
    },
    {
        "loss": 2.3839,
        "grad_norm": 2.593890428543091,
        "learning_rate": 2.0260679972267548e-05,
        "epoch": 0.2661152097771816,
        "step": 3571
    },
    {
        "loss": 2.3061,
        "grad_norm": 1.5388820171356201,
        "learning_rate": 2.021823684222064e-05,
        "epoch": 0.2661897309784634,
        "step": 3572
    },
    {
        "loss": 1.9771,
        "grad_norm": 2.361884593963623,
        "learning_rate": 2.0175833214737083e-05,
        "epoch": 0.26626425217974514,
        "step": 3573
    },
    {
        "loss": 2.3259,
        "grad_norm": 2.7639245986938477,
        "learning_rate": 2.013346911081215e-05,
        "epoch": 0.2663387733810269,
        "step": 3574
    },
    {
        "loss": 2.6078,
        "grad_norm": 2.727191209793091,
        "learning_rate": 2.0091144551421825e-05,
        "epoch": 0.26641329458230867,
        "step": 3575
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.7475694417953491,
        "learning_rate": 2.004885955752235e-05,
        "epoch": 0.26648781578359043,
        "step": 3576
    },
    {
        "loss": 2.3984,
        "grad_norm": 2.627068519592285,
        "learning_rate": 2.000661415005043e-05,
        "epoch": 0.2665623369848722,
        "step": 3577
    },
    {
        "loss": 2.3328,
        "grad_norm": 2.3908069133758545,
        "learning_rate": 1.9964408349923124e-05,
        "epoch": 0.26663685818615396,
        "step": 3578
    },
    {
        "loss": 1.9957,
        "grad_norm": 2.48394513130188,
        "learning_rate": 1.9922242178037864e-05,
        "epoch": 0.2667113793874357,
        "step": 3579
    },
    {
        "loss": 2.4473,
        "grad_norm": 1.859769582748413,
        "learning_rate": 1.988011565527257e-05,
        "epoch": 0.2667859005887175,
        "step": 3580
    },
    {
        "loss": 2.1981,
        "grad_norm": 2.7443034648895264,
        "learning_rate": 1.983802880248543e-05,
        "epoch": 0.26686042178999925,
        "step": 3581
    },
    {
        "loss": 2.4059,
        "grad_norm": 2.783024549484253,
        "learning_rate": 1.9795981640515072e-05,
        "epoch": 0.266934942991281,
        "step": 3582
    },
    {
        "loss": 2.4229,
        "grad_norm": 2.796894073486328,
        "learning_rate": 1.975397419018037e-05,
        "epoch": 0.2670094641925628,
        "step": 3583
    },
    {
        "loss": 2.7513,
        "grad_norm": 2.2948029041290283,
        "learning_rate": 1.9712006472280587e-05,
        "epoch": 0.26708398539384454,
        "step": 3584
    },
    {
        "loss": 1.9212,
        "grad_norm": 3.401859998703003,
        "learning_rate": 1.967007850759529e-05,
        "epoch": 0.2671585065951263,
        "step": 3585
    },
    {
        "loss": 2.5196,
        "grad_norm": 1.8343548774719238,
        "learning_rate": 1.9628190316884487e-05,
        "epoch": 0.26723302779640806,
        "step": 3586
    },
    {
        "loss": 2.6367,
        "grad_norm": 2.585330009460449,
        "learning_rate": 1.958634192088833e-05,
        "epoch": 0.2673075489976898,
        "step": 3587
    },
    {
        "loss": 2.6708,
        "grad_norm": 2.3584656715393066,
        "learning_rate": 1.9544533340327297e-05,
        "epoch": 0.2673820701989716,
        "step": 3588
    },
    {
        "loss": 2.6014,
        "grad_norm": 2.5321805477142334,
        "learning_rate": 1.9502764595902224e-05,
        "epoch": 0.26745659140025335,
        "step": 3589
    },
    {
        "loss": 2.222,
        "grad_norm": 2.825530767440796,
        "learning_rate": 1.9461035708294163e-05,
        "epoch": 0.2675311126015351,
        "step": 3590
    },
    {
        "loss": 2.1921,
        "grad_norm": 2.315568208694458,
        "learning_rate": 1.941934669816451e-05,
        "epoch": 0.2676056338028169,
        "step": 3591
    },
    {
        "loss": 2.6242,
        "grad_norm": 3.1123697757720947,
        "learning_rate": 1.937769758615482e-05,
        "epoch": 0.26768015500409864,
        "step": 3592
    },
    {
        "loss": 2.4982,
        "grad_norm": 2.974632501602173,
        "learning_rate": 1.933608839288691e-05,
        "epoch": 0.2677546762053804,
        "step": 3593
    },
    {
        "loss": 2.695,
        "grad_norm": 1.9254673719406128,
        "learning_rate": 1.9294519138962875e-05,
        "epoch": 0.2678291974066622,
        "step": 3594
    },
    {
        "loss": 1.6948,
        "grad_norm": 2.5181705951690674,
        "learning_rate": 1.9252989844965008e-05,
        "epoch": 0.267903718607944,
        "step": 3595
    },
    {
        "loss": 2.5121,
        "grad_norm": 2.2131195068359375,
        "learning_rate": 1.9211500531455828e-05,
        "epoch": 0.26797823980922575,
        "step": 3596
    },
    {
        "loss": 2.8514,
        "grad_norm": 1.9413864612579346,
        "learning_rate": 1.917005121897808e-05,
        "epoch": 0.2680527610105075,
        "step": 3597
    },
    {
        "loss": 2.558,
        "grad_norm": 2.9207911491394043,
        "learning_rate": 1.9128641928054624e-05,
        "epoch": 0.2681272822117893,
        "step": 3598
    },
    {
        "loss": 1.7905,
        "grad_norm": 4.784724712371826,
        "learning_rate": 1.9087272679188574e-05,
        "epoch": 0.26820180341307104,
        "step": 3599
    },
    {
        "loss": 2.0276,
        "grad_norm": 2.914484739303589,
        "learning_rate": 1.9045943492863204e-05,
        "epoch": 0.2682763246143528,
        "step": 3600
    },
    {
        "loss": 2.7675,
        "grad_norm": 1.810444951057434,
        "learning_rate": 1.9004654389541954e-05,
        "epoch": 0.26835084581563456,
        "step": 3601
    },
    {
        "loss": 2.2135,
        "grad_norm": 3.462440252304077,
        "learning_rate": 1.8963405389668433e-05,
        "epoch": 0.2684253670169163,
        "step": 3602
    },
    {
        "loss": 2.5892,
        "grad_norm": 1.571243405342102,
        "learning_rate": 1.8922196513666345e-05,
        "epoch": 0.2684998882181981,
        "step": 3603
    },
    {
        "loss": 1.8142,
        "grad_norm": 4.928289413452148,
        "learning_rate": 1.8881027781939497e-05,
        "epoch": 0.26857440941947985,
        "step": 3604
    },
    {
        "loss": 2.8302,
        "grad_norm": 1.414884328842163,
        "learning_rate": 1.883989921487196e-05,
        "epoch": 0.2686489306207616,
        "step": 3605
    },
    {
        "loss": 2.6201,
        "grad_norm": 3.566540479660034,
        "learning_rate": 1.879881083282784e-05,
        "epoch": 0.2687234518220434,
        "step": 3606
    },
    {
        "loss": 1.9573,
        "grad_norm": 4.365630626678467,
        "learning_rate": 1.875776265615127e-05,
        "epoch": 0.26879797302332514,
        "step": 3607
    },
    {
        "loss": 2.2719,
        "grad_norm": 2.7444007396698,
        "learning_rate": 1.871675470516662e-05,
        "epoch": 0.2688724942246069,
        "step": 3608
    },
    {
        "loss": 1.9143,
        "grad_norm": 1.751137614250183,
        "learning_rate": 1.8675787000178168e-05,
        "epoch": 0.26894701542588867,
        "step": 3609
    },
    {
        "loss": 2.2596,
        "grad_norm": 2.751380681991577,
        "learning_rate": 1.8634859561470464e-05,
        "epoch": 0.26902153662717043,
        "step": 3610
    },
    {
        "loss": 1.9154,
        "grad_norm": 2.939685344696045,
        "learning_rate": 1.8593972409308013e-05,
        "epoch": 0.2690960578284522,
        "step": 3611
    },
    {
        "loss": 1.732,
        "grad_norm": 3.3421878814697266,
        "learning_rate": 1.8553125563935358e-05,
        "epoch": 0.26917057902973396,
        "step": 3612
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.8805967569351196,
        "learning_rate": 1.8512319045577085e-05,
        "epoch": 0.2692451002310157,
        "step": 3613
    },
    {
        "loss": 2.2308,
        "grad_norm": 2.7538392543792725,
        "learning_rate": 1.8471552874437868e-05,
        "epoch": 0.2693196214322975,
        "step": 3614
    },
    {
        "loss": 2.3267,
        "grad_norm": 2.6064083576202393,
        "learning_rate": 1.843082707070235e-05,
        "epoch": 0.26939414263357925,
        "step": 3615
    },
    {
        "loss": 2.4091,
        "grad_norm": 3.296825647354126,
        "learning_rate": 1.8390141654535265e-05,
        "epoch": 0.269468663834861,
        "step": 3616
    },
    {
        "loss": 2.2083,
        "grad_norm": 5.2713165283203125,
        "learning_rate": 1.8349496646081265e-05,
        "epoch": 0.2695431850361428,
        "step": 3617
    },
    {
        "loss": 2.5553,
        "grad_norm": 2.074561357498169,
        "learning_rate": 1.8308892065465e-05,
        "epoch": 0.26961770623742454,
        "step": 3618
    },
    {
        "loss": 1.4718,
        "grad_norm": 3.2920243740081787,
        "learning_rate": 1.826832793279113e-05,
        "epoch": 0.2696922274387063,
        "step": 3619
    },
    {
        "loss": 1.6152,
        "grad_norm": 3.0300846099853516,
        "learning_rate": 1.8227804268144345e-05,
        "epoch": 0.26976674863998806,
        "step": 3620
    },
    {
        "loss": 2.4092,
        "grad_norm": 2.481963634490967,
        "learning_rate": 1.8187321091589126e-05,
        "epoch": 0.2698412698412698,
        "step": 3621
    },
    {
        "loss": 2.1407,
        "grad_norm": 3.102750301361084,
        "learning_rate": 1.8146878423170143e-05,
        "epoch": 0.2699157910425516,
        "step": 3622
    },
    {
        "loss": 2.302,
        "grad_norm": 3.3982982635498047,
        "learning_rate": 1.810647628291181e-05,
        "epoch": 0.26999031224383335,
        "step": 3623
    },
    {
        "loss": 2.7559,
        "grad_norm": 2.050234079360962,
        "learning_rate": 1.806611469081856e-05,
        "epoch": 0.2700648334451151,
        "step": 3624
    },
    {
        "loss": 2.6539,
        "grad_norm": 3.4270553588867188,
        "learning_rate": 1.802579366687478e-05,
        "epoch": 0.2701393546463969,
        "step": 3625
    },
    {
        "loss": 2.7249,
        "grad_norm": 2.324794054031372,
        "learning_rate": 1.798551323104467e-05,
        "epoch": 0.27021387584767864,
        "step": 3626
    },
    {
        "loss": 2.5869,
        "grad_norm": 2.8366823196411133,
        "learning_rate": 1.7945273403272422e-05,
        "epoch": 0.2702883970489604,
        "step": 3627
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.9871668815612793,
        "learning_rate": 1.7905074203482086e-05,
        "epoch": 0.27036291825024217,
        "step": 3628
    },
    {
        "loss": 2.7204,
        "grad_norm": 2.258183002471924,
        "learning_rate": 1.7864915651577608e-05,
        "epoch": 0.27043743945152393,
        "step": 3629
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.277498483657837,
        "learning_rate": 1.7824797767442825e-05,
        "epoch": 0.27051196065280575,
        "step": 3630
    },
    {
        "loss": 1.4573,
        "grad_norm": 4.0334672927856445,
        "learning_rate": 1.778472057094137e-05,
        "epoch": 0.2705864818540875,
        "step": 3631
    },
    {
        "loss": 2.2104,
        "grad_norm": 3.524216651916504,
        "learning_rate": 1.7744684081916796e-05,
        "epoch": 0.2706610030553693,
        "step": 3632
    },
    {
        "loss": 2.585,
        "grad_norm": 2.6087851524353027,
        "learning_rate": 1.77046883201925e-05,
        "epoch": 0.27073552425665104,
        "step": 3633
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.744038701057434,
        "learning_rate": 1.7664733305571666e-05,
        "epoch": 0.2708100454579328,
        "step": 3634
    },
    {
        "loss": 3.0014,
        "grad_norm": 1.4412920475006104,
        "learning_rate": 1.7624819057837307e-05,
        "epoch": 0.27088456665921457,
        "step": 3635
    },
    {
        "loss": 1.9468,
        "grad_norm": 3.2875821590423584,
        "learning_rate": 1.758494559675231e-05,
        "epoch": 0.27095908786049633,
        "step": 3636
    },
    {
        "loss": 1.4879,
        "grad_norm": 3.4340226650238037,
        "learning_rate": 1.7545112942059326e-05,
        "epoch": 0.2710336090617781,
        "step": 3637
    },
    {
        "loss": 2.8907,
        "grad_norm": 2.8068490028381348,
        "learning_rate": 1.7505321113480843e-05,
        "epoch": 0.27110813026305985,
        "step": 3638
    },
    {
        "loss": 2.6941,
        "grad_norm": 2.3318848609924316,
        "learning_rate": 1.7465570130719043e-05,
        "epoch": 0.2711826514643416,
        "step": 3639
    },
    {
        "loss": 2.2137,
        "grad_norm": 4.098592758178711,
        "learning_rate": 1.742586001345595e-05,
        "epoch": 0.2712571726656234,
        "step": 3640
    },
    {
        "loss": 1.7753,
        "grad_norm": 3.6998674869537354,
        "learning_rate": 1.7386190781353317e-05,
        "epoch": 0.27133169386690514,
        "step": 3641
    },
    {
        "loss": 1.3866,
        "grad_norm": 4.438902378082275,
        "learning_rate": 1.734656245405274e-05,
        "epoch": 0.2714062150681869,
        "step": 3642
    },
    {
        "loss": 2.1899,
        "grad_norm": 2.6555614471435547,
        "learning_rate": 1.7306975051175488e-05,
        "epoch": 0.27148073626946867,
        "step": 3643
    },
    {
        "loss": 1.8445,
        "grad_norm": 2.503467559814453,
        "learning_rate": 1.7267428592322578e-05,
        "epoch": 0.27155525747075043,
        "step": 3644
    },
    {
        "loss": 2.4469,
        "grad_norm": 2.920003652572632,
        "learning_rate": 1.7227923097074717e-05,
        "epoch": 0.2716297786720322,
        "step": 3645
    },
    {
        "loss": 2.9376,
        "grad_norm": 3.4334659576416016,
        "learning_rate": 1.7188458584992373e-05,
        "epoch": 0.27170429987331396,
        "step": 3646
    },
    {
        "loss": 2.1846,
        "grad_norm": 2.392338275909424,
        "learning_rate": 1.7149035075615794e-05,
        "epoch": 0.2717788210745957,
        "step": 3647
    },
    {
        "loss": 1.7806,
        "grad_norm": 2.177912712097168,
        "learning_rate": 1.7109652588464775e-05,
        "epoch": 0.2718533422758775,
        "step": 3648
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.7589194774627686,
        "learning_rate": 1.707031114303892e-05,
        "epoch": 0.27192786347715925,
        "step": 3649
    },
    {
        "loss": 3.028,
        "grad_norm": 2.448270320892334,
        "learning_rate": 1.7031010758817424e-05,
        "epoch": 0.272002384678441,
        "step": 3650
    },
    {
        "loss": 1.9201,
        "grad_norm": 3.087186574935913,
        "learning_rate": 1.6991751455259187e-05,
        "epoch": 0.2720769058797228,
        "step": 3651
    },
    {
        "loss": 2.5796,
        "grad_norm": 2.142753839492798,
        "learning_rate": 1.695253325180286e-05,
        "epoch": 0.27215142708100454,
        "step": 3652
    },
    {
        "loss": 2.2729,
        "grad_norm": 2.43870210647583,
        "learning_rate": 1.69133561678666e-05,
        "epoch": 0.2722259482822863,
        "step": 3653
    },
    {
        "loss": 2.0998,
        "grad_norm": 2.9905056953430176,
        "learning_rate": 1.6874220222848246e-05,
        "epoch": 0.27230046948356806,
        "step": 3654
    },
    {
        "loss": 2.3017,
        "grad_norm": 2.845003843307495,
        "learning_rate": 1.683512543612531e-05,
        "epoch": 0.2723749906848498,
        "step": 3655
    },
    {
        "loss": 2.7015,
        "grad_norm": 2.0091071128845215,
        "learning_rate": 1.6796071827054893e-05,
        "epoch": 0.2724495118861316,
        "step": 3656
    },
    {
        "loss": 2.0318,
        "grad_norm": 2.924443483352661,
        "learning_rate": 1.675705941497373e-05,
        "epoch": 0.27252403308741335,
        "step": 3657
    },
    {
        "loss": 1.2463,
        "grad_norm": 2.352332353591919,
        "learning_rate": 1.6718088219198156e-05,
        "epoch": 0.2725985542886951,
        "step": 3658
    },
    {
        "loss": 1.9507,
        "grad_norm": 2.6443140506744385,
        "learning_rate": 1.6679158259024064e-05,
        "epoch": 0.2726730754899769,
        "step": 3659
    },
    {
        "loss": 2.5262,
        "grad_norm": 3.2319624423980713,
        "learning_rate": 1.6640269553726907e-05,
        "epoch": 0.27274759669125864,
        "step": 3660
    },
    {
        "loss": 2.0312,
        "grad_norm": 2.64040207862854,
        "learning_rate": 1.6601422122561825e-05,
        "epoch": 0.2728221178925404,
        "step": 3661
    },
    {
        "loss": 2.0408,
        "grad_norm": 2.3748836517333984,
        "learning_rate": 1.6562615984763443e-05,
        "epoch": 0.27289663909382217,
        "step": 3662
    },
    {
        "loss": 2.3521,
        "grad_norm": 5.481337070465088,
        "learning_rate": 1.6523851159545946e-05,
        "epoch": 0.27297116029510393,
        "step": 3663
    },
    {
        "loss": 1.0945,
        "grad_norm": 1.2344721555709839,
        "learning_rate": 1.648512766610306e-05,
        "epoch": 0.2730456814963857,
        "step": 3664
    },
    {
        "loss": 2.6905,
        "grad_norm": 2.4554409980773926,
        "learning_rate": 1.644644552360802e-05,
        "epoch": 0.2731202026976675,
        "step": 3665
    },
    {
        "loss": 2.6817,
        "grad_norm": 3.9314591884613037,
        "learning_rate": 1.6407804751213673e-05,
        "epoch": 0.2731947238989493,
        "step": 3666
    },
    {
        "loss": 2.4081,
        "grad_norm": 2.752192735671997,
        "learning_rate": 1.6369205368052342e-05,
        "epoch": 0.27326924510023104,
        "step": 3667
    },
    {
        "loss": 2.9371,
        "grad_norm": 3.3441033363342285,
        "learning_rate": 1.63306473932358e-05,
        "epoch": 0.2733437663015128,
        "step": 3668
    },
    {
        "loss": 2.3528,
        "grad_norm": 2.9183547496795654,
        "learning_rate": 1.629213084585539e-05,
        "epoch": 0.27341828750279457,
        "step": 3669
    },
    {
        "loss": 2.2573,
        "grad_norm": 3.0420634746551514,
        "learning_rate": 1.6253655744981866e-05,
        "epoch": 0.27349280870407633,
        "step": 3670
    },
    {
        "loss": 2.8553,
        "grad_norm": 2.0849955081939697,
        "learning_rate": 1.6215222109665573e-05,
        "epoch": 0.2735673299053581,
        "step": 3671
    },
    {
        "loss": 2.6311,
        "grad_norm": 2.116219997406006,
        "learning_rate": 1.617682995893627e-05,
        "epoch": 0.27364185110663986,
        "step": 3672
    },
    {
        "loss": 2.6005,
        "grad_norm": 1.7909578084945679,
        "learning_rate": 1.6138479311803135e-05,
        "epoch": 0.2737163723079216,
        "step": 3673
    },
    {
        "loss": 2.4992,
        "grad_norm": 2.053410291671753,
        "learning_rate": 1.6100170187254804e-05,
        "epoch": 0.2737908935092034,
        "step": 3674
    },
    {
        "loss": 2.0345,
        "grad_norm": 3.198756694793701,
        "learning_rate": 1.6061902604259415e-05,
        "epoch": 0.27386541471048514,
        "step": 3675
    },
    {
        "loss": 2.3539,
        "grad_norm": 3.4244933128356934,
        "learning_rate": 1.6023676581764456e-05,
        "epoch": 0.2739399359117669,
        "step": 3676
    },
    {
        "loss": 2.2649,
        "grad_norm": 4.166845321655273,
        "learning_rate": 1.598549213869698e-05,
        "epoch": 0.27401445711304867,
        "step": 3677
    },
    {
        "loss": 1.8235,
        "grad_norm": 2.065535306930542,
        "learning_rate": 1.59473492939633e-05,
        "epoch": 0.27408897831433043,
        "step": 3678
    },
    {
        "loss": 2.5935,
        "grad_norm": 1.7559702396392822,
        "learning_rate": 1.5909248066449147e-05,
        "epoch": 0.2741634995156122,
        "step": 3679
    },
    {
        "loss": 1.8967,
        "grad_norm": 3.09053897857666,
        "learning_rate": 1.5871188475019714e-05,
        "epoch": 0.27423802071689396,
        "step": 3680
    },
    {
        "loss": 2.5322,
        "grad_norm": 2.1865694522857666,
        "learning_rate": 1.5833170538519605e-05,
        "epoch": 0.2743125419181757,
        "step": 3681
    },
    {
        "loss": 2.1816,
        "grad_norm": 3.074300765991211,
        "learning_rate": 1.579519427577266e-05,
        "epoch": 0.2743870631194575,
        "step": 3682
    },
    {
        "loss": 2.059,
        "grad_norm": 2.274716854095459,
        "learning_rate": 1.5757259705582206e-05,
        "epoch": 0.27446158432073925,
        "step": 3683
    },
    {
        "loss": 1.8384,
        "grad_norm": 3.6369473934173584,
        "learning_rate": 1.57193668467309e-05,
        "epoch": 0.274536105522021,
        "step": 3684
    },
    {
        "loss": 1.8547,
        "grad_norm": 3.7321226596832275,
        "learning_rate": 1.5681515717980732e-05,
        "epoch": 0.2746106267233028,
        "step": 3685
    },
    {
        "loss": 2.2102,
        "grad_norm": 3.701956033706665,
        "learning_rate": 1.5643706338073062e-05,
        "epoch": 0.27468514792458454,
        "step": 3686
    },
    {
        "loss": 2.5784,
        "grad_norm": 3.1120057106018066,
        "learning_rate": 1.5605938725728484e-05,
        "epoch": 0.2747596691258663,
        "step": 3687
    },
    {
        "loss": 2.8093,
        "grad_norm": 3.362367630004883,
        "learning_rate": 1.556821289964704e-05,
        "epoch": 0.27483419032714806,
        "step": 3688
    },
    {
        "loss": 2.7858,
        "grad_norm": 1.7117401361465454,
        "learning_rate": 1.553052887850799e-05,
        "epoch": 0.2749087115284298,
        "step": 3689
    },
    {
        "loss": 2.7527,
        "grad_norm": 2.6828582286834717,
        "learning_rate": 1.5492886680969943e-05,
        "epoch": 0.2749832327297116,
        "step": 3690
    },
    {
        "loss": 2.5168,
        "grad_norm": 2.5982916355133057,
        "learning_rate": 1.545528632567079e-05,
        "epoch": 0.27505775393099335,
        "step": 3691
    },
    {
        "loss": 2.4946,
        "grad_norm": 2.6728665828704834,
        "learning_rate": 1.541772783122768e-05,
        "epoch": 0.2751322751322751,
        "step": 3692
    },
    {
        "loss": 2.1654,
        "grad_norm": 4.774075508117676,
        "learning_rate": 1.538021121623705e-05,
        "epoch": 0.2752067963335569,
        "step": 3693
    },
    {
        "loss": 2.4262,
        "grad_norm": 4.65946102142334,
        "learning_rate": 1.5342736499274646e-05,
        "epoch": 0.27528131753483864,
        "step": 3694
    },
    {
        "loss": 2.897,
        "grad_norm": 3.0803542137145996,
        "learning_rate": 1.5305303698895347e-05,
        "epoch": 0.2753558387361204,
        "step": 3695
    },
    {
        "loss": 2.4765,
        "grad_norm": 2.1985459327697754,
        "learning_rate": 1.5267912833633448e-05,
        "epoch": 0.27543035993740217,
        "step": 3696
    },
    {
        "loss": 2.5507,
        "grad_norm": 2.3634564876556396,
        "learning_rate": 1.5230563922002327e-05,
        "epoch": 0.27550488113868393,
        "step": 3697
    },
    {
        "loss": 2.3699,
        "grad_norm": 1.251841425895691,
        "learning_rate": 1.5193256982494687e-05,
        "epoch": 0.2755794023399657,
        "step": 3698
    },
    {
        "loss": 2.424,
        "grad_norm": 1.87081778049469,
        "learning_rate": 1.5155992033582422e-05,
        "epoch": 0.27565392354124746,
        "step": 3699
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.919037103652954,
        "learning_rate": 1.5118769093716611e-05,
        "epoch": 0.2757284447425293,
        "step": 3700
    },
    {
        "loss": 2.047,
        "grad_norm": 3.0541276931762695,
        "learning_rate": 1.5081588181327521e-05,
        "epoch": 0.27580296594381104,
        "step": 3701
    },
    {
        "loss": 1.7905,
        "grad_norm": 2.1705121994018555,
        "learning_rate": 1.5044449314824649e-05,
        "epoch": 0.2758774871450928,
        "step": 3702
    },
    {
        "loss": 2.5476,
        "grad_norm": 2.746424674987793,
        "learning_rate": 1.5007352512596728e-05,
        "epoch": 0.27595200834637457,
        "step": 3703
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.8857125043869019,
        "learning_rate": 1.4970297793011545e-05,
        "epoch": 0.27602652954765633,
        "step": 3704
    },
    {
        "loss": 2.4865,
        "grad_norm": 2.1868703365325928,
        "learning_rate": 1.4933285174416122e-05,
        "epoch": 0.2761010507489381,
        "step": 3705
    },
    {
        "loss": 2.4341,
        "grad_norm": 1.3956001996994019,
        "learning_rate": 1.489631467513659e-05,
        "epoch": 0.27617557195021986,
        "step": 3706
    },
    {
        "loss": 1.541,
        "grad_norm": 2.8527848720550537,
        "learning_rate": 1.4859386313478263e-05,
        "epoch": 0.2762500931515016,
        "step": 3707
    },
    {
        "loss": 2.7954,
        "grad_norm": 3.0240986347198486,
        "learning_rate": 1.4822500107725624e-05,
        "epoch": 0.2763246143527834,
        "step": 3708
    },
    {
        "loss": 2.5706,
        "grad_norm": 2.406684637069702,
        "learning_rate": 1.4785656076142197e-05,
        "epoch": 0.27639913555406515,
        "step": 3709
    },
    {
        "loss": 2.6752,
        "grad_norm": 1.8116375207901,
        "learning_rate": 1.474885423697071e-05,
        "epoch": 0.2764736567553469,
        "step": 3710
    },
    {
        "loss": 2.3395,
        "grad_norm": 4.305047988891602,
        "learning_rate": 1.4712094608432903e-05,
        "epoch": 0.27654817795662867,
        "step": 3711
    },
    {
        "loss": 1.7952,
        "grad_norm": 3.726729393005371,
        "learning_rate": 1.467537720872968e-05,
        "epoch": 0.27662269915791043,
        "step": 3712
    },
    {
        "loss": 3.3873,
        "grad_norm": 3.6255810260772705,
        "learning_rate": 1.4638702056041087e-05,
        "epoch": 0.2766972203591922,
        "step": 3713
    },
    {
        "loss": 1.8857,
        "grad_norm": 2.7132697105407715,
        "learning_rate": 1.4602069168526155e-05,
        "epoch": 0.27677174156047396,
        "step": 3714
    },
    {
        "loss": 1.1511,
        "grad_norm": 4.288925647735596,
        "learning_rate": 1.456547856432301e-05,
        "epoch": 0.2768462627617557,
        "step": 3715
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.3680458068847656,
        "learning_rate": 1.4528930261548868e-05,
        "epoch": 0.2769207839630375,
        "step": 3716
    },
    {
        "loss": 1.7973,
        "grad_norm": 3.52166485786438,
        "learning_rate": 1.44924242783e-05,
        "epoch": 0.27699530516431925,
        "step": 3717
    },
    {
        "loss": 1.8738,
        "grad_norm": 3.095059871673584,
        "learning_rate": 1.44559606326517e-05,
        "epoch": 0.277069826365601,
        "step": 3718
    },
    {
        "loss": 2.3998,
        "grad_norm": 3.310412883758545,
        "learning_rate": 1.441953934265835e-05,
        "epoch": 0.2771443475668828,
        "step": 3719
    },
    {
        "loss": 2.0431,
        "grad_norm": 2.8940165042877197,
        "learning_rate": 1.4383160426353292e-05,
        "epoch": 0.27721886876816454,
        "step": 3720
    },
    {
        "loss": 2.334,
        "grad_norm": 2.6633219718933105,
        "learning_rate": 1.4346823901748884e-05,
        "epoch": 0.2772933899694463,
        "step": 3721
    },
    {
        "loss": 2.2697,
        "grad_norm": 3.452953815460205,
        "learning_rate": 1.4310529786836591e-05,
        "epoch": 0.27736791117072807,
        "step": 3722
    },
    {
        "loss": 2.4592,
        "grad_norm": 2.4816081523895264,
        "learning_rate": 1.427427809958678e-05,
        "epoch": 0.27744243237200983,
        "step": 3723
    },
    {
        "loss": 2.8864,
        "grad_norm": 2.0448944568634033,
        "learning_rate": 1.4238068857948894e-05,
        "epoch": 0.2775169535732916,
        "step": 3724
    },
    {
        "loss": 2.5694,
        "grad_norm": 2.1800308227539062,
        "learning_rate": 1.420190207985128e-05,
        "epoch": 0.27759147477457335,
        "step": 3725
    },
    {
        "loss": 2.9716,
        "grad_norm": 1.0688432455062866,
        "learning_rate": 1.4165777783201262e-05,
        "epoch": 0.2776659959758551,
        "step": 3726
    },
    {
        "loss": 2.5816,
        "grad_norm": 3.197282314300537,
        "learning_rate": 1.4129695985885206e-05,
        "epoch": 0.2777405171771369,
        "step": 3727
    },
    {
        "loss": 2.2717,
        "grad_norm": 2.3550760746002197,
        "learning_rate": 1.409365670576841e-05,
        "epoch": 0.27781503837841864,
        "step": 3728
    },
    {
        "loss": 1.9105,
        "grad_norm": 3.96049427986145,
        "learning_rate": 1.4057659960695069e-05,
        "epoch": 0.2778895595797004,
        "step": 3729
    },
    {
        "loss": 2.2347,
        "grad_norm": 3.0532498359680176,
        "learning_rate": 1.4021705768488336e-05,
        "epoch": 0.27796408078098217,
        "step": 3730
    },
    {
        "loss": 2.5249,
        "grad_norm": 3.4274306297302246,
        "learning_rate": 1.3985794146950338e-05,
        "epoch": 0.27803860198226393,
        "step": 3731
    },
    {
        "loss": 2.6336,
        "grad_norm": 2.228576183319092,
        "learning_rate": 1.394992511386205e-05,
        "epoch": 0.2781131231835457,
        "step": 3732
    },
    {
        "loss": 1.7868,
        "grad_norm": 2.629575729370117,
        "learning_rate": 1.3914098686983479e-05,
        "epoch": 0.27818764438482746,
        "step": 3733
    },
    {
        "loss": 2.1949,
        "grad_norm": 2.6073813438415527,
        "learning_rate": 1.3878314884053434e-05,
        "epoch": 0.2782621655861092,
        "step": 3734
    },
    {
        "loss": 1.8667,
        "grad_norm": 2.355682611465454,
        "learning_rate": 1.3842573722789599e-05,
        "epoch": 0.278336686787391,
        "step": 3735
    },
    {
        "loss": 2.555,
        "grad_norm": 1.8794461488723755,
        "learning_rate": 1.3806875220888637e-05,
        "epoch": 0.2784112079886728,
        "step": 3736
    },
    {
        "loss": 2.2963,
        "grad_norm": 4.238868236541748,
        "learning_rate": 1.3771219396026024e-05,
        "epoch": 0.27848572918995457,
        "step": 3737
    },
    {
        "loss": 3.1394,
        "grad_norm": 3.0394091606140137,
        "learning_rate": 1.3735606265856172e-05,
        "epoch": 0.27856025039123633,
        "step": 3738
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.7529361248016357,
        "learning_rate": 1.3700035848012283e-05,
        "epoch": 0.2786347715925181,
        "step": 3739
    },
    {
        "loss": 0.9955,
        "grad_norm": 3.0260426998138428,
        "learning_rate": 1.3664508160106404e-05,
        "epoch": 0.27870929279379986,
        "step": 3740
    },
    {
        "loss": 2.7627,
        "grad_norm": 1.260860562324524,
        "learning_rate": 1.3629023219729487e-05,
        "epoch": 0.2787838139950816,
        "step": 3741
    },
    {
        "loss": 2.0275,
        "grad_norm": 3.0689761638641357,
        "learning_rate": 1.3593581044451253e-05,
        "epoch": 0.2788583351963634,
        "step": 3742
    },
    {
        "loss": 1.7956,
        "grad_norm": 3.7177350521087646,
        "learning_rate": 1.355818165182039e-05,
        "epoch": 0.27893285639764515,
        "step": 3743
    },
    {
        "loss": 3.2825,
        "grad_norm": 3.0663468837738037,
        "learning_rate": 1.3522825059364164e-05,
        "epoch": 0.2790073775989269,
        "step": 3744
    },
    {
        "loss": 2.2434,
        "grad_norm": 2.739962339401245,
        "learning_rate": 1.3487511284588838e-05,
        "epoch": 0.27908189880020867,
        "step": 3745
    },
    {
        "loss": 2.1473,
        "grad_norm": 2.1082887649536133,
        "learning_rate": 1.3452240344979417e-05,
        "epoch": 0.27915642000149044,
        "step": 3746
    },
    {
        "loss": 2.3361,
        "grad_norm": 3.009780168533325,
        "learning_rate": 1.3417012257999729e-05,
        "epoch": 0.2792309412027722,
        "step": 3747
    },
    {
        "loss": 2.329,
        "grad_norm": 3.1904191970825195,
        "learning_rate": 1.3381827041092287e-05,
        "epoch": 0.27930546240405396,
        "step": 3748
    },
    {
        "loss": 2.1288,
        "grad_norm": 2.034214496612549,
        "learning_rate": 1.3346684711678492e-05,
        "epoch": 0.2793799836053357,
        "step": 3749
    },
    {
        "loss": 2.6196,
        "grad_norm": 2.1645689010620117,
        "learning_rate": 1.3311585287158467e-05,
        "epoch": 0.2794545048066175,
        "step": 3750
    },
    {
        "loss": 2.3963,
        "grad_norm": 3.176177978515625,
        "learning_rate": 1.3276528784911025e-05,
        "epoch": 0.27952902600789925,
        "step": 3751
    },
    {
        "loss": 2.4881,
        "grad_norm": 3.2377829551696777,
        "learning_rate": 1.3241515222293876e-05,
        "epoch": 0.279603547209181,
        "step": 3752
    },
    {
        "loss": 2.194,
        "grad_norm": 2.819272756576538,
        "learning_rate": 1.3206544616643313e-05,
        "epoch": 0.2796780684104628,
        "step": 3753
    },
    {
        "loss": 2.5644,
        "grad_norm": 1.8631073236465454,
        "learning_rate": 1.3171616985274448e-05,
        "epoch": 0.27975258961174454,
        "step": 3754
    },
    {
        "loss": 2.2183,
        "grad_norm": 1.4174829721450806,
        "learning_rate": 1.3136732345481129e-05,
        "epoch": 0.2798271108130263,
        "step": 3755
    },
    {
        "loss": 2.7218,
        "grad_norm": 3.5203781127929688,
        "learning_rate": 1.3101890714535813e-05,
        "epoch": 0.27990163201430807,
        "step": 3756
    },
    {
        "loss": 2.2865,
        "grad_norm": 2.885305404663086,
        "learning_rate": 1.3067092109689805e-05,
        "epoch": 0.27997615321558983,
        "step": 3757
    },
    {
        "loss": 2.6685,
        "grad_norm": 1.8985344171524048,
        "learning_rate": 1.3032336548172985e-05,
        "epoch": 0.2800506744168716,
        "step": 3758
    },
    {
        "loss": 2.5206,
        "grad_norm": 1.9566657543182373,
        "learning_rate": 1.299762404719399e-05,
        "epoch": 0.28012519561815336,
        "step": 3759
    },
    {
        "loss": 2.5647,
        "grad_norm": 3.2287304401397705,
        "learning_rate": 1.2962954623940127e-05,
        "epoch": 0.2801997168194351,
        "step": 3760
    },
    {
        "loss": 2.4073,
        "grad_norm": 1.722253680229187,
        "learning_rate": 1.2928328295577352e-05,
        "epoch": 0.2802742380207169,
        "step": 3761
    },
    {
        "loss": 1.9619,
        "grad_norm": 3.130561351776123,
        "learning_rate": 1.2893745079250274e-05,
        "epoch": 0.28034875922199864,
        "step": 3762
    },
    {
        "loss": 2.072,
        "grad_norm": 3.288130283355713,
        "learning_rate": 1.2859204992082164e-05,
        "epoch": 0.2804232804232804,
        "step": 3763
    },
    {
        "loss": 2.9811,
        "grad_norm": 3.452226400375366,
        "learning_rate": 1.2824708051175016e-05,
        "epoch": 0.28049780162456217,
        "step": 3764
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.3380002975463867,
        "learning_rate": 1.2790254273609338e-05,
        "epoch": 0.28057232282584393,
        "step": 3765
    },
    {
        "loss": 2.4415,
        "grad_norm": 1.8144443035125732,
        "learning_rate": 1.2755843676444367e-05,
        "epoch": 0.2806468440271257,
        "step": 3766
    },
    {
        "loss": 2.8274,
        "grad_norm": 2.2708470821380615,
        "learning_rate": 1.2721476276717869e-05,
        "epoch": 0.28072136522840746,
        "step": 3767
    },
    {
        "loss": 2.3262,
        "grad_norm": 4.221375465393066,
        "learning_rate": 1.2687152091446265e-05,
        "epoch": 0.2807958864296892,
        "step": 3768
    },
    {
        "loss": 2.5465,
        "grad_norm": 1.7272833585739136,
        "learning_rate": 1.2652871137624656e-05,
        "epoch": 0.280870407630971,
        "step": 3769
    },
    {
        "loss": 2.5843,
        "grad_norm": 2.6242516040802,
        "learning_rate": 1.2618633432226602e-05,
        "epoch": 0.28094492883225275,
        "step": 3770
    },
    {
        "loss": 2.3034,
        "grad_norm": 2.8011770248413086,
        "learning_rate": 1.258443899220434e-05,
        "epoch": 0.28101945003353457,
        "step": 3771
    },
    {
        "loss": 2.0733,
        "grad_norm": 4.8297271728515625,
        "learning_rate": 1.2550287834488628e-05,
        "epoch": 0.28109397123481633,
        "step": 3772
    },
    {
        "loss": 2.758,
        "grad_norm": 2.847949981689453,
        "learning_rate": 1.251617997598884e-05,
        "epoch": 0.2811684924360981,
        "step": 3773
    },
    {
        "loss": 2.3984,
        "grad_norm": 2.0705299377441406,
        "learning_rate": 1.2482115433592889e-05,
        "epoch": 0.28124301363737986,
        "step": 3774
    },
    {
        "loss": 2.5572,
        "grad_norm": 2.4765963554382324,
        "learning_rate": 1.2448094224167273e-05,
        "epoch": 0.2813175348386616,
        "step": 3775
    },
    {
        "loss": 2.2175,
        "grad_norm": 2.6085383892059326,
        "learning_rate": 1.2414116364556983e-05,
        "epoch": 0.2813920560399434,
        "step": 3776
    },
    {
        "loss": 1.6516,
        "grad_norm": 2.4334490299224854,
        "learning_rate": 1.2380181871585527e-05,
        "epoch": 0.28146657724122515,
        "step": 3777
    },
    {
        "loss": 1.8711,
        "grad_norm": 2.812495470046997,
        "learning_rate": 1.2346290762055046e-05,
        "epoch": 0.2815410984425069,
        "step": 3778
    },
    {
        "loss": 2.4649,
        "grad_norm": 1.7916440963745117,
        "learning_rate": 1.2312443052746126e-05,
        "epoch": 0.2816156196437887,
        "step": 3779
    },
    {
        "loss": 2.5289,
        "grad_norm": 2.2651748657226562,
        "learning_rate": 1.2278638760417882e-05,
        "epoch": 0.28169014084507044,
        "step": 3780
    },
    {
        "loss": 2.7741,
        "grad_norm": 2.641690254211426,
        "learning_rate": 1.2244877901807916e-05,
        "epoch": 0.2817646620463522,
        "step": 3781
    },
    {
        "loss": 2.069,
        "grad_norm": 2.6352760791778564,
        "learning_rate": 1.221116049363229e-05,
        "epoch": 0.28183918324763396,
        "step": 3782
    },
    {
        "loss": 2.1848,
        "grad_norm": 2.7108118534088135,
        "learning_rate": 1.217748655258566e-05,
        "epoch": 0.2819137044489157,
        "step": 3783
    },
    {
        "loss": 2.4985,
        "grad_norm": 3.008301019668579,
        "learning_rate": 1.2143856095341067e-05,
        "epoch": 0.2819882256501975,
        "step": 3784
    },
    {
        "loss": 2.0534,
        "grad_norm": 4.470012664794922,
        "learning_rate": 1.211026913855009e-05,
        "epoch": 0.28206274685147925,
        "step": 3785
    },
    {
        "loss": 1.9452,
        "grad_norm": 3.178269624710083,
        "learning_rate": 1.2076725698842694e-05,
        "epoch": 0.282137268052761,
        "step": 3786
    },
    {
        "loss": 1.9759,
        "grad_norm": 2.425334930419922,
        "learning_rate": 1.2043225792827306e-05,
        "epoch": 0.2822117892540428,
        "step": 3787
    },
    {
        "loss": 2.2901,
        "grad_norm": 4.484609127044678,
        "learning_rate": 1.2009769437090878e-05,
        "epoch": 0.28228631045532454,
        "step": 3788
    },
    {
        "loss": 2.5282,
        "grad_norm": 3.7558701038360596,
        "learning_rate": 1.1976356648198728e-05,
        "epoch": 0.2823608316566063,
        "step": 3789
    },
    {
        "loss": 2.7132,
        "grad_norm": 1.984201431274414,
        "learning_rate": 1.1942987442694663e-05,
        "epoch": 0.28243535285788807,
        "step": 3790
    },
    {
        "loss": 2.2062,
        "grad_norm": 4.452252388000488,
        "learning_rate": 1.1909661837100805e-05,
        "epoch": 0.28250987405916983,
        "step": 3791
    },
    {
        "loss": 2.6705,
        "grad_norm": 1.5257071256637573,
        "learning_rate": 1.1876379847917762e-05,
        "epoch": 0.2825843952604516,
        "step": 3792
    },
    {
        "loss": 1.5307,
        "grad_norm": 4.280598163604736,
        "learning_rate": 1.1843141491624544e-05,
        "epoch": 0.28265891646173336,
        "step": 3793
    },
    {
        "loss": 2.6947,
        "grad_norm": 2.5402276515960693,
        "learning_rate": 1.1809946784678594e-05,
        "epoch": 0.2827334376630151,
        "step": 3794
    },
    {
        "loss": 1.77,
        "grad_norm": 4.367441177368164,
        "learning_rate": 1.1776795743515657e-05,
        "epoch": 0.2828079588642969,
        "step": 3795
    },
    {
        "loss": 2.8735,
        "grad_norm": 1.8544459342956543,
        "learning_rate": 1.174368838454989e-05,
        "epoch": 0.28288248006557865,
        "step": 3796
    },
    {
        "loss": 3.1538,
        "grad_norm": 3.61075496673584,
        "learning_rate": 1.1710624724173847e-05,
        "epoch": 0.2829570012668604,
        "step": 3797
    },
    {
        "loss": 2.4602,
        "grad_norm": 1.8314377069473267,
        "learning_rate": 1.16776047787584e-05,
        "epoch": 0.28303152246814217,
        "step": 3798
    },
    {
        "loss": 3.1765,
        "grad_norm": 3.367380380630493,
        "learning_rate": 1.1644628564652903e-05,
        "epoch": 0.28310604366942393,
        "step": 3799
    },
    {
        "loss": 2.1575,
        "grad_norm": 4.669183254241943,
        "learning_rate": 1.1611696098184844e-05,
        "epoch": 0.2831805648707057,
        "step": 3800
    },
    {
        "loss": 1.7569,
        "grad_norm": 3.699329137802124,
        "learning_rate": 1.1578807395660207e-05,
        "epoch": 0.28325508607198746,
        "step": 3801
    },
    {
        "loss": 2.7308,
        "grad_norm": 3.330991506576538,
        "learning_rate": 1.154596247336327e-05,
        "epoch": 0.2833296072732692,
        "step": 3802
    },
    {
        "loss": 2.0894,
        "grad_norm": 4.099934101104736,
        "learning_rate": 1.1513161347556622e-05,
        "epoch": 0.283404128474551,
        "step": 3803
    },
    {
        "loss": 2.5303,
        "grad_norm": 2.449366569519043,
        "learning_rate": 1.1480404034481207e-05,
        "epoch": 0.28347864967583275,
        "step": 3804
    },
    {
        "loss": 2.7736,
        "grad_norm": 1.988916039466858,
        "learning_rate": 1.1447690550356205e-05,
        "epoch": 0.2835531708771145,
        "step": 3805
    },
    {
        "loss": 1.6479,
        "grad_norm": 2.193049192428589,
        "learning_rate": 1.1415020911379138e-05,
        "epoch": 0.2836276920783963,
        "step": 3806
    },
    {
        "loss": 1.8407,
        "grad_norm": 3.493300437927246,
        "learning_rate": 1.1382395133725809e-05,
        "epoch": 0.2837022132796781,
        "step": 3807
    },
    {
        "loss": 1.5674,
        "grad_norm": 1.991655945777893,
        "learning_rate": 1.1349813233550355e-05,
        "epoch": 0.28377673448095986,
        "step": 3808
    },
    {
        "loss": 2.368,
        "grad_norm": 3.23201584815979,
        "learning_rate": 1.131727522698508e-05,
        "epoch": 0.2838512556822416,
        "step": 3809
    },
    {
        "loss": 1.936,
        "grad_norm": 2.2558705806732178,
        "learning_rate": 1.1284781130140653e-05,
        "epoch": 0.2839257768835234,
        "step": 3810
    },
    {
        "loss": 2.1937,
        "grad_norm": 3.1961545944213867,
        "learning_rate": 1.1252330959105971e-05,
        "epoch": 0.28400029808480515,
        "step": 3811
    },
    {
        "loss": 1.9064,
        "grad_norm": 3.366122007369995,
        "learning_rate": 1.1219924729948116e-05,
        "epoch": 0.2840748192860869,
        "step": 3812
    },
    {
        "loss": 2.3449,
        "grad_norm": 2.6976876258850098,
        "learning_rate": 1.1187562458712563e-05,
        "epoch": 0.2841493404873687,
        "step": 3813
    },
    {
        "loss": 2.5863,
        "grad_norm": 3.001251220703125,
        "learning_rate": 1.1155244161422874e-05,
        "epoch": 0.28422386168865044,
        "step": 3814
    },
    {
        "loss": 3.1065,
        "grad_norm": 1.667128086090088,
        "learning_rate": 1.11229698540809e-05,
        "epoch": 0.2842983828899322,
        "step": 3815
    },
    {
        "loss": 2.672,
        "grad_norm": 1.9498696327209473,
        "learning_rate": 1.109073955266674e-05,
        "epoch": 0.28437290409121396,
        "step": 3816
    },
    {
        "loss": 3.017,
        "grad_norm": 1.754280924797058,
        "learning_rate": 1.1058553273138606e-05,
        "epoch": 0.2844474252924957,
        "step": 3817
    },
    {
        "loss": 1.8013,
        "grad_norm": 3.101386308670044,
        "learning_rate": 1.1026411031433093e-05,
        "epoch": 0.2845219464937775,
        "step": 3818
    },
    {
        "loss": 1.9514,
        "grad_norm": 3.1752562522888184,
        "learning_rate": 1.0994312843464738e-05,
        "epoch": 0.28459646769505925,
        "step": 3819
    },
    {
        "loss": 2.2929,
        "grad_norm": 5.270883560180664,
        "learning_rate": 1.0962258725126507e-05,
        "epoch": 0.284670988896341,
        "step": 3820
    },
    {
        "loss": 2.2648,
        "grad_norm": 2.468604803085327,
        "learning_rate": 1.0930248692289402e-05,
        "epoch": 0.2847455100976228,
        "step": 3821
    },
    {
        "loss": 2.4891,
        "grad_norm": 3.5984108448028564,
        "learning_rate": 1.0898282760802658e-05,
        "epoch": 0.28482003129890454,
        "step": 3822
    },
    {
        "loss": 2.3245,
        "grad_norm": 2.906344413757324,
        "learning_rate": 1.0866360946493625e-05,
        "epoch": 0.2848945525001863,
        "step": 3823
    },
    {
        "loss": 2.2772,
        "grad_norm": 4.345218658447266,
        "learning_rate": 1.0834483265167828e-05,
        "epoch": 0.28496907370146807,
        "step": 3824
    },
    {
        "loss": 2.4015,
        "grad_norm": 3.3913965225219727,
        "learning_rate": 1.0802649732609028e-05,
        "epoch": 0.28504359490274983,
        "step": 3825
    },
    {
        "loss": 2.8197,
        "grad_norm": 1.4009698629379272,
        "learning_rate": 1.0770860364578984e-05,
        "epoch": 0.2851181161040316,
        "step": 3826
    },
    {
        "loss": 2.6276,
        "grad_norm": 2.43047833442688,
        "learning_rate": 1.0739115176817672e-05,
        "epoch": 0.28519263730531336,
        "step": 3827
    },
    {
        "loss": 1.7729,
        "grad_norm": 3.2009220123291016,
        "learning_rate": 1.0707414185043163e-05,
        "epoch": 0.2852671585065951,
        "step": 3828
    },
    {
        "loss": 2.6999,
        "grad_norm": 2.5813608169555664,
        "learning_rate": 1.0675757404951647e-05,
        "epoch": 0.2853416797078769,
        "step": 3829
    },
    {
        "loss": 2.5456,
        "grad_norm": 1.7717715501785278,
        "learning_rate": 1.0644144852217474e-05,
        "epoch": 0.28541620090915865,
        "step": 3830
    },
    {
        "loss": 1.7276,
        "grad_norm": 3.1549558639526367,
        "learning_rate": 1.0612576542493025e-05,
        "epoch": 0.2854907221104404,
        "step": 3831
    },
    {
        "loss": 1.7269,
        "grad_norm": 2.239938497543335,
        "learning_rate": 1.0581052491408815e-05,
        "epoch": 0.28556524331172217,
        "step": 3832
    },
    {
        "loss": 2.5821,
        "grad_norm": 2.7497541904449463,
        "learning_rate": 1.054957271457342e-05,
        "epoch": 0.28563976451300394,
        "step": 3833
    },
    {
        "loss": 2.0415,
        "grad_norm": 2.798286199569702,
        "learning_rate": 1.051813722757351e-05,
        "epoch": 0.2857142857142857,
        "step": 3834
    },
    {
        "loss": 2.2645,
        "grad_norm": 2.3127264976501465,
        "learning_rate": 1.0486746045973828e-05,
        "epoch": 0.28578880691556746,
        "step": 3835
    },
    {
        "loss": 2.2139,
        "grad_norm": 2.765550136566162,
        "learning_rate": 1.0455399185317172e-05,
        "epoch": 0.2858633281168492,
        "step": 3836
    },
    {
        "loss": 1.906,
        "grad_norm": 2.5746402740478516,
        "learning_rate": 1.042409666112445e-05,
        "epoch": 0.285937849318131,
        "step": 3837
    },
    {
        "loss": 2.2975,
        "grad_norm": 2.430680751800537,
        "learning_rate": 1.0392838488894463e-05,
        "epoch": 0.28601237051941275,
        "step": 3838
    },
    {
        "loss": 2.0166,
        "grad_norm": 3.6194937229156494,
        "learning_rate": 1.036162468410422e-05,
        "epoch": 0.2860868917206945,
        "step": 3839
    },
    {
        "loss": 2.5594,
        "grad_norm": 1.6820274591445923,
        "learning_rate": 1.0330455262208704e-05,
        "epoch": 0.2861614129219763,
        "step": 3840
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.671293258666992,
        "learning_rate": 1.0299330238640925e-05,
        "epoch": 0.28623593412325804,
        "step": 3841
    },
    {
        "loss": 2.3562,
        "grad_norm": 3.3214824199676514,
        "learning_rate": 1.0268249628811866e-05,
        "epoch": 0.28631045532453986,
        "step": 3842
    },
    {
        "loss": 2.5724,
        "grad_norm": 3.642787218093872,
        "learning_rate": 1.0237213448110539e-05,
        "epoch": 0.2863849765258216,
        "step": 3843
    },
    {
        "loss": 2.1693,
        "grad_norm": 2.4762938022613525,
        "learning_rate": 1.020622171190403e-05,
        "epoch": 0.2864594977271034,
        "step": 3844
    },
    {
        "loss": 2.39,
        "grad_norm": 1.8809236288070679,
        "learning_rate": 1.0175274435537319e-05,
        "epoch": 0.28653401892838515,
        "step": 3845
    },
    {
        "loss": 2.4925,
        "grad_norm": 2.714973211288452,
        "learning_rate": 1.0144371634333482e-05,
        "epoch": 0.2866085401296669,
        "step": 3846
    },
    {
        "loss": 1.877,
        "grad_norm": 2.6484079360961914,
        "learning_rate": 1.0113513323593416e-05,
        "epoch": 0.2866830613309487,
        "step": 3847
    },
    {
        "loss": 1.8611,
        "grad_norm": 3.150923728942871,
        "learning_rate": 1.0082699518596117e-05,
        "epoch": 0.28675758253223044,
        "step": 3848
    },
    {
        "loss": 2.4578,
        "grad_norm": 1.990078091621399,
        "learning_rate": 1.0051930234598505e-05,
        "epoch": 0.2868321037335122,
        "step": 3849
    },
    {
        "loss": 2.4785,
        "grad_norm": 2.204756736755371,
        "learning_rate": 1.0021205486835505e-05,
        "epoch": 0.28690662493479396,
        "step": 3850
    },
    {
        "loss": 2.3165,
        "grad_norm": 2.8383073806762695,
        "learning_rate": 9.990525290519914e-06,
        "epoch": 0.2869811461360757,
        "step": 3851
    },
    {
        "loss": 2.5644,
        "grad_norm": 3.0273361206054688,
        "learning_rate": 9.95988966084247e-06,
        "epoch": 0.2870556673373575,
        "step": 3852
    },
    {
        "loss": 1.8125,
        "grad_norm": 2.732692003250122,
        "learning_rate": 9.929298612971904e-06,
        "epoch": 0.28713018853863925,
        "step": 3853
    },
    {
        "loss": 2.6041,
        "grad_norm": 2.69796085357666,
        "learning_rate": 9.898752162054836e-06,
        "epoch": 0.287204709739921,
        "step": 3854
    },
    {
        "loss": 2.7674,
        "grad_norm": 1.6294853687286377,
        "learning_rate": 9.868250323215855e-06,
        "epoch": 0.2872792309412028,
        "step": 3855
    },
    {
        "loss": 2.268,
        "grad_norm": 3.9170379638671875,
        "learning_rate": 9.837793111557392e-06,
        "epoch": 0.28735375214248454,
        "step": 3856
    },
    {
        "loss": 2.4,
        "grad_norm": 1.6757962703704834,
        "learning_rate": 9.807380542159795e-06,
        "epoch": 0.2874282733437663,
        "step": 3857
    },
    {
        "loss": 2.6524,
        "grad_norm": 3.2262513637542725,
        "learning_rate": 9.777012630081317e-06,
        "epoch": 0.28750279454504807,
        "step": 3858
    },
    {
        "loss": 2.5387,
        "grad_norm": 1.8522430658340454,
        "learning_rate": 9.746689390358121e-06,
        "epoch": 0.28757731574632983,
        "step": 3859
    },
    {
        "loss": 2.8441,
        "grad_norm": 2.22886323928833,
        "learning_rate": 9.716410838004286e-06,
        "epoch": 0.2876518369476116,
        "step": 3860
    },
    {
        "loss": 2.2551,
        "grad_norm": 3.3031396865844727,
        "learning_rate": 9.68617698801163e-06,
        "epoch": 0.28772635814889336,
        "step": 3861
    },
    {
        "loss": 2.1644,
        "grad_norm": 4.587440013885498,
        "learning_rate": 9.655987855349969e-06,
        "epoch": 0.2878008793501751,
        "step": 3862
    },
    {
        "loss": 1.7357,
        "grad_norm": 2.772195339202881,
        "learning_rate": 9.62584345496691e-06,
        "epoch": 0.2878754005514569,
        "step": 3863
    },
    {
        "loss": 1.7872,
        "grad_norm": 1.762892246246338,
        "learning_rate": 9.595743801787948e-06,
        "epoch": 0.28794992175273865,
        "step": 3864
    },
    {
        "loss": 1.9293,
        "grad_norm": 3.6763393878936768,
        "learning_rate": 9.56568891071642e-06,
        "epoch": 0.2880244429540204,
        "step": 3865
    },
    {
        "loss": 2.6874,
        "grad_norm": 2.435061454772949,
        "learning_rate": 9.53567879663345e-06,
        "epoch": 0.2880989641553022,
        "step": 3866
    },
    {
        "loss": 2.2851,
        "grad_norm": 2.1229803562164307,
        "learning_rate": 9.50571347439807e-06,
        "epoch": 0.28817348535658394,
        "step": 3867
    },
    {
        "loss": 2.5217,
        "grad_norm": 3.3061788082122803,
        "learning_rate": 9.475792958847029e-06,
        "epoch": 0.2882480065578657,
        "step": 3868
    },
    {
        "loss": 2.6148,
        "grad_norm": 2.5516538619995117,
        "learning_rate": 9.445917264795034e-06,
        "epoch": 0.28832252775914746,
        "step": 3869
    },
    {
        "loss": 2.5678,
        "grad_norm": 3.6469192504882812,
        "learning_rate": 9.416086407034464e-06,
        "epoch": 0.2883970489604292,
        "step": 3870
    },
    {
        "loss": 2.7415,
        "grad_norm": 3.0260848999023438,
        "learning_rate": 9.386300400335567e-06,
        "epoch": 0.288471570161711,
        "step": 3871
    },
    {
        "loss": 3.1547,
        "grad_norm": 2.1330888271331787,
        "learning_rate": 9.356559259446396e-06,
        "epoch": 0.28854609136299275,
        "step": 3872
    },
    {
        "loss": 1.9294,
        "grad_norm": 2.9321537017822266,
        "learning_rate": 9.3268629990927e-06,
        "epoch": 0.2886206125642745,
        "step": 3873
    },
    {
        "loss": 3.0502,
        "grad_norm": 3.032496452331543,
        "learning_rate": 9.297211633978142e-06,
        "epoch": 0.2886951337655563,
        "step": 3874
    },
    {
        "loss": 2.5632,
        "grad_norm": 2.505610227584839,
        "learning_rate": 9.267605178784033e-06,
        "epoch": 0.28876965496683804,
        "step": 3875
    },
    {
        "loss": 2.194,
        "grad_norm": 2.905686140060425,
        "learning_rate": 9.238043648169525e-06,
        "epoch": 0.2888441761681198,
        "step": 3876
    },
    {
        "loss": 2.4374,
        "grad_norm": 1.899207353591919,
        "learning_rate": 9.20852705677151e-06,
        "epoch": 0.2889186973694016,
        "step": 3877
    },
    {
        "loss": 2.5456,
        "grad_norm": 3.64734148979187,
        "learning_rate": 9.179055419204563e-06,
        "epoch": 0.2889932185706834,
        "step": 3878
    },
    {
        "loss": 2.6711,
        "grad_norm": 1.777968168258667,
        "learning_rate": 9.149628750061146e-06,
        "epoch": 0.28906773977196515,
        "step": 3879
    },
    {
        "loss": 2.5656,
        "grad_norm": 1.7041006088256836,
        "learning_rate": 9.12024706391127e-06,
        "epoch": 0.2891422609732469,
        "step": 3880
    },
    {
        "loss": 2.7457,
        "grad_norm": 2.5082004070281982,
        "learning_rate": 9.090910375302842e-06,
        "epoch": 0.2892167821745287,
        "step": 3881
    },
    {
        "loss": 2.4887,
        "grad_norm": 3.9295780658721924,
        "learning_rate": 9.061618698761376e-06,
        "epoch": 0.28929130337581044,
        "step": 3882
    },
    {
        "loss": 2.9312,
        "grad_norm": 2.629896879196167,
        "learning_rate": 9.03237204879015e-06,
        "epoch": 0.2893658245770922,
        "step": 3883
    },
    {
        "loss": 2.315,
        "grad_norm": 3.4220306873321533,
        "learning_rate": 9.003170439870168e-06,
        "epoch": 0.28944034577837396,
        "step": 3884
    },
    {
        "loss": 1.7765,
        "grad_norm": 4.212730884552002,
        "learning_rate": 8.974013886460042e-06,
        "epoch": 0.2895148669796557,
        "step": 3885
    },
    {
        "loss": 2.7583,
        "grad_norm": 2.9426300525665283,
        "learning_rate": 8.944902402996203e-06,
        "epoch": 0.2895893881809375,
        "step": 3886
    },
    {
        "loss": 2.9811,
        "grad_norm": 3.1546895503997803,
        "learning_rate": 8.915836003892652e-06,
        "epoch": 0.28966390938221925,
        "step": 3887
    },
    {
        "loss": 2.5187,
        "grad_norm": 2.487605094909668,
        "learning_rate": 8.886814703541147e-06,
        "epoch": 0.289738430583501,
        "step": 3888
    },
    {
        "loss": 1.7448,
        "grad_norm": 5.288761138916016,
        "learning_rate": 8.857838516311046e-06,
        "epoch": 0.2898129517847828,
        "step": 3889
    },
    {
        "loss": 1.9812,
        "grad_norm": 4.207014083862305,
        "learning_rate": 8.828907456549441e-06,
        "epoch": 0.28988747298606454,
        "step": 3890
    },
    {
        "loss": 2.6372,
        "grad_norm": 2.01383113861084,
        "learning_rate": 8.800021538581027e-06,
        "epoch": 0.2899619941873463,
        "step": 3891
    },
    {
        "loss": 2.1945,
        "grad_norm": 2.555943250656128,
        "learning_rate": 8.771180776708188e-06,
        "epoch": 0.29003651538862807,
        "step": 3892
    },
    {
        "loss": 2.3262,
        "grad_norm": 2.0067522525787354,
        "learning_rate": 8.742385185210944e-06,
        "epoch": 0.29011103658990983,
        "step": 3893
    },
    {
        "loss": 1.3673,
        "grad_norm": 2.5694150924682617,
        "learning_rate": 8.713634778346868e-06,
        "epoch": 0.2901855577911916,
        "step": 3894
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.5365407466888428,
        "learning_rate": 8.684929570351275e-06,
        "epoch": 0.29026007899247336,
        "step": 3895
    },
    {
        "loss": 2.5889,
        "grad_norm": 2.4873766899108887,
        "learning_rate": 8.65626957543706e-06,
        "epoch": 0.2903346001937551,
        "step": 3896
    },
    {
        "loss": 2.6517,
        "grad_norm": 2.58957576751709,
        "learning_rate": 8.627654807794705e-06,
        "epoch": 0.2904091213950369,
        "step": 3897
    },
    {
        "loss": 1.1906,
        "grad_norm": 3.1625030040740967,
        "learning_rate": 8.599085281592355e-06,
        "epoch": 0.29048364259631865,
        "step": 3898
    },
    {
        "loss": 2.4967,
        "grad_norm": 4.600278854370117,
        "learning_rate": 8.570561010975654e-06,
        "epoch": 0.2905581637976004,
        "step": 3899
    },
    {
        "loss": 2.5643,
        "grad_norm": 2.4284071922302246,
        "learning_rate": 8.542082010067942e-06,
        "epoch": 0.2906326849988822,
        "step": 3900
    },
    {
        "loss": 2.6016,
        "grad_norm": 2.454988479614258,
        "learning_rate": 8.513648292970111e-06,
        "epoch": 0.29070720620016394,
        "step": 3901
    },
    {
        "loss": 2.7622,
        "grad_norm": 2.693964719772339,
        "learning_rate": 8.48525987376062e-06,
        "epoch": 0.2907817274014457,
        "step": 3902
    },
    {
        "loss": 2.7754,
        "grad_norm": 2.9499802589416504,
        "learning_rate": 8.456916766495515e-06,
        "epoch": 0.29085624860272746,
        "step": 3903
    },
    {
        "loss": 1.7894,
        "grad_norm": 1.9965109825134277,
        "learning_rate": 8.428618985208336e-06,
        "epoch": 0.2909307698040092,
        "step": 3904
    },
    {
        "loss": 2.8441,
        "grad_norm": 3.086965560913086,
        "learning_rate": 8.400366543910298e-06,
        "epoch": 0.291005291005291,
        "step": 3905
    },
    {
        "loss": 2.1252,
        "grad_norm": 1.606933832168579,
        "learning_rate": 8.372159456590112e-06,
        "epoch": 0.29107981220657275,
        "step": 3906
    },
    {
        "loss": 2.1257,
        "grad_norm": 3.731029510498047,
        "learning_rate": 8.343997737214048e-06,
        "epoch": 0.2911543334078545,
        "step": 3907
    },
    {
        "loss": 1.9609,
        "grad_norm": 3.525184392929077,
        "learning_rate": 8.315881399725823e-06,
        "epoch": 0.2912288546091363,
        "step": 3908
    },
    {
        "loss": 2.7711,
        "grad_norm": 2.3949501514434814,
        "learning_rate": 8.287810458046808e-06,
        "epoch": 0.29130337581041804,
        "step": 3909
    },
    {
        "loss": 1.0696,
        "grad_norm": 3.0724971294403076,
        "learning_rate": 8.259784926075808e-06,
        "epoch": 0.2913778970116998,
        "step": 3910
    },
    {
        "loss": 2.337,
        "grad_norm": 1.7843953371047974,
        "learning_rate": 8.231804817689248e-06,
        "epoch": 0.29145241821298157,
        "step": 3911
    },
    {
        "loss": 2.3365,
        "grad_norm": 3.3661766052246094,
        "learning_rate": 8.203870146740956e-06,
        "epoch": 0.29152693941426333,
        "step": 3912
    },
    {
        "loss": 2.5164,
        "grad_norm": 2.7943789958953857,
        "learning_rate": 8.175980927062288e-06,
        "epoch": 0.29160146061554515,
        "step": 3913
    },
    {
        "loss": 2.3336,
        "grad_norm": 2.3308193683624268,
        "learning_rate": 8.148137172462112e-06,
        "epoch": 0.2916759818168269,
        "step": 3914
    },
    {
        "loss": 2.2358,
        "grad_norm": 5.0285444259643555,
        "learning_rate": 8.120338896726786e-06,
        "epoch": 0.2917505030181087,
        "step": 3915
    },
    {
        "loss": 1.3535,
        "grad_norm": 3.791435718536377,
        "learning_rate": 8.092586113620215e-06,
        "epoch": 0.29182502421939044,
        "step": 3916
    },
    {
        "loss": 1.8724,
        "grad_norm": 3.1636810302734375,
        "learning_rate": 8.064878836883605e-06,
        "epoch": 0.2918995454206722,
        "step": 3917
    },
    {
        "loss": 1.7655,
        "grad_norm": 3.8485536575317383,
        "learning_rate": 8.037217080235792e-06,
        "epoch": 0.29197406662195396,
        "step": 3918
    },
    {
        "loss": 2.7527,
        "grad_norm": 3.9940669536590576,
        "learning_rate": 8.009600857373e-06,
        "epoch": 0.29204858782323573,
        "step": 3919
    },
    {
        "loss": 1.9342,
        "grad_norm": 3.920152425765991,
        "learning_rate": 7.982030181968914e-06,
        "epoch": 0.2921231090245175,
        "step": 3920
    },
    {
        "loss": 2.4857,
        "grad_norm": 2.270183801651001,
        "learning_rate": 7.954505067674756e-06,
        "epoch": 0.29219763022579925,
        "step": 3921
    },
    {
        "loss": 2.8115,
        "grad_norm": 2.1284735202789307,
        "learning_rate": 7.927025528119014e-06,
        "epoch": 0.292272151427081,
        "step": 3922
    },
    {
        "loss": 2.202,
        "grad_norm": 3.272050619125366,
        "learning_rate": 7.89959157690775e-06,
        "epoch": 0.2923466726283628,
        "step": 3923
    },
    {
        "loss": 2.0236,
        "grad_norm": 3.3753318786621094,
        "learning_rate": 7.872203227624398e-06,
        "epoch": 0.29242119382964454,
        "step": 3924
    },
    {
        "loss": 2.4822,
        "grad_norm": 2.3907430171966553,
        "learning_rate": 7.844860493829841e-06,
        "epoch": 0.2924957150309263,
        "step": 3925
    },
    {
        "loss": 2.5404,
        "grad_norm": 1.8872301578521729,
        "learning_rate": 7.817563389062377e-06,
        "epoch": 0.29257023623220807,
        "step": 3926
    },
    {
        "loss": 0.9957,
        "grad_norm": 3.06018328666687,
        "learning_rate": 7.790311926837656e-06,
        "epoch": 0.29264475743348983,
        "step": 3927
    },
    {
        "loss": 1.0346,
        "grad_norm": 3.2346439361572266,
        "learning_rate": 7.763106120648812e-06,
        "epoch": 0.2927192786347716,
        "step": 3928
    },
    {
        "loss": 2.4634,
        "grad_norm": 2.762927532196045,
        "learning_rate": 7.735945983966275e-06,
        "epoch": 0.29279379983605336,
        "step": 3929
    },
    {
        "loss": 2.3187,
        "grad_norm": 4.054633140563965,
        "learning_rate": 7.708831530237959e-06,
        "epoch": 0.2928683210373351,
        "step": 3930
    },
    {
        "loss": 2.2212,
        "grad_norm": 2.70414137840271,
        "learning_rate": 7.681762772889134e-06,
        "epoch": 0.2929428422386169,
        "step": 3931
    },
    {
        "loss": 2.5236,
        "grad_norm": 3.29818058013916,
        "learning_rate": 7.654739725322391e-06,
        "epoch": 0.29301736343989865,
        "step": 3932
    },
    {
        "loss": 2.376,
        "grad_norm": 2.112297296524048,
        "learning_rate": 7.627762400917748e-06,
        "epoch": 0.2930918846411804,
        "step": 3933
    },
    {
        "loss": 1.5954,
        "grad_norm": 3.296226978302002,
        "learning_rate": 7.600830813032511e-06,
        "epoch": 0.2931664058424622,
        "step": 3934
    },
    {
        "loss": 2.5216,
        "grad_norm": 1.8940752744674683,
        "learning_rate": 7.573944975001502e-06,
        "epoch": 0.29324092704374394,
        "step": 3935
    },
    {
        "loss": 1.5793,
        "grad_norm": 5.306678771972656,
        "learning_rate": 7.547104900136648e-06,
        "epoch": 0.2933154482450257,
        "step": 3936
    },
    {
        "loss": 2.8517,
        "grad_norm": 1.923518180847168,
        "learning_rate": 7.5203106017274425e-06,
        "epoch": 0.29338996944630746,
        "step": 3937
    },
    {
        "loss": 2.7816,
        "grad_norm": 3.7661423683166504,
        "learning_rate": 7.493562093040574e-06,
        "epoch": 0.2934644906475892,
        "step": 3938
    },
    {
        "loss": 2.344,
        "grad_norm": 3.982523202896118,
        "learning_rate": 7.466859387320124e-06,
        "epoch": 0.293539011848871,
        "step": 3939
    },
    {
        "loss": 2.7719,
        "grad_norm": 2.2115347385406494,
        "learning_rate": 7.440202497787485e-06,
        "epoch": 0.29361353305015275,
        "step": 3940
    },
    {
        "loss": 2.6523,
        "grad_norm": 4.132196426391602,
        "learning_rate": 7.41359143764131e-06,
        "epoch": 0.2936880542514345,
        "step": 3941
    },
    {
        "loss": 1.1763,
        "grad_norm": 2.6025962829589844,
        "learning_rate": 7.38702622005768e-06,
        "epoch": 0.2937625754527163,
        "step": 3942
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.8665695190429688,
        "learning_rate": 7.3605068581898414e-06,
        "epoch": 0.29383709665399804,
        "step": 3943
    },
    {
        "loss": 1.824,
        "grad_norm": 3.193521738052368,
        "learning_rate": 7.334033365168413e-06,
        "epoch": 0.2939116178552798,
        "step": 3944
    },
    {
        "loss": 2.2423,
        "grad_norm": 2.317976951599121,
        "learning_rate": 7.307605754101321e-06,
        "epoch": 0.29398613905656157,
        "step": 3945
    },
    {
        "loss": 1.7398,
        "grad_norm": 3.605787515640259,
        "learning_rate": 7.281224038073675e-06,
        "epoch": 0.29406066025784333,
        "step": 3946
    },
    {
        "loss": 2.7116,
        "grad_norm": 2.0199944972991943,
        "learning_rate": 7.254888230148005e-06,
        "epoch": 0.2941351814591251,
        "step": 3947
    },
    {
        "loss": 2.6051,
        "grad_norm": 2.4600465297698975,
        "learning_rate": 7.2285983433639795e-06,
        "epoch": 0.2942097026604069,
        "step": 3948
    },
    {
        "loss": 2.4935,
        "grad_norm": 2.2149205207824707,
        "learning_rate": 7.202354390738608e-06,
        "epoch": 0.2942842238616887,
        "step": 3949
    },
    {
        "loss": 1.786,
        "grad_norm": 2.678257703781128,
        "learning_rate": 7.176156385266098e-06,
        "epoch": 0.29435874506297044,
        "step": 3950
    },
    {
        "loss": 2.2377,
        "grad_norm": 3.40547513961792,
        "learning_rate": 7.15000433991797e-06,
        "epoch": 0.2944332662642522,
        "step": 3951
    },
    {
        "loss": 2.2847,
        "grad_norm": 2.1198012828826904,
        "learning_rate": 7.123898267642947e-06,
        "epoch": 0.29450778746553397,
        "step": 3952
    },
    {
        "loss": 2.0099,
        "grad_norm": 3.9371438026428223,
        "learning_rate": 7.097838181367e-06,
        "epoch": 0.29458230866681573,
        "step": 3953
    },
    {
        "loss": 2.135,
        "grad_norm": 2.7631924152374268,
        "learning_rate": 7.071824093993373e-06,
        "epoch": 0.2946568298680975,
        "step": 3954
    },
    {
        "loss": 2.0103,
        "grad_norm": 3.6488349437713623,
        "learning_rate": 7.045856018402397e-06,
        "epoch": 0.29473135106937925,
        "step": 3955
    },
    {
        "loss": 1.627,
        "grad_norm": 3.5515472888946533,
        "learning_rate": 7.019933967451786e-06,
        "epoch": 0.294805872270661,
        "step": 3956
    },
    {
        "loss": 2.4086,
        "grad_norm": 2.6665706634521484,
        "learning_rate": 6.994057953976385e-06,
        "epoch": 0.2948803934719428,
        "step": 3957
    },
    {
        "loss": 2.1877,
        "grad_norm": 2.0496387481689453,
        "learning_rate": 6.968227990788267e-06,
        "epoch": 0.29495491467322454,
        "step": 3958
    },
    {
        "loss": 2.6845,
        "grad_norm": 2.049462080001831,
        "learning_rate": 6.9424440906766805e-06,
        "epoch": 0.2950294358745063,
        "step": 3959
    },
    {
        "loss": 1.5934,
        "grad_norm": 5.536543369293213,
        "learning_rate": 6.916706266408046e-06,
        "epoch": 0.29510395707578807,
        "step": 3960
    },
    {
        "loss": 1.6224,
        "grad_norm": 2.6726572513580322,
        "learning_rate": 6.891014530726059e-06,
        "epoch": 0.29517847827706983,
        "step": 3961
    },
    {
        "loss": 1.758,
        "grad_norm": 2.9573826789855957,
        "learning_rate": 6.8653688963514985e-06,
        "epoch": 0.2952529994783516,
        "step": 3962
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.5002477169036865,
        "learning_rate": 6.839769375982408e-06,
        "epoch": 0.29532752067963336,
        "step": 3963
    },
    {
        "loss": 2.2126,
        "grad_norm": 2.0473527908325195,
        "learning_rate": 6.814215982293892e-06,
        "epoch": 0.2954020418809151,
        "step": 3964
    },
    {
        "loss": 1.9145,
        "grad_norm": 5.726983547210693,
        "learning_rate": 6.788708727938264e-06,
        "epoch": 0.2954765630821969,
        "step": 3965
    },
    {
        "loss": 2.5219,
        "grad_norm": 2.0395779609680176,
        "learning_rate": 6.763247625545055e-06,
        "epoch": 0.29555108428347865,
        "step": 3966
    },
    {
        "loss": 2.1012,
        "grad_norm": 3.7905030250549316,
        "learning_rate": 6.737832687720869e-06,
        "epoch": 0.2956256054847604,
        "step": 3967
    },
    {
        "loss": 2.4567,
        "grad_norm": 2.5711257457733154,
        "learning_rate": 6.7124639270494945e-06,
        "epoch": 0.2957001266860422,
        "step": 3968
    },
    {
        "loss": 2.7561,
        "grad_norm": 2.4864277839660645,
        "learning_rate": 6.687141356091786e-06,
        "epoch": 0.29577464788732394,
        "step": 3969
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.7577192783355713,
        "learning_rate": 6.661864987385802e-06,
        "epoch": 0.2958491690886057,
        "step": 3970
    },
    {
        "loss": 2.7401,
        "grad_norm": 3.807832717895508,
        "learning_rate": 6.636634833446676e-06,
        "epoch": 0.29592369028988746,
        "step": 3971
    },
    {
        "loss": 2.5653,
        "grad_norm": 2.4131813049316406,
        "learning_rate": 6.6114509067667585e-06,
        "epoch": 0.2959982114911692,
        "step": 3972
    },
    {
        "loss": 2.5724,
        "grad_norm": 2.029639720916748,
        "learning_rate": 6.5863132198153765e-06,
        "epoch": 0.296072732692451,
        "step": 3973
    },
    {
        "loss": 2.5334,
        "grad_norm": 2.9459829330444336,
        "learning_rate": 6.561221785039018e-06,
        "epoch": 0.29614725389373275,
        "step": 3974
    },
    {
        "loss": 1.976,
        "grad_norm": 2.9691951274871826,
        "learning_rate": 6.536176614861289e-06,
        "epoch": 0.2962217750950145,
        "step": 3975
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.50726580619812,
        "learning_rate": 6.51117772168286e-06,
        "epoch": 0.2962962962962963,
        "step": 3976
    },
    {
        "loss": 2.0624,
        "grad_norm": 2.131749153137207,
        "learning_rate": 6.48622511788154e-06,
        "epoch": 0.29637081749757804,
        "step": 3977
    },
    {
        "loss": 2.4959,
        "grad_norm": 3.2169406414031982,
        "learning_rate": 6.461318815812156e-06,
        "epoch": 0.2964453386988598,
        "step": 3978
    },
    {
        "loss": 2.5722,
        "grad_norm": 2.660658836364746,
        "learning_rate": 6.436458827806624e-06,
        "epoch": 0.29651985990014157,
        "step": 3979
    },
    {
        "loss": 1.373,
        "grad_norm": 2.6132311820983887,
        "learning_rate": 6.411645166173941e-06,
        "epoch": 0.29659438110142333,
        "step": 3980
    },
    {
        "loss": 2.2649,
        "grad_norm": 2.6740353107452393,
        "learning_rate": 6.386877843200167e-06,
        "epoch": 0.2966689023027051,
        "step": 3981
    },
    {
        "loss": 2.3294,
        "grad_norm": 2.713054895401001,
        "learning_rate": 6.362156871148428e-06,
        "epoch": 0.29674342350398686,
        "step": 3982
    },
    {
        "loss": 1.6418,
        "grad_norm": 2.816072702407837,
        "learning_rate": 6.337482262258864e-06,
        "epoch": 0.2968179447052686,
        "step": 3983
    },
    {
        "loss": 1.8799,
        "grad_norm": 3.923525810241699,
        "learning_rate": 6.312854028748705e-06,
        "epoch": 0.29689246590655044,
        "step": 3984
    },
    {
        "loss": 1.525,
        "grad_norm": 2.541536808013916,
        "learning_rate": 6.28827218281216e-06,
        "epoch": 0.2969669871078322,
        "step": 3985
    },
    {
        "loss": 2.2935,
        "grad_norm": 2.9440040588378906,
        "learning_rate": 6.26373673662054e-06,
        "epoch": 0.29704150830911397,
        "step": 3986
    },
    {
        "loss": 1.4638,
        "grad_norm": 3.4343979358673096,
        "learning_rate": 6.239247702322149e-06,
        "epoch": 0.29711602951039573,
        "step": 3987
    },
    {
        "loss": 2.2103,
        "grad_norm": 3.7021799087524414,
        "learning_rate": 6.214805092042286e-06,
        "epoch": 0.2971905507116775,
        "step": 3988
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.8608849048614502,
        "learning_rate": 6.1904089178833166e-06,
        "epoch": 0.29726507191295926,
        "step": 3989
    },
    {
        "loss": 2.2155,
        "grad_norm": 3.056511163711548,
        "learning_rate": 6.166059191924534e-06,
        "epoch": 0.297339593114241,
        "step": 3990
    },
    {
        "loss": 2.5301,
        "grad_norm": 3.0243892669677734,
        "learning_rate": 6.141755926222337e-06,
        "epoch": 0.2974141143155228,
        "step": 3991
    },
    {
        "loss": 2.6189,
        "grad_norm": 2.7590041160583496,
        "learning_rate": 6.117499132810067e-06,
        "epoch": 0.29748863551680454,
        "step": 3992
    },
    {
        "loss": 2.7952,
        "grad_norm": 2.352405309677124,
        "learning_rate": 6.093288823698029e-06,
        "epoch": 0.2975631567180863,
        "step": 3993
    },
    {
        "loss": 2.7229,
        "grad_norm": 2.302220582962036,
        "learning_rate": 6.069125010873555e-06,
        "epoch": 0.29763767791936807,
        "step": 3994
    },
    {
        "loss": 2.4303,
        "grad_norm": 3.053431272506714,
        "learning_rate": 6.045007706300909e-06,
        "epoch": 0.29771219912064983,
        "step": 3995
    },
    {
        "loss": 2.1599,
        "grad_norm": 2.7202532291412354,
        "learning_rate": 6.020936921921439e-06,
        "epoch": 0.2977867203219316,
        "step": 3996
    },
    {
        "loss": 1.7652,
        "grad_norm": 3.5037431716918945,
        "learning_rate": 5.996912669653265e-06,
        "epoch": 0.29786124152321336,
        "step": 3997
    },
    {
        "loss": 1.8708,
        "grad_norm": 3.3477659225463867,
        "learning_rate": 5.972934961391685e-06,
        "epoch": 0.2979357627244951,
        "step": 3998
    },
    {
        "loss": 2.1113,
        "grad_norm": 2.9144740104675293,
        "learning_rate": 5.949003809008779e-06,
        "epoch": 0.2980102839257769,
        "step": 3999
    },
    {
        "loss": 2.6609,
        "grad_norm": 2.473369598388672,
        "learning_rate": 5.92511922435367e-06,
        "epoch": 0.29808480512705865,
        "step": 4000
    },
    {
        "loss": 2.1667,
        "grad_norm": 2.887971878051758,
        "learning_rate": 5.901281219252408e-06,
        "epoch": 0.2981593263283404,
        "step": 4001
    },
    {
        "loss": 2.5785,
        "grad_norm": 3.341515302658081,
        "learning_rate": 5.8774898055079295e-06,
        "epoch": 0.2982338475296222,
        "step": 4002
    },
    {
        "loss": 2.3782,
        "grad_norm": 2.4239625930786133,
        "learning_rate": 5.853744994900201e-06,
        "epoch": 0.29830836873090394,
        "step": 4003
    },
    {
        "loss": 1.9148,
        "grad_norm": 2.687896251678467,
        "learning_rate": 5.830046799186018e-06,
        "epoch": 0.2983828899321857,
        "step": 4004
    },
    {
        "loss": 2.3392,
        "grad_norm": 3.0128109455108643,
        "learning_rate": 5.8063952300991374e-06,
        "epoch": 0.29845741113346747,
        "step": 4005
    },
    {
        "loss": 1.8594,
        "grad_norm": 2.244927406311035,
        "learning_rate": 5.782790299350238e-06,
        "epoch": 0.29853193233474923,
        "step": 4006
    },
    {
        "loss": 2.8139,
        "grad_norm": 1.6208547353744507,
        "learning_rate": 5.75923201862687e-06,
        "epoch": 0.298606453536031,
        "step": 4007
    },
    {
        "loss": 2.6641,
        "grad_norm": 2.22898006439209,
        "learning_rate": 5.735720399593536e-06,
        "epoch": 0.29868097473731275,
        "step": 4008
    },
    {
        "loss": 1.8512,
        "grad_norm": 2.880523204803467,
        "learning_rate": 5.71225545389158e-06,
        "epoch": 0.2987554959385945,
        "step": 4009
    },
    {
        "loss": 2.0083,
        "grad_norm": 2.727752447128296,
        "learning_rate": 5.688837193139307e-06,
        "epoch": 0.2988300171398763,
        "step": 4010
    },
    {
        "loss": 2.414,
        "grad_norm": 4.661719799041748,
        "learning_rate": 5.6654656289318206e-06,
        "epoch": 0.29890453834115804,
        "step": 4011
    },
    {
        "loss": 1.75,
        "grad_norm": 3.395476818084717,
        "learning_rate": 5.6421407728411755e-06,
        "epoch": 0.2989790595424398,
        "step": 4012
    },
    {
        "loss": 1.9447,
        "grad_norm": 2.96890926361084,
        "learning_rate": 5.618862636416256e-06,
        "epoch": 0.29905358074372157,
        "step": 4013
    },
    {
        "loss": 2.1321,
        "grad_norm": 2.8086371421813965,
        "learning_rate": 5.595631231182829e-06,
        "epoch": 0.29912810194500333,
        "step": 4014
    },
    {
        "loss": 2.8083,
        "grad_norm": 2.6304166316986084,
        "learning_rate": 5.5724465686435635e-06,
        "epoch": 0.2992026231462851,
        "step": 4015
    },
    {
        "loss": 1.7175,
        "grad_norm": 3.043853998184204,
        "learning_rate": 5.549308660277874e-06,
        "epoch": 0.29927714434756686,
        "step": 4016
    },
    {
        "loss": 2.5412,
        "grad_norm": 3.274290084838867,
        "learning_rate": 5.526217517542154e-06,
        "epoch": 0.2993516655488486,
        "step": 4017
    },
    {
        "loss": 2.3989,
        "grad_norm": 1.9094477891921997,
        "learning_rate": 5.503173151869556e-06,
        "epoch": 0.2994261867501304,
        "step": 4018
    },
    {
        "loss": 2.3135,
        "grad_norm": 2.0849194526672363,
        "learning_rate": 5.480175574670132e-06,
        "epoch": 0.2995007079514122,
        "step": 4019
    },
    {
        "loss": 2.8101,
        "grad_norm": 2.303161382675171,
        "learning_rate": 5.457224797330706e-06,
        "epoch": 0.29957522915269397,
        "step": 4020
    },
    {
        "loss": 2.5393,
        "grad_norm": 1.967667579650879,
        "learning_rate": 5.434320831214945e-06,
        "epoch": 0.29964975035397573,
        "step": 4021
    },
    {
        "loss": 2.4664,
        "grad_norm": 2.5481200218200684,
        "learning_rate": 5.411463687663377e-06,
        "epoch": 0.2997242715552575,
        "step": 4022
    },
    {
        "loss": 1.6785,
        "grad_norm": 2.6045706272125244,
        "learning_rate": 5.388653377993324e-06,
        "epoch": 0.29979879275653926,
        "step": 4023
    },
    {
        "loss": 2.1967,
        "grad_norm": 2.9675302505493164,
        "learning_rate": 5.365889913498934e-06,
        "epoch": 0.299873313957821,
        "step": 4024
    },
    {
        "loss": 2.7409,
        "grad_norm": 2.7604293823242188,
        "learning_rate": 5.343173305451121e-06,
        "epoch": 0.2999478351591028,
        "step": 4025
    },
    {
        "loss": 2.0573,
        "grad_norm": 2.8259594440460205,
        "learning_rate": 5.320503565097601e-06,
        "epoch": 0.30002235636038455,
        "step": 4026
    },
    {
        "loss": 2.3929,
        "grad_norm": 3.0455262660980225,
        "learning_rate": 5.2978807036629165e-06,
        "epoch": 0.3000968775616663,
        "step": 4027
    },
    {
        "loss": 1.5143,
        "grad_norm": 3.8099865913391113,
        "learning_rate": 5.2753047323484185e-06,
        "epoch": 0.30017139876294807,
        "step": 4028
    },
    {
        "loss": 0.8492,
        "grad_norm": 3.89755916595459,
        "learning_rate": 5.252775662332199e-06,
        "epoch": 0.30024591996422983,
        "step": 4029
    },
    {
        "loss": 1.6862,
        "grad_norm": 4.155319690704346,
        "learning_rate": 5.2302935047691015e-06,
        "epoch": 0.3003204411655116,
        "step": 4030
    },
    {
        "loss": 2.3974,
        "grad_norm": 1.8331133127212524,
        "learning_rate": 5.2078582707908105e-06,
        "epoch": 0.30039496236679336,
        "step": 4031
    },
    {
        "loss": 2.577,
        "grad_norm": 2.4312708377838135,
        "learning_rate": 5.18546997150573e-06,
        "epoch": 0.3004694835680751,
        "step": 4032
    },
    {
        "loss": 2.8415,
        "grad_norm": 2.36044979095459,
        "learning_rate": 5.163128617999069e-06,
        "epoch": 0.3005440047693569,
        "step": 4033
    },
    {
        "loss": 1.9753,
        "grad_norm": 2.357882499694824,
        "learning_rate": 5.140834221332746e-06,
        "epoch": 0.30061852597063865,
        "step": 4034
    },
    {
        "loss": 2.5135,
        "grad_norm": 2.1042871475219727,
        "learning_rate": 5.118586792545421e-06,
        "epoch": 0.3006930471719204,
        "step": 4035
    },
    {
        "loss": 2.4428,
        "grad_norm": 2.068193197250366,
        "learning_rate": 5.096386342652559e-06,
        "epoch": 0.3007675683732022,
        "step": 4036
    },
    {
        "loss": 2.817,
        "grad_norm": 3.32161283493042,
        "learning_rate": 5.0742328826463015e-06,
        "epoch": 0.30084208957448394,
        "step": 4037
    },
    {
        "loss": 2.6818,
        "grad_norm": 1.6899199485778809,
        "learning_rate": 5.052126423495585e-06,
        "epoch": 0.3009166107757657,
        "step": 4038
    },
    {
        "loss": 2.557,
        "grad_norm": 2.4482579231262207,
        "learning_rate": 5.030066976146031e-06,
        "epoch": 0.30099113197704747,
        "step": 4039
    },
    {
        "loss": 2.6914,
        "grad_norm": 3.028414011001587,
        "learning_rate": 5.00805455151998e-06,
        "epoch": 0.30106565317832923,
        "step": 4040
    },
    {
        "loss": 2.6818,
        "grad_norm": 3.2995100021362305,
        "learning_rate": 4.986089160516505e-06,
        "epoch": 0.301140174379611,
        "step": 4041
    },
    {
        "loss": 2.2742,
        "grad_norm": 3.639310836791992,
        "learning_rate": 4.964170814011404e-06,
        "epoch": 0.30121469558089276,
        "step": 4042
    },
    {
        "loss": 1.9125,
        "grad_norm": 2.682507038116455,
        "learning_rate": 4.942299522857163e-06,
        "epoch": 0.3012892167821745,
        "step": 4043
    },
    {
        "loss": 3.0075,
        "grad_norm": 2.411804676055908,
        "learning_rate": 4.920475297882965e-06,
        "epoch": 0.3013637379834563,
        "step": 4044
    },
    {
        "loss": 2.3709,
        "grad_norm": 4.969048976898193,
        "learning_rate": 4.89869814989472e-06,
        "epoch": 0.30143825918473804,
        "step": 4045
    },
    {
        "loss": 3.0777,
        "grad_norm": 2.843632698059082,
        "learning_rate": 4.8769680896749606e-06,
        "epoch": 0.3015127803860198,
        "step": 4046
    },
    {
        "loss": 2.5098,
        "grad_norm": 2.1071970462799072,
        "learning_rate": 4.855285127983e-06,
        "epoch": 0.30158730158730157,
        "step": 4047
    },
    {
        "loss": 2.4996,
        "grad_norm": 3.3350045680999756,
        "learning_rate": 4.833649275554786e-06,
        "epoch": 0.30166182278858333,
        "step": 4048
    },
    {
        "loss": 2.2503,
        "grad_norm": 3.121812343597412,
        "learning_rate": 4.812060543102903e-06,
        "epoch": 0.3017363439898651,
        "step": 4049
    },
    {
        "loss": 1.833,
        "grad_norm": 1.7278425693511963,
        "learning_rate": 4.7905189413166575e-06,
        "epoch": 0.30181086519114686,
        "step": 4050
    },
    {
        "loss": 2.5633,
        "grad_norm": 1.722050666809082,
        "learning_rate": 4.769024480861972e-06,
        "epoch": 0.3018853863924286,
        "step": 4051
    },
    {
        "loss": 2.4512,
        "grad_norm": 2.3975772857666016,
        "learning_rate": 4.747577172381512e-06,
        "epoch": 0.3019599075937104,
        "step": 4052
    },
    {
        "loss": 1.8511,
        "grad_norm": 4.454302787780762,
        "learning_rate": 4.726177026494516e-06,
        "epoch": 0.30203442879499215,
        "step": 4053
    },
    {
        "loss": 2.447,
        "grad_norm": 2.764439582824707,
        "learning_rate": 4.704824053796897e-06,
        "epoch": 0.3021089499962739,
        "step": 4054
    },
    {
        "loss": 1.956,
        "grad_norm": 2.2381327152252197,
        "learning_rate": 4.683518264861186e-06,
        "epoch": 0.30218347119755573,
        "step": 4055
    },
    {
        "loss": 2.4626,
        "grad_norm": 2.5542376041412354,
        "learning_rate": 4.6622596702366126e-06,
        "epoch": 0.3022579923988375,
        "step": 4056
    },
    {
        "loss": 1.9046,
        "grad_norm": 3.0056614875793457,
        "learning_rate": 4.641048280449001e-06,
        "epoch": 0.30233251360011926,
        "step": 4057
    },
    {
        "loss": 2.0775,
        "grad_norm": 3.390080451965332,
        "learning_rate": 4.619884106000771e-06,
        "epoch": 0.302407034801401,
        "step": 4058
    },
    {
        "loss": 2.6655,
        "grad_norm": 2.1511390209198,
        "learning_rate": 4.598767157371042e-06,
        "epoch": 0.3024815560026828,
        "step": 4059
    },
    {
        "loss": 2.451,
        "grad_norm": 2.126849889755249,
        "learning_rate": 4.577697445015472e-06,
        "epoch": 0.30255607720396455,
        "step": 4060
    },
    {
        "loss": 2.237,
        "grad_norm": 4.111941337585449,
        "learning_rate": 4.556674979366382e-06,
        "epoch": 0.3026305984052463,
        "step": 4061
    },
    {
        "loss": 2.1535,
        "grad_norm": 4.096710681915283,
        "learning_rate": 4.535699770832691e-06,
        "epoch": 0.3027051196065281,
        "step": 4062
    },
    {
        "loss": 1.8939,
        "grad_norm": 3.094757318496704,
        "learning_rate": 4.51477182979988e-06,
        "epoch": 0.30277964080780984,
        "step": 4063
    },
    {
        "loss": 2.8802,
        "grad_norm": 2.873528480529785,
        "learning_rate": 4.493891166630104e-06,
        "epoch": 0.3028541620090916,
        "step": 4064
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.4419209957122803,
        "learning_rate": 4.473057791662017e-06,
        "epoch": 0.30292868321037336,
        "step": 4065
    },
    {
        "loss": 2.3909,
        "grad_norm": 3.992534637451172,
        "learning_rate": 4.452271715210943e-06,
        "epoch": 0.3030032044116551,
        "step": 4066
    },
    {
        "loss": 2.5804,
        "grad_norm": 2.5951380729675293,
        "learning_rate": 4.4315329475687284e-06,
        "epoch": 0.3030777256129369,
        "step": 4067
    },
    {
        "loss": 1.9297,
        "grad_norm": 3.907252073287964,
        "learning_rate": 4.410841499003815e-06,
        "epoch": 0.30315224681421865,
        "step": 4068
    },
    {
        "loss": 2.5975,
        "grad_norm": 2.2076897621154785,
        "learning_rate": 4.390197379761218e-06,
        "epoch": 0.3032267680155004,
        "step": 4069
    },
    {
        "loss": 2.5391,
        "grad_norm": 2.3563172817230225,
        "learning_rate": 4.369600600062529e-06,
        "epoch": 0.3033012892167822,
        "step": 4070
    },
    {
        "loss": 2.5545,
        "grad_norm": 2.620797872543335,
        "learning_rate": 4.3490511701058775e-06,
        "epoch": 0.30337581041806394,
        "step": 4071
    },
    {
        "loss": 2.6141,
        "grad_norm": 3.4992527961730957,
        "learning_rate": 4.32854910006597e-06,
        "epoch": 0.3034503316193457,
        "step": 4072
    },
    {
        "loss": 3.0446,
        "grad_norm": 1.217428207397461,
        "learning_rate": 4.308094400094031e-06,
        "epoch": 0.30352485282062747,
        "step": 4073
    },
    {
        "loss": 1.95,
        "grad_norm": 3.048492670059204,
        "learning_rate": 4.2876870803178595e-06,
        "epoch": 0.30359937402190923,
        "step": 4074
    },
    {
        "loss": 2.3993,
        "grad_norm": 2.6270246505737305,
        "learning_rate": 4.2673271508418155e-06,
        "epoch": 0.303673895223191,
        "step": 4075
    },
    {
        "loss": 2.4997,
        "grad_norm": 3.06941556930542,
        "learning_rate": 4.247014621746736e-06,
        "epoch": 0.30374841642447276,
        "step": 4076
    },
    {
        "loss": 2.004,
        "grad_norm": 3.474872350692749,
        "learning_rate": 4.226749503090011e-06,
        "epoch": 0.3038229376257545,
        "step": 4077
    },
    {
        "loss": 2.5636,
        "grad_norm": 3.2482755184173584,
        "learning_rate": 4.206531804905589e-06,
        "epoch": 0.3038974588270363,
        "step": 4078
    },
    {
        "loss": 2.5576,
        "grad_norm": 2.1583478450775146,
        "learning_rate": 4.186361537203909e-06,
        "epoch": 0.30397198002831805,
        "step": 4079
    },
    {
        "loss": 2.5572,
        "grad_norm": 2.446647882461548,
        "learning_rate": 4.166238709971937e-06,
        "epoch": 0.3040465012295998,
        "step": 4080
    },
    {
        "loss": 2.0725,
        "grad_norm": 1.7117317914962769,
        "learning_rate": 4.146163333173136e-06,
        "epoch": 0.30412102243088157,
        "step": 4081
    },
    {
        "loss": 3.0388,
        "grad_norm": 2.9885361194610596,
        "learning_rate": 4.1261354167474565e-06,
        "epoch": 0.30419554363216333,
        "step": 4082
    },
    {
        "loss": 2.3,
        "grad_norm": 3.891878843307495,
        "learning_rate": 4.106154970611409e-06,
        "epoch": 0.3042700648334451,
        "step": 4083
    },
    {
        "loss": 2.3058,
        "grad_norm": 2.9415998458862305,
        "learning_rate": 4.086222004657969e-06,
        "epoch": 0.30434458603472686,
        "step": 4084
    },
    {
        "loss": 1.6199,
        "grad_norm": 5.150083065032959,
        "learning_rate": 4.066336528756587e-06,
        "epoch": 0.3044191072360086,
        "step": 4085
    },
    {
        "loss": 1.8075,
        "grad_norm": 3.5608651638031006,
        "learning_rate": 4.046498552753231e-06,
        "epoch": 0.3044936284372904,
        "step": 4086
    },
    {
        "loss": 2.4579,
        "grad_norm": 2.0422675609588623,
        "learning_rate": 4.026708086470299e-06,
        "epoch": 0.30456814963857215,
        "step": 4087
    },
    {
        "loss": 2.6877,
        "grad_norm": 2.5878398418426514,
        "learning_rate": 4.006965139706709e-06,
        "epoch": 0.3046426708398539,
        "step": 4088
    },
    {
        "loss": 2.5305,
        "grad_norm": 2.6025307178497314,
        "learning_rate": 3.987269722237863e-06,
        "epoch": 0.3047171920411357,
        "step": 4089
    },
    {
        "loss": 2.1413,
        "grad_norm": 3.1110334396362305,
        "learning_rate": 3.967621843815605e-06,
        "epoch": 0.3047917132424175,
        "step": 4090
    },
    {
        "loss": 1.2353,
        "grad_norm": 3.521198272705078,
        "learning_rate": 3.94802151416821e-06,
        "epoch": 0.30486623444369926,
        "step": 4091
    },
    {
        "loss": 2.3179,
        "grad_norm": 3.1084253787994385,
        "learning_rate": 3.928468743000469e-06,
        "epoch": 0.304940755644981,
        "step": 4092
    },
    {
        "loss": 2.0071,
        "grad_norm": 3.3719840049743652,
        "learning_rate": 3.908963539993582e-06,
        "epoch": 0.3050152768462628,
        "step": 4093
    },
    {
        "loss": 2.0327,
        "grad_norm": 2.901952028274536,
        "learning_rate": 3.889505914805247e-06,
        "epoch": 0.30508979804754455,
        "step": 4094
    },
    {
        "loss": 1.632,
        "grad_norm": 3.1290011405944824,
        "learning_rate": 3.870095877069557e-06,
        "epoch": 0.3051643192488263,
        "step": 4095
    },
    {
        "loss": 2.2972,
        "grad_norm": 3.230260133743286,
        "learning_rate": 3.850733436397025e-06,
        "epoch": 0.3052388404501081,
        "step": 4096
    },
    {
        "loss": 2.6408,
        "grad_norm": 1.6454042196273804,
        "learning_rate": 3.83141860237467e-06,
        "epoch": 0.30531336165138984,
        "step": 4097
    },
    {
        "loss": 2.4594,
        "grad_norm": 2.1943905353546143,
        "learning_rate": 3.8121513845658764e-06,
        "epoch": 0.3053878828526716,
        "step": 4098
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.746021270751953,
        "learning_rate": 3.792931792510479e-06,
        "epoch": 0.30546240405395336,
        "step": 4099
    },
    {
        "loss": 2.1866,
        "grad_norm": 3.23530650138855,
        "learning_rate": 3.7737598357247437e-06,
        "epoch": 0.3055369252552351,
        "step": 4100
    },
    {
        "loss": 2.1713,
        "grad_norm": 2.949927568435669,
        "learning_rate": 3.754635523701322e-06,
        "epoch": 0.3056114464565169,
        "step": 4101
    },
    {
        "loss": 2.7866,
        "grad_norm": 3.0333657264709473,
        "learning_rate": 3.735558865909261e-06,
        "epoch": 0.30568596765779865,
        "step": 4102
    },
    {
        "loss": 2.4262,
        "grad_norm": 2.4625298976898193,
        "learning_rate": 3.716529871794072e-06,
        "epoch": 0.3057604888590804,
        "step": 4103
    },
    {
        "loss": 1.9144,
        "grad_norm": 3.955864429473877,
        "learning_rate": 3.697548550777641e-06,
        "epoch": 0.3058350100603622,
        "step": 4104
    },
    {
        "loss": 1.6099,
        "grad_norm": 2.816573143005371,
        "learning_rate": 3.678614912258216e-06,
        "epoch": 0.30590953126164394,
        "step": 4105
    },
    {
        "loss": 1.7124,
        "grad_norm": 3.289733409881592,
        "learning_rate": 3.659728965610476e-06,
        "epoch": 0.3059840524629257,
        "step": 4106
    },
    {
        "loss": 1.8936,
        "grad_norm": 3.0951170921325684,
        "learning_rate": 3.6408907201854413e-06,
        "epoch": 0.30605857366420747,
        "step": 4107
    },
    {
        "loss": 2.7075,
        "grad_norm": 2.1384899616241455,
        "learning_rate": 3.622100185310573e-06,
        "epoch": 0.30613309486548923,
        "step": 4108
    },
    {
        "loss": 2.3498,
        "grad_norm": 2.2798314094543457,
        "learning_rate": 3.603357370289695e-06,
        "epoch": 0.306207616066771,
        "step": 4109
    },
    {
        "loss": 2.8971,
        "grad_norm": 2.8996522426605225,
        "learning_rate": 3.584662284402962e-06,
        "epoch": 0.30628213726805276,
        "step": 4110
    },
    {
        "loss": 2.7107,
        "grad_norm": 2.3093864917755127,
        "learning_rate": 3.566014936906936e-06,
        "epoch": 0.3063566584693345,
        "step": 4111
    },
    {
        "loss": 2.3432,
        "grad_norm": 2.8054327964782715,
        "learning_rate": 3.5474153370344986e-06,
        "epoch": 0.3064311796706163,
        "step": 4112
    },
    {
        "loss": 2.5573,
        "grad_norm": 3.2314157485961914,
        "learning_rate": 3.5288634939949607e-06,
        "epoch": 0.30650570087189805,
        "step": 4113
    },
    {
        "loss": 3.2438,
        "grad_norm": 2.5348172187805176,
        "learning_rate": 3.5103594169739516e-06,
        "epoch": 0.3065802220731798,
        "step": 4114
    },
    {
        "loss": 2.403,
        "grad_norm": 2.9994702339172363,
        "learning_rate": 3.49190311513341e-06,
        "epoch": 0.30665474327446157,
        "step": 4115
    },
    {
        "loss": 1.4386,
        "grad_norm": 2.709928035736084,
        "learning_rate": 3.4734945976116705e-06,
        "epoch": 0.30672926447574334,
        "step": 4116
    },
    {
        "loss": 1.8119,
        "grad_norm": 2.4236016273498535,
        "learning_rate": 3.4551338735233975e-06,
        "epoch": 0.3068037856770251,
        "step": 4117
    },
    {
        "loss": 2.6049,
        "grad_norm": 3.277094602584839,
        "learning_rate": 3.4368209519595627e-06,
        "epoch": 0.30687830687830686,
        "step": 4118
    },
    {
        "loss": 1.4916,
        "grad_norm": 3.1771411895751953,
        "learning_rate": 3.4185558419875362e-06,
        "epoch": 0.3069528280795886,
        "step": 4119
    },
    {
        "loss": 2.4773,
        "grad_norm": 1.8335182666778564,
        "learning_rate": 3.4003385526509613e-06,
        "epoch": 0.3070273492808704,
        "step": 4120
    },
    {
        "loss": 2.7665,
        "grad_norm": 1.9104069471359253,
        "learning_rate": 3.382169092969778e-06,
        "epoch": 0.30710187048215215,
        "step": 4121
    },
    {
        "loss": 2.8508,
        "grad_norm": 2.7349066734313965,
        "learning_rate": 3.3640474719403127e-06,
        "epoch": 0.3071763916834339,
        "step": 4122
    },
    {
        "loss": 2.0505,
        "grad_norm": 2.1448633670806885,
        "learning_rate": 3.3459736985351765e-06,
        "epoch": 0.3072509128847157,
        "step": 4123
    },
    {
        "loss": 1.6994,
        "grad_norm": 3.05297589302063,
        "learning_rate": 3.327947781703267e-06,
        "epoch": 0.30732543408599744,
        "step": 4124
    },
    {
        "loss": 2.1901,
        "grad_norm": 2.8200268745422363,
        "learning_rate": 3.3099697303698105e-06,
        "epoch": 0.30739995528727926,
        "step": 4125
    },
    {
        "loss": 1.6001,
        "grad_norm": 3.3735623359680176,
        "learning_rate": 3.2920395534363323e-06,
        "epoch": 0.307474476488561,
        "step": 4126
    },
    {
        "loss": 1.6455,
        "grad_norm": 2.53206467628479,
        "learning_rate": 3.2741572597806527e-06,
        "epoch": 0.3075489976898428,
        "step": 4127
    },
    {
        "loss": 2.4669,
        "grad_norm": 2.364312171936035,
        "learning_rate": 3.2563228582568883e-06,
        "epoch": 0.30762351889112455,
        "step": 4128
    },
    {
        "loss": 2.0884,
        "grad_norm": 2.205559492111206,
        "learning_rate": 3.238536357695421e-06,
        "epoch": 0.3076980400924063,
        "step": 4129
    },
    {
        "loss": 2.2464,
        "grad_norm": 2.126901865005493,
        "learning_rate": 3.2207977669029277e-06,
        "epoch": 0.3077725612936881,
        "step": 4130
    },
    {
        "loss": 1.9267,
        "grad_norm": 2.3500707149505615,
        "learning_rate": 3.203107094662383e-06,
        "epoch": 0.30784708249496984,
        "step": 4131
    },
    {
        "loss": 1.7219,
        "grad_norm": 2.0283286571502686,
        "learning_rate": 3.185464349733003e-06,
        "epoch": 0.3079216036962516,
        "step": 4132
    },
    {
        "loss": 2.9805,
        "grad_norm": 2.985156297683716,
        "learning_rate": 3.167869540850299e-06,
        "epoch": 0.30799612489753336,
        "step": 4133
    },
    {
        "loss": 1.926,
        "grad_norm": 2.9499099254608154,
        "learning_rate": 3.1503226767260252e-06,
        "epoch": 0.3080706460988151,
        "step": 4134
    },
    {
        "loss": 2.8239,
        "grad_norm": 2.077944278717041,
        "learning_rate": 3.1328237660482096e-06,
        "epoch": 0.3081451673000969,
        "step": 4135
    },
    {
        "loss": 2.5223,
        "grad_norm": 3.2799174785614014,
        "learning_rate": 3.1153728174811548e-06,
        "epoch": 0.30821968850137865,
        "step": 4136
    },
    {
        "loss": 2.1408,
        "grad_norm": 2.4807217121124268,
        "learning_rate": 3.0979698396653823e-06,
        "epoch": 0.3082942097026604,
        "step": 4137
    },
    {
        "loss": 2.821,
        "grad_norm": 2.6436290740966797,
        "learning_rate": 3.080614841217655e-06,
        "epoch": 0.3083687309039422,
        "step": 4138
    },
    {
        "loss": 1.6774,
        "grad_norm": 4.6986823081970215,
        "learning_rate": 3.0633078307310324e-06,
        "epoch": 0.30844325210522394,
        "step": 4139
    },
    {
        "loss": 1.404,
        "grad_norm": 3.4932801723480225,
        "learning_rate": 3.0460488167747715e-06,
        "epoch": 0.3085177733065057,
        "step": 4140
    },
    {
        "loss": 2.3296,
        "grad_norm": 2.0431323051452637,
        "learning_rate": 3.028837807894391e-06,
        "epoch": 0.30859229450778747,
        "step": 4141
    },
    {
        "loss": 1.0809,
        "grad_norm": 3.2869975566864014,
        "learning_rate": 3.011674812611609e-06,
        "epoch": 0.30866681570906923,
        "step": 4142
    },
    {
        "loss": 2.8198,
        "grad_norm": 2.9362761974334717,
        "learning_rate": 2.9945598394243713e-06,
        "epoch": 0.308741336910351,
        "step": 4143
    },
    {
        "loss": 2.5591,
        "grad_norm": 2.4705564975738525,
        "learning_rate": 2.977492896806866e-06,
        "epoch": 0.30881585811163276,
        "step": 4144
    },
    {
        "loss": 2.7482,
        "grad_norm": 2.776667833328247,
        "learning_rate": 2.9604739932095338e-06,
        "epoch": 0.3088903793129145,
        "step": 4145
    },
    {
        "loss": 2.8129,
        "grad_norm": 2.1848697662353516,
        "learning_rate": 2.943503137058956e-06,
        "epoch": 0.3089649005141963,
        "step": 4146
    },
    {
        "loss": 2.4968,
        "grad_norm": 2.446110725402832,
        "learning_rate": 2.926580336757978e-06,
        "epoch": 0.30903942171547805,
        "step": 4147
    },
    {
        "loss": 2.6018,
        "grad_norm": 1.53912353515625,
        "learning_rate": 2.9097056006856082e-06,
        "epoch": 0.3091139429167598,
        "step": 4148
    },
    {
        "loss": 1.9205,
        "grad_norm": 2.807169198989868,
        "learning_rate": 2.8928789371970745e-06,
        "epoch": 0.3091884641180416,
        "step": 4149
    },
    {
        "loss": 2.1724,
        "grad_norm": 3.492398977279663,
        "learning_rate": 2.8761003546238564e-06,
        "epoch": 0.30926298531932334,
        "step": 4150
    },
    {
        "loss": 1.3809,
        "grad_norm": 3.555344581604004,
        "learning_rate": 2.859369861273564e-06,
        "epoch": 0.3093375065206051,
        "step": 4151
    },
    {
        "loss": 2.3927,
        "grad_norm": 2.2855002880096436,
        "learning_rate": 2.8426874654299718e-06,
        "epoch": 0.30941202772188686,
        "step": 4152
    },
    {
        "loss": 2.3895,
        "grad_norm": 2.5930421352386475,
        "learning_rate": 2.826053175353116e-06,
        "epoch": 0.3094865489231686,
        "step": 4153
    },
    {
        "loss": 2.3124,
        "grad_norm": 2.7752420902252197,
        "learning_rate": 2.8094669992791645e-06,
        "epoch": 0.3095610701244504,
        "step": 4154
    },
    {
        "loss": 2.2005,
        "grad_norm": 2.607492446899414,
        "learning_rate": 2.7929289454205034e-06,
        "epoch": 0.30963559132573215,
        "step": 4155
    },
    {
        "loss": 2.3445,
        "grad_norm": 3.0904364585876465,
        "learning_rate": 2.7764390219656377e-06,
        "epoch": 0.3097101125270139,
        "step": 4156
    },
    {
        "loss": 2.0436,
        "grad_norm": 2.511989116668701,
        "learning_rate": 2.7599972370792703e-06,
        "epoch": 0.3097846337282957,
        "step": 4157
    },
    {
        "loss": 2.7732,
        "grad_norm": 2.248504877090454,
        "learning_rate": 2.7436035989022667e-06,
        "epoch": 0.30985915492957744,
        "step": 4158
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.2543704509735107,
        "learning_rate": 2.7272581155516674e-06,
        "epoch": 0.3099336761308592,
        "step": 4159
    },
    {
        "loss": 2.5414,
        "grad_norm": 1.4148588180541992,
        "learning_rate": 2.7109607951206426e-06,
        "epoch": 0.31000819733214097,
        "step": 4160
    },
    {
        "loss": 2.1787,
        "grad_norm": 1.9064173698425293,
        "learning_rate": 2.694711645678538e-06,
        "epoch": 0.3100827185334228,
        "step": 4161
    },
    {
        "loss": 2.7202,
        "grad_norm": 2.568000078201294,
        "learning_rate": 2.67851067527084e-06,
        "epoch": 0.31015723973470455,
        "step": 4162
    },
    {
        "loss": 2.2711,
        "grad_norm": 2.202392339706421,
        "learning_rate": 2.662357891919165e-06,
        "epoch": 0.3102317609359863,
        "step": 4163
    },
    {
        "loss": 2.202,
        "grad_norm": 3.104611873626709,
        "learning_rate": 2.6462533036212933e-06,
        "epoch": 0.3103062821372681,
        "step": 4164
    },
    {
        "loss": 1.9683,
        "grad_norm": 3.1174910068511963,
        "learning_rate": 2.6301969183511467e-06,
        "epoch": 0.31038080333854984,
        "step": 4165
    },
    {
        "loss": 2.7087,
        "grad_norm": 3.8848330974578857,
        "learning_rate": 2.614188744058765e-06,
        "epoch": 0.3104553245398316,
        "step": 4166
    },
    {
        "loss": 1.4659,
        "grad_norm": 3.353397846221924,
        "learning_rate": 2.5982287886703095e-06,
        "epoch": 0.31052984574111336,
        "step": 4167
    },
    {
        "loss": 2.6116,
        "grad_norm": 2.5755646228790283,
        "learning_rate": 2.5823170600880574e-06,
        "epoch": 0.3106043669423951,
        "step": 4168
    },
    {
        "loss": 2.3045,
        "grad_norm": 2.3217520713806152,
        "learning_rate": 2.566453566190452e-06,
        "epoch": 0.3106788881436769,
        "step": 4169
    },
    {
        "loss": 1.9128,
        "grad_norm": 4.076948165893555,
        "learning_rate": 2.5506383148320435e-06,
        "epoch": 0.31075340934495865,
        "step": 4170
    },
    {
        "loss": 2.3956,
        "grad_norm": 3.3436949253082275,
        "learning_rate": 2.5348713138434564e-06,
        "epoch": 0.3108279305462404,
        "step": 4171
    },
    {
        "loss": 2.5737,
        "grad_norm": 2.356562376022339,
        "learning_rate": 2.519152571031447e-06,
        "epoch": 0.3109024517475222,
        "step": 4172
    },
    {
        "loss": 2.1394,
        "grad_norm": 2.3787789344787598,
        "learning_rate": 2.503482094178877e-06,
        "epoch": 0.31097697294880394,
        "step": 4173
    },
    {
        "loss": 2.7118,
        "grad_norm": 2.315389394760132,
        "learning_rate": 2.4878598910447303e-06,
        "epoch": 0.3110514941500857,
        "step": 4174
    },
    {
        "loss": 2.4964,
        "grad_norm": 3.2750022411346436,
        "learning_rate": 2.4722859693640745e-06,
        "epoch": 0.31112601535136747,
        "step": 4175
    },
    {
        "loss": 1.5591,
        "grad_norm": 1.518108606338501,
        "learning_rate": 2.456760336848052e-06,
        "epoch": 0.31120053655264923,
        "step": 4176
    },
    {
        "loss": 1.9898,
        "grad_norm": 4.309950351715088,
        "learning_rate": 2.441283001183914e-06,
        "epoch": 0.311275057753931,
        "step": 4177
    },
    {
        "loss": 1.4655,
        "grad_norm": 2.637928009033203,
        "learning_rate": 2.425853970034997e-06,
        "epoch": 0.31134957895521276,
        "step": 4178
    },
    {
        "loss": 1.2963,
        "grad_norm": 3.3772449493408203,
        "learning_rate": 2.4104732510407123e-06,
        "epoch": 0.3114241001564945,
        "step": 4179
    },
    {
        "loss": 2.0631,
        "grad_norm": 4.108042240142822,
        "learning_rate": 2.39514085181658e-06,
        "epoch": 0.3114986213577763,
        "step": 4180
    },
    {
        "loss": 2.3011,
        "grad_norm": 1.8649364709854126,
        "learning_rate": 2.3798567799541706e-06,
        "epoch": 0.31157314255905805,
        "step": 4181
    },
    {
        "loss": 2.0424,
        "grad_norm": 3.279628276824951,
        "learning_rate": 2.364621043021098e-06,
        "epoch": 0.3116476637603398,
        "step": 4182
    },
    {
        "loss": 2.1863,
        "grad_norm": 2.3658156394958496,
        "learning_rate": 2.3494336485610945e-06,
        "epoch": 0.3117221849616216,
        "step": 4183
    },
    {
        "loss": 2.2094,
        "grad_norm": 2.8822004795074463,
        "learning_rate": 2.334294604093956e-06,
        "epoch": 0.31179670616290334,
        "step": 4184
    },
    {
        "loss": 1.9216,
        "grad_norm": 5.385519981384277,
        "learning_rate": 2.3192039171154755e-06,
        "epoch": 0.3118712273641851,
        "step": 4185
    },
    {
        "loss": 1.9208,
        "grad_norm": 3.390162467956543,
        "learning_rate": 2.304161595097587e-06,
        "epoch": 0.31194574856546686,
        "step": 4186
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.009599208831787,
        "learning_rate": 2.2891676454881996e-06,
        "epoch": 0.3120202697667486,
        "step": 4187
    },
    {
        "loss": 2.2073,
        "grad_norm": 2.9858906269073486,
        "learning_rate": 2.2742220757113407e-06,
        "epoch": 0.3120947909680304,
        "step": 4188
    },
    {
        "loss": 1.9867,
        "grad_norm": 2.8255348205566406,
        "learning_rate": 2.2593248931670473e-06,
        "epoch": 0.31216931216931215,
        "step": 4189
    },
    {
        "loss": 2.2203,
        "grad_norm": 3.586670398712158,
        "learning_rate": 2.2444761052313856e-06,
        "epoch": 0.3122438333705939,
        "step": 4190
    },
    {
        "loss": 2.6873,
        "grad_norm": 1.7631151676177979,
        "learning_rate": 2.2296757192564855e-06,
        "epoch": 0.3123183545718757,
        "step": 4191
    },
    {
        "loss": 1.8971,
        "grad_norm": 3.0847158432006836,
        "learning_rate": 2.2149237425705073e-06,
        "epoch": 0.31239287577315744,
        "step": 4192
    },
    {
        "loss": 2.6599,
        "grad_norm": 2.714628219604492,
        "learning_rate": 2.2002201824776193e-06,
        "epoch": 0.3124673969744392,
        "step": 4193
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.628464698791504,
        "learning_rate": 2.1855650462580646e-06,
        "epoch": 0.31254191817572097,
        "step": 4194
    },
    {
        "loss": 2.1209,
        "grad_norm": 2.125121831893921,
        "learning_rate": 2.17095834116805e-06,
        "epoch": 0.31261643937700273,
        "step": 4195
    },
    {
        "loss": 2.5897,
        "grad_norm": 3.1327226161956787,
        "learning_rate": 2.1564000744398573e-06,
        "epoch": 0.31269096057828455,
        "step": 4196
    },
    {
        "loss": 2.3841,
        "grad_norm": 3.0824618339538574,
        "learning_rate": 2.141890253281753e-06,
        "epoch": 0.3127654817795663,
        "step": 4197
    },
    {
        "loss": 2.6661,
        "grad_norm": 2.2309601306915283,
        "learning_rate": 2.1274288848780355e-06,
        "epoch": 0.3128400029808481,
        "step": 4198
    },
    {
        "loss": 1.9884,
        "grad_norm": 2.3326833248138428,
        "learning_rate": 2.1130159763889654e-06,
        "epoch": 0.31291452418212984,
        "step": 4199
    },
    {
        "loss": 2.2343,
        "grad_norm": 2.161914825439453,
        "learning_rate": 2.0986515349508907e-06,
        "epoch": 0.3129890453834116,
        "step": 4200
    },
    {
        "loss": 2.5623,
        "grad_norm": 2.420623540878296,
        "learning_rate": 2.0843355676760878e-06,
        "epoch": 0.31306356658469336,
        "step": 4201
    },
    {
        "loss": 3.0645,
        "grad_norm": 2.1491425037384033,
        "learning_rate": 2.070068081652876e-06,
        "epoch": 0.3131380877859751,
        "step": 4202
    },
    {
        "loss": 2.8394,
        "grad_norm": 2.496762752532959,
        "learning_rate": 2.055849083945549e-06,
        "epoch": 0.3132126089872569,
        "step": 4203
    },
    {
        "loss": 2.204,
        "grad_norm": 3.5083084106445312,
        "learning_rate": 2.0416785815943973e-06,
        "epoch": 0.31328713018853865,
        "step": 4204
    },
    {
        "loss": 2.6285,
        "grad_norm": 2.23555326461792,
        "learning_rate": 2.027556581615686e-06,
        "epoch": 0.3133616513898204,
        "step": 4205
    },
    {
        "loss": 2.8179,
        "grad_norm": 2.477421760559082,
        "learning_rate": 2.013483091001711e-06,
        "epoch": 0.3134361725911022,
        "step": 4206
    },
    {
        "loss": 2.553,
        "grad_norm": 1.7795897722244263,
        "learning_rate": 1.999458116720698e-06,
        "epoch": 0.31351069379238394,
        "step": 4207
    },
    {
        "loss": 2.041,
        "grad_norm": 2.6154932975769043,
        "learning_rate": 1.985481665716882e-06,
        "epoch": 0.3135852149936657,
        "step": 4208
    },
    {
        "loss": 1.8441,
        "grad_norm": 3.3979687690734863,
        "learning_rate": 1.971553744910448e-06,
        "epoch": 0.31365973619494747,
        "step": 4209
    },
    {
        "loss": 2.2841,
        "grad_norm": 3.809556245803833,
        "learning_rate": 1.95767436119757e-06,
        "epoch": 0.31373425739622923,
        "step": 4210
    },
    {
        "loss": 1.5024,
        "grad_norm": 4.1756134033203125,
        "learning_rate": 1.9438435214503947e-06,
        "epoch": 0.313808778597511,
        "step": 4211
    },
    {
        "loss": 2.1889,
        "grad_norm": 2.885821580886841,
        "learning_rate": 1.930061232517011e-06,
        "epoch": 0.31388329979879276,
        "step": 4212
    },
    {
        "loss": 1.2275,
        "grad_norm": 2.701754570007324,
        "learning_rate": 1.916327501221493e-06,
        "epoch": 0.3139578210000745,
        "step": 4213
    },
    {
        "loss": 2.2483,
        "grad_norm": 2.4936673641204834,
        "learning_rate": 1.9026423343638466e-06,
        "epoch": 0.3140323422013563,
        "step": 4214
    },
    {
        "loss": 2.3196,
        "grad_norm": 2.794947385787964,
        "learning_rate": 1.8890057387200622e-06,
        "epoch": 0.31410686340263805,
        "step": 4215
    },
    {
        "loss": 2.1577,
        "grad_norm": 3.038579225540161,
        "learning_rate": 1.8754177210420498e-06,
        "epoch": 0.3141813846039198,
        "step": 4216
    },
    {
        "loss": 2.8112,
        "grad_norm": 2.4702653884887695,
        "learning_rate": 1.8618782880576945e-06,
        "epoch": 0.3142559058052016,
        "step": 4217
    },
    {
        "loss": 2.4749,
        "grad_norm": 1.725873589515686,
        "learning_rate": 1.8483874464708118e-06,
        "epoch": 0.31433042700648334,
        "step": 4218
    },
    {
        "loss": 2.3432,
        "grad_norm": 3.2009990215301514,
        "learning_rate": 1.834945202961147e-06,
        "epoch": 0.3144049482077651,
        "step": 4219
    },
    {
        "loss": 2.289,
        "grad_norm": 3.48055362701416,
        "learning_rate": 1.8215515641843983e-06,
        "epoch": 0.31447946940904686,
        "step": 4220
    },
    {
        "loss": 2.5935,
        "grad_norm": 2.268372058868408,
        "learning_rate": 1.8082065367722057e-06,
        "epoch": 0.3145539906103286,
        "step": 4221
    },
    {
        "loss": 2.4947,
        "grad_norm": 3.4269800186157227,
        "learning_rate": 1.794910127332128e-06,
        "epoch": 0.3146285118116104,
        "step": 4222
    },
    {
        "loss": 2.6704,
        "grad_norm": 1.61463463306427,
        "learning_rate": 1.7816623424476542e-06,
        "epoch": 0.31470303301289215,
        "step": 4223
    },
    {
        "loss": 1.4618,
        "grad_norm": 4.886035442352295,
        "learning_rate": 1.76846318867816e-06,
        "epoch": 0.3147775542141739,
        "step": 4224
    },
    {
        "loss": 2.3148,
        "grad_norm": 2.9285573959350586,
        "learning_rate": 1.7553126725590286e-06,
        "epoch": 0.3148520754154557,
        "step": 4225
    },
    {
        "loss": 2.8175,
        "grad_norm": 2.037539005279541,
        "learning_rate": 1.7422108006014847e-06,
        "epoch": 0.31492659661673744,
        "step": 4226
    },
    {
        "loss": 2.441,
        "grad_norm": 3.079481840133667,
        "learning_rate": 1.7291575792927173e-06,
        "epoch": 0.3150011178180192,
        "step": 4227
    },
    {
        "loss": 1.6657,
        "grad_norm": 1.6128482818603516,
        "learning_rate": 1.7161530150957784e-06,
        "epoch": 0.31507563901930097,
        "step": 4228
    },
    {
        "loss": 1.6133,
        "grad_norm": 3.2401435375213623,
        "learning_rate": 1.7031971144496506e-06,
        "epoch": 0.31515016022058273,
        "step": 4229
    },
    {
        "loss": 2.6418,
        "grad_norm": 2.9398984909057617,
        "learning_rate": 1.6902898837692361e-06,
        "epoch": 0.3152246814218645,
        "step": 4230
    },
    {
        "loss": 2.3216,
        "grad_norm": 3.5457229614257812,
        "learning_rate": 1.6774313294453448e-06,
        "epoch": 0.31529920262314626,
        "step": 4231
    },
    {
        "loss": 2.9208,
        "grad_norm": 2.409506320953369,
        "learning_rate": 1.6646214578446506e-06,
        "epoch": 0.3153737238244281,
        "step": 4232
    },
    {
        "loss": 2.0762,
        "grad_norm": 5.139829635620117,
        "learning_rate": 1.6518602753097246e-06,
        "epoch": 0.31544824502570984,
        "step": 4233
    },
    {
        "loss": 1.9812,
        "grad_norm": 4.035037040710449,
        "learning_rate": 1.6391477881590677e-06,
        "epoch": 0.3155227662269916,
        "step": 4234
    },
    {
        "loss": 2.3711,
        "grad_norm": 3.0672903060913086,
        "learning_rate": 1.6264840026870344e-06,
        "epoch": 0.31559728742827337,
        "step": 4235
    },
    {
        "loss": 2.3033,
        "grad_norm": 2.460587501525879,
        "learning_rate": 1.6138689251638972e-06,
        "epoch": 0.31567180862955513,
        "step": 4236
    },
    {
        "loss": 2.1224,
        "grad_norm": 2.7773029804229736,
        "learning_rate": 1.6013025618357936e-06,
        "epoch": 0.3157463298308369,
        "step": 4237
    },
    {
        "loss": 2.4074,
        "grad_norm": 2.845484972000122,
        "learning_rate": 1.588784918924724e-06,
        "epoch": 0.31582085103211865,
        "step": 4238
    },
    {
        "loss": 2.2222,
        "grad_norm": 6.012717247009277,
        "learning_rate": 1.5763160026285862e-06,
        "epoch": 0.3158953722334004,
        "step": 4239
    },
    {
        "loss": 1.7274,
        "grad_norm": 3.0491416454315186,
        "learning_rate": 1.5638958191211417e-06,
        "epoch": 0.3159698934346822,
        "step": 4240
    },
    {
        "loss": 2.4811,
        "grad_norm": 3.5048770904541016,
        "learning_rate": 1.5515243745520825e-06,
        "epoch": 0.31604441463596394,
        "step": 4241
    },
    {
        "loss": 2.5743,
        "grad_norm": 2.252328634262085,
        "learning_rate": 1.5392016750468418e-06,
        "epoch": 0.3161189358372457,
        "step": 4242
    },
    {
        "loss": 1.816,
        "grad_norm": 3.804938793182373,
        "learning_rate": 1.5269277267068394e-06,
        "epoch": 0.31619345703852747,
        "step": 4243
    },
    {
        "loss": 2.6851,
        "grad_norm": 2.284583330154419,
        "learning_rate": 1.5147025356092914e-06,
        "epoch": 0.31626797823980923,
        "step": 4244
    },
    {
        "loss": 2.2746,
        "grad_norm": 2.9885005950927734,
        "learning_rate": 1.5025261078073005e-06,
        "epoch": 0.316342499441091,
        "step": 4245
    },
    {
        "loss": 2.4089,
        "grad_norm": 1.955540657043457,
        "learning_rate": 1.4903984493298106e-06,
        "epoch": 0.31641702064237276,
        "step": 4246
    },
    {
        "loss": 2.4161,
        "grad_norm": 2.886122941970825,
        "learning_rate": 1.4783195661816074e-06,
        "epoch": 0.3164915418436545,
        "step": 4247
    },
    {
        "loss": 2.6364,
        "grad_norm": 2.0142433643341064,
        "learning_rate": 1.4662894643433623e-06,
        "epoch": 0.3165660630449363,
        "step": 4248
    },
    {
        "loss": 1.9819,
        "grad_norm": 2.5786666870117188,
        "learning_rate": 1.4543081497715661e-06,
        "epoch": 0.31664058424621805,
        "step": 4249
    },
    {
        "loss": 1.1931,
        "grad_norm": 3.6895341873168945,
        "learning_rate": 1.4423756283985623e-06,
        "epoch": 0.3167151054474998,
        "step": 4250
    },
    {
        "loss": 1.8454,
        "grad_norm": 2.19675874710083,
        "learning_rate": 1.430491906132525e-06,
        "epoch": 0.3167896266487816,
        "step": 4251
    },
    {
        "loss": 2.0612,
        "grad_norm": 3.9124624729156494,
        "learning_rate": 1.4186569888574808e-06,
        "epoch": 0.31686414785006334,
        "step": 4252
    },
    {
        "loss": 2.1509,
        "grad_norm": 3.2257440090179443,
        "learning_rate": 1.4068708824332866e-06,
        "epoch": 0.3169386690513451,
        "step": 4253
    },
    {
        "loss": 2.5667,
        "grad_norm": 2.0220677852630615,
        "learning_rate": 1.395133592695619e-06,
        "epoch": 0.31701319025262686,
        "step": 4254
    },
    {
        "loss": 2.5738,
        "grad_norm": 2.85127592086792,
        "learning_rate": 1.3834451254560066e-06,
        "epoch": 0.3170877114539086,
        "step": 4255
    },
    {
        "loss": 1.9864,
        "grad_norm": 4.344861030578613,
        "learning_rate": 1.371805486501776e-06,
        "epoch": 0.3171622326551904,
        "step": 4256
    },
    {
        "loss": 1.9779,
        "grad_norm": 2.0652568340301514,
        "learning_rate": 1.360214681596117e-06,
        "epoch": 0.31723675385647215,
        "step": 4257
    },
    {
        "loss": 1.7119,
        "grad_norm": 1.7517589330673218,
        "learning_rate": 1.3486727164780055e-06,
        "epoch": 0.3173112750577539,
        "step": 4258
    },
    {
        "loss": 2.6771,
        "grad_norm": 3.261824607849121,
        "learning_rate": 1.3371795968622257e-06,
        "epoch": 0.3173857962590357,
        "step": 4259
    },
    {
        "loss": 2.6309,
        "grad_norm": 2.445708990097046,
        "learning_rate": 1.325735328439448e-06,
        "epoch": 0.31746031746031744,
        "step": 4260
    },
    {
        "loss": 2.8753,
        "grad_norm": 2.1655008792877197,
        "learning_rate": 1.3143399168760506e-06,
        "epoch": 0.3175348386615992,
        "step": 4261
    },
    {
        "loss": 2.0777,
        "grad_norm": 3.674731969833374,
        "learning_rate": 1.3029933678142981e-06,
        "epoch": 0.31760935986288097,
        "step": 4262
    },
    {
        "loss": 1.9504,
        "grad_norm": 3.504457473754883,
        "learning_rate": 1.2916956868722407e-06,
        "epoch": 0.31768388106416273,
        "step": 4263
    },
    {
        "loss": 2.5132,
        "grad_norm": 2.2599079608917236,
        "learning_rate": 1.2804468796437374e-06,
        "epoch": 0.3177584022654445,
        "step": 4264
    },
    {
        "loss": 2.7062,
        "grad_norm": 2.3088316917419434,
        "learning_rate": 1.2692469516984219e-06,
        "epoch": 0.31783292346672626,
        "step": 4265
    },
    {
        "loss": 2.8085,
        "grad_norm": 2.2256789207458496,
        "learning_rate": 1.2580959085817467e-06,
        "epoch": 0.317907444668008,
        "step": 4266
    },
    {
        "loss": 2.5735,
        "grad_norm": 3.0204646587371826,
        "learning_rate": 1.2469937558149846e-06,
        "epoch": 0.31798196586928984,
        "step": 4267
    },
    {
        "loss": 2.1501,
        "grad_norm": 2.604016065597534,
        "learning_rate": 1.2359404988951383e-06,
        "epoch": 0.3180564870705716,
        "step": 4268
    },
    {
        "loss": 2.7864,
        "grad_norm": 2.804612636566162,
        "learning_rate": 1.2249361432950746e-06,
        "epoch": 0.31813100827185337,
        "step": 4269
    },
    {
        "loss": 2.3369,
        "grad_norm": 2.821614980697632,
        "learning_rate": 1.213980694463368e-06,
        "epoch": 0.31820552947313513,
        "step": 4270
    },
    {
        "loss": 2.6291,
        "grad_norm": 3.021651029586792,
        "learning_rate": 1.2030741578244465e-06,
        "epoch": 0.3182800506744169,
        "step": 4271
    },
    {
        "loss": 2.3459,
        "grad_norm": 2.9603400230407715,
        "learning_rate": 1.192216538778501e-06,
        "epoch": 0.31835457187569866,
        "step": 4272
    },
    {
        "loss": 2.7604,
        "grad_norm": 2.301158905029297,
        "learning_rate": 1.1814078427014874e-06,
        "epoch": 0.3184290930769804,
        "step": 4273
    },
    {
        "loss": 2.7198,
        "grad_norm": 2.0916483402252197,
        "learning_rate": 1.1706480749451354e-06,
        "epoch": 0.3185036142782622,
        "step": 4274
    },
    {
        "loss": 2.2244,
        "grad_norm": 2.891611337661743,
        "learning_rate": 1.1599372408369614e-06,
        "epoch": 0.31857813547954394,
        "step": 4275
    },
    {
        "loss": 2.0519,
        "grad_norm": 4.015895843505859,
        "learning_rate": 1.1492753456802452e-06,
        "epoch": 0.3186526566808257,
        "step": 4276
    },
    {
        "loss": 2.5977,
        "grad_norm": 3.312971591949463,
        "learning_rate": 1.138662394754053e-06,
        "epoch": 0.31872717788210747,
        "step": 4277
    },
    {
        "loss": 3.0147,
        "grad_norm": 2.2651119232177734,
        "learning_rate": 1.1280983933132038e-06,
        "epoch": 0.31880169908338923,
        "step": 4278
    },
    {
        "loss": 2.327,
        "grad_norm": 2.4484782218933105,
        "learning_rate": 1.11758334658828e-06,
        "epoch": 0.318876220284671,
        "step": 4279
    },
    {
        "loss": 2.2339,
        "grad_norm": 2.310290813446045,
        "learning_rate": 1.107117259785606e-06,
        "epoch": 0.31895074148595276,
        "step": 4280
    },
    {
        "loss": 0.9899,
        "grad_norm": 3.637989044189453,
        "learning_rate": 1.0967001380873031e-06,
        "epoch": 0.3190252626872345,
        "step": 4281
    },
    {
        "loss": 2.4897,
        "grad_norm": 2.157606601715088,
        "learning_rate": 1.0863319866512234e-06,
        "epoch": 0.3190997838885163,
        "step": 4282
    },
    {
        "loss": 2.3285,
        "grad_norm": 2.2282979488372803,
        "learning_rate": 1.0760128106109935e-06,
        "epoch": 0.31917430508979805,
        "step": 4283
    },
    {
        "loss": 2.9107,
        "grad_norm": 2.277254343032837,
        "learning_rate": 1.0657426150759597e-06,
        "epoch": 0.3192488262910798,
        "step": 4284
    },
    {
        "loss": 2.281,
        "grad_norm": 2.344682455062866,
        "learning_rate": 1.0555214051312102e-06,
        "epoch": 0.3193233474923616,
        "step": 4285
    },
    {
        "loss": 2.6922,
        "grad_norm": 2.2334630489349365,
        "learning_rate": 1.0453491858376408e-06,
        "epoch": 0.31939786869364334,
        "step": 4286
    },
    {
        "loss": 2.6968,
        "grad_norm": 2.4309237003326416,
        "learning_rate": 1.035225962231834e-06,
        "epoch": 0.3194723898949251,
        "step": 4287
    },
    {
        "loss": 2.5241,
        "grad_norm": 2.2912142276763916,
        "learning_rate": 1.0251517393261355e-06,
        "epoch": 0.31954691109620686,
        "step": 4288
    },
    {
        "loss": 2.6452,
        "grad_norm": 2.077425003051758,
        "learning_rate": 1.0151265221086225e-06,
        "epoch": 0.31962143229748863,
        "step": 4289
    },
    {
        "loss": 2.1919,
        "grad_norm": 2.9119741916656494,
        "learning_rate": 1.0051503155430908e-06,
        "epoch": 0.3196959534987704,
        "step": 4290
    },
    {
        "loss": 2.6077,
        "grad_norm": 3.7255420684814453,
        "learning_rate": 9.952231245691e-07,
        "epoch": 0.31977047470005215,
        "step": 4291
    },
    {
        "loss": 2.1544,
        "grad_norm": 2.4151384830474854,
        "learning_rate": 9.8534495410193e-07,
        "epoch": 0.3198449959013339,
        "step": 4292
    },
    {
        "loss": 2.2824,
        "grad_norm": 2.175412654876709,
        "learning_rate": 9.755158090325789e-07,
        "epoch": 0.3199195171026157,
        "step": 4293
    },
    {
        "loss": 2.9272,
        "grad_norm": 2.406371831893921,
        "learning_rate": 9.657356942277873e-07,
        "epoch": 0.31999403830389744,
        "step": 4294
    },
    {
        "loss": 2.4021,
        "grad_norm": 3.6252527236938477,
        "learning_rate": 9.560046145300038e-07,
        "epoch": 0.3200685595051792,
        "step": 4295
    },
    {
        "loss": 1.6719,
        "grad_norm": 4.240085601806641,
        "learning_rate": 9.463225747573967e-07,
        "epoch": 0.32014308070646097,
        "step": 4296
    },
    {
        "loss": 2.3105,
        "grad_norm": 8.284883499145508,
        "learning_rate": 9.366895797038866e-07,
        "epoch": 0.32021760190774273,
        "step": 4297
    },
    {
        "loss": 2.3659,
        "grad_norm": 2.151043653488159,
        "learning_rate": 9.271056341390582e-07,
        "epoch": 0.3202921231090245,
        "step": 4298
    },
    {
        "loss": 2.5291,
        "grad_norm": 1.6606686115264893,
        "learning_rate": 9.175707428082492e-07,
        "epoch": 0.32036664431030626,
        "step": 4299
    },
    {
        "loss": 2.5894,
        "grad_norm": 3.167368173599243,
        "learning_rate": 9.08084910432494e-07,
        "epoch": 0.320441165511588,
        "step": 4300
    },
    {
        "loss": 2.9171,
        "grad_norm": 2.3495116233825684,
        "learning_rate": 8.986481417085246e-07,
        "epoch": 0.3205156867128698,
        "step": 4301
    },
    {
        "loss": 2.5508,
        "grad_norm": 2.776867151260376,
        "learning_rate": 8.892604413088368e-07,
        "epoch": 0.3205902079141516,
        "step": 4302
    },
    {
        "loss": 2.7752,
        "grad_norm": 2.0079896450042725,
        "learning_rate": 8.799218138815457e-07,
        "epoch": 0.32066472911543337,
        "step": 4303
    },
    {
        "loss": 1.714,
        "grad_norm": 3.219066858291626,
        "learning_rate": 8.706322640505304e-07,
        "epoch": 0.32073925031671513,
        "step": 4304
    },
    {
        "loss": 2.3917,
        "grad_norm": 2.2217400074005127,
        "learning_rate": 8.613917964153339e-07,
        "epoch": 0.3208137715179969,
        "step": 4305
    },
    {
        "loss": 1.7136,
        "grad_norm": 2.726713180541992,
        "learning_rate": 8.522004155512409e-07,
        "epoch": 0.32088829271927866,
        "step": 4306
    },
    {
        "loss": 2.2201,
        "grad_norm": 3.2738823890686035,
        "learning_rate": 8.430581260091886e-07,
        "epoch": 0.3209628139205604,
        "step": 4307
    },
    {
        "loss": 2.3086,
        "grad_norm": 2.5910391807556152,
        "learning_rate": 8.339649323158227e-07,
        "epoch": 0.3210373351218422,
        "step": 4308
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.5822291374206543,
        "learning_rate": 8.249208389734974e-07,
        "epoch": 0.32111185632312395,
        "step": 4309
    },
    {
        "loss": 2.0604,
        "grad_norm": 2.2912352085113525,
        "learning_rate": 8.159258504602085e-07,
        "epoch": 0.3211863775244057,
        "step": 4310
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.508998394012451,
        "learning_rate": 8.069799712297044e-07,
        "epoch": 0.32126089872568747,
        "step": 4311
    },
    {
        "loss": 1.923,
        "grad_norm": 3.414780855178833,
        "learning_rate": 7.980832057113641e-07,
        "epoch": 0.32133541992696923,
        "step": 4312
    },
    {
        "loss": 2.1272,
        "grad_norm": 2.3948006629943848,
        "learning_rate": 7.89235558310264e-07,
        "epoch": 0.321409941128251,
        "step": 4313
    },
    {
        "loss": 1.929,
        "grad_norm": 2.4661805629730225,
        "learning_rate": 7.804370334071665e-07,
        "epoch": 0.32148446232953276,
        "step": 4314
    },
    {
        "loss": 2.3478,
        "grad_norm": 2.6523261070251465,
        "learning_rate": 7.7168763535852e-07,
        "epoch": 0.3215589835308145,
        "step": 4315
    },
    {
        "loss": 1.8479,
        "grad_norm": 1.9639002084732056,
        "learning_rate": 7.629873684964262e-07,
        "epoch": 0.3216335047320963,
        "step": 4316
    },
    {
        "loss": 2.5647,
        "grad_norm": 2.25801944732666,
        "learning_rate": 7.543362371286833e-07,
        "epoch": 0.32170802593337805,
        "step": 4317
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.8483858108520508,
        "learning_rate": 7.457342455387429e-07,
        "epoch": 0.3217825471346598,
        "step": 4318
    },
    {
        "loss": 2.6583,
        "grad_norm": 3.8208658695220947,
        "learning_rate": 7.371813979857312e-07,
        "epoch": 0.3218570683359416,
        "step": 4319
    },
    {
        "loss": 2.1311,
        "grad_norm": 2.599393367767334,
        "learning_rate": 7.286776987044385e-07,
        "epoch": 0.32193158953722334,
        "step": 4320
    },
    {
        "loss": 2.5117,
        "grad_norm": 2.5079073905944824,
        "learning_rate": 7.202231519053416e-07,
        "epoch": 0.3220061107385051,
        "step": 4321
    },
    {
        "loss": 2.6874,
        "grad_norm": 2.032801628112793,
        "learning_rate": 7.118177617745359e-07,
        "epoch": 0.32208063193978687,
        "step": 4322
    },
    {
        "loss": 2.8112,
        "grad_norm": 2.35793399810791,
        "learning_rate": 7.034615324738369e-07,
        "epoch": 0.32215515314106863,
        "step": 4323
    },
    {
        "loss": 2.1662,
        "grad_norm": 3.273465871810913,
        "learning_rate": 6.951544681406685e-07,
        "epoch": 0.3222296743423504,
        "step": 4324
    },
    {
        "loss": 2.3235,
        "grad_norm": 2.8964734077453613,
        "learning_rate": 6.868965728881405e-07,
        "epoch": 0.32230419554363215,
        "step": 4325
    },
    {
        "loss": 1.7494,
        "grad_norm": 1.9803637266159058,
        "learning_rate": 6.786878508049931e-07,
        "epoch": 0.3223787167449139,
        "step": 4326
    },
    {
        "loss": 2.3981,
        "grad_norm": 2.4285192489624023,
        "learning_rate": 6.705283059556422e-07,
        "epoch": 0.3224532379461957,
        "step": 4327
    },
    {
        "loss": 2.1839,
        "grad_norm": 2.356652021408081,
        "learning_rate": 6.624179423801558e-07,
        "epoch": 0.32252775914747744,
        "step": 4328
    },
    {
        "loss": 2.4345,
        "grad_norm": 2.049325704574585,
        "learning_rate": 6.543567640942216e-07,
        "epoch": 0.3226022803487592,
        "step": 4329
    },
    {
        "loss": 2.0189,
        "grad_norm": 1.8311963081359863,
        "learning_rate": 6.463447750892027e-07,
        "epoch": 0.32267680155004097,
        "step": 4330
    },
    {
        "loss": 2.2356,
        "grad_norm": 4.488183498382568,
        "learning_rate": 6.383819793320922e-07,
        "epoch": 0.32275132275132273,
        "step": 4331
    },
    {
        "loss": 2.0731,
        "grad_norm": 2.747236728668213,
        "learning_rate": 6.304683807655365e-07,
        "epoch": 0.3228258439526045,
        "step": 4332
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.1669321060180664,
        "learning_rate": 6.226039833078123e-07,
        "epoch": 0.32290036515388626,
        "step": 4333
    },
    {
        "loss": 2.432,
        "grad_norm": 3.958925247192383,
        "learning_rate": 6.147887908528382e-07,
        "epoch": 0.322974886355168,
        "step": 4334
    },
    {
        "loss": 2.4061,
        "grad_norm": 1.7700753211975098,
        "learning_rate": 6.070228072701855e-07,
        "epoch": 0.3230494075564498,
        "step": 4335
    },
    {
        "loss": 2.114,
        "grad_norm": 2.8567233085632324,
        "learning_rate": 5.99306036405034e-07,
        "epoch": 0.32312392875773155,
        "step": 4336
    },
    {
        "loss": 1.8476,
        "grad_norm": 3.2697715759277344,
        "learning_rate": 5.916384820782161e-07,
        "epoch": 0.3231984499590133,
        "step": 4337
    },
    {
        "loss": 2.3571,
        "grad_norm": 2.038687229156494,
        "learning_rate": 5.840201480861951e-07,
        "epoch": 0.32327297116029513,
        "step": 4338
    },
    {
        "loss": 2.4192,
        "grad_norm": 3.3792197704315186,
        "learning_rate": 5.764510382010424e-07,
        "epoch": 0.3233474923615769,
        "step": 4339
    },
    {
        "loss": 1.8809,
        "grad_norm": 2.8214657306671143,
        "learning_rate": 5.689311561704824e-07,
        "epoch": 0.32342201356285866,
        "step": 4340
    },
    {
        "loss": 2.6607,
        "grad_norm": 2.0250070095062256,
        "learning_rate": 5.614605057178368e-07,
        "epoch": 0.3234965347641404,
        "step": 4341
    },
    {
        "loss": 1.7338,
        "grad_norm": 2.492685317993164,
        "learning_rate": 5.540390905420912e-07,
        "epoch": 0.3235710559654222,
        "step": 4342
    },
    {
        "loss": 1.9247,
        "grad_norm": 3.2697927951812744,
        "learning_rate": 5.466669143178282e-07,
        "epoch": 0.32364557716670395,
        "step": 4343
    },
    {
        "loss": 2.2363,
        "grad_norm": 1.7888771295547485,
        "learning_rate": 5.393439806952505e-07,
        "epoch": 0.3237200983679857,
        "step": 4344
    },
    {
        "loss": 2.5788,
        "grad_norm": 3.86995530128479,
        "learning_rate": 5.320702933001909e-07,
        "epoch": 0.32379461956926747,
        "step": 4345
    },
    {
        "loss": 1.7269,
        "grad_norm": 3.9878954887390137,
        "learning_rate": 5.248458557340574e-07,
        "epoch": 0.32386914077054924,
        "step": 4346
    },
    {
        "loss": 2.0761,
        "grad_norm": 2.6265034675598145,
        "learning_rate": 5.176706715739443e-07,
        "epoch": 0.323943661971831,
        "step": 4347
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.2600600719451904,
        "learning_rate": 5.105447443724986e-07,
        "epoch": 0.32401818317311276,
        "step": 4348
    },
    {
        "loss": 1.937,
        "grad_norm": 2.4723048210144043,
        "learning_rate": 5.034680776580091e-07,
        "epoch": 0.3240927043743945,
        "step": 4349
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.2165474891662598,
        "learning_rate": 4.96440674934362e-07,
        "epoch": 0.3241672255756763,
        "step": 4350
    },
    {
        "loss": 2.3033,
        "grad_norm": 3.1176607608795166,
        "learning_rate": 4.894625396810625e-07,
        "epoch": 0.32424174677695805,
        "step": 4351
    },
    {
        "loss": 2.6035,
        "grad_norm": 3.725156545639038,
        "learning_rate": 4.825336753531917e-07,
        "epoch": 0.3243162679782398,
        "step": 4352
    },
    {
        "loss": 1.7624,
        "grad_norm": 3.0144217014312744,
        "learning_rate": 4.7565408538148283e-07,
        "epoch": 0.3243907891795216,
        "step": 4353
    },
    {
        "loss": 2.4115,
        "grad_norm": 2.4874050617218018,
        "learning_rate": 4.6882377317223336e-07,
        "epoch": 0.32446531038080334,
        "step": 4354
    },
    {
        "loss": 1.477,
        "grad_norm": 2.5524051189422607,
        "learning_rate": 4.620427421073492e-07,
        "epoch": 0.3245398315820851,
        "step": 4355
    },
    {
        "loss": 2.6191,
        "grad_norm": 2.575253486633301,
        "learning_rate": 4.5531099554435576e-07,
        "epoch": 0.32461435278336687,
        "step": 4356
    },
    {
        "loss": 2.3248,
        "grad_norm": 3.1347222328186035,
        "learning_rate": 4.486285368163423e-07,
        "epoch": 0.32468887398464863,
        "step": 4357
    },
    {
        "loss": 1.3899,
        "grad_norm": 3.268528461456299,
        "learning_rate": 4.4199536923204e-07,
        "epoch": 0.3247633951859304,
        "step": 4358
    },
    {
        "loss": 1.5363,
        "grad_norm": 2.988656520843506,
        "learning_rate": 4.354114960757327e-07,
        "epoch": 0.32483791638721216,
        "step": 4359
    },
    {
        "loss": 2.2975,
        "grad_norm": 1.7932384014129639,
        "learning_rate": 4.2887692060729066e-07,
        "epoch": 0.3249124375884939,
        "step": 4360
    },
    {
        "loss": 2.9005,
        "grad_norm": 2.548518419265747,
        "learning_rate": 4.223916460622257e-07,
        "epoch": 0.3249869587897757,
        "step": 4361
    },
    {
        "loss": 2.5882,
        "grad_norm": 2.258817434310913,
        "learning_rate": 4.1595567565160255e-07,
        "epoch": 0.32506147999105744,
        "step": 4362
    },
    {
        "loss": 2.2155,
        "grad_norm": 2.965156316757202,
        "learning_rate": 4.095690125620832e-07,
        "epoch": 0.3251360011923392,
        "step": 4363
    },
    {
        "loss": 2.8971,
        "grad_norm": 3.166409492492676,
        "learning_rate": 4.0323165995589386e-07,
        "epoch": 0.32521052239362097,
        "step": 4364
    },
    {
        "loss": 1.6487,
        "grad_norm": 1.6546967029571533,
        "learning_rate": 3.96943620970891e-07,
        "epoch": 0.32528504359490273,
        "step": 4365
    },
    {
        "loss": 2.4427,
        "grad_norm": 2.290118455886841,
        "learning_rate": 3.907048987204731e-07,
        "epoch": 0.3253595647961845,
        "step": 4366
    },
    {
        "loss": 2.4599,
        "grad_norm": 2.9445061683654785,
        "learning_rate": 3.845154962936359e-07,
        "epoch": 0.32543408599746626,
        "step": 4367
    },
    {
        "loss": 2.3477,
        "grad_norm": 2.473384380340576,
        "learning_rate": 3.7837541675497246e-07,
        "epoch": 0.325508607198748,
        "step": 4368
    },
    {
        "loss": 2.5761,
        "grad_norm": 2.834780693054199,
        "learning_rate": 3.722846631446175e-07,
        "epoch": 0.3255831284000298,
        "step": 4369
    },
    {
        "loss": 2.8606,
        "grad_norm": 1.709971308708191,
        "learning_rate": 3.662432384783032e-07,
        "epoch": 0.32565764960131155,
        "step": 4370
    },
    {
        "loss": 1.9188,
        "grad_norm": 3.4344706535339355,
        "learning_rate": 3.6025114574734785e-07,
        "epoch": 0.3257321708025933,
        "step": 4371
    },
    {
        "loss": 2.5509,
        "grad_norm": 2.18131685256958,
        "learning_rate": 3.5430838791863373e-07,
        "epoch": 0.3258066920038751,
        "step": 4372
    },
    {
        "loss": 1.9863,
        "grad_norm": 4.669402122497559,
        "learning_rate": 3.4841496793459604e-07,
        "epoch": 0.3258812132051569,
        "step": 4373
    },
    {
        "loss": 2.3761,
        "grad_norm": 1.9396605491638184,
        "learning_rate": 3.4257088871327837e-07,
        "epoch": 0.32595573440643866,
        "step": 4374
    },
    {
        "loss": 1.6417,
        "grad_norm": 4.334839344024658,
        "learning_rate": 3.3677615314827714e-07,
        "epoch": 0.3260302556077204,
        "step": 4375
    },
    {
        "loss": 2.5474,
        "grad_norm": 2.616926431655884,
        "learning_rate": 3.310307641087418e-07,
        "epoch": 0.3261047768090022,
        "step": 4376
    },
    {
        "loss": 2.3636,
        "grad_norm": 2.8341126441955566,
        "learning_rate": 3.2533472443941893e-07,
        "epoch": 0.32617929801028395,
        "step": 4377
    },
    {
        "loss": 2.3065,
        "grad_norm": 2.391982078552246,
        "learning_rate": 3.196880369605859e-07,
        "epoch": 0.3262538192115657,
        "step": 4378
    },
    {
        "loss": 1.5311,
        "grad_norm": 4.1128764152526855,
        "learning_rate": 3.140907044681063e-07,
        "epoch": 0.3263283404128475,
        "step": 4379
    },
    {
        "loss": 2.7212,
        "grad_norm": 2.8404955863952637,
        "learning_rate": 3.085427297334187e-07,
        "epoch": 0.32640286161412924,
        "step": 4380
    },
    {
        "loss": 1.5038,
        "grad_norm": 3.3881819248199463,
        "learning_rate": 3.030441155034924e-07,
        "epoch": 0.326477382815411,
        "step": 4381
    },
    {
        "loss": 2.5527,
        "grad_norm": 3.362682580947876,
        "learning_rate": 2.9759486450087194e-07,
        "epoch": 0.32655190401669276,
        "step": 4382
    },
    {
        "loss": 2.6035,
        "grad_norm": 3.44385027885437,
        "learning_rate": 2.921949794236656e-07,
        "epoch": 0.3266264252179745,
        "step": 4383
    },
    {
        "loss": 3.0542,
        "grad_norm": 1.8268194198608398,
        "learning_rate": 2.868444629455347e-07,
        "epoch": 0.3267009464192563,
        "step": 4384
    },
    {
        "loss": 2.3657,
        "grad_norm": 3.6559152603149414,
        "learning_rate": 2.8154331771569343e-07,
        "epoch": 0.32677546762053805,
        "step": 4385
    },
    {
        "loss": 2.3325,
        "grad_norm": 4.097930908203125,
        "learning_rate": 2.7629154635889775e-07,
        "epoch": 0.3268499888218198,
        "step": 4386
    },
    {
        "loss": 2.2699,
        "grad_norm": 2.0931828022003174,
        "learning_rate": 2.7108915147550097e-07,
        "epoch": 0.3269245100231016,
        "step": 4387
    },
    {
        "loss": 2.3378,
        "grad_norm": 2.9284157752990723,
        "learning_rate": 2.6593613564134256e-07,
        "epoch": 0.32699903122438334,
        "step": 4388
    },
    {
        "loss": 1.6892,
        "grad_norm": 2.939267158508301,
        "learning_rate": 2.6083250140788163e-07,
        "epoch": 0.3270735524256651,
        "step": 4389
    },
    {
        "loss": 2.179,
        "grad_norm": 2.229249954223633,
        "learning_rate": 2.5577825130208566e-07,
        "epoch": 0.32714807362694687,
        "step": 4390
    },
    {
        "loss": 1.6568,
        "grad_norm": 3.0979602336883545,
        "learning_rate": 2.50773387826464e-07,
        "epoch": 0.32722259482822863,
        "step": 4391
    },
    {
        "loss": 2.4768,
        "grad_norm": 3.620234489440918,
        "learning_rate": 2.458179134591121e-07,
        "epoch": 0.3272971160295104,
        "step": 4392
    },
    {
        "loss": 2.5548,
        "grad_norm": 2.371372938156128,
        "learning_rate": 2.409118306536229e-07,
        "epoch": 0.32737163723079216,
        "step": 4393
    },
    {
        "loss": 2.55,
        "grad_norm": 2.537296772003174,
        "learning_rate": 2.3605514183918652e-07,
        "epoch": 0.3274461584320739,
        "step": 4394
    },
    {
        "loss": 1.7603,
        "grad_norm": 2.3025612831115723,
        "learning_rate": 2.312478494204795e-07,
        "epoch": 0.3275206796333557,
        "step": 4395
    },
    {
        "loss": 2.564,
        "grad_norm": 2.3455867767333984,
        "learning_rate": 2.2648995577777554e-07,
        "epoch": 0.32759520083463745,
        "step": 4396
    },
    {
        "loss": 2.3162,
        "grad_norm": 3.092961311340332,
        "learning_rate": 2.217814632668458e-07,
        "epoch": 0.3276697220359192,
        "step": 4397
    },
    {
        "loss": 2.6709,
        "grad_norm": 2.108106851577759,
        "learning_rate": 2.1712237421902537e-07,
        "epoch": 0.32774424323720097,
        "step": 4398
    },
    {
        "loss": 1.4794,
        "grad_norm": 2.915938377380371,
        "learning_rate": 2.1251269094118008e-07,
        "epoch": 0.32781876443848273,
        "step": 4399
    },
    {
        "loss": 2.3843,
        "grad_norm": 3.0998828411102295,
        "learning_rate": 2.0795241571572866e-07,
        "epoch": 0.3278932856397645,
        "step": 4400
    },
    {
        "loss": 2.3572,
        "grad_norm": 2.733121395111084,
        "learning_rate": 2.0344155080058712e-07,
        "epoch": 0.32796780684104626,
        "step": 4401
    },
    {
        "loss": 2.4034,
        "grad_norm": 3.3701698780059814,
        "learning_rate": 1.989800984292467e-07,
        "epoch": 0.328042328042328,
        "step": 4402
    },
    {
        "loss": 2.5032,
        "grad_norm": 3.076870918273926,
        "learning_rate": 1.945680608107181e-07,
        "epoch": 0.3281168492436098,
        "step": 4403
    },
    {
        "loss": 2.6632,
        "grad_norm": 3.584839105606079,
        "learning_rate": 1.9020544012955388e-07,
        "epoch": 0.32819137044489155,
        "step": 4404
    },
    {
        "loss": 2.7602,
        "grad_norm": 2.917551040649414,
        "learning_rate": 1.8589223854581506e-07,
        "epoch": 0.3282658916461733,
        "step": 4405
    },
    {
        "loss": 2.1593,
        "grad_norm": 3.245438575744629,
        "learning_rate": 1.8162845819511553e-07,
        "epoch": 0.3283404128474551,
        "step": 4406
    },
    {
        "loss": 2.1411,
        "grad_norm": 2.699235677719116,
        "learning_rate": 1.7741410118858881e-07,
        "epoch": 0.32841493404873684,
        "step": 4407
    },
    {
        "loss": 2.3102,
        "grad_norm": 3.1968626976013184,
        "learning_rate": 1.7324916961291015e-07,
        "epoch": 0.3284894552500186,
        "step": 4408
    },
    {
        "loss": 2.1236,
        "grad_norm": 3.1683146953582764,
        "learning_rate": 1.6913366553027443e-07,
        "epoch": 0.3285639764513004,
        "step": 4409
    },
    {
        "loss": 2.6676,
        "grad_norm": 2.7968664169311523,
        "learning_rate": 1.6506759097839609e-07,
        "epoch": 0.3286384976525822,
        "step": 4410
    },
    {
        "loss": 1.9789,
        "grad_norm": 2.7487285137176514,
        "learning_rate": 1.6105094797050912e-07,
        "epoch": 0.32871301885386395,
        "step": 4411
    },
    {
        "loss": 2.5767,
        "grad_norm": 1.9447698593139648,
        "learning_rate": 1.5708373849541158e-07,
        "epoch": 0.3287875400551457,
        "step": 4412
    },
    {
        "loss": 1.819,
        "grad_norm": 3.0601747035980225,
        "learning_rate": 1.5316596451736554e-07,
        "epoch": 0.3288620612564275,
        "step": 4413
    },
    {
        "loss": 1.792,
        "grad_norm": 2.329338550567627,
        "learning_rate": 1.4929762797623037e-07,
        "epoch": 0.32893658245770924,
        "step": 4414
    },
    {
        "loss": 2.299,
        "grad_norm": 2.068216562271118,
        "learning_rate": 1.4547873078731845e-07,
        "epoch": 0.329011103658991,
        "step": 4415
    },
    {
        "loss": 2.6207,
        "grad_norm": 2.820769786834717,
        "learning_rate": 1.41709274841495e-07,
        "epoch": 0.32908562486027276,
        "step": 4416
    },
    {
        "loss": 2.7796,
        "grad_norm": 2.2962422370910645,
        "learning_rate": 1.3798926200513374e-07,
        "epoch": 0.3291601460615545,
        "step": 4417
    },
    {
        "loss": 1.1421,
        "grad_norm": 3.4878487586975098,
        "learning_rate": 1.343186941201502e-07,
        "epoch": 0.3292346672628363,
        "step": 4418
    },
    {
        "loss": 2.0833,
        "grad_norm": 2.9487533569335938,
        "learning_rate": 1.3069757300396833e-07,
        "epoch": 0.32930918846411805,
        "step": 4419
    },
    {
        "loss": 2.4079,
        "grad_norm": 2.5063393115997314,
        "learning_rate": 1.2712590044949845e-07,
        "epoch": 0.3293837096653998,
        "step": 4420
    },
    {
        "loss": 2.6303,
        "grad_norm": 3.347083568572998,
        "learning_rate": 1.2360367822521478e-07,
        "epoch": 0.3294582308666816,
        "step": 4421
    },
    {
        "loss": 2.444,
        "grad_norm": 2.5969581604003906,
        "learning_rate": 1.2013090807506678e-07,
        "epoch": 0.32953275206796334,
        "step": 4422
    },
    {
        "loss": 2.5869,
        "grad_norm": 2.1790549755096436,
        "learning_rate": 1.1670759171855672e-07,
        "epoch": 0.3296072732692451,
        "step": 4423
    },
    {
        "loss": 1.3492,
        "grad_norm": 0.999395489692688,
        "learning_rate": 1.1333373085066213e-07,
        "epoch": 0.32968179447052687,
        "step": 4424
    },
    {
        "loss": 2.6411,
        "grad_norm": 2.4682416915893555,
        "learning_rate": 1.1000932714190226e-07,
        "epoch": 0.32975631567180863,
        "step": 4425
    },
    {
        "loss": 3.1107,
        "grad_norm": 2.0440235137939453,
        "learning_rate": 1.067343822382938e-07,
        "epoch": 0.3298308368730904,
        "step": 4426
    },
    {
        "loss": 2.4904,
        "grad_norm": 2.7314960956573486,
        "learning_rate": 1.0350889776138406e-07,
        "epoch": 0.32990535807437216,
        "step": 4427
    },
    {
        "loss": 1.5552,
        "grad_norm": 3.4817349910736084,
        "learning_rate": 1.0033287530818447e-07,
        "epoch": 0.3299798792756539,
        "step": 4428
    },
    {
        "loss": 1.9311,
        "grad_norm": 3.074864149093628,
        "learning_rate": 9.720631645128154e-08,
        "epoch": 0.3300544004769357,
        "step": 4429
    },
    {
        "loss": 2.7807,
        "grad_norm": 4.313757419586182,
        "learning_rate": 9.412922273871471e-08,
        "epoch": 0.33012892167821745,
        "step": 4430
    },
    {
        "loss": 2.6546,
        "grad_norm": 1.8545048236846924,
        "learning_rate": 9.110159569406529e-08,
        "epoch": 0.3302034428794992,
        "step": 4431
    },
    {
        "loss": 2.383,
        "grad_norm": 3.2323262691497803,
        "learning_rate": 8.812343681640079e-08,
        "epoch": 0.330277964080781,
        "step": 4432
    },
    {
        "loss": 2.3568,
        "grad_norm": 4.185401439666748,
        "learning_rate": 8.519474758030832e-08,
        "epoch": 0.33035248528206274,
        "step": 4433
    },
    {
        "loss": 2.9984,
        "grad_norm": 4.339685440063477,
        "learning_rate": 8.23155294358946e-08,
        "epoch": 0.3304270064833445,
        "step": 4434
    },
    {
        "loss": 2.2047,
        "grad_norm": 2.9180588722229004,
        "learning_rate": 7.948578380873039e-08,
        "epoch": 0.33050152768462626,
        "step": 4435
    },
    {
        "loss": 2.5243,
        "grad_norm": 3.80873441696167,
        "learning_rate": 7.670551209992826e-08,
        "epoch": 0.330576048885908,
        "step": 4436
    },
    {
        "loss": 2.6886,
        "grad_norm": 2.453873872756958,
        "learning_rate": 7.397471568607595e-08,
        "epoch": 0.3306505700871898,
        "step": 4437
    },
    {
        "loss": 2.3879,
        "grad_norm": 3.2227156162261963,
        "learning_rate": 7.129339591931406e-08,
        "epoch": 0.33072509128847155,
        "step": 4438
    },
    {
        "loss": 2.3751,
        "grad_norm": 2.0886764526367188,
        "learning_rate": 6.866155412721398e-08,
        "epoch": 0.3307996124897533,
        "step": 4439
    },
    {
        "loss": 2.4618,
        "grad_norm": 2.288175582885742,
        "learning_rate": 6.60791916129222e-08,
        "epoch": 0.3308741336910351,
        "step": 4440
    },
    {
        "loss": 2.3264,
        "grad_norm": 1.9315526485443115,
        "learning_rate": 6.354630965501595e-08,
        "epoch": 0.33094865489231684,
        "step": 4441
    },
    {
        "loss": 1.8796,
        "grad_norm": 1.6761326789855957,
        "learning_rate": 6.106290950762539e-08,
        "epoch": 0.3310231760935986,
        "step": 4442
    },
    {
        "loss": 2.8216,
        "grad_norm": 2.681596040725708,
        "learning_rate": 5.8628992400378e-08,
        "epoch": 0.33109769729488037,
        "step": 4443
    },
    {
        "loss": 1.7314,
        "grad_norm": 3.6921708583831787,
        "learning_rate": 5.624455953835428e-08,
        "epoch": 0.3311722184961622,
        "step": 4444
    },
    {
        "loss": 2.4831,
        "grad_norm": 2.444636583328247,
        "learning_rate": 5.390961210218759e-08,
        "epoch": 0.33124673969744395,
        "step": 4445
    },
    {
        "loss": 1.8274,
        "grad_norm": 3.2872560024261475,
        "learning_rate": 5.162415124797537e-08,
        "epoch": 0.3313212608987257,
        "step": 4446
    },
    {
        "loss": 1.9135,
        "grad_norm": 1.8978701829910278,
        "learning_rate": 4.938817810733465e-08,
        "epoch": 0.3313957821000075,
        "step": 4447
    },
    {
        "loss": 2.4358,
        "grad_norm": 4.131961345672607,
        "learning_rate": 4.7201693787357616e-08,
        "epoch": 0.33147030330128924,
        "step": 4448
    },
    {
        "loss": 2.1397,
        "grad_norm": 2.0517680644989014,
        "learning_rate": 4.506469937065605e-08,
        "epoch": 0.331544824502571,
        "step": 4449
    },
    {
        "loss": 2.4183,
        "grad_norm": 2.657261848449707,
        "learning_rate": 4.2977195915316906e-08,
        "epoch": 0.33161934570385276,
        "step": 4450
    },
    {
        "loss": 1.6412,
        "grad_norm": 1.9910657405853271,
        "learning_rate": 4.093918445493561e-08,
        "epoch": 0.3316938669051345,
        "step": 4451
    },
    {
        "loss": 2.1217,
        "grad_norm": 2.922487497329712,
        "learning_rate": 3.8950665998593874e-08,
        "epoch": 0.3317683881064163,
        "step": 4452
    },
    {
        "loss": 2.8203,
        "grad_norm": 3.2391440868377686,
        "learning_rate": 3.701164153088188e-08,
        "epoch": 0.33184290930769805,
        "step": 4453
    },
    {
        "loss": 2.3802,
        "grad_norm": 1.7214159965515137,
        "learning_rate": 3.5122112011865e-08,
        "epoch": 0.3319174305089798,
        "step": 4454
    },
    {
        "loss": 2.1356,
        "grad_norm": 2.4843194484710693,
        "learning_rate": 3.328207837711705e-08,
        "epoch": 0.3319919517102616,
        "step": 4455
    },
    {
        "loss": 2.6862,
        "grad_norm": 1.737391471862793,
        "learning_rate": 3.1491541537687076e-08,
        "epoch": 0.33206647291154334,
        "step": 4456
    },
    {
        "loss": 2.0678,
        "grad_norm": 2.703152894973755,
        "learning_rate": 2.975050238014365e-08,
        "epoch": 0.3321409941128251,
        "step": 4457
    },
    {
        "loss": 2.196,
        "grad_norm": 1.9293941259384155,
        "learning_rate": 2.8058961766519452e-08,
        "epoch": 0.33221551531410687,
        "step": 4458
    },
    {
        "loss": 2.3749,
        "grad_norm": 3.1557464599609375,
        "learning_rate": 2.6416920534366728e-08,
        "epoch": 0.33229003651538863,
        "step": 4459
    },
    {
        "loss": 2.5296,
        "grad_norm": 3.937854051589966,
        "learning_rate": 2.48243794967018e-08,
        "epoch": 0.3323645577166704,
        "step": 4460
    },
    {
        "loss": 1.734,
        "grad_norm": 2.5557796955108643,
        "learning_rate": 2.3281339442049466e-08,
        "epoch": 0.33243907891795216,
        "step": 4461
    },
    {
        "loss": 2.4459,
        "grad_norm": 2.1052958965301514,
        "learning_rate": 2.1787801134409702e-08,
        "epoch": 0.3325136001192339,
        "step": 4462
    },
    {
        "loss": 2.8331,
        "grad_norm": 1.8979851007461548,
        "learning_rate": 2.0343765313302066e-08,
        "epoch": 0.3325881213205157,
        "step": 4463
    },
    {
        "loss": 1.7262,
        "grad_norm": 2.2571046352386475,
        "learning_rate": 1.894923269368798e-08,
        "epoch": 0.33266264252179745,
        "step": 4464
    },
    {
        "loss": 1.7885,
        "grad_norm": 2.2638800144195557,
        "learning_rate": 1.7604203966070655e-08,
        "epoch": 0.3327371637230792,
        "step": 4465
    },
    {
        "loss": 2.6578,
        "grad_norm": 2.577270746231079,
        "learning_rate": 1.630867979641737e-08,
        "epoch": 0.332811684924361,
        "step": 4466
    },
    {
        "loss": 2.4136,
        "grad_norm": 2.3813960552215576,
        "learning_rate": 1.506266082615948e-08,
        "epoch": 0.33288620612564274,
        "step": 4467
    },
    {
        "loss": 2.1545,
        "grad_norm": 3.684955358505249,
        "learning_rate": 1.3866147672270124e-08,
        "epoch": 0.3329607273269245,
        "step": 4468
    },
    {
        "loss": 1.4847,
        "grad_norm": 2.7891459465026855,
        "learning_rate": 1.2719140927164308e-08,
        "epoch": 0.33303524852820626,
        "step": 4469
    },
    {
        "loss": 1.6243,
        "grad_norm": 2.515061616897583,
        "learning_rate": 1.162164115877662e-08,
        "epoch": 0.333109769729488,
        "step": 4470
    },
    {
        "loss": 2.6235,
        "grad_norm": 2.128960132598877,
        "learning_rate": 1.0573648910494616e-08,
        "epoch": 0.3331842909307698,
        "step": 4471
    },
    {
        "loss": 2.2515,
        "grad_norm": 1.9002162218093872,
        "learning_rate": 9.575164701236538e-09,
        "epoch": 0.33325881213205155,
        "step": 4472
    },
    {
        "loss": 2.7039,
        "grad_norm": 2.2020833492279053,
        "learning_rate": 8.6261890253736e-09,
        "epoch": 0.3333333333333333,
        "step": 4473
    },
    {
        "loss": 2.4247,
        "grad_norm": 3.5470457077026367,
        "learning_rate": 7.726722352774386e-09,
        "epoch": 0.3334078545346151,
        "step": 4474
    },
    {
        "loss": 1.9858,
        "grad_norm": 1.7573001384735107,
        "learning_rate": 6.876765128793761e-09,
        "epoch": 0.33348237573589684,
        "step": 4475
    },
    {
        "loss": 2.468,
        "grad_norm": 2.5181944370269775,
        "learning_rate": 6.0763177742839686e-09,
        "epoch": 0.3335568969371786,
        "step": 4476
    },
    {
        "loss": 2.7287,
        "grad_norm": 2.5973050594329834,
        "learning_rate": 5.325380685550219e-09,
        "epoch": 0.33363141813846037,
        "step": 4477
    },
    {
        "loss": 2.5336,
        "grad_norm": 1.9382904767990112,
        "learning_rate": 4.623954234428407e-09,
        "epoch": 0.33370593933974213,
        "step": 4478
    },
    {
        "loss": 2.8878,
        "grad_norm": 3.0442540645599365,
        "learning_rate": 3.972038768207398e-09,
        "epoch": 0.33378046054102395,
        "step": 4479
    },
    {
        "loss": 2.7089,
        "grad_norm": 2.3482234477996826,
        "learning_rate": 3.3696346096734334e-09,
        "epoch": 0.3338549817423057,
        "step": 4480
    },
    {
        "loss": 2.2308,
        "grad_norm": 2.926738739013672,
        "learning_rate": 2.816742057087929e-09,
        "epoch": 0.3339295029435875,
        "step": 4481
    },
    {
        "loss": 2.4532,
        "grad_norm": 1.3268108367919922,
        "learning_rate": 2.3133613842318824e-09,
        "epoch": 0.33400402414486924,
        "step": 4482
    },
    {
        "loss": 2.2827,
        "grad_norm": 2.4242117404937744,
        "learning_rate": 1.8594928403170565e-09,
        "epoch": 0.334078545346151,
        "step": 4483
    },
    {
        "loss": 2.7936,
        "grad_norm": 1.7933403253555298,
        "learning_rate": 1.4551366500747953e-09,
        "epoch": 0.33415306654743276,
        "step": 4484
    },
    {
        "loss": 2.1762,
        "grad_norm": 3.7653329372406006,
        "learning_rate": 1.1002930137338218e-09,
        "epoch": 0.3342275877487145,
        "step": 4485
    },
    {
        "loss": 2.3958,
        "grad_norm": 3.0189929008483887,
        "learning_rate": 7.949621069647251e-10,
        "epoch": 0.3343021089499963,
        "step": 4486
    },
    {
        "loss": 2.7613,
        "grad_norm": 2.1682138442993164,
        "learning_rate": 5.391440809576765e-10,
        "epoch": 0.33437663015127805,
        "step": 4487
    },
    {
        "loss": 1.3335,
        "grad_norm": 2.2205069065093994,
        "learning_rate": 3.328390623891231e-10,
        "epoch": 0.3344511513525598,
        "step": 4488
    },
    {
        "loss": 1.9236,
        "grad_norm": 3.4378750324249268,
        "learning_rate": 1.7604715337737888e-10,
        "epoch": 0.3345256725538416,
        "step": 4489
    },
    {
        "loss": 1.8126,
        "grad_norm": 3.6502788066864014,
        "learning_rate": 6.876843158254432e-11,
        "epoch": 0.33460019375512334,
        "step": 4490
    },
    {
        "loss": 1.7863,
        "grad_norm": 3.1638593673706055,
        "learning_rate": 1.1002950117688926e-11,
        "epoch": 0.3346747149564051,
        "step": 4491
    },
    {
        "loss": 2.8885,
        "grad_norm": 2.3825201988220215,
        "learning_rate": 0.00019999999724926244,
        "epoch": 0.33474923615768687,
        "step": 4492
    },
    {
        "loss": 1.8164,
        "grad_norm": 4.874750137329102,
        "learning_rate": 0.00019999995598820198,
        "epoch": 0.33482375735896863,
        "step": 4493
    },
    {
        "loss": 1.77,
        "grad_norm": 2.383906841278076,
        "learning_rate": 0.00019999986521388894,
        "epoch": 0.3348982785602504,
        "step": 4494
    },
    {
        "loss": 2.1745,
        "grad_norm": 2.5756428241729736,
        "learning_rate": 0.00019999972492636825,
        "epoch": 0.33497279976153216,
        "step": 4495
    },
    {
        "loss": 2.6132,
        "grad_norm": 2.6271440982818604,
        "learning_rate": 0.0001999995351257094,
        "epoch": 0.3350473209628139,
        "step": 4496
    },
    {
        "loss": 2.7081,
        "grad_norm": 2.8948910236358643,
        "learning_rate": 0.00019999929581200634,
        "epoch": 0.3351218421640957,
        "step": 4497
    },
    {
        "loss": 2.5067,
        "grad_norm": 2.069202423095703,
        "learning_rate": 0.00019999900698537758,
        "epoch": 0.33519636336537745,
        "step": 4498
    },
    {
        "loss": 2.2384,
        "grad_norm": 3.7042007446289062,
        "learning_rate": 0.0001999986686459661,
        "epoch": 0.3352708845666592,
        "step": 4499
    },
    {
        "loss": 2.5968,
        "grad_norm": 2.2020750045776367,
        "learning_rate": 0.00019999828079393948,
        "epoch": 0.335345405767941,
        "step": 4500
    },
    {
        "loss": 2.3395,
        "grad_norm": 2.9232115745544434,
        "learning_rate": 0.00019999784342948968,
        "epoch": 0.33541992696922274,
        "step": 4501
    },
    {
        "loss": 2.4571,
        "grad_norm": 3.6542065143585205,
        "learning_rate": 0.0001999973565528333,
        "epoch": 0.3354944481705045,
        "step": 4502
    },
    {
        "loss": 2.3754,
        "grad_norm": 2.748652219772339,
        "learning_rate": 0.00019999682016421145,
        "epoch": 0.33556896937178626,
        "step": 4503
    },
    {
        "loss": 2.2914,
        "grad_norm": 2.364849805831909,
        "learning_rate": 0.00019999623426388962,
        "epoch": 0.335643490573068,
        "step": 4504
    },
    {
        "loss": 2.3494,
        "grad_norm": 2.191450595855713,
        "learning_rate": 0.00019999559885215798,
        "epoch": 0.3357180117743498,
        "step": 4505
    },
    {
        "loss": 2.1009,
        "grad_norm": 3.1753592491149902,
        "learning_rate": 0.0001999949139293311,
        "epoch": 0.33579253297563155,
        "step": 4506
    },
    {
        "loss": 2.0737,
        "grad_norm": 2.9308054447174072,
        "learning_rate": 0.00019999417949574815,
        "epoch": 0.3358670541769133,
        "step": 4507
    },
    {
        "loss": 2.4418,
        "grad_norm": 2.460765838623047,
        "learning_rate": 0.0001999933955517727,
        "epoch": 0.3359415753781951,
        "step": 4508
    },
    {
        "loss": 2.9176,
        "grad_norm": 3.4191694259643555,
        "learning_rate": 0.00019999256209779303,
        "epoch": 0.33601609657947684,
        "step": 4509
    },
    {
        "loss": 2.271,
        "grad_norm": 2.226588249206543,
        "learning_rate": 0.0001999916791342217,
        "epoch": 0.3360906177807586,
        "step": 4510
    },
    {
        "loss": 2.3988,
        "grad_norm": 2.446793556213379,
        "learning_rate": 0.0001999907466614959,
        "epoch": 0.33616513898204037,
        "step": 4511
    },
    {
        "loss": 1.6368,
        "grad_norm": 2.3109512329101562,
        "learning_rate": 0.00019998976468007742,
        "epoch": 0.33623966018332213,
        "step": 4512
    },
    {
        "loss": 2.6709,
        "grad_norm": 3.1518869400024414,
        "learning_rate": 0.00019998873319045237,
        "epoch": 0.3363141813846039,
        "step": 4513
    },
    {
        "loss": 2.4289,
        "grad_norm": 2.2001612186431885,
        "learning_rate": 0.00019998765219313153,
        "epoch": 0.33638870258588566,
        "step": 4514
    },
    {
        "loss": 2.6217,
        "grad_norm": 2.8530917167663574,
        "learning_rate": 0.00019998652168865008,
        "epoch": 0.3364632237871675,
        "step": 4515
    },
    {
        "loss": 1.8286,
        "grad_norm": 3.5308265686035156,
        "learning_rate": 0.00019998534167756785,
        "epoch": 0.33653774498844924,
        "step": 4516
    },
    {
        "loss": 2.4331,
        "grad_norm": 2.197457790374756,
        "learning_rate": 0.00019998411216046904,
        "epoch": 0.336612266189731,
        "step": 4517
    },
    {
        "loss": 1.8022,
        "grad_norm": 3.2541182041168213,
        "learning_rate": 0.00019998283313796245,
        "epoch": 0.33668678739101277,
        "step": 4518
    },
    {
        "loss": 2.4867,
        "grad_norm": 2.536921739578247,
        "learning_rate": 0.00019998150461068135,
        "epoch": 0.33676130859229453,
        "step": 4519
    },
    {
        "loss": 2.5308,
        "grad_norm": 2.147684335708618,
        "learning_rate": 0.0001999801265792836,
        "epoch": 0.3368358297935763,
        "step": 4520
    },
    {
        "loss": 2.3252,
        "grad_norm": 2.712678909301758,
        "learning_rate": 0.00019997869904445138,
        "epoch": 0.33691035099485805,
        "step": 4521
    },
    {
        "loss": 2.3712,
        "grad_norm": 3.5357329845428467,
        "learning_rate": 0.00019997722200689163,
        "epoch": 0.3369848721961398,
        "step": 4522
    },
    {
        "loss": 3.1368,
        "grad_norm": 3.790614366531372,
        "learning_rate": 0.00019997569546733565,
        "epoch": 0.3370593933974216,
        "step": 4523
    },
    {
        "loss": 2.2466,
        "grad_norm": 3.347008228302002,
        "learning_rate": 0.00019997411942653925,
        "epoch": 0.33713391459870334,
        "step": 4524
    },
    {
        "loss": 2.7155,
        "grad_norm": 3.5245864391326904,
        "learning_rate": 0.0001999724938852828,
        "epoch": 0.3372084357999851,
        "step": 4525
    },
    {
        "loss": 2.1955,
        "grad_norm": 4.521346569061279,
        "learning_rate": 0.00019997081884437113,
        "epoch": 0.33728295700126687,
        "step": 4526
    },
    {
        "loss": 2.6124,
        "grad_norm": 2.255125045776367,
        "learning_rate": 0.0001999690943046337,
        "epoch": 0.33735747820254863,
        "step": 4527
    },
    {
        "loss": 1.9824,
        "grad_norm": 3.4083971977233887,
        "learning_rate": 0.00019996732026692423,
        "epoch": 0.3374319994038304,
        "step": 4528
    },
    {
        "loss": 1.633,
        "grad_norm": 4.036614894866943,
        "learning_rate": 0.00019996549673212126,
        "epoch": 0.33750652060511216,
        "step": 4529
    },
    {
        "loss": 2.7151,
        "grad_norm": 2.539440631866455,
        "learning_rate": 0.00019996362370112758,
        "epoch": 0.3375810418063939,
        "step": 4530
    },
    {
        "loss": 2.2293,
        "grad_norm": 3.0239758491516113,
        "learning_rate": 0.00019996170117487063,
        "epoch": 0.3376555630076757,
        "step": 4531
    },
    {
        "loss": 2.5488,
        "grad_norm": 3.2547659873962402,
        "learning_rate": 0.00019995972915430233,
        "epoch": 0.33773008420895745,
        "step": 4532
    },
    {
        "loss": 2.8077,
        "grad_norm": 2.6101784706115723,
        "learning_rate": 0.00019995770764039903,
        "epoch": 0.3378046054102392,
        "step": 4533
    },
    {
        "loss": 1.6068,
        "grad_norm": 3.002074718475342,
        "learning_rate": 0.00019995563663416174,
        "epoch": 0.337879126611521,
        "step": 4534
    },
    {
        "loss": 2.7739,
        "grad_norm": 2.49454402923584,
        "learning_rate": 0.00019995351613661576,
        "epoch": 0.33795364781280274,
        "step": 4535
    },
    {
        "loss": 2.9338,
        "grad_norm": 2.985694169998169,
        "learning_rate": 0.00019995134614881114,
        "epoch": 0.3380281690140845,
        "step": 4536
    },
    {
        "loss": 1.9276,
        "grad_norm": 3.5904042720794678,
        "learning_rate": 0.00019994912667182228,
        "epoch": 0.33810269021536626,
        "step": 4537
    },
    {
        "loss": 2.0944,
        "grad_norm": 2.931684970855713,
        "learning_rate": 0.00019994685770674805,
        "epoch": 0.338177211416648,
        "step": 4538
    },
    {
        "loss": 2.2056,
        "grad_norm": 3.222118616104126,
        "learning_rate": 0.00019994453925471195,
        "epoch": 0.3382517326179298,
        "step": 4539
    },
    {
        "loss": 2.421,
        "grad_norm": 2.739427089691162,
        "learning_rate": 0.00019994217131686192,
        "epoch": 0.33832625381921155,
        "step": 4540
    },
    {
        "loss": 1.6496,
        "grad_norm": 4.5644917488098145,
        "learning_rate": 0.00019993975389437038,
        "epoch": 0.3384007750204933,
        "step": 4541
    },
    {
        "loss": 2.461,
        "grad_norm": 3.4560399055480957,
        "learning_rate": 0.0001999372869884343,
        "epoch": 0.3384752962217751,
        "step": 4542
    },
    {
        "loss": 2.2931,
        "grad_norm": 2.5703866481781006,
        "learning_rate": 0.0001999347706002751,
        "epoch": 0.33854981742305684,
        "step": 4543
    },
    {
        "loss": 2.3198,
        "grad_norm": 2.757547378540039,
        "learning_rate": 0.00019993220473113875,
        "epoch": 0.3386243386243386,
        "step": 4544
    },
    {
        "loss": 2.5509,
        "grad_norm": 3.8170218467712402,
        "learning_rate": 0.00019992958938229566,
        "epoch": 0.33869885982562037,
        "step": 4545
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.351296901702881,
        "learning_rate": 0.0001999269245550408,
        "epoch": 0.33877338102690213,
        "step": 4546
    },
    {
        "loss": 1.7924,
        "grad_norm": 2.932640314102173,
        "learning_rate": 0.00019992421025069365,
        "epoch": 0.3388479022281839,
        "step": 4547
    },
    {
        "loss": 3.1599,
        "grad_norm": 2.1243319511413574,
        "learning_rate": 0.00019992144647059808,
        "epoch": 0.33892242342946566,
        "step": 4548
    },
    {
        "loss": 1.6519,
        "grad_norm": 4.629426002502441,
        "learning_rate": 0.00019991863321612257,
        "epoch": 0.3389969446307474,
        "step": 4549
    },
    {
        "loss": 2.2283,
        "grad_norm": 2.5296714305877686,
        "learning_rate": 0.00019991577048866004,
        "epoch": 0.33907146583202924,
        "step": 4550
    },
    {
        "loss": 2.856,
        "grad_norm": 2.5715267658233643,
        "learning_rate": 0.0001999128582896279,
        "epoch": 0.339145987033311,
        "step": 4551
    },
    {
        "loss": 2.7556,
        "grad_norm": 2.4686081409454346,
        "learning_rate": 0.00019990989662046815,
        "epoch": 0.33922050823459277,
        "step": 4552
    },
    {
        "loss": 2.2932,
        "grad_norm": 2.752065658569336,
        "learning_rate": 0.00019990688548264713,
        "epoch": 0.33929502943587453,
        "step": 4553
    },
    {
        "loss": 2.2654,
        "grad_norm": 2.7849442958831787,
        "learning_rate": 0.00019990382487765582,
        "epoch": 0.3393695506371563,
        "step": 4554
    },
    {
        "loss": 2.3765,
        "grad_norm": 3.2959625720977783,
        "learning_rate": 0.00019990071480700957,
        "epoch": 0.33944407183843806,
        "step": 4555
    },
    {
        "loss": 2.8406,
        "grad_norm": 2.6911439895629883,
        "learning_rate": 0.00019989755527224828,
        "epoch": 0.3395185930397198,
        "step": 4556
    },
    {
        "loss": 2.3202,
        "grad_norm": 3.5523481369018555,
        "learning_rate": 0.00019989434627493634,
        "epoch": 0.3395931142410016,
        "step": 4557
    },
    {
        "loss": 3.0507,
        "grad_norm": 3.4016354084014893,
        "learning_rate": 0.0001998910878166627,
        "epoch": 0.33966763544228334,
        "step": 4558
    },
    {
        "loss": 2.9056,
        "grad_norm": 2.1635618209838867,
        "learning_rate": 0.0001998877798990406,
        "epoch": 0.3397421566435651,
        "step": 4559
    },
    {
        "loss": 2.5517,
        "grad_norm": 2.312243938446045,
        "learning_rate": 0.00019988442252370805,
        "epoch": 0.33981667784484687,
        "step": 4560
    },
    {
        "loss": 1.9836,
        "grad_norm": 5.74997091293335,
        "learning_rate": 0.0001998810156923273,
        "epoch": 0.33989119904612863,
        "step": 4561
    },
    {
        "loss": 1.6571,
        "grad_norm": 2.9441068172454834,
        "learning_rate": 0.00019987755940658518,
        "epoch": 0.3399657202474104,
        "step": 4562
    },
    {
        "loss": 2.4863,
        "grad_norm": 2.0975277423858643,
        "learning_rate": 0.00019987405366819302,
        "epoch": 0.34004024144869216,
        "step": 4563
    },
    {
        "loss": 2.5804,
        "grad_norm": 3.117929220199585,
        "learning_rate": 0.00019987049847888666,
        "epoch": 0.3401147626499739,
        "step": 4564
    },
    {
        "loss": 2.5639,
        "grad_norm": 2.7513253688812256,
        "learning_rate": 0.00019986689384042638,
        "epoch": 0.3401892838512557,
        "step": 4565
    },
    {
        "loss": 2.8208,
        "grad_norm": 3.368795156478882,
        "learning_rate": 0.00019986323975459692,
        "epoch": 0.34026380505253745,
        "step": 4566
    },
    {
        "loss": 2.2078,
        "grad_norm": 3.6542820930480957,
        "learning_rate": 0.00019985953622320759,
        "epoch": 0.3403383262538192,
        "step": 4567
    },
    {
        "loss": 2.6885,
        "grad_norm": 3.8770828247070312,
        "learning_rate": 0.00019985578324809212,
        "epoch": 0.340412847455101,
        "step": 4568
    },
    {
        "loss": 2.6267,
        "grad_norm": 3.5817720890045166,
        "learning_rate": 0.00019985198083110866,
        "epoch": 0.34048736865638274,
        "step": 4569
    },
    {
        "loss": 2.1492,
        "grad_norm": 3.281071901321411,
        "learning_rate": 0.00019984812897414,
        "epoch": 0.3405618898576645,
        "step": 4570
    },
    {
        "loss": 1.967,
        "grad_norm": 2.3434090614318848,
        "learning_rate": 0.00019984422767909326,
        "epoch": 0.34063641105894626,
        "step": 4571
    },
    {
        "loss": 1.9444,
        "grad_norm": 2.7936909198760986,
        "learning_rate": 0.00019984027694790012,
        "epoch": 0.340710932260228,
        "step": 4572
    },
    {
        "loss": 2.3299,
        "grad_norm": 3.9533207416534424,
        "learning_rate": 0.00019983627678251672,
        "epoch": 0.3407854534615098,
        "step": 4573
    },
    {
        "loss": 2.2501,
        "grad_norm": 4.7226972579956055,
        "learning_rate": 0.0001998322271849237,
        "epoch": 0.34085997466279155,
        "step": 4574
    },
    {
        "loss": 2.2984,
        "grad_norm": 7.111193656921387,
        "learning_rate": 0.0001998281281571261,
        "epoch": 0.3409344958640733,
        "step": 4575
    },
    {
        "loss": 2.3353,
        "grad_norm": 2.7639148235321045,
        "learning_rate": 0.00019982397970115352,
        "epoch": 0.3410090170653551,
        "step": 4576
    },
    {
        "loss": 2.2189,
        "grad_norm": 3.3630118370056152,
        "learning_rate": 0.00019981978181905996,
        "epoch": 0.34108353826663684,
        "step": 4577
    },
    {
        "loss": 2.1499,
        "grad_norm": 2.858863353729248,
        "learning_rate": 0.00019981553451292396,
        "epoch": 0.3411580594679186,
        "step": 4578
    },
    {
        "loss": 2.2592,
        "grad_norm": 3.509836435317993,
        "learning_rate": 0.0001998112377848485,
        "epoch": 0.34123258066920037,
        "step": 4579
    },
    {
        "loss": 2.115,
        "grad_norm": 2.7733161449432373,
        "learning_rate": 0.000199806891636961,
        "epoch": 0.34130710187048213,
        "step": 4580
    },
    {
        "loss": 2.6142,
        "grad_norm": 1.869311809539795,
        "learning_rate": 0.00019980249607141342,
        "epoch": 0.3413816230717639,
        "step": 4581
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.9920763969421387,
        "learning_rate": 0.0001997980510903821,
        "epoch": 0.34145614427304566,
        "step": 4582
    },
    {
        "loss": 2.207,
        "grad_norm": 4.812090873718262,
        "learning_rate": 0.00019979355669606794,
        "epoch": 0.3415306654743274,
        "step": 4583
    },
    {
        "loss": 2.275,
        "grad_norm": 2.909485340118408,
        "learning_rate": 0.00019978901289069624,
        "epoch": 0.3416051866756092,
        "step": 4584
    },
    {
        "loss": 1.0932,
        "grad_norm": 3.637509822845459,
        "learning_rate": 0.0001997844196765168,
        "epoch": 0.34167970787689095,
        "step": 4585
    },
    {
        "loss": 2.3297,
        "grad_norm": 2.9241180419921875,
        "learning_rate": 0.00019977977705580387,
        "epoch": 0.34175422907817277,
        "step": 4586
    },
    {
        "loss": 2.9871,
        "grad_norm": 3.9493250846862793,
        "learning_rate": 0.00019977508503085615,
        "epoch": 0.34182875027945453,
        "step": 4587
    },
    {
        "loss": 2.5828,
        "grad_norm": 2.5907840728759766,
        "learning_rate": 0.0001997703436039968,
        "epoch": 0.3419032714807363,
        "step": 4588
    },
    {
        "loss": 2.3798,
        "grad_norm": 2.2944388389587402,
        "learning_rate": 0.00019976555277757352,
        "epoch": 0.34197779268201806,
        "step": 4589
    },
    {
        "loss": 2.7666,
        "grad_norm": 1.7490869760513306,
        "learning_rate": 0.00019976071255395833,
        "epoch": 0.3420523138832998,
        "step": 4590
    },
    {
        "loss": 2.5579,
        "grad_norm": 2.901898145675659,
        "learning_rate": 0.00019975582293554783,
        "epoch": 0.3421268350845816,
        "step": 4591
    },
    {
        "loss": 2.8551,
        "grad_norm": 3.1439671516418457,
        "learning_rate": 0.00019975088392476303,
        "epoch": 0.34220135628586335,
        "step": 4592
    },
    {
        "loss": 2.8934,
        "grad_norm": 3.196225881576538,
        "learning_rate": 0.00019974589552404937,
        "epoch": 0.3422758774871451,
        "step": 4593
    },
    {
        "loss": 2.0533,
        "grad_norm": 1.8412837982177734,
        "learning_rate": 0.00019974085773587677,
        "epoch": 0.34235039868842687,
        "step": 4594
    },
    {
        "loss": 2.6047,
        "grad_norm": 2.0706777572631836,
        "learning_rate": 0.00019973577056273963,
        "epoch": 0.34242491988970863,
        "step": 4595
    },
    {
        "loss": 2.3673,
        "grad_norm": 3.838820457458496,
        "learning_rate": 0.00019973063400715674,
        "epoch": 0.3424994410909904,
        "step": 4596
    },
    {
        "loss": 1.8901,
        "grad_norm": 3.557651996612549,
        "learning_rate": 0.00019972544807167144,
        "epoch": 0.34257396229227216,
        "step": 4597
    },
    {
        "loss": 2.734,
        "grad_norm": 2.787304639816284,
        "learning_rate": 0.0001997202127588514,
        "epoch": 0.3426484834935539,
        "step": 4598
    },
    {
        "loss": 2.6093,
        "grad_norm": 2.527090311050415,
        "learning_rate": 0.0001997149280712888,
        "epoch": 0.3427230046948357,
        "step": 4599
    },
    {
        "loss": 2.5941,
        "grad_norm": 2.2568700313568115,
        "learning_rate": 0.00019970959401160028,
        "epoch": 0.34279752589611745,
        "step": 4600
    },
    {
        "loss": 2.5839,
        "grad_norm": 2.1352157592773438,
        "learning_rate": 0.0001997042105824269,
        "epoch": 0.3428720470973992,
        "step": 4601
    },
    {
        "loss": 2.0766,
        "grad_norm": 3.8197810649871826,
        "learning_rate": 0.0001996987777864342,
        "epoch": 0.342946568298681,
        "step": 4602
    },
    {
        "loss": 2.2903,
        "grad_norm": 2.924409866333008,
        "learning_rate": 0.00019969329562631206,
        "epoch": 0.34302108949996274,
        "step": 4603
    },
    {
        "loss": 1.8725,
        "grad_norm": 2.6984565258026123,
        "learning_rate": 0.0001996877641047749,
        "epoch": 0.3430956107012445,
        "step": 4604
    },
    {
        "loss": 2.131,
        "grad_norm": 2.9800140857696533,
        "learning_rate": 0.00019968218322456168,
        "epoch": 0.34317013190252627,
        "step": 4605
    },
    {
        "loss": 2.4265,
        "grad_norm": 2.688415288925171,
        "learning_rate": 0.0001996765529884355,
        "epoch": 0.34324465310380803,
        "step": 4606
    },
    {
        "loss": 1.7823,
        "grad_norm": 3.597526788711548,
        "learning_rate": 0.00019967087339918418,
        "epoch": 0.3433191743050898,
        "step": 4607
    },
    {
        "loss": 2.0271,
        "grad_norm": 2.7854788303375244,
        "learning_rate": 0.00019966514445961982,
        "epoch": 0.34339369550637155,
        "step": 4608
    },
    {
        "loss": 2.376,
        "grad_norm": 2.6290509700775146,
        "learning_rate": 0.00019965936617257905,
        "epoch": 0.3434682167076533,
        "step": 4609
    },
    {
        "loss": 2.4103,
        "grad_norm": 2.4024157524108887,
        "learning_rate": 0.00019965353854092287,
        "epoch": 0.3435427379089351,
        "step": 4610
    },
    {
        "loss": 2.4442,
        "grad_norm": 3.7996788024902344,
        "learning_rate": 0.00019964766156753674,
        "epoch": 0.34361725911021684,
        "step": 4611
    },
    {
        "loss": 1.7559,
        "grad_norm": 3.3465700149536133,
        "learning_rate": 0.0001996417352553305,
        "epoch": 0.3436917803114986,
        "step": 4612
    },
    {
        "loss": 1.9575,
        "grad_norm": 4.031247615814209,
        "learning_rate": 0.0001996357596072385,
        "epoch": 0.34376630151278037,
        "step": 4613
    },
    {
        "loss": 1.9327,
        "grad_norm": 3.064619779586792,
        "learning_rate": 0.0001996297346262195,
        "epoch": 0.34384082271406213,
        "step": 4614
    },
    {
        "loss": 2.3223,
        "grad_norm": 4.036864280700684,
        "learning_rate": 0.00019962366031525664,
        "epoch": 0.3439153439153439,
        "step": 4615
    },
    {
        "loss": 2.4527,
        "grad_norm": 2.9788010120391846,
        "learning_rate": 0.00019961753667735747,
        "epoch": 0.34398986511662566,
        "step": 4616
    },
    {
        "loss": 2.1843,
        "grad_norm": 3.2596435546875,
        "learning_rate": 0.00019961136371555408,
        "epoch": 0.3440643863179074,
        "step": 4617
    },
    {
        "loss": 2.2391,
        "grad_norm": 4.598379135131836,
        "learning_rate": 0.00019960514143290282,
        "epoch": 0.3441389075191892,
        "step": 4618
    },
    {
        "loss": 2.1717,
        "grad_norm": 4.801976203918457,
        "learning_rate": 0.00019959886983248463,
        "epoch": 0.34421342872047095,
        "step": 4619
    },
    {
        "loss": 2.1354,
        "grad_norm": 2.727886438369751,
        "learning_rate": 0.0001995925489174047,
        "epoch": 0.3442879499217527,
        "step": 4620
    },
    {
        "loss": 2.2906,
        "grad_norm": 3.3825185298919678,
        "learning_rate": 0.00019958617869079282,
        "epoch": 0.34436247112303453,
        "step": 4621
    },
    {
        "loss": 1.8558,
        "grad_norm": 4.006228446960449,
        "learning_rate": 0.00019957975915580302,
        "epoch": 0.3444369923243163,
        "step": 4622
    },
    {
        "loss": 2.0089,
        "grad_norm": 3.899768114089966,
        "learning_rate": 0.00019957329031561384,
        "epoch": 0.34451151352559806,
        "step": 4623
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.4398398399353027,
        "learning_rate": 0.00019956677217342826,
        "epoch": 0.3445860347268798,
        "step": 4624
    },
    {
        "loss": 2.8723,
        "grad_norm": 3.0830466747283936,
        "learning_rate": 0.00019956020473247356,
        "epoch": 0.3446605559281616,
        "step": 4625
    },
    {
        "loss": 2.6759,
        "grad_norm": 2.730370283126831,
        "learning_rate": 0.00019955358799600154,
        "epoch": 0.34473507712944335,
        "step": 4626
    },
    {
        "loss": 2.087,
        "grad_norm": 2.8446450233459473,
        "learning_rate": 0.0001995469219672883,
        "epoch": 0.3448095983307251,
        "step": 4627
    },
    {
        "loss": 2.8504,
        "grad_norm": 1.8920599222183228,
        "learning_rate": 0.00019954020664963453,
        "epoch": 0.34488411953200687,
        "step": 4628
    },
    {
        "loss": 2.6317,
        "grad_norm": 2.2203898429870605,
        "learning_rate": 0.0001995334420463651,
        "epoch": 0.34495864073328864,
        "step": 4629
    },
    {
        "loss": 1.909,
        "grad_norm": 3.71352219581604,
        "learning_rate": 0.0001995266281608294,
        "epoch": 0.3450331619345704,
        "step": 4630
    },
    {
        "loss": 3.6713,
        "grad_norm": 3.194385051727295,
        "learning_rate": 0.00019951976499640124,
        "epoch": 0.34510768313585216,
        "step": 4631
    },
    {
        "loss": 2.2239,
        "grad_norm": 3.377532482147217,
        "learning_rate": 0.00019951285255647875,
        "epoch": 0.3451822043371339,
        "step": 4632
    },
    {
        "loss": 2.2187,
        "grad_norm": 4.165255546569824,
        "learning_rate": 0.0001995058908444846,
        "epoch": 0.3452567255384157,
        "step": 4633
    },
    {
        "loss": 2.7725,
        "grad_norm": 3.376926898956299,
        "learning_rate": 0.00019949887986386565,
        "epoch": 0.34533124673969745,
        "step": 4634
    },
    {
        "loss": 1.8616,
        "grad_norm": 2.2353463172912598,
        "learning_rate": 0.0001994918196180933,
        "epoch": 0.3454057679409792,
        "step": 4635
    },
    {
        "loss": 2.8088,
        "grad_norm": 3.391718864440918,
        "learning_rate": 0.00019948471011066338,
        "epoch": 0.345480289142261,
        "step": 4636
    },
    {
        "loss": 2.6345,
        "grad_norm": 2.3407950401306152,
        "learning_rate": 0.00019947755134509595,
        "epoch": 0.34555481034354274,
        "step": 4637
    },
    {
        "loss": 2.5978,
        "grad_norm": 2.105854034423828,
        "learning_rate": 0.00019947034332493558,
        "epoch": 0.3456293315448245,
        "step": 4638
    },
    {
        "loss": 2.2799,
        "grad_norm": 2.666112184524536,
        "learning_rate": 0.00019946308605375119,
        "epoch": 0.34570385274610627,
        "step": 4639
    },
    {
        "loss": 1.969,
        "grad_norm": 3.0422823429107666,
        "learning_rate": 0.00019945577953513612,
        "epoch": 0.34577837394738803,
        "step": 4640
    },
    {
        "loss": 2.301,
        "grad_norm": 2.6773581504821777,
        "learning_rate": 0.00019944842377270802,
        "epoch": 0.3458528951486698,
        "step": 4641
    },
    {
        "loss": 2.5857,
        "grad_norm": 2.4182209968566895,
        "learning_rate": 0.00019944101877010905,
        "epoch": 0.34592741634995156,
        "step": 4642
    },
    {
        "loss": 1.9482,
        "grad_norm": 4.098315715789795,
        "learning_rate": 0.00019943356453100557,
        "epoch": 0.3460019375512333,
        "step": 4643
    },
    {
        "loss": 2.7072,
        "grad_norm": 2.0690855979919434,
        "learning_rate": 0.00019942606105908847,
        "epoch": 0.3460764587525151,
        "step": 4644
    },
    {
        "loss": 2.8137,
        "grad_norm": 3.3663737773895264,
        "learning_rate": 0.00019941850835807298,
        "epoch": 0.34615097995379684,
        "step": 4645
    },
    {
        "loss": 2.6889,
        "grad_norm": 2.645134925842285,
        "learning_rate": 0.00019941090643169865,
        "epoch": 0.3462255011550786,
        "step": 4646
    },
    {
        "loss": 2.4548,
        "grad_norm": 2.9061636924743652,
        "learning_rate": 0.0001994032552837295,
        "epoch": 0.34630002235636037,
        "step": 4647
    },
    {
        "loss": 2.5681,
        "grad_norm": 2.9167873859405518,
        "learning_rate": 0.0001993955549179538,
        "epoch": 0.34637454355764213,
        "step": 4648
    },
    {
        "loss": 1.4983,
        "grad_norm": 3.1729912757873535,
        "learning_rate": 0.00019938780533818428,
        "epoch": 0.3464490647589239,
        "step": 4649
    },
    {
        "loss": 1.6983,
        "grad_norm": 3.2777175903320312,
        "learning_rate": 0.00019938000654825798,
        "epoch": 0.34652358596020566,
        "step": 4650
    },
    {
        "loss": 1.5853,
        "grad_norm": 3.483355760574341,
        "learning_rate": 0.00019937215855203637,
        "epoch": 0.3465981071614874,
        "step": 4651
    },
    {
        "loss": 2.9101,
        "grad_norm": 2.124152183532715,
        "learning_rate": 0.00019936426135340528,
        "epoch": 0.3466726283627692,
        "step": 4652
    },
    {
        "loss": 2.5611,
        "grad_norm": 1.9371141195297241,
        "learning_rate": 0.0001993563149562748,
        "epoch": 0.34674714956405095,
        "step": 4653
    },
    {
        "loss": 2.7173,
        "grad_norm": 2.7692575454711914,
        "learning_rate": 0.00019934831936457953,
        "epoch": 0.3468216707653327,
        "step": 4654
    },
    {
        "loss": 2.5981,
        "grad_norm": 1.6346209049224854,
        "learning_rate": 0.00019934027458227827,
        "epoch": 0.3468961919666145,
        "step": 4655
    },
    {
        "loss": 2.3697,
        "grad_norm": 2.914975643157959,
        "learning_rate": 0.00019933218061335433,
        "epoch": 0.3469707131678963,
        "step": 4656
    },
    {
        "loss": 2.3153,
        "grad_norm": 3.5375213623046875,
        "learning_rate": 0.00019932403746181522,
        "epoch": 0.34704523436917806,
        "step": 4657
    },
    {
        "loss": 2.8165,
        "grad_norm": 2.9314913749694824,
        "learning_rate": 0.00019931584513169294,
        "epoch": 0.3471197555704598,
        "step": 4658
    },
    {
        "loss": 1.9797,
        "grad_norm": 3.6415278911590576,
        "learning_rate": 0.00019930760362704377,
        "epoch": 0.3471942767717416,
        "step": 4659
    },
    {
        "loss": 2.8285,
        "grad_norm": 2.0978589057922363,
        "learning_rate": 0.00019929931295194835,
        "epoch": 0.34726879797302335,
        "step": 4660
    },
    {
        "loss": 1.6822,
        "grad_norm": 2.571446418762207,
        "learning_rate": 0.00019929097311051166,
        "epoch": 0.3473433191743051,
        "step": 4661
    },
    {
        "loss": 2.6816,
        "grad_norm": 2.0867955684661865,
        "learning_rate": 0.00019928258410686298,
        "epoch": 0.3474178403755869,
        "step": 4662
    },
    {
        "loss": 2.5817,
        "grad_norm": 1.6780363321304321,
        "learning_rate": 0.00019927414594515604,
        "epoch": 0.34749236157686864,
        "step": 4663
    },
    {
        "loss": 2.1246,
        "grad_norm": 3.452667236328125,
        "learning_rate": 0.00019926565862956886,
        "epoch": 0.3475668827781504,
        "step": 4664
    },
    {
        "loss": 2.4331,
        "grad_norm": 3.1177544593811035,
        "learning_rate": 0.00019925712216430379,
        "epoch": 0.34764140397943216,
        "step": 4665
    },
    {
        "loss": 2.0374,
        "grad_norm": 2.8767826557159424,
        "learning_rate": 0.00019924853655358744,
        "epoch": 0.3477159251807139,
        "step": 4666
    },
    {
        "loss": 2.3272,
        "grad_norm": 3.0460331439971924,
        "learning_rate": 0.0001992399018016709,
        "epoch": 0.3477904463819957,
        "step": 4667
    },
    {
        "loss": 1.8178,
        "grad_norm": 4.694604873657227,
        "learning_rate": 0.00019923121791282948,
        "epoch": 0.34786496758327745,
        "step": 4668
    },
    {
        "loss": 2.7528,
        "grad_norm": 1.7024078369140625,
        "learning_rate": 0.00019922248489136289,
        "epoch": 0.3479394887845592,
        "step": 4669
    },
    {
        "loss": 1.7982,
        "grad_norm": 1.6142706871032715,
        "learning_rate": 0.0001992137027415951,
        "epoch": 0.348014009985841,
        "step": 4670
    },
    {
        "loss": 2.581,
        "grad_norm": 1.920827031135559,
        "learning_rate": 0.00019920487146787443,
        "epoch": 0.34808853118712274,
        "step": 4671
    },
    {
        "loss": 2.5018,
        "grad_norm": 2.653923988342285,
        "learning_rate": 0.00019919599107457358,
        "epoch": 0.3481630523884045,
        "step": 4672
    },
    {
        "loss": 1.554,
        "grad_norm": 3.7749576568603516,
        "learning_rate": 0.00019918706156608952,
        "epoch": 0.34823757358968627,
        "step": 4673
    },
    {
        "loss": 2.5958,
        "grad_norm": 2.434459686279297,
        "learning_rate": 0.00019917808294684348,
        "epoch": 0.34831209479096803,
        "step": 4674
    },
    {
        "loss": 2.5706,
        "grad_norm": 3.353214740753174,
        "learning_rate": 0.00019916905522128118,
        "epoch": 0.3483866159922498,
        "step": 4675
    },
    {
        "loss": 2.0703,
        "grad_norm": 3.4379990100860596,
        "learning_rate": 0.00019915997839387243,
        "epoch": 0.34846113719353156,
        "step": 4676
    },
    {
        "loss": 2.1979,
        "grad_norm": 2.0670764446258545,
        "learning_rate": 0.00019915085246911148,
        "epoch": 0.3485356583948133,
        "step": 4677
    },
    {
        "loss": 3.0728,
        "grad_norm": 2.0816714763641357,
        "learning_rate": 0.00019914167745151696,
        "epoch": 0.3486101795960951,
        "step": 4678
    },
    {
        "loss": 2.9241,
        "grad_norm": 2.368333339691162,
        "learning_rate": 0.00019913245334563165,
        "epoch": 0.34868470079737685,
        "step": 4679
    },
    {
        "loss": 3.0953,
        "grad_norm": 1.7878879308700562,
        "learning_rate": 0.00019912318015602272,
        "epoch": 0.3487592219986586,
        "step": 4680
    },
    {
        "loss": 3.0062,
        "grad_norm": 2.4134955406188965,
        "learning_rate": 0.0001991138578872816,
        "epoch": 0.34883374319994037,
        "step": 4681
    },
    {
        "loss": 2.4554,
        "grad_norm": 1.7325412034988403,
        "learning_rate": 0.0001991044865440241,
        "epoch": 0.34890826440122213,
        "step": 4682
    },
    {
        "loss": 2.3444,
        "grad_norm": 2.813124179840088,
        "learning_rate": 0.00019909506613089025,
        "epoch": 0.3489827856025039,
        "step": 4683
    },
    {
        "loss": 2.2434,
        "grad_norm": 2.3579068183898926,
        "learning_rate": 0.00019908559665254445,
        "epoch": 0.34905730680378566,
        "step": 4684
    },
    {
        "loss": 3.0167,
        "grad_norm": 3.815816879272461,
        "learning_rate": 0.0001990760781136753,
        "epoch": 0.3491318280050674,
        "step": 4685
    },
    {
        "loss": 2.7073,
        "grad_norm": 2.6101362705230713,
        "learning_rate": 0.00019906651051899575,
        "epoch": 0.3492063492063492,
        "step": 4686
    },
    {
        "loss": 2.4562,
        "grad_norm": 1.6059989929199219,
        "learning_rate": 0.00019905689387324306,
        "epoch": 0.34928087040763095,
        "step": 4687
    },
    {
        "loss": 2.5958,
        "grad_norm": 3.055234670639038,
        "learning_rate": 0.0001990472281811787,
        "epoch": 0.3493553916089127,
        "step": 4688
    },
    {
        "loss": 1.8772,
        "grad_norm": 2.1769866943359375,
        "learning_rate": 0.0001990375134475885,
        "epoch": 0.3494299128101945,
        "step": 4689
    },
    {
        "loss": 2.9483,
        "grad_norm": 2.63897705078125,
        "learning_rate": 0.0001990277496772825,
        "epoch": 0.34950443401147624,
        "step": 4690
    },
    {
        "loss": 2.3211,
        "grad_norm": 2.0957677364349365,
        "learning_rate": 0.00019901793687509512,
        "epoch": 0.349578955212758,
        "step": 4691
    },
    {
        "loss": 2.5259,
        "grad_norm": 3.2128894329071045,
        "learning_rate": 0.00019900807504588496,
        "epoch": 0.3496534764140398,
        "step": 4692
    },
    {
        "loss": 2.5592,
        "grad_norm": 2.514648675918579,
        "learning_rate": 0.00019899816419453498,
        "epoch": 0.3497279976153216,
        "step": 4693
    },
    {
        "loss": 2.1889,
        "grad_norm": 4.558204650878906,
        "learning_rate": 0.00019898820432595233,
        "epoch": 0.34980251881660335,
        "step": 4694
    },
    {
        "loss": 2.7009,
        "grad_norm": 1.9660438299179077,
        "learning_rate": 0.00019897819544506846,
        "epoch": 0.3498770400178851,
        "step": 4695
    },
    {
        "loss": 1.9777,
        "grad_norm": 4.226736068725586,
        "learning_rate": 0.00019896813755683913,
        "epoch": 0.3499515612191669,
        "step": 4696
    },
    {
        "loss": 1.7757,
        "grad_norm": 4.255334377288818,
        "learning_rate": 0.0001989580306662443,
        "epoch": 0.35002608242044864,
        "step": 4697
    },
    {
        "loss": 2.3817,
        "grad_norm": 2.5508456230163574,
        "learning_rate": 0.00019894787477828824,
        "epoch": 0.3501006036217304,
        "step": 4698
    },
    {
        "loss": 3.0185,
        "grad_norm": 2.3268399238586426,
        "learning_rate": 0.00019893766989799945,
        "epoch": 0.35017512482301216,
        "step": 4699
    },
    {
        "loss": 2.7912,
        "grad_norm": 2.56612229347229,
        "learning_rate": 0.00019892741603043067,
        "epoch": 0.3502496460242939,
        "step": 4700
    },
    {
        "loss": 2.2245,
        "grad_norm": 4.659337520599365,
        "learning_rate": 0.000198917113180659,
        "epoch": 0.3503241672255757,
        "step": 4701
    },
    {
        "loss": 2.3534,
        "grad_norm": 4.100590705871582,
        "learning_rate": 0.00019890676135378565,
        "epoch": 0.35039868842685745,
        "step": 4702
    },
    {
        "loss": 2.2452,
        "grad_norm": 4.850930690765381,
        "learning_rate": 0.0001988963605549362,
        "epoch": 0.3504732096281392,
        "step": 4703
    },
    {
        "loss": 1.9444,
        "grad_norm": 2.7951834201812744,
        "learning_rate": 0.0001988859107892604,
        "epoch": 0.350547730829421,
        "step": 4704
    },
    {
        "loss": 2.1465,
        "grad_norm": 3.0651936531066895,
        "learning_rate": 0.0001988754120619323,
        "epoch": 0.35062225203070274,
        "step": 4705
    },
    {
        "loss": 1.5225,
        "grad_norm": 3.6721813678741455,
        "learning_rate": 0.0001988648643781501,
        "epoch": 0.3506967732319845,
        "step": 4706
    },
    {
        "loss": 2.4546,
        "grad_norm": 3.5097124576568604,
        "learning_rate": 0.00019885426774313633,
        "epoch": 0.35077129443326627,
        "step": 4707
    },
    {
        "loss": 2.6822,
        "grad_norm": 3.4751651287078857,
        "learning_rate": 0.00019884362216213773,
        "epoch": 0.35084581563454803,
        "step": 4708
    },
    {
        "loss": 3.0889,
        "grad_norm": 2.5532517433166504,
        "learning_rate": 0.00019883292764042535,
        "epoch": 0.3509203368358298,
        "step": 4709
    },
    {
        "loss": 2.5544,
        "grad_norm": 2.1377813816070557,
        "learning_rate": 0.0001988221841832943,
        "epoch": 0.35099485803711156,
        "step": 4710
    },
    {
        "loss": 1.5525,
        "grad_norm": 3.848646879196167,
        "learning_rate": 0.00019881139179606406,
        "epoch": 0.3510693792383933,
        "step": 4711
    },
    {
        "loss": 2.0171,
        "grad_norm": 3.826195001602173,
        "learning_rate": 0.00019880055048407828,
        "epoch": 0.3511439004396751,
        "step": 4712
    },
    {
        "loss": 1.8838,
        "grad_norm": 3.0260281562805176,
        "learning_rate": 0.00019878966025270486,
        "epoch": 0.35121842164095685,
        "step": 4713
    },
    {
        "loss": 1.4012,
        "grad_norm": 3.631622791290283,
        "learning_rate": 0.00019877872110733594,
        "epoch": 0.3512929428422386,
        "step": 4714
    },
    {
        "loss": 2.6953,
        "grad_norm": 2.7652344703674316,
        "learning_rate": 0.00019876773305338777,
        "epoch": 0.35136746404352037,
        "step": 4715
    },
    {
        "loss": 2.5424,
        "grad_norm": 1.9845547676086426,
        "learning_rate": 0.00019875669609630095,
        "epoch": 0.35144198524480214,
        "step": 4716
    },
    {
        "loss": 1.9691,
        "grad_norm": 2.9545302391052246,
        "learning_rate": 0.00019874561024154022,
        "epoch": 0.3515165064460839,
        "step": 4717
    },
    {
        "loss": 2.8763,
        "grad_norm": 2.3497486114501953,
        "learning_rate": 0.0001987344754945946,
        "epoch": 0.35159102764736566,
        "step": 4718
    },
    {
        "loss": 1.7733,
        "grad_norm": 4.811161994934082,
        "learning_rate": 0.00019872329186097721,
        "epoch": 0.3516655488486474,
        "step": 4719
    },
    {
        "loss": 2.8697,
        "grad_norm": 4.193211555480957,
        "learning_rate": 0.00019871205934622545,
        "epoch": 0.3517400700499292,
        "step": 4720
    },
    {
        "loss": 2.5056,
        "grad_norm": 3.625485897064209,
        "learning_rate": 0.00019870077795590088,
        "epoch": 0.35181459125121095,
        "step": 4721
    },
    {
        "loss": 2.9472,
        "grad_norm": 1.341050386428833,
        "learning_rate": 0.00019868944769558934,
        "epoch": 0.3518891124524927,
        "step": 4722
    },
    {
        "loss": 1.816,
        "grad_norm": 3.1746323108673096,
        "learning_rate": 0.00019867806857090073,
        "epoch": 0.3519636336537745,
        "step": 4723
    },
    {
        "loss": 1.1831,
        "grad_norm": 3.6630160808563232,
        "learning_rate": 0.00019866664058746937,
        "epoch": 0.35203815485505624,
        "step": 4724
    },
    {
        "loss": 2.7908,
        "grad_norm": 2.059722661972046,
        "learning_rate": 0.0001986551637509535,
        "epoch": 0.352112676056338,
        "step": 4725
    },
    {
        "loss": 2.2899,
        "grad_norm": 2.3651084899902344,
        "learning_rate": 0.0001986436380670357,
        "epoch": 0.35218719725761977,
        "step": 4726
    },
    {
        "loss": 2.2072,
        "grad_norm": 2.7415926456451416,
        "learning_rate": 0.00019863206354142277,
        "epoch": 0.3522617184589016,
        "step": 4727
    },
    {
        "loss": 2.37,
        "grad_norm": 1.9958136081695557,
        "learning_rate": 0.00019862044017984553,
        "epoch": 0.35233623966018335,
        "step": 4728
    },
    {
        "loss": 2.1775,
        "grad_norm": 2.0580053329467773,
        "learning_rate": 0.00019860876798805923,
        "epoch": 0.3524107608614651,
        "step": 4729
    },
    {
        "loss": 2.5583,
        "grad_norm": 2.835010528564453,
        "learning_rate": 0.00019859704697184304,
        "epoch": 0.3524852820627469,
        "step": 4730
    },
    {
        "loss": 1.8626,
        "grad_norm": 3.076209306716919,
        "learning_rate": 0.00019858527713700048,
        "epoch": 0.35255980326402864,
        "step": 4731
    },
    {
        "loss": 2.2184,
        "grad_norm": 2.59051513671875,
        "learning_rate": 0.00019857345848935912,
        "epoch": 0.3526343244653104,
        "step": 4732
    },
    {
        "loss": 1.8851,
        "grad_norm": 2.9544692039489746,
        "learning_rate": 0.00019856159103477086,
        "epoch": 0.35270884566659216,
        "step": 4733
    },
    {
        "loss": 2.5315,
        "grad_norm": 3.092179298400879,
        "learning_rate": 0.00019854967477911156,
        "epoch": 0.3527833668678739,
        "step": 4734
    },
    {
        "loss": 1.8333,
        "grad_norm": 3.4926013946533203,
        "learning_rate": 0.0001985377097282814,
        "epoch": 0.3528578880691557,
        "step": 4735
    },
    {
        "loss": 2.0494,
        "grad_norm": 2.8389744758605957,
        "learning_rate": 0.0001985256958882047,
        "epoch": 0.35293240927043745,
        "step": 4736
    },
    {
        "loss": 2.5362,
        "grad_norm": 2.2957170009613037,
        "learning_rate": 0.0001985136332648298,
        "epoch": 0.3530069304717192,
        "step": 4737
    },
    {
        "loss": 2.3079,
        "grad_norm": 2.6142475605010986,
        "learning_rate": 0.0001985015218641294,
        "epoch": 0.353081451673001,
        "step": 4738
    },
    {
        "loss": 2.4209,
        "grad_norm": 3.4672420024871826,
        "learning_rate": 0.00019848936169210022,
        "epoch": 0.35315597287428274,
        "step": 4739
    },
    {
        "loss": 2.594,
        "grad_norm": 2.6519548892974854,
        "learning_rate": 0.00019847715275476317,
        "epoch": 0.3532304940755645,
        "step": 4740
    },
    {
        "loss": 2.1695,
        "grad_norm": 3.6451733112335205,
        "learning_rate": 0.00019846489505816325,
        "epoch": 0.35330501527684627,
        "step": 4741
    },
    {
        "loss": 2.2706,
        "grad_norm": 3.1352829933166504,
        "learning_rate": 0.00019845258860836968,
        "epoch": 0.35337953647812803,
        "step": 4742
    },
    {
        "loss": 2.1387,
        "grad_norm": 5.7983503341674805,
        "learning_rate": 0.0001984402334114758,
        "epoch": 0.3534540576794098,
        "step": 4743
    },
    {
        "loss": 2.5695,
        "grad_norm": 4.019225120544434,
        "learning_rate": 0.00019842782947359906,
        "epoch": 0.35352857888069156,
        "step": 4744
    },
    {
        "loss": 2.0139,
        "grad_norm": 3.545933246612549,
        "learning_rate": 0.00019841537680088104,
        "epoch": 0.3536031000819733,
        "step": 4745
    },
    {
        "loss": 2.2192,
        "grad_norm": 3.2542147636413574,
        "learning_rate": 0.0001984028753994875,
        "epoch": 0.3536776212832551,
        "step": 4746
    },
    {
        "loss": 1.5075,
        "grad_norm": 4.527189254760742,
        "learning_rate": 0.00019839032527560824,
        "epoch": 0.35375214248453685,
        "step": 4747
    },
    {
        "loss": 2.7346,
        "grad_norm": 3.095494270324707,
        "learning_rate": 0.0001983777264354573,
        "epoch": 0.3538266636858186,
        "step": 4748
    },
    {
        "loss": 2.4089,
        "grad_norm": 2.1165006160736084,
        "learning_rate": 0.0001983650788852727,
        "epoch": 0.3539011848871004,
        "step": 4749
    },
    {
        "loss": 1.5202,
        "grad_norm": 3.5358119010925293,
        "learning_rate": 0.00019835238263131677,
        "epoch": 0.35397570608838214,
        "step": 4750
    },
    {
        "loss": 2.6218,
        "grad_norm": 3.4744162559509277,
        "learning_rate": 0.00019833963767987574,
        "epoch": 0.3540502272896639,
        "step": 4751
    },
    {
        "loss": 2.4572,
        "grad_norm": 3.2835335731506348,
        "learning_rate": 0.00019832684403726009,
        "epoch": 0.35412474849094566,
        "step": 4752
    },
    {
        "loss": 2.3771,
        "grad_norm": 2.924769401550293,
        "learning_rate": 0.00019831400170980436,
        "epoch": 0.3541992696922274,
        "step": 4753
    },
    {
        "loss": 2.2909,
        "grad_norm": 4.278014183044434,
        "learning_rate": 0.0001983011107038672,
        "epoch": 0.3542737908935092,
        "step": 4754
    },
    {
        "loss": 2.3646,
        "grad_norm": 3.4379804134368896,
        "learning_rate": 0.0001982881710258314,
        "epoch": 0.35434831209479095,
        "step": 4755
    },
    {
        "loss": 2.1378,
        "grad_norm": 3.114926338195801,
        "learning_rate": 0.00019827518268210383,
        "epoch": 0.3544228332960727,
        "step": 4756
    },
    {
        "loss": 2.0823,
        "grad_norm": 3.7544729709625244,
        "learning_rate": 0.0001982621456791154,
        "epoch": 0.3544973544973545,
        "step": 4757
    },
    {
        "loss": 2.6611,
        "grad_norm": 1.2273733615875244,
        "learning_rate": 0.0001982490600233212,
        "epoch": 0.35457187569863624,
        "step": 4758
    },
    {
        "loss": 2.5461,
        "grad_norm": 2.44682240486145,
        "learning_rate": 0.0001982359257212003,
        "epoch": 0.354646396899918,
        "step": 4759
    },
    {
        "loss": 1.9894,
        "grad_norm": 2.8186068534851074,
        "learning_rate": 0.00019822274277925598,
        "epoch": 0.35472091810119977,
        "step": 4760
    },
    {
        "loss": 2.7806,
        "grad_norm": 2.9976296424865723,
        "learning_rate": 0.0001982095112040155,
        "epoch": 0.35479543930248153,
        "step": 4761
    },
    {
        "loss": 3.0959,
        "grad_norm": 3.2155632972717285,
        "learning_rate": 0.00019819623100203034,
        "epoch": 0.3548699605037633,
        "step": 4762
    },
    {
        "loss": 2.8262,
        "grad_norm": 2.288879156112671,
        "learning_rate": 0.00019818290217987587,
        "epoch": 0.3549444817050451,
        "step": 4763
    },
    {
        "loss": 2.714,
        "grad_norm": 3.084158182144165,
        "learning_rate": 0.00019816952474415163,
        "epoch": 0.3550190029063269,
        "step": 4764
    },
    {
        "loss": 2.3768,
        "grad_norm": 2.5084753036499023,
        "learning_rate": 0.0001981560987014813,
        "epoch": 0.35509352410760864,
        "step": 4765
    },
    {
        "loss": 2.783,
        "grad_norm": 3.611656904220581,
        "learning_rate": 0.00019814262405851247,
        "epoch": 0.3551680453088904,
        "step": 4766
    },
    {
        "loss": 2.7631,
        "grad_norm": 2.2838475704193115,
        "learning_rate": 0.0001981291008219169,
        "epoch": 0.35524256651017216,
        "step": 4767
    },
    {
        "loss": 2.3387,
        "grad_norm": 2.8814570903778076,
        "learning_rate": 0.00019811552899839043,
        "epoch": 0.3553170877114539,
        "step": 4768
    },
    {
        "loss": 2.1314,
        "grad_norm": 3.7704572677612305,
        "learning_rate": 0.0001981019085946529,
        "epoch": 0.3553916089127357,
        "step": 4769
    },
    {
        "loss": 2.271,
        "grad_norm": 3.386359453201294,
        "learning_rate": 0.00019808823961744816,
        "epoch": 0.35546613011401745,
        "step": 4770
    },
    {
        "loss": 2.5422,
        "grad_norm": 2.880316972732544,
        "learning_rate": 0.00019807452207354421,
        "epoch": 0.3555406513152992,
        "step": 4771
    },
    {
        "loss": 2.1004,
        "grad_norm": 2.273378849029541,
        "learning_rate": 0.0001980607559697331,
        "epoch": 0.355615172516581,
        "step": 4772
    },
    {
        "loss": 2.8638,
        "grad_norm": 3.5652711391448975,
        "learning_rate": 0.0001980469413128308,
        "epoch": 0.35568969371786274,
        "step": 4773
    },
    {
        "loss": 2.5342,
        "grad_norm": 2.0752201080322266,
        "learning_rate": 0.00019803307810967744,
        "epoch": 0.3557642149191445,
        "step": 4774
    },
    {
        "loss": 2.0326,
        "grad_norm": 4.65524435043335,
        "learning_rate": 0.00019801916636713712,
        "epoch": 0.35583873612042627,
        "step": 4775
    },
    {
        "loss": 1.8869,
        "grad_norm": 4.09748649597168,
        "learning_rate": 0.00019800520609209804,
        "epoch": 0.35591325732170803,
        "step": 4776
    },
    {
        "loss": 2.0519,
        "grad_norm": 5.552121639251709,
        "learning_rate": 0.0001979911972914724,
        "epoch": 0.3559877785229898,
        "step": 4777
    },
    {
        "loss": 2.6751,
        "grad_norm": 3.0600619316101074,
        "learning_rate": 0.00019797713997219633,
        "epoch": 0.35606229972427156,
        "step": 4778
    },
    {
        "loss": 2.1215,
        "grad_norm": 3.181997060775757,
        "learning_rate": 0.00019796303414123013,
        "epoch": 0.3561368209255533,
        "step": 4779
    },
    {
        "loss": 2.436,
        "grad_norm": 3.127230405807495,
        "learning_rate": 0.00019794887980555806,
        "epoch": 0.3562113421268351,
        "step": 4780
    },
    {
        "loss": 2.4547,
        "grad_norm": 3.429865837097168,
        "learning_rate": 0.0001979346769721884,
        "epoch": 0.35628586332811685,
        "step": 4781
    },
    {
        "loss": 2.8241,
        "grad_norm": 1.8660030364990234,
        "learning_rate": 0.00019792042564815342,
        "epoch": 0.3563603845293986,
        "step": 4782
    },
    {
        "loss": 2.4543,
        "grad_norm": 2.645336389541626,
        "learning_rate": 0.0001979061258405094,
        "epoch": 0.3564349057306804,
        "step": 4783
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.131261110305786,
        "learning_rate": 0.00019789177755633665,
        "epoch": 0.35650942693196214,
        "step": 4784
    },
    {
        "loss": 2.1636,
        "grad_norm": 3.1440446376800537,
        "learning_rate": 0.00019787738080273952,
        "epoch": 0.3565839481332439,
        "step": 4785
    },
    {
        "loss": 2.4425,
        "grad_norm": 3.588890552520752,
        "learning_rate": 0.0001978629355868463,
        "epoch": 0.35665846933452566,
        "step": 4786
    },
    {
        "loss": 2.2202,
        "grad_norm": 2.6654515266418457,
        "learning_rate": 0.00019784844191580926,
        "epoch": 0.3567329905358074,
        "step": 4787
    },
    {
        "loss": 1.973,
        "grad_norm": 2.106811285018921,
        "learning_rate": 0.00019783389979680468,
        "epoch": 0.3568075117370892,
        "step": 4788
    },
    {
        "loss": 2.7121,
        "grad_norm": 2.6770312786102295,
        "learning_rate": 0.00019781930923703288,
        "epoch": 0.35688203293837095,
        "step": 4789
    },
    {
        "loss": 2.5264,
        "grad_norm": 2.4435160160064697,
        "learning_rate": 0.0001978046702437181,
        "epoch": 0.3569565541396527,
        "step": 4790
    },
    {
        "loss": 2.7344,
        "grad_norm": 2.048976421356201,
        "learning_rate": 0.0001977899828241086,
        "epoch": 0.3570310753409345,
        "step": 4791
    },
    {
        "loss": 2.0906,
        "grad_norm": 2.317046880722046,
        "learning_rate": 0.0001977752469854766,
        "epoch": 0.35710559654221624,
        "step": 4792
    },
    {
        "loss": 1.6949,
        "grad_norm": 1.3772342205047607,
        "learning_rate": 0.00019776046273511827,
        "epoch": 0.357180117743498,
        "step": 4793
    },
    {
        "loss": 1.9175,
        "grad_norm": 2.8688571453094482,
        "learning_rate": 0.00019774563008035382,
        "epoch": 0.35725463894477977,
        "step": 4794
    },
    {
        "loss": 1.7379,
        "grad_norm": 2.8867547512054443,
        "learning_rate": 0.00019773074902852735,
        "epoch": 0.35732916014606153,
        "step": 4795
    },
    {
        "loss": 2.7226,
        "grad_norm": 2.8157811164855957,
        "learning_rate": 0.00019771581958700696,
        "epoch": 0.3574036813473433,
        "step": 4796
    },
    {
        "loss": 2.88,
        "grad_norm": 3.2742950916290283,
        "learning_rate": 0.00019770084176318472,
        "epoch": 0.35747820254862506,
        "step": 4797
    },
    {
        "loss": 2.6769,
        "grad_norm": 2.481487989425659,
        "learning_rate": 0.00019768581556447662,
        "epoch": 0.3575527237499069,
        "step": 4798
    },
    {
        "loss": 1.7772,
        "grad_norm": 3.8451666831970215,
        "learning_rate": 0.00019767074099832265,
        "epoch": 0.35762724495118864,
        "step": 4799
    },
    {
        "loss": 2.5786,
        "grad_norm": 1.968912959098816,
        "learning_rate": 0.0001976556180721867,
        "epoch": 0.3577017661524704,
        "step": 4800
    },
    {
        "loss": 2.5819,
        "grad_norm": 5.091798305511475,
        "learning_rate": 0.00019764044679355665,
        "epoch": 0.35777628735375216,
        "step": 4801
    },
    {
        "loss": 2.6401,
        "grad_norm": 2.5526232719421387,
        "learning_rate": 0.00019762522716994428,
        "epoch": 0.35785080855503393,
        "step": 4802
    },
    {
        "loss": 2.2646,
        "grad_norm": 3.7898478507995605,
        "learning_rate": 0.00019760995920888527,
        "epoch": 0.3579253297563157,
        "step": 4803
    },
    {
        "loss": 1.4019,
        "grad_norm": 4.188633441925049,
        "learning_rate": 0.0001975946429179394,
        "epoch": 0.35799985095759745,
        "step": 4804
    },
    {
        "loss": 3.1947,
        "grad_norm": 3.3712317943573,
        "learning_rate": 0.0001975792783046902,
        "epoch": 0.3580743721588792,
        "step": 4805
    },
    {
        "loss": 2.6571,
        "grad_norm": 3.247037649154663,
        "learning_rate": 0.0001975638653767452,
        "epoch": 0.358148893360161,
        "step": 4806
    },
    {
        "loss": 2.0982,
        "grad_norm": 2.611170768737793,
        "learning_rate": 0.0001975484041417358,
        "epoch": 0.35822341456144274,
        "step": 4807
    },
    {
        "loss": 2.1397,
        "grad_norm": 3.652215003967285,
        "learning_rate": 0.00019753289460731744,
        "epoch": 0.3582979357627245,
        "step": 4808
    },
    {
        "loss": 2.2544,
        "grad_norm": 3.509824514389038,
        "learning_rate": 0.00019751733678116939,
        "epoch": 0.35837245696400627,
        "step": 4809
    },
    {
        "loss": 1.8275,
        "grad_norm": 3.1286022663116455,
        "learning_rate": 0.0001975017306709948,
        "epoch": 0.35844697816528803,
        "step": 4810
    },
    {
        "loss": 1.9193,
        "grad_norm": 3.5935542583465576,
        "learning_rate": 0.0001974860762845208,
        "epoch": 0.3585214993665698,
        "step": 4811
    },
    {
        "loss": 2.4827,
        "grad_norm": 2.4151716232299805,
        "learning_rate": 0.00019747037362949837,
        "epoch": 0.35859602056785156,
        "step": 4812
    },
    {
        "loss": 2.0565,
        "grad_norm": 2.573922634124756,
        "learning_rate": 0.00019745462271370236,
        "epoch": 0.3586705417691333,
        "step": 4813
    },
    {
        "loss": 2.7098,
        "grad_norm": 2.5191702842712402,
        "learning_rate": 0.0001974388235449317,
        "epoch": 0.3587450629704151,
        "step": 4814
    },
    {
        "loss": 1.9735,
        "grad_norm": 2.4467110633850098,
        "learning_rate": 0.00019742297613100892,
        "epoch": 0.35881958417169685,
        "step": 4815
    },
    {
        "loss": 2.2443,
        "grad_norm": 3.2706100940704346,
        "learning_rate": 0.0001974070804797807,
        "epoch": 0.3588941053729786,
        "step": 4816
    },
    {
        "loss": 2.5741,
        "grad_norm": 2.9785513877868652,
        "learning_rate": 0.00019739113659911746,
        "epoch": 0.3589686265742604,
        "step": 4817
    },
    {
        "loss": 1.6177,
        "grad_norm": 2.884603977203369,
        "learning_rate": 0.00019737514449691355,
        "epoch": 0.35904314777554214,
        "step": 4818
    },
    {
        "loss": 2.8986,
        "grad_norm": 2.024467945098877,
        "learning_rate": 0.00019735910418108716,
        "epoch": 0.3591176689768239,
        "step": 4819
    },
    {
        "loss": 2.879,
        "grad_norm": 2.6709561347961426,
        "learning_rate": 0.00019734301565958037,
        "epoch": 0.35919219017810566,
        "step": 4820
    },
    {
        "loss": 2.1793,
        "grad_norm": 4.7900519371032715,
        "learning_rate": 0.0001973268789403592,
        "epoch": 0.3592667113793874,
        "step": 4821
    },
    {
        "loss": 2.8823,
        "grad_norm": 1.6609708070755005,
        "learning_rate": 0.0001973106940314134,
        "epoch": 0.3593412325806692,
        "step": 4822
    },
    {
        "loss": 1.7189,
        "grad_norm": 5.514105796813965,
        "learning_rate": 0.00019729446094075666,
        "epoch": 0.35941575378195095,
        "step": 4823
    },
    {
        "loss": 2.8291,
        "grad_norm": 1.7708860635757446,
        "learning_rate": 0.00019727817967642652,
        "epoch": 0.3594902749832327,
        "step": 4824
    },
    {
        "loss": 2.6448,
        "grad_norm": 2.79430890083313,
        "learning_rate": 0.00019726185024648436,
        "epoch": 0.3595647961845145,
        "step": 4825
    },
    {
        "loss": 1.9871,
        "grad_norm": 3.2751049995422363,
        "learning_rate": 0.00019724547265901544,
        "epoch": 0.35963931738579624,
        "step": 4826
    },
    {
        "loss": 2.6109,
        "grad_norm": 2.332566499710083,
        "learning_rate": 0.00019722904692212885,
        "epoch": 0.359713838587078,
        "step": 4827
    },
    {
        "loss": 1.9301,
        "grad_norm": 4.560573101043701,
        "learning_rate": 0.0001972125730439575,
        "epoch": 0.35978835978835977,
        "step": 4828
    },
    {
        "loss": 1.5385,
        "grad_norm": 1.5152959823608398,
        "learning_rate": 0.00019719605103265807,
        "epoch": 0.35986288098964153,
        "step": 4829
    },
    {
        "loss": 2.7135,
        "grad_norm": 2.295254707336426,
        "learning_rate": 0.00019717948089641122,
        "epoch": 0.3599374021909233,
        "step": 4830
    },
    {
        "loss": 2.3358,
        "grad_norm": 2.640411853790283,
        "learning_rate": 0.0001971628626434214,
        "epoch": 0.36001192339220506,
        "step": 4831
    },
    {
        "loss": 2.6885,
        "grad_norm": 3.1513681411743164,
        "learning_rate": 0.00019714619628191679,
        "epoch": 0.3600864445934868,
        "step": 4832
    },
    {
        "loss": 2.9608,
        "grad_norm": 1.5903987884521484,
        "learning_rate": 0.0001971294818201495,
        "epoch": 0.36016096579476864,
        "step": 4833
    },
    {
        "loss": 2.6924,
        "grad_norm": 4.423744201660156,
        "learning_rate": 0.00019711271926639535,
        "epoch": 0.3602354869960504,
        "step": 4834
    },
    {
        "loss": 1.6559,
        "grad_norm": 5.552110195159912,
        "learning_rate": 0.00019709590862895403,
        "epoch": 0.36031000819733217,
        "step": 4835
    },
    {
        "loss": 1.6874,
        "grad_norm": 1.6940079927444458,
        "learning_rate": 0.00019707904991614908,
        "epoch": 0.36038452939861393,
        "step": 4836
    },
    {
        "loss": 2.5165,
        "grad_norm": 2.694096565246582,
        "learning_rate": 0.00019706214313632781,
        "epoch": 0.3604590505998957,
        "step": 4837
    },
    {
        "loss": 2.5273,
        "grad_norm": 1.7028145790100098,
        "learning_rate": 0.00019704518829786132,
        "epoch": 0.36053357180117745,
        "step": 4838
    },
    {
        "loss": 1.9104,
        "grad_norm": 5.236850261688232,
        "learning_rate": 0.00019702818540914442,
        "epoch": 0.3606080930024592,
        "step": 4839
    },
    {
        "loss": 2.0069,
        "grad_norm": 3.8131721019744873,
        "learning_rate": 0.0001970111344785959,
        "epoch": 0.360682614203741,
        "step": 4840
    },
    {
        "loss": 3.0273,
        "grad_norm": 2.229720115661621,
        "learning_rate": 0.0001969940355146582,
        "epoch": 0.36075713540502274,
        "step": 4841
    },
    {
        "loss": 2.3623,
        "grad_norm": 1.8149724006652832,
        "learning_rate": 0.00019697688852579755,
        "epoch": 0.3608316566063045,
        "step": 4842
    },
    {
        "loss": 2.1512,
        "grad_norm": 4.284094333648682,
        "learning_rate": 0.00019695969352050401,
        "epoch": 0.36090617780758627,
        "step": 4843
    },
    {
        "loss": 2.1162,
        "grad_norm": 4.18717098236084,
        "learning_rate": 0.00019694245050729136,
        "epoch": 0.36098069900886803,
        "step": 4844
    },
    {
        "loss": 1.6583,
        "grad_norm": 3.3552894592285156,
        "learning_rate": 0.00019692515949469724,
        "epoch": 0.3610552202101498,
        "step": 4845
    },
    {
        "loss": 2.9921,
        "grad_norm": 2.8922293186187744,
        "learning_rate": 0.00019690782049128292,
        "epoch": 0.36112974141143156,
        "step": 4846
    },
    {
        "loss": 2.8701,
        "grad_norm": 2.7571256160736084,
        "learning_rate": 0.0001968904335056336,
        "epoch": 0.3612042626127133,
        "step": 4847
    },
    {
        "loss": 2.5146,
        "grad_norm": 2.2433931827545166,
        "learning_rate": 0.00019687299854635808,
        "epoch": 0.3612787838139951,
        "step": 4848
    },
    {
        "loss": 1.7372,
        "grad_norm": 2.600658655166626,
        "learning_rate": 0.00019685551562208898,
        "epoch": 0.36135330501527685,
        "step": 4849
    },
    {
        "loss": 2.5671,
        "grad_norm": 2.438851833343506,
        "learning_rate": 0.0001968379847414827,
        "epoch": 0.3614278262165586,
        "step": 4850
    },
    {
        "loss": 2.1987,
        "grad_norm": 3.9151883125305176,
        "learning_rate": 0.00019682040591321935,
        "epoch": 0.3615023474178404,
        "step": 4851
    },
    {
        "loss": 2.3498,
        "grad_norm": 2.7748849391937256,
        "learning_rate": 0.00019680277914600275,
        "epoch": 0.36157686861912214,
        "step": 4852
    },
    {
        "loss": 2.6415,
        "grad_norm": 2.1597328186035156,
        "learning_rate": 0.0001967851044485605,
        "epoch": 0.3616513898204039,
        "step": 4853
    },
    {
        "loss": 1.639,
        "grad_norm": 3.159271717071533,
        "learning_rate": 0.00019676738182964395,
        "epoch": 0.36172591102168566,
        "step": 4854
    },
    {
        "loss": 1.9689,
        "grad_norm": 3.683213233947754,
        "learning_rate": 0.00019674961129802814,
        "epoch": 0.3618004322229674,
        "step": 4855
    },
    {
        "loss": 2.1444,
        "grad_norm": 2.838773250579834,
        "learning_rate": 0.00019673179286251182,
        "epoch": 0.3618749534242492,
        "step": 4856
    },
    {
        "loss": 2.8386,
        "grad_norm": 2.2540297508239746,
        "learning_rate": 0.00019671392653191749,
        "epoch": 0.36194947462553095,
        "step": 4857
    },
    {
        "loss": 2.0788,
        "grad_norm": 2.5507423877716064,
        "learning_rate": 0.00019669601231509139,
        "epoch": 0.3620239958268127,
        "step": 4858
    },
    {
        "loss": 1.7519,
        "grad_norm": 5.754187107086182,
        "learning_rate": 0.00019667805022090335,
        "epoch": 0.3620985170280945,
        "step": 4859
    },
    {
        "loss": 2.3229,
        "grad_norm": 3.1113624572753906,
        "learning_rate": 0.00019666004025824707,
        "epoch": 0.36217303822937624,
        "step": 4860
    },
    {
        "loss": 1.7122,
        "grad_norm": 3.444115161895752,
        "learning_rate": 0.00019664198243603986,
        "epoch": 0.362247559430658,
        "step": 4861
    },
    {
        "loss": 2.3282,
        "grad_norm": 2.883009672164917,
        "learning_rate": 0.00019662387676322272,
        "epoch": 0.36232208063193977,
        "step": 4862
    },
    {
        "loss": 2.1897,
        "grad_norm": 3.3326048851013184,
        "learning_rate": 0.00019660572324876037,
        "epoch": 0.36239660183322153,
        "step": 4863
    },
    {
        "loss": 2.2224,
        "grad_norm": 4.044922351837158,
        "learning_rate": 0.00019658752190164117,
        "epoch": 0.3624711230345033,
        "step": 4864
    },
    {
        "loss": 2.1165,
        "grad_norm": 4.333373069763184,
        "learning_rate": 0.00019656927273087725,
        "epoch": 0.36254564423578506,
        "step": 4865
    },
    {
        "loss": 2.5559,
        "grad_norm": 3.6290204524993896,
        "learning_rate": 0.00019655097574550442,
        "epoch": 0.3626201654370668,
        "step": 4866
    },
    {
        "loss": 2.2869,
        "grad_norm": 2.589816093444824,
        "learning_rate": 0.00019653263095458203,
        "epoch": 0.3626946866383486,
        "step": 4867
    },
    {
        "loss": 2.6912,
        "grad_norm": 2.0423898696899414,
        "learning_rate": 0.0001965142383671932,
        "epoch": 0.36276920783963035,
        "step": 4868
    },
    {
        "loss": 1.7423,
        "grad_norm": 3.4400227069854736,
        "learning_rate": 0.0001964957979924447,
        "epoch": 0.36284372904091217,
        "step": 4869
    },
    {
        "loss": 2.4355,
        "grad_norm": 2.039839744567871,
        "learning_rate": 0.00019647730983946704,
        "epoch": 0.36291825024219393,
        "step": 4870
    },
    {
        "loss": 2.5771,
        "grad_norm": 2.92057728767395,
        "learning_rate": 0.0001964587739174142,
        "epoch": 0.3629927714434757,
        "step": 4871
    },
    {
        "loss": 1.8353,
        "grad_norm": 3.1524009704589844,
        "learning_rate": 0.000196440190235464,
        "epoch": 0.36306729264475746,
        "step": 4872
    },
    {
        "loss": 3.2334,
        "grad_norm": 2.7792301177978516,
        "learning_rate": 0.0001964215588028178,
        "epoch": 0.3631418138460392,
        "step": 4873
    },
    {
        "loss": 2.3417,
        "grad_norm": 3.366650342941284,
        "learning_rate": 0.00019640287962870062,
        "epoch": 0.363216335047321,
        "step": 4874
    },
    {
        "loss": 2.2509,
        "grad_norm": 3.505783796310425,
        "learning_rate": 0.00019638415272236115,
        "epoch": 0.36329085624860274,
        "step": 4875
    },
    {
        "loss": 2.4908,
        "grad_norm": 2.1802847385406494,
        "learning_rate": 0.00019636537809307168,
        "epoch": 0.3633653774498845,
        "step": 4876
    },
    {
        "loss": 2.1857,
        "grad_norm": 3.3020715713500977,
        "learning_rate": 0.00019634655575012818,
        "epoch": 0.36343989865116627,
        "step": 4877
    },
    {
        "loss": 2.9846,
        "grad_norm": 3.0962212085723877,
        "learning_rate": 0.00019632768570285015,
        "epoch": 0.36351441985244803,
        "step": 4878
    },
    {
        "loss": 2.3013,
        "grad_norm": 2.906639575958252,
        "learning_rate": 0.00019630876796058083,
        "epoch": 0.3635889410537298,
        "step": 4879
    },
    {
        "loss": 2.4315,
        "grad_norm": 1.3185919523239136,
        "learning_rate": 0.000196289802532687,
        "epoch": 0.36366346225501156,
        "step": 4880
    },
    {
        "loss": 2.3905,
        "grad_norm": 2.159224510192871,
        "learning_rate": 0.000196270789428559,
        "epoch": 0.3637379834562933,
        "step": 4881
    },
    {
        "loss": 2.2647,
        "grad_norm": 6.493100166320801,
        "learning_rate": 0.00019625172865761094,
        "epoch": 0.3638125046575751,
        "step": 4882
    },
    {
        "loss": 2.5268,
        "grad_norm": 1.24091374874115,
        "learning_rate": 0.0001962326202292803,
        "epoch": 0.36388702585885685,
        "step": 4883
    },
    {
        "loss": 2.3407,
        "grad_norm": 3.5593745708465576,
        "learning_rate": 0.00019621346415302844,
        "epoch": 0.3639615470601386,
        "step": 4884
    },
    {
        "loss": 2.4195,
        "grad_norm": 3.3314414024353027,
        "learning_rate": 0.0001961942604383401,
        "epoch": 0.3640360682614204,
        "step": 4885
    },
    {
        "loss": 2.398,
        "grad_norm": 2.473451852798462,
        "learning_rate": 0.0001961750090947236,
        "epoch": 0.36411058946270214,
        "step": 4886
    },
    {
        "loss": 2.0192,
        "grad_norm": 3.4010510444641113,
        "learning_rate": 0.000196155710131711,
        "epoch": 0.3641851106639839,
        "step": 4887
    },
    {
        "loss": 2.5174,
        "grad_norm": 3.0584564208984375,
        "learning_rate": 0.0001961363635588578,
        "epoch": 0.36425963186526567,
        "step": 4888
    },
    {
        "loss": 2.5225,
        "grad_norm": 3.0727744102478027,
        "learning_rate": 0.00019611696938574315,
        "epoch": 0.36433415306654743,
        "step": 4889
    },
    {
        "loss": 2.0182,
        "grad_norm": 4.057241439819336,
        "learning_rate": 0.00019609752762196975,
        "epoch": 0.3644086742678292,
        "step": 4890
    },
    {
        "loss": 2.32,
        "grad_norm": 2.4519734382629395,
        "learning_rate": 0.00019607803827716378,
        "epoch": 0.36448319546911095,
        "step": 4891
    },
    {
        "loss": 2.8182,
        "grad_norm": 1.915004849433899,
        "learning_rate": 0.00019605850136097515,
        "epoch": 0.3645577166703927,
        "step": 4892
    },
    {
        "loss": 2.8392,
        "grad_norm": 2.997007369995117,
        "learning_rate": 0.00019603891688307713,
        "epoch": 0.3646322378716745,
        "step": 4893
    },
    {
        "loss": 2.4666,
        "grad_norm": 3.537440776824951,
        "learning_rate": 0.00019601928485316677,
        "epoch": 0.36470675907295624,
        "step": 4894
    },
    {
        "loss": 1.928,
        "grad_norm": 2.9692764282226562,
        "learning_rate": 0.00019599960528096438,
        "epoch": 0.364781280274238,
        "step": 4895
    },
    {
        "loss": 1.1294,
        "grad_norm": 3.7593982219696045,
        "learning_rate": 0.00019597987817621405,
        "epoch": 0.36485580147551977,
        "step": 4896
    },
    {
        "loss": 1.8128,
        "grad_norm": 3.50144362449646,
        "learning_rate": 0.0001959601035486833,
        "epoch": 0.36493032267680153,
        "step": 4897
    },
    {
        "loss": 2.4406,
        "grad_norm": 3.9661378860473633,
        "learning_rate": 0.00019594028140816318,
        "epoch": 0.3650048438780833,
        "step": 4898
    },
    {
        "loss": 1.7358,
        "grad_norm": 3.601991891860962,
        "learning_rate": 0.00019592041176446831,
        "epoch": 0.36507936507936506,
        "step": 4899
    },
    {
        "loss": 2.6168,
        "grad_norm": 3.6799352169036865,
        "learning_rate": 0.0001959004946274368,
        "epoch": 0.3651538862806468,
        "step": 4900
    },
    {
        "loss": 2.6321,
        "grad_norm": 2.696629524230957,
        "learning_rate": 0.00019588053000693022,
        "epoch": 0.3652284074819286,
        "step": 4901
    },
    {
        "loss": 2.9296,
        "grad_norm": 2.1240108013153076,
        "learning_rate": 0.00019586051791283375,
        "epoch": 0.36530292868321035,
        "step": 4902
    },
    {
        "loss": 2.2128,
        "grad_norm": 2.3103654384613037,
        "learning_rate": 0.00019584045835505605,
        "epoch": 0.3653774498844921,
        "step": 4903
    },
    {
        "loss": 1.4222,
        "grad_norm": 1.6515858173370361,
        "learning_rate": 0.00019582035134352923,
        "epoch": 0.36545197108577393,
        "step": 4904
    },
    {
        "loss": 2.4704,
        "grad_norm": 3.040510416030884,
        "learning_rate": 0.00019580019688820896,
        "epoch": 0.3655264922870557,
        "step": 4905
    },
    {
        "loss": 1.0607,
        "grad_norm": 1.5352864265441895,
        "learning_rate": 0.00019577999499907433,
        "epoch": 0.36560101348833746,
        "step": 4906
    },
    {
        "loss": 2.6434,
        "grad_norm": 3.2798211574554443,
        "learning_rate": 0.000195759745686128,
        "epoch": 0.3656755346896192,
        "step": 4907
    },
    {
        "loss": 1.9504,
        "grad_norm": 2.3309991359710693,
        "learning_rate": 0.00019573944895939603,
        "epoch": 0.365750055890901,
        "step": 4908
    },
    {
        "loss": 2.5477,
        "grad_norm": 3.144587278366089,
        "learning_rate": 0.00019571910482892805,
        "epoch": 0.36582457709218275,
        "step": 4909
    },
    {
        "loss": 2.8543,
        "grad_norm": 2.6861839294433594,
        "learning_rate": 0.00019569871330479702,
        "epoch": 0.3658990982934645,
        "step": 4910
    },
    {
        "loss": 2.6016,
        "grad_norm": 3.681457757949829,
        "learning_rate": 0.00019567827439709952,
        "epoch": 0.36597361949474627,
        "step": 4911
    },
    {
        "loss": 2.8698,
        "grad_norm": 2.747041940689087,
        "learning_rate": 0.0001956577881159555,
        "epoch": 0.36604814069602803,
        "step": 4912
    },
    {
        "loss": 1.2594,
        "grad_norm": 3.7963085174560547,
        "learning_rate": 0.00019563725447150842,
        "epoch": 0.3661226618973098,
        "step": 4913
    },
    {
        "loss": 3.1169,
        "grad_norm": 2.671679735183716,
        "learning_rate": 0.0001956166734739251,
        "epoch": 0.36619718309859156,
        "step": 4914
    },
    {
        "loss": 2.3994,
        "grad_norm": 2.4041523933410645,
        "learning_rate": 0.00019559604513339588,
        "epoch": 0.3662717042998733,
        "step": 4915
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.1706972122192383,
        "learning_rate": 0.0001955753694601346,
        "epoch": 0.3663462255011551,
        "step": 4916
    },
    {
        "loss": 2.2765,
        "grad_norm": 2.7039055824279785,
        "learning_rate": 0.00019555464646437836,
        "epoch": 0.36642074670243685,
        "step": 4917
    },
    {
        "loss": 2.7848,
        "grad_norm": 1.9343242645263672,
        "learning_rate": 0.00019553387615638785,
        "epoch": 0.3664952679037186,
        "step": 4918
    },
    {
        "loss": 2.7722,
        "grad_norm": 2.349781036376953,
        "learning_rate": 0.00019551305854644712,
        "epoch": 0.3665697891050004,
        "step": 4919
    },
    {
        "loss": 2.0615,
        "grad_norm": 4.147756099700928,
        "learning_rate": 0.00019549219364486364,
        "epoch": 0.36664431030628214,
        "step": 4920
    },
    {
        "loss": 1.3636,
        "grad_norm": 4.0228166580200195,
        "learning_rate": 0.0001954712814619683,
        "epoch": 0.3667188315075639,
        "step": 4921
    },
    {
        "loss": 1.9352,
        "grad_norm": 3.230529308319092,
        "learning_rate": 0.0001954503220081155,
        "epoch": 0.36679335270884567,
        "step": 4922
    },
    {
        "loss": 2.5429,
        "grad_norm": 2.455627918243408,
        "learning_rate": 0.0001954293152936828,
        "epoch": 0.36686787391012743,
        "step": 4923
    },
    {
        "loss": 1.3736,
        "grad_norm": 4.130669116973877,
        "learning_rate": 0.0001954082613290714,
        "epoch": 0.3669423951114092,
        "step": 4924
    },
    {
        "loss": 2.2618,
        "grad_norm": 2.9240965843200684,
        "learning_rate": 0.0001953871601247058,
        "epoch": 0.36701691631269096,
        "step": 4925
    },
    {
        "loss": 2.2545,
        "grad_norm": 2.63834810256958,
        "learning_rate": 0.00019536601169103387,
        "epoch": 0.3670914375139727,
        "step": 4926
    },
    {
        "loss": 1.8915,
        "grad_norm": 1.7486976385116577,
        "learning_rate": 0.00019534481603852695,
        "epoch": 0.3671659587152545,
        "step": 4927
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.222086191177368,
        "learning_rate": 0.00019532357317767965,
        "epoch": 0.36724047991653624,
        "step": 4928
    },
    {
        "loss": 1.7065,
        "grad_norm": 6.351395130157471,
        "learning_rate": 0.00019530228311901002,
        "epoch": 0.367315001117818,
        "step": 4929
    },
    {
        "loss": 1.7378,
        "grad_norm": 3.9002301692962646,
        "learning_rate": 0.00019528094587305942,
        "epoch": 0.36738952231909977,
        "step": 4930
    },
    {
        "loss": 2.6213,
        "grad_norm": 2.6404926776885986,
        "learning_rate": 0.0001952595614503927,
        "epoch": 0.36746404352038153,
        "step": 4931
    },
    {
        "loss": 2.082,
        "grad_norm": 3.5686280727386475,
        "learning_rate": 0.000195238129861598,
        "epoch": 0.3675385647216633,
        "step": 4932
    },
    {
        "loss": 2.5182,
        "grad_norm": 3.8560187816619873,
        "learning_rate": 0.0001952166511172867,
        "epoch": 0.36761308592294506,
        "step": 4933
    },
    {
        "loss": 2.6594,
        "grad_norm": 3.7697203159332275,
        "learning_rate": 0.00019519512522809369,
        "epoch": 0.3676876071242268,
        "step": 4934
    },
    {
        "loss": 2.7368,
        "grad_norm": 3.1568055152893066,
        "learning_rate": 0.00019517355220467712,
        "epoch": 0.3677621283255086,
        "step": 4935
    },
    {
        "loss": 2.2442,
        "grad_norm": 2.8639166355133057,
        "learning_rate": 0.00019515193205771856,
        "epoch": 0.36783664952679035,
        "step": 4936
    },
    {
        "loss": 2.2911,
        "grad_norm": 2.15082049369812,
        "learning_rate": 0.00019513026479792278,
        "epoch": 0.3679111707280721,
        "step": 4937
    },
    {
        "loss": 2.6775,
        "grad_norm": 3.8631439208984375,
        "learning_rate": 0.00019510855043601794,
        "epoch": 0.3679856919293539,
        "step": 4938
    },
    {
        "loss": 2.1433,
        "grad_norm": 2.7045023441314697,
        "learning_rate": 0.00019508678898275564,
        "epoch": 0.36806021313063564,
        "step": 4939
    },
    {
        "loss": 2.1284,
        "grad_norm": 3.077730894088745,
        "learning_rate": 0.0001950649804489106,
        "epoch": 0.36813473433191746,
        "step": 4940
    },
    {
        "loss": 1.447,
        "grad_norm": 3.5730013847351074,
        "learning_rate": 0.00019504312484528095,
        "epoch": 0.3682092555331992,
        "step": 4941
    },
    {
        "loss": 2.0381,
        "grad_norm": 3.0485596656799316,
        "learning_rate": 0.0001950212221826881,
        "epoch": 0.368283776734481,
        "step": 4942
    },
    {
        "loss": 2.1793,
        "grad_norm": 4.2311296463012695,
        "learning_rate": 0.00019499927247197682,
        "epoch": 0.36835829793576275,
        "step": 4943
    },
    {
        "loss": 2.5057,
        "grad_norm": 2.9296956062316895,
        "learning_rate": 0.00019497727572401509,
        "epoch": 0.3684328191370445,
        "step": 4944
    },
    {
        "loss": 2.6019,
        "grad_norm": 2.3438141345977783,
        "learning_rate": 0.00019495523194969425,
        "epoch": 0.3685073403383263,
        "step": 4945
    },
    {
        "loss": 2.6725,
        "grad_norm": 3.9775550365448,
        "learning_rate": 0.00019493314115992888,
        "epoch": 0.36858186153960804,
        "step": 4946
    },
    {
        "loss": 2.5866,
        "grad_norm": 2.118192434310913,
        "learning_rate": 0.00019491100336565685,
        "epoch": 0.3686563827408898,
        "step": 4947
    },
    {
        "loss": 2.7589,
        "grad_norm": 2.330223560333252,
        "learning_rate": 0.00019488881857783935,
        "epoch": 0.36873090394217156,
        "step": 4948
    },
    {
        "loss": 2.391,
        "grad_norm": 2.1184542179107666,
        "learning_rate": 0.00019486658680746072,
        "epoch": 0.3688054251434533,
        "step": 4949
    },
    {
        "loss": 2.7148,
        "grad_norm": 4.2492475509643555,
        "learning_rate": 0.00019484430806552871,
        "epoch": 0.3688799463447351,
        "step": 4950
    },
    {
        "loss": 2.461,
        "grad_norm": 2.641939401626587,
        "learning_rate": 0.00019482198236307418,
        "epoch": 0.36895446754601685,
        "step": 4951
    },
    {
        "loss": 2.614,
        "grad_norm": 2.740964889526367,
        "learning_rate": 0.00019479960971115136,
        "epoch": 0.3690289887472986,
        "step": 4952
    },
    {
        "loss": 1.7929,
        "grad_norm": 4.6251540184021,
        "learning_rate": 0.0001947771901208377,
        "epoch": 0.3691035099485804,
        "step": 4953
    },
    {
        "loss": 2.2465,
        "grad_norm": 3.232856035232544,
        "learning_rate": 0.00019475472360323384,
        "epoch": 0.36917803114986214,
        "step": 4954
    },
    {
        "loss": 2.4294,
        "grad_norm": 2.982909917831421,
        "learning_rate": 0.0001947322101694637,
        "epoch": 0.3692525523511439,
        "step": 4955
    },
    {
        "loss": 1.6708,
        "grad_norm": 3.192735195159912,
        "learning_rate": 0.0001947096498306744,
        "epoch": 0.36932707355242567,
        "step": 4956
    },
    {
        "loss": 2.7046,
        "grad_norm": 2.3080198764801025,
        "learning_rate": 0.00019468704259803637,
        "epoch": 0.36940159475370743,
        "step": 4957
    },
    {
        "loss": 2.0475,
        "grad_norm": 4.243078231811523,
        "learning_rate": 0.0001946643884827431,
        "epoch": 0.3694761159549892,
        "step": 4958
    },
    {
        "loss": 2.7155,
        "grad_norm": 3.395073175430298,
        "learning_rate": 0.00019464168749601144,
        "epoch": 0.36955063715627096,
        "step": 4959
    },
    {
        "loss": 2.9091,
        "grad_norm": 2.4858486652374268,
        "learning_rate": 0.0001946189396490814,
        "epoch": 0.3696251583575527,
        "step": 4960
    },
    {
        "loss": 1.8621,
        "grad_norm": 3.2238166332244873,
        "learning_rate": 0.00019459614495321613,
        "epoch": 0.3696996795588345,
        "step": 4961
    },
    {
        "loss": 2.3274,
        "grad_norm": 4.197110652923584,
        "learning_rate": 0.00019457330341970206,
        "epoch": 0.36977420076011625,
        "step": 4962
    },
    {
        "loss": 2.3384,
        "grad_norm": 2.950101852416992,
        "learning_rate": 0.0001945504150598488,
        "epoch": 0.369848721961398,
        "step": 4963
    },
    {
        "loss": 1.891,
        "grad_norm": 1.3396021127700806,
        "learning_rate": 0.00019452747988498917,
        "epoch": 0.36992324316267977,
        "step": 4964
    },
    {
        "loss": 2.9164,
        "grad_norm": 2.6671547889709473,
        "learning_rate": 0.00019450449790647895,
        "epoch": 0.36999776436396153,
        "step": 4965
    },
    {
        "loss": 2.9669,
        "grad_norm": 2.48325514793396,
        "learning_rate": 0.00019448146913569745,
        "epoch": 0.3700722855652433,
        "step": 4966
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.44805908203125,
        "learning_rate": 0.0001944583935840469,
        "epoch": 0.37014680676652506,
        "step": 4967
    },
    {
        "loss": 0.9999,
        "grad_norm": 3.1769068241119385,
        "learning_rate": 0.00019443527126295274,
        "epoch": 0.3702213279678068,
        "step": 4968
    },
    {
        "loss": 2.773,
        "grad_norm": 2.7277326583862305,
        "learning_rate": 0.00019441210218386362,
        "epoch": 0.3702958491690886,
        "step": 4969
    },
    {
        "loss": 2.6668,
        "grad_norm": 1.873945713043213,
        "learning_rate": 0.00019438888635825134,
        "epoch": 0.37037037037037035,
        "step": 4970
    },
    {
        "loss": 2.1347,
        "grad_norm": 6.954877853393555,
        "learning_rate": 0.00019436562379761076,
        "epoch": 0.3704448915716521,
        "step": 4971
    },
    {
        "loss": 2.3437,
        "grad_norm": 2.7250723838806152,
        "learning_rate": 0.00019434231451345997,
        "epoch": 0.3705194127729339,
        "step": 4972
    },
    {
        "loss": 2.6396,
        "grad_norm": 2.367079973220825,
        "learning_rate": 0.00019431895851734012,
        "epoch": 0.37059393397421564,
        "step": 4973
    },
    {
        "loss": 2.3845,
        "grad_norm": 5.569098472595215,
        "learning_rate": 0.00019429555582081557,
        "epoch": 0.3706684551754974,
        "step": 4974
    },
    {
        "loss": 3.0293,
        "grad_norm": 3.4547476768493652,
        "learning_rate": 0.00019427210643547374,
        "epoch": 0.3707429763767792,
        "step": 4975
    },
    {
        "loss": 2.6328,
        "grad_norm": 2.1362435817718506,
        "learning_rate": 0.00019424861037292516,
        "epoch": 0.370817497578061,
        "step": 4976
    },
    {
        "loss": 2.3085,
        "grad_norm": 3.0549235343933105,
        "learning_rate": 0.00019422506764480354,
        "epoch": 0.37089201877934275,
        "step": 4977
    },
    {
        "loss": 2.607,
        "grad_norm": 3.8318583965301514,
        "learning_rate": 0.0001942014782627657,
        "epoch": 0.3709665399806245,
        "step": 4978
    },
    {
        "loss": 2.6118,
        "grad_norm": 1.7715975046157837,
        "learning_rate": 0.00019417784223849143,
        "epoch": 0.3710410611819063,
        "step": 4979
    },
    {
        "loss": 2.3981,
        "grad_norm": 1.8544681072235107,
        "learning_rate": 0.00019415415958368374,
        "epoch": 0.37111558238318804,
        "step": 4980
    },
    {
        "loss": 1.8505,
        "grad_norm": 2.9129817485809326,
        "learning_rate": 0.00019413043031006868,
        "epoch": 0.3711901035844698,
        "step": 4981
    },
    {
        "loss": 2.7749,
        "grad_norm": 2.0397157669067383,
        "learning_rate": 0.00019410665442939537,
        "epoch": 0.37126462478575156,
        "step": 4982
    },
    {
        "loss": 2.0839,
        "grad_norm": 3.464509963989258,
        "learning_rate": 0.0001940828319534361,
        "epoch": 0.3713391459870333,
        "step": 4983
    },
    {
        "loss": 2.3197,
        "grad_norm": 2.8833112716674805,
        "learning_rate": 0.00019405896289398608,
        "epoch": 0.3714136671883151,
        "step": 4984
    },
    {
        "loss": 2.6815,
        "grad_norm": 2.1673519611358643,
        "learning_rate": 0.0001940350472628637,
        "epoch": 0.37148818838959685,
        "step": 4985
    },
    {
        "loss": 2.0908,
        "grad_norm": 4.27409029006958,
        "learning_rate": 0.0001940110850719103,
        "epoch": 0.3715627095908786,
        "step": 4986
    },
    {
        "loss": 2.5457,
        "grad_norm": 4.5858473777771,
        "learning_rate": 0.00019398707633299045,
        "epoch": 0.3716372307921604,
        "step": 4987
    },
    {
        "loss": 2.2107,
        "grad_norm": 3.5717196464538574,
        "learning_rate": 0.00019396302105799165,
        "epoch": 0.37171175199344214,
        "step": 4988
    },
    {
        "loss": 2.6034,
        "grad_norm": 2.7298004627227783,
        "learning_rate": 0.00019393891925882436,
        "epoch": 0.3717862731947239,
        "step": 4989
    },
    {
        "loss": 2.8473,
        "grad_norm": 2.7646429538726807,
        "learning_rate": 0.0001939147709474223,
        "epoch": 0.37186079439600567,
        "step": 4990
    },
    {
        "loss": 2.5318,
        "grad_norm": 4.392303466796875,
        "learning_rate": 0.00019389057613574198,
        "epoch": 0.37193531559728743,
        "step": 4991
    },
    {
        "loss": 2.8273,
        "grad_norm": 5.006261348724365,
        "learning_rate": 0.0001938663348357631,
        "epoch": 0.3720098367985692,
        "step": 4992
    },
    {
        "loss": 1.8988,
        "grad_norm": 5.69909143447876,
        "learning_rate": 0.00019384204705948828,
        "epoch": 0.37208435799985096,
        "step": 4993
    },
    {
        "loss": 2.5487,
        "grad_norm": 3.0291640758514404,
        "learning_rate": 0.0001938177128189433,
        "epoch": 0.3721588792011327,
        "step": 4994
    },
    {
        "loss": 2.7812,
        "grad_norm": 2.836852788925171,
        "learning_rate": 0.0001937933321261767,
        "epoch": 0.3722334004024145,
        "step": 4995
    },
    {
        "loss": 2.7057,
        "grad_norm": 2.1289637088775635,
        "learning_rate": 0.00019376890499326021,
        "epoch": 0.37230792160369625,
        "step": 4996
    },
    {
        "loss": 1.6894,
        "grad_norm": 2.182551383972168,
        "learning_rate": 0.00019374443143228856,
        "epoch": 0.372382442804978,
        "step": 4997
    },
    {
        "loss": 2.1257,
        "grad_norm": 3.145636796951294,
        "learning_rate": 0.00019371991145537934,
        "epoch": 0.37245696400625977,
        "step": 4998
    },
    {
        "loss": 1.639,
        "grad_norm": 2.4867072105407715,
        "learning_rate": 0.0001936953450746732,
        "epoch": 0.37253148520754154,
        "step": 4999
    },
    {
        "loss": 2.2457,
        "grad_norm": 1.5933867692947388,
        "learning_rate": 0.00019367073230233382,
        "epoch": 0.3726060064088233,
        "step": 5000
    },
    {
        "loss": 2.7478,
        "grad_norm": 2.3767576217651367,
        "learning_rate": 0.0001936460731505477,
        "epoch": 0.37268052761010506,
        "step": 5001
    },
    {
        "loss": 1.7841,
        "grad_norm": 2.432950973510742,
        "learning_rate": 0.00019362136763152447,
        "epoch": 0.3727550488113868,
        "step": 5002
    },
    {
        "loss": 2.143,
        "grad_norm": 3.7510125637054443,
        "learning_rate": 0.00019359661575749662,
        "epoch": 0.3728295700126686,
        "step": 5003
    },
    {
        "loss": 2.7163,
        "grad_norm": 2.0347213745117188,
        "learning_rate": 0.0001935718175407196,
        "epoch": 0.37290409121395035,
        "step": 5004
    },
    {
        "loss": 1.7112,
        "grad_norm": 2.191882371902466,
        "learning_rate": 0.0001935469729934718,
        "epoch": 0.3729786124152321,
        "step": 5005
    },
    {
        "loss": 2.0644,
        "grad_norm": 3.067697525024414,
        "learning_rate": 0.0001935220821280546,
        "epoch": 0.3730531336165139,
        "step": 5006
    },
    {
        "loss": 2.5206,
        "grad_norm": 4.368468284606934,
        "learning_rate": 0.0001934971449567923,
        "epoch": 0.37312765481779564,
        "step": 5007
    },
    {
        "loss": 2.4904,
        "grad_norm": 2.6828067302703857,
        "learning_rate": 0.00019347216149203207,
        "epoch": 0.3732021760190774,
        "step": 5008
    },
    {
        "loss": 2.7974,
        "grad_norm": 2.2330667972564697,
        "learning_rate": 0.00019344713174614404,
        "epoch": 0.37327669722035917,
        "step": 5009
    },
    {
        "loss": 2.8631,
        "grad_norm": 3.7120954990386963,
        "learning_rate": 0.0001934220557315213,
        "epoch": 0.373351218421641,
        "step": 5010
    },
    {
        "loss": 2.0602,
        "grad_norm": 3.5498201847076416,
        "learning_rate": 0.0001933969334605798,
        "epoch": 0.37342573962292275,
        "step": 5011
    },
    {
        "loss": 2.3021,
        "grad_norm": 2.7505788803100586,
        "learning_rate": 0.00019337176494575834,
        "epoch": 0.3735002608242045,
        "step": 5012
    },
    {
        "loss": 2.6854,
        "grad_norm": 1.9944171905517578,
        "learning_rate": 0.00019334655019951873,
        "epoch": 0.3735747820254863,
        "step": 5013
    },
    {
        "loss": 1.7839,
        "grad_norm": 2.6091063022613525,
        "learning_rate": 0.0001933212892343456,
        "epoch": 0.37364930322676804,
        "step": 5014
    },
    {
        "loss": 2.9101,
        "grad_norm": 3.1285901069641113,
        "learning_rate": 0.00019329598206274648,
        "epoch": 0.3737238244280498,
        "step": 5015
    },
    {
        "loss": 2.7778,
        "grad_norm": 2.1597378253936768,
        "learning_rate": 0.00019327062869725178,
        "epoch": 0.37379834562933156,
        "step": 5016
    },
    {
        "loss": 1.9289,
        "grad_norm": 3.1256027221679688,
        "learning_rate": 0.0001932452291504148,
        "epoch": 0.3738728668306133,
        "step": 5017
    },
    {
        "loss": 2.8048,
        "grad_norm": 2.1803579330444336,
        "learning_rate": 0.00019321978343481163,
        "epoch": 0.3739473880318951,
        "step": 5018
    },
    {
        "loss": 2.9047,
        "grad_norm": 2.8990228176116943,
        "learning_rate": 0.00019319429156304136,
        "epoch": 0.37402190923317685,
        "step": 5019
    },
    {
        "loss": 2.7669,
        "grad_norm": 1.9915257692337036,
        "learning_rate": 0.00019316875354772578,
        "epoch": 0.3740964304344586,
        "step": 5020
    },
    {
        "loss": 2.1146,
        "grad_norm": 5.3733744621276855,
        "learning_rate": 0.00019314316940150967,
        "epoch": 0.3741709516357404,
        "step": 5021
    },
    {
        "loss": 2.9202,
        "grad_norm": 3.592789888381958,
        "learning_rate": 0.0001931175391370605,
        "epoch": 0.37424547283702214,
        "step": 5022
    },
    {
        "loss": 1.1729,
        "grad_norm": 4.961484909057617,
        "learning_rate": 0.00019309186276706867,
        "epoch": 0.3743199940383039,
        "step": 5023
    },
    {
        "loss": 2.7566,
        "grad_norm": 2.3714799880981445,
        "learning_rate": 0.00019306614030424745,
        "epoch": 0.37439451523958567,
        "step": 5024
    },
    {
        "loss": 2.2061,
        "grad_norm": 2.34704852104187,
        "learning_rate": 0.0001930403717613328,
        "epoch": 0.37446903644086743,
        "step": 5025
    },
    {
        "loss": 1.8518,
        "grad_norm": 3.701768159866333,
        "learning_rate": 0.00019301455715108363,
        "epoch": 0.3745435576421492,
        "step": 5026
    },
    {
        "loss": 2.4904,
        "grad_norm": 2.8083295822143555,
        "learning_rate": 0.00019298869648628152,
        "epoch": 0.37461807884343096,
        "step": 5027
    },
    {
        "loss": 2.7051,
        "grad_norm": 3.169529914855957,
        "learning_rate": 0.00019296278977973103,
        "epoch": 0.3746926000447127,
        "step": 5028
    },
    {
        "loss": 1.7596,
        "grad_norm": 2.4610588550567627,
        "learning_rate": 0.00019293683704425934,
        "epoch": 0.3747671212459945,
        "step": 5029
    },
    {
        "loss": 2.753,
        "grad_norm": 1.8941116333007812,
        "learning_rate": 0.00019291083829271657,
        "epoch": 0.37484164244727625,
        "step": 5030
    },
    {
        "loss": 2.3694,
        "grad_norm": 2.150665521621704,
        "learning_rate": 0.0001928847935379755,
        "epoch": 0.374916163648558,
        "step": 5031
    },
    {
        "loss": 2.3078,
        "grad_norm": 2.650716543197632,
        "learning_rate": 0.00019285870279293175,
        "epoch": 0.3749906848498398,
        "step": 5032
    },
    {
        "loss": 1.8014,
        "grad_norm": 3.707500696182251,
        "learning_rate": 0.0001928325660705037,
        "epoch": 0.37506520605112154,
        "step": 5033
    },
    {
        "loss": 2.3795,
        "grad_norm": 3.1391780376434326,
        "learning_rate": 0.0001928063833836325,
        "epoch": 0.3751397272524033,
        "step": 5034
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.66572904586792,
        "learning_rate": 0.00019278015474528208,
        "epoch": 0.37521424845368506,
        "step": 5035
    },
    {
        "loss": 2.128,
        "grad_norm": 2.832880973815918,
        "learning_rate": 0.00019275388016843904,
        "epoch": 0.3752887696549668,
        "step": 5036
    },
    {
        "loss": 1.9124,
        "grad_norm": 3.1182103157043457,
        "learning_rate": 0.00019272755966611284,
        "epoch": 0.3753632908562486,
        "step": 5037
    },
    {
        "loss": 2.6037,
        "grad_norm": 2.252305746078491,
        "learning_rate": 0.00019270119325133557,
        "epoch": 0.37543781205753035,
        "step": 5038
    },
    {
        "loss": 2.7342,
        "grad_norm": 2.5790040493011475,
        "learning_rate": 0.00019267478093716218,
        "epoch": 0.3755123332588121,
        "step": 5039
    },
    {
        "loss": 2.4788,
        "grad_norm": 2.545398235321045,
        "learning_rate": 0.00019264832273667025,
        "epoch": 0.3755868544600939,
        "step": 5040
    },
    {
        "loss": 2.4512,
        "grad_norm": 3.438629150390625,
        "learning_rate": 0.00019262181866296004,
        "epoch": 0.37566137566137564,
        "step": 5041
    },
    {
        "loss": 2.2502,
        "grad_norm": 2.72355580329895,
        "learning_rate": 0.00019259526872915465,
        "epoch": 0.3757358968626574,
        "step": 5042
    },
    {
        "loss": 2.7842,
        "grad_norm": 2.8983983993530273,
        "learning_rate": 0.00019256867294839975,
        "epoch": 0.37581041806393917,
        "step": 5043
    },
    {
        "loss": 2.05,
        "grad_norm": 3.9109535217285156,
        "learning_rate": 0.00019254203133386392,
        "epoch": 0.37588493926522093,
        "step": 5044
    },
    {
        "loss": 1.9577,
        "grad_norm": 2.931427478790283,
        "learning_rate": 0.00019251534389873816,
        "epoch": 0.3759594604665027,
        "step": 5045
    },
    {
        "loss": 2.5638,
        "grad_norm": 2.6838576793670654,
        "learning_rate": 0.0001924886106562363,
        "epoch": 0.3760339816677845,
        "step": 5046
    },
    {
        "loss": 2.6384,
        "grad_norm": 2.217670440673828,
        "learning_rate": 0.00019246183161959488,
        "epoch": 0.3761085028690663,
        "step": 5047
    },
    {
        "loss": 2.5292,
        "grad_norm": 4.635533809661865,
        "learning_rate": 0.0001924350068020731,
        "epoch": 0.37618302407034804,
        "step": 5048
    },
    {
        "loss": 2.5791,
        "grad_norm": 2.6136133670806885,
        "learning_rate": 0.00019240813621695282,
        "epoch": 0.3762575452716298,
        "step": 5049
    },
    {
        "loss": 2.925,
        "grad_norm": 4.427445888519287,
        "learning_rate": 0.00019238121987753844,
        "epoch": 0.37633206647291156,
        "step": 5050
    },
    {
        "loss": 2.5069,
        "grad_norm": 3.624518632888794,
        "learning_rate": 0.0001923542577971573,
        "epoch": 0.3764065876741933,
        "step": 5051
    },
    {
        "loss": 1.3839,
        "grad_norm": 3.0001628398895264,
        "learning_rate": 0.00019232724998915902,
        "epoch": 0.3764811088754751,
        "step": 5052
    },
    {
        "loss": 1.7438,
        "grad_norm": 3.3182616233825684,
        "learning_rate": 0.00019230019646691613,
        "epoch": 0.37655563007675685,
        "step": 5053
    },
    {
        "loss": 2.8453,
        "grad_norm": 2.913010597229004,
        "learning_rate": 0.00019227309724382375,
        "epoch": 0.3766301512780386,
        "step": 5054
    },
    {
        "loss": 2.364,
        "grad_norm": 2.1695921421051025,
        "learning_rate": 0.00019224595233329955,
        "epoch": 0.3767046724793204,
        "step": 5055
    },
    {
        "loss": 2.4553,
        "grad_norm": 2.4421370029449463,
        "learning_rate": 0.00019221876174878386,
        "epoch": 0.37677919368060214,
        "step": 5056
    },
    {
        "loss": 2.548,
        "grad_norm": 1.8012735843658447,
        "learning_rate": 0.00019219152550373965,
        "epoch": 0.3768537148818839,
        "step": 5057
    },
    {
        "loss": 1.9267,
        "grad_norm": 2.99235200881958,
        "learning_rate": 0.00019216424361165246,
        "epoch": 0.37692823608316567,
        "step": 5058
    },
    {
        "loss": 2.8318,
        "grad_norm": 2.5781538486480713,
        "learning_rate": 0.00019213691608603047,
        "epoch": 0.37700275728444743,
        "step": 5059
    },
    {
        "loss": 2.7168,
        "grad_norm": 2.205853223800659,
        "learning_rate": 0.00019210954294040437,
        "epoch": 0.3770772784857292,
        "step": 5060
    },
    {
        "loss": 1.5615,
        "grad_norm": 4.58921480178833,
        "learning_rate": 0.00019208212418832764,
        "epoch": 0.37715179968701096,
        "step": 5061
    },
    {
        "loss": 2.4184,
        "grad_norm": 4.433248043060303,
        "learning_rate": 0.00019205465984337603,
        "epoch": 0.3772263208882927,
        "step": 5062
    },
    {
        "loss": 2.8874,
        "grad_norm": 2.1336281299591064,
        "learning_rate": 0.00019202714991914816,
        "epoch": 0.3773008420895745,
        "step": 5063
    },
    {
        "loss": 2.6914,
        "grad_norm": 2.4511587619781494,
        "learning_rate": 0.00019199959442926505,
        "epoch": 0.37737536329085625,
        "step": 5064
    },
    {
        "loss": 2.3154,
        "grad_norm": 4.149937152862549,
        "learning_rate": 0.00019197199338737034,
        "epoch": 0.377449884492138,
        "step": 5065
    },
    {
        "loss": 2.6153,
        "grad_norm": 2.8870320320129395,
        "learning_rate": 0.00019194434680713015,
        "epoch": 0.3775244056934198,
        "step": 5066
    },
    {
        "loss": 2.4414,
        "grad_norm": 2.0800623893737793,
        "learning_rate": 0.00019191665470223327,
        "epoch": 0.37759892689470154,
        "step": 5067
    },
    {
        "loss": 2.3011,
        "grad_norm": 3.624530076980591,
        "learning_rate": 0.000191888917086391,
        "epoch": 0.3776734480959833,
        "step": 5068
    },
    {
        "loss": 2.6239,
        "grad_norm": 3.1281206607818604,
        "learning_rate": 0.00019186113397333702,
        "epoch": 0.37774796929726506,
        "step": 5069
    },
    {
        "loss": 2.2783,
        "grad_norm": 2.094104766845703,
        "learning_rate": 0.00019183330537682775,
        "epoch": 0.3778224904985468,
        "step": 5070
    },
    {
        "loss": 1.4604,
        "grad_norm": 4.026646137237549,
        "learning_rate": 0.0001918054313106421,
        "epoch": 0.3778970116998286,
        "step": 5071
    },
    {
        "loss": 2.6634,
        "grad_norm": 2.0473153591156006,
        "learning_rate": 0.00019177751178858126,
        "epoch": 0.37797153290111035,
        "step": 5072
    },
    {
        "loss": 2.5954,
        "grad_norm": 2.405733585357666,
        "learning_rate": 0.00019174954682446926,
        "epoch": 0.3780460541023921,
        "step": 5073
    },
    {
        "loss": 2.2921,
        "grad_norm": 2.7402892112731934,
        "learning_rate": 0.0001917215364321524,
        "epoch": 0.3781205753036739,
        "step": 5074
    },
    {
        "loss": 2.1845,
        "grad_norm": 3.3139145374298096,
        "learning_rate": 0.00019169348062549952,
        "epoch": 0.37819509650495564,
        "step": 5075
    },
    {
        "loss": 2.7179,
        "grad_norm": 2.5968575477600098,
        "learning_rate": 0.00019166537941840196,
        "epoch": 0.3782696177062374,
        "step": 5076
    },
    {
        "loss": 2.0435,
        "grad_norm": 3.6968953609466553,
        "learning_rate": 0.00019163723282477366,
        "epoch": 0.37834413890751917,
        "step": 5077
    },
    {
        "loss": 2.643,
        "grad_norm": 2.378596544265747,
        "learning_rate": 0.00019160904085855078,
        "epoch": 0.37841866010880093,
        "step": 5078
    },
    {
        "loss": 2.6825,
        "grad_norm": 3.5135583877563477,
        "learning_rate": 0.00019158080353369217,
        "epoch": 0.3784931813100827,
        "step": 5079
    },
    {
        "loss": 2.4631,
        "grad_norm": 2.357086181640625,
        "learning_rate": 0.00019155252086417904,
        "epoch": 0.37856770251136446,
        "step": 5080
    },
    {
        "loss": 2.2408,
        "grad_norm": 2.595878839492798,
        "learning_rate": 0.00019152419286401506,
        "epoch": 0.3786422237126463,
        "step": 5081
    },
    {
        "loss": 1.7988,
        "grad_norm": 4.1006011962890625,
        "learning_rate": 0.00019149581954722638,
        "epoch": 0.37871674491392804,
        "step": 5082
    },
    {
        "loss": 2.5193,
        "grad_norm": 3.2038049697875977,
        "learning_rate": 0.00019146740092786146,
        "epoch": 0.3787912661152098,
        "step": 5083
    },
    {
        "loss": 2.7567,
        "grad_norm": 2.124648094177246,
        "learning_rate": 0.00019143893701999138,
        "epoch": 0.37886578731649156,
        "step": 5084
    },
    {
        "loss": 2.5438,
        "grad_norm": 3.4108660221099854,
        "learning_rate": 0.00019141042783770958,
        "epoch": 0.3789403085177733,
        "step": 5085
    },
    {
        "loss": 2.1914,
        "grad_norm": 3.686392068862915,
        "learning_rate": 0.00019138187339513176,
        "epoch": 0.3790148297190551,
        "step": 5086
    },
    {
        "loss": 2.8505,
        "grad_norm": 2.3375511169433594,
        "learning_rate": 0.0001913532737063963,
        "epoch": 0.37908935092033685,
        "step": 5087
    },
    {
        "loss": 1.005,
        "grad_norm": 3.4245445728302,
        "learning_rate": 0.00019132462878566374,
        "epoch": 0.3791638721216186,
        "step": 5088
    },
    {
        "loss": 3.0689,
        "grad_norm": 2.637420654296875,
        "learning_rate": 0.00019129593864711716,
        "epoch": 0.3792383933229004,
        "step": 5089
    },
    {
        "loss": 2.6168,
        "grad_norm": 2.819117307662964,
        "learning_rate": 0.000191267203304962,
        "epoch": 0.37931291452418214,
        "step": 5090
    },
    {
        "loss": 2.5521,
        "grad_norm": 2.625833749771118,
        "learning_rate": 0.00019123842277342606,
        "epoch": 0.3793874357254639,
        "step": 5091
    },
    {
        "loss": 2.3028,
        "grad_norm": 3.9606621265411377,
        "learning_rate": 0.00019120959706675955,
        "epoch": 0.37946195692674567,
        "step": 5092
    },
    {
        "loss": 1.9922,
        "grad_norm": 4.490250110626221,
        "learning_rate": 0.00019118072619923494,
        "epoch": 0.37953647812802743,
        "step": 5093
    },
    {
        "loss": 2.4051,
        "grad_norm": 3.3848395347595215,
        "learning_rate": 0.0001911518101851472,
        "epoch": 0.3796109993293092,
        "step": 5094
    },
    {
        "loss": 2.7705,
        "grad_norm": 2.090099334716797,
        "learning_rate": 0.0001911228490388136,
        "epoch": 0.37968552053059096,
        "step": 5095
    },
    {
        "loss": 2.7983,
        "grad_norm": 2.3566417694091797,
        "learning_rate": 0.00019109384277457374,
        "epoch": 0.3797600417318727,
        "step": 5096
    },
    {
        "loss": 2.9484,
        "grad_norm": 3.1562724113464355,
        "learning_rate": 0.00019106479140678953,
        "epoch": 0.3798345629331545,
        "step": 5097
    },
    {
        "loss": 2.311,
        "grad_norm": 2.301168203353882,
        "learning_rate": 0.0001910356949498453,
        "epoch": 0.37990908413443625,
        "step": 5098
    },
    {
        "loss": 2.4758,
        "grad_norm": 3.487893581390381,
        "learning_rate": 0.00019100655341814767,
        "epoch": 0.379983605335718,
        "step": 5099
    },
    {
        "loss": 1.832,
        "grad_norm": 3.084716796875,
        "learning_rate": 0.00019097736682612554,
        "epoch": 0.3800581265369998,
        "step": 5100
    },
    {
        "loss": 2.1752,
        "grad_norm": 4.051534652709961,
        "learning_rate": 0.00019094813518823021,
        "epoch": 0.38013264773828154,
        "step": 5101
    },
    {
        "loss": 2.5748,
        "grad_norm": 3.0975539684295654,
        "learning_rate": 0.0001909188585189351,
        "epoch": 0.3802071689395633,
        "step": 5102
    },
    {
        "loss": 1.7367,
        "grad_norm": 3.369546413421631,
        "learning_rate": 0.00019088953683273614,
        "epoch": 0.38028169014084506,
        "step": 5103
    },
    {
        "loss": 2.1567,
        "grad_norm": 2.79893159866333,
        "learning_rate": 0.00019086017014415143,
        "epoch": 0.3803562113421268,
        "step": 5104
    },
    {
        "loss": 2.7356,
        "grad_norm": 2.7451443672180176,
        "learning_rate": 0.0001908307584677214,
        "epoch": 0.3804307325434086,
        "step": 5105
    },
    {
        "loss": 2.4058,
        "grad_norm": 2.3022818565368652,
        "learning_rate": 0.00019080130181800867,
        "epoch": 0.38050525374469035,
        "step": 5106
    },
    {
        "loss": 2.5043,
        "grad_norm": 3.297684907913208,
        "learning_rate": 0.00019077180020959827,
        "epoch": 0.3805797749459721,
        "step": 5107
    },
    {
        "loss": 3.0583,
        "grad_norm": 2.4365665912628174,
        "learning_rate": 0.00019074225365709735,
        "epoch": 0.3806542961472539,
        "step": 5108
    },
    {
        "loss": 2.5433,
        "grad_norm": 3.2432332038879395,
        "learning_rate": 0.00019071266217513542,
        "epoch": 0.38072881734853564,
        "step": 5109
    },
    {
        "loss": 2.2279,
        "grad_norm": 2.8026466369628906,
        "learning_rate": 0.0001906830257783642,
        "epoch": 0.3808033385498174,
        "step": 5110
    },
    {
        "loss": 2.5033,
        "grad_norm": 2.450385808944702,
        "learning_rate": 0.00019065334448145757,
        "epoch": 0.38087785975109917,
        "step": 5111
    },
    {
        "loss": 2.3761,
        "grad_norm": 3.543031930923462,
        "learning_rate": 0.00019062361829911181,
        "epoch": 0.38095238095238093,
        "step": 5112
    },
    {
        "loss": 2.1079,
        "grad_norm": 3.8563711643218994,
        "learning_rate": 0.00019059384724604524,
        "epoch": 0.3810269021536627,
        "step": 5113
    },
    {
        "loss": 2.3629,
        "grad_norm": 2.5565433502197266,
        "learning_rate": 0.0001905640313369985,
        "epoch": 0.38110142335494446,
        "step": 5114
    },
    {
        "loss": 2.1978,
        "grad_norm": 3.3636317253112793,
        "learning_rate": 0.00019053417058673448,
        "epoch": 0.3811759445562262,
        "step": 5115
    },
    {
        "loss": 1.8467,
        "grad_norm": 3.9103965759277344,
        "learning_rate": 0.00019050426501003817,
        "epoch": 0.381250465757508,
        "step": 5116
    },
    {
        "loss": 2.7437,
        "grad_norm": 2.4447391033172607,
        "learning_rate": 0.00019047431462171677,
        "epoch": 0.3813249869587898,
        "step": 5117
    },
    {
        "loss": 2.7411,
        "grad_norm": 3.2995951175689697,
        "learning_rate": 0.00019044431943659977,
        "epoch": 0.38139950816007157,
        "step": 5118
    },
    {
        "loss": 2.5092,
        "grad_norm": 2.7174875736236572,
        "learning_rate": 0.0001904142794695387,
        "epoch": 0.38147402936135333,
        "step": 5119
    },
    {
        "loss": 2.4045,
        "grad_norm": 2.774319887161255,
        "learning_rate": 0.00019038419473540742,
        "epoch": 0.3815485505626351,
        "step": 5120
    },
    {
        "loss": 2.2481,
        "grad_norm": 3.044494867324829,
        "learning_rate": 0.00019035406524910172,
        "epoch": 0.38162307176391685,
        "step": 5121
    },
    {
        "loss": 2.0449,
        "grad_norm": 3.659972906112671,
        "learning_rate": 0.0001903238910255399,
        "epoch": 0.3816975929651986,
        "step": 5122
    },
    {
        "loss": 2.6745,
        "grad_norm": 4.4947686195373535,
        "learning_rate": 0.000190293672079662,
        "epoch": 0.3817721141664804,
        "step": 5123
    },
    {
        "loss": 1.7513,
        "grad_norm": 5.3620781898498535,
        "learning_rate": 0.00019026340842643057,
        "epoch": 0.38184663536776214,
        "step": 5124
    },
    {
        "loss": 2.6562,
        "grad_norm": 2.959611654281616,
        "learning_rate": 0.00019023310008082998,
        "epoch": 0.3819211565690439,
        "step": 5125
    },
    {
        "loss": 2.3337,
        "grad_norm": 3.0536458492279053,
        "learning_rate": 0.00019020274705786706,
        "epoch": 0.38199567777032567,
        "step": 5126
    },
    {
        "loss": 1.4628,
        "grad_norm": 3.890265941619873,
        "learning_rate": 0.00019017234937257043,
        "epoch": 0.38207019897160743,
        "step": 5127
    },
    {
        "loss": 2.5373,
        "grad_norm": 1.84867525100708,
        "learning_rate": 0.00019014190703999108,
        "epoch": 0.3821447201728892,
        "step": 5128
    },
    {
        "loss": 2.2797,
        "grad_norm": 2.7152903079986572,
        "learning_rate": 0.00019011142007520197,
        "epoch": 0.38221924137417096,
        "step": 5129
    },
    {
        "loss": 2.259,
        "grad_norm": 2.8344383239746094,
        "learning_rate": 0.0001900808884932982,
        "epoch": 0.3822937625754527,
        "step": 5130
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.345552444458008,
        "learning_rate": 0.00019005031230939694,
        "epoch": 0.3823682837767345,
        "step": 5131
    },
    {
        "loss": 2.5469,
        "grad_norm": 3.3931243419647217,
        "learning_rate": 0.00019001969153863748,
        "epoch": 0.38244280497801625,
        "step": 5132
    },
    {
        "loss": 1.1759,
        "grad_norm": 2.3098673820495605,
        "learning_rate": 0.00018998902619618116,
        "epoch": 0.382517326179298,
        "step": 5133
    },
    {
        "loss": 2.2028,
        "grad_norm": 3.993591070175171,
        "learning_rate": 0.0001899583162972114,
        "epoch": 0.3825918473805798,
        "step": 5134
    },
    {
        "loss": 1.8218,
        "grad_norm": 2.2167842388153076,
        "learning_rate": 0.00018992756185693365,
        "epoch": 0.38266636858186154,
        "step": 5135
    },
    {
        "loss": 1.9955,
        "grad_norm": 3.0525660514831543,
        "learning_rate": 0.00018989676289057545,
        "epoch": 0.3827408897831433,
        "step": 5136
    },
    {
        "loss": 1.8668,
        "grad_norm": 2.7759623527526855,
        "learning_rate": 0.0001898659194133864,
        "epoch": 0.38281541098442506,
        "step": 5137
    },
    {
        "loss": 2.5978,
        "grad_norm": 3.4696807861328125,
        "learning_rate": 0.0001898350314406381,
        "epoch": 0.3828899321857068,
        "step": 5138
    },
    {
        "loss": 3.215,
        "grad_norm": 3.7538092136383057,
        "learning_rate": 0.00018980409898762424,
        "epoch": 0.3829644533869886,
        "step": 5139
    },
    {
        "loss": 2.2354,
        "grad_norm": 3.690598487854004,
        "learning_rate": 0.0001897731220696604,
        "epoch": 0.38303897458827035,
        "step": 5140
    },
    {
        "loss": 1.9001,
        "grad_norm": 2.3790268898010254,
        "learning_rate": 0.00018974210070208433,
        "epoch": 0.3831134957895521,
        "step": 5141
    },
    {
        "loss": 2.2195,
        "grad_norm": 3.4753689765930176,
        "learning_rate": 0.0001897110349002557,
        "epoch": 0.3831880169908339,
        "step": 5142
    },
    {
        "loss": 2.6943,
        "grad_norm": 2.4098880290985107,
        "learning_rate": 0.00018967992467955626,
        "epoch": 0.38326253819211564,
        "step": 5143
    },
    {
        "loss": 1.8176,
        "grad_norm": 2.2879977226257324,
        "learning_rate": 0.00018964877005538962,
        "epoch": 0.3833370593933974,
        "step": 5144
    },
    {
        "loss": 2.7382,
        "grad_norm": 3.14416241645813,
        "learning_rate": 0.00018961757104318146,
        "epoch": 0.38341158059467917,
        "step": 5145
    },
    {
        "loss": 2.4145,
        "grad_norm": 2.914586305618286,
        "learning_rate": 0.00018958632765837954,
        "epoch": 0.38348610179596093,
        "step": 5146
    },
    {
        "loss": 2.8328,
        "grad_norm": 2.464996576309204,
        "learning_rate": 0.00018955503991645336,
        "epoch": 0.3835606229972427,
        "step": 5147
    },
    {
        "loss": 1.6155,
        "grad_norm": 4.582289695739746,
        "learning_rate": 0.00018952370783289455,
        "epoch": 0.38363514419852446,
        "step": 5148
    },
    {
        "loss": 2.1418,
        "grad_norm": 3.0341320037841797,
        "learning_rate": 0.00018949233142321665,
        "epoch": 0.3837096653998062,
        "step": 5149
    },
    {
        "loss": 2.8587,
        "grad_norm": 2.1347532272338867,
        "learning_rate": 0.00018946091070295515,
        "epoch": 0.383784186601088,
        "step": 5150
    },
    {
        "loss": 2.4343,
        "grad_norm": 2.002993583679199,
        "learning_rate": 0.00018942944568766748,
        "epoch": 0.38385870780236975,
        "step": 5151
    },
    {
        "loss": 2.6265,
        "grad_norm": 2.5079379081726074,
        "learning_rate": 0.000189397936392933,
        "epoch": 0.38393322900365157,
        "step": 5152
    },
    {
        "loss": 2.7758,
        "grad_norm": 2.241544008255005,
        "learning_rate": 0.000189366382834353,
        "epoch": 0.38400775020493333,
        "step": 5153
    },
    {
        "loss": 2.0941,
        "grad_norm": 2.296862840652466,
        "learning_rate": 0.00018933478502755066,
        "epoch": 0.3840822714062151,
        "step": 5154
    },
    {
        "loss": 2.1554,
        "grad_norm": 2.05724835395813,
        "learning_rate": 0.00018930314298817104,
        "epoch": 0.38415679260749686,
        "step": 5155
    },
    {
        "loss": 2.7512,
        "grad_norm": 4.428500175476074,
        "learning_rate": 0.00018927145673188132,
        "epoch": 0.3842313138087786,
        "step": 5156
    },
    {
        "loss": 2.1407,
        "grad_norm": 6.092476844787598,
        "learning_rate": 0.00018923972627437024,
        "epoch": 0.3843058350100604,
        "step": 5157
    },
    {
        "loss": 2.2717,
        "grad_norm": 2.452517509460449,
        "learning_rate": 0.0001892079516313486,
        "epoch": 0.38438035621134214,
        "step": 5158
    },
    {
        "loss": 2.2584,
        "grad_norm": 3.8371474742889404,
        "learning_rate": 0.00018917613281854915,
        "epoch": 0.3844548774126239,
        "step": 5159
    },
    {
        "loss": 1.7537,
        "grad_norm": 3.082526922225952,
        "learning_rate": 0.00018914426985172634,
        "epoch": 0.38452939861390567,
        "step": 5160
    },
    {
        "loss": 2.0318,
        "grad_norm": 3.2922561168670654,
        "learning_rate": 0.00018911236274665662,
        "epoch": 0.38460391981518743,
        "step": 5161
    },
    {
        "loss": 2.6731,
        "grad_norm": 2.819572687149048,
        "learning_rate": 0.0001890804115191383,
        "epoch": 0.3846784410164692,
        "step": 5162
    },
    {
        "loss": 2.2286,
        "grad_norm": 3.751038074493408,
        "learning_rate": 0.00018904841618499137,
        "epoch": 0.38475296221775096,
        "step": 5163
    },
    {
        "loss": 2.5811,
        "grad_norm": 3.571676731109619,
        "learning_rate": 0.0001890163767600578,
        "epoch": 0.3848274834190327,
        "step": 5164
    },
    {
        "loss": 1.6656,
        "grad_norm": 6.215460777282715,
        "learning_rate": 0.00018898429326020134,
        "epoch": 0.3849020046203145,
        "step": 5165
    },
    {
        "loss": 2.295,
        "grad_norm": 3.7034595012664795,
        "learning_rate": 0.00018895216570130767,
        "epoch": 0.38497652582159625,
        "step": 5166
    },
    {
        "loss": 1.8295,
        "grad_norm": 3.649024248123169,
        "learning_rate": 0.0001889199940992841,
        "epoch": 0.385051047022878,
        "step": 5167
    },
    {
        "loss": 1.0808,
        "grad_norm": 4.794836521148682,
        "learning_rate": 0.00018888777847005987,
        "epoch": 0.3851255682241598,
        "step": 5168
    },
    {
        "loss": 2.2188,
        "grad_norm": 3.8655641078948975,
        "learning_rate": 0.00018885551882958602,
        "epoch": 0.38520008942544154,
        "step": 5169
    },
    {
        "loss": 2.0064,
        "grad_norm": 2.7825441360473633,
        "learning_rate": 0.0001888232151938353,
        "epoch": 0.3852746106267233,
        "step": 5170
    },
    {
        "loss": 2.4224,
        "grad_norm": 3.585946559906006,
        "learning_rate": 0.0001887908675788024,
        "epoch": 0.38534913182800506,
        "step": 5171
    },
    {
        "loss": 2.6456,
        "grad_norm": 3.0746445655822754,
        "learning_rate": 0.00018875847600050355,
        "epoch": 0.38542365302928683,
        "step": 5172
    },
    {
        "loss": 2.7179,
        "grad_norm": 2.864119291305542,
        "learning_rate": 0.00018872604047497703,
        "epoch": 0.3854981742305686,
        "step": 5173
    },
    {
        "loss": 2.8075,
        "grad_norm": 2.6017656326293945,
        "learning_rate": 0.00018869356101828256,
        "epoch": 0.38557269543185035,
        "step": 5174
    },
    {
        "loss": 1.7925,
        "grad_norm": 3.274137258529663,
        "learning_rate": 0.0001886610376465019,
        "epoch": 0.3856472166331321,
        "step": 5175
    },
    {
        "loss": 3.0443,
        "grad_norm": 3.7097322940826416,
        "learning_rate": 0.00018862847037573842,
        "epoch": 0.3857217378344139,
        "step": 5176
    },
    {
        "loss": 2.0507,
        "grad_norm": 3.3279833793640137,
        "learning_rate": 0.00018859585922211726,
        "epoch": 0.38579625903569564,
        "step": 5177
    },
    {
        "loss": 2.7272,
        "grad_norm": 1.8119244575500488,
        "learning_rate": 0.00018856320420178522,
        "epoch": 0.3858707802369774,
        "step": 5178
    },
    {
        "loss": 1.3895,
        "grad_norm": 4.6191792488098145,
        "learning_rate": 0.00018853050533091086,
        "epoch": 0.38594530143825917,
        "step": 5179
    },
    {
        "loss": 2.5755,
        "grad_norm": 3.7822964191436768,
        "learning_rate": 0.00018849776262568448,
        "epoch": 0.38601982263954093,
        "step": 5180
    },
    {
        "loss": 1.3394,
        "grad_norm": 3.9970920085906982,
        "learning_rate": 0.00018846497610231812,
        "epoch": 0.3860943438408227,
        "step": 5181
    },
    {
        "loss": 2.269,
        "grad_norm": 3.242879629135132,
        "learning_rate": 0.0001884321457770454,
        "epoch": 0.38616886504210446,
        "step": 5182
    },
    {
        "loss": 1.9731,
        "grad_norm": 3.00478196144104,
        "learning_rate": 0.00018839927166612174,
        "epoch": 0.3862433862433862,
        "step": 5183
    },
    {
        "loss": 2.5219,
        "grad_norm": 3.709507703781128,
        "learning_rate": 0.0001883663537858241,
        "epoch": 0.386317907444668,
        "step": 5184
    },
    {
        "loss": 2.1276,
        "grad_norm": 2.420395612716675,
        "learning_rate": 0.0001883333921524513,
        "epoch": 0.38639242864594975,
        "step": 5185
    },
    {
        "loss": 2.6696,
        "grad_norm": 1.8999234437942505,
        "learning_rate": 0.00018830038678232364,
        "epoch": 0.3864669498472315,
        "step": 5186
    },
    {
        "loss": 2.5282,
        "grad_norm": 3.2607781887054443,
        "learning_rate": 0.00018826733769178325,
        "epoch": 0.3865414710485133,
        "step": 5187
    },
    {
        "loss": 2.2208,
        "grad_norm": 3.342247724533081,
        "learning_rate": 0.0001882342448971937,
        "epoch": 0.3866159922497951,
        "step": 5188
    },
    {
        "loss": 2.4858,
        "grad_norm": 2.2785866260528564,
        "learning_rate": 0.00018820110841494042,
        "epoch": 0.38669051345107686,
        "step": 5189
    },
    {
        "loss": 2.2812,
        "grad_norm": 2.4040842056274414,
        "learning_rate": 0.00018816792826143036,
        "epoch": 0.3867650346523586,
        "step": 5190
    },
    {
        "loss": 2.2931,
        "grad_norm": 2.45664644241333,
        "learning_rate": 0.00018813470445309204,
        "epoch": 0.3868395558536404,
        "step": 5191
    },
    {
        "loss": 2.8881,
        "grad_norm": 2.6320903301239014,
        "learning_rate": 0.00018810143700637565,
        "epoch": 0.38691407705492215,
        "step": 5192
    },
    {
        "loss": 1.9627,
        "grad_norm": 3.27541184425354,
        "learning_rate": 0.00018806812593775307,
        "epoch": 0.3869885982562039,
        "step": 5193
    },
    {
        "loss": 2.8174,
        "grad_norm": 2.5237698554992676,
        "learning_rate": 0.00018803477126371764,
        "epoch": 0.38706311945748567,
        "step": 5194
    },
    {
        "loss": 1.7052,
        "grad_norm": 3.4541196823120117,
        "learning_rate": 0.0001880013730007844,
        "epoch": 0.38713764065876743,
        "step": 5195
    },
    {
        "loss": 2.6868,
        "grad_norm": 2.3428797721862793,
        "learning_rate": 0.0001879679311654899,
        "epoch": 0.3872121618600492,
        "step": 5196
    },
    {
        "loss": 2.493,
        "grad_norm": 3.9059832096099854,
        "learning_rate": 0.00018793444577439226,
        "epoch": 0.38728668306133096,
        "step": 5197
    },
    {
        "loss": 2.8304,
        "grad_norm": 3.6422104835510254,
        "learning_rate": 0.0001879009168440712,
        "epoch": 0.3873612042626127,
        "step": 5198
    },
    {
        "loss": 1.3564,
        "grad_norm": 2.870110511779785,
        "learning_rate": 0.00018786734439112804,
        "epoch": 0.3874357254638945,
        "step": 5199
    },
    {
        "loss": 3.0749,
        "grad_norm": 4.251691818237305,
        "learning_rate": 0.00018783372843218552,
        "epoch": 0.38751024666517625,
        "step": 5200
    },
    {
        "loss": 1.7083,
        "grad_norm": 4.527686595916748,
        "learning_rate": 0.00018780006898388806,
        "epoch": 0.387584767866458,
        "step": 5201
    },
    {
        "loss": 2.2943,
        "grad_norm": 3.594527006149292,
        "learning_rate": 0.00018776636606290155,
        "epoch": 0.3876592890677398,
        "step": 5202
    },
    {
        "loss": 2.5667,
        "grad_norm": 2.4858944416046143,
        "learning_rate": 0.00018773261968591342,
        "epoch": 0.38773381026902154,
        "step": 5203
    },
    {
        "loss": 2.062,
        "grad_norm": 3.5039477348327637,
        "learning_rate": 0.0001876988298696326,
        "epoch": 0.3878083314703033,
        "step": 5204
    },
    {
        "loss": 2.3124,
        "grad_norm": 2.0401859283447266,
        "learning_rate": 0.00018766499663078952,
        "epoch": 0.38788285267158507,
        "step": 5205
    },
    {
        "loss": 1.8024,
        "grad_norm": 2.070812702178955,
        "learning_rate": 0.00018763111998613608,
        "epoch": 0.38795737387286683,
        "step": 5206
    },
    {
        "loss": 2.9304,
        "grad_norm": 3.5234375,
        "learning_rate": 0.00018759719995244584,
        "epoch": 0.3880318950741486,
        "step": 5207
    },
    {
        "loss": 2.8854,
        "grad_norm": 2.358161687850952,
        "learning_rate": 0.00018756323654651354,
        "epoch": 0.38810641627543035,
        "step": 5208
    },
    {
        "loss": 2.1796,
        "grad_norm": 2.1388728618621826,
        "learning_rate": 0.0001875292297851558,
        "epoch": 0.3881809374767121,
        "step": 5209
    },
    {
        "loss": 2.9371,
        "grad_norm": 3.1120846271514893,
        "learning_rate": 0.0001874951796852103,
        "epoch": 0.3882554586779939,
        "step": 5210
    },
    {
        "loss": 2.5061,
        "grad_norm": 3.7462000846862793,
        "learning_rate": 0.00018746108626353637,
        "epoch": 0.38832997987927564,
        "step": 5211
    },
    {
        "loss": 2.0308,
        "grad_norm": 4.113392353057861,
        "learning_rate": 0.00018742694953701488,
        "epoch": 0.3884045010805574,
        "step": 5212
    },
    {
        "loss": 2.4385,
        "grad_norm": 2.540236711502075,
        "learning_rate": 0.00018739276952254802,
        "epoch": 0.38847902228183917,
        "step": 5213
    },
    {
        "loss": 2.0662,
        "grad_norm": 3.0289833545684814,
        "learning_rate": 0.00018735854623705937,
        "epoch": 0.38855354348312093,
        "step": 5214
    },
    {
        "loss": 2.5649,
        "grad_norm": 2.11348032951355,
        "learning_rate": 0.00018732427969749403,
        "epoch": 0.3886280646844027,
        "step": 5215
    },
    {
        "loss": 2.3562,
        "grad_norm": 2.814838409423828,
        "learning_rate": 0.00018728996992081845,
        "epoch": 0.38870258588568446,
        "step": 5216
    },
    {
        "loss": 2.631,
        "grad_norm": 2.6702768802642822,
        "learning_rate": 0.00018725561692402065,
        "epoch": 0.3887771070869662,
        "step": 5217
    },
    {
        "loss": 2.6909,
        "grad_norm": 2.916348695755005,
        "learning_rate": 0.0001872212207241098,
        "epoch": 0.388851628288248,
        "step": 5218
    },
    {
        "loss": 2.7416,
        "grad_norm": 3.01330304145813,
        "learning_rate": 0.0001871867813381166,
        "epoch": 0.38892614948952975,
        "step": 5219
    },
    {
        "loss": 2.62,
        "grad_norm": 3.9547231197357178,
        "learning_rate": 0.00018715229878309316,
        "epoch": 0.3890006706908115,
        "step": 5220
    },
    {
        "loss": 2.3556,
        "grad_norm": 3.110975503921509,
        "learning_rate": 0.00018711777307611285,
        "epoch": 0.3890751918920933,
        "step": 5221
    },
    {
        "loss": 2.353,
        "grad_norm": 2.8842687606811523,
        "learning_rate": 0.00018708320423427058,
        "epoch": 0.38914971309337504,
        "step": 5222
    },
    {
        "loss": 1.9936,
        "grad_norm": 3.2713820934295654,
        "learning_rate": 0.00018704859227468247,
        "epoch": 0.38922423429465686,
        "step": 5223
    },
    {
        "loss": 2.6216,
        "grad_norm": 2.570457935333252,
        "learning_rate": 0.00018701393721448604,
        "epoch": 0.3892987554959386,
        "step": 5224
    },
    {
        "loss": 2.6558,
        "grad_norm": 2.6941885948181152,
        "learning_rate": 0.00018697923907084007,
        "epoch": 0.3893732766972204,
        "step": 5225
    },
    {
        "loss": 2.2078,
        "grad_norm": 3.341498374938965,
        "learning_rate": 0.00018694449786092485,
        "epoch": 0.38944779789850215,
        "step": 5226
    },
    {
        "loss": 2.9443,
        "grad_norm": 2.972700834274292,
        "learning_rate": 0.00018690971360194186,
        "epoch": 0.3895223190997839,
        "step": 5227
    },
    {
        "loss": 2.3632,
        "grad_norm": 3.4597151279449463,
        "learning_rate": 0.00018687488631111392,
        "epoch": 0.38959684030106567,
        "step": 5228
    },
    {
        "loss": 1.5959,
        "grad_norm": 2.618117094039917,
        "learning_rate": 0.00018684001600568513,
        "epoch": 0.38967136150234744,
        "step": 5229
    },
    {
        "loss": 2.4653,
        "grad_norm": 1.9697293043136597,
        "learning_rate": 0.00018680510270292094,
        "epoch": 0.3897458827036292,
        "step": 5230
    },
    {
        "loss": 2.2342,
        "grad_norm": 3.2528653144836426,
        "learning_rate": 0.0001867701464201081,
        "epoch": 0.38982040390491096,
        "step": 5231
    },
    {
        "loss": 1.7507,
        "grad_norm": 2.932933807373047,
        "learning_rate": 0.0001867351471745546,
        "epoch": 0.3898949251061927,
        "step": 5232
    },
    {
        "loss": 2.2879,
        "grad_norm": 1.742509365081787,
        "learning_rate": 0.0001867001049835897,
        "epoch": 0.3899694463074745,
        "step": 5233
    },
    {
        "loss": 2.7631,
        "grad_norm": 3.5677716732025146,
        "learning_rate": 0.00018666501986456396,
        "epoch": 0.39004396750875625,
        "step": 5234
    },
    {
        "loss": 2.7524,
        "grad_norm": 2.177548408508301,
        "learning_rate": 0.00018662989183484909,
        "epoch": 0.390118488710038,
        "step": 5235
    },
    {
        "loss": 2.1147,
        "grad_norm": 2.1819708347320557,
        "learning_rate": 0.0001865947209118382,
        "epoch": 0.3901930099113198,
        "step": 5236
    },
    {
        "loss": 2.1938,
        "grad_norm": 2.6595141887664795,
        "learning_rate": 0.0001865595071129456,
        "epoch": 0.39026753111260154,
        "step": 5237
    },
    {
        "loss": 1.5701,
        "grad_norm": 4.298093318939209,
        "learning_rate": 0.00018652425045560673,
        "epoch": 0.3903420523138833,
        "step": 5238
    },
    {
        "loss": 2.4825,
        "grad_norm": 3.1137452125549316,
        "learning_rate": 0.0001864889509572783,
        "epoch": 0.39041657351516507,
        "step": 5239
    },
    {
        "loss": 2.7385,
        "grad_norm": 2.995497703552246,
        "learning_rate": 0.00018645360863543827,
        "epoch": 0.39049109471644683,
        "step": 5240
    },
    {
        "loss": 2.9475,
        "grad_norm": 3.9207310676574707,
        "learning_rate": 0.0001864182235075858,
        "epoch": 0.3905656159177286,
        "step": 5241
    },
    {
        "loss": 2.6082,
        "grad_norm": 3.1652119159698486,
        "learning_rate": 0.00018638279559124126,
        "epoch": 0.39064013711901036,
        "step": 5242
    },
    {
        "loss": 2.3544,
        "grad_norm": 4.045443058013916,
        "learning_rate": 0.0001863473249039461,
        "epoch": 0.3907146583202921,
        "step": 5243
    },
    {
        "loss": 1.6593,
        "grad_norm": 3.27750301361084,
        "learning_rate": 0.00018631181146326305,
        "epoch": 0.3907891795215739,
        "step": 5244
    },
    {
        "loss": 2.365,
        "grad_norm": 3.0797085762023926,
        "learning_rate": 0.00018627625528677596,
        "epoch": 0.39086370072285564,
        "step": 5245
    },
    {
        "loss": 2.3664,
        "grad_norm": 2.953524112701416,
        "learning_rate": 0.00018624065639208988,
        "epoch": 0.3909382219241374,
        "step": 5246
    },
    {
        "loss": 2.7868,
        "grad_norm": 2.747450590133667,
        "learning_rate": 0.00018620501479683097,
        "epoch": 0.39101274312541917,
        "step": 5247
    },
    {
        "loss": 2.4241,
        "grad_norm": 1.874860405921936,
        "learning_rate": 0.0001861693305186466,
        "epoch": 0.39108726432670093,
        "step": 5248
    },
    {
        "loss": 2.5769,
        "grad_norm": 1.9784406423568726,
        "learning_rate": 0.00018613360357520512,
        "epoch": 0.3911617855279827,
        "step": 5249
    },
    {
        "loss": 1.6209,
        "grad_norm": 3.795510768890381,
        "learning_rate": 0.00018609783398419621,
        "epoch": 0.39123630672926446,
        "step": 5250
    },
    {
        "loss": 1.9616,
        "grad_norm": 1.8474642038345337,
        "learning_rate": 0.00018606202176333055,
        "epoch": 0.3913108279305462,
        "step": 5251
    },
    {
        "loss": 2.0005,
        "grad_norm": 2.944169044494629,
        "learning_rate": 0.00018602616693033988,
        "epoch": 0.391385349131828,
        "step": 5252
    },
    {
        "loss": 3.021,
        "grad_norm": 4.070009231567383,
        "learning_rate": 0.00018599026950297714,
        "epoch": 0.39145987033310975,
        "step": 5253
    },
    {
        "loss": 2.7338,
        "grad_norm": 2.5078964233398438,
        "learning_rate": 0.0001859543294990164,
        "epoch": 0.3915343915343915,
        "step": 5254
    },
    {
        "loss": 1.73,
        "grad_norm": 4.242542266845703,
        "learning_rate": 0.00018591834693625257,
        "epoch": 0.3916089127356733,
        "step": 5255
    },
    {
        "loss": 2.2943,
        "grad_norm": 2.848792791366577,
        "learning_rate": 0.00018588232183250195,
        "epoch": 0.39168343393695504,
        "step": 5256
    },
    {
        "loss": 2.758,
        "grad_norm": 2.3979365825653076,
        "learning_rate": 0.00018584625420560165,
        "epoch": 0.3917579551382368,
        "step": 5257
    },
    {
        "loss": 2.6993,
        "grad_norm": 2.010253429412842,
        "learning_rate": 0.00018581014407341,
        "epoch": 0.3918324763395186,
        "step": 5258
    },
    {
        "loss": 2.2984,
        "grad_norm": 2.55226731300354,
        "learning_rate": 0.00018577399145380623,
        "epoch": 0.3919069975408004,
        "step": 5259
    },
    {
        "loss": 2.7103,
        "grad_norm": 2.446108818054199,
        "learning_rate": 0.00018573779636469072,
        "epoch": 0.39198151874208215,
        "step": 5260
    },
    {
        "loss": 1.9267,
        "grad_norm": 2.9698710441589355,
        "learning_rate": 0.0001857015588239849,
        "epoch": 0.3920560399433639,
        "step": 5261
    },
    {
        "loss": 2.5521,
        "grad_norm": 4.410268783569336,
        "learning_rate": 0.0001856652788496311,
        "epoch": 0.3921305611446457,
        "step": 5262
    },
    {
        "loss": 2.4892,
        "grad_norm": 4.613513469696045,
        "learning_rate": 0.00018562895645959273,
        "epoch": 0.39220508234592744,
        "step": 5263
    },
    {
        "loss": 2.1596,
        "grad_norm": 3.2907159328460693,
        "learning_rate": 0.00018559259167185422,
        "epoch": 0.3922796035472092,
        "step": 5264
    },
    {
        "loss": 2.3365,
        "grad_norm": 3.035719871520996,
        "learning_rate": 0.00018555618450442096,
        "epoch": 0.39235412474849096,
        "step": 5265
    },
    {
        "loss": 2.6629,
        "grad_norm": 2.3670544624328613,
        "learning_rate": 0.0001855197349753193,
        "epoch": 0.3924286459497727,
        "step": 5266
    },
    {
        "loss": 1.8195,
        "grad_norm": 3.025113821029663,
        "learning_rate": 0.0001854832431025966,
        "epoch": 0.3925031671510545,
        "step": 5267
    },
    {
        "loss": 1.8007,
        "grad_norm": 3.1851770877838135,
        "learning_rate": 0.00018544670890432126,
        "epoch": 0.39257768835233625,
        "step": 5268
    },
    {
        "loss": 2.7253,
        "grad_norm": 2.295851707458496,
        "learning_rate": 0.00018541013239858244,
        "epoch": 0.392652209553618,
        "step": 5269
    },
    {
        "loss": 2.1926,
        "grad_norm": 4.876350402832031,
        "learning_rate": 0.00018537351360349046,
        "epoch": 0.3927267307548998,
        "step": 5270
    },
    {
        "loss": 2.4972,
        "grad_norm": 3.5085337162017822,
        "learning_rate": 0.00018533685253717644,
        "epoch": 0.39280125195618154,
        "step": 5271
    },
    {
        "loss": 2.2161,
        "grad_norm": 4.297602653503418,
        "learning_rate": 0.00018530014921779245,
        "epoch": 0.3928757731574633,
        "step": 5272
    },
    {
        "loss": 2.0945,
        "grad_norm": 4.885307312011719,
        "learning_rate": 0.00018526340366351157,
        "epoch": 0.39295029435874507,
        "step": 5273
    },
    {
        "loss": 2.2371,
        "grad_norm": 2.9628305435180664,
        "learning_rate": 0.00018522661589252768,
        "epoch": 0.39302481556002683,
        "step": 5274
    },
    {
        "loss": 2.8428,
        "grad_norm": 2.2045536041259766,
        "learning_rate": 0.00018518978592305566,
        "epoch": 0.3930993367613086,
        "step": 5275
    },
    {
        "loss": 2.4395,
        "grad_norm": 2.1367294788360596,
        "learning_rate": 0.00018515291377333115,
        "epoch": 0.39317385796259036,
        "step": 5276
    },
    {
        "loss": 2.832,
        "grad_norm": 2.889763832092285,
        "learning_rate": 0.0001851159994616108,
        "epoch": 0.3932483791638721,
        "step": 5277
    },
    {
        "loss": 2.7054,
        "grad_norm": 1.7163625955581665,
        "learning_rate": 0.00018507904300617213,
        "epoch": 0.3933229003651539,
        "step": 5278
    },
    {
        "loss": 2.6485,
        "grad_norm": 3.0311341285705566,
        "learning_rate": 0.0001850420444253135,
        "epoch": 0.39339742156643565,
        "step": 5279
    },
    {
        "loss": 2.6122,
        "grad_norm": 1.834803819656372,
        "learning_rate": 0.000185005003737354,
        "epoch": 0.3934719427677174,
        "step": 5280
    },
    {
        "loss": 2.4596,
        "grad_norm": 2.9488701820373535,
        "learning_rate": 0.0001849679209606338,
        "epoch": 0.39354646396899917,
        "step": 5281
    },
    {
        "loss": 2.6366,
        "grad_norm": 3.0075860023498535,
        "learning_rate": 0.00018493079611351377,
        "epoch": 0.39362098517028093,
        "step": 5282
    },
    {
        "loss": 2.1936,
        "grad_norm": 2.205334424972534,
        "learning_rate": 0.0001848936292143756,
        "epoch": 0.3936955063715627,
        "step": 5283
    },
    {
        "loss": 2.6535,
        "grad_norm": 2.319295883178711,
        "learning_rate": 0.00018485642028162193,
        "epoch": 0.39377002757284446,
        "step": 5284
    },
    {
        "loss": 1.9967,
        "grad_norm": 3.218082904815674,
        "learning_rate": 0.00018481916933367606,
        "epoch": 0.3938445487741262,
        "step": 5285
    },
    {
        "loss": 2.8023,
        "grad_norm": 1.834290862083435,
        "learning_rate": 0.00018478187638898213,
        "epoch": 0.393919069975408,
        "step": 5286
    },
    {
        "loss": 2.1276,
        "grad_norm": 3.83811616897583,
        "learning_rate": 0.0001847445414660051,
        "epoch": 0.39399359117668975,
        "step": 5287
    },
    {
        "loss": 1.9786,
        "grad_norm": 2.6760642528533936,
        "learning_rate": 0.0001847071645832308,
        "epoch": 0.3940681123779715,
        "step": 5288
    },
    {
        "loss": 1.9879,
        "grad_norm": 5.9214653968811035,
        "learning_rate": 0.00018466974575916565,
        "epoch": 0.3941426335792533,
        "step": 5289
    },
    {
        "loss": 2.8481,
        "grad_norm": 3.3588154315948486,
        "learning_rate": 0.00018463228501233695,
        "epoch": 0.39421715478053504,
        "step": 5290
    },
    {
        "loss": 2.8262,
        "grad_norm": 2.0651662349700928,
        "learning_rate": 0.0001845947823612928,
        "epoch": 0.3942916759818168,
        "step": 5291
    },
    {
        "loss": 2.5759,
        "grad_norm": 3.1509833335876465,
        "learning_rate": 0.0001845572378246019,
        "epoch": 0.39436619718309857,
        "step": 5292
    },
    {
        "loss": 2.5448,
        "grad_norm": 2.7259409427642822,
        "learning_rate": 0.0001845196514208539,
        "epoch": 0.39444071838438033,
        "step": 5293
    },
    {
        "loss": 2.321,
        "grad_norm": 4.122546195983887,
        "learning_rate": 0.00018448202316865894,
        "epoch": 0.39451523958566215,
        "step": 5294
    },
    {
        "loss": 1.4543,
        "grad_norm": 2.862457036972046,
        "learning_rate": 0.0001844443530866481,
        "epoch": 0.3945897607869439,
        "step": 5295
    },
    {
        "loss": 2.5691,
        "grad_norm": 2.2409727573394775,
        "learning_rate": 0.000184406641193473,
        "epoch": 0.3946642819882257,
        "step": 5296
    },
    {
        "loss": 2.1278,
        "grad_norm": 2.3885467052459717,
        "learning_rate": 0.00018436888750780604,
        "epoch": 0.39473880318950744,
        "step": 5297
    },
    {
        "loss": 2.6739,
        "grad_norm": 2.9630069732666016,
        "learning_rate": 0.00018433109204834037,
        "epoch": 0.3948133243907892,
        "step": 5298
    },
    {
        "loss": 2.8008,
        "grad_norm": 2.9654178619384766,
        "learning_rate": 0.00018429325483378968,
        "epoch": 0.39488784559207096,
        "step": 5299
    },
    {
        "loss": 2.6342,
        "grad_norm": 2.773914337158203,
        "learning_rate": 0.00018425537588288843,
        "epoch": 0.3949623667933527,
        "step": 5300
    },
    {
        "loss": 2.5883,
        "grad_norm": 3.8329057693481445,
        "learning_rate": 0.00018421745521439176,
        "epoch": 0.3950368879946345,
        "step": 5301
    },
    {
        "loss": 1.9762,
        "grad_norm": 3.1626996994018555,
        "learning_rate": 0.00018417949284707538,
        "epoch": 0.39511140919591625,
        "step": 5302
    },
    {
        "loss": 2.299,
        "grad_norm": 3.7958974838256836,
        "learning_rate": 0.00018414148879973582,
        "epoch": 0.395185930397198,
        "step": 5303
    },
    {
        "loss": 2.5116,
        "grad_norm": 2.224369525909424,
        "learning_rate": 0.00018410344309118998,
        "epoch": 0.3952604515984798,
        "step": 5304
    },
    {
        "loss": 2.3891,
        "grad_norm": 3.4924678802490234,
        "learning_rate": 0.00018406535574027566,
        "epoch": 0.39533497279976154,
        "step": 5305
    },
    {
        "loss": 2.6483,
        "grad_norm": 2.5728790760040283,
        "learning_rate": 0.00018402722676585107,
        "epoch": 0.3954094940010433,
        "step": 5306
    },
    {
        "loss": 2.4506,
        "grad_norm": 2.0442347526550293,
        "learning_rate": 0.00018398905618679514,
        "epoch": 0.39548401520232507,
        "step": 5307
    },
    {
        "loss": 2.3328,
        "grad_norm": 2.749600410461426,
        "learning_rate": 0.00018395084402200737,
        "epoch": 0.39555853640360683,
        "step": 5308
    },
    {
        "loss": 2.4991,
        "grad_norm": 2.2112019062042236,
        "learning_rate": 0.0001839125902904079,
        "epoch": 0.3956330576048886,
        "step": 5309
    },
    {
        "loss": 1.6387,
        "grad_norm": 3.3094589710235596,
        "learning_rate": 0.00018387429501093733,
        "epoch": 0.39570757880617036,
        "step": 5310
    },
    {
        "loss": 2.096,
        "grad_norm": 3.3471519947052,
        "learning_rate": 0.00018383595820255694,
        "epoch": 0.3957821000074521,
        "step": 5311
    },
    {
        "loss": 2.3014,
        "grad_norm": 3.1303889751434326,
        "learning_rate": 0.00018379757988424856,
        "epoch": 0.3958566212087339,
        "step": 5312
    },
    {
        "loss": 2.1277,
        "grad_norm": 2.162506103515625,
        "learning_rate": 0.00018375916007501452,
        "epoch": 0.39593114241001565,
        "step": 5313
    },
    {
        "loss": 2.5166,
        "grad_norm": 2.306422233581543,
        "learning_rate": 0.00018372069879387773,
        "epoch": 0.3960056636112974,
        "step": 5314
    },
    {
        "loss": 2.4757,
        "grad_norm": 2.1024060249328613,
        "learning_rate": 0.00018368219605988166,
        "epoch": 0.3960801848125792,
        "step": 5315
    },
    {
        "loss": 2.7714,
        "grad_norm": 2.692490816116333,
        "learning_rate": 0.00018364365189209024,
        "epoch": 0.39615470601386094,
        "step": 5316
    },
    {
        "loss": 1.9899,
        "grad_norm": 2.8613107204437256,
        "learning_rate": 0.000183605066309588,
        "epoch": 0.3962292272151427,
        "step": 5317
    },
    {
        "loss": 2.3903,
        "grad_norm": 3.063992500305176,
        "learning_rate": 0.00018356643933147986,
        "epoch": 0.39630374841642446,
        "step": 5318
    },
    {
        "loss": 2.2693,
        "grad_norm": 3.5181217193603516,
        "learning_rate": 0.00018352777097689133,
        "epoch": 0.3963782696177062,
        "step": 5319
    },
    {
        "loss": 1.9284,
        "grad_norm": 4.477797031402588,
        "learning_rate": 0.00018348906126496836,
        "epoch": 0.396452790818988,
        "step": 5320
    },
    {
        "loss": 1.974,
        "grad_norm": 2.8145742416381836,
        "learning_rate": 0.00018345031021487743,
        "epoch": 0.39652731202026975,
        "step": 5321
    },
    {
        "loss": 2.5006,
        "grad_norm": 2.690319061279297,
        "learning_rate": 0.00018341151784580543,
        "epoch": 0.3966018332215515,
        "step": 5322
    },
    {
        "loss": 2.036,
        "grad_norm": 3.395038366317749,
        "learning_rate": 0.00018337268417695975,
        "epoch": 0.3966763544228333,
        "step": 5323
    },
    {
        "loss": 1.9214,
        "grad_norm": 3.8303050994873047,
        "learning_rate": 0.0001833338092275682,
        "epoch": 0.39675087562411504,
        "step": 5324
    },
    {
        "loss": 1.4222,
        "grad_norm": 2.97943115234375,
        "learning_rate": 0.00018329489301687903,
        "epoch": 0.3968253968253968,
        "step": 5325
    },
    {
        "loss": 2.4289,
        "grad_norm": 3.1861989498138428,
        "learning_rate": 0.00018325593556416098,
        "epoch": 0.39689991802667857,
        "step": 5326
    },
    {
        "loss": 1.2073,
        "grad_norm": 3.575453519821167,
        "learning_rate": 0.00018321693688870308,
        "epoch": 0.39697443922796033,
        "step": 5327
    },
    {
        "loss": 2.6313,
        "grad_norm": 3.0000669956207275,
        "learning_rate": 0.0001831778970098149,
        "epoch": 0.3970489604292421,
        "step": 5328
    },
    {
        "loss": 2.6055,
        "grad_norm": 3.4045655727386475,
        "learning_rate": 0.00018313881594682636,
        "epoch": 0.3971234816305239,
        "step": 5329
    },
    {
        "loss": 2.291,
        "grad_norm": 2.0508880615234375,
        "learning_rate": 0.00018309969371908775,
        "epoch": 0.3971980028318057,
        "step": 5330
    },
    {
        "loss": 1.5629,
        "grad_norm": 0.989662230014801,
        "learning_rate": 0.0001830605303459698,
        "epoch": 0.39727252403308744,
        "step": 5331
    },
    {
        "loss": 2.6922,
        "grad_norm": 2.535886526107788,
        "learning_rate": 0.00018302132584686358,
        "epoch": 0.3973470452343692,
        "step": 5332
    },
    {
        "loss": 2.7686,
        "grad_norm": 2.555830478668213,
        "learning_rate": 0.0001829820802411805,
        "epoch": 0.39742156643565096,
        "step": 5333
    },
    {
        "loss": 2.5249,
        "grad_norm": 2.012049436569214,
        "learning_rate": 0.0001829427935483523,
        "epoch": 0.3974960876369327,
        "step": 5334
    },
    {
        "loss": 2.7078,
        "grad_norm": 2.2952606678009033,
        "learning_rate": 0.00018290346578783123,
        "epoch": 0.3975706088382145,
        "step": 5335
    },
    {
        "loss": 3.0049,
        "grad_norm": 2.7718210220336914,
        "learning_rate": 0.00018286409697908967,
        "epoch": 0.39764513003949625,
        "step": 5336
    },
    {
        "loss": 2.4616,
        "grad_norm": 3.003955125808716,
        "learning_rate": 0.00018282468714162037,
        "epoch": 0.397719651240778,
        "step": 5337
    },
    {
        "loss": 2.5942,
        "grad_norm": 3.5379858016967773,
        "learning_rate": 0.00018278523629493648,
        "epoch": 0.3977941724420598,
        "step": 5338
    },
    {
        "loss": 3.1019,
        "grad_norm": 1.918988823890686,
        "learning_rate": 0.00018274574445857145,
        "epoch": 0.39786869364334154,
        "step": 5339
    },
    {
        "loss": 2.3612,
        "grad_norm": 2.2362499237060547,
        "learning_rate": 0.00018270621165207893,
        "epoch": 0.3979432148446233,
        "step": 5340
    },
    {
        "loss": 2.9137,
        "grad_norm": 2.3152658939361572,
        "learning_rate": 0.00018266663789503289,
        "epoch": 0.39801773604590507,
        "step": 5341
    },
    {
        "loss": 2.2726,
        "grad_norm": 2.6021533012390137,
        "learning_rate": 0.0001826270232070276,
        "epoch": 0.39809225724718683,
        "step": 5342
    },
    {
        "loss": 2.726,
        "grad_norm": 2.419593572616577,
        "learning_rate": 0.0001825873676076776,
        "epoch": 0.3981667784484686,
        "step": 5343
    },
    {
        "loss": 2.7215,
        "grad_norm": 1.8597956895828247,
        "learning_rate": 0.00018254767111661765,
        "epoch": 0.39824129964975036,
        "step": 5344
    },
    {
        "loss": 2.1075,
        "grad_norm": 3.988492965698242,
        "learning_rate": 0.0001825079337535029,
        "epoch": 0.3983158208510321,
        "step": 5345
    },
    {
        "loss": 1.9378,
        "grad_norm": 4.039486408233643,
        "learning_rate": 0.0001824681555380085,
        "epoch": 0.3983903420523139,
        "step": 5346
    },
    {
        "loss": 1.9778,
        "grad_norm": 1.8164315223693848,
        "learning_rate": 0.00018242833648982995,
        "epoch": 0.39846486325359565,
        "step": 5347
    },
    {
        "loss": 2.0007,
        "grad_norm": 3.4881114959716797,
        "learning_rate": 0.00018238847662868296,
        "epoch": 0.3985393844548774,
        "step": 5348
    },
    {
        "loss": 2.09,
        "grad_norm": 3.213987350463867,
        "learning_rate": 0.00018234857597430358,
        "epoch": 0.3986139056561592,
        "step": 5349
    },
    {
        "loss": 2.2507,
        "grad_norm": 4.18733024597168,
        "learning_rate": 0.0001823086345464478,
        "epoch": 0.39868842685744094,
        "step": 5350
    },
    {
        "loss": 2.6482,
        "grad_norm": 2.5281624794006348,
        "learning_rate": 0.00018226865236489195,
        "epoch": 0.3987629480587227,
        "step": 5351
    },
    {
        "loss": 2.625,
        "grad_norm": 2.658214807510376,
        "learning_rate": 0.0001822286294494325,
        "epoch": 0.39883746926000446,
        "step": 5352
    },
    {
        "loss": 2.7265,
        "grad_norm": 2.444286584854126,
        "learning_rate": 0.00018218856581988615,
        "epoch": 0.3989119904612862,
        "step": 5353
    },
    {
        "loss": 2.606,
        "grad_norm": 2.4347357749938965,
        "learning_rate": 0.0001821484614960897,
        "epoch": 0.398986511662568,
        "step": 5354
    },
    {
        "loss": 2.2464,
        "grad_norm": 3.6925618648529053,
        "learning_rate": 0.00018210831649790015,
        "epoch": 0.39906103286384975,
        "step": 5355
    },
    {
        "loss": 2.3324,
        "grad_norm": 2.5677173137664795,
        "learning_rate": 0.0001820681308451946,
        "epoch": 0.3991355540651315,
        "step": 5356
    },
    {
        "loss": 2.8087,
        "grad_norm": 2.028047561645508,
        "learning_rate": 0.00018202790455787017,
        "epoch": 0.3992100752664133,
        "step": 5357
    },
    {
        "loss": 1.2157,
        "grad_norm": 4.099404811859131,
        "learning_rate": 0.0001819876376558443,
        "epoch": 0.39928459646769504,
        "step": 5358
    },
    {
        "loss": 2.0825,
        "grad_norm": 2.931973695755005,
        "learning_rate": 0.0001819473301590545,
        "epoch": 0.3993591176689768,
        "step": 5359
    },
    {
        "loss": 2.47,
        "grad_norm": 3.0526058673858643,
        "learning_rate": 0.00018190698208745828,
        "epoch": 0.39943363887025857,
        "step": 5360
    },
    {
        "loss": 1.6149,
        "grad_norm": 2.0780770778656006,
        "learning_rate": 0.00018186659346103326,
        "epoch": 0.39950816007154033,
        "step": 5361
    },
    {
        "loss": 2.7995,
        "grad_norm": 2.741532325744629,
        "learning_rate": 0.00018182616429977716,
        "epoch": 0.3995826812728221,
        "step": 5362
    },
    {
        "loss": 2.0511,
        "grad_norm": 2.897099256515503,
        "learning_rate": 0.00018178569462370785,
        "epoch": 0.39965720247410386,
        "step": 5363
    },
    {
        "loss": 2.5258,
        "grad_norm": 3.0516505241394043,
        "learning_rate": 0.0001817451844528632,
        "epoch": 0.3997317236753856,
        "step": 5364
    },
    {
        "loss": 2.1895,
        "grad_norm": 4.404052734375,
        "learning_rate": 0.00018170463380730103,
        "epoch": 0.39980624487666744,
        "step": 5365
    },
    {
        "loss": 2.1748,
        "grad_norm": 2.6228487491607666,
        "learning_rate": 0.0001816640427070994,
        "epoch": 0.3998807660779492,
        "step": 5366
    },
    {
        "loss": 2.59,
        "grad_norm": 2.573549509048462,
        "learning_rate": 0.0001816234111723562,
        "epoch": 0.39995528727923096,
        "step": 5367
    },
    {
        "loss": 2.3553,
        "grad_norm": 4.425942897796631,
        "learning_rate": 0.00018158273922318948,
        "epoch": 0.4000298084805127,
        "step": 5368
    },
    {
        "loss": 2.4541,
        "grad_norm": 2.963618278503418,
        "learning_rate": 0.00018154202687973724,
        "epoch": 0.4001043296817945,
        "step": 5369
    },
    {
        "loss": 2.7502,
        "grad_norm": 3.257591962814331,
        "learning_rate": 0.00018150127416215756,
        "epoch": 0.40017885088307625,
        "step": 5370
    },
    {
        "loss": 2.8328,
        "grad_norm": 1.8235437870025635,
        "learning_rate": 0.00018146048109062831,
        "epoch": 0.400253372084358,
        "step": 5371
    },
    {
        "loss": 2.0204,
        "grad_norm": 2.7304677963256836,
        "learning_rate": 0.0001814196476853476,
        "epoch": 0.4003278932856398,
        "step": 5372
    },
    {
        "loss": 1.9722,
        "grad_norm": 3.061192512512207,
        "learning_rate": 0.0001813787739665333,
        "epoch": 0.40040241448692154,
        "step": 5373
    },
    {
        "loss": 2.7525,
        "grad_norm": 1.5460957288742065,
        "learning_rate": 0.00018133785995442337,
        "epoch": 0.4004769356882033,
        "step": 5374
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.8952388763427734,
        "learning_rate": 0.00018129690566927567,
        "epoch": 0.40055145688948507,
        "step": 5375
    },
    {
        "loss": 2.6488,
        "grad_norm": 2.553605556488037,
        "learning_rate": 0.00018125591113136806,
        "epoch": 0.40062597809076683,
        "step": 5376
    },
    {
        "loss": 2.2175,
        "grad_norm": 3.6280713081359863,
        "learning_rate": 0.00018121487636099816,
        "epoch": 0.4007004992920486,
        "step": 5377
    },
    {
        "loss": 2.4227,
        "grad_norm": 2.8172311782836914,
        "learning_rate": 0.00018117380137848375,
        "epoch": 0.40077502049333036,
        "step": 5378
    },
    {
        "loss": 2.2544,
        "grad_norm": 3.1435372829437256,
        "learning_rate": 0.0001811326862041623,
        "epoch": 0.4008495416946121,
        "step": 5379
    },
    {
        "loss": 2.9438,
        "grad_norm": 4.020564556121826,
        "learning_rate": 0.00018109153085839138,
        "epoch": 0.4009240628958939,
        "step": 5380
    },
    {
        "loss": 2.3002,
        "grad_norm": 3.653285026550293,
        "learning_rate": 0.00018105033536154822,
        "epoch": 0.40099858409717565,
        "step": 5381
    },
    {
        "loss": 2.3555,
        "grad_norm": 3.103956460952759,
        "learning_rate": 0.00018100909973403019,
        "epoch": 0.4010731052984574,
        "step": 5382
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.104454278945923,
        "learning_rate": 0.00018096782399625432,
        "epoch": 0.4011476264997392,
        "step": 5383
    },
    {
        "loss": 1.8736,
        "grad_norm": 3.9106762409210205,
        "learning_rate": 0.00018092650816865758,
        "epoch": 0.40122214770102094,
        "step": 5384
    },
    {
        "loss": 2.3393,
        "grad_norm": 3.0912528038024902,
        "learning_rate": 0.0001808851522716968,
        "epoch": 0.4012966689023027,
        "step": 5385
    },
    {
        "loss": 2.4542,
        "grad_norm": 3.085874080657959,
        "learning_rate": 0.00018084375632584871,
        "epoch": 0.40137119010358446,
        "step": 5386
    },
    {
        "loss": 2.312,
        "grad_norm": 2.7743914127349854,
        "learning_rate": 0.00018080232035160972,
        "epoch": 0.4014457113048662,
        "step": 5387
    },
    {
        "loss": 2.2274,
        "grad_norm": 2.908522129058838,
        "learning_rate": 0.0001807608443694961,
        "epoch": 0.401520232506148,
        "step": 5388
    },
    {
        "loss": 2.7877,
        "grad_norm": 1.8540462255477905,
        "learning_rate": 0.00018071932840004406,
        "epoch": 0.40159475370742975,
        "step": 5389
    },
    {
        "loss": 2.167,
        "grad_norm": 2.7922816276550293,
        "learning_rate": 0.00018067777246380952,
        "epoch": 0.4016692749087115,
        "step": 5390
    },
    {
        "loss": 2.8679,
        "grad_norm": 2.134305477142334,
        "learning_rate": 0.00018063617658136807,
        "epoch": 0.4017437961099933,
        "step": 5391
    },
    {
        "loss": 1.9757,
        "grad_norm": 3.3699333667755127,
        "learning_rate": 0.0001805945407733153,
        "epoch": 0.40181831731127504,
        "step": 5392
    },
    {
        "loss": 1.6446,
        "grad_norm": 2.508554220199585,
        "learning_rate": 0.0001805528650602664,
        "epoch": 0.4018928385125568,
        "step": 5393
    },
    {
        "loss": 2.1221,
        "grad_norm": 1.7260377407073975,
        "learning_rate": 0.0001805111494628564,
        "epoch": 0.40196735971383857,
        "step": 5394
    },
    {
        "loss": 1.7718,
        "grad_norm": 3.5420889854431152,
        "learning_rate": 0.00018046939400174008,
        "epoch": 0.40204188091512033,
        "step": 5395
    },
    {
        "loss": 3.0519,
        "grad_norm": 4.675795555114746,
        "learning_rate": 0.00018042759869759195,
        "epoch": 0.4021164021164021,
        "step": 5396
    },
    {
        "loss": 2.0192,
        "grad_norm": 3.390855312347412,
        "learning_rate": 0.00018038576357110622,
        "epoch": 0.40219092331768386,
        "step": 5397
    },
    {
        "loss": 2.6133,
        "grad_norm": 2.1202688217163086,
        "learning_rate": 0.00018034388864299677,
        "epoch": 0.4022654445189656,
        "step": 5398
    },
    {
        "loss": 1.4411,
        "grad_norm": 5.6485466957092285,
        "learning_rate": 0.0001803019739339973,
        "epoch": 0.4023399657202474,
        "step": 5399
    },
    {
        "loss": 2.403,
        "grad_norm": 4.111351490020752,
        "learning_rate": 0.00018026001946486116,
        "epoch": 0.4024144869215292,
        "step": 5400
    },
    {
        "loss": 2.0826,
        "grad_norm": 3.1971051692962646,
        "learning_rate": 0.00018021802525636135,
        "epoch": 0.40248900812281097,
        "step": 5401
    },
    {
        "loss": 2.5785,
        "grad_norm": 1.9724888801574707,
        "learning_rate": 0.00018017599132929063,
        "epoch": 0.40256352932409273,
        "step": 5402
    },
    {
        "loss": 2.7215,
        "grad_norm": 4.170967102050781,
        "learning_rate": 0.0001801339177044613,
        "epoch": 0.4026380505253745,
        "step": 5403
    },
    {
        "loss": 2.7109,
        "grad_norm": 2.933627128601074,
        "learning_rate": 0.00018009180440270542,
        "epoch": 0.40271257172665625,
        "step": 5404
    },
    {
        "loss": 2.8019,
        "grad_norm": 4.082972526550293,
        "learning_rate": 0.0001800496514448747,
        "epoch": 0.402787092927938,
        "step": 5405
    },
    {
        "loss": 2.3859,
        "grad_norm": 3.2924020290374756,
        "learning_rate": 0.00018000745885184046,
        "epoch": 0.4028616141292198,
        "step": 5406
    },
    {
        "loss": 2.7998,
        "grad_norm": 2.5496811866760254,
        "learning_rate": 0.0001799652266444936,
        "epoch": 0.40293613533050154,
        "step": 5407
    },
    {
        "loss": 2.3335,
        "grad_norm": 2.507905960083008,
        "learning_rate": 0.00017992295484374463,
        "epoch": 0.4030106565317833,
        "step": 5408
    },
    {
        "loss": 2.7974,
        "grad_norm": 2.3316361904144287,
        "learning_rate": 0.00017988064347052374,
        "epoch": 0.40308517773306507,
        "step": 5409
    },
    {
        "loss": 2.0437,
        "grad_norm": 2.648474931716919,
        "learning_rate": 0.0001798382925457807,
        "epoch": 0.40315969893434683,
        "step": 5410
    },
    {
        "loss": 2.8116,
        "grad_norm": 2.3002307415008545,
        "learning_rate": 0.00017979590209048484,
        "epoch": 0.4032342201356286,
        "step": 5411
    },
    {
        "loss": 2.7998,
        "grad_norm": 3.0059871673583984,
        "learning_rate": 0.000179753472125625,
        "epoch": 0.40330874133691036,
        "step": 5412
    },
    {
        "loss": 2.647,
        "grad_norm": 1.839788556098938,
        "learning_rate": 0.00017971100267220966,
        "epoch": 0.4033832625381921,
        "step": 5413
    },
    {
        "loss": 1.7484,
        "grad_norm": 4.783387184143066,
        "learning_rate": 0.00017966849375126688,
        "epoch": 0.4034577837394739,
        "step": 5414
    },
    {
        "loss": 2.9459,
        "grad_norm": 3.436189889907837,
        "learning_rate": 0.0001796259453838442,
        "epoch": 0.40353230494075565,
        "step": 5415
    },
    {
        "loss": 1.6786,
        "grad_norm": 3.8558900356292725,
        "learning_rate": 0.00017958335759100873,
        "epoch": 0.4036068261420374,
        "step": 5416
    },
    {
        "loss": 2.5407,
        "grad_norm": 2.169213056564331,
        "learning_rate": 0.00017954073039384708,
        "epoch": 0.4036813473433192,
        "step": 5417
    },
    {
        "loss": 2.1422,
        "grad_norm": 3.2211647033691406,
        "learning_rate": 0.0001794980638134653,
        "epoch": 0.40375586854460094,
        "step": 5418
    },
    {
        "loss": 2.798,
        "grad_norm": 2.387758255004883,
        "learning_rate": 0.00017945535787098905,
        "epoch": 0.4038303897458827,
        "step": 5419
    },
    {
        "loss": 1.5228,
        "grad_norm": 1.1934489011764526,
        "learning_rate": 0.00017941261258756352,
        "epoch": 0.40390491094716446,
        "step": 5420
    },
    {
        "loss": 2.7557,
        "grad_norm": 2.7505104541778564,
        "learning_rate": 0.00017936982798435325,
        "epoch": 0.4039794321484462,
        "step": 5421
    },
    {
        "loss": 2.506,
        "grad_norm": 2.819441318511963,
        "learning_rate": 0.0001793270040825422,
        "epoch": 0.404053953349728,
        "step": 5422
    },
    {
        "loss": 1.6303,
        "grad_norm": 4.150768280029297,
        "learning_rate": 0.00017928414090333396,
        "epoch": 0.40412847455100975,
        "step": 5423
    },
    {
        "loss": 2.3966,
        "grad_norm": 1.7921056747436523,
        "learning_rate": 0.00017924123846795154,
        "epoch": 0.4042029957522915,
        "step": 5424
    },
    {
        "loss": 1.0137,
        "grad_norm": 1.8687793016433716,
        "learning_rate": 0.00017919829679763732,
        "epoch": 0.4042775169535733,
        "step": 5425
    },
    {
        "loss": 1.9846,
        "grad_norm": 3.115647554397583,
        "learning_rate": 0.00017915531591365305,
        "epoch": 0.40435203815485504,
        "step": 5426
    },
    {
        "loss": 2.4671,
        "grad_norm": 3.3781163692474365,
        "learning_rate": 0.0001791122958372801,
        "epoch": 0.4044265593561368,
        "step": 5427
    },
    {
        "loss": 1.9146,
        "grad_norm": 3.5256900787353516,
        "learning_rate": 0.000179069236589819,
        "epoch": 0.40450108055741857,
        "step": 5428
    },
    {
        "loss": 2.3982,
        "grad_norm": 4.502908706665039,
        "learning_rate": 0.00017902613819258982,
        "epoch": 0.40457560175870033,
        "step": 5429
    },
    {
        "loss": 1.3572,
        "grad_norm": 4.932766914367676,
        "learning_rate": 0.00017898300066693205,
        "epoch": 0.4046501229599821,
        "step": 5430
    },
    {
        "loss": 2.7058,
        "grad_norm": 3.409816265106201,
        "learning_rate": 0.00017893982403420448,
        "epoch": 0.40472464416126386,
        "step": 5431
    },
    {
        "loss": 1.9011,
        "grad_norm": 3.7911806106567383,
        "learning_rate": 0.0001788966083157852,
        "epoch": 0.4047991653625456,
        "step": 5432
    },
    {
        "loss": 2.1302,
        "grad_norm": 3.4584059715270996,
        "learning_rate": 0.00017885335353307177,
        "epoch": 0.4048736865638274,
        "step": 5433
    },
    {
        "loss": 2.3221,
        "grad_norm": 4.374067306518555,
        "learning_rate": 0.0001788100597074811,
        "epoch": 0.40494820776510915,
        "step": 5434
    },
    {
        "loss": 3.0566,
        "grad_norm": 2.0332143306732178,
        "learning_rate": 0.0001787667268604493,
        "epoch": 0.40502272896639097,
        "step": 5435
    },
    {
        "loss": 2.6116,
        "grad_norm": 2.958082437515259,
        "learning_rate": 0.00017872335501343186,
        "epoch": 0.40509725016767273,
        "step": 5436
    },
    {
        "loss": 2.2468,
        "grad_norm": 3.5111422538757324,
        "learning_rate": 0.00017867994418790375,
        "epoch": 0.4051717713689545,
        "step": 5437
    },
    {
        "loss": 2.1488,
        "grad_norm": 2.941838502883911,
        "learning_rate": 0.00017863649440535894,
        "epoch": 0.40524629257023626,
        "step": 5438
    },
    {
        "loss": 2.1941,
        "grad_norm": 3.3198211193084717,
        "learning_rate": 0.00017859300568731095,
        "epoch": 0.405320813771518,
        "step": 5439
    },
    {
        "loss": 2.0856,
        "grad_norm": 2.786179780960083,
        "learning_rate": 0.00017854947805529236,
        "epoch": 0.4053953349727998,
        "step": 5440
    },
    {
        "loss": 1.8853,
        "grad_norm": 5.2564826011657715,
        "learning_rate": 0.00017850591153085522,
        "epoch": 0.40546985617408154,
        "step": 5441
    },
    {
        "loss": 2.7751,
        "grad_norm": 2.145040988922119,
        "learning_rate": 0.00017846230613557067,
        "epoch": 0.4055443773753633,
        "step": 5442
    },
    {
        "loss": 2.125,
        "grad_norm": 4.159252166748047,
        "learning_rate": 0.0001784186618910292,
        "epoch": 0.40561889857664507,
        "step": 5443
    },
    {
        "loss": 2.0705,
        "grad_norm": 2.927793025970459,
        "learning_rate": 0.00017837497881884057,
        "epoch": 0.40569341977792683,
        "step": 5444
    },
    {
        "loss": 2.3182,
        "grad_norm": 3.202324628829956,
        "learning_rate": 0.00017833125694063356,
        "epoch": 0.4057679409792086,
        "step": 5445
    },
    {
        "loss": 2.1696,
        "grad_norm": 3.068847417831421,
        "learning_rate": 0.00017828749627805642,
        "epoch": 0.40584246218049036,
        "step": 5446
    },
    {
        "loss": 1.1528,
        "grad_norm": 3.4576640129089355,
        "learning_rate": 0.00017824369685277643,
        "epoch": 0.4059169833817721,
        "step": 5447
    },
    {
        "loss": 2.4834,
        "grad_norm": 2.805107831954956,
        "learning_rate": 0.00017819985868648014,
        "epoch": 0.4059915045830539,
        "step": 5448
    },
    {
        "loss": 2.3333,
        "grad_norm": 2.7481632232666016,
        "learning_rate": 0.00017815598180087328,
        "epoch": 0.40606602578433565,
        "step": 5449
    },
    {
        "loss": 2.7099,
        "grad_norm": 1.8632164001464844,
        "learning_rate": 0.00017811206621768072,
        "epoch": 0.4061405469856174,
        "step": 5450
    },
    {
        "loss": 2.2903,
        "grad_norm": 4.267066478729248,
        "learning_rate": 0.00017806811195864649,
        "epoch": 0.4062150681868992,
        "step": 5451
    },
    {
        "loss": 2.4602,
        "grad_norm": 2.6059765815734863,
        "learning_rate": 0.00017802411904553375,
        "epoch": 0.40628958938818094,
        "step": 5452
    },
    {
        "loss": 2.536,
        "grad_norm": 2.2319931983947754,
        "learning_rate": 0.0001779800875001249,
        "epoch": 0.4063641105894627,
        "step": 5453
    },
    {
        "loss": 1.1405,
        "grad_norm": 2.506190538406372,
        "learning_rate": 0.00017793601734422137,
        "epoch": 0.40643863179074446,
        "step": 5454
    },
    {
        "loss": 2.4585,
        "grad_norm": 2.718095064163208,
        "learning_rate": 0.00017789190859964372,
        "epoch": 0.4065131529920262,
        "step": 5455
    },
    {
        "loss": 3.2341,
        "grad_norm": 4.049829483032227,
        "learning_rate": 0.00017784776128823164,
        "epoch": 0.406587674193308,
        "step": 5456
    },
    {
        "loss": 2.8017,
        "grad_norm": 2.7687387466430664,
        "learning_rate": 0.00017780357543184397,
        "epoch": 0.40666219539458975,
        "step": 5457
    },
    {
        "loss": 1.6277,
        "grad_norm": 3.0576462745666504,
        "learning_rate": 0.0001777593510523585,
        "epoch": 0.4067367165958715,
        "step": 5458
    },
    {
        "loss": 2.4527,
        "grad_norm": 4.519251346588135,
        "learning_rate": 0.00017771508817167217,
        "epoch": 0.4068112377971533,
        "step": 5459
    },
    {
        "loss": 3.0297,
        "grad_norm": 5.403295993804932,
        "learning_rate": 0.000177670786811701,
        "epoch": 0.40688575899843504,
        "step": 5460
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.457200765609741,
        "learning_rate": 0.0001776264469943801,
        "epoch": 0.4069602801997168,
        "step": 5461
    },
    {
        "loss": 2.5478,
        "grad_norm": 3.520296335220337,
        "learning_rate": 0.00017758206874166345,
        "epoch": 0.40703480140099857,
        "step": 5462
    },
    {
        "loss": 1.6078,
        "grad_norm": 4.474518775939941,
        "learning_rate": 0.00017753765207552432,
        "epoch": 0.40710932260228033,
        "step": 5463
    },
    {
        "loss": 2.6571,
        "grad_norm": 2.8157174587249756,
        "learning_rate": 0.00017749319701795472,
        "epoch": 0.4071838438035621,
        "step": 5464
    },
    {
        "loss": 2.2817,
        "grad_norm": 3.5287585258483887,
        "learning_rate": 0.00017744870359096582,
        "epoch": 0.40725836500484386,
        "step": 5465
    },
    {
        "loss": 2.8021,
        "grad_norm": 2.9172136783599854,
        "learning_rate": 0.00017740417181658785,
        "epoch": 0.4073328862061256,
        "step": 5466
    },
    {
        "loss": 2.6391,
        "grad_norm": 3.067154884338379,
        "learning_rate": 0.00017735960171686994,
        "epoch": 0.4074074074074074,
        "step": 5467
    },
    {
        "loss": 2.5813,
        "grad_norm": 3.5975377559661865,
        "learning_rate": 0.00017731499331388018,
        "epoch": 0.40748192860868915,
        "step": 5468
    },
    {
        "loss": 2.9149,
        "grad_norm": 3.865114450454712,
        "learning_rate": 0.0001772703466297056,
        "epoch": 0.4075564498099709,
        "step": 5469
    },
    {
        "loss": 2.6422,
        "grad_norm": 3.161447286605835,
        "learning_rate": 0.00017722566168645228,
        "epoch": 0.4076309710112527,
        "step": 5470
    },
    {
        "loss": 2.1873,
        "grad_norm": 2.901151418685913,
        "learning_rate": 0.00017718093850624527,
        "epoch": 0.4077054922125345,
        "step": 5471
    },
    {
        "loss": 2.8661,
        "grad_norm": 2.7037456035614014,
        "learning_rate": 0.0001771361771112284,
        "epoch": 0.40778001341381626,
        "step": 5472
    },
    {
        "loss": 2.0974,
        "grad_norm": 3.2812659740448,
        "learning_rate": 0.00017709137752356445,
        "epoch": 0.407854534615098,
        "step": 5473
    },
    {
        "loss": 1.8273,
        "grad_norm": 3.575558662414551,
        "learning_rate": 0.0001770465397654353,
        "epoch": 0.4079290558163798,
        "step": 5474
    },
    {
        "loss": 2.3122,
        "grad_norm": 2.855767250061035,
        "learning_rate": 0.00017700166385904145,
        "epoch": 0.40800357701766155,
        "step": 5475
    },
    {
        "loss": 2.5179,
        "grad_norm": 2.2198355197906494,
        "learning_rate": 0.0001769567498266025,
        "epoch": 0.4080780982189433,
        "step": 5476
    },
    {
        "loss": 2.5974,
        "grad_norm": 4.065317153930664,
        "learning_rate": 0.00017691179769035694,
        "epoch": 0.40815261942022507,
        "step": 5477
    },
    {
        "loss": 2.2741,
        "grad_norm": 4.039942264556885,
        "learning_rate": 0.00017686680747256196,
        "epoch": 0.40822714062150683,
        "step": 5478
    },
    {
        "loss": 2.5264,
        "grad_norm": 3.029810905456543,
        "learning_rate": 0.00017682177919549363,
        "epoch": 0.4083016618227886,
        "step": 5479
    },
    {
        "loss": 2.7885,
        "grad_norm": 4.507599830627441,
        "learning_rate": 0.000176776712881447,
        "epoch": 0.40837618302407036,
        "step": 5480
    },
    {
        "loss": 2.544,
        "grad_norm": 4.298109531402588,
        "learning_rate": 0.00017673160855273589,
        "epoch": 0.4084507042253521,
        "step": 5481
    },
    {
        "loss": 2.8706,
        "grad_norm": 1.9057296514511108,
        "learning_rate": 0.00017668646623169294,
        "epoch": 0.4085252254266339,
        "step": 5482
    },
    {
        "loss": 2.623,
        "grad_norm": 3.821300745010376,
        "learning_rate": 0.00017664128594066948,
        "epoch": 0.40859974662791565,
        "step": 5483
    },
    {
        "loss": 2.2694,
        "grad_norm": 2.4692344665527344,
        "learning_rate": 0.00017659606770203587,
        "epoch": 0.4086742678291974,
        "step": 5484
    },
    {
        "loss": 1.3785,
        "grad_norm": 3.159933567047119,
        "learning_rate": 0.00017655081153818104,
        "epoch": 0.4087487890304792,
        "step": 5485
    },
    {
        "loss": 2.8027,
        "grad_norm": 3.0962302684783936,
        "learning_rate": 0.00017650551747151296,
        "epoch": 0.40882331023176094,
        "step": 5486
    },
    {
        "loss": 1.8527,
        "grad_norm": 2.887005567550659,
        "learning_rate": 0.00017646018552445805,
        "epoch": 0.4088978314330427,
        "step": 5487
    },
    {
        "loss": 2.7874,
        "grad_norm": 4.303029537200928,
        "learning_rate": 0.0001764148157194617,
        "epoch": 0.40897235263432447,
        "step": 5488
    },
    {
        "loss": 1.5711,
        "grad_norm": 5.8146772384643555,
        "learning_rate": 0.00017636940807898795,
        "epoch": 0.40904687383560623,
        "step": 5489
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.260946035385132,
        "learning_rate": 0.00017632396262551967,
        "epoch": 0.409121395036888,
        "step": 5490
    },
    {
        "loss": 2.5598,
        "grad_norm": 1.034103512763977,
        "learning_rate": 0.00017627847938155838,
        "epoch": 0.40919591623816975,
        "step": 5491
    },
    {
        "loss": 2.429,
        "grad_norm": 2.5346851348876953,
        "learning_rate": 0.00017623295836962432,
        "epoch": 0.4092704374394515,
        "step": 5492
    },
    {
        "loss": 2.0817,
        "grad_norm": 3.084561347961426,
        "learning_rate": 0.00017618739961225636,
        "epoch": 0.4093449586407333,
        "step": 5493
    },
    {
        "loss": 2.5919,
        "grad_norm": 2.307610273361206,
        "learning_rate": 0.0001761418031320122,
        "epoch": 0.40941947984201504,
        "step": 5494
    },
    {
        "loss": 2.4487,
        "grad_norm": 3.1685526371002197,
        "learning_rate": 0.00017609616895146815,
        "epoch": 0.4094940010432968,
        "step": 5495
    },
    {
        "loss": 1.8966,
        "grad_norm": 3.1837732791900635,
        "learning_rate": 0.0001760504970932192,
        "epoch": 0.40956852224457857,
        "step": 5496
    },
    {
        "loss": 1.6559,
        "grad_norm": 3.3362202644348145,
        "learning_rate": 0.00017600478757987898,
        "epoch": 0.40964304344586033,
        "step": 5497
    },
    {
        "loss": 2.3888,
        "grad_norm": 3.027583122253418,
        "learning_rate": 0.00017595904043407972,
        "epoch": 0.4097175646471421,
        "step": 5498
    },
    {
        "loss": 2.337,
        "grad_norm": 3.924288749694824,
        "learning_rate": 0.00017591325567847235,
        "epoch": 0.40979208584842386,
        "step": 5499
    },
    {
        "loss": 2.3487,
        "grad_norm": 2.35404109954834,
        "learning_rate": 0.00017586743333572645,
        "epoch": 0.4098666070497056,
        "step": 5500
    },
    {
        "loss": 2.5331,
        "grad_norm": 2.8938026428222656,
        "learning_rate": 0.00017582157342853008,
        "epoch": 0.4099411282509874,
        "step": 5501
    },
    {
        "loss": 2.8172,
        "grad_norm": 2.483245849609375,
        "learning_rate": 0.00017577567597959007,
        "epoch": 0.41001564945226915,
        "step": 5502
    },
    {
        "loss": 2.7882,
        "grad_norm": 2.4819583892822266,
        "learning_rate": 0.00017572974101163165,
        "epoch": 0.4100901706535509,
        "step": 5503
    },
    {
        "loss": 2.4271,
        "grad_norm": 3.3023295402526855,
        "learning_rate": 0.00017568376854739875,
        "epoch": 0.4101646918548327,
        "step": 5504
    },
    {
        "loss": 2.523,
        "grad_norm": 2.919321060180664,
        "learning_rate": 0.00017563775860965396,
        "epoch": 0.41023921305611444,
        "step": 5505
    },
    {
        "loss": 1.6709,
        "grad_norm": 3.7139480113983154,
        "learning_rate": 0.00017559171122117818,
        "epoch": 0.41031373425739626,
        "step": 5506
    },
    {
        "loss": 2.487,
        "grad_norm": 2.0468082427978516,
        "learning_rate": 0.000175545626404771,
        "epoch": 0.410388255458678,
        "step": 5507
    },
    {
        "loss": 2.3399,
        "grad_norm": 3.8382480144500732,
        "learning_rate": 0.00017549950418325062,
        "epoch": 0.4104627766599598,
        "step": 5508
    },
    {
        "loss": 1.9379,
        "grad_norm": 2.5798215866088867,
        "learning_rate": 0.0001754533445794535,
        "epoch": 0.41053729786124155,
        "step": 5509
    },
    {
        "loss": 2.3543,
        "grad_norm": 2.1432223320007324,
        "learning_rate": 0.0001754071476162349,
        "epoch": 0.4106118190625233,
        "step": 5510
    },
    {
        "loss": 1.8272,
        "grad_norm": 4.713052272796631,
        "learning_rate": 0.00017536091331646835,
        "epoch": 0.41068634026380507,
        "step": 5511
    },
    {
        "loss": 1.8964,
        "grad_norm": 3.565105438232422,
        "learning_rate": 0.0001753146417030461,
        "epoch": 0.41076086146508684,
        "step": 5512
    },
    {
        "loss": 2.8033,
        "grad_norm": 2.6244547367095947,
        "learning_rate": 0.0001752683327988786,
        "epoch": 0.4108353826663686,
        "step": 5513
    },
    {
        "loss": 2.8843,
        "grad_norm": 2.879417896270752,
        "learning_rate": 0.00017522198662689496,
        "epoch": 0.41090990386765036,
        "step": 5514
    },
    {
        "loss": 1.7771,
        "grad_norm": 3.455881357192993,
        "learning_rate": 0.00017517560321004272,
        "epoch": 0.4109844250689321,
        "step": 5515
    },
    {
        "loss": 2.64,
        "grad_norm": 2.4948878288269043,
        "learning_rate": 0.00017512918257128778,
        "epoch": 0.4110589462702139,
        "step": 5516
    },
    {
        "loss": 2.4037,
        "grad_norm": 2.653994560241699,
        "learning_rate": 0.0001750827247336145,
        "epoch": 0.41113346747149565,
        "step": 5517
    },
    {
        "loss": 2.578,
        "grad_norm": 2.400087594985962,
        "learning_rate": 0.00017503622972002582,
        "epoch": 0.4112079886727774,
        "step": 5518
    },
    {
        "loss": 1.7158,
        "grad_norm": 4.066806316375732,
        "learning_rate": 0.0001749896975535428,
        "epoch": 0.4112825098740592,
        "step": 5519
    },
    {
        "loss": 1.9024,
        "grad_norm": 2.9838807582855225,
        "learning_rate": 0.00017494312825720504,
        "epoch": 0.41135703107534094,
        "step": 5520
    },
    {
        "loss": 1.8953,
        "grad_norm": 4.890087604522705,
        "learning_rate": 0.00017489652185407058,
        "epoch": 0.4114315522766227,
        "step": 5521
    },
    {
        "loss": 2.0307,
        "grad_norm": 4.008059024810791,
        "learning_rate": 0.0001748498783672158,
        "epoch": 0.41150607347790447,
        "step": 5522
    },
    {
        "loss": 2.6112,
        "grad_norm": 3.360079288482666,
        "learning_rate": 0.0001748031978197353,
        "epoch": 0.41158059467918623,
        "step": 5523
    },
    {
        "loss": 2.5751,
        "grad_norm": 2.9009146690368652,
        "learning_rate": 0.0001747564802347423,
        "epoch": 0.411655115880468,
        "step": 5524
    },
    {
        "loss": 2.4817,
        "grad_norm": 3.704450845718384,
        "learning_rate": 0.00017470972563536806,
        "epoch": 0.41172963708174976,
        "step": 5525
    },
    {
        "loss": 2.6939,
        "grad_norm": 2.4363083839416504,
        "learning_rate": 0.0001746629340447624,
        "epoch": 0.4118041582830315,
        "step": 5526
    },
    {
        "loss": 2.7842,
        "grad_norm": 2.698734760284424,
        "learning_rate": 0.00017461610548609335,
        "epoch": 0.4118786794843133,
        "step": 5527
    },
    {
        "loss": 2.2365,
        "grad_norm": 2.7159390449523926,
        "learning_rate": 0.0001745692399825473,
        "epoch": 0.41195320068559504,
        "step": 5528
    },
    {
        "loss": 2.8044,
        "grad_norm": 2.328667163848877,
        "learning_rate": 0.00017452233755732888,
        "epoch": 0.4120277218868768,
        "step": 5529
    },
    {
        "loss": 2.0567,
        "grad_norm": 1.9081939458847046,
        "learning_rate": 0.00017447539823366092,
        "epoch": 0.41210224308815857,
        "step": 5530
    },
    {
        "loss": 2.2412,
        "grad_norm": 3.45996356010437,
        "learning_rate": 0.00017442842203478471,
        "epoch": 0.41217676428944033,
        "step": 5531
    },
    {
        "loss": 2.7605,
        "grad_norm": 3.4281606674194336,
        "learning_rate": 0.00017438140898395975,
        "epoch": 0.4122512854907221,
        "step": 5532
    },
    {
        "loss": 2.8537,
        "grad_norm": 1.9514909982681274,
        "learning_rate": 0.00017433435910446364,
        "epoch": 0.41232580669200386,
        "step": 5533
    },
    {
        "loss": 1.8935,
        "grad_norm": 3.1055805683135986,
        "learning_rate": 0.00017428727241959233,
        "epoch": 0.4124003278932856,
        "step": 5534
    },
    {
        "loss": 2.1124,
        "grad_norm": 4.608537673950195,
        "learning_rate": 0.00017424014895265998,
        "epoch": 0.4124748490945674,
        "step": 5535
    },
    {
        "loss": 1.603,
        "grad_norm": 4.260482311248779,
        "learning_rate": 0.000174192988726999,
        "epoch": 0.41254937029584915,
        "step": 5536
    },
    {
        "loss": 1.4434,
        "grad_norm": 5.752885341644287,
        "learning_rate": 0.00017414579176595996,
        "epoch": 0.4126238914971309,
        "step": 5537
    },
    {
        "loss": 2.6668,
        "grad_norm": 2.2856674194335938,
        "learning_rate": 0.00017409855809291162,
        "epoch": 0.4126984126984127,
        "step": 5538
    },
    {
        "loss": 2.7895,
        "grad_norm": 3.2973392009735107,
        "learning_rate": 0.00017405128773124092,
        "epoch": 0.41277293389969444,
        "step": 5539
    },
    {
        "loss": 2.5231,
        "grad_norm": 2.0661184787750244,
        "learning_rate": 0.00017400398070435293,
        "epoch": 0.4128474551009762,
        "step": 5540
    },
    {
        "loss": 2.5481,
        "grad_norm": 3.1326541900634766,
        "learning_rate": 0.0001739566370356709,
        "epoch": 0.41292197630225796,
        "step": 5541
    },
    {
        "loss": 2.6285,
        "grad_norm": 2.335684061050415,
        "learning_rate": 0.0001739092567486363,
        "epoch": 0.4129964975035398,
        "step": 5542
    },
    {
        "loss": 1.64,
        "grad_norm": 3.7633094787597656,
        "learning_rate": 0.0001738618398667086,
        "epoch": 0.41307101870482155,
        "step": 5543
    },
    {
        "loss": 2.283,
        "grad_norm": 3.322298288345337,
        "learning_rate": 0.00017381438641336544,
        "epoch": 0.4131455399061033,
        "step": 5544
    },
    {
        "loss": 1.6716,
        "grad_norm": 4.464407444000244,
        "learning_rate": 0.00017376689641210264,
        "epoch": 0.4132200611073851,
        "step": 5545
    },
    {
        "loss": 2.0632,
        "grad_norm": 2.675471544265747,
        "learning_rate": 0.00017371936988643398,
        "epoch": 0.41329458230866684,
        "step": 5546
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.8026063442230225,
        "learning_rate": 0.00017367180685989153,
        "epoch": 0.4133691035099486,
        "step": 5547
    },
    {
        "loss": 2.5711,
        "grad_norm": 1.6849877834320068,
        "learning_rate": 0.0001736242073560251,
        "epoch": 0.41344362471123036,
        "step": 5548
    },
    {
        "loss": 2.007,
        "grad_norm": 3.413120746612549,
        "learning_rate": 0.000173576571398403,
        "epoch": 0.4135181459125121,
        "step": 5549
    },
    {
        "loss": 2.454,
        "grad_norm": 2.8506104946136475,
        "learning_rate": 0.00017352889901061112,
        "epoch": 0.4135926671137939,
        "step": 5550
    },
    {
        "loss": 3.2352,
        "grad_norm": 3.680748224258423,
        "learning_rate": 0.0001734811902162538,
        "epoch": 0.41366718831507565,
        "step": 5551
    },
    {
        "loss": 2.0589,
        "grad_norm": 4.6901068687438965,
        "learning_rate": 0.00017343344503895314,
        "epoch": 0.4137417095163574,
        "step": 5552
    },
    {
        "loss": 2.0081,
        "grad_norm": 4.440647125244141,
        "learning_rate": 0.00017338566350234938,
        "epoch": 0.4138162307176392,
        "step": 5553
    },
    {
        "loss": 2.5759,
        "grad_norm": 2.714829683303833,
        "learning_rate": 0.00017333784563010064,
        "epoch": 0.41389075191892094,
        "step": 5554
    },
    {
        "loss": 1.9615,
        "grad_norm": 3.315661668777466,
        "learning_rate": 0.00017328999144588316,
        "epoch": 0.4139652731202027,
        "step": 5555
    },
    {
        "loss": 2.4402,
        "grad_norm": 2.515935182571411,
        "learning_rate": 0.00017324210097339115,
        "epoch": 0.41403979432148447,
        "step": 5556
    },
    {
        "loss": 1.7325,
        "grad_norm": 3.992344856262207,
        "learning_rate": 0.00017319417423633678,
        "epoch": 0.41411431552276623,
        "step": 5557
    },
    {
        "loss": 2.472,
        "grad_norm": 3.0277018547058105,
        "learning_rate": 0.00017314621125845005,
        "epoch": 0.414188836724048,
        "step": 5558
    },
    {
        "loss": 1.9056,
        "grad_norm": 4.119094371795654,
        "learning_rate": 0.00017309821206347908,
        "epoch": 0.41426335792532976,
        "step": 5559
    },
    {
        "loss": 2.7903,
        "grad_norm": 4.140275001525879,
        "learning_rate": 0.00017305017667518976,
        "epoch": 0.4143378791266115,
        "step": 5560
    },
    {
        "loss": 2.8924,
        "grad_norm": 1.9398261308670044,
        "learning_rate": 0.00017300210511736607,
        "epoch": 0.4144124003278933,
        "step": 5561
    },
    {
        "loss": 2.5281,
        "grad_norm": 2.561264991760254,
        "learning_rate": 0.00017295399741380974,
        "epoch": 0.41448692152917505,
        "step": 5562
    },
    {
        "loss": 2.357,
        "grad_norm": 2.3374154567718506,
        "learning_rate": 0.00017290585358834055,
        "epoch": 0.4145614427304568,
        "step": 5563
    },
    {
        "loss": 2.577,
        "grad_norm": 3.76525616645813,
        "learning_rate": 0.000172857673664796,
        "epoch": 0.41463596393173857,
        "step": 5564
    },
    {
        "loss": 1.808,
        "grad_norm": 3.295222282409668,
        "learning_rate": 0.00017280945766703158,
        "epoch": 0.41471048513302033,
        "step": 5565
    },
    {
        "loss": 2.3005,
        "grad_norm": 2.7023537158966064,
        "learning_rate": 0.00017276120561892066,
        "epoch": 0.4147850063343021,
        "step": 5566
    },
    {
        "loss": 2.3636,
        "grad_norm": 2.344754934310913,
        "learning_rate": 0.00017271291754435432,
        "epoch": 0.41485952753558386,
        "step": 5567
    },
    {
        "loss": 2.5261,
        "grad_norm": 3.9716267585754395,
        "learning_rate": 0.00017266459346724163,
        "epoch": 0.4149340487368656,
        "step": 5568
    },
    {
        "loss": 1.0757,
        "grad_norm": 3.9873158931732178,
        "learning_rate": 0.00017261623341150944,
        "epoch": 0.4150085699381474,
        "step": 5569
    },
    {
        "loss": 2.3162,
        "grad_norm": 2.3645858764648438,
        "learning_rate": 0.0001725678374011023,
        "epoch": 0.41508309113942915,
        "step": 5570
    },
    {
        "loss": 2.5148,
        "grad_norm": 2.9823036193847656,
        "learning_rate": 0.00017251940545998275,
        "epoch": 0.4151576123407109,
        "step": 5571
    },
    {
        "loss": 2.588,
        "grad_norm": 3.503190755844116,
        "learning_rate": 0.000172470937612131,
        "epoch": 0.4152321335419927,
        "step": 5572
    },
    {
        "loss": 1.6267,
        "grad_norm": 2.6535675525665283,
        "learning_rate": 0.0001724224338815451,
        "epoch": 0.41530665474327444,
        "step": 5573
    },
    {
        "loss": 1.8264,
        "grad_norm": 4.6066155433654785,
        "learning_rate": 0.00017237389429224074,
        "epoch": 0.4153811759445562,
        "step": 5574
    },
    {
        "loss": 1.8494,
        "grad_norm": 2.8805532455444336,
        "learning_rate": 0.00017232531886825157,
        "epoch": 0.41545569714583797,
        "step": 5575
    },
    {
        "loss": 1.7672,
        "grad_norm": 2.5147130489349365,
        "learning_rate": 0.0001722767076336288,
        "epoch": 0.41553021834711973,
        "step": 5576
    },
    {
        "loss": 1.7054,
        "grad_norm": 2.186605930328369,
        "learning_rate": 0.00017222806061244146,
        "epoch": 0.41560473954840155,
        "step": 5577
    },
    {
        "loss": 2.0465,
        "grad_norm": 3.735382318496704,
        "learning_rate": 0.0001721793778287763,
        "epoch": 0.4156792607496833,
        "step": 5578
    },
    {
        "loss": 2.7501,
        "grad_norm": 2.0239369869232178,
        "learning_rate": 0.0001721306593067378,
        "epoch": 0.4157537819509651,
        "step": 5579
    },
    {
        "loss": 1.6105,
        "grad_norm": 4.048635959625244,
        "learning_rate": 0.00017208190507044804,
        "epoch": 0.41582830315224684,
        "step": 5580
    },
    {
        "loss": 2.0788,
        "grad_norm": 3.3663711547851562,
        "learning_rate": 0.00017203311514404681,
        "epoch": 0.4159028243535286,
        "step": 5581
    },
    {
        "loss": 1.8778,
        "grad_norm": 2.481531858444214,
        "learning_rate": 0.00017198428955169165,
        "epoch": 0.41597734555481036,
        "step": 5582
    },
    {
        "loss": 2.8788,
        "grad_norm": 2.4440596103668213,
        "learning_rate": 0.00017193542831755773,
        "epoch": 0.4160518667560921,
        "step": 5583
    },
    {
        "loss": 2.8597,
        "grad_norm": 2.622127056121826,
        "learning_rate": 0.00017188653146583778,
        "epoch": 0.4161263879573739,
        "step": 5584
    },
    {
        "loss": 2.8108,
        "grad_norm": 3.580437183380127,
        "learning_rate": 0.0001718375990207423,
        "epoch": 0.41620090915865565,
        "step": 5585
    },
    {
        "loss": 1.8442,
        "grad_norm": 3.260174512863159,
        "learning_rate": 0.00017178863100649928,
        "epoch": 0.4162754303599374,
        "step": 5586
    },
    {
        "loss": 2.7152,
        "grad_norm": 2.4131641387939453,
        "learning_rate": 0.0001717396274473544,
        "epoch": 0.4163499515612192,
        "step": 5587
    },
    {
        "loss": 1.9134,
        "grad_norm": 5.231455326080322,
        "learning_rate": 0.00017169058836757098,
        "epoch": 0.41642447276250094,
        "step": 5588
    },
    {
        "loss": 2.8458,
        "grad_norm": 2.7736968994140625,
        "learning_rate": 0.00017164151379142988,
        "epoch": 0.4164989939637827,
        "step": 5589
    },
    {
        "loss": 1.7881,
        "grad_norm": 3.4227235317230225,
        "learning_rate": 0.00017159240374322948,
        "epoch": 0.41657351516506447,
        "step": 5590
    },
    {
        "loss": 2.6503,
        "grad_norm": 3.4743330478668213,
        "learning_rate": 0.00017154325824728575,
        "epoch": 0.41664803636634623,
        "step": 5591
    },
    {
        "loss": 2.4981,
        "grad_norm": 2.6692748069763184,
        "learning_rate": 0.00017149407732793223,
        "epoch": 0.416722557567628,
        "step": 5592
    },
    {
        "loss": 2.4019,
        "grad_norm": 3.0954160690307617,
        "learning_rate": 0.00017144486100952012,
        "epoch": 0.41679707876890976,
        "step": 5593
    },
    {
        "loss": 2.6594,
        "grad_norm": 1.743186116218567,
        "learning_rate": 0.00017139560931641795,
        "epoch": 0.4168715999701915,
        "step": 5594
    },
    {
        "loss": 2.215,
        "grad_norm": 3.669079065322876,
        "learning_rate": 0.0001713463222730118,
        "epoch": 0.4169461211714733,
        "step": 5595
    },
    {
        "loss": 2.3077,
        "grad_norm": 2.116583824157715,
        "learning_rate": 0.00017129699990370536,
        "epoch": 0.41702064237275505,
        "step": 5596
    },
    {
        "loss": 2.2736,
        "grad_norm": 3.213517904281616,
        "learning_rate": 0.00017124764223291973,
        "epoch": 0.4170951635740368,
        "step": 5597
    },
    {
        "loss": 1.9832,
        "grad_norm": 3.8513004779815674,
        "learning_rate": 0.00017119824928509353,
        "epoch": 0.41716968477531857,
        "step": 5598
    },
    {
        "loss": 2.7328,
        "grad_norm": 3.5448436737060547,
        "learning_rate": 0.0001711488210846828,
        "epoch": 0.41724420597660034,
        "step": 5599
    },
    {
        "loss": 3.1924,
        "grad_norm": 2.44513201713562,
        "learning_rate": 0.0001710993576561611,
        "epoch": 0.4173187271778821,
        "step": 5600
    },
    {
        "loss": 1.8923,
        "grad_norm": 2.91001296043396,
        "learning_rate": 0.00017104985902401937,
        "epoch": 0.41739324837916386,
        "step": 5601
    },
    {
        "loss": 2.7432,
        "grad_norm": 3.4745936393737793,
        "learning_rate": 0.00017100032521276596,
        "epoch": 0.4174677695804456,
        "step": 5602
    },
    {
        "loss": 2.5709,
        "grad_norm": 3.184089422225952,
        "learning_rate": 0.00017095075624692677,
        "epoch": 0.4175422907817274,
        "step": 5603
    },
    {
        "loss": 2.8519,
        "grad_norm": 2.6050336360931396,
        "learning_rate": 0.000170901152151045,
        "epoch": 0.41761681198300915,
        "step": 5604
    },
    {
        "loss": 2.9288,
        "grad_norm": 2.313453435897827,
        "learning_rate": 0.00017085151294968112,
        "epoch": 0.4176913331842909,
        "step": 5605
    },
    {
        "loss": 2.0708,
        "grad_norm": 3.5913023948669434,
        "learning_rate": 0.0001708018386674133,
        "epoch": 0.4177658543855727,
        "step": 5606
    },
    {
        "loss": 2.1116,
        "grad_norm": 4.150018692016602,
        "learning_rate": 0.00017075212932883676,
        "epoch": 0.41784037558685444,
        "step": 5607
    },
    {
        "loss": 2.571,
        "grad_norm": 3.193683624267578,
        "learning_rate": 0.0001707023849585644,
        "epoch": 0.4179148967881362,
        "step": 5608
    },
    {
        "loss": 1.9306,
        "grad_norm": 5.050698757171631,
        "learning_rate": 0.00017065260558122615,
        "epoch": 0.41798941798941797,
        "step": 5609
    },
    {
        "loss": 2.5036,
        "grad_norm": 3.590405225753784,
        "learning_rate": 0.00017060279122146948,
        "epoch": 0.41806393919069973,
        "step": 5610
    },
    {
        "loss": 2.0876,
        "grad_norm": 2.6430504322052,
        "learning_rate": 0.00017055294190395902,
        "epoch": 0.4181384603919815,
        "step": 5611
    },
    {
        "loss": 2.8706,
        "grad_norm": 3.115464210510254,
        "learning_rate": 0.00017050305765337685,
        "epoch": 0.4182129815932633,
        "step": 5612
    },
    {
        "loss": 2.4365,
        "grad_norm": 3.482421875,
        "learning_rate": 0.00017045313849442238,
        "epoch": 0.4182875027945451,
        "step": 5613
    },
    {
        "loss": 2.2084,
        "grad_norm": 3.757488489151001,
        "learning_rate": 0.00017040318445181213,
        "epoch": 0.41836202399582684,
        "step": 5614
    },
    {
        "loss": 1.7814,
        "grad_norm": 4.054891586303711,
        "learning_rate": 0.00017035319555027995,
        "epoch": 0.4184365451971086,
        "step": 5615
    },
    {
        "loss": 2.3259,
        "grad_norm": 2.9732773303985596,
        "learning_rate": 0.000170303171814577,
        "epoch": 0.41851106639839036,
        "step": 5616
    },
    {
        "loss": 2.3361,
        "grad_norm": 2.987259864807129,
        "learning_rate": 0.00017025311326947175,
        "epoch": 0.4185855875996721,
        "step": 5617
    },
    {
        "loss": 2.3093,
        "grad_norm": 3.8407340049743652,
        "learning_rate": 0.00017020301993974979,
        "epoch": 0.4186601088009539,
        "step": 5618
    },
    {
        "loss": 2.1049,
        "grad_norm": 3.1682705879211426,
        "learning_rate": 0.0001701528918502139,
        "epoch": 0.41873463000223565,
        "step": 5619
    },
    {
        "loss": 2.6652,
        "grad_norm": 4.039228439331055,
        "learning_rate": 0.00017010272902568424,
        "epoch": 0.4188091512035174,
        "step": 5620
    },
    {
        "loss": 2.6801,
        "grad_norm": 3.2472784519195557,
        "learning_rate": 0.00017005253149099793,
        "epoch": 0.4188836724047992,
        "step": 5621
    },
    {
        "loss": 2.6964,
        "grad_norm": 2.491321563720703,
        "learning_rate": 0.0001700022992710096,
        "epoch": 0.41895819360608094,
        "step": 5622
    },
    {
        "loss": 2.4629,
        "grad_norm": 2.796917676925659,
        "learning_rate": 0.00016995203239059069,
        "epoch": 0.4190327148073627,
        "step": 5623
    },
    {
        "loss": 2.5987,
        "grad_norm": 3.873056650161743,
        "learning_rate": 0.0001699017308746301,
        "epoch": 0.41910723600864447,
        "step": 5624
    },
    {
        "loss": 2.4031,
        "grad_norm": 3.0514180660247803,
        "learning_rate": 0.00016985139474803366,
        "epoch": 0.41918175720992623,
        "step": 5625
    },
    {
        "loss": 2.2486,
        "grad_norm": 3.176640033721924,
        "learning_rate": 0.0001698010240357245,
        "epoch": 0.419256278411208,
        "step": 5626
    },
    {
        "loss": 2.6113,
        "grad_norm": 2.9996325969696045,
        "learning_rate": 0.00016975061876264283,
        "epoch": 0.41933079961248976,
        "step": 5627
    },
    {
        "loss": 2.7626,
        "grad_norm": 3.221587657928467,
        "learning_rate": 0.00016970017895374587,
        "epoch": 0.4194053208137715,
        "step": 5628
    },
    {
        "loss": 2.9828,
        "grad_norm": 2.5873591899871826,
        "learning_rate": 0.00016964970463400803,
        "epoch": 0.4194798420150533,
        "step": 5629
    },
    {
        "loss": 1.7874,
        "grad_norm": 3.7008442878723145,
        "learning_rate": 0.00016959919582842093,
        "epoch": 0.41955436321633505,
        "step": 5630
    },
    {
        "loss": 3.0971,
        "grad_norm": 2.410947799682617,
        "learning_rate": 0.00016954865256199296,
        "epoch": 0.4196288844176168,
        "step": 5631
    },
    {
        "loss": 1.941,
        "grad_norm": 2.742588520050049,
        "learning_rate": 0.00016949807485974988,
        "epoch": 0.4197034056188986,
        "step": 5632
    },
    {
        "loss": 2.841,
        "grad_norm": 2.8608508110046387,
        "learning_rate": 0.0001694474627467343,
        "epoch": 0.41977792682018034,
        "step": 5633
    },
    {
        "loss": 2.8612,
        "grad_norm": 2.0702319145202637,
        "learning_rate": 0.00016939681624800597,
        "epoch": 0.4198524480214621,
        "step": 5634
    },
    {
        "loss": 2.0201,
        "grad_norm": 3.177675247192383,
        "learning_rate": 0.00016934613538864156,
        "epoch": 0.41992696922274386,
        "step": 5635
    },
    {
        "loss": 2.8438,
        "grad_norm": 3.585144519805908,
        "learning_rate": 0.0001692954201937348,
        "epoch": 0.4200014904240256,
        "step": 5636
    },
    {
        "loss": 2.8625,
        "grad_norm": 2.034323215484619,
        "learning_rate": 0.00016924467068839665,
        "epoch": 0.4200760116253074,
        "step": 5637
    },
    {
        "loss": 2.062,
        "grad_norm": 3.668294668197632,
        "learning_rate": 0.0001691938868977546,
        "epoch": 0.42015053282658915,
        "step": 5638
    },
    {
        "loss": 2.8533,
        "grad_norm": 2.8890116214752197,
        "learning_rate": 0.00016914306884695352,
        "epoch": 0.4202250540278709,
        "step": 5639
    },
    {
        "loss": 2.1767,
        "grad_norm": 3.3679966926574707,
        "learning_rate": 0.0001690922165611551,
        "epoch": 0.4202995752291527,
        "step": 5640
    },
    {
        "loss": 2.4182,
        "grad_norm": 2.4378888607025146,
        "learning_rate": 0.00016904133006553793,
        "epoch": 0.42037409643043444,
        "step": 5641
    },
    {
        "loss": 2.0682,
        "grad_norm": 3.0061404705047607,
        "learning_rate": 0.0001689904093852975,
        "epoch": 0.4204486176317162,
        "step": 5642
    },
    {
        "loss": 1.9539,
        "grad_norm": 3.7358109951019287,
        "learning_rate": 0.00016893945454564638,
        "epoch": 0.42052313883299797,
        "step": 5643
    },
    {
        "loss": 2.023,
        "grad_norm": 3.4623379707336426,
        "learning_rate": 0.00016888846557181402,
        "epoch": 0.42059766003427973,
        "step": 5644
    },
    {
        "loss": 2.2526,
        "grad_norm": 3.1494760513305664,
        "learning_rate": 0.00016883744248904668,
        "epoch": 0.4206721812355615,
        "step": 5645
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.2787692546844482,
        "learning_rate": 0.00016878638532260757,
        "epoch": 0.42074670243684326,
        "step": 5646
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.612743854522705,
        "learning_rate": 0.00016873529409777675,
        "epoch": 0.420821223638125,
        "step": 5647
    },
    {
        "loss": 2.5738,
        "grad_norm": 2.2768003940582275,
        "learning_rate": 0.0001686841688398512,
        "epoch": 0.42089574483940684,
        "step": 5648
    },
    {
        "loss": 2.0361,
        "grad_norm": 5.036404132843018,
        "learning_rate": 0.0001686330095741446,
        "epoch": 0.4209702660406886,
        "step": 5649
    },
    {
        "loss": 2.2476,
        "grad_norm": 2.454871416091919,
        "learning_rate": 0.00016858181632598774,
        "epoch": 0.42104478724197036,
        "step": 5650
    },
    {
        "loss": 2.2707,
        "grad_norm": 2.1951732635498047,
        "learning_rate": 0.00016853058912072802,
        "epoch": 0.4211193084432521,
        "step": 5651
    },
    {
        "loss": 1.4763,
        "grad_norm": 3.4673664569854736,
        "learning_rate": 0.0001684793279837296,
        "epoch": 0.4211938296445339,
        "step": 5652
    },
    {
        "loss": 2.1996,
        "grad_norm": 3.720585584640503,
        "learning_rate": 0.00016842803294037366,
        "epoch": 0.42126835084581565,
        "step": 5653
    },
    {
        "loss": 2.805,
        "grad_norm": 1.5917010307312012,
        "learning_rate": 0.00016837670401605802,
        "epoch": 0.4213428720470974,
        "step": 5654
    },
    {
        "loss": 2.3808,
        "grad_norm": 2.750701427459717,
        "learning_rate": 0.00016832534123619735,
        "epoch": 0.4214173932483792,
        "step": 5655
    },
    {
        "loss": 2.2112,
        "grad_norm": 4.204471588134766,
        "learning_rate": 0.00016827394462622298,
        "epoch": 0.42149191444966094,
        "step": 5656
    },
    {
        "loss": 1.9773,
        "grad_norm": 3.0656468868255615,
        "learning_rate": 0.00016822251421158303,
        "epoch": 0.4215664356509427,
        "step": 5657
    },
    {
        "loss": 2.3812,
        "grad_norm": 3.2141776084899902,
        "learning_rate": 0.00016817105001774247,
        "epoch": 0.42164095685222447,
        "step": 5658
    },
    {
        "loss": 1.918,
        "grad_norm": 3.051384925842285,
        "learning_rate": 0.00016811955207018283,
        "epoch": 0.42171547805350623,
        "step": 5659
    },
    {
        "loss": 2.0529,
        "grad_norm": 3.311323404312134,
        "learning_rate": 0.00016806802039440256,
        "epoch": 0.421789999254788,
        "step": 5660
    },
    {
        "loss": 1.6847,
        "grad_norm": 2.976701259613037,
        "learning_rate": 0.00016801645501591657,
        "epoch": 0.42186452045606976,
        "step": 5661
    },
    {
        "loss": 1.6936,
        "grad_norm": 3.863629102706909,
        "learning_rate": 0.00016796485596025655,
        "epoch": 0.4219390416573515,
        "step": 5662
    },
    {
        "loss": 2.8899,
        "grad_norm": 2.1556203365325928,
        "learning_rate": 0.0001679132232529709,
        "epoch": 0.4220135628586333,
        "step": 5663
    },
    {
        "loss": 2.6246,
        "grad_norm": 2.7654943466186523,
        "learning_rate": 0.00016786155691962478,
        "epoch": 0.42208808405991505,
        "step": 5664
    },
    {
        "loss": 2.8095,
        "grad_norm": 2.2008697986602783,
        "learning_rate": 0.00016780985698579975,
        "epoch": 0.4221626052611968,
        "step": 5665
    },
    {
        "loss": 2.0472,
        "grad_norm": 3.800008773803711,
        "learning_rate": 0.00016775812347709414,
        "epoch": 0.4222371264624786,
        "step": 5666
    },
    {
        "loss": 2.7744,
        "grad_norm": 2.463463068008423,
        "learning_rate": 0.00016770635641912294,
        "epoch": 0.42231164766376034,
        "step": 5667
    },
    {
        "loss": 2.5382,
        "grad_norm": 2.900651454925537,
        "learning_rate": 0.00016765455583751771,
        "epoch": 0.4223861688650421,
        "step": 5668
    },
    {
        "loss": 2.3486,
        "grad_norm": 2.8583786487579346,
        "learning_rate": 0.0001676027217579267,
        "epoch": 0.42246069006632386,
        "step": 5669
    },
    {
        "loss": 1.9936,
        "grad_norm": 3.7454330921173096,
        "learning_rate": 0.0001675508542060145,
        "epoch": 0.4225352112676056,
        "step": 5670
    },
    {
        "loss": 2.2283,
        "grad_norm": 2.8659720420837402,
        "learning_rate": 0.00016749895320746262,
        "epoch": 0.4226097324688874,
        "step": 5671
    },
    {
        "loss": 1.9717,
        "grad_norm": 2.540769338607788,
        "learning_rate": 0.00016744701878796872,
        "epoch": 0.42268425367016915,
        "step": 5672
    },
    {
        "loss": 2.1514,
        "grad_norm": 3.768129348754883,
        "learning_rate": 0.00016739505097324738,
        "epoch": 0.4227587748714509,
        "step": 5673
    },
    {
        "loss": 2.6386,
        "grad_norm": 3.0473387241363525,
        "learning_rate": 0.0001673430497890296,
        "epoch": 0.4228332960727327,
        "step": 5674
    },
    {
        "loss": 2.3629,
        "grad_norm": 4.186264514923096,
        "learning_rate": 0.00016729101526106278,
        "epoch": 0.42290781727401444,
        "step": 5675
    },
    {
        "loss": 2.8384,
        "grad_norm": 3.8263344764709473,
        "learning_rate": 0.0001672389474151109,
        "epoch": 0.4229823384752962,
        "step": 5676
    },
    {
        "loss": 1.8741,
        "grad_norm": 3.866000175476074,
        "learning_rate": 0.00016718684627695454,
        "epoch": 0.42305685967657797,
        "step": 5677
    },
    {
        "loss": 1.6952,
        "grad_norm": 4.060552597045898,
        "learning_rate": 0.0001671347118723906,
        "epoch": 0.42313138087785973,
        "step": 5678
    },
    {
        "loss": 1.7676,
        "grad_norm": 3.20249605178833,
        "learning_rate": 0.00016708254422723263,
        "epoch": 0.4232059020791415,
        "step": 5679
    },
    {
        "loss": 2.0986,
        "grad_norm": 3.8627769947052,
        "learning_rate": 0.00016703034336731042,
        "epoch": 0.42328042328042326,
        "step": 5680
    },
    {
        "loss": 2.5733,
        "grad_norm": 2.401702642440796,
        "learning_rate": 0.00016697810931847045,
        "epoch": 0.423354944481705,
        "step": 5681
    },
    {
        "loss": 2.1189,
        "grad_norm": 3.224209785461426,
        "learning_rate": 0.0001669258421065754,
        "epoch": 0.4234294656829868,
        "step": 5682
    },
    {
        "loss": 2.2141,
        "grad_norm": 3.0280942916870117,
        "learning_rate": 0.0001668735417575045,
        "epoch": 0.4235039868842686,
        "step": 5683
    },
    {
        "loss": 2.7717,
        "grad_norm": 2.8316521644592285,
        "learning_rate": 0.00016682120829715345,
        "epoch": 0.42357850808555036,
        "step": 5684
    },
    {
        "loss": 2.7509,
        "grad_norm": 2.5380325317382812,
        "learning_rate": 0.00016676884175143418,
        "epoch": 0.42365302928683213,
        "step": 5685
    },
    {
        "loss": 2.5099,
        "grad_norm": 2.6221370697021484,
        "learning_rate": 0.00016671644214627507,
        "epoch": 0.4237275504881139,
        "step": 5686
    },
    {
        "loss": 2.4969,
        "grad_norm": 2.815700054168701,
        "learning_rate": 0.0001666640095076209,
        "epoch": 0.42380207168939565,
        "step": 5687
    },
    {
        "loss": 2.2333,
        "grad_norm": 3.3697926998138428,
        "learning_rate": 0.00016661154386143283,
        "epoch": 0.4238765928906774,
        "step": 5688
    },
    {
        "loss": 2.6802,
        "grad_norm": 2.2117667198181152,
        "learning_rate": 0.00016655904523368823,
        "epoch": 0.4239511140919592,
        "step": 5689
    },
    {
        "loss": 1.8219,
        "grad_norm": 3.634943962097168,
        "learning_rate": 0.00016650651365038096,
        "epoch": 0.42402563529324094,
        "step": 5690
    },
    {
        "loss": 1.2147,
        "grad_norm": 2.228919267654419,
        "learning_rate": 0.00016645394913752118,
        "epoch": 0.4241001564945227,
        "step": 5691
    },
    {
        "loss": 1.8125,
        "grad_norm": 3.6478211879730225,
        "learning_rate": 0.00016640135172113512,
        "epoch": 0.42417467769580447,
        "step": 5692
    },
    {
        "loss": 2.6113,
        "grad_norm": 2.2029054164886475,
        "learning_rate": 0.0001663487214272657,
        "epoch": 0.42424919889708623,
        "step": 5693
    },
    {
        "loss": 2.4816,
        "grad_norm": 3.13980770111084,
        "learning_rate": 0.00016629605828197175,
        "epoch": 0.424323720098368,
        "step": 5694
    },
    {
        "loss": 2.3286,
        "grad_norm": 2.471998691558838,
        "learning_rate": 0.0001662433623113286,
        "epoch": 0.42439824129964976,
        "step": 5695
    },
    {
        "loss": 2.4953,
        "grad_norm": 2.5139176845550537,
        "learning_rate": 0.00016619063354142766,
        "epoch": 0.4244727625009315,
        "step": 5696
    },
    {
        "loss": 1.9571,
        "grad_norm": 4.453747749328613,
        "learning_rate": 0.0001661378719983767,
        "epoch": 0.4245472837022133,
        "step": 5697
    },
    {
        "loss": 2.2121,
        "grad_norm": 1.7938193082809448,
        "learning_rate": 0.0001660850777082998,
        "epoch": 0.42462180490349505,
        "step": 5698
    },
    {
        "loss": 1.9846,
        "grad_norm": 4.1872968673706055,
        "learning_rate": 0.00016603225069733698,
        "epoch": 0.4246963261047768,
        "step": 5699
    },
    {
        "loss": 2.7197,
        "grad_norm": 2.100217580795288,
        "learning_rate": 0.0001659793909916447,
        "epoch": 0.4247708473060586,
        "step": 5700
    },
    {
        "loss": 2.1935,
        "grad_norm": 4.1549835205078125,
        "learning_rate": 0.00016592649861739558,
        "epoch": 0.42484536850734034,
        "step": 5701
    },
    {
        "loss": 2.5936,
        "grad_norm": 2.763328790664673,
        "learning_rate": 0.00016587357360077834,
        "epoch": 0.4249198897086221,
        "step": 5702
    },
    {
        "loss": 2.6711,
        "grad_norm": 2.6471304893493652,
        "learning_rate": 0.00016582061596799778,
        "epoch": 0.42499441090990386,
        "step": 5703
    },
    {
        "loss": 1.3086,
        "grad_norm": 3.3496475219726562,
        "learning_rate": 0.00016576762574527508,
        "epoch": 0.4250689321111856,
        "step": 5704
    },
    {
        "loss": 1.1139,
        "grad_norm": 3.296628475189209,
        "learning_rate": 0.0001657146029588474,
        "epoch": 0.4251434533124674,
        "step": 5705
    },
    {
        "loss": 2.7032,
        "grad_norm": 1.2094942331314087,
        "learning_rate": 0.00016566154763496802,
        "epoch": 0.42521797451374915,
        "step": 5706
    },
    {
        "loss": 2.9675,
        "grad_norm": 3.770883560180664,
        "learning_rate": 0.00016560845979990643,
        "epoch": 0.4252924957150309,
        "step": 5707
    },
    {
        "loss": 2.0401,
        "grad_norm": 2.8939969539642334,
        "learning_rate": 0.00016555533947994812,
        "epoch": 0.4253670169163127,
        "step": 5708
    },
    {
        "loss": 2.5862,
        "grad_norm": 2.7179572582244873,
        "learning_rate": 0.0001655021867013947,
        "epoch": 0.42544153811759444,
        "step": 5709
    },
    {
        "loss": 2.5171,
        "grad_norm": 2.7260549068450928,
        "learning_rate": 0.00016544900149056385,
        "epoch": 0.4255160593188762,
        "step": 5710
    },
    {
        "loss": 1.6751,
        "grad_norm": 2.3447020053863525,
        "learning_rate": 0.00016539578387378935,
        "epoch": 0.42559058052015797,
        "step": 5711
    },
    {
        "loss": 2.5774,
        "grad_norm": 3.1461055278778076,
        "learning_rate": 0.00016534253387742097,
        "epoch": 0.42566510172143973,
        "step": 5712
    },
    {
        "loss": 1.2489,
        "grad_norm": 3.758338212966919,
        "learning_rate": 0.00016528925152782447,
        "epoch": 0.4257396229227215,
        "step": 5713
    },
    {
        "loss": 1.9713,
        "grad_norm": 2.1597986221313477,
        "learning_rate": 0.0001652359368513817,
        "epoch": 0.42581414412400326,
        "step": 5714
    },
    {
        "loss": 1.6544,
        "grad_norm": 2.4946227073669434,
        "learning_rate": 0.00016518258987449058,
        "epoch": 0.425888665325285,
        "step": 5715
    },
    {
        "loss": 2.5995,
        "grad_norm": 2.142139434814453,
        "learning_rate": 0.0001651292106235649,
        "epoch": 0.4259631865265668,
        "step": 5716
    },
    {
        "loss": 2.0866,
        "grad_norm": 2.316594123840332,
        "learning_rate": 0.00016507579912503442,
        "epoch": 0.42603770772784855,
        "step": 5717
    },
    {
        "loss": 2.4081,
        "grad_norm": 4.4598283767700195,
        "learning_rate": 0.00016502235540534496,
        "epoch": 0.4261122289291303,
        "step": 5718
    },
    {
        "loss": 2.5077,
        "grad_norm": 1.9462628364562988,
        "learning_rate": 0.00016496887949095826,
        "epoch": 0.42618675013041213,
        "step": 5719
    },
    {
        "loss": 2.8728,
        "grad_norm": 3.072706460952759,
        "learning_rate": 0.000164915371408352,
        "epoch": 0.4262612713316939,
        "step": 5720
    },
    {
        "loss": 1.7718,
        "grad_norm": 2.8330671787261963,
        "learning_rate": 0.00016486183118401987,
        "epoch": 0.42633579253297565,
        "step": 5721
    },
    {
        "loss": 2.9789,
        "grad_norm": 3.8500776290893555,
        "learning_rate": 0.00016480825884447126,
        "epoch": 0.4264103137342574,
        "step": 5722
    },
    {
        "loss": 2.0674,
        "grad_norm": 2.813707113265991,
        "learning_rate": 0.0001647546544162316,
        "epoch": 0.4264848349355392,
        "step": 5723
    },
    {
        "loss": 2.4112,
        "grad_norm": 4.4005889892578125,
        "learning_rate": 0.00016470101792584221,
        "epoch": 0.42655935613682094,
        "step": 5724
    },
    {
        "loss": 2.6657,
        "grad_norm": 1.996525526046753,
        "learning_rate": 0.00016464734939986036,
        "epoch": 0.4266338773381027,
        "step": 5725
    },
    {
        "loss": 2.6479,
        "grad_norm": 2.0270464420318604,
        "learning_rate": 0.00016459364886485905,
        "epoch": 0.42670839853938447,
        "step": 5726
    },
    {
        "loss": 2.4818,
        "grad_norm": 3.1193835735321045,
        "learning_rate": 0.00016453991634742706,
        "epoch": 0.42678291974066623,
        "step": 5727
    },
    {
        "loss": 2.0132,
        "grad_norm": 3.481822967529297,
        "learning_rate": 0.0001644861518741692,
        "epoch": 0.426857440941948,
        "step": 5728
    },
    {
        "loss": 2.2997,
        "grad_norm": 2.1019320487976074,
        "learning_rate": 0.00016443235547170605,
        "epoch": 0.42693196214322976,
        "step": 5729
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.508044481277466,
        "learning_rate": 0.00016437852716667397,
        "epoch": 0.4270064833445115,
        "step": 5730
    },
    {
        "loss": 2.391,
        "grad_norm": 3.3017311096191406,
        "learning_rate": 0.0001643246669857251,
        "epoch": 0.4270810045457933,
        "step": 5731
    },
    {
        "loss": 2.4906,
        "grad_norm": 2.2279372215270996,
        "learning_rate": 0.0001642707749555274,
        "epoch": 0.42715552574707505,
        "step": 5732
    },
    {
        "loss": 2.0769,
        "grad_norm": 3.0823864936828613,
        "learning_rate": 0.00016421685110276453,
        "epoch": 0.4272300469483568,
        "step": 5733
    },
    {
        "loss": 2.3848,
        "grad_norm": 4.183388710021973,
        "learning_rate": 0.00016416289545413598,
        "epoch": 0.4273045681496386,
        "step": 5734
    },
    {
        "loss": 2.3403,
        "grad_norm": 3.7084972858428955,
        "learning_rate": 0.00016410890803635696,
        "epoch": 0.42737908935092034,
        "step": 5735
    },
    {
        "loss": 2.1138,
        "grad_norm": 2.675340175628662,
        "learning_rate": 0.00016405488887615843,
        "epoch": 0.4274536105522021,
        "step": 5736
    },
    {
        "loss": 2.6514,
        "grad_norm": 1.9568172693252563,
        "learning_rate": 0.00016400083800028695,
        "epoch": 0.42752813175348386,
        "step": 5737
    },
    {
        "loss": 2.4859,
        "grad_norm": 2.356863498687744,
        "learning_rate": 0.00016394675543550497,
        "epoch": 0.4276026529547656,
        "step": 5738
    },
    {
        "loss": 2.4315,
        "grad_norm": 2.457728147506714,
        "learning_rate": 0.00016389264120859055,
        "epoch": 0.4276771741560474,
        "step": 5739
    },
    {
        "loss": 2.3499,
        "grad_norm": 2.5255868434906006,
        "learning_rate": 0.0001638384953463374,
        "epoch": 0.42775169535732915,
        "step": 5740
    },
    {
        "loss": 2.306,
        "grad_norm": 3.7909610271453857,
        "learning_rate": 0.00016378431787555483,
        "epoch": 0.4278262165586109,
        "step": 5741
    },
    {
        "loss": 2.5448,
        "grad_norm": 2.484584331512451,
        "learning_rate": 0.000163730108823068,
        "epoch": 0.4279007377598927,
        "step": 5742
    },
    {
        "loss": 2.4605,
        "grad_norm": 3.0652644634246826,
        "learning_rate": 0.0001636758682157175,
        "epoch": 0.42797525896117444,
        "step": 5743
    },
    {
        "loss": 2.2025,
        "grad_norm": 2.829033374786377,
        "learning_rate": 0.00016362159608035963,
        "epoch": 0.4280497801624562,
        "step": 5744
    },
    {
        "loss": 2.2381,
        "grad_norm": 2.969869613647461,
        "learning_rate": 0.00016356729244386645,
        "epoch": 0.42812430136373797,
        "step": 5745
    },
    {
        "loss": 2.3003,
        "grad_norm": 2.3893849849700928,
        "learning_rate": 0.00016351295733312528,
        "epoch": 0.42819882256501973,
        "step": 5746
    },
    {
        "loss": 2.0029,
        "grad_norm": 3.1297872066497803,
        "learning_rate": 0.00016345859077503925,
        "epoch": 0.4282733437663015,
        "step": 5747
    },
    {
        "loss": 2.2999,
        "grad_norm": 3.6198642253875732,
        "learning_rate": 0.00016340419279652706,
        "epoch": 0.42834786496758326,
        "step": 5748
    },
    {
        "loss": 2.4757,
        "grad_norm": 2.625286102294922,
        "learning_rate": 0.00016334976342452298,
        "epoch": 0.428422386168865,
        "step": 5749
    },
    {
        "loss": 2.8939,
        "grad_norm": 3.244533061981201,
        "learning_rate": 0.00016329530268597667,
        "epoch": 0.4284969073701468,
        "step": 5750
    },
    {
        "loss": 2.3926,
        "grad_norm": 2.667607545852661,
        "learning_rate": 0.00016324081060785345,
        "epoch": 0.42857142857142855,
        "step": 5751
    },
    {
        "loss": 2.5706,
        "grad_norm": 3.5233705043792725,
        "learning_rate": 0.00016318628721713425,
        "epoch": 0.4286459497727103,
        "step": 5752
    },
    {
        "loss": 2.0586,
        "grad_norm": 3.7902884483337402,
        "learning_rate": 0.0001631317325408152,
        "epoch": 0.4287204709739921,
        "step": 5753
    },
    {
        "loss": 2.1658,
        "grad_norm": 3.1885697841644287,
        "learning_rate": 0.0001630771466059083,
        "epoch": 0.4287949921752739,
        "step": 5754
    },
    {
        "loss": 2.372,
        "grad_norm": 3.5826656818389893,
        "learning_rate": 0.00016302252943944067,
        "epoch": 0.42886951337655566,
        "step": 5755
    },
    {
        "loss": 2.1823,
        "grad_norm": 3.709857225418091,
        "learning_rate": 0.00016296788106845512,
        "epoch": 0.4289440345778374,
        "step": 5756
    },
    {
        "loss": 2.0776,
        "grad_norm": 3.899664878845215,
        "learning_rate": 0.00016291320152000987,
        "epoch": 0.4290185557791192,
        "step": 5757
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.935694694519043,
        "learning_rate": 0.0001628584908211785,
        "epoch": 0.42909307698040094,
        "step": 5758
    },
    {
        "loss": 1.6665,
        "grad_norm": 4.359002113342285,
        "learning_rate": 0.00016280374899905017,
        "epoch": 0.4291675981816827,
        "step": 5759
    },
    {
        "loss": 2.6022,
        "grad_norm": 1.8505042791366577,
        "learning_rate": 0.0001627489760807292,
        "epoch": 0.42924211938296447,
        "step": 5760
    },
    {
        "loss": 2.4471,
        "grad_norm": 3.381347894668579,
        "learning_rate": 0.00016269417209333552,
        "epoch": 0.42931664058424623,
        "step": 5761
    },
    {
        "loss": 1.7751,
        "grad_norm": 3.8240973949432373,
        "learning_rate": 0.00016263933706400451,
        "epoch": 0.429391161785528,
        "step": 5762
    },
    {
        "loss": 2.2865,
        "grad_norm": 2.421708345413208,
        "learning_rate": 0.00016258447101988663,
        "epoch": 0.42946568298680976,
        "step": 5763
    },
    {
        "loss": 2.5532,
        "grad_norm": 1.764602780342102,
        "learning_rate": 0.00016252957398814788,
        "epoch": 0.4295402041880915,
        "step": 5764
    },
    {
        "loss": 2.3066,
        "grad_norm": 3.0062146186828613,
        "learning_rate": 0.00016247464599596957,
        "epoch": 0.4296147253893733,
        "step": 5765
    },
    {
        "loss": 2.4557,
        "grad_norm": 2.9717915058135986,
        "learning_rate": 0.00016241968707054842,
        "epoch": 0.42968924659065505,
        "step": 5766
    },
    {
        "loss": 2.5745,
        "grad_norm": 3.285231828689575,
        "learning_rate": 0.00016236469723909634,
        "epoch": 0.4297637677919368,
        "step": 5767
    },
    {
        "loss": 2.9283,
        "grad_norm": 2.045909881591797,
        "learning_rate": 0.0001623096765288406,
        "epoch": 0.4298382889932186,
        "step": 5768
    },
    {
        "loss": 1.8188,
        "grad_norm": 3.3204078674316406,
        "learning_rate": 0.00016225462496702374,
        "epoch": 0.42991281019450034,
        "step": 5769
    },
    {
        "loss": 1.2565,
        "grad_norm": 2.8623812198638916,
        "learning_rate": 0.0001621995425809036,
        "epoch": 0.4299873313957821,
        "step": 5770
    },
    {
        "loss": 2.4511,
        "grad_norm": 1.660725712776184,
        "learning_rate": 0.0001621444293977533,
        "epoch": 0.43006185259706387,
        "step": 5771
    },
    {
        "loss": 3.0424,
        "grad_norm": 3.058516502380371,
        "learning_rate": 0.0001620892854448612,
        "epoch": 0.43013637379834563,
        "step": 5772
    },
    {
        "loss": 2.9835,
        "grad_norm": 2.867692470550537,
        "learning_rate": 0.00016203411074953083,
        "epoch": 0.4302108949996274,
        "step": 5773
    },
    {
        "loss": 2.204,
        "grad_norm": 4.058262825012207,
        "learning_rate": 0.00016197890533908095,
        "epoch": 0.43028541620090915,
        "step": 5774
    },
    {
        "loss": 2.1174,
        "grad_norm": 2.795452356338501,
        "learning_rate": 0.00016192366924084564,
        "epoch": 0.4303599374021909,
        "step": 5775
    },
    {
        "loss": 2.4484,
        "grad_norm": 3.5268912315368652,
        "learning_rate": 0.0001618684024821741,
        "epoch": 0.4304344586034727,
        "step": 5776
    },
    {
        "loss": 2.4583,
        "grad_norm": 2.926074266433716,
        "learning_rate": 0.00016181310509043065,
        "epoch": 0.43050897980475444,
        "step": 5777
    },
    {
        "loss": 2.8284,
        "grad_norm": 3.322676658630371,
        "learning_rate": 0.00016175777709299496,
        "epoch": 0.4305835010060362,
        "step": 5778
    },
    {
        "loss": 2.0071,
        "grad_norm": 2.6374332904815674,
        "learning_rate": 0.00016170241851726154,
        "epoch": 0.43065802220731797,
        "step": 5779
    },
    {
        "loss": 2.1614,
        "grad_norm": 3.0380401611328125,
        "learning_rate": 0.00016164702939064037,
        "epoch": 0.43073254340859973,
        "step": 5780
    },
    {
        "loss": 1.5032,
        "grad_norm": 1.8484036922454834,
        "learning_rate": 0.0001615916097405564,
        "epoch": 0.4308070646098815,
        "step": 5781
    },
    {
        "loss": 2.711,
        "grad_norm": 2.151193141937256,
        "learning_rate": 0.00016153615959444976,
        "epoch": 0.43088158581116326,
        "step": 5782
    },
    {
        "loss": 2.4528,
        "grad_norm": 3.039532423019409,
        "learning_rate": 0.0001614806789797756,
        "epoch": 0.430956107012445,
        "step": 5783
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.473177433013916,
        "learning_rate": 0.00016142516792400408,
        "epoch": 0.4310306282137268,
        "step": 5784
    },
    {
        "loss": 2.2755,
        "grad_norm": 3.418851613998413,
        "learning_rate": 0.00016136962645462065,
        "epoch": 0.43110514941500855,
        "step": 5785
    },
    {
        "loss": 2.3247,
        "grad_norm": 5.187865734100342,
        "learning_rate": 0.00016131405459912575,
        "epoch": 0.4311796706162903,
        "step": 5786
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.937908411026001,
        "learning_rate": 0.00016125845238503477,
        "epoch": 0.4312541918175721,
        "step": 5787
    },
    {
        "loss": 3.1342,
        "grad_norm": 1.2651275396347046,
        "learning_rate": 0.0001612028198398781,
        "epoch": 0.43132871301885384,
        "step": 5788
    },
    {
        "loss": 1.5363,
        "grad_norm": 3.690717935562134,
        "learning_rate": 0.00016114715699120136,
        "epoch": 0.43140323422013566,
        "step": 5789
    },
    {
        "loss": 1.91,
        "grad_norm": 3.0738718509674072,
        "learning_rate": 0.00016109146386656496,
        "epoch": 0.4314777554214174,
        "step": 5790
    },
    {
        "loss": 2.5522,
        "grad_norm": 1.9607539176940918,
        "learning_rate": 0.00016103574049354446,
        "epoch": 0.4315522766226992,
        "step": 5791
    },
    {
        "loss": 3.3168,
        "grad_norm": 3.368119716644287,
        "learning_rate": 0.00016097998689973037,
        "epoch": 0.43162679782398095,
        "step": 5792
    },
    {
        "loss": 2.8981,
        "grad_norm": 3.327540636062622,
        "learning_rate": 0.00016092420311272803,
        "epoch": 0.4317013190252627,
        "step": 5793
    },
    {
        "loss": 2.8416,
        "grad_norm": 4.211977958679199,
        "learning_rate": 0.0001608683891601578,
        "epoch": 0.43177584022654447,
        "step": 5794
    },
    {
        "loss": 1.9956,
        "grad_norm": 4.161088466644287,
        "learning_rate": 0.00016081254506965505,
        "epoch": 0.43185036142782623,
        "step": 5795
    },
    {
        "loss": 1.5136,
        "grad_norm": 2.651292324066162,
        "learning_rate": 0.00016075667086887005,
        "epoch": 0.431924882629108,
        "step": 5796
    },
    {
        "loss": 1.7417,
        "grad_norm": 2.8397560119628906,
        "learning_rate": 0.0001607007665854679,
        "epoch": 0.43199940383038976,
        "step": 5797
    },
    {
        "loss": 2.4865,
        "grad_norm": 2.963524580001831,
        "learning_rate": 0.0001606448322471286,
        "epoch": 0.4320739250316715,
        "step": 5798
    },
    {
        "loss": 2.1031,
        "grad_norm": 4.352886199951172,
        "learning_rate": 0.0001605888678815471,
        "epoch": 0.4321484462329533,
        "step": 5799
    },
    {
        "loss": 2.2681,
        "grad_norm": 3.1019768714904785,
        "learning_rate": 0.00016053287351643322,
        "epoch": 0.43222296743423505,
        "step": 5800
    },
    {
        "loss": 2.5987,
        "grad_norm": 3.754054307937622,
        "learning_rate": 0.00016047684917951165,
        "epoch": 0.4322974886355168,
        "step": 5801
    },
    {
        "loss": 2.4699,
        "grad_norm": 2.5363147258758545,
        "learning_rate": 0.00016042079489852172,
        "epoch": 0.4323720098367986,
        "step": 5802
    },
    {
        "loss": 2.1459,
        "grad_norm": 3.224294424057007,
        "learning_rate": 0.00016036471070121793,
        "epoch": 0.43244653103808034,
        "step": 5803
    },
    {
        "loss": 2.425,
        "grad_norm": 2.5141148567199707,
        "learning_rate": 0.00016030859661536923,
        "epoch": 0.4325210522393621,
        "step": 5804
    },
    {
        "loss": 2.031,
        "grad_norm": 3.41030216217041,
        "learning_rate": 0.00016025245266875962,
        "epoch": 0.43259557344064387,
        "step": 5805
    },
    {
        "loss": 2.5672,
        "grad_norm": 1.8980321884155273,
        "learning_rate": 0.00016019627888918788,
        "epoch": 0.43267009464192563,
        "step": 5806
    },
    {
        "loss": 2.2851,
        "grad_norm": 3.929367780685425,
        "learning_rate": 0.00016014007530446738,
        "epoch": 0.4327446158432074,
        "step": 5807
    },
    {
        "loss": 2.0827,
        "grad_norm": 3.3411171436309814,
        "learning_rate": 0.00016008384194242633,
        "epoch": 0.43281913704448916,
        "step": 5808
    },
    {
        "loss": 2.5357,
        "grad_norm": 2.092008113861084,
        "learning_rate": 0.0001600275788309078,
        "epoch": 0.4328936582457709,
        "step": 5809
    },
    {
        "loss": 2.3309,
        "grad_norm": 4.037750244140625,
        "learning_rate": 0.00015997128599776948,
        "epoch": 0.4329681794470527,
        "step": 5810
    },
    {
        "loss": 2.7122,
        "grad_norm": 3.8274598121643066,
        "learning_rate": 0.00015991496347088376,
        "epoch": 0.43304270064833444,
        "step": 5811
    },
    {
        "loss": 2.1641,
        "grad_norm": 3.1686999797821045,
        "learning_rate": 0.00015985861127813778,
        "epoch": 0.4331172218496162,
        "step": 5812
    },
    {
        "loss": 2.0047,
        "grad_norm": 2.9290738105773926,
        "learning_rate": 0.00015980222944743337,
        "epoch": 0.43319174305089797,
        "step": 5813
    },
    {
        "loss": 2.546,
        "grad_norm": 3.035191297531128,
        "learning_rate": 0.000159745818006687,
        "epoch": 0.43326626425217973,
        "step": 5814
    },
    {
        "loss": 1.5648,
        "grad_norm": 3.2105963230133057,
        "learning_rate": 0.0001596893769838299,
        "epoch": 0.4333407854534615,
        "step": 5815
    },
    {
        "loss": 1.7004,
        "grad_norm": 3.6365647315979004,
        "learning_rate": 0.0001596329064068077,
        "epoch": 0.43341530665474326,
        "step": 5816
    },
    {
        "loss": 2.5575,
        "grad_norm": 3.6129322052001953,
        "learning_rate": 0.00015957640630358104,
        "epoch": 0.433489827856025,
        "step": 5817
    },
    {
        "loss": 2.567,
        "grad_norm": 5.461704730987549,
        "learning_rate": 0.00015951987670212477,
        "epoch": 0.4335643490573068,
        "step": 5818
    },
    {
        "loss": 2.3311,
        "grad_norm": 1.829317331314087,
        "learning_rate": 0.00015946331763042865,
        "epoch": 0.43363887025858855,
        "step": 5819
    },
    {
        "loss": 2.754,
        "grad_norm": 2.389180898666382,
        "learning_rate": 0.00015940672911649698,
        "epoch": 0.4337133914598703,
        "step": 5820
    },
    {
        "loss": 3.3221,
        "grad_norm": 4.719667911529541,
        "learning_rate": 0.00015935011118834846,
        "epoch": 0.4337879126611521,
        "step": 5821
    },
    {
        "loss": 2.4343,
        "grad_norm": 3.319369077682495,
        "learning_rate": 0.00015929346387401659,
        "epoch": 0.43386243386243384,
        "step": 5822
    },
    {
        "loss": 1.6354,
        "grad_norm": 3.661708116531372,
        "learning_rate": 0.00015923678720154927,
        "epoch": 0.4339369550637156,
        "step": 5823
    },
    {
        "loss": 2.2755,
        "grad_norm": 2.7171430587768555,
        "learning_rate": 0.00015918008119900895,
        "epoch": 0.43401147626499736,
        "step": 5824
    },
    {
        "loss": 2.7655,
        "grad_norm": 2.298784017562866,
        "learning_rate": 0.00015912334589447268,
        "epoch": 0.4340859974662792,
        "step": 5825
    },
    {
        "loss": 2.1297,
        "grad_norm": 3.0582659244537354,
        "learning_rate": 0.0001590665813160319,
        "epoch": 0.43416051866756095,
        "step": 5826
    },
    {
        "loss": 2.4209,
        "grad_norm": 2.07281494140625,
        "learning_rate": 0.00015900978749179272,
        "epoch": 0.4342350398688427,
        "step": 5827
    },
    {
        "loss": 2.6442,
        "grad_norm": 2.153423547744751,
        "learning_rate": 0.00015895296444987545,
        "epoch": 0.4343095610701245,
        "step": 5828
    },
    {
        "loss": 2.1689,
        "grad_norm": 2.2310845851898193,
        "learning_rate": 0.00015889611221841526,
        "epoch": 0.43438408227140624,
        "step": 5829
    },
    {
        "loss": 2.5288,
        "grad_norm": 2.1294751167297363,
        "learning_rate": 0.00015883923082556133,
        "epoch": 0.434458603472688,
        "step": 5830
    },
    {
        "loss": 2.1817,
        "grad_norm": 2.127882719039917,
        "learning_rate": 0.0001587823202994776,
        "epoch": 0.43453312467396976,
        "step": 5831
    },
    {
        "loss": 2.0558,
        "grad_norm": 5.748689651489258,
        "learning_rate": 0.00015872538066834238,
        "epoch": 0.4346076458752515,
        "step": 5832
    },
    {
        "loss": 2.2596,
        "grad_norm": 3.0839691162109375,
        "learning_rate": 0.00015866841196034831,
        "epoch": 0.4346821670765333,
        "step": 5833
    },
    {
        "loss": 2.2867,
        "grad_norm": 2.5751638412475586,
        "learning_rate": 0.00015861141420370247,
        "epoch": 0.43475668827781505,
        "step": 5834
    },
    {
        "loss": 2.3664,
        "grad_norm": 2.740231513977051,
        "learning_rate": 0.00015855438742662628,
        "epoch": 0.4348312094790968,
        "step": 5835
    },
    {
        "loss": 2.6009,
        "grad_norm": 3.1238248348236084,
        "learning_rate": 0.00015849733165735553,
        "epoch": 0.4349057306803786,
        "step": 5836
    },
    {
        "loss": 2.4933,
        "grad_norm": 1.7519265413284302,
        "learning_rate": 0.00015844024692414054,
        "epoch": 0.43498025188166034,
        "step": 5837
    },
    {
        "loss": 2.4654,
        "grad_norm": 2.2573354244232178,
        "learning_rate": 0.0001583831332552457,
        "epoch": 0.4350547730829421,
        "step": 5838
    },
    {
        "loss": 2.7099,
        "grad_norm": 4.659080505371094,
        "learning_rate": 0.00015832599067894997,
        "epoch": 0.43512929428422387,
        "step": 5839
    },
    {
        "loss": 2.6059,
        "grad_norm": 2.411740779876709,
        "learning_rate": 0.00015826881922354632,
        "epoch": 0.43520381548550563,
        "step": 5840
    },
    {
        "loss": 1.9189,
        "grad_norm": 2.830298900604248,
        "learning_rate": 0.0001582116189173424,
        "epoch": 0.4352783366867874,
        "step": 5841
    },
    {
        "loss": 1.8086,
        "grad_norm": 3.2404372692108154,
        "learning_rate": 0.00015815438978865985,
        "epoch": 0.43535285788806916,
        "step": 5842
    },
    {
        "loss": 1.9264,
        "grad_norm": 4.08834171295166,
        "learning_rate": 0.00015809713186583482,
        "epoch": 0.4354273790893509,
        "step": 5843
    },
    {
        "loss": 2.4478,
        "grad_norm": 3.5890655517578125,
        "learning_rate": 0.00015803984517721747,
        "epoch": 0.4355019002906327,
        "step": 5844
    },
    {
        "loss": 1.0402,
        "grad_norm": 3.3530569076538086,
        "learning_rate": 0.00015798252975117224,
        "epoch": 0.43557642149191445,
        "step": 5845
    },
    {
        "loss": 2.8912,
        "grad_norm": 2.1436712741851807,
        "learning_rate": 0.00015792518561607799,
        "epoch": 0.4356509426931962,
        "step": 5846
    },
    {
        "loss": 2.9556,
        "grad_norm": 3.066300392150879,
        "learning_rate": 0.00015786781280032773,
        "epoch": 0.43572546389447797,
        "step": 5847
    },
    {
        "loss": 2.3903,
        "grad_norm": 2.5541789531707764,
        "learning_rate": 0.00015781041133232853,
        "epoch": 0.43579998509575973,
        "step": 5848
    },
    {
        "loss": 2.4573,
        "grad_norm": 3.221919059753418,
        "learning_rate": 0.00015775298124050168,
        "epoch": 0.4358745062970415,
        "step": 5849
    },
    {
        "loss": 3.1485,
        "grad_norm": 3.382136106491089,
        "learning_rate": 0.0001576955225532828,
        "epoch": 0.43594902749832326,
        "step": 5850
    },
    {
        "loss": 2.2861,
        "grad_norm": 2.480410575866699,
        "learning_rate": 0.00015763803529912152,
        "epoch": 0.436023548699605,
        "step": 5851
    },
    {
        "loss": 2.2971,
        "grad_norm": 5.297915935516357,
        "learning_rate": 0.00015758051950648166,
        "epoch": 0.4360980699008868,
        "step": 5852
    },
    {
        "loss": 2.3944,
        "grad_norm": 3.4133718013763428,
        "learning_rate": 0.00015752297520384126,
        "epoch": 0.43617259110216855,
        "step": 5853
    },
    {
        "loss": 3.0383,
        "grad_norm": 2.0824692249298096,
        "learning_rate": 0.00015746540241969235,
        "epoch": 0.4362471123034503,
        "step": 5854
    },
    {
        "loss": 2.5401,
        "grad_norm": 2.1330275535583496,
        "learning_rate": 0.00015740780118254097,
        "epoch": 0.4363216335047321,
        "step": 5855
    },
    {
        "loss": 2.3631,
        "grad_norm": 2.029352903366089,
        "learning_rate": 0.0001573501715209075,
        "epoch": 0.43639615470601384,
        "step": 5856
    },
    {
        "loss": 2.438,
        "grad_norm": 3.9321868419647217,
        "learning_rate": 0.00015729251346332628,
        "epoch": 0.4364706759072956,
        "step": 5857
    },
    {
        "loss": 2.5011,
        "grad_norm": 2.5065605640411377,
        "learning_rate": 0.00015723482703834573,
        "epoch": 0.43654519710857737,
        "step": 5858
    },
    {
        "loss": 2.7519,
        "grad_norm": 2.4231858253479004,
        "learning_rate": 0.00015717711227452812,
        "epoch": 0.43661971830985913,
        "step": 5859
    },
    {
        "loss": 2.9067,
        "grad_norm": 2.7005128860473633,
        "learning_rate": 0.00015711936920045008,
        "epoch": 0.43669423951114095,
        "step": 5860
    },
    {
        "loss": 2.1101,
        "grad_norm": 3.1400389671325684,
        "learning_rate": 0.000157061597844702,
        "epoch": 0.4367687607124227,
        "step": 5861
    },
    {
        "loss": 1.4556,
        "grad_norm": 2.8642876148223877,
        "learning_rate": 0.0001570037982358885,
        "epoch": 0.4368432819137045,
        "step": 5862
    },
    {
        "loss": 1.6299,
        "grad_norm": 3.093029737472534,
        "learning_rate": 0.00015694597040262793,
        "epoch": 0.43691780311498624,
        "step": 5863
    },
    {
        "loss": 2.1283,
        "grad_norm": 2.219900131225586,
        "learning_rate": 0.00015688811437355283,
        "epoch": 0.436992324316268,
        "step": 5864
    },
    {
        "loss": 2.7019,
        "grad_norm": 2.5471420288085938,
        "learning_rate": 0.0001568302301773095,
        "epoch": 0.43706684551754976,
        "step": 5865
    },
    {
        "loss": 2.3882,
        "grad_norm": 2.5448620319366455,
        "learning_rate": 0.0001567723178425584,
        "epoch": 0.4371413667188315,
        "step": 5866
    },
    {
        "loss": 2.229,
        "grad_norm": 2.6009011268615723,
        "learning_rate": 0.00015671437739797384,
        "epoch": 0.4372158879201133,
        "step": 5867
    },
    {
        "loss": 2.3452,
        "grad_norm": 1.6305598020553589,
        "learning_rate": 0.00015665640887224403,
        "epoch": 0.43729040912139505,
        "step": 5868
    },
    {
        "loss": 2.5645,
        "grad_norm": 3.9158480167388916,
        "learning_rate": 0.00015659841229407096,
        "epoch": 0.4373649303226768,
        "step": 5869
    },
    {
        "loss": 2.5687,
        "grad_norm": 2.7099668979644775,
        "learning_rate": 0.0001565403876921707,
        "epoch": 0.4374394515239586,
        "step": 5870
    },
    {
        "loss": 2.652,
        "grad_norm": 2.4747490882873535,
        "learning_rate": 0.00015648233509527314,
        "epoch": 0.43751397272524034,
        "step": 5871
    },
    {
        "loss": 1.6423,
        "grad_norm": 3.5661888122558594,
        "learning_rate": 0.00015642425453212214,
        "epoch": 0.4375884939265221,
        "step": 5872
    },
    {
        "loss": 3.1924,
        "grad_norm": 1.9644455909729004,
        "learning_rate": 0.0001563661460314751,
        "epoch": 0.43766301512780387,
        "step": 5873
    },
    {
        "loss": 2.8231,
        "grad_norm": 2.1234166622161865,
        "learning_rate": 0.00015630800962210356,
        "epoch": 0.43773753632908563,
        "step": 5874
    },
    {
        "loss": 2.5864,
        "grad_norm": 2.2315962314605713,
        "learning_rate": 0.0001562498453327927,
        "epoch": 0.4378120575303674,
        "step": 5875
    },
    {
        "loss": 1.9478,
        "grad_norm": 2.7527377605438232,
        "learning_rate": 0.0001561916531923416,
        "epoch": 0.43788657873164916,
        "step": 5876
    },
    {
        "loss": 2.6159,
        "grad_norm": 3.095541000366211,
        "learning_rate": 0.00015613343322956307,
        "epoch": 0.4379610999329309,
        "step": 5877
    },
    {
        "loss": 3.0955,
        "grad_norm": 2.481966257095337,
        "learning_rate": 0.00015607518547328377,
        "epoch": 0.4380356211342127,
        "step": 5878
    },
    {
        "loss": 2.184,
        "grad_norm": 2.3679637908935547,
        "learning_rate": 0.00015601690995234395,
        "epoch": 0.43811014233549445,
        "step": 5879
    },
    {
        "loss": 1.9952,
        "grad_norm": 5.735113143920898,
        "learning_rate": 0.00015595860669559783,
        "epoch": 0.4381846635367762,
        "step": 5880
    },
    {
        "loss": 2.188,
        "grad_norm": 4.631777286529541,
        "learning_rate": 0.00015590027573191332,
        "epoch": 0.43825918473805797,
        "step": 5881
    },
    {
        "loss": 2.5162,
        "grad_norm": 2.9613540172576904,
        "learning_rate": 0.0001558419170901718,
        "epoch": 0.43833370593933974,
        "step": 5882
    },
    {
        "loss": 2.2606,
        "grad_norm": 3.1437487602233887,
        "learning_rate": 0.00015578353079926868,
        "epoch": 0.4384082271406215,
        "step": 5883
    },
    {
        "loss": 2.7925,
        "grad_norm": 2.2509264945983887,
        "learning_rate": 0.000155725116888113,
        "epoch": 0.43848274834190326,
        "step": 5884
    },
    {
        "loss": 2.316,
        "grad_norm": 3.0809690952301025,
        "learning_rate": 0.00015566667538562717,
        "epoch": 0.438557269543185,
        "step": 5885
    },
    {
        "loss": 2.2646,
        "grad_norm": 3.3140904903411865,
        "learning_rate": 0.00015560820632074774,
        "epoch": 0.4386317907444668,
        "step": 5886
    },
    {
        "loss": 2.4152,
        "grad_norm": 2.9097583293914795,
        "learning_rate": 0.00015554970972242444,
        "epoch": 0.43870631194574855,
        "step": 5887
    },
    {
        "loss": 2.3915,
        "grad_norm": 2.0784387588500977,
        "learning_rate": 0.00015549118561962103,
        "epoch": 0.4387808331470303,
        "step": 5888
    },
    {
        "loss": 2.3261,
        "grad_norm": 2.2944674491882324,
        "learning_rate": 0.0001554326340413146,
        "epoch": 0.4388553543483121,
        "step": 5889
    },
    {
        "loss": 2.6779,
        "grad_norm": 2.6978893280029297,
        "learning_rate": 0.00015537405501649598,
        "epoch": 0.43892987554959384,
        "step": 5890
    },
    {
        "loss": 2.0107,
        "grad_norm": 3.156402587890625,
        "learning_rate": 0.00015531544857416956,
        "epoch": 0.4390043967508756,
        "step": 5891
    },
    {
        "loss": 2.9628,
        "grad_norm": 2.4532558917999268,
        "learning_rate": 0.0001552568147433533,
        "epoch": 0.43907891795215737,
        "step": 5892
    },
    {
        "loss": 1.9284,
        "grad_norm": 2.9580094814300537,
        "learning_rate": 0.00015519815355307873,
        "epoch": 0.43915343915343913,
        "step": 5893
    },
    {
        "loss": 2.2373,
        "grad_norm": 4.478479385375977,
        "learning_rate": 0.000155139465032391,
        "epoch": 0.4392279603547209,
        "step": 5894
    },
    {
        "loss": 1.62,
        "grad_norm": 2.440481662750244,
        "learning_rate": 0.00015508074921034867,
        "epoch": 0.43930248155600266,
        "step": 5895
    },
    {
        "loss": 2.547,
        "grad_norm": 2.645608901977539,
        "learning_rate": 0.00015502200611602383,
        "epoch": 0.4393770027572845,
        "step": 5896
    },
    {
        "loss": 2.4273,
        "grad_norm": 2.0048110485076904,
        "learning_rate": 0.00015496323577850212,
        "epoch": 0.43945152395856624,
        "step": 5897
    },
    {
        "loss": 2.7144,
        "grad_norm": 2.563201427459717,
        "learning_rate": 0.0001549044382268827,
        "epoch": 0.439526045159848,
        "step": 5898
    },
    {
        "loss": 2.1755,
        "grad_norm": 2.5012269020080566,
        "learning_rate": 0.0001548456134902781,
        "epoch": 0.43960056636112976,
        "step": 5899
    },
    {
        "loss": 2.3354,
        "grad_norm": 2.5074141025543213,
        "learning_rate": 0.00015478676159781445,
        "epoch": 0.4396750875624115,
        "step": 5900
    },
    {
        "loss": 2.6117,
        "grad_norm": 3.8042244911193848,
        "learning_rate": 0.00015472788257863123,
        "epoch": 0.4397496087636933,
        "step": 5901
    },
    {
        "loss": 1.9048,
        "grad_norm": 3.3762364387512207,
        "learning_rate": 0.00015466897646188125,
        "epoch": 0.43982412996497505,
        "step": 5902
    },
    {
        "loss": 2.2255,
        "grad_norm": 3.420637845993042,
        "learning_rate": 0.000154610043276731,
        "epoch": 0.4398986511662568,
        "step": 5903
    },
    {
        "loss": 2.627,
        "grad_norm": 2.4843084812164307,
        "learning_rate": 0.00015455108305236027,
        "epoch": 0.4399731723675386,
        "step": 5904
    },
    {
        "loss": 2.8309,
        "grad_norm": 2.2462379932403564,
        "learning_rate": 0.0001544920958179621,
        "epoch": 0.44004769356882034,
        "step": 5905
    },
    {
        "loss": 2.6983,
        "grad_norm": 2.831916570663452,
        "learning_rate": 0.00015443308160274292,
        "epoch": 0.4401222147701021,
        "step": 5906
    },
    {
        "loss": 2.0404,
        "grad_norm": 2.5168380737304688,
        "learning_rate": 0.0001543740404359227,
        "epoch": 0.44019673597138387,
        "step": 5907
    },
    {
        "loss": 2.4378,
        "grad_norm": 3.551720142364502,
        "learning_rate": 0.00015431497234673475,
        "epoch": 0.44027125717266563,
        "step": 5908
    },
    {
        "loss": 2.4772,
        "grad_norm": 3.178074359893799,
        "learning_rate": 0.0001542558773644255,
        "epoch": 0.4403457783739474,
        "step": 5909
    },
    {
        "loss": 2.679,
        "grad_norm": 1.5387872457504272,
        "learning_rate": 0.00015419675551825475,
        "epoch": 0.44042029957522916,
        "step": 5910
    },
    {
        "loss": 2.5315,
        "grad_norm": 2.3670716285705566,
        "learning_rate": 0.00015413760683749577,
        "epoch": 0.4404948207765109,
        "step": 5911
    },
    {
        "loss": 2.0561,
        "grad_norm": 2.3396971225738525,
        "learning_rate": 0.00015407843135143497,
        "epoch": 0.4405693419777927,
        "step": 5912
    },
    {
        "loss": 2.6305,
        "grad_norm": 1.4896354675292969,
        "learning_rate": 0.00015401922908937204,
        "epoch": 0.44064386317907445,
        "step": 5913
    },
    {
        "loss": 2.2689,
        "grad_norm": 6.8706536293029785,
        "learning_rate": 0.00015396000008062013,
        "epoch": 0.4407183843803562,
        "step": 5914
    },
    {
        "loss": 2.4649,
        "grad_norm": 3.6596550941467285,
        "learning_rate": 0.00015390074435450525,
        "epoch": 0.440792905581638,
        "step": 5915
    },
    {
        "loss": 1.8147,
        "grad_norm": 3.870645761489868,
        "learning_rate": 0.00015384146194036692,
        "epoch": 0.44086742678291974,
        "step": 5916
    },
    {
        "loss": 2.768,
        "grad_norm": 2.0510449409484863,
        "learning_rate": 0.0001537821528675578,
        "epoch": 0.4409419479842015,
        "step": 5917
    },
    {
        "loss": 1.3716,
        "grad_norm": 3.8116462230682373,
        "learning_rate": 0.0001537228171654438,
        "epoch": 0.44101646918548326,
        "step": 5918
    },
    {
        "loss": 2.8875,
        "grad_norm": 2.7637698650360107,
        "learning_rate": 0.00015366345486340398,
        "epoch": 0.441090990386765,
        "step": 5919
    },
    {
        "loss": 2.9816,
        "grad_norm": 5.065192222595215,
        "learning_rate": 0.00015360406599083042,
        "epoch": 0.4411655115880468,
        "step": 5920
    },
    {
        "loss": 2.962,
        "grad_norm": 3.562507152557373,
        "learning_rate": 0.0001535446505771286,
        "epoch": 0.44124003278932855,
        "step": 5921
    },
    {
        "loss": 1.8888,
        "grad_norm": 3.733607769012451,
        "learning_rate": 0.00015348520865171705,
        "epoch": 0.4413145539906103,
        "step": 5922
    },
    {
        "loss": 1.5158,
        "grad_norm": 3.5474965572357178,
        "learning_rate": 0.00015342574024402743,
        "epoch": 0.4413890751918921,
        "step": 5923
    },
    {
        "loss": 2.059,
        "grad_norm": 3.5126938819885254,
        "learning_rate": 0.0001533662453835044,
        "epoch": 0.44146359639317384,
        "step": 5924
    },
    {
        "loss": 2.9212,
        "grad_norm": 2.647873878479004,
        "learning_rate": 0.00015330672409960593,
        "epoch": 0.4415381175944556,
        "step": 5925
    },
    {
        "loss": 2.1032,
        "grad_norm": 2.942748546600342,
        "learning_rate": 0.00015324717642180283,
        "epoch": 0.44161263879573737,
        "step": 5926
    },
    {
        "loss": 2.4759,
        "grad_norm": 3.31016206741333,
        "learning_rate": 0.00015318760237957916,
        "epoch": 0.44168715999701913,
        "step": 5927
    },
    {
        "loss": 2.4101,
        "grad_norm": 3.8542025089263916,
        "learning_rate": 0.00015312800200243206,
        "epoch": 0.4417616811983009,
        "step": 5928
    },
    {
        "loss": 2.6376,
        "grad_norm": 3.484182357788086,
        "learning_rate": 0.0001530683753198716,
        "epoch": 0.44183620239958266,
        "step": 5929
    },
    {
        "loss": 2.3446,
        "grad_norm": 4.4196882247924805,
        "learning_rate": 0.00015300872236142076,
        "epoch": 0.4419107236008644,
        "step": 5930
    },
    {
        "loss": 2.3082,
        "grad_norm": 3.6376445293426514,
        "learning_rate": 0.0001529490431566158,
        "epoch": 0.44198524480214624,
        "step": 5931
    },
    {
        "loss": 1.955,
        "grad_norm": 2.4251630306243896,
        "learning_rate": 0.0001528893377350058,
        "epoch": 0.442059766003428,
        "step": 5932
    },
    {
        "loss": 2.591,
        "grad_norm": 2.6091551780700684,
        "learning_rate": 0.00015282960612615298,
        "epoch": 0.44213428720470976,
        "step": 5933
    },
    {
        "loss": 2.9142,
        "grad_norm": 2.590195655822754,
        "learning_rate": 0.00015276984835963225,
        "epoch": 0.4422088084059915,
        "step": 5934
    },
    {
        "loss": 2.3709,
        "grad_norm": 2.8938562870025635,
        "learning_rate": 0.00015271006446503175,
        "epoch": 0.4422833296072733,
        "step": 5935
    },
    {
        "loss": 2.4558,
        "grad_norm": 3.7046926021575928,
        "learning_rate": 0.00015265025447195238,
        "epoch": 0.44235785080855505,
        "step": 5936
    },
    {
        "loss": 2.1554,
        "grad_norm": 2.848546028137207,
        "learning_rate": 0.0001525904184100081,
        "epoch": 0.4424323720098368,
        "step": 5937
    },
    {
        "loss": 2.1271,
        "grad_norm": 2.59013295173645,
        "learning_rate": 0.00015253055630882564,
        "epoch": 0.4425068932111186,
        "step": 5938
    },
    {
        "loss": 1.5633,
        "grad_norm": 4.352569103240967,
        "learning_rate": 0.00015247066819804473,
        "epoch": 0.44258141441240034,
        "step": 5939
    },
    {
        "loss": 1.7111,
        "grad_norm": 4.787845134735107,
        "learning_rate": 0.00015241075410731786,
        "epoch": 0.4426559356136821,
        "step": 5940
    },
    {
        "loss": 2.3573,
        "grad_norm": 4.381232738494873,
        "learning_rate": 0.0001523508140663105,
        "epoch": 0.44273045681496387,
        "step": 5941
    },
    {
        "loss": 2.4717,
        "grad_norm": 2.486931324005127,
        "learning_rate": 0.000152290848104701,
        "epoch": 0.44280497801624563,
        "step": 5942
    },
    {
        "loss": 1.9978,
        "grad_norm": 3.3834824562072754,
        "learning_rate": 0.00015223085625218032,
        "epoch": 0.4428794992175274,
        "step": 5943
    },
    {
        "loss": 0.8433,
        "grad_norm": 4.031637191772461,
        "learning_rate": 0.00015217083853845252,
        "epoch": 0.44295402041880916,
        "step": 5944
    },
    {
        "loss": 2.2519,
        "grad_norm": 3.0058937072753906,
        "learning_rate": 0.0001521107949932343,
        "epoch": 0.4430285416200909,
        "step": 5945
    },
    {
        "loss": 2.1959,
        "grad_norm": 3.430973768234253,
        "learning_rate": 0.00015205072564625514,
        "epoch": 0.4431030628213727,
        "step": 5946
    },
    {
        "loss": 2.8949,
        "grad_norm": 3.7451276779174805,
        "learning_rate": 0.00015199063052725745,
        "epoch": 0.44317758402265445,
        "step": 5947
    },
    {
        "loss": 2.8568,
        "grad_norm": 2.7294204235076904,
        "learning_rate": 0.00015193050966599616,
        "epoch": 0.4432521052239362,
        "step": 5948
    },
    {
        "loss": 2.5998,
        "grad_norm": 2.5425057411193848,
        "learning_rate": 0.00015187036309223919,
        "epoch": 0.443326626425218,
        "step": 5949
    },
    {
        "loss": 2.8909,
        "grad_norm": 4.44584321975708,
        "learning_rate": 0.00015181019083576695,
        "epoch": 0.44340114762649974,
        "step": 5950
    },
    {
        "loss": 2.5959,
        "grad_norm": 4.087203502655029,
        "learning_rate": 0.00015174999292637286,
        "epoch": 0.4434756688277815,
        "step": 5951
    },
    {
        "loss": 2.244,
        "grad_norm": 2.413881778717041,
        "learning_rate": 0.00015168976939386268,
        "epoch": 0.44355019002906326,
        "step": 5952
    },
    {
        "loss": 2.6634,
        "grad_norm": 2.6456470489501953,
        "learning_rate": 0.0001516295202680552,
        "epoch": 0.443624711230345,
        "step": 5953
    },
    {
        "loss": 2.6619,
        "grad_norm": 3.0237550735473633,
        "learning_rate": 0.00015156924557878165,
        "epoch": 0.4436992324316268,
        "step": 5954
    },
    {
        "loss": 2.6779,
        "grad_norm": 2.0971901416778564,
        "learning_rate": 0.00015150894535588614,
        "epoch": 0.44377375363290855,
        "step": 5955
    },
    {
        "loss": 2.3636,
        "grad_norm": 2.956641674041748,
        "learning_rate": 0.00015144861962922517,
        "epoch": 0.4438482748341903,
        "step": 5956
    },
    {
        "loss": 2.4204,
        "grad_norm": 2.7027955055236816,
        "learning_rate": 0.00015138826842866796,
        "epoch": 0.4439227960354721,
        "step": 5957
    },
    {
        "loss": 2.196,
        "grad_norm": 2.723707437515259,
        "learning_rate": 0.0001513278917840964,
        "epoch": 0.44399731723675384,
        "step": 5958
    },
    {
        "loss": 2.052,
        "grad_norm": 4.270866870880127,
        "learning_rate": 0.000151267489725405,
        "epoch": 0.4440718384380356,
        "step": 5959
    },
    {
        "loss": 2.9467,
        "grad_norm": 2.729400396347046,
        "learning_rate": 0.00015120706228250068,
        "epoch": 0.44414635963931737,
        "step": 5960
    },
    {
        "loss": 2.6365,
        "grad_norm": 2.928988456726074,
        "learning_rate": 0.00015114660948530316,
        "epoch": 0.44422088084059913,
        "step": 5961
    },
    {
        "loss": 2.6101,
        "grad_norm": 2.3031704425811768,
        "learning_rate": 0.00015108613136374452,
        "epoch": 0.4442954020418809,
        "step": 5962
    },
    {
        "loss": 2.7715,
        "grad_norm": 2.209087371826172,
        "learning_rate": 0.00015102562794776952,
        "epoch": 0.44436992324316266,
        "step": 5963
    },
    {
        "loss": 1.7225,
        "grad_norm": 3.9218358993530273,
        "learning_rate": 0.00015096509926733532,
        "epoch": 0.4444444444444444,
        "step": 5964
    },
    {
        "loss": 2.2559,
        "grad_norm": 3.6285645961761475,
        "learning_rate": 0.00015090454535241174,
        "epoch": 0.4445189656457262,
        "step": 5965
    },
    {
        "loss": 1.9586,
        "grad_norm": 2.434171438217163,
        "learning_rate": 0.00015084396623298095,
        "epoch": 0.444593486847008,
        "step": 5966
    },
    {
        "loss": 2.7619,
        "grad_norm": 1.998173475265503,
        "learning_rate": 0.00015078336193903762,
        "epoch": 0.44466800804828976,
        "step": 5967
    },
    {
        "loss": 2.1311,
        "grad_norm": 3.279294490814209,
        "learning_rate": 0.00015072273250058894,
        "epoch": 0.4447425292495715,
        "step": 5968
    },
    {
        "loss": 2.0707,
        "grad_norm": 3.240924596786499,
        "learning_rate": 0.0001506620779476546,
        "epoch": 0.4448170504508533,
        "step": 5969
    },
    {
        "loss": 2.5882,
        "grad_norm": 1.8637175559997559,
        "learning_rate": 0.00015060139831026662,
        "epoch": 0.44489157165213505,
        "step": 5970
    },
    {
        "loss": 2.6606,
        "grad_norm": 2.0476953983306885,
        "learning_rate": 0.0001505406936184694,
        "epoch": 0.4449660928534168,
        "step": 5971
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.4081733226776123,
        "learning_rate": 0.00015047996390231985,
        "epoch": 0.4450406140546986,
        "step": 5972
    },
    {
        "loss": 2.4568,
        "grad_norm": 3.572791337966919,
        "learning_rate": 0.00015041920919188728,
        "epoch": 0.44511513525598034,
        "step": 5973
    },
    {
        "loss": 2.3876,
        "grad_norm": 3.299712896347046,
        "learning_rate": 0.00015035842951725332,
        "epoch": 0.4451896564572621,
        "step": 5974
    },
    {
        "loss": 2.5885,
        "grad_norm": 3.1339080333709717,
        "learning_rate": 0.0001502976249085121,
        "epoch": 0.44526417765854387,
        "step": 5975
    },
    {
        "loss": 1.6973,
        "grad_norm": 3.289085865020752,
        "learning_rate": 0.00015023679539576977,
        "epoch": 0.44533869885982563,
        "step": 5976
    },
    {
        "loss": 2.9086,
        "grad_norm": 3.0404417514801025,
        "learning_rate": 0.0001501759410091451,
        "epoch": 0.4454132200611074,
        "step": 5977
    },
    {
        "loss": 2.065,
        "grad_norm": 3.959791660308838,
        "learning_rate": 0.0001501150617787691,
        "epoch": 0.44548774126238916,
        "step": 5978
    },
    {
        "loss": 2.4317,
        "grad_norm": 2.785740613937378,
        "learning_rate": 0.00015005415773478506,
        "epoch": 0.4455622624636709,
        "step": 5979
    },
    {
        "loss": 1.8396,
        "grad_norm": 3.561627149581909,
        "learning_rate": 0.0001499932289073486,
        "epoch": 0.4456367836649527,
        "step": 5980
    },
    {
        "loss": 2.7556,
        "grad_norm": 3.0582687854766846,
        "learning_rate": 0.00014993227532662746,
        "epoch": 0.44571130486623445,
        "step": 5981
    },
    {
        "loss": 2.6409,
        "grad_norm": 3.123356580734253,
        "learning_rate": 0.00014987129702280188,
        "epoch": 0.4457858260675162,
        "step": 5982
    },
    {
        "loss": 2.6626,
        "grad_norm": 4.043176174163818,
        "learning_rate": 0.00014981029402606413,
        "epoch": 0.445860347268798,
        "step": 5983
    },
    {
        "loss": 2.026,
        "grad_norm": 2.283430814743042,
        "learning_rate": 0.0001497492663666189,
        "epoch": 0.44593486847007974,
        "step": 5984
    },
    {
        "loss": 2.8698,
        "grad_norm": 2.0808427333831787,
        "learning_rate": 0.00014968821407468285,
        "epoch": 0.4460093896713615,
        "step": 5985
    },
    {
        "loss": 1.7838,
        "grad_norm": 5.138067722320557,
        "learning_rate": 0.00014962713718048512,
        "epoch": 0.44608391087264326,
        "step": 5986
    },
    {
        "loss": 1.4662,
        "grad_norm": 1.7980048656463623,
        "learning_rate": 0.00014956603571426673,
        "epoch": 0.446158432073925,
        "step": 5987
    },
    {
        "loss": 2.2426,
        "grad_norm": 6.07496976852417,
        "learning_rate": 0.00014950490970628107,
        "epoch": 0.4462329532752068,
        "step": 5988
    },
    {
        "loss": 3.0039,
        "grad_norm": 2.657149314880371,
        "learning_rate": 0.0001494437591867937,
        "epoch": 0.44630747447648855,
        "step": 5989
    },
    {
        "loss": 2.4952,
        "grad_norm": 3.0270440578460693,
        "learning_rate": 0.00014938258418608217,
        "epoch": 0.4463819956777703,
        "step": 5990
    },
    {
        "loss": 2.4578,
        "grad_norm": 2.037400960922241,
        "learning_rate": 0.0001493213847344362,
        "epoch": 0.4464565168790521,
        "step": 5991
    },
    {
        "loss": 2.7323,
        "grad_norm": 2.332604169845581,
        "learning_rate": 0.00014926016086215765,
        "epoch": 0.44653103808033384,
        "step": 5992
    },
    {
        "loss": 2.2413,
        "grad_norm": 3.2596096992492676,
        "learning_rate": 0.00014919891259956053,
        "epoch": 0.4466055592816156,
        "step": 5993
    },
    {
        "loss": 1.8153,
        "grad_norm": 3.789608955383301,
        "learning_rate": 0.0001491376399769709,
        "epoch": 0.44668008048289737,
        "step": 5994
    },
    {
        "loss": 2.6445,
        "grad_norm": 1.6208922863006592,
        "learning_rate": 0.00014907634302472667,
        "epoch": 0.44675460168417913,
        "step": 5995
    },
    {
        "loss": 2.4732,
        "grad_norm": 2.511453151702881,
        "learning_rate": 0.00014901502177317813,
        "epoch": 0.4468291228854609,
        "step": 5996
    },
    {
        "loss": 3.0167,
        "grad_norm": 2.3935458660125732,
        "learning_rate": 0.00014895367625268735,
        "epoch": 0.44690364408674266,
        "step": 5997
    },
    {
        "loss": 2.7941,
        "grad_norm": 2.8000118732452393,
        "learning_rate": 0.00014889230649362854,
        "epoch": 0.4469781652880244,
        "step": 5998
    },
    {
        "loss": 2.2522,
        "grad_norm": 3.3792829513549805,
        "learning_rate": 0.00014883091252638784,
        "epoch": 0.4470526864893062,
        "step": 5999
    },
    {
        "loss": 1.1301,
        "grad_norm": 2.3655953407287598,
        "learning_rate": 0.00014876949438136352,
        "epoch": 0.44712720769058795,
        "step": 6000
    },
    {
        "loss": 1.5151,
        "grad_norm": 4.165312767028809,
        "learning_rate": 0.00014870805208896552,
        "epoch": 0.4472017288918697,
        "step": 6001
    },
    {
        "loss": 2.4068,
        "grad_norm": 2.847952127456665,
        "learning_rate": 0.00014864658567961608,
        "epoch": 0.44727625009315153,
        "step": 6002
    },
    {
        "loss": 1.9607,
        "grad_norm": 3.2251198291778564,
        "learning_rate": 0.00014858509518374924,
        "epoch": 0.4473507712944333,
        "step": 6003
    },
    {
        "loss": 2.6448,
        "grad_norm": 2.7050843238830566,
        "learning_rate": 0.00014852358063181087,
        "epoch": 0.44742529249571505,
        "step": 6004
    },
    {
        "loss": 2.4279,
        "grad_norm": 4.695058345794678,
        "learning_rate": 0.00014846204205425892,
        "epoch": 0.4474998136969968,
        "step": 6005
    },
    {
        "loss": 2.3162,
        "grad_norm": 2.302983045578003,
        "learning_rate": 0.00014840047948156315,
        "epoch": 0.4475743348982786,
        "step": 6006
    },
    {
        "loss": 2.635,
        "grad_norm": 2.6969821453094482,
        "learning_rate": 0.0001483388929442051,
        "epoch": 0.44764885609956034,
        "step": 6007
    },
    {
        "loss": 1.8685,
        "grad_norm": 3.1533350944519043,
        "learning_rate": 0.00014827728247267846,
        "epoch": 0.4477233773008421,
        "step": 6008
    },
    {
        "loss": 2.54,
        "grad_norm": 2.7312235832214355,
        "learning_rate": 0.0001482156480974884,
        "epoch": 0.44779789850212387,
        "step": 6009
    },
    {
        "loss": 2.725,
        "grad_norm": 4.306424140930176,
        "learning_rate": 0.00014815398984915229,
        "epoch": 0.44787241970340563,
        "step": 6010
    },
    {
        "loss": 1.9605,
        "grad_norm": 5.566289901733398,
        "learning_rate": 0.00014809230775819895,
        "epoch": 0.4479469409046874,
        "step": 6011
    },
    {
        "loss": 3.3068,
        "grad_norm": 3.0094833374023438,
        "learning_rate": 0.00014803060185516936,
        "epoch": 0.44802146210596916,
        "step": 6012
    },
    {
        "loss": 2.2085,
        "grad_norm": 2.6412765979766846,
        "learning_rate": 0.00014796887217061614,
        "epoch": 0.4480959833072509,
        "step": 6013
    },
    {
        "loss": 2.7719,
        "grad_norm": 2.716493844985962,
        "learning_rate": 0.00014790711873510356,
        "epoch": 0.4481705045085327,
        "step": 6014
    },
    {
        "loss": 1.7797,
        "grad_norm": 3.6865925788879395,
        "learning_rate": 0.00014784534157920786,
        "epoch": 0.44824502570981445,
        "step": 6015
    },
    {
        "loss": 2.9063,
        "grad_norm": 2.4715688228607178,
        "learning_rate": 0.00014778354073351695,
        "epoch": 0.4483195469110962,
        "step": 6016
    },
    {
        "loss": 1.9121,
        "grad_norm": 3.3480162620544434,
        "learning_rate": 0.00014772171622863043,
        "epoch": 0.448394068112378,
        "step": 6017
    },
    {
        "loss": 1.8905,
        "grad_norm": 3.6968963146209717,
        "learning_rate": 0.00014765986809515952,
        "epoch": 0.44846858931365974,
        "step": 6018
    },
    {
        "loss": 1.9044,
        "grad_norm": 2.1068735122680664,
        "learning_rate": 0.00014759799636372738,
        "epoch": 0.4485431105149415,
        "step": 6019
    },
    {
        "loss": 1.8498,
        "grad_norm": 2.627307891845703,
        "learning_rate": 0.0001475361010649688,
        "epoch": 0.44861763171622326,
        "step": 6020
    },
    {
        "loss": 1.6995,
        "grad_norm": 3.3467371463775635,
        "learning_rate": 0.00014747418222952995,
        "epoch": 0.448692152917505,
        "step": 6021
    },
    {
        "loss": 2.2967,
        "grad_norm": 2.7970619201660156,
        "learning_rate": 0.00014741223988806908,
        "epoch": 0.4487666741187868,
        "step": 6022
    },
    {
        "loss": 2.2968,
        "grad_norm": 2.920720100402832,
        "learning_rate": 0.00014735027407125574,
        "epoch": 0.44884119532006855,
        "step": 6023
    },
    {
        "loss": 2.1361,
        "grad_norm": 4.66244649887085,
        "learning_rate": 0.00014728828480977128,
        "epoch": 0.4489157165213503,
        "step": 6024
    },
    {
        "loss": 2.4142,
        "grad_norm": 2.643742799758911,
        "learning_rate": 0.0001472262721343086,
        "epoch": 0.4489902377226321,
        "step": 6025
    },
    {
        "loss": 2.6957,
        "grad_norm": 2.871183395385742,
        "learning_rate": 0.0001471642360755723,
        "epoch": 0.44906475892391384,
        "step": 6026
    },
    {
        "loss": 2.3023,
        "grad_norm": 2.1048524379730225,
        "learning_rate": 0.00014710217666427837,
        "epoch": 0.4491392801251956,
        "step": 6027
    },
    {
        "loss": 2.5636,
        "grad_norm": 2.8862416744232178,
        "learning_rate": 0.00014704009393115441,
        "epoch": 0.44921380132647737,
        "step": 6028
    },
    {
        "loss": 2.2439,
        "grad_norm": 4.275190830230713,
        "learning_rate": 0.0001469779879069397,
        "epoch": 0.44928832252775913,
        "step": 6029
    },
    {
        "loss": 2.8365,
        "grad_norm": 2.7265751361846924,
        "learning_rate": 0.00014691585862238496,
        "epoch": 0.4493628437290409,
        "step": 6030
    },
    {
        "loss": 1.5516,
        "grad_norm": 3.325591564178467,
        "learning_rate": 0.00014685370610825245,
        "epoch": 0.44943736493032266,
        "step": 6031
    },
    {
        "loss": 2.2416,
        "grad_norm": 4.976834774017334,
        "learning_rate": 0.00014679153039531581,
        "epoch": 0.4495118861316044,
        "step": 6032
    },
    {
        "loss": 1.8419,
        "grad_norm": 3.489462375640869,
        "learning_rate": 0.00014672933151436035,
        "epoch": 0.4495864073328862,
        "step": 6033
    },
    {
        "loss": 2.1412,
        "grad_norm": 2.84775447845459,
        "learning_rate": 0.00014666710949618275,
        "epoch": 0.44966092853416795,
        "step": 6034
    },
    {
        "loss": 2.1105,
        "grad_norm": 2.637192964553833,
        "learning_rate": 0.0001466048643715912,
        "epoch": 0.4497354497354497,
        "step": 6035
    },
    {
        "loss": 2.6165,
        "grad_norm": 2.9389243125915527,
        "learning_rate": 0.00014654259617140531,
        "epoch": 0.4498099709367315,
        "step": 6036
    },
    {
        "loss": 2.4385,
        "grad_norm": 3.0534844398498535,
        "learning_rate": 0.0001464803049264561,
        "epoch": 0.4498844921380133,
        "step": 6037
    },
    {
        "loss": 2.0887,
        "grad_norm": 3.237102746963501,
        "learning_rate": 0.00014641799066758592,
        "epoch": 0.44995901333929506,
        "step": 6038
    },
    {
        "loss": 2.6684,
        "grad_norm": 2.7180724143981934,
        "learning_rate": 0.00014635565342564866,
        "epoch": 0.4500335345405768,
        "step": 6039
    },
    {
        "loss": 2.7418,
        "grad_norm": 3.9288947582244873,
        "learning_rate": 0.00014629329323150963,
        "epoch": 0.4501080557418586,
        "step": 6040
    },
    {
        "loss": 2.4797,
        "grad_norm": 2.8897366523742676,
        "learning_rate": 0.00014623091011604523,
        "epoch": 0.45018257694314034,
        "step": 6041
    },
    {
        "loss": 1.8635,
        "grad_norm": 3.4169795513153076,
        "learning_rate": 0.00014616850411014348,
        "epoch": 0.4502570981444221,
        "step": 6042
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.2217531204223633,
        "learning_rate": 0.00014610607524470358,
        "epoch": 0.45033161934570387,
        "step": 6043
    },
    {
        "loss": 2.6867,
        "grad_norm": 3.736299991607666,
        "learning_rate": 0.00014604362355063612,
        "epoch": 0.45040614054698563,
        "step": 6044
    },
    {
        "loss": 2.4216,
        "grad_norm": 2.849761724472046,
        "learning_rate": 0.0001459811490588631,
        "epoch": 0.4504806617482674,
        "step": 6045
    },
    {
        "loss": 1.7669,
        "grad_norm": 2.929448127746582,
        "learning_rate": 0.0001459186518003175,
        "epoch": 0.45055518294954916,
        "step": 6046
    },
    {
        "loss": 2.9227,
        "grad_norm": 2.9072325229644775,
        "learning_rate": 0.00014585613180594388,
        "epoch": 0.4506297041508309,
        "step": 6047
    },
    {
        "loss": 2.599,
        "grad_norm": 3.0650689601898193,
        "learning_rate": 0.0001457935891066979,
        "epoch": 0.4507042253521127,
        "step": 6048
    },
    {
        "loss": 3.2043,
        "grad_norm": 4.64199161529541,
        "learning_rate": 0.00014573102373354645,
        "epoch": 0.45077874655339445,
        "step": 6049
    },
    {
        "loss": 2.7448,
        "grad_norm": 2.725619077682495,
        "learning_rate": 0.00014566843571746783,
        "epoch": 0.4508532677546762,
        "step": 6050
    },
    {
        "loss": 2.8654,
        "grad_norm": 3.410815954208374,
        "learning_rate": 0.00014560582508945134,
        "epoch": 0.450927788955958,
        "step": 6051
    },
    {
        "loss": 2.3452,
        "grad_norm": 3.7109739780426025,
        "learning_rate": 0.00014554319188049745,
        "epoch": 0.45100231015723974,
        "step": 6052
    },
    {
        "loss": 2.0085,
        "grad_norm": 3.018958806991577,
        "learning_rate": 0.00014548053612161802,
        "epoch": 0.4510768313585215,
        "step": 6053
    },
    {
        "loss": 1.8873,
        "grad_norm": 3.371995210647583,
        "learning_rate": 0.00014541785784383599,
        "epoch": 0.45115135255980326,
        "step": 6054
    },
    {
        "loss": 2.8874,
        "grad_norm": 2.6765987873077393,
        "learning_rate": 0.0001453551570781854,
        "epoch": 0.45122587376108503,
        "step": 6055
    },
    {
        "loss": 2.4688,
        "grad_norm": 3.451160192489624,
        "learning_rate": 0.0001452924338557114,
        "epoch": 0.4513003949623668,
        "step": 6056
    },
    {
        "loss": 2.3148,
        "grad_norm": 3.8645358085632324,
        "learning_rate": 0.00014522968820747045,
        "epoch": 0.45137491616364855,
        "step": 6057
    },
    {
        "loss": 2.917,
        "grad_norm": 3.7911970615386963,
        "learning_rate": 0.00014516692016452976,
        "epoch": 0.4514494373649303,
        "step": 6058
    },
    {
        "loss": 2.2889,
        "grad_norm": 2.8619296550750732,
        "learning_rate": 0.00014510412975796805,
        "epoch": 0.4515239585662121,
        "step": 6059
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.667431354522705,
        "learning_rate": 0.00014504131701887477,
        "epoch": 0.45159847976749384,
        "step": 6060
    },
    {
        "loss": 1.9081,
        "grad_norm": 3.62260365486145,
        "learning_rate": 0.00014497848197835066,
        "epoch": 0.4516730009687756,
        "step": 6061
    },
    {
        "loss": 2.2211,
        "grad_norm": 4.089382648468018,
        "learning_rate": 0.00014491562466750733,
        "epoch": 0.45174752217005737,
        "step": 6062
    },
    {
        "loss": 2.2585,
        "grad_norm": 3.5555694103240967,
        "learning_rate": 0.00014485274511746746,
        "epoch": 0.45182204337133913,
        "step": 6063
    },
    {
        "loss": 2.4983,
        "grad_norm": 2.558042526245117,
        "learning_rate": 0.00014478984335936494,
        "epoch": 0.4518965645726209,
        "step": 6064
    },
    {
        "loss": 1.4105,
        "grad_norm": 2.089376926422119,
        "learning_rate": 0.00014472691942434433,
        "epoch": 0.45197108577390266,
        "step": 6065
    },
    {
        "loss": 2.1957,
        "grad_norm": 2.3966846466064453,
        "learning_rate": 0.00014466397334356136,
        "epoch": 0.4520456069751844,
        "step": 6066
    },
    {
        "loss": 2.9767,
        "grad_norm": 3.4278604984283447,
        "learning_rate": 0.0001446010051481828,
        "epoch": 0.4521201281764662,
        "step": 6067
    },
    {
        "loss": 2.9341,
        "grad_norm": 2.368957757949829,
        "learning_rate": 0.00014453801486938612,
        "epoch": 0.45219464937774795,
        "step": 6068
    },
    {
        "loss": 1.8352,
        "grad_norm": 4.32163143157959,
        "learning_rate": 0.00014447500253836,
        "epoch": 0.4522691705790297,
        "step": 6069
    },
    {
        "loss": 2.9114,
        "grad_norm": 3.3346259593963623,
        "learning_rate": 0.00014441196818630378,
        "epoch": 0.4523436917803115,
        "step": 6070
    },
    {
        "loss": 2.5802,
        "grad_norm": 3.125264883041382,
        "learning_rate": 0.00014434891184442796,
        "epoch": 0.45241821298159324,
        "step": 6071
    },
    {
        "loss": 2.5604,
        "grad_norm": 2.741983413696289,
        "learning_rate": 0.0001442858335439537,
        "epoch": 0.452492734182875,
        "step": 6072
    },
    {
        "loss": 2.3011,
        "grad_norm": 3.1015098094940186,
        "learning_rate": 0.00014422273331611313,
        "epoch": 0.4525672553841568,
        "step": 6073
    },
    {
        "loss": 2.4232,
        "grad_norm": 1.821874737739563,
        "learning_rate": 0.00014415961119214937,
        "epoch": 0.4526417765854386,
        "step": 6074
    },
    {
        "loss": 2.5094,
        "grad_norm": 3.0468597412109375,
        "learning_rate": 0.00014409646720331607,
        "epoch": 0.45271629778672035,
        "step": 6075
    },
    {
        "loss": 2.1968,
        "grad_norm": 3.8440864086151123,
        "learning_rate": 0.00014403330138087797,
        "epoch": 0.4527908189880021,
        "step": 6076
    },
    {
        "loss": 2.8658,
        "grad_norm": 2.0480244159698486,
        "learning_rate": 0.00014397011375611064,
        "epoch": 0.45286534018928387,
        "step": 6077
    },
    {
        "loss": 2.5882,
        "grad_norm": 2.4834048748016357,
        "learning_rate": 0.00014390690436030023,
        "epoch": 0.45293986139056563,
        "step": 6078
    },
    {
        "loss": 2.3171,
        "grad_norm": 2.9261465072631836,
        "learning_rate": 0.00014384367322474372,
        "epoch": 0.4530143825918474,
        "step": 6079
    },
    {
        "loss": 2.3806,
        "grad_norm": 3.9822700023651123,
        "learning_rate": 0.00014378042038074902,
        "epoch": 0.45308890379312916,
        "step": 6080
    },
    {
        "loss": 2.0683,
        "grad_norm": 3.053785800933838,
        "learning_rate": 0.0001437171458596347,
        "epoch": 0.4531634249944109,
        "step": 6081
    },
    {
        "loss": 2.2796,
        "grad_norm": 4.936056613922119,
        "learning_rate": 0.00014365384969272993,
        "epoch": 0.4532379461956927,
        "step": 6082
    },
    {
        "loss": 1.6975,
        "grad_norm": 2.7185821533203125,
        "learning_rate": 0.00014359053191137484,
        "epoch": 0.45331246739697445,
        "step": 6083
    },
    {
        "loss": 2.1114,
        "grad_norm": 3.505545139312744,
        "learning_rate": 0.00014352719254692004,
        "epoch": 0.4533869885982562,
        "step": 6084
    },
    {
        "loss": 2.693,
        "grad_norm": 3.736220359802246,
        "learning_rate": 0.00014346383163072696,
        "epoch": 0.453461509799538,
        "step": 6085
    },
    {
        "loss": 1.5903,
        "grad_norm": 2.85717511177063,
        "learning_rate": 0.0001434004491941677,
        "epoch": 0.45353603100081974,
        "step": 6086
    },
    {
        "loss": 2.4875,
        "grad_norm": 2.1213126182556152,
        "learning_rate": 0.00014333704526862498,
        "epoch": 0.4536105522021015,
        "step": 6087
    },
    {
        "loss": 2.77,
        "grad_norm": 2.216984272003174,
        "learning_rate": 0.00014327361988549213,
        "epoch": 0.45368507340338327,
        "step": 6088
    },
    {
        "loss": 2.4698,
        "grad_norm": 3.3148574829101562,
        "learning_rate": 0.00014321017307617308,
        "epoch": 0.45375959460466503,
        "step": 6089
    },
    {
        "loss": 1.9432,
        "grad_norm": 2.536200761795044,
        "learning_rate": 0.00014314670487208247,
        "epoch": 0.4538341158059468,
        "step": 6090
    },
    {
        "loss": 2.4617,
        "grad_norm": 3.875875949859619,
        "learning_rate": 0.0001430832153046456,
        "epoch": 0.45390863700722855,
        "step": 6091
    },
    {
        "loss": 2.4772,
        "grad_norm": 2.938410758972168,
        "learning_rate": 0.00014301970440529807,
        "epoch": 0.4539831582085103,
        "step": 6092
    },
    {
        "loss": 2.287,
        "grad_norm": 4.945489883422852,
        "learning_rate": 0.00014295617220548622,
        "epoch": 0.4540576794097921,
        "step": 6093
    },
    {
        "loss": 3.6746,
        "grad_norm": 5.650345325469971,
        "learning_rate": 0.00014289261873666698,
        "epoch": 0.45413220061107384,
        "step": 6094
    },
    {
        "loss": 2.1952,
        "grad_norm": 2.1657228469848633,
        "learning_rate": 0.00014282904403030772,
        "epoch": 0.4542067218123556,
        "step": 6095
    },
    {
        "loss": 1.8702,
        "grad_norm": 3.0441694259643555,
        "learning_rate": 0.00014276544811788638,
        "epoch": 0.45428124301363737,
        "step": 6096
    },
    {
        "loss": 2.0341,
        "grad_norm": 3.7885799407958984,
        "learning_rate": 0.00014270183103089144,
        "epoch": 0.45435576421491913,
        "step": 6097
    },
    {
        "loss": 2.1092,
        "grad_norm": 2.7927558422088623,
        "learning_rate": 0.00014263819280082175,
        "epoch": 0.4544302854162009,
        "step": 6098
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.1283392906188965,
        "learning_rate": 0.00014257453345918662,
        "epoch": 0.45450480661748266,
        "step": 6099
    },
    {
        "loss": 2.1338,
        "grad_norm": 3.086998224258423,
        "learning_rate": 0.00014251085303750588,
        "epoch": 0.4545793278187644,
        "step": 6100
    },
    {
        "loss": 1.273,
        "grad_norm": 2.0003416538238525,
        "learning_rate": 0.00014244715156730995,
        "epoch": 0.4546538490200462,
        "step": 6101
    },
    {
        "loss": 1.893,
        "grad_norm": 2.8862833976745605,
        "learning_rate": 0.00014238342908013936,
        "epoch": 0.45472837022132795,
        "step": 6102
    },
    {
        "loss": 2.0733,
        "grad_norm": 5.417831897735596,
        "learning_rate": 0.0001423196856075452,
        "epoch": 0.4548028914226097,
        "step": 6103
    },
    {
        "loss": 2.8809,
        "grad_norm": 3.162567377090454,
        "learning_rate": 0.000142255921181089,
        "epoch": 0.4548774126238915,
        "step": 6104
    },
    {
        "loss": 2.2005,
        "grad_norm": 2.927295207977295,
        "learning_rate": 0.00014219213583234258,
        "epoch": 0.45495193382517324,
        "step": 6105
    },
    {
        "loss": 2.5695,
        "grad_norm": 3.6675562858581543,
        "learning_rate": 0.00014212832959288824,
        "epoch": 0.455026455026455,
        "step": 6106
    },
    {
        "loss": 2.3457,
        "grad_norm": 2.1347947120666504,
        "learning_rate": 0.00014206450249431844,
        "epoch": 0.45510097622773676,
        "step": 6107
    },
    {
        "loss": 2.5745,
        "grad_norm": 3.194918394088745,
        "learning_rate": 0.00014200065456823616,
        "epoch": 0.4551754974290186,
        "step": 6108
    },
    {
        "loss": 2.076,
        "grad_norm": 2.8936002254486084,
        "learning_rate": 0.0001419367858462545,
        "epoch": 0.45525001863030035,
        "step": 6109
    },
    {
        "loss": 2.3861,
        "grad_norm": 2.249969720840454,
        "learning_rate": 0.00014187289635999697,
        "epoch": 0.4553245398315821,
        "step": 6110
    },
    {
        "loss": 2.728,
        "grad_norm": 2.866086721420288,
        "learning_rate": 0.0001418089861410975,
        "epoch": 0.45539906103286387,
        "step": 6111
    },
    {
        "loss": 2.0354,
        "grad_norm": 2.703382730484009,
        "learning_rate": 0.00014174505522119993,
        "epoch": 0.45547358223414564,
        "step": 6112
    },
    {
        "loss": 2.4066,
        "grad_norm": 3.3745322227478027,
        "learning_rate": 0.0001416811036319586,
        "epoch": 0.4555481034354274,
        "step": 6113
    },
    {
        "loss": 2.1027,
        "grad_norm": 4.672111511230469,
        "learning_rate": 0.00014161713140503808,
        "epoch": 0.45562262463670916,
        "step": 6114
    },
    {
        "loss": 2.6985,
        "grad_norm": 3.217416524887085,
        "learning_rate": 0.00014155313857211309,
        "epoch": 0.4556971458379909,
        "step": 6115
    },
    {
        "loss": 2.2184,
        "grad_norm": 3.4303858280181885,
        "learning_rate": 0.00014148912516486864,
        "epoch": 0.4557716670392727,
        "step": 6116
    },
    {
        "loss": 2.7872,
        "grad_norm": 2.749263286590576,
        "learning_rate": 0.00014142509121499975,
        "epoch": 0.45584618824055445,
        "step": 6117
    },
    {
        "loss": 1.9334,
        "grad_norm": 2.2523322105407715,
        "learning_rate": 0.00014136103675421187,
        "epoch": 0.4559207094418362,
        "step": 6118
    },
    {
        "loss": 2.857,
        "grad_norm": 2.304825782775879,
        "learning_rate": 0.00014129696181422032,
        "epoch": 0.455995230643118,
        "step": 6119
    },
    {
        "loss": 1.9643,
        "grad_norm": 3.0633418560028076,
        "learning_rate": 0.00014123286642675074,
        "epoch": 0.45606975184439974,
        "step": 6120
    },
    {
        "loss": 2.5465,
        "grad_norm": 2.6761109828948975,
        "learning_rate": 0.00014116875062353894,
        "epoch": 0.4561442730456815,
        "step": 6121
    },
    {
        "loss": 2.4619,
        "grad_norm": 2.0306589603424072,
        "learning_rate": 0.0001411046144363307,
        "epoch": 0.45621879424696327,
        "step": 6122
    },
    {
        "loss": 2.6063,
        "grad_norm": 1.954904317855835,
        "learning_rate": 0.00014104045789688186,
        "epoch": 0.45629331544824503,
        "step": 6123
    },
    {
        "loss": 2.3729,
        "grad_norm": 2.5400478839874268,
        "learning_rate": 0.00014097628103695848,
        "epoch": 0.4563678366495268,
        "step": 6124
    },
    {
        "loss": 2.3927,
        "grad_norm": 3.5668394565582275,
        "learning_rate": 0.00014091208388833674,
        "epoch": 0.45644235785080856,
        "step": 6125
    },
    {
        "loss": 2.22,
        "grad_norm": 2.466622829437256,
        "learning_rate": 0.00014084786648280257,
        "epoch": 0.4565168790520903,
        "step": 6126
    },
    {
        "loss": 2.8711,
        "grad_norm": 2.3405137062072754,
        "learning_rate": 0.00014078362885215217,
        "epoch": 0.4565914002533721,
        "step": 6127
    },
    {
        "loss": 2.013,
        "grad_norm": 3.652667760848999,
        "learning_rate": 0.00014071937102819182,
        "epoch": 0.45666592145465384,
        "step": 6128
    },
    {
        "loss": 2.0622,
        "grad_norm": 4.148393630981445,
        "learning_rate": 0.0001406550930427375,
        "epoch": 0.4567404426559356,
        "step": 6129
    },
    {
        "loss": 2.3035,
        "grad_norm": 2.4341979026794434,
        "learning_rate": 0.00014059079492761547,
        "epoch": 0.45681496385721737,
        "step": 6130
    },
    {
        "loss": 2.955,
        "grad_norm": 3.247239828109741,
        "learning_rate": 0.00014052647671466167,
        "epoch": 0.45688948505849913,
        "step": 6131
    },
    {
        "loss": 2.6782,
        "grad_norm": 2.526862621307373,
        "learning_rate": 0.00014046213843572238,
        "epoch": 0.4569640062597809,
        "step": 6132
    },
    {
        "loss": 2.6931,
        "grad_norm": 2.830716371536255,
        "learning_rate": 0.0001403977801226534,
        "epoch": 0.45703852746106266,
        "step": 6133
    },
    {
        "loss": 3.0025,
        "grad_norm": 2.086527109146118,
        "learning_rate": 0.0001403334018073207,
        "epoch": 0.4571130486623444,
        "step": 6134
    },
    {
        "loss": 2.6392,
        "grad_norm": 2.312451124191284,
        "learning_rate": 0.00014026900352160016,
        "epoch": 0.4571875698636262,
        "step": 6135
    },
    {
        "loss": 2.4591,
        "grad_norm": 1.8681797981262207,
        "learning_rate": 0.00014020458529737738,
        "epoch": 0.45726209106490795,
        "step": 6136
    },
    {
        "loss": 2.0352,
        "grad_norm": 1.8939907550811768,
        "learning_rate": 0.00014014014716654793,
        "epoch": 0.4573366122661897,
        "step": 6137
    },
    {
        "loss": 1.6336,
        "grad_norm": 2.9260497093200684,
        "learning_rate": 0.00014007568916101737,
        "epoch": 0.4574111334674715,
        "step": 6138
    },
    {
        "loss": 2.0449,
        "grad_norm": 3.1938657760620117,
        "learning_rate": 0.00014001121131270084,
        "epoch": 0.45748565466875324,
        "step": 6139
    },
    {
        "loss": 2.7263,
        "grad_norm": 3.3498318195343018,
        "learning_rate": 0.0001399467136535235,
        "epoch": 0.457560175870035,
        "step": 6140
    },
    {
        "loss": 2.2782,
        "grad_norm": 2.3056602478027344,
        "learning_rate": 0.00013988219621542013,
        "epoch": 0.45763469707131677,
        "step": 6141
    },
    {
        "loss": 2.5401,
        "grad_norm": 2.677577018737793,
        "learning_rate": 0.0001398176590303356,
        "epoch": 0.45770921827259853,
        "step": 6142
    },
    {
        "loss": 2.535,
        "grad_norm": 3.278634548187256,
        "learning_rate": 0.00013975310213022426,
        "epoch": 0.45778373947388035,
        "step": 6143
    },
    {
        "loss": 2.8969,
        "grad_norm": 1.9041168689727783,
        "learning_rate": 0.00013968852554705047,
        "epoch": 0.4578582606751621,
        "step": 6144
    },
    {
        "loss": 2.5718,
        "grad_norm": 2.445760726928711,
        "learning_rate": 0.00013962392931278806,
        "epoch": 0.4579327818764439,
        "step": 6145
    },
    {
        "loss": 2.3169,
        "grad_norm": 2.891087532043457,
        "learning_rate": 0.00013955931345942078,
        "epoch": 0.45800730307772564,
        "step": 6146
    },
    {
        "loss": 2.6422,
        "grad_norm": 2.4634745121002197,
        "learning_rate": 0.00013949467801894211,
        "epoch": 0.4580818242790074,
        "step": 6147
    },
    {
        "loss": 2.4118,
        "grad_norm": 3.3790202140808105,
        "learning_rate": 0.00013943002302335523,
        "epoch": 0.45815634548028916,
        "step": 6148
    },
    {
        "loss": 2.3169,
        "grad_norm": 1.664563775062561,
        "learning_rate": 0.00013936534850467284,
        "epoch": 0.4582308666815709,
        "step": 6149
    },
    {
        "loss": 2.1794,
        "grad_norm": 2.93172025680542,
        "learning_rate": 0.00013930065449491738,
        "epoch": 0.4583053878828527,
        "step": 6150
    },
    {
        "loss": 2.5171,
        "grad_norm": 2.281618595123291,
        "learning_rate": 0.00013923594102612102,
        "epoch": 0.45837990908413445,
        "step": 6151
    },
    {
        "loss": 1.8156,
        "grad_norm": 2.749265670776367,
        "learning_rate": 0.00013917120813032564,
        "epoch": 0.4584544302854162,
        "step": 6152
    },
    {
        "loss": 2.6906,
        "grad_norm": 2.7805397510528564,
        "learning_rate": 0.00013910645583958245,
        "epoch": 0.458528951486698,
        "step": 6153
    },
    {
        "loss": 2.0437,
        "grad_norm": 2.7689993381500244,
        "learning_rate": 0.00013904168418595243,
        "epoch": 0.45860347268797974,
        "step": 6154
    },
    {
        "loss": 2.7753,
        "grad_norm": 2.1292827129364014,
        "learning_rate": 0.00013897689320150619,
        "epoch": 0.4586779938892615,
        "step": 6155
    },
    {
        "loss": 2.5542,
        "grad_norm": 2.4806082248687744,
        "learning_rate": 0.00013891208291832391,
        "epoch": 0.45875251509054327,
        "step": 6156
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.276811122894287,
        "learning_rate": 0.00013884725336849523,
        "epoch": 0.45882703629182503,
        "step": 6157
    },
    {
        "loss": 2.5664,
        "grad_norm": 2.460026264190674,
        "learning_rate": 0.00013878240458411944,
        "epoch": 0.4589015574931068,
        "step": 6158
    },
    {
        "loss": 2.2589,
        "grad_norm": 2.7417256832122803,
        "learning_rate": 0.0001387175365973053,
        "epoch": 0.45897607869438856,
        "step": 6159
    },
    {
        "loss": 2.8392,
        "grad_norm": 2.878674030303955,
        "learning_rate": 0.00013865264944017095,
        "epoch": 0.4590505998956703,
        "step": 6160
    },
    {
        "loss": 2.7306,
        "grad_norm": 2.5723955631256104,
        "learning_rate": 0.00013858774314484424,
        "epoch": 0.4591251210969521,
        "step": 6161
    },
    {
        "loss": 2.6377,
        "grad_norm": 1.51261568069458,
        "learning_rate": 0.00013852281774346246,
        "epoch": 0.45919964229823385,
        "step": 6162
    },
    {
        "loss": 2.0533,
        "grad_norm": 3.238323211669922,
        "learning_rate": 0.00013845787326817224,
        "epoch": 0.4592741634995156,
        "step": 6163
    },
    {
        "loss": 2.1888,
        "grad_norm": 3.3820090293884277,
        "learning_rate": 0.0001383929097511296,
        "epoch": 0.45934868470079737,
        "step": 6164
    },
    {
        "loss": 2.5368,
        "grad_norm": 3.5428824424743652,
        "learning_rate": 0.00013832792722450024,
        "epoch": 0.45942320590207913,
        "step": 6165
    },
    {
        "loss": 2.0074,
        "grad_norm": 3.077530860900879,
        "learning_rate": 0.00013826292572045914,
        "epoch": 0.4594977271033609,
        "step": 6166
    },
    {
        "loss": 2.646,
        "grad_norm": 2.874095916748047,
        "learning_rate": 0.00013819790527119062,
        "epoch": 0.45957224830464266,
        "step": 6167
    },
    {
        "loss": 2.4985,
        "grad_norm": 3.302861452102661,
        "learning_rate": 0.00013813286590888852,
        "epoch": 0.4596467695059244,
        "step": 6168
    },
    {
        "loss": 2.1621,
        "grad_norm": 2.8591198921203613,
        "learning_rate": 0.0001380678076657559,
        "epoch": 0.4597212907072062,
        "step": 6169
    },
    {
        "loss": 2.8492,
        "grad_norm": 4.187933921813965,
        "learning_rate": 0.00013800273057400518,
        "epoch": 0.45979581190848795,
        "step": 6170
    },
    {
        "loss": 2.35,
        "grad_norm": 3.7660820484161377,
        "learning_rate": 0.00013793763466585818,
        "epoch": 0.4598703331097697,
        "step": 6171
    },
    {
        "loss": 2.5701,
        "grad_norm": 2.487149477005005,
        "learning_rate": 0.00013787251997354608,
        "epoch": 0.4599448543110515,
        "step": 6172
    },
    {
        "loss": 1.8975,
        "grad_norm": 3.320119619369507,
        "learning_rate": 0.00013780738652930928,
        "epoch": 0.46001937551233324,
        "step": 6173
    },
    {
        "loss": 2.282,
        "grad_norm": 1.9001061916351318,
        "learning_rate": 0.0001377422343653974,
        "epoch": 0.460093896713615,
        "step": 6174
    },
    {
        "loss": 2.5616,
        "grad_norm": 2.242166757583618,
        "learning_rate": 0.00013767706351406944,
        "epoch": 0.46016841791489677,
        "step": 6175
    },
    {
        "loss": 2.9785,
        "grad_norm": 3.61247181892395,
        "learning_rate": 0.00013761187400759364,
        "epoch": 0.46024293911617853,
        "step": 6176
    },
    {
        "loss": 2.2277,
        "grad_norm": 2.6100244522094727,
        "learning_rate": 0.00013754666587824756,
        "epoch": 0.4603174603174603,
        "step": 6177
    },
    {
        "loss": 2.7773,
        "grad_norm": 3.1911725997924805,
        "learning_rate": 0.0001374814391583177,
        "epoch": 0.46039198151874206,
        "step": 6178
    },
    {
        "loss": 2.2796,
        "grad_norm": 3.644990921020508,
        "learning_rate": 0.0001374161938801001,
        "epoch": 0.4604665027200239,
        "step": 6179
    },
    {
        "loss": 2.3873,
        "grad_norm": 3.9169232845306396,
        "learning_rate": 0.0001373509300758997,
        "epoch": 0.46054102392130564,
        "step": 6180
    },
    {
        "loss": 1.6393,
        "grad_norm": 4.074645042419434,
        "learning_rate": 0.00013728564777803083,
        "epoch": 0.4606155451225874,
        "step": 6181
    },
    {
        "loss": 2.1938,
        "grad_norm": 3.6644325256347656,
        "learning_rate": 0.0001372203470188169,
        "epoch": 0.46069006632386916,
        "step": 6182
    },
    {
        "loss": 2.4769,
        "grad_norm": 2.573493003845215,
        "learning_rate": 0.00013715502783059048,
        "epoch": 0.4607645875251509,
        "step": 6183
    },
    {
        "loss": 2.5242,
        "grad_norm": 1.9464467763900757,
        "learning_rate": 0.00013708969024569308,
        "epoch": 0.4608391087264327,
        "step": 6184
    },
    {
        "loss": 2.1073,
        "grad_norm": 3.5727694034576416,
        "learning_rate": 0.0001370243342964756,
        "epoch": 0.46091362992771445,
        "step": 6185
    },
    {
        "loss": 2.2332,
        "grad_norm": 2.7584197521209717,
        "learning_rate": 0.00013695896001529796,
        "epoch": 0.4609881511289962,
        "step": 6186
    },
    {
        "loss": 2.4031,
        "grad_norm": 2.463047742843628,
        "learning_rate": 0.00013689356743452896,
        "epoch": 0.461062672330278,
        "step": 6187
    },
    {
        "loss": 2.5307,
        "grad_norm": 2.3486552238464355,
        "learning_rate": 0.0001368281565865467,
        "epoch": 0.46113719353155974,
        "step": 6188
    },
    {
        "loss": 1.6403,
        "grad_norm": 4.529880046844482,
        "learning_rate": 0.00013676272750373828,
        "epoch": 0.4612117147328415,
        "step": 6189
    },
    {
        "loss": 2.3815,
        "grad_norm": 3.1614110469818115,
        "learning_rate": 0.00013669728021849969,
        "epoch": 0.46128623593412327,
        "step": 6190
    },
    {
        "loss": 2.4864,
        "grad_norm": 4.027811527252197,
        "learning_rate": 0.00013663181476323609,
        "epoch": 0.46136075713540503,
        "step": 6191
    },
    {
        "loss": 2.3377,
        "grad_norm": 3.183131694793701,
        "learning_rate": 0.00013656633117036147,
        "epoch": 0.4614352783366868,
        "step": 6192
    },
    {
        "loss": 2.7387,
        "grad_norm": 3.2672512531280518,
        "learning_rate": 0.00013650082947229907,
        "epoch": 0.46150979953796856,
        "step": 6193
    },
    {
        "loss": 2.657,
        "grad_norm": 2.0144968032836914,
        "learning_rate": 0.00013643530970148073,
        "epoch": 0.4615843207392503,
        "step": 6194
    },
    {
        "loss": 2.6253,
        "grad_norm": 3.5188021659851074,
        "learning_rate": 0.00013636977189034755,
        "epoch": 0.4616588419405321,
        "step": 6195
    },
    {
        "loss": 2.2416,
        "grad_norm": 2.673578977584839,
        "learning_rate": 0.00013630421607134947,
        "epoch": 0.46173336314181385,
        "step": 6196
    },
    {
        "loss": 1.8413,
        "grad_norm": 6.060081481933594,
        "learning_rate": 0.00013623864227694523,
        "epoch": 0.4618078843430956,
        "step": 6197
    },
    {
        "loss": 2.3236,
        "grad_norm": 4.8574700355529785,
        "learning_rate": 0.0001361730505396026,
        "epoch": 0.4618824055443774,
        "step": 6198
    },
    {
        "loss": 2.792,
        "grad_norm": 3.2746341228485107,
        "learning_rate": 0.00013610744089179825,
        "epoch": 0.46195692674565914,
        "step": 6199
    },
    {
        "loss": 2.5473,
        "grad_norm": 2.85809063911438,
        "learning_rate": 0.00013604181336601765,
        "epoch": 0.4620314479469409,
        "step": 6200
    },
    {
        "loss": 0.7138,
        "grad_norm": 2.829298734664917,
        "learning_rate": 0.00013597616799475504,
        "epoch": 0.46210596914822266,
        "step": 6201
    },
    {
        "loss": 2.575,
        "grad_norm": 3.312177896499634,
        "learning_rate": 0.00013591050481051367,
        "epoch": 0.4621804903495044,
        "step": 6202
    },
    {
        "loss": 2.6285,
        "grad_norm": 1.8559750318527222,
        "learning_rate": 0.00013584482384580558,
        "epoch": 0.4622550115507862,
        "step": 6203
    },
    {
        "loss": 2.0557,
        "grad_norm": 2.5041675567626953,
        "learning_rate": 0.00013577912513315142,
        "epoch": 0.46232953275206795,
        "step": 6204
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.871146321296692,
        "learning_rate": 0.00013571340870508096,
        "epoch": 0.4624040539533497,
        "step": 6205
    },
    {
        "loss": 1.7638,
        "grad_norm": 3.14986252784729,
        "learning_rate": 0.0001356476745941324,
        "epoch": 0.4624785751546315,
        "step": 6206
    },
    {
        "loss": 2.5921,
        "grad_norm": 2.4450714588165283,
        "learning_rate": 0.0001355819228328529,
        "epoch": 0.46255309635591324,
        "step": 6207
    },
    {
        "loss": 2.5204,
        "grad_norm": 3.8849198818206787,
        "learning_rate": 0.0001355161534537983,
        "epoch": 0.462627617557195,
        "step": 6208
    },
    {
        "loss": 2.6303,
        "grad_norm": 2.212066411972046,
        "learning_rate": 0.0001354503664895333,
        "epoch": 0.46270213875847677,
        "step": 6209
    },
    {
        "loss": 2.7742,
        "grad_norm": 4.018128871917725,
        "learning_rate": 0.00013538456197263103,
        "epoch": 0.46277665995975853,
        "step": 6210
    },
    {
        "loss": 3.1214,
        "grad_norm": 2.995408058166504,
        "learning_rate": 0.00013531873993567346,
        "epoch": 0.4628511811610403,
        "step": 6211
    },
    {
        "loss": 2.3021,
        "grad_norm": 2.3203160762786865,
        "learning_rate": 0.00013525290041125127,
        "epoch": 0.46292570236232206,
        "step": 6212
    },
    {
        "loss": 1.861,
        "grad_norm": 4.39328670501709,
        "learning_rate": 0.00013518704343196385,
        "epoch": 0.4630002235636038,
        "step": 6213
    },
    {
        "loss": 2.4413,
        "grad_norm": 2.585191488265991,
        "learning_rate": 0.00013512116903041904,
        "epoch": 0.46307474476488564,
        "step": 6214
    },
    {
        "loss": 2.4962,
        "grad_norm": 3.060965061187744,
        "learning_rate": 0.00013505527723923347,
        "epoch": 0.4631492659661674,
        "step": 6215
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.6353771686553955,
        "learning_rate": 0.00013498936809103226,
        "epoch": 0.46322378716744916,
        "step": 6216
    },
    {
        "loss": 2.8795,
        "grad_norm": 2.7550368309020996,
        "learning_rate": 0.00013492344161844918,
        "epoch": 0.4632983083687309,
        "step": 6217
    },
    {
        "loss": 2.4467,
        "grad_norm": 3.1172351837158203,
        "learning_rate": 0.00013485749785412666,
        "epoch": 0.4633728295700127,
        "step": 6218
    },
    {
        "loss": 2.7579,
        "grad_norm": 2.169914960861206,
        "learning_rate": 0.00013479153683071564,
        "epoch": 0.46344735077129445,
        "step": 6219
    },
    {
        "loss": 2.0978,
        "grad_norm": 3.1617062091827393,
        "learning_rate": 0.00013472555858087555,
        "epoch": 0.4635218719725762,
        "step": 6220
    },
    {
        "loss": 2.2247,
        "grad_norm": 2.5522727966308594,
        "learning_rate": 0.00013465956313727428,
        "epoch": 0.463596393173858,
        "step": 6221
    },
    {
        "loss": 2.5785,
        "grad_norm": 2.1021249294281006,
        "learning_rate": 0.00013459355053258842,
        "epoch": 0.46367091437513974,
        "step": 6222
    },
    {
        "loss": 2.1351,
        "grad_norm": 4.3450846672058105,
        "learning_rate": 0.000134527520799503,
        "epoch": 0.4637454355764215,
        "step": 6223
    },
    {
        "loss": 2.0637,
        "grad_norm": 2.3299410343170166,
        "learning_rate": 0.00013446147397071155,
        "epoch": 0.46381995677770327,
        "step": 6224
    },
    {
        "loss": 2.2377,
        "grad_norm": 3.3463711738586426,
        "learning_rate": 0.00013439541007891583,
        "epoch": 0.46389447797898503,
        "step": 6225
    },
    {
        "loss": 2.7359,
        "grad_norm": 2.923008918762207,
        "learning_rate": 0.00013432932915682634,
        "epoch": 0.4639689991802668,
        "step": 6226
    },
    {
        "loss": 2.714,
        "grad_norm": 2.361163854598999,
        "learning_rate": 0.00013426323123716193,
        "epoch": 0.46404352038154856,
        "step": 6227
    },
    {
        "loss": 2.0009,
        "grad_norm": 3.8982412815093994,
        "learning_rate": 0.00013419711635264983,
        "epoch": 0.4641180415828303,
        "step": 6228
    },
    {
        "loss": 2.2951,
        "grad_norm": 2.3329083919525146,
        "learning_rate": 0.00013413098453602572,
        "epoch": 0.4641925627841121,
        "step": 6229
    },
    {
        "loss": 1.5357,
        "grad_norm": 4.140532970428467,
        "learning_rate": 0.0001340648358200336,
        "epoch": 0.46426708398539385,
        "step": 6230
    },
    {
        "loss": 2.4442,
        "grad_norm": 3.0745110511779785,
        "learning_rate": 0.00013399867023742586,
        "epoch": 0.4643416051866756,
        "step": 6231
    },
    {
        "loss": 2.0569,
        "grad_norm": 4.082712173461914,
        "learning_rate": 0.00013393248782096314,
        "epoch": 0.4644161263879574,
        "step": 6232
    },
    {
        "loss": 2.9255,
        "grad_norm": 2.635974884033203,
        "learning_rate": 0.00013386628860341475,
        "epoch": 0.46449064758923914,
        "step": 6233
    },
    {
        "loss": 1.9308,
        "grad_norm": 1.603266954421997,
        "learning_rate": 0.00013380007261755794,
        "epoch": 0.4645651687905209,
        "step": 6234
    },
    {
        "loss": 2.5142,
        "grad_norm": 2.7260122299194336,
        "learning_rate": 0.00013373383989617838,
        "epoch": 0.46463968999180266,
        "step": 6235
    },
    {
        "loss": 2.5757,
        "grad_norm": 2.364814281463623,
        "learning_rate": 0.0001336675904720701,
        "epoch": 0.4647142111930844,
        "step": 6236
    },
    {
        "loss": 2.2961,
        "grad_norm": 2.8668155670166016,
        "learning_rate": 0.00013360132437803537,
        "epoch": 0.4647887323943662,
        "step": 6237
    },
    {
        "loss": 2.8595,
        "grad_norm": 2.3447988033294678,
        "learning_rate": 0.0001335350416468848,
        "epoch": 0.46486325359564795,
        "step": 6238
    },
    {
        "loss": 2.2571,
        "grad_norm": 2.959521532058716,
        "learning_rate": 0.000133468742311437,
        "epoch": 0.4649377747969297,
        "step": 6239
    },
    {
        "loss": 2.7177,
        "grad_norm": 2.5649964809417725,
        "learning_rate": 0.000133402426404519,
        "epoch": 0.4650122959982115,
        "step": 6240
    },
    {
        "loss": 1.6992,
        "grad_norm": 3.2494993209838867,
        "learning_rate": 0.00013333609395896595,
        "epoch": 0.46508681719949324,
        "step": 6241
    },
    {
        "loss": 2.4791,
        "grad_norm": 2.3081860542297363,
        "learning_rate": 0.0001332697450076212,
        "epoch": 0.465161338400775,
        "step": 6242
    },
    {
        "loss": 2.3343,
        "grad_norm": 3.003291130065918,
        "learning_rate": 0.0001332033795833364,
        "epoch": 0.46523585960205677,
        "step": 6243
    },
    {
        "loss": 1.8196,
        "grad_norm": 2.408720016479492,
        "learning_rate": 0.00013313699771897114,
        "epoch": 0.46531038080333853,
        "step": 6244
    },
    {
        "loss": 2.8653,
        "grad_norm": 3.070666790008545,
        "learning_rate": 0.00013307059944739325,
        "epoch": 0.4653849020046203,
        "step": 6245
    },
    {
        "loss": 2.7397,
        "grad_norm": 3.5034561157226562,
        "learning_rate": 0.0001330041848014787,
        "epoch": 0.46545942320590206,
        "step": 6246
    },
    {
        "loss": 2.3226,
        "grad_norm": 2.204226493835449,
        "learning_rate": 0.00013293775381411165,
        "epoch": 0.4655339444071838,
        "step": 6247
    },
    {
        "loss": 2.4932,
        "grad_norm": 2.2866876125335693,
        "learning_rate": 0.00013287130651818413,
        "epoch": 0.4656084656084656,
        "step": 6248
    },
    {
        "loss": 2.9424,
        "grad_norm": 2.9486093521118164,
        "learning_rate": 0.0001328048429465964,
        "epoch": 0.46568298680974735,
        "step": 6249
    },
    {
        "loss": 1.4352,
        "grad_norm": 3.251499891281128,
        "learning_rate": 0.00013273836313225685,
        "epoch": 0.46575750801102916,
        "step": 6250
    },
    {
        "loss": 2.8112,
        "grad_norm": 3.719569683074951,
        "learning_rate": 0.00013267186710808168,
        "epoch": 0.4658320292123109,
        "step": 6251
    },
    {
        "loss": 2.6032,
        "grad_norm": 3.986687421798706,
        "learning_rate": 0.00013260535490699536,
        "epoch": 0.4659065504135927,
        "step": 6252
    },
    {
        "loss": 2.3314,
        "grad_norm": 2.7896697521209717,
        "learning_rate": 0.00013253882656193012,
        "epoch": 0.46598107161487445,
        "step": 6253
    },
    {
        "loss": 2.3251,
        "grad_norm": 2.696173667907715,
        "learning_rate": 0.0001324722821058265,
        "epoch": 0.4660555928161562,
        "step": 6254
    },
    {
        "loss": 2.2457,
        "grad_norm": 2.1014983654022217,
        "learning_rate": 0.00013240572157163268,
        "epoch": 0.466130114017438,
        "step": 6255
    },
    {
        "loss": 2.145,
        "grad_norm": 3.1106135845184326,
        "learning_rate": 0.000132339144992305,
        "epoch": 0.46620463521871974,
        "step": 6256
    },
    {
        "loss": 2.6696,
        "grad_norm": 2.2847018241882324,
        "learning_rate": 0.0001322725524008078,
        "epoch": 0.4662791564200015,
        "step": 6257
    },
    {
        "loss": 2.445,
        "grad_norm": 3.1595230102539062,
        "learning_rate": 0.00013220594383011314,
        "epoch": 0.46635367762128327,
        "step": 6258
    },
    {
        "loss": 2.5631,
        "grad_norm": 2.245056390762329,
        "learning_rate": 0.00013213931931320113,
        "epoch": 0.46642819882256503,
        "step": 6259
    },
    {
        "loss": 2.0576,
        "grad_norm": 3.664957046508789,
        "learning_rate": 0.00013207267888305983,
        "epoch": 0.4665027200238468,
        "step": 6260
    },
    {
        "loss": 1.895,
        "grad_norm": 3.3156533241271973,
        "learning_rate": 0.00013200602257268498,
        "epoch": 0.46657724122512856,
        "step": 6261
    },
    {
        "loss": 2.7141,
        "grad_norm": 3.0353434085845947,
        "learning_rate": 0.00013193935041508044,
        "epoch": 0.4666517624264103,
        "step": 6262
    },
    {
        "loss": 2.3991,
        "grad_norm": 3.109741449356079,
        "learning_rate": 0.00013187266244325758,
        "epoch": 0.4667262836276921,
        "step": 6263
    },
    {
        "loss": 2.6528,
        "grad_norm": 3.1033036708831787,
        "learning_rate": 0.00013180595869023603,
        "epoch": 0.46680080482897385,
        "step": 6264
    },
    {
        "loss": 2.6198,
        "grad_norm": 2.7054734230041504,
        "learning_rate": 0.0001317392391890428,
        "epoch": 0.4668753260302556,
        "step": 6265
    },
    {
        "loss": 2.1943,
        "grad_norm": 5.130122184753418,
        "learning_rate": 0.00013167250397271306,
        "epoch": 0.4669498472315374,
        "step": 6266
    },
    {
        "loss": 2.7494,
        "grad_norm": 2.653127670288086,
        "learning_rate": 0.00013160575307428945,
        "epoch": 0.46702436843281914,
        "step": 6267
    },
    {
        "loss": 2.2337,
        "grad_norm": 3.0918819904327393,
        "learning_rate": 0.00013153898652682258,
        "epoch": 0.4670988896341009,
        "step": 6268
    },
    {
        "loss": 2.0527,
        "grad_norm": 2.689558506011963,
        "learning_rate": 0.0001314722043633708,
        "epoch": 0.46717341083538266,
        "step": 6269
    },
    {
        "loss": 2.9188,
        "grad_norm": 2.821350574493408,
        "learning_rate": 0.0001314054066170002,
        "epoch": 0.4672479320366644,
        "step": 6270
    },
    {
        "loss": 1.4318,
        "grad_norm": 2.87520694732666,
        "learning_rate": 0.00013133859332078445,
        "epoch": 0.4673224532379462,
        "step": 6271
    },
    {
        "loss": 1.7622,
        "grad_norm": 2.3304905891418457,
        "learning_rate": 0.00013127176450780493,
        "epoch": 0.46739697443922795,
        "step": 6272
    },
    {
        "loss": 1.7856,
        "grad_norm": 4.040963172912598,
        "learning_rate": 0.00013120492021115085,
        "epoch": 0.4674714956405097,
        "step": 6273
    },
    {
        "loss": 2.0928,
        "grad_norm": 3.2031948566436768,
        "learning_rate": 0.00013113806046391907,
        "epoch": 0.4675460168417915,
        "step": 6274
    },
    {
        "loss": 1.7044,
        "grad_norm": 2.0643906593322754,
        "learning_rate": 0.00013107118529921393,
        "epoch": 0.46762053804307324,
        "step": 6275
    },
    {
        "loss": 2.735,
        "grad_norm": 3.138643264770508,
        "learning_rate": 0.00013100429475014762,
        "epoch": 0.467695059244355,
        "step": 6276
    },
    {
        "loss": 1.5656,
        "grad_norm": 4.177434921264648,
        "learning_rate": 0.0001309373888498397,
        "epoch": 0.46776958044563677,
        "step": 6277
    },
    {
        "loss": 2.0569,
        "grad_norm": 3.287396192550659,
        "learning_rate": 0.00013087046763141758,
        "epoch": 0.46784410164691853,
        "step": 6278
    },
    {
        "loss": 2.631,
        "grad_norm": 2.6848864555358887,
        "learning_rate": 0.00013080353112801608,
        "epoch": 0.4679186228482003,
        "step": 6279
    },
    {
        "loss": 1.9065,
        "grad_norm": 4.152795314788818,
        "learning_rate": 0.00013073657937277778,
        "epoch": 0.46799314404948206,
        "step": 6280
    },
    {
        "loss": 2.7121,
        "grad_norm": 3.4268341064453125,
        "learning_rate": 0.00013066961239885264,
        "epoch": 0.4680676652507638,
        "step": 6281
    },
    {
        "loss": 1.346,
        "grad_norm": 1.9029645919799805,
        "learning_rate": 0.00013060263023939808,
        "epoch": 0.4681421864520456,
        "step": 6282
    },
    {
        "loss": 2.2605,
        "grad_norm": 2.5631046295166016,
        "learning_rate": 0.00013053563292757923,
        "epoch": 0.46821670765332735,
        "step": 6283
    },
    {
        "loss": 2.1348,
        "grad_norm": 3.9091708660125732,
        "learning_rate": 0.00013046862049656874,
        "epoch": 0.4682912288546091,
        "step": 6284
    },
    {
        "loss": 2.489,
        "grad_norm": 2.730998992919922,
        "learning_rate": 0.00013040159297954658,
        "epoch": 0.46836575005589093,
        "step": 6285
    },
    {
        "loss": 3.3908,
        "grad_norm": 3.2912039756774902,
        "learning_rate": 0.00013033455040970017,
        "epoch": 0.4684402712571727,
        "step": 6286
    },
    {
        "loss": 2.5848,
        "grad_norm": 2.196589469909668,
        "learning_rate": 0.00013026749282022464,
        "epoch": 0.46851479245845445,
        "step": 6287
    },
    {
        "loss": 1.9104,
        "grad_norm": 3.8386075496673584,
        "learning_rate": 0.00013020042024432233,
        "epoch": 0.4685893136597362,
        "step": 6288
    },
    {
        "loss": 2.3282,
        "grad_norm": 3.120474100112915,
        "learning_rate": 0.0001301333327152031,
        "epoch": 0.468663834861018,
        "step": 6289
    },
    {
        "loss": 2.4497,
        "grad_norm": 3.3026206493377686,
        "learning_rate": 0.0001300662302660842,
        "epoch": 0.46873835606229974,
        "step": 6290
    },
    {
        "loss": 2.3916,
        "grad_norm": 3.4037749767303467,
        "learning_rate": 0.0001299991129301902,
        "epoch": 0.4688128772635815,
        "step": 6291
    },
    {
        "loss": 2.4257,
        "grad_norm": 1.9257886409759521,
        "learning_rate": 0.0001299319807407531,
        "epoch": 0.46888739846486327,
        "step": 6292
    },
    {
        "loss": 2.4135,
        "grad_norm": 3.0835788249969482,
        "learning_rate": 0.00012986483373101222,
        "epoch": 0.46896191966614503,
        "step": 6293
    },
    {
        "loss": 2.0411,
        "grad_norm": 1.6388682126998901,
        "learning_rate": 0.00012979767193421431,
        "epoch": 0.4690364408674268,
        "step": 6294
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.478020668029785,
        "learning_rate": 0.00012973049538361338,
        "epoch": 0.46911096206870856,
        "step": 6295
    },
    {
        "loss": 2.2489,
        "grad_norm": 3.7710678577423096,
        "learning_rate": 0.0001296633041124706,
        "epoch": 0.4691854832699903,
        "step": 6296
    },
    {
        "loss": 2.5154,
        "grad_norm": 2.1985576152801514,
        "learning_rate": 0.00012959609815405468,
        "epoch": 0.4692600044712721,
        "step": 6297
    },
    {
        "loss": 2.9512,
        "grad_norm": 3.303629159927368,
        "learning_rate": 0.0001295288775416415,
        "epoch": 0.46933452567255385,
        "step": 6298
    },
    {
        "loss": 2.5864,
        "grad_norm": 2.622962236404419,
        "learning_rate": 0.0001294616423085142,
        "epoch": 0.4694090468738356,
        "step": 6299
    },
    {
        "loss": 2.769,
        "grad_norm": 2.6969778537750244,
        "learning_rate": 0.00012939439248796304,
        "epoch": 0.4694835680751174,
        "step": 6300
    },
    {
        "loss": 2.8473,
        "grad_norm": 2.053166389465332,
        "learning_rate": 0.00012932712811328579,
        "epoch": 0.46955808927639914,
        "step": 6301
    },
    {
        "loss": 1.9663,
        "grad_norm": 3.2270240783691406,
        "learning_rate": 0.000129259849217787,
        "epoch": 0.4696326104776809,
        "step": 6302
    },
    {
        "loss": 1.8247,
        "grad_norm": 2.8735642433166504,
        "learning_rate": 0.00012919255583477882,
        "epoch": 0.46970713167896266,
        "step": 6303
    },
    {
        "loss": 3.0486,
        "grad_norm": 3.1110525131225586,
        "learning_rate": 0.00012912524799758043,
        "epoch": 0.4697816528802444,
        "step": 6304
    },
    {
        "loss": 2.3678,
        "grad_norm": 3.3465631008148193,
        "learning_rate": 0.0001290579257395181,
        "epoch": 0.4698561740815262,
        "step": 6305
    },
    {
        "loss": 2.3551,
        "grad_norm": 2.1066524982452393,
        "learning_rate": 0.0001289905890939252,
        "epoch": 0.46993069528280795,
        "step": 6306
    },
    {
        "loss": 2.4568,
        "grad_norm": 2.292170286178589,
        "learning_rate": 0.00012892323809414238,
        "epoch": 0.4700052164840897,
        "step": 6307
    },
    {
        "loss": 2.1445,
        "grad_norm": 4.20120906829834,
        "learning_rate": 0.00012885587277351732,
        "epoch": 0.4700797376853715,
        "step": 6308
    },
    {
        "loss": 1.6101,
        "grad_norm": 4.1104631423950195,
        "learning_rate": 0.00012878849316540488,
        "epoch": 0.47015425888665324,
        "step": 6309
    },
    {
        "loss": 2.1253,
        "grad_norm": 2.656989574432373,
        "learning_rate": 0.00012872109930316673,
        "epoch": 0.470228780087935,
        "step": 6310
    },
    {
        "loss": 2.4605,
        "grad_norm": 2.4065651893615723,
        "learning_rate": 0.00012865369122017198,
        "epoch": 0.47030330128921677,
        "step": 6311
    },
    {
        "loss": 2.2673,
        "grad_norm": 2.8445942401885986,
        "learning_rate": 0.00012858626894979639,
        "epoch": 0.47037782249049853,
        "step": 6312
    },
    {
        "loss": 2.8497,
        "grad_norm": 2.207937717437744,
        "learning_rate": 0.0001285188325254231,
        "epoch": 0.4704523436917803,
        "step": 6313
    },
    {
        "loss": 2.1307,
        "grad_norm": 3.118903398513794,
        "learning_rate": 0.00012845138198044195,
        "epoch": 0.47052686489306206,
        "step": 6314
    },
    {
        "loss": 2.014,
        "grad_norm": 3.104278326034546,
        "learning_rate": 0.00012838391734825005,
        "epoch": 0.4706013860943438,
        "step": 6315
    },
    {
        "loss": 1.564,
        "grad_norm": 2.625786781311035,
        "learning_rate": 0.00012831643866225117,
        "epoch": 0.4706759072956256,
        "step": 6316
    },
    {
        "loss": 1.9029,
        "grad_norm": 2.8714466094970703,
        "learning_rate": 0.00012824894595585634,
        "epoch": 0.47075042849690735,
        "step": 6317
    },
    {
        "loss": 2.6806,
        "grad_norm": 3.283724546432495,
        "learning_rate": 0.00012818143926248348,
        "epoch": 0.4708249496981891,
        "step": 6318
    },
    {
        "loss": 1.5377,
        "grad_norm": 3.1732161045074463,
        "learning_rate": 0.00012811391861555722,
        "epoch": 0.4708994708994709,
        "step": 6319
    },
    {
        "loss": 2.2338,
        "grad_norm": 3.2574048042297363,
        "learning_rate": 0.00012804638404850926,
        "epoch": 0.47097399210075264,
        "step": 6320
    },
    {
        "loss": 2.5707,
        "grad_norm": 3.47636079788208,
        "learning_rate": 0.00012797883559477827,
        "epoch": 0.47104851330203446,
        "step": 6321
    },
    {
        "loss": 2.2132,
        "grad_norm": 3.1416478157043457,
        "learning_rate": 0.00012791127328780965,
        "epoch": 0.4711230345033162,
        "step": 6322
    },
    {
        "loss": 2.247,
        "grad_norm": 3.0090901851654053,
        "learning_rate": 0.0001278436971610557,
        "epoch": 0.471197555704598,
        "step": 6323
    },
    {
        "loss": 2.2929,
        "grad_norm": 3.408403158187866,
        "learning_rate": 0.00012777610724797555,
        "epoch": 0.47127207690587974,
        "step": 6324
    },
    {
        "loss": 1.4622,
        "grad_norm": 1.3853017091751099,
        "learning_rate": 0.00012770850358203526,
        "epoch": 0.4713465981071615,
        "step": 6325
    },
    {
        "loss": 1.7991,
        "grad_norm": 3.9761033058166504,
        "learning_rate": 0.0001276408861967075,
        "epoch": 0.47142111930844327,
        "step": 6326
    },
    {
        "loss": 2.335,
        "grad_norm": 2.030247449874878,
        "learning_rate": 0.000127573255125472,
        "epoch": 0.47149564050972503,
        "step": 6327
    },
    {
        "loss": 2.38,
        "grad_norm": 3.6626522541046143,
        "learning_rate": 0.00012750561040181496,
        "epoch": 0.4715701617110068,
        "step": 6328
    },
    {
        "loss": 1.3811,
        "grad_norm": 3.277794122695923,
        "learning_rate": 0.00012743795205922954,
        "epoch": 0.47164468291228856,
        "step": 6329
    },
    {
        "loss": 2.1625,
        "grad_norm": 4.665518760681152,
        "learning_rate": 0.0001273702801312157,
        "epoch": 0.4717192041135703,
        "step": 6330
    },
    {
        "loss": 1.7492,
        "grad_norm": 2.9219295978546143,
        "learning_rate": 0.00012730259465128,
        "epoch": 0.4717937253148521,
        "step": 6331
    },
    {
        "loss": 2.6869,
        "grad_norm": 2.1335558891296387,
        "learning_rate": 0.00012723489565293568,
        "epoch": 0.47186824651613385,
        "step": 6332
    },
    {
        "loss": 2.3592,
        "grad_norm": 3.605722427368164,
        "learning_rate": 0.0001271671831697027,
        "epoch": 0.4719427677174156,
        "step": 6333
    },
    {
        "loss": 2.4053,
        "grad_norm": 3.692112684249878,
        "learning_rate": 0.00012709945723510778,
        "epoch": 0.4720172889186974,
        "step": 6334
    },
    {
        "loss": 2.6518,
        "grad_norm": 3.014376163482666,
        "learning_rate": 0.0001270317178826843,
        "epoch": 0.47209181011997914,
        "step": 6335
    },
    {
        "loss": 3.0413,
        "grad_norm": 2.2615978717803955,
        "learning_rate": 0.0001269639651459721,
        "epoch": 0.4721663313212609,
        "step": 6336
    },
    {
        "loss": 2.1325,
        "grad_norm": 2.619692087173462,
        "learning_rate": 0.00012689619905851797,
        "epoch": 0.47224085252254266,
        "step": 6337
    },
    {
        "loss": 1.6682,
        "grad_norm": 2.701089382171631,
        "learning_rate": 0.00012682841965387488,
        "epoch": 0.4723153737238244,
        "step": 6338
    },
    {
        "loss": 2.7608,
        "grad_norm": 2.913853645324707,
        "learning_rate": 0.0001267606269656028,
        "epoch": 0.4723898949251062,
        "step": 6339
    },
    {
        "loss": 2.2017,
        "grad_norm": 6.096031188964844,
        "learning_rate": 0.00012669282102726804,
        "epoch": 0.47246441612638795,
        "step": 6340
    },
    {
        "loss": 2.4161,
        "grad_norm": 2.6050498485565186,
        "learning_rate": 0.00012662500187244366,
        "epoch": 0.4725389373276697,
        "step": 6341
    },
    {
        "loss": 2.7007,
        "grad_norm": 2.6140365600585938,
        "learning_rate": 0.000126557169534709,
        "epoch": 0.4726134585289515,
        "step": 6342
    },
    {
        "loss": 2.0335,
        "grad_norm": 3.89099383354187,
        "learning_rate": 0.0001264893240476501,
        "epoch": 0.47268797973023324,
        "step": 6343
    },
    {
        "loss": 2.4874,
        "grad_norm": 3.354428291320801,
        "learning_rate": 0.00012642146544485948,
        "epoch": 0.472762500931515,
        "step": 6344
    },
    {
        "loss": 2.5426,
        "grad_norm": 2.7502641677856445,
        "learning_rate": 0.0001263535937599363,
        "epoch": 0.47283702213279677,
        "step": 6345
    },
    {
        "loss": 2.6509,
        "grad_norm": 3.9072089195251465,
        "learning_rate": 0.0001262857090264859,
        "epoch": 0.47291154333407853,
        "step": 6346
    },
    {
        "loss": 2.6937,
        "grad_norm": 3.2792913913726807,
        "learning_rate": 0.00012621781127812018,
        "epoch": 0.4729860645353603,
        "step": 6347
    },
    {
        "loss": 2.2856,
        "grad_norm": 3.3953351974487305,
        "learning_rate": 0.00012614990054845765,
        "epoch": 0.47306058573664206,
        "step": 6348
    },
    {
        "loss": 2.0193,
        "grad_norm": 4.948731422424316,
        "learning_rate": 0.0001260819768711231,
        "epoch": 0.4731351069379238,
        "step": 6349
    },
    {
        "loss": 2.5703,
        "grad_norm": 2.520186185836792,
        "learning_rate": 0.0001260140402797478,
        "epoch": 0.4732096281392056,
        "step": 6350
    },
    {
        "loss": 2.385,
        "grad_norm": 2.8851683139801025,
        "learning_rate": 0.0001259460908079694,
        "epoch": 0.47328414934048735,
        "step": 6351
    },
    {
        "loss": 2.899,
        "grad_norm": 2.9429409503936768,
        "learning_rate": 0.0001258781284894319,
        "epoch": 0.4733586705417691,
        "step": 6352
    },
    {
        "loss": 2.7248,
        "grad_norm": 2.2094812393188477,
        "learning_rate": 0.00012581015335778555,
        "epoch": 0.4734331917430509,
        "step": 6353
    },
    {
        "loss": 2.6626,
        "grad_norm": 3.3531482219696045,
        "learning_rate": 0.00012574216544668714,
        "epoch": 0.47350771294433264,
        "step": 6354
    },
    {
        "loss": 1.3136,
        "grad_norm": 4.076025009155273,
        "learning_rate": 0.00012567416478979982,
        "epoch": 0.4735822341456144,
        "step": 6355
    },
    {
        "loss": 2.5654,
        "grad_norm": 3.2110188007354736,
        "learning_rate": 0.00012560615142079287,
        "epoch": 0.4736567553468962,
        "step": 6356
    },
    {
        "loss": 2.1628,
        "grad_norm": 4.212607383728027,
        "learning_rate": 0.00012553812537334177,
        "epoch": 0.473731276548178,
        "step": 6357
    },
    {
        "loss": 2.4193,
        "grad_norm": 3.3677756786346436,
        "learning_rate": 0.00012547008668112864,
        "epoch": 0.47380579774945975,
        "step": 6358
    },
    {
        "loss": 2.3908,
        "grad_norm": 2.256434440612793,
        "learning_rate": 0.00012540203537784157,
        "epoch": 0.4738803189507415,
        "step": 6359
    },
    {
        "loss": 2.3822,
        "grad_norm": 3.4237184524536133,
        "learning_rate": 0.00012533397149717512,
        "epoch": 0.47395484015202327,
        "step": 6360
    },
    {
        "loss": 2.7015,
        "grad_norm": 2.3401689529418945,
        "learning_rate": 0.00012526589507282977,
        "epoch": 0.47402936135330503,
        "step": 6361
    },
    {
        "loss": 2.1732,
        "grad_norm": 3.327160358428955,
        "learning_rate": 0.00012519780613851254,
        "epoch": 0.4741038825545868,
        "step": 6362
    },
    {
        "loss": 2.0144,
        "grad_norm": 4.469804763793945,
        "learning_rate": 0.00012512970472793638,
        "epoch": 0.47417840375586856,
        "step": 6363
    },
    {
        "loss": 2.7231,
        "grad_norm": 3.518224000930786,
        "learning_rate": 0.00012506159087482055,
        "epoch": 0.4742529249571503,
        "step": 6364
    },
    {
        "loss": 2.3791,
        "grad_norm": 4.372869968414307,
        "learning_rate": 0.00012499346461289053,
        "epoch": 0.4743274461584321,
        "step": 6365
    },
    {
        "loss": 2.2188,
        "grad_norm": 2.1005444526672363,
        "learning_rate": 0.0001249253259758778,
        "epoch": 0.47440196735971385,
        "step": 6366
    },
    {
        "loss": 2.5277,
        "grad_norm": 2.936385154724121,
        "learning_rate": 0.00012485717499751998,
        "epoch": 0.4744764885609956,
        "step": 6367
    },
    {
        "loss": 1.8024,
        "grad_norm": 2.488259792327881,
        "learning_rate": 0.00012478901171156088,
        "epoch": 0.4745510097622774,
        "step": 6368
    },
    {
        "loss": 2.1528,
        "grad_norm": 2.620770215988159,
        "learning_rate": 0.0001247208361517504,
        "epoch": 0.47462553096355914,
        "step": 6369
    },
    {
        "loss": 1.619,
        "grad_norm": 3.7709691524505615,
        "learning_rate": 0.00012465264835184454,
        "epoch": 0.4747000521648409,
        "step": 6370
    },
    {
        "loss": 2.904,
        "grad_norm": 2.5919013023376465,
        "learning_rate": 0.00012458444834560523,
        "epoch": 0.47477457336612267,
        "step": 6371
    },
    {
        "loss": 2.9641,
        "grad_norm": 2.4466655254364014,
        "learning_rate": 0.0001245162361668006,
        "epoch": 0.47484909456740443,
        "step": 6372
    },
    {
        "loss": 2.6192,
        "grad_norm": 1.6512986421585083,
        "learning_rate": 0.00012444801184920463,
        "epoch": 0.4749236157686862,
        "step": 6373
    },
    {
        "loss": 2.2726,
        "grad_norm": 3.128748893737793,
        "learning_rate": 0.00012437977542659754,
        "epoch": 0.47499813696996795,
        "step": 6374
    },
    {
        "loss": 2.2074,
        "grad_norm": 3.509577751159668,
        "learning_rate": 0.00012431152693276532,
        "epoch": 0.4750726581712497,
        "step": 6375
    },
    {
        "loss": 1.8189,
        "grad_norm": 4.203389644622803,
        "learning_rate": 0.00012424326640150007,
        "epoch": 0.4751471793725315,
        "step": 6376
    },
    {
        "loss": 2.501,
        "grad_norm": 2.39272141456604,
        "learning_rate": 0.00012417499386659978,
        "epoch": 0.47522170057381324,
        "step": 6377
    },
    {
        "loss": 2.6413,
        "grad_norm": 2.8755252361297607,
        "learning_rate": 0.00012410670936186845,
        "epoch": 0.475296221775095,
        "step": 6378
    },
    {
        "loss": 2.7983,
        "grad_norm": 2.5548434257507324,
        "learning_rate": 0.000124038412921116,
        "epoch": 0.47537074297637677,
        "step": 6379
    },
    {
        "loss": 2.1608,
        "grad_norm": 2.9344398975372314,
        "learning_rate": 0.0001239701045781582,
        "epoch": 0.47544526417765853,
        "step": 6380
    },
    {
        "loss": 2.5181,
        "grad_norm": 2.572429656982422,
        "learning_rate": 0.0001239017843668167,
        "epoch": 0.4755197853789403,
        "step": 6381
    },
    {
        "loss": 2.4999,
        "grad_norm": 2.427651882171631,
        "learning_rate": 0.00012383345232091922,
        "epoch": 0.47559430658022206,
        "step": 6382
    },
    {
        "loss": 2.9046,
        "grad_norm": 2.7939364910125732,
        "learning_rate": 0.00012376510847429903,
        "epoch": 0.4756688277815038,
        "step": 6383
    },
    {
        "loss": 2.5993,
        "grad_norm": 2.474513292312622,
        "learning_rate": 0.00012369675286079553,
        "epoch": 0.4757433489827856,
        "step": 6384
    },
    {
        "loss": 2.6432,
        "grad_norm": 2.3431789875030518,
        "learning_rate": 0.00012362838551425372,
        "epoch": 0.47581787018406735,
        "step": 6385
    },
    {
        "loss": 2.7861,
        "grad_norm": 2.787245750427246,
        "learning_rate": 0.00012356000646852463,
        "epoch": 0.4758923913853491,
        "step": 6386
    },
    {
        "loss": 2.3893,
        "grad_norm": 2.3749806880950928,
        "learning_rate": 0.0001234916157574648,
        "epoch": 0.4759669125866309,
        "step": 6387
    },
    {
        "loss": 2.3702,
        "grad_norm": 3.3798677921295166,
        "learning_rate": 0.00012342321341493687,
        "epoch": 0.47604143378791264,
        "step": 6388
    },
    {
        "loss": 2.0303,
        "grad_norm": 2.336120843887329,
        "learning_rate": 0.00012335479947480897,
        "epoch": 0.4761159549891944,
        "step": 6389
    },
    {
        "loss": 2.07,
        "grad_norm": 3.154528856277466,
        "learning_rate": 0.00012328637397095515,
        "epoch": 0.47619047619047616,
        "step": 6390
    },
    {
        "loss": 1.9928,
        "grad_norm": 4.340917587280273,
        "learning_rate": 0.00012321793693725506,
        "epoch": 0.476264997391758,
        "step": 6391
    },
    {
        "loss": 2.8099,
        "grad_norm": 2.949338674545288,
        "learning_rate": 0.0001231494884075942,
        "epoch": 0.47633951859303975,
        "step": 6392
    },
    {
        "loss": 2.4528,
        "grad_norm": 3.0982437133789062,
        "learning_rate": 0.00012308102841586367,
        "epoch": 0.4764140397943215,
        "step": 6393
    },
    {
        "loss": 2.5726,
        "grad_norm": 4.1637396812438965,
        "learning_rate": 0.00012301255699596013,
        "epoch": 0.47648856099560327,
        "step": 6394
    },
    {
        "loss": 1.9538,
        "grad_norm": 3.9680283069610596,
        "learning_rate": 0.0001229440741817861,
        "epoch": 0.47656308219688504,
        "step": 6395
    },
    {
        "loss": 2.6437,
        "grad_norm": 4.325686931610107,
        "learning_rate": 0.0001228755800072497,
        "epoch": 0.4766376033981668,
        "step": 6396
    },
    {
        "loss": 2.4316,
        "grad_norm": 2.555981159210205,
        "learning_rate": 0.00012280707450626458,
        "epoch": 0.47671212459944856,
        "step": 6397
    },
    {
        "loss": 2.2331,
        "grad_norm": 2.946563720703125,
        "learning_rate": 0.00012273855771275017,
        "epoch": 0.4767866458007303,
        "step": 6398
    },
    {
        "loss": 2.1328,
        "grad_norm": 2.737550973892212,
        "learning_rate": 0.0001226700296606312,
        "epoch": 0.4768611670020121,
        "step": 6399
    },
    {
        "loss": 2.4608,
        "grad_norm": 2.88539719581604,
        "learning_rate": 0.00012260149038383823,
        "epoch": 0.47693568820329385,
        "step": 6400
    },
    {
        "loss": 2.1562,
        "grad_norm": 4.557793617248535,
        "learning_rate": 0.00012253293991630731,
        "epoch": 0.4770102094045756,
        "step": 6401
    },
    {
        "loss": 2.4961,
        "grad_norm": 2.907923460006714,
        "learning_rate": 0.00012246437829198015,
        "epoch": 0.4770847306058574,
        "step": 6402
    },
    {
        "loss": 2.2924,
        "grad_norm": 4.527344226837158,
        "learning_rate": 0.00012239580554480368,
        "epoch": 0.47715925180713914,
        "step": 6403
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.0923638343811035,
        "learning_rate": 0.0001223272217087305,
        "epoch": 0.4772337730084209,
        "step": 6404
    },
    {
        "loss": 2.204,
        "grad_norm": 3.1567463874816895,
        "learning_rate": 0.0001222586268177188,
        "epoch": 0.47730829420970267,
        "step": 6405
    },
    {
        "loss": 2.3979,
        "grad_norm": 3.8041064739227295,
        "learning_rate": 0.00012219002090573216,
        "epoch": 0.47738281541098443,
        "step": 6406
    },
    {
        "loss": 2.7722,
        "grad_norm": 3.4946210384368896,
        "learning_rate": 0.0001221214040067396,
        "epoch": 0.4774573366122662,
        "step": 6407
    },
    {
        "loss": 2.1829,
        "grad_norm": 3.5935235023498535,
        "learning_rate": 0.0001220527761547155,
        "epoch": 0.47753185781354796,
        "step": 6408
    },
    {
        "loss": 2.3324,
        "grad_norm": 3.358182668685913,
        "learning_rate": 0.00012198413738363987,
        "epoch": 0.4776063790148297,
        "step": 6409
    },
    {
        "loss": 2.2862,
        "grad_norm": 2.626307249069214,
        "learning_rate": 0.00012191548772749797,
        "epoch": 0.4776809002161115,
        "step": 6410
    },
    {
        "loss": 2.7126,
        "grad_norm": 1.7106348276138306,
        "learning_rate": 0.00012184682722028049,
        "epoch": 0.47775542141739324,
        "step": 6411
    },
    {
        "loss": 2.505,
        "grad_norm": 2.224332094192505,
        "learning_rate": 0.00012177815589598355,
        "epoch": 0.477829942618675,
        "step": 6412
    },
    {
        "loss": 2.1267,
        "grad_norm": 3.667092800140381,
        "learning_rate": 0.00012170947378860858,
        "epoch": 0.47790446381995677,
        "step": 6413
    },
    {
        "loss": 2.1934,
        "grad_norm": 3.188260078430176,
        "learning_rate": 0.00012164078093216218,
        "epoch": 0.47797898502123853,
        "step": 6414
    },
    {
        "loss": 1.4651,
        "grad_norm": 4.476808547973633,
        "learning_rate": 0.00012157207736065657,
        "epoch": 0.4780535062225203,
        "step": 6415
    },
    {
        "loss": 2.5955,
        "grad_norm": 3.677917242050171,
        "learning_rate": 0.00012150336310810916,
        "epoch": 0.47812802742380206,
        "step": 6416
    },
    {
        "loss": 2.3197,
        "grad_norm": 2.983370065689087,
        "learning_rate": 0.00012143463820854258,
        "epoch": 0.4782025486250838,
        "step": 6417
    },
    {
        "loss": 2.2931,
        "grad_norm": 1.9560754299163818,
        "learning_rate": 0.0001213659026959847,
        "epoch": 0.4782770698263656,
        "step": 6418
    },
    {
        "loss": 1.9687,
        "grad_norm": 3.372091770172119,
        "learning_rate": 0.00012129715660446876,
        "epoch": 0.47835159102764735,
        "step": 6419
    },
    {
        "loss": 2.3771,
        "grad_norm": 3.2548253536224365,
        "learning_rate": 0.00012122839996803327,
        "epoch": 0.4784261122289291,
        "step": 6420
    },
    {
        "loss": 1.5158,
        "grad_norm": 4.955907821655273,
        "learning_rate": 0.00012115963282072195,
        "epoch": 0.4785006334302109,
        "step": 6421
    },
    {
        "loss": 2.767,
        "grad_norm": 3.549088716506958,
        "learning_rate": 0.00012109085519658348,
        "epoch": 0.47857515463149264,
        "step": 6422
    },
    {
        "loss": 1.8341,
        "grad_norm": 3.6691596508026123,
        "learning_rate": 0.00012102206712967208,
        "epoch": 0.4786496758327744,
        "step": 6423
    },
    {
        "loss": 2.6437,
        "grad_norm": 2.1708014011383057,
        "learning_rate": 0.00012095326865404686,
        "epoch": 0.47872419703405616,
        "step": 6424
    },
    {
        "loss": 1.9625,
        "grad_norm": 3.5079116821289062,
        "learning_rate": 0.00012088445980377221,
        "epoch": 0.47879871823533793,
        "step": 6425
    },
    {
        "loss": 2.4368,
        "grad_norm": 3.5931077003479004,
        "learning_rate": 0.00012081564061291775,
        "epoch": 0.4788732394366197,
        "step": 6426
    },
    {
        "loss": 2.1098,
        "grad_norm": 2.900341033935547,
        "learning_rate": 0.00012074681111555806,
        "epoch": 0.4789477606379015,
        "step": 6427
    },
    {
        "loss": 1.3528,
        "grad_norm": 2.808872938156128,
        "learning_rate": 0.00012067797134577276,
        "epoch": 0.4790222818391833,
        "step": 6428
    },
    {
        "loss": 3.188,
        "grad_norm": 2.6337616443634033,
        "learning_rate": 0.0001206091213376468,
        "epoch": 0.47909680304046504,
        "step": 6429
    },
    {
        "loss": 1.6421,
        "grad_norm": 3.737445592880249,
        "learning_rate": 0.00012054026112527002,
        "epoch": 0.4791713242417468,
        "step": 6430
    },
    {
        "loss": 2.0988,
        "grad_norm": 2.3949921131134033,
        "learning_rate": 0.00012047139074273746,
        "epoch": 0.47924584544302856,
        "step": 6431
    },
    {
        "loss": 2.9309,
        "grad_norm": 2.21799898147583,
        "learning_rate": 0.00012040251022414899,
        "epoch": 0.4793203666443103,
        "step": 6432
    },
    {
        "loss": 2.8339,
        "grad_norm": 2.739262104034424,
        "learning_rate": 0.00012033361960360971,
        "epoch": 0.4793948878455921,
        "step": 6433
    },
    {
        "loss": 2.23,
        "grad_norm": 3.1054930686950684,
        "learning_rate": 0.0001202647189152295,
        "epoch": 0.47946940904687385,
        "step": 6434
    },
    {
        "loss": 2.8909,
        "grad_norm": 2.123589038848877,
        "learning_rate": 0.00012019580819312348,
        "epoch": 0.4795439302481556,
        "step": 6435
    },
    {
        "loss": 2.6016,
        "grad_norm": 2.346649646759033,
        "learning_rate": 0.00012012688747141147,
        "epoch": 0.4796184514494374,
        "step": 6436
    },
    {
        "loss": 2.6908,
        "grad_norm": 2.702364444732666,
        "learning_rate": 0.0001200579567842185,
        "epoch": 0.47969297265071914,
        "step": 6437
    },
    {
        "loss": 2.5976,
        "grad_norm": 2.644460916519165,
        "learning_rate": 0.00011998901616567429,
        "epoch": 0.4797674938520009,
        "step": 6438
    },
    {
        "loss": 2.7277,
        "grad_norm": 1.9060684442520142,
        "learning_rate": 0.00011992006564991368,
        "epoch": 0.47984201505328267,
        "step": 6439
    },
    {
        "loss": 2.2789,
        "grad_norm": 3.6056816577911377,
        "learning_rate": 0.00011985110527107635,
        "epoch": 0.47991653625456443,
        "step": 6440
    },
    {
        "loss": 2.0853,
        "grad_norm": 1.9871745109558105,
        "learning_rate": 0.00011978213506330676,
        "epoch": 0.4799910574558462,
        "step": 6441
    },
    {
        "loss": 2.1541,
        "grad_norm": 3.9646027088165283,
        "learning_rate": 0.00011971315506075434,
        "epoch": 0.48006557865712796,
        "step": 6442
    },
    {
        "loss": 2.0983,
        "grad_norm": 3.106440305709839,
        "learning_rate": 0.00011964416529757345,
        "epoch": 0.4801400998584097,
        "step": 6443
    },
    {
        "loss": 1.7692,
        "grad_norm": 2.914682626724243,
        "learning_rate": 0.00011957516580792301,
        "epoch": 0.4802146210596915,
        "step": 6444
    },
    {
        "loss": 2.144,
        "grad_norm": 3.3882639408111572,
        "learning_rate": 0.00011950615662596707,
        "epoch": 0.48028914226097325,
        "step": 6445
    },
    {
        "loss": 1.5186,
        "grad_norm": 4.293605327606201,
        "learning_rate": 0.00011943713778587424,
        "epoch": 0.480363663462255,
        "step": 6446
    },
    {
        "loss": 2.4626,
        "grad_norm": 2.275155782699585,
        "learning_rate": 0.00011936810932181812,
        "epoch": 0.48043818466353677,
        "step": 6447
    },
    {
        "loss": 2.0873,
        "grad_norm": 4.890300750732422,
        "learning_rate": 0.00011929907126797681,
        "epoch": 0.48051270586481853,
        "step": 6448
    },
    {
        "loss": 2.5125,
        "grad_norm": 2.718423366546631,
        "learning_rate": 0.00011923002365853339,
        "epoch": 0.4805872270661003,
        "step": 6449
    },
    {
        "loss": 2.492,
        "grad_norm": 2.759434700012207,
        "learning_rate": 0.00011916096652767567,
        "epoch": 0.48066174826738206,
        "step": 6450
    },
    {
        "loss": 2.5138,
        "grad_norm": 1.857277512550354,
        "learning_rate": 0.00011909189990959595,
        "epoch": 0.4807362694686638,
        "step": 6451
    },
    {
        "loss": 2.4745,
        "grad_norm": 2.190855026245117,
        "learning_rate": 0.00011902282383849143,
        "epoch": 0.4808107906699456,
        "step": 6452
    },
    {
        "loss": 1.7769,
        "grad_norm": 2.9854867458343506,
        "learning_rate": 0.00011895373834856405,
        "epoch": 0.48088531187122735,
        "step": 6453
    },
    {
        "loss": 2.151,
        "grad_norm": 3.0958468914031982,
        "learning_rate": 0.00011888464347402017,
        "epoch": 0.4809598330725091,
        "step": 6454
    },
    {
        "loss": 2.3258,
        "grad_norm": 2.392364978790283,
        "learning_rate": 0.0001188155392490709,
        "epoch": 0.4810343542737909,
        "step": 6455
    },
    {
        "loss": 2.0867,
        "grad_norm": 2.400848388671875,
        "learning_rate": 0.00011874642570793205,
        "epoch": 0.48110887547507264,
        "step": 6456
    },
    {
        "loss": 2.6654,
        "grad_norm": 2.606505870819092,
        "learning_rate": 0.00011867730288482406,
        "epoch": 0.4811833966763544,
        "step": 6457
    },
    {
        "loss": 2.5548,
        "grad_norm": 3.4003303050994873,
        "learning_rate": 0.0001186081708139718,
        "epoch": 0.48125791787763617,
        "step": 6458
    },
    {
        "loss": 2.4348,
        "grad_norm": 3.373708486557007,
        "learning_rate": 0.00011853902952960494,
        "epoch": 0.48133243907891793,
        "step": 6459
    },
    {
        "loss": 1.9118,
        "grad_norm": 2.3880064487457275,
        "learning_rate": 0.00011846987906595743,
        "epoch": 0.4814069602801997,
        "step": 6460
    },
    {
        "loss": 1.9055,
        "grad_norm": 5.239226341247559,
        "learning_rate": 0.00011840071945726803,
        "epoch": 0.48148148148148145,
        "step": 6461
    },
    {
        "loss": 2.4605,
        "grad_norm": 2.348438262939453,
        "learning_rate": 0.00011833155073777993,
        "epoch": 0.4815560026827633,
        "step": 6462
    },
    {
        "loss": 2.7464,
        "grad_norm": 2.5162594318389893,
        "learning_rate": 0.00011826237294174087,
        "epoch": 0.48163052388404504,
        "step": 6463
    },
    {
        "loss": 2.629,
        "grad_norm": 4.150735855102539,
        "learning_rate": 0.00011819318610340299,
        "epoch": 0.4817050450853268,
        "step": 6464
    },
    {
        "loss": 2.6058,
        "grad_norm": 2.1380393505096436,
        "learning_rate": 0.0001181239902570229,
        "epoch": 0.48177956628660856,
        "step": 6465
    },
    {
        "loss": 3.0735,
        "grad_norm": 3.238987684249878,
        "learning_rate": 0.0001180547854368618,
        "epoch": 0.4818540874878903,
        "step": 6466
    },
    {
        "loss": 2.8837,
        "grad_norm": 3.3417601585388184,
        "learning_rate": 0.00011798557167718528,
        "epoch": 0.4819286086891721,
        "step": 6467
    },
    {
        "loss": 2.8638,
        "grad_norm": 2.8540537357330322,
        "learning_rate": 0.00011791634901226332,
        "epoch": 0.48200312989045385,
        "step": 6468
    },
    {
        "loss": 2.4938,
        "grad_norm": 3.366854429244995,
        "learning_rate": 0.00011784711747637028,
        "epoch": 0.4820776510917356,
        "step": 6469
    },
    {
        "loss": 1.7449,
        "grad_norm": 3.289552688598633,
        "learning_rate": 0.00011777787710378494,
        "epoch": 0.4821521722930174,
        "step": 6470
    },
    {
        "loss": 2.4553,
        "grad_norm": 2.851436138153076,
        "learning_rate": 0.00011770862792879057,
        "epoch": 0.48222669349429914,
        "step": 6471
    },
    {
        "loss": 3.0368,
        "grad_norm": 3.017292022705078,
        "learning_rate": 0.00011763936998567464,
        "epoch": 0.4823012146955809,
        "step": 6472
    },
    {
        "loss": 2.4904,
        "grad_norm": 2.341418743133545,
        "learning_rate": 0.00011757010330872915,
        "epoch": 0.48237573589686267,
        "step": 6473
    },
    {
        "loss": 1.7667,
        "grad_norm": 3.2155001163482666,
        "learning_rate": 0.0001175008279322502,
        "epoch": 0.48245025709814443,
        "step": 6474
    },
    {
        "loss": 2.3092,
        "grad_norm": 2.5432257652282715,
        "learning_rate": 0.0001174315438905382,
        "epoch": 0.4825247782994262,
        "step": 6475
    },
    {
        "loss": 2.6924,
        "grad_norm": 2.1528756618499756,
        "learning_rate": 0.00011736225121789805,
        "epoch": 0.48259929950070796,
        "step": 6476
    },
    {
        "loss": 2.171,
        "grad_norm": 2.5902321338653564,
        "learning_rate": 0.00011729294994863893,
        "epoch": 0.4826738207019897,
        "step": 6477
    },
    {
        "loss": 1.8216,
        "grad_norm": 4.252017021179199,
        "learning_rate": 0.00011722364011707401,
        "epoch": 0.4827483419032715,
        "step": 6478
    },
    {
        "loss": 2.4742,
        "grad_norm": 3.041322946548462,
        "learning_rate": 0.00011715432175752084,
        "epoch": 0.48282286310455325,
        "step": 6479
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.3943161964416504,
        "learning_rate": 0.00011708499490430127,
        "epoch": 0.482897384305835,
        "step": 6480
    },
    {
        "loss": 1.6986,
        "grad_norm": 3.6955766677856445,
        "learning_rate": 0.00011701565959174132,
        "epoch": 0.48297190550711677,
        "step": 6481
    },
    {
        "loss": 1.9004,
        "grad_norm": 4.060744762420654,
        "learning_rate": 0.0001169463158541712,
        "epoch": 0.48304642670839854,
        "step": 6482
    },
    {
        "loss": 2.551,
        "grad_norm": 1.8009175062179565,
        "learning_rate": 0.00011687696372592514,
        "epoch": 0.4831209479096803,
        "step": 6483
    },
    {
        "loss": 1.466,
        "grad_norm": 4.844956874847412,
        "learning_rate": 0.00011680760324134182,
        "epoch": 0.48319546911096206,
        "step": 6484
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.5386598110198975,
        "learning_rate": 0.00011673823443476371,
        "epoch": 0.4832699903122438,
        "step": 6485
    },
    {
        "loss": 1.961,
        "grad_norm": 3.215715169906616,
        "learning_rate": 0.00011666885734053764,
        "epoch": 0.4833445115135256,
        "step": 6486
    },
    {
        "loss": 2.6464,
        "grad_norm": 3.880755662918091,
        "learning_rate": 0.00011659947199301458,
        "epoch": 0.48341903271480735,
        "step": 6487
    },
    {
        "loss": 2.1738,
        "grad_norm": 3.2836146354675293,
        "learning_rate": 0.00011653007842654942,
        "epoch": 0.4834935539160891,
        "step": 6488
    },
    {
        "loss": 1.6517,
        "grad_norm": 4.094588279724121,
        "learning_rate": 0.0001164606766755011,
        "epoch": 0.4835680751173709,
        "step": 6489
    },
    {
        "loss": 2.724,
        "grad_norm": 2.1719908714294434,
        "learning_rate": 0.00011639126677423275,
        "epoch": 0.48364259631865264,
        "step": 6490
    },
    {
        "loss": 1.9029,
        "grad_norm": 3.3272056579589844,
        "learning_rate": 0.00011632184875711154,
        "epoch": 0.4837171175199344,
        "step": 6491
    },
    {
        "loss": 2.7914,
        "grad_norm": 2.0669658184051514,
        "learning_rate": 0.00011625242265850864,
        "epoch": 0.48379163872121617,
        "step": 6492
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.8165037631988525,
        "learning_rate": 0.00011618298851279903,
        "epoch": 0.48386615992249793,
        "step": 6493
    },
    {
        "loss": 1.4859,
        "grad_norm": 3.637479305267334,
        "learning_rate": 0.00011611354635436198,
        "epoch": 0.4839406811237797,
        "step": 6494
    },
    {
        "loss": 2.4947,
        "grad_norm": 2.0090088844299316,
        "learning_rate": 0.00011604409621758045,
        "epoch": 0.48401520232506146,
        "step": 6495
    },
    {
        "loss": 2.6237,
        "grad_norm": 3.306654214859009,
        "learning_rate": 0.00011597463813684157,
        "epoch": 0.4840897235263432,
        "step": 6496
    },
    {
        "loss": 2.0956,
        "grad_norm": 3.1535804271698,
        "learning_rate": 0.0001159051721465363,
        "epoch": 0.484164244727625,
        "step": 6497
    },
    {
        "loss": 2.0666,
        "grad_norm": 3.446707248687744,
        "learning_rate": 0.00011583569828105954,
        "epoch": 0.4842387659289068,
        "step": 6498
    },
    {
        "loss": 2.4528,
        "grad_norm": 2.716689109802246,
        "learning_rate": 0.00011576621657480995,
        "epoch": 0.48431328713018856,
        "step": 6499
    },
    {
        "loss": 2.4813,
        "grad_norm": 5.721410274505615,
        "learning_rate": 0.0001156967270621903,
        "epoch": 0.4843878083314703,
        "step": 6500
    },
    {
        "loss": 1.541,
        "grad_norm": 3.457724094390869,
        "learning_rate": 0.00011562722977760719,
        "epoch": 0.4844623295327521,
        "step": 6501
    },
    {
        "loss": 2.8141,
        "grad_norm": 2.048844814300537,
        "learning_rate": 0.00011555772475547084,
        "epoch": 0.48453685073403385,
        "step": 6502
    },
    {
        "loss": 1.8269,
        "grad_norm": 4.420341968536377,
        "learning_rate": 0.00011548821203019553,
        "epoch": 0.4846113719353156,
        "step": 6503
    },
    {
        "loss": 2.0594,
        "grad_norm": 4.187878608703613,
        "learning_rate": 0.0001154186916361994,
        "epoch": 0.4846858931365974,
        "step": 6504
    },
    {
        "loss": 2.5576,
        "grad_norm": 2.5122852325439453,
        "learning_rate": 0.0001153491636079041,
        "epoch": 0.48476041433787914,
        "step": 6505
    },
    {
        "loss": 2.3821,
        "grad_norm": 2.356032371520996,
        "learning_rate": 0.00011527962797973537,
        "epoch": 0.4848349355391609,
        "step": 6506
    },
    {
        "loss": 2.4176,
        "grad_norm": 3.119718551635742,
        "learning_rate": 0.00011521008478612248,
        "epoch": 0.48490945674044267,
        "step": 6507
    },
    {
        "loss": 2.3416,
        "grad_norm": 2.151524543762207,
        "learning_rate": 0.00011514053406149861,
        "epoch": 0.48498397794172443,
        "step": 6508
    },
    {
        "loss": 2.868,
        "grad_norm": 2.4635226726531982,
        "learning_rate": 0.00011507097584030049,
        "epoch": 0.4850584991430062,
        "step": 6509
    },
    {
        "loss": 2.6325,
        "grad_norm": 2.0801546573638916,
        "learning_rate": 0.00011500141015696876,
        "epoch": 0.48513302034428796,
        "step": 6510
    },
    {
        "loss": 2.5016,
        "grad_norm": 3.7317817211151123,
        "learning_rate": 0.00011493183704594776,
        "epoch": 0.4852075415455697,
        "step": 6511
    },
    {
        "loss": 2.8323,
        "grad_norm": 2.9305481910705566,
        "learning_rate": 0.0001148622565416852,
        "epoch": 0.4852820627468515,
        "step": 6512
    },
    {
        "loss": 2.4768,
        "grad_norm": 2.5701537132263184,
        "learning_rate": 0.0001147926686786328,
        "epoch": 0.48535658394813325,
        "step": 6513
    },
    {
        "loss": 2.1598,
        "grad_norm": 2.545240640640259,
        "learning_rate": 0.00011472307349124585,
        "epoch": 0.485431105149415,
        "step": 6514
    },
    {
        "loss": 2.7094,
        "grad_norm": 2.684821128845215,
        "learning_rate": 0.00011465347101398313,
        "epoch": 0.4855056263506968,
        "step": 6515
    },
    {
        "loss": 1.6193,
        "grad_norm": 3.399569511413574,
        "learning_rate": 0.00011458386128130703,
        "epoch": 0.48558014755197854,
        "step": 6516
    },
    {
        "loss": 2.5389,
        "grad_norm": 3.363675117492676,
        "learning_rate": 0.00011451424432768365,
        "epoch": 0.4856546687532603,
        "step": 6517
    },
    {
        "loss": 2.613,
        "grad_norm": 3.577855110168457,
        "learning_rate": 0.00011444462018758276,
        "epoch": 0.48572918995454206,
        "step": 6518
    },
    {
        "loss": 2.1738,
        "grad_norm": 2.0960490703582764,
        "learning_rate": 0.00011437498889547736,
        "epoch": 0.4858037111558238,
        "step": 6519
    },
    {
        "loss": 1.7233,
        "grad_norm": 4.261162757873535,
        "learning_rate": 0.00011430535048584432,
        "epoch": 0.4858782323571056,
        "step": 6520
    },
    {
        "loss": 2.3342,
        "grad_norm": 3.3470141887664795,
        "learning_rate": 0.00011423570499316379,
        "epoch": 0.48595275355838735,
        "step": 6521
    },
    {
        "loss": 2.45,
        "grad_norm": 2.7269561290740967,
        "learning_rate": 0.0001141660524519196,
        "epoch": 0.4860272747596691,
        "step": 6522
    },
    {
        "loss": 1.9994,
        "grad_norm": 3.0498199462890625,
        "learning_rate": 0.00011409639289659896,
        "epoch": 0.4861017959609509,
        "step": 6523
    },
    {
        "loss": 2.369,
        "grad_norm": 3.2318766117095947,
        "learning_rate": 0.00011402672636169271,
        "epoch": 0.48617631716223264,
        "step": 6524
    },
    {
        "loss": 0.9151,
        "grad_norm": 4.442239761352539,
        "learning_rate": 0.00011395705288169496,
        "epoch": 0.4862508383635144,
        "step": 6525
    },
    {
        "loss": 2.4284,
        "grad_norm": 2.551234006881714,
        "learning_rate": 0.00011388737249110326,
        "epoch": 0.48632535956479617,
        "step": 6526
    },
    {
        "loss": 2.0433,
        "grad_norm": 2.6217734813690186,
        "learning_rate": 0.0001138176852244187,
        "epoch": 0.48639988076607793,
        "step": 6527
    },
    {
        "loss": 2.0396,
        "grad_norm": 2.56170392036438,
        "learning_rate": 0.0001137479911161458,
        "epoch": 0.4864744019673597,
        "step": 6528
    },
    {
        "loss": 2.1804,
        "grad_norm": 3.3583781719207764,
        "learning_rate": 0.00011367829020079237,
        "epoch": 0.48654892316864146,
        "step": 6529
    },
    {
        "loss": 2.7708,
        "grad_norm": 4.182982444763184,
        "learning_rate": 0.0001136085825128695,
        "epoch": 0.4866234443699232,
        "step": 6530
    },
    {
        "loss": 2.8044,
        "grad_norm": 2.499312400817871,
        "learning_rate": 0.00011353886808689182,
        "epoch": 0.486697965571205,
        "step": 6531
    },
    {
        "loss": 2.2548,
        "grad_norm": 1.9094761610031128,
        "learning_rate": 0.00011346914695737725,
        "epoch": 0.48677248677248675,
        "step": 6532
    },
    {
        "loss": 2.3438,
        "grad_norm": 2.500333309173584,
        "learning_rate": 0.00011339941915884697,
        "epoch": 0.48684700797376856,
        "step": 6533
    },
    {
        "loss": 2.7142,
        "grad_norm": 4.213569641113281,
        "learning_rate": 0.00011332968472582561,
        "epoch": 0.4869215291750503,
        "step": 6534
    },
    {
        "loss": 2.2677,
        "grad_norm": 5.258057594299316,
        "learning_rate": 0.00011325994369284088,
        "epoch": 0.4869960503763321,
        "step": 6535
    },
    {
        "loss": 2.3962,
        "grad_norm": 2.9913082122802734,
        "learning_rate": 0.00011319019609442376,
        "epoch": 0.48707057157761385,
        "step": 6536
    },
    {
        "loss": 2.8998,
        "grad_norm": 3.2602202892303467,
        "learning_rate": 0.00011312044196510866,
        "epoch": 0.4871450927788956,
        "step": 6537
    },
    {
        "loss": 2.1185,
        "grad_norm": 3.1777448654174805,
        "learning_rate": 0.00011305068133943323,
        "epoch": 0.4872196139801774,
        "step": 6538
    },
    {
        "loss": 2.2559,
        "grad_norm": 3.063534736633301,
        "learning_rate": 0.0001129809142519381,
        "epoch": 0.48729413518145914,
        "step": 6539
    },
    {
        "loss": 2.6161,
        "grad_norm": 3.686713933944702,
        "learning_rate": 0.00011291114073716724,
        "epoch": 0.4873686563827409,
        "step": 6540
    },
    {
        "loss": 1.9918,
        "grad_norm": 4.121730804443359,
        "learning_rate": 0.00011284136082966785,
        "epoch": 0.48744317758402267,
        "step": 6541
    },
    {
        "loss": 2.4266,
        "grad_norm": 2.8376169204711914,
        "learning_rate": 0.00011277157456399021,
        "epoch": 0.48751769878530443,
        "step": 6542
    },
    {
        "loss": 2.4406,
        "grad_norm": 2.870436191558838,
        "learning_rate": 0.00011270178197468783,
        "epoch": 0.4875922199865862,
        "step": 6543
    },
    {
        "loss": 2.1744,
        "grad_norm": 3.1807680130004883,
        "learning_rate": 0.00011263198309631735,
        "epoch": 0.48766674118786796,
        "step": 6544
    },
    {
        "loss": 1.5414,
        "grad_norm": 3.859348773956299,
        "learning_rate": 0.00011256217796343844,
        "epoch": 0.4877412623891497,
        "step": 6545
    },
    {
        "loss": 2.6344,
        "grad_norm": 3.774557590484619,
        "learning_rate": 0.0001124923666106138,
        "epoch": 0.4878157835904315,
        "step": 6546
    },
    {
        "loss": 2.6227,
        "grad_norm": 2.249753475189209,
        "learning_rate": 0.0001124225490724094,
        "epoch": 0.48789030479171325,
        "step": 6547
    },
    {
        "loss": 1.9164,
        "grad_norm": 2.9776999950408936,
        "learning_rate": 0.00011235272538339426,
        "epoch": 0.487964825992995,
        "step": 6548
    },
    {
        "loss": 2.7214,
        "grad_norm": 1.590602159500122,
        "learning_rate": 0.00011228289557814034,
        "epoch": 0.4880393471942768,
        "step": 6549
    },
    {
        "loss": 0.6354,
        "grad_norm": 2.847731590270996,
        "learning_rate": 0.00011221305969122254,
        "epoch": 0.48811386839555854,
        "step": 6550
    },
    {
        "loss": 2.2405,
        "grad_norm": 3.0834553241729736,
        "learning_rate": 0.00011214321775721898,
        "epoch": 0.4881883895968403,
        "step": 6551
    },
    {
        "loss": 2.9024,
        "grad_norm": 2.005782127380371,
        "learning_rate": 0.00011207336981071071,
        "epoch": 0.48826291079812206,
        "step": 6552
    },
    {
        "loss": 1.9948,
        "grad_norm": 3.104982376098633,
        "learning_rate": 0.00011200351588628178,
        "epoch": 0.4883374319994038,
        "step": 6553
    },
    {
        "loss": 2.4622,
        "grad_norm": 2.25732684135437,
        "learning_rate": 0.00011193365601851902,
        "epoch": 0.4884119532006856,
        "step": 6554
    },
    {
        "loss": 2.9676,
        "grad_norm": 2.057438611984253,
        "learning_rate": 0.00011186379024201248,
        "epoch": 0.48848647440196735,
        "step": 6555
    },
    {
        "loss": 2.7999,
        "grad_norm": 3.7285404205322266,
        "learning_rate": 0.00011179391859135488,
        "epoch": 0.4885609956032491,
        "step": 6556
    },
    {
        "loss": 1.9432,
        "grad_norm": 2.8450124263763428,
        "learning_rate": 0.00011172404110114201,
        "epoch": 0.4886355168045309,
        "step": 6557
    },
    {
        "loss": 2.3008,
        "grad_norm": 3.5435242652893066,
        "learning_rate": 0.00011165415780597253,
        "epoch": 0.48871003800581264,
        "step": 6558
    },
    {
        "loss": 2.0718,
        "grad_norm": 1.4535772800445557,
        "learning_rate": 0.00011158426874044794,
        "epoch": 0.4887845592070944,
        "step": 6559
    },
    {
        "loss": 2.3108,
        "grad_norm": 4.717660427093506,
        "learning_rate": 0.00011151437393917254,
        "epoch": 0.48885908040837617,
        "step": 6560
    },
    {
        "loss": 2.462,
        "grad_norm": 3.150636911392212,
        "learning_rate": 0.00011144447343675354,
        "epoch": 0.48893360160965793,
        "step": 6561
    },
    {
        "loss": 2.1591,
        "grad_norm": 3.0647342205047607,
        "learning_rate": 0.00011137456726780111,
        "epoch": 0.4890081228109397,
        "step": 6562
    },
    {
        "loss": 2.4905,
        "grad_norm": 1.586717128753662,
        "learning_rate": 0.00011130465546692788,
        "epoch": 0.48908264401222146,
        "step": 6563
    },
    {
        "loss": 2.5376,
        "grad_norm": 2.990786075592041,
        "learning_rate": 0.0001112347380687496,
        "epoch": 0.4891571652135032,
        "step": 6564
    },
    {
        "loss": 2.2154,
        "grad_norm": 3.524521589279175,
        "learning_rate": 0.00011116481510788471,
        "epoch": 0.489231686414785,
        "step": 6565
    },
    {
        "loss": 1.6215,
        "grad_norm": 3.1054279804229736,
        "learning_rate": 0.00011109488661895418,
        "epoch": 0.48930620761606675,
        "step": 6566
    },
    {
        "loss": 2.2865,
        "grad_norm": 3.531510829925537,
        "learning_rate": 0.00011102495263658209,
        "epoch": 0.4893807288173485,
        "step": 6567
    },
    {
        "loss": 2.6372,
        "grad_norm": 3.7045085430145264,
        "learning_rate": 0.00011095501319539487,
        "epoch": 0.48945525001863033,
        "step": 6568
    },
    {
        "loss": 1.9543,
        "grad_norm": 2.299030065536499,
        "learning_rate": 0.00011088506833002197,
        "epoch": 0.4895297712199121,
        "step": 6569
    },
    {
        "loss": 1.7576,
        "grad_norm": 3.8998348712921143,
        "learning_rate": 0.00011081511807509526,
        "epoch": 0.48960429242119385,
        "step": 6570
    },
    {
        "loss": 2.485,
        "grad_norm": 2.092250347137451,
        "learning_rate": 0.00011074516246524946,
        "epoch": 0.4896788136224756,
        "step": 6571
    },
    {
        "loss": 2.2696,
        "grad_norm": 2.473834276199341,
        "learning_rate": 0.00011067520153512199,
        "epoch": 0.4897533348237574,
        "step": 6572
    },
    {
        "loss": 1.8202,
        "grad_norm": 2.793447494506836,
        "learning_rate": 0.00011060523531935259,
        "epoch": 0.48982785602503914,
        "step": 6573
    },
    {
        "loss": 2.542,
        "grad_norm": 2.152332067489624,
        "learning_rate": 0.00011053526385258393,
        "epoch": 0.4899023772263209,
        "step": 6574
    },
    {
        "loss": 2.5115,
        "grad_norm": 3.155099868774414,
        "learning_rate": 0.00011046528716946125,
        "epoch": 0.48997689842760267,
        "step": 6575
    },
    {
        "loss": 2.6144,
        "grad_norm": 3.799717426300049,
        "learning_rate": 0.00011039530530463222,
        "epoch": 0.49005141962888443,
        "step": 6576
    },
    {
        "loss": 2.4132,
        "grad_norm": 3.3429818153381348,
        "learning_rate": 0.00011032531829274708,
        "epoch": 0.4901259408301662,
        "step": 6577
    },
    {
        "loss": 1.6776,
        "grad_norm": 2.9568960666656494,
        "learning_rate": 0.00011025532616845875,
        "epoch": 0.49020046203144796,
        "step": 6578
    },
    {
        "loss": 2.0838,
        "grad_norm": 2.97841477394104,
        "learning_rate": 0.00011018532896642274,
        "epoch": 0.4902749832327297,
        "step": 6579
    },
    {
        "loss": 1.9874,
        "grad_norm": 3.549363613128662,
        "learning_rate": 0.00011011532672129676,
        "epoch": 0.4903495044340115,
        "step": 6580
    },
    {
        "loss": 2.7988,
        "grad_norm": 2.829073905944824,
        "learning_rate": 0.00011004531946774137,
        "epoch": 0.49042402563529325,
        "step": 6581
    },
    {
        "loss": 1.7698,
        "grad_norm": 3.816091299057007,
        "learning_rate": 0.00010997530724041933,
        "epoch": 0.490498546836575,
        "step": 6582
    },
    {
        "loss": 2.4102,
        "grad_norm": 3.223541021347046,
        "learning_rate": 0.00010990529007399606,
        "epoch": 0.4905730680378568,
        "step": 6583
    },
    {
        "loss": 2.577,
        "grad_norm": 1.863917350769043,
        "learning_rate": 0.00010983526800313933,
        "epoch": 0.49064758923913854,
        "step": 6584
    },
    {
        "loss": 2.3139,
        "grad_norm": 2.732941150665283,
        "learning_rate": 0.00010976524106251942,
        "epoch": 0.4907221104404203,
        "step": 6585
    },
    {
        "loss": 2.1448,
        "grad_norm": 3.0143001079559326,
        "learning_rate": 0.00010969520928680894,
        "epoch": 0.49079663164170206,
        "step": 6586
    },
    {
        "loss": 2.6031,
        "grad_norm": 2.4138262271881104,
        "learning_rate": 0.0001096251727106828,
        "epoch": 0.4908711528429838,
        "step": 6587
    },
    {
        "loss": 2.6684,
        "grad_norm": 2.03228759765625,
        "learning_rate": 0.00010955513136881847,
        "epoch": 0.4909456740442656,
        "step": 6588
    },
    {
        "loss": 2.6841,
        "grad_norm": 4.125883102416992,
        "learning_rate": 0.00010948508529589581,
        "epoch": 0.49102019524554735,
        "step": 6589
    },
    {
        "loss": 2.5802,
        "grad_norm": 2.023775339126587,
        "learning_rate": 0.0001094150345265968,
        "epoch": 0.4910947164468291,
        "step": 6590
    },
    {
        "loss": 2.2017,
        "grad_norm": 2.1724674701690674,
        "learning_rate": 0.00010934497909560594,
        "epoch": 0.4911692376481109,
        "step": 6591
    },
    {
        "loss": 2.6828,
        "grad_norm": 2.2747604846954346,
        "learning_rate": 0.00010927491903760989,
        "epoch": 0.49124375884939264,
        "step": 6592
    },
    {
        "loss": 0.8408,
        "grad_norm": 4.0573039054870605,
        "learning_rate": 0.00010920485438729773,
        "epoch": 0.4913182800506744,
        "step": 6593
    },
    {
        "loss": 2.8431,
        "grad_norm": 2.5466082096099854,
        "learning_rate": 0.00010913478517936075,
        "epoch": 0.49139280125195617,
        "step": 6594
    },
    {
        "loss": 1.6452,
        "grad_norm": 3.8418526649475098,
        "learning_rate": 0.00010906471144849262,
        "epoch": 0.49146732245323793,
        "step": 6595
    },
    {
        "loss": 2.9029,
        "grad_norm": 3.6073102951049805,
        "learning_rate": 0.00010899463322938902,
        "epoch": 0.4915418436545197,
        "step": 6596
    },
    {
        "loss": 2.2862,
        "grad_norm": 2.942945957183838,
        "learning_rate": 0.00010892455055674791,
        "epoch": 0.49161636485580146,
        "step": 6597
    },
    {
        "loss": 2.6386,
        "grad_norm": 2.640519618988037,
        "learning_rate": 0.00010885446346526962,
        "epoch": 0.4916908860570832,
        "step": 6598
    },
    {
        "loss": 1.7693,
        "grad_norm": 2.137648105621338,
        "learning_rate": 0.00010878437198965659,
        "epoch": 0.491765407258365,
        "step": 6599
    },
    {
        "loss": 2.9715,
        "grad_norm": 1.8692129850387573,
        "learning_rate": 0.00010871427616461334,
        "epoch": 0.49183992845964675,
        "step": 6600
    },
    {
        "loss": 2.3858,
        "grad_norm": 3.760779857635498,
        "learning_rate": 0.0001086441760248466,
        "epoch": 0.4919144496609285,
        "step": 6601
    },
    {
        "loss": 2.1147,
        "grad_norm": 3.075596332550049,
        "learning_rate": 0.00010857407160506523,
        "epoch": 0.4919889708622103,
        "step": 6602
    },
    {
        "loss": 2.0442,
        "grad_norm": 2.1429967880249023,
        "learning_rate": 0.00010850396293998029,
        "epoch": 0.49206349206349204,
        "step": 6603
    },
    {
        "loss": 2.3667,
        "grad_norm": 2.592890977859497,
        "learning_rate": 0.00010843385006430482,
        "epoch": 0.49213801326477385,
        "step": 6604
    },
    {
        "loss": 2.031,
        "grad_norm": 3.786855936050415,
        "learning_rate": 0.00010836373301275408,
        "epoch": 0.4922125344660556,
        "step": 6605
    },
    {
        "loss": 2.4783,
        "grad_norm": 4.010356426239014,
        "learning_rate": 0.00010829361182004531,
        "epoch": 0.4922870556673374,
        "step": 6606
    },
    {
        "loss": 2.4197,
        "grad_norm": 3.424082040786743,
        "learning_rate": 0.00010822348652089769,
        "epoch": 0.49236157686861914,
        "step": 6607
    },
    {
        "loss": 2.9331,
        "grad_norm": 2.7472641468048096,
        "learning_rate": 0.0001081533571500326,
        "epoch": 0.4924360980699009,
        "step": 6608
    },
    {
        "loss": 2.5486,
        "grad_norm": 2.990494728088379,
        "learning_rate": 0.0001080832237421735,
        "epoch": 0.49251061927118267,
        "step": 6609
    },
    {
        "loss": 2.7241,
        "grad_norm": 1.9612131118774414,
        "learning_rate": 0.00010801308633204565,
        "epoch": 0.49258514047246443,
        "step": 6610
    },
    {
        "loss": 2.4936,
        "grad_norm": 2.5123486518859863,
        "learning_rate": 0.0001079429449543763,
        "epoch": 0.4926596616737462,
        "step": 6611
    },
    {
        "loss": 2.6492,
        "grad_norm": 2.273237943649292,
        "learning_rate": 0.00010787279964389483,
        "epoch": 0.49273418287502796,
        "step": 6612
    },
    {
        "loss": 2.9712,
        "grad_norm": 2.61203670501709,
        "learning_rate": 0.00010780265043533249,
        "epoch": 0.4928087040763097,
        "step": 6613
    },
    {
        "loss": 1.6814,
        "grad_norm": 3.283025026321411,
        "learning_rate": 0.00010773249736342245,
        "epoch": 0.4928832252775915,
        "step": 6614
    },
    {
        "loss": 2.5608,
        "grad_norm": 2.8367998600006104,
        "learning_rate": 0.00010766234046289973,
        "epoch": 0.49295774647887325,
        "step": 6615
    },
    {
        "loss": 2.759,
        "grad_norm": 2.4627740383148193,
        "learning_rate": 0.00010759217976850142,
        "epoch": 0.493032267680155,
        "step": 6616
    },
    {
        "loss": 1.9537,
        "grad_norm": 3.960453510284424,
        "learning_rate": 0.00010752201531496626,
        "epoch": 0.4931067888814368,
        "step": 6617
    },
    {
        "loss": 2.4471,
        "grad_norm": 2.4518039226531982,
        "learning_rate": 0.00010745184713703503,
        "epoch": 0.49318131008271854,
        "step": 6618
    },
    {
        "loss": 2.3361,
        "grad_norm": 2.6814019680023193,
        "learning_rate": 0.00010738167526945031,
        "epoch": 0.4932558312840003,
        "step": 6619
    },
    {
        "loss": 2.793,
        "grad_norm": 1.6509424448013306,
        "learning_rate": 0.0001073114997469565,
        "epoch": 0.49333035248528206,
        "step": 6620
    },
    {
        "loss": 1.5432,
        "grad_norm": 1.9999380111694336,
        "learning_rate": 0.00010724132060429966,
        "epoch": 0.4934048736865638,
        "step": 6621
    },
    {
        "loss": 2.4759,
        "grad_norm": 3.807922601699829,
        "learning_rate": 0.00010717113787622789,
        "epoch": 0.4934793948878456,
        "step": 6622
    },
    {
        "loss": 1.9848,
        "grad_norm": 2.789428234100342,
        "learning_rate": 0.00010710095159749098,
        "epoch": 0.49355391608912735,
        "step": 6623
    },
    {
        "loss": 2.8591,
        "grad_norm": 1.9760520458221436,
        "learning_rate": 0.00010703076180284042,
        "epoch": 0.4936284372904091,
        "step": 6624
    },
    {
        "loss": 2.4941,
        "grad_norm": 2.6601474285125732,
        "learning_rate": 0.00010696056852702944,
        "epoch": 0.4937029584916909,
        "step": 6625
    },
    {
        "loss": 1.6534,
        "grad_norm": 3.9755215644836426,
        "learning_rate": 0.00010689037180481314,
        "epoch": 0.49377747969297264,
        "step": 6626
    },
    {
        "loss": 2.4731,
        "grad_norm": 3.4115986824035645,
        "learning_rate": 0.00010682017167094807,
        "epoch": 0.4938520008942544,
        "step": 6627
    },
    {
        "loss": 1.9625,
        "grad_norm": 2.996370553970337,
        "learning_rate": 0.00010674996816019278,
        "epoch": 0.49392652209553617,
        "step": 6628
    },
    {
        "loss": 2.7809,
        "grad_norm": 3.5345375537872314,
        "learning_rate": 0.00010667976130730715,
        "epoch": 0.49400104329681793,
        "step": 6629
    },
    {
        "loss": 2.9087,
        "grad_norm": 2.3172249794006348,
        "learning_rate": 0.00010660955114705303,
        "epoch": 0.4940755644980997,
        "step": 6630
    },
    {
        "loss": 2.6438,
        "grad_norm": 2.325049638748169,
        "learning_rate": 0.00010653933771419364,
        "epoch": 0.49415008569938146,
        "step": 6631
    },
    {
        "loss": 2.4366,
        "grad_norm": 2.876232147216797,
        "learning_rate": 0.00010646912104349408,
        "epoch": 0.4942246069006632,
        "step": 6632
    },
    {
        "loss": 1.8961,
        "grad_norm": 3.1777820587158203,
        "learning_rate": 0.00010639890116972088,
        "epoch": 0.494299128101945,
        "step": 6633
    },
    {
        "loss": 2.4996,
        "grad_norm": 2.230008363723755,
        "learning_rate": 0.00010632867812764215,
        "epoch": 0.49437364930322675,
        "step": 6634
    },
    {
        "loss": 3.0459,
        "grad_norm": 2.3954315185546875,
        "learning_rate": 0.00010625845195202764,
        "epoch": 0.4944481705045085,
        "step": 6635
    },
    {
        "loss": 2.5003,
        "grad_norm": 3.093320369720459,
        "learning_rate": 0.00010618822267764875,
        "epoch": 0.4945226917057903,
        "step": 6636
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.8422722816467285,
        "learning_rate": 0.00010611799033927807,
        "epoch": 0.49459721290707204,
        "step": 6637
    },
    {
        "loss": 1.8612,
        "grad_norm": 3.624558925628662,
        "learning_rate": 0.00010604775497169013,
        "epoch": 0.4946717341083538,
        "step": 6638
    },
    {
        "loss": 2.534,
        "grad_norm": 2.761307954788208,
        "learning_rate": 0.0001059775166096606,
        "epoch": 0.4947462553096356,
        "step": 6639
    },
    {
        "loss": 2.6205,
        "grad_norm": 1.6147710084915161,
        "learning_rate": 0.00010590727528796696,
        "epoch": 0.4948207765109174,
        "step": 6640
    },
    {
        "loss": 1.248,
        "grad_norm": 3.9537172317504883,
        "learning_rate": 0.00010583703104138783,
        "epoch": 0.49489529771219914,
        "step": 6641
    },
    {
        "loss": 2.6218,
        "grad_norm": 2.4046475887298584,
        "learning_rate": 0.00010576678390470356,
        "epoch": 0.4949698189134809,
        "step": 6642
    },
    {
        "loss": 2.5929,
        "grad_norm": 3.162881851196289,
        "learning_rate": 0.0001056965339126957,
        "epoch": 0.49504434011476267,
        "step": 6643
    },
    {
        "loss": 2.3029,
        "grad_norm": 3.657374382019043,
        "learning_rate": 0.00010562628110014736,
        "epoch": 0.49511886131604443,
        "step": 6644
    },
    {
        "loss": 2.5194,
        "grad_norm": 2.2062482833862305,
        "learning_rate": 0.000105556025501843,
        "epoch": 0.4951933825173262,
        "step": 6645
    },
    {
        "loss": 2.3849,
        "grad_norm": 2.2087976932525635,
        "learning_rate": 0.0001054857671525686,
        "epoch": 0.49526790371860796,
        "step": 6646
    },
    {
        "loss": 1.7899,
        "grad_norm": 2.7618532180786133,
        "learning_rate": 0.00010541550608711124,
        "epoch": 0.4953424249198897,
        "step": 6647
    },
    {
        "loss": 2.8885,
        "grad_norm": 4.195546627044678,
        "learning_rate": 0.00010534524234025942,
        "epoch": 0.4954169461211715,
        "step": 6648
    },
    {
        "loss": 2.6831,
        "grad_norm": 2.3194305896759033,
        "learning_rate": 0.00010527497594680309,
        "epoch": 0.49549146732245325,
        "step": 6649
    },
    {
        "loss": 2.4102,
        "grad_norm": 2.6596333980560303,
        "learning_rate": 0.00010520470694153353,
        "epoch": 0.495565988523735,
        "step": 6650
    },
    {
        "loss": 2.7725,
        "grad_norm": 3.390798807144165,
        "learning_rate": 0.00010513443535924308,
        "epoch": 0.4956405097250168,
        "step": 6651
    },
    {
        "loss": 2.3114,
        "grad_norm": 2.9500720500946045,
        "learning_rate": 0.0001050641612347256,
        "epoch": 0.49571503092629854,
        "step": 6652
    },
    {
        "loss": 2.9079,
        "grad_norm": 2.2832531929016113,
        "learning_rate": 0.00010499388460277602,
        "epoch": 0.4957895521275803,
        "step": 6653
    },
    {
        "loss": 2.6175,
        "grad_norm": 3.10764741897583,
        "learning_rate": 0.00010492360549819065,
        "epoch": 0.49586407332886207,
        "step": 6654
    },
    {
        "loss": 2.7732,
        "grad_norm": 3.095092535018921,
        "learning_rate": 0.00010485332395576702,
        "epoch": 0.49593859453014383,
        "step": 6655
    },
    {
        "loss": 2.4146,
        "grad_norm": 1.5822519063949585,
        "learning_rate": 0.00010478304001030379,
        "epoch": 0.4960131157314256,
        "step": 6656
    },
    {
        "loss": 1.8633,
        "grad_norm": 1.6974486112594604,
        "learning_rate": 0.00010471275369660087,
        "epoch": 0.49608763693270735,
        "step": 6657
    },
    {
        "loss": 2.1566,
        "grad_norm": 2.867436647415161,
        "learning_rate": 0.00010464246504945922,
        "epoch": 0.4961621581339891,
        "step": 6658
    },
    {
        "loss": 2.9335,
        "grad_norm": 3.545236825942993,
        "learning_rate": 0.0001045721741036811,
        "epoch": 0.4962366793352709,
        "step": 6659
    },
    {
        "loss": 1.4324,
        "grad_norm": 3.335451364517212,
        "learning_rate": 0.00010450188089406994,
        "epoch": 0.49631120053655264,
        "step": 6660
    },
    {
        "loss": 2.2256,
        "grad_norm": 3.0291879177093506,
        "learning_rate": 0.00010443158545543012,
        "epoch": 0.4963857217378344,
        "step": 6661
    },
    {
        "loss": 2.1073,
        "grad_norm": 2.8295769691467285,
        "learning_rate": 0.00010436128782256722,
        "epoch": 0.49646024293911617,
        "step": 6662
    },
    {
        "loss": 1.7346,
        "grad_norm": 3.245849370956421,
        "learning_rate": 0.00010429098803028787,
        "epoch": 0.49653476414039793,
        "step": 6663
    },
    {
        "loss": 2.3046,
        "grad_norm": 2.4068920612335205,
        "learning_rate": 0.00010422068611339986,
        "epoch": 0.4966092853416797,
        "step": 6664
    },
    {
        "loss": 2.9013,
        "grad_norm": 3.152669906616211,
        "learning_rate": 0.00010415038210671192,
        "epoch": 0.49668380654296146,
        "step": 6665
    },
    {
        "loss": 2.1193,
        "grad_norm": 3.456256866455078,
        "learning_rate": 0.00010408007604503402,
        "epoch": 0.4967583277442432,
        "step": 6666
    },
    {
        "loss": 1.5383,
        "grad_norm": 3.620373487472534,
        "learning_rate": 0.00010400976796317686,
        "epoch": 0.496832848945525,
        "step": 6667
    },
    {
        "loss": 2.6696,
        "grad_norm": 3.0512824058532715,
        "learning_rate": 0.0001039394578959522,
        "epoch": 0.49690737014680675,
        "step": 6668
    },
    {
        "loss": 2.2802,
        "grad_norm": 3.706580638885498,
        "learning_rate": 0.00010386914587817296,
        "epoch": 0.4969818913480885,
        "step": 6669
    },
    {
        "loss": 2.6763,
        "grad_norm": 2.7160372734069824,
        "learning_rate": 0.00010379883194465296,
        "epoch": 0.4970564125493703,
        "step": 6670
    },
    {
        "loss": 2.2334,
        "grad_norm": 2.574416160583496,
        "learning_rate": 0.00010372851613020689,
        "epoch": 0.49713093375065204,
        "step": 6671
    },
    {
        "loss": 2.3887,
        "grad_norm": 2.8802084922790527,
        "learning_rate": 0.0001036581984696503,
        "epoch": 0.4972054549519338,
        "step": 6672
    },
    {
        "loss": 2.4435,
        "grad_norm": 2.1956515312194824,
        "learning_rate": 0.00010358787899779989,
        "epoch": 0.49727997615321556,
        "step": 6673
    },
    {
        "loss": 2.0237,
        "grad_norm": 3.047090768814087,
        "learning_rate": 0.00010351755774947313,
        "epoch": 0.4973544973544973,
        "step": 6674
    },
    {
        "loss": 3.3329,
        "grad_norm": 2.9897782802581787,
        "learning_rate": 0.00010344723475948839,
        "epoch": 0.49742901855577915,
        "step": 6675
    },
    {
        "loss": 2.3782,
        "grad_norm": 4.548553943634033,
        "learning_rate": 0.00010337691006266476,
        "epoch": 0.4975035397570609,
        "step": 6676
    },
    {
        "loss": 2.6448,
        "grad_norm": 2.416402578353882,
        "learning_rate": 0.00010330658369382249,
        "epoch": 0.49757806095834267,
        "step": 6677
    },
    {
        "loss": 1.4166,
        "grad_norm": 3.8435370922088623,
        "learning_rate": 0.00010323625568778228,
        "epoch": 0.49765258215962443,
        "step": 6678
    },
    {
        "loss": 2.0165,
        "grad_norm": 3.783585786819458,
        "learning_rate": 0.00010316592607936591,
        "epoch": 0.4977271033609062,
        "step": 6679
    },
    {
        "loss": 2.138,
        "grad_norm": 3.5079946517944336,
        "learning_rate": 0.00010309559490339593,
        "epoch": 0.49780162456218796,
        "step": 6680
    },
    {
        "loss": 2.2892,
        "grad_norm": 2.5391101837158203,
        "learning_rate": 0.0001030252621946956,
        "epoch": 0.4978761457634697,
        "step": 6681
    },
    {
        "loss": 2.4973,
        "grad_norm": 3.4396848678588867,
        "learning_rate": 0.00010295492798808876,
        "epoch": 0.4979506669647515,
        "step": 6682
    },
    {
        "loss": 1.9311,
        "grad_norm": 3.0460047721862793,
        "learning_rate": 0.00010288459231840035,
        "epoch": 0.49802518816603325,
        "step": 6683
    },
    {
        "loss": 2.367,
        "grad_norm": 1.5126506090164185,
        "learning_rate": 0.00010281425522045585,
        "epoch": 0.498099709367315,
        "step": 6684
    },
    {
        "loss": 2.5738,
        "grad_norm": 2.839487314224243,
        "learning_rate": 0.0001027439167290815,
        "epoch": 0.4981742305685968,
        "step": 6685
    },
    {
        "loss": 2.3199,
        "grad_norm": 2.3101227283477783,
        "learning_rate": 0.00010267357687910405,
        "epoch": 0.49824875176987854,
        "step": 6686
    },
    {
        "loss": 2.5501,
        "grad_norm": 2.7093231678009033,
        "learning_rate": 0.00010260323570535121,
        "epoch": 0.4983232729711603,
        "step": 6687
    },
    {
        "loss": 2.8398,
        "grad_norm": 1.6026194095611572,
        "learning_rate": 0.00010253289324265107,
        "epoch": 0.49839779417244207,
        "step": 6688
    },
    {
        "loss": 2.1949,
        "grad_norm": 3.1719512939453125,
        "learning_rate": 0.00010246254952583263,
        "epoch": 0.49847231537372383,
        "step": 6689
    },
    {
        "loss": 1.6312,
        "grad_norm": 2.9910738468170166,
        "learning_rate": 0.0001023922045897252,
        "epoch": 0.4985468365750056,
        "step": 6690
    },
    {
        "loss": 1.9691,
        "grad_norm": 3.3467071056365967,
        "learning_rate": 0.00010232185846915903,
        "epoch": 0.49862135777628736,
        "step": 6691
    },
    {
        "loss": 2.606,
        "grad_norm": 3.756120204925537,
        "learning_rate": 0.00010225151119896465,
        "epoch": 0.4986958789775691,
        "step": 6692
    },
    {
        "loss": 2.2865,
        "grad_norm": 5.256350517272949,
        "learning_rate": 0.00010218116281397334,
        "epoch": 0.4987704001788509,
        "step": 6693
    },
    {
        "loss": 1.7372,
        "grad_norm": 2.476384401321411,
        "learning_rate": 0.00010211081334901696,
        "epoch": 0.49884492138013264,
        "step": 6694
    },
    {
        "loss": 2.1399,
        "grad_norm": 3.066237449645996,
        "learning_rate": 0.0001020404628389277,
        "epoch": 0.4989194425814144,
        "step": 6695
    },
    {
        "loss": 2.8917,
        "grad_norm": 3.0140509605407715,
        "learning_rate": 0.00010197011131853851,
        "epoch": 0.49899396378269617,
        "step": 6696
    },
    {
        "loss": 3.1664,
        "grad_norm": 3.581188678741455,
        "learning_rate": 0.00010189975882268272,
        "epoch": 0.49906848498397793,
        "step": 6697
    },
    {
        "loss": 2.9736,
        "grad_norm": 2.0373361110687256,
        "learning_rate": 0.0001018294053861941,
        "epoch": 0.4991430061852597,
        "step": 6698
    },
    {
        "loss": 1.8538,
        "grad_norm": 2.7621939182281494,
        "learning_rate": 0.00010175905104390702,
        "epoch": 0.49921752738654146,
        "step": 6699
    },
    {
        "loss": 2.3004,
        "grad_norm": 3.977508068084717,
        "learning_rate": 0.00010168869583065612,
        "epoch": 0.4992920485878232,
        "step": 6700
    },
    {
        "loss": 2.544,
        "grad_norm": 2.9088873863220215,
        "learning_rate": 0.00010161833978127666,
        "epoch": 0.499366569789105,
        "step": 6701
    },
    {
        "loss": 1.8684,
        "grad_norm": 3.894397735595703,
        "learning_rate": 0.00010154798293060415,
        "epoch": 0.49944109099038675,
        "step": 6702
    },
    {
        "loss": 2.0801,
        "grad_norm": 2.8005456924438477,
        "learning_rate": 0.00010147762531347467,
        "epoch": 0.4995156121916685,
        "step": 6703
    },
    {
        "loss": 1.7248,
        "grad_norm": 3.5867621898651123,
        "learning_rate": 0.00010140726696472443,
        "epoch": 0.4995901333929503,
        "step": 6704
    },
    {
        "loss": 2.7028,
        "grad_norm": 3.3967738151550293,
        "learning_rate": 0.00010133690791919022,
        "epoch": 0.49966465459423204,
        "step": 6705
    },
    {
        "loss": 2.4856,
        "grad_norm": 2.1820130348205566,
        "learning_rate": 0.00010126654821170913,
        "epoch": 0.4997391757955138,
        "step": 6706
    },
    {
        "loss": 2.9864,
        "grad_norm": 2.8110499382019043,
        "learning_rate": 0.00010119618787711862,
        "epoch": 0.49981369699679556,
        "step": 6707
    },
    {
        "loss": 2.0351,
        "grad_norm": 2.764716148376465,
        "learning_rate": 0.00010112582695025636,
        "epoch": 0.4998882181980773,
        "step": 6708
    },
    {
        "loss": 2.5561,
        "grad_norm": 2.875631093978882,
        "learning_rate": 0.00010105546546596021,
        "epoch": 0.4999627393993591,
        "step": 6709
    },
    {
        "loss": 2.2552,
        "grad_norm": 3.307955026626587,
        "learning_rate": 0.00010098510345906855,
        "epoch": 0.5000372606006409,
        "step": 6710
    },
    {
        "loss": 2.212,
        "grad_norm": 2.4740660190582275,
        "learning_rate": 0.00010091474096441999,
        "epoch": 0.5001117818019226,
        "step": 6711
    },
    {
        "loss": 2.6954,
        "grad_norm": 1.4841153621673584,
        "learning_rate": 0.00010084437801685316,
        "epoch": 0.5001863030032044,
        "step": 6712
    },
    {
        "loss": 2.8991,
        "grad_norm": 2.8885862827301025,
        "learning_rate": 0.00010077401465120718,
        "epoch": 0.5002608242044861,
        "step": 6713
    },
    {
        "loss": 2.0901,
        "grad_norm": 2.498551607131958,
        "learning_rate": 0.00010070365090232114,
        "epoch": 0.500335345405768,
        "step": 6714
    },
    {
        "loss": 3.1422,
        "grad_norm": 2.9328479766845703,
        "learning_rate": 0.00010063328680503453,
        "epoch": 0.5004098666070497,
        "step": 6715
    },
    {
        "loss": 2.3844,
        "grad_norm": 3.108611822128296,
        "learning_rate": 0.00010056292239418683,
        "epoch": 0.5004843878083315,
        "step": 6716
    },
    {
        "loss": 2.7678,
        "grad_norm": 2.5427005290985107,
        "learning_rate": 0.00010049255770461792,
        "epoch": 0.5005589090096132,
        "step": 6717
    },
    {
        "loss": 2.8916,
        "grad_norm": 2.421138048171997,
        "learning_rate": 0.00010042219277116755,
        "epoch": 0.500633430210895,
        "step": 6718
    },
    {
        "loss": 2.3105,
        "grad_norm": 3.4560625553131104,
        "learning_rate": 0.00010035182762867568,
        "epoch": 0.5007079514121767,
        "step": 6719
    },
    {
        "loss": 1.9743,
        "grad_norm": 3.3247761726379395,
        "learning_rate": 0.00010028146231198238,
        "epoch": 0.5007824726134585,
        "step": 6720
    },
    {
        "loss": 2.8126,
        "grad_norm": 2.9761812686920166,
        "learning_rate": 0.00010021109685592797,
        "epoch": 0.5008569938147402,
        "step": 6721
    },
    {
        "loss": 2.7226,
        "grad_norm": 2.1821961402893066,
        "learning_rate": 0.0001001407312953526,
        "epoch": 0.5009315150160221,
        "step": 6722
    },
    {
        "loss": 2.3839,
        "grad_norm": 2.934645175933838,
        "learning_rate": 0.00010007036566509649,
        "epoch": 0.5010060362173038,
        "step": 6723
    },
    {
        "loss": 2.8408,
        "grad_norm": 3.50754976272583,
        "learning_rate": 0.0001,
        "epoch": 0.5010805574185856,
        "step": 6724
    },
    {
        "loss": 2.7398,
        "grad_norm": 2.643770694732666,
        "learning_rate": 9.992963433490361e-05,
        "epoch": 0.5011550786198673,
        "step": 6725
    },
    {
        "loss": 2.6253,
        "grad_norm": 4.273741245269775,
        "learning_rate": 9.985926870464741e-05,
        "epoch": 0.5012295998211491,
        "step": 6726
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.5523102283477783,
        "learning_rate": 9.978890314407205e-05,
        "epoch": 0.5013041210224308,
        "step": 6727
    },
    {
        "loss": 2.4769,
        "grad_norm": 3.4789233207702637,
        "learning_rate": 9.971853768801756e-05,
        "epoch": 0.5013786422237126,
        "step": 6728
    },
    {
        "loss": 1.9922,
        "grad_norm": 2.6497044563293457,
        "learning_rate": 9.964817237132436e-05,
        "epoch": 0.5014531634249945,
        "step": 6729
    },
    {
        "loss": 2.6284,
        "grad_norm": 3.249711751937866,
        "learning_rate": 9.957780722883254e-05,
        "epoch": 0.5015276846262762,
        "step": 6730
    },
    {
        "loss": 2.5082,
        "grad_norm": 3.9180517196655273,
        "learning_rate": 9.95074422953821e-05,
        "epoch": 0.501602205827558,
        "step": 6731
    },
    {
        "loss": 1.6064,
        "grad_norm": 2.7976837158203125,
        "learning_rate": 9.943707760581319e-05,
        "epoch": 0.5016767270288397,
        "step": 6732
    },
    {
        "loss": 2.7847,
        "grad_norm": 2.593703269958496,
        "learning_rate": 9.936671319496544e-05,
        "epoch": 0.5017512482301215,
        "step": 6733
    },
    {
        "loss": 2.4913,
        "grad_norm": 3.5026488304138184,
        "learning_rate": 9.929634909767888e-05,
        "epoch": 0.5018257694314032,
        "step": 6734
    },
    {
        "loss": 2.3526,
        "grad_norm": 2.4213666915893555,
        "learning_rate": 9.92259853487929e-05,
        "epoch": 0.501900290632685,
        "step": 6735
    },
    {
        "loss": 2.0108,
        "grad_norm": 2.3293371200561523,
        "learning_rate": 9.91556219831468e-05,
        "epoch": 0.5019748118339667,
        "step": 6736
    },
    {
        "loss": 2.2481,
        "grad_norm": 2.0997469425201416,
        "learning_rate": 9.908525903558002e-05,
        "epoch": 0.5020493330352486,
        "step": 6737
    },
    {
        "loss": 2.5784,
        "grad_norm": 2.251110553741455,
        "learning_rate": 9.90148965409314e-05,
        "epoch": 0.5021238542365303,
        "step": 6738
    },
    {
        "loss": 2.8453,
        "grad_norm": 1.9935039281845093,
        "learning_rate": 9.894453453403981e-05,
        "epoch": 0.5021983754378121,
        "step": 6739
    },
    {
        "loss": 2.9618,
        "grad_norm": 2.757084369659424,
        "learning_rate": 9.887417304974374e-05,
        "epoch": 0.5022728966390938,
        "step": 6740
    },
    {
        "loss": 2.413,
        "grad_norm": 3.2922451496124268,
        "learning_rate": 9.88038121228814e-05,
        "epoch": 0.5023474178403756,
        "step": 6741
    },
    {
        "loss": 2.3175,
        "grad_norm": 1.8986986875534058,
        "learning_rate": 9.873345178829089e-05,
        "epoch": 0.5024219390416573,
        "step": 6742
    },
    {
        "loss": 2.7447,
        "grad_norm": 2.246455192565918,
        "learning_rate": 9.866309208080987e-05,
        "epoch": 0.5024964602429391,
        "step": 6743
    },
    {
        "loss": 3.0387,
        "grad_norm": 2.5281689167022705,
        "learning_rate": 9.859273303527561e-05,
        "epoch": 0.5025709814442209,
        "step": 6744
    },
    {
        "loss": 2.2503,
        "grad_norm": 2.3130266666412354,
        "learning_rate": 9.852237468652545e-05,
        "epoch": 0.5026455026455027,
        "step": 6745
    },
    {
        "loss": 1.4586,
        "grad_norm": 3.3219830989837646,
        "learning_rate": 9.845201706939582e-05,
        "epoch": 0.5027200238467844,
        "step": 6746
    },
    {
        "loss": 2.321,
        "grad_norm": 4.320340633392334,
        "learning_rate": 9.838166021872335e-05,
        "epoch": 0.5027945450480662,
        "step": 6747
    },
    {
        "loss": 1.7627,
        "grad_norm": 3.6401078701019287,
        "learning_rate": 9.83113041693439e-05,
        "epoch": 0.5028690662493479,
        "step": 6748
    },
    {
        "loss": 2.5818,
        "grad_norm": 2.036097526550293,
        "learning_rate": 9.824094895609301e-05,
        "epoch": 0.5029435874506297,
        "step": 6749
    },
    {
        "loss": 2.5739,
        "grad_norm": 1.952681541442871,
        "learning_rate": 9.817059461380594e-05,
        "epoch": 0.5030181086519114,
        "step": 6750
    },
    {
        "loss": 2.5232,
        "grad_norm": 3.6727731227874756,
        "learning_rate": 9.810024117731729e-05,
        "epoch": 0.5030926298531933,
        "step": 6751
    },
    {
        "loss": 1.6266,
        "grad_norm": 1.9201899766921997,
        "learning_rate": 9.802988868146153e-05,
        "epoch": 0.503167151054475,
        "step": 6752
    },
    {
        "loss": 2.2672,
        "grad_norm": 2.997875690460205,
        "learning_rate": 9.795953716107239e-05,
        "epoch": 0.5032416722557568,
        "step": 6753
    },
    {
        "loss": 2.5369,
        "grad_norm": 1.9585773944854736,
        "learning_rate": 9.788918665098306e-05,
        "epoch": 0.5033161934570385,
        "step": 6754
    },
    {
        "loss": 2.8176,
        "grad_norm": 1.9559319019317627,
        "learning_rate": 9.781883718602668e-05,
        "epoch": 0.5033907146583203,
        "step": 6755
    },
    {
        "loss": 2.2062,
        "grad_norm": 3.0835018157958984,
        "learning_rate": 9.774848880103529e-05,
        "epoch": 0.503465235859602,
        "step": 6756
    },
    {
        "loss": 1.1097,
        "grad_norm": 3.4136645793914795,
        "learning_rate": 9.767814153084098e-05,
        "epoch": 0.5035397570608838,
        "step": 6757
    },
    {
        "loss": 2.917,
        "grad_norm": 5.090997219085693,
        "learning_rate": 9.760779541027481e-05,
        "epoch": 0.5036142782621655,
        "step": 6758
    },
    {
        "loss": 2.4606,
        "grad_norm": 2.39186692237854,
        "learning_rate": 9.753745047416738e-05,
        "epoch": 0.5036887994634474,
        "step": 6759
    },
    {
        "loss": 2.0869,
        "grad_norm": 2.525499105453491,
        "learning_rate": 9.746710675734895e-05,
        "epoch": 0.5037633206647291,
        "step": 6760
    },
    {
        "loss": 2.8437,
        "grad_norm": 3.7386744022369385,
        "learning_rate": 9.739676429464881e-05,
        "epoch": 0.5038378418660109,
        "step": 6761
    },
    {
        "loss": 2.7591,
        "grad_norm": 1.8616255521774292,
        "learning_rate": 9.732642312089597e-05,
        "epoch": 0.5039123630672926,
        "step": 6762
    },
    {
        "loss": 2.7523,
        "grad_norm": 3.015836000442505,
        "learning_rate": 9.725608327091858e-05,
        "epoch": 0.5039868842685744,
        "step": 6763
    },
    {
        "loss": 2.394,
        "grad_norm": 3.856323719024658,
        "learning_rate": 9.718574477954412e-05,
        "epoch": 0.5040614054698562,
        "step": 6764
    },
    {
        "loss": 2.7595,
        "grad_norm": 2.9992308616638184,
        "learning_rate": 9.711540768159965e-05,
        "epoch": 0.5041359266711379,
        "step": 6765
    },
    {
        "loss": 2.4724,
        "grad_norm": 2.007789373397827,
        "learning_rate": 9.704507201191118e-05,
        "epoch": 0.5042104478724198,
        "step": 6766
    },
    {
        "loss": 2.7041,
        "grad_norm": 2.477952241897583,
        "learning_rate": 9.697473780530444e-05,
        "epoch": 0.5042849690737015,
        "step": 6767
    },
    {
        "loss": 2.8773,
        "grad_norm": 3.8223936557769775,
        "learning_rate": 9.69044050966041e-05,
        "epoch": 0.5043594902749833,
        "step": 6768
    },
    {
        "loss": 2.6295,
        "grad_norm": 2.3221020698547363,
        "learning_rate": 9.683407392063405e-05,
        "epoch": 0.504434011476265,
        "step": 6769
    },
    {
        "loss": 2.5098,
        "grad_norm": 3.581364870071411,
        "learning_rate": 9.676374431221774e-05,
        "epoch": 0.5045085326775468,
        "step": 6770
    },
    {
        "loss": 2.5782,
        "grad_norm": 2.5167899131774902,
        "learning_rate": 9.66934163061776e-05,
        "epoch": 0.5045830538788285,
        "step": 6771
    },
    {
        "loss": 2.5552,
        "grad_norm": 2.444180488586426,
        "learning_rate": 9.662308993733525e-05,
        "epoch": 0.5046575750801103,
        "step": 6772
    },
    {
        "loss": 2.7712,
        "grad_norm": 4.454316139221191,
        "learning_rate": 9.655276524051171e-05,
        "epoch": 0.504732096281392,
        "step": 6773
    },
    {
        "loss": 1.7175,
        "grad_norm": 3.6917145252227783,
        "learning_rate": 9.648244225052682e-05,
        "epoch": 0.5048066174826739,
        "step": 6774
    },
    {
        "loss": 1.9539,
        "grad_norm": 3.0803794860839844,
        "learning_rate": 9.641212100220013e-05,
        "epoch": 0.5048811386839556,
        "step": 6775
    },
    {
        "loss": 2.1595,
        "grad_norm": 3.306976795196533,
        "learning_rate": 9.634180153034977e-05,
        "epoch": 0.5049556598852374,
        "step": 6776
    },
    {
        "loss": 2.9331,
        "grad_norm": 2.186180830001831,
        "learning_rate": 9.627148386979314e-05,
        "epoch": 0.5050301810865191,
        "step": 6777
    },
    {
        "loss": 2.6744,
        "grad_norm": 3.1016781330108643,
        "learning_rate": 9.620116805534706e-05,
        "epoch": 0.5051047022878009,
        "step": 6778
    },
    {
        "loss": 2.3848,
        "grad_norm": 3.2529044151306152,
        "learning_rate": 9.613085412182698e-05,
        "epoch": 0.5051792234890826,
        "step": 6779
    },
    {
        "loss": 3.1863,
        "grad_norm": 3.2272820472717285,
        "learning_rate": 9.606054210404782e-05,
        "epoch": 0.5052537446903644,
        "step": 6780
    },
    {
        "loss": 2.6476,
        "grad_norm": 2.731412172317505,
        "learning_rate": 9.599023203682323e-05,
        "epoch": 0.5053282658916461,
        "step": 6781
    },
    {
        "loss": 2.4194,
        "grad_norm": 1.997167706489563,
        "learning_rate": 9.591992395496599e-05,
        "epoch": 0.505402787092928,
        "step": 6782
    },
    {
        "loss": 2.6449,
        "grad_norm": 3.62335205078125,
        "learning_rate": 9.584961789328809e-05,
        "epoch": 0.5054773082942097,
        "step": 6783
    },
    {
        "loss": 2.2101,
        "grad_norm": 2.2902534008026123,
        "learning_rate": 9.57793138866001e-05,
        "epoch": 0.5055518294954915,
        "step": 6784
    },
    {
        "loss": 1.9235,
        "grad_norm": 2.0253190994262695,
        "learning_rate": 9.570901196971214e-05,
        "epoch": 0.5056263506967732,
        "step": 6785
    },
    {
        "loss": 2.3534,
        "grad_norm": 2.710918664932251,
        "learning_rate": 9.563871217743288e-05,
        "epoch": 0.505700871898055,
        "step": 6786
    },
    {
        "loss": 2.5104,
        "grad_norm": 2.201186180114746,
        "learning_rate": 9.55684145445699e-05,
        "epoch": 0.5057753930993367,
        "step": 6787
    },
    {
        "loss": 2.278,
        "grad_norm": 2.8037607669830322,
        "learning_rate": 9.549811910593009e-05,
        "epoch": 0.5058499143006185,
        "step": 6788
    },
    {
        "loss": 2.935,
        "grad_norm": 3.388307571411133,
        "learning_rate": 9.542782589631886e-05,
        "epoch": 0.5059244355019002,
        "step": 6789
    },
    {
        "loss": 1.8032,
        "grad_norm": 4.428123474121094,
        "learning_rate": 9.535753495054079e-05,
        "epoch": 0.5059989567031821,
        "step": 6790
    },
    {
        "loss": 2.7521,
        "grad_norm": 2.201676368713379,
        "learning_rate": 9.528724630339922e-05,
        "epoch": 0.5060734779044638,
        "step": 6791
    },
    {
        "loss": 2.5742,
        "grad_norm": 3.1324338912963867,
        "learning_rate": 9.521695998969622e-05,
        "epoch": 0.5061479991057456,
        "step": 6792
    },
    {
        "loss": 2.4442,
        "grad_norm": 2.268193483352661,
        "learning_rate": 9.514667604423302e-05,
        "epoch": 0.5062225203070273,
        "step": 6793
    },
    {
        "loss": 2.3235,
        "grad_norm": 2.738680601119995,
        "learning_rate": 9.507639450180928e-05,
        "epoch": 0.5062970415083091,
        "step": 6794
    },
    {
        "loss": 1.7248,
        "grad_norm": 2.249863624572754,
        "learning_rate": 9.500611539722401e-05,
        "epoch": 0.5063715627095908,
        "step": 6795
    },
    {
        "loss": 2.4276,
        "grad_norm": 2.323363780975342,
        "learning_rate": 9.493583876527449e-05,
        "epoch": 0.5064460839108726,
        "step": 6796
    },
    {
        "loss": 2.4084,
        "grad_norm": 2.1556293964385986,
        "learning_rate": 9.486556464075689e-05,
        "epoch": 0.5065206051121544,
        "step": 6797
    },
    {
        "loss": 1.5278,
        "grad_norm": 4.355642318725586,
        "learning_rate": 9.479529305846651e-05,
        "epoch": 0.5065951263134362,
        "step": 6798
    },
    {
        "loss": 2.2277,
        "grad_norm": 2.749830722808838,
        "learning_rate": 9.472502405319686e-05,
        "epoch": 0.506669647514718,
        "step": 6799
    },
    {
        "loss": 1.6919,
        "grad_norm": 3.7399959564208984,
        "learning_rate": 9.46547576597406e-05,
        "epoch": 0.5067441687159997,
        "step": 6800
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.9148669242858887,
        "learning_rate": 9.458449391288885e-05,
        "epoch": 0.5068186899172815,
        "step": 6801
    },
    {
        "loss": 2.427,
        "grad_norm": 4.302377700805664,
        "learning_rate": 9.451423284743144e-05,
        "epoch": 0.5068932111185632,
        "step": 6802
    },
    {
        "loss": 2.4778,
        "grad_norm": 3.3488192558288574,
        "learning_rate": 9.4443974498157e-05,
        "epoch": 0.506967732319845,
        "step": 6803
    },
    {
        "loss": 2.7926,
        "grad_norm": 3.537588357925415,
        "learning_rate": 9.437371889985273e-05,
        "epoch": 0.5070422535211268,
        "step": 6804
    },
    {
        "loss": 2.3656,
        "grad_norm": 3.498667001724243,
        "learning_rate": 9.430346608730433e-05,
        "epoch": 0.5071167747224086,
        "step": 6805
    },
    {
        "loss": 2.8533,
        "grad_norm": 2.5326454639434814,
        "learning_rate": 9.423321609529654e-05,
        "epoch": 0.5071912959236903,
        "step": 6806
    },
    {
        "loss": 2.5152,
        "grad_norm": 2.3909692764282227,
        "learning_rate": 9.416296895861213e-05,
        "epoch": 0.5072658171249721,
        "step": 6807
    },
    {
        "loss": 2.5754,
        "grad_norm": 3.1688101291656494,
        "learning_rate": 9.409272471203305e-05,
        "epoch": 0.5073403383262538,
        "step": 6808
    },
    {
        "loss": 1.7683,
        "grad_norm": 4.1460442543029785,
        "learning_rate": 9.402248339033942e-05,
        "epoch": 0.5074148595275356,
        "step": 6809
    },
    {
        "loss": 2.7277,
        "grad_norm": 2.220196008682251,
        "learning_rate": 9.39522450283099e-05,
        "epoch": 0.5074893807288173,
        "step": 6810
    },
    {
        "loss": 2.5185,
        "grad_norm": 3.8275768756866455,
        "learning_rate": 9.388200966072195e-05,
        "epoch": 0.5075639019300991,
        "step": 6811
    },
    {
        "loss": 2.0028,
        "grad_norm": 3.0521457195281982,
        "learning_rate": 9.381177732235128e-05,
        "epoch": 0.5076384231313809,
        "step": 6812
    },
    {
        "loss": 2.1865,
        "grad_norm": 2.7593610286712646,
        "learning_rate": 9.374154804797238e-05,
        "epoch": 0.5077129443326627,
        "step": 6813
    },
    {
        "loss": 2.6307,
        "grad_norm": 3.1546072959899902,
        "learning_rate": 9.367132187235794e-05,
        "epoch": 0.5077874655339444,
        "step": 6814
    },
    {
        "loss": 2.8321,
        "grad_norm": 2.588717222213745,
        "learning_rate": 9.360109883027913e-05,
        "epoch": 0.5078619867352262,
        "step": 6815
    },
    {
        "loss": 2.3848,
        "grad_norm": 3.6803197860717773,
        "learning_rate": 9.353087895650593e-05,
        "epoch": 0.5079365079365079,
        "step": 6816
    },
    {
        "loss": 2.805,
        "grad_norm": 2.062211513519287,
        "learning_rate": 9.346066228580628e-05,
        "epoch": 0.5080110291377897,
        "step": 6817
    },
    {
        "loss": 2.8213,
        "grad_norm": 3.490361452102661,
        "learning_rate": 9.3390448852947e-05,
        "epoch": 0.5080855503390714,
        "step": 6818
    },
    {
        "loss": 1.9414,
        "grad_norm": 2.7997217178344727,
        "learning_rate": 9.332023869269288e-05,
        "epoch": 0.5081600715403533,
        "step": 6819
    },
    {
        "loss": 2.7534,
        "grad_norm": 2.4186813831329346,
        "learning_rate": 9.325003183980726e-05,
        "epoch": 0.508234592741635,
        "step": 6820
    },
    {
        "loss": 1.7888,
        "grad_norm": 3.882861375808716,
        "learning_rate": 9.317982832905195e-05,
        "epoch": 0.5083091139429168,
        "step": 6821
    },
    {
        "loss": 1.9655,
        "grad_norm": 2.186596632003784,
        "learning_rate": 9.310962819518689e-05,
        "epoch": 0.5083836351441985,
        "step": 6822
    },
    {
        "loss": 2.315,
        "grad_norm": 2.5317370891571045,
        "learning_rate": 9.303943147297058e-05,
        "epoch": 0.5084581563454803,
        "step": 6823
    },
    {
        "loss": 2.2636,
        "grad_norm": 3.2207651138305664,
        "learning_rate": 9.29692381971597e-05,
        "epoch": 0.508532677546762,
        "step": 6824
    },
    {
        "loss": 2.6655,
        "grad_norm": 2.8861560821533203,
        "learning_rate": 9.289904840250903e-05,
        "epoch": 0.5086071987480438,
        "step": 6825
    },
    {
        "loss": 1.6384,
        "grad_norm": 3.6182758808135986,
        "learning_rate": 9.282886212377215e-05,
        "epoch": 0.5086817199493255,
        "step": 6826
    },
    {
        "loss": 2.4295,
        "grad_norm": 2.8710668087005615,
        "learning_rate": 9.275867939570031e-05,
        "epoch": 0.5087562411506074,
        "step": 6827
    },
    {
        "loss": 3.1131,
        "grad_norm": 2.625800609588623,
        "learning_rate": 9.268850025304354e-05,
        "epoch": 0.5088307623518891,
        "step": 6828
    },
    {
        "loss": 2.0456,
        "grad_norm": 2.566115617752075,
        "learning_rate": 9.261832473054973e-05,
        "epoch": 0.5089052835531709,
        "step": 6829
    },
    {
        "loss": 2.3232,
        "grad_norm": 3.781348466873169,
        "learning_rate": 9.254815286296494e-05,
        "epoch": 0.5089798047544526,
        "step": 6830
    },
    {
        "loss": 2.64,
        "grad_norm": 2.1407887935638428,
        "learning_rate": 9.247798468503375e-05,
        "epoch": 0.5090543259557344,
        "step": 6831
    },
    {
        "loss": 2.9465,
        "grad_norm": 2.078409194946289,
        "learning_rate": 9.24078202314986e-05,
        "epoch": 0.5091288471570161,
        "step": 6832
    },
    {
        "loss": 2.221,
        "grad_norm": 3.6244826316833496,
        "learning_rate": 9.233765953710029e-05,
        "epoch": 0.5092033683582979,
        "step": 6833
    },
    {
        "loss": 1.9882,
        "grad_norm": 1.7404552698135376,
        "learning_rate": 9.226750263657763e-05,
        "epoch": 0.5092778895595798,
        "step": 6834
    },
    {
        "loss": 1.8297,
        "grad_norm": 3.0489888191223145,
        "learning_rate": 9.219734956466748e-05,
        "epoch": 0.5093524107608615,
        "step": 6835
    },
    {
        "loss": 2.2259,
        "grad_norm": 3.1084210872650146,
        "learning_rate": 9.212720035610521e-05,
        "epoch": 0.5094269319621433,
        "step": 6836
    },
    {
        "loss": 1.6301,
        "grad_norm": 3.551264762878418,
        "learning_rate": 9.205705504562378e-05,
        "epoch": 0.509501453163425,
        "step": 6837
    },
    {
        "loss": 1.8005,
        "grad_norm": 3.026597738265991,
        "learning_rate": 9.198691366795437e-05,
        "epoch": 0.5095759743647068,
        "step": 6838
    },
    {
        "loss": 2.3525,
        "grad_norm": 2.461379051208496,
        "learning_rate": 9.191677625782651e-05,
        "epoch": 0.5096504955659885,
        "step": 6839
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.478444814682007,
        "learning_rate": 9.184664284996735e-05,
        "epoch": 0.5097250167672703,
        "step": 6840
    },
    {
        "loss": 2.5319,
        "grad_norm": 1.5236068964004517,
        "learning_rate": 9.177651347910235e-05,
        "epoch": 0.509799537968552,
        "step": 6841
    },
    {
        "loss": 2.5913,
        "grad_norm": 4.069669723510742,
        "learning_rate": 9.170638817995477e-05,
        "epoch": 0.5098740591698339,
        "step": 6842
    },
    {
        "loss": 2.919,
        "grad_norm": 2.3870511054992676,
        "learning_rate": 9.163626698724594e-05,
        "epoch": 0.5099485803711156,
        "step": 6843
    },
    {
        "loss": 2.1879,
        "grad_norm": 3.985607385635376,
        "learning_rate": 9.156614993569522e-05,
        "epoch": 0.5100231015723974,
        "step": 6844
    },
    {
        "loss": 2.0722,
        "grad_norm": 2.775787591934204,
        "learning_rate": 9.149603706001968e-05,
        "epoch": 0.5100976227736791,
        "step": 6845
    },
    {
        "loss": 2.9673,
        "grad_norm": 1.9912580251693726,
        "learning_rate": 9.142592839493478e-05,
        "epoch": 0.5101721439749609,
        "step": 6846
    },
    {
        "loss": 2.1296,
        "grad_norm": 2.4767892360687256,
        "learning_rate": 9.135582397515351e-05,
        "epoch": 0.5102466651762426,
        "step": 6847
    },
    {
        "loss": 2.3073,
        "grad_norm": 3.9278838634490967,
        "learning_rate": 9.128572383538667e-05,
        "epoch": 0.5103211863775244,
        "step": 6848
    },
    {
        "loss": 1.7169,
        "grad_norm": 3.8083958625793457,
        "learning_rate": 9.121562801034344e-05,
        "epoch": 0.5103957075788061,
        "step": 6849
    },
    {
        "loss": 2.4721,
        "grad_norm": 6.226480484008789,
        "learning_rate": 9.114553653473034e-05,
        "epoch": 0.510470228780088,
        "step": 6850
    },
    {
        "loss": 2.2372,
        "grad_norm": 3.1673457622528076,
        "learning_rate": 9.107544944325211e-05,
        "epoch": 0.5105447499813697,
        "step": 6851
    },
    {
        "loss": 1.6955,
        "grad_norm": 2.2328124046325684,
        "learning_rate": 9.100536677061107e-05,
        "epoch": 0.5106192711826515,
        "step": 6852
    },
    {
        "loss": 2.3826,
        "grad_norm": 2.3889291286468506,
        "learning_rate": 9.09352885515074e-05,
        "epoch": 0.5106937923839332,
        "step": 6853
    },
    {
        "loss": 2.3535,
        "grad_norm": 2.4933197498321533,
        "learning_rate": 9.086521482063928e-05,
        "epoch": 0.510768313585215,
        "step": 6854
    },
    {
        "loss": 2.424,
        "grad_norm": 4.042395114898682,
        "learning_rate": 9.079514561270222e-05,
        "epoch": 0.5108428347864967,
        "step": 6855
    },
    {
        "loss": 1.9783,
        "grad_norm": 2.879462957382202,
        "learning_rate": 9.072508096239014e-05,
        "epoch": 0.5109173559877785,
        "step": 6856
    },
    {
        "loss": 2.154,
        "grad_norm": 2.812985420227051,
        "learning_rate": 9.065502090439413e-05,
        "epoch": 0.5109918771890603,
        "step": 6857
    },
    {
        "loss": 2.7719,
        "grad_norm": 1.6436495780944824,
        "learning_rate": 9.058496547340316e-05,
        "epoch": 0.5110663983903421,
        "step": 6858
    },
    {
        "loss": 2.5799,
        "grad_norm": 1.9675010442733765,
        "learning_rate": 9.051491470410422e-05,
        "epoch": 0.5111409195916238,
        "step": 6859
    },
    {
        "loss": 2.5586,
        "grad_norm": 3.080679416656494,
        "learning_rate": 9.044486863118147e-05,
        "epoch": 0.5112154407929056,
        "step": 6860
    },
    {
        "loss": 1.7682,
        "grad_norm": 3.311163902282715,
        "learning_rate": 9.037482728931724e-05,
        "epoch": 0.5112899619941873,
        "step": 6861
    },
    {
        "loss": 2.3147,
        "grad_norm": 3.8646180629730225,
        "learning_rate": 9.030479071319117e-05,
        "epoch": 0.5113644831954691,
        "step": 6862
    },
    {
        "loss": 2.8204,
        "grad_norm": 2.0764713287353516,
        "learning_rate": 9.02347589374806e-05,
        "epoch": 0.5114390043967508,
        "step": 6863
    },
    {
        "loss": 2.8723,
        "grad_norm": 2.0734434127807617,
        "learning_rate": 9.01647319968607e-05,
        "epoch": 0.5115135255980326,
        "step": 6864
    },
    {
        "loss": 2.5397,
        "grad_norm": 2.4364676475524902,
        "learning_rate": 9.009470992600402e-05,
        "epoch": 0.5115880467993144,
        "step": 6865
    },
    {
        "loss": 2.3308,
        "grad_norm": 4.2926836013793945,
        "learning_rate": 9.00246927595807e-05,
        "epoch": 0.5116625680005962,
        "step": 6866
    },
    {
        "loss": 2.6137,
        "grad_norm": 2.9632675647735596,
        "learning_rate": 8.995468053225872e-05,
        "epoch": 0.5117370892018779,
        "step": 6867
    },
    {
        "loss": 2.6816,
        "grad_norm": 3.240060806274414,
        "learning_rate": 8.98846732787032e-05,
        "epoch": 0.5118116104031597,
        "step": 6868
    },
    {
        "loss": 2.6078,
        "grad_norm": 1.9270302057266235,
        "learning_rate": 8.981467103357727e-05,
        "epoch": 0.5118861316044415,
        "step": 6869
    },
    {
        "loss": 2.1985,
        "grad_norm": 3.996715784072876,
        "learning_rate": 8.974467383154126e-05,
        "epoch": 0.5119606528057232,
        "step": 6870
    },
    {
        "loss": 1.8441,
        "grad_norm": 4.047506332397461,
        "learning_rate": 8.967468170725295e-05,
        "epoch": 0.512035174007005,
        "step": 6871
    },
    {
        "loss": 2.499,
        "grad_norm": 2.7825303077697754,
        "learning_rate": 8.960469469536787e-05,
        "epoch": 0.5121096952082868,
        "step": 6872
    },
    {
        "loss": 2.7983,
        "grad_norm": 3.5150156021118164,
        "learning_rate": 8.953471283053876e-05,
        "epoch": 0.5121842164095686,
        "step": 6873
    },
    {
        "loss": 2.4385,
        "grad_norm": 2.6244113445281982,
        "learning_rate": 8.946473614741609e-05,
        "epoch": 0.5122587376108503,
        "step": 6874
    },
    {
        "loss": 1.6469,
        "grad_norm": 2.5177671909332275,
        "learning_rate": 8.93947646806475e-05,
        "epoch": 0.5123332588121321,
        "step": 6875
    },
    {
        "loss": 1.4812,
        "grad_norm": 2.3967247009277344,
        "learning_rate": 8.932479846487805e-05,
        "epoch": 0.5124077800134138,
        "step": 6876
    },
    {
        "loss": 2.6792,
        "grad_norm": 2.6730294227600098,
        "learning_rate": 8.925483753475056e-05,
        "epoch": 0.5124823012146956,
        "step": 6877
    },
    {
        "loss": 1.9151,
        "grad_norm": 3.0267889499664307,
        "learning_rate": 8.918488192490469e-05,
        "epoch": 0.5125568224159773,
        "step": 6878
    },
    {
        "loss": 2.1266,
        "grad_norm": 2.348073720932007,
        "learning_rate": 8.911493166997805e-05,
        "epoch": 0.5126313436172591,
        "step": 6879
    },
    {
        "loss": 2.4264,
        "grad_norm": 1.7782536745071411,
        "learning_rate": 8.904498680460516e-05,
        "epoch": 0.5127058648185409,
        "step": 6880
    },
    {
        "loss": 2.4256,
        "grad_norm": 2.61372709274292,
        "learning_rate": 8.897504736341794e-05,
        "epoch": 0.5127803860198227,
        "step": 6881
    },
    {
        "loss": 1.3764,
        "grad_norm": 3.0728559494018555,
        "learning_rate": 8.890511338104585e-05,
        "epoch": 0.5128549072211044,
        "step": 6882
    },
    {
        "loss": 2.7205,
        "grad_norm": 3.1310768127441406,
        "learning_rate": 8.883518489211533e-05,
        "epoch": 0.5129294284223862,
        "step": 6883
    },
    {
        "loss": 1.9022,
        "grad_norm": 3.007827043533325,
        "learning_rate": 8.87652619312504e-05,
        "epoch": 0.5130039496236679,
        "step": 6884
    },
    {
        "loss": 1.6765,
        "grad_norm": 1.6487482786178589,
        "learning_rate": 8.86953445330722e-05,
        "epoch": 0.5130784708249497,
        "step": 6885
    },
    {
        "loss": 2.116,
        "grad_norm": 1.6091185808181763,
        "learning_rate": 8.862543273219892e-05,
        "epoch": 0.5131529920262314,
        "step": 6886
    },
    {
        "loss": 2.2506,
        "grad_norm": 1.7561583518981934,
        "learning_rate": 8.855552656324646e-05,
        "epoch": 0.5132275132275133,
        "step": 6887
    },
    {
        "loss": 2.668,
        "grad_norm": 2.213331460952759,
        "learning_rate": 8.848562606082742e-05,
        "epoch": 0.513302034428795,
        "step": 6888
    },
    {
        "loss": 2.4199,
        "grad_norm": 2.1498448848724365,
        "learning_rate": 8.841573125955208e-05,
        "epoch": 0.5133765556300768,
        "step": 6889
    },
    {
        "loss": 2.4903,
        "grad_norm": 3.2085280418395996,
        "learning_rate": 8.834584219402748e-05,
        "epoch": 0.5134510768313585,
        "step": 6890
    },
    {
        "loss": 2.9466,
        "grad_norm": 4.651569843292236,
        "learning_rate": 8.827595889885796e-05,
        "epoch": 0.5135255980326403,
        "step": 6891
    },
    {
        "loss": 2.5295,
        "grad_norm": 4.31752872467041,
        "learning_rate": 8.820608140864514e-05,
        "epoch": 0.513600119233922,
        "step": 6892
    },
    {
        "loss": 2.4263,
        "grad_norm": 4.0468645095825195,
        "learning_rate": 8.813620975798753e-05,
        "epoch": 0.5136746404352038,
        "step": 6893
    },
    {
        "loss": 1.394,
        "grad_norm": 3.0989954471588135,
        "learning_rate": 8.806634398148099e-05,
        "epoch": 0.5137491616364855,
        "step": 6894
    },
    {
        "loss": 2.9092,
        "grad_norm": 2.814443826675415,
        "learning_rate": 8.79964841137183e-05,
        "epoch": 0.5138236828377674,
        "step": 6895
    },
    {
        "loss": 2.4988,
        "grad_norm": 3.2027480602264404,
        "learning_rate": 8.792663018928923e-05,
        "epoch": 0.5138982040390491,
        "step": 6896
    },
    {
        "loss": 1.9496,
        "grad_norm": 3.495551109313965,
        "learning_rate": 8.785678224278104e-05,
        "epoch": 0.5139727252403309,
        "step": 6897
    },
    {
        "loss": 1.991,
        "grad_norm": 3.7347893714904785,
        "learning_rate": 8.778694030877755e-05,
        "epoch": 0.5140472464416126,
        "step": 6898
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.270404815673828,
        "learning_rate": 8.771710442185968e-05,
        "epoch": 0.5141217676428944,
        "step": 6899
    },
    {
        "loss": 2.4233,
        "grad_norm": 3.741368055343628,
        "learning_rate": 8.764727461660575e-05,
        "epoch": 0.5141962888441761,
        "step": 6900
    },
    {
        "loss": 2.5215,
        "grad_norm": 2.466096878051758,
        "learning_rate": 8.757745092759056e-05,
        "epoch": 0.5142708100454579,
        "step": 6901
    },
    {
        "loss": 2.1281,
        "grad_norm": 3.1923420429229736,
        "learning_rate": 8.750763338938623e-05,
        "epoch": 0.5143453312467396,
        "step": 6902
    },
    {
        "loss": 2.6461,
        "grad_norm": 2.4292078018188477,
        "learning_rate": 8.743782203656166e-05,
        "epoch": 0.5144198524480215,
        "step": 6903
    },
    {
        "loss": 2.1282,
        "grad_norm": 2.302704095840454,
        "learning_rate": 8.736801690368269e-05,
        "epoch": 0.5144943736493032,
        "step": 6904
    },
    {
        "loss": 1.3616,
        "grad_norm": 3.5894739627838135,
        "learning_rate": 8.729821802531219e-05,
        "epoch": 0.514568894850585,
        "step": 6905
    },
    {
        "loss": 1.3859,
        "grad_norm": 2.6759085655212402,
        "learning_rate": 8.722842543600974e-05,
        "epoch": 0.5146434160518668,
        "step": 6906
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.2649526596069336,
        "learning_rate": 8.715863917033218e-05,
        "epoch": 0.5147179372531485,
        "step": 6907
    },
    {
        "loss": 2.204,
        "grad_norm": 3.622252941131592,
        "learning_rate": 8.708885926283286e-05,
        "epoch": 0.5147924584544303,
        "step": 6908
    },
    {
        "loss": 2.5712,
        "grad_norm": 4.215021133422852,
        "learning_rate": 8.701908574806192e-05,
        "epoch": 0.514866979655712,
        "step": 6909
    },
    {
        "loss": 1.6245,
        "grad_norm": 4.935760974884033,
        "learning_rate": 8.694931866056679e-05,
        "epoch": 0.5149415008569939,
        "step": 6910
    },
    {
        "loss": 2.5356,
        "grad_norm": 3.6924147605895996,
        "learning_rate": 8.687955803489128e-05,
        "epoch": 0.5150160220582756,
        "step": 6911
    },
    {
        "loss": 2.8972,
        "grad_norm": 2.7268664836883545,
        "learning_rate": 8.680980390557626e-05,
        "epoch": 0.5150905432595574,
        "step": 6912
    },
    {
        "loss": 2.0399,
        "grad_norm": 4.314132213592529,
        "learning_rate": 8.674005630715921e-05,
        "epoch": 0.5151650644608391,
        "step": 6913
    },
    {
        "loss": 2.2017,
        "grad_norm": 2.203831434249878,
        "learning_rate": 8.66703152741744e-05,
        "epoch": 0.5152395856621209,
        "step": 6914
    },
    {
        "loss": 2.4127,
        "grad_norm": 3.5485641956329346,
        "learning_rate": 8.660058084115304e-05,
        "epoch": 0.5153141068634026,
        "step": 6915
    },
    {
        "loss": 2.6578,
        "grad_norm": 1.8031259775161743,
        "learning_rate": 8.653085304262272e-05,
        "epoch": 0.5153886280646844,
        "step": 6916
    },
    {
        "loss": 2.8119,
        "grad_norm": 1.9620038270950317,
        "learning_rate": 8.64611319131082e-05,
        "epoch": 0.5154631492659661,
        "step": 6917
    },
    {
        "loss": 2.7751,
        "grad_norm": 2.345372438430786,
        "learning_rate": 8.63914174871306e-05,
        "epoch": 0.515537670467248,
        "step": 6918
    },
    {
        "loss": 1.9941,
        "grad_norm": 3.2108237743377686,
        "learning_rate": 8.632170979920765e-05,
        "epoch": 0.5156121916685297,
        "step": 6919
    },
    {
        "loss": 2.2412,
        "grad_norm": 2.0287957191467285,
        "learning_rate": 8.625200888385422e-05,
        "epoch": 0.5156867128698115,
        "step": 6920
    },
    {
        "loss": 2.401,
        "grad_norm": 2.5886359214782715,
        "learning_rate": 8.618231477558126e-05,
        "epoch": 0.5157612340710932,
        "step": 6921
    },
    {
        "loss": 2.6088,
        "grad_norm": 1.447893738746643,
        "learning_rate": 8.611262750889676e-05,
        "epoch": 0.515835755272375,
        "step": 6922
    },
    {
        "loss": 2.5916,
        "grad_norm": 2.6321969032287598,
        "learning_rate": 8.604294711830514e-05,
        "epoch": 0.5159102764736567,
        "step": 6923
    },
    {
        "loss": 2.0095,
        "grad_norm": 2.845494270324707,
        "learning_rate": 8.597327363830731e-05,
        "epoch": 0.5159847976749385,
        "step": 6924
    },
    {
        "loss": 1.3942,
        "grad_norm": 6.213014125823975,
        "learning_rate": 8.590360710340106e-05,
        "epoch": 0.5160593188762203,
        "step": 6925
    },
    {
        "loss": 2.3956,
        "grad_norm": 2.299835681915283,
        "learning_rate": 8.583394754808037e-05,
        "epoch": 0.5161338400775021,
        "step": 6926
    },
    {
        "loss": 2.8631,
        "grad_norm": 2.1116650104522705,
        "learning_rate": 8.576429500683623e-05,
        "epoch": 0.5162083612787838,
        "step": 6927
    },
    {
        "loss": 1.4799,
        "grad_norm": 5.84000825881958,
        "learning_rate": 8.569464951415577e-05,
        "epoch": 0.5162828824800656,
        "step": 6928
    },
    {
        "loss": 2.2037,
        "grad_norm": 2.934558868408203,
        "learning_rate": 8.56250111045226e-05,
        "epoch": 0.5163574036813473,
        "step": 6929
    },
    {
        "loss": 2.7164,
        "grad_norm": 3.676913022994995,
        "learning_rate": 8.555537981241725e-05,
        "epoch": 0.5164319248826291,
        "step": 6930
    },
    {
        "loss": 2.6739,
        "grad_norm": 1.150113582611084,
        "learning_rate": 8.548575567231637e-05,
        "epoch": 0.5165064460839108,
        "step": 6931
    },
    {
        "loss": 2.1826,
        "grad_norm": 2.7249248027801514,
        "learning_rate": 8.541613871869301e-05,
        "epoch": 0.5165809672851926,
        "step": 6932
    },
    {
        "loss": 2.7869,
        "grad_norm": 2.638301134109497,
        "learning_rate": 8.534652898601696e-05,
        "epoch": 0.5166554884864744,
        "step": 6933
    },
    {
        "loss": 2.5435,
        "grad_norm": 2.4868485927581787,
        "learning_rate": 8.527692650875417e-05,
        "epoch": 0.5167300096877562,
        "step": 6934
    },
    {
        "loss": 1.6584,
        "grad_norm": 2.940436601638794,
        "learning_rate": 8.52073313213672e-05,
        "epoch": 0.5168045308890379,
        "step": 6935
    },
    {
        "loss": 2.0813,
        "grad_norm": 3.3885812759399414,
        "learning_rate": 8.513774345831487e-05,
        "epoch": 0.5168790520903197,
        "step": 6936
    },
    {
        "loss": 2.3515,
        "grad_norm": 3.0242607593536377,
        "learning_rate": 8.506816295405225e-05,
        "epoch": 0.5169535732916014,
        "step": 6937
    },
    {
        "loss": 2.3637,
        "grad_norm": 2.7543046474456787,
        "learning_rate": 8.499858984303124e-05,
        "epoch": 0.5170280944928832,
        "step": 6938
    },
    {
        "loss": 2.3319,
        "grad_norm": 3.139969825744629,
        "learning_rate": 8.492902415969945e-05,
        "epoch": 0.5171026156941649,
        "step": 6939
    },
    {
        "loss": 1.8606,
        "grad_norm": 2.870307683944702,
        "learning_rate": 8.485946593850141e-05,
        "epoch": 0.5171771368954468,
        "step": 6940
    },
    {
        "loss": 2.5828,
        "grad_norm": 3.1243135929107666,
        "learning_rate": 8.478991521387755e-05,
        "epoch": 0.5172516580967286,
        "step": 6941
    },
    {
        "loss": 2.2066,
        "grad_norm": 2.520834445953369,
        "learning_rate": 8.472037202026465e-05,
        "epoch": 0.5173261792980103,
        "step": 6942
    },
    {
        "loss": 2.646,
        "grad_norm": 2.6541543006896973,
        "learning_rate": 8.46508363920959e-05,
        "epoch": 0.5174007004992921,
        "step": 6943
    },
    {
        "loss": 2.5708,
        "grad_norm": 2.2016754150390625,
        "learning_rate": 8.458130836380061e-05,
        "epoch": 0.5174752217005738,
        "step": 6944
    },
    {
        "loss": 1.7616,
        "grad_norm": 3.5681722164154053,
        "learning_rate": 8.451178796980448e-05,
        "epoch": 0.5175497429018556,
        "step": 6945
    },
    {
        "loss": 2.1538,
        "grad_norm": 2.5642473697662354,
        "learning_rate": 8.444227524452925e-05,
        "epoch": 0.5176242641031373,
        "step": 6946
    },
    {
        "loss": 2.4588,
        "grad_norm": 3.1357054710388184,
        "learning_rate": 8.437277022239283e-05,
        "epoch": 0.5176987853044192,
        "step": 6947
    },
    {
        "loss": 2.4525,
        "grad_norm": 2.8591432571411133,
        "learning_rate": 8.43032729378097e-05,
        "epoch": 0.5177733065057009,
        "step": 6948
    },
    {
        "loss": 2.7525,
        "grad_norm": 2.6597049236297607,
        "learning_rate": 8.423378342518999e-05,
        "epoch": 0.5178478277069827,
        "step": 6949
    },
    {
        "loss": 2.7648,
        "grad_norm": 4.843245983123779,
        "learning_rate": 8.41643017189405e-05,
        "epoch": 0.5179223489082644,
        "step": 6950
    },
    {
        "loss": 2.2387,
        "grad_norm": 2.8115768432617188,
        "learning_rate": 8.40948278534637e-05,
        "epoch": 0.5179968701095462,
        "step": 6951
    },
    {
        "loss": 2.3358,
        "grad_norm": 3.153538465499878,
        "learning_rate": 8.40253618631584e-05,
        "epoch": 0.5180713913108279,
        "step": 6952
    },
    {
        "loss": 1.9505,
        "grad_norm": 3.246025323867798,
        "learning_rate": 8.395590378241957e-05,
        "epoch": 0.5181459125121097,
        "step": 6953
    },
    {
        "loss": 2.4985,
        "grad_norm": 2.2639546394348145,
        "learning_rate": 8.388645364563804e-05,
        "epoch": 0.5182204337133914,
        "step": 6954
    },
    {
        "loss": 2.5717,
        "grad_norm": 2.590921401977539,
        "learning_rate": 8.381701148720099e-05,
        "epoch": 0.5182949549146733,
        "step": 6955
    },
    {
        "loss": 2.2657,
        "grad_norm": 3.7661287784576416,
        "learning_rate": 8.374757734149148e-05,
        "epoch": 0.518369476115955,
        "step": 6956
    },
    {
        "loss": 1.3132,
        "grad_norm": 2.5591037273406982,
        "learning_rate": 8.367815124288842e-05,
        "epoch": 0.5184439973172368,
        "step": 6957
    },
    {
        "loss": 2.8144,
        "grad_norm": 2.694488048553467,
        "learning_rate": 8.360873322576727e-05,
        "epoch": 0.5185185185185185,
        "step": 6958
    },
    {
        "loss": 2.6542,
        "grad_norm": 2.3276402950286865,
        "learning_rate": 8.3539323324499e-05,
        "epoch": 0.5185930397198003,
        "step": 6959
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.2815144062042236,
        "learning_rate": 8.34699215734506e-05,
        "epoch": 0.518667560921082,
        "step": 6960
    },
    {
        "loss": 1.9706,
        "grad_norm": 3.1858322620391846,
        "learning_rate": 8.340052800698545e-05,
        "epoch": 0.5187420821223638,
        "step": 6961
    },
    {
        "loss": 2.1727,
        "grad_norm": 2.372122287750244,
        "learning_rate": 8.333114265946232e-05,
        "epoch": 0.5188166033236455,
        "step": 6962
    },
    {
        "loss": 2.1558,
        "grad_norm": 2.3416171073913574,
        "learning_rate": 8.326176556523633e-05,
        "epoch": 0.5188911245249274,
        "step": 6963
    },
    {
        "loss": 2.1792,
        "grad_norm": 3.000267744064331,
        "learning_rate": 8.319239675865829e-05,
        "epoch": 0.5189656457262091,
        "step": 6964
    },
    {
        "loss": 2.7002,
        "grad_norm": 1.6734862327575684,
        "learning_rate": 8.312303627407487e-05,
        "epoch": 0.5190401669274909,
        "step": 6965
    },
    {
        "loss": 2.282,
        "grad_norm": 3.8250420093536377,
        "learning_rate": 8.305368414582889e-05,
        "epoch": 0.5191146881287726,
        "step": 6966
    },
    {
        "loss": 2.2713,
        "grad_norm": 3.0756447315216064,
        "learning_rate": 8.298434040825863e-05,
        "epoch": 0.5191892093300544,
        "step": 6967
    },
    {
        "loss": 2.4877,
        "grad_norm": 2.271524429321289,
        "learning_rate": 8.291500509569875e-05,
        "epoch": 0.5192637305313361,
        "step": 6968
    },
    {
        "loss": 2.3679,
        "grad_norm": 2.1087758541107178,
        "learning_rate": 8.284567824247924e-05,
        "epoch": 0.5193382517326179,
        "step": 6969
    },
    {
        "loss": 2.4411,
        "grad_norm": 3.635711193084717,
        "learning_rate": 8.277635988292601e-05,
        "epoch": 0.5194127729338996,
        "step": 6970
    },
    {
        "loss": 2.3996,
        "grad_norm": 3.348726987838745,
        "learning_rate": 8.27070500513611e-05,
        "epoch": 0.5194872941351815,
        "step": 6971
    },
    {
        "loss": 1.8437,
        "grad_norm": 2.4756155014038086,
        "learning_rate": 8.26377487821019e-05,
        "epoch": 0.5195618153364632,
        "step": 6972
    },
    {
        "loss": 2.4257,
        "grad_norm": 4.095832347869873,
        "learning_rate": 8.256845610946182e-05,
        "epoch": 0.519636336537745,
        "step": 6973
    },
    {
        "loss": 2.2081,
        "grad_norm": 4.504249095916748,
        "learning_rate": 8.24991720677499e-05,
        "epoch": 0.5197108577390267,
        "step": 6974
    },
    {
        "loss": 2.2016,
        "grad_norm": 2.6108241081237793,
        "learning_rate": 8.242989669127086e-05,
        "epoch": 0.5197853789403085,
        "step": 6975
    },
    {
        "loss": 2.3227,
        "grad_norm": 3.2778873443603516,
        "learning_rate": 8.236063001432537e-05,
        "epoch": 0.5198599001415903,
        "step": 6976
    },
    {
        "loss": 2.6627,
        "grad_norm": 2.2541682720184326,
        "learning_rate": 8.229137207120939e-05,
        "epoch": 0.519934421342872,
        "step": 6977
    },
    {
        "loss": 2.1194,
        "grad_norm": 2.727076768875122,
        "learning_rate": 8.222212289621507e-05,
        "epoch": 0.5200089425441539,
        "step": 6978
    },
    {
        "loss": 2.1934,
        "grad_norm": 4.2124433517456055,
        "learning_rate": 8.215288252362985e-05,
        "epoch": 0.5200834637454356,
        "step": 6979
    },
    {
        "loss": 2.6313,
        "grad_norm": 2.209972858428955,
        "learning_rate": 8.208365098773672e-05,
        "epoch": 0.5201579849467174,
        "step": 6980
    },
    {
        "loss": 2.692,
        "grad_norm": 2.2258408069610596,
        "learning_rate": 8.201442832281473e-05,
        "epoch": 0.5202325061479991,
        "step": 6981
    },
    {
        "loss": 2.3891,
        "grad_norm": 2.66863751411438,
        "learning_rate": 8.194521456313817e-05,
        "epoch": 0.5203070273492809,
        "step": 6982
    },
    {
        "loss": 2.2763,
        "grad_norm": 2.4733150005340576,
        "learning_rate": 8.187600974297714e-05,
        "epoch": 0.5203815485505626,
        "step": 6983
    },
    {
        "loss": 2.7858,
        "grad_norm": 2.357243061065674,
        "learning_rate": 8.18068138965971e-05,
        "epoch": 0.5204560697518444,
        "step": 6984
    },
    {
        "loss": 2.2779,
        "grad_norm": 2.5716075897216797,
        "learning_rate": 8.173762705825915e-05,
        "epoch": 0.5205305909531261,
        "step": 6985
    },
    {
        "loss": 2.5397,
        "grad_norm": 2.935256242752075,
        "learning_rate": 8.16684492622201e-05,
        "epoch": 0.520605112154408,
        "step": 6986
    },
    {
        "loss": 2.4731,
        "grad_norm": 2.0993144512176514,
        "learning_rate": 8.159928054273191e-05,
        "epoch": 0.5206796333556897,
        "step": 6987
    },
    {
        "loss": 2.082,
        "grad_norm": 2.9535677433013916,
        "learning_rate": 8.153012093404259e-05,
        "epoch": 0.5207541545569715,
        "step": 6988
    },
    {
        "loss": 2.1963,
        "grad_norm": 2.957561731338501,
        "learning_rate": 8.146097047039516e-05,
        "epoch": 0.5208286757582532,
        "step": 6989
    },
    {
        "loss": 2.4403,
        "grad_norm": 2.653817892074585,
        "learning_rate": 8.139182918602814e-05,
        "epoch": 0.520903196959535,
        "step": 6990
    },
    {
        "loss": 2.2352,
        "grad_norm": 2.6952669620513916,
        "learning_rate": 8.132269711517596e-05,
        "epoch": 0.5209777181608167,
        "step": 6991
    },
    {
        "loss": 2.0432,
        "grad_norm": 3.0040159225463867,
        "learning_rate": 8.125357429206796e-05,
        "epoch": 0.5210522393620985,
        "step": 6992
    },
    {
        "loss": 2.5373,
        "grad_norm": 2.2452750205993652,
        "learning_rate": 8.118446075092911e-05,
        "epoch": 0.5211267605633803,
        "step": 6993
    },
    {
        "loss": 2.8368,
        "grad_norm": 2.2472970485687256,
        "learning_rate": 8.111535652597991e-05,
        "epoch": 0.5212012817646621,
        "step": 6994
    },
    {
        "loss": 2.0666,
        "grad_norm": 3.4809346199035645,
        "learning_rate": 8.104626165143599e-05,
        "epoch": 0.5212758029659438,
        "step": 6995
    },
    {
        "loss": 1.8731,
        "grad_norm": 4.106901168823242,
        "learning_rate": 8.097717616150858e-05,
        "epoch": 0.5213503241672256,
        "step": 6996
    },
    {
        "loss": 2.7094,
        "grad_norm": 3.2893199920654297,
        "learning_rate": 8.090810009040412e-05,
        "epoch": 0.5214248453685073,
        "step": 6997
    },
    {
        "loss": 2.849,
        "grad_norm": 2.6978919506073,
        "learning_rate": 8.083903347232433e-05,
        "epoch": 0.5214993665697891,
        "step": 6998
    },
    {
        "loss": 2.0823,
        "grad_norm": 3.2796521186828613,
        "learning_rate": 8.076997634146663e-05,
        "epoch": 0.5215738877710708,
        "step": 6999
    },
    {
        "loss": 2.8399,
        "grad_norm": 2.3061952590942383,
        "learning_rate": 8.070092873202314e-05,
        "epoch": 0.5216484089723527,
        "step": 7000
    },
    {
        "loss": 1.4282,
        "grad_norm": 3.830524206161499,
        "learning_rate": 8.063189067818189e-05,
        "epoch": 0.5217229301736344,
        "step": 7001
    },
    {
        "loss": 1.8543,
        "grad_norm": 4.16110372543335,
        "learning_rate": 8.056286221412577e-05,
        "epoch": 0.5217974513749162,
        "step": 7002
    },
    {
        "loss": 2.7091,
        "grad_norm": 3.4395911693573,
        "learning_rate": 8.049384337403294e-05,
        "epoch": 0.5218719725761979,
        "step": 7003
    },
    {
        "loss": 1.2025,
        "grad_norm": 3.9921178817749023,
        "learning_rate": 8.042483419207702e-05,
        "epoch": 0.5219464937774797,
        "step": 7004
    },
    {
        "loss": 2.0053,
        "grad_norm": 2.6006500720977783,
        "learning_rate": 8.035583470242657e-05,
        "epoch": 0.5220210149787614,
        "step": 7005
    },
    {
        "loss": 2.4723,
        "grad_norm": 2.3046610355377197,
        "learning_rate": 8.02868449392457e-05,
        "epoch": 0.5220955361800432,
        "step": 7006
    },
    {
        "loss": 2.2929,
        "grad_norm": 2.792271375656128,
        "learning_rate": 8.021786493669335e-05,
        "epoch": 0.5221700573813249,
        "step": 7007
    },
    {
        "loss": 2.8096,
        "grad_norm": 2.79118013381958,
        "learning_rate": 8.014889472892366e-05,
        "epoch": 0.5222445785826068,
        "step": 7008
    },
    {
        "loss": 2.6921,
        "grad_norm": 3.0421762466430664,
        "learning_rate": 8.007993435008633e-05,
        "epoch": 0.5223190997838885,
        "step": 7009
    },
    {
        "loss": 2.713,
        "grad_norm": 2.156867265701294,
        "learning_rate": 8.001098383432567e-05,
        "epoch": 0.5223936209851703,
        "step": 7010
    },
    {
        "loss": 2.3448,
        "grad_norm": 2.921633720397949,
        "learning_rate": 7.994204321578153e-05,
        "epoch": 0.5224681421864521,
        "step": 7011
    },
    {
        "loss": 2.1525,
        "grad_norm": 3.3651273250579834,
        "learning_rate": 7.987311252858855e-05,
        "epoch": 0.5225426633877338,
        "step": 7012
    },
    {
        "loss": 2.6103,
        "grad_norm": 3.1641290187835693,
        "learning_rate": 7.980419180687654e-05,
        "epoch": 0.5226171845890156,
        "step": 7013
    },
    {
        "loss": 1.8946,
        "grad_norm": 2.579604148864746,
        "learning_rate": 7.973528108477053e-05,
        "epoch": 0.5226917057902973,
        "step": 7014
    },
    {
        "loss": 2.1622,
        "grad_norm": 2.7421274185180664,
        "learning_rate": 7.966638039639031e-05,
        "epoch": 0.5227662269915792,
        "step": 7015
    },
    {
        "loss": 1.7781,
        "grad_norm": 3.5422651767730713,
        "learning_rate": 7.959748977585104e-05,
        "epoch": 0.5228407481928609,
        "step": 7016
    },
    {
        "loss": 2.258,
        "grad_norm": 2.1822710037231445,
        "learning_rate": 7.952860925726263e-05,
        "epoch": 0.5229152693941427,
        "step": 7017
    },
    {
        "loss": 2.6072,
        "grad_norm": 1.7300132513046265,
        "learning_rate": 7.945973887472995e-05,
        "epoch": 0.5229897905954244,
        "step": 7018
    },
    {
        "loss": 1.7674,
        "grad_norm": 3.3380467891693115,
        "learning_rate": 7.939087866235324e-05,
        "epoch": 0.5230643117967062,
        "step": 7019
    },
    {
        "loss": 3.2539,
        "grad_norm": 2.193838357925415,
        "learning_rate": 7.93220286542272e-05,
        "epoch": 0.5231388329979879,
        "step": 7020
    },
    {
        "loss": 2.4019,
        "grad_norm": 3.1366095542907715,
        "learning_rate": 7.925318888444196e-05,
        "epoch": 0.5232133541992697,
        "step": 7021
    },
    {
        "loss": 2.829,
        "grad_norm": 4.086246490478516,
        "learning_rate": 7.918435938708227e-05,
        "epoch": 0.5232878754005514,
        "step": 7022
    },
    {
        "loss": 2.6941,
        "grad_norm": 2.6860482692718506,
        "learning_rate": 7.911554019622775e-05,
        "epoch": 0.5233623966018333,
        "step": 7023
    },
    {
        "loss": 2.1781,
        "grad_norm": 4.526634216308594,
        "learning_rate": 7.904673134595316e-05,
        "epoch": 0.523436917803115,
        "step": 7024
    },
    {
        "loss": 2.233,
        "grad_norm": 2.9310269355773926,
        "learning_rate": 7.897793287032801e-05,
        "epoch": 0.5235114390043968,
        "step": 7025
    },
    {
        "loss": 2.553,
        "grad_norm": 2.6528494358062744,
        "learning_rate": 7.890914480341653e-05,
        "epoch": 0.5235859602056785,
        "step": 7026
    },
    {
        "loss": 2.2749,
        "grad_norm": 2.2534379959106445,
        "learning_rate": 7.884036717927812e-05,
        "epoch": 0.5236604814069603,
        "step": 7027
    },
    {
        "loss": 1.5645,
        "grad_norm": 4.037700176239014,
        "learning_rate": 7.877160003196665e-05,
        "epoch": 0.523735002608242,
        "step": 7028
    },
    {
        "loss": 2.8243,
        "grad_norm": 2.161902666091919,
        "learning_rate": 7.870284339553125e-05,
        "epoch": 0.5238095238095238,
        "step": 7029
    },
    {
        "loss": 1.5119,
        "grad_norm": 3.610442876815796,
        "learning_rate": 7.863409730401541e-05,
        "epoch": 0.5238840450108055,
        "step": 7030
    },
    {
        "loss": 2.4684,
        "grad_norm": 2.272826910018921,
        "learning_rate": 7.856536179145746e-05,
        "epoch": 0.5239585662120874,
        "step": 7031
    },
    {
        "loss": 2.599,
        "grad_norm": 2.4369072914123535,
        "learning_rate": 7.849663689189086e-05,
        "epoch": 0.5240330874133691,
        "step": 7032
    },
    {
        "loss": 2.6546,
        "grad_norm": 2.908379316329956,
        "learning_rate": 7.84279226393434e-05,
        "epoch": 0.5241076086146509,
        "step": 7033
    },
    {
        "loss": 1.901,
        "grad_norm": 3.143298387527466,
        "learning_rate": 7.835921906783783e-05,
        "epoch": 0.5241821298159326,
        "step": 7034
    },
    {
        "loss": 2.5369,
        "grad_norm": 2.599531888961792,
        "learning_rate": 7.82905262113915e-05,
        "epoch": 0.5242566510172144,
        "step": 7035
    },
    {
        "loss": 1.5364,
        "grad_norm": 2.5032942295074463,
        "learning_rate": 7.822184410401646e-05,
        "epoch": 0.5243311722184961,
        "step": 7036
    },
    {
        "loss": 1.4993,
        "grad_norm": 3.65108323097229,
        "learning_rate": 7.815317277971954e-05,
        "epoch": 0.5244056934197779,
        "step": 7037
    },
    {
        "loss": 2.4219,
        "grad_norm": 2.1046159267425537,
        "learning_rate": 7.808451227250199e-05,
        "epoch": 0.5244802146210596,
        "step": 7038
    },
    {
        "loss": 2.2368,
        "grad_norm": 3.9065887928009033,
        "learning_rate": 7.801586261636014e-05,
        "epoch": 0.5245547358223415,
        "step": 7039
    },
    {
        "loss": 1.7828,
        "grad_norm": 4.182128429412842,
        "learning_rate": 7.794722384528457e-05,
        "epoch": 0.5246292570236232,
        "step": 7040
    },
    {
        "loss": 1.4115,
        "grad_norm": 2.597485065460205,
        "learning_rate": 7.787859599326042e-05,
        "epoch": 0.524703778224905,
        "step": 7041
    },
    {
        "loss": 2.2862,
        "grad_norm": 2.912947416305542,
        "learning_rate": 7.780997909426786e-05,
        "epoch": 0.5247782994261867,
        "step": 7042
    },
    {
        "loss": 1.8465,
        "grad_norm": 2.7231457233428955,
        "learning_rate": 7.774137318228117e-05,
        "epoch": 0.5248528206274685,
        "step": 7043
    },
    {
        "loss": 2.792,
        "grad_norm": 4.138571739196777,
        "learning_rate": 7.767277829126952e-05,
        "epoch": 0.5249273418287502,
        "step": 7044
    },
    {
        "loss": 2.5791,
        "grad_norm": 2.723771810531616,
        "learning_rate": 7.760419445519641e-05,
        "epoch": 0.525001863030032,
        "step": 7045
    },
    {
        "loss": 1.9029,
        "grad_norm": 3.846116781234741,
        "learning_rate": 7.753562170801987e-05,
        "epoch": 0.5250763842313139,
        "step": 7046
    },
    {
        "loss": 2.2269,
        "grad_norm": 3.496093273162842,
        "learning_rate": 7.746706008369268e-05,
        "epoch": 0.5251509054325956,
        "step": 7047
    },
    {
        "loss": 2.4688,
        "grad_norm": 3.027097463607788,
        "learning_rate": 7.739850961616172e-05,
        "epoch": 0.5252254266338774,
        "step": 7048
    },
    {
        "loss": 2.1617,
        "grad_norm": 2.795933961868286,
        "learning_rate": 7.732997033936882e-05,
        "epoch": 0.5252999478351591,
        "step": 7049
    },
    {
        "loss": 2.4951,
        "grad_norm": 2.1506969928741455,
        "learning_rate": 7.72614422872499e-05,
        "epoch": 0.5253744690364409,
        "step": 7050
    },
    {
        "loss": 2.0726,
        "grad_norm": 3.792877674102783,
        "learning_rate": 7.719292549373533e-05,
        "epoch": 0.5254489902377226,
        "step": 7051
    },
    {
        "loss": 1.8592,
        "grad_norm": 3.5027029514312744,
        "learning_rate": 7.71244199927503e-05,
        "epoch": 0.5255235114390044,
        "step": 7052
    },
    {
        "loss": 2.522,
        "grad_norm": 1.9079643487930298,
        "learning_rate": 7.705592581821392e-05,
        "epoch": 0.5255980326402862,
        "step": 7053
    },
    {
        "loss": 2.7406,
        "grad_norm": 2.657932996749878,
        "learning_rate": 7.698744300403989e-05,
        "epoch": 0.525672553841568,
        "step": 7054
    },
    {
        "loss": 1.9348,
        "grad_norm": 3.24430251121521,
        "learning_rate": 7.691897158413644e-05,
        "epoch": 0.5257470750428497,
        "step": 7055
    },
    {
        "loss": 2.6701,
        "grad_norm": 2.826524496078491,
        "learning_rate": 7.685051159240582e-05,
        "epoch": 0.5258215962441315,
        "step": 7056
    },
    {
        "loss": 2.3184,
        "grad_norm": 2.084805488586426,
        "learning_rate": 7.678206306274497e-05,
        "epoch": 0.5258961174454132,
        "step": 7057
    },
    {
        "loss": 2.8367,
        "grad_norm": 2.0097436904907227,
        "learning_rate": 7.671362602904498e-05,
        "epoch": 0.525970638646695,
        "step": 7058
    },
    {
        "loss": 2.5479,
        "grad_norm": 4.079064846038818,
        "learning_rate": 7.664520052519105e-05,
        "epoch": 0.5260451598479767,
        "step": 7059
    },
    {
        "loss": 2.6768,
        "grad_norm": 2.4593007564544678,
        "learning_rate": 7.65767865850632e-05,
        "epoch": 0.5261196810492585,
        "step": 7060
    },
    {
        "loss": 1.8898,
        "grad_norm": 4.526342391967773,
        "learning_rate": 7.650838424253516e-05,
        "epoch": 0.5261942022505403,
        "step": 7061
    },
    {
        "loss": 2.3128,
        "grad_norm": 2.4602880477905273,
        "learning_rate": 7.64399935314754e-05,
        "epoch": 0.5262687234518221,
        "step": 7062
    },
    {
        "loss": 2.7694,
        "grad_norm": 2.3071751594543457,
        "learning_rate": 7.637161448574631e-05,
        "epoch": 0.5263432446531038,
        "step": 7063
    },
    {
        "loss": 1.9395,
        "grad_norm": 3.3325841426849365,
        "learning_rate": 7.630324713920449e-05,
        "epoch": 0.5264177658543856,
        "step": 7064
    },
    {
        "loss": 2.0117,
        "grad_norm": 1.8685364723205566,
        "learning_rate": 7.623489152570099e-05,
        "epoch": 0.5264922870556673,
        "step": 7065
    },
    {
        "loss": 2.931,
        "grad_norm": 3.630877733230591,
        "learning_rate": 7.616654767908079e-05,
        "epoch": 0.5265668082569491,
        "step": 7066
    },
    {
        "loss": 2.1862,
        "grad_norm": 3.676171064376831,
        "learning_rate": 7.609821563318331e-05,
        "epoch": 0.5266413294582308,
        "step": 7067
    },
    {
        "loss": 2.0109,
        "grad_norm": 3.888697624206543,
        "learning_rate": 7.602989542184188e-05,
        "epoch": 0.5267158506595127,
        "step": 7068
    },
    {
        "loss": 2.0714,
        "grad_norm": 3.6377153396606445,
        "learning_rate": 7.596158707888399e-05,
        "epoch": 0.5267903718607944,
        "step": 7069
    },
    {
        "loss": 2.5197,
        "grad_norm": 2.9092962741851807,
        "learning_rate": 7.589329063813159e-05,
        "epoch": 0.5268648930620762,
        "step": 7070
    },
    {
        "loss": 2.0245,
        "grad_norm": 3.30082106590271,
        "learning_rate": 7.582500613340018e-05,
        "epoch": 0.5269394142633579,
        "step": 7071
    },
    {
        "loss": 1.7764,
        "grad_norm": 3.4522314071655273,
        "learning_rate": 7.575673359849994e-05,
        "epoch": 0.5270139354646397,
        "step": 7072
    },
    {
        "loss": 2.909,
        "grad_norm": 3.455683946609497,
        "learning_rate": 7.568847306723472e-05,
        "epoch": 0.5270884566659214,
        "step": 7073
    },
    {
        "loss": 2.325,
        "grad_norm": 2.43986439704895,
        "learning_rate": 7.562022457340247e-05,
        "epoch": 0.5271629778672032,
        "step": 7074
    },
    {
        "loss": 2.2187,
        "grad_norm": 3.2113161087036133,
        "learning_rate": 7.555198815079538e-05,
        "epoch": 0.5272374990684849,
        "step": 7075
    },
    {
        "loss": 1.4991,
        "grad_norm": 4.450458526611328,
        "learning_rate": 7.548376383319942e-05,
        "epoch": 0.5273120202697668,
        "step": 7076
    },
    {
        "loss": 2.6755,
        "grad_norm": 2.6383423805236816,
        "learning_rate": 7.54155516543948e-05,
        "epoch": 0.5273865414710485,
        "step": 7077
    },
    {
        "loss": 2.58,
        "grad_norm": 2.968932867050171,
        "learning_rate": 7.534735164815554e-05,
        "epoch": 0.5274610626723303,
        "step": 7078
    },
    {
        "loss": 2.2227,
        "grad_norm": 2.4197616577148438,
        "learning_rate": 7.527916384824957e-05,
        "epoch": 0.527535583873612,
        "step": 7079
    },
    {
        "loss": 2.2261,
        "grad_norm": 2.5232629776000977,
        "learning_rate": 7.521098828843916e-05,
        "epoch": 0.5276101050748938,
        "step": 7080
    },
    {
        "loss": 2.1598,
        "grad_norm": 3.029705286026001,
        "learning_rate": 7.514282500248e-05,
        "epoch": 0.5276846262761755,
        "step": 7081
    },
    {
        "loss": 2.164,
        "grad_norm": 2.9322516918182373,
        "learning_rate": 7.507467402412223e-05,
        "epoch": 0.5277591474774573,
        "step": 7082
    },
    {
        "loss": 2.0082,
        "grad_norm": 2.9603734016418457,
        "learning_rate": 7.500653538710952e-05,
        "epoch": 0.5278336686787392,
        "step": 7083
    },
    {
        "loss": 2.3207,
        "grad_norm": 4.036471843719482,
        "learning_rate": 7.493840912517942e-05,
        "epoch": 0.5279081898800209,
        "step": 7084
    },
    {
        "loss": 1.7489,
        "grad_norm": 4.810783863067627,
        "learning_rate": 7.487029527206366e-05,
        "epoch": 0.5279827110813027,
        "step": 7085
    },
    {
        "loss": 2.1861,
        "grad_norm": 3.2863757610321045,
        "learning_rate": 7.480219386148755e-05,
        "epoch": 0.5280572322825844,
        "step": 7086
    },
    {
        "loss": 2.2854,
        "grad_norm": 3.517604351043701,
        "learning_rate": 7.473410492717024e-05,
        "epoch": 0.5281317534838662,
        "step": 7087
    },
    {
        "loss": 2.5228,
        "grad_norm": 2.2882041931152344,
        "learning_rate": 7.466602850282496e-05,
        "epoch": 0.5282062746851479,
        "step": 7088
    },
    {
        "loss": 1.516,
        "grad_norm": 3.3029332160949707,
        "learning_rate": 7.459796462215839e-05,
        "epoch": 0.5282807958864297,
        "step": 7089
    },
    {
        "loss": 2.1234,
        "grad_norm": 2.571772336959839,
        "learning_rate": 7.452991331887141e-05,
        "epoch": 0.5283553170877114,
        "step": 7090
    },
    {
        "loss": 2.7142,
        "grad_norm": 3.2276675701141357,
        "learning_rate": 7.44618746266583e-05,
        "epoch": 0.5284298382889933,
        "step": 7091
    },
    {
        "loss": 1.7676,
        "grad_norm": 3.3969781398773193,
        "learning_rate": 7.439384857920716e-05,
        "epoch": 0.528504359490275,
        "step": 7092
    },
    {
        "loss": 2.075,
        "grad_norm": 3.742588520050049,
        "learning_rate": 7.43258352102002e-05,
        "epoch": 0.5285788806915568,
        "step": 7093
    },
    {
        "loss": 2.2292,
        "grad_norm": 1.774869441986084,
        "learning_rate": 7.425783455331279e-05,
        "epoch": 0.5286534018928385,
        "step": 7094
    },
    {
        "loss": 2.2507,
        "grad_norm": 2.751810073852539,
        "learning_rate": 7.418984664221447e-05,
        "epoch": 0.5287279230941203,
        "step": 7095
    },
    {
        "loss": 2.5118,
        "grad_norm": 1.7954868078231812,
        "learning_rate": 7.41218715105682e-05,
        "epoch": 0.528802444295402,
        "step": 7096
    },
    {
        "loss": 1.6639,
        "grad_norm": 3.452077627182007,
        "learning_rate": 7.405390919203061e-05,
        "epoch": 0.5288769654966838,
        "step": 7097
    },
    {
        "loss": 2.2483,
        "grad_norm": 2.5948097705841064,
        "learning_rate": 7.398595972025222e-05,
        "epoch": 0.5289514866979655,
        "step": 7098
    },
    {
        "loss": 2.6476,
        "grad_norm": 4.333328723907471,
        "learning_rate": 7.391802312887683e-05,
        "epoch": 0.5290260078992474,
        "step": 7099
    },
    {
        "loss": 1.8904,
        "grad_norm": 4.002030372619629,
        "learning_rate": 7.385009945154236e-05,
        "epoch": 0.5291005291005291,
        "step": 7100
    },
    {
        "loss": 2.8519,
        "grad_norm": 3.084839344024658,
        "learning_rate": 7.378218872187991e-05,
        "epoch": 0.5291750503018109,
        "step": 7101
    },
    {
        "loss": 2.1737,
        "grad_norm": 3.9922685623168945,
        "learning_rate": 7.371429097351414e-05,
        "epoch": 0.5292495715030926,
        "step": 7102
    },
    {
        "loss": 2.6285,
        "grad_norm": 3.258660078048706,
        "learning_rate": 7.364640624006373e-05,
        "epoch": 0.5293240927043744,
        "step": 7103
    },
    {
        "loss": 2.0044,
        "grad_norm": 3.869764804840088,
        "learning_rate": 7.357853455514043e-05,
        "epoch": 0.5293986139056561,
        "step": 7104
    },
    {
        "loss": 2.572,
        "grad_norm": 2.2718703746795654,
        "learning_rate": 7.35106759523499e-05,
        "epoch": 0.5294731351069379,
        "step": 7105
    },
    {
        "loss": 2.0702,
        "grad_norm": 3.198814868927002,
        "learning_rate": 7.344283046529105e-05,
        "epoch": 0.5295476563082196,
        "step": 7106
    },
    {
        "loss": 1.888,
        "grad_norm": 3.280017852783203,
        "learning_rate": 7.337499812755637e-05,
        "epoch": 0.5296221775095015,
        "step": 7107
    },
    {
        "loss": 2.5525,
        "grad_norm": 2.573760747909546,
        "learning_rate": 7.330717897273199e-05,
        "epoch": 0.5296966987107832,
        "step": 7108
    },
    {
        "loss": 2.2165,
        "grad_norm": 3.9569590091705322,
        "learning_rate": 7.323937303439716e-05,
        "epoch": 0.529771219912065,
        "step": 7109
    },
    {
        "loss": 2.7338,
        "grad_norm": 2.5827372074127197,
        "learning_rate": 7.317158034612513e-05,
        "epoch": 0.5298457411133467,
        "step": 7110
    },
    {
        "loss": 2.5208,
        "grad_norm": 2.574469566345215,
        "learning_rate": 7.310380094148213e-05,
        "epoch": 0.5299202623146285,
        "step": 7111
    },
    {
        "loss": 2.8322,
        "grad_norm": 2.2252259254455566,
        "learning_rate": 7.303603485402785e-05,
        "epoch": 0.5299947835159102,
        "step": 7112
    },
    {
        "loss": 1.7658,
        "grad_norm": 3.0293004512786865,
        "learning_rate": 7.296828211731572e-05,
        "epoch": 0.530069304717192,
        "step": 7113
    },
    {
        "loss": 2.6286,
        "grad_norm": 2.2316651344299316,
        "learning_rate": 7.290054276489218e-05,
        "epoch": 0.5301438259184738,
        "step": 7114
    },
    {
        "loss": 2.1223,
        "grad_norm": 4.7977423667907715,
        "learning_rate": 7.283281683029731e-05,
        "epoch": 0.5302183471197556,
        "step": 7115
    },
    {
        "loss": 1.9111,
        "grad_norm": 3.563291311264038,
        "learning_rate": 7.276510434706441e-05,
        "epoch": 0.5302928683210373,
        "step": 7116
    },
    {
        "loss": 1.9142,
        "grad_norm": 2.8087117671966553,
        "learning_rate": 7.269740534872003e-05,
        "epoch": 0.5303673895223191,
        "step": 7117
    },
    {
        "loss": 1.586,
        "grad_norm": 3.1825270652770996,
        "learning_rate": 7.262971986878433e-05,
        "epoch": 0.5304419107236009,
        "step": 7118
    },
    {
        "loss": 2.3678,
        "grad_norm": 2.685333490371704,
        "learning_rate": 7.256204794077052e-05,
        "epoch": 0.5305164319248826,
        "step": 7119
    },
    {
        "loss": 2.1316,
        "grad_norm": 2.666588068008423,
        "learning_rate": 7.249438959818508e-05,
        "epoch": 0.5305909531261644,
        "step": 7120
    },
    {
        "loss": 2.3635,
        "grad_norm": 4.067837238311768,
        "learning_rate": 7.24267448745281e-05,
        "epoch": 0.5306654743274462,
        "step": 7121
    },
    {
        "loss": 2.1968,
        "grad_norm": 3.7015914916992188,
        "learning_rate": 7.235911380329245e-05,
        "epoch": 0.530739995528728,
        "step": 7122
    },
    {
        "loss": 2.179,
        "grad_norm": 3.073201894760132,
        "learning_rate": 7.229149641796478e-05,
        "epoch": 0.5308145167300097,
        "step": 7123
    },
    {
        "loss": 2.0026,
        "grad_norm": 3.8259756565093994,
        "learning_rate": 7.222389275202448e-05,
        "epoch": 0.5308890379312915,
        "step": 7124
    },
    {
        "loss": 2.6343,
        "grad_norm": 2.243381977081299,
        "learning_rate": 7.215630283894432e-05,
        "epoch": 0.5309635591325732,
        "step": 7125
    },
    {
        "loss": 2.1777,
        "grad_norm": 3.0941004753112793,
        "learning_rate": 7.208872671219038e-05,
        "epoch": 0.531038080333855,
        "step": 7126
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.477036714553833,
        "learning_rate": 7.202116440522173e-05,
        "epoch": 0.5311126015351367,
        "step": 7127
    },
    {
        "loss": 2.6833,
        "grad_norm": 3.5605344772338867,
        "learning_rate": 7.195361595149076e-05,
        "epoch": 0.5311871227364185,
        "step": 7128
    },
    {
        "loss": 2.7657,
        "grad_norm": 2.0421743392944336,
        "learning_rate": 7.188608138444288e-05,
        "epoch": 0.5312616439377003,
        "step": 7129
    },
    {
        "loss": 2.9085,
        "grad_norm": 2.584630250930786,
        "learning_rate": 7.181856073751651e-05,
        "epoch": 0.5313361651389821,
        "step": 7130
    },
    {
        "loss": 2.2788,
        "grad_norm": 2.789510488510132,
        "learning_rate": 7.175105404414366e-05,
        "epoch": 0.5314106863402638,
        "step": 7131
    },
    {
        "loss": 1.5961,
        "grad_norm": 2.318577289581299,
        "learning_rate": 7.168356133774878e-05,
        "epoch": 0.5314852075415456,
        "step": 7132
    },
    {
        "loss": 2.3194,
        "grad_norm": 2.9146370887756348,
        "learning_rate": 7.161608265174999e-05,
        "epoch": 0.5315597287428273,
        "step": 7133
    },
    {
        "loss": 1.7826,
        "grad_norm": 2.4436886310577393,
        "learning_rate": 7.154861801955807e-05,
        "epoch": 0.5316342499441091,
        "step": 7134
    },
    {
        "loss": 2.5187,
        "grad_norm": 2.785433530807495,
        "learning_rate": 7.148116747457693e-05,
        "epoch": 0.5317087711453908,
        "step": 7135
    },
    {
        "loss": 2.3632,
        "grad_norm": 2.786363124847412,
        "learning_rate": 7.141373105020362e-05,
        "epoch": 0.5317832923466727,
        "step": 7136
    },
    {
        "loss": 2.1271,
        "grad_norm": 3.4576592445373535,
        "learning_rate": 7.134630877982805e-05,
        "epoch": 0.5318578135479544,
        "step": 7137
    },
    {
        "loss": 2.372,
        "grad_norm": 3.5066940784454346,
        "learning_rate": 7.12789006968333e-05,
        "epoch": 0.5319323347492362,
        "step": 7138
    },
    {
        "loss": 2.5132,
        "grad_norm": 3.0124728679656982,
        "learning_rate": 7.121150683459523e-05,
        "epoch": 0.5320068559505179,
        "step": 7139
    },
    {
        "loss": 1.9139,
        "grad_norm": 3.323714017868042,
        "learning_rate": 7.114412722648264e-05,
        "epoch": 0.5320813771517997,
        "step": 7140
    },
    {
        "loss": 3.2073,
        "grad_norm": 2.6497700214385986,
        "learning_rate": 7.107676190585764e-05,
        "epoch": 0.5321558983530814,
        "step": 7141
    },
    {
        "loss": 2.4199,
        "grad_norm": 2.4209775924682617,
        "learning_rate": 7.100941090607475e-05,
        "epoch": 0.5322304195543632,
        "step": 7142
    },
    {
        "loss": 2.1117,
        "grad_norm": 2.7941596508026123,
        "learning_rate": 7.094207426048193e-05,
        "epoch": 0.5323049407556449,
        "step": 7143
    },
    {
        "loss": 2.0417,
        "grad_norm": 3.023601531982422,
        "learning_rate": 7.087475200241958e-05,
        "epoch": 0.5323794619569268,
        "step": 7144
    },
    {
        "loss": 2.8407,
        "grad_norm": 4.2745161056518555,
        "learning_rate": 7.080744416522112e-05,
        "epoch": 0.5324539831582085,
        "step": 7145
    },
    {
        "loss": 2.0208,
        "grad_norm": 3.1459765434265137,
        "learning_rate": 7.074015078221303e-05,
        "epoch": 0.5325285043594903,
        "step": 7146
    },
    {
        "loss": 0.9782,
        "grad_norm": 1.4612181186676025,
        "learning_rate": 7.067287188671432e-05,
        "epoch": 0.532603025560772,
        "step": 7147
    },
    {
        "loss": 1.3478,
        "grad_norm": 4.653373718261719,
        "learning_rate": 7.060560751203697e-05,
        "epoch": 0.5326775467620538,
        "step": 7148
    },
    {
        "loss": 2.0266,
        "grad_norm": 2.109964370727539,
        "learning_rate": 7.05383576914859e-05,
        "epoch": 0.5327520679633355,
        "step": 7149
    },
    {
        "loss": 2.1258,
        "grad_norm": 1.8011996746063232,
        "learning_rate": 7.047112245835847e-05,
        "epoch": 0.5328265891646173,
        "step": 7150
    },
    {
        "loss": 2.5126,
        "grad_norm": 2.261971950531006,
        "learning_rate": 7.040390184594533e-05,
        "epoch": 0.532901110365899,
        "step": 7151
    },
    {
        "loss": 2.9484,
        "grad_norm": 2.0777153968811035,
        "learning_rate": 7.033669588752947e-05,
        "epoch": 0.5329756315671809,
        "step": 7152
    },
    {
        "loss": 2.8627,
        "grad_norm": 2.662684440612793,
        "learning_rate": 7.026950461638664e-05,
        "epoch": 0.5330501527684627,
        "step": 7153
    },
    {
        "loss": 2.8747,
        "grad_norm": 2.467054843902588,
        "learning_rate": 7.020232806578571e-05,
        "epoch": 0.5331246739697444,
        "step": 7154
    },
    {
        "loss": 2.8051,
        "grad_norm": 2.2653181552886963,
        "learning_rate": 7.013516626898775e-05,
        "epoch": 0.5331991951710262,
        "step": 7155
    },
    {
        "loss": 2.1576,
        "grad_norm": 1.8156780004501343,
        "learning_rate": 7.006801925924693e-05,
        "epoch": 0.5332737163723079,
        "step": 7156
    },
    {
        "loss": 2.5595,
        "grad_norm": 3.703991174697876,
        "learning_rate": 7.000088706980987e-05,
        "epoch": 0.5333482375735897,
        "step": 7157
    },
    {
        "loss": 1.7325,
        "grad_norm": 3.980081081390381,
        "learning_rate": 6.993376973391583e-05,
        "epoch": 0.5334227587748714,
        "step": 7158
    },
    {
        "loss": 2.0931,
        "grad_norm": 2.7523598670959473,
        "learning_rate": 6.986666728479694e-05,
        "epoch": 0.5334972799761533,
        "step": 7159
    },
    {
        "loss": 2.3513,
        "grad_norm": 3.0102341175079346,
        "learning_rate": 6.979957975567762e-05,
        "epoch": 0.533571801177435,
        "step": 7160
    },
    {
        "loss": 2.1798,
        "grad_norm": 2.747546434402466,
        "learning_rate": 6.973250717977538e-05,
        "epoch": 0.5336463223787168,
        "step": 7161
    },
    {
        "loss": 1.3911,
        "grad_norm": 4.388935565948486,
        "learning_rate": 6.96654495902999e-05,
        "epoch": 0.5337208435799985,
        "step": 7162
    },
    {
        "loss": 2.5585,
        "grad_norm": 2.688951253890991,
        "learning_rate": 6.959840702045345e-05,
        "epoch": 0.5337953647812803,
        "step": 7163
    },
    {
        "loss": 2.1642,
        "grad_norm": 4.6915998458862305,
        "learning_rate": 6.95313795034313e-05,
        "epoch": 0.533869885982562,
        "step": 7164
    },
    {
        "loss": 2.7724,
        "grad_norm": 2.3595798015594482,
        "learning_rate": 6.946436707242074e-05,
        "epoch": 0.5339444071838438,
        "step": 7165
    },
    {
        "loss": 3.0181,
        "grad_norm": 2.870889186859131,
        "learning_rate": 6.939736976060197e-05,
        "epoch": 0.5340189283851255,
        "step": 7166
    },
    {
        "loss": 2.8316,
        "grad_norm": 3.069718837738037,
        "learning_rate": 6.933038760114746e-05,
        "epoch": 0.5340934495864074,
        "step": 7167
    },
    {
        "loss": 2.233,
        "grad_norm": 3.9933979511260986,
        "learning_rate": 6.926342062722223e-05,
        "epoch": 0.5341679707876891,
        "step": 7168
    },
    {
        "loss": 1.9187,
        "grad_norm": 2.5665905475616455,
        "learning_rate": 6.919646887198393e-05,
        "epoch": 0.5342424919889709,
        "step": 7169
    },
    {
        "loss": 2.5389,
        "grad_norm": 3.0815951824188232,
        "learning_rate": 6.91295323685824e-05,
        "epoch": 0.5343170131902526,
        "step": 7170
    },
    {
        "loss": 2.194,
        "grad_norm": 2.801398754119873,
        "learning_rate": 6.906261115016032e-05,
        "epoch": 0.5343915343915344,
        "step": 7171
    },
    {
        "loss": 2.1249,
        "grad_norm": 2.5679385662078857,
        "learning_rate": 6.899570524985248e-05,
        "epoch": 0.5344660555928161,
        "step": 7172
    },
    {
        "loss": 2.0839,
        "grad_norm": 4.669336318969727,
        "learning_rate": 6.892881470078604e-05,
        "epoch": 0.5345405767940979,
        "step": 7173
    },
    {
        "loss": 2.5967,
        "grad_norm": 2.4909563064575195,
        "learning_rate": 6.886193953608094e-05,
        "epoch": 0.5346150979953797,
        "step": 7174
    },
    {
        "loss": 2.6592,
        "grad_norm": 3.49700665473938,
        "learning_rate": 6.879507978884909e-05,
        "epoch": 0.5346896191966615,
        "step": 7175
    },
    {
        "loss": 2.1313,
        "grad_norm": 3.571488857269287,
        "learning_rate": 6.872823549219509e-05,
        "epoch": 0.5347641403979432,
        "step": 7176
    },
    {
        "loss": 2.3889,
        "grad_norm": 2.597080707550049,
        "learning_rate": 6.866140667921564e-05,
        "epoch": 0.534838661599225,
        "step": 7177
    },
    {
        "loss": 2.2978,
        "grad_norm": 2.7223060131073,
        "learning_rate": 6.85945933829998e-05,
        "epoch": 0.5349131828005067,
        "step": 7178
    },
    {
        "loss": 1.9246,
        "grad_norm": 5.863339900970459,
        "learning_rate": 6.852779563662919e-05,
        "epoch": 0.5349877040017885,
        "step": 7179
    },
    {
        "loss": 2.2054,
        "grad_norm": 1.8367366790771484,
        "learning_rate": 6.846101347317751e-05,
        "epoch": 0.5350622252030702,
        "step": 7180
    },
    {
        "loss": 2.1998,
        "grad_norm": 2.9942901134490967,
        "learning_rate": 6.839424692571059e-05,
        "epoch": 0.535136746404352,
        "step": 7181
    },
    {
        "loss": 1.9217,
        "grad_norm": 3.167382001876831,
        "learning_rate": 6.832749602728705e-05,
        "epoch": 0.5352112676056338,
        "step": 7182
    },
    {
        "loss": 2.3918,
        "grad_norm": 1.9245539903640747,
        "learning_rate": 6.826076081095716e-05,
        "epoch": 0.5352857888069156,
        "step": 7183
    },
    {
        "loss": 1.9086,
        "grad_norm": 2.894542932510376,
        "learning_rate": 6.819404130976402e-05,
        "epoch": 0.5353603100081973,
        "step": 7184
    },
    {
        "loss": 2.4732,
        "grad_norm": 3.2019994258880615,
        "learning_rate": 6.812733755674243e-05,
        "epoch": 0.5354348312094791,
        "step": 7185
    },
    {
        "loss": 2.0402,
        "grad_norm": 3.069556951522827,
        "learning_rate": 6.80606495849196e-05,
        "epoch": 0.5355093524107608,
        "step": 7186
    },
    {
        "loss": 2.4433,
        "grad_norm": 4.21213436126709,
        "learning_rate": 6.799397742731505e-05,
        "epoch": 0.5355838736120426,
        "step": 7187
    },
    {
        "loss": 2.5403,
        "grad_norm": 3.042358875274658,
        "learning_rate": 6.79273211169402e-05,
        "epoch": 0.5356583948133244,
        "step": 7188
    },
    {
        "loss": 1.9049,
        "grad_norm": 3.2736058235168457,
        "learning_rate": 6.78606806867989e-05,
        "epoch": 0.5357329160146062,
        "step": 7189
    },
    {
        "loss": 2.2686,
        "grad_norm": 4.679862976074219,
        "learning_rate": 6.779405616988695e-05,
        "epoch": 0.535807437215888,
        "step": 7190
    },
    {
        "loss": 2.618,
        "grad_norm": 2.0417938232421875,
        "learning_rate": 6.772744759919223e-05,
        "epoch": 0.5358819584171697,
        "step": 7191
    },
    {
        "loss": 3.4705,
        "grad_norm": 4.362217426300049,
        "learning_rate": 6.766085500769503e-05,
        "epoch": 0.5359564796184515,
        "step": 7192
    },
    {
        "loss": 2.601,
        "grad_norm": 2.07790207862854,
        "learning_rate": 6.759427842836729e-05,
        "epoch": 0.5360310008197332,
        "step": 7193
    },
    {
        "loss": 2.4377,
        "grad_norm": 3.008362054824829,
        "learning_rate": 6.752771789417353e-05,
        "epoch": 0.536105522021015,
        "step": 7194
    },
    {
        "loss": 2.6114,
        "grad_norm": 2.7165958881378174,
        "learning_rate": 6.74611734380699e-05,
        "epoch": 0.5361800432222967,
        "step": 7195
    },
    {
        "loss": 1.9139,
        "grad_norm": 3.339890480041504,
        "learning_rate": 6.739464509300469e-05,
        "epoch": 0.5362545644235786,
        "step": 7196
    },
    {
        "loss": 1.9808,
        "grad_norm": 3.80469012260437,
        "learning_rate": 6.732813289191835e-05,
        "epoch": 0.5363290856248603,
        "step": 7197
    },
    {
        "loss": 2.6425,
        "grad_norm": 2.0113024711608887,
        "learning_rate": 6.726163686774316e-05,
        "epoch": 0.5364036068261421,
        "step": 7198
    },
    {
        "loss": 2.2076,
        "grad_norm": 3.6794846057891846,
        "learning_rate": 6.719515705340362e-05,
        "epoch": 0.5364781280274238,
        "step": 7199
    },
    {
        "loss": 2.2005,
        "grad_norm": 2.3058865070343018,
        "learning_rate": 6.712869348181596e-05,
        "epoch": 0.5365526492287056,
        "step": 7200
    },
    {
        "loss": 1.9905,
        "grad_norm": 5.59005069732666,
        "learning_rate": 6.706224618588836e-05,
        "epoch": 0.5366271704299873,
        "step": 7201
    },
    {
        "loss": 2.3883,
        "grad_norm": 2.830244541168213,
        "learning_rate": 6.699581519852132e-05,
        "epoch": 0.5367016916312691,
        "step": 7202
    },
    {
        "loss": 2.5411,
        "grad_norm": 2.456778049468994,
        "learning_rate": 6.69294005526067e-05,
        "epoch": 0.5367762128325508,
        "step": 7203
    },
    {
        "loss": 2.5261,
        "grad_norm": 2.063555955886841,
        "learning_rate": 6.686300228102887e-05,
        "epoch": 0.5368507340338327,
        "step": 7204
    },
    {
        "loss": 2.2284,
        "grad_norm": 2.7410831451416016,
        "learning_rate": 6.679662041666362e-05,
        "epoch": 0.5369252552351144,
        "step": 7205
    },
    {
        "loss": 2.1708,
        "grad_norm": 4.479730606079102,
        "learning_rate": 6.673025499237875e-05,
        "epoch": 0.5369997764363962,
        "step": 7206
    },
    {
        "loss": 2.7339,
        "grad_norm": 3.474571466445923,
        "learning_rate": 6.666390604103409e-05,
        "epoch": 0.5370742976376779,
        "step": 7207
    },
    {
        "loss": 2.3671,
        "grad_norm": 2.039301872253418,
        "learning_rate": 6.659757359548101e-05,
        "epoch": 0.5371488188389597,
        "step": 7208
    },
    {
        "loss": 2.4943,
        "grad_norm": 3.1767544746398926,
        "learning_rate": 6.653125768856304e-05,
        "epoch": 0.5372233400402414,
        "step": 7209
    },
    {
        "loss": 2.6631,
        "grad_norm": 2.3169970512390137,
        "learning_rate": 6.646495835311528e-05,
        "epoch": 0.5372978612415232,
        "step": 7210
    },
    {
        "loss": 3.0978,
        "grad_norm": 2.7715210914611816,
        "learning_rate": 6.639867562196457e-05,
        "epoch": 0.5373723824428049,
        "step": 7211
    },
    {
        "loss": 2.3067,
        "grad_norm": 3.3953633308410645,
        "learning_rate": 6.633240952792992e-05,
        "epoch": 0.5374469036440868,
        "step": 7212
    },
    {
        "loss": 2.0815,
        "grad_norm": 4.009670257568359,
        "learning_rate": 6.62661601038217e-05,
        "epoch": 0.5375214248453685,
        "step": 7213
    },
    {
        "loss": 2.5964,
        "grad_norm": 2.171818971633911,
        "learning_rate": 6.61999273824421e-05,
        "epoch": 0.5375959460466503,
        "step": 7214
    },
    {
        "loss": 2.3589,
        "grad_norm": 2.8497695922851562,
        "learning_rate": 6.613371139658529e-05,
        "epoch": 0.537670467247932,
        "step": 7215
    },
    {
        "loss": 1.0805,
        "grad_norm": 1.535338282585144,
        "learning_rate": 6.606751217903679e-05,
        "epoch": 0.5377449884492138,
        "step": 7216
    },
    {
        "loss": 2.4212,
        "grad_norm": 2.466080665588379,
        "learning_rate": 6.600132976257419e-05,
        "epoch": 0.5378195096504955,
        "step": 7217
    },
    {
        "loss": 2.0375,
        "grad_norm": 3.562344551086426,
        "learning_rate": 6.593516417996645e-05,
        "epoch": 0.5378940308517773,
        "step": 7218
    },
    {
        "loss": 2.6429,
        "grad_norm": 2.2506775856018066,
        "learning_rate": 6.586901546397429e-05,
        "epoch": 0.537968552053059,
        "step": 7219
    },
    {
        "loss": 2.0482,
        "grad_norm": 3.353543281555176,
        "learning_rate": 6.580288364735019e-05,
        "epoch": 0.5380430732543409,
        "step": 7220
    },
    {
        "loss": 2.3226,
        "grad_norm": 3.577347755432129,
        "learning_rate": 6.573676876283801e-05,
        "epoch": 0.5381175944556226,
        "step": 7221
    },
    {
        "loss": 2.436,
        "grad_norm": 3.089437246322632,
        "learning_rate": 6.567067084317368e-05,
        "epoch": 0.5381921156569044,
        "step": 7222
    },
    {
        "loss": 2.7656,
        "grad_norm": 2.2851438522338867,
        "learning_rate": 6.560458992108425e-05,
        "epoch": 0.5382666368581862,
        "step": 7223
    },
    {
        "loss": 2.5098,
        "grad_norm": 2.714980125427246,
        "learning_rate": 6.553852602928847e-05,
        "epoch": 0.5383411580594679,
        "step": 7224
    },
    {
        "loss": 2.2108,
        "grad_norm": 3.5517971515655518,
        "learning_rate": 6.547247920049699e-05,
        "epoch": 0.5384156792607497,
        "step": 7225
    },
    {
        "loss": 1.9656,
        "grad_norm": 2.792638063430786,
        "learning_rate": 6.540644946741155e-05,
        "epoch": 0.5384902004620314,
        "step": 7226
    },
    {
        "loss": 2.0955,
        "grad_norm": 3.522564172744751,
        "learning_rate": 6.534043686272576e-05,
        "epoch": 0.5385647216633133,
        "step": 7227
    },
    {
        "loss": 2.5251,
        "grad_norm": 3.755323648452759,
        "learning_rate": 6.527444141912453e-05,
        "epoch": 0.538639242864595,
        "step": 7228
    },
    {
        "loss": 2.5311,
        "grad_norm": 2.874709129333496,
        "learning_rate": 6.52084631692844e-05,
        "epoch": 0.5387137640658768,
        "step": 7229
    },
    {
        "loss": 2.3518,
        "grad_norm": 2.9528815746307373,
        "learning_rate": 6.514250214587337e-05,
        "epoch": 0.5387882852671585,
        "step": 7230
    },
    {
        "loss": 1.8945,
        "grad_norm": 3.3116350173950195,
        "learning_rate": 6.507655838155078e-05,
        "epoch": 0.5388628064684403,
        "step": 7231
    },
    {
        "loss": 1.8883,
        "grad_norm": 3.347867965698242,
        "learning_rate": 6.50106319089678e-05,
        "epoch": 0.538937327669722,
        "step": 7232
    },
    {
        "loss": 1.8443,
        "grad_norm": 3.124464988708496,
        "learning_rate": 6.494472276076663e-05,
        "epoch": 0.5390118488710038,
        "step": 7233
    },
    {
        "loss": 1.8466,
        "grad_norm": 5.625803470611572,
        "learning_rate": 6.487883096958094e-05,
        "epoch": 0.5390863700722855,
        "step": 7234
    },
    {
        "loss": 2.0861,
        "grad_norm": 2.621152877807617,
        "learning_rate": 6.481295656803614e-05,
        "epoch": 0.5391608912735674,
        "step": 7235
    },
    {
        "loss": 1.2609,
        "grad_norm": 2.9166314601898193,
        "learning_rate": 6.474709958874866e-05,
        "epoch": 0.5392354124748491,
        "step": 7236
    },
    {
        "loss": 2.7892,
        "grad_norm": 2.799574613571167,
        "learning_rate": 6.468126006432656e-05,
        "epoch": 0.5393099336761309,
        "step": 7237
    },
    {
        "loss": 2.5044,
        "grad_norm": 1.8823459148406982,
        "learning_rate": 6.461543802736905e-05,
        "epoch": 0.5393844548774126,
        "step": 7238
    },
    {
        "loss": 2.0135,
        "grad_norm": 2.891789436340332,
        "learning_rate": 6.454963351046672e-05,
        "epoch": 0.5394589760786944,
        "step": 7239
    },
    {
        "loss": 2.0942,
        "grad_norm": 2.6849281787872314,
        "learning_rate": 6.448384654620172e-05,
        "epoch": 0.5395334972799761,
        "step": 7240
    },
    {
        "loss": 2.4464,
        "grad_norm": 2.169806718826294,
        "learning_rate": 6.441807716714719e-05,
        "epoch": 0.539608018481258,
        "step": 7241
    },
    {
        "loss": 2.5354,
        "grad_norm": 2.2932398319244385,
        "learning_rate": 6.435232540586763e-05,
        "epoch": 0.5396825396825397,
        "step": 7242
    },
    {
        "loss": 2.4376,
        "grad_norm": 2.5965211391448975,
        "learning_rate": 6.428659129491912e-05,
        "epoch": 0.5397570608838215,
        "step": 7243
    },
    {
        "loss": 2.3049,
        "grad_norm": 2.272920608520508,
        "learning_rate": 6.422087486684851e-05,
        "epoch": 0.5398315820851032,
        "step": 7244
    },
    {
        "loss": 2.3855,
        "grad_norm": 2.545412063598633,
        "learning_rate": 6.415517615419446e-05,
        "epoch": 0.539906103286385,
        "step": 7245
    },
    {
        "loss": 1.8458,
        "grad_norm": 3.808717727661133,
        "learning_rate": 6.408949518948636e-05,
        "epoch": 0.5399806244876667,
        "step": 7246
    },
    {
        "loss": 2.656,
        "grad_norm": 2.557319402694702,
        "learning_rate": 6.402383200524498e-05,
        "epoch": 0.5400551456889485,
        "step": 7247
    },
    {
        "loss": 2.6341,
        "grad_norm": 3.4172840118408203,
        "learning_rate": 6.395818663398244e-05,
        "epoch": 0.5401296668902302,
        "step": 7248
    },
    {
        "loss": 2.5219,
        "grad_norm": 2.6976306438446045,
        "learning_rate": 6.389255910820177e-05,
        "epoch": 0.540204188091512,
        "step": 7249
    },
    {
        "loss": 2.8897,
        "grad_norm": 3.3259005546569824,
        "learning_rate": 6.382694946039742e-05,
        "epoch": 0.5402787092927938,
        "step": 7250
    },
    {
        "loss": 2.5288,
        "grad_norm": 3.2079973220825195,
        "learning_rate": 6.376135772305487e-05,
        "epoch": 0.5403532304940756,
        "step": 7251
    },
    {
        "loss": 2.2852,
        "grad_norm": 2.429793119430542,
        "learning_rate": 6.369578392865057e-05,
        "epoch": 0.5404277516953573,
        "step": 7252
    },
    {
        "loss": 2.4359,
        "grad_norm": 2.887561082839966,
        "learning_rate": 6.363022810965248e-05,
        "epoch": 0.5405022728966391,
        "step": 7253
    },
    {
        "loss": 1.8912,
        "grad_norm": 2.985015392303467,
        "learning_rate": 6.356469029851921e-05,
        "epoch": 0.5405767940979208,
        "step": 7254
    },
    {
        "loss": 3.0483,
        "grad_norm": 3.0746209621429443,
        "learning_rate": 6.349917052770095e-05,
        "epoch": 0.5406513152992026,
        "step": 7255
    },
    {
        "loss": 2.5732,
        "grad_norm": 2.3841121196746826,
        "learning_rate": 6.343366882963854e-05,
        "epoch": 0.5407258365004843,
        "step": 7256
    },
    {
        "loss": 2.4535,
        "grad_norm": 2.883162021636963,
        "learning_rate": 6.336818523676394e-05,
        "epoch": 0.5408003577017662,
        "step": 7257
    },
    {
        "loss": 2.1656,
        "grad_norm": 3.328486204147339,
        "learning_rate": 6.330271978150032e-05,
        "epoch": 0.5408748789030479,
        "step": 7258
    },
    {
        "loss": 2.0229,
        "grad_norm": 2.6891355514526367,
        "learning_rate": 6.323727249626174e-05,
        "epoch": 0.5409494001043297,
        "step": 7259
    },
    {
        "loss": 2.2904,
        "grad_norm": 2.400057554244995,
        "learning_rate": 6.317184341345333e-05,
        "epoch": 0.5410239213056115,
        "step": 7260
    },
    {
        "loss": 2.415,
        "grad_norm": 3.1122097969055176,
        "learning_rate": 6.310643256547112e-05,
        "epoch": 0.5410984425068932,
        "step": 7261
    },
    {
        "loss": 2.9511,
        "grad_norm": 1.539126992225647,
        "learning_rate": 6.304103998470205e-05,
        "epoch": 0.541172963708175,
        "step": 7262
    },
    {
        "loss": 2.5264,
        "grad_norm": 3.4788694381713867,
        "learning_rate": 6.297566570352442e-05,
        "epoch": 0.5412474849094567,
        "step": 7263
    },
    {
        "loss": 2.551,
        "grad_norm": 2.418145179748535,
        "learning_rate": 6.291030975430687e-05,
        "epoch": 0.5413220061107386,
        "step": 7264
    },
    {
        "loss": 2.7667,
        "grad_norm": 2.5911524295806885,
        "learning_rate": 6.284497216940955e-05,
        "epoch": 0.5413965273120203,
        "step": 7265
    },
    {
        "loss": 2.0081,
        "grad_norm": 3.7485506534576416,
        "learning_rate": 6.277965298118308e-05,
        "epoch": 0.5414710485133021,
        "step": 7266
    },
    {
        "loss": 2.4412,
        "grad_norm": 3.0558395385742188,
        "learning_rate": 6.271435222196913e-05,
        "epoch": 0.5415455697145838,
        "step": 7267
    },
    {
        "loss": 2.3045,
        "grad_norm": 3.056929111480713,
        "learning_rate": 6.26490699241003e-05,
        "epoch": 0.5416200909158656,
        "step": 7268
    },
    {
        "loss": 2.7666,
        "grad_norm": 2.3981528282165527,
        "learning_rate": 6.258380611989988e-05,
        "epoch": 0.5416946121171473,
        "step": 7269
    },
    {
        "loss": 2.3954,
        "grad_norm": 3.685955762863159,
        "learning_rate": 6.251856084168231e-05,
        "epoch": 0.5417691333184291,
        "step": 7270
    },
    {
        "loss": 2.4962,
        "grad_norm": 2.289422035217285,
        "learning_rate": 6.245333412175252e-05,
        "epoch": 0.5418436545197108,
        "step": 7271
    },
    {
        "loss": 2.4986,
        "grad_norm": 4.296425819396973,
        "learning_rate": 6.238812599240629e-05,
        "epoch": 0.5419181757209927,
        "step": 7272
    },
    {
        "loss": 1.7855,
        "grad_norm": 4.526834964752197,
        "learning_rate": 6.232293648593057e-05,
        "epoch": 0.5419926969222744,
        "step": 7273
    },
    {
        "loss": 2.4113,
        "grad_norm": 2.5484890937805176,
        "learning_rate": 6.225776563460271e-05,
        "epoch": 0.5420672181235562,
        "step": 7274
    },
    {
        "loss": 2.9605,
        "grad_norm": 3.3944783210754395,
        "learning_rate": 6.219261347069074e-05,
        "epoch": 0.5421417393248379,
        "step": 7275
    },
    {
        "loss": 2.7065,
        "grad_norm": 3.1006195545196533,
        "learning_rate": 6.212748002645393e-05,
        "epoch": 0.5422162605261197,
        "step": 7276
    },
    {
        "loss": 2.3485,
        "grad_norm": 2.3224596977233887,
        "learning_rate": 6.206236533414178e-05,
        "epoch": 0.5422907817274014,
        "step": 7277
    },
    {
        "loss": 1.8925,
        "grad_norm": 2.114152193069458,
        "learning_rate": 6.199726942599487e-05,
        "epoch": 0.5423653029286832,
        "step": 7278
    },
    {
        "loss": 2.2465,
        "grad_norm": 3.0401172637939453,
        "learning_rate": 6.193219233424418e-05,
        "epoch": 0.5424398241299649,
        "step": 7279
    },
    {
        "loss": 1.8226,
        "grad_norm": 2.8229551315307617,
        "learning_rate": 6.186713409111149e-05,
        "epoch": 0.5425143453312468,
        "step": 7280
    },
    {
        "loss": 2.7637,
        "grad_norm": 2.3962578773498535,
        "learning_rate": 6.180209472880941e-05,
        "epoch": 0.5425888665325285,
        "step": 7281
    },
    {
        "loss": 1.6552,
        "grad_norm": 3.007619857788086,
        "learning_rate": 6.173707427954083e-05,
        "epoch": 0.5426633877338103,
        "step": 7282
    },
    {
        "loss": 2.3917,
        "grad_norm": 3.965291738510132,
        "learning_rate": 6.167207277549977e-05,
        "epoch": 0.542737908935092,
        "step": 7283
    },
    {
        "loss": 2.521,
        "grad_norm": 1.9165793657302856,
        "learning_rate": 6.160709024887046e-05,
        "epoch": 0.5428124301363738,
        "step": 7284
    },
    {
        "loss": 1.6581,
        "grad_norm": 2.902493476867676,
        "learning_rate": 6.154212673182779e-05,
        "epoch": 0.5428869513376555,
        "step": 7285
    },
    {
        "loss": 1.8954,
        "grad_norm": 3.519779920578003,
        "learning_rate": 6.147718225653757e-05,
        "epoch": 0.5429614725389373,
        "step": 7286
    },
    {
        "loss": 2.3728,
        "grad_norm": 5.099063873291016,
        "learning_rate": 6.141225685515573e-05,
        "epoch": 0.543035993740219,
        "step": 7287
    },
    {
        "loss": 3.0249,
        "grad_norm": 1.9885748624801636,
        "learning_rate": 6.134735055982907e-05,
        "epoch": 0.5431105149415009,
        "step": 7288
    },
    {
        "loss": 2.4214,
        "grad_norm": 2.099050283432007,
        "learning_rate": 6.128246340269478e-05,
        "epoch": 0.5431850361427826,
        "step": 7289
    },
    {
        "loss": 2.7908,
        "grad_norm": 3.334134340286255,
        "learning_rate": 6.121759541588057e-05,
        "epoch": 0.5432595573440644,
        "step": 7290
    },
    {
        "loss": 2.5091,
        "grad_norm": 1.5545337200164795,
        "learning_rate": 6.11527466315048e-05,
        "epoch": 0.5433340785453461,
        "step": 7291
    },
    {
        "loss": 2.7489,
        "grad_norm": 1.8233674764633179,
        "learning_rate": 6.108791708167605e-05,
        "epoch": 0.5434085997466279,
        "step": 7292
    },
    {
        "loss": 2.613,
        "grad_norm": 2.1063976287841797,
        "learning_rate": 6.102310679849383e-05,
        "epoch": 0.5434831209479096,
        "step": 7293
    },
    {
        "loss": 1.2422,
        "grad_norm": 3.2729883193969727,
        "learning_rate": 6.0958315814047665e-05,
        "epoch": 0.5435576421491914,
        "step": 7294
    },
    {
        "loss": 2.62,
        "grad_norm": 2.1385610103607178,
        "learning_rate": 6.0893544160417574e-05,
        "epoch": 0.5436321633504733,
        "step": 7295
    },
    {
        "loss": 1.3751,
        "grad_norm": 2.1738107204437256,
        "learning_rate": 6.0828791869674394e-05,
        "epoch": 0.543706684551755,
        "step": 7296
    },
    {
        "loss": 2.5233,
        "grad_norm": 2.635040044784546,
        "learning_rate": 6.076405897387893e-05,
        "epoch": 0.5437812057530368,
        "step": 7297
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.6308786869049072,
        "learning_rate": 6.069934550508263e-05,
        "epoch": 0.5438557269543185,
        "step": 7298
    },
    {
        "loss": 2.4342,
        "grad_norm": 3.5569021701812744,
        "learning_rate": 6.063465149532723e-05,
        "epoch": 0.5439302481556003,
        "step": 7299
    },
    {
        "loss": 2.6541,
        "grad_norm": 2.704843521118164,
        "learning_rate": 6.056997697664478e-05,
        "epoch": 0.544004769356882,
        "step": 7300
    },
    {
        "loss": 2.4295,
        "grad_norm": 3.9618079662323,
        "learning_rate": 6.0505321981057895e-05,
        "epoch": 0.5440792905581638,
        "step": 7301
    },
    {
        "loss": 2.3416,
        "grad_norm": 3.0048787593841553,
        "learning_rate": 6.044068654057917e-05,
        "epoch": 0.5441538117594455,
        "step": 7302
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.0621798038482666,
        "learning_rate": 6.037607068721196e-05,
        "epoch": 0.5442283329607274,
        "step": 7303
    },
    {
        "loss": 2.3284,
        "grad_norm": 2.86657452583313,
        "learning_rate": 6.0311474452949624e-05,
        "epoch": 0.5443028541620091,
        "step": 7304
    },
    {
        "loss": 1.5913,
        "grad_norm": 3.4644415378570557,
        "learning_rate": 6.0246897869775686e-05,
        "epoch": 0.5443773753632909,
        "step": 7305
    },
    {
        "loss": 1.8387,
        "grad_norm": 3.1155974864959717,
        "learning_rate": 6.018234096966441e-05,
        "epoch": 0.5444518965645726,
        "step": 7306
    },
    {
        "loss": 2.1059,
        "grad_norm": 4.238935947418213,
        "learning_rate": 6.01178037845799e-05,
        "epoch": 0.5445264177658544,
        "step": 7307
    },
    {
        "loss": 2.3878,
        "grad_norm": 2.5160059928894043,
        "learning_rate": 6.0053286346476555e-05,
        "epoch": 0.5446009389671361,
        "step": 7308
    },
    {
        "loss": 2.5655,
        "grad_norm": 2.0826902389526367,
        "learning_rate": 5.998878868729922e-05,
        "epoch": 0.544675460168418,
        "step": 7309
    },
    {
        "loss": 2.4368,
        "grad_norm": 4.29584264755249,
        "learning_rate": 5.9924310838982664e-05,
        "epoch": 0.5447499813696997,
        "step": 7310
    },
    {
        "loss": 1.9954,
        "grad_norm": 3.2592551708221436,
        "learning_rate": 5.9859852833452076e-05,
        "epoch": 0.5448245025709815,
        "step": 7311
    },
    {
        "loss": 2.4362,
        "grad_norm": 2.952395439147949,
        "learning_rate": 5.979541470262273e-05,
        "epoch": 0.5448990237722632,
        "step": 7312
    },
    {
        "loss": 2.5401,
        "grad_norm": 2.585566997528076,
        "learning_rate": 5.973099647839986e-05,
        "epoch": 0.544973544973545,
        "step": 7313
    },
    {
        "loss": 2.4666,
        "grad_norm": 3.955091714859009,
        "learning_rate": 5.966659819267931e-05,
        "epoch": 0.5450480661748267,
        "step": 7314
    },
    {
        "loss": 2.1002,
        "grad_norm": 2.9817726612091064,
        "learning_rate": 5.960221987734656e-05,
        "epoch": 0.5451225873761085,
        "step": 7315
    },
    {
        "loss": 2.7927,
        "grad_norm": 3.1334924697875977,
        "learning_rate": 5.953786156427763e-05,
        "epoch": 0.5451971085773902,
        "step": 7316
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.851197361946106,
        "learning_rate": 5.947352328533833e-05,
        "epoch": 0.545271629778672,
        "step": 7317
    },
    {
        "loss": 1.759,
        "grad_norm": 4.690968036651611,
        "learning_rate": 5.940920507238456e-05,
        "epoch": 0.5453461509799538,
        "step": 7318
    },
    {
        "loss": 3.0046,
        "grad_norm": 2.8222122192382812,
        "learning_rate": 5.9344906957262527e-05,
        "epoch": 0.5454206721812356,
        "step": 7319
    },
    {
        "loss": 2.5775,
        "grad_norm": 3.2945568561553955,
        "learning_rate": 5.928062897180822e-05,
        "epoch": 0.5454951933825173,
        "step": 7320
    },
    {
        "loss": 2.3352,
        "grad_norm": 4.631350994110107,
        "learning_rate": 5.921637114784784e-05,
        "epoch": 0.5455697145837991,
        "step": 7321
    },
    {
        "loss": 2.6754,
        "grad_norm": 3.010751247406006,
        "learning_rate": 5.915213351719753e-05,
        "epoch": 0.5456442357850808,
        "step": 7322
    },
    {
        "loss": 2.5175,
        "grad_norm": 3.588038444519043,
        "learning_rate": 5.908791611166329e-05,
        "epoch": 0.5457187569863626,
        "step": 7323
    },
    {
        "loss": 3.0158,
        "grad_norm": 1.9032942056655884,
        "learning_rate": 5.9023718963041554e-05,
        "epoch": 0.5457932781876443,
        "step": 7324
    },
    {
        "loss": 2.5706,
        "grad_norm": 3.222618579864502,
        "learning_rate": 5.8959542103118115e-05,
        "epoch": 0.5458677993889262,
        "step": 7325
    },
    {
        "loss": 2.5409,
        "grad_norm": 2.7136924266815186,
        "learning_rate": 5.8895385563669334e-05,
        "epoch": 0.5459423205902079,
        "step": 7326
    },
    {
        "loss": 2.2869,
        "grad_norm": 2.6423301696777344,
        "learning_rate": 5.88312493764611e-05,
        "epoch": 0.5460168417914897,
        "step": 7327
    },
    {
        "loss": 1.055,
        "grad_norm": 3.3968405723571777,
        "learning_rate": 5.876713357324923e-05,
        "epoch": 0.5460913629927714,
        "step": 7328
    },
    {
        "loss": 2.6302,
        "grad_norm": 2.1669301986694336,
        "learning_rate": 5.870303818577971e-05,
        "epoch": 0.5461658841940532,
        "step": 7329
    },
    {
        "loss": 2.9794,
        "grad_norm": 2.297278881072998,
        "learning_rate": 5.863896324578815e-05,
        "epoch": 0.546240405395335,
        "step": 7330
    },
    {
        "loss": 2.288,
        "grad_norm": 3.37362003326416,
        "learning_rate": 5.857490878500027e-05,
        "epoch": 0.5463149265966167,
        "step": 7331
    },
    {
        "loss": 2.099,
        "grad_norm": 3.7968509197235107,
        "learning_rate": 5.851087483513145e-05,
        "epoch": 0.5463894477978986,
        "step": 7332
    },
    {
        "loss": 2.1738,
        "grad_norm": 2.845886707305908,
        "learning_rate": 5.844686142788687e-05,
        "epoch": 0.5464639689991803,
        "step": 7333
    },
    {
        "loss": 1.6806,
        "grad_norm": 4.23587703704834,
        "learning_rate": 5.838286859496196e-05,
        "epoch": 0.5465384902004621,
        "step": 7334
    },
    {
        "loss": 2.078,
        "grad_norm": 1.922630786895752,
        "learning_rate": 5.8318896368041485e-05,
        "epoch": 0.5466130114017438,
        "step": 7335
    },
    {
        "loss": 1.7382,
        "grad_norm": 2.60152530670166,
        "learning_rate": 5.825494477880009e-05,
        "epoch": 0.5466875326030256,
        "step": 7336
    },
    {
        "loss": 0.5734,
        "grad_norm": 2.8819761276245117,
        "learning_rate": 5.819101385890256e-05,
        "epoch": 0.5467620538043073,
        "step": 7337
    },
    {
        "loss": 2.9663,
        "grad_norm": 2.4212727546691895,
        "learning_rate": 5.812710364000299e-05,
        "epoch": 0.5468365750055891,
        "step": 7338
    },
    {
        "loss": 2.4499,
        "grad_norm": 3.8761987686157227,
        "learning_rate": 5.8063214153745535e-05,
        "epoch": 0.5469110962068708,
        "step": 7339
    },
    {
        "loss": 2.0059,
        "grad_norm": 2.98313045501709,
        "learning_rate": 5.799934543176391e-05,
        "epoch": 0.5469856174081527,
        "step": 7340
    },
    {
        "loss": 2.4751,
        "grad_norm": 3.3871028423309326,
        "learning_rate": 5.793549750568159e-05,
        "epoch": 0.5470601386094344,
        "step": 7341
    },
    {
        "loss": 1.8763,
        "grad_norm": 3.6456799507141113,
        "learning_rate": 5.787167040711183e-05,
        "epoch": 0.5471346598107162,
        "step": 7342
    },
    {
        "loss": 2.2857,
        "grad_norm": 2.8928234577178955,
        "learning_rate": 5.780786416765738e-05,
        "epoch": 0.5472091810119979,
        "step": 7343
    },
    {
        "loss": 2.1887,
        "grad_norm": 3.4463555812835693,
        "learning_rate": 5.774407881891104e-05,
        "epoch": 0.5472837022132797,
        "step": 7344
    },
    {
        "loss": 1.7248,
        "grad_norm": 4.054482936859131,
        "learning_rate": 5.768031439245487e-05,
        "epoch": 0.5473582234145614,
        "step": 7345
    },
    {
        "loss": 2.681,
        "grad_norm": 3.188999652862549,
        "learning_rate": 5.761657091986065e-05,
        "epoch": 0.5474327446158432,
        "step": 7346
    },
    {
        "loss": 2.8036,
        "grad_norm": 1.8225181102752686,
        "learning_rate": 5.7552848432690064e-05,
        "epoch": 0.5475072658171249,
        "step": 7347
    },
    {
        "loss": 1.7223,
        "grad_norm": 2.6287732124328613,
        "learning_rate": 5.7489146962494066e-05,
        "epoch": 0.5475817870184068,
        "step": 7348
    },
    {
        "loss": 1.2717,
        "grad_norm": 3.252256155014038,
        "learning_rate": 5.742546654081341e-05,
        "epoch": 0.5476563082196885,
        "step": 7349
    },
    {
        "loss": 3.0934,
        "grad_norm": 3.9284238815307617,
        "learning_rate": 5.7361807199178316e-05,
        "epoch": 0.5477308294209703,
        "step": 7350
    },
    {
        "loss": 2.3223,
        "grad_norm": 2.627729892730713,
        "learning_rate": 5.729816896910858e-05,
        "epoch": 0.547805350622252,
        "step": 7351
    },
    {
        "loss": 2.0248,
        "grad_norm": 2.5744879245758057,
        "learning_rate": 5.723455188211363e-05,
        "epoch": 0.5478798718235338,
        "step": 7352
    },
    {
        "loss": 2.0173,
        "grad_norm": 3.0866615772247314,
        "learning_rate": 5.717095596969223e-05,
        "epoch": 0.5479543930248155,
        "step": 7353
    },
    {
        "loss": 2.1093,
        "grad_norm": 3.012230157852173,
        "learning_rate": 5.710738126333304e-05,
        "epoch": 0.5480289142260973,
        "step": 7354
    },
    {
        "loss": 2.2998,
        "grad_norm": 2.634453773498535,
        "learning_rate": 5.7043827794513874e-05,
        "epoch": 0.548103435427379,
        "step": 7355
    },
    {
        "loss": 2.2547,
        "grad_norm": 2.2314341068267822,
        "learning_rate": 5.698029559470197e-05,
        "epoch": 0.5481779566286609,
        "step": 7356
    },
    {
        "loss": 2.5503,
        "grad_norm": 3.433931827545166,
        "learning_rate": 5.691678469535445e-05,
        "epoch": 0.5482524778299426,
        "step": 7357
    },
    {
        "loss": 2.7428,
        "grad_norm": 3.9129903316497803,
        "learning_rate": 5.685329512791746e-05,
        "epoch": 0.5483269990312244,
        "step": 7358
    },
    {
        "loss": 2.2996,
        "grad_norm": 2.9729208946228027,
        "learning_rate": 5.678982692382696e-05,
        "epoch": 0.5484015202325061,
        "step": 7359
    },
    {
        "loss": 2.7475,
        "grad_norm": 2.486900806427002,
        "learning_rate": 5.672638011450797e-05,
        "epoch": 0.5484760414337879,
        "step": 7360
    },
    {
        "loss": 2.8047,
        "grad_norm": 2.4002137184143066,
        "learning_rate": 5.666295473137504e-05,
        "epoch": 0.5485505626350696,
        "step": 7361
    },
    {
        "loss": 1.3655,
        "grad_norm": 4.966769218444824,
        "learning_rate": 5.6599550805832325e-05,
        "epoch": 0.5486250838363514,
        "step": 7362
    },
    {
        "loss": 2.586,
        "grad_norm": 1.7044461965560913,
        "learning_rate": 5.653616836927298e-05,
        "epoch": 0.5486996050376332,
        "step": 7363
    },
    {
        "loss": 2.2894,
        "grad_norm": 2.7482335567474365,
        "learning_rate": 5.647280745307999e-05,
        "epoch": 0.548774126238915,
        "step": 7364
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.7636923789978027,
        "learning_rate": 5.640946808862524e-05,
        "epoch": 0.5488486474401968,
        "step": 7365
    },
    {
        "loss": 1.7835,
        "grad_norm": 3.5584897994995117,
        "learning_rate": 5.634615030727004e-05,
        "epoch": 0.5489231686414785,
        "step": 7366
    },
    {
        "loss": 2.8697,
        "grad_norm": 2.004511833190918,
        "learning_rate": 5.628285414036534e-05,
        "epoch": 0.5489976898427603,
        "step": 7367
    },
    {
        "loss": 2.2693,
        "grad_norm": 2.6242356300354004,
        "learning_rate": 5.6219579619251004e-05,
        "epoch": 0.549072211044042,
        "step": 7368
    },
    {
        "loss": 2.0845,
        "grad_norm": 2.8888208866119385,
        "learning_rate": 5.61563267752563e-05,
        "epoch": 0.5491467322453238,
        "step": 7369
    },
    {
        "loss": 1.8243,
        "grad_norm": 2.9134461879730225,
        "learning_rate": 5.609309563969985e-05,
        "epoch": 0.5492212534466056,
        "step": 7370
    },
    {
        "loss": 2.5983,
        "grad_norm": 2.356585741043091,
        "learning_rate": 5.602988624388939e-05,
        "epoch": 0.5492957746478874,
        "step": 7371
    },
    {
        "loss": 2.3204,
        "grad_norm": 2.4371578693389893,
        "learning_rate": 5.596669861912204e-05,
        "epoch": 0.5493702958491691,
        "step": 7372
    },
    {
        "loss": 1.8366,
        "grad_norm": 3.6933138370513916,
        "learning_rate": 5.590353279668401e-05,
        "epoch": 0.5494448170504509,
        "step": 7373
    },
    {
        "loss": 2.4216,
        "grad_norm": 3.5411436557769775,
        "learning_rate": 5.5840388807850655e-05,
        "epoch": 0.5495193382517326,
        "step": 7374
    },
    {
        "loss": 1.8472,
        "grad_norm": 2.9373838901519775,
        "learning_rate": 5.577726668388687e-05,
        "epoch": 0.5495938594530144,
        "step": 7375
    },
    {
        "loss": 3.1384,
        "grad_norm": 2.478461265563965,
        "learning_rate": 5.5714166456046255e-05,
        "epoch": 0.5496683806542961,
        "step": 7376
    },
    {
        "loss": 2.9255,
        "grad_norm": 2.1068544387817383,
        "learning_rate": 5.565108815557203e-05,
        "epoch": 0.549742901855578,
        "step": 7377
    },
    {
        "loss": 2.0119,
        "grad_norm": 2.5256400108337402,
        "learning_rate": 5.558803181369625e-05,
        "epoch": 0.5498174230568597,
        "step": 7378
    },
    {
        "loss": 2.234,
        "grad_norm": 1.8803218603134155,
        "learning_rate": 5.552499746164002e-05,
        "epoch": 0.5498919442581415,
        "step": 7379
    },
    {
        "loss": 2.8713,
        "grad_norm": 3.247220277786255,
        "learning_rate": 5.5461985130613894e-05,
        "epoch": 0.5499664654594232,
        "step": 7380
    },
    {
        "loss": 2.4301,
        "grad_norm": 2.565124750137329,
        "learning_rate": 5.539899485181723e-05,
        "epoch": 0.550040986660705,
        "step": 7381
    },
    {
        "loss": 2.6756,
        "grad_norm": 2.736377239227295,
        "learning_rate": 5.533602665643865e-05,
        "epoch": 0.5501155078619867,
        "step": 7382
    },
    {
        "loss": 2.224,
        "grad_norm": 3.516963243484497,
        "learning_rate": 5.5273080575655744e-05,
        "epoch": 0.5501900290632685,
        "step": 7383
    },
    {
        "loss": 2.3674,
        "grad_norm": 2.06352162361145,
        "learning_rate": 5.521015664063507e-05,
        "epoch": 0.5502645502645502,
        "step": 7384
    },
    {
        "loss": 2.2581,
        "grad_norm": 2.4756810665130615,
        "learning_rate": 5.5147254882532526e-05,
        "epoch": 0.550339071465832,
        "step": 7385
    },
    {
        "loss": 1.5926,
        "grad_norm": 2.845459222793579,
        "learning_rate": 5.508437533249267e-05,
        "epoch": 0.5504135926671138,
        "step": 7386
    },
    {
        "loss": 1.2159,
        "grad_norm": 2.240832567214966,
        "learning_rate": 5.502151802164937e-05,
        "epoch": 0.5504881138683956,
        "step": 7387
    },
    {
        "loss": 2.2434,
        "grad_norm": 2.8145864009857178,
        "learning_rate": 5.495868298112525e-05,
        "epoch": 0.5505626350696773,
        "step": 7388
    },
    {
        "loss": 2.713,
        "grad_norm": 2.5309293270111084,
        "learning_rate": 5.4895870242031935e-05,
        "epoch": 0.5506371562709591,
        "step": 7389
    },
    {
        "loss": 1.6939,
        "grad_norm": 3.765923261642456,
        "learning_rate": 5.483307983547026e-05,
        "epoch": 0.5507116774722408,
        "step": 7390
    },
    {
        "loss": 1.1614,
        "grad_norm": 4.8028340339660645,
        "learning_rate": 5.4770311792529585e-05,
        "epoch": 0.5507861986735226,
        "step": 7391
    },
    {
        "loss": 2.2176,
        "grad_norm": 3.294306993484497,
        "learning_rate": 5.470756614428859e-05,
        "epoch": 0.5508607198748043,
        "step": 7392
    },
    {
        "loss": 2.3817,
        "grad_norm": 3.649909496307373,
        "learning_rate": 5.4644842921814685e-05,
        "epoch": 0.5509352410760862,
        "step": 7393
    },
    {
        "loss": 1.5239,
        "grad_norm": 2.006082773208618,
        "learning_rate": 5.4582142156164e-05,
        "epoch": 0.5510097622773679,
        "step": 7394
    },
    {
        "loss": 2.3739,
        "grad_norm": 3.0313074588775635,
        "learning_rate": 5.451946387838199e-05,
        "epoch": 0.5510842834786497,
        "step": 7395
    },
    {
        "loss": 2.7132,
        "grad_norm": 2.2624075412750244,
        "learning_rate": 5.4456808119502504e-05,
        "epoch": 0.5511588046799314,
        "step": 7396
    },
    {
        "loss": 2.1103,
        "grad_norm": 2.8096160888671875,
        "learning_rate": 5.439417491054869e-05,
        "epoch": 0.5512333258812132,
        "step": 7397
    },
    {
        "loss": 2.0979,
        "grad_norm": 3.3283565044403076,
        "learning_rate": 5.4331564282532185e-05,
        "epoch": 0.5513078470824949,
        "step": 7398
    },
    {
        "loss": 2.595,
        "grad_norm": 3.5705461502075195,
        "learning_rate": 5.426897626645351e-05,
        "epoch": 0.5513823682837767,
        "step": 7399
    },
    {
        "loss": 2.3887,
        "grad_norm": 3.56630539894104,
        "learning_rate": 5.420641089330214e-05,
        "epoch": 0.5514568894850586,
        "step": 7400
    },
    {
        "loss": 2.8458,
        "grad_norm": 4.109831809997559,
        "learning_rate": 5.414386819405619e-05,
        "epoch": 0.5515314106863403,
        "step": 7401
    },
    {
        "loss": 2.166,
        "grad_norm": 2.333447217941284,
        "learning_rate": 5.408134819968255e-05,
        "epoch": 0.5516059318876221,
        "step": 7402
    },
    {
        "loss": 2.3503,
        "grad_norm": 3.241685152053833,
        "learning_rate": 5.401885094113701e-05,
        "epoch": 0.5516804530889038,
        "step": 7403
    },
    {
        "loss": 2.5608,
        "grad_norm": 2.3350844383239746,
        "learning_rate": 5.3956376449363845e-05,
        "epoch": 0.5517549742901856,
        "step": 7404
    },
    {
        "loss": 2.3551,
        "grad_norm": 3.147822856903076,
        "learning_rate": 5.389392475529644e-05,
        "epoch": 0.5518294954914673,
        "step": 7405
    },
    {
        "loss": 2.111,
        "grad_norm": 3.6696619987487793,
        "learning_rate": 5.383149588985661e-05,
        "epoch": 0.5519040166927491,
        "step": 7406
    },
    {
        "loss": 2.6599,
        "grad_norm": 2.1024770736694336,
        "learning_rate": 5.376908988395475e-05,
        "epoch": 0.5519785378940308,
        "step": 7407
    },
    {
        "loss": 2.7551,
        "grad_norm": 2.090484619140625,
        "learning_rate": 5.370670676849039e-05,
        "epoch": 0.5520530590953127,
        "step": 7408
    },
    {
        "loss": 2.6927,
        "grad_norm": 2.9424614906311035,
        "learning_rate": 5.364434657435128e-05,
        "epoch": 0.5521275802965944,
        "step": 7409
    },
    {
        "loss": 2.2448,
        "grad_norm": 4.25093412399292,
        "learning_rate": 5.3582009332414076e-05,
        "epoch": 0.5522021014978762,
        "step": 7410
    },
    {
        "loss": 0.9987,
        "grad_norm": 4.287237167358398,
        "learning_rate": 5.351969507354395e-05,
        "epoch": 0.5522766226991579,
        "step": 7411
    },
    {
        "loss": 2.3966,
        "grad_norm": 3.2539188861846924,
        "learning_rate": 5.3457403828594696e-05,
        "epoch": 0.5523511439004397,
        "step": 7412
    },
    {
        "loss": 2.3609,
        "grad_norm": 2.7462165355682373,
        "learning_rate": 5.339513562840883e-05,
        "epoch": 0.5524256651017214,
        "step": 7413
    },
    {
        "loss": 2.068,
        "grad_norm": 1.7133591175079346,
        "learning_rate": 5.33328905038172e-05,
        "epoch": 0.5525001863030032,
        "step": 7414
    },
    {
        "loss": 2.2914,
        "grad_norm": 2.586238384246826,
        "learning_rate": 5.327066848563966e-05,
        "epoch": 0.552574707504285,
        "step": 7415
    },
    {
        "loss": 1.8797,
        "grad_norm": 3.2762460708618164,
        "learning_rate": 5.3208469604684284e-05,
        "epoch": 0.5526492287055668,
        "step": 7416
    },
    {
        "loss": 1.5413,
        "grad_norm": 3.6839981079101562,
        "learning_rate": 5.314629389174759e-05,
        "epoch": 0.5527237499068485,
        "step": 7417
    },
    {
        "loss": 2.2095,
        "grad_norm": 3.4369115829467773,
        "learning_rate": 5.308414137761506e-05,
        "epoch": 0.5527982711081303,
        "step": 7418
    },
    {
        "loss": 2.4055,
        "grad_norm": 2.8450710773468018,
        "learning_rate": 5.302201209306027e-05,
        "epoch": 0.552872792309412,
        "step": 7419
    },
    {
        "loss": 2.1596,
        "grad_norm": 2.6362147331237793,
        "learning_rate": 5.2959906068845636e-05,
        "epoch": 0.5529473135106938,
        "step": 7420
    },
    {
        "loss": 2.9526,
        "grad_norm": 3.0612688064575195,
        "learning_rate": 5.289782333572173e-05,
        "epoch": 0.5530218347119755,
        "step": 7421
    },
    {
        "loss": 2.0224,
        "grad_norm": 3.850043535232544,
        "learning_rate": 5.283576392442773e-05,
        "epoch": 0.5530963559132573,
        "step": 7422
    },
    {
        "loss": 2.7616,
        "grad_norm": 2.2418911457061768,
        "learning_rate": 5.277372786569145e-05,
        "epoch": 0.553170877114539,
        "step": 7423
    },
    {
        "loss": 1.4296,
        "grad_norm": 3.436856269836426,
        "learning_rate": 5.2711715190228706e-05,
        "epoch": 0.5532453983158209,
        "step": 7424
    },
    {
        "loss": 2.6877,
        "grad_norm": 3.68237566947937,
        "learning_rate": 5.26497259287443e-05,
        "epoch": 0.5533199195171026,
        "step": 7425
    },
    {
        "loss": 2.2959,
        "grad_norm": 2.618878126144409,
        "learning_rate": 5.258776011193101e-05,
        "epoch": 0.5533944407183844,
        "step": 7426
    },
    {
        "loss": 2.2519,
        "grad_norm": 3.676356792449951,
        "learning_rate": 5.252581777047001e-05,
        "epoch": 0.5534689619196661,
        "step": 7427
    },
    {
        "loss": 2.9956,
        "grad_norm": 3.5000545978546143,
        "learning_rate": 5.246389893503124e-05,
        "epoch": 0.5535434831209479,
        "step": 7428
    },
    {
        "loss": 2.1776,
        "grad_norm": 3.732819080352783,
        "learning_rate": 5.240200363627261e-05,
        "epoch": 0.5536180043222296,
        "step": 7429
    },
    {
        "loss": 2.6223,
        "grad_norm": 2.1166632175445557,
        "learning_rate": 5.234013190484048e-05,
        "epoch": 0.5536925255235114,
        "step": 7430
    },
    {
        "loss": 1.2014,
        "grad_norm": 3.1280317306518555,
        "learning_rate": 5.227828377136965e-05,
        "epoch": 0.5537670467247932,
        "step": 7431
    },
    {
        "loss": 2.4042,
        "grad_norm": 2.8672077655792236,
        "learning_rate": 5.2216459266483076e-05,
        "epoch": 0.553841567926075,
        "step": 7432
    },
    {
        "loss": 2.5638,
        "grad_norm": 3.565159320831299,
        "learning_rate": 5.215465842079217e-05,
        "epoch": 0.5539160891273567,
        "step": 7433
    },
    {
        "loss": 2.1771,
        "grad_norm": 3.668686866760254,
        "learning_rate": 5.209288126489651e-05,
        "epoch": 0.5539906103286385,
        "step": 7434
    },
    {
        "loss": 2.4974,
        "grad_norm": 2.087411880493164,
        "learning_rate": 5.2031127829383875e-05,
        "epoch": 0.5540651315299202,
        "step": 7435
    },
    {
        "loss": 2.1444,
        "grad_norm": 3.3133270740509033,
        "learning_rate": 5.196939814483065e-05,
        "epoch": 0.554139652731202,
        "step": 7436
    },
    {
        "loss": 2.3956,
        "grad_norm": 3.002408981323242,
        "learning_rate": 5.190769224180099e-05,
        "epoch": 0.5542141739324838,
        "step": 7437
    },
    {
        "loss": 1.9458,
        "grad_norm": 2.695723295211792,
        "learning_rate": 5.184601015084774e-05,
        "epoch": 0.5542886951337656,
        "step": 7438
    },
    {
        "loss": 2.2331,
        "grad_norm": 2.1911957263946533,
        "learning_rate": 5.178435190251164e-05,
        "epoch": 0.5543632163350474,
        "step": 7439
    },
    {
        "loss": 2.5203,
        "grad_norm": 2.378573179244995,
        "learning_rate": 5.172271752732156e-05,
        "epoch": 0.5544377375363291,
        "step": 7440
    },
    {
        "loss": 2.7897,
        "grad_norm": 3.0655336380004883,
        "learning_rate": 5.1661107055794885e-05,
        "epoch": 0.5545122587376109,
        "step": 7441
    },
    {
        "loss": 2.2638,
        "grad_norm": 2.9481759071350098,
        "learning_rate": 5.159952051843687e-05,
        "epoch": 0.5545867799388926,
        "step": 7442
    },
    {
        "loss": 2.3316,
        "grad_norm": 2.7823877334594727,
        "learning_rate": 5.15379579457411e-05,
        "epoch": 0.5546613011401744,
        "step": 7443
    },
    {
        "loss": 2.7255,
        "grad_norm": 2.829599618911743,
        "learning_rate": 5.147641936818917e-05,
        "epoch": 0.5547358223414561,
        "step": 7444
    },
    {
        "loss": 2.0205,
        "grad_norm": 3.297165870666504,
        "learning_rate": 5.141490481625074e-05,
        "epoch": 0.554810343542738,
        "step": 7445
    },
    {
        "loss": 2.1839,
        "grad_norm": 3.995518684387207,
        "learning_rate": 5.135341432038395e-05,
        "epoch": 0.5548848647440197,
        "step": 7446
    },
    {
        "loss": 2.2446,
        "grad_norm": 3.1235241889953613,
        "learning_rate": 5.129194791103444e-05,
        "epoch": 0.5549593859453015,
        "step": 7447
    },
    {
        "loss": 2.2858,
        "grad_norm": 3.0139801502227783,
        "learning_rate": 5.1230505618636534e-05,
        "epoch": 0.5550339071465832,
        "step": 7448
    },
    {
        "loss": 2.418,
        "grad_norm": 2.883176803588867,
        "learning_rate": 5.116908747361218e-05,
        "epoch": 0.555108428347865,
        "step": 7449
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.1951687335968018,
        "learning_rate": 5.110769350637149e-05,
        "epoch": 0.5551829495491467,
        "step": 7450
    },
    {
        "loss": 2.2505,
        "grad_norm": 3.047759771347046,
        "learning_rate": 5.1046323747312696e-05,
        "epoch": 0.5552574707504285,
        "step": 7451
    },
    {
        "loss": 2.3133,
        "grad_norm": 3.099214792251587,
        "learning_rate": 5.0984978226821864e-05,
        "epoch": 0.5553319919517102,
        "step": 7452
    },
    {
        "loss": 2.918,
        "grad_norm": 2.4690423011779785,
        "learning_rate": 5.0923656975273323e-05,
        "epoch": 0.555406513152992,
        "step": 7453
    },
    {
        "loss": 1.8869,
        "grad_norm": 1.7983986139297485,
        "learning_rate": 5.08623600230292e-05,
        "epoch": 0.5554810343542738,
        "step": 7454
    },
    {
        "loss": 2.3685,
        "grad_norm": 3.1575441360473633,
        "learning_rate": 5.080108740043943e-05,
        "epoch": 0.5555555555555556,
        "step": 7455
    },
    {
        "loss": 2.4727,
        "grad_norm": 2.6116254329681396,
        "learning_rate": 5.0739839137842345e-05,
        "epoch": 0.5556300767568373,
        "step": 7456
    },
    {
        "loss": 1.7417,
        "grad_norm": 3.5524401664733887,
        "learning_rate": 5.067861526556377e-05,
        "epoch": 0.5557045979581191,
        "step": 7457
    },
    {
        "loss": 2.3459,
        "grad_norm": 2.1242868900299072,
        "learning_rate": 5.061741581391784e-05,
        "epoch": 0.5557791191594008,
        "step": 7458
    },
    {
        "loss": 2.5698,
        "grad_norm": 2.396400213241577,
        "learning_rate": 5.055624081320631e-05,
        "epoch": 0.5558536403606826,
        "step": 7459
    },
    {
        "loss": 2.1684,
        "grad_norm": 2.9605188369750977,
        "learning_rate": 5.04950902937189e-05,
        "epoch": 0.5559281615619643,
        "step": 7460
    },
    {
        "loss": 2.3706,
        "grad_norm": 4.338804244995117,
        "learning_rate": 5.0433964285733304e-05,
        "epoch": 0.5560026827632462,
        "step": 7461
    },
    {
        "loss": 1.7048,
        "grad_norm": 4.211106777191162,
        "learning_rate": 5.0372862819514944e-05,
        "epoch": 0.5560772039645279,
        "step": 7462
    },
    {
        "loss": 2.6468,
        "grad_norm": 3.7826178073883057,
        "learning_rate": 5.0311785925317156e-05,
        "epoch": 0.5561517251658097,
        "step": 7463
    },
    {
        "loss": 2.4307,
        "grad_norm": 3.543215036392212,
        "learning_rate": 5.025073363338117e-05,
        "epoch": 0.5562262463670914,
        "step": 7464
    },
    {
        "loss": 1.4812,
        "grad_norm": 3.7884392738342285,
        "learning_rate": 5.018970597393584e-05,
        "epoch": 0.5563007675683732,
        "step": 7465
    },
    {
        "loss": 2.5548,
        "grad_norm": 2.3677127361297607,
        "learning_rate": 5.012870297719816e-05,
        "epoch": 0.5563752887696549,
        "step": 7466
    },
    {
        "loss": 2.6312,
        "grad_norm": 1.7232437133789062,
        "learning_rate": 5.006772467337259e-05,
        "epoch": 0.5564498099709367,
        "step": 7467
    },
    {
        "loss": 1.0295,
        "grad_norm": 3.2843008041381836,
        "learning_rate": 5.0006771092651416e-05,
        "epoch": 0.5565243311722184,
        "step": 7468
    },
    {
        "loss": 2.4906,
        "grad_norm": 2.1964163780212402,
        "learning_rate": 4.994584226521498e-05,
        "epoch": 0.5565988523735003,
        "step": 7469
    },
    {
        "loss": 1.6433,
        "grad_norm": 3.377174139022827,
        "learning_rate": 4.988493822123088e-05,
        "epoch": 0.556673373574782,
        "step": 7470
    },
    {
        "loss": 2.0592,
        "grad_norm": 4.090672969818115,
        "learning_rate": 4.982405899085491e-05,
        "epoch": 0.5567478947760638,
        "step": 7471
    },
    {
        "loss": 2.3729,
        "grad_norm": 3.906444549560547,
        "learning_rate": 4.976320460423032e-05,
        "epoch": 0.5568224159773456,
        "step": 7472
    },
    {
        "loss": 1.9824,
        "grad_norm": 4.339268207550049,
        "learning_rate": 4.970237509148794e-05,
        "epoch": 0.5568969371786273,
        "step": 7473
    },
    {
        "loss": 2.6877,
        "grad_norm": 2.6738221645355225,
        "learning_rate": 4.964157048274667e-05,
        "epoch": 0.5569714583799091,
        "step": 7474
    },
    {
        "loss": 2.2718,
        "grad_norm": 2.528762102127075,
        "learning_rate": 4.958079080811266e-05,
        "epoch": 0.5570459795811908,
        "step": 7475
    },
    {
        "loss": 2.5049,
        "grad_norm": 2.9739763736724854,
        "learning_rate": 4.952003609768016e-05,
        "epoch": 0.5571205007824727,
        "step": 7476
    },
    {
        "loss": 1.8119,
        "grad_norm": 3.729875326156616,
        "learning_rate": 4.945930638153071e-05,
        "epoch": 0.5571950219837544,
        "step": 7477
    },
    {
        "loss": 1.8514,
        "grad_norm": 3.934798240661621,
        "learning_rate": 4.939860168973343e-05,
        "epoch": 0.5572695431850362,
        "step": 7478
    },
    {
        "loss": 2.3674,
        "grad_norm": 3.8083670139312744,
        "learning_rate": 4.933792205234543e-05,
        "epoch": 0.5573440643863179,
        "step": 7479
    },
    {
        "loss": 2.2626,
        "grad_norm": 2.628042459487915,
        "learning_rate": 4.927726749941102e-05,
        "epoch": 0.5574185855875997,
        "step": 7480
    },
    {
        "loss": 2.8886,
        "grad_norm": 2.242452383041382,
        "learning_rate": 4.921663806096243e-05,
        "epoch": 0.5574931067888814,
        "step": 7481
    },
    {
        "loss": 2.6449,
        "grad_norm": 2.0208091735839844,
        "learning_rate": 4.915603376701914e-05,
        "epoch": 0.5575676279901632,
        "step": 7482
    },
    {
        "loss": 2.6115,
        "grad_norm": 1.6995208263397217,
        "learning_rate": 4.909545464758827e-05,
        "epoch": 0.557642149191445,
        "step": 7483
    },
    {
        "loss": 1.9516,
        "grad_norm": 3.1620726585388184,
        "learning_rate": 4.903490073266472e-05,
        "epoch": 0.5577166703927268,
        "step": 7484
    },
    {
        "loss": 2.4851,
        "grad_norm": 1.9495441913604736,
        "learning_rate": 4.8974372052230464e-05,
        "epoch": 0.5577911915940085,
        "step": 7485
    },
    {
        "loss": 2.129,
        "grad_norm": 4.7322845458984375,
        "learning_rate": 4.891386863625549e-05,
        "epoch": 0.5578657127952903,
        "step": 7486
    },
    {
        "loss": 2.6692,
        "grad_norm": 2.2753405570983887,
        "learning_rate": 4.885339051469691e-05,
        "epoch": 0.557940233996572,
        "step": 7487
    },
    {
        "loss": 2.6499,
        "grad_norm": 2.144395112991333,
        "learning_rate": 4.879293771749929e-05,
        "epoch": 0.5580147551978538,
        "step": 7488
    },
    {
        "loss": 2.3326,
        "grad_norm": 2.8301284313201904,
        "learning_rate": 4.873251027459503e-05,
        "epoch": 0.5580892763991355,
        "step": 7489
    },
    {
        "loss": 1.7182,
        "grad_norm": 3.825648784637451,
        "learning_rate": 4.867210821590359e-05,
        "epoch": 0.5581637976004173,
        "step": 7490
    },
    {
        "loss": 2.5117,
        "grad_norm": 3.4996836185455322,
        "learning_rate": 4.861173157133208e-05,
        "epoch": 0.558238318801699,
        "step": 7491
    },
    {
        "loss": 2.6469,
        "grad_norm": 1.997851848602295,
        "learning_rate": 4.855138037077491e-05,
        "epoch": 0.5583128400029809,
        "step": 7492
    },
    {
        "loss": 2.7714,
        "grad_norm": 1.8990739583969116,
        "learning_rate": 4.84910546441139e-05,
        "epoch": 0.5583873612042626,
        "step": 7493
    },
    {
        "loss": 2.9559,
        "grad_norm": 2.5071659088134766,
        "learning_rate": 4.843075442121837e-05,
        "epoch": 0.5584618824055444,
        "step": 7494
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.3706040382385254,
        "learning_rate": 4.8370479731944896e-05,
        "epoch": 0.5585364036068261,
        "step": 7495
    },
    {
        "loss": 2.469,
        "grad_norm": 3.562361001968384,
        "learning_rate": 4.831023060613733e-05,
        "epoch": 0.5586109248081079,
        "step": 7496
    },
    {
        "loss": 2.787,
        "grad_norm": 2.1434249877929688,
        "learning_rate": 4.825000707362722e-05,
        "epoch": 0.5586854460093896,
        "step": 7497
    },
    {
        "loss": 1.9302,
        "grad_norm": 3.5379045009613037,
        "learning_rate": 4.818980916423299e-05,
        "epoch": 0.5587599672106714,
        "step": 7498
    },
    {
        "loss": 1.5168,
        "grad_norm": 3.820478916168213,
        "learning_rate": 4.812963690776086e-05,
        "epoch": 0.5588344884119532,
        "step": 7499
    },
    {
        "loss": 1.0707,
        "grad_norm": 3.148582935333252,
        "learning_rate": 4.806949033400387e-05,
        "epoch": 0.558909009613235,
        "step": 7500
    },
    {
        "loss": 2.0214,
        "grad_norm": 3.439570903778076,
        "learning_rate": 4.800936947274255e-05,
        "epoch": 0.5589835308145167,
        "step": 7501
    },
    {
        "loss": 2.3895,
        "grad_norm": 2.562228202819824,
        "learning_rate": 4.7949274353744846e-05,
        "epoch": 0.5590580520157985,
        "step": 7502
    },
    {
        "loss": 2.7677,
        "grad_norm": 2.8262178897857666,
        "learning_rate": 4.7889205006765715e-05,
        "epoch": 0.5591325732170802,
        "step": 7503
    },
    {
        "loss": 2.3493,
        "grad_norm": 1.6990607976913452,
        "learning_rate": 4.7829161461547514e-05,
        "epoch": 0.559207094418362,
        "step": 7504
    },
    {
        "loss": 2.5054,
        "grad_norm": 2.7010650634765625,
        "learning_rate": 4.776914374781974e-05,
        "epoch": 0.5592816156196437,
        "step": 7505
    },
    {
        "loss": 2.2633,
        "grad_norm": 3.2091915607452393,
        "learning_rate": 4.770915189529902e-05,
        "epoch": 0.5593561368209256,
        "step": 7506
    },
    {
        "loss": 1.8361,
        "grad_norm": 3.654636859893799,
        "learning_rate": 4.7649185933689544e-05,
        "epoch": 0.5594306580222074,
        "step": 7507
    },
    {
        "loss": 1.923,
        "grad_norm": 3.5866620540618896,
        "learning_rate": 4.758924589268212e-05,
        "epoch": 0.5595051792234891,
        "step": 7508
    },
    {
        "loss": 2.9008,
        "grad_norm": 3.214657783508301,
        "learning_rate": 4.75293318019553e-05,
        "epoch": 0.5595797004247709,
        "step": 7509
    },
    {
        "loss": 2.1231,
        "grad_norm": 2.6476027965545654,
        "learning_rate": 4.7469443691174386e-05,
        "epoch": 0.5596542216260526,
        "step": 7510
    },
    {
        "loss": 2.1588,
        "grad_norm": 2.7174103260040283,
        "learning_rate": 4.740958158999191e-05,
        "epoch": 0.5597287428273344,
        "step": 7511
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.793196201324463,
        "learning_rate": 4.734974552804764e-05,
        "epoch": 0.5598032640286161,
        "step": 7512
    },
    {
        "loss": 2.3729,
        "grad_norm": 2.1161351203918457,
        "learning_rate": 4.728993553496824e-05,
        "epoch": 0.559877785229898,
        "step": 7513
    },
    {
        "loss": 2.1501,
        "grad_norm": 2.8191440105438232,
        "learning_rate": 4.723015164036779e-05,
        "epoch": 0.5599523064311797,
        "step": 7514
    },
    {
        "loss": 2.3165,
        "grad_norm": 3.2737040519714355,
        "learning_rate": 4.7170393873847116e-05,
        "epoch": 0.5600268276324615,
        "step": 7515
    },
    {
        "loss": 1.8709,
        "grad_norm": 3.1384613513946533,
        "learning_rate": 4.711066226499417e-05,
        "epoch": 0.5601013488337432,
        "step": 7516
    },
    {
        "loss": 2.1282,
        "grad_norm": 3.485994338989258,
        "learning_rate": 4.705095684338423e-05,
        "epoch": 0.560175870035025,
        "step": 7517
    },
    {
        "loss": 2.1402,
        "grad_norm": 4.354301929473877,
        "learning_rate": 4.699127763857919e-05,
        "epoch": 0.5602503912363067,
        "step": 7518
    },
    {
        "loss": 2.5347,
        "grad_norm": 3.578972578048706,
        "learning_rate": 4.6931624680128414e-05,
        "epoch": 0.5603249124375885,
        "step": 7519
    },
    {
        "loss": 1.5757,
        "grad_norm": 2.8576765060424805,
        "learning_rate": 4.687199799756792e-05,
        "epoch": 0.5603994336388702,
        "step": 7520
    },
    {
        "loss": 2.8611,
        "grad_norm": 3.200357675552368,
        "learning_rate": 4.6812397620420786e-05,
        "epoch": 0.560473954840152,
        "step": 7521
    },
    {
        "loss": 2.7453,
        "grad_norm": 1.6458369493484497,
        "learning_rate": 4.675282357819718e-05,
        "epoch": 0.5605484760414338,
        "step": 7522
    },
    {
        "loss": 2.2879,
        "grad_norm": 2.506291389465332,
        "learning_rate": 4.669327590039414e-05,
        "epoch": 0.5606229972427156,
        "step": 7523
    },
    {
        "loss": 2.0476,
        "grad_norm": 3.9527218341827393,
        "learning_rate": 4.6633754616495626e-05,
        "epoch": 0.5606975184439973,
        "step": 7524
    },
    {
        "loss": 1.8116,
        "grad_norm": 2.051748037338257,
        "learning_rate": 4.657425975597265e-05,
        "epoch": 0.5607720396452791,
        "step": 7525
    },
    {
        "loss": 2.2375,
        "grad_norm": 2.900754690170288,
        "learning_rate": 4.651479134828292e-05,
        "epoch": 0.5608465608465608,
        "step": 7526
    },
    {
        "loss": 2.497,
        "grad_norm": 2.5069479942321777,
        "learning_rate": 4.64553494228714e-05,
        "epoch": 0.5609210820478426,
        "step": 7527
    },
    {
        "loss": 1.9894,
        "grad_norm": 3.405735969543457,
        "learning_rate": 4.639593400916963e-05,
        "epoch": 0.5609956032491243,
        "step": 7528
    },
    {
        "loss": 1.6035,
        "grad_norm": 3.5692050457000732,
        "learning_rate": 4.633654513659602e-05,
        "epoch": 0.5610701244504062,
        "step": 7529
    },
    {
        "loss": 2.5237,
        "grad_norm": 3.0442733764648438,
        "learning_rate": 4.627718283455621e-05,
        "epoch": 0.5611446456516879,
        "step": 7530
    },
    {
        "loss": 2.4504,
        "grad_norm": 2.3745040893554688,
        "learning_rate": 4.621784713244215e-05,
        "epoch": 0.5612191668529697,
        "step": 7531
    },
    {
        "loss": 2.5021,
        "grad_norm": 2.972078800201416,
        "learning_rate": 4.6158538059633084e-05,
        "epoch": 0.5612936880542514,
        "step": 7532
    },
    {
        "loss": 2.3372,
        "grad_norm": 2.4318504333496094,
        "learning_rate": 4.609925564549482e-05,
        "epoch": 0.5613682092555332,
        "step": 7533
    },
    {
        "loss": 1.6659,
        "grad_norm": 3.7631497383117676,
        "learning_rate": 4.603999991937989e-05,
        "epoch": 0.5614427304568149,
        "step": 7534
    },
    {
        "loss": 2.5708,
        "grad_norm": 1.9967660903930664,
        "learning_rate": 4.5980770910627933e-05,
        "epoch": 0.5615172516580967,
        "step": 7535
    },
    {
        "loss": 2.4849,
        "grad_norm": 3.913381576538086,
        "learning_rate": 4.592156864856497e-05,
        "epoch": 0.5615917728593784,
        "step": 7536
    },
    {
        "loss": 1.9769,
        "grad_norm": 2.875396251678467,
        "learning_rate": 4.586239316250426e-05,
        "epoch": 0.5616662940606603,
        "step": 7537
    },
    {
        "loss": 2.8587,
        "grad_norm": 3.6488747596740723,
        "learning_rate": 4.580324448174531e-05,
        "epoch": 0.561740815261942,
        "step": 7538
    },
    {
        "loss": 1.6757,
        "grad_norm": 5.030403137207031,
        "learning_rate": 4.574412263557453e-05,
        "epoch": 0.5618153364632238,
        "step": 7539
    },
    {
        "loss": 2.563,
        "grad_norm": 2.430037498474121,
        "learning_rate": 4.5685027653265264e-05,
        "epoch": 0.5618898576645055,
        "step": 7540
    },
    {
        "loss": 2.3272,
        "grad_norm": 3.248859405517578,
        "learning_rate": 4.562595956407725e-05,
        "epoch": 0.5619643788657873,
        "step": 7541
    },
    {
        "loss": 2.5644,
        "grad_norm": 2.4811277389526367,
        "learning_rate": 4.5566918397257105e-05,
        "epoch": 0.5620389000670691,
        "step": 7542
    },
    {
        "loss": 2.3383,
        "grad_norm": 3.8791496753692627,
        "learning_rate": 4.550790418203801e-05,
        "epoch": 0.5621134212683508,
        "step": 7543
    },
    {
        "loss": 1.909,
        "grad_norm": 2.238177537918091,
        "learning_rate": 4.544891694763974e-05,
        "epoch": 0.5621879424696327,
        "step": 7544
    },
    {
        "loss": 2.4053,
        "grad_norm": 2.9112708568573,
        "learning_rate": 4.538995672326902e-05,
        "epoch": 0.5622624636709144,
        "step": 7545
    },
    {
        "loss": 1.5645,
        "grad_norm": 2.5985655784606934,
        "learning_rate": 4.533102353811871e-05,
        "epoch": 0.5623369848721962,
        "step": 7546
    },
    {
        "loss": 2.7238,
        "grad_norm": 2.6072070598602295,
        "learning_rate": 4.527211742136882e-05,
        "epoch": 0.5624115060734779,
        "step": 7547
    },
    {
        "loss": 2.0958,
        "grad_norm": 3.53995418548584,
        "learning_rate": 4.5213238402185595e-05,
        "epoch": 0.5624860272747597,
        "step": 7548
    },
    {
        "loss": 1.9558,
        "grad_norm": 2.7779428958892822,
        "learning_rate": 4.515438650972185e-05,
        "epoch": 0.5625605484760414,
        "step": 7549
    },
    {
        "loss": 2.6849,
        "grad_norm": 1.8383831977844238,
        "learning_rate": 4.5095561773117304e-05,
        "epoch": 0.5626350696773232,
        "step": 7550
    },
    {
        "loss": 1.6886,
        "grad_norm": 3.5841448307037354,
        "learning_rate": 4.503676422149786e-05,
        "epoch": 0.562709590878605,
        "step": 7551
    },
    {
        "loss": 2.7685,
        "grad_norm": 3.1845526695251465,
        "learning_rate": 4.497799388397619e-05,
        "epoch": 0.5627841120798868,
        "step": 7552
    },
    {
        "loss": 2.4567,
        "grad_norm": 2.954044818878174,
        "learning_rate": 4.4919250789651376e-05,
        "epoch": 0.5628586332811685,
        "step": 7553
    },
    {
        "loss": 2.0042,
        "grad_norm": 3.6647109985351562,
        "learning_rate": 4.486053496760901e-05,
        "epoch": 0.5629331544824503,
        "step": 7554
    },
    {
        "loss": 2.5811,
        "grad_norm": 2.004037857055664,
        "learning_rate": 4.480184644692129e-05,
        "epoch": 0.563007675683732,
        "step": 7555
    },
    {
        "loss": 2.545,
        "grad_norm": 3.655076503753662,
        "learning_rate": 4.474318525664679e-05,
        "epoch": 0.5630821968850138,
        "step": 7556
    },
    {
        "loss": 2.5146,
        "grad_norm": 2.1653058528900146,
        "learning_rate": 4.468455142583047e-05,
        "epoch": 0.5631567180862955,
        "step": 7557
    },
    {
        "loss": 2.8519,
        "grad_norm": 2.032771587371826,
        "learning_rate": 4.462594498350407e-05,
        "epoch": 0.5632312392875773,
        "step": 7558
    },
    {
        "loss": 2.5727,
        "grad_norm": 3.718775987625122,
        "learning_rate": 4.456736595868538e-05,
        "epoch": 0.563305760488859,
        "step": 7559
    },
    {
        "loss": 2.1993,
        "grad_norm": 2.7929513454437256,
        "learning_rate": 4.4508814380379005e-05,
        "epoch": 0.5633802816901409,
        "step": 7560
    },
    {
        "loss": 2.4695,
        "grad_norm": 2.9174671173095703,
        "learning_rate": 4.445029027757558e-05,
        "epoch": 0.5634548028914226,
        "step": 7561
    },
    {
        "loss": 2.2861,
        "grad_norm": 2.820453643798828,
        "learning_rate": 4.439179367925228e-05,
        "epoch": 0.5635293240927044,
        "step": 7562
    },
    {
        "loss": 2.5312,
        "grad_norm": 3.200284957885742,
        "learning_rate": 4.433332461437285e-05,
        "epoch": 0.5636038452939861,
        "step": 7563
    },
    {
        "loss": 2.4753,
        "grad_norm": 2.413668632507324,
        "learning_rate": 4.4274883111887034e-05,
        "epoch": 0.5636783664952679,
        "step": 7564
    },
    {
        "loss": 2.5496,
        "grad_norm": 2.2832894325256348,
        "learning_rate": 4.421646920073131e-05,
        "epoch": 0.5637528876965496,
        "step": 7565
    },
    {
        "loss": 1.8096,
        "grad_norm": 2.3275070190429688,
        "learning_rate": 4.415808290982826e-05,
        "epoch": 0.5638274088978315,
        "step": 7566
    },
    {
        "loss": 1.1712,
        "grad_norm": 3.3781251907348633,
        "learning_rate": 4.409972426808674e-05,
        "epoch": 0.5639019300991132,
        "step": 7567
    },
    {
        "loss": 2.8979,
        "grad_norm": 2.593296766281128,
        "learning_rate": 4.4041393304402204e-05,
        "epoch": 0.563976451300395,
        "step": 7568
    },
    {
        "loss": 2.5004,
        "grad_norm": 3.9856884479522705,
        "learning_rate": 4.398309004765602e-05,
        "epoch": 0.5640509725016767,
        "step": 7569
    },
    {
        "loss": 1.5777,
        "grad_norm": 2.483421802520752,
        "learning_rate": 4.3924814526716266e-05,
        "epoch": 0.5641254937029585,
        "step": 7570
    },
    {
        "loss": 2.1489,
        "grad_norm": 2.189675807952881,
        "learning_rate": 4.3866566770436955e-05,
        "epoch": 0.5642000149042402,
        "step": 7571
    },
    {
        "loss": 1.8507,
        "grad_norm": 4.8456597328186035,
        "learning_rate": 4.3808346807658415e-05,
        "epoch": 0.564274536105522,
        "step": 7572
    },
    {
        "loss": 2.0348,
        "grad_norm": 3.762328863143921,
        "learning_rate": 4.375015466720733e-05,
        "epoch": 0.5643490573068037,
        "step": 7573
    },
    {
        "loss": 2.7695,
        "grad_norm": 3.3194613456726074,
        "learning_rate": 4.369199037789643e-05,
        "epoch": 0.5644235785080856,
        "step": 7574
    },
    {
        "loss": 2.9119,
        "grad_norm": 2.1362850666046143,
        "learning_rate": 4.3633853968524943e-05,
        "epoch": 0.5644980997093673,
        "step": 7575
    },
    {
        "loss": 3.0371,
        "grad_norm": 1.9302241802215576,
        "learning_rate": 4.357574546787795e-05,
        "epoch": 0.5645726209106491,
        "step": 7576
    },
    {
        "loss": 2.0771,
        "grad_norm": 3.4722118377685547,
        "learning_rate": 4.351766490472681e-05,
        "epoch": 0.5646471421119309,
        "step": 7577
    },
    {
        "loss": 2.0665,
        "grad_norm": 2.573319435119629,
        "learning_rate": 4.345961230782932e-05,
        "epoch": 0.5647216633132126,
        "step": 7578
    },
    {
        "loss": 2.5773,
        "grad_norm": 2.5202338695526123,
        "learning_rate": 4.340158770592903e-05,
        "epoch": 0.5647961845144944,
        "step": 7579
    },
    {
        "loss": 2.4211,
        "grad_norm": 2.7546541690826416,
        "learning_rate": 4.3343591127756e-05,
        "epoch": 0.5648707057157761,
        "step": 7580
    },
    {
        "loss": 2.9707,
        "grad_norm": 3.011159896850586,
        "learning_rate": 4.328562260202616e-05,
        "epoch": 0.564945226917058,
        "step": 7581
    },
    {
        "loss": 2.2503,
        "grad_norm": 3.073880195617676,
        "learning_rate": 4.322768215744156e-05,
        "epoch": 0.5650197481183397,
        "step": 7582
    },
    {
        "loss": 2.095,
        "grad_norm": 3.729440689086914,
        "learning_rate": 4.31697698226905e-05,
        "epoch": 0.5650942693196215,
        "step": 7583
    },
    {
        "loss": 2.3415,
        "grad_norm": 3.57372784614563,
        "learning_rate": 4.31118856264472e-05,
        "epoch": 0.5651687905209032,
        "step": 7584
    },
    {
        "loss": 1.8685,
        "grad_norm": 3.0130844116210938,
        "learning_rate": 4.3054029597372104e-05,
        "epoch": 0.565243311722185,
        "step": 7585
    },
    {
        "loss": 2.5031,
        "grad_norm": 2.670563220977783,
        "learning_rate": 4.299620176411157e-05,
        "epoch": 0.5653178329234667,
        "step": 7586
    },
    {
        "loss": 2.7701,
        "grad_norm": 2.9065756797790527,
        "learning_rate": 4.293840215529794e-05,
        "epoch": 0.5653923541247485,
        "step": 7587
    },
    {
        "loss": 2.6491,
        "grad_norm": 1.8023582696914673,
        "learning_rate": 4.288063079954994e-05,
        "epoch": 0.5654668753260302,
        "step": 7588
    },
    {
        "loss": 2.7374,
        "grad_norm": 5.7076029777526855,
        "learning_rate": 4.282288772547192e-05,
        "epoch": 0.5655413965273121,
        "step": 7589
    },
    {
        "loss": 1.6431,
        "grad_norm": 3.349886178970337,
        "learning_rate": 4.276517296165431e-05,
        "epoch": 0.5656159177285938,
        "step": 7590
    },
    {
        "loss": 2.5583,
        "grad_norm": 1.6925604343414307,
        "learning_rate": 4.2707486536673725e-05,
        "epoch": 0.5656904389298756,
        "step": 7591
    },
    {
        "loss": 1.8521,
        "grad_norm": 3.2394001483917236,
        "learning_rate": 4.264982847909245e-05,
        "epoch": 0.5657649601311573,
        "step": 7592
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.004591703414917,
        "learning_rate": 4.2592198817459025e-05,
        "epoch": 0.5658394813324391,
        "step": 7593
    },
    {
        "loss": 1.9646,
        "grad_norm": 3.6830642223358154,
        "learning_rate": 4.253459758030776e-05,
        "epoch": 0.5659140025337208,
        "step": 7594
    },
    {
        "loss": 1.7357,
        "grad_norm": 4.073303699493408,
        "learning_rate": 4.247702479615874e-05,
        "epoch": 0.5659885237350026,
        "step": 7595
    },
    {
        "loss": 2.2888,
        "grad_norm": 4.556748867034912,
        "learning_rate": 4.241948049351834e-05,
        "epoch": 0.5660630449362843,
        "step": 7596
    },
    {
        "loss": 2.6079,
        "grad_norm": 1.922467827796936,
        "learning_rate": 4.236196470087844e-05,
        "epoch": 0.5661375661375662,
        "step": 7597
    },
    {
        "loss": 1.4436,
        "grad_norm": 3.4980955123901367,
        "learning_rate": 4.230447744671724e-05,
        "epoch": 0.5662120873388479,
        "step": 7598
    },
    {
        "loss": 2.5115,
        "grad_norm": 2.3263988494873047,
        "learning_rate": 4.2247018759498394e-05,
        "epoch": 0.5662866085401297,
        "step": 7599
    },
    {
        "loss": 2.583,
        "grad_norm": 2.054882764816284,
        "learning_rate": 4.21895886676715e-05,
        "epoch": 0.5663611297414114,
        "step": 7600
    },
    {
        "loss": 1.5746,
        "grad_norm": 3.8060617446899414,
        "learning_rate": 4.213218719967229e-05,
        "epoch": 0.5664356509426932,
        "step": 7601
    },
    {
        "loss": 2.3778,
        "grad_norm": 2.064589738845825,
        "learning_rate": 4.207481438392198e-05,
        "epoch": 0.5665101721439749,
        "step": 7602
    },
    {
        "loss": 2.2531,
        "grad_norm": 2.579514980316162,
        "learning_rate": 4.201747024882779e-05,
        "epoch": 0.5665846933452567,
        "step": 7603
    },
    {
        "loss": 2.6231,
        "grad_norm": 3.3021931648254395,
        "learning_rate": 4.1960154822782624e-05,
        "epoch": 0.5666592145465384,
        "step": 7604
    },
    {
        "loss": 2.0778,
        "grad_norm": 2.9323151111602783,
        "learning_rate": 4.190286813416523e-05,
        "epoch": 0.5667337357478203,
        "step": 7605
    },
    {
        "loss": 2.3859,
        "grad_norm": 3.214381456375122,
        "learning_rate": 4.184561021134017e-05,
        "epoch": 0.566808256949102,
        "step": 7606
    },
    {
        "loss": 1.9951,
        "grad_norm": 2.1201376914978027,
        "learning_rate": 4.178838108265758e-05,
        "epoch": 0.5668827781503838,
        "step": 7607
    },
    {
        "loss": 2.4833,
        "grad_norm": 3.403378486633301,
        "learning_rate": 4.173118077645368e-05,
        "epoch": 0.5669572993516655,
        "step": 7608
    },
    {
        "loss": 2.4183,
        "grad_norm": 2.447404623031616,
        "learning_rate": 4.167400932105013e-05,
        "epoch": 0.5670318205529473,
        "step": 7609
    },
    {
        "loss": 2.5745,
        "grad_norm": 2.5978550910949707,
        "learning_rate": 4.1616866744754266e-05,
        "epoch": 0.567106341754229,
        "step": 7610
    },
    {
        "loss": 2.3475,
        "grad_norm": 2.471874713897705,
        "learning_rate": 4.1559753075859464e-05,
        "epoch": 0.5671808629555108,
        "step": 7611
    },
    {
        "loss": 2.6771,
        "grad_norm": 2.474123477935791,
        "learning_rate": 4.150266834264442e-05,
        "epoch": 0.5672553841567926,
        "step": 7612
    },
    {
        "loss": 2.4676,
        "grad_norm": 1.6289806365966797,
        "learning_rate": 4.144561257337375e-05,
        "epoch": 0.5673299053580744,
        "step": 7613
    },
    {
        "loss": 2.4664,
        "grad_norm": 4.25106954574585,
        "learning_rate": 4.138858579629758e-05,
        "epoch": 0.5674044265593562,
        "step": 7614
    },
    {
        "loss": 2.4343,
        "grad_norm": 2.0434012413024902,
        "learning_rate": 4.13315880396517e-05,
        "epoch": 0.5674789477606379,
        "step": 7615
    },
    {
        "loss": 2.5922,
        "grad_norm": 2.4290828704833984,
        "learning_rate": 4.1274619331657646e-05,
        "epoch": 0.5675534689619197,
        "step": 7616
    },
    {
        "loss": 2.1766,
        "grad_norm": 2.350538969039917,
        "learning_rate": 4.121767970052245e-05,
        "epoch": 0.5676279901632014,
        "step": 7617
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.624271035194397,
        "learning_rate": 4.116076917443867e-05,
        "epoch": 0.5677025113644832,
        "step": 7618
    },
    {
        "loss": 2.5393,
        "grad_norm": 2.6045141220092773,
        "learning_rate": 4.1103887781584814e-05,
        "epoch": 0.567777032565765,
        "step": 7619
    },
    {
        "loss": 2.6912,
        "grad_norm": 3.5381698608398438,
        "learning_rate": 4.104703555012447e-05,
        "epoch": 0.5678515537670468,
        "step": 7620
    },
    {
        "loss": 2.1559,
        "grad_norm": 2.121833086013794,
        "learning_rate": 4.099021250820731e-05,
        "epoch": 0.5679260749683285,
        "step": 7621
    },
    {
        "loss": 2.146,
        "grad_norm": 2.3977887630462646,
        "learning_rate": 4.093341868396812e-05,
        "epoch": 0.5680005961696103,
        "step": 7622
    },
    {
        "loss": 2.2989,
        "grad_norm": 2.4525299072265625,
        "learning_rate": 4.087665410552731e-05,
        "epoch": 0.568075117370892,
        "step": 7623
    },
    {
        "loss": 2.9041,
        "grad_norm": 2.0708014965057373,
        "learning_rate": 4.0819918800991083e-05,
        "epoch": 0.5681496385721738,
        "step": 7624
    },
    {
        "loss": 2.0668,
        "grad_norm": 3.1349315643310547,
        "learning_rate": 4.076321279845073e-05,
        "epoch": 0.5682241597734555,
        "step": 7625
    },
    {
        "loss": 2.5669,
        "grad_norm": 3.340707302093506,
        "learning_rate": 4.070653612598343e-05,
        "epoch": 0.5682986809747373,
        "step": 7626
    },
    {
        "loss": 1.8118,
        "grad_norm": 3.8800647258758545,
        "learning_rate": 4.0649888811651614e-05,
        "epoch": 0.568373202176019,
        "step": 7627
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.0181655883789062,
        "learning_rate": 4.059327088350304e-05,
        "epoch": 0.5684477233773009,
        "step": 7628
    },
    {
        "loss": 1.8997,
        "grad_norm": 3.3596031665802,
        "learning_rate": 4.0536682369571376e-05,
        "epoch": 0.5685222445785826,
        "step": 7629
    },
    {
        "loss": 2.9707,
        "grad_norm": 4.09245491027832,
        "learning_rate": 4.0480123297875204e-05,
        "epoch": 0.5685967657798644,
        "step": 7630
    },
    {
        "loss": 2.4644,
        "grad_norm": 3.503380537033081,
        "learning_rate": 4.0423593696418995e-05,
        "epoch": 0.5686712869811461,
        "step": 7631
    },
    {
        "loss": 2.2863,
        "grad_norm": 2.09527325630188,
        "learning_rate": 4.036709359319231e-05,
        "epoch": 0.5687458081824279,
        "step": 7632
    },
    {
        "loss": 2.5925,
        "grad_norm": 2.3418283462524414,
        "learning_rate": 4.0310623016170146e-05,
        "epoch": 0.5688203293837096,
        "step": 7633
    },
    {
        "loss": 2.4013,
        "grad_norm": 1.8084965944290161,
        "learning_rate": 4.025418199331302e-05,
        "epoch": 0.5688948505849915,
        "step": 7634
    },
    {
        "loss": 2.8112,
        "grad_norm": 2.8922207355499268,
        "learning_rate": 4.019777055256665e-05,
        "epoch": 0.5689693717862732,
        "step": 7635
    },
    {
        "loss": 2.1801,
        "grad_norm": 4.337101936340332,
        "learning_rate": 4.014138872186227e-05,
        "epoch": 0.569043892987555,
        "step": 7636
    },
    {
        "loss": 2.9301,
        "grad_norm": 3.3789937496185303,
        "learning_rate": 4.008503652911633e-05,
        "epoch": 0.5691184141888367,
        "step": 7637
    },
    {
        "loss": 2.656,
        "grad_norm": 1.781812071800232,
        "learning_rate": 4.0028714002230526e-05,
        "epoch": 0.5691929353901185,
        "step": 7638
    },
    {
        "loss": 2.1045,
        "grad_norm": 3.128993034362793,
        "learning_rate": 3.997242116909222e-05,
        "epoch": 0.5692674565914002,
        "step": 7639
    },
    {
        "loss": 2.333,
        "grad_norm": 2.45184588432312,
        "learning_rate": 3.9916158057573616e-05,
        "epoch": 0.569341977792682,
        "step": 7640
    },
    {
        "loss": 2.0084,
        "grad_norm": 3.440314292907715,
        "learning_rate": 3.985992469553263e-05,
        "epoch": 0.5694164989939637,
        "step": 7641
    },
    {
        "loss": 2.0751,
        "grad_norm": 3.350727081298828,
        "learning_rate": 3.9803721110812134e-05,
        "epoch": 0.5694910201952456,
        "step": 7642
    },
    {
        "loss": 2.8147,
        "grad_norm": 3.205451488494873,
        "learning_rate": 3.974754733124033e-05,
        "epoch": 0.5695655413965273,
        "step": 7643
    },
    {
        "loss": 2.6467,
        "grad_norm": 1.872437596321106,
        "learning_rate": 3.9691403384630776e-05,
        "epoch": 0.5696400625978091,
        "step": 7644
    },
    {
        "loss": 2.5291,
        "grad_norm": 2.944681167602539,
        "learning_rate": 3.9635289298782097e-05,
        "epoch": 0.5697145837990908,
        "step": 7645
    },
    {
        "loss": 2.9547,
        "grad_norm": 2.020923137664795,
        "learning_rate": 3.9579205101478277e-05,
        "epoch": 0.5697891050003726,
        "step": 7646
    },
    {
        "loss": 2.8323,
        "grad_norm": 2.7911908626556396,
        "learning_rate": 3.952315082048843e-05,
        "epoch": 0.5698636262016543,
        "step": 7647
    },
    {
        "loss": 1.7436,
        "grad_norm": 4.186251163482666,
        "learning_rate": 3.9467126483566744e-05,
        "epoch": 0.5699381474029361,
        "step": 7648
    },
    {
        "loss": 3.0122,
        "grad_norm": 2.476475954055786,
        "learning_rate": 3.9411132118452896e-05,
        "epoch": 0.570012668604218,
        "step": 7649
    },
    {
        "loss": 1.789,
        "grad_norm": 4.7351226806640625,
        "learning_rate": 3.935516775287149e-05,
        "epoch": 0.5700871898054997,
        "step": 7650
    },
    {
        "loss": 2.0698,
        "grad_norm": 2.4163994789123535,
        "learning_rate": 3.929923341453213e-05,
        "epoch": 0.5701617110067815,
        "step": 7651
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.638599157333374,
        "learning_rate": 3.924332913112998e-05,
        "epoch": 0.5702362322080632,
        "step": 7652
    },
    {
        "loss": 2.1648,
        "grad_norm": 2.913367748260498,
        "learning_rate": 3.9187454930344904e-05,
        "epoch": 0.570310753409345,
        "step": 7653
    },
    {
        "loss": 2.6598,
        "grad_norm": 2.862161636352539,
        "learning_rate": 3.9131610839842224e-05,
        "epoch": 0.5703852746106267,
        "step": 7654
    },
    {
        "loss": 1.8431,
        "grad_norm": 3.053663492202759,
        "learning_rate": 3.9075796887272046e-05,
        "epoch": 0.5704597958119085,
        "step": 7655
    },
    {
        "loss": 2.6887,
        "grad_norm": 1.8349432945251465,
        "learning_rate": 3.902001310026964e-05,
        "epoch": 0.5705343170131902,
        "step": 7656
    },
    {
        "loss": 2.1193,
        "grad_norm": 3.017366647720337,
        "learning_rate": 3.896425950645554e-05,
        "epoch": 0.5706088382144721,
        "step": 7657
    },
    {
        "loss": 2.7208,
        "grad_norm": 2.327667474746704,
        "learning_rate": 3.890853613343501e-05,
        "epoch": 0.5706833594157538,
        "step": 7658
    },
    {
        "loss": 2.3447,
        "grad_norm": 2.5575711727142334,
        "learning_rate": 3.8852843008798686e-05,
        "epoch": 0.5707578806170356,
        "step": 7659
    },
    {
        "loss": 2.4846,
        "grad_norm": 1.9397674798965454,
        "learning_rate": 3.8797180160121984e-05,
        "epoch": 0.5708324018183173,
        "step": 7660
    },
    {
        "loss": 2.2607,
        "grad_norm": 2.504826307296753,
        "learning_rate": 3.874154761496527e-05,
        "epoch": 0.5709069230195991,
        "step": 7661
    },
    {
        "loss": 2.6186,
        "grad_norm": 2.5900774002075195,
        "learning_rate": 3.868594540087428e-05,
        "epoch": 0.5709814442208808,
        "step": 7662
    },
    {
        "loss": 1.5702,
        "grad_norm": 3.4126036167144775,
        "learning_rate": 3.863037354537933e-05,
        "epoch": 0.5710559654221626,
        "step": 7663
    },
    {
        "loss": 2.5084,
        "grad_norm": 4.357270240783691,
        "learning_rate": 3.857483207599595e-05,
        "epoch": 0.5711304866234443,
        "step": 7664
    },
    {
        "loss": 1.8629,
        "grad_norm": 3.446943759918213,
        "learning_rate": 3.8519321020224483e-05,
        "epoch": 0.5712050078247262,
        "step": 7665
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.8451924324035645,
        "learning_rate": 3.8463840405550267e-05,
        "epoch": 0.5712795290260079,
        "step": 7666
    },
    {
        "loss": 2.6568,
        "grad_norm": 3.273545265197754,
        "learning_rate": 3.8408390259443626e-05,
        "epoch": 0.5713540502272897,
        "step": 7667
    },
    {
        "loss": 2.1073,
        "grad_norm": 3.3503499031066895,
        "learning_rate": 3.83529706093596e-05,
        "epoch": 0.5714285714285714,
        "step": 7668
    },
    {
        "loss": 2.334,
        "grad_norm": 1.8833986520767212,
        "learning_rate": 3.8297581482738475e-05,
        "epoch": 0.5715030926298532,
        "step": 7669
    },
    {
        "loss": 2.5763,
        "grad_norm": 2.3581016063690186,
        "learning_rate": 3.8242222907005134e-05,
        "epoch": 0.5715776138311349,
        "step": 7670
    },
    {
        "loss": 2.6038,
        "grad_norm": 3.128244400024414,
        "learning_rate": 3.8186894909569304e-05,
        "epoch": 0.5716521350324167,
        "step": 7671
    },
    {
        "loss": 2.5682,
        "grad_norm": 2.656867027282715,
        "learning_rate": 3.81315975178259e-05,
        "epoch": 0.5717266562336984,
        "step": 7672
    },
    {
        "loss": 2.8985,
        "grad_norm": 1.5239033699035645,
        "learning_rate": 3.807633075915433e-05,
        "epoch": 0.5718011774349803,
        "step": 7673
    },
    {
        "loss": 2.3041,
        "grad_norm": 2.5025320053100586,
        "learning_rate": 3.802109466091906e-05,
        "epoch": 0.571875698636262,
        "step": 7674
    },
    {
        "loss": 2.7857,
        "grad_norm": 2.51830792427063,
        "learning_rate": 3.796588925046923e-05,
        "epoch": 0.5719502198375438,
        "step": 7675
    },
    {
        "loss": 2.5463,
        "grad_norm": 2.6151185035705566,
        "learning_rate": 3.7910714555138835e-05,
        "epoch": 0.5720247410388255,
        "step": 7676
    },
    {
        "loss": 2.0239,
        "grad_norm": 3.0587377548217773,
        "learning_rate": 3.7855570602246736e-05,
        "epoch": 0.5720992622401073,
        "step": 7677
    },
    {
        "loss": 3.088,
        "grad_norm": 2.889151096343994,
        "learning_rate": 3.780045741909636e-05,
        "epoch": 0.572173783441389,
        "step": 7678
    },
    {
        "loss": 1.8334,
        "grad_norm": 3.676097869873047,
        "learning_rate": 3.774537503297627e-05,
        "epoch": 0.5722483046426708,
        "step": 7679
    },
    {
        "loss": 2.7716,
        "grad_norm": 2.479701280593872,
        "learning_rate": 3.7690323471159485e-05,
        "epoch": 0.5723228258439526,
        "step": 7680
    },
    {
        "loss": 2.2137,
        "grad_norm": 3.105543375015259,
        "learning_rate": 3.7635302760903656e-05,
        "epoch": 0.5723973470452344,
        "step": 7681
    },
    {
        "loss": 1.9154,
        "grad_norm": 3.7476260662078857,
        "learning_rate": 3.75803129294516e-05,
        "epoch": 0.5724718682465161,
        "step": 7682
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.6379283666610718,
        "learning_rate": 3.752535400403045e-05,
        "epoch": 0.5725463894477979,
        "step": 7683
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.6232147216796875,
        "learning_rate": 3.7470426011852136e-05,
        "epoch": 0.5726209106490797,
        "step": 7684
    },
    {
        "loss": 2.8215,
        "grad_norm": 3.0321972370147705,
        "learning_rate": 3.741552898011344e-05,
        "epoch": 0.5726954318503614,
        "step": 7685
    },
    {
        "loss": 2.1223,
        "grad_norm": 3.9997928142547607,
        "learning_rate": 3.73606629359955e-05,
        "epoch": 0.5727699530516432,
        "step": 7686
    },
    {
        "loss": 1.9026,
        "grad_norm": 2.9707024097442627,
        "learning_rate": 3.730582790666446e-05,
        "epoch": 0.572844474252925,
        "step": 7687
    },
    {
        "loss": 1.5883,
        "grad_norm": 4.275876998901367,
        "learning_rate": 3.725102391927089e-05,
        "epoch": 0.5729189954542068,
        "step": 7688
    },
    {
        "loss": 2.703,
        "grad_norm": 3.0534567832946777,
        "learning_rate": 3.719625100094988e-05,
        "epoch": 0.5729935166554885,
        "step": 7689
    },
    {
        "loss": 2.5367,
        "grad_norm": 3.885307788848877,
        "learning_rate": 3.714150917882153e-05,
        "epoch": 0.5730680378567703,
        "step": 7690
    },
    {
        "loss": 1.8046,
        "grad_norm": 5.762183666229248,
        "learning_rate": 3.708679847999011e-05,
        "epoch": 0.573142559058052,
        "step": 7691
    },
    {
        "loss": 2.0496,
        "grad_norm": 3.328047275543213,
        "learning_rate": 3.703211893154488e-05,
        "epoch": 0.5732170802593338,
        "step": 7692
    },
    {
        "loss": 2.0959,
        "grad_norm": 2.6904449462890625,
        "learning_rate": 3.697747056055937e-05,
        "epoch": 0.5732916014606155,
        "step": 7693
    },
    {
        "loss": 2.7005,
        "grad_norm": 2.734510898590088,
        "learning_rate": 3.692285339409174e-05,
        "epoch": 0.5733661226618973,
        "step": 7694
    },
    {
        "loss": 1.9442,
        "grad_norm": 5.107265472412109,
        "learning_rate": 3.6868267459184804e-05,
        "epoch": 0.5734406438631791,
        "step": 7695
    },
    {
        "loss": 2.556,
        "grad_norm": 2.1988773345947266,
        "learning_rate": 3.681371278286579e-05,
        "epoch": 0.5735151650644609,
        "step": 7696
    },
    {
        "loss": 1.8087,
        "grad_norm": 3.488227128982544,
        "learning_rate": 3.675918939214656e-05,
        "epoch": 0.5735896862657426,
        "step": 7697
    },
    {
        "loss": 2.2199,
        "grad_norm": 2.515324115753174,
        "learning_rate": 3.670469731402342e-05,
        "epoch": 0.5736642074670244,
        "step": 7698
    },
    {
        "loss": 2.0597,
        "grad_norm": 2.0729541778564453,
        "learning_rate": 3.6650236575477046e-05,
        "epoch": 0.5737387286683061,
        "step": 7699
    },
    {
        "loss": 2.7707,
        "grad_norm": 4.1549506187438965,
        "learning_rate": 3.659580720347295e-05,
        "epoch": 0.5738132498695879,
        "step": 7700
    },
    {
        "loss": 2.4873,
        "grad_norm": 2.1121153831481934,
        "learning_rate": 3.654140922496071e-05,
        "epoch": 0.5738877710708696,
        "step": 7701
    },
    {
        "loss": 1.9647,
        "grad_norm": 2.705227851867676,
        "learning_rate": 3.648704266687474e-05,
        "epoch": 0.5739622922721515,
        "step": 7702
    },
    {
        "loss": 2.3583,
        "grad_norm": 3.352931261062622,
        "learning_rate": 3.643270755613362e-05,
        "epoch": 0.5740368134734332,
        "step": 7703
    },
    {
        "loss": 2.4257,
        "grad_norm": 4.304421901702881,
        "learning_rate": 3.637840391964031e-05,
        "epoch": 0.574111334674715,
        "step": 7704
    },
    {
        "loss": 1.8225,
        "grad_norm": 3.0278100967407227,
        "learning_rate": 3.632413178428251e-05,
        "epoch": 0.5741858558759967,
        "step": 7705
    },
    {
        "loss": 2.768,
        "grad_norm": 3.0471224784851074,
        "learning_rate": 3.6269891176932015e-05,
        "epoch": 0.5742603770772785,
        "step": 7706
    },
    {
        "loss": 1.8039,
        "grad_norm": 3.81343674659729,
        "learning_rate": 3.621568212444518e-05,
        "epoch": 0.5743348982785602,
        "step": 7707
    },
    {
        "loss": 2.3145,
        "grad_norm": 2.292450428009033,
        "learning_rate": 3.616150465366267e-05,
        "epoch": 0.574409419479842,
        "step": 7708
    },
    {
        "loss": 2.5672,
        "grad_norm": 2.2443554401397705,
        "learning_rate": 3.6107358791409416e-05,
        "epoch": 0.5744839406811237,
        "step": 7709
    },
    {
        "loss": 2.5707,
        "grad_norm": 2.7767302989959717,
        "learning_rate": 3.6053244564495014e-05,
        "epoch": 0.5745584618824056,
        "step": 7710
    },
    {
        "loss": 2.0133,
        "grad_norm": 3.4868154525756836,
        "learning_rate": 3.5999161999713115e-05,
        "epoch": 0.5746329830836873,
        "step": 7711
    },
    {
        "loss": 2.0565,
        "grad_norm": 2.7972209453582764,
        "learning_rate": 3.59451111238416e-05,
        "epoch": 0.5747075042849691,
        "step": 7712
    },
    {
        "loss": 2.2152,
        "grad_norm": 2.775545835494995,
        "learning_rate": 3.5891091963643066e-05,
        "epoch": 0.5747820254862508,
        "step": 7713
    },
    {
        "loss": 2.4175,
        "grad_norm": 2.622913360595703,
        "learning_rate": 3.5837104545864e-05,
        "epoch": 0.5748565466875326,
        "step": 7714
    },
    {
        "loss": 2.4188,
        "grad_norm": 3.9989516735076904,
        "learning_rate": 3.578314889723551e-05,
        "epoch": 0.5749310678888143,
        "step": 7715
    },
    {
        "loss": 2.8022,
        "grad_norm": 2.4835803508758545,
        "learning_rate": 3.572922504447266e-05,
        "epoch": 0.5750055890900961,
        "step": 7716
    },
    {
        "loss": 1.7525,
        "grad_norm": 2.530395030975342,
        "learning_rate": 3.567533301427488e-05,
        "epoch": 0.5750801102913778,
        "step": 7717
    },
    {
        "loss": 1.8338,
        "grad_norm": 1.9634687900543213,
        "learning_rate": 3.562147283332608e-05,
        "epoch": 0.5751546314926597,
        "step": 7718
    },
    {
        "loss": 2.4793,
        "grad_norm": 2.9167089462280273,
        "learning_rate": 3.556764452829392e-05,
        "epoch": 0.5752291526939415,
        "step": 7719
    },
    {
        "loss": 2.9375,
        "grad_norm": 1.917934536933899,
        "learning_rate": 3.551384812583082e-05,
        "epoch": 0.5753036738952232,
        "step": 7720
    },
    {
        "loss": 2.8341,
        "grad_norm": 1.8118866682052612,
        "learning_rate": 3.546008365257302e-05,
        "epoch": 0.575378195096505,
        "step": 7721
    },
    {
        "loss": 1.827,
        "grad_norm": 4.104218006134033,
        "learning_rate": 3.540635113514099e-05,
        "epoch": 0.5754527162977867,
        "step": 7722
    },
    {
        "loss": 2.0465,
        "grad_norm": Infinity,
        "learning_rate": 3.540635113514099e-05,
        "epoch": 0.5755272374990685,
        "step": 7723
    },
    {
        "loss": 1.9665,
        "grad_norm": 3.1129345893859863,
        "learning_rate": 3.535265060013965e-05,
        "epoch": 0.5756017587003502,
        "step": 7724
    },
    {
        "loss": 2.1099,
        "grad_norm": 3.4874324798583984,
        "learning_rate": 3.529898207415775e-05,
        "epoch": 0.5756762799016321,
        "step": 7725
    },
    {
        "loss": 2.2687,
        "grad_norm": 2.8878567218780518,
        "learning_rate": 3.524534558376842e-05,
        "epoch": 0.5757508011029138,
        "step": 7726
    },
    {
        "loss": 2.6442,
        "grad_norm": 4.435276508331299,
        "learning_rate": 3.5191741155528804e-05,
        "epoch": 0.5758253223041956,
        "step": 7727
    },
    {
        "loss": 1.9842,
        "grad_norm": 3.4364547729492188,
        "learning_rate": 3.5138168815980165e-05,
        "epoch": 0.5758998435054773,
        "step": 7728
    },
    {
        "loss": 2.7188,
        "grad_norm": 2.036355972290039,
        "learning_rate": 3.5084628591648003e-05,
        "epoch": 0.5759743647067591,
        "step": 7729
    },
    {
        "loss": 2.5587,
        "grad_norm": 3.7333476543426514,
        "learning_rate": 3.50311205090417e-05,
        "epoch": 0.5760488859080408,
        "step": 7730
    },
    {
        "loss": 1.7597,
        "grad_norm": 3.30987286567688,
        "learning_rate": 3.497764459465506e-05,
        "epoch": 0.5761234071093226,
        "step": 7731
    },
    {
        "loss": 2.1413,
        "grad_norm": 2.3854260444641113,
        "learning_rate": 3.492420087496564e-05,
        "epoch": 0.5761979283106043,
        "step": 7732
    },
    {
        "loss": 2.4927,
        "grad_norm": 1.98861825466156,
        "learning_rate": 3.48707893764351e-05,
        "epoch": 0.5762724495118862,
        "step": 7733
    },
    {
        "loss": 2.8756,
        "grad_norm": 2.2541098594665527,
        "learning_rate": 3.481741012550941e-05,
        "epoch": 0.5763469707131679,
        "step": 7734
    },
    {
        "loss": 2.7385,
        "grad_norm": 2.922999143600464,
        "learning_rate": 3.476406314861827e-05,
        "epoch": 0.5764214919144497,
        "step": 7735
    },
    {
        "loss": 2.3669,
        "grad_norm": 3.1656579971313477,
        "learning_rate": 3.4710748472175546e-05,
        "epoch": 0.5764960131157314,
        "step": 7736
    },
    {
        "loss": 2.6621,
        "grad_norm": 2.2272584438323975,
        "learning_rate": 3.4657466122579076e-05,
        "epoch": 0.5765705343170132,
        "step": 7737
    },
    {
        "loss": 2.1627,
        "grad_norm": 2.486605167388916,
        "learning_rate": 3.4604216126210654e-05,
        "epoch": 0.5766450555182949,
        "step": 7738
    },
    {
        "loss": 2.9151,
        "grad_norm": 1.731148362159729,
        "learning_rate": 3.455099850943617e-05,
        "epoch": 0.5767195767195767,
        "step": 7739
    },
    {
        "loss": 2.1735,
        "grad_norm": 3.6924264430999756,
        "learning_rate": 3.4497813298605266e-05,
        "epoch": 0.5767940979208585,
        "step": 7740
    },
    {
        "loss": 2.5896,
        "grad_norm": 2.1081769466400146,
        "learning_rate": 3.444466052005187e-05,
        "epoch": 0.5768686191221403,
        "step": 7741
    },
    {
        "loss": 2.7725,
        "grad_norm": 2.2227437496185303,
        "learning_rate": 3.439154020009363e-05,
        "epoch": 0.576943140323422,
        "step": 7742
    },
    {
        "loss": 2.7173,
        "grad_norm": 2.0285346508026123,
        "learning_rate": 3.433845236503196e-05,
        "epoch": 0.5770176615247038,
        "step": 7743
    },
    {
        "loss": 2.798,
        "grad_norm": 1.924602746963501,
        "learning_rate": 3.428539704115262e-05,
        "epoch": 0.5770921827259855,
        "step": 7744
    },
    {
        "loss": 2.6431,
        "grad_norm": 2.5087335109710693,
        "learning_rate": 3.423237425472496e-05,
        "epoch": 0.5771667039272673,
        "step": 7745
    },
    {
        "loss": 2.4548,
        "grad_norm": 2.8126742839813232,
        "learning_rate": 3.417938403200225e-05,
        "epoch": 0.577241225128549,
        "step": 7746
    },
    {
        "loss": 2.8608,
        "grad_norm": 2.8737475872039795,
        "learning_rate": 3.412642639922174e-05,
        "epoch": 0.5773157463298308,
        "step": 7747
    },
    {
        "loss": 2.6982,
        "grad_norm": 1.995714783668518,
        "learning_rate": 3.4073501382604424e-05,
        "epoch": 0.5773902675311126,
        "step": 7748
    },
    {
        "loss": 2.1229,
        "grad_norm": 3.5885732173919678,
        "learning_rate": 3.402060900835533e-05,
        "epoch": 0.5774647887323944,
        "step": 7749
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.5751736164093018,
        "learning_rate": 3.396774930266311e-05,
        "epoch": 0.5775393099336761,
        "step": 7750
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.1624574661254883,
        "learning_rate": 3.391492229170025e-05,
        "epoch": 0.5776138311349579,
        "step": 7751
    },
    {
        "loss": 1.9552,
        "grad_norm": 2.8230769634246826,
        "learning_rate": 3.3862128001623314e-05,
        "epoch": 0.5776883523362396,
        "step": 7752
    },
    {
        "loss": 2.8814,
        "grad_norm": 2.0076379776000977,
        "learning_rate": 3.380936645857233e-05,
        "epoch": 0.5777628735375214,
        "step": 7753
    },
    {
        "loss": 2.0651,
        "grad_norm": 2.1547484397888184,
        "learning_rate": 3.375663768867143e-05,
        "epoch": 0.5778373947388032,
        "step": 7754
    },
    {
        "loss": 2.8081,
        "grad_norm": 3.712651014328003,
        "learning_rate": 3.370394171802828e-05,
        "epoch": 0.577911915940085,
        "step": 7755
    },
    {
        "loss": 1.8467,
        "grad_norm": 3.413451910018921,
        "learning_rate": 3.365127857273431e-05,
        "epoch": 0.5779864371413668,
        "step": 7756
    },
    {
        "loss": 1.1121,
        "grad_norm": 3.3325366973876953,
        "learning_rate": 3.3598648278864874e-05,
        "epoch": 0.5780609583426485,
        "step": 7757
    },
    {
        "loss": 2.5943,
        "grad_norm": 2.4444146156311035,
        "learning_rate": 3.354605086247886e-05,
        "epoch": 0.5781354795439303,
        "step": 7758
    },
    {
        "loss": 2.1228,
        "grad_norm": 2.2779901027679443,
        "learning_rate": 3.349348634961905e-05,
        "epoch": 0.578210000745212,
        "step": 7759
    },
    {
        "loss": 2.6798,
        "grad_norm": 2.678436040878296,
        "learning_rate": 3.344095476631183e-05,
        "epoch": 0.5782845219464938,
        "step": 7760
    },
    {
        "loss": 2.009,
        "grad_norm": 3.2745361328125,
        "learning_rate": 3.338845613856719e-05,
        "epoch": 0.5783590431477755,
        "step": 7761
    },
    {
        "loss": 2.2378,
        "grad_norm": 2.6693475246429443,
        "learning_rate": 3.333599049237912e-05,
        "epoch": 0.5784335643490573,
        "step": 7762
    },
    {
        "loss": 2.8449,
        "grad_norm": 3.9506406784057617,
        "learning_rate": 3.32835578537249e-05,
        "epoch": 0.5785080855503391,
        "step": 7763
    },
    {
        "loss": 2.4756,
        "grad_norm": 3.702226161956787,
        "learning_rate": 3.3231158248565816e-05,
        "epoch": 0.5785826067516209,
        "step": 7764
    },
    {
        "loss": 2.4356,
        "grad_norm": 2.015441417694092,
        "learning_rate": 3.317879170284659e-05,
        "epoch": 0.5786571279529026,
        "step": 7765
    },
    {
        "loss": 2.7531,
        "grad_norm": 2.8568532466888428,
        "learning_rate": 3.3126458242495464e-05,
        "epoch": 0.5787316491541844,
        "step": 7766
    },
    {
        "loss": 2.1057,
        "grad_norm": 1.9143531322479248,
        "learning_rate": 3.3074157893424615e-05,
        "epoch": 0.5788061703554661,
        "step": 7767
    },
    {
        "loss": 2.0116,
        "grad_norm": 3.6448464393615723,
        "learning_rate": 3.302189068152956e-05,
        "epoch": 0.5788806915567479,
        "step": 7768
    },
    {
        "loss": 2.537,
        "grad_norm": 4.230597019195557,
        "learning_rate": 3.296965663268958e-05,
        "epoch": 0.5789552127580296,
        "step": 7769
    },
    {
        "loss": 2.0051,
        "grad_norm": 3.513209581375122,
        "learning_rate": 3.291745577276744e-05,
        "epoch": 0.5790297339593115,
        "step": 7770
    },
    {
        "loss": 2.0353,
        "grad_norm": 1.5769153833389282,
        "learning_rate": 3.286528812760936e-05,
        "epoch": 0.5791042551605932,
        "step": 7771
    },
    {
        "loss": 2.9596,
        "grad_norm": 2.693620204925537,
        "learning_rate": 3.281315372304551e-05,
        "epoch": 0.579178776361875,
        "step": 7772
    },
    {
        "loss": 1.4613,
        "grad_norm": 3.4265201091766357,
        "learning_rate": 3.276105258488906e-05,
        "epoch": 0.5792532975631567,
        "step": 7773
    },
    {
        "loss": 2.8045,
        "grad_norm": 2.85048246383667,
        "learning_rate": 3.270898473893723e-05,
        "epoch": 0.5793278187644385,
        "step": 7774
    },
    {
        "loss": 2.5739,
        "grad_norm": 2.8665030002593994,
        "learning_rate": 3.265695021097043e-05,
        "epoch": 0.5794023399657202,
        "step": 7775
    },
    {
        "loss": 2.2068,
        "grad_norm": 4.108340740203857,
        "learning_rate": 3.26049490267526e-05,
        "epoch": 0.579476861167002,
        "step": 7776
    },
    {
        "loss": 2.1075,
        "grad_norm": 4.727356910705566,
        "learning_rate": 3.25529812120313e-05,
        "epoch": 0.5795513823682837,
        "step": 7777
    },
    {
        "loss": 2.3691,
        "grad_norm": 1.596435308456421,
        "learning_rate": 3.250104679253747e-05,
        "epoch": 0.5796259035695656,
        "step": 7778
    },
    {
        "loss": 2.6162,
        "grad_norm": 1.9659794569015503,
        "learning_rate": 3.244914579398548e-05,
        "epoch": 0.5797004247708473,
        "step": 7779
    },
    {
        "loss": 2.4279,
        "grad_norm": 2.9814810752868652,
        "learning_rate": 3.239727824207337e-05,
        "epoch": 0.5797749459721291,
        "step": 7780
    },
    {
        "loss": 1.8124,
        "grad_norm": 2.73356556892395,
        "learning_rate": 3.234544416248224e-05,
        "epoch": 0.5798494671734108,
        "step": 7781
    },
    {
        "loss": 1.871,
        "grad_norm": 2.538949966430664,
        "learning_rate": 3.2293643580877065e-05,
        "epoch": 0.5799239883746926,
        "step": 7782
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.570040225982666,
        "learning_rate": 3.224187652290591e-05,
        "epoch": 0.5799985095759743,
        "step": 7783
    },
    {
        "loss": 1.8395,
        "grad_norm": 2.891984224319458,
        "learning_rate": 3.2190143014200256e-05,
        "epoch": 0.5800730307772561,
        "step": 7784
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.382509708404541,
        "learning_rate": 3.213844308037522e-05,
        "epoch": 0.5801475519785378,
        "step": 7785
    },
    {
        "loss": 2.4244,
        "grad_norm": 3.4419376850128174,
        "learning_rate": 3.2086776747029046e-05,
        "epoch": 0.5802220731798197,
        "step": 7786
    },
    {
        "loss": 2.4171,
        "grad_norm": 2.746544361114502,
        "learning_rate": 3.203514403974346e-05,
        "epoch": 0.5802965943811014,
        "step": 7787
    },
    {
        "loss": 2.1815,
        "grad_norm": 3.275224447250366,
        "learning_rate": 3.198354498408348e-05,
        "epoch": 0.5803711155823832,
        "step": 7788
    },
    {
        "loss": 2.4687,
        "grad_norm": 3.1207027435302734,
        "learning_rate": 3.193197960559746e-05,
        "epoch": 0.5804456367836649,
        "step": 7789
    },
    {
        "loss": 2.5168,
        "grad_norm": 3.395928382873535,
        "learning_rate": 3.188044792981718e-05,
        "epoch": 0.5805201579849467,
        "step": 7790
    },
    {
        "loss": 1.5092,
        "grad_norm": 3.9506731033325195,
        "learning_rate": 3.1828949982257505e-05,
        "epoch": 0.5805946791862285,
        "step": 7791
    },
    {
        "loss": 2.3931,
        "grad_norm": 3.4550302028656006,
        "learning_rate": 3.1777485788416984e-05,
        "epoch": 0.5806692003875102,
        "step": 7792
    },
    {
        "loss": 1.9694,
        "grad_norm": 2.857891798019409,
        "learning_rate": 3.17260553737771e-05,
        "epoch": 0.5807437215887921,
        "step": 7793
    },
    {
        "loss": 2.1996,
        "grad_norm": 3.4131627082824707,
        "learning_rate": 3.1674658763802645e-05,
        "epoch": 0.5808182427900738,
        "step": 7794
    },
    {
        "loss": 2.4928,
        "grad_norm": 1.654080867767334,
        "learning_rate": 3.162329598394199e-05,
        "epoch": 0.5808927639913556,
        "step": 7795
    },
    {
        "loss": 2.5468,
        "grad_norm": 2.10333514213562,
        "learning_rate": 3.157196705962632e-05,
        "epoch": 0.5809672851926373,
        "step": 7796
    },
    {
        "loss": 2.4744,
        "grad_norm": 4.003829479217529,
        "learning_rate": 3.152067201627039e-05,
        "epoch": 0.5810418063939191,
        "step": 7797
    },
    {
        "loss": 2.3628,
        "grad_norm": 2.0201804637908936,
        "learning_rate": 3.146941087927203e-05,
        "epoch": 0.5811163275952008,
        "step": 7798
    },
    {
        "loss": 3.024,
        "grad_norm": 2.7572526931762695,
        "learning_rate": 3.1418183674012255e-05,
        "epoch": 0.5811908487964826,
        "step": 7799
    },
    {
        "loss": 2.0229,
        "grad_norm": 3.535367012023926,
        "learning_rate": 3.13669904258554e-05,
        "epoch": 0.5812653699977643,
        "step": 7800
    },
    {
        "loss": 2.2848,
        "grad_norm": 3.8111422061920166,
        "learning_rate": 3.131583116014879e-05,
        "epoch": 0.5813398911990462,
        "step": 7801
    },
    {
        "loss": 2.5907,
        "grad_norm": 2.462233543395996,
        "learning_rate": 3.126470590222325e-05,
        "epoch": 0.5814144124003279,
        "step": 7802
    },
    {
        "loss": 1.4706,
        "grad_norm": 3.0467567443847656,
        "learning_rate": 3.121361467739249e-05,
        "epoch": 0.5814889336016097,
        "step": 7803
    },
    {
        "loss": 2.4198,
        "grad_norm": 2.6969962120056152,
        "learning_rate": 3.116255751095331e-05,
        "epoch": 0.5815634548028914,
        "step": 7804
    },
    {
        "loss": 1.9239,
        "grad_norm": 3.611842632293701,
        "learning_rate": 3.1111534428185985e-05,
        "epoch": 0.5816379760041732,
        "step": 7805
    },
    {
        "loss": 2.8525,
        "grad_norm": 2.6385910511016846,
        "learning_rate": 3.106054545435364e-05,
        "epoch": 0.5817124972054549,
        "step": 7806
    },
    {
        "loss": 1.2511,
        "grad_norm": 6.007662773132324,
        "learning_rate": 3.100959061470253e-05,
        "epoch": 0.5817870184067367,
        "step": 7807
    },
    {
        "loss": 2.6964,
        "grad_norm": 3.4944233894348145,
        "learning_rate": 3.095866993446216e-05,
        "epoch": 0.5818615396080185,
        "step": 7808
    },
    {
        "loss": 2.5515,
        "grad_norm": 2.572009325027466,
        "learning_rate": 3.0907783438844906e-05,
        "epoch": 0.5819360608093003,
        "step": 7809
    },
    {
        "loss": 2.9926,
        "grad_norm": 2.7711966037750244,
        "learning_rate": 3.08569311530465e-05,
        "epoch": 0.582010582010582,
        "step": 7810
    },
    {
        "loss": 1.9675,
        "grad_norm": 3.0424580574035645,
        "learning_rate": 3.080611310224546e-05,
        "epoch": 0.5820851032118638,
        "step": 7811
    },
    {
        "loss": 1.6579,
        "grad_norm": 3.1176788806915283,
        "learning_rate": 3.075532931160339e-05,
        "epoch": 0.5821596244131455,
        "step": 7812
    },
    {
        "loss": 2.1992,
        "grad_norm": 5.241898059844971,
        "learning_rate": 3.070457980626519e-05,
        "epoch": 0.5822341456144273,
        "step": 7813
    },
    {
        "loss": 2.2766,
        "grad_norm": 2.7747867107391357,
        "learning_rate": 3.065386461135844e-05,
        "epoch": 0.582308666815709,
        "step": 7814
    },
    {
        "loss": 2.0203,
        "grad_norm": 4.358502388000488,
        "learning_rate": 3.060318375199406e-05,
        "epoch": 0.5823831880169908,
        "step": 7815
    },
    {
        "loss": 2.4056,
        "grad_norm": 3.159559488296509,
        "learning_rate": 3.0552537253265715e-05,
        "epoch": 0.5824577092182726,
        "step": 7816
    },
    {
        "loss": 2.0878,
        "grad_norm": 2.3859336376190186,
        "learning_rate": 3.050192514025012e-05,
        "epoch": 0.5825322304195544,
        "step": 7817
    },
    {
        "loss": 2.298,
        "grad_norm": 3.3550093173980713,
        "learning_rate": 3.045134743800704e-05,
        "epoch": 0.5826067516208361,
        "step": 7818
    },
    {
        "loss": 2.1636,
        "grad_norm": 2.523545265197754,
        "learning_rate": 3.040080417157909e-05,
        "epoch": 0.5826812728221179,
        "step": 7819
    },
    {
        "loss": 2.0819,
        "grad_norm": 2.6499814987182617,
        "learning_rate": 3.0350295365991975e-05,
        "epoch": 0.5827557940233996,
        "step": 7820
    },
    {
        "loss": 2.4996,
        "grad_norm": 2.8567099571228027,
        "learning_rate": 3.0299821046254208e-05,
        "epoch": 0.5828303152246814,
        "step": 7821
    },
    {
        "loss": 2.3869,
        "grad_norm": 2.4123141765594482,
        "learning_rate": 3.02493812373572e-05,
        "epoch": 0.5829048364259631,
        "step": 7822
    },
    {
        "loss": 3.0453,
        "grad_norm": 2.9991798400878906,
        "learning_rate": 3.0198975964275523e-05,
        "epoch": 0.582979357627245,
        "step": 7823
    },
    {
        "loss": 2.4564,
        "grad_norm": 2.924808979034424,
        "learning_rate": 3.01486052519663e-05,
        "epoch": 0.5830538788285267,
        "step": 7824
    },
    {
        "loss": 2.265,
        "grad_norm": 4.089293003082275,
        "learning_rate": 3.0098269125369904e-05,
        "epoch": 0.5831284000298085,
        "step": 7825
    },
    {
        "loss": 2.9643,
        "grad_norm": 1.6258984804153442,
        "learning_rate": 3.0047967609409344e-05,
        "epoch": 0.5832029212310903,
        "step": 7826
    },
    {
        "loss": 2.3188,
        "grad_norm": 3.4766674041748047,
        "learning_rate": 2.9997700728990418e-05,
        "epoch": 0.583277442432372,
        "step": 7827
    },
    {
        "loss": 2.0559,
        "grad_norm": 2.757345676422119,
        "learning_rate": 2.9947468509002054e-05,
        "epoch": 0.5833519636336538,
        "step": 7828
    },
    {
        "loss": 2.6302,
        "grad_norm": 3.2977304458618164,
        "learning_rate": 2.9897270974315782e-05,
        "epoch": 0.5834264848349355,
        "step": 7829
    },
    {
        "loss": 2.0118,
        "grad_norm": 3.4618353843688965,
        "learning_rate": 2.9847108149786118e-05,
        "epoch": 0.5835010060362174,
        "step": 7830
    },
    {
        "loss": 1.5465,
        "grad_norm": 3.133824110031128,
        "learning_rate": 2.9796980060250268e-05,
        "epoch": 0.5835755272374991,
        "step": 7831
    },
    {
        "loss": 2.5547,
        "grad_norm": 2.5196146965026855,
        "learning_rate": 2.97468867305282e-05,
        "epoch": 0.5836500484387809,
        "step": 7832
    },
    {
        "loss": 1.7647,
        "grad_norm": 3.427440643310547,
        "learning_rate": 2.9696828185422996e-05,
        "epoch": 0.5837245696400626,
        "step": 7833
    },
    {
        "loss": 2.397,
        "grad_norm": 3.7388617992401123,
        "learning_rate": 2.9646804449720024e-05,
        "epoch": 0.5837990908413444,
        "step": 7834
    },
    {
        "loss": 2.7759,
        "grad_norm": 2.50288724899292,
        "learning_rate": 2.9596815548187874e-05,
        "epoch": 0.5838736120426261,
        "step": 7835
    },
    {
        "loss": 2.7595,
        "grad_norm": 2.877054452896118,
        "learning_rate": 2.954686150557763e-05,
        "epoch": 0.5839481332439079,
        "step": 7836
    },
    {
        "loss": 2.1818,
        "grad_norm": 2.968994379043579,
        "learning_rate": 2.94969423466231e-05,
        "epoch": 0.5840226544451896,
        "step": 7837
    },
    {
        "loss": 2.0228,
        "grad_norm": 3.131239891052246,
        "learning_rate": 2.944705809604099e-05,
        "epoch": 0.5840971756464715,
        "step": 7838
    },
    {
        "loss": 2.4332,
        "grad_norm": 3.3211264610290527,
        "learning_rate": 2.9397208778530583e-05,
        "epoch": 0.5841716968477532,
        "step": 7839
    },
    {
        "loss": 2.3045,
        "grad_norm": 2.738227605819702,
        "learning_rate": 2.934739441877388e-05,
        "epoch": 0.584246218049035,
        "step": 7840
    },
    {
        "loss": 2.4951,
        "grad_norm": 3.127673625946045,
        "learning_rate": 2.9297615041435656e-05,
        "epoch": 0.5843207392503167,
        "step": 7841
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.266096353530884,
        "learning_rate": 2.92478706711632e-05,
        "epoch": 0.5843952604515985,
        "step": 7842
    },
    {
        "loss": 2.5568,
        "grad_norm": 3.9797935485839844,
        "learning_rate": 2.919816133258675e-05,
        "epoch": 0.5844697816528802,
        "step": 7843
    },
    {
        "loss": 2.6067,
        "grad_norm": 2.0756757259368896,
        "learning_rate": 2.914848705031894e-05,
        "epoch": 0.584544302854162,
        "step": 7844
    },
    {
        "loss": 2.5626,
        "grad_norm": 2.1651742458343506,
        "learning_rate": 2.9098847848955046e-05,
        "epoch": 0.5846188240554437,
        "step": 7845
    },
    {
        "loss": 1.7918,
        "grad_norm": 1.7678042650222778,
        "learning_rate": 2.904924375307324e-05,
        "epoch": 0.5846933452567256,
        "step": 7846
    },
    {
        "loss": 2.622,
        "grad_norm": 2.039522647857666,
        "learning_rate": 2.8999674787234022e-05,
        "epoch": 0.5847678664580073,
        "step": 7847
    },
    {
        "loss": 2.209,
        "grad_norm": 3.4820034503936768,
        "learning_rate": 2.8950140975980654e-05,
        "epoch": 0.5848423876592891,
        "step": 7848
    },
    {
        "loss": 2.3319,
        "grad_norm": 5.2481560707092285,
        "learning_rate": 2.8900642343838934e-05,
        "epoch": 0.5849169088605708,
        "step": 7849
    },
    {
        "loss": 2.4972,
        "grad_norm": 2.2389214038848877,
        "learning_rate": 2.8851178915317212e-05,
        "epoch": 0.5849914300618526,
        "step": 7850
    },
    {
        "loss": 2.491,
        "grad_norm": 3.560687780380249,
        "learning_rate": 2.8801750714906517e-05,
        "epoch": 0.5850659512631343,
        "step": 7851
    },
    {
        "loss": 1.8995,
        "grad_norm": 3.281038761138916,
        "learning_rate": 2.8752357767080252e-05,
        "epoch": 0.5851404724644161,
        "step": 7852
    },
    {
        "loss": 3.009,
        "grad_norm": 3.374498128890991,
        "learning_rate": 2.8703000096294652e-05,
        "epoch": 0.5852149936656978,
        "step": 7853
    },
    {
        "loss": 2.239,
        "grad_norm": 2.3786847591400146,
        "learning_rate": 2.865367772698825e-05,
        "epoch": 0.5852895148669797,
        "step": 7854
    },
    {
        "loss": 2.3777,
        "grad_norm": 3.2747344970703125,
        "learning_rate": 2.860439068358205e-05,
        "epoch": 0.5853640360682614,
        "step": 7855
    },
    {
        "loss": 2.717,
        "grad_norm": 1.998589038848877,
        "learning_rate": 2.8555138990479902e-05,
        "epoch": 0.5854385572695432,
        "step": 7856
    },
    {
        "loss": 2.3105,
        "grad_norm": 4.223048210144043,
        "learning_rate": 2.850592267206772e-05,
        "epoch": 0.5855130784708249,
        "step": 7857
    },
    {
        "loss": 2.0398,
        "grad_norm": 3.1145455837249756,
        "learning_rate": 2.8456741752714277e-05,
        "epoch": 0.5855875996721067,
        "step": 7858
    },
    {
        "loss": 2.5536,
        "grad_norm": 1.9501917362213135,
        "learning_rate": 2.840759625677061e-05,
        "epoch": 0.5856621208733884,
        "step": 7859
    },
    {
        "loss": 2.6226,
        "grad_norm": 3.1639251708984375,
        "learning_rate": 2.8358486208570145e-05,
        "epoch": 0.5857366420746702,
        "step": 7860
    },
    {
        "loss": 1.937,
        "grad_norm": 2.042814254760742,
        "learning_rate": 2.830941163242903e-05,
        "epoch": 0.5858111632759521,
        "step": 7861
    },
    {
        "loss": 1.4596,
        "grad_norm": 3.926734447479248,
        "learning_rate": 2.8260372552645543e-05,
        "epoch": 0.5858856844772338,
        "step": 7862
    },
    {
        "loss": 2.6796,
        "grad_norm": 2.474205255508423,
        "learning_rate": 2.8211368993500754e-05,
        "epoch": 0.5859602056785156,
        "step": 7863
    },
    {
        "loss": 2.1485,
        "grad_norm": 2.748229742050171,
        "learning_rate": 2.8162400979257775e-05,
        "epoch": 0.5860347268797973,
        "step": 7864
    },
    {
        "loss": 3.0182,
        "grad_norm": 1.8599107265472412,
        "learning_rate": 2.8113468534162202e-05,
        "epoch": 0.5861092480810791,
        "step": 7865
    },
    {
        "loss": 1.9564,
        "grad_norm": 2.115983247756958,
        "learning_rate": 2.8064571682442285e-05,
        "epoch": 0.5861837692823608,
        "step": 7866
    },
    {
        "loss": 2.1014,
        "grad_norm": 4.341733932495117,
        "learning_rate": 2.801571044830834e-05,
        "epoch": 0.5862582904836426,
        "step": 7867
    },
    {
        "loss": 2.2571,
        "grad_norm": 3.1867964267730713,
        "learning_rate": 2.796688485595321e-05,
        "epoch": 0.5863328116849243,
        "step": 7868
    },
    {
        "loss": 2.1475,
        "grad_norm": 2.813387155532837,
        "learning_rate": 2.7918094929552018e-05,
        "epoch": 0.5864073328862062,
        "step": 7869
    },
    {
        "loss": 2.6205,
        "grad_norm": 2.9071178436279297,
        "learning_rate": 2.7869340693262204e-05,
        "epoch": 0.5864818540874879,
        "step": 7870
    },
    {
        "loss": 1.9159,
        "grad_norm": 2.870434045791626,
        "learning_rate": 2.7820622171223732e-05,
        "epoch": 0.5865563752887697,
        "step": 7871
    },
    {
        "loss": 2.9233,
        "grad_norm": 3.1610710620880127,
        "learning_rate": 2.777193938755861e-05,
        "epoch": 0.5866308964900514,
        "step": 7872
    },
    {
        "loss": 2.0015,
        "grad_norm": 3.7184135913848877,
        "learning_rate": 2.7723292366371224e-05,
        "epoch": 0.5867054176913332,
        "step": 7873
    },
    {
        "loss": 2.5488,
        "grad_norm": 1.8588142395019531,
        "learning_rate": 2.7674681131748493e-05,
        "epoch": 0.5867799388926149,
        "step": 7874
    },
    {
        "loss": 3.1009,
        "grad_norm": 2.6436779499053955,
        "learning_rate": 2.762610570775923e-05,
        "epoch": 0.5868544600938967,
        "step": 7875
    },
    {
        "loss": 2.4345,
        "grad_norm": 2.3655691146850586,
        "learning_rate": 2.7577566118454934e-05,
        "epoch": 0.5869289812951785,
        "step": 7876
    },
    {
        "loss": 2.394,
        "grad_norm": 3.040159225463867,
        "learning_rate": 2.752906238786902e-05,
        "epoch": 0.5870035024964603,
        "step": 7877
    },
    {
        "loss": 2.8321,
        "grad_norm": 2.484309673309326,
        "learning_rate": 2.7480594540017256e-05,
        "epoch": 0.587078023697742,
        "step": 7878
    },
    {
        "loss": 2.6048,
        "grad_norm": 3.5569467544555664,
        "learning_rate": 2.7432162598897726e-05,
        "epoch": 0.5871525448990238,
        "step": 7879
    },
    {
        "loss": 1.5102,
        "grad_norm": 2.268099069595337,
        "learning_rate": 2.73837665884906e-05,
        "epoch": 0.5872270661003055,
        "step": 7880
    },
    {
        "loss": 2.5089,
        "grad_norm": 3.557612180709839,
        "learning_rate": 2.73354065327584e-05,
        "epoch": 0.5873015873015873,
        "step": 7881
    },
    {
        "loss": 2.4438,
        "grad_norm": 2.956054210662842,
        "learning_rate": 2.728708245564574e-05,
        "epoch": 0.587376108502869,
        "step": 7882
    },
    {
        "loss": 2.0229,
        "grad_norm": 3.3769822120666504,
        "learning_rate": 2.7238794381079348e-05,
        "epoch": 0.5874506297041509,
        "step": 7883
    },
    {
        "loss": 2.4983,
        "grad_norm": 3.208078384399414,
        "learning_rate": 2.7190542332968426e-05,
        "epoch": 0.5875251509054326,
        "step": 7884
    },
    {
        "loss": 2.538,
        "grad_norm": 1.9885109663009644,
        "learning_rate": 2.7142326335203948e-05,
        "epoch": 0.5875996721067144,
        "step": 7885
    },
    {
        "loss": 2.8299,
        "grad_norm": 2.229128122329712,
        "learning_rate": 2.709414641165947e-05,
        "epoch": 0.5876741933079961,
        "step": 7886
    },
    {
        "loss": 2.4437,
        "grad_norm": 2.7705078125,
        "learning_rate": 2.7046002586190276e-05,
        "epoch": 0.5877487145092779,
        "step": 7887
    },
    {
        "loss": 1.2298,
        "grad_norm": 2.626103401184082,
        "learning_rate": 2.6997894882633933e-05,
        "epoch": 0.5878232357105596,
        "step": 7888
    },
    {
        "loss": 2.1629,
        "grad_norm": 1.40811288356781,
        "learning_rate": 2.6949823324810274e-05,
        "epoch": 0.5878977569118414,
        "step": 7889
    },
    {
        "loss": 2.4335,
        "grad_norm": 1.8071210384368896,
        "learning_rate": 2.690178793652095e-05,
        "epoch": 0.5879722781131231,
        "step": 7890
    },
    {
        "loss": 2.1839,
        "grad_norm": 4.593775272369385,
        "learning_rate": 2.6853788741549967e-05,
        "epoch": 0.588046799314405,
        "step": 7891
    },
    {
        "loss": 2.2349,
        "grad_norm": 2.567495346069336,
        "learning_rate": 2.6805825763663274e-05,
        "epoch": 0.5881213205156867,
        "step": 7892
    },
    {
        "loss": 2.849,
        "grad_norm": 2.888576030731201,
        "learning_rate": 2.6757899026608802e-05,
        "epoch": 0.5881958417169685,
        "step": 7893
    },
    {
        "loss": 2.5387,
        "grad_norm": 3.8108861446380615,
        "learning_rate": 2.671000855411686e-05,
        "epoch": 0.5882703629182502,
        "step": 7894
    },
    {
        "loss": 1.9406,
        "grad_norm": 3.2805774211883545,
        "learning_rate": 2.666215436989935e-05,
        "epoch": 0.588344884119532,
        "step": 7895
    },
    {
        "loss": 2.3416,
        "grad_norm": 3.4015920162200928,
        "learning_rate": 2.661433649765066e-05,
        "epoch": 0.5884194053208138,
        "step": 7896
    },
    {
        "loss": 1.9627,
        "grad_norm": 4.120317459106445,
        "learning_rate": 2.6566554961046887e-05,
        "epoch": 0.5884939265220955,
        "step": 7897
    },
    {
        "loss": 1.6271,
        "grad_norm": 3.1111931800842285,
        "learning_rate": 2.6518809783746212e-05,
        "epoch": 0.5885684477233774,
        "step": 7898
    },
    {
        "loss": 2.1953,
        "grad_norm": 3.9180831909179688,
        "learning_rate": 2.6471100989388887e-05,
        "epoch": 0.5886429689246591,
        "step": 7899
    },
    {
        "loss": 2.3676,
        "grad_norm": 3.9211769104003906,
        "learning_rate": 2.642342860159708e-05,
        "epoch": 0.5887174901259409,
        "step": 7900
    },
    {
        "loss": 2.5895,
        "grad_norm": 2.5337963104248047,
        "learning_rate": 2.6375792643974905e-05,
        "epoch": 0.5887920113272226,
        "step": 7901
    },
    {
        "loss": 2.2588,
        "grad_norm": 3.396378755569458,
        "learning_rate": 2.632819314010856e-05,
        "epoch": 0.5888665325285044,
        "step": 7902
    },
    {
        "loss": 1.892,
        "grad_norm": 3.2317535877227783,
        "learning_rate": 2.6280630113565986e-05,
        "epoch": 0.5889410537297861,
        "step": 7903
    },
    {
        "loss": 2.6364,
        "grad_norm": 3.289222002029419,
        "learning_rate": 2.623310358789738e-05,
        "epoch": 0.5890155749310679,
        "step": 7904
    },
    {
        "loss": 2.1743,
        "grad_norm": 2.1655287742614746,
        "learning_rate": 2.6185613586634593e-05,
        "epoch": 0.5890900961323496,
        "step": 7905
    },
    {
        "loss": 2.7979,
        "grad_norm": 2.1109349727630615,
        "learning_rate": 2.6138160133291402e-05,
        "epoch": 0.5891646173336315,
        "step": 7906
    },
    {
        "loss": 2.3531,
        "grad_norm": 1.6601406335830688,
        "learning_rate": 2.6090743251363714e-05,
        "epoch": 0.5892391385349132,
        "step": 7907
    },
    {
        "loss": 2.067,
        "grad_norm": 3.0827672481536865,
        "learning_rate": 2.604336296432909e-05,
        "epoch": 0.589313659736195,
        "step": 7908
    },
    {
        "loss": 2.874,
        "grad_norm": 3.3461625576019287,
        "learning_rate": 2.599601929564709e-05,
        "epoch": 0.5893881809374767,
        "step": 7909
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.6146469116210938,
        "learning_rate": 2.5948712268759123e-05,
        "epoch": 0.5894627021387585,
        "step": 7910
    },
    {
        "loss": 2.3451,
        "grad_norm": 2.8597614765167236,
        "learning_rate": 2.5901441907088387e-05,
        "epoch": 0.5895372233400402,
        "step": 7911
    },
    {
        "loss": 2.3676,
        "grad_norm": 1.7165337800979614,
        "learning_rate": 2.5854208234040058e-05,
        "epoch": 0.589611744541322,
        "step": 7912
    },
    {
        "loss": 2.3424,
        "grad_norm": 2.719848155975342,
        "learning_rate": 2.5807011273000957e-05,
        "epoch": 0.5896862657426037,
        "step": 7913
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.8644310235977173,
        "learning_rate": 2.575985104734001e-05,
        "epoch": 0.5897607869438856,
        "step": 7914
    },
    {
        "loss": 2.603,
        "grad_norm": 1.869956135749817,
        "learning_rate": 2.5712727580407724e-05,
        "epoch": 0.5898353081451673,
        "step": 7915
    },
    {
        "loss": 1.7067,
        "grad_norm": 2.882139205932617,
        "learning_rate": 2.566564089553639e-05,
        "epoch": 0.5899098293464491,
        "step": 7916
    },
    {
        "loss": 2.1848,
        "grad_norm": 4.319835662841797,
        "learning_rate": 2.5618591016040284e-05,
        "epoch": 0.5899843505477308,
        "step": 7917
    },
    {
        "loss": 2.2548,
        "grad_norm": 2.317621946334839,
        "learning_rate": 2.5571577965215244e-05,
        "epoch": 0.5900588717490126,
        "step": 7918
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.9439890384674072,
        "learning_rate": 2.5524601766339075e-05,
        "epoch": 0.5901333929502943,
        "step": 7919
    },
    {
        "loss": 2.5794,
        "grad_norm": 3.0825023651123047,
        "learning_rate": 2.5477662442671203e-05,
        "epoch": 0.5902079141515761,
        "step": 7920
    },
    {
        "loss": 2.67,
        "grad_norm": 3.393954277038574,
        "learning_rate": 2.5430760017452705e-05,
        "epoch": 0.5902824353528578,
        "step": 7921
    },
    {
        "loss": 2.7445,
        "grad_norm": 2.33888840675354,
        "learning_rate": 2.538389451390665e-05,
        "epoch": 0.5903569565541397,
        "step": 7922
    },
    {
        "loss": 1.8613,
        "grad_norm": 2.315089225769043,
        "learning_rate": 2.533706595523755e-05,
        "epoch": 0.5904314777554214,
        "step": 7923
    },
    {
        "loss": 2.5607,
        "grad_norm": 3.33248233795166,
        "learning_rate": 2.529027436463196e-05,
        "epoch": 0.5905059989567032,
        "step": 7924
    },
    {
        "loss": 2.1738,
        "grad_norm": 4.728876113891602,
        "learning_rate": 2.5243519765257773e-05,
        "epoch": 0.5905805201579849,
        "step": 7925
    },
    {
        "loss": 2.3276,
        "grad_norm": 3.105907440185547,
        "learning_rate": 2.5196802180264665e-05,
        "epoch": 0.5906550413592667,
        "step": 7926
    },
    {
        "loss": 2.3929,
        "grad_norm": 1.833163857460022,
        "learning_rate": 2.515012163278423e-05,
        "epoch": 0.5907295625605484,
        "step": 7927
    },
    {
        "loss": 2.6874,
        "grad_norm": 3.7456817626953125,
        "learning_rate": 2.5103478145929405e-05,
        "epoch": 0.5908040837618302,
        "step": 7928
    },
    {
        "loss": 1.8085,
        "grad_norm": 2.595703601837158,
        "learning_rate": 2.5056871742794973e-05,
        "epoch": 0.590878604963112,
        "step": 7929
    },
    {
        "loss": 2.6142,
        "grad_norm": 1.8251726627349854,
        "learning_rate": 2.5010302446457255e-05,
        "epoch": 0.5909531261643938,
        "step": 7930
    },
    {
        "loss": 2.6078,
        "grad_norm": 2.2596962451934814,
        "learning_rate": 2.496377027997422e-05,
        "epoch": 0.5910276473656755,
        "step": 7931
    },
    {
        "loss": 2.886,
        "grad_norm": 2.269066333770752,
        "learning_rate": 2.491727526638551e-05,
        "epoch": 0.5911021685669573,
        "step": 7932
    },
    {
        "loss": 1.593,
        "grad_norm": 4.195025444030762,
        "learning_rate": 2.48708174287123e-05,
        "epoch": 0.5911766897682391,
        "step": 7933
    },
    {
        "loss": 2.3302,
        "grad_norm": 2.878068447113037,
        "learning_rate": 2.4824396789957304e-05,
        "epoch": 0.5912512109695208,
        "step": 7934
    },
    {
        "loss": 1.7003,
        "grad_norm": 3.1147043704986572,
        "learning_rate": 2.477801337310508e-05,
        "epoch": 0.5913257321708026,
        "step": 7935
    },
    {
        "loss": 2.2146,
        "grad_norm": 2.737299680709839,
        "learning_rate": 2.473166720112139e-05,
        "epoch": 0.5914002533720844,
        "step": 7936
    },
    {
        "loss": 2.9044,
        "grad_norm": 2.658409357070923,
        "learning_rate": 2.468535829695392e-05,
        "epoch": 0.5914747745733662,
        "step": 7937
    },
    {
        "loss": 2.4912,
        "grad_norm": 3.593492031097412,
        "learning_rate": 2.4639086683531652e-05,
        "epoch": 0.5915492957746479,
        "step": 7938
    },
    {
        "loss": 1.5395,
        "grad_norm": 3.652710437774658,
        "learning_rate": 2.459285238376513e-05,
        "epoch": 0.5916238169759297,
        "step": 7939
    },
    {
        "loss": 1.6434,
        "grad_norm": 3.3376083374023438,
        "learning_rate": 2.4546655420546526e-05,
        "epoch": 0.5916983381772114,
        "step": 7940
    },
    {
        "loss": 2.6403,
        "grad_norm": 2.615368127822876,
        "learning_rate": 2.4500495816749436e-05,
        "epoch": 0.5917728593784932,
        "step": 7941
    },
    {
        "loss": 1.7585,
        "grad_norm": 3.5697085857391357,
        "learning_rate": 2.445437359522902e-05,
        "epoch": 0.5918473805797749,
        "step": 7942
    },
    {
        "loss": 2.467,
        "grad_norm": 3.6449971199035645,
        "learning_rate": 2.4408288778821887e-05,
        "epoch": 0.5919219017810567,
        "step": 7943
    },
    {
        "loss": 2.1815,
        "grad_norm": 2.0895113945007324,
        "learning_rate": 2.436224139034604e-05,
        "epoch": 0.5919964229823385,
        "step": 7944
    },
    {
        "loss": 2.2274,
        "grad_norm": 2.6139414310455322,
        "learning_rate": 2.4316231452601235e-05,
        "epoch": 0.5920709441836203,
        "step": 7945
    },
    {
        "loss": 2.351,
        "grad_norm": 2.1510086059570312,
        "learning_rate": 2.427025898836832e-05,
        "epoch": 0.592145465384902,
        "step": 7946
    },
    {
        "loss": 2.0962,
        "grad_norm": 3.939957618713379,
        "learning_rate": 2.422432402040997e-05,
        "epoch": 0.5922199865861838,
        "step": 7947
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.21592378616333,
        "learning_rate": 2.4178426571469948e-05,
        "epoch": 0.5922945077874655,
        "step": 7948
    },
    {
        "loss": 2.3568,
        "grad_norm": 2.6304829120635986,
        "learning_rate": 2.4132566664273558e-05,
        "epoch": 0.5923690289887473,
        "step": 7949
    },
    {
        "loss": 1.4408,
        "grad_norm": 2.775146722793579,
        "learning_rate": 2.4086744321527677e-05,
        "epoch": 0.592443550190029,
        "step": 7950
    },
    {
        "loss": 2.8406,
        "grad_norm": 3.5755553245544434,
        "learning_rate": 2.4040959565920283e-05,
        "epoch": 0.5925180713913109,
        "step": 7951
    },
    {
        "loss": 2.6892,
        "grad_norm": 3.4559273719787598,
        "learning_rate": 2.399521242012105e-05,
        "epoch": 0.5925925925925926,
        "step": 7952
    },
    {
        "loss": 2.9938,
        "grad_norm": 3.193836212158203,
        "learning_rate": 2.3949502906780853e-05,
        "epoch": 0.5926671137938744,
        "step": 7953
    },
    {
        "loss": 2.6633,
        "grad_norm": 2.4366157054901123,
        "learning_rate": 2.390383104853182e-05,
        "epoch": 0.5927416349951561,
        "step": 7954
    },
    {
        "loss": 1.9234,
        "grad_norm": 4.944211006164551,
        "learning_rate": 2.3858196867987804e-05,
        "epoch": 0.5928161561964379,
        "step": 7955
    },
    {
        "loss": 2.3268,
        "grad_norm": 3.0793254375457764,
        "learning_rate": 2.38126003877436e-05,
        "epoch": 0.5928906773977196,
        "step": 7956
    },
    {
        "loss": 2.7486,
        "grad_norm": 2.033742666244507,
        "learning_rate": 2.3767041630375696e-05,
        "epoch": 0.5929651985990014,
        "step": 7957
    },
    {
        "loss": 2.2831,
        "grad_norm": 2.81738543510437,
        "learning_rate": 2.372152061844163e-05,
        "epoch": 0.5930397198002831,
        "step": 7958
    },
    {
        "loss": 2.55,
        "grad_norm": 2.746873617172241,
        "learning_rate": 2.367603737448031e-05,
        "epoch": 0.593114241001565,
        "step": 7959
    },
    {
        "loss": 2.3679,
        "grad_norm": 3.110769271850586,
        "learning_rate": 2.3630591921012045e-05,
        "epoch": 0.5931887622028467,
        "step": 7960
    },
    {
        "loss": 1.9131,
        "grad_norm": 2.881725788116455,
        "learning_rate": 2.358518428053832e-05,
        "epoch": 0.5932632834041285,
        "step": 7961
    },
    {
        "loss": 2.7553,
        "grad_norm": 3.349741220474243,
        "learning_rate": 2.3539814475542e-05,
        "epoch": 0.5933378046054102,
        "step": 7962
    },
    {
        "loss": 2.1059,
        "grad_norm": 3.1866140365600586,
        "learning_rate": 2.3494482528487106e-05,
        "epoch": 0.593412325806692,
        "step": 7963
    },
    {
        "loss": 2.4691,
        "grad_norm": 2.3586795330047607,
        "learning_rate": 2.3449188461818917e-05,
        "epoch": 0.5934868470079737,
        "step": 7964
    },
    {
        "loss": 1.4469,
        "grad_norm": 5.5717597007751465,
        "learning_rate": 2.340393229796416e-05,
        "epoch": 0.5935613682092555,
        "step": 7965
    },
    {
        "loss": 2.4177,
        "grad_norm": 2.4409842491149902,
        "learning_rate": 2.3358714059330567e-05,
        "epoch": 0.5936358894105372,
        "step": 7966
    },
    {
        "loss": 2.8476,
        "grad_norm": 2.4085614681243896,
        "learning_rate": 2.3313533768307083e-05,
        "epoch": 0.5937104106118191,
        "step": 7967
    },
    {
        "loss": 1.4457,
        "grad_norm": 4.351484775543213,
        "learning_rate": 2.3268391447264104e-05,
        "epoch": 0.5937849318131009,
        "step": 7968
    },
    {
        "loss": 1.7071,
        "grad_norm": 1.9633065462112427,
        "learning_rate": 2.3223287118552984e-05,
        "epoch": 0.5938594530143826,
        "step": 7969
    },
    {
        "loss": 2.4668,
        "grad_norm": 3.4387378692626953,
        "learning_rate": 2.317822080450639e-05,
        "epoch": 0.5939339742156644,
        "step": 7970
    },
    {
        "loss": 2.1633,
        "grad_norm": 2.637570381164551,
        "learning_rate": 2.3133192527438108e-05,
        "epoch": 0.5940084954169461,
        "step": 7971
    },
    {
        "loss": 2.5824,
        "grad_norm": 2.9604034423828125,
        "learning_rate": 2.308820230964308e-05,
        "epoch": 0.5940830166182279,
        "step": 7972
    },
    {
        "loss": 2.3761,
        "grad_norm": 2.983551263809204,
        "learning_rate": 2.30432501733975e-05,
        "epoch": 0.5941575378195096,
        "step": 7973
    },
    {
        "loss": 2.2092,
        "grad_norm": 2.6813604831695557,
        "learning_rate": 2.2998336140958532e-05,
        "epoch": 0.5942320590207915,
        "step": 7974
    },
    {
        "loss": 2.6403,
        "grad_norm": 2.116705894470215,
        "learning_rate": 2.2953460234564738e-05,
        "epoch": 0.5943065802220732,
        "step": 7975
    },
    {
        "loss": 2.4315,
        "grad_norm": 2.918668746948242,
        "learning_rate": 2.2908622476435605e-05,
        "epoch": 0.594381101423355,
        "step": 7976
    },
    {
        "loss": 2.2955,
        "grad_norm": 2.3476099967956543,
        "learning_rate": 2.2863822888771634e-05,
        "epoch": 0.5944556226246367,
        "step": 7977
    },
    {
        "loss": 1.916,
        "grad_norm": 2.8361356258392334,
        "learning_rate": 2.2819061493754756e-05,
        "epoch": 0.5945301438259185,
        "step": 7978
    },
    {
        "loss": 2.1188,
        "grad_norm": 2.708120584487915,
        "learning_rate": 2.277433831354767e-05,
        "epoch": 0.5946046650272002,
        "step": 7979
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.424622058868408,
        "learning_rate": 2.2729653370294434e-05,
        "epoch": 0.594679186228482,
        "step": 7980
    },
    {
        "loss": 1.5898,
        "grad_norm": 4.163552284240723,
        "learning_rate": 2.2685006686119903e-05,
        "epoch": 0.5947537074297637,
        "step": 7981
    },
    {
        "loss": 3.2801,
        "grad_norm": 3.1167855262756348,
        "learning_rate": 2.2640398283130082e-05,
        "epoch": 0.5948282286310456,
        "step": 7982
    },
    {
        "loss": 1.9237,
        "grad_norm": 2.7745201587677,
        "learning_rate": 2.2595828183412172e-05,
        "epoch": 0.5949027498323273,
        "step": 7983
    },
    {
        "loss": 2.2894,
        "grad_norm": 5.351101398468018,
        "learning_rate": 2.2551296409034173e-05,
        "epoch": 0.5949772710336091,
        "step": 7984
    },
    {
        "loss": 1.6126,
        "grad_norm": 3.2665133476257324,
        "learning_rate": 2.250680298204534e-05,
        "epoch": 0.5950517922348908,
        "step": 7985
    },
    {
        "loss": 2.4788,
        "grad_norm": 2.720693349838257,
        "learning_rate": 2.246234792447576e-05,
        "epoch": 0.5951263134361726,
        "step": 7986
    },
    {
        "loss": 2.1992,
        "grad_norm": 2.996368408203125,
        "learning_rate": 2.241793125833651e-05,
        "epoch": 0.5952008346374543,
        "step": 7987
    },
    {
        "loss": 2.6855,
        "grad_norm": 2.487279176712036,
        "learning_rate": 2.2373553005619918e-05,
        "epoch": 0.5952753558387361,
        "step": 7988
    },
    {
        "loss": 2.2106,
        "grad_norm": 3.6557092666625977,
        "learning_rate": 2.232921318829898e-05,
        "epoch": 0.5953498770400178,
        "step": 7989
    },
    {
        "loss": 1.9829,
        "grad_norm": 3.038346529006958,
        "learning_rate": 2.228491182832785e-05,
        "epoch": 0.5954243982412997,
        "step": 7990
    },
    {
        "loss": 2.3545,
        "grad_norm": 3.194873332977295,
        "learning_rate": 2.2240648947641552e-05,
        "epoch": 0.5954989194425814,
        "step": 7991
    },
    {
        "loss": 2.1683,
        "grad_norm": 3.5432205200195312,
        "learning_rate": 2.2196424568156073e-05,
        "epoch": 0.5955734406438632,
        "step": 7992
    },
    {
        "loss": 1.7467,
        "grad_norm": 3.7607274055480957,
        "learning_rate": 2.215223871176839e-05,
        "epoch": 0.5956479618451449,
        "step": 7993
    },
    {
        "loss": 2.5034,
        "grad_norm": 2.634949207305908,
        "learning_rate": 2.2108091400356345e-05,
        "epoch": 0.5957224830464267,
        "step": 7994
    },
    {
        "loss": 2.1316,
        "grad_norm": 3.2433865070343018,
        "learning_rate": 2.206398265577865e-05,
        "epoch": 0.5957970042477084,
        "step": 7995
    },
    {
        "loss": 2.0199,
        "grad_norm": 2.8626294136047363,
        "learning_rate": 2.2019912499875127e-05,
        "epoch": 0.5958715254489902,
        "step": 7996
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.9713118076324463,
        "learning_rate": 2.197588095446621e-05,
        "epoch": 0.595946046650272,
        "step": 7997
    },
    {
        "loss": 2.3413,
        "grad_norm": 2.9246158599853516,
        "learning_rate": 2.193188804135351e-05,
        "epoch": 0.5960205678515538,
        "step": 7998
    },
    {
        "loss": 2.5066,
        "grad_norm": 2.6533164978027344,
        "learning_rate": 2.1887933782319313e-05,
        "epoch": 0.5960950890528355,
        "step": 7999
    },
    {
        "loss": 1.694,
        "grad_norm": 3.145648241043091,
        "learning_rate": 2.1844018199126704e-05,
        "epoch": 0.5961696102541173,
        "step": 8000
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.059276819229126,
        "learning_rate": 2.180014131351985e-05,
        "epoch": 0.596244131455399,
        "step": 8001
    },
    {
        "loss": 1.8336,
        "grad_norm": 3.2125558853149414,
        "learning_rate": 2.1756303147223568e-05,
        "epoch": 0.5963186526566808,
        "step": 8002
    },
    {
        "loss": 1.1822,
        "grad_norm": 4.747722148895264,
        "learning_rate": 2.171250372194361e-05,
        "epoch": 0.5963931738579626,
        "step": 8003
    },
    {
        "loss": 2.4702,
        "grad_norm": 2.7580485343933105,
        "learning_rate": 2.1668743059366492e-05,
        "epoch": 0.5964676950592444,
        "step": 8004
    },
    {
        "loss": 2.3587,
        "grad_norm": 3.69948148727417,
        "learning_rate": 2.1625021181159454e-05,
        "epoch": 0.5965422162605262,
        "step": 8005
    },
    {
        "loss": 2.0957,
        "grad_norm": 3.7718982696533203,
        "learning_rate": 2.1581338108970806e-05,
        "epoch": 0.5966167374618079,
        "step": 8006
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.510932683944702,
        "learning_rate": 2.153769386442932e-05,
        "epoch": 0.5966912586630897,
        "step": 8007
    },
    {
        "loss": 2.5062,
        "grad_norm": 1.9508631229400635,
        "learning_rate": 2.1494088469144802e-05,
        "epoch": 0.5967657798643714,
        "step": 8008
    },
    {
        "loss": 2.7078,
        "grad_norm": 2.2965846061706543,
        "learning_rate": 2.145052194470767e-05,
        "epoch": 0.5968403010656532,
        "step": 8009
    },
    {
        "loss": 2.6876,
        "grad_norm": 2.232811212539673,
        "learning_rate": 2.1406994312689067e-05,
        "epoch": 0.5969148222669349,
        "step": 8010
    },
    {
        "loss": 2.4698,
        "grad_norm": 2.334547996520996,
        "learning_rate": 2.1363505594641086e-05,
        "epoch": 0.5969893434682167,
        "step": 8011
    },
    {
        "loss": 2.3974,
        "grad_norm": 3.271787166595459,
        "learning_rate": 2.1320055812096263e-05,
        "epoch": 0.5970638646694985,
        "step": 8012
    },
    {
        "loss": 2.5692,
        "grad_norm": 4.835491180419922,
        "learning_rate": 2.127664498656813e-05,
        "epoch": 0.5971383858707803,
        "step": 8013
    },
    {
        "loss": 2.4781,
        "grad_norm": 3.0188279151916504,
        "learning_rate": 2.123327313955079e-05,
        "epoch": 0.597212907072062,
        "step": 8014
    },
    {
        "loss": 2.4168,
        "grad_norm": 3.006476402282715,
        "learning_rate": 2.118994029251893e-05,
        "epoch": 0.5972874282733438,
        "step": 8015
    },
    {
        "loss": 2.5006,
        "grad_norm": 2.2730791568756104,
        "learning_rate": 2.1146646466928245e-05,
        "epoch": 0.5973619494746255,
        "step": 8016
    },
    {
        "loss": 1.93,
        "grad_norm": 2.6563234329223633,
        "learning_rate": 2.1103391684214767e-05,
        "epoch": 0.5974364706759073,
        "step": 8017
    },
    {
        "loss": 2.5215,
        "grad_norm": 3.1812174320220947,
        "learning_rate": 2.1060175965795525e-05,
        "epoch": 0.597510991877189,
        "step": 8018
    },
    {
        "loss": 2.6979,
        "grad_norm": 2.184995651245117,
        "learning_rate": 2.1016999333067944e-05,
        "epoch": 0.5975855130784709,
        "step": 8019
    },
    {
        "loss": 1.7788,
        "grad_norm": 2.984639883041382,
        "learning_rate": 2.0973861807410155e-05,
        "epoch": 0.5976600342797526,
        "step": 8020
    },
    {
        "loss": 2.7048,
        "grad_norm": 3.2557239532470703,
        "learning_rate": 2.0930763410181033e-05,
        "epoch": 0.5977345554810344,
        "step": 8021
    },
    {
        "loss": 2.1971,
        "grad_norm": 2.879591464996338,
        "learning_rate": 2.0887704162719933e-05,
        "epoch": 0.5978090766823161,
        "step": 8022
    },
    {
        "loss": 2.2948,
        "grad_norm": 3.9993157386779785,
        "learning_rate": 2.0844684086346966e-05,
        "epoch": 0.5978835978835979,
        "step": 8023
    },
    {
        "loss": 1.5226,
        "grad_norm": 3.212416648864746,
        "learning_rate": 2.0801703202362754e-05,
        "epoch": 0.5979581190848796,
        "step": 8024
    },
    {
        "loss": 2.8341,
        "grad_norm": 2.300666093826294,
        "learning_rate": 2.0758761532048444e-05,
        "epoch": 0.5980326402861614,
        "step": 8025
    },
    {
        "loss": 2.7578,
        "grad_norm": 2.5255026817321777,
        "learning_rate": 2.0715859096666047e-05,
        "epoch": 0.5981071614874431,
        "step": 8026
    },
    {
        "loss": 2.8538,
        "grad_norm": 2.1093904972076416,
        "learning_rate": 2.067299591745786e-05,
        "epoch": 0.598181682688725,
        "step": 8027
    },
    {
        "loss": 1.7598,
        "grad_norm": 1.6700034141540527,
        "learning_rate": 2.0630172015646777e-05,
        "epoch": 0.5982562038900067,
        "step": 8028
    },
    {
        "loss": 2.6462,
        "grad_norm": 2.1629292964935303,
        "learning_rate": 2.05873874124365e-05,
        "epoch": 0.5983307250912885,
        "step": 8029
    },
    {
        "loss": 1.9246,
        "grad_norm": 2.5047430992126465,
        "learning_rate": 2.0544642129010905e-05,
        "epoch": 0.5984052462925702,
        "step": 8030
    },
    {
        "loss": 1.6699,
        "grad_norm": 2.251232147216797,
        "learning_rate": 2.050193618653471e-05,
        "epoch": 0.598479767493852,
        "step": 8031
    },
    {
        "loss": 2.727,
        "grad_norm": 2.6729300022125244,
        "learning_rate": 2.0459269606152963e-05,
        "epoch": 0.5985542886951337,
        "step": 8032
    },
    {
        "loss": 2.5551,
        "grad_norm": 2.654632806777954,
        "learning_rate": 2.0416642408991284e-05,
        "epoch": 0.5986288098964155,
        "step": 8033
    },
    {
        "loss": 1.7178,
        "grad_norm": 6.322178840637207,
        "learning_rate": 2.0374054616155824e-05,
        "epoch": 0.5987033310976972,
        "step": 8034
    },
    {
        "loss": 2.7061,
        "grad_norm": 2.176419734954834,
        "learning_rate": 2.0331506248733088e-05,
        "epoch": 0.5987778522989791,
        "step": 8035
    },
    {
        "loss": 1.9401,
        "grad_norm": 3.922722339630127,
        "learning_rate": 2.0288997327790336e-05,
        "epoch": 0.5988523735002608,
        "step": 8036
    },
    {
        "loss": 1.8307,
        "grad_norm": 3.3227739334106445,
        "learning_rate": 2.0246527874375076e-05,
        "epoch": 0.5989268947015426,
        "step": 8037
    },
    {
        "loss": 2.0206,
        "grad_norm": 2.5814521312713623,
        "learning_rate": 2.0204097909515183e-05,
        "epoch": 0.5990014159028244,
        "step": 8038
    },
    {
        "loss": 2.5587,
        "grad_norm": 3.485050678253174,
        "learning_rate": 2.0161707454219303e-05,
        "epoch": 0.5990759371041061,
        "step": 8039
    },
    {
        "loss": 2.2394,
        "grad_norm": 3.0889108180999756,
        "learning_rate": 2.0119356529476218e-05,
        "epoch": 0.5991504583053879,
        "step": 8040
    },
    {
        "loss": 1.8301,
        "grad_norm": 2.99519944190979,
        "learning_rate": 2.007704515625539e-05,
        "epoch": 0.5992249795066696,
        "step": 8041
    },
    {
        "loss": 2.2102,
        "grad_norm": 2.423039197921753,
        "learning_rate": 2.0034773355506453e-05,
        "epoch": 0.5992995007079515,
        "step": 8042
    },
    {
        "loss": 2.5269,
        "grad_norm": 3.7403552532196045,
        "learning_rate": 1.9992541148159538e-05,
        "epoch": 0.5993740219092332,
        "step": 8043
    },
    {
        "loss": 2.5854,
        "grad_norm": 2.1045172214508057,
        "learning_rate": 1.995034855512532e-05,
        "epoch": 0.599448543110515,
        "step": 8044
    },
    {
        "loss": 2.8057,
        "grad_norm": 3.4409821033477783,
        "learning_rate": 1.9908195597294542e-05,
        "epoch": 0.5995230643117967,
        "step": 8045
    },
    {
        "loss": 1.4571,
        "grad_norm": 4.936995983123779,
        "learning_rate": 1.9866082295538713e-05,
        "epoch": 0.5995975855130785,
        "step": 8046
    },
    {
        "loss": 2.5662,
        "grad_norm": 2.284740924835205,
        "learning_rate": 1.9824008670709426e-05,
        "epoch": 0.5996721067143602,
        "step": 8047
    },
    {
        "loss": 2.6362,
        "grad_norm": 1.738847255706787,
        "learning_rate": 1.978197474363863e-05,
        "epoch": 0.599746627915642,
        "step": 8048
    },
    {
        "loss": 2.3438,
        "grad_norm": 4.43782901763916,
        "learning_rate": 1.9739980535138858e-05,
        "epoch": 0.5998211491169237,
        "step": 8049
    },
    {
        "loss": 2.4244,
        "grad_norm": 3.361717700958252,
        "learning_rate": 1.9698026066002706e-05,
        "epoch": 0.5998956703182056,
        "step": 8050
    },
    {
        "loss": 2.4323,
        "grad_norm": 2.9091763496398926,
        "learning_rate": 1.965611135700326e-05,
        "epoch": 0.5999701915194873,
        "step": 8051
    },
    {
        "loss": 2.4056,
        "grad_norm": 2.745108127593994,
        "learning_rate": 1.961423642889384e-05,
        "epoch": 0.6000447127207691,
        "step": 8052
    },
    {
        "loss": 2.3873,
        "grad_norm": 2.1778552532196045,
        "learning_rate": 1.957240130240807e-05,
        "epoch": 0.6001192339220508,
        "step": 8053
    },
    {
        "loss": 2.8271,
        "grad_norm": 2.6692519187927246,
        "learning_rate": 1.9530605998259932e-05,
        "epoch": 0.6001937551233326,
        "step": 8054
    },
    {
        "loss": 2.1597,
        "grad_norm": 1.9887797832489014,
        "learning_rate": 1.9488850537143567e-05,
        "epoch": 0.6002682763246143,
        "step": 8055
    },
    {
        "loss": 1.9967,
        "grad_norm": 2.955765724182129,
        "learning_rate": 1.9447134939733614e-05,
        "epoch": 0.6003427975258961,
        "step": 8056
    },
    {
        "loss": 2.3077,
        "grad_norm": 2.2157866954803467,
        "learning_rate": 1.9405459226684753e-05,
        "epoch": 0.6004173187271779,
        "step": 8057
    },
    {
        "loss": 2.3867,
        "grad_norm": 2.0745608806610107,
        "learning_rate": 1.936382341863191e-05,
        "epoch": 0.6004918399284597,
        "step": 8058
    },
    {
        "loss": 1.2794,
        "grad_norm": 3.8646483421325684,
        "learning_rate": 1.9322227536190507e-05,
        "epoch": 0.6005663611297414,
        "step": 8059
    },
    {
        "loss": 2.9631,
        "grad_norm": 2.8171472549438477,
        "learning_rate": 1.9280671599955968e-05,
        "epoch": 0.6006408823310232,
        "step": 8060
    },
    {
        "loss": 2.8751,
        "grad_norm": 2.6622204780578613,
        "learning_rate": 1.923915563050389e-05,
        "epoch": 0.6007154035323049,
        "step": 8061
    },
    {
        "loss": 2.49,
        "grad_norm": 3.0770628452301025,
        "learning_rate": 1.919767964839032e-05,
        "epoch": 0.6007899247335867,
        "step": 8062
    },
    {
        "loss": 2.2336,
        "grad_norm": 2.5055243968963623,
        "learning_rate": 1.915624367415131e-05,
        "epoch": 0.6008644459348684,
        "step": 8063
    },
    {
        "loss": 2.7869,
        "grad_norm": 1.9305161237716675,
        "learning_rate": 1.9114847728303208e-05,
        "epoch": 0.6009389671361502,
        "step": 8064
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.8435399532318115,
        "learning_rate": 1.907349183134247e-05,
        "epoch": 0.601013488337432,
        "step": 8065
    },
    {
        "loss": 2.4027,
        "grad_norm": 3.519252300262451,
        "learning_rate": 1.9032176003745693e-05,
        "epoch": 0.6010880095387138,
        "step": 8066
    },
    {
        "loss": 2.8987,
        "grad_norm": 2.4538662433624268,
        "learning_rate": 1.8990900265969868e-05,
        "epoch": 0.6011625307399955,
        "step": 8067
    },
    {
        "loss": 2.2263,
        "grad_norm": 3.472203016281128,
        "learning_rate": 1.894966463845175e-05,
        "epoch": 0.6012370519412773,
        "step": 8068
    },
    {
        "loss": 2.3142,
        "grad_norm": 3.651348352432251,
        "learning_rate": 1.890846914160864e-05,
        "epoch": 0.601311573142559,
        "step": 8069
    },
    {
        "loss": 2.3809,
        "grad_norm": 2.396353006362915,
        "learning_rate": 1.8867313795837704e-05,
        "epoch": 0.6013860943438408,
        "step": 8070
    },
    {
        "loss": 3.0727,
        "grad_norm": 2.916257381439209,
        "learning_rate": 1.8826198621516267e-05,
        "epoch": 0.6014606155451225,
        "step": 8071
    },
    {
        "loss": 2.8713,
        "grad_norm": 3.6292026042938232,
        "learning_rate": 1.8785123639001856e-05,
        "epoch": 0.6015351367464044,
        "step": 8072
    },
    {
        "loss": 2.035,
        "grad_norm": 3.2545390129089355,
        "learning_rate": 1.8744088868631948e-05,
        "epoch": 0.6016096579476862,
        "step": 8073
    },
    {
        "loss": 2.0344,
        "grad_norm": 4.04791784286499,
        "learning_rate": 1.8703094330724347e-05,
        "epoch": 0.6016841791489679,
        "step": 8074
    },
    {
        "loss": 2.648,
        "grad_norm": 2.4381539821624756,
        "learning_rate": 1.8662140045576683e-05,
        "epoch": 0.6017587003502497,
        "step": 8075
    },
    {
        "loss": 2.5942,
        "grad_norm": 2.6266028881073,
        "learning_rate": 1.862122603346671e-05,
        "epoch": 0.6018332215515314,
        "step": 8076
    },
    {
        "loss": 1.8933,
        "grad_norm": 3.0401291847229004,
        "learning_rate": 1.858035231465245e-05,
        "epoch": 0.6019077427528132,
        "step": 8077
    },
    {
        "loss": 2.2406,
        "grad_norm": 3.460557460784912,
        "learning_rate": 1.8539518909371655e-05,
        "epoch": 0.6019822639540949,
        "step": 8078
    },
    {
        "loss": 2.4833,
        "grad_norm": 3.5320966243743896,
        "learning_rate": 1.8498725837842458e-05,
        "epoch": 0.6020567851553768,
        "step": 8079
    },
    {
        "loss": 1.927,
        "grad_norm": 4.253800868988037,
        "learning_rate": 1.845797312026275e-05,
        "epoch": 0.6021313063566585,
        "step": 8080
    },
    {
        "loss": 2.1569,
        "grad_norm": 3.8520359992980957,
        "learning_rate": 1.841726077681052e-05,
        "epoch": 0.6022058275579403,
        "step": 8081
    },
    {
        "loss": 2.7216,
        "grad_norm": 2.2987217903137207,
        "learning_rate": 1.8376588827643824e-05,
        "epoch": 0.602280348759222,
        "step": 8082
    },
    {
        "loss": 1.0831,
        "grad_norm": 4.495330333709717,
        "learning_rate": 1.833595729290063e-05,
        "epoch": 0.6023548699605038,
        "step": 8083
    },
    {
        "loss": 2.2073,
        "grad_norm": 2.6923775672912598,
        "learning_rate": 1.8295366192698994e-05,
        "epoch": 0.6024293911617855,
        "step": 8084
    },
    {
        "loss": 2.2522,
        "grad_norm": 4.114468097686768,
        "learning_rate": 1.825481554713686e-05,
        "epoch": 0.6025039123630673,
        "step": 8085
    },
    {
        "loss": 2.8537,
        "grad_norm": 2.242856979370117,
        "learning_rate": 1.821430537629213e-05,
        "epoch": 0.602578433564349,
        "step": 8086
    },
    {
        "loss": 2.3038,
        "grad_norm": 3.4514145851135254,
        "learning_rate": 1.817383570022284e-05,
        "epoch": 0.6026529547656309,
        "step": 8087
    },
    {
        "loss": 2.8244,
        "grad_norm": 2.339169979095459,
        "learning_rate": 1.81334065389668e-05,
        "epoch": 0.6027274759669126,
        "step": 8088
    },
    {
        "loss": 2.1053,
        "grad_norm": 2.418919086456299,
        "learning_rate": 1.809301791254172e-05,
        "epoch": 0.6028019971681944,
        "step": 8089
    },
    {
        "loss": 2.557,
        "grad_norm": 2.1600847244262695,
        "learning_rate": 1.8052669840945514e-05,
        "epoch": 0.6028765183694761,
        "step": 8090
    },
    {
        "loss": 2.3802,
        "grad_norm": 2.6213529109954834,
        "learning_rate": 1.8012362344155653e-05,
        "epoch": 0.6029510395707579,
        "step": 8091
    },
    {
        "loss": 1.8282,
        "grad_norm": 2.930887460708618,
        "learning_rate": 1.7972095442129833e-05,
        "epoch": 0.6030255607720396,
        "step": 8092
    },
    {
        "loss": 1.8236,
        "grad_norm": 3.535118579864502,
        "learning_rate": 1.793186915480548e-05,
        "epoch": 0.6031000819733214,
        "step": 8093
    },
    {
        "loss": 2.6382,
        "grad_norm": 2.138319253921509,
        "learning_rate": 1.7891683502099854e-05,
        "epoch": 0.6031746031746031,
        "step": 8094
    },
    {
        "loss": 2.3897,
        "grad_norm": 3.8584349155426025,
        "learning_rate": 1.7851538503910303e-05,
        "epoch": 0.603249124375885,
        "step": 8095
    },
    {
        "loss": 1.9441,
        "grad_norm": 2.754939317703247,
        "learning_rate": 1.7811434180113818e-05,
        "epoch": 0.6033236455771667,
        "step": 8096
    },
    {
        "loss": 2.1043,
        "grad_norm": 2.117387056350708,
        "learning_rate": 1.7771370550567535e-05,
        "epoch": 0.6033981667784485,
        "step": 8097
    },
    {
        "loss": 2.7323,
        "grad_norm": 2.1540777683258057,
        "learning_rate": 1.7731347635108132e-05,
        "epoch": 0.6034726879797302,
        "step": 8098
    },
    {
        "loss": 2.5761,
        "grad_norm": 3.3045430183410645,
        "learning_rate": 1.7691365453552224e-05,
        "epoch": 0.603547209181012,
        "step": 8099
    },
    {
        "loss": 2.4033,
        "grad_norm": 2.365241527557373,
        "learning_rate": 1.765142402569644e-05,
        "epoch": 0.6036217303822937,
        "step": 8100
    },
    {
        "loss": 2.4214,
        "grad_norm": 2.9418957233428955,
        "learning_rate": 1.7611523371317006e-05,
        "epoch": 0.6036962515835755,
        "step": 8101
    },
    {
        "loss": 2.412,
        "grad_norm": 3.028822422027588,
        "learning_rate": 1.7571663510170077e-05,
        "epoch": 0.6037707727848572,
        "step": 8102
    },
    {
        "loss": 1.7528,
        "grad_norm": 3.92024564743042,
        "learning_rate": 1.7531844461991564e-05,
        "epoch": 0.6038452939861391,
        "step": 8103
    },
    {
        "loss": 2.4885,
        "grad_norm": 4.141720294952393,
        "learning_rate": 1.749206624649712e-05,
        "epoch": 0.6039198151874208,
        "step": 8104
    },
    {
        "loss": 1.8566,
        "grad_norm": 3.5774505138397217,
        "learning_rate": 1.7452328883382362e-05,
        "epoch": 0.6039943363887026,
        "step": 8105
    },
    {
        "loss": 2.6853,
        "grad_norm": 2.6570701599121094,
        "learning_rate": 1.7412632392322402e-05,
        "epoch": 0.6040688575899843,
        "step": 8106
    },
    {
        "loss": 2.1497,
        "grad_norm": 3.2254996299743652,
        "learning_rate": 1.7372976792972427e-05,
        "epoch": 0.6041433787912661,
        "step": 8107
    },
    {
        "loss": 2.3788,
        "grad_norm": 3.1075310707092285,
        "learning_rate": 1.7333362104967177e-05,
        "epoch": 0.6042178999925478,
        "step": 8108
    },
    {
        "loss": 2.1855,
        "grad_norm": 1.8953741788864136,
        "learning_rate": 1.729378834792107e-05,
        "epoch": 0.6042924211938296,
        "step": 8109
    },
    {
        "loss": 2.3725,
        "grad_norm": 2.0750255584716797,
        "learning_rate": 1.7254255541428554e-05,
        "epoch": 0.6043669423951115,
        "step": 8110
    },
    {
        "loss": 2.0651,
        "grad_norm": 1.8307061195373535,
        "learning_rate": 1.7214763705063485e-05,
        "epoch": 0.6044414635963932,
        "step": 8111
    },
    {
        "loss": 1.7971,
        "grad_norm": 2.3067591190338135,
        "learning_rate": 1.7175312858379632e-05,
        "epoch": 0.604515984797675,
        "step": 8112
    },
    {
        "loss": 2.4606,
        "grad_norm": 3.2753522396087646,
        "learning_rate": 1.7135903020910382e-05,
        "epoch": 0.6045905059989567,
        "step": 8113
    },
    {
        "loss": 2.1702,
        "grad_norm": 2.2664215564727783,
        "learning_rate": 1.709653421216879e-05,
        "epoch": 0.6046650272002385,
        "step": 8114
    },
    {
        "loss": 2.1013,
        "grad_norm": 2.634277105331421,
        "learning_rate": 1.705720645164771e-05,
        "epoch": 0.6047395484015202,
        "step": 8115
    },
    {
        "loss": 2.318,
        "grad_norm": 2.4568214416503906,
        "learning_rate": 1.70179197588195e-05,
        "epoch": 0.604814069602802,
        "step": 8116
    },
    {
        "loss": 2.1148,
        "grad_norm": 2.65313982963562,
        "learning_rate": 1.697867415313643e-05,
        "epoch": 0.6048885908040837,
        "step": 8117
    },
    {
        "loss": 2.4559,
        "grad_norm": 2.7329719066619873,
        "learning_rate": 1.6939469654030217e-05,
        "epoch": 0.6049631120053656,
        "step": 8118
    },
    {
        "loss": 2.2552,
        "grad_norm": 2.529613733291626,
        "learning_rate": 1.6900306280912214e-05,
        "epoch": 0.6050376332066473,
        "step": 8119
    },
    {
        "loss": 2.2647,
        "grad_norm": 2.860846519470215,
        "learning_rate": 1.6861184053173662e-05,
        "epoch": 0.6051121544079291,
        "step": 8120
    },
    {
        "loss": 2.6681,
        "grad_norm": 1.8589361906051636,
        "learning_rate": 1.6822102990185152e-05,
        "epoch": 0.6051866756092108,
        "step": 8121
    },
    {
        "loss": 1.9169,
        "grad_norm": 3.217064619064331,
        "learning_rate": 1.678306311129694e-05,
        "epoch": 0.6052611968104926,
        "step": 8122
    },
    {
        "loss": 2.1403,
        "grad_norm": 2.6802423000335693,
        "learning_rate": 1.6744064435839068e-05,
        "epoch": 0.6053357180117743,
        "step": 8123
    },
    {
        "loss": 1.6502,
        "grad_norm": 3.296149253845215,
        "learning_rate": 1.6705106983120977e-05,
        "epoch": 0.6054102392130561,
        "step": 8124
    },
    {
        "loss": 1.5242,
        "grad_norm": 3.1611318588256836,
        "learning_rate": 1.6666190772431844e-05,
        "epoch": 0.6054847604143379,
        "step": 8125
    },
    {
        "loss": 2.0279,
        "grad_norm": 3.8193328380584717,
        "learning_rate": 1.6627315823040302e-05,
        "epoch": 0.6055592816156197,
        "step": 8126
    },
    {
        "loss": 2.4839,
        "grad_norm": 2.619431734085083,
        "learning_rate": 1.6588482154194562e-05,
        "epoch": 0.6056338028169014,
        "step": 8127
    },
    {
        "loss": 2.4399,
        "grad_norm": 2.217874050140381,
        "learning_rate": 1.6549689785122613e-05,
        "epoch": 0.6057083240181832,
        "step": 8128
    },
    {
        "loss": 2.1781,
        "grad_norm": 3.6904873847961426,
        "learning_rate": 1.6510938735031633e-05,
        "epoch": 0.6057828452194649,
        "step": 8129
    },
    {
        "loss": 2.2094,
        "grad_norm": 4.447644233703613,
        "learning_rate": 1.6472229023108686e-05,
        "epoch": 0.6058573664207467,
        "step": 8130
    },
    {
        "loss": 1.9401,
        "grad_norm": 3.7502658367156982,
        "learning_rate": 1.6433560668520176e-05,
        "epoch": 0.6059318876220284,
        "step": 8131
    },
    {
        "loss": 2.4522,
        "grad_norm": 2.2817437648773193,
        "learning_rate": 1.639493369041203e-05,
        "epoch": 0.6060064088233102,
        "step": 8132
    },
    {
        "loss": 1.9865,
        "grad_norm": 3.6266403198242188,
        "learning_rate": 1.635634810790977e-05,
        "epoch": 0.606080930024592,
        "step": 8133
    },
    {
        "loss": 2.2264,
        "grad_norm": 2.9151201248168945,
        "learning_rate": 1.6317803940118327e-05,
        "epoch": 0.6061554512258738,
        "step": 8134
    },
    {
        "loss": 2.5651,
        "grad_norm": 2.0224292278289795,
        "learning_rate": 1.62793012061223e-05,
        "epoch": 0.6062299724271555,
        "step": 8135
    },
    {
        "loss": 2.2333,
        "grad_norm": 2.5293266773223877,
        "learning_rate": 1.624083992498554e-05,
        "epoch": 0.6063044936284373,
        "step": 8136
    },
    {
        "loss": 2.7059,
        "grad_norm": 2.1987740993499756,
        "learning_rate": 1.6202420115751447e-05,
        "epoch": 0.606379014829719,
        "step": 8137
    },
    {
        "loss": 2.0173,
        "grad_norm": 3.684695243835449,
        "learning_rate": 1.6164041797443073e-05,
        "epoch": 0.6064535360310008,
        "step": 8138
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.189605951309204,
        "learning_rate": 1.612570498906264e-05,
        "epoch": 0.6065280572322825,
        "step": 8139
    },
    {
        "loss": 2.1924,
        "grad_norm": 2.6853692531585693,
        "learning_rate": 1.6087409709592106e-05,
        "epoch": 0.6066025784335644,
        "step": 8140
    },
    {
        "loss": 2.3397,
        "grad_norm": 2.79333233833313,
        "learning_rate": 1.6049155977992626e-05,
        "epoch": 0.6066770996348461,
        "step": 8141
    },
    {
        "loss": 2.6074,
        "grad_norm": 3.664276599884033,
        "learning_rate": 1.601094381320486e-05,
        "epoch": 0.6067516208361279,
        "step": 8142
    },
    {
        "loss": 2.03,
        "grad_norm": 2.02549409866333,
        "learning_rate": 1.5972773234148953e-05,
        "epoch": 0.6068261420374096,
        "step": 8143
    },
    {
        "loss": 2.5462,
        "grad_norm": 2.075807809829712,
        "learning_rate": 1.5934644259724363e-05,
        "epoch": 0.6069006632386914,
        "step": 8144
    },
    {
        "loss": 1.8182,
        "grad_norm": 3.2409768104553223,
        "learning_rate": 1.5896556908810034e-05,
        "epoch": 0.6069751844399732,
        "step": 8145
    },
    {
        "loss": 2.1807,
        "grad_norm": 2.653297185897827,
        "learning_rate": 1.5858511200264238e-05,
        "epoch": 0.6070497056412549,
        "step": 8146
    },
    {
        "loss": 2.5956,
        "grad_norm": 3.243774652481079,
        "learning_rate": 1.582050715292458e-05,
        "epoch": 0.6071242268425368,
        "step": 8147
    },
    {
        "loss": 2.7637,
        "grad_norm": 2.136591672897339,
        "learning_rate": 1.5782544785608256e-05,
        "epoch": 0.6071987480438185,
        "step": 8148
    },
    {
        "loss": 2.0015,
        "grad_norm": 3.241826057434082,
        "learning_rate": 1.5744624117111606e-05,
        "epoch": 0.6072732692451003,
        "step": 8149
    },
    {
        "loss": 2.5837,
        "grad_norm": 1.501821517944336,
        "learning_rate": 1.5706745166210324e-05,
        "epoch": 0.607347790446382,
        "step": 8150
    },
    {
        "loss": 1.7582,
        "grad_norm": 3.3788883686065674,
        "learning_rate": 1.5668907951659674e-05,
        "epoch": 0.6074223116476638,
        "step": 8151
    },
    {
        "loss": 2.5106,
        "grad_norm": 4.310866832733154,
        "learning_rate": 1.5631112492193945e-05,
        "epoch": 0.6074968328489455,
        "step": 8152
    },
    {
        "loss": 2.3397,
        "grad_norm": 3.2418835163116455,
        "learning_rate": 1.5593358806527016e-05,
        "epoch": 0.6075713540502273,
        "step": 8153
    },
    {
        "loss": 2.3339,
        "grad_norm": 3.2422237396240234,
        "learning_rate": 1.555564691335195e-05,
        "epoch": 0.607645875251509,
        "step": 8154
    },
    {
        "loss": 2.363,
        "grad_norm": 4.921991348266602,
        "learning_rate": 1.5517976831341063e-05,
        "epoch": 0.6077203964527909,
        "step": 8155
    },
    {
        "loss": 2.7052,
        "grad_norm": 2.208597421646118,
        "learning_rate": 1.5480348579146143e-05,
        "epoch": 0.6077949176540726,
        "step": 8156
    },
    {
        "loss": 2.6834,
        "grad_norm": 2.7329328060150146,
        "learning_rate": 1.5442762175398063e-05,
        "epoch": 0.6078694388553544,
        "step": 8157
    },
    {
        "loss": 1.4838,
        "grad_norm": 2.4126389026641846,
        "learning_rate": 1.5405217638707236e-05,
        "epoch": 0.6079439600566361,
        "step": 8158
    },
    {
        "loss": 2.186,
        "grad_norm": 2.1231493949890137,
        "learning_rate": 1.5367714987663097e-05,
        "epoch": 0.6080184812579179,
        "step": 8159
    },
    {
        "loss": 1.867,
        "grad_norm": 3.279564380645752,
        "learning_rate": 1.5330254240834363e-05,
        "epoch": 0.6080930024591996,
        "step": 8160
    },
    {
        "loss": 2.5035,
        "grad_norm": 2.323908567428589,
        "learning_rate": 1.5292835416769236e-05,
        "epoch": 0.6081675236604814,
        "step": 8161
    },
    {
        "loss": 2.6143,
        "grad_norm": 2.297943353652954,
        "learning_rate": 1.5255458533994893e-05,
        "epoch": 0.6082420448617631,
        "step": 8162
    },
    {
        "loss": 2.3845,
        "grad_norm": 2.9881906509399414,
        "learning_rate": 1.521812361101791e-05,
        "epoch": 0.608316566063045,
        "step": 8163
    },
    {
        "loss": 2.2766,
        "grad_norm": 2.353121519088745,
        "learning_rate": 1.5180830666323998e-05,
        "epoch": 0.6083910872643267,
        "step": 8164
    },
    {
        "loss": 1.7587,
        "grad_norm": 2.1401946544647217,
        "learning_rate": 1.5143579718378098e-05,
        "epoch": 0.6084656084656085,
        "step": 8165
    },
    {
        "loss": 2.7122,
        "grad_norm": 2.054887056350708,
        "learning_rate": 1.5106370785624413e-05,
        "epoch": 0.6085401296668902,
        "step": 8166
    },
    {
        "loss": 1.5726,
        "grad_norm": 3.7470531463623047,
        "learning_rate": 1.5069203886486227e-05,
        "epoch": 0.608614650868172,
        "step": 8167
    },
    {
        "loss": 2.549,
        "grad_norm": 2.6384804248809814,
        "learning_rate": 1.5032079039366209e-05,
        "epoch": 0.6086891720694537,
        "step": 8168
    },
    {
        "loss": 2.8809,
        "grad_norm": 3.6119048595428467,
        "learning_rate": 1.4994996262646034e-05,
        "epoch": 0.6087636932707355,
        "step": 8169
    },
    {
        "loss": 1.9403,
        "grad_norm": 2.5679867267608643,
        "learning_rate": 1.4957955574686522e-05,
        "epoch": 0.6088382144720172,
        "step": 8170
    },
    {
        "loss": 1.6333,
        "grad_norm": 3.85201096534729,
        "learning_rate": 1.4920956993827861e-05,
        "epoch": 0.6089127356732991,
        "step": 8171
    },
    {
        "loss": 1.6853,
        "grad_norm": 3.737009286880493,
        "learning_rate": 1.4884000538389186e-05,
        "epoch": 0.6089872568745808,
        "step": 8172
    },
    {
        "loss": 2.2686,
        "grad_norm": 3.4329168796539307,
        "learning_rate": 1.4847086226668872e-05,
        "epoch": 0.6090617780758626,
        "step": 8173
    },
    {
        "loss": 1.9812,
        "grad_norm": 3.090378761291504,
        "learning_rate": 1.4810214076944396e-05,
        "epoch": 0.6091362992771443,
        "step": 8174
    },
    {
        "loss": 1.8945,
        "grad_norm": 4.002479553222656,
        "learning_rate": 1.4773384107472343e-05,
        "epoch": 0.6092108204784261,
        "step": 8175
    },
    {
        "loss": 2.6475,
        "grad_norm": 1.5391011238098145,
        "learning_rate": 1.4736596336488473e-05,
        "epoch": 0.6092853416797078,
        "step": 8176
    },
    {
        "loss": 2.6058,
        "grad_norm": 2.578328847885132,
        "learning_rate": 1.4699850782207525e-05,
        "epoch": 0.6093598628809896,
        "step": 8177
    },
    {
        "loss": 2.2806,
        "grad_norm": 3.9444096088409424,
        "learning_rate": 1.4663147462823578e-05,
        "epoch": 0.6094343840822714,
        "step": 8178
    },
    {
        "loss": 2.6306,
        "grad_norm": 3.281250238418579,
        "learning_rate": 1.4626486396509575e-05,
        "epoch": 0.6095089052835532,
        "step": 8179
    },
    {
        "loss": 1.6409,
        "grad_norm": 4.431107521057129,
        "learning_rate": 1.4589867601417528e-05,
        "epoch": 0.609583426484835,
        "step": 8180
    },
    {
        "loss": 2.3536,
        "grad_norm": 3.4694135189056396,
        "learning_rate": 1.4553291095678767e-05,
        "epoch": 0.6096579476861167,
        "step": 8181
    },
    {
        "loss": 2.3448,
        "grad_norm": 3.2024495601654053,
        "learning_rate": 1.4516756897403416e-05,
        "epoch": 0.6097324688873985,
        "step": 8182
    },
    {
        "loss": 2.2904,
        "grad_norm": 3.0485026836395264,
        "learning_rate": 1.4480265024680706e-05,
        "epoch": 0.6098069900886802,
        "step": 8183
    },
    {
        "loss": 2.4972,
        "grad_norm": 2.0786774158477783,
        "learning_rate": 1.4443815495579094e-05,
        "epoch": 0.609881511289962,
        "step": 8184
    },
    {
        "loss": 2.7255,
        "grad_norm": 2.6934571266174316,
        "learning_rate": 1.4407408328145799e-05,
        "epoch": 0.6099560324912437,
        "step": 8185
    },
    {
        "loss": 2.3861,
        "grad_norm": 2.9479317665100098,
        "learning_rate": 1.4371043540407293e-05,
        "epoch": 0.6100305536925256,
        "step": 8186
    },
    {
        "loss": 1.8936,
        "grad_norm": 2.7298309803009033,
        "learning_rate": 1.433472115036898e-05,
        "epoch": 0.6101050748938073,
        "step": 8187
    },
    {
        "loss": 2.4295,
        "grad_norm": 3.3766980171203613,
        "learning_rate": 1.4298441176015121e-05,
        "epoch": 0.6101795960950891,
        "step": 8188
    },
    {
        "loss": 2.1247,
        "grad_norm": 3.282921552658081,
        "learning_rate": 1.4262203635309302e-05,
        "epoch": 0.6102541172963708,
        "step": 8189
    },
    {
        "loss": 2.2177,
        "grad_norm": 2.611417531967163,
        "learning_rate": 1.4226008546193769e-05,
        "epoch": 0.6103286384976526,
        "step": 8190
    },
    {
        "loss": 1.8857,
        "grad_norm": 3.1092634201049805,
        "learning_rate": 1.4189855926590034e-05,
        "epoch": 0.6104031596989343,
        "step": 8191
    },
    {
        "loss": 2.3009,
        "grad_norm": 3.0187106132507324,
        "learning_rate": 1.4153745794398377e-05,
        "epoch": 0.6104776809002161,
        "step": 8192
    },
    {
        "loss": 2.04,
        "grad_norm": 3.393606185913086,
        "learning_rate": 1.4117678167498072e-05,
        "epoch": 0.6105522021014979,
        "step": 8193
    },
    {
        "loss": 1.6937,
        "grad_norm": 3.3430991172790527,
        "learning_rate": 1.4081653063747446e-05,
        "epoch": 0.6106267233027797,
        "step": 8194
    },
    {
        "loss": 2.2073,
        "grad_norm": 3.354282855987549,
        "learning_rate": 1.4045670500983621e-05,
        "epoch": 0.6107012445040614,
        "step": 8195
    },
    {
        "loss": 2.5569,
        "grad_norm": 1.917090654373169,
        "learning_rate": 1.4009730497022877e-05,
        "epoch": 0.6107757657053432,
        "step": 8196
    },
    {
        "loss": 2.4584,
        "grad_norm": 2.135816812515259,
        "learning_rate": 1.3973833069660181e-05,
        "epoch": 0.6108502869066249,
        "step": 8197
    },
    {
        "loss": 1.7499,
        "grad_norm": 1.6999671459197998,
        "learning_rate": 1.3937978236669468e-05,
        "epoch": 0.6109248081079067,
        "step": 8198
    },
    {
        "loss": 1.6921,
        "grad_norm": 3.9168570041656494,
        "learning_rate": 1.3902166015803808e-05,
        "epoch": 0.6109993293091884,
        "step": 8199
    },
    {
        "loss": 1.698,
        "grad_norm": 4.173089027404785,
        "learning_rate": 1.3866396424794847e-05,
        "epoch": 0.6110738505104703,
        "step": 8200
    },
    {
        "loss": 2.4044,
        "grad_norm": 2.5366642475128174,
        "learning_rate": 1.383066948135341e-05,
        "epoch": 0.611148371711752,
        "step": 8201
    },
    {
        "loss": 1.4234,
        "grad_norm": 1.674662470817566,
        "learning_rate": 1.3794985203169031e-05,
        "epoch": 0.6112228929130338,
        "step": 8202
    },
    {
        "loss": 2.2746,
        "grad_norm": 3.204765558242798,
        "learning_rate": 1.3759343607910125e-05,
        "epoch": 0.6112974141143155,
        "step": 8203
    },
    {
        "loss": 2.8888,
        "grad_norm": 3.3126158714294434,
        "learning_rate": 1.3723744713224052e-05,
        "epoch": 0.6113719353155973,
        "step": 8204
    },
    {
        "loss": 2.3496,
        "grad_norm": 1.9722659587860107,
        "learning_rate": 1.3688188536736968e-05,
        "epoch": 0.611446456516879,
        "step": 8205
    },
    {
        "loss": 2.6437,
        "grad_norm": 3.30802321434021,
        "learning_rate": 1.3652675096053935e-05,
        "epoch": 0.6115209777181608,
        "step": 8206
    },
    {
        "loss": 2.877,
        "grad_norm": 3.5332882404327393,
        "learning_rate": 1.3617204408758788e-05,
        "epoch": 0.6115954989194425,
        "step": 8207
    },
    {
        "loss": 1.8199,
        "grad_norm": 3.5430424213409424,
        "learning_rate": 1.3581776492414166e-05,
        "epoch": 0.6116700201207244,
        "step": 8208
    },
    {
        "loss": 2.2716,
        "grad_norm": 3.2564966678619385,
        "learning_rate": 1.3546391364561729e-05,
        "epoch": 0.6117445413220061,
        "step": 8209
    },
    {
        "loss": 2.3437,
        "grad_norm": 3.1911888122558594,
        "learning_rate": 1.351104904272168e-05,
        "epoch": 0.6118190625232879,
        "step": 8210
    },
    {
        "loss": 2.8228,
        "grad_norm": 2.1953306198120117,
        "learning_rate": 1.34757495443933e-05,
        "epoch": 0.6118935837245696,
        "step": 8211
    },
    {
        "loss": 2.9369,
        "grad_norm": 2.626577615737915,
        "learning_rate": 1.3440492887054435e-05,
        "epoch": 0.6119681049258514,
        "step": 8212
    },
    {
        "loss": 2.2837,
        "grad_norm": 2.9224655628204346,
        "learning_rate": 1.340527908816177e-05,
        "epoch": 0.6120426261271331,
        "step": 8213
    },
    {
        "loss": 1.7652,
        "grad_norm": 2.8090336322784424,
        "learning_rate": 1.337010816515093e-05,
        "epoch": 0.6121171473284149,
        "step": 8214
    },
    {
        "loss": 1.7723,
        "grad_norm": 3.0143721103668213,
        "learning_rate": 1.3334980135436115e-05,
        "epoch": 0.6121916685296968,
        "step": 8215
    },
    {
        "loss": 2.5618,
        "grad_norm": 2.7459168434143066,
        "learning_rate": 1.3299895016410314e-05,
        "epoch": 0.6122661897309785,
        "step": 8216
    },
    {
        "loss": 2.3853,
        "grad_norm": 2.2671144008636475,
        "learning_rate": 1.3264852825445417e-05,
        "epoch": 0.6123407109322603,
        "step": 8217
    },
    {
        "loss": 2.3143,
        "grad_norm": 3.171689510345459,
        "learning_rate": 1.322985357989186e-05,
        "epoch": 0.612415232133542,
        "step": 8218
    },
    {
        "loss": 2.7615,
        "grad_norm": 3.6796445846557617,
        "learning_rate": 1.3194897297079067e-05,
        "epoch": 0.6124897533348238,
        "step": 8219
    },
    {
        "loss": 2.5906,
        "grad_norm": 2.0282845497131348,
        "learning_rate": 1.3159983994314917e-05,
        "epoch": 0.6125642745361055,
        "step": 8220
    },
    {
        "loss": 2.3653,
        "grad_norm": 3.206413507461548,
        "learning_rate": 1.3125113688886092e-05,
        "epoch": 0.6126387957373873,
        "step": 8221
    },
    {
        "loss": 1.9741,
        "grad_norm": 2.375112533569336,
        "learning_rate": 1.3090286398058149e-05,
        "epoch": 0.612713316938669,
        "step": 8222
    },
    {
        "loss": 2.1316,
        "grad_norm": 3.304149866104126,
        "learning_rate": 1.3055502139075149e-05,
        "epoch": 0.6127878381399509,
        "step": 8223
    },
    {
        "loss": 2.9981,
        "grad_norm": 3.37841796875,
        "learning_rate": 1.3020760929159937e-05,
        "epoch": 0.6128623593412326,
        "step": 8224
    },
    {
        "loss": 2.0879,
        "grad_norm": 1.8944159746170044,
        "learning_rate": 1.2986062785514019e-05,
        "epoch": 0.6129368805425144,
        "step": 8225
    },
    {
        "loss": 2.4305,
        "grad_norm": 1.9063193798065186,
        "learning_rate": 1.2951407725317566e-05,
        "epoch": 0.6130114017437961,
        "step": 8226
    },
    {
        "loss": 1.5753,
        "grad_norm": 3.2211833000183105,
        "learning_rate": 1.291679576572945e-05,
        "epoch": 0.6130859229450779,
        "step": 8227
    },
    {
        "loss": 2.4463,
        "grad_norm": 2.982104778289795,
        "learning_rate": 1.288222692388712e-05,
        "epoch": 0.6131604441463596,
        "step": 8228
    },
    {
        "loss": 2.3156,
        "grad_norm": 3.693922996520996,
        "learning_rate": 1.284770121690687e-05,
        "epoch": 0.6132349653476414,
        "step": 8229
    },
    {
        "loss": 3.0727,
        "grad_norm": 2.6641364097595215,
        "learning_rate": 1.2813218661883441e-05,
        "epoch": 0.6133094865489231,
        "step": 8230
    },
    {
        "loss": 2.4,
        "grad_norm": 2.163698434829712,
        "learning_rate": 1.2778779275890196e-05,
        "epoch": 0.613384007750205,
        "step": 8231
    },
    {
        "loss": 1.4302,
        "grad_norm": 2.652628183364868,
        "learning_rate": 1.274438307597936e-05,
        "epoch": 0.6134585289514867,
        "step": 8232
    },
    {
        "loss": 2.1566,
        "grad_norm": 3.509594202041626,
        "learning_rate": 1.2710030079181511e-05,
        "epoch": 0.6135330501527685,
        "step": 8233
    },
    {
        "loss": 1.9499,
        "grad_norm": 3.910015106201172,
        "learning_rate": 1.2675720302505988e-05,
        "epoch": 0.6136075713540502,
        "step": 8234
    },
    {
        "loss": 2.94,
        "grad_norm": 2.3813343048095703,
        "learning_rate": 1.2641453762940669e-05,
        "epoch": 0.613682092555332,
        "step": 8235
    },
    {
        "loss": 2.1129,
        "grad_norm": 2.8567259311676025,
        "learning_rate": 1.2607230477452014e-05,
        "epoch": 0.6137566137566137,
        "step": 8236
    },
    {
        "loss": 2.5299,
        "grad_norm": 1.9563549757003784,
        "learning_rate": 1.2573050462985136e-05,
        "epoch": 0.6138311349578955,
        "step": 8237
    },
    {
        "loss": 1.9825,
        "grad_norm": 3.2516090869903564,
        "learning_rate": 1.2538913736463597e-05,
        "epoch": 0.6139056561591772,
        "step": 8238
    },
    {
        "loss": 2.3174,
        "grad_norm": 2.5262179374694824,
        "learning_rate": 1.2504820314789733e-05,
        "epoch": 0.6139801773604591,
        "step": 8239
    },
    {
        "loss": 2.0202,
        "grad_norm": 3.3948264122009277,
        "learning_rate": 1.2470770214844251e-05,
        "epoch": 0.6140546985617408,
        "step": 8240
    },
    {
        "loss": 1.9461,
        "grad_norm": 2.936176061630249,
        "learning_rate": 1.24367634534864e-05,
        "epoch": 0.6141292197630226,
        "step": 8241
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.5987420082092285,
        "learning_rate": 1.2402800047554208e-05,
        "epoch": 0.6142037409643043,
        "step": 8242
    },
    {
        "loss": 2.4348,
        "grad_norm": 2.416754722595215,
        "learning_rate": 1.2368880013863937e-05,
        "epoch": 0.6142782621655861,
        "step": 8243
    },
    {
        "loss": 1.9276,
        "grad_norm": 3.8656411170959473,
        "learning_rate": 1.2335003369210507e-05,
        "epoch": 0.6143527833668678,
        "step": 8244
    },
    {
        "loss": 2.8616,
        "grad_norm": 2.043402671813965,
        "learning_rate": 1.2301170130367445e-05,
        "epoch": 0.6144273045681496,
        "step": 8245
    },
    {
        "loss": 1.4731,
        "grad_norm": 4.052769660949707,
        "learning_rate": 1.2267380314086585e-05,
        "epoch": 0.6145018257694314,
        "step": 8246
    },
    {
        "loss": 1.9627,
        "grad_norm": 3.668909788131714,
        "learning_rate": 1.2233633937098465e-05,
        "epoch": 0.6145763469707132,
        "step": 8247
    },
    {
        "loss": 1.8317,
        "grad_norm": 2.894524097442627,
        "learning_rate": 1.2199931016111998e-05,
        "epoch": 0.6146508681719949,
        "step": 8248
    },
    {
        "loss": 2.8457,
        "grad_norm": 2.5725531578063965,
        "learning_rate": 1.2166271567814502e-05,
        "epoch": 0.6147253893732767,
        "step": 8249
    },
    {
        "loss": 1.7861,
        "grad_norm": 2.325052499771118,
        "learning_rate": 1.2132655608872023e-05,
        "epoch": 0.6147999105745585,
        "step": 8250
    },
    {
        "loss": 2.3112,
        "grad_norm": 2.7363905906677246,
        "learning_rate": 1.2099083155928804e-05,
        "epoch": 0.6148744317758402,
        "step": 8251
    },
    {
        "loss": 1.9564,
        "grad_norm": 2.6737139225006104,
        "learning_rate": 1.2065554225607767e-05,
        "epoch": 0.614948952977122,
        "step": 8252
    },
    {
        "loss": 0.8076,
        "grad_norm": 4.273701190948486,
        "learning_rate": 1.2032068834510135e-05,
        "epoch": 0.6150234741784038,
        "step": 8253
    },
    {
        "loss": 2.0877,
        "grad_norm": 2.5206470489501953,
        "learning_rate": 1.1998626999215611e-05,
        "epoch": 0.6150979953796856,
        "step": 8254
    },
    {
        "loss": 2.9467,
        "grad_norm": 2.707812786102295,
        "learning_rate": 1.1965228736282375e-05,
        "epoch": 0.6151725165809673,
        "step": 8255
    },
    {
        "loss": 2.0199,
        "grad_norm": 3.286532402038574,
        "learning_rate": 1.1931874062246951e-05,
        "epoch": 0.6152470377822491,
        "step": 8256
    },
    {
        "loss": 2.9821,
        "grad_norm": 3.442394256591797,
        "learning_rate": 1.1898562993624373e-05,
        "epoch": 0.6153215589835308,
        "step": 8257
    },
    {
        "loss": 2.4062,
        "grad_norm": 2.389019012451172,
        "learning_rate": 1.186529554690804e-05,
        "epoch": 0.6153960801848126,
        "step": 8258
    },
    {
        "loss": 2.5553,
        "grad_norm": 2.219721794128418,
        "learning_rate": 1.1832071738569672e-05,
        "epoch": 0.6154706013860943,
        "step": 8259
    },
    {
        "loss": 2.6224,
        "grad_norm": 3.376199960708618,
        "learning_rate": 1.1798891585059602e-05,
        "epoch": 0.6155451225873761,
        "step": 8260
    },
    {
        "loss": 2.6358,
        "grad_norm": 2.013314723968506,
        "learning_rate": 1.1765755102806275e-05,
        "epoch": 0.6156196437886579,
        "step": 8261
    },
    {
        "loss": 2.3987,
        "grad_norm": 4.70762300491333,
        "learning_rate": 1.1732662308216768e-05,
        "epoch": 0.6156941649899397,
        "step": 8262
    },
    {
        "loss": 3.0668,
        "grad_norm": 3.540447950363159,
        "learning_rate": 1.1699613217676364e-05,
        "epoch": 0.6157686861912214,
        "step": 8263
    },
    {
        "loss": 1.9154,
        "grad_norm": 2.801584482192993,
        "learning_rate": 1.1666607847548728e-05,
        "epoch": 0.6158432073925032,
        "step": 8264
    },
    {
        "loss": 2.4687,
        "grad_norm": 2.419146776199341,
        "learning_rate": 1.163364621417592e-05,
        "epoch": 0.6159177285937849,
        "step": 8265
    },
    {
        "loss": 2.8818,
        "grad_norm": 4.998250484466553,
        "learning_rate": 1.1600728333878296e-05,
        "epoch": 0.6159922497950667,
        "step": 8266
    },
    {
        "loss": 2.1749,
        "grad_norm": 3.864497184753418,
        "learning_rate": 1.1567854222954622e-05,
        "epoch": 0.6160667709963484,
        "step": 8267
    },
    {
        "loss": 2.2185,
        "grad_norm": 2.992943525314331,
        "learning_rate": 1.1535023897681918e-05,
        "epoch": 0.6161412921976303,
        "step": 8268
    },
    {
        "loss": 1.8611,
        "grad_norm": 3.985822916030884,
        "learning_rate": 1.1502237374315505e-05,
        "epoch": 0.616215813398912,
        "step": 8269
    },
    {
        "loss": 2.6008,
        "grad_norm": 2.880671977996826,
        "learning_rate": 1.1469494669089165e-05,
        "epoch": 0.6162903346001938,
        "step": 8270
    },
    {
        "loss": 2.3813,
        "grad_norm": 3.4419445991516113,
        "learning_rate": 1.1436795798214773e-05,
        "epoch": 0.6163648558014755,
        "step": 8271
    },
    {
        "loss": 2.8505,
        "grad_norm": 4.286209583282471,
        "learning_rate": 1.1404140777882756e-05,
        "epoch": 0.6164393770027573,
        "step": 8272
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.252568483352661,
        "learning_rate": 1.1371529624261578e-05,
        "epoch": 0.616513898204039,
        "step": 8273
    },
    {
        "loss": 2.2373,
        "grad_norm": 3.29945707321167,
        "learning_rate": 1.1338962353498073e-05,
        "epoch": 0.6165884194053208,
        "step": 8274
    },
    {
        "loss": 1.7165,
        "grad_norm": 3.6889848709106445,
        "learning_rate": 1.130643898171746e-05,
        "epoch": 0.6166629406066025,
        "step": 8275
    },
    {
        "loss": 2.1998,
        "grad_norm": 3.6401336193084717,
        "learning_rate": 1.1273959525023037e-05,
        "epoch": 0.6167374618078844,
        "step": 8276
    },
    {
        "loss": 2.2004,
        "grad_norm": 1.94851815700531,
        "learning_rate": 1.1241523999496439e-05,
        "epoch": 0.6168119830091661,
        "step": 8277
    },
    {
        "loss": 2.4668,
        "grad_norm": 2.384455919265747,
        "learning_rate": 1.1209132421197665e-05,
        "epoch": 0.6168865042104479,
        "step": 8278
    },
    {
        "loss": 1.9833,
        "grad_norm": 2.4006786346435547,
        "learning_rate": 1.1176784806164676e-05,
        "epoch": 0.6169610254117296,
        "step": 8279
    },
    {
        "loss": 1.3596,
        "grad_norm": 3.302619695663452,
        "learning_rate": 1.1144481170414e-05,
        "epoch": 0.6170355466130114,
        "step": 8280
    },
    {
        "loss": 1.2414,
        "grad_norm": 3.2824738025665283,
        "learning_rate": 1.1112221529940159e-05,
        "epoch": 0.6171100678142931,
        "step": 8281
    },
    {
        "loss": 2.16,
        "grad_norm": 3.8062403202056885,
        "learning_rate": 1.1080005900715906e-05,
        "epoch": 0.6171845890155749,
        "step": 8282
    },
    {
        "loss": 2.3938,
        "grad_norm": 2.6756765842437744,
        "learning_rate": 1.104783429869235e-05,
        "epoch": 0.6172591102168566,
        "step": 8283
    },
    {
        "loss": 2.3765,
        "grad_norm": 2.157970905303955,
        "learning_rate": 1.1015706739798647e-05,
        "epoch": 0.6173336314181385,
        "step": 8284
    },
    {
        "loss": 2.5319,
        "grad_norm": 2.6229186058044434,
        "learning_rate": 1.098362323994223e-05,
        "epoch": 0.6174081526194202,
        "step": 8285
    },
    {
        "loss": 2.5166,
        "grad_norm": 2.520563840866089,
        "learning_rate": 1.0951583815008682e-05,
        "epoch": 0.617482673820702,
        "step": 8286
    },
    {
        "loss": 1.5493,
        "grad_norm": 4.402828216552734,
        "learning_rate": 1.0919588480861743e-05,
        "epoch": 0.6175571950219838,
        "step": 8287
    },
    {
        "loss": 2.9981,
        "grad_norm": 2.855252265930176,
        "learning_rate": 1.0887637253343386e-05,
        "epoch": 0.6176317162232655,
        "step": 8288
    },
    {
        "loss": 1.8839,
        "grad_norm": 4.510989189147949,
        "learning_rate": 1.0855730148273646e-05,
        "epoch": 0.6177062374245473,
        "step": 8289
    },
    {
        "loss": 2.6184,
        "grad_norm": 2.2721097469329834,
        "learning_rate": 1.0823867181450876e-05,
        "epoch": 0.617780758625829,
        "step": 8290
    },
    {
        "loss": 2.4308,
        "grad_norm": 2.114057779312134,
        "learning_rate": 1.0792048368651431e-05,
        "epoch": 0.6178552798271109,
        "step": 8291
    },
    {
        "loss": 2.5386,
        "grad_norm": 4.012732028961182,
        "learning_rate": 1.0760273725629777e-05,
        "epoch": 0.6179298010283926,
        "step": 8292
    },
    {
        "loss": 1.9583,
        "grad_norm": 3.1703670024871826,
        "learning_rate": 1.0728543268118706e-05,
        "epoch": 0.6180043222296744,
        "step": 8293
    },
    {
        "loss": 2.5601,
        "grad_norm": 3.1254098415374756,
        "learning_rate": 1.069685701182892e-05,
        "epoch": 0.6180788434309561,
        "step": 8294
    },
    {
        "loss": 2.2435,
        "grad_norm": 3.51763653755188,
        "learning_rate": 1.0665214972449366e-05,
        "epoch": 0.6181533646322379,
        "step": 8295
    },
    {
        "loss": 2.5255,
        "grad_norm": 1.9498963356018066,
        "learning_rate": 1.0633617165647048e-05,
        "epoch": 0.6182278858335196,
        "step": 8296
    },
    {
        "loss": 2.652,
        "grad_norm": 3.235065221786499,
        "learning_rate": 1.0602063607067026e-05,
        "epoch": 0.6183024070348014,
        "step": 8297
    },
    {
        "loss": 1.7925,
        "grad_norm": 2.450425624847412,
        "learning_rate": 1.0570554312332558e-05,
        "epoch": 0.6183769282360831,
        "step": 8298
    },
    {
        "loss": 1.7854,
        "grad_norm": 2.9894232749938965,
        "learning_rate": 1.0539089297044846e-05,
        "epoch": 0.618451449437365,
        "step": 8299
    },
    {
        "loss": 1.8072,
        "grad_norm": 3.46390438079834,
        "learning_rate": 1.0507668576783359e-05,
        "epoch": 0.6185259706386467,
        "step": 8300
    },
    {
        "loss": 2.2144,
        "grad_norm": 3.138485908508301,
        "learning_rate": 1.0476292167105495e-05,
        "epoch": 0.6186004918399285,
        "step": 8301
    },
    {
        "loss": 4.5388,
        "grad_norm": 4.218829154968262,
        "learning_rate": 1.044496008354665e-05,
        "epoch": 0.6186750130412102,
        "step": 8302
    },
    {
        "loss": 1.9719,
        "grad_norm": 2.9944987297058105,
        "learning_rate": 1.0413672341620483e-05,
        "epoch": 0.618749534242492,
        "step": 8303
    },
    {
        "loss": 3.3454,
        "grad_norm": 3.1386473178863525,
        "learning_rate": 1.0382428956818501e-05,
        "epoch": 0.6188240554437737,
        "step": 8304
    },
    {
        "loss": 2.3645,
        "grad_norm": 4.157098770141602,
        "learning_rate": 1.0351229944610407e-05,
        "epoch": 0.6188985766450555,
        "step": 8305
    },
    {
        "loss": 2.3857,
        "grad_norm": 3.94467830657959,
        "learning_rate": 1.032007532044379e-05,
        "epoch": 0.6189730978463373,
        "step": 8306
    },
    {
        "loss": 2.6521,
        "grad_norm": 2.341323137283325,
        "learning_rate": 1.0288965099744296e-05,
        "epoch": 0.6190476190476191,
        "step": 8307
    },
    {
        "loss": 1.694,
        "grad_norm": 4.045708656311035,
        "learning_rate": 1.0257899297915697e-05,
        "epoch": 0.6191221402489008,
        "step": 8308
    },
    {
        "loss": 2.4359,
        "grad_norm": 2.606442451477051,
        "learning_rate": 1.0226877930339662e-05,
        "epoch": 0.6191966614501826,
        "step": 8309
    },
    {
        "loss": 1.7241,
        "grad_norm": 4.8604416847229,
        "learning_rate": 1.0195901012375786e-05,
        "epoch": 0.6192711826514643,
        "step": 8310
    },
    {
        "loss": 2.4694,
        "grad_norm": 2.0996453762054443,
        "learning_rate": 1.016496855936191e-05,
        "epoch": 0.6193457038527461,
        "step": 8311
    },
    {
        "loss": 2.7747,
        "grad_norm": 2.1574838161468506,
        "learning_rate": 1.0134080586613581e-05,
        "epoch": 0.6194202250540278,
        "step": 8312
    },
    {
        "loss": 2.6295,
        "grad_norm": 2.3269436359405518,
        "learning_rate": 1.0103237109424546e-05,
        "epoch": 0.6194947462553096,
        "step": 8313
    },
    {
        "loss": 2.6269,
        "grad_norm": 2.9259963035583496,
        "learning_rate": 1.0072438143066376e-05,
        "epoch": 0.6195692674565914,
        "step": 8314
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.645235776901245,
        "learning_rate": 1.004168370278863e-05,
        "epoch": 0.6196437886578732,
        "step": 8315
    },
    {
        "loss": 2.1525,
        "grad_norm": 2.890439510345459,
        "learning_rate": 1.0010973803818879e-05,
        "epoch": 0.6197183098591549,
        "step": 8316
    },
    {
        "loss": 3.1219,
        "grad_norm": 4.648390293121338,
        "learning_rate": 9.980308461362541e-06,
        "epoch": 0.6197928310604367,
        "step": 8317
    },
    {
        "loss": 1.8961,
        "grad_norm": 2.377598285675049,
        "learning_rate": 9.949687690603094e-06,
        "epoch": 0.6198673522617184,
        "step": 8318
    },
    {
        "loss": 2.6091,
        "grad_norm": 2.696009635925293,
        "learning_rate": 9.919111506701851e-06,
        "epoch": 0.6199418734630002,
        "step": 8319
    },
    {
        "loss": 2.537,
        "grad_norm": 2.519869804382324,
        "learning_rate": 9.888579924798036e-06,
        "epoch": 0.6200163946642819,
        "step": 8320
    },
    {
        "loss": 2.8157,
        "grad_norm": 1.414923071861267,
        "learning_rate": 9.858092960008936e-06,
        "epoch": 0.6200909158655638,
        "step": 8321
    },
    {
        "loss": 1.6019,
        "grad_norm": 3.435534954071045,
        "learning_rate": 9.82765062742953e-06,
        "epoch": 0.6201654370668456,
        "step": 8322
    },
    {
        "loss": 2.3995,
        "grad_norm": 2.321026563644409,
        "learning_rate": 9.79725294213294e-06,
        "epoch": 0.6202399582681273,
        "step": 8323
    },
    {
        "loss": 2.6961,
        "grad_norm": 2.3088908195495605,
        "learning_rate": 9.766899919170014e-06,
        "epoch": 0.6203144794694091,
        "step": 8324
    },
    {
        "loss": 1.7701,
        "grad_norm": 3.2886269092559814,
        "learning_rate": 9.736591573569454e-06,
        "epoch": 0.6203890006706908,
        "step": 8325
    },
    {
        "loss": 1.8573,
        "grad_norm": 3.530682325363159,
        "learning_rate": 9.706327920337999e-06,
        "epoch": 0.6204635218719726,
        "step": 8326
    },
    {
        "loss": 2.1291,
        "grad_norm": 3.5041725635528564,
        "learning_rate": 9.67610897446013e-06,
        "epoch": 0.6205380430732543,
        "step": 8327
    },
    {
        "loss": 2.29,
        "grad_norm": 2.851846694946289,
        "learning_rate": 9.645934750898267e-06,
        "epoch": 0.6206125642745361,
        "step": 8328
    },
    {
        "loss": 1.8752,
        "grad_norm": 3.3676507472991943,
        "learning_rate": 9.615805264592636e-06,
        "epoch": 0.6206870854758179,
        "step": 8329
    },
    {
        "loss": 1.7694,
        "grad_norm": 2.3447229862213135,
        "learning_rate": 9.585720530461273e-06,
        "epoch": 0.6207616066770997,
        "step": 8330
    },
    {
        "loss": 2.3677,
        "grad_norm": 2.7612500190734863,
        "learning_rate": 9.555680563400237e-06,
        "epoch": 0.6208361278783814,
        "step": 8331
    },
    {
        "loss": 2.1093,
        "grad_norm": 2.506260871887207,
        "learning_rate": 9.5256853782832e-06,
        "epoch": 0.6209106490796632,
        "step": 8332
    },
    {
        "loss": 1.6117,
        "grad_norm": 3.194770097732544,
        "learning_rate": 9.495734989961847e-06,
        "epoch": 0.6209851702809449,
        "step": 8333
    },
    {
        "loss": 2.7356,
        "grad_norm": 2.8060665130615234,
        "learning_rate": 9.465829413265536e-06,
        "epoch": 0.6210596914822267,
        "step": 8334
    },
    {
        "loss": 2.3077,
        "grad_norm": 2.823637008666992,
        "learning_rate": 9.435968663001482e-06,
        "epoch": 0.6211342126835084,
        "step": 8335
    },
    {
        "loss": 2.4682,
        "grad_norm": 2.9317452907562256,
        "learning_rate": 9.406152753954789e-06,
        "epoch": 0.6212087338847903,
        "step": 8336
    },
    {
        "loss": 2.3574,
        "grad_norm": 2.606699228286743,
        "learning_rate": 9.376381700888248e-06,
        "epoch": 0.621283255086072,
        "step": 8337
    },
    {
        "loss": 2.4212,
        "grad_norm": 3.0982446670532227,
        "learning_rate": 9.346655518542435e-06,
        "epoch": 0.6213577762873538,
        "step": 8338
    },
    {
        "loss": 2.6405,
        "grad_norm": 3.275974988937378,
        "learning_rate": 9.316974221635855e-06,
        "epoch": 0.6214322974886355,
        "step": 8339
    },
    {
        "loss": 2.3083,
        "grad_norm": 2.538079261779785,
        "learning_rate": 9.287337824864573e-06,
        "epoch": 0.6215068186899173,
        "step": 8340
    },
    {
        "loss": 1.9416,
        "grad_norm": 3.1661152839660645,
        "learning_rate": 9.257746342902663e-06,
        "epoch": 0.621581339891199,
        "step": 8341
    },
    {
        "loss": 2.2799,
        "grad_norm": 2.981555461883545,
        "learning_rate": 9.228199790401771e-06,
        "epoch": 0.6216558610924808,
        "step": 8342
    },
    {
        "loss": 2.5973,
        "grad_norm": 3.1076555252075195,
        "learning_rate": 9.198698181991317e-06,
        "epoch": 0.6217303822937625,
        "step": 8343
    },
    {
        "loss": 2.1426,
        "grad_norm": 3.5474655628204346,
        "learning_rate": 9.169241532278627e-06,
        "epoch": 0.6218049034950444,
        "step": 8344
    },
    {
        "loss": 2.7626,
        "grad_norm": 1.6514256000518799,
        "learning_rate": 9.13982985584857e-06,
        "epoch": 0.6218794246963261,
        "step": 8345
    },
    {
        "loss": 2.2076,
        "grad_norm": 2.3948328495025635,
        "learning_rate": 9.110463167263871e-06,
        "epoch": 0.6219539458976079,
        "step": 8346
    },
    {
        "loss": 2.2559,
        "grad_norm": 3.6699836254119873,
        "learning_rate": 9.081141481064926e-06,
        "epoch": 0.6220284670988896,
        "step": 8347
    },
    {
        "loss": 2.6262,
        "grad_norm": 2.7348904609680176,
        "learning_rate": 9.051864811769828e-06,
        "epoch": 0.6221029883001714,
        "step": 8348
    },
    {
        "loss": 2.5916,
        "grad_norm": 2.611633539199829,
        "learning_rate": 9.022633173874472e-06,
        "epoch": 0.6221775095014531,
        "step": 8349
    },
    {
        "loss": 2.6579,
        "grad_norm": 2.581418037414551,
        "learning_rate": 8.993446581852305e-06,
        "epoch": 0.6222520307027349,
        "step": 8350
    },
    {
        "loss": 1.9988,
        "grad_norm": 3.9185333251953125,
        "learning_rate": 8.964305050154698e-06,
        "epoch": 0.6223265519040166,
        "step": 8351
    },
    {
        "loss": 2.4257,
        "grad_norm": 2.7240068912506104,
        "learning_rate": 8.935208593210498e-06,
        "epoch": 0.6224010731052985,
        "step": 8352
    },
    {
        "loss": 2.7651,
        "grad_norm": 1.7831676006317139,
        "learning_rate": 8.906157225426281e-06,
        "epoch": 0.6224755943065802,
        "step": 8353
    },
    {
        "loss": 1.8254,
        "grad_norm": 4.292774677276611,
        "learning_rate": 8.87715096118642e-06,
        "epoch": 0.622550115507862,
        "step": 8354
    },
    {
        "loss": 2.3847,
        "grad_norm": 4.2817277908325195,
        "learning_rate": 8.848189814852803e-06,
        "epoch": 0.6226246367091437,
        "step": 8355
    },
    {
        "loss": 1.7471,
        "grad_norm": 3.2830216884613037,
        "learning_rate": 8.819273800765082e-06,
        "epoch": 0.6226991579104255,
        "step": 8356
    },
    {
        "loss": 2.331,
        "grad_norm": 2.2754859924316406,
        "learning_rate": 8.790402933240493e-06,
        "epoch": 0.6227736791117073,
        "step": 8357
    },
    {
        "loss": 2.4134,
        "grad_norm": 3.716053009033203,
        "learning_rate": 8.761577226573947e-06,
        "epoch": 0.622848200312989,
        "step": 8358
    },
    {
        "loss": 2.0739,
        "grad_norm": 2.7152884006500244,
        "learning_rate": 8.732796695038014e-06,
        "epoch": 0.6229227215142709,
        "step": 8359
    },
    {
        "loss": 1.8414,
        "grad_norm": 2.506883382797241,
        "learning_rate": 8.704061352882808e-06,
        "epoch": 0.6229972427155526,
        "step": 8360
    },
    {
        "loss": 1.712,
        "grad_norm": 5.28401517868042,
        "learning_rate": 8.675371214336258e-06,
        "epoch": 0.6230717639168344,
        "step": 8361
    },
    {
        "loss": 2.6528,
        "grad_norm": 2.2117395401000977,
        "learning_rate": 8.646726293603747e-06,
        "epoch": 0.6231462851181161,
        "step": 8362
    },
    {
        "loss": 2.0735,
        "grad_norm": 3.5249183177948,
        "learning_rate": 8.618126604868226e-06,
        "epoch": 0.6232208063193979,
        "step": 8363
    },
    {
        "loss": 1.812,
        "grad_norm": 3.3459622859954834,
        "learning_rate": 8.58957216229046e-06,
        "epoch": 0.6232953275206796,
        "step": 8364
    },
    {
        "loss": 2.7869,
        "grad_norm": 3.2239818572998047,
        "learning_rate": 8.561062980008593e-06,
        "epoch": 0.6233698487219614,
        "step": 8365
    },
    {
        "loss": 1.7088,
        "grad_norm": 2.9112329483032227,
        "learning_rate": 8.532599072138548e-06,
        "epoch": 0.6234443699232431,
        "step": 8366
    },
    {
        "loss": 2.5129,
        "grad_norm": 4.479864597320557,
        "learning_rate": 8.50418045277368e-06,
        "epoch": 0.623518891124525,
        "step": 8367
    },
    {
        "loss": 2.9967,
        "grad_norm": 2.04473876953125,
        "learning_rate": 8.475807135984936e-06,
        "epoch": 0.6235934123258067,
        "step": 8368
    },
    {
        "loss": 2.8146,
        "grad_norm": 2.8838367462158203,
        "learning_rate": 8.44747913582099e-06,
        "epoch": 0.6236679335270885,
        "step": 8369
    },
    {
        "loss": 2.2327,
        "grad_norm": 2.5910890102386475,
        "learning_rate": 8.419196466307866e-06,
        "epoch": 0.6237424547283702,
        "step": 8370
    },
    {
        "loss": 2.8268,
        "grad_norm": 2.1201906204223633,
        "learning_rate": 8.390959141449228e-06,
        "epoch": 0.623816975929652,
        "step": 8371
    },
    {
        "loss": 2.21,
        "grad_norm": 2.264688491821289,
        "learning_rate": 8.362767175226382e-06,
        "epoch": 0.6238914971309337,
        "step": 8372
    },
    {
        "loss": 1.971,
        "grad_norm": 3.4205079078674316,
        "learning_rate": 8.334620581598007e-06,
        "epoch": 0.6239660183322155,
        "step": 8373
    },
    {
        "loss": 2.793,
        "grad_norm": 1.910143256187439,
        "learning_rate": 8.3065193745005e-06,
        "epoch": 0.6240405395334973,
        "step": 8374
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.3244590759277344,
        "learning_rate": 8.278463567847638e-06,
        "epoch": 0.6241150607347791,
        "step": 8375
    },
    {
        "loss": 2.7605,
        "grad_norm": 2.2902398109436035,
        "learning_rate": 8.25045317553076e-06,
        "epoch": 0.6241895819360608,
        "step": 8376
    },
    {
        "loss": 1.8267,
        "grad_norm": 2.5698561668395996,
        "learning_rate": 8.222488211418744e-06,
        "epoch": 0.6242641031373426,
        "step": 8377
    },
    {
        "loss": 2.2277,
        "grad_norm": 3.073643922805786,
        "learning_rate": 8.19456868935794e-06,
        "epoch": 0.6243386243386243,
        "step": 8378
    },
    {
        "loss": 1.8039,
        "grad_norm": 2.4279096126556396,
        "learning_rate": 8.166694623172244e-06,
        "epoch": 0.6244131455399061,
        "step": 8379
    },
    {
        "loss": 2.4197,
        "grad_norm": 2.003052234649658,
        "learning_rate": 8.138866026663006e-06,
        "epoch": 0.6244876667411878,
        "step": 8380
    },
    {
        "loss": 2.1381,
        "grad_norm": 3.3418195247650146,
        "learning_rate": 8.111082913609025e-06,
        "epoch": 0.6245621879424696,
        "step": 8381
    },
    {
        "loss": 2.0829,
        "grad_norm": 3.522449254989624,
        "learning_rate": 8.083345297766742e-06,
        "epoch": 0.6246367091437514,
        "step": 8382
    },
    {
        "loss": 1.7624,
        "grad_norm": 4.5790910720825195,
        "learning_rate": 8.055653192869827e-06,
        "epoch": 0.6247112303450332,
        "step": 8383
    },
    {
        "loss": 1.321,
        "grad_norm": 3.4413015842437744,
        "learning_rate": 8.028006612629679e-06,
        "epoch": 0.6247857515463149,
        "step": 8384
    },
    {
        "loss": 2.2256,
        "grad_norm": 1.6064420938491821,
        "learning_rate": 8.000405570734982e-06,
        "epoch": 0.6248602727475967,
        "step": 8385
    },
    {
        "loss": 2.4364,
        "grad_norm": 2.3023521900177,
        "learning_rate": 7.972850080851845e-06,
        "epoch": 0.6249347939488784,
        "step": 8386
    },
    {
        "loss": 2.2819,
        "grad_norm": 3.0209550857543945,
        "learning_rate": 7.94534015662397e-06,
        "epoch": 0.6250093151501602,
        "step": 8387
    },
    {
        "loss": 2.0831,
        "grad_norm": 3.1401078701019287,
        "learning_rate": 7.917875811672392e-06,
        "epoch": 0.6250838363514419,
        "step": 8388
    },
    {
        "loss": 2.0189,
        "grad_norm": 2.2567250728607178,
        "learning_rate": 7.890457059595625e-06,
        "epoch": 0.6251583575527238,
        "step": 8389
    },
    {
        "loss": 1.5391,
        "grad_norm": 4.769618034362793,
        "learning_rate": 7.863083913969581e-06,
        "epoch": 0.6252328787540055,
        "step": 8390
    },
    {
        "loss": 2.7001,
        "grad_norm": 2.6623740196228027,
        "learning_rate": 7.835756388347537e-06,
        "epoch": 0.6253073999552873,
        "step": 8391
    },
    {
        "loss": 1.8875,
        "grad_norm": 3.6795778274536133,
        "learning_rate": 7.808474496260387e-06,
        "epoch": 0.6253819211565691,
        "step": 8392
    },
    {
        "loss": 2.47,
        "grad_norm": 2.705744743347168,
        "learning_rate": 7.781238251216138e-06,
        "epoch": 0.6254564423578508,
        "step": 8393
    },
    {
        "loss": 2.1233,
        "grad_norm": 2.432039499282837,
        "learning_rate": 7.75404766670047e-06,
        "epoch": 0.6255309635591326,
        "step": 8394
    },
    {
        "loss": 1.4141,
        "grad_norm": 4.040193557739258,
        "learning_rate": 7.72690275617627e-06,
        "epoch": 0.6256054847604143,
        "step": 8395
    },
    {
        "loss": 2.3574,
        "grad_norm": 3.7276878356933594,
        "learning_rate": 7.699803533083872e-06,
        "epoch": 0.6256800059616962,
        "step": 8396
    },
    {
        "loss": 2.1639,
        "grad_norm": 4.0969414710998535,
        "learning_rate": 7.672750010841012e-06,
        "epoch": 0.6257545271629779,
        "step": 8397
    },
    {
        "loss": 2.4177,
        "grad_norm": 4.078001022338867,
        "learning_rate": 7.645742202842732e-06,
        "epoch": 0.6258290483642597,
        "step": 8398
    },
    {
        "loss": 2.2832,
        "grad_norm": 3.446868419647217,
        "learning_rate": 7.618780122461533e-06,
        "epoch": 0.6259035695655414,
        "step": 8399
    },
    {
        "loss": 2.4569,
        "grad_norm": 3.4449779987335205,
        "learning_rate": 7.591863783047226e-06,
        "epoch": 0.6259780907668232,
        "step": 8400
    },
    {
        "loss": 1.9251,
        "grad_norm": 3.3185174465179443,
        "learning_rate": 7.564993197926884e-06,
        "epoch": 0.6260526119681049,
        "step": 8401
    },
    {
        "loss": 1.7462,
        "grad_norm": 3.877211093902588,
        "learning_rate": 7.53816838040512e-06,
        "epoch": 0.6261271331693867,
        "step": 8402
    },
    {
        "loss": 1.1011,
        "grad_norm": 1.7954821586608887,
        "learning_rate": 7.511389343763741e-06,
        "epoch": 0.6262016543706684,
        "step": 8403
    },
    {
        "loss": 2.7046,
        "grad_norm": 2.6544382572174072,
        "learning_rate": 7.484656101261878e-06,
        "epoch": 0.6262761755719503,
        "step": 8404
    },
    {
        "loss": 2.2454,
        "grad_norm": 3.224740505218506,
        "learning_rate": 7.457968666136117e-06,
        "epoch": 0.626350696773232,
        "step": 8405
    },
    {
        "loss": 1.2139,
        "grad_norm": 2.090092658996582,
        "learning_rate": 7.431327051600223e-06,
        "epoch": 0.6264252179745138,
        "step": 8406
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.924131155014038,
        "learning_rate": 7.4047312708453666e-06,
        "epoch": 0.6264997391757955,
        "step": 8407
    },
    {
        "loss": 2.3926,
        "grad_norm": 2.826685667037964,
        "learning_rate": 7.37818133703998e-06,
        "epoch": 0.6265742603770773,
        "step": 8408
    },
    {
        "loss": 2.603,
        "grad_norm": 2.6611645221710205,
        "learning_rate": 7.351677263329782e-06,
        "epoch": 0.626648781578359,
        "step": 8409
    },
    {
        "loss": 2.3254,
        "grad_norm": 3.7971534729003906,
        "learning_rate": 7.32521906283783e-06,
        "epoch": 0.6267233027796408,
        "step": 8410
    },
    {
        "loss": 2.4169,
        "grad_norm": 3.4039087295532227,
        "learning_rate": 7.2988067486644e-06,
        "epoch": 0.6267978239809225,
        "step": 8411
    },
    {
        "loss": 2.3634,
        "grad_norm": 2.6579651832580566,
        "learning_rate": 7.272440333887176e-06,
        "epoch": 0.6268723451822044,
        "step": 8412
    },
    {
        "loss": 2.2057,
        "grad_norm": 2.7272090911865234,
        "learning_rate": 7.246119831560983e-06,
        "epoch": 0.6269468663834861,
        "step": 8413
    },
    {
        "loss": 2.535,
        "grad_norm": 2.5588953495025635,
        "learning_rate": 7.219845254717927e-06,
        "epoch": 0.6270213875847679,
        "step": 8414
    },
    {
        "loss": 2.5258,
        "grad_norm": 3.8172447681427,
        "learning_rate": 7.193616616367516e-06,
        "epoch": 0.6270959087860496,
        "step": 8415
    },
    {
        "loss": 2.3273,
        "grad_norm": 2.497619152069092,
        "learning_rate": 7.167433929496304e-06,
        "epoch": 0.6271704299873314,
        "step": 8416
    },
    {
        "loss": 2.4843,
        "grad_norm": 2.3019003868103027,
        "learning_rate": 7.141297207068265e-06,
        "epoch": 0.6272449511886131,
        "step": 8417
    },
    {
        "loss": 1.6215,
        "grad_norm": 3.7025232315063477,
        "learning_rate": 7.115206462024538e-06,
        "epoch": 0.6273194723898949,
        "step": 8418
    },
    {
        "loss": 1.2872,
        "grad_norm": 4.558406829833984,
        "learning_rate": 7.0891617072834426e-06,
        "epoch": 0.6273939935911766,
        "step": 8419
    },
    {
        "loss": 1.9752,
        "grad_norm": 3.2360095977783203,
        "learning_rate": 7.063162955740665e-06,
        "epoch": 0.6274685147924585,
        "step": 8420
    },
    {
        "loss": 2.731,
        "grad_norm": 2.080925226211548,
        "learning_rate": 7.037210220268953e-06,
        "epoch": 0.6275430359937402,
        "step": 8421
    },
    {
        "loss": 2.4181,
        "grad_norm": 3.0760622024536133,
        "learning_rate": 7.011303513718481e-06,
        "epoch": 0.627617557195022,
        "step": 8422
    },
    {
        "loss": 2.1654,
        "grad_norm": 2.3556735515594482,
        "learning_rate": 6.985442848916412e-06,
        "epoch": 0.6276920783963037,
        "step": 8423
    },
    {
        "loss": 2.1123,
        "grad_norm": 3.679748773574829,
        "learning_rate": 6.959628238667193e-06,
        "epoch": 0.6277665995975855,
        "step": 8424
    },
    {
        "loss": 2.3241,
        "grad_norm": 2.3863911628723145,
        "learning_rate": 6.933859695752565e-06,
        "epoch": 0.6278411207988672,
        "step": 8425
    },
    {
        "loss": 1.8057,
        "grad_norm": 4.289910793304443,
        "learning_rate": 6.9081372329312995e-06,
        "epoch": 0.627915642000149,
        "step": 8426
    },
    {
        "loss": 2.8524,
        "grad_norm": 3.910109758377075,
        "learning_rate": 6.882460862939522e-06,
        "epoch": 0.6279901632014309,
        "step": 8427
    },
    {
        "loss": 2.4716,
        "grad_norm": 2.064368486404419,
        "learning_rate": 6.8568305984903715e-06,
        "epoch": 0.6280646844027126,
        "step": 8428
    },
    {
        "loss": 2.4886,
        "grad_norm": 2.0547919273376465,
        "learning_rate": 6.831246452274209e-06,
        "epoch": 0.6281392056039944,
        "step": 8429
    },
    {
        "loss": 2.4128,
        "grad_norm": 3.0140221118927,
        "learning_rate": 6.805708436958669e-06,
        "epoch": 0.6282137268052761,
        "step": 8430
    },
    {
        "loss": 2.0232,
        "grad_norm": 2.5931217670440674,
        "learning_rate": 6.780216565188402e-06,
        "epoch": 0.6282882480065579,
        "step": 8431
    },
    {
        "loss": 2.6404,
        "grad_norm": 2.4416637420654297,
        "learning_rate": 6.754770849585223e-06,
        "epoch": 0.6283627692078396,
        "step": 8432
    },
    {
        "loss": 2.0563,
        "grad_norm": 2.639667272567749,
        "learning_rate": 6.729371302748244e-06,
        "epoch": 0.6284372904091214,
        "step": 8433
    },
    {
        "loss": 1.8343,
        "grad_norm": 2.760760545730591,
        "learning_rate": 6.704017937253504e-06,
        "epoch": 0.6285118116104031,
        "step": 8434
    },
    {
        "loss": 2.1019,
        "grad_norm": 4.014684677124023,
        "learning_rate": 6.678710765654406e-06,
        "epoch": 0.628586332811685,
        "step": 8435
    },
    {
        "loss": 2.8304,
        "grad_norm": 1.6201452016830444,
        "learning_rate": 6.6534498004812815e-06,
        "epoch": 0.6286608540129667,
        "step": 8436
    },
    {
        "loss": 2.1457,
        "grad_norm": 4.6704888343811035,
        "learning_rate": 6.628235054241672e-06,
        "epoch": 0.6287353752142485,
        "step": 8437
    },
    {
        "loss": 1.8325,
        "grad_norm": 2.849414825439453,
        "learning_rate": 6.603066539420233e-06,
        "epoch": 0.6288098964155302,
        "step": 8438
    },
    {
        "loss": 2.278,
        "grad_norm": 3.043123483657837,
        "learning_rate": 6.5779442684787085e-06,
        "epoch": 0.628884417616812,
        "step": 8439
    },
    {
        "loss": 1.6776,
        "grad_norm": 2.3274240493774414,
        "learning_rate": 6.55286825385597e-06,
        "epoch": 0.6289589388180937,
        "step": 8440
    },
    {
        "loss": 2.8601,
        "grad_norm": 2.3913087844848633,
        "learning_rate": 6.527838507967976e-06,
        "epoch": 0.6290334600193755,
        "step": 8441
    },
    {
        "loss": 2.4882,
        "grad_norm": 3.4353058338165283,
        "learning_rate": 6.502855043207712e-06,
        "epoch": 0.6291079812206573,
        "step": 8442
    },
    {
        "loss": 2.8609,
        "grad_norm": 2.689929962158203,
        "learning_rate": 6.477917871945416e-06,
        "epoch": 0.6291825024219391,
        "step": 8443
    },
    {
        "loss": 2.3353,
        "grad_norm": 1.8302983045578003,
        "learning_rate": 6.453027006528189e-06,
        "epoch": 0.6292570236232208,
        "step": 8444
    },
    {
        "loss": 3.0585,
        "grad_norm": 3.5948760509490967,
        "learning_rate": 6.4281824592804295e-06,
        "epoch": 0.6293315448245026,
        "step": 8445
    },
    {
        "loss": 2.6935,
        "grad_norm": 2.0565099716186523,
        "learning_rate": 6.4033842425034186e-06,
        "epoch": 0.6294060660257843,
        "step": 8446
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.821859836578369,
        "learning_rate": 6.378632368475534e-06,
        "epoch": 0.6294805872270661,
        "step": 8447
    },
    {
        "loss": 1.6656,
        "grad_norm": 4.07390832901001,
        "learning_rate": 6.353926849452308e-06,
        "epoch": 0.6295551084283478,
        "step": 8448
    },
    {
        "loss": 2.5788,
        "grad_norm": 1.8602416515350342,
        "learning_rate": 6.329267697666208e-06,
        "epoch": 0.6296296296296297,
        "step": 8449
    },
    {
        "loss": 2.3295,
        "grad_norm": 2.886462926864624,
        "learning_rate": 6.304654925326814e-06,
        "epoch": 0.6297041508309114,
        "step": 8450
    },
    {
        "loss": 2.4451,
        "grad_norm": 3.193883180618286,
        "learning_rate": 6.28008854462071e-06,
        "epoch": 0.6297786720321932,
        "step": 8451
    },
    {
        "loss": 1.7048,
        "grad_norm": 4.633732318878174,
        "learning_rate": 6.255568567711456e-06,
        "epoch": 0.6298531932334749,
        "step": 8452
    },
    {
        "loss": 2.6471,
        "grad_norm": 3.623506784439087,
        "learning_rate": 6.2310950067398064e-06,
        "epoch": 0.6299277144347567,
        "step": 8453
    },
    {
        "loss": 1.2131,
        "grad_norm": 4.10020637512207,
        "learning_rate": 6.206667873823302e-06,
        "epoch": 0.6300022356360384,
        "step": 8454
    },
    {
        "loss": 2.154,
        "grad_norm": 3.0530412197113037,
        "learning_rate": 6.1822871810567225e-06,
        "epoch": 0.6300767568373202,
        "step": 8455
    },
    {
        "loss": 2.595,
        "grad_norm": 2.6525750160217285,
        "learning_rate": 6.157952940511702e-06,
        "epoch": 0.6301512780386019,
        "step": 8456
    },
    {
        "loss": 2.4687,
        "grad_norm": 2.9016366004943848,
        "learning_rate": 6.133665164236912e-06,
        "epoch": 0.6302257992398838,
        "step": 8457
    },
    {
        "loss": 1.7386,
        "grad_norm": 3.1386024951934814,
        "learning_rate": 6.109423864258046e-06,
        "epoch": 0.6303003204411655,
        "step": 8458
    },
    {
        "loss": 2.6732,
        "grad_norm": 2.462195634841919,
        "learning_rate": 6.085229052577713e-06,
        "epoch": 0.6303748416424473,
        "step": 8459
    },
    {
        "loss": 2.1434,
        "grad_norm": 3.3690481185913086,
        "learning_rate": 6.061080741175651e-06,
        "epoch": 0.630449362843729,
        "step": 8460
    },
    {
        "loss": 1.6158,
        "grad_norm": 2.2917592525482178,
        "learning_rate": 6.0369789420084085e-06,
        "epoch": 0.6305238840450108,
        "step": 8461
    },
    {
        "loss": 1.7875,
        "grad_norm": 4.058632850646973,
        "learning_rate": 6.01292366700954e-06,
        "epoch": 0.6305984052462925,
        "step": 8462
    },
    {
        "loss": 1.785,
        "grad_norm": 2.9982941150665283,
        "learning_rate": 5.988914928089706e-06,
        "epoch": 0.6306729264475743,
        "step": 8463
    },
    {
        "loss": 2.821,
        "grad_norm": 3.0336074829101562,
        "learning_rate": 5.964952737136364e-06,
        "epoch": 0.6307474476488562,
        "step": 8464
    },
    {
        "loss": 2.43,
        "grad_norm": 2.8259851932525635,
        "learning_rate": 5.9410371060139355e-06,
        "epoch": 0.6308219688501379,
        "step": 8465
    },
    {
        "loss": 2.4392,
        "grad_norm": 1.9586840867996216,
        "learning_rate": 5.917168046563915e-06,
        "epoch": 0.6308964900514197,
        "step": 8466
    },
    {
        "loss": 3.1807,
        "grad_norm": 2.4524312019348145,
        "learning_rate": 5.893345570604614e-06,
        "epoch": 0.6309710112527014,
        "step": 8467
    },
    {
        "loss": 2.1431,
        "grad_norm": 2.7755367755889893,
        "learning_rate": 5.8695696899313315e-06,
        "epoch": 0.6310455324539832,
        "step": 8468
    },
    {
        "loss": 2.3016,
        "grad_norm": 3.0902514457702637,
        "learning_rate": 5.845840416316284e-06,
        "epoch": 0.6311200536552649,
        "step": 8469
    },
    {
        "loss": 2.2425,
        "grad_norm": 2.994478940963745,
        "learning_rate": 5.822157761508584e-06,
        "epoch": 0.6311945748565467,
        "step": 8470
    },
    {
        "loss": 2.2519,
        "grad_norm": 1.5971325635910034,
        "learning_rate": 5.79852173723433e-06,
        "epoch": 0.6312690960578284,
        "step": 8471
    },
    {
        "loss": 2.3141,
        "grad_norm": 2.356950521469116,
        "learning_rate": 5.774932355196427e-06,
        "epoch": 0.6313436172591103,
        "step": 8472
    },
    {
        "loss": 2.2302,
        "grad_norm": 2.288076162338257,
        "learning_rate": 5.751389627074832e-06,
        "epoch": 0.631418138460392,
        "step": 8473
    },
    {
        "loss": 2.7562,
        "grad_norm": 2.5458614826202393,
        "learning_rate": 5.727893564526299e-06,
        "epoch": 0.6314926596616738,
        "step": 8474
    },
    {
        "loss": 2.2615,
        "grad_norm": 2.953458786010742,
        "learning_rate": 5.7044441791844315e-06,
        "epoch": 0.6315671808629555,
        "step": 8475
    },
    {
        "loss": 2.0467,
        "grad_norm": 3.120262384414673,
        "learning_rate": 5.681041482659899e-06,
        "epoch": 0.6316417020642373,
        "step": 8476
    },
    {
        "loss": 2.2157,
        "grad_norm": 3.0651090145111084,
        "learning_rate": 5.657685486540032e-06,
        "epoch": 0.631716223265519,
        "step": 8477
    },
    {
        "loss": 2.3533,
        "grad_norm": 2.5360701084136963,
        "learning_rate": 5.634376202389236e-06,
        "epoch": 0.6317907444668008,
        "step": 8478
    },
    {
        "loss": 2.1358,
        "grad_norm": 4.93702507019043,
        "learning_rate": 5.611113641748688e-06,
        "epoch": 0.6318652656680825,
        "step": 8479
    },
    {
        "loss": 2.4452,
        "grad_norm": 3.501077175140381,
        "learning_rate": 5.587897816136367e-06,
        "epoch": 0.6319397868693644,
        "step": 8480
    },
    {
        "loss": 2.123,
        "grad_norm": 3.9173026084899902,
        "learning_rate": 5.564728737047275e-06,
        "epoch": 0.6320143080706461,
        "step": 8481
    },
    {
        "loss": 2.7736,
        "grad_norm": 3.528555154800415,
        "learning_rate": 5.541606415953093e-06,
        "epoch": 0.6320888292719279,
        "step": 8482
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.809023141860962,
        "learning_rate": 5.518530864302574e-06,
        "epoch": 0.6321633504732096,
        "step": 8483
    },
    {
        "loss": 2.8037,
        "grad_norm": 2.264530658721924,
        "learning_rate": 5.495502093521066e-06,
        "epoch": 0.6322378716744914,
        "step": 8484
    },
    {
        "loss": 2.4656,
        "grad_norm": 3.2030391693115234,
        "learning_rate": 5.472520115010871e-06,
        "epoch": 0.6323123928757731,
        "step": 8485
    },
    {
        "loss": 2.5425,
        "grad_norm": 3.0313360691070557,
        "learning_rate": 5.449584940151186e-06,
        "epoch": 0.6323869140770549,
        "step": 8486
    },
    {
        "loss": 1.5807,
        "grad_norm": 4.447654724121094,
        "learning_rate": 5.426696580297929e-06,
        "epoch": 0.6324614352783366,
        "step": 8487
    },
    {
        "loss": 2.2945,
        "grad_norm": 2.2159698009490967,
        "learning_rate": 5.403855046783879e-06,
        "epoch": 0.6325359564796185,
        "step": 8488
    },
    {
        "loss": 2.1884,
        "grad_norm": 2.350832223892212,
        "learning_rate": 5.381060350918632e-06,
        "epoch": 0.6326104776809002,
        "step": 8489
    },
    {
        "loss": 2.016,
        "grad_norm": 1.6831501722335815,
        "learning_rate": 5.3583125039885715e-06,
        "epoch": 0.632684998882182,
        "step": 8490
    },
    {
        "loss": 2.714,
        "grad_norm": 2.148890972137451,
        "learning_rate": 5.335611517256922e-06,
        "epoch": 0.6327595200834637,
        "step": 8491
    },
    {
        "loss": 2.2624,
        "grad_norm": 3.000908374786377,
        "learning_rate": 5.312957401963636e-06,
        "epoch": 0.6328340412847455,
        "step": 8492
    },
    {
        "loss": 1.5006,
        "grad_norm": 4.776158332824707,
        "learning_rate": 5.290350169325586e-06,
        "epoch": 0.6329085624860272,
        "step": 8493
    },
    {
        "loss": 2.2554,
        "grad_norm": 1.3576059341430664,
        "learning_rate": 5.267789830536318e-06,
        "epoch": 0.632983083687309,
        "step": 8494
    },
    {
        "loss": 2.5445,
        "grad_norm": 2.0233707427978516,
        "learning_rate": 5.245276396766152e-06,
        "epoch": 0.6330576048885908,
        "step": 8495
    },
    {
        "loss": 2.3875,
        "grad_norm": 3.435732126235962,
        "learning_rate": 5.222809879162305e-06,
        "epoch": 0.6331321260898726,
        "step": 8496
    },
    {
        "loss": 2.392,
        "grad_norm": 2.124390125274658,
        "learning_rate": 5.2003902888486446e-06,
        "epoch": 0.6332066472911543,
        "step": 8497
    },
    {
        "loss": 2.2262,
        "grad_norm": 2.774613857269287,
        "learning_rate": 5.178017636925836e-06,
        "epoch": 0.6332811684924361,
        "step": 8498
    },
    {
        "loss": 1.7873,
        "grad_norm": 2.9646689891815186,
        "learning_rate": 5.155691934471329e-06,
        "epoch": 0.6333556896937179,
        "step": 8499
    },
    {
        "loss": 2.6401,
        "grad_norm": 2.729980945587158,
        "learning_rate": 5.1334131925392934e-06,
        "epoch": 0.6334302108949996,
        "step": 8500
    },
    {
        "loss": 2.3362,
        "grad_norm": 2.0670719146728516,
        "learning_rate": 5.111181422160682e-06,
        "epoch": 0.6335047320962814,
        "step": 8501
    },
    {
        "loss": 1.9272,
        "grad_norm": 4.185100555419922,
        "learning_rate": 5.088996634343168e-06,
        "epoch": 0.6335792532975631,
        "step": 8502
    },
    {
        "loss": 2.3786,
        "grad_norm": 3.117715835571289,
        "learning_rate": 5.066858840071121e-06,
        "epoch": 0.633653774498845,
        "step": 8503
    },
    {
        "loss": 2.3173,
        "grad_norm": 2.006524085998535,
        "learning_rate": 5.044768050305759e-06,
        "epoch": 0.6337282957001267,
        "step": 8504
    },
    {
        "loss": 2.0967,
        "grad_norm": 3.2451329231262207,
        "learning_rate": 5.022724275984891e-06,
        "epoch": 0.6338028169014085,
        "step": 8505
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.0847866535186768,
        "learning_rate": 5.000727528023197e-06,
        "epoch": 0.6338773381026902,
        "step": 8506
    },
    {
        "loss": 2.1747,
        "grad_norm": 3.906775712966919,
        "learning_rate": 4.978777817311919e-06,
        "epoch": 0.633951859303972,
        "step": 8507
    },
    {
        "loss": 2.4982,
        "grad_norm": 2.2947819232940674,
        "learning_rate": 4.956875154719076e-06,
        "epoch": 0.6340263805052537,
        "step": 8508
    },
    {
        "loss": 2.4694,
        "grad_norm": 4.544052600860596,
        "learning_rate": 4.935019551089437e-06,
        "epoch": 0.6341009017065355,
        "step": 8509
    },
    {
        "loss": 2.4426,
        "grad_norm": 2.1694912910461426,
        "learning_rate": 4.913211017244368e-06,
        "epoch": 0.6341754229078173,
        "step": 8510
    },
    {
        "loss": 1.1965,
        "grad_norm": 4.415717124938965,
        "learning_rate": 4.891449563982053e-06,
        "epoch": 0.6342499441090991,
        "step": 8511
    },
    {
        "loss": 2.6263,
        "grad_norm": 2.5925960540771484,
        "learning_rate": 4.8697352020772635e-06,
        "epoch": 0.6343244653103808,
        "step": 8512
    },
    {
        "loss": 2.5257,
        "grad_norm": 2.380718469619751,
        "learning_rate": 4.848067942281464e-06,
        "epoch": 0.6343989865116626,
        "step": 8513
    },
    {
        "loss": 1.8653,
        "grad_norm": 3.8621959686279297,
        "learning_rate": 4.826447795322897e-06,
        "epoch": 0.6344735077129443,
        "step": 8514
    },
    {
        "loss": 2.1258,
        "grad_norm": 2.7339327335357666,
        "learning_rate": 4.80487477190631e-06,
        "epoch": 0.6345480289142261,
        "step": 8515
    },
    {
        "loss": 2.3927,
        "grad_norm": 2.8260176181793213,
        "learning_rate": 4.783348882713312e-06,
        "epoch": 0.6346225501155078,
        "step": 8516
    },
    {
        "loss": 2.3716,
        "grad_norm": 3.8497374057769775,
        "learning_rate": 4.761870138402025e-06,
        "epoch": 0.6346970713167897,
        "step": 8517
    },
    {
        "loss": 2.3358,
        "grad_norm": 2.3340139389038086,
        "learning_rate": 4.740438549607285e-06,
        "epoch": 0.6347715925180714,
        "step": 8518
    },
    {
        "loss": 2.3147,
        "grad_norm": 2.440351963043213,
        "learning_rate": 4.719054126940581e-06,
        "epoch": 0.6348461137193532,
        "step": 8519
    },
    {
        "loss": 2.6961,
        "grad_norm": 1.8545912504196167,
        "learning_rate": 4.69771688099e-06,
        "epoch": 0.6349206349206349,
        "step": 8520
    },
    {
        "loss": 2.103,
        "grad_norm": 2.3107478618621826,
        "learning_rate": 4.67642682232039e-06,
        "epoch": 0.6349951561219167,
        "step": 8521
    },
    {
        "loss": 2.4847,
        "grad_norm": 3.9476542472839355,
        "learning_rate": 4.655183961473086e-06,
        "epoch": 0.6350696773231984,
        "step": 8522
    },
    {
        "loss": 3.1319,
        "grad_norm": 2.9063222408294678,
        "learning_rate": 4.633988308966119e-06,
        "epoch": 0.6351441985244802,
        "step": 8523
    },
    {
        "loss": 1.1679,
        "grad_norm": 2.198930263519287,
        "learning_rate": 4.612839875294217e-06,
        "epoch": 0.6352187197257619,
        "step": 8524
    },
    {
        "loss": 2.8188,
        "grad_norm": 3.0819478034973145,
        "learning_rate": 4.5917386709286246e-06,
        "epoch": 0.6352932409270438,
        "step": 8525
    },
    {
        "loss": 2.2965,
        "grad_norm": 2.6030991077423096,
        "learning_rate": 4.570684706317196e-06,
        "epoch": 0.6353677621283255,
        "step": 8526
    },
    {
        "loss": 2.1743,
        "grad_norm": 3.743844747543335,
        "learning_rate": 4.549677991884527e-06,
        "epoch": 0.6354422833296073,
        "step": 8527
    },
    {
        "loss": 2.2569,
        "grad_norm": 3.1606154441833496,
        "learning_rate": 4.528718538031674e-06,
        "epoch": 0.635516804530889,
        "step": 8528
    },
    {
        "loss": 1.4561,
        "grad_norm": 4.170897006988525,
        "learning_rate": 4.507806355136368e-06,
        "epoch": 0.6355913257321708,
        "step": 8529
    },
    {
        "loss": 2.9799,
        "grad_norm": 2.886958599090576,
        "learning_rate": 4.486941453552906e-06,
        "epoch": 0.6356658469334525,
        "step": 8530
    },
    {
        "loss": 2.5925,
        "grad_norm": 3.074418306350708,
        "learning_rate": 4.466123843612169e-06,
        "epoch": 0.6357403681347343,
        "step": 8531
    },
    {
        "loss": 2.7018,
        "grad_norm": 3.5053815841674805,
        "learning_rate": 4.445353535621666e-06,
        "epoch": 0.635814889336016,
        "step": 8532
    },
    {
        "loss": 2.0841,
        "grad_norm": 3.8455982208251953,
        "learning_rate": 4.424630539865415e-06,
        "epoch": 0.6358894105372979,
        "step": 8533
    },
    {
        "loss": 2.3497,
        "grad_norm": 3.2930920124053955,
        "learning_rate": 4.40395486660411e-06,
        "epoch": 0.6359639317385797,
        "step": 8534
    },
    {
        "loss": 1.9354,
        "grad_norm": 2.4650237560272217,
        "learning_rate": 4.383326526074927e-06,
        "epoch": 0.6360384529398614,
        "step": 8535
    },
    {
        "loss": 2.6277,
        "grad_norm": 2.7954440116882324,
        "learning_rate": 4.3627455284915855e-06,
        "epoch": 0.6361129741411432,
        "step": 8536
    },
    {
        "loss": 2.4635,
        "grad_norm": 2.7816340923309326,
        "learning_rate": 4.34221188404449e-06,
        "epoch": 0.6361874953424249,
        "step": 8537
    },
    {
        "loss": 2.0084,
        "grad_norm": 2.3843190670013428,
        "learning_rate": 4.321725602900461e-06,
        "epoch": 0.6362620165437067,
        "step": 8538
    },
    {
        "loss": 1.5734,
        "grad_norm": 3.3605570793151855,
        "learning_rate": 4.301286695202977e-06,
        "epoch": 0.6363365377449884,
        "step": 8539
    },
    {
        "loss": 2.1138,
        "grad_norm": 1.7839603424072266,
        "learning_rate": 4.2808951710719854e-06,
        "epoch": 0.6364110589462703,
        "step": 8540
    },
    {
        "loss": 1.9337,
        "grad_norm": 4.283929824829102,
        "learning_rate": 4.2605510406039664e-06,
        "epoch": 0.636485580147552,
        "step": 8541
    },
    {
        "loss": 2.5238,
        "grad_norm": 2.3579370975494385,
        "learning_rate": 4.240254313872028e-06,
        "epoch": 0.6365601013488338,
        "step": 8542
    },
    {
        "loss": 2.529,
        "grad_norm": 1.8847825527191162,
        "learning_rate": 4.2200050009256535e-06,
        "epoch": 0.6366346225501155,
        "step": 8543
    },
    {
        "loss": 1.723,
        "grad_norm": 4.538609981536865,
        "learning_rate": 4.199803111791068e-06,
        "epoch": 0.6367091437513973,
        "step": 8544
    },
    {
        "loss": 2.7379,
        "grad_norm": 2.0920145511627197,
        "learning_rate": 4.179648656470791e-06,
        "epoch": 0.636783664952679,
        "step": 8545
    },
    {
        "loss": 1.8666,
        "grad_norm": 2.7620370388031006,
        "learning_rate": 4.1595416449439424e-06,
        "epoch": 0.6368581861539608,
        "step": 8546
    },
    {
        "loss": 2.717,
        "grad_norm": 2.120173931121826,
        "learning_rate": 4.139482087166246e-06,
        "epoch": 0.6369327073552425,
        "step": 8547
    },
    {
        "loss": 1.5468,
        "grad_norm": 2.3107404708862305,
        "learning_rate": 4.11946999306978e-06,
        "epoch": 0.6370072285565244,
        "step": 8548
    },
    {
        "loss": 2.2184,
        "grad_norm": 3.5466396808624268,
        "learning_rate": 4.0995053725632285e-06,
        "epoch": 0.6370817497578061,
        "step": 8549
    },
    {
        "loss": 2.6361,
        "grad_norm": 3.3430604934692383,
        "learning_rate": 4.079588235531706e-06,
        "epoch": 0.6371562709590879,
        "step": 8550
    },
    {
        "loss": 2.8188,
        "grad_norm": 1.9681488275527954,
        "learning_rate": 4.059718591836825e-06,
        "epoch": 0.6372307921603696,
        "step": 8551
    },
    {
        "loss": 2.3316,
        "grad_norm": 3.852705240249634,
        "learning_rate": 4.039896451316716e-06,
        "epoch": 0.6373053133616514,
        "step": 8552
    },
    {
        "loss": 2.261,
        "grad_norm": 2.898085594177246,
        "learning_rate": 4.020121823785938e-06,
        "epoch": 0.6373798345629331,
        "step": 8553
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.9696202278137207,
        "learning_rate": 4.000394719035627e-06,
        "epoch": 0.6374543557642149,
        "step": 8554
    },
    {
        "loss": 2.2679,
        "grad_norm": 2.5666146278381348,
        "learning_rate": 3.980715146833269e-06,
        "epoch": 0.6375288769654966,
        "step": 8555
    },
    {
        "loss": 3.0344,
        "grad_norm": 3.5215389728546143,
        "learning_rate": 3.961083116922837e-06,
        "epoch": 0.6376033981667785,
        "step": 8556
    },
    {
        "loss": 2.6418,
        "grad_norm": 3.1298296451568604,
        "learning_rate": 3.941498639024866e-06,
        "epoch": 0.6376779193680602,
        "step": 8557
    },
    {
        "loss": 1.7946,
        "grad_norm": 3.507321834564209,
        "learning_rate": 3.921961722836242e-06,
        "epoch": 0.637752440569342,
        "step": 8558
    },
    {
        "loss": 2.2941,
        "grad_norm": 3.3014132976531982,
        "learning_rate": 3.902472378030286e-06,
        "epoch": 0.6378269617706237,
        "step": 8559
    },
    {
        "loss": 2.6229,
        "grad_norm": 1.6879611015319824,
        "learning_rate": 3.883030614256866e-06,
        "epoch": 0.6379014829719055,
        "step": 8560
    },
    {
        "loss": 2.3808,
        "grad_norm": 3.1999781131744385,
        "learning_rate": 3.863636441142215e-06,
        "epoch": 0.6379760041731872,
        "step": 8561
    },
    {
        "loss": 1.9349,
        "grad_norm": 2.631589651107788,
        "learning_rate": 3.844289868289041e-06,
        "epoch": 0.638050525374469,
        "step": 8562
    },
    {
        "loss": 1.9982,
        "grad_norm": 3.0954372882843018,
        "learning_rate": 3.824990905276438e-06,
        "epoch": 0.6381250465757508,
        "step": 8563
    },
    {
        "loss": 2.6634,
        "grad_norm": 1.9028809070587158,
        "learning_rate": 3.805739561659938e-06,
        "epoch": 0.6381995677770326,
        "step": 8564
    },
    {
        "loss": 2.2926,
        "grad_norm": 3.556440830230713,
        "learning_rate": 3.786535846971573e-06,
        "epoch": 0.6382740889783143,
        "step": 8565
    },
    {
        "loss": 2.0561,
        "grad_norm": 2.158639907836914,
        "learning_rate": 3.76737977071967e-06,
        "epoch": 0.6383486101795961,
        "step": 8566
    },
    {
        "loss": 2.3479,
        "grad_norm": 3.0056259632110596,
        "learning_rate": 3.748271342389098e-06,
        "epoch": 0.6384231313808778,
        "step": 8567
    },
    {
        "loss": 2.6402,
        "grad_norm": 2.508742570877075,
        "learning_rate": 3.729210571441022e-06,
        "epoch": 0.6384976525821596,
        "step": 8568
    },
    {
        "loss": 3.0363,
        "grad_norm": 3.206204652786255,
        "learning_rate": 3.7101974673130256e-06,
        "epoch": 0.6385721737834414,
        "step": 8569
    },
    {
        "loss": 1.8457,
        "grad_norm": 4.564388275146484,
        "learning_rate": 3.69123203941919e-06,
        "epoch": 0.6386466949847232,
        "step": 8570
    },
    {
        "loss": 2.9584,
        "grad_norm": 2.2671058177948,
        "learning_rate": 3.672314297149837e-06,
        "epoch": 0.638721216186005,
        "step": 8571
    },
    {
        "loss": 2.0076,
        "grad_norm": 2.2290825843811035,
        "learning_rate": 3.6534442498718404e-06,
        "epoch": 0.6387957373872867,
        "step": 8572
    },
    {
        "loss": 2.1982,
        "grad_norm": 3.399862289428711,
        "learning_rate": 3.634621906928348e-06,
        "epoch": 0.6388702585885685,
        "step": 8573
    },
    {
        "loss": 1.554,
        "grad_norm": 2.2194061279296875,
        "learning_rate": 3.615847277638862e-06,
        "epoch": 0.6389447797898502,
        "step": 8574
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.854677200317383,
        "learning_rate": 3.5971203712994005e-06,
        "epoch": 0.639019300991132,
        "step": 8575
    },
    {
        "loss": 2.7913,
        "grad_norm": 1.5174497365951538,
        "learning_rate": 3.578441197182203e-06,
        "epoch": 0.6390938221924137,
        "step": 8576
    },
    {
        "loss": 2.185,
        "grad_norm": 2.115023136138916,
        "learning_rate": 3.559809764536004e-06,
        "epoch": 0.6391683433936955,
        "step": 8577
    },
    {
        "loss": 2.9408,
        "grad_norm": 2.4678404331207275,
        "learning_rate": 3.5412260825858025e-06,
        "epoch": 0.6392428645949773,
        "step": 8578
    },
    {
        "loss": 2.1322,
        "grad_norm": 2.8587119579315186,
        "learning_rate": 3.522690160532971e-06,
        "epoch": 0.6393173857962591,
        "step": 8579
    },
    {
        "loss": 2.8174,
        "grad_norm": 3.1009089946746826,
        "learning_rate": 3.5042020075552907e-06,
        "epoch": 0.6393919069975408,
        "step": 8580
    },
    {
        "loss": 1.2692,
        "grad_norm": 4.045265197753906,
        "learning_rate": 3.4857616328068166e-06,
        "epoch": 0.6394664281988226,
        "step": 8581
    },
    {
        "loss": 1.9874,
        "grad_norm": 3.1180105209350586,
        "learning_rate": 3.467369045418001e-06,
        "epoch": 0.6395409494001043,
        "step": 8582
    },
    {
        "loss": 1.7553,
        "grad_norm": 3.0266835689544678,
        "learning_rate": 3.4490242544956143e-06,
        "epoch": 0.6396154706013861,
        "step": 8583
    },
    {
        "loss": 1.6257,
        "grad_norm": 2.7368381023406982,
        "learning_rate": 3.4307272691227132e-06,
        "epoch": 0.6396899918026678,
        "step": 8584
    },
    {
        "loss": 2.4936,
        "grad_norm": 1.3267834186553955,
        "learning_rate": 3.412478098358829e-06,
        "epoch": 0.6397645130039497,
        "step": 8585
    },
    {
        "loss": 1.9024,
        "grad_norm": 3.289911985397339,
        "learning_rate": 3.3942767512396336e-06,
        "epoch": 0.6398390342052314,
        "step": 8586
    },
    {
        "loss": 1.8248,
        "grad_norm": 3.0206170082092285,
        "learning_rate": 3.376123236777284e-06,
        "epoch": 0.6399135554065132,
        "step": 8587
    },
    {
        "loss": 2.7868,
        "grad_norm": 1.8268777132034302,
        "learning_rate": 3.3580175639601476e-06,
        "epoch": 0.6399880766077949,
        "step": 8588
    },
    {
        "loss": 2.0985,
        "grad_norm": 3.5525338649749756,
        "learning_rate": 3.3399597417529295e-06,
        "epoch": 0.6400625978090767,
        "step": 8589
    },
    {
        "loss": 2.6216,
        "grad_norm": 2.191657781600952,
        "learning_rate": 3.321949779096656e-06,
        "epoch": 0.6401371190103584,
        "step": 8590
    },
    {
        "loss": 2.2865,
        "grad_norm": 3.3997914791107178,
        "learning_rate": 3.3039876849086494e-06,
        "epoch": 0.6402116402116402,
        "step": 8591
    },
    {
        "loss": 2.0887,
        "grad_norm": 3.4757916927337646,
        "learning_rate": 3.2860734680825176e-06,
        "epoch": 0.6402861614129219,
        "step": 8592
    },
    {
        "loss": 2.5611,
        "grad_norm": 2.350322961807251,
        "learning_rate": 3.2682071374881996e-06,
        "epoch": 0.6403606826142038,
        "step": 8593
    },
    {
        "loss": 2.7592,
        "grad_norm": 2.9178996086120605,
        "learning_rate": 3.2503887019718515e-06,
        "epoch": 0.6404352038154855,
        "step": 8594
    },
    {
        "loss": 1.9221,
        "grad_norm": 2.8736279010772705,
        "learning_rate": 3.2326181703560387e-06,
        "epoch": 0.6405097250167673,
        "step": 8595
    },
    {
        "loss": 3.1967,
        "grad_norm": 3.5767979621887207,
        "learning_rate": 3.214895551439512e-06,
        "epoch": 0.640584246218049,
        "step": 8596
    },
    {
        "loss": 2.9676,
        "grad_norm": 2.74828839302063,
        "learning_rate": 3.1972208539972515e-06,
        "epoch": 0.6406587674193308,
        "step": 8597
    },
    {
        "loss": 2.3771,
        "grad_norm": 2.665189027786255,
        "learning_rate": 3.1795940867806683e-06,
        "epoch": 0.6407332886206125,
        "step": 8598
    },
    {
        "loss": 1.0654,
        "grad_norm": 1.472165584564209,
        "learning_rate": 3.1620152585172814e-06,
        "epoch": 0.6408078098218943,
        "step": 8599
    },
    {
        "loss": 2.5303,
        "grad_norm": 2.5324950218200684,
        "learning_rate": 3.1444843779110167e-06,
        "epoch": 0.640882331023176,
        "step": 8600
    },
    {
        "loss": 2.5329,
        "grad_norm": 1.9331681728363037,
        "learning_rate": 3.1270014536419425e-06,
        "epoch": 0.6409568522244579,
        "step": 8601
    },
    {
        "loss": 2.3071,
        "grad_norm": 3.0419559478759766,
        "learning_rate": 3.1095664943664003e-06,
        "epoch": 0.6410313734257396,
        "step": 8602
    },
    {
        "loss": 2.1038,
        "grad_norm": 2.644462823867798,
        "learning_rate": 3.092179508717086e-06,
        "epoch": 0.6411058946270214,
        "step": 8603
    },
    {
        "loss": 1.8519,
        "grad_norm": 2.3031113147735596,
        "learning_rate": 3.0748405053027673e-06,
        "epoch": 0.6411804158283032,
        "step": 8604
    },
    {
        "loss": 1.9368,
        "grad_norm": 3.1959025859832764,
        "learning_rate": 3.0575494927086446e-06,
        "epoch": 0.6412549370295849,
        "step": 8605
    },
    {
        "loss": 1.2732,
        "grad_norm": 3.4226791858673096,
        "learning_rate": 3.0403064794960244e-06,
        "epoch": 0.6413294582308667,
        "step": 8606
    },
    {
        "loss": 2.4262,
        "grad_norm": 2.581937313079834,
        "learning_rate": 3.023111474202456e-06,
        "epoch": 0.6414039794321484,
        "step": 8607
    },
    {
        "loss": 2.8838,
        "grad_norm": 2.2536845207214355,
        "learning_rate": 3.0059644853418167e-06,
        "epoch": 0.6414785006334303,
        "step": 8608
    },
    {
        "loss": 2.2832,
        "grad_norm": 2.9038007259368896,
        "learning_rate": 2.988865521404094e-06,
        "epoch": 0.641553021834712,
        "step": 8609
    },
    {
        "loss": 2.8002,
        "grad_norm": 1.9966766834259033,
        "learning_rate": 2.9718145908555705e-06,
        "epoch": 0.6416275430359938,
        "step": 8610
    },
    {
        "loss": 1.7783,
        "grad_norm": 3.0349574089050293,
        "learning_rate": 2.9548117021387156e-06,
        "epoch": 0.6417020642372755,
        "step": 8611
    },
    {
        "loss": 2.5717,
        "grad_norm": 2.534282684326172,
        "learning_rate": 2.9378568636721835e-06,
        "epoch": 0.6417765854385573,
        "step": 8612
    },
    {
        "loss": 2.6674,
        "grad_norm": 2.3887550830841064,
        "learning_rate": 2.9209500838509153e-06,
        "epoch": 0.641851106639839,
        "step": 8613
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.9403536319732666,
        "learning_rate": 2.9040913710459583e-06,
        "epoch": 0.6419256278411208,
        "step": 8614
    },
    {
        "loss": 1.9623,
        "grad_norm": 3.0265746116638184,
        "learning_rate": 2.8872807336046805e-06,
        "epoch": 0.6420001490424025,
        "step": 8615
    },
    {
        "loss": 2.6493,
        "grad_norm": 2.5937423706054688,
        "learning_rate": 2.8705181798505455e-06,
        "epoch": 0.6420746702436844,
        "step": 8616
    },
    {
        "loss": 1.5251,
        "grad_norm": 3.251077651977539,
        "learning_rate": 2.8538037180832034e-06,
        "epoch": 0.6421491914449661,
        "step": 8617
    },
    {
        "loss": 2.8074,
        "grad_norm": 2.3422136306762695,
        "learning_rate": 2.837137356578601e-06,
        "epoch": 0.6422237126462479,
        "step": 8618
    },
    {
        "loss": 2.2877,
        "grad_norm": 2.3009541034698486,
        "learning_rate": 2.8205191035887814e-06,
        "epoch": 0.6422982338475296,
        "step": 8619
    },
    {
        "loss": 1.7445,
        "grad_norm": 3.65104341506958,
        "learning_rate": 2.8039489673419407e-06,
        "epoch": 0.6423727550488114,
        "step": 8620
    },
    {
        "loss": 2.5911,
        "grad_norm": 2.9445478916168213,
        "learning_rate": 2.787426956042549e-06,
        "epoch": 0.6424472762500931,
        "step": 8621
    },
    {
        "loss": 2.4748,
        "grad_norm": 1.9370903968811035,
        "learning_rate": 2.7709530778711744e-06,
        "epoch": 0.6425217974513749,
        "step": 8622
    },
    {
        "loss": 2.4344,
        "grad_norm": 2.7331438064575195,
        "learning_rate": 2.7545273409845696e-06,
        "epoch": 0.6425963186526567,
        "step": 8623
    },
    {
        "loss": 2.7058,
        "grad_norm": 2.3327929973602295,
        "learning_rate": 2.738149753515662e-06,
        "epoch": 0.6426708398539385,
        "step": 8624
    },
    {
        "loss": 2.5138,
        "grad_norm": 2.621086359024048,
        "learning_rate": 2.7218203235734985e-06,
        "epoch": 0.6427453610552202,
        "step": 8625
    },
    {
        "loss": 1.8943,
        "grad_norm": 2.909294843673706,
        "learning_rate": 2.7055390592433895e-06,
        "epoch": 0.642819882256502,
        "step": 8626
    },
    {
        "loss": 0.3538,
        "grad_norm": 0.6705323457717896,
        "learning_rate": 2.6893059685866307e-06,
        "epoch": 0.6428944034577837,
        "step": 8627
    },
    {
        "loss": 1.9776,
        "grad_norm": 3.347355365753174,
        "learning_rate": 2.6731210596408263e-06,
        "epoch": 0.6429689246590655,
        "step": 8628
    },
    {
        "loss": 2.8177,
        "grad_norm": 2.5810582637786865,
        "learning_rate": 2.6569843404196325e-06,
        "epoch": 0.6430434458603472,
        "step": 8629
    },
    {
        "loss": 2.4881,
        "grad_norm": 2.5870118141174316,
        "learning_rate": 2.6408958189128697e-06,
        "epoch": 0.643117967061629,
        "step": 8630
    },
    {
        "loss": 2.4477,
        "grad_norm": 2.944840431213379,
        "learning_rate": 2.6248555030864873e-06,
        "epoch": 0.6431924882629108,
        "step": 8631
    },
    {
        "loss": 2.2845,
        "grad_norm": 3.080203056335449,
        "learning_rate": 2.6088634008825443e-06,
        "epoch": 0.6432670094641926,
        "step": 8632
    },
    {
        "loss": 1.8137,
        "grad_norm": 2.5352447032928467,
        "learning_rate": 2.592919520219317e-06,
        "epoch": 0.6433415306654743,
        "step": 8633
    },
    {
        "loss": 1.8106,
        "grad_norm": 3.4669995307922363,
        "learning_rate": 2.577023868991102e-06,
        "epoch": 0.6434160518667561,
        "step": 8634
    },
    {
        "loss": 1.864,
        "grad_norm": 3.3021371364593506,
        "learning_rate": 2.561176455068337e-06,
        "epoch": 0.6434905730680378,
        "step": 8635
    },
    {
        "loss": 2.7753,
        "grad_norm": 2.286081314086914,
        "learning_rate": 2.5453772862976345e-06,
        "epoch": 0.6435650942693196,
        "step": 8636
    },
    {
        "loss": 3.2904,
        "grad_norm": 2.3471062183380127,
        "learning_rate": 2.5296263705016477e-06,
        "epoch": 0.6436396154706013,
        "step": 8637
    },
    {
        "loss": 2.207,
        "grad_norm": 2.6541850566864014,
        "learning_rate": 2.513923715479216e-06,
        "epoch": 0.6437141366718832,
        "step": 8638
    },
    {
        "loss": 2.7283,
        "grad_norm": 2.2397232055664062,
        "learning_rate": 2.4982693290052096e-06,
        "epoch": 0.6437886578731649,
        "step": 8639
    },
    {
        "loss": 2.2504,
        "grad_norm": 3.088047504425049,
        "learning_rate": 2.4826632188306163e-06,
        "epoch": 0.6438631790744467,
        "step": 8640
    },
    {
        "loss": 2.7797,
        "grad_norm": 2.118828058242798,
        "learning_rate": 2.4671053926825562e-06,
        "epoch": 0.6439377002757285,
        "step": 8641
    },
    {
        "loss": 2.5281,
        "grad_norm": 2.1919286251068115,
        "learning_rate": 2.4515958582641997e-06,
        "epoch": 0.6440122214770102,
        "step": 8642
    },
    {
        "loss": 1.9682,
        "grad_norm": 3.0731420516967773,
        "learning_rate": 2.4361346232548487e-06,
        "epoch": 0.644086742678292,
        "step": 8643
    },
    {
        "loss": 1.2111,
        "grad_norm": 3.3006818294525146,
        "learning_rate": 2.4207216953098353e-06,
        "epoch": 0.6441612638795737,
        "step": 8644
    },
    {
        "loss": 3.0927,
        "grad_norm": 2.0905799865722656,
        "learning_rate": 2.405357082060611e-06,
        "epoch": 0.6442357850808555,
        "step": 8645
    },
    {
        "loss": 2.1755,
        "grad_norm": 2.3918344974517822,
        "learning_rate": 2.390040791114723e-06,
        "epoch": 0.6443103062821373,
        "step": 8646
    },
    {
        "loss": 2.7001,
        "grad_norm": 2.3462679386138916,
        "learning_rate": 2.374772830055727e-06,
        "epoch": 0.6443848274834191,
        "step": 8647
    },
    {
        "loss": 2.1661,
        "grad_norm": 3.0739586353302,
        "learning_rate": 2.3595532064433545e-06,
        "epoch": 0.6444593486847008,
        "step": 8648
    },
    {
        "loss": 2.267,
        "grad_norm": 3.1002695560455322,
        "learning_rate": 2.3443819278132996e-06,
        "epoch": 0.6445338698859826,
        "step": 8649
    },
    {
        "loss": 1.9367,
        "grad_norm": 2.5099239349365234,
        "learning_rate": 2.329259001677342e-06,
        "epoch": 0.6446083910872643,
        "step": 8650
    },
    {
        "loss": 2.5568,
        "grad_norm": 2.453059434890747,
        "learning_rate": 2.314184435523381e-06,
        "epoch": 0.6446829122885461,
        "step": 8651
    },
    {
        "loss": 2.4043,
        "grad_norm": 1.1349397897720337,
        "learning_rate": 2.299158236815291e-06,
        "epoch": 0.6447574334898278,
        "step": 8652
    },
    {
        "loss": 2.0842,
        "grad_norm": 2.696657657623291,
        "learning_rate": 2.284180412993053e-06,
        "epoch": 0.6448319546911097,
        "step": 8653
    },
    {
        "loss": 2.4846,
        "grad_norm": 2.2083957195281982,
        "learning_rate": 2.2692509714726695e-06,
        "epoch": 0.6449064758923914,
        "step": 8654
    },
    {
        "loss": 1.6694,
        "grad_norm": 2.703284740447998,
        "learning_rate": 2.254369919646182e-06,
        "epoch": 0.6449809970936732,
        "step": 8655
    },
    {
        "loss": 2.0258,
        "grad_norm": 2.4598288536071777,
        "learning_rate": 2.2395372648817304e-06,
        "epoch": 0.6450555182949549,
        "step": 8656
    },
    {
        "loss": 2.2683,
        "grad_norm": 1.4131921529769897,
        "learning_rate": 2.2247530145234286e-06,
        "epoch": 0.6451300394962367,
        "step": 8657
    },
    {
        "loss": 2.5152,
        "grad_norm": 2.4424073696136475,
        "learning_rate": 2.210017175891399e-06,
        "epoch": 0.6452045606975184,
        "step": 8658
    },
    {
        "loss": 1.4275,
        "grad_norm": 2.6714115142822266,
        "learning_rate": 2.195329756281905e-06,
        "epoch": 0.6452790818988002,
        "step": 8659
    },
    {
        "loss": 2.2869,
        "grad_norm": 2.996812105178833,
        "learning_rate": 2.1806907629671192e-06,
        "epoch": 0.6453536031000819,
        "step": 8660
    },
    {
        "loss": 1.8884,
        "grad_norm": 3.258600950241089,
        "learning_rate": 2.1661002031953316e-06,
        "epoch": 0.6454281243013638,
        "step": 8661
    },
    {
        "loss": 2.3262,
        "grad_norm": 3.3400328159332275,
        "learning_rate": 2.1515580841907745e-06,
        "epoch": 0.6455026455026455,
        "step": 8662
    },
    {
        "loss": 2.5406,
        "grad_norm": 4.132058620452881,
        "learning_rate": 2.137064413153711e-06,
        "epoch": 0.6455771667039273,
        "step": 8663
    },
    {
        "loss": 1.9298,
        "grad_norm": 3.9744131565093994,
        "learning_rate": 2.122619197260489e-06,
        "epoch": 0.645651687905209,
        "step": 8664
    },
    {
        "loss": 2.398,
        "grad_norm": 2.315804958343506,
        "learning_rate": 2.1082224436633324e-06,
        "epoch": 0.6457262091064908,
        "step": 8665
    },
    {
        "loss": 0.8877,
        "grad_norm": 3.4050021171569824,
        "learning_rate": 2.0938741594906162e-06,
        "epoch": 0.6458007303077725,
        "step": 8666
    },
    {
        "loss": 1.4011,
        "grad_norm": 4.719418525695801,
        "learning_rate": 2.079574351846614e-06,
        "epoch": 0.6458752515090543,
        "step": 8667
    },
    {
        "loss": 1.7592,
        "grad_norm": 2.4902803897857666,
        "learning_rate": 2.0653230278116055e-06,
        "epoch": 0.645949772710336,
        "step": 8668
    },
    {
        "loss": 2.6072,
        "grad_norm": 2.6389968395233154,
        "learning_rate": 2.0511201944419356e-06,
        "epoch": 0.6460242939116179,
        "step": 8669
    },
    {
        "loss": 1.7518,
        "grad_norm": 2.5532493591308594,
        "learning_rate": 2.036965858769868e-06,
        "epoch": 0.6460988151128996,
        "step": 8670
    },
    {
        "loss": 2.503,
        "grad_norm": 2.388638734817505,
        "learning_rate": 2.0228600278036857e-06,
        "epoch": 0.6461733363141814,
        "step": 8671
    },
    {
        "loss": 2.7623,
        "grad_norm": 3.064239740371704,
        "learning_rate": 2.0088027085276353e-06,
        "epoch": 0.6462478575154631,
        "step": 8672
    },
    {
        "loss": 1.416,
        "grad_norm": 2.425373077392578,
        "learning_rate": 1.9947939079019596e-06,
        "epoch": 0.6463223787167449,
        "step": 8673
    },
    {
        "loss": 2.5861,
        "grad_norm": 2.2856807708740234,
        "learning_rate": 1.9808336328628773e-06,
        "epoch": 0.6463968999180266,
        "step": 8674
    },
    {
        "loss": 2.08,
        "grad_norm": 4.115756034851074,
        "learning_rate": 1.9669218903225596e-06,
        "epoch": 0.6464714211193084,
        "step": 8675
    },
    {
        "loss": 2.3654,
        "grad_norm": 2.563471794128418,
        "learning_rate": 1.953058687169207e-06,
        "epoch": 0.6465459423205903,
        "step": 8676
    },
    {
        "loss": 2.3868,
        "grad_norm": 2.1701884269714355,
        "learning_rate": 1.9392440302669177e-06,
        "epoch": 0.646620463521872,
        "step": 8677
    },
    {
        "loss": 2.6924,
        "grad_norm": 4.004327297210693,
        "learning_rate": 1.9254779264557653e-06,
        "epoch": 0.6466949847231538,
        "step": 8678
    },
    {
        "loss": 2.6085,
        "grad_norm": 2.1332602500915527,
        "learning_rate": 1.9117603825518417e-06,
        "epoch": 0.6467695059244355,
        "step": 8679
    },
    {
        "loss": 1.2282,
        "grad_norm": 1.6772243976593018,
        "learning_rate": 1.8980914053471132e-06,
        "epoch": 0.6468440271257173,
        "step": 8680
    },
    {
        "loss": 2.6672,
        "grad_norm": 3.236485481262207,
        "learning_rate": 1.8844710016095667e-06,
        "epoch": 0.646918548326999,
        "step": 8681
    },
    {
        "loss": 1.3474,
        "grad_norm": 1.8523956537246704,
        "learning_rate": 1.8708991780830964e-06,
        "epoch": 0.6469930695282808,
        "step": 8682
    },
    {
        "loss": 2.3269,
        "grad_norm": 3.5051231384277344,
        "learning_rate": 1.857375941487549e-06,
        "epoch": 0.6470675907295625,
        "step": 8683
    },
    {
        "loss": 2.1418,
        "grad_norm": 3.1512954235076904,
        "learning_rate": 1.8439012985187353e-06,
        "epoch": 0.6471421119308444,
        "step": 8684
    },
    {
        "loss": 2.4996,
        "grad_norm": 1.8487783670425415,
        "learning_rate": 1.830475255848385e-06,
        "epoch": 0.6472166331321261,
        "step": 8685
    },
    {
        "loss": 2.3413,
        "grad_norm": 3.2485623359680176,
        "learning_rate": 1.8170978201241474e-06,
        "epoch": 0.6472911543334079,
        "step": 8686
    },
    {
        "loss": 2.5246,
        "grad_norm": 2.592156171798706,
        "learning_rate": 1.8037689979696904e-06,
        "epoch": 0.6473656755346896,
        "step": 8687
    },
    {
        "loss": 1.8046,
        "grad_norm": 4.214146137237549,
        "learning_rate": 1.7904887959844684e-06,
        "epoch": 0.6474401967359714,
        "step": 8688
    },
    {
        "loss": 2.088,
        "grad_norm": 2.9400100708007812,
        "learning_rate": 1.777257220744033e-06,
        "epoch": 0.6475147179372531,
        "step": 8689
    },
    {
        "loss": 2.6834,
        "grad_norm": 2.880183696746826,
        "learning_rate": 1.76407427879971e-06,
        "epoch": 0.6475892391385349,
        "step": 8690
    },
    {
        "loss": 2.0383,
        "grad_norm": 3.077542543411255,
        "learning_rate": 1.7509399766788336e-06,
        "epoch": 0.6476637603398167,
        "step": 8691
    },
    {
        "loss": 2.1463,
        "grad_norm": 2.6910784244537354,
        "learning_rate": 1.7378543208846133e-06,
        "epoch": 0.6477382815410985,
        "step": 8692
    },
    {
        "loss": 2.1665,
        "grad_norm": 1.7955584526062012,
        "learning_rate": 1.7248173178961657e-06,
        "epoch": 0.6478128027423802,
        "step": 8693
    },
    {
        "loss": 2.0234,
        "grad_norm": 3.2082085609436035,
        "learning_rate": 1.7118289741685944e-06,
        "epoch": 0.647887323943662,
        "step": 8694
    },
    {
        "loss": 2.2301,
        "grad_norm": 2.1085314750671387,
        "learning_rate": 1.6988892961328106e-06,
        "epoch": 0.6479618451449437,
        "step": 8695
    },
    {
        "loss": 1.9977,
        "grad_norm": 2.9313809871673584,
        "learning_rate": 1.6859982901956561e-06,
        "epoch": 0.6480363663462255,
        "step": 8696
    },
    {
        "loss": 2.9984,
        "grad_norm": 2.7660443782806396,
        "learning_rate": 1.6731559627399363e-06,
        "epoch": 0.6481108875475072,
        "step": 8697
    },
    {
        "loss": 1.994,
        "grad_norm": 1.6581928730010986,
        "learning_rate": 1.6603623201242757e-06,
        "epoch": 0.648185408748789,
        "step": 8698
    },
    {
        "loss": 2.7344,
        "grad_norm": 3.0082387924194336,
        "learning_rate": 1.647617368683252e-06,
        "epoch": 0.6482599299500708,
        "step": 8699
    },
    {
        "loss": 2.6662,
        "grad_norm": 2.6841530799865723,
        "learning_rate": 1.6349211147272946e-06,
        "epoch": 0.6483344511513526,
        "step": 8700
    },
    {
        "loss": 2.9575,
        "grad_norm": 1.628352165222168,
        "learning_rate": 1.62227356454272e-06,
        "epoch": 0.6484089723526343,
        "step": 8701
    },
    {
        "loss": 1.8597,
        "grad_norm": 3.440809488296509,
        "learning_rate": 1.6096747243917741e-06,
        "epoch": 0.6484834935539161,
        "step": 8702
    },
    {
        "loss": 2.0798,
        "grad_norm": 3.3731532096862793,
        "learning_rate": 1.597124600512534e-06,
        "epoch": 0.6485580147551978,
        "step": 8703
    },
    {
        "loss": 2.0816,
        "grad_norm": 1.9881641864776611,
        "learning_rate": 1.584623199118973e-06,
        "epoch": 0.6486325359564796,
        "step": 8704
    },
    {
        "loss": 2.954,
        "grad_norm": 2.9047036170959473,
        "learning_rate": 1.5721705264009733e-06,
        "epoch": 0.6487070571577613,
        "step": 8705
    },
    {
        "loss": 1.3221,
        "grad_norm": 3.450049877166748,
        "learning_rate": 1.559766588524203e-06,
        "epoch": 0.6487815783590432,
        "step": 8706
    },
    {
        "loss": 2.4896,
        "grad_norm": 2.8052260875701904,
        "learning_rate": 1.5474113916303267e-06,
        "epoch": 0.6488560995603249,
        "step": 8707
    },
    {
        "loss": 2.7797,
        "grad_norm": 2.389134168624878,
        "learning_rate": 1.5351049418367514e-06,
        "epoch": 0.6489306207616067,
        "step": 8708
    },
    {
        "loss": 0.9391,
        "grad_norm": 3.9554622173309326,
        "learning_rate": 1.522847245236847e-06,
        "epoch": 0.6490051419628884,
        "step": 8709
    },
    {
        "loss": 2.3638,
        "grad_norm": 2.7840969562530518,
        "learning_rate": 1.510638307899792e-06,
        "epoch": 0.6490796631641702,
        "step": 8710
    },
    {
        "loss": 2.4619,
        "grad_norm": 2.7323200702667236,
        "learning_rate": 1.4984781358705956e-06,
        "epoch": 0.649154184365452,
        "step": 8711
    },
    {
        "loss": 2.2468,
        "grad_norm": 3.3699591159820557,
        "learning_rate": 1.486366735170197e-06,
        "epoch": 0.6492287055667337,
        "step": 8712
    },
    {
        "loss": 2.3473,
        "grad_norm": 2.004455089569092,
        "learning_rate": 1.4743041117953438e-06,
        "epoch": 0.6493032267680156,
        "step": 8713
    },
    {
        "loss": 2.0813,
        "grad_norm": 4.183905601501465,
        "learning_rate": 1.4622902717186028e-06,
        "epoch": 0.6493777479692973,
        "step": 8714
    },
    {
        "loss": 2.4445,
        "grad_norm": 3.816293954849243,
        "learning_rate": 1.4503252208884487e-06,
        "epoch": 0.6494522691705791,
        "step": 8715
    },
    {
        "loss": 2.0568,
        "grad_norm": 2.382356882095337,
        "learning_rate": 1.4384089652291432e-06,
        "epoch": 0.6495267903718608,
        "step": 8716
    },
    {
        "loss": 2.0201,
        "grad_norm": 2.605560541152954,
        "learning_rate": 1.4265415106408664e-06,
        "epoch": 0.6496013115731426,
        "step": 8717
    },
    {
        "loss": 2.7969,
        "grad_norm": 3.357570171356201,
        "learning_rate": 1.41472286299954e-06,
        "epoch": 0.6496758327744243,
        "step": 8718
    },
    {
        "loss": 2.6202,
        "grad_norm": 2.9955031871795654,
        "learning_rate": 1.4029530281569614e-06,
        "epoch": 0.6497503539757061,
        "step": 8719
    },
    {
        "loss": 2.7773,
        "grad_norm": 2.632145404815674,
        "learning_rate": 1.3912320119407795e-06,
        "epoch": 0.6498248751769878,
        "step": 8720
    },
    {
        "loss": 2.3279,
        "grad_norm": 3.3031065464019775,
        "learning_rate": 1.3795598201544525e-06,
        "epoch": 0.6498993963782697,
        "step": 8721
    },
    {
        "loss": 1.5619,
        "grad_norm": 3.6941707134246826,
        "learning_rate": 1.367936458577268e-06,
        "epoch": 0.6499739175795514,
        "step": 8722
    },
    {
        "loss": 2.354,
        "grad_norm": 2.4612088203430176,
        "learning_rate": 1.3563619329643229e-06,
        "epoch": 0.6500484387808332,
        "step": 8723
    },
    {
        "loss": 2.5878,
        "grad_norm": 2.3650104999542236,
        "learning_rate": 1.3448362490465217e-06,
        "epoch": 0.6501229599821149,
        "step": 8724
    },
    {
        "loss": 2.6646,
        "grad_norm": 1.2631895542144775,
        "learning_rate": 1.3333594125306659e-06,
        "epoch": 0.6501974811833967,
        "step": 8725
    },
    {
        "loss": 1.9551,
        "grad_norm": 4.626589298248291,
        "learning_rate": 1.3219314290992434e-06,
        "epoch": 0.6502720023846784,
        "step": 8726
    },
    {
        "loss": 1.5681,
        "grad_norm": 3.6960220336914062,
        "learning_rate": 1.3105523044106837e-06,
        "epoch": 0.6503465235859602,
        "step": 8727
    },
    {
        "loss": 2.5083,
        "grad_norm": 2.9712023735046387,
        "learning_rate": 1.2992220440991466e-06,
        "epoch": 0.6504210447872419,
        "step": 8728
    },
    {
        "loss": 2.2371,
        "grad_norm": 1.8697227239608765,
        "learning_rate": 1.2879406537745775e-06,
        "epoch": 0.6504955659885238,
        "step": 8729
    },
    {
        "loss": 2.0406,
        "grad_norm": 3.371018409729004,
        "learning_rate": 1.2767081390228087e-06,
        "epoch": 0.6505700871898055,
        "step": 8730
    },
    {
        "loss": 2.6275,
        "grad_norm": 1.9245331287384033,
        "learning_rate": 1.2655245054054133e-06,
        "epoch": 0.6506446083910873,
        "step": 8731
    },
    {
        "loss": 2.4656,
        "grad_norm": 1.8326839208602905,
        "learning_rate": 1.254389758459773e-06,
        "epoch": 0.650719129592369,
        "step": 8732
    },
    {
        "loss": 2.5489,
        "grad_norm": 2.8217687606811523,
        "learning_rate": 1.2433039036990668e-06,
        "epoch": 0.6507936507936508,
        "step": 8733
    },
    {
        "loss": 2.4889,
        "grad_norm": 2.546658754348755,
        "learning_rate": 1.2322669466122483e-06,
        "epoch": 0.6508681719949325,
        "step": 8734
    },
    {
        "loss": 2.7301,
        "grad_norm": 2.5480220317840576,
        "learning_rate": 1.2212788926641017e-06,
        "epoch": 0.6509426931962143,
        "step": 8735
    },
    {
        "loss": 2.5378,
        "grad_norm": 2.509779691696167,
        "learning_rate": 1.2103397472951306e-06,
        "epoch": 0.651017214397496,
        "step": 8736
    },
    {
        "loss": 1.1907,
        "grad_norm": 2.7644338607788086,
        "learning_rate": 1.1994495159217246e-06,
        "epoch": 0.6510917355987779,
        "step": 8737
    },
    {
        "loss": 2.4867,
        "grad_norm": 2.6966376304626465,
        "learning_rate": 1.188608203935948e-06,
        "epoch": 0.6511662568000596,
        "step": 8738
    },
    {
        "loss": 2.5821,
        "grad_norm": 2.385054349899292,
        "learning_rate": 1.1778158167056964e-06,
        "epoch": 0.6512407780013414,
        "step": 8739
    },
    {
        "loss": 2.5371,
        "grad_norm": 3.6684365272521973,
        "learning_rate": 1.1670723595746613e-06,
        "epoch": 0.6513152992026231,
        "step": 8740
    },
    {
        "loss": 1.992,
        "grad_norm": 3.0405893325805664,
        "learning_rate": 1.1563778378622436e-06,
        "epoch": 0.6513898204039049,
        "step": 8741
    },
    {
        "loss": 3.2907,
        "grad_norm": 4.340357303619385,
        "learning_rate": 1.1457322568636852e-06,
        "epoch": 0.6514643416051866,
        "step": 8742
    },
    {
        "loss": 2.4516,
        "grad_norm": 2.529632568359375,
        "learning_rate": 1.1351356218499477e-06,
        "epoch": 0.6515388628064684,
        "step": 8743
    },
    {
        "loss": 2.0396,
        "grad_norm": 3.210526943206787,
        "learning_rate": 1.124587938067745e-06,
        "epoch": 0.6516133840077502,
        "step": 8744
    },
    {
        "loss": 2.0781,
        "grad_norm": 4.55886173248291,
        "learning_rate": 1.1140892107396216e-06,
        "epoch": 0.651687905209032,
        "step": 8745
    },
    {
        "loss": 1.9883,
        "grad_norm": 3.6539804935455322,
        "learning_rate": 1.1036394450638199e-06,
        "epoch": 0.6517624264103138,
        "step": 8746
    },
    {
        "loss": 2.2769,
        "grad_norm": 3.0994904041290283,
        "learning_rate": 1.093238646214345e-06,
        "epoch": 0.6518369476115955,
        "step": 8747
    },
    {
        "loss": 2.2026,
        "grad_norm": 3.2777910232543945,
        "learning_rate": 1.0828868193410224e-06,
        "epoch": 0.6519114688128773,
        "step": 8748
    },
    {
        "loss": 2.5247,
        "grad_norm": 2.1610803604125977,
        "learning_rate": 1.0725839695693297e-06,
        "epoch": 0.651985990014159,
        "step": 8749
    },
    {
        "loss": 2.2977,
        "grad_norm": 2.6793065071105957,
        "learning_rate": 1.062330102000586e-06,
        "epoch": 0.6520605112154408,
        "step": 8750
    },
    {
        "loss": 2.2973,
        "grad_norm": 2.5541508197784424,
        "learning_rate": 1.052125221711786e-06,
        "epoch": 0.6521350324167225,
        "step": 8751
    },
    {
        "loss": 2.5443,
        "grad_norm": 3.2683184146881104,
        "learning_rate": 1.0419693337557213e-06,
        "epoch": 0.6522095536180044,
        "step": 8752
    },
    {
        "loss": 2.406,
        "grad_norm": 3.786168098449707,
        "learning_rate": 1.0318624431608914e-06,
        "epoch": 0.6522840748192861,
        "step": 8753
    },
    {
        "loss": 2.8161,
        "grad_norm": 2.262091875076294,
        "learning_rate": 1.021804554931538e-06,
        "epoch": 0.6523585960205679,
        "step": 8754
    },
    {
        "loss": 2.3324,
        "grad_norm": 3.312432289123535,
        "learning_rate": 1.0117956740476886e-06,
        "epoch": 0.6524331172218496,
        "step": 8755
    },
    {
        "loss": 2.4831,
        "grad_norm": 2.6190948486328125,
        "learning_rate": 1.0018358054650345e-06,
        "epoch": 0.6525076384231314,
        "step": 8756
    },
    {
        "loss": 2.0032,
        "grad_norm": 2.9197239875793457,
        "learning_rate": 9.91924954115031e-07,
        "epoch": 0.6525821596244131,
        "step": 8757
    },
    {
        "loss": 1.5479,
        "grad_norm": 4.8364129066467285,
        "learning_rate": 9.820631249048973e-07,
        "epoch": 0.652656680825695,
        "step": 8758
    },
    {
        "loss": 2.4767,
        "grad_norm": 2.6919496059417725,
        "learning_rate": 9.722503227174939e-07,
        "epoch": 0.6527312020269767,
        "step": 8759
    },
    {
        "loss": 1.9504,
        "grad_norm": 3.4050493240356445,
        "learning_rate": 9.624865524115234e-07,
        "epoch": 0.6528057232282585,
        "step": 8760
    },
    {
        "loss": 2.4883,
        "grad_norm": 2.8087658882141113,
        "learning_rate": 9.527718188213186e-07,
        "epoch": 0.6528802444295402,
        "step": 8761
    },
    {
        "loss": 3.0771,
        "grad_norm": 2.8682637214660645,
        "learning_rate": 9.431061267569541e-07,
        "epoch": 0.652954765630822,
        "step": 8762
    },
    {
        "loss": 2.4017,
        "grad_norm": 2.0585596561431885,
        "learning_rate": 9.334894810042461e-07,
        "epoch": 0.6530292868321037,
        "step": 8763
    },
    {
        "loss": 1.6623,
        "grad_norm": 3.2017629146575928,
        "learning_rate": 9.239218863247079e-07,
        "epoch": 0.6531038080333855,
        "step": 8764
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.9110839366912842,
        "learning_rate": 9.144033474555614e-07,
        "epoch": 0.6531783292346672,
        "step": 8765
    },
    {
        "loss": 2.624,
        "grad_norm": 2.231165647506714,
        "learning_rate": 9.049338691097476e-07,
        "epoch": 0.653252850435949,
        "step": 8766
    },
    {
        "loss": 2.7032,
        "grad_norm": 2.240565776824951,
        "learning_rate": 8.955134559758938e-07,
        "epoch": 0.6533273716372308,
        "step": 8767
    },
    {
        "loss": 2.2082,
        "grad_norm": 2.7638888359069824,
        "learning_rate": 8.861421127184133e-07,
        "epoch": 0.6534018928385126,
        "step": 8768
    },
    {
        "loss": 2.2372,
        "grad_norm": 2.422966718673706,
        "learning_rate": 8.768198439773057e-07,
        "epoch": 0.6534764140397943,
        "step": 8769
    },
    {
        "loss": 2.6019,
        "grad_norm": 1.920426368713379,
        "learning_rate": 8.675466543683785e-07,
        "epoch": 0.6535509352410761,
        "step": 8770
    },
    {
        "loss": 2.1303,
        "grad_norm": 3.076569080352783,
        "learning_rate": 8.583225484830592e-07,
        "epoch": 0.6536254564423578,
        "step": 8771
    },
    {
        "loss": 2.1086,
        "grad_norm": 1.8856405019760132,
        "learning_rate": 8.491475308885055e-07,
        "epoch": 0.6536999776436396,
        "step": 8772
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.84832501411438,
        "learning_rate": 8.400216061275945e-07,
        "epoch": 0.6537744988449213,
        "step": 8773
    },
    {
        "loss": 2.2195,
        "grad_norm": 3.070782423019409,
        "learning_rate": 8.309447787188452e-07,
        "epoch": 0.6538490200462032,
        "step": 8774
    },
    {
        "loss": 2.2328,
        "grad_norm": 3.228567600250244,
        "learning_rate": 8.219170531565179e-07,
        "epoch": 0.6539235412474849,
        "step": 8775
    },
    {
        "loss": 1.7042,
        "grad_norm": 1.599144458770752,
        "learning_rate": 8.129384339105039e-07,
        "epoch": 0.6539980624487667,
        "step": 8776
    },
    {
        "loss": 2.0052,
        "grad_norm": 3.6081161499023438,
        "learning_rate": 8.040089254264138e-07,
        "epoch": 0.6540725836500484,
        "step": 8777
    },
    {
        "loss": 2.3694,
        "grad_norm": 2.210414409637451,
        "learning_rate": 7.951285321255775e-07,
        "epoch": 0.6541471048513302,
        "step": 8778
    },
    {
        "loss": 2.6115,
        "grad_norm": 2.1817820072174072,
        "learning_rate": 7.862972584049333e-07,
        "epoch": 0.6542216260526119,
        "step": 8779
    },
    {
        "loss": 1.9364,
        "grad_norm": 2.494274854660034,
        "learning_rate": 7.775151086371279e-07,
        "epoch": 0.6542961472538937,
        "step": 8780
    },
    {
        "loss": 2.2852,
        "grad_norm": 2.4706437587738037,
        "learning_rate": 7.687820871705276e-07,
        "epoch": 0.6543706684551756,
        "step": 8781
    },
    {
        "loss": 2.4894,
        "grad_norm": 2.7810397148132324,
        "learning_rate": 7.600981983291067e-07,
        "epoch": 0.6544451896564573,
        "step": 8782
    },
    {
        "loss": 2.0538,
        "grad_norm": 3.445145606994629,
        "learning_rate": 7.514634464125703e-07,
        "epoch": 0.6545197108577391,
        "step": 8783
    },
    {
        "loss": 1.5093,
        "grad_norm": 3.4687411785125732,
        "learning_rate": 7.428778356962318e-07,
        "epoch": 0.6545942320590208,
        "step": 8784
    },
    {
        "loss": 1.7531,
        "grad_norm": 2.620879650115967,
        "learning_rate": 7.343413704311353e-07,
        "epoch": 0.6546687532603026,
        "step": 8785
    },
    {
        "loss": 1.8156,
        "grad_norm": 3.7439608573913574,
        "learning_rate": 7.258540548439441e-07,
        "epoch": 0.6547432744615843,
        "step": 8786
    },
    {
        "loss": 2.1156,
        "grad_norm": 2.142835855484009,
        "learning_rate": 7.174158931370078e-07,
        "epoch": 0.6548177956628661,
        "step": 8787
    },
    {
        "loss": 2.2934,
        "grad_norm": 2.152390241622925,
        "learning_rate": 7.090268894883734e-07,
        "epoch": 0.6548923168641478,
        "step": 8788
    },
    {
        "loss": 2.2304,
        "grad_norm": 2.6619980335235596,
        "learning_rate": 7.006870480516625e-07,
        "epoch": 0.6549668380654297,
        "step": 8789
    },
    {
        "loss": 2.6138,
        "grad_norm": 3.3643031120300293,
        "learning_rate": 6.923963729562277e-07,
        "epoch": 0.6550413592667114,
        "step": 8790
    },
    {
        "loss": 2.5008,
        "grad_norm": 2.1636316776275635,
        "learning_rate": 6.84154868307052e-07,
        "epoch": 0.6551158804679932,
        "step": 8791
    },
    {
        "loss": 2.6657,
        "grad_norm": 2.678051233291626,
        "learning_rate": 6.759625381847712e-07,
        "epoch": 0.6551904016692749,
        "step": 8792
    },
    {
        "loss": 2.3444,
        "grad_norm": 2.712407112121582,
        "learning_rate": 6.67819386645685e-07,
        "epoch": 0.6552649228705567,
        "step": 8793
    },
    {
        "loss": 2.6625,
        "grad_norm": 1.6165817975997925,
        "learning_rate": 6.59725417721735e-07,
        "epoch": 0.6553394440718384,
        "step": 8794
    },
    {
        "loss": 1.4489,
        "grad_norm": 3.336214542388916,
        "learning_rate": 6.516806354204819e-07,
        "epoch": 0.6554139652731202,
        "step": 8795
    },
    {
        "loss": 1.9461,
        "grad_norm": 4.242701053619385,
        "learning_rate": 6.436850437251951e-07,
        "epoch": 0.6554884864744019,
        "step": 8796
    },
    {
        "loss": 2.2804,
        "grad_norm": 3.1433205604553223,
        "learning_rate": 6.357386465947301e-07,
        "epoch": 0.6555630076756838,
        "step": 8797
    },
    {
        "loss": 2.3116,
        "grad_norm": 4.078238010406494,
        "learning_rate": 6.278414479636286e-07,
        "epoch": 0.6556375288769655,
        "step": 8798
    },
    {
        "loss": 2.1772,
        "grad_norm": 3.106037139892578,
        "learning_rate": 6.199934517420403e-07,
        "epoch": 0.6557120500782473,
        "step": 8799
    },
    {
        "loss": 2.5602,
        "grad_norm": 2.0238142013549805,
        "learning_rate": 6.121946618157459e-07,
        "epoch": 0.655786571279529,
        "step": 8800
    },
    {
        "loss": 2.588,
        "grad_norm": 2.2832953929901123,
        "learning_rate": 6.044450820462344e-07,
        "epoch": 0.6558610924808108,
        "step": 8801
    },
    {
        "loss": 3.0116,
        "grad_norm": 4.251858711242676,
        "learning_rate": 5.967447162705142e-07,
        "epoch": 0.6559356136820925,
        "step": 8802
    },
    {
        "loss": 2.4325,
        "grad_norm": 2.7495474815368652,
        "learning_rate": 5.890935683013465e-07,
        "epoch": 0.6560101348833743,
        "step": 8803
    },
    {
        "loss": 2.4655,
        "grad_norm": 2.7302281856536865,
        "learning_rate": 5.814916419270344e-07,
        "epoch": 0.656084656084656,
        "step": 8804
    },
    {
        "loss": 2.5061,
        "grad_norm": 2.9632225036621094,
        "learning_rate": 5.739389409115226e-07,
        "epoch": 0.6561591772859379,
        "step": 8805
    },
    {
        "loss": 1.9953,
        "grad_norm": 2.8518588542938232,
        "learning_rate": 5.664354689944418e-07,
        "epoch": 0.6562336984872196,
        "step": 8806
    },
    {
        "loss": 2.0138,
        "grad_norm": 3.162449836730957,
        "learning_rate": 5.589812298909869e-07,
        "epoch": 0.6563082196885014,
        "step": 8807
    },
    {
        "loss": 2.635,
        "grad_norm": 3.541012763977051,
        "learning_rate": 5.515762272919722e-07,
        "epoch": 0.6563827408897831,
        "step": 8808
    },
    {
        "loss": 2.4017,
        "grad_norm": 2.7439146041870117,
        "learning_rate": 5.442204648638982e-07,
        "epoch": 0.6564572620910649,
        "step": 8809
    },
    {
        "loss": 2.348,
        "grad_norm": 3.482057571411133,
        "learning_rate": 5.369139462488182e-07,
        "epoch": 0.6565317832923466,
        "step": 8810
    },
    {
        "loss": 2.7272,
        "grad_norm": 2.814444065093994,
        "learning_rate": 5.296566750644383e-07,
        "epoch": 0.6566063044936284,
        "step": 8811
    },
    {
        "loss": 2.5583,
        "grad_norm": 3.0486361980438232,
        "learning_rate": 5.224486549040842e-07,
        "epoch": 0.6566808256949102,
        "step": 8812
    },
    {
        "loss": 2.0766,
        "grad_norm": 2.7930831909179688,
        "learning_rate": 5.152898893366453e-07,
        "epoch": 0.656755346896192,
        "step": 8813
    },
    {
        "loss": 1.6794,
        "grad_norm": 4.122762203216553,
        "learning_rate": 5.081803819066977e-07,
        "epoch": 0.6568298680974737,
        "step": 8814
    },
    {
        "loss": 2.6709,
        "grad_norm": 1.9674988985061646,
        "learning_rate": 5.011201361343699e-07,
        "epoch": 0.6569043892987555,
        "step": 8815
    },
    {
        "loss": 2.4372,
        "grad_norm": 2.2291738986968994,
        "learning_rate": 4.941091555154431e-07,
        "epoch": 0.6569789105000372,
        "step": 8816
    },
    {
        "loss": 2.9523,
        "grad_norm": 2.5869088172912598,
        "learning_rate": 4.87147443521252e-07,
        "epoch": 0.657053431701319,
        "step": 8817
    },
    {
        "loss": 2.115,
        "grad_norm": 2.9399380683898926,
        "learning_rate": 4.802350035987835e-07,
        "epoch": 0.6571279529026008,
        "step": 8818
    },
    {
        "loss": 2.3844,
        "grad_norm": 2.2491066455841064,
        "learning_rate": 4.733718391706221e-07,
        "epoch": 0.6572024741038826,
        "step": 8819
    },
    {
        "loss": 1.6189,
        "grad_norm": 4.334135055541992,
        "learning_rate": 4.6655795363491626e-07,
        "epoch": 0.6572769953051644,
        "step": 8820
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.1982388496398926,
        "learning_rate": 4.597933503654894e-07,
        "epoch": 0.6573515165064461,
        "step": 8821
    },
    {
        "loss": 1.8551,
        "grad_norm": 2.3143274784088135,
        "learning_rate": 4.530780327116846e-07,
        "epoch": 0.6574260377077279,
        "step": 8822
    },
    {
        "loss": 2.6425,
        "grad_norm": 2.0401225090026855,
        "learning_rate": 4.464120039984754e-07,
        "epoch": 0.6575005589090096,
        "step": 8823
    },
    {
        "loss": 2.6006,
        "grad_norm": 4.616971015930176,
        "learning_rate": 4.3979526752645495e-07,
        "epoch": 0.6575750801102914,
        "step": 8824
    },
    {
        "loss": 2.2369,
        "grad_norm": 2.411212205886841,
        "learning_rate": 4.332278265717582e-07,
        "epoch": 0.6576496013115731,
        "step": 8825
    },
    {
        "loss": 2.3191,
        "grad_norm": 2.0906224250793457,
        "learning_rate": 4.2670968438616174e-07,
        "epoch": 0.657724122512855,
        "step": 8826
    },
    {
        "loss": 2.576,
        "grad_norm": 2.73360276222229,
        "learning_rate": 4.2024084419699514e-07,
        "epoch": 0.6577986437141367,
        "step": 8827
    },
    {
        "loss": 2.6783,
        "grad_norm": 2.816091299057007,
        "learning_rate": 4.138213092071963e-07,
        "epoch": 0.6578731649154185,
        "step": 8828
    },
    {
        "loss": 2.2951,
        "grad_norm": 3.6720547676086426,
        "learning_rate": 4.074510825953004e-07,
        "epoch": 0.6579476861167002,
        "step": 8829
    },
    {
        "loss": 2.6224,
        "grad_norm": 2.15083384513855,
        "learning_rate": 4.011301675153956e-07,
        "epoch": 0.658022207317982,
        "step": 8830
    },
    {
        "loss": 2.1076,
        "grad_norm": 3.0290069580078125,
        "learning_rate": 3.948585670971894e-07,
        "epoch": 0.6580967285192637,
        "step": 8831
    },
    {
        "loss": 2.1623,
        "grad_norm": 3.4756364822387695,
        "learning_rate": 3.886362844459646e-07,
        "epoch": 0.6581712497205455,
        "step": 8832
    },
    {
        "loss": 2.1421,
        "grad_norm": 1.9799396991729736,
        "learning_rate": 3.8246332264254557e-07,
        "epoch": 0.6582457709218272,
        "step": 8833
    },
    {
        "loss": 2.4541,
        "grad_norm": 2.5347039699554443,
        "learning_rate": 3.763396847433875e-07,
        "epoch": 0.658320292123109,
        "step": 8834
    },
    {
        "loss": 2.0405,
        "grad_norm": 3.588636636734009,
        "learning_rate": 3.702653737805095e-07,
        "epoch": 0.6583948133243908,
        "step": 8835
    },
    {
        "loss": 2.6815,
        "grad_norm": 4.041102409362793,
        "learning_rate": 3.6424039276149456e-07,
        "epoch": 0.6584693345256726,
        "step": 8836
    },
    {
        "loss": 2.3274,
        "grad_norm": 3.381427049636841,
        "learning_rate": 3.5826474466950086e-07,
        "epoch": 0.6585438557269543,
        "step": 8837
    },
    {
        "loss": 2.7255,
        "grad_norm": 2.5146427154541016,
        "learning_rate": 3.523384324632728e-07,
        "epoch": 0.6586183769282361,
        "step": 8838
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.98860239982605,
        "learning_rate": 3.464614590771298e-07,
        "epoch": 0.6586928981295178,
        "step": 8839
    },
    {
        "loss": 2.681,
        "grad_norm": 2.602184772491455,
        "learning_rate": 3.4063382742095527e-07,
        "epoch": 0.6587674193307996,
        "step": 8840
    },
    {
        "loss": 2.651,
        "grad_norm": 2.9579124450683594,
        "learning_rate": 3.3485554038017453e-07,
        "epoch": 0.6588419405320813,
        "step": 8841
    },
    {
        "loss": 2.0251,
        "grad_norm": 3.1657488346099854,
        "learning_rate": 3.2912660081583226e-07,
        "epoch": 0.6589164617333632,
        "step": 8842
    },
    {
        "loss": 2.3275,
        "grad_norm": 4.016694068908691,
        "learning_rate": 3.2344701156451497e-07,
        "epoch": 0.6589909829346449,
        "step": 8843
    },
    {
        "loss": 2.0614,
        "grad_norm": 4.115044593811035,
        "learning_rate": 3.1781677543835096e-07,
        "epoch": 0.6590655041359267,
        "step": 8844
    },
    {
        "loss": 1.8721,
        "grad_norm": 2.6683712005615234,
        "learning_rate": 3.1223589522508813e-07,
        "epoch": 0.6591400253372084,
        "step": 8845
    },
    {
        "loss": 2.6378,
        "grad_norm": 2.474220037460327,
        "learning_rate": 3.0670437368797154e-07,
        "epoch": 0.6592145465384902,
        "step": 8846
    },
    {
        "loss": 1.3985,
        "grad_norm": 3.6473705768585205,
        "learning_rate": 3.012222135658438e-07,
        "epoch": 0.6592890677397719,
        "step": 8847
    },
    {
        "loss": 2.6128,
        "grad_norm": 1.7446928024291992,
        "learning_rate": 2.957894175731113e-07,
        "epoch": 0.6593635889410537,
        "step": 8848
    },
    {
        "loss": 2.0182,
        "grad_norm": 3.285926103591919,
        "learning_rate": 2.9040598839973345e-07,
        "epoch": 0.6594381101423354,
        "step": 8849
    },
    {
        "loss": 2.2764,
        "grad_norm": 1.5393831729888916,
        "learning_rate": 2.850719287112114e-07,
        "epoch": 0.6595126313436173,
        "step": 8850
    },
    {
        "loss": 2.5438,
        "grad_norm": 3.3687148094177246,
        "learning_rate": 2.7978724114861023e-07,
        "epoch": 0.659587152544899,
        "step": 8851
    },
    {
        "loss": 2.0642,
        "grad_norm": 3.9108784198760986,
        "learning_rate": 2.7455192832857026e-07,
        "epoch": 0.6596616737461808,
        "step": 8852
    },
    {
        "loss": 2.1222,
        "grad_norm": 4.442926406860352,
        "learning_rate": 2.6936599284325126e-07,
        "epoch": 0.6597361949474626,
        "step": 8853
    },
    {
        "loss": 1.9193,
        "grad_norm": 2.667792320251465,
        "learning_rate": 2.642294372603771e-07,
        "epoch": 0.6598107161487443,
        "step": 8854
    },
    {
        "loss": 2.2084,
        "grad_norm": 3.750316619873047,
        "learning_rate": 2.591422641232355e-07,
        "epoch": 0.6598852373500261,
        "step": 8855
    },
    {
        "loss": 2.6469,
        "grad_norm": 2.4677352905273438,
        "learning_rate": 2.54104475950645e-07,
        "epoch": 0.6599597585513078,
        "step": 8856
    },
    {
        "loss": 2.0191,
        "grad_norm": 3.4623336791992188,
        "learning_rate": 2.491160752369881e-07,
        "epoch": 0.6600342797525897,
        "step": 8857
    },
    {
        "loss": 2.7779,
        "grad_norm": 2.134078025817871,
        "learning_rate": 2.4417706445216683e-07,
        "epoch": 0.6601088009538714,
        "step": 8858
    },
    {
        "loss": 2.4475,
        "grad_norm": 1.8935463428497314,
        "learning_rate": 2.3928744604166944e-07,
        "epoch": 0.6601833221551532,
        "step": 8859
    },
    {
        "loss": 2.6731,
        "grad_norm": 3.490786075592041,
        "learning_rate": 2.3444722242649265e-07,
        "epoch": 0.6602578433564349,
        "step": 8860
    },
    {
        "loss": 2.3109,
        "grad_norm": 4.252028465270996,
        "learning_rate": 2.2965639600318612e-07,
        "epoch": 0.6603323645577167,
        "step": 8861
    },
    {
        "loss": 1.788,
        "grad_norm": 2.9159438610076904,
        "learning_rate": 2.2491496914386346e-07,
        "epoch": 0.6604068857589984,
        "step": 8862
    },
    {
        "loss": 2.354,
        "grad_norm": 1.7804375886917114,
        "learning_rate": 2.2022294419613565e-07,
        "epoch": 0.6604814069602802,
        "step": 8863
    },
    {
        "loss": 2.2425,
        "grad_norm": 2.5616695880889893,
        "learning_rate": 2.155803234831999e-07,
        "epoch": 0.660555928161562,
        "step": 8864
    },
    {
        "loss": 2.4926,
        "grad_norm": 2.2755327224731445,
        "learning_rate": 2.1098710930376188e-07,
        "epoch": 0.6606304493628438,
        "step": 8865
    },
    {
        "loss": 3.1766,
        "grad_norm": 2.884352922439575,
        "learning_rate": 2.06443303932069e-07,
        "epoch": 0.6607049705641255,
        "step": 8866
    },
    {
        "loss": 2.8418,
        "grad_norm": 2.202425718307495,
        "learning_rate": 2.0194890961791058e-07,
        "epoch": 0.6607794917654073,
        "step": 8867
    },
    {
        "loss": 1.6653,
        "grad_norm": 2.7110328674316406,
        "learning_rate": 1.9750392858659538e-07,
        "epoch": 0.660854012966689,
        "step": 8868
    },
    {
        "loss": 2.0543,
        "grad_norm": 2.999577760696411,
        "learning_rate": 1.9310836303900738e-07,
        "epoch": 0.6609285341679708,
        "step": 8869
    },
    {
        "loss": 2.5187,
        "grad_norm": 3.2445240020751953,
        "learning_rate": 1.8876221515151672e-07,
        "epoch": 0.6610030553692525,
        "step": 8870
    },
    {
        "loss": 2.0338,
        "grad_norm": 3.60361647605896,
        "learning_rate": 1.844654870760354e-07,
        "epoch": 0.6610775765705343,
        "step": 8871
    },
    {
        "loss": 2.5698,
        "grad_norm": 3.618056058883667,
        "learning_rate": 1.8021818094003939e-07,
        "epoch": 0.661152097771816,
        "step": 8872
    },
    {
        "loss": 1.9847,
        "grad_norm": 3.240111827850342,
        "learning_rate": 1.7602029884649096e-07,
        "epoch": 0.6612266189730979,
        "step": 8873
    },
    {
        "loss": 2.7281,
        "grad_norm": 2.8317089080810547,
        "learning_rate": 1.7187184287389414e-07,
        "epoch": 0.6613011401743796,
        "step": 8874
    },
    {
        "loss": 0.9871,
        "grad_norm": 3.7844302654266357,
        "learning_rate": 1.6777281507630583e-07,
        "epoch": 0.6613756613756614,
        "step": 8875
    },
    {
        "loss": 2.4544,
        "grad_norm": 2.7643253803253174,
        "learning_rate": 1.6372321748326923e-07,
        "epoch": 0.6614501825769431,
        "step": 8876
    },
    {
        "loss": 2.5951,
        "grad_norm": 2.554811954498291,
        "learning_rate": 1.5972305209988047e-07,
        "epoch": 0.6615247037782249,
        "step": 8877
    },
    {
        "loss": 2.0923,
        "grad_norm": 3.6206445693969727,
        "learning_rate": 1.557723209067663e-07,
        "epoch": 0.6615992249795066,
        "step": 8878
    },
    {
        "loss": 1.8957,
        "grad_norm": 4.076087951660156,
        "learning_rate": 1.5187102586002866e-07,
        "epoch": 0.6616737461807884,
        "step": 8879
    },
    {
        "loss": 2.4485,
        "grad_norm": 1.8788048028945923,
        "learning_rate": 1.480191688913557e-07,
        "epoch": 0.6617482673820702,
        "step": 8880
    },
    {
        "loss": 2.3546,
        "grad_norm": 2.665395736694336,
        "learning_rate": 1.4421675190791074e-07,
        "epoch": 0.661822788583352,
        "step": 8881
    },
    {
        "loss": 2.5682,
        "grad_norm": 3.0617570877075195,
        "learning_rate": 1.4046377679240995e-07,
        "epoch": 0.6618973097846337,
        "step": 8882
    },
    {
        "loss": 2.665,
        "grad_norm": 3.2024829387664795,
        "learning_rate": 1.3676024540307808e-07,
        "epoch": 0.6619718309859155,
        "step": 8883
    },
    {
        "loss": 2.3681,
        "grad_norm": 3.5595133304595947,
        "learning_rate": 1.3310615957362603e-07,
        "epoch": 0.6620463521871972,
        "step": 8884
    },
    {
        "loss": 2.3002,
        "grad_norm": 3.0764946937561035,
        "learning_rate": 1.2950152111333992e-07,
        "epoch": 0.662120873388479,
        "step": 8885
    },
    {
        "loss": 1.7896,
        "grad_norm": 2.8464317321777344,
        "learning_rate": 1.259463318069809e-07,
        "epoch": 0.6621953945897607,
        "step": 8886
    },
    {
        "loss": 2.4928,
        "grad_norm": 3.261025905609131,
        "learning_rate": 1.2244059341484093e-07,
        "epoch": 0.6622699157910426,
        "step": 8887
    },
    {
        "loss": 1.6873,
        "grad_norm": 3.9753072261810303,
        "learning_rate": 1.189843076727315e-07,
        "epoch": 0.6623444369923244,
        "step": 8888
    },
    {
        "loss": 2.6113,
        "grad_norm": 2.8798255920410156,
        "learning_rate": 1.1557747629196147e-07,
        "epoch": 0.6624189581936061,
        "step": 8889
    },
    {
        "loss": 2.7979,
        "grad_norm": 3.380089044570923,
        "learning_rate": 1.1222010095938152e-07,
        "epoch": 0.6624934793948879,
        "step": 8890
    },
    {
        "loss": 1.2001,
        "grad_norm": 1.7963014841079712,
        "learning_rate": 1.0891218333732856e-07,
        "epoch": 0.6625680005961696,
        "step": 8891
    },
    {
        "loss": 2.4259,
        "grad_norm": 2.4057953357696533,
        "learning_rate": 1.0565372506365911e-07,
        "epoch": 0.6626425217974514,
        "step": 8892
    },
    {
        "loss": 1.5404,
        "grad_norm": 1.9480187892913818,
        "learning_rate": 1.0244472775173818e-07,
        "epoch": 0.6627170429987331,
        "step": 8893
    },
    {
        "loss": 2.5069,
        "grad_norm": 2.4331135749816895,
        "learning_rate": 9.928519299046146e-08,
        "epoch": 0.662791564200015,
        "step": 8894
    },
    {
        "loss": 2.4677,
        "grad_norm": 2.693708896636963,
        "learning_rate": 9.617512234419978e-08,
        "epoch": 0.6628660854012967,
        "step": 8895
    },
    {
        "loss": 2.1545,
        "grad_norm": 3.1572749614715576,
        "learning_rate": 9.31145173528658e-08,
        "epoch": 0.6629406066025785,
        "step": 8896
    },
    {
        "loss": 2.3675,
        "grad_norm": 2.5112102031707764,
        "learning_rate": 9.010337953185843e-08,
        "epoch": 0.6630151278038602,
        "step": 8897
    },
    {
        "loss": 2.5796,
        "grad_norm": 2.487186908721924,
        "learning_rate": 8.714171037208508e-08,
        "epoch": 0.663089649005142,
        "step": 8898
    },
    {
        "loss": 2.7371,
        "grad_norm": 2.015835762023926,
        "learning_rate": 8.42295113399727e-08,
        "epoch": 0.6631641702064237,
        "step": 8899
    },
    {
        "loss": 2.1348,
        "grad_norm": 1.9795974493026733,
        "learning_rate": 8.136678387744568e-08,
        "epoch": 0.6632386914077055,
        "step": 8900
    },
    {
        "loss": 2.549,
        "grad_norm": 2.5829391479492188,
        "learning_rate": 7.855352940193683e-08,
        "epoch": 0.6633132126089872,
        "step": 8901
    },
    {
        "loss": 2.1683,
        "grad_norm": 3.3006765842437744,
        "learning_rate": 7.578974930637639e-08,
        "epoch": 0.663387733810269,
        "step": 8902
    },
    {
        "loss": 2.8008,
        "grad_norm": 2.608588457107544,
        "learning_rate": 7.307544495919193e-08,
        "epoch": 0.6634622550115508,
        "step": 8903
    },
    {
        "loss": 2.4653,
        "grad_norm": 2.360278367996216,
        "learning_rate": 7.041061770435286e-08,
        "epoch": 0.6635367762128326,
        "step": 8904
    },
    {
        "loss": 2.2362,
        "grad_norm": 2.3072562217712402,
        "learning_rate": 6.77952688612704e-08,
        "epoch": 0.6636112974141143,
        "step": 8905
    },
    {
        "loss": 1.9959,
        "grad_norm": 3.1102190017700195,
        "learning_rate": 6.522939972491982e-08,
        "epoch": 0.6636858186153961,
        "step": 8906
    },
    {
        "loss": 2.6882,
        "grad_norm": 2.1291797161102295,
        "learning_rate": 6.271301156571818e-08,
        "epoch": 0.6637603398166778,
        "step": 8907
    },
    {
        "loss": 1.1258,
        "grad_norm": 2.201917886734009,
        "learning_rate": 6.024610562962441e-08,
        "epoch": 0.6638348610179596,
        "step": 8908
    },
    {
        "loss": 2.2634,
        "grad_norm": 3.3630473613739014,
        "learning_rate": 5.782868313808365e-08,
        "epoch": 0.6639093822192413,
        "step": 8909
    },
    {
        "loss": 1.9775,
        "grad_norm": 2.795908212661743,
        "learning_rate": 5.5460745288049566e-08,
        "epoch": 0.6639839034205232,
        "step": 8910
    },
    {
        "loss": 1.7552,
        "grad_norm": 3.27876615524292,
        "learning_rate": 5.314229325196207e-08,
        "epoch": 0.6640584246218049,
        "step": 8911
    },
    {
        "loss": 2.0988,
        "grad_norm": 2.7180395126342773,
        "learning_rate": 5.087332817774737e-08,
        "epoch": 0.6641329458230867,
        "step": 8912
    },
    {
        "loss": 2.2287,
        "grad_norm": 2.3625831604003906,
        "learning_rate": 4.865385118886234e-08,
        "epoch": 0.6642074670243684,
        "step": 8913
    },
    {
        "loss": 2.4256,
        "grad_norm": 1.9333082437515259,
        "learning_rate": 4.648386338422794e-08,
        "epoch": 0.6642819882256502,
        "step": 8914
    },
    {
        "loss": 2.4358,
        "grad_norm": 2.574678421020508,
        "learning_rate": 4.4363365838295814e-08,
        "epoch": 0.6643565094269319,
        "step": 8915
    },
    {
        "loss": 2.2563,
        "grad_norm": 4.120283603668213,
        "learning_rate": 4.2292359600970555e-08,
        "epoch": 0.6644310306282137,
        "step": 8916
    },
    {
        "loss": 2.7206,
        "grad_norm": 2.7603769302368164,
        "learning_rate": 4.027084569769857e-08,
        "epoch": 0.6645055518294954,
        "step": 8917
    },
    {
        "loss": 1.9191,
        "grad_norm": 3.86460542678833,
        "learning_rate": 3.829882512937921e-08,
        "epoch": 0.6645800730307773,
        "step": 8918
    },
    {
        "loss": 2.9633,
        "grad_norm": 2.497002601623535,
        "learning_rate": 3.6376298872420335e-08,
        "epoch": 0.664654594232059,
        "step": 8919
    },
    {
        "loss": 2.4874,
        "grad_norm": 2.716141700744629,
        "learning_rate": 3.4503267878760456e-08,
        "epoch": 0.6647291154333408,
        "step": 8920
    },
    {
        "loss": 1.9988,
        "grad_norm": 2.1897385120391846,
        "learning_rate": 3.2679733075757776e-08,
        "epoch": 0.6648036366346225,
        "step": 8921
    },
    {
        "loss": 0.6173,
        "grad_norm": 3.0210392475128174,
        "learning_rate": 3.0905695366334475e-08,
        "epoch": 0.6648781578359043,
        "step": 8922
    },
    {
        "loss": 2.3471,
        "grad_norm": 1.9662959575653076,
        "learning_rate": 2.9181155628854596e-08,
        "epoch": 0.6649526790371861,
        "step": 8923
    },
    {
        "loss": 1.913,
        "grad_norm": 2.2866768836975098,
        "learning_rate": 2.750611471721287e-08,
        "epoch": 0.6650272002384678,
        "step": 8924
    },
    {
        "loss": 2.79,
        "grad_norm": 4.454990863800049,
        "learning_rate": 2.5880573460757007e-08,
        "epoch": 0.6651017214397497,
        "step": 8925
    },
    {
        "loss": 1.8548,
        "grad_norm": 5.8070831298828125,
        "learning_rate": 2.4304532664365387e-08,
        "epoch": 0.6651762426410314,
        "step": 8926
    },
    {
        "loss": 2.3021,
        "grad_norm": 1.821110725402832,
        "learning_rate": 2.2777993108369367e-08,
        "epoch": 0.6652507638423132,
        "step": 8927
    },
    {
        "loss": 2.189,
        "grad_norm": 3.604804277420044,
        "learning_rate": 2.1300955548619884e-08,
        "epoch": 0.6653252850435949,
        "step": 8928
    },
    {
        "loss": 2.5801,
        "grad_norm": 3.028738260269165,
        "learning_rate": 1.9873420716431945e-08,
        "epoch": 0.6653998062448767,
        "step": 8929
    },
    {
        "loss": 2.1116,
        "grad_norm": 3.516476631164551,
        "learning_rate": 1.8495389318651246e-08,
        "epoch": 0.6654743274461584,
        "step": 8930
    },
    {
        "loss": 2.2621,
        "grad_norm": 2.9878766536712646,
        "learning_rate": 1.7166862037565345e-08,
        "epoch": 0.6655488486474402,
        "step": 8931
    },
    {
        "loss": 2.7273,
        "grad_norm": 5.039078235626221,
        "learning_rate": 1.5887839530981385e-08,
        "epoch": 0.665623369848722,
        "step": 8932
    },
    {
        "loss": 2.0973,
        "grad_norm": 2.754960298538208,
        "learning_rate": 1.465832243217058e-08,
        "epoch": 0.6656978910500038,
        "step": 8933
    },
    {
        "loss": 2.7179,
        "grad_norm": 1.8410001993179321,
        "learning_rate": 1.3478311349934824e-08,
        "epoch": 0.6657724122512855,
        "step": 8934
    },
    {
        "loss": 2.5884,
        "grad_norm": 4.899200439453125,
        "learning_rate": 1.2347806868506784e-08,
        "epoch": 0.6658469334525673,
        "step": 8935
    },
    {
        "loss": 2.2219,
        "grad_norm": 3.118187665939331,
        "learning_rate": 1.1266809547649803e-08,
        "epoch": 0.665921454653849,
        "step": 8936
    },
    {
        "loss": 2.3257,
        "grad_norm": 2.8288321495056152,
        "learning_rate": 1.0235319922602404e-08,
        "epoch": 0.6659959758551308,
        "step": 8937
    },
    {
        "loss": 2.477,
        "grad_norm": 2.8818936347961426,
        "learning_rate": 9.253338504089381e-09,
        "epoch": 0.6660704970564125,
        "step": 8938
    },
    {
        "loss": 1.698,
        "grad_norm": 4.395875930786133,
        "learning_rate": 8.320865778310705e-09,
        "epoch": 0.6661450182576943,
        "step": 8939
    },
    {
        "loss": 2.2255,
        "grad_norm": 2.7156453132629395,
        "learning_rate": 7.437902206985925e-09,
        "epoch": 0.666219539458976,
        "step": 8940
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.204132556915283,
        "learning_rate": 6.604448227276461e-09,
        "epoch": 0.6662940606602579,
        "step": 8941
    },
    {
        "loss": 2.3198,
        "grad_norm": 3.440661907196045,
        "learning_rate": 5.8205042518633124e-09,
        "epoch": 0.6663685818615396,
        "step": 8942
    },
    {
        "loss": 2.6152,
        "grad_norm": 3.251248836517334,
        "learning_rate": 5.086070668902654e-09,
        "epoch": 0.6664431030628214,
        "step": 8943
    },
    {
        "loss": 2.4127,
        "grad_norm": 2.9204273223876953,
        "learning_rate": 4.4011478420258325e-09,
        "epoch": 0.6665176242641031,
        "step": 8944
    },
    {
        "loss": 2.0212,
        "grad_norm": 1.3681046962738037,
        "learning_rate": 3.7657361103837776e-09,
        "epoch": 0.6665921454653849,
        "step": 8945
    },
    {
        "loss": 2.8124,
        "grad_norm": 1.838484525680542,
        "learning_rate": 3.1798357885692854e-09,
        "epoch": 0.6666666666666666,
        "step": 8946
    },
    {
        "loss": 2.5381,
        "grad_norm": 2.4706361293792725,
        "learning_rate": 2.6434471666947348e-09,
        "epoch": 0.6667411878679484,
        "step": 8947
    },
    {
        "loss": 2.0024,
        "grad_norm": 4.788440704345703,
        "learning_rate": 2.156570510325473e-09,
        "epoch": 0.6668157090692302,
        "step": 8948
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.6205756664276123,
        "learning_rate": 1.71920606054643e-09,
        "epoch": 0.666890230270512,
        "step": 8949
    },
    {
        "loss": 2.8139,
        "grad_norm": 2.133840560913086,
        "learning_rate": 1.3313540339066067e-09,
        "epoch": 0.6669647514717937,
        "step": 8950
    },
    {
        "loss": 2.2882,
        "grad_norm": 2.6249334812164307,
        "learning_rate": 9.930146224412796e-10,
        "epoch": 0.6670392726730755,
        "step": 8951
    },
    {
        "loss": 2.5114,
        "grad_norm": 2.1220767498016357,
        "learning_rate": 7.041879936720008e-10,
        "epoch": 0.6671137938743572,
        "step": 8952
    },
    {
        "loss": 2.5084,
        "grad_norm": 2.353532075881958,
        "learning_rate": 4.648742906177006e-10,
        "epoch": 0.667188315075639,
        "step": 8953
    },
    {
        "loss": 2.5439,
        "grad_norm": 2.1589910984039307,
        "learning_rate": 2.7507363176138e-10,
        "epoch": 0.6672628362769207,
        "step": 8954
    },
    {
        "loss": 2.2862,
        "grad_norm": 1.7846062183380127,
        "learning_rate": 1.3478611107231587e-10,
        "epoch": 0.6673373574782026,
        "step": 8955
    },
    {
        "loss": 1.9295,
        "grad_norm": 3.557647466659546,
        "learning_rate": 4.401179802826505e-11,
        "epoch": 0.6674118786794843,
        "step": 8956
    },
    {
        "loss": 2.1094,
        "grad_norm": 2.661773681640625,
        "learning_rate": 2.7507375710555948e-12,
        "epoch": 0.6674863998807661,
        "step": 8957
    },
    {
        "loss": 2.5567,
        "grad_norm": 1.4043430089950562,
        "learning_rate": 0.0001999999889970499,
        "epoch": 0.6675609210820479,
        "step": 8958
    },
    {
        "loss": 2.4663,
        "grad_norm": 2.5458765029907227,
        "learning_rate": 0.00019999993123156844,
        "epoch": 0.6676354422833296,
        "step": 8959
    },
    {
        "loss": 2.4154,
        "grad_norm": 2.4387543201446533,
        "learning_rate": 0.00019999982395284662,
        "epoch": 0.6677099634846114,
        "step": 8960
    },
    {
        "loss": 2.3858,
        "grad_norm": 2.5337841510772705,
        "learning_rate": 0.0001999996671609376,
        "epoch": 0.6677844846858931,
        "step": 8961
    },
    {
        "loss": 3.0204,
        "grad_norm": 2.194573402404785,
        "learning_rate": 0.00019999946085591904,
        "epoch": 0.667859005887175,
        "step": 8962
    },
    {
        "loss": 2.658,
        "grad_norm": 2.9208950996398926,
        "learning_rate": 0.00019999920503789304,
        "epoch": 0.6679335270884567,
        "step": 8963
    },
    {
        "loss": 2.4326,
        "grad_norm": 3.099778652191162,
        "learning_rate": 0.0001999988997069863,
        "epoch": 0.6680080482897385,
        "step": 8964
    },
    {
        "loss": 2.4314,
        "grad_norm": 1.6091183423995972,
        "learning_rate": 0.00019999854486334995,
        "epoch": 0.6680825694910202,
        "step": 8965
    },
    {
        "loss": 2.0929,
        "grad_norm": 3.419381618499756,
        "learning_rate": 0.00019999814050715968,
        "epoch": 0.668157090692302,
        "step": 8966
    },
    {
        "loss": 2.241,
        "grad_norm": 4.393548965454102,
        "learning_rate": 0.00019999768663861577,
        "epoch": 0.6682316118935837,
        "step": 8967
    },
    {
        "loss": 2.6528,
        "grad_norm": 2.3859171867370605,
        "learning_rate": 0.0001999971832579429,
        "epoch": 0.6683061330948655,
        "step": 8968
    },
    {
        "loss": 2.7345,
        "grad_norm": 3.566164970397949,
        "learning_rate": 0.00019999663036539034,
        "epoch": 0.6683806542961472,
        "step": 8969
    },
    {
        "loss": 1.8641,
        "grad_norm": 2.027785539627075,
        "learning_rate": 0.0001999960279612318,
        "epoch": 0.668455175497429,
        "step": 8970
    },
    {
        "loss": 2.0712,
        "grad_norm": 4.33260440826416,
        "learning_rate": 0.00019999537604576556,
        "epoch": 0.6685296966987108,
        "step": 8971
    },
    {
        "loss": 1.668,
        "grad_norm": 1.5170316696166992,
        "learning_rate": 0.00019999467461931448,
        "epoch": 0.6686042178999926,
        "step": 8972
    },
    {
        "loss": 1.9931,
        "grad_norm": 3.714524984359741,
        "learning_rate": 0.00019999392368222574,
        "epoch": 0.6686787391012743,
        "step": 8973
    },
    {
        "loss": 2.5361,
        "grad_norm": 2.4892661571502686,
        "learning_rate": 0.0001999931232348712,
        "epoch": 0.6687532603025561,
        "step": 8974
    },
    {
        "loss": 2.5358,
        "grad_norm": 3.6431384086608887,
        "learning_rate": 0.00019999227327764722,
        "epoch": 0.6688277815038378,
        "step": 8975
    },
    {
        "loss": 2.5814,
        "grad_norm": 2.1793699264526367,
        "learning_rate": 0.00019999137381097464,
        "epoch": 0.6689023027051196,
        "step": 8976
    },
    {
        "loss": 1.6664,
        "grad_norm": 1.9972007274627686,
        "learning_rate": 0.00019999042483529876,
        "epoch": 0.6689768239064013,
        "step": 8977
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.980477809906006,
        "learning_rate": 0.00019998942635108952,
        "epoch": 0.6690513451076832,
        "step": 8978
    },
    {
        "loss": 1.5248,
        "grad_norm": 4.445982933044434,
        "learning_rate": 0.00019998837835884123,
        "epoch": 0.6691258663089649,
        "step": 8979
    },
    {
        "loss": 2.3989,
        "grad_norm": 2.1866345405578613,
        "learning_rate": 0.00019998728085907286,
        "epoch": 0.6692003875102467,
        "step": 8980
    },
    {
        "loss": 2.009,
        "grad_norm": 4.159119606018066,
        "learning_rate": 0.00019998613385232775,
        "epoch": 0.6692749087115284,
        "step": 8981
    },
    {
        "loss": 2.5522,
        "grad_norm": 2.5934300422668457,
        "learning_rate": 0.00019998493733917384,
        "epoch": 0.6693494299128102,
        "step": 8982
    },
    {
        "loss": 1.6446,
        "grad_norm": 3.0592379570007324,
        "learning_rate": 0.0001999836913202036,
        "epoch": 0.6694239511140919,
        "step": 8983
    },
    {
        "loss": 2.8272,
        "grad_norm": 2.714906692504883,
        "learning_rate": 0.00019998239579603394,
        "epoch": 0.6694984723153737,
        "step": 8984
    },
    {
        "loss": 2.8026,
        "grad_norm": 2.965579032897949,
        "learning_rate": 0.0001999810507673063,
        "epoch": 0.6695729935166554,
        "step": 8985
    },
    {
        "loss": 2.2238,
        "grad_norm": 2.63053560256958,
        "learning_rate": 0.00019997965623468674,
        "epoch": 0.6696475147179373,
        "step": 8986
    },
    {
        "loss": 2.6341,
        "grad_norm": 2.375603437423706,
        "learning_rate": 0.0001999782121988656,
        "epoch": 0.669722035919219,
        "step": 8987
    },
    {
        "loss": 2.195,
        "grad_norm": 2.928251028060913,
        "learning_rate": 0.00019997671866055797,
        "epoch": 0.6697965571205008,
        "step": 8988
    },
    {
        "loss": 2.0269,
        "grad_norm": 3.4773988723754883,
        "learning_rate": 0.00019997517562050332,
        "epoch": 0.6698710783217825,
        "step": 8989
    },
    {
        "loss": 2.1333,
        "grad_norm": 3.3235270977020264,
        "learning_rate": 0.00019997358307946564,
        "epoch": 0.6699455995230643,
        "step": 8990
    },
    {
        "loss": 3.4107,
        "grad_norm": 2.9258227348327637,
        "learning_rate": 0.0001999719410382335,
        "epoch": 0.670020120724346,
        "step": 8991
    },
    {
        "loss": 2.4093,
        "grad_norm": 3.527900218963623,
        "learning_rate": 0.00019997024949761985,
        "epoch": 0.6700946419256278,
        "step": 8992
    },
    {
        "loss": 2.0936,
        "grad_norm": 2.0894107818603516,
        "learning_rate": 0.00019996850845846234,
        "epoch": 0.6701691631269096,
        "step": 8993
    },
    {
        "loss": 2.2449,
        "grad_norm": 3.7489569187164307,
        "learning_rate": 0.00019996671792162288,
        "epoch": 0.6702436843281914,
        "step": 8994
    },
    {
        "loss": 2.0927,
        "grad_norm": 3.1532485485076904,
        "learning_rate": 0.00019996487788798815,
        "epoch": 0.6703182055294732,
        "step": 8995
    },
    {
        "loss": 2.1504,
        "grad_norm": 4.759928226470947,
        "learning_rate": 0.00019996298835846913,
        "epoch": 0.6703927267307549,
        "step": 8996
    },
    {
        "loss": 2.3569,
        "grad_norm": 2.3972206115722656,
        "learning_rate": 0.00019996104933400142,
        "epoch": 0.6704672479320367,
        "step": 8997
    },
    {
        "loss": 2.8823,
        "grad_norm": 2.1540334224700928,
        "learning_rate": 0.00019995906081554508,
        "epoch": 0.6705417691333184,
        "step": 8998
    },
    {
        "loss": 1.9112,
        "grad_norm": 3.5762667655944824,
        "learning_rate": 0.0001999570228040847,
        "epoch": 0.6706162903346002,
        "step": 8999
    },
    {
        "loss": 1.9409,
        "grad_norm": 3.044158458709717,
        "learning_rate": 0.00019995493530062936,
        "epoch": 0.670690811535882,
        "step": 9000
    },
    {
        "loss": 3.2339,
        "grad_norm": 3.1008923053741455,
        "learning_rate": 0.00019995279830621265,
        "epoch": 0.6707653327371638,
        "step": 9001
    },
    {
        "loss": 2.6808,
        "grad_norm": 4.681761741638184,
        "learning_rate": 0.00019995061182189266,
        "epoch": 0.6708398539384455,
        "step": 9002
    },
    {
        "loss": 1.9644,
        "grad_norm": 3.0928471088409424,
        "learning_rate": 0.00019994837584875203,
        "epoch": 0.6709143751397273,
        "step": 9003
    },
    {
        "loss": 2.7267,
        "grad_norm": 3.0827512741088867,
        "learning_rate": 0.00019994609038789782,
        "epoch": 0.670988896341009,
        "step": 9004
    },
    {
        "loss": 1.882,
        "grad_norm": 4.230639934539795,
        "learning_rate": 0.00019994375544046166,
        "epoch": 0.6710634175422908,
        "step": 9005
    },
    {
        "loss": 1.644,
        "grad_norm": 2.011192798614502,
        "learning_rate": 0.00019994137100759964,
        "epoch": 0.6711379387435725,
        "step": 9006
    },
    {
        "loss": 2.4345,
        "grad_norm": 1.9073270559310913,
        "learning_rate": 0.00019993893709049237,
        "epoch": 0.6712124599448543,
        "step": 9007
    },
    {
        "loss": 2.5926,
        "grad_norm": 3.663351535797119,
        "learning_rate": 0.00019993645369034498,
        "epoch": 0.671286981146136,
        "step": 9008
    },
    {
        "loss": 2.5177,
        "grad_norm": 2.11492657661438,
        "learning_rate": 0.0001999339208083871,
        "epoch": 0.6713615023474179,
        "step": 9009
    },
    {
        "loss": 2.0415,
        "grad_norm": 3.3594908714294434,
        "learning_rate": 0.0001999313384458728,
        "epoch": 0.6714360235486996,
        "step": 9010
    },
    {
        "loss": 2.4901,
        "grad_norm": 2.8177850246429443,
        "learning_rate": 0.0001999287066040807,
        "epoch": 0.6715105447499814,
        "step": 9011
    },
    {
        "loss": 2.1391,
        "grad_norm": 3.0498013496398926,
        "learning_rate": 0.00019992602528431393,
        "epoch": 0.6715850659512631,
        "step": 9012
    },
    {
        "loss": 2.3411,
        "grad_norm": 2.8682126998901367,
        "learning_rate": 0.0001999232944879001,
        "epoch": 0.6716595871525449,
        "step": 9013
    },
    {
        "loss": 2.692,
        "grad_norm": 3.0881612300872803,
        "learning_rate": 0.0001999205142161913,
        "epoch": 0.6717341083538266,
        "step": 9014
    },
    {
        "loss": 2.6201,
        "grad_norm": 2.5964303016662598,
        "learning_rate": 0.0001999176844705641,
        "epoch": 0.6718086295551084,
        "step": 9015
    },
    {
        "loss": 2.5033,
        "grad_norm": 2.4191482067108154,
        "learning_rate": 0.00019991480525241971,
        "epoch": 0.6718831507563902,
        "step": 9016
    },
    {
        "loss": 1.9651,
        "grad_norm": 3.283480405807495,
        "learning_rate": 0.0001999118765631836,
        "epoch": 0.671957671957672,
        "step": 9017
    },
    {
        "loss": 2.5683,
        "grad_norm": 2.5046169757843018,
        "learning_rate": 0.00019990889840430594,
        "epoch": 0.6720321931589537,
        "step": 9018
    },
    {
        "loss": 2.2626,
        "grad_norm": 3.051687717437744,
        "learning_rate": 0.00019990587077726128,
        "epoch": 0.6721067143602355,
        "step": 9019
    },
    {
        "loss": 2.9903,
        "grad_norm": 3.8995461463928223,
        "learning_rate": 0.00019990279368354875,
        "epoch": 0.6721812355615172,
        "step": 9020
    },
    {
        "loss": 2.8391,
        "grad_norm": 3.01771879196167,
        "learning_rate": 0.00019989966712469182,
        "epoch": 0.672255756762799,
        "step": 9021
    },
    {
        "loss": 2.7093,
        "grad_norm": 1.9255203008651733,
        "learning_rate": 0.00019989649110223865,
        "epoch": 0.6723302779640807,
        "step": 9022
    },
    {
        "loss": 2.2316,
        "grad_norm": 2.563164710998535,
        "learning_rate": 0.0001998932656177617,
        "epoch": 0.6724047991653626,
        "step": 9023
    },
    {
        "loss": 2.0955,
        "grad_norm": 2.223114252090454,
        "learning_rate": 0.0001998899906728581,
        "epoch": 0.6724793203666443,
        "step": 9024
    },
    {
        "loss": 2.1089,
        "grad_norm": 3.203653335571289,
        "learning_rate": 0.00019988666626914934,
        "epoch": 0.6725538415679261,
        "step": 9025
    },
    {
        "loss": 2.2219,
        "grad_norm": 3.7040884494781494,
        "learning_rate": 0.00019988329240828146,
        "epoch": 0.6726283627692078,
        "step": 9026
    },
    {
        "loss": 2.5899,
        "grad_norm": 1.7348188161849976,
        "learning_rate": 0.00019987986909192492,
        "epoch": 0.6727028839704896,
        "step": 9027
    },
    {
        "loss": 2.607,
        "grad_norm": 1.9408308267593384,
        "learning_rate": 0.0001998763963217748,
        "epoch": 0.6727774051717713,
        "step": 9028
    },
    {
        "loss": 2.2031,
        "grad_norm": 5.012028694152832,
        "learning_rate": 0.0001998728740995505,
        "epoch": 0.6728519263730531,
        "step": 9029
    },
    {
        "loss": 1.8736,
        "grad_norm": 3.157822608947754,
        "learning_rate": 0.00019986930242699605,
        "epoch": 0.672926447574335,
        "step": 9030
    },
    {
        "loss": 2.7984,
        "grad_norm": 5.7232279777526855,
        "learning_rate": 0.00019986568130587985,
        "epoch": 0.6730009687756167,
        "step": 9031
    },
    {
        "loss": 2.2218,
        "grad_norm": 3.753934144973755,
        "learning_rate": 0.0001998620107379949,
        "epoch": 0.6730754899768985,
        "step": 9032
    },
    {
        "loss": 2.5386,
        "grad_norm": 3.5381460189819336,
        "learning_rate": 0.00019985829072515854,
        "epoch": 0.6731500111781802,
        "step": 9033
    },
    {
        "loss": 1.8337,
        "grad_norm": 4.308666706085205,
        "learning_rate": 0.00019985452126921268,
        "epoch": 0.673224532379462,
        "step": 9034
    },
    {
        "loss": 2.5261,
        "grad_norm": 2.036133289337158,
        "learning_rate": 0.0001998507023720238,
        "epoch": 0.6732990535807437,
        "step": 9035
    },
    {
        "loss": 2.7287,
        "grad_norm": 2.6335813999176025,
        "learning_rate": 0.00019984683403548263,
        "epoch": 0.6733735747820255,
        "step": 9036
    },
    {
        "loss": 2.2422,
        "grad_norm": 2.1669423580169678,
        "learning_rate": 0.00019984291626150462,
        "epoch": 0.6734480959833072,
        "step": 9037
    },
    {
        "loss": 2.0688,
        "grad_norm": 2.883347272872925,
        "learning_rate": 0.00019983894905202952,
        "epoch": 0.6735226171845891,
        "step": 9038
    },
    {
        "loss": 2.5994,
        "grad_norm": 2.0927255153656006,
        "learning_rate": 0.00019983493240902163,
        "epoch": 0.6735971383858708,
        "step": 9039
    },
    {
        "loss": 2.7407,
        "grad_norm": 2.3113160133361816,
        "learning_rate": 0.00019983086633446973,
        "epoch": 0.6736716595871526,
        "step": 9040
    },
    {
        "loss": 1.7768,
        "grad_norm": 3.0735254287719727,
        "learning_rate": 0.00019982675083038708,
        "epoch": 0.6737461807884343,
        "step": 9041
    },
    {
        "loss": 2.1129,
        "grad_norm": 3.438847303390503,
        "learning_rate": 0.00019982258589881143,
        "epoch": 0.6738207019897161,
        "step": 9042
    },
    {
        "loss": 2.3983,
        "grad_norm": 1.6376742124557495,
        "learning_rate": 0.00019981837154180488,
        "epoch": 0.6738952231909978,
        "step": 9043
    },
    {
        "loss": 2.3284,
        "grad_norm": 2.352226972579956,
        "learning_rate": 0.0001998141077614542,
        "epoch": 0.6739697443922796,
        "step": 9044
    },
    {
        "loss": 2.612,
        "grad_norm": 3.173004627227783,
        "learning_rate": 0.00019980979455987046,
        "epoch": 0.6740442655935613,
        "step": 9045
    },
    {
        "loss": 2.7387,
        "grad_norm": 2.105544090270996,
        "learning_rate": 0.00019980543193918928,
        "epoch": 0.6741187867948432,
        "step": 9046
    },
    {
        "loss": 2.4661,
        "grad_norm": 2.5306801795959473,
        "learning_rate": 0.00019980101990157073,
        "epoch": 0.6741933079961249,
        "step": 9047
    },
    {
        "loss": 2.544,
        "grad_norm": 3.1840054988861084,
        "learning_rate": 0.00019979655844919942,
        "epoch": 0.6742678291974067,
        "step": 9048
    },
    {
        "loss": 1.9275,
        "grad_norm": 2.1843690872192383,
        "learning_rate": 0.0001997920475842843,
        "epoch": 0.6743423503986884,
        "step": 9049
    },
    {
        "loss": 2.2114,
        "grad_norm": 3.3498189449310303,
        "learning_rate": 0.00019978748730905882,
        "epoch": 0.6744168715999702,
        "step": 9050
    },
    {
        "loss": 2.8225,
        "grad_norm": 3.0134668350219727,
        "learning_rate": 0.000199782877625781,
        "epoch": 0.6744913928012519,
        "step": 9051
    },
    {
        "loss": 2.4353,
        "grad_norm": 2.629366397857666,
        "learning_rate": 0.00019977821853673318,
        "epoch": 0.6745659140025337,
        "step": 9052
    },
    {
        "loss": 2.3148,
        "grad_norm": 3.000474214553833,
        "learning_rate": 0.00019977351004422223,
        "epoch": 0.6746404352038154,
        "step": 9053
    },
    {
        "loss": 2.4431,
        "grad_norm": 2.8455076217651367,
        "learning_rate": 0.00019976875215057952,
        "epoch": 0.6747149564050973,
        "step": 9054
    },
    {
        "loss": 2.6449,
        "grad_norm": 2.8597309589385986,
        "learning_rate": 0.00019976394485816083,
        "epoch": 0.674789477606379,
        "step": 9055
    },
    {
        "loss": 1.804,
        "grad_norm": 2.8113296031951904,
        "learning_rate": 0.0001997590881693464,
        "epoch": 0.6748639988076608,
        "step": 9056
    },
    {
        "loss": 2.5537,
        "grad_norm": 2.0374228954315186,
        "learning_rate": 0.0001997541820865409,
        "epoch": 0.6749385200089425,
        "step": 9057
    },
    {
        "loss": 2.8054,
        "grad_norm": 3.429492712020874,
        "learning_rate": 0.00019974922661217354,
        "epoch": 0.6750130412102243,
        "step": 9058
    },
    {
        "loss": 2.245,
        "grad_norm": 2.292811393737793,
        "learning_rate": 0.00019974422174869793,
        "epoch": 0.675087562411506,
        "step": 9059
    },
    {
        "loss": 2.4051,
        "grad_norm": 2.217433452606201,
        "learning_rate": 0.00019973916749859214,
        "epoch": 0.6751620836127878,
        "step": 9060
    },
    {
        "loss": 2.523,
        "grad_norm": 3.9962503910064697,
        "learning_rate": 0.00019973406386435868,
        "epoch": 0.6752366048140696,
        "step": 9061
    },
    {
        "loss": 2.2609,
        "grad_norm": 4.001662254333496,
        "learning_rate": 0.00019972891084852452,
        "epoch": 0.6753111260153514,
        "step": 9062
    },
    {
        "loss": 1.7294,
        "grad_norm": 2.93219256401062,
        "learning_rate": 0.00019972370845364111,
        "epoch": 0.6753856472166331,
        "step": 9063
    },
    {
        "loss": 2.5153,
        "grad_norm": 2.7679519653320312,
        "learning_rate": 0.00019971845668228432,
        "epoch": 0.6754601684179149,
        "step": 9064
    },
    {
        "loss": 2.0098,
        "grad_norm": 1.904320478439331,
        "learning_rate": 0.00019971315553705447,
        "epoch": 0.6755346896191967,
        "step": 9065
    },
    {
        "loss": 2.4257,
        "grad_norm": 2.751237154006958,
        "learning_rate": 0.00019970780502057633,
        "epoch": 0.6756092108204784,
        "step": 9066
    },
    {
        "loss": 2.2041,
        "grad_norm": 4.387176513671875,
        "learning_rate": 0.0001997024051354991,
        "epoch": 0.6756837320217602,
        "step": 9067
    },
    {
        "loss": 2.7295,
        "grad_norm": 3.999030828475952,
        "learning_rate": 0.00019969695588449652,
        "epoch": 0.675758253223042,
        "step": 9068
    },
    {
        "loss": 2.3969,
        "grad_norm": 1.1515223979949951,
        "learning_rate": 0.0001996914572702666,
        "epoch": 0.6758327744243238,
        "step": 9069
    },
    {
        "loss": 2.3605,
        "grad_norm": 1.7405606508255005,
        "learning_rate": 0.0001996859092955319,
        "epoch": 0.6759072956256055,
        "step": 9070
    },
    {
        "loss": 2.1774,
        "grad_norm": 3.8352479934692383,
        "learning_rate": 0.00019968031196303945,
        "epoch": 0.6759818168268873,
        "step": 9071
    },
    {
        "loss": 2.4431,
        "grad_norm": 3.152015447616577,
        "learning_rate": 0.0001996746652755606,
        "epoch": 0.676056338028169,
        "step": 9072
    },
    {
        "loss": 2.455,
        "grad_norm": 2.823089361190796,
        "learning_rate": 0.00019966896923589127,
        "epoch": 0.6761308592294508,
        "step": 9073
    },
    {
        "loss": 2.1833,
        "grad_norm": 2.9509470462799072,
        "learning_rate": 0.00019966322384685172,
        "epoch": 0.6762053804307325,
        "step": 9074
    },
    {
        "loss": 2.7524,
        "grad_norm": 1.132536768913269,
        "learning_rate": 0.0001996574291112867,
        "epoch": 0.6762799016320143,
        "step": 9075
    },
    {
        "loss": 1.834,
        "grad_norm": 3.2707107067108154,
        "learning_rate": 0.0001996515850320654,
        "epoch": 0.676354422833296,
        "step": 9076
    },
    {
        "loss": 2.7108,
        "grad_norm": 3.3393445014953613,
        "learning_rate": 0.00019964569161208138,
        "epoch": 0.6764289440345779,
        "step": 9077
    },
    {
        "loss": 2.0706,
        "grad_norm": 3.634901523590088,
        "learning_rate": 0.00019963974885425266,
        "epoch": 0.6765034652358596,
        "step": 9078
    },
    {
        "loss": 2.0195,
        "grad_norm": 3.306434154510498,
        "learning_rate": 0.00019963375676152172,
        "epoch": 0.6765779864371414,
        "step": 9079
    },
    {
        "loss": 2.7879,
        "grad_norm": 2.527744770050049,
        "learning_rate": 0.00019962771533685542,
        "epoch": 0.6766525076384231,
        "step": 9080
    },
    {
        "loss": 1.887,
        "grad_norm": 5.1491827964782715,
        "learning_rate": 0.00019962162458324505,
        "epoch": 0.6767270288397049,
        "step": 9081
    },
    {
        "loss": 2.4458,
        "grad_norm": 2.346766710281372,
        "learning_rate": 0.00019961548450370638,
        "epoch": 0.6768015500409866,
        "step": 9082
    },
    {
        "loss": 2.428,
        "grad_norm": 3.3615164756774902,
        "learning_rate": 0.00019960929510127952,
        "epoch": 0.6768760712422685,
        "step": 9083
    },
    {
        "loss": 1.4675,
        "grad_norm": 5.303150653839111,
        "learning_rate": 0.00019960305637902912,
        "epoch": 0.6769505924435502,
        "step": 9084
    },
    {
        "loss": 1.0002,
        "grad_norm": 4.527484893798828,
        "learning_rate": 0.0001995967683400441,
        "epoch": 0.677025113644832,
        "step": 9085
    },
    {
        "loss": 2.0607,
        "grad_norm": 3.665597677230835,
        "learning_rate": 0.00019959043098743794,
        "epoch": 0.6770996348461137,
        "step": 9086
    },
    {
        "loss": 2.117,
        "grad_norm": 3.5560271739959717,
        "learning_rate": 0.0001995840443243484,
        "epoch": 0.6771741560473955,
        "step": 9087
    },
    {
        "loss": 1.8995,
        "grad_norm": 5.070786476135254,
        "learning_rate": 0.0001995776083539378,
        "epoch": 0.6772486772486772,
        "step": 9088
    },
    {
        "loss": 2.6096,
        "grad_norm": 3.244938850402832,
        "learning_rate": 0.0001995711230793927,
        "epoch": 0.677323198449959,
        "step": 9089
    },
    {
        "loss": 2.0139,
        "grad_norm": 2.8211610317230225,
        "learning_rate": 0.0001995645885039243,
        "epoch": 0.6773977196512407,
        "step": 9090
    },
    {
        "loss": 2.0991,
        "grad_norm": 4.318629264831543,
        "learning_rate": 0.00019955800463076798,
        "epoch": 0.6774722408525226,
        "step": 9091
    },
    {
        "loss": 2.1673,
        "grad_norm": 3.2726263999938965,
        "learning_rate": 0.00019955137146318366,
        "epoch": 0.6775467620538043,
        "step": 9092
    },
    {
        "loss": 2.3048,
        "grad_norm": 3.740044355392456,
        "learning_rate": 0.00019954468900445566,
        "epoch": 0.6776212832550861,
        "step": 9093
    },
    {
        "loss": 2.3661,
        "grad_norm": 2.752290725708008,
        "learning_rate": 0.00019953795725789265,
        "epoch": 0.6776958044563678,
        "step": 9094
    },
    {
        "loss": 1.699,
        "grad_norm": 1.4985483884811401,
        "learning_rate": 0.00019953117622682778,
        "epoch": 0.6777703256576496,
        "step": 9095
    },
    {
        "loss": 1.57,
        "grad_norm": 4.518766403198242,
        "learning_rate": 0.00019952434591461853,
        "epoch": 0.6778448468589313,
        "step": 9096
    },
    {
        "loss": 2.6248,
        "grad_norm": 2.5023539066314697,
        "learning_rate": 0.0001995174663246468,
        "epoch": 0.6779193680602131,
        "step": 9097
    },
    {
        "loss": 2.6213,
        "grad_norm": 3.0859177112579346,
        "learning_rate": 0.00019951053746031895,
        "epoch": 0.6779938892614948,
        "step": 9098
    },
    {
        "loss": 2.2819,
        "grad_norm": 2.5965256690979004,
        "learning_rate": 0.00019950355932506566,
        "epoch": 0.6780684104627767,
        "step": 9099
    },
    {
        "loss": 2.954,
        "grad_norm": 2.33701491355896,
        "learning_rate": 0.000199496531922342,
        "epoch": 0.6781429316640585,
        "step": 9100
    },
    {
        "loss": 2.71,
        "grad_norm": 1.9787763357162476,
        "learning_rate": 0.0001994894552556275,
        "epoch": 0.6782174528653402,
        "step": 9101
    },
    {
        "loss": 2.3772,
        "grad_norm": 3.095472574234009,
        "learning_rate": 0.0001994823293284261,
        "epoch": 0.678291974066622,
        "step": 9102
    },
    {
        "loss": 2.3448,
        "grad_norm": 2.535597324371338,
        "learning_rate": 0.00019947515414426597,
        "epoch": 0.6783664952679037,
        "step": 9103
    },
    {
        "loss": 2.2146,
        "grad_norm": 3.7208917140960693,
        "learning_rate": 0.00019946792970669983,
        "epoch": 0.6784410164691855,
        "step": 9104
    },
    {
        "loss": 2.7774,
        "grad_norm": 2.7450978755950928,
        "learning_rate": 0.00019946065601930476,
        "epoch": 0.6785155376704672,
        "step": 9105
    },
    {
        "loss": 1.9188,
        "grad_norm": 3.72135853767395,
        "learning_rate": 0.00019945333308568218,
        "epoch": 0.6785900588717491,
        "step": 9106
    },
    {
        "loss": 1.9986,
        "grad_norm": 3.9316020011901855,
        "learning_rate": 0.0001994459609094579,
        "epoch": 0.6786645800730308,
        "step": 9107
    },
    {
        "loss": 2.5029,
        "grad_norm": 3.768000364303589,
        "learning_rate": 0.0001994385394942822,
        "epoch": 0.6787391012743126,
        "step": 9108
    },
    {
        "loss": 2.5927,
        "grad_norm": 2.8540146350860596,
        "learning_rate": 0.00019943106884382953,
        "epoch": 0.6788136224755943,
        "step": 9109
    },
    {
        "loss": 2.0821,
        "grad_norm": 3.3348944187164307,
        "learning_rate": 0.00019942354896179898,
        "epoch": 0.6788881436768761,
        "step": 9110
    },
    {
        "loss": 1.6501,
        "grad_norm": 3.511314868927002,
        "learning_rate": 0.00019941597985191381,
        "epoch": 0.6789626648781578,
        "step": 9111
    },
    {
        "loss": 2.6795,
        "grad_norm": 2.51602840423584,
        "learning_rate": 0.00019940836151792176,
        "epoch": 0.6790371860794396,
        "step": 9112
    },
    {
        "loss": 2.558,
        "grad_norm": 1.7871358394622803,
        "learning_rate": 0.00019940069396359497,
        "epoch": 0.6791117072807213,
        "step": 9113
    },
    {
        "loss": 1.9489,
        "grad_norm": 3.5563597679138184,
        "learning_rate": 0.00019939297719272981,
        "epoch": 0.6791862284820032,
        "step": 9114
    },
    {
        "loss": 1.5456,
        "grad_norm": 3.089355230331421,
        "learning_rate": 0.00019938521120914718,
        "epoch": 0.6792607496832849,
        "step": 9115
    },
    {
        "loss": 1.7575,
        "grad_norm": 3.826604127883911,
        "learning_rate": 0.00019937739601669222,
        "epoch": 0.6793352708845667,
        "step": 9116
    },
    {
        "loss": 2.3348,
        "grad_norm": 3.5890884399414062,
        "learning_rate": 0.0001993695316192345,
        "epoch": 0.6794097920858484,
        "step": 9117
    },
    {
        "loss": 2.2211,
        "grad_norm": 2.613187313079834,
        "learning_rate": 0.00019936161802066793,
        "epoch": 0.6794843132871302,
        "step": 9118
    },
    {
        "loss": 2.608,
        "grad_norm": 2.461937427520752,
        "learning_rate": 0.00019935365522491083,
        "epoch": 0.6795588344884119,
        "step": 9119
    },
    {
        "loss": 1.8493,
        "grad_norm": 4.574549674987793,
        "learning_rate": 0.00019934564323590578,
        "epoch": 0.6796333556896937,
        "step": 9120
    },
    {
        "loss": 2.1629,
        "grad_norm": 3.9683785438537598,
        "learning_rate": 0.00019933758205761985,
        "epoch": 0.6797078768909754,
        "step": 9121
    },
    {
        "loss": 2.4602,
        "grad_norm": 2.9524717330932617,
        "learning_rate": 0.00019932947169404438,
        "epoch": 0.6797823980922573,
        "step": 9122
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.754779577255249,
        "learning_rate": 0.00019932131214919503,
        "epoch": 0.679856919293539,
        "step": 9123
    },
    {
        "loss": 1.8949,
        "grad_norm": 2.0617661476135254,
        "learning_rate": 0.00019931310342711188,
        "epoch": 0.6799314404948208,
        "step": 9124
    },
    {
        "loss": 1.8998,
        "grad_norm": 3.7644152641296387,
        "learning_rate": 0.00019930484553185932,
        "epoch": 0.6800059616961025,
        "step": 9125
    },
    {
        "loss": 3.0765,
        "grad_norm": 2.3001532554626465,
        "learning_rate": 0.0001992965384675262,
        "epoch": 0.6800804828973843,
        "step": 9126
    },
    {
        "loss": 2.1098,
        "grad_norm": 3.0151021480560303,
        "learning_rate": 0.00019928818223822548,
        "epoch": 0.680155004098666,
        "step": 9127
    },
    {
        "loss": 3.1794,
        "grad_norm": 3.919342279434204,
        "learning_rate": 0.00019927977684809468,
        "epoch": 0.6802295252999478,
        "step": 9128
    },
    {
        "loss": 2.2792,
        "grad_norm": 4.476105690002441,
        "learning_rate": 0.00019927132230129557,
        "epoch": 0.6803040465012296,
        "step": 9129
    },
    {
        "loss": 2.0175,
        "grad_norm": 4.1138081550598145,
        "learning_rate": 0.0001992628186020143,
        "epoch": 0.6803785677025114,
        "step": 9130
    },
    {
        "loss": 1.8663,
        "grad_norm": 4.289584636688232,
        "learning_rate": 0.0001992542657544613,
        "epoch": 0.6804530889037931,
        "step": 9131
    },
    {
        "loss": 3.2311,
        "grad_norm": 2.2778537273406982,
        "learning_rate": 0.00019924566376287132,
        "epoch": 0.6805276101050749,
        "step": 9132
    },
    {
        "loss": 1.6547,
        "grad_norm": 5.872551918029785,
        "learning_rate": 0.0001992370126315036,
        "epoch": 0.6806021313063566,
        "step": 9133
    },
    {
        "loss": 2.1181,
        "grad_norm": 1.4706302881240845,
        "learning_rate": 0.0001992283123646415,
        "epoch": 0.6806766525076384,
        "step": 9134
    },
    {
        "loss": 2.5846,
        "grad_norm": 2.889411449432373,
        "learning_rate": 0.00019921956296659282,
        "epoch": 0.6807511737089202,
        "step": 9135
    },
    {
        "loss": 1.721,
        "grad_norm": 2.300229787826538,
        "learning_rate": 0.00019921076444168976,
        "epoch": 0.680825694910202,
        "step": 9136
    },
    {
        "loss": 2.4904,
        "grad_norm": 3.465916395187378,
        "learning_rate": 0.00019920191679428866,
        "epoch": 0.6809002161114838,
        "step": 9137
    },
    {
        "loss": 2.7087,
        "grad_norm": 2.4558043479919434,
        "learning_rate": 0.0001991930200287703,
        "epoch": 0.6809747373127655,
        "step": 9138
    },
    {
        "loss": 2.1296,
        "grad_norm": 3.315706729888916,
        "learning_rate": 0.00019918407414953982,
        "epoch": 0.6810492585140473,
        "step": 9139
    },
    {
        "loss": 1.9772,
        "grad_norm": 3.0156872272491455,
        "learning_rate": 0.0001991750791610265,
        "epoch": 0.681123779715329,
        "step": 9140
    },
    {
        "loss": 2.2906,
        "grad_norm": 2.4781622886657715,
        "learning_rate": 0.00019916603506768419,
        "epoch": 0.6811983009166108,
        "step": 9141
    },
    {
        "loss": 2.337,
        "grad_norm": 2.342780828475952,
        "learning_rate": 0.00019915694187399083,
        "epoch": 0.6812728221178925,
        "step": 9142
    },
    {
        "loss": 2.1371,
        "grad_norm": 4.201558589935303,
        "learning_rate": 0.00019914779958444875,
        "epoch": 0.6813473433191743,
        "step": 9143
    },
    {
        "loss": 2.4714,
        "grad_norm": 2.9561033248901367,
        "learning_rate": 0.00019913860820358468,
        "epoch": 0.681421864520456,
        "step": 9144
    },
    {
        "loss": 2.1481,
        "grad_norm": 3.0775821208953857,
        "learning_rate": 0.00019912936773594945,
        "epoch": 0.6814963857217379,
        "step": 9145
    },
    {
        "loss": 2.3386,
        "grad_norm": 2.7958130836486816,
        "learning_rate": 0.00019912007818611847,
        "epoch": 0.6815709069230196,
        "step": 9146
    },
    {
        "loss": 2.3908,
        "grad_norm": 3.935040235519409,
        "learning_rate": 0.00019911073955869118,
        "epoch": 0.6816454281243014,
        "step": 9147
    },
    {
        "loss": 2.236,
        "grad_norm": 3.229854106903076,
        "learning_rate": 0.00019910135185829148,
        "epoch": 0.6817199493255831,
        "step": 9148
    },
    {
        "loss": 1.902,
        "grad_norm": 3.654449224472046,
        "learning_rate": 0.0001990919150895675,
        "epoch": 0.6817944705268649,
        "step": 9149
    },
    {
        "loss": 1.9908,
        "grad_norm": 3.6007304191589355,
        "learning_rate": 0.00019908242925719175,
        "epoch": 0.6818689917281466,
        "step": 9150
    },
    {
        "loss": 2.4407,
        "grad_norm": 6.888108253479004,
        "learning_rate": 0.00019907289436586097,
        "epoch": 0.6819435129294285,
        "step": 9151
    },
    {
        "loss": 2.1194,
        "grad_norm": 2.5990874767303467,
        "learning_rate": 0.00019906331042029613,
        "epoch": 0.6820180341307102,
        "step": 9152
    },
    {
        "loss": 1.7488,
        "grad_norm": 2.900601387023926,
        "learning_rate": 0.0001990536774252426,
        "epoch": 0.682092555331992,
        "step": 9153
    },
    {
        "loss": 2.6001,
        "grad_norm": 2.2812681198120117,
        "learning_rate": 0.00019904399538547,
        "epoch": 0.6821670765332737,
        "step": 9154
    },
    {
        "loss": 2.4182,
        "grad_norm": 2.4889183044433594,
        "learning_rate": 0.00019903426430577223,
        "epoch": 0.6822415977345555,
        "step": 9155
    },
    {
        "loss": 2.5316,
        "grad_norm": 3.3585638999938965,
        "learning_rate": 0.00019902448419096741,
        "epoch": 0.6823161189358372,
        "step": 9156
    },
    {
        "loss": 2.1556,
        "grad_norm": 7.304368495941162,
        "learning_rate": 0.0001990146550458981,
        "epoch": 0.682390640137119,
        "step": 9157
    },
    {
        "loss": 2.6052,
        "grad_norm": 2.093017339706421,
        "learning_rate": 0.0001990047768754309,
        "epoch": 0.6824651613384007,
        "step": 9158
    },
    {
        "loss": 2.5875,
        "grad_norm": 2.9597089290618896,
        "learning_rate": 0.0001989948496844569,
        "epoch": 0.6825396825396826,
        "step": 9159
    },
    {
        "loss": 2.4764,
        "grad_norm": 3.0776596069335938,
        "learning_rate": 0.0001989848734778914,
        "epoch": 0.6826142037409643,
        "step": 9160
    },
    {
        "loss": 3.2006,
        "grad_norm": 4.19429874420166,
        "learning_rate": 0.00019897484826067387,
        "epoch": 0.6826887249422461,
        "step": 9161
    },
    {
        "loss": 2.5343,
        "grad_norm": 3.5019474029541016,
        "learning_rate": 0.00019896477403776817,
        "epoch": 0.6827632461435278,
        "step": 9162
    },
    {
        "loss": 2.3747,
        "grad_norm": 3.606790781021118,
        "learning_rate": 0.00019895465081416235,
        "epoch": 0.6828377673448096,
        "step": 9163
    },
    {
        "loss": 2.3684,
        "grad_norm": 3.3301920890808105,
        "learning_rate": 0.0001989444785948688,
        "epoch": 0.6829122885460913,
        "step": 9164
    },
    {
        "loss": 1.9506,
        "grad_norm": 4.346118450164795,
        "learning_rate": 0.0001989342573849241,
        "epoch": 0.6829868097473731,
        "step": 9165
    },
    {
        "loss": 2.1849,
        "grad_norm": 5.675090312957764,
        "learning_rate": 0.000198923987189389,
        "epoch": 0.6830613309486548,
        "step": 9166
    },
    {
        "loss": 1.9529,
        "grad_norm": 3.020918369293213,
        "learning_rate": 0.0001989136680133488,
        "epoch": 0.6831358521499367,
        "step": 9167
    },
    {
        "loss": 2.0236,
        "grad_norm": 4.870595455169678,
        "learning_rate": 0.00019890329986191273,
        "epoch": 0.6832103733512184,
        "step": 9168
    },
    {
        "loss": 2.1355,
        "grad_norm": 3.250274658203125,
        "learning_rate": 0.0001988928827402144,
        "epoch": 0.6832848945525002,
        "step": 9169
    },
    {
        "loss": 2.0654,
        "grad_norm": 4.263281345367432,
        "learning_rate": 0.00019888241665341175,
        "epoch": 0.6833594157537819,
        "step": 9170
    },
    {
        "loss": 2.4778,
        "grad_norm": 2.6759798526763916,
        "learning_rate": 0.0001988719016066868,
        "epoch": 0.6834339369550637,
        "step": 9171
    },
    {
        "loss": 2.2058,
        "grad_norm": 2.301079034805298,
        "learning_rate": 0.00019886133760524597,
        "epoch": 0.6835084581563455,
        "step": 9172
    },
    {
        "loss": 2.309,
        "grad_norm": 3.6206393241882324,
        "learning_rate": 0.00019885072465431974,
        "epoch": 0.6835829793576272,
        "step": 9173
    },
    {
        "loss": 2.272,
        "grad_norm": 4.1712799072265625,
        "learning_rate": 0.00019884006275916306,
        "epoch": 0.6836575005589091,
        "step": 9174
    },
    {
        "loss": 2.5259,
        "grad_norm": 3.418057680130005,
        "learning_rate": 0.00019882935192505488,
        "epoch": 0.6837320217601908,
        "step": 9175
    },
    {
        "loss": 1.8626,
        "grad_norm": 3.0651941299438477,
        "learning_rate": 0.00019881859215729853,
        "epoch": 0.6838065429614726,
        "step": 9176
    },
    {
        "loss": 2.6485,
        "grad_norm": 1.9077531099319458,
        "learning_rate": 0.0001988077834612215,
        "epoch": 0.6838810641627543,
        "step": 9177
    },
    {
        "loss": 2.5081,
        "grad_norm": 3.9505646228790283,
        "learning_rate": 0.00019879692584217552,
        "epoch": 0.6839555853640361,
        "step": 9178
    },
    {
        "loss": 2.6432,
        "grad_norm": 2.181924343109131,
        "learning_rate": 0.00019878601930553664,
        "epoch": 0.6840301065653178,
        "step": 9179
    },
    {
        "loss": 2.3692,
        "grad_norm": 3.230184555053711,
        "learning_rate": 0.0001987750638567049,
        "epoch": 0.6841046277665996,
        "step": 9180
    },
    {
        "loss": 2.3203,
        "grad_norm": 2.4965250492095947,
        "learning_rate": 0.00019876405950110487,
        "epoch": 0.6841791489678813,
        "step": 9181
    },
    {
        "loss": 2.3165,
        "grad_norm": 3.1761507987976074,
        "learning_rate": 0.00019875300624418505,
        "epoch": 0.6842536701691632,
        "step": 9182
    },
    {
        "loss": 1.601,
        "grad_norm": 3.812697649002075,
        "learning_rate": 0.0001987419040914183,
        "epoch": 0.6843281913704449,
        "step": 9183
    },
    {
        "loss": 2.7627,
        "grad_norm": 2.388115406036377,
        "learning_rate": 0.00019873075304830158,
        "epoch": 0.6844027125717267,
        "step": 9184
    },
    {
        "loss": 2.6777,
        "grad_norm": 3.4949564933776855,
        "learning_rate": 0.00019871955312035627,
        "epoch": 0.6844772337730084,
        "step": 9185
    },
    {
        "loss": 2.4384,
        "grad_norm": 3.2063353061676025,
        "learning_rate": 0.00019870830431312775,
        "epoch": 0.6845517549742902,
        "step": 9186
    },
    {
        "loss": 1.9729,
        "grad_norm": 3.518458366394043,
        "learning_rate": 0.0001986970066321857,
        "epoch": 0.6846262761755719,
        "step": 9187
    },
    {
        "loss": 2.1018,
        "grad_norm": 3.604518413543701,
        "learning_rate": 0.00019868566008312397,
        "epoch": 0.6847007973768537,
        "step": 9188
    },
    {
        "loss": 1.893,
        "grad_norm": 3.776472330093384,
        "learning_rate": 0.00019867426467156055,
        "epoch": 0.6847753185781355,
        "step": 9189
    },
    {
        "loss": 1.273,
        "grad_norm": 4.1426849365234375,
        "learning_rate": 0.0001986628204031378,
        "epoch": 0.6848498397794173,
        "step": 9190
    },
    {
        "loss": 2.5036,
        "grad_norm": 2.3994033336639404,
        "learning_rate": 0.000198651327283522,
        "epoch": 0.684924360980699,
        "step": 9191
    },
    {
        "loss": 2.2139,
        "grad_norm": 2.3437721729278564,
        "learning_rate": 0.00019863978531840391,
        "epoch": 0.6849988821819808,
        "step": 9192
    },
    {
        "loss": 2.6381,
        "grad_norm": 2.471318483352661,
        "learning_rate": 0.00019862819451349823,
        "epoch": 0.6850734033832625,
        "step": 9193
    },
    {
        "loss": 2.8617,
        "grad_norm": 2.1581788063049316,
        "learning_rate": 0.000198616554874544,
        "epoch": 0.6851479245845443,
        "step": 9194
    },
    {
        "loss": 2.2253,
        "grad_norm": 2.8117167949676514,
        "learning_rate": 0.0001986048664073044,
        "epoch": 0.685222445785826,
        "step": 9195
    },
    {
        "loss": 2.6035,
        "grad_norm": 2.1156156063079834,
        "learning_rate": 0.0001985931291175667,
        "epoch": 0.6852969669871078,
        "step": 9196
    },
    {
        "loss": 2.1261,
        "grad_norm": 3.3768041133880615,
        "learning_rate": 0.0001985813430111425,
        "epoch": 0.6853714881883896,
        "step": 9197
    },
    {
        "loss": 1.6487,
        "grad_norm": 3.5220370292663574,
        "learning_rate": 0.00019856950809386746,
        "epoch": 0.6854460093896714,
        "step": 9198
    },
    {
        "loss": 2.6091,
        "grad_norm": 2.864586353302002,
        "learning_rate": 0.00019855762437160146,
        "epoch": 0.6855205305909531,
        "step": 9199
    },
    {
        "loss": 2.1176,
        "grad_norm": 3.7161173820495605,
        "learning_rate": 0.00019854569185022842,
        "epoch": 0.6855950517922349,
        "step": 9200
    },
    {
        "loss": 2.0739,
        "grad_norm": 3.311940908432007,
        "learning_rate": 0.00019853371053565666,
        "epoch": 0.6856695729935166,
        "step": 9201
    },
    {
        "loss": 2.0949,
        "grad_norm": 2.5240836143493652,
        "learning_rate": 0.0001985216804338184,
        "epoch": 0.6857440941947984,
        "step": 9202
    },
    {
        "loss": 3.074,
        "grad_norm": 3.2651898860931396,
        "learning_rate": 0.00019850960155067023,
        "epoch": 0.6858186153960801,
        "step": 9203
    },
    {
        "loss": 2.1708,
        "grad_norm": 3.1457812786102295,
        "learning_rate": 0.00019849747389219272,
        "epoch": 0.685893136597362,
        "step": 9204
    },
    {
        "loss": 1.7392,
        "grad_norm": 4.457386493682861,
        "learning_rate": 0.00019848529746439072,
        "epoch": 0.6859676577986437,
        "step": 9205
    },
    {
        "loss": 2.2156,
        "grad_norm": 2.2691118717193604,
        "learning_rate": 0.00019847307227329318,
        "epoch": 0.6860421789999255,
        "step": 9206
    },
    {
        "loss": 2.5987,
        "grad_norm": 2.2790415287017822,
        "learning_rate": 0.00019846079832495318,
        "epoch": 0.6861167002012073,
        "step": 9207
    },
    {
        "loss": 2.917,
        "grad_norm": 1.117215633392334,
        "learning_rate": 0.00019844847562544793,
        "epoch": 0.686191221402489,
        "step": 9208
    },
    {
        "loss": 1.952,
        "grad_norm": 3.186920642852783,
        "learning_rate": 0.00019843610418087884,
        "epoch": 0.6862657426037708,
        "step": 9209
    },
    {
        "loss": 2.7439,
        "grad_norm": 3.259999990463257,
        "learning_rate": 0.00019842368399737145,
        "epoch": 0.6863402638050525,
        "step": 9210
    },
    {
        "loss": 1.9833,
        "grad_norm": 4.391883373260498,
        "learning_rate": 0.0001984112150810753,
        "epoch": 0.6864147850063343,
        "step": 9211
    },
    {
        "loss": 2.0482,
        "grad_norm": 3.6407268047332764,
        "learning_rate": 0.00019839869743816423,
        "epoch": 0.6864893062076161,
        "step": 9212
    },
    {
        "loss": 2.8078,
        "grad_norm": 2.3670573234558105,
        "learning_rate": 0.00019838613107483612,
        "epoch": 0.6865638274088979,
        "step": 9213
    },
    {
        "loss": 2.4945,
        "grad_norm": 1.9455406665802002,
        "learning_rate": 0.00019837351599731297,
        "epoch": 0.6866383486101796,
        "step": 9214
    },
    {
        "loss": 1.972,
        "grad_norm": 1.7768759727478027,
        "learning_rate": 0.00019836085221184096,
        "epoch": 0.6867128698114614,
        "step": 9215
    },
    {
        "loss": 1.9585,
        "grad_norm": 3.5550358295440674,
        "learning_rate": 0.00019834813972469028,
        "epoch": 0.6867873910127431,
        "step": 9216
    },
    {
        "loss": 1.5468,
        "grad_norm": 3.8645124435424805,
        "learning_rate": 0.00019833537854215536,
        "epoch": 0.6868619122140249,
        "step": 9217
    },
    {
        "loss": 2.3796,
        "grad_norm": 2.1467196941375732,
        "learning_rate": 0.00019832256867055467,
        "epoch": 0.6869364334153066,
        "step": 9218
    },
    {
        "loss": 2.5603,
        "grad_norm": 2.5431275367736816,
        "learning_rate": 0.0001983097101162308,
        "epoch": 0.6870109546165885,
        "step": 9219
    },
    {
        "loss": 2.4551,
        "grad_norm": 2.7796213626861572,
        "learning_rate": 0.00019829680288555035,
        "epoch": 0.6870854758178702,
        "step": 9220
    },
    {
        "loss": 2.711,
        "grad_norm": 2.3062803745269775,
        "learning_rate": 0.00019828384698490425,
        "epoch": 0.687159997019152,
        "step": 9221
    },
    {
        "loss": 3.2674,
        "grad_norm": 4.35165548324585,
        "learning_rate": 0.0001982708424207073,
        "epoch": 0.6872345182204337,
        "step": 9222
    },
    {
        "loss": 2.0841,
        "grad_norm": 2.748009443283081,
        "learning_rate": 0.00019825778919939854,
        "epoch": 0.6873090394217155,
        "step": 9223
    },
    {
        "loss": 2.8576,
        "grad_norm": 2.6099345684051514,
        "learning_rate": 0.00019824468732744098,
        "epoch": 0.6873835606229972,
        "step": 9224
    },
    {
        "loss": 2.434,
        "grad_norm": 3.2406911849975586,
        "learning_rate": 0.00019823153681132184,
        "epoch": 0.687458081824279,
        "step": 9225
    },
    {
        "loss": 2.3415,
        "grad_norm": 3.0381150245666504,
        "learning_rate": 0.00019821833765755237,
        "epoch": 0.6875326030255607,
        "step": 9226
    },
    {
        "loss": 1.2113,
        "grad_norm": 4.419267177581787,
        "learning_rate": 0.00019820508987266786,
        "epoch": 0.6876071242268426,
        "step": 9227
    },
    {
        "loss": 2.7761,
        "grad_norm": 2.1735177040100098,
        "learning_rate": 0.0001981917934632278,
        "epoch": 0.6876816454281243,
        "step": 9228
    },
    {
        "loss": 2.2814,
        "grad_norm": 2.559812307357788,
        "learning_rate": 0.00019817844843581558,
        "epoch": 0.6877561666294061,
        "step": 9229
    },
    {
        "loss": 2.2257,
        "grad_norm": 2.4189820289611816,
        "learning_rate": 0.00019816505479703888,
        "epoch": 0.6878306878306878,
        "step": 9230
    },
    {
        "loss": 1.6625,
        "grad_norm": 4.090475559234619,
        "learning_rate": 0.0001981516125535292,
        "epoch": 0.6879052090319696,
        "step": 9231
    },
    {
        "loss": 2.4034,
        "grad_norm": 4.75192928314209,
        "learning_rate": 0.00019813812171194233,
        "epoch": 0.6879797302332513,
        "step": 9232
    },
    {
        "loss": 2.5776,
        "grad_norm": 2.126842737197876,
        "learning_rate": 0.00019812458227895795,
        "epoch": 0.6880542514345331,
        "step": 9233
    },
    {
        "loss": 1.8181,
        "grad_norm": 3.0894224643707275,
        "learning_rate": 0.00019811099426127996,
        "epoch": 0.6881287726358148,
        "step": 9234
    },
    {
        "loss": 2.1097,
        "grad_norm": 3.56276798248291,
        "learning_rate": 0.00019809735766563615,
        "epoch": 0.6882032938370967,
        "step": 9235
    },
    {
        "loss": 2.7915,
        "grad_norm": 3.0014138221740723,
        "learning_rate": 0.00019808367249877853,
        "epoch": 0.6882778150383784,
        "step": 9236
    },
    {
        "loss": 2.9714,
        "grad_norm": 3.4949073791503906,
        "learning_rate": 0.00019806993876748298,
        "epoch": 0.6883523362396602,
        "step": 9237
    },
    {
        "loss": 2.5117,
        "grad_norm": 2.671440839767456,
        "learning_rate": 0.0001980561564785496,
        "epoch": 0.6884268574409419,
        "step": 9238
    },
    {
        "loss": 2.8344,
        "grad_norm": 2.1505839824676514,
        "learning_rate": 0.00019804232563880243,
        "epoch": 0.6885013786422237,
        "step": 9239
    },
    {
        "loss": 2.0066,
        "grad_norm": 3.5830745697021484,
        "learning_rate": 0.00019802844625508957,
        "epoch": 0.6885758998435054,
        "step": 9240
    },
    {
        "loss": 2.6656,
        "grad_norm": 2.7416012287139893,
        "learning_rate": 0.00019801451833428315,
        "epoch": 0.6886504210447872,
        "step": 9241
    },
    {
        "loss": 2.8259,
        "grad_norm": 2.0493717193603516,
        "learning_rate": 0.0001980005418832793,
        "epoch": 0.6887249422460691,
        "step": 9242
    },
    {
        "loss": 2.4061,
        "grad_norm": 2.69378399848938,
        "learning_rate": 0.0001979865169089983,
        "epoch": 0.6887994634473508,
        "step": 9243
    },
    {
        "loss": 2.2934,
        "grad_norm": 3.5254886150360107,
        "learning_rate": 0.00019797244341838432,
        "epoch": 0.6888739846486326,
        "step": 9244
    },
    {
        "loss": 2.4191,
        "grad_norm": 4.000360012054443,
        "learning_rate": 0.00019795832141840562,
        "epoch": 0.6889485058499143,
        "step": 9245
    },
    {
        "loss": 2.3108,
        "grad_norm": 3.328869342803955,
        "learning_rate": 0.00019794415091605448,
        "epoch": 0.6890230270511961,
        "step": 9246
    },
    {
        "loss": 2.1895,
        "grad_norm": 3.7914371490478516,
        "learning_rate": 0.00019792993191834713,
        "epoch": 0.6890975482524778,
        "step": 9247
    },
    {
        "loss": 2.1044,
        "grad_norm": 3.353060483932495,
        "learning_rate": 0.00019791566443232392,
        "epoch": 0.6891720694537596,
        "step": 9248
    },
    {
        "loss": 1.4971,
        "grad_norm": 2.1407623291015625,
        "learning_rate": 0.00019790134846504913,
        "epoch": 0.6892465906550413,
        "step": 9249
    },
    {
        "loss": 1.4293,
        "grad_norm": 2.5049779415130615,
        "learning_rate": 0.00019788698402361106,
        "epoch": 0.6893211118563232,
        "step": 9250
    },
    {
        "loss": 2.7847,
        "grad_norm": 3.495044231414795,
        "learning_rate": 0.00019787257111512197,
        "epoch": 0.6893956330576049,
        "step": 9251
    },
    {
        "loss": 2.5282,
        "grad_norm": 2.584746837615967,
        "learning_rate": 0.00019785810974671825,
        "epoch": 0.6894701542588867,
        "step": 9252
    },
    {
        "loss": 2.5874,
        "grad_norm": 2.924478769302368,
        "learning_rate": 0.00019784359992556017,
        "epoch": 0.6895446754601684,
        "step": 9253
    },
    {
        "loss": 1.0751,
        "grad_norm": 3.8210086822509766,
        "learning_rate": 0.000197829041658832,
        "epoch": 0.6896191966614502,
        "step": 9254
    },
    {
        "loss": 2.5093,
        "grad_norm": 3.6656899452209473,
        "learning_rate": 0.00019781443495374196,
        "epoch": 0.6896937178627319,
        "step": 9255
    },
    {
        "loss": 2.4391,
        "grad_norm": 1.9232745170593262,
        "learning_rate": 0.0001977997798175224,
        "epoch": 0.6897682390640137,
        "step": 9256
    },
    {
        "loss": 2.2703,
        "grad_norm": 2.508936643600464,
        "learning_rate": 0.0001977850762574295,
        "epoch": 0.6898427602652955,
        "step": 9257
    },
    {
        "loss": 2.6771,
        "grad_norm": 2.958875894546509,
        "learning_rate": 0.0001977703242807435,
        "epoch": 0.6899172814665773,
        "step": 9258
    },
    {
        "loss": 3.1874,
        "grad_norm": 2.7627782821655273,
        "learning_rate": 0.00019775552389476864,
        "epoch": 0.689991802667859,
        "step": 9259
    },
    {
        "loss": 2.7012,
        "grad_norm": 2.556779146194458,
        "learning_rate": 0.00019774067510683297,
        "epoch": 0.6900663238691408,
        "step": 9260
    },
    {
        "loss": 2.22,
        "grad_norm": 3.4879558086395264,
        "learning_rate": 0.00019772577792428867,
        "epoch": 0.6901408450704225,
        "step": 9261
    },
    {
        "loss": 2.9714,
        "grad_norm": 3.0526208877563477,
        "learning_rate": 0.00019771083235451182,
        "epoch": 0.6902153662717043,
        "step": 9262
    },
    {
        "loss": 2.4272,
        "grad_norm": 4.101118564605713,
        "learning_rate": 0.00019769583840490245,
        "epoch": 0.690289887472986,
        "step": 9263
    },
    {
        "loss": 1.6664,
        "grad_norm": 2.144209861755371,
        "learning_rate": 0.00019768079608288455,
        "epoch": 0.6903644086742678,
        "step": 9264
    },
    {
        "loss": 2.6204,
        "grad_norm": 3.3102469444274902,
        "learning_rate": 0.00019766570539590605,
        "epoch": 0.6904389298755496,
        "step": 9265
    },
    {
        "loss": 1.6795,
        "grad_norm": 5.711986064910889,
        "learning_rate": 0.00019765056635143892,
        "epoch": 0.6905134510768314,
        "step": 9266
    },
    {
        "loss": 2.8148,
        "grad_norm": 2.9499669075012207,
        "learning_rate": 0.0001976353789569789,
        "epoch": 0.6905879722781131,
        "step": 9267
    },
    {
        "loss": 2.5103,
        "grad_norm": 1.7724618911743164,
        "learning_rate": 0.00019762014322004582,
        "epoch": 0.6906624934793949,
        "step": 9268
    },
    {
        "loss": 2.2647,
        "grad_norm": 3.8117752075195312,
        "learning_rate": 0.00019760485914818344,
        "epoch": 0.6907370146806766,
        "step": 9269
    },
    {
        "loss": 2.0813,
        "grad_norm": 2.588383913040161,
        "learning_rate": 0.0001975895267489593,
        "epoch": 0.6908115358819584,
        "step": 9270
    },
    {
        "loss": 2.5391,
        "grad_norm": 3.218834161758423,
        "learning_rate": 0.000197574146029965,
        "epoch": 0.6908860570832401,
        "step": 9271
    },
    {
        "loss": 2.5973,
        "grad_norm": 2.2073049545288086,
        "learning_rate": 0.0001975587169988161,
        "epoch": 0.690960578284522,
        "step": 9272
    },
    {
        "loss": 2.6456,
        "grad_norm": 3.876338005065918,
        "learning_rate": 0.00019754323966315198,
        "epoch": 0.6910350994858037,
        "step": 9273
    },
    {
        "loss": 2.2964,
        "grad_norm": 2.9745092391967773,
        "learning_rate": 0.00019752771403063594,
        "epoch": 0.6911096206870855,
        "step": 9274
    },
    {
        "loss": 2.7524,
        "grad_norm": 2.983912467956543,
        "learning_rate": 0.00019751214010895526,
        "epoch": 0.6911841418883672,
        "step": 9275
    },
    {
        "loss": 2.4608,
        "grad_norm": 2.926389455795288,
        "learning_rate": 0.00019749651790582114,
        "epoch": 0.691258663089649,
        "step": 9276
    },
    {
        "loss": 2.8641,
        "grad_norm": 3.171631336212158,
        "learning_rate": 0.00019748084742896856,
        "epoch": 0.6913331842909308,
        "step": 9277
    },
    {
        "loss": 2.2304,
        "grad_norm": 3.309401035308838,
        "learning_rate": 0.00019746512868615653,
        "epoch": 0.6914077054922125,
        "step": 9278
    },
    {
        "loss": 3.0053,
        "grad_norm": 2.7350475788116455,
        "learning_rate": 0.00019744936168516797,
        "epoch": 0.6914822266934944,
        "step": 9279
    },
    {
        "loss": 2.4037,
        "grad_norm": 2.4465556144714355,
        "learning_rate": 0.0001974335464338095,
        "epoch": 0.6915567478947761,
        "step": 9280
    },
    {
        "loss": 2.0334,
        "grad_norm": 3.577688694000244,
        "learning_rate": 0.00019741768293991198,
        "epoch": 0.6916312690960579,
        "step": 9281
    },
    {
        "loss": 2.7546,
        "grad_norm": 2.582629680633545,
        "learning_rate": 0.00019740177121132973,
        "epoch": 0.6917057902973396,
        "step": 9282
    },
    {
        "loss": 2.605,
        "grad_norm": 2.7598636150360107,
        "learning_rate": 0.00019738581125594126,
        "epoch": 0.6917803114986214,
        "step": 9283
    },
    {
        "loss": 2.3118,
        "grad_norm": 3.174185276031494,
        "learning_rate": 0.00019736980308164887,
        "epoch": 0.6918548326999031,
        "step": 9284
    },
    {
        "loss": 2.1555,
        "grad_norm": 2.886605739593506,
        "learning_rate": 0.0001973537466963787,
        "epoch": 0.6919293539011849,
        "step": 9285
    },
    {
        "loss": 3.0873,
        "grad_norm": 2.233651876449585,
        "learning_rate": 0.00019733764210808086,
        "epoch": 0.6920038751024666,
        "step": 9286
    },
    {
        "loss": 2.337,
        "grad_norm": 3.1110291481018066,
        "learning_rate": 0.00019732148932472918,
        "epoch": 0.6920783963037485,
        "step": 9287
    },
    {
        "loss": 1.8962,
        "grad_norm": 3.2372617721557617,
        "learning_rate": 0.00019730528835432148,
        "epoch": 0.6921529175050302,
        "step": 9288
    },
    {
        "loss": 1.8706,
        "grad_norm": 4.922585487365723,
        "learning_rate": 0.00019728903920487935,
        "epoch": 0.692227438706312,
        "step": 9289
    },
    {
        "loss": 2.7711,
        "grad_norm": 5.185653209686279,
        "learning_rate": 0.00019727274188444834,
        "epoch": 0.6923019599075937,
        "step": 9290
    },
    {
        "loss": 2.0436,
        "grad_norm": 2.928783416748047,
        "learning_rate": 0.00019725639640109775,
        "epoch": 0.6923764811088755,
        "step": 9291
    },
    {
        "loss": 2.346,
        "grad_norm": 4.955038547515869,
        "learning_rate": 0.00019724000276292078,
        "epoch": 0.6924510023101572,
        "step": 9292
    },
    {
        "loss": 2.2934,
        "grad_norm": 2.4261841773986816,
        "learning_rate": 0.00019722356097803438,
        "epoch": 0.692525523511439,
        "step": 9293
    },
    {
        "loss": 2.5745,
        "grad_norm": 2.537707567214966,
        "learning_rate": 0.0001972070710545795,
        "epoch": 0.6926000447127207,
        "step": 9294
    },
    {
        "loss": 1.984,
        "grad_norm": 3.0562336444854736,
        "learning_rate": 0.00019719053300072084,
        "epoch": 0.6926745659140026,
        "step": 9295
    },
    {
        "loss": 2.7537,
        "grad_norm": 3.7007172107696533,
        "learning_rate": 0.0001971739468246469,
        "epoch": 0.6927490871152843,
        "step": 9296
    },
    {
        "loss": 3.326,
        "grad_norm": 2.010713815689087,
        "learning_rate": 0.00019715731253457002,
        "epoch": 0.6928236083165661,
        "step": 9297
    },
    {
        "loss": 2.3866,
        "grad_norm": 1.6741489171981812,
        "learning_rate": 0.00019714063013872646,
        "epoch": 0.6928981295178478,
        "step": 9298
    },
    {
        "loss": 1.8115,
        "grad_norm": 3.7334394454956055,
        "learning_rate": 0.00019712389964537613,
        "epoch": 0.6929726507191296,
        "step": 9299
    },
    {
        "loss": 2.3847,
        "grad_norm": 2.277231454849243,
        "learning_rate": 0.00019710712106280293,
        "epoch": 0.6930471719204113,
        "step": 9300
    },
    {
        "loss": 2.5898,
        "grad_norm": 3.1969149112701416,
        "learning_rate": 0.00019709029439931444,
        "epoch": 0.6931216931216931,
        "step": 9301
    },
    {
        "loss": 2.4128,
        "grad_norm": 2.183696985244751,
        "learning_rate": 0.00019707341966324204,
        "epoch": 0.6931962143229748,
        "step": 9302
    },
    {
        "loss": 2.0582,
        "grad_norm": 4.486412525177002,
        "learning_rate": 0.00019705649686294104,
        "epoch": 0.6932707355242567,
        "step": 9303
    },
    {
        "loss": 2.5767,
        "grad_norm": 2.649120569229126,
        "learning_rate": 0.0001970395260067905,
        "epoch": 0.6933452567255384,
        "step": 9304
    },
    {
        "loss": 2.3752,
        "grad_norm": 3.7180733680725098,
        "learning_rate": 0.0001970225071031931,
        "epoch": 0.6934197779268202,
        "step": 9305
    },
    {
        "loss": 2.4389,
        "grad_norm": 2.653181314468384,
        "learning_rate": 0.00019700544016057565,
        "epoch": 0.6934942991281019,
        "step": 9306
    },
    {
        "loss": 2.7565,
        "grad_norm": 2.4891140460968018,
        "learning_rate": 0.00019698832518738843,
        "epoch": 0.6935688203293837,
        "step": 9307
    },
    {
        "loss": 2.4191,
        "grad_norm": 2.230731248855591,
        "learning_rate": 0.0001969711621921056,
        "epoch": 0.6936433415306654,
        "step": 9308
    },
    {
        "loss": 1.8871,
        "grad_norm": 3.826805591583252,
        "learning_rate": 0.00019695395118322524,
        "epoch": 0.6937178627319472,
        "step": 9309
    },
    {
        "loss": 2.2502,
        "grad_norm": 3.239492177963257,
        "learning_rate": 0.000196936692169269,
        "epoch": 0.693792383933229,
        "step": 9310
    },
    {
        "loss": 2.6366,
        "grad_norm": 2.1534576416015625,
        "learning_rate": 0.00019691938515878233,
        "epoch": 0.6938669051345108,
        "step": 9311
    },
    {
        "loss": 2.3369,
        "grad_norm": 2.5272419452667236,
        "learning_rate": 0.00019690203016033467,
        "epoch": 0.6939414263357926,
        "step": 9312
    },
    {
        "loss": 2.4101,
        "grad_norm": 2.843047618865967,
        "learning_rate": 0.00019688462718251887,
        "epoch": 0.6940159475370743,
        "step": 9313
    },
    {
        "loss": 2.4792,
        "grad_norm": 3.544786214828491,
        "learning_rate": 0.0001968671762339518,
        "epoch": 0.6940904687383561,
        "step": 9314
    },
    {
        "loss": 2.4908,
        "grad_norm": 2.6050431728363037,
        "learning_rate": 0.00019684967732327402,
        "epoch": 0.6941649899396378,
        "step": 9315
    },
    {
        "loss": 2.6236,
        "grad_norm": 2.12904691696167,
        "learning_rate": 0.0001968321304591497,
        "epoch": 0.6942395111409196,
        "step": 9316
    },
    {
        "loss": 2.3816,
        "grad_norm": 2.7498157024383545,
        "learning_rate": 0.00019681453565026702,
        "epoch": 0.6943140323422013,
        "step": 9317
    },
    {
        "loss": 2.0414,
        "grad_norm": 2.894324541091919,
        "learning_rate": 0.00019679689290533762,
        "epoch": 0.6943885535434832,
        "step": 9318
    },
    {
        "loss": 1.8832,
        "grad_norm": 3.724384069442749,
        "learning_rate": 0.00019677920223309708,
        "epoch": 0.6944630747447649,
        "step": 9319
    },
    {
        "loss": 2.2299,
        "grad_norm": 4.161212921142578,
        "learning_rate": 0.0001967614636423046,
        "epoch": 0.6945375959460467,
        "step": 9320
    },
    {
        "loss": 2.1123,
        "grad_norm": 3.2956724166870117,
        "learning_rate": 0.00019674367714174313,
        "epoch": 0.6946121171473284,
        "step": 9321
    },
    {
        "loss": 1.8551,
        "grad_norm": 3.468435287475586,
        "learning_rate": 0.00019672584274021935,
        "epoch": 0.6946866383486102,
        "step": 9322
    },
    {
        "loss": 1.8082,
        "grad_norm": 3.8926587104797363,
        "learning_rate": 0.00019670796044656368,
        "epoch": 0.6947611595498919,
        "step": 9323
    },
    {
        "loss": 2.0763,
        "grad_norm": 4.143113613128662,
        "learning_rate": 0.00019669003026963022,
        "epoch": 0.6948356807511737,
        "step": 9324
    },
    {
        "loss": 2.9239,
        "grad_norm": 3.0468978881835938,
        "learning_rate": 0.00019667205221829677,
        "epoch": 0.6949102019524555,
        "step": 9325
    },
    {
        "loss": 2.0402,
        "grad_norm": 3.789116621017456,
        "learning_rate": 0.00019665402630146482,
        "epoch": 0.6949847231537373,
        "step": 9326
    },
    {
        "loss": 2.4108,
        "grad_norm": 2.0218803882598877,
        "learning_rate": 0.0001966359525280597,
        "epoch": 0.695059244355019,
        "step": 9327
    },
    {
        "loss": 1.8641,
        "grad_norm": 4.366370677947998,
        "learning_rate": 0.0001966178309070302,
        "epoch": 0.6951337655563008,
        "step": 9328
    },
    {
        "loss": 2.1785,
        "grad_norm": 3.3561997413635254,
        "learning_rate": 0.00019659966144734906,
        "epoch": 0.6952082867575825,
        "step": 9329
    },
    {
        "loss": 1.8242,
        "grad_norm": 3.4128708839416504,
        "learning_rate": 0.00019658144415801246,
        "epoch": 0.6952828079588643,
        "step": 9330
    },
    {
        "loss": 2.5914,
        "grad_norm": 1.9265005588531494,
        "learning_rate": 0.0001965631790480404,
        "epoch": 0.695357329160146,
        "step": 9331
    },
    {
        "loss": 2.2331,
        "grad_norm": 3.961582660675049,
        "learning_rate": 0.00019654486612647666,
        "epoch": 0.6954318503614279,
        "step": 9332
    },
    {
        "loss": 2.5293,
        "grad_norm": 2.5136425495147705,
        "learning_rate": 0.00019652650540238832,
        "epoch": 0.6955063715627096,
        "step": 9333
    },
    {
        "loss": 2.0364,
        "grad_norm": 3.7452776432037354,
        "learning_rate": 0.00019650809688486663,
        "epoch": 0.6955808927639914,
        "step": 9334
    },
    {
        "loss": 0.9288,
        "grad_norm": 4.306247234344482,
        "learning_rate": 0.00019648964058302607,
        "epoch": 0.6956554139652731,
        "step": 9335
    },
    {
        "loss": 2.0396,
        "grad_norm": 2.6542952060699463,
        "learning_rate": 0.00019647113650600502,
        "epoch": 0.6957299351665549,
        "step": 9336
    },
    {
        "loss": 2.0659,
        "grad_norm": 2.314901828765869,
        "learning_rate": 0.0001964525846629655,
        "epoch": 0.6958044563678366,
        "step": 9337
    },
    {
        "loss": 2.3369,
        "grad_norm": 3.304312229156494,
        "learning_rate": 0.00019643398506309306,
        "epoch": 0.6958789775691184,
        "step": 9338
    },
    {
        "loss": 2.3285,
        "grad_norm": 3.2299246788024902,
        "learning_rate": 0.00019641533771559705,
        "epoch": 0.6959534987704001,
        "step": 9339
    },
    {
        "loss": 2.4801,
        "grad_norm": 1.3116455078125,
        "learning_rate": 0.00019639664262971032,
        "epoch": 0.696028019971682,
        "step": 9340
    },
    {
        "loss": 1.4715,
        "grad_norm": 7.66941499710083,
        "learning_rate": 0.00019637789981468944,
        "epoch": 0.6961025411729637,
        "step": 9341
    },
    {
        "loss": 2.7593,
        "grad_norm": 2.589770793914795,
        "learning_rate": 0.00019635910927981456,
        "epoch": 0.6961770623742455,
        "step": 9342
    },
    {
        "loss": 2.5436,
        "grad_norm": 2.0531516075134277,
        "learning_rate": 0.00019634027103438957,
        "epoch": 0.6962515835755272,
        "step": 9343
    },
    {
        "loss": 2.4839,
        "grad_norm": 2.726839065551758,
        "learning_rate": 0.0001963213850877418,
        "epoch": 0.696326104776809,
        "step": 9344
    },
    {
        "loss": 2.8506,
        "grad_norm": 1.9208828210830688,
        "learning_rate": 0.00019630245144922238,
        "epoch": 0.6964006259780907,
        "step": 9345
    },
    {
        "loss": 2.5067,
        "grad_norm": 2.5846779346466064,
        "learning_rate": 0.00019628347012820593,
        "epoch": 0.6964751471793725,
        "step": 9346
    },
    {
        "loss": 2.1369,
        "grad_norm": 4.743516445159912,
        "learning_rate": 0.00019626444113409074,
        "epoch": 0.6965496683806542,
        "step": 9347
    },
    {
        "loss": 2.803,
        "grad_norm": 3.1186695098876953,
        "learning_rate": 0.0001962453644762987,
        "epoch": 0.6966241895819361,
        "step": 9348
    },
    {
        "loss": 2.4202,
        "grad_norm": 3.135545253753662,
        "learning_rate": 0.00019622624016427524,
        "epoch": 0.6966987107832179,
        "step": 9349
    },
    {
        "loss": 2.0627,
        "grad_norm": 3.5736732482910156,
        "learning_rate": 0.00019620706820748952,
        "epoch": 0.6967732319844996,
        "step": 9350
    },
    {
        "loss": 2.3067,
        "grad_norm": 2.3063716888427734,
        "learning_rate": 0.0001961878486154341,
        "epoch": 0.6968477531857814,
        "step": 9351
    },
    {
        "loss": 2.2019,
        "grad_norm": 3.346489667892456,
        "learning_rate": 0.00019616858139762537,
        "epoch": 0.6969222743870631,
        "step": 9352
    },
    {
        "loss": 2.3138,
        "grad_norm": 3.2981419563293457,
        "learning_rate": 0.000196149266563603,
        "epoch": 0.6969967955883449,
        "step": 9353
    },
    {
        "loss": 2.0964,
        "grad_norm": 4.573561668395996,
        "learning_rate": 0.00019612990412293049,
        "epoch": 0.6970713167896266,
        "step": 9354
    },
    {
        "loss": 1.9741,
        "grad_norm": 3.4852941036224365,
        "learning_rate": 0.00019611049408519479,
        "epoch": 0.6971458379909085,
        "step": 9355
    },
    {
        "loss": 2.6643,
        "grad_norm": 2.2211341857910156,
        "learning_rate": 0.00019609103646000642,
        "epoch": 0.6972203591921902,
        "step": 9356
    },
    {
        "loss": 2.1531,
        "grad_norm": 2.9843955039978027,
        "learning_rate": 0.00019607153125699952,
        "epoch": 0.697294880393472,
        "step": 9357
    },
    {
        "loss": 1.4896,
        "grad_norm": 1.6920948028564453,
        "learning_rate": 0.0001960519784858318,
        "epoch": 0.6973694015947537,
        "step": 9358
    },
    {
        "loss": 1.2184,
        "grad_norm": 3.7876994609832764,
        "learning_rate": 0.0001960323781561844,
        "epoch": 0.6974439227960355,
        "step": 9359
    },
    {
        "loss": 2.1112,
        "grad_norm": 3.1882712841033936,
        "learning_rate": 0.00019601273027776214,
        "epoch": 0.6975184439973172,
        "step": 9360
    },
    {
        "loss": 2.2032,
        "grad_norm": 2.820901870727539,
        "learning_rate": 0.0001959930348602933,
        "epoch": 0.697592965198599,
        "step": 9361
    },
    {
        "loss": 2.6855,
        "grad_norm": 1.8501142263412476,
        "learning_rate": 0.0001959732919135297,
        "epoch": 0.6976674863998807,
        "step": 9362
    },
    {
        "loss": 2.4829,
        "grad_norm": 2.6194686889648438,
        "learning_rate": 0.00019595350144724683,
        "epoch": 0.6977420076011626,
        "step": 9363
    },
    {
        "loss": 2.3728,
        "grad_norm": 3.8439292907714844,
        "learning_rate": 0.00019593366347124344,
        "epoch": 0.6978165288024443,
        "step": 9364
    },
    {
        "loss": 2.5561,
        "grad_norm": 3.5804476737976074,
        "learning_rate": 0.00019591377799534204,
        "epoch": 0.6978910500037261,
        "step": 9365
    },
    {
        "loss": 3.0583,
        "grad_norm": 2.137026071548462,
        "learning_rate": 0.00019589384502938857,
        "epoch": 0.6979655712050078,
        "step": 9366
    },
    {
        "loss": 1.941,
        "grad_norm": 3.446209669113159,
        "learning_rate": 0.00019587386458325257,
        "epoch": 0.6980400924062896,
        "step": 9367
    },
    {
        "loss": 2.6141,
        "grad_norm": 2.9531807899475098,
        "learning_rate": 0.0001958538366668269,
        "epoch": 0.6981146136075713,
        "step": 9368
    },
    {
        "loss": 2.5088,
        "grad_norm": 2.6013705730438232,
        "learning_rate": 0.00019583376129002805,
        "epoch": 0.6981891348088531,
        "step": 9369
    },
    {
        "loss": 2.2889,
        "grad_norm": 2.800995111465454,
        "learning_rate": 0.0001958136384627961,
        "epoch": 0.6982636560101348,
        "step": 9370
    },
    {
        "loss": 2.5008,
        "grad_norm": 2.2589943408966064,
        "learning_rate": 0.00019579346819509444,
        "epoch": 0.6983381772114167,
        "step": 9371
    },
    {
        "loss": 1.7886,
        "grad_norm": 3.8594532012939453,
        "learning_rate": 0.00019577325049691002,
        "epoch": 0.6984126984126984,
        "step": 9372
    },
    {
        "loss": 1.5848,
        "grad_norm": 3.126063585281372,
        "learning_rate": 0.0001957529853782533,
        "epoch": 0.6984872196139802,
        "step": 9373
    },
    {
        "loss": 1.82,
        "grad_norm": 4.091209888458252,
        "learning_rate": 0.0001957326728491582,
        "epoch": 0.6985617408152619,
        "step": 9374
    },
    {
        "loss": 2.592,
        "grad_norm": 3.4315404891967773,
        "learning_rate": 0.00019571231291968215,
        "epoch": 0.6986362620165437,
        "step": 9375
    },
    {
        "loss": 2.499,
        "grad_norm": 2.337236166000366,
        "learning_rate": 0.00019569190559990603,
        "epoch": 0.6987107832178254,
        "step": 9376
    },
    {
        "loss": 1.8366,
        "grad_norm": 3.788874387741089,
        "learning_rate": 0.00019567145089993403,
        "epoch": 0.6987853044191072,
        "step": 9377
    },
    {
        "loss": 2.6136,
        "grad_norm": 2.4595398902893066,
        "learning_rate": 0.00019565094882989414,
        "epoch": 0.698859825620389,
        "step": 9378
    },
    {
        "loss": 2.0191,
        "grad_norm": 3.1934587955474854,
        "learning_rate": 0.00019563039939993746,
        "epoch": 0.6989343468216708,
        "step": 9379
    },
    {
        "loss": 2.3196,
        "grad_norm": 2.895501136779785,
        "learning_rate": 0.0001956098026202388,
        "epoch": 0.6990088680229525,
        "step": 9380
    },
    {
        "loss": 1.9126,
        "grad_norm": 4.584874153137207,
        "learning_rate": 0.0001955891585009962,
        "epoch": 0.6990833892242343,
        "step": 9381
    },
    {
        "loss": 2.5903,
        "grad_norm": 4.310543537139893,
        "learning_rate": 0.00019556846705243126,
        "epoch": 0.699157910425516,
        "step": 9382
    },
    {
        "loss": 1.9236,
        "grad_norm": 1.9522948265075684,
        "learning_rate": 0.0001955477282847891,
        "epoch": 0.6992324316267978,
        "step": 9383
    },
    {
        "loss": 2.7836,
        "grad_norm": 3.592047691345215,
        "learning_rate": 0.00019552694220833797,
        "epoch": 0.6993069528280796,
        "step": 9384
    },
    {
        "loss": 2.553,
        "grad_norm": 2.7667946815490723,
        "learning_rate": 0.00019550610883336994,
        "epoch": 0.6993814740293613,
        "step": 9385
    },
    {
        "loss": 2.554,
        "grad_norm": 4.4888458251953125,
        "learning_rate": 0.00019548522817020017,
        "epoch": 0.6994559952306432,
        "step": 9386
    },
    {
        "loss": 2.5689,
        "grad_norm": 2.3402440547943115,
        "learning_rate": 0.0001954643002291673,
        "epoch": 0.6995305164319249,
        "step": 9387
    },
    {
        "loss": 2.6694,
        "grad_norm": 2.80548095703125,
        "learning_rate": 0.00019544332502063365,
        "epoch": 0.6996050376332067,
        "step": 9388
    },
    {
        "loss": 1.9555,
        "grad_norm": 3.6452083587646484,
        "learning_rate": 0.00019542230255498454,
        "epoch": 0.6996795588344884,
        "step": 9389
    },
    {
        "loss": 2.8641,
        "grad_norm": 2.1534502506256104,
        "learning_rate": 0.00019540123284262896,
        "epoch": 0.6997540800357702,
        "step": 9390
    },
    {
        "loss": 1.6641,
        "grad_norm": 3.4937992095947266,
        "learning_rate": 0.00019538011589399922,
        "epoch": 0.6998286012370519,
        "step": 9391
    },
    {
        "loss": 2.7891,
        "grad_norm": 3.0312979221343994,
        "learning_rate": 0.00019535895171955106,
        "epoch": 0.6999031224383337,
        "step": 9392
    },
    {
        "loss": 2.2204,
        "grad_norm": 2.757441282272339,
        "learning_rate": 0.00019533774032976339,
        "epoch": 0.6999776436396155,
        "step": 9393
    },
    {
        "loss": 1.8147,
        "grad_norm": 3.3977468013763428,
        "learning_rate": 0.00019531648173513882,
        "epoch": 0.7000521648408973,
        "step": 9394
    },
    {
        "loss": 2.0991,
        "grad_norm": 2.672231435775757,
        "learning_rate": 0.00019529517594620312,
        "epoch": 0.700126686042179,
        "step": 9395
    },
    {
        "loss": 1.9978,
        "grad_norm": 3.4897871017456055,
        "learning_rate": 0.00019527382297350552,
        "epoch": 0.7002012072434608,
        "step": 9396
    },
    {
        "loss": 2.0407,
        "grad_norm": 3.0109877586364746,
        "learning_rate": 0.0001952524228276185,
        "epoch": 0.7002757284447425,
        "step": 9397
    },
    {
        "loss": 2.4338,
        "grad_norm": 4.6018548011779785,
        "learning_rate": 0.00019523097551913803,
        "epoch": 0.7003502496460243,
        "step": 9398
    },
    {
        "loss": 2.0747,
        "grad_norm": 2.7140157222747803,
        "learning_rate": 0.00019520948105868335,
        "epoch": 0.700424770847306,
        "step": 9399
    },
    {
        "loss": 1.8642,
        "grad_norm": 4.010965824127197,
        "learning_rate": 0.00019518793945689708,
        "epoch": 0.7004992920485879,
        "step": 9400
    },
    {
        "loss": 2.0757,
        "grad_norm": 3.4664647579193115,
        "learning_rate": 0.00019516635072444524,
        "epoch": 0.7005738132498696,
        "step": 9401
    },
    {
        "loss": 1.2698,
        "grad_norm": 2.020965576171875,
        "learning_rate": 0.00019514471487201696,
        "epoch": 0.7006483344511514,
        "step": 9402
    },
    {
        "loss": 2.2514,
        "grad_norm": 2.977201223373413,
        "learning_rate": 0.00019512303191032509,
        "epoch": 0.7007228556524331,
        "step": 9403
    },
    {
        "loss": 2.5755,
        "grad_norm": 2.7444519996643066,
        "learning_rate": 0.00019510130185010528,
        "epoch": 0.7007973768537149,
        "step": 9404
    },
    {
        "loss": 2.0219,
        "grad_norm": 2.681401252746582,
        "learning_rate": 0.00019507952470211706,
        "epoch": 0.7008718980549966,
        "step": 9405
    },
    {
        "loss": 2.2835,
        "grad_norm": 2.641984462738037,
        "learning_rate": 0.00019505770047714287,
        "epoch": 0.7009464192562784,
        "step": 9406
    },
    {
        "loss": 2.8807,
        "grad_norm": 2.4822280406951904,
        "learning_rate": 0.0001950358291859886,
        "epoch": 0.7010209404575601,
        "step": 9407
    },
    {
        "loss": 2.6595,
        "grad_norm": 3.1165592670440674,
        "learning_rate": 0.0001950139108394835,
        "epoch": 0.701095461658842,
        "step": 9408
    },
    {
        "loss": 2.2154,
        "grad_norm": 3.2942652702331543,
        "learning_rate": 0.00019499194544848005,
        "epoch": 0.7011699828601237,
        "step": 9409
    },
    {
        "loss": 2.7223,
        "grad_norm": 3.8954825401306152,
        "learning_rate": 0.00019496993302385395,
        "epoch": 0.7012445040614055,
        "step": 9410
    },
    {
        "loss": 1.9548,
        "grad_norm": 4.1983113288879395,
        "learning_rate": 0.0001949478735765044,
        "epoch": 0.7013190252626872,
        "step": 9411
    },
    {
        "loss": 2.9141,
        "grad_norm": 3.259998321533203,
        "learning_rate": 0.0001949257671173537,
        "epoch": 0.701393546463969,
        "step": 9412
    },
    {
        "loss": 2.5861,
        "grad_norm": 2.5373780727386475,
        "learning_rate": 0.00019490361365734744,
        "epoch": 0.7014680676652507,
        "step": 9413
    },
    {
        "loss": 2.4986,
        "grad_norm": 3.0753984451293945,
        "learning_rate": 0.00019488141320745463,
        "epoch": 0.7015425888665325,
        "step": 9414
    },
    {
        "loss": 2.1122,
        "grad_norm": 3.8782236576080322,
        "learning_rate": 0.00019485916577866727,
        "epoch": 0.7016171100678142,
        "step": 9415
    },
    {
        "loss": 1.6033,
        "grad_norm": 5.944339275360107,
        "learning_rate": 0.00019483687138200097,
        "epoch": 0.7016916312690961,
        "step": 9416
    },
    {
        "loss": 1.6999,
        "grad_norm": 2.185314893722534,
        "learning_rate": 0.00019481453002849426,
        "epoch": 0.7017661524703778,
        "step": 9417
    },
    {
        "loss": 2.057,
        "grad_norm": 4.655731201171875,
        "learning_rate": 0.0001947921417292092,
        "epoch": 0.7018406736716596,
        "step": 9418
    },
    {
        "loss": 3.2751,
        "grad_norm": 3.2102649211883545,
        "learning_rate": 0.0001947697064952309,
        "epoch": 0.7019151948729414,
        "step": 9419
    },
    {
        "loss": 1.6056,
        "grad_norm": 2.2866108417510986,
        "learning_rate": 0.0001947472243376678,
        "epoch": 0.7019897160742231,
        "step": 9420
    },
    {
        "loss": 1.8138,
        "grad_norm": 3.550699234008789,
        "learning_rate": 0.00019472469526765157,
        "epoch": 0.7020642372755049,
        "step": 9421
    },
    {
        "loss": 2.2824,
        "grad_norm": 3.170591115951538,
        "learning_rate": 0.00019470211929633707,
        "epoch": 0.7021387584767866,
        "step": 9422
    },
    {
        "loss": 1.604,
        "grad_norm": 3.5869057178497314,
        "learning_rate": 0.00019467949643490245,
        "epoch": 0.7022132796780685,
        "step": 9423
    },
    {
        "loss": 2.486,
        "grad_norm": 3.419647455215454,
        "learning_rate": 0.0001946568266945489,
        "epoch": 0.7022878008793502,
        "step": 9424
    },
    {
        "loss": 2.736,
        "grad_norm": 4.666159629821777,
        "learning_rate": 0.0001946341100865011,
        "epoch": 0.702362322080632,
        "step": 9425
    },
    {
        "loss": 2.4712,
        "grad_norm": 2.006774663925171,
        "learning_rate": 0.00019461134662200668,
        "epoch": 0.7024368432819137,
        "step": 9426
    },
    {
        "loss": 2.3955,
        "grad_norm": 3.0379176139831543,
        "learning_rate": 0.00019458853631233662,
        "epoch": 0.7025113644831955,
        "step": 9427
    },
    {
        "loss": 2.5872,
        "grad_norm": 2.210359811782837,
        "learning_rate": 0.00019456567916878505,
        "epoch": 0.7025858856844772,
        "step": 9428
    },
    {
        "loss": 2.3346,
        "grad_norm": 3.200716972351074,
        "learning_rate": 0.00019454277520266936,
        "epoch": 0.702660406885759,
        "step": 9429
    },
    {
        "loss": 1.6854,
        "grad_norm": 3.411496162414551,
        "learning_rate": 0.00019451982442532986,
        "epoch": 0.7027349280870407,
        "step": 9430
    },
    {
        "loss": 2.4831,
        "grad_norm": 3.199820041656494,
        "learning_rate": 0.00019449682684813044,
        "epoch": 0.7028094492883226,
        "step": 9431
    },
    {
        "loss": 1.8413,
        "grad_norm": 5.179537773132324,
        "learning_rate": 0.00019447378248245786,
        "epoch": 0.7028839704896043,
        "step": 9432
    },
    {
        "loss": 3.1534,
        "grad_norm": 4.071610450744629,
        "learning_rate": 0.0001944506913397221,
        "epoch": 0.7029584916908861,
        "step": 9433
    },
    {
        "loss": 2.5974,
        "grad_norm": 3.217515230178833,
        "learning_rate": 0.0001944275534313565,
        "epoch": 0.7030330128921678,
        "step": 9434
    },
    {
        "loss": 2.2905,
        "grad_norm": 3.976595401763916,
        "learning_rate": 0.00019440436876881718,
        "epoch": 0.7031075340934496,
        "step": 9435
    },
    {
        "loss": 1.6332,
        "grad_norm": 3.584550619125366,
        "learning_rate": 0.00019438113736358376,
        "epoch": 0.7031820552947313,
        "step": 9436
    },
    {
        "loss": 1.6029,
        "grad_norm": 3.825103282928467,
        "learning_rate": 0.0001943578592271589,
        "epoch": 0.7032565764960131,
        "step": 9437
    },
    {
        "loss": 2.3563,
        "grad_norm": 2.422537088394165,
        "learning_rate": 0.0001943345343710682,
        "epoch": 0.7033310976972948,
        "step": 9438
    },
    {
        "loss": 2.7674,
        "grad_norm": 2.5727295875549316,
        "learning_rate": 0.00019431116280686073,
        "epoch": 0.7034056188985767,
        "step": 9439
    },
    {
        "loss": 2.4824,
        "grad_norm": 2.856365919113159,
        "learning_rate": 0.0001942877445461084,
        "epoch": 0.7034801400998584,
        "step": 9440
    },
    {
        "loss": 2.0022,
        "grad_norm": 2.2149882316589355,
        "learning_rate": 0.00019426427960040648,
        "epoch": 0.7035546613011402,
        "step": 9441
    },
    {
        "loss": 2.4518,
        "grad_norm": 3.3467068672180176,
        "learning_rate": 0.00019424076798137314,
        "epoch": 0.7036291825024219,
        "step": 9442
    },
    {
        "loss": 2.1973,
        "grad_norm": 3.4222662448883057,
        "learning_rate": 0.0001942172097006498,
        "epoch": 0.7037037037037037,
        "step": 9443
    },
    {
        "loss": 2.7114,
        "grad_norm": 2.267908811569214,
        "learning_rate": 0.00019419360476990085,
        "epoch": 0.7037782249049854,
        "step": 9444
    },
    {
        "loss": 2.1567,
        "grad_norm": 3.5586655139923096,
        "learning_rate": 0.000194169953200814,
        "epoch": 0.7038527461062672,
        "step": 9445
    },
    {
        "loss": 2.1943,
        "grad_norm": 2.308842420578003,
        "learning_rate": 0.00019414625500509982,
        "epoch": 0.703927267307549,
        "step": 9446
    },
    {
        "loss": 1.6333,
        "grad_norm": 2.5987157821655273,
        "learning_rate": 0.0001941225101944921,
        "epoch": 0.7040017885088308,
        "step": 9447
    },
    {
        "loss": 2.6575,
        "grad_norm": 2.540719509124756,
        "learning_rate": 0.00019409871878074762,
        "epoch": 0.7040763097101125,
        "step": 9448
    },
    {
        "loss": 2.1549,
        "grad_norm": 2.572554349899292,
        "learning_rate": 0.00019407488077564634,
        "epoch": 0.7041508309113943,
        "step": 9449
    },
    {
        "loss": 1.8314,
        "grad_norm": 3.398097276687622,
        "learning_rate": 0.00019405099619099122,
        "epoch": 0.704225352112676,
        "step": 9450
    },
    {
        "loss": 2.4972,
        "grad_norm": 1.985586404800415,
        "learning_rate": 0.00019402706503860833,
        "epoch": 0.7042998733139578,
        "step": 9451
    },
    {
        "loss": 2.6014,
        "grad_norm": 1.9992395639419556,
        "learning_rate": 0.00019400308733034675,
        "epoch": 0.7043743945152395,
        "step": 9452
    },
    {
        "loss": 2.5202,
        "grad_norm": 3.1557769775390625,
        "learning_rate": 0.00019397906307807856,
        "epoch": 0.7044489157165214,
        "step": 9453
    },
    {
        "loss": 2.7194,
        "grad_norm": 2.1428849697113037,
        "learning_rate": 0.00019395499229369912,
        "epoch": 0.7045234369178032,
        "step": 9454
    },
    {
        "loss": 2.2255,
        "grad_norm": 2.9831697940826416,
        "learning_rate": 0.00019393087498912646,
        "epoch": 0.7045979581190849,
        "step": 9455
    },
    {
        "loss": 2.8949,
        "grad_norm": 4.182041645050049,
        "learning_rate": 0.000193906711176302,
        "epoch": 0.7046724793203667,
        "step": 9456
    },
    {
        "loss": 2.3845,
        "grad_norm": 2.2795984745025635,
        "learning_rate": 0.00019388250086718997,
        "epoch": 0.7047470005216484,
        "step": 9457
    },
    {
        "loss": 1.9448,
        "grad_norm": 4.337062358856201,
        "learning_rate": 0.00019385824407377764,
        "epoch": 0.7048215217229302,
        "step": 9458
    },
    {
        "loss": 2.3625,
        "grad_norm": 3.1693832874298096,
        "learning_rate": 0.00019383394080807546,
        "epoch": 0.7048960429242119,
        "step": 9459
    },
    {
        "loss": 2.3259,
        "grad_norm": 3.522400140762329,
        "learning_rate": 0.00019380959108211668,
        "epoch": 0.7049705641254937,
        "step": 9460
    },
    {
        "loss": 2.1106,
        "grad_norm": 3.3628475666046143,
        "learning_rate": 0.0001937851949079577,
        "epoch": 0.7050450853267755,
        "step": 9461
    },
    {
        "loss": 1.2544,
        "grad_norm": 1.0674773454666138,
        "learning_rate": 0.00019376075229767786,
        "epoch": 0.7051196065280573,
        "step": 9462
    },
    {
        "loss": 3.0441,
        "grad_norm": 2.804332733154297,
        "learning_rate": 0.00019373626326337948,
        "epoch": 0.705194127729339,
        "step": 9463
    },
    {
        "loss": 2.1316,
        "grad_norm": 3.325191020965576,
        "learning_rate": 0.00019371172781718781,
        "epoch": 0.7052686489306208,
        "step": 9464
    },
    {
        "loss": 2.3262,
        "grad_norm": 3.4349961280822754,
        "learning_rate": 0.00019368714597125134,
        "epoch": 0.7053431701319025,
        "step": 9465
    },
    {
        "loss": 2.2693,
        "grad_norm": 3.374735116958618,
        "learning_rate": 0.00019366251773774115,
        "epoch": 0.7054176913331843,
        "step": 9466
    },
    {
        "loss": 2.7467,
        "grad_norm": 2.184737205505371,
        "learning_rate": 0.0001936378431288516,
        "epoch": 0.705492212534466,
        "step": 9467
    },
    {
        "loss": 2.7408,
        "grad_norm": 3.4346256256103516,
        "learning_rate": 0.00019361312215679984,
        "epoch": 0.7055667337357479,
        "step": 9468
    },
    {
        "loss": 2.3462,
        "grad_norm": 2.0470974445343018,
        "learning_rate": 0.00019358835483382608,
        "epoch": 0.7056412549370296,
        "step": 9469
    },
    {
        "loss": 2.0998,
        "grad_norm": 2.9123096466064453,
        "learning_rate": 0.0001935635411721934,
        "epoch": 0.7057157761383114,
        "step": 9470
    },
    {
        "loss": 2.5974,
        "grad_norm": 2.944364309310913,
        "learning_rate": 0.00019353868118418785,
        "epoch": 0.7057902973395931,
        "step": 9471
    },
    {
        "loss": 2.7871,
        "grad_norm": 3.2026827335357666,
        "learning_rate": 0.00019351377488211846,
        "epoch": 0.7058648185408749,
        "step": 9472
    },
    {
        "loss": 2.4268,
        "grad_norm": 2.794271945953369,
        "learning_rate": 0.0001934888222783171,
        "epoch": 0.7059393397421566,
        "step": 9473
    },
    {
        "loss": 1.9329,
        "grad_norm": 3.5039143562316895,
        "learning_rate": 0.00019346382338513874,
        "epoch": 0.7060138609434384,
        "step": 9474
    },
    {
        "loss": 2.5283,
        "grad_norm": 2.609919786453247,
        "learning_rate": 0.000193438778214961,
        "epoch": 0.7060883821447201,
        "step": 9475
    },
    {
        "loss": 2.8215,
        "grad_norm": 2.5495569705963135,
        "learning_rate": 0.00019341368678018464,
        "epoch": 0.706162903346002,
        "step": 9476
    },
    {
        "loss": 2.8567,
        "grad_norm": 3.1679766178131104,
        "learning_rate": 0.00019338854909323327,
        "epoch": 0.7062374245472837,
        "step": 9477
    },
    {
        "loss": 2.3575,
        "grad_norm": 2.2926242351531982,
        "learning_rate": 0.00019336336516655333,
        "epoch": 0.7063119457485655,
        "step": 9478
    },
    {
        "loss": 2.7247,
        "grad_norm": 3.125377893447876,
        "learning_rate": 0.00019333813501261422,
        "epoch": 0.7063864669498472,
        "step": 9479
    },
    {
        "loss": 2.1331,
        "grad_norm": 3.166445255279541,
        "learning_rate": 0.00019331285864390826,
        "epoch": 0.706460988151129,
        "step": 9480
    },
    {
        "loss": 2.5494,
        "grad_norm": 2.2688114643096924,
        "learning_rate": 0.0001932875360729505,
        "epoch": 0.7065355093524107,
        "step": 9481
    },
    {
        "loss": 1.415,
        "grad_norm": 4.287525653839111,
        "learning_rate": 0.00019326216731227914,
        "epoch": 0.7066100305536925,
        "step": 9482
    },
    {
        "loss": 2.7939,
        "grad_norm": 3.414900541305542,
        "learning_rate": 0.00019323675237445495,
        "epoch": 0.7066845517549742,
        "step": 9483
    },
    {
        "loss": 2.3091,
        "grad_norm": 2.8739001750946045,
        "learning_rate": 0.0001932112912720617,
        "epoch": 0.7067590729562561,
        "step": 9484
    },
    {
        "loss": 1.631,
        "grad_norm": 3.566891670227051,
        "learning_rate": 0.00019318578401770619,
        "epoch": 0.7068335941575378,
        "step": 9485
    },
    {
        "loss": 2.2082,
        "grad_norm": 3.8041348457336426,
        "learning_rate": 0.00019316023062401763,
        "epoch": 0.7069081153588196,
        "step": 9486
    },
    {
        "loss": 2.2895,
        "grad_norm": 2.3644862174987793,
        "learning_rate": 0.0001931346311036485,
        "epoch": 0.7069826365601013,
        "step": 9487
    },
    {
        "loss": 2.5798,
        "grad_norm": 1.9776839017868042,
        "learning_rate": 0.00019310898546927394,
        "epoch": 0.7070571577613831,
        "step": 9488
    },
    {
        "loss": 1.5679,
        "grad_norm": 4.031125545501709,
        "learning_rate": 0.00019308329373359196,
        "epoch": 0.7071316789626649,
        "step": 9489
    },
    {
        "loss": 2.1477,
        "grad_norm": 3.7079062461853027,
        "learning_rate": 0.00019305755590932338,
        "epoch": 0.7072062001639466,
        "step": 9490
    },
    {
        "loss": 1.9926,
        "grad_norm": 4.64948034286499,
        "learning_rate": 0.00019303177200921171,
        "epoch": 0.7072807213652285,
        "step": 9491
    },
    {
        "loss": 2.6189,
        "grad_norm": 3.2165157794952393,
        "learning_rate": 0.0001930059420460236,
        "epoch": 0.7073552425665102,
        "step": 9492
    },
    {
        "loss": 2.3162,
        "grad_norm": 2.0695831775665283,
        "learning_rate": 0.00019298006603254818,
        "epoch": 0.707429763767792,
        "step": 9493
    },
    {
        "loss": 1.8531,
        "grad_norm": 3.368459701538086,
        "learning_rate": 0.00019295414398159765,
        "epoch": 0.7075042849690737,
        "step": 9494
    },
    {
        "loss": 2.3001,
        "grad_norm": 3.375471591949463,
        "learning_rate": 0.00019292817590600665,
        "epoch": 0.7075788061703555,
        "step": 9495
    },
    {
        "loss": 2.5458,
        "grad_norm": 2.1368656158447266,
        "learning_rate": 0.00019290216181863301,
        "epoch": 0.7076533273716372,
        "step": 9496
    },
    {
        "loss": 2.4171,
        "grad_norm": 3.1278932094573975,
        "learning_rate": 0.00019287610173235708,
        "epoch": 0.707727848572919,
        "step": 9497
    },
    {
        "loss": 1.4518,
        "grad_norm": 2.787856340408325,
        "learning_rate": 0.0001928499956600821,
        "epoch": 0.7078023697742007,
        "step": 9498
    },
    {
        "loss": 2.6608,
        "grad_norm": 3.34834361076355,
        "learning_rate": 0.0001928238436147339,
        "epoch": 0.7078768909754826,
        "step": 9499
    },
    {
        "loss": 2.3183,
        "grad_norm": 3.36415696144104,
        "learning_rate": 0.00019279764560926142,
        "epoch": 0.7079514121767643,
        "step": 9500
    },
    {
        "loss": 2.2882,
        "grad_norm": 3.6475279331207275,
        "learning_rate": 0.000192771401656636,
        "epoch": 0.7080259333780461,
        "step": 9501
    },
    {
        "loss": 2.5174,
        "grad_norm": 2.9776723384857178,
        "learning_rate": 0.000192745111769852,
        "epoch": 0.7081004545793278,
        "step": 9502
    },
    {
        "loss": 2.5279,
        "grad_norm": 2.512939453125,
        "learning_rate": 0.00019271877596192634,
        "epoch": 0.7081749757806096,
        "step": 9503
    },
    {
        "loss": 2.4741,
        "grad_norm": 3.876948833465576,
        "learning_rate": 0.00019269239424589868,
        "epoch": 0.7082494969818913,
        "step": 9504
    },
    {
        "loss": 2.1409,
        "grad_norm": 3.808596134185791,
        "learning_rate": 0.00019266596663483163,
        "epoch": 0.7083240181831731,
        "step": 9505
    },
    {
        "loss": 1.9666,
        "grad_norm": 4.215734004974365,
        "learning_rate": 0.00019263949314181017,
        "epoch": 0.7083985393844549,
        "step": 9506
    },
    {
        "loss": 2.3102,
        "grad_norm": 2.0746748447418213,
        "learning_rate": 0.00019261297377994233,
        "epoch": 0.7084730605857367,
        "step": 9507
    },
    {
        "loss": 1.8393,
        "grad_norm": 3.1035540103912354,
        "learning_rate": 0.0001925864085623587,
        "epoch": 0.7085475817870184,
        "step": 9508
    },
    {
        "loss": 1.9112,
        "grad_norm": 2.926543712615967,
        "learning_rate": 0.00019255979750221254,
        "epoch": 0.7086221029883002,
        "step": 9509
    },
    {
        "loss": 2.4598,
        "grad_norm": 3.4792685508728027,
        "learning_rate": 0.0001925331406126799,
        "epoch": 0.7086966241895819,
        "step": 9510
    },
    {
        "loss": 2.8347,
        "grad_norm": 2.4816300868988037,
        "learning_rate": 0.00019250643790695942,
        "epoch": 0.7087711453908637,
        "step": 9511
    },
    {
        "loss": 2.3878,
        "grad_norm": 2.1423304080963135,
        "learning_rate": 0.00019247968939827256,
        "epoch": 0.7088456665921454,
        "step": 9512
    },
    {
        "loss": 2.689,
        "grad_norm": 2.7382118701934814,
        "learning_rate": 0.00019245289509986336,
        "epoch": 0.7089201877934272,
        "step": 9513
    },
    {
        "loss": 1.6979,
        "grad_norm": 1.9802125692367554,
        "learning_rate": 0.00019242605502499855,
        "epoch": 0.708994708994709,
        "step": 9514
    },
    {
        "loss": 2.6394,
        "grad_norm": 3.43473482131958,
        "learning_rate": 0.00019239916918696747,
        "epoch": 0.7090692301959908,
        "step": 9515
    },
    {
        "loss": 2.3793,
        "grad_norm": 3.016143321990967,
        "learning_rate": 0.00019237223759908226,
        "epoch": 0.7091437513972725,
        "step": 9516
    },
    {
        "loss": 2.3421,
        "grad_norm": 3.127368211746216,
        "learning_rate": 0.00019234526027467764,
        "epoch": 0.7092182725985543,
        "step": 9517
    },
    {
        "loss": 2.3224,
        "grad_norm": 4.891345500946045,
        "learning_rate": 0.00019231823722711092,
        "epoch": 0.709292793799836,
        "step": 9518
    },
    {
        "loss": 2.7757,
        "grad_norm": 2.7224738597869873,
        "learning_rate": 0.00019229116846976202,
        "epoch": 0.7093673150011178,
        "step": 9519
    },
    {
        "loss": 1.8629,
        "grad_norm": 4.197668552398682,
        "learning_rate": 0.00019226405401603374,
        "epoch": 0.7094418362023995,
        "step": 9520
    },
    {
        "loss": 1.867,
        "grad_norm": 4.039157390594482,
        "learning_rate": 0.0001922368938793512,
        "epoch": 0.7095163574036814,
        "step": 9521
    },
    {
        "loss": 2.6026,
        "grad_norm": 2.8266799449920654,
        "learning_rate": 0.00019220968807316232,
        "epoch": 0.7095908786049631,
        "step": 9522
    },
    {
        "loss": 3.0857,
        "grad_norm": 2.2182235717773438,
        "learning_rate": 0.00019218243661093761,
        "epoch": 0.7096653998062449,
        "step": 9523
    },
    {
        "loss": 2.4618,
        "grad_norm": 2.4922423362731934,
        "learning_rate": 0.00019215513950617013,
        "epoch": 0.7097399210075266,
        "step": 9524
    },
    {
        "loss": 2.6503,
        "grad_norm": 2.9026663303375244,
        "learning_rate": 0.00019212779677237562,
        "epoch": 0.7098144422088084,
        "step": 9525
    },
    {
        "loss": 2.6627,
        "grad_norm": 3.2621994018554688,
        "learning_rate": 0.00019210040842309228,
        "epoch": 0.7098889634100902,
        "step": 9526
    },
    {
        "loss": 2.2053,
        "grad_norm": 2.842406988143921,
        "learning_rate": 0.000192072974471881,
        "epoch": 0.7099634846113719,
        "step": 9527
    },
    {
        "loss": 2.1497,
        "grad_norm": 2.474775552749634,
        "learning_rate": 0.00019204549493232528,
        "epoch": 0.7100380058126537,
        "step": 9528
    },
    {
        "loss": 2.8449,
        "grad_norm": 2.888080358505249,
        "learning_rate": 0.00019201796981803106,
        "epoch": 0.7101125270139355,
        "step": 9529
    },
    {
        "loss": 2.4246,
        "grad_norm": 3.0190467834472656,
        "learning_rate": 0.00019199039914262703,
        "epoch": 0.7101870482152173,
        "step": 9530
    },
    {
        "loss": 2.5896,
        "grad_norm": 2.7536425590515137,
        "learning_rate": 0.00019196278291976423,
        "epoch": 0.710261569416499,
        "step": 9531
    },
    {
        "loss": 2.6905,
        "grad_norm": 2.9139254093170166,
        "learning_rate": 0.00019193512116311642,
        "epoch": 0.7103360906177808,
        "step": 9532
    },
    {
        "loss": 2.5765,
        "grad_norm": 3.3404250144958496,
        "learning_rate": 0.00019190741388637982,
        "epoch": 0.7104106118190625,
        "step": 9533
    },
    {
        "loss": 2.7103,
        "grad_norm": 2.9168434143066406,
        "learning_rate": 0.00019187966110327322,
        "epoch": 0.7104851330203443,
        "step": 9534
    },
    {
        "loss": 1.9931,
        "grad_norm": 2.1981818675994873,
        "learning_rate": 0.00019185186282753787,
        "epoch": 0.710559654221626,
        "step": 9535
    },
    {
        "loss": 2.8949,
        "grad_norm": 1.9156773090362549,
        "learning_rate": 0.00019182401907293778,
        "epoch": 0.7106341754229079,
        "step": 9536
    },
    {
        "loss": 2.2999,
        "grad_norm": 2.8352551460266113,
        "learning_rate": 0.00019179612985325908,
        "epoch": 0.7107086966241896,
        "step": 9537
    },
    {
        "loss": 2.5655,
        "grad_norm": 5.395220756530762,
        "learning_rate": 0.00019176819518231078,
        "epoch": 0.7107832178254714,
        "step": 9538
    },
    {
        "loss": 2.9093,
        "grad_norm": 3.2812552452087402,
        "learning_rate": 0.00019174021507392417,
        "epoch": 0.7108577390267531,
        "step": 9539
    },
    {
        "loss": 2.4801,
        "grad_norm": 3.0145585536956787,
        "learning_rate": 0.00019171218954195323,
        "epoch": 0.7109322602280349,
        "step": 9540
    },
    {
        "loss": 1.951,
        "grad_norm": 5.3080525398254395,
        "learning_rate": 0.00019168411860027423,
        "epoch": 0.7110067814293166,
        "step": 9541
    },
    {
        "loss": 2.2257,
        "grad_norm": 3.20180606842041,
        "learning_rate": 0.00019165600226278597,
        "epoch": 0.7110813026305984,
        "step": 9542
    },
    {
        "loss": 2.5323,
        "grad_norm": 2.8647170066833496,
        "learning_rate": 0.00019162784054340987,
        "epoch": 0.7111558238318801,
        "step": 9543
    },
    {
        "loss": 2.6014,
        "grad_norm": 2.2447168827056885,
        "learning_rate": 0.00019159963345608967,
        "epoch": 0.711230345033162,
        "step": 9544
    },
    {
        "loss": 2.0029,
        "grad_norm": 2.6489667892456055,
        "learning_rate": 0.0001915713810147917,
        "epoch": 0.7113048662344437,
        "step": 9545
    },
    {
        "loss": 2.2935,
        "grad_norm": 3.8600692749023438,
        "learning_rate": 0.00019154308323350453,
        "epoch": 0.7113793874357255,
        "step": 9546
    },
    {
        "loss": 1.6989,
        "grad_norm": 3.9122886657714844,
        "learning_rate": 0.0001915147401262394,
        "epoch": 0.7114539086370072,
        "step": 9547
    },
    {
        "loss": 1.8052,
        "grad_norm": 3.181812286376953,
        "learning_rate": 0.00019148635170702992,
        "epoch": 0.711528429838289,
        "step": 9548
    },
    {
        "loss": 2.6459,
        "grad_norm": 2.606261730194092,
        "learning_rate": 0.00019145791798993206,
        "epoch": 0.7116029510395707,
        "step": 9549
    },
    {
        "loss": 2.3211,
        "grad_norm": 2.3295106887817383,
        "learning_rate": 0.00019142943898902437,
        "epoch": 0.7116774722408525,
        "step": 9550
    },
    {
        "loss": 2.7434,
        "grad_norm": 2.379629611968994,
        "learning_rate": 0.0001914009147184077,
        "epoch": 0.7117519934421342,
        "step": 9551
    },
    {
        "loss": 2.1657,
        "grad_norm": 2.1809637546539307,
        "learning_rate": 0.0001913723451922053,
        "epoch": 0.7118265146434161,
        "step": 9552
    },
    {
        "loss": 2.3162,
        "grad_norm": 3.5829966068267822,
        "learning_rate": 0.00019134373042456292,
        "epoch": 0.7119010358446978,
        "step": 9553
    },
    {
        "loss": 2.6038,
        "grad_norm": 3.910351514816284,
        "learning_rate": 0.00019131507042964875,
        "epoch": 0.7119755570459796,
        "step": 9554
    },
    {
        "loss": 2.4462,
        "grad_norm": 2.764373302459717,
        "learning_rate": 0.00019128636522165314,
        "epoch": 0.7120500782472613,
        "step": 9555
    },
    {
        "loss": 1.5241,
        "grad_norm": 3.9807565212249756,
        "learning_rate": 0.00019125761481478916,
        "epoch": 0.7121245994485431,
        "step": 9556
    },
    {
        "loss": 2.4574,
        "grad_norm": 3.413949966430664,
        "learning_rate": 0.00019122881922329184,
        "epoch": 0.7121991206498248,
        "step": 9557
    },
    {
        "loss": 2.2583,
        "grad_norm": 3.270127296447754,
        "learning_rate": 0.00019119997846141898,
        "epoch": 0.7122736418511066,
        "step": 9558
    },
    {
        "loss": 2.9654,
        "grad_norm": 3.369415760040283,
        "learning_rate": 0.00019117109254345062,
        "epoch": 0.7123481630523884,
        "step": 9559
    },
    {
        "loss": 2.326,
        "grad_norm": 2.665829658508301,
        "learning_rate": 0.00019114216148368896,
        "epoch": 0.7124226842536702,
        "step": 9560
    },
    {
        "loss": 2.1942,
        "grad_norm": 3.209808826446533,
        "learning_rate": 0.0001911131852964589,
        "epoch": 0.712497205454952,
        "step": 9561
    },
    {
        "loss": 2.7299,
        "grad_norm": 3.866198778152466,
        "learning_rate": 0.00019108416399610733,
        "epoch": 0.7125717266562337,
        "step": 9562
    },
    {
        "loss": 2.7124,
        "grad_norm": 3.2379236221313477,
        "learning_rate": 0.0001910550975970038,
        "epoch": 0.7126462478575155,
        "step": 9563
    },
    {
        "loss": 1.307,
        "grad_norm": 3.8764514923095703,
        "learning_rate": 0.00019102598611353997,
        "epoch": 0.7127207690587972,
        "step": 9564
    },
    {
        "loss": 1.7931,
        "grad_norm": 2.625408411026001,
        "learning_rate": 0.00019099682956012987,
        "epoch": 0.712795290260079,
        "step": 9565
    },
    {
        "loss": 2.205,
        "grad_norm": 3.787893533706665,
        "learning_rate": 0.00019096762795120983,
        "epoch": 0.7128698114613607,
        "step": 9566
    },
    {
        "loss": 2.5626,
        "grad_norm": 3.1173503398895264,
        "learning_rate": 0.00019093838130123864,
        "epoch": 0.7129443326626426,
        "step": 9567
    },
    {
        "loss": 1.6758,
        "grad_norm": 4.604046821594238,
        "learning_rate": 0.0001909090896246972,
        "epoch": 0.7130188538639243,
        "step": 9568
    },
    {
        "loss": 2.5139,
        "grad_norm": 2.601956844329834,
        "learning_rate": 0.00019087975293608878,
        "epoch": 0.7130933750652061,
        "step": 9569
    },
    {
        "loss": 2.7347,
        "grad_norm": 1.8186105489730835,
        "learning_rate": 0.00019085037124993886,
        "epoch": 0.7131678962664878,
        "step": 9570
    },
    {
        "loss": 1.7849,
        "grad_norm": 3.0201969146728516,
        "learning_rate": 0.00019082094458079544,
        "epoch": 0.7132424174677696,
        "step": 9571
    },
    {
        "loss": 1.7284,
        "grad_norm": 3.7593307495117188,
        "learning_rate": 0.00019079147294322847,
        "epoch": 0.7133169386690513,
        "step": 9572
    },
    {
        "loss": 2.2108,
        "grad_norm": 3.036109209060669,
        "learning_rate": 0.00019076195635183045,
        "epoch": 0.7133914598703331,
        "step": 9573
    },
    {
        "loss": 2.7489,
        "grad_norm": 2.7673285007476807,
        "learning_rate": 0.000190732394821216,
        "epoch": 0.7134659810716149,
        "step": 9574
    },
    {
        "loss": 2.2264,
        "grad_norm": 3.80737566947937,
        "learning_rate": 0.00019070278836602183,
        "epoch": 0.7135405022728967,
        "step": 9575
    },
    {
        "loss": 2.3431,
        "grad_norm": 3.3051257133483887,
        "learning_rate": 0.00019067313700090735,
        "epoch": 0.7136150234741784,
        "step": 9576
    },
    {
        "loss": 2.56,
        "grad_norm": 2.658452272415161,
        "learning_rate": 0.00019064344074055362,
        "epoch": 0.7136895446754602,
        "step": 9577
    },
    {
        "loss": 2.6372,
        "grad_norm": 2.8876149654388428,
        "learning_rate": 0.00019061369959966445,
        "epoch": 0.7137640658767419,
        "step": 9578
    },
    {
        "loss": 2.852,
        "grad_norm": 4.038264274597168,
        "learning_rate": 0.0001905839135929656,
        "epoch": 0.7138385870780237,
        "step": 9579
    },
    {
        "loss": 2.9095,
        "grad_norm": 2.259700298309326,
        "learning_rate": 0.00019055408273520498,
        "epoch": 0.7139131082793054,
        "step": 9580
    },
    {
        "loss": 2.2165,
        "grad_norm": 3.9542877674102783,
        "learning_rate": 0.000190524207041153,
        "epoch": 0.7139876294805872,
        "step": 9581
    },
    {
        "loss": 2.4755,
        "grad_norm": 2.008054733276367,
        "learning_rate": 0.00019049428652560192,
        "epoch": 0.714062150681869,
        "step": 9582
    },
    {
        "loss": 3.4282,
        "grad_norm": 3.908881187438965,
        "learning_rate": 0.00019046432120336656,
        "epoch": 0.7141366718831508,
        "step": 9583
    },
    {
        "loss": 2.4223,
        "grad_norm": 4.228017330169678,
        "learning_rate": 0.0001904343110892836,
        "epoch": 0.7142111930844325,
        "step": 9584
    },
    {
        "loss": 2.549,
        "grad_norm": 2.0858376026153564,
        "learning_rate": 0.00019040425619821206,
        "epoch": 0.7142857142857143,
        "step": 9585
    },
    {
        "loss": 2.4865,
        "grad_norm": 2.4209792613983154,
        "learning_rate": 0.00019037415654503308,
        "epoch": 0.714360235486996,
        "step": 9586
    },
    {
        "loss": 2.6089,
        "grad_norm": 2.751316785812378,
        "learning_rate": 0.00019034401214465004,
        "epoch": 0.7144347566882778,
        "step": 9587
    },
    {
        "loss": 2.5268,
        "grad_norm": 3.1612164974212646,
        "learning_rate": 0.0001903138230119884,
        "epoch": 0.7145092778895595,
        "step": 9588
    },
    {
        "loss": 2.5413,
        "grad_norm": 2.2897865772247314,
        "learning_rate": 0.00019028358916199577,
        "epoch": 0.7145837990908414,
        "step": 9589
    },
    {
        "loss": 2.6765,
        "grad_norm": 3.5824074745178223,
        "learning_rate": 0.00019025331060964183,
        "epoch": 0.7146583202921231,
        "step": 9590
    },
    {
        "loss": 1.6588,
        "grad_norm": 3.2234787940979004,
        "learning_rate": 0.0001902229873699187,
        "epoch": 0.7147328414934049,
        "step": 9591
    },
    {
        "loss": 2.9871,
        "grad_norm": 3.475970506668091,
        "learning_rate": 0.00019019261945784025,
        "epoch": 0.7148073626946866,
        "step": 9592
    },
    {
        "loss": 2.0785,
        "grad_norm": 2.967899799346924,
        "learning_rate": 0.0001901622068884426,
        "epoch": 0.7148818838959684,
        "step": 9593
    },
    {
        "loss": 2.6827,
        "grad_norm": 3.0940775871276855,
        "learning_rate": 0.00019013174967678415,
        "epoch": 0.7149564050972501,
        "step": 9594
    },
    {
        "loss": 2.7129,
        "grad_norm": 3.076645851135254,
        "learning_rate": 0.00019010124783794513,
        "epoch": 0.7150309262985319,
        "step": 9595
    },
    {
        "loss": 2.6149,
        "grad_norm": 3.7498221397399902,
        "learning_rate": 0.00019007070138702813,
        "epoch": 0.7151054474998138,
        "step": 9596
    },
    {
        "loss": 1.8217,
        "grad_norm": 3.4200236797332764,
        "learning_rate": 0.00019004011033915753,
        "epoch": 0.7151799687010955,
        "step": 9597
    },
    {
        "loss": 2.6572,
        "grad_norm": 2.5695343017578125,
        "learning_rate": 0.00019000947470948013,
        "epoch": 0.7152544899023773,
        "step": 9598
    },
    {
        "loss": 1.8747,
        "grad_norm": 4.2453837394714355,
        "learning_rate": 0.00018997879451316453,
        "epoch": 0.715329011103659,
        "step": 9599
    },
    {
        "loss": 2.7175,
        "grad_norm": 1.9301199913024902,
        "learning_rate": 0.00018994806976540147,
        "epoch": 0.7154035323049408,
        "step": 9600
    },
    {
        "loss": 1.9673,
        "grad_norm": 2.0850701332092285,
        "learning_rate": 0.0001899173004814039,
        "epoch": 0.7154780535062225,
        "step": 9601
    },
    {
        "loss": 2.8105,
        "grad_norm": 3.4178402423858643,
        "learning_rate": 0.00018988648667640662,
        "epoch": 0.7155525747075043,
        "step": 9602
    },
    {
        "loss": 1.9306,
        "grad_norm": 3.371764659881592,
        "learning_rate": 0.00018985562836566652,
        "epoch": 0.715627095908786,
        "step": 9603
    },
    {
        "loss": 2.3616,
        "grad_norm": 2.311357259750366,
        "learning_rate": 0.00018982472556446268,
        "epoch": 0.7157016171100679,
        "step": 9604
    },
    {
        "loss": 2.1432,
        "grad_norm": 3.483196973800659,
        "learning_rate": 0.000189793778288096,
        "epoch": 0.7157761383113496,
        "step": 9605
    },
    {
        "loss": 2.3881,
        "grad_norm": 3.9003806114196777,
        "learning_rate": 0.00018976278655188943,
        "epoch": 0.7158506595126314,
        "step": 9606
    },
    {
        "loss": 1.8499,
        "grad_norm": 3.092660665512085,
        "learning_rate": 0.00018973175037118823,
        "epoch": 0.7159251807139131,
        "step": 9607
    },
    {
        "loss": 2.51,
        "grad_norm": 2.7149128913879395,
        "learning_rate": 0.00018970066976135912,
        "epoch": 0.7159997019151949,
        "step": 9608
    },
    {
        "loss": 2.1816,
        "grad_norm": 3.1624562740325928,
        "learning_rate": 0.00018966954473779134,
        "epoch": 0.7160742231164766,
        "step": 9609
    },
    {
        "loss": 2.1526,
        "grad_norm": 3.0938498973846436,
        "learning_rate": 0.00018963837531589577,
        "epoch": 0.7161487443177584,
        "step": 9610
    },
    {
        "loss": 2.5101,
        "grad_norm": 2.4023280143737793,
        "learning_rate": 0.00018960716151110554,
        "epoch": 0.7162232655190401,
        "step": 9611
    },
    {
        "loss": 2.5137,
        "grad_norm": 2.4675891399383545,
        "learning_rate": 0.0001895759033388756,
        "epoch": 0.716297786720322,
        "step": 9612
    },
    {
        "loss": 2.4286,
        "grad_norm": 2.74509859085083,
        "learning_rate": 0.0001895446008146828,
        "epoch": 0.7163723079216037,
        "step": 9613
    },
    {
        "loss": 1.9624,
        "grad_norm": 3.2746925354003906,
        "learning_rate": 0.00018951325395402617,
        "epoch": 0.7164468291228855,
        "step": 9614
    },
    {
        "loss": 2.024,
        "grad_norm": 4.42682409286499,
        "learning_rate": 0.00018948186277242644,
        "epoch": 0.7165213503241672,
        "step": 9615
    },
    {
        "loss": 1.6186,
        "grad_norm": 3.1371731758117676,
        "learning_rate": 0.00018945042728542663,
        "epoch": 0.716595871525449,
        "step": 9616
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.3840935230255127,
        "learning_rate": 0.00018941894750859118,
        "epoch": 0.7166703927267307,
        "step": 9617
    },
    {
        "loss": 2.7836,
        "grad_norm": 2.513742208480835,
        "learning_rate": 0.000189387423457507,
        "epoch": 0.7167449139280125,
        "step": 9618
    },
    {
        "loss": 2.4866,
        "grad_norm": 3.128894329071045,
        "learning_rate": 0.00018935585514778255,
        "epoch": 0.7168194351292942,
        "step": 9619
    },
    {
        "loss": 2.5244,
        "grad_norm": 3.0246129035949707,
        "learning_rate": 0.0001893242425950484,
        "epoch": 0.7168939563305761,
        "step": 9620
    },
    {
        "loss": 2.0576,
        "grad_norm": 3.325160264968872,
        "learning_rate": 0.00018929258581495685,
        "epoch": 0.7169684775318578,
        "step": 9621
    },
    {
        "loss": 1.8335,
        "grad_norm": 3.692049026489258,
        "learning_rate": 0.00018926088482318235,
        "epoch": 0.7170429987331396,
        "step": 9622
    },
    {
        "loss": 2.1331,
        "grad_norm": 2.1869359016418457,
        "learning_rate": 0.00018922913963542102,
        "epoch": 0.7171175199344213,
        "step": 9623
    },
    {
        "loss": 2.3815,
        "grad_norm": 3.441835641860962,
        "learning_rate": 0.000189197350267391,
        "epoch": 0.7171920411357031,
        "step": 9624
    },
    {
        "loss": 2.241,
        "grad_norm": 3.6990485191345215,
        "learning_rate": 0.00018916551673483215,
        "epoch": 0.7172665623369848,
        "step": 9625
    },
    {
        "loss": 1.7038,
        "grad_norm": 2.054767370223999,
        "learning_rate": 0.00018913363905350637,
        "epoch": 0.7173410835382666,
        "step": 9626
    },
    {
        "loss": 2.678,
        "grad_norm": 2.8911805152893066,
        "learning_rate": 0.00018910171723919743,
        "epoch": 0.7174156047395484,
        "step": 9627
    },
    {
        "loss": 2.6759,
        "grad_norm": 4.532327651977539,
        "learning_rate": 0.0001890697513077106,
        "epoch": 0.7174901259408302,
        "step": 9628
    },
    {
        "loss": 2.3747,
        "grad_norm": 3.835024118423462,
        "learning_rate": 0.00018903774127487351,
        "epoch": 0.7175646471421119,
        "step": 9629
    },
    {
        "loss": 2.7716,
        "grad_norm": 2.4557502269744873,
        "learning_rate": 0.0001890056871565353,
        "epoch": 0.7176391683433937,
        "step": 9630
    },
    {
        "loss": 2.4254,
        "grad_norm": 2.574066162109375,
        "learning_rate": 0.00018897358896856695,
        "epoch": 0.7177136895446755,
        "step": 9631
    },
    {
        "loss": 2.7056,
        "grad_norm": 2.6781234741210938,
        "learning_rate": 0.0001889414467268614,
        "epoch": 0.7177882107459572,
        "step": 9632
    },
    {
        "loss": 1.6389,
        "grad_norm": 2.128700017929077,
        "learning_rate": 0.00018890926044733326,
        "epoch": 0.717862731947239,
        "step": 9633
    },
    {
        "loss": 2.5111,
        "grad_norm": 3.3310885429382324,
        "learning_rate": 0.00018887703014591907,
        "epoch": 0.7179372531485207,
        "step": 9634
    },
    {
        "loss": 2.7452,
        "grad_norm": 2.8504066467285156,
        "learning_rate": 0.00018884475583857713,
        "epoch": 0.7180117743498026,
        "step": 9635
    },
    {
        "loss": 2.4607,
        "grad_norm": 1.7091832160949707,
        "learning_rate": 0.00018881243754128747,
        "epoch": 0.7180862955510843,
        "step": 9636
    },
    {
        "loss": 2.4302,
        "grad_norm": 1.6816786527633667,
        "learning_rate": 0.00018878007527005185,
        "epoch": 0.7181608167523661,
        "step": 9637
    },
    {
        "loss": 2.3895,
        "grad_norm": 2.4160284996032715,
        "learning_rate": 0.00018874766904089404,
        "epoch": 0.7182353379536478,
        "step": 9638
    },
    {
        "loss": 2.6211,
        "grad_norm": 2.0914273262023926,
        "learning_rate": 0.00018871521886985935,
        "epoch": 0.7183098591549296,
        "step": 9639
    },
    {
        "loss": 1.815,
        "grad_norm": 3.3189122676849365,
        "learning_rate": 0.00018868272477301495,
        "epoch": 0.7183843803562113,
        "step": 9640
    },
    {
        "loss": 2.1721,
        "grad_norm": 3.3749160766601562,
        "learning_rate": 0.00018865018676644964,
        "epoch": 0.7184589015574931,
        "step": 9641
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.5809013843536377,
        "learning_rate": 0.0001886176048662742,
        "epoch": 0.7185334227587749,
        "step": 9642
    },
    {
        "loss": 3.0798,
        "grad_norm": 2.881223440170288,
        "learning_rate": 0.00018858497908862086,
        "epoch": 0.7186079439600567,
        "step": 9643
    },
    {
        "loss": 3.0971,
        "grad_norm": 2.2302751541137695,
        "learning_rate": 0.00018855230944964379,
        "epoch": 0.7186824651613384,
        "step": 9644
    },
    {
        "loss": 1.8084,
        "grad_norm": 2.1779797077178955,
        "learning_rate": 0.0001885195959655188,
        "epoch": 0.7187569863626202,
        "step": 9645
    },
    {
        "loss": 1.9482,
        "grad_norm": 2.9940760135650635,
        "learning_rate": 0.00018848683865244333,
        "epoch": 0.7188315075639019,
        "step": 9646
    },
    {
        "loss": 2.3099,
        "grad_norm": 2.8090438842773438,
        "learning_rate": 0.00018845403752663678,
        "epoch": 0.7189060287651837,
        "step": 9647
    },
    {
        "loss": 2.3668,
        "grad_norm": 3.4322752952575684,
        "learning_rate": 0.00018842119260433982,
        "epoch": 0.7189805499664654,
        "step": 9648
    },
    {
        "loss": 2.5266,
        "grad_norm": 2.729302406311035,
        "learning_rate": 0.0001883883039018152,
        "epoch": 0.7190550711677473,
        "step": 9649
    },
    {
        "loss": 1.6535,
        "grad_norm": 4.733497142791748,
        "learning_rate": 0.00018835537143534716,
        "epoch": 0.719129592369029,
        "step": 9650
    },
    {
        "loss": 2.5704,
        "grad_norm": 2.5901095867156982,
        "learning_rate": 0.00018832239522124157,
        "epoch": 0.7192041135703108,
        "step": 9651
    },
    {
        "loss": 2.6043,
        "grad_norm": 3.6195895671844482,
        "learning_rate": 0.00018828937527582616,
        "epoch": 0.7192786347715925,
        "step": 9652
    },
    {
        "loss": 2.7864,
        "grad_norm": 2.0194284915924072,
        "learning_rate": 0.00018825631161545014,
        "epoch": 0.7193531559728743,
        "step": 9653
    },
    {
        "loss": 2.2644,
        "grad_norm": 2.396942377090454,
        "learning_rate": 0.00018822320425648434,
        "epoch": 0.719427677174156,
        "step": 9654
    },
    {
        "loss": 2.2129,
        "grad_norm": 2.649859666824341,
        "learning_rate": 0.00018819005321532142,
        "epoch": 0.7195021983754378,
        "step": 9655
    },
    {
        "loss": 1.8521,
        "grad_norm": 2.047332286834717,
        "learning_rate": 0.00018815685850837545,
        "epoch": 0.7195767195767195,
        "step": 9656
    },
    {
        "loss": 2.7875,
        "grad_norm": 2.9486472606658936,
        "learning_rate": 0.00018812362015208222,
        "epoch": 0.7196512407780014,
        "step": 9657
    },
    {
        "loss": 2.5364,
        "grad_norm": 2.1921608448028564,
        "learning_rate": 0.0001880903381628993,
        "epoch": 0.7197257619792831,
        "step": 9658
    },
    {
        "loss": 2.5952,
        "grad_norm": 1.8883018493652344,
        "learning_rate": 0.00018805701255730537,
        "epoch": 0.7198002831805649,
        "step": 9659
    },
    {
        "loss": 2.4062,
        "grad_norm": 3.178684949874878,
        "learning_rate": 0.0001880236433518013,
        "epoch": 0.7198748043818466,
        "step": 9660
    },
    {
        "loss": 1.7279,
        "grad_norm": 2.3682990074157715,
        "learning_rate": 0.0001879902305629091,
        "epoch": 0.7199493255831284,
        "step": 9661
    },
    {
        "loss": 2.3526,
        "grad_norm": 2.9066779613494873,
        "learning_rate": 0.0001879567742071727,
        "epoch": 0.7200238467844101,
        "step": 9662
    },
    {
        "loss": 2.9092,
        "grad_norm": 3.4996237754821777,
        "learning_rate": 0.00018792327430115736,
        "epoch": 0.7200983679856919,
        "step": 9663
    },
    {
        "loss": 2.6259,
        "grad_norm": 2.776916265487671,
        "learning_rate": 0.0001878897308614499,
        "epoch": 0.7201728891869736,
        "step": 9664
    },
    {
        "loss": 2.2417,
        "grad_norm": 3.0874316692352295,
        "learning_rate": 0.00018785614390465894,
        "epoch": 0.7202474103882555,
        "step": 9665
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.9086830615997314,
        "learning_rate": 0.0001878225134474143,
        "epoch": 0.7203219315895373,
        "step": 9666
    },
    {
        "loss": 2.0763,
        "grad_norm": 2.8333277702331543,
        "learning_rate": 0.00018778883950636775,
        "epoch": 0.720396452790819,
        "step": 9667
    },
    {
        "loss": 2.483,
        "grad_norm": 1.9498237371444702,
        "learning_rate": 0.0001877551220981921,
        "epoch": 0.7204709739921008,
        "step": 9668
    },
    {
        "loss": 2.3729,
        "grad_norm": 2.173448324203491,
        "learning_rate": 0.00018772136123958215,
        "epoch": 0.7205454951933825,
        "step": 9669
    },
    {
        "loss": 2.6882,
        "grad_norm": 2.217043161392212,
        "learning_rate": 0.0001876875569472539,
        "epoch": 0.7206200163946643,
        "step": 9670
    },
    {
        "loss": 2.0869,
        "grad_norm": 4.288941860198975,
        "learning_rate": 0.00018765370923794493,
        "epoch": 0.720694537595946,
        "step": 9671
    },
    {
        "loss": 3.0007,
        "grad_norm": 3.1998612880706787,
        "learning_rate": 0.0001876198181284145,
        "epoch": 0.7207690587972279,
        "step": 9672
    },
    {
        "loss": 2.4756,
        "grad_norm": 3.130976915359497,
        "learning_rate": 0.00018758588363544307,
        "epoch": 0.7208435799985096,
        "step": 9673
    },
    {
        "loss": 2.0724,
        "grad_norm": 2.85095477104187,
        "learning_rate": 0.00018755190577583272,
        "epoch": 0.7209181011997914,
        "step": 9674
    },
    {
        "loss": 1.8277,
        "grad_norm": 3.2608985900878906,
        "learning_rate": 0.0001875178845664071,
        "epoch": 0.7209926224010731,
        "step": 9675
    },
    {
        "loss": 1.883,
        "grad_norm": 2.737215995788574,
        "learning_rate": 0.00018748382002401119,
        "epoch": 0.7210671436023549,
        "step": 9676
    },
    {
        "loss": 2.4156,
        "grad_norm": 2.6561145782470703,
        "learning_rate": 0.00018744971216551134,
        "epoch": 0.7211416648036366,
        "step": 9677
    },
    {
        "loss": 2.6203,
        "grad_norm": 3.3739006519317627,
        "learning_rate": 0.00018741556100779574,
        "epoch": 0.7212161860049184,
        "step": 9678
    },
    {
        "loss": 1.6331,
        "grad_norm": 4.031800270080566,
        "learning_rate": 0.0001873813665677734,
        "epoch": 0.7212907072062001,
        "step": 9679
    },
    {
        "loss": 2.6947,
        "grad_norm": 2.383396863937378,
        "learning_rate": 0.00018734712886237537,
        "epoch": 0.721365228407482,
        "step": 9680
    },
    {
        "loss": 2.4758,
        "grad_norm": 3.4347589015960693,
        "learning_rate": 0.0001873128479085537,
        "epoch": 0.7214397496087637,
        "step": 9681
    },
    {
        "loss": 2.2561,
        "grad_norm": 3.435129165649414,
        "learning_rate": 0.00018727852372328214,
        "epoch": 0.7215142708100455,
        "step": 9682
    },
    {
        "loss": 2.8263,
        "grad_norm": 2.6395461559295654,
        "learning_rate": 0.00018724415632355567,
        "epoch": 0.7215887920113272,
        "step": 9683
    },
    {
        "loss": 2.3118,
        "grad_norm": 4.8952813148498535,
        "learning_rate": 0.00018720974572639066,
        "epoch": 0.721663313212609,
        "step": 9684
    },
    {
        "loss": 2.6358,
        "grad_norm": 3.463047742843628,
        "learning_rate": 0.000187175291948825,
        "epoch": 0.7217378344138907,
        "step": 9685
    },
    {
        "loss": 2.5227,
        "grad_norm": 2.650803565979004,
        "learning_rate": 0.00018714079500791787,
        "epoch": 0.7218123556151725,
        "step": 9686
    },
    {
        "loss": 2.3322,
        "grad_norm": 2.2802767753601074,
        "learning_rate": 0.0001871062549207498,
        "epoch": 0.7218868768164542,
        "step": 9687
    },
    {
        "loss": 2.5699,
        "grad_norm": 2.7779581546783447,
        "learning_rate": 0.00018707167170442266,
        "epoch": 0.7219613980177361,
        "step": 9688
    },
    {
        "loss": 2.5096,
        "grad_norm": 3.122253656387329,
        "learning_rate": 0.00018703704537605988,
        "epoch": 0.7220359192190178,
        "step": 9689
    },
    {
        "loss": 2.4875,
        "grad_norm": 4.20591402053833,
        "learning_rate": 0.00018700237595280602,
        "epoch": 0.7221104404202996,
        "step": 9690
    },
    {
        "loss": 2.4633,
        "grad_norm": 2.6077632904052734,
        "learning_rate": 0.00018696766345182705,
        "epoch": 0.7221849616215813,
        "step": 9691
    },
    {
        "loss": 2.262,
        "grad_norm": 2.380913257598877,
        "learning_rate": 0.0001869329078903102,
        "epoch": 0.7222594828228631,
        "step": 9692
    },
    {
        "loss": 2.7578,
        "grad_norm": 2.824148654937744,
        "learning_rate": 0.0001868981092854642,
        "epoch": 0.7223340040241448,
        "step": 9693
    },
    {
        "loss": 2.5732,
        "grad_norm": 2.9063522815704346,
        "learning_rate": 0.00018686326765451886,
        "epoch": 0.7224085252254266,
        "step": 9694
    },
    {
        "loss": 2.3729,
        "grad_norm": 1.714821696281433,
        "learning_rate": 0.00018682838301472553,
        "epoch": 0.7224830464267084,
        "step": 9695
    },
    {
        "loss": 1.6708,
        "grad_norm": 3.028144121170044,
        "learning_rate": 0.00018679345538335671,
        "epoch": 0.7225575676279902,
        "step": 9696
    },
    {
        "loss": 2.3253,
        "grad_norm": 3.134887933731079,
        "learning_rate": 0.0001867584847777061,
        "epoch": 0.7226320888292719,
        "step": 9697
    },
    {
        "loss": 1.7316,
        "grad_norm": 3.1941559314727783,
        "learning_rate": 0.00018672347121508902,
        "epoch": 0.7227066100305537,
        "step": 9698
    },
    {
        "loss": 2.2767,
        "grad_norm": 1.9754528999328613,
        "learning_rate": 0.00018668841471284155,
        "epoch": 0.7227811312318354,
        "step": 9699
    },
    {
        "loss": 2.4975,
        "grad_norm": 2.5750503540039062,
        "learning_rate": 0.00018665331528832155,
        "epoch": 0.7228556524331172,
        "step": 9700
    },
    {
        "loss": 2.6719,
        "grad_norm": 2.794614553451538,
        "learning_rate": 0.00018661817295890777,
        "epoch": 0.7229301736343989,
        "step": 9701
    },
    {
        "loss": 1.8747,
        "grad_norm": 4.437653541564941,
        "learning_rate": 0.00018658298774200028,
        "epoch": 0.7230046948356808,
        "step": 9702
    },
    {
        "loss": 2.3149,
        "grad_norm": 2.3777122497558594,
        "learning_rate": 0.0001865477596550206,
        "epoch": 0.7230792160369626,
        "step": 9703
    },
    {
        "loss": 2.6121,
        "grad_norm": 6.128371715545654,
        "learning_rate": 0.00018651248871541114,
        "epoch": 0.7231537372382443,
        "step": 9704
    },
    {
        "loss": 1.8355,
        "grad_norm": 3.2259044647216797,
        "learning_rate": 0.00018647717494063584,
        "epoch": 0.7232282584395261,
        "step": 9705
    },
    {
        "loss": 2.8294,
        "grad_norm": 3.0172836780548096,
        "learning_rate": 0.00018644181834817964,
        "epoch": 0.7233027796408078,
        "step": 9706
    },
    {
        "loss": 2.6521,
        "grad_norm": 2.676280975341797,
        "learning_rate": 0.00018640641895554874,
        "epoch": 0.7233773008420896,
        "step": 9707
    },
    {
        "loss": 2.6769,
        "grad_norm": 2.4643311500549316,
        "learning_rate": 0.00018637097678027048,
        "epoch": 0.7234518220433713,
        "step": 9708
    },
    {
        "loss": 2.2969,
        "grad_norm": 1.525418758392334,
        "learning_rate": 0.0001863354918398936,
        "epoch": 0.7235263432446531,
        "step": 9709
    },
    {
        "loss": 2.5723,
        "grad_norm": 3.8197460174560547,
        "learning_rate": 0.00018629996415198775,
        "epoch": 0.7236008644459349,
        "step": 9710
    },
    {
        "loss": 1.1517,
        "grad_norm": 1.7056326866149902,
        "learning_rate": 0.00018626439373414387,
        "epoch": 0.7236753856472167,
        "step": 9711
    },
    {
        "loss": 2.2408,
        "grad_norm": 2.124413013458252,
        "learning_rate": 0.00018622878060397396,
        "epoch": 0.7237499068484984,
        "step": 9712
    },
    {
        "loss": 2.0991,
        "grad_norm": 3.787571668624878,
        "learning_rate": 0.00018619312477911138,
        "epoch": 0.7238244280497802,
        "step": 9713
    },
    {
        "loss": 2.0029,
        "grad_norm": 2.0917418003082275,
        "learning_rate": 0.00018615742627721044,
        "epoch": 0.7238989492510619,
        "step": 9714
    },
    {
        "loss": 2.2214,
        "grad_norm": 2.6463303565979004,
        "learning_rate": 0.0001861216851159466,
        "epoch": 0.7239734704523437,
        "step": 9715
    },
    {
        "loss": 1.3141,
        "grad_norm": 5.3289475440979,
        "learning_rate": 0.00018608590131301652,
        "epoch": 0.7240479916536254,
        "step": 9716
    },
    {
        "loss": 2.1238,
        "grad_norm": 2.1939990520477295,
        "learning_rate": 0.0001860500748861379,
        "epoch": 0.7241225128549073,
        "step": 9717
    },
    {
        "loss": 2.5416,
        "grad_norm": 3.1209676265716553,
        "learning_rate": 0.00018601420585304972,
        "epoch": 0.724197034056189,
        "step": 9718
    },
    {
        "loss": 2.9268,
        "grad_norm": 3.7542781829833984,
        "learning_rate": 0.0001859782942315117,
        "epoch": 0.7242715552574708,
        "step": 9719
    },
    {
        "loss": 2.4934,
        "grad_norm": 2.5114145278930664,
        "learning_rate": 0.00018594234003930498,
        "epoch": 0.7243460764587525,
        "step": 9720
    },
    {
        "loss": 2.7035,
        "grad_norm": 3.24945330619812,
        "learning_rate": 0.00018590634329423164,
        "epoch": 0.7244205976600343,
        "step": 9721
    },
    {
        "loss": 1.9186,
        "grad_norm": 3.258294105529785,
        "learning_rate": 0.00018587030401411475,
        "epoch": 0.724495118861316,
        "step": 9722
    },
    {
        "loss": 2.4021,
        "grad_norm": 2.2936465740203857,
        "learning_rate": 0.00018583422221679876,
        "epoch": 0.7245696400625978,
        "step": 9723
    },
    {
        "loss": 2.3212,
        "grad_norm": 2.8727262020111084,
        "learning_rate": 0.00018579809792014878,
        "epoch": 0.7246441612638795,
        "step": 9724
    },
    {
        "loss": 2.1098,
        "grad_norm": 3.0604822635650635,
        "learning_rate": 0.0001857619311420511,
        "epoch": 0.7247186824651614,
        "step": 9725
    },
    {
        "loss": 2.416,
        "grad_norm": 3.1880416870117188,
        "learning_rate": 0.0001857257219004132,
        "epoch": 0.7247932036664431,
        "step": 9726
    },
    {
        "loss": 2.4014,
        "grad_norm": 2.3817455768585205,
        "learning_rate": 0.00018568947021316344,
        "epoch": 0.7248677248677249,
        "step": 9727
    },
    {
        "loss": 2.8659,
        "grad_norm": 3.4673473834991455,
        "learning_rate": 0.00018565317609825109,
        "epoch": 0.7249422460690066,
        "step": 9728
    },
    {
        "loss": 2.2648,
        "grad_norm": 3.1429944038391113,
        "learning_rate": 0.0001856168395736468,
        "epoch": 0.7250167672702884,
        "step": 9729
    },
    {
        "loss": 1.6829,
        "grad_norm": 4.003059387207031,
        "learning_rate": 0.0001855804606573417,
        "epoch": 0.7250912884715701,
        "step": 9730
    },
    {
        "loss": 1.5668,
        "grad_norm": 3.086430311203003,
        "learning_rate": 0.00018554403936734835,
        "epoch": 0.7251658096728519,
        "step": 9731
    },
    {
        "loss": 2.5096,
        "grad_norm": 2.9085702896118164,
        "learning_rate": 0.00018550757572169998,
        "epoch": 0.7252403308741336,
        "step": 9732
    },
    {
        "loss": 2.6016,
        "grad_norm": 2.1009483337402344,
        "learning_rate": 0.00018547106973845114,
        "epoch": 0.7253148520754155,
        "step": 9733
    },
    {
        "loss": 2.4396,
        "grad_norm": 2.2442080974578857,
        "learning_rate": 0.00018543452143567703,
        "epoch": 0.7253893732766972,
        "step": 9734
    },
    {
        "loss": 2.501,
        "grad_norm": 2.6346707344055176,
        "learning_rate": 0.00018539793083147384,
        "epoch": 0.725463894477979,
        "step": 9735
    },
    {
        "loss": 1.8245,
        "grad_norm": 3.7674341201782227,
        "learning_rate": 0.0001853612979439589,
        "epoch": 0.7255384156792607,
        "step": 9736
    },
    {
        "loss": 1.5106,
        "grad_norm": 5.315145492553711,
        "learning_rate": 0.00018532462279127025,
        "epoch": 0.7256129368805425,
        "step": 9737
    },
    {
        "loss": 2.5192,
        "grad_norm": 3.5138473510742188,
        "learning_rate": 0.00018528790539156716,
        "epoch": 0.7256874580818243,
        "step": 9738
    },
    {
        "loss": 2.7881,
        "grad_norm": 2.587758779525757,
        "learning_rate": 0.0001852511457630293,
        "epoch": 0.725761979283106,
        "step": 9739
    },
    {
        "loss": 1.8179,
        "grad_norm": 2.901113510131836,
        "learning_rate": 0.00018521434392385782,
        "epoch": 0.7258365004843879,
        "step": 9740
    },
    {
        "loss": 1.9096,
        "grad_norm": 2.677189350128174,
        "learning_rate": 0.0001851774998922744,
        "epoch": 0.7259110216856696,
        "step": 9741
    },
    {
        "loss": 1.8621,
        "grad_norm": 2.97460675239563,
        "learning_rate": 0.00018514061368652172,
        "epoch": 0.7259855428869514,
        "step": 9742
    },
    {
        "loss": 1.7238,
        "grad_norm": 2.6687300205230713,
        "learning_rate": 0.00018510368532486342,
        "epoch": 0.7260600640882331,
        "step": 9743
    },
    {
        "loss": 2.6117,
        "grad_norm": 2.994016647338867,
        "learning_rate": 0.00018506671482558393,
        "epoch": 0.7261345852895149,
        "step": 9744
    },
    {
        "loss": 2.2718,
        "grad_norm": 3.8617701530456543,
        "learning_rate": 0.00018502970220698846,
        "epoch": 0.7262091064907966,
        "step": 9745
    },
    {
        "loss": 1.9817,
        "grad_norm": 3.365941286087036,
        "learning_rate": 0.00018499264748740327,
        "epoch": 0.7262836276920784,
        "step": 9746
    },
    {
        "loss": 2.1239,
        "grad_norm": 4.174761772155762,
        "learning_rate": 0.00018495555068517535,
        "epoch": 0.7263581488933601,
        "step": 9747
    },
    {
        "loss": 2.3714,
        "grad_norm": 5.001448631286621,
        "learning_rate": 0.00018491841181867246,
        "epoch": 0.726432670094642,
        "step": 9748
    },
    {
        "loss": 2.7069,
        "grad_norm": 2.953324794769287,
        "learning_rate": 0.00018488123090628348,
        "epoch": 0.7265071912959237,
        "step": 9749
    },
    {
        "loss": 2.4184,
        "grad_norm": 2.643704891204834,
        "learning_rate": 0.0001848440079664176,
        "epoch": 0.7265817124972055,
        "step": 9750
    },
    {
        "loss": 2.2296,
        "grad_norm": 2.538135051727295,
        "learning_rate": 0.00018480674301750533,
        "epoch": 0.7266562336984872,
        "step": 9751
    },
    {
        "loss": 2.7359,
        "grad_norm": 2.0033984184265137,
        "learning_rate": 0.00018476943607799773,
        "epoch": 0.726730754899769,
        "step": 9752
    },
    {
        "loss": 2.3209,
        "grad_norm": 2.8001017570495605,
        "learning_rate": 0.00018473208716636655,
        "epoch": 0.7268052761010507,
        "step": 9753
    },
    {
        "loss": 2.5534,
        "grad_norm": 3.6506733894348145,
        "learning_rate": 0.00018469469630110464,
        "epoch": 0.7268797973023325,
        "step": 9754
    },
    {
        "loss": 1.2992,
        "grad_norm": 3.4493961334228516,
        "learning_rate": 0.00018465726350072535,
        "epoch": 0.7269543185036142,
        "step": 9755
    },
    {
        "loss": 1.8619,
        "grad_norm": 3.1907410621643066,
        "learning_rate": 0.00018461978878376293,
        "epoch": 0.7270288397048961,
        "step": 9756
    },
    {
        "loss": 2.4301,
        "grad_norm": 2.032362461090088,
        "learning_rate": 0.00018458227216877236,
        "epoch": 0.7271033609061778,
        "step": 9757
    },
    {
        "loss": 2.0495,
        "grad_norm": 3.7503018379211426,
        "learning_rate": 0.00018454471367432924,
        "epoch": 0.7271778821074596,
        "step": 9758
    },
    {
        "loss": 2.562,
        "grad_norm": 2.016784906387329,
        "learning_rate": 0.00018450711331903004,
        "epoch": 0.7272524033087413,
        "step": 9759
    },
    {
        "loss": 2.9173,
        "grad_norm": 2.96590256690979,
        "learning_rate": 0.000184469471121492,
        "epoch": 0.7273269245100231,
        "step": 9760
    },
    {
        "loss": 2.2802,
        "grad_norm": 3.6898250579833984,
        "learning_rate": 0.000184431787100353,
        "epoch": 0.7274014457113048,
        "step": 9761
    },
    {
        "loss": 2.4941,
        "grad_norm": 3.322051525115967,
        "learning_rate": 0.00018439406127427157,
        "epoch": 0.7274759669125866,
        "step": 9762
    },
    {
        "loss": 2.9259,
        "grad_norm": 2.193031072616577,
        "learning_rate": 0.00018435629366192696,
        "epoch": 0.7275504881138684,
        "step": 9763
    },
    {
        "loss": 1.8216,
        "grad_norm": 1.358747124671936,
        "learning_rate": 0.00018431848428201926,
        "epoch": 0.7276250093151502,
        "step": 9764
    },
    {
        "loss": 2.1656,
        "grad_norm": 2.583575487136841,
        "learning_rate": 0.00018428063315326906,
        "epoch": 0.7276995305164319,
        "step": 9765
    },
    {
        "loss": 2.4363,
        "grad_norm": 2.716099262237549,
        "learning_rate": 0.0001842427402944178,
        "epoch": 0.7277740517177137,
        "step": 9766
    },
    {
        "loss": 2.4149,
        "grad_norm": 2.8700308799743652,
        "learning_rate": 0.00018420480572422737,
        "epoch": 0.7278485729189954,
        "step": 9767
    },
    {
        "loss": 2.761,
        "grad_norm": 4.588101863861084,
        "learning_rate": 0.0001841668294614804,
        "epoch": 0.7279230941202772,
        "step": 9768
    },
    {
        "loss": 2.3,
        "grad_norm": 4.073363780975342,
        "learning_rate": 0.00018412881152498034,
        "epoch": 0.7279976153215589,
        "step": 9769
    },
    {
        "loss": 2.5375,
        "grad_norm": 2.0231270790100098,
        "learning_rate": 0.00018409075193355086,
        "epoch": 0.7280721365228408,
        "step": 9770
    },
    {
        "loss": 2.1845,
        "grad_norm": 3.4896256923675537,
        "learning_rate": 0.00018405265070603675,
        "epoch": 0.7281466577241225,
        "step": 9771
    },
    {
        "loss": 2.801,
        "grad_norm": 3.893829822540283,
        "learning_rate": 0.00018401450786130306,
        "epoch": 0.7282211789254043,
        "step": 9772
    },
    {
        "loss": 1.9541,
        "grad_norm": 2.7330591678619385,
        "learning_rate": 0.0001839763234182355,
        "epoch": 0.7282957001266861,
        "step": 9773
    },
    {
        "loss": 1.789,
        "grad_norm": 3.120116949081421,
        "learning_rate": 0.00018393809739574062,
        "epoch": 0.7283702213279678,
        "step": 9774
    },
    {
        "loss": 1.981,
        "grad_norm": 4.582147598266602,
        "learning_rate": 0.0001838998298127452,
        "epoch": 0.7284447425292496,
        "step": 9775
    },
    {
        "loss": 2.7064,
        "grad_norm": 2.1609976291656494,
        "learning_rate": 0.00018386152068819688,
        "epoch": 0.7285192637305313,
        "step": 9776
    },
    {
        "loss": 2.8321,
        "grad_norm": 2.8163208961486816,
        "learning_rate": 0.00018382317004106375,
        "epoch": 0.7285937849318131,
        "step": 9777
    },
    {
        "loss": 2.5875,
        "grad_norm": 3.8023011684417725,
        "learning_rate": 0.00018378477789033444,
        "epoch": 0.7286683061330949,
        "step": 9778
    },
    {
        "loss": 2.4262,
        "grad_norm": 3.410447120666504,
        "learning_rate": 0.0001837463442550181,
        "epoch": 0.7287428273343767,
        "step": 9779
    },
    {
        "loss": 2.61,
        "grad_norm": 1.9840489625930786,
        "learning_rate": 0.0001837078691541447,
        "epoch": 0.7288173485356584,
        "step": 9780
    },
    {
        "loss": 1.7775,
        "grad_norm": 3.6225807666778564,
        "learning_rate": 0.00018366935260676423,
        "epoch": 0.7288918697369402,
        "step": 9781
    },
    {
        "loss": 1.9117,
        "grad_norm": 3.265068292617798,
        "learning_rate": 0.00018363079463194773,
        "epoch": 0.7289663909382219,
        "step": 9782
    },
    {
        "loss": 2.3925,
        "grad_norm": 2.695535659790039,
        "learning_rate": 0.0001835921952487863,
        "epoch": 0.7290409121395037,
        "step": 9783
    },
    {
        "loss": 2.323,
        "grad_norm": 2.867107391357422,
        "learning_rate": 0.000183553554476392,
        "epoch": 0.7291154333407854,
        "step": 9784
    },
    {
        "loss": 2.5657,
        "grad_norm": 3.9798667430877686,
        "learning_rate": 0.000183514872333897,
        "epoch": 0.7291899545420673,
        "step": 9785
    },
    {
        "loss": 1.5108,
        "grad_norm": 4.205379962921143,
        "learning_rate": 0.00018347614884045403,
        "epoch": 0.729264475743349,
        "step": 9786
    },
    {
        "loss": 2.4166,
        "grad_norm": 1.8462738990783691,
        "learning_rate": 0.00018343738401523655,
        "epoch": 0.7293389969446308,
        "step": 9787
    },
    {
        "loss": 1.987,
        "grad_norm": 3.091796636581421,
        "learning_rate": 0.00018339857787743813,
        "epoch": 0.7294135181459125,
        "step": 9788
    },
    {
        "loss": 2.0428,
        "grad_norm": 5.535642623901367,
        "learning_rate": 0.00018335973044627315,
        "epoch": 0.7294880393471943,
        "step": 9789
    },
    {
        "loss": 1.7341,
        "grad_norm": 4.557949066162109,
        "learning_rate": 0.00018332084174097597,
        "epoch": 0.729562560548476,
        "step": 9790
    },
    {
        "loss": 2.3474,
        "grad_norm": 3.205174207687378,
        "learning_rate": 0.00018328191178080188,
        "epoch": 0.7296370817497578,
        "step": 9791
    },
    {
        "loss": 2.3682,
        "grad_norm": 3.047924041748047,
        "learning_rate": 0.0001832429405850263,
        "epoch": 0.7297116029510395,
        "step": 9792
    },
    {
        "loss": 2.9037,
        "grad_norm": 4.103609561920166,
        "learning_rate": 0.0001832039281729451,
        "epoch": 0.7297861241523214,
        "step": 9793
    },
    {
        "loss": 1.8157,
        "grad_norm": 2.8404603004455566,
        "learning_rate": 0.0001831648745638747,
        "epoch": 0.7298606453536031,
        "step": 9794
    },
    {
        "loss": 2.7447,
        "grad_norm": 5.178351879119873,
        "learning_rate": 0.0001831257797771518,
        "epoch": 0.7299351665548849,
        "step": 9795
    },
    {
        "loss": 2.5862,
        "grad_norm": 2.450240135192871,
        "learning_rate": 0.0001830866438321334,
        "epoch": 0.7300096877561666,
        "step": 9796
    },
    {
        "loss": 2.3594,
        "grad_norm": 2.2371771335601807,
        "learning_rate": 0.00018304746674819712,
        "epoch": 0.7300842089574484,
        "step": 9797
    },
    {
        "loss": 2.4683,
        "grad_norm": 2.3953278064727783,
        "learning_rate": 0.00018300824854474083,
        "epoch": 0.7301587301587301,
        "step": 9798
    },
    {
        "loss": 2.4237,
        "grad_norm": 3.0779788494110107,
        "learning_rate": 0.00018296898924118255,
        "epoch": 0.7302332513600119,
        "step": 9799
    },
    {
        "loss": 2.287,
        "grad_norm": 3.445253610610962,
        "learning_rate": 0.00018292968885696116,
        "epoch": 0.7303077725612936,
        "step": 9800
    },
    {
        "loss": 2.6061,
        "grad_norm": 3.612539529800415,
        "learning_rate": 0.00018289034741153523,
        "epoch": 0.7303822937625755,
        "step": 9801
    },
    {
        "loss": 2.6269,
        "grad_norm": 2.8946046829223633,
        "learning_rate": 0.00018285096492438424,
        "epoch": 0.7304568149638572,
        "step": 9802
    },
    {
        "loss": 2.7426,
        "grad_norm": 2.327863931655884,
        "learning_rate": 0.00018281154141500762,
        "epoch": 0.730531336165139,
        "step": 9803
    },
    {
        "loss": 2.3255,
        "grad_norm": 2.464602470397949,
        "learning_rate": 0.00018277207690292532,
        "epoch": 0.7306058573664207,
        "step": 9804
    },
    {
        "loss": 2.0881,
        "grad_norm": 3.9542136192321777,
        "learning_rate": 0.00018273257140767748,
        "epoch": 0.7306803785677025,
        "step": 9805
    },
    {
        "loss": 1.4801,
        "grad_norm": 4.239051342010498,
        "learning_rate": 0.00018269302494882447,
        "epoch": 0.7307548997689842,
        "step": 9806
    },
    {
        "loss": 2.6248,
        "grad_norm": 1.878096342086792,
        "learning_rate": 0.00018265343754594727,
        "epoch": 0.730829420970266,
        "step": 9807
    },
    {
        "loss": 2.4997,
        "grad_norm": 3.1696653366088867,
        "learning_rate": 0.00018261380921864672,
        "epoch": 0.7309039421715479,
        "step": 9808
    },
    {
        "loss": 1.6184,
        "grad_norm": 2.873504400253296,
        "learning_rate": 0.0001825741399865441,
        "epoch": 0.7309784633728296,
        "step": 9809
    },
    {
        "loss": 2.8345,
        "grad_norm": 3.6126229763031006,
        "learning_rate": 0.00018253442986928094,
        "epoch": 0.7310529845741114,
        "step": 9810
    },
    {
        "loss": 2.1018,
        "grad_norm": 3.2605669498443604,
        "learning_rate": 0.0001824946788865192,
        "epoch": 0.7311275057753931,
        "step": 9811
    },
    {
        "loss": 2.5252,
        "grad_norm": 3.4821670055389404,
        "learning_rate": 0.0001824548870579407,
        "epoch": 0.7312020269766749,
        "step": 9812
    },
    {
        "loss": 2.3228,
        "grad_norm": 2.235607624053955,
        "learning_rate": 0.00018241505440324773,
        "epoch": 0.7312765481779566,
        "step": 9813
    },
    {
        "loss": 2.1405,
        "grad_norm": 3.3379786014556885,
        "learning_rate": 0.0001823751809421627,
        "epoch": 0.7313510693792384,
        "step": 9814
    },
    {
        "loss": 1.8036,
        "grad_norm": 3.785282611846924,
        "learning_rate": 0.00018233526669442836,
        "epoch": 0.7314255905805201,
        "step": 9815
    },
    {
        "loss": 1.7315,
        "grad_norm": 3.691392183303833,
        "learning_rate": 0.0001822953116798075,
        "epoch": 0.731500111781802,
        "step": 9816
    },
    {
        "loss": 1.4442,
        "grad_norm": 6.545423984527588,
        "learning_rate": 0.0001822553159180832,
        "epoch": 0.7315746329830837,
        "step": 9817
    },
    {
        "loss": 2.6046,
        "grad_norm": 2.5937764644622803,
        "learning_rate": 0.00018221527942905864,
        "epoch": 0.7316491541843655,
        "step": 9818
    },
    {
        "loss": 1.6348,
        "grad_norm": 4.013156890869141,
        "learning_rate": 0.00018217520223255714,
        "epoch": 0.7317236753856472,
        "step": 9819
    },
    {
        "loss": 2.5336,
        "grad_norm": 2.991891622543335,
        "learning_rate": 0.00018213508434842247,
        "epoch": 0.731798196586929,
        "step": 9820
    },
    {
        "loss": 2.3676,
        "grad_norm": 2.434394121170044,
        "learning_rate": 0.00018209492579651792,
        "epoch": 0.7318727177882107,
        "step": 9821
    },
    {
        "loss": 2.8069,
        "grad_norm": 3.2422921657562256,
        "learning_rate": 0.00018205472659672762,
        "epoch": 0.7319472389894925,
        "step": 9822
    },
    {
        "loss": 2.5251,
        "grad_norm": 2.7467868328094482,
        "learning_rate": 0.0001820144867689554,
        "epoch": 0.7320217601907743,
        "step": 9823
    },
    {
        "loss": 2.698,
        "grad_norm": 2.5257062911987305,
        "learning_rate": 0.00018197420633312526,
        "epoch": 0.7320962813920561,
        "step": 9824
    },
    {
        "loss": 2.021,
        "grad_norm": 3.573072671890259,
        "learning_rate": 0.00018193388530918147,
        "epoch": 0.7321708025933378,
        "step": 9825
    },
    {
        "loss": 1.9861,
        "grad_norm": 2.0378220081329346,
        "learning_rate": 0.00018189352371708817,
        "epoch": 0.7322453237946196,
        "step": 9826
    },
    {
        "loss": 2.1842,
        "grad_norm": 2.3894805908203125,
        "learning_rate": 0.00018185312157682984,
        "epoch": 0.7323198449959013,
        "step": 9827
    },
    {
        "loss": 2.0233,
        "grad_norm": 3.3903589248657227,
        "learning_rate": 0.00018181267890841088,
        "epoch": 0.7323943661971831,
        "step": 9828
    },
    {
        "loss": 3.0647,
        "grad_norm": 3.189763069152832,
        "learning_rate": 0.0001817721957318557,
        "epoch": 0.7324688873984648,
        "step": 9829
    },
    {
        "loss": 3.003,
        "grad_norm": 2.0756452083587646,
        "learning_rate": 0.00018173167206720885,
        "epoch": 0.7325434085997466,
        "step": 9830
    },
    {
        "loss": 1.9108,
        "grad_norm": 3.947899103164673,
        "learning_rate": 0.000181691107934535,
        "epoch": 0.7326179298010284,
        "step": 9831
    },
    {
        "loss": 2.4211,
        "grad_norm": 2.4740469455718994,
        "learning_rate": 0.00018165050335391876,
        "epoch": 0.7326924510023102,
        "step": 9832
    },
    {
        "loss": 2.3668,
        "grad_norm": 3.102109432220459,
        "learning_rate": 0.00018160985834546478,
        "epoch": 0.7327669722035919,
        "step": 9833
    },
    {
        "loss": 2.396,
        "grad_norm": 2.8334546089172363,
        "learning_rate": 0.0001815691729292976,
        "epoch": 0.7328414934048737,
        "step": 9834
    },
    {
        "loss": 2.1272,
        "grad_norm": 3.6979892253875732,
        "learning_rate": 0.00018152844712556216,
        "epoch": 0.7329160146061554,
        "step": 9835
    },
    {
        "loss": 1.9711,
        "grad_norm": 3.869572639465332,
        "learning_rate": 0.0001814876809544229,
        "epoch": 0.7329905358074372,
        "step": 9836
    },
    {
        "loss": 2.5339,
        "grad_norm": 2.4231131076812744,
        "learning_rate": 0.00018144687443606464,
        "epoch": 0.7330650570087189,
        "step": 9837
    },
    {
        "loss": 2.4154,
        "grad_norm": 2.8493587970733643,
        "learning_rate": 0.000181406027590692,
        "epoch": 0.7331395782100008,
        "step": 9838
    },
    {
        "loss": 1.6994,
        "grad_norm": 3.8760550022125244,
        "learning_rate": 0.00018136514043852947,
        "epoch": 0.7332140994112825,
        "step": 9839
    },
    {
        "loss": 1.952,
        "grad_norm": 2.330665111541748,
        "learning_rate": 0.00018132421299982187,
        "epoch": 0.7332886206125643,
        "step": 9840
    },
    {
        "loss": 1.7879,
        "grad_norm": 3.4410314559936523,
        "learning_rate": 0.00018128324529483343,
        "epoch": 0.733363141813846,
        "step": 9841
    },
    {
        "loss": 2.6162,
        "grad_norm": 2.5434885025024414,
        "learning_rate": 0.00018124223734384875,
        "epoch": 0.7334376630151278,
        "step": 9842
    },
    {
        "loss": 1.592,
        "grad_norm": 4.614675521850586,
        "learning_rate": 0.00018120118916717222,
        "epoch": 0.7335121842164096,
        "step": 9843
    },
    {
        "loss": 2.1542,
        "grad_norm": 2.4935102462768555,
        "learning_rate": 0.00018116010078512803,
        "epoch": 0.7335867054176913,
        "step": 9844
    },
    {
        "loss": 1.6202,
        "grad_norm": 3.0181210041046143,
        "learning_rate": 0.0001811189722180605,
        "epoch": 0.7336612266189732,
        "step": 9845
    },
    {
        "loss": 3.0204,
        "grad_norm": 2.415678024291992,
        "learning_rate": 0.00018107780348633373,
        "epoch": 0.7337357478202549,
        "step": 9846
    },
    {
        "loss": 2.3169,
        "grad_norm": 3.4993538856506348,
        "learning_rate": 0.00018103659461033155,
        "epoch": 0.7338102690215367,
        "step": 9847
    },
    {
        "loss": 3.1318,
        "grad_norm": 2.4013257026672363,
        "learning_rate": 0.00018099534561045804,
        "epoch": 0.7338847902228184,
        "step": 9848
    },
    {
        "loss": 2.3209,
        "grad_norm": 2.173238754272461,
        "learning_rate": 0.00018095405650713682,
        "epoch": 0.7339593114241002,
        "step": 9849
    },
    {
        "loss": 2.1208,
        "grad_norm": 4.764830589294434,
        "learning_rate": 0.00018091272732081138,
        "epoch": 0.7340338326253819,
        "step": 9850
    },
    {
        "loss": 2.3009,
        "grad_norm": 2.9105327129364014,
        "learning_rate": 0.00018087135807194545,
        "epoch": 0.7341083538266637,
        "step": 9851
    },
    {
        "loss": 1.5485,
        "grad_norm": 3.4002909660339355,
        "learning_rate": 0.00018082994878102195,
        "epoch": 0.7341828750279454,
        "step": 9852
    },
    {
        "loss": 2.1985,
        "grad_norm": 3.6310455799102783,
        "learning_rate": 0.00018078849946854422,
        "epoch": 0.7342573962292273,
        "step": 9853
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.6563282012939453,
        "learning_rate": 0.000180747010155035,
        "epoch": 0.734331917430509,
        "step": 9854
    },
    {
        "loss": 2.8257,
        "grad_norm": 3.0681424140930176,
        "learning_rate": 0.00018070548086103716,
        "epoch": 0.7344064386317908,
        "step": 9855
    },
    {
        "loss": 2.5344,
        "grad_norm": 2.8882906436920166,
        "learning_rate": 0.00018066391160711314,
        "epoch": 0.7344809598330725,
        "step": 9856
    },
    {
        "loss": 2.266,
        "grad_norm": 2.602626085281372,
        "learning_rate": 0.00018062230241384518,
        "epoch": 0.7345554810343543,
        "step": 9857
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.8615564107894897,
        "learning_rate": 0.0001805806533018355,
        "epoch": 0.734630002235636,
        "step": 9858
    },
    {
        "loss": 1.4524,
        "grad_norm": 3.666191577911377,
        "learning_rate": 0.00018053896429170578,
        "epoch": 0.7347045234369178,
        "step": 9859
    },
    {
        "loss": 1.782,
        "grad_norm": 2.8262856006622314,
        "learning_rate": 0.00018049723540409783,
        "epoch": 0.7347790446381995,
        "step": 9860
    },
    {
        "loss": 2.2876,
        "grad_norm": 2.21962833404541,
        "learning_rate": 0.00018045546665967273,
        "epoch": 0.7348535658394814,
        "step": 9861
    },
    {
        "loss": 2.1932,
        "grad_norm": 3.0975191593170166,
        "learning_rate": 0.00018041365807911172,
        "epoch": 0.7349280870407631,
        "step": 9862
    },
    {
        "loss": 2.5487,
        "grad_norm": 3.067915916442871,
        "learning_rate": 0.00018037180968311556,
        "epoch": 0.7350026082420449,
        "step": 9863
    },
    {
        "loss": 2.4468,
        "grad_norm": 3.303433656692505,
        "learning_rate": 0.0001803299214924047,
        "epoch": 0.7350771294433266,
        "step": 9864
    },
    {
        "loss": 2.8471,
        "grad_norm": 3.604931116104126,
        "learning_rate": 0.00018028799352771944,
        "epoch": 0.7351516506446084,
        "step": 9865
    },
    {
        "loss": 0.8172,
        "grad_norm": 3.5036473274230957,
        "learning_rate": 0.00018024602580981968,
        "epoch": 0.7352261718458901,
        "step": 9866
    },
    {
        "loss": 1.9424,
        "grad_norm": 3.9258737564086914,
        "learning_rate": 0.0001802040183594849,
        "epoch": 0.7353006930471719,
        "step": 9867
    },
    {
        "loss": 2.3778,
        "grad_norm": 2.8783905506134033,
        "learning_rate": 0.00018016197119751458,
        "epoch": 0.7353752142484536,
        "step": 9868
    },
    {
        "loss": 1.9141,
        "grad_norm": 3.7545015811920166,
        "learning_rate": 0.00018011988434472745,
        "epoch": 0.7354497354497355,
        "step": 9869
    },
    {
        "loss": 1.2029,
        "grad_norm": 4.079398155212402,
        "learning_rate": 0.0001800777578219621,
        "epoch": 0.7355242566510172,
        "step": 9870
    },
    {
        "loss": 2.4791,
        "grad_norm": 2.340003252029419,
        "learning_rate": 0.00018003559165007696,
        "epoch": 0.735598777852299,
        "step": 9871
    },
    {
        "loss": 2.3977,
        "grad_norm": 2.92704701423645,
        "learning_rate": 0.00017999338584994958,
        "epoch": 0.7356732990535807,
        "step": 9872
    },
    {
        "loss": 1.6623,
        "grad_norm": 4.89960241317749,
        "learning_rate": 0.00017995114044247766,
        "epoch": 0.7357478202548625,
        "step": 9873
    },
    {
        "loss": 1.3985,
        "grad_norm": 3.160884380340576,
        "learning_rate": 0.00017990885544857824,
        "epoch": 0.7358223414561442,
        "step": 9874
    },
    {
        "loss": 1.9911,
        "grad_norm": 2.4287357330322266,
        "learning_rate": 0.00017986653088918785,
        "epoch": 0.735896862657426,
        "step": 9875
    },
    {
        "loss": 1.2917,
        "grad_norm": 3.5673625469207764,
        "learning_rate": 0.000179824166785263,
        "epoch": 0.7359713838587078,
        "step": 9876
    },
    {
        "loss": 2.4584,
        "grad_norm": 3.3127737045288086,
        "learning_rate": 0.00017978176315777933,
        "epoch": 0.7360459050599896,
        "step": 9877
    },
    {
        "loss": 2.1697,
        "grad_norm": 2.802626848220825,
        "learning_rate": 0.00017973932002773244,
        "epoch": 0.7361204262612713,
        "step": 9878
    },
    {
        "loss": 2.5728,
        "grad_norm": 3.997549057006836,
        "learning_rate": 0.00017969683741613727,
        "epoch": 0.7361949474625531,
        "step": 9879
    },
    {
        "loss": 1.9761,
        "grad_norm": 2.9982497692108154,
        "learning_rate": 0.0001796543153440283,
        "epoch": 0.7362694686638349,
        "step": 9880
    },
    {
        "loss": 2.9057,
        "grad_norm": 2.3067212104797363,
        "learning_rate": 0.00017961175383245955,
        "epoch": 0.7363439898651166,
        "step": 9881
    },
    {
        "loss": 1.4213,
        "grad_norm": 4.999257564544678,
        "learning_rate": 0.0001795691529025048,
        "epoch": 0.7364185110663984,
        "step": 9882
    },
    {
        "loss": 2.6614,
        "grad_norm": 2.4944801330566406,
        "learning_rate": 0.00017952651257525698,
        "epoch": 0.7364930322676801,
        "step": 9883
    },
    {
        "loss": 2.7709,
        "grad_norm": 3.0002548694610596,
        "learning_rate": 0.00017948383287182885,
        "epoch": 0.736567553468962,
        "step": 9884
    },
    {
        "loss": 1.7239,
        "grad_norm": 4.524297714233398,
        "learning_rate": 0.00017944111381335237,
        "epoch": 0.7366420746702437,
        "step": 9885
    },
    {
        "loss": 2.691,
        "grad_norm": 2.848129987716675,
        "learning_rate": 0.00017939835542097937,
        "epoch": 0.7367165958715255,
        "step": 9886
    },
    {
        "loss": 2.3622,
        "grad_norm": 2.3667571544647217,
        "learning_rate": 0.00017935555771588066,
        "epoch": 0.7367911170728072,
        "step": 9887
    },
    {
        "loss": 2.6288,
        "grad_norm": 2.299323320388794,
        "learning_rate": 0.000179312720719247,
        "epoch": 0.736865638274089,
        "step": 9888
    },
    {
        "loss": 2.1011,
        "grad_norm": 3.7836055755615234,
        "learning_rate": 0.00017926984445228836,
        "epoch": 0.7369401594753707,
        "step": 9889
    },
    {
        "loss": 2.6438,
        "grad_norm": 2.975325107574463,
        "learning_rate": 0.00017922692893623405,
        "epoch": 0.7370146806766525,
        "step": 9890
    },
    {
        "loss": 2.2657,
        "grad_norm": 3.031493902206421,
        "learning_rate": 0.00017918397419233322,
        "epoch": 0.7370892018779343,
        "step": 9891
    },
    {
        "loss": 2.1032,
        "grad_norm": 2.7229881286621094,
        "learning_rate": 0.00017914098024185377,
        "epoch": 0.7371637230792161,
        "step": 9892
    },
    {
        "loss": 2.5965,
        "grad_norm": 2.714794397354126,
        "learning_rate": 0.00017909794710608377,
        "epoch": 0.7372382442804978,
        "step": 9893
    },
    {
        "loss": 2.3168,
        "grad_norm": 2.9062764644622803,
        "learning_rate": 0.00017905487480633013,
        "epoch": 0.7373127654817796,
        "step": 9894
    },
    {
        "loss": 3.0505,
        "grad_norm": 2.920517921447754,
        "learning_rate": 0.0001790117633639194,
        "epoch": 0.7373872866830613,
        "step": 9895
    },
    {
        "loss": 2.2206,
        "grad_norm": 4.504673957824707,
        "learning_rate": 0.00017896861280019755,
        "epoch": 0.7374618078843431,
        "step": 9896
    },
    {
        "loss": 2.5714,
        "grad_norm": 2.143855094909668,
        "learning_rate": 0.0001789254231365297,
        "epoch": 0.7375363290856248,
        "step": 9897
    },
    {
        "loss": 2.6938,
        "grad_norm": 2.399912118911743,
        "learning_rate": 0.0001788821943943006,
        "epoch": 0.7376108502869066,
        "step": 9898
    },
    {
        "loss": 1.3322,
        "grad_norm": 2.8197195529937744,
        "learning_rate": 0.0001788389265949142,
        "epoch": 0.7376853714881884,
        "step": 9899
    },
    {
        "loss": 2.2422,
        "grad_norm": 3.3554306030273438,
        "learning_rate": 0.0001787956197597937,
        "epoch": 0.7377598926894702,
        "step": 9900
    },
    {
        "loss": 2.738,
        "grad_norm": 2.5040085315704346,
        "learning_rate": 0.00017875227391038175,
        "epoch": 0.7378344138907519,
        "step": 9901
    },
    {
        "loss": 1.6712,
        "grad_norm": 3.4376890659332275,
        "learning_rate": 0.00017870888906814053,
        "epoch": 0.7379089350920337,
        "step": 9902
    },
    {
        "loss": 2.9267,
        "grad_norm": 3.1533725261688232,
        "learning_rate": 0.00017866546525455092,
        "epoch": 0.7379834562933154,
        "step": 9903
    },
    {
        "loss": 2.5063,
        "grad_norm": 4.259678363800049,
        "learning_rate": 0.00017862200249111373,
        "epoch": 0.7380579774945972,
        "step": 9904
    },
    {
        "loss": 2.5314,
        "grad_norm": 3.4253017902374268,
        "learning_rate": 0.00017857850079934867,
        "epoch": 0.7381324986958789,
        "step": 9905
    },
    {
        "loss": 2.6237,
        "grad_norm": 4.980979919433594,
        "learning_rate": 0.00017853496020079496,
        "epoch": 0.7382070198971608,
        "step": 9906
    },
    {
        "loss": 2.1788,
        "grad_norm": 2.9118497371673584,
        "learning_rate": 0.00017849138071701095,
        "epoch": 0.7382815410984425,
        "step": 9907
    },
    {
        "loss": 2.7079,
        "grad_norm": 2.7463295459747314,
        "learning_rate": 0.00017844776236957416,
        "epoch": 0.7383560622997243,
        "step": 9908
    },
    {
        "loss": 2.1591,
        "grad_norm": 2.027905225753784,
        "learning_rate": 0.00017840410518008164,
        "epoch": 0.738430583501006,
        "step": 9909
    },
    {
        "loss": 2.2024,
        "grad_norm": 2.8788113594055176,
        "learning_rate": 0.0001783604091701493,
        "epoch": 0.7385051047022878,
        "step": 9910
    },
    {
        "loss": 2.5674,
        "grad_norm": 2.8436367511749268,
        "learning_rate": 0.00017831667436141277,
        "epoch": 0.7385796259035695,
        "step": 9911
    },
    {
        "loss": 2.2655,
        "grad_norm": 3.435154914855957,
        "learning_rate": 0.00017827290077552618,
        "epoch": 0.7386541471048513,
        "step": 9912
    },
    {
        "loss": 2.4288,
        "grad_norm": 3.0687272548675537,
        "learning_rate": 0.00017822908843416357,
        "epoch": 0.738728668306133,
        "step": 9913
    },
    {
        "loss": 1.9789,
        "grad_norm": 2.9604921340942383,
        "learning_rate": 0.00017818523735901778,
        "epoch": 0.7388031895074149,
        "step": 9914
    },
    {
        "loss": 2.5393,
        "grad_norm": 2.2106757164001465,
        "learning_rate": 0.00017814134757180078,
        "epoch": 0.7388777107086967,
        "step": 9915
    },
    {
        "loss": 2.5356,
        "grad_norm": 2.6841254234313965,
        "learning_rate": 0.00017809741909424406,
        "epoch": 0.7389522319099784,
        "step": 9916
    },
    {
        "loss": 2.7746,
        "grad_norm": 3.653510570526123,
        "learning_rate": 0.00017805345194809793,
        "epoch": 0.7390267531112602,
        "step": 9917
    },
    {
        "loss": 1.9433,
        "grad_norm": 3.488978147506714,
        "learning_rate": 0.0001780094461551319,
        "epoch": 0.7391012743125419,
        "step": 9918
    },
    {
        "loss": 1.5446,
        "grad_norm": 3.8268277645111084,
        "learning_rate": 0.00017796540173713483,
        "epoch": 0.7391757955138237,
        "step": 9919
    },
    {
        "loss": 2.5804,
        "grad_norm": 2.2425291538238525,
        "learning_rate": 0.00017792131871591452,
        "epoch": 0.7392503167151054,
        "step": 9920
    },
    {
        "loss": 1.753,
        "grad_norm": 3.418691635131836,
        "learning_rate": 0.00017787719711329775,
        "epoch": 0.7393248379163873,
        "step": 9921
    },
    {
        "loss": 2.4795,
        "grad_norm": 4.039937973022461,
        "learning_rate": 0.00017783303695113092,
        "epoch": 0.739399359117669,
        "step": 9922
    },
    {
        "loss": 1.8756,
        "grad_norm": 4.3194169998168945,
        "learning_rate": 0.00017778883825127873,
        "epoch": 0.7394738803189508,
        "step": 9923
    },
    {
        "loss": 1.8692,
        "grad_norm": 2.1407458782196045,
        "learning_rate": 0.00017774460103562573,
        "epoch": 0.7395484015202325,
        "step": 9924
    },
    {
        "loss": 2.8081,
        "grad_norm": 3.680335521697998,
        "learning_rate": 0.00017770032532607504,
        "epoch": 0.7396229227215143,
        "step": 9925
    },
    {
        "loss": 2.4222,
        "grad_norm": 2.9916160106658936,
        "learning_rate": 0.00017765601114454915,
        "epoch": 0.739697443922796,
        "step": 9926
    },
    {
        "loss": 2.3533,
        "grad_norm": 2.4766242504119873,
        "learning_rate": 0.0001776116585129894,
        "epoch": 0.7397719651240778,
        "step": 9927
    },
    {
        "loss": 2.0953,
        "grad_norm": 3.096796751022339,
        "learning_rate": 0.00017756726745335612,
        "epoch": 0.7398464863253595,
        "step": 9928
    },
    {
        "loss": 2.0771,
        "grad_norm": 3.5742530822753906,
        "learning_rate": 0.000177522837987629,
        "epoch": 0.7399210075266414,
        "step": 9929
    },
    {
        "loss": 2.2068,
        "grad_norm": 2.5606906414031982,
        "learning_rate": 0.0001774783701378063,
        "epoch": 0.7399955287279231,
        "step": 9930
    },
    {
        "loss": 2.3923,
        "grad_norm": 4.161112308502197,
        "learning_rate": 0.00017743386392590578,
        "epoch": 0.7400700499292049,
        "step": 9931
    },
    {
        "loss": 2.4603,
        "grad_norm": 2.68253755569458,
        "learning_rate": 0.00017738931937396357,
        "epoch": 0.7401445711304866,
        "step": 9932
    },
    {
        "loss": 2.325,
        "grad_norm": 2.766561985015869,
        "learning_rate": 0.00017734473650403545,
        "epoch": 0.7402190923317684,
        "step": 9933
    },
    {
        "loss": 2.1359,
        "grad_norm": 3.596024751663208,
        "learning_rate": 0.0001773001153381957,
        "epoch": 0.7402936135330501,
        "step": 9934
    },
    {
        "loss": 2.7134,
        "grad_norm": 2.3093316555023193,
        "learning_rate": 0.00017725545589853775,
        "epoch": 0.7403681347343319,
        "step": 9935
    },
    {
        "loss": 2.0826,
        "grad_norm": 3.9447240829467773,
        "learning_rate": 0.0001772107582071739,
        "epoch": 0.7404426559356136,
        "step": 9936
    },
    {
        "loss": 2.8537,
        "grad_norm": 3.513962507247925,
        "learning_rate": 0.00017716602228623555,
        "epoch": 0.7405171771368955,
        "step": 9937
    },
    {
        "loss": 2.4426,
        "grad_norm": 3.496446132659912,
        "learning_rate": 0.00017712124815787278,
        "epoch": 0.7405916983381772,
        "step": 9938
    },
    {
        "loss": 2.5666,
        "grad_norm": 2.5579049587249756,
        "learning_rate": 0.00017707643584425492,
        "epoch": 0.740666219539459,
        "step": 9939
    },
    {
        "loss": 2.7441,
        "grad_norm": 2.8412015438079834,
        "learning_rate": 0.00017703158536756987,
        "epoch": 0.7407407407407407,
        "step": 9940
    },
    {
        "loss": 2.5938,
        "grad_norm": 2.871553659439087,
        "learning_rate": 0.00017698669675002453,
        "epoch": 0.7408152619420225,
        "step": 9941
    },
    {
        "loss": 2.4868,
        "grad_norm": 3.1822621822357178,
        "learning_rate": 0.000176941770013845,
        "epoch": 0.7408897831433042,
        "step": 9942
    },
    {
        "loss": 2.6024,
        "grad_norm": 3.575000047683716,
        "learning_rate": 0.00017689680518127553,
        "epoch": 0.740964304344586,
        "step": 9943
    },
    {
        "loss": 1.5983,
        "grad_norm": 3.361246109008789,
        "learning_rate": 0.00017685180227458008,
        "epoch": 0.7410388255458678,
        "step": 9944
    },
    {
        "loss": 2.3176,
        "grad_norm": 4.145411014556885,
        "learning_rate": 0.00017680676131604086,
        "epoch": 0.7411133467471496,
        "step": 9945
    },
    {
        "loss": 2.2252,
        "grad_norm": 3.5497751235961914,
        "learning_rate": 0.0001767616823279591,
        "epoch": 0.7411878679484313,
        "step": 9946
    },
    {
        "loss": 2.2526,
        "grad_norm": 2.238865852355957,
        "learning_rate": 0.00017671656533265506,
        "epoch": 0.7412623891497131,
        "step": 9947
    },
    {
        "loss": 1.5745,
        "grad_norm": 4.044584274291992,
        "learning_rate": 0.00017667141035246744,
        "epoch": 0.7413369103509948,
        "step": 9948
    },
    {
        "loss": 1.9652,
        "grad_norm": 3.442709445953369,
        "learning_rate": 0.00017662621740975414,
        "epoch": 0.7414114315522766,
        "step": 9949
    },
    {
        "loss": 2.1374,
        "grad_norm": 2.578420877456665,
        "learning_rate": 0.00017658098652689156,
        "epoch": 0.7414859527535584,
        "step": 9950
    },
    {
        "loss": 2.3404,
        "grad_norm": 2.863967180252075,
        "learning_rate": 0.00017653571772627502,
        "epoch": 0.7415604739548401,
        "step": 9951
    },
    {
        "loss": 2.3543,
        "grad_norm": 2.9044623374938965,
        "learning_rate": 0.00017649041103031844,
        "epoch": 0.741634995156122,
        "step": 9952
    },
    {
        "loss": 2.7712,
        "grad_norm": 3.753694772720337,
        "learning_rate": 0.00017644506646145492,
        "epoch": 0.7417095163574037,
        "step": 9953
    },
    {
        "loss": 3.1019,
        "grad_norm": 2.231340169906616,
        "learning_rate": 0.00017639968404213586,
        "epoch": 0.7417840375586855,
        "step": 9954
    },
    {
        "loss": 2.0902,
        "grad_norm": 2.9341351985931396,
        "learning_rate": 0.00017635426379483164,
        "epoch": 0.7418585587599672,
        "step": 9955
    },
    {
        "loss": 1.5997,
        "grad_norm": 4.182596206665039,
        "learning_rate": 0.0001763088057420312,
        "epoch": 0.741933079961249,
        "step": 9956
    },
    {
        "loss": 1.967,
        "grad_norm": 4.0549163818359375,
        "learning_rate": 0.00017626330990624248,
        "epoch": 0.7420076011625307,
        "step": 9957
    },
    {
        "loss": 2.2184,
        "grad_norm": 3.2633049488067627,
        "learning_rate": 0.0001762177763099918,
        "epoch": 0.7420821223638125,
        "step": 9958
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.362719774246216,
        "learning_rate": 0.0001761722049758245,
        "epoch": 0.7421566435650943,
        "step": 9959
    },
    {
        "loss": 1.458,
        "grad_norm": 3.720994710922241,
        "learning_rate": 0.00017612659592630434,
        "epoch": 0.7422311647663761,
        "step": 9960
    },
    {
        "loss": 2.7384,
        "grad_norm": 3.6490373611450195,
        "learning_rate": 0.00017608094918401379,
        "epoch": 0.7423056859676578,
        "step": 9961
    },
    {
        "loss": 2.0274,
        "grad_norm": 2.808584213256836,
        "learning_rate": 0.0001760352647715543,
        "epoch": 0.7423802071689396,
        "step": 9962
    },
    {
        "loss": 1.5425,
        "grad_norm": 2.162140130996704,
        "learning_rate": 0.00017598954271154537,
        "epoch": 0.7424547283702213,
        "step": 9963
    },
    {
        "loss": 2.0321,
        "grad_norm": 3.7499585151672363,
        "learning_rate": 0.00017594378302662576,
        "epoch": 0.7425292495715031,
        "step": 9964
    },
    {
        "loss": 2.3096,
        "grad_norm": 4.161113739013672,
        "learning_rate": 0.0001758979857394525,
        "epoch": 0.7426037707727848,
        "step": 9965
    },
    {
        "loss": 2.4839,
        "grad_norm": 4.420353889465332,
        "learning_rate": 0.00017585215087270118,
        "epoch": 0.7426782919740667,
        "step": 9966
    },
    {
        "loss": 2.8827,
        "grad_norm": 2.8243186473846436,
        "learning_rate": 0.00017580627844906638,
        "epoch": 0.7427528131753484,
        "step": 9967
    },
    {
        "loss": 2.6869,
        "grad_norm": 2.826068162918091,
        "learning_rate": 0.00017576036849126098,
        "epoch": 0.7428273343766302,
        "step": 9968
    },
    {
        "loss": 2.3796,
        "grad_norm": 2.6394593715667725,
        "learning_rate": 0.00017571442102201634,
        "epoch": 0.7429018555779119,
        "step": 9969
    },
    {
        "loss": 2.437,
        "grad_norm": 3.333686351776123,
        "learning_rate": 0.0001756684360640828,
        "epoch": 0.7429763767791937,
        "step": 9970
    },
    {
        "loss": 2.4477,
        "grad_norm": 2.8118653297424316,
        "learning_rate": 0.00017562241364022887,
        "epoch": 0.7430508979804754,
        "step": 9971
    },
    {
        "loss": 2.0462,
        "grad_norm": 3.0876080989837646,
        "learning_rate": 0.00017557635377324167,
        "epoch": 0.7431254191817572,
        "step": 9972
    },
    {
        "loss": 2.5651,
        "grad_norm": 2.6864125728607178,
        "learning_rate": 0.0001755302564859273,
        "epoch": 0.7431999403830389,
        "step": 9973
    },
    {
        "loss": 2.4712,
        "grad_norm": 2.998656749725342,
        "learning_rate": 0.0001754841218011096,
        "epoch": 0.7432744615843208,
        "step": 9974
    },
    {
        "loss": 2.792,
        "grad_norm": 3.1463510990142822,
        "learning_rate": 0.00017543794974163163,
        "epoch": 0.7433489827856025,
        "step": 9975
    },
    {
        "loss": 2.2829,
        "grad_norm": 2.689218521118164,
        "learning_rate": 0.00017539174033035456,
        "epoch": 0.7434235039868843,
        "step": 9976
    },
    {
        "loss": 2.6178,
        "grad_norm": 2.2963929176330566,
        "learning_rate": 0.00017534549359015834,
        "epoch": 0.743498025188166,
        "step": 9977
    },
    {
        "loss": 2.4103,
        "grad_norm": 2.918323278427124,
        "learning_rate": 0.00017529920954394113,
        "epoch": 0.7435725463894478,
        "step": 9978
    },
    {
        "loss": 2.3692,
        "grad_norm": 3.3509626388549805,
        "learning_rate": 0.00017525288821461962,
        "epoch": 0.7436470675907295,
        "step": 9979
    },
    {
        "loss": 2.609,
        "grad_norm": 4.138700008392334,
        "learning_rate": 0.0001752065296251292,
        "epoch": 0.7437215887920113,
        "step": 9980
    },
    {
        "loss": 2.8602,
        "grad_norm": 2.123974084854126,
        "learning_rate": 0.00017516013379842332,
        "epoch": 0.743796109993293,
        "step": 9981
    },
    {
        "loss": 2.1342,
        "grad_norm": 3.4780993461608887,
        "learning_rate": 0.00017511370075747433,
        "epoch": 0.7438706311945749,
        "step": 9982
    },
    {
        "loss": 2.9891,
        "grad_norm": 1.7309081554412842,
        "learning_rate": 0.00017506723052527242,
        "epoch": 0.7439451523958566,
        "step": 9983
    },
    {
        "loss": 3.1097,
        "grad_norm": 2.6562883853912354,
        "learning_rate": 0.0001750207231248268,
        "epoch": 0.7440196735971384,
        "step": 9984
    },
    {
        "loss": 2.0579,
        "grad_norm": 2.840031862258911,
        "learning_rate": 0.0001749741785791647,
        "epoch": 0.7440941947984202,
        "step": 9985
    },
    {
        "loss": 2.475,
        "grad_norm": 2.929063558578491,
        "learning_rate": 0.00017492759691133176,
        "epoch": 0.7441687159997019,
        "step": 9986
    },
    {
        "loss": 1.9705,
        "grad_norm": 3.12642765045166,
        "learning_rate": 0.00017488097814439226,
        "epoch": 0.7442432372009837,
        "step": 9987
    },
    {
        "loss": 2.6252,
        "grad_norm": 3.149176597595215,
        "learning_rate": 0.0001748343223014286,
        "epoch": 0.7443177584022654,
        "step": 9988
    },
    {
        "loss": 2.2174,
        "grad_norm": 4.2325029373168945,
        "learning_rate": 0.00017478762940554153,
        "epoch": 0.7443922796035473,
        "step": 9989
    },
    {
        "loss": 2.7244,
        "grad_norm": 2.580043077468872,
        "learning_rate": 0.00017474089947985038,
        "epoch": 0.744466800804829,
        "step": 9990
    },
    {
        "loss": 2.6181,
        "grad_norm": 3.3235318660736084,
        "learning_rate": 0.00017469413254749263,
        "epoch": 0.7445413220061108,
        "step": 9991
    },
    {
        "loss": 1.8541,
        "grad_norm": 3.9668827056884766,
        "learning_rate": 0.000174647328631624,
        "epoch": 0.7446158432073925,
        "step": 9992
    },
    {
        "loss": 1.7388,
        "grad_norm": 4.615148544311523,
        "learning_rate": 0.00017460048775541891,
        "epoch": 0.7446903644086743,
        "step": 9993
    },
    {
        "loss": 2.4489,
        "grad_norm": 4.03980827331543,
        "learning_rate": 0.00017455360994206943,
        "epoch": 0.744764885609956,
        "step": 9994
    },
    {
        "loss": 2.2914,
        "grad_norm": 2.709822654724121,
        "learning_rate": 0.00017450669521478664,
        "epoch": 0.7448394068112378,
        "step": 9995
    },
    {
        "loss": 2.6527,
        "grad_norm": 2.0332210063934326,
        "learning_rate": 0.0001744597435967994,
        "epoch": 0.7449139280125195,
        "step": 9996
    },
    {
        "loss": 2.1591,
        "grad_norm": 2.92952561378479,
        "learning_rate": 0.00017441275511135488,
        "epoch": 0.7449884492138014,
        "step": 9997
    },
    {
        "loss": 2.7005,
        "grad_norm": 3.850989580154419,
        "learning_rate": 0.0001743657297817189,
        "epoch": 0.7450629704150831,
        "step": 9998
    },
    {
        "loss": 1.8366,
        "grad_norm": 6.175938606262207,
        "learning_rate": 0.000174318667631175,
        "epoch": 0.7451374916163649,
        "step": 9999
    },
    {
        "loss": 2.663,
        "grad_norm": 1.513209342956543,
        "learning_rate": 0.00017427156868302535,
        "epoch": 0.7452120128176466,
        "step": 10000
    },
    {
        "loss": 2.3291,
        "grad_norm": 3.0260651111602783,
        "learning_rate": 0.00017422443296059013,
        "epoch": 0.7452865340189284,
        "step": 10001
    },
    {
        "loss": 2.7468,
        "grad_norm": 2.5400068759918213,
        "learning_rate": 0.00017417726048720775,
        "epoch": 0.7453610552202101,
        "step": 10002
    },
    {
        "loss": 1.5797,
        "grad_norm": 5.759977340698242,
        "learning_rate": 0.0001741300512862348,
        "epoch": 0.7454355764214919,
        "step": 10003
    },
    {
        "loss": 1.4547,
        "grad_norm": 3.0528571605682373,
        "learning_rate": 0.00017408280538104624,
        "epoch": 0.7455100976227736,
        "step": 10004
    },
    {
        "loss": 2.3029,
        "grad_norm": 3.458906888961792,
        "learning_rate": 0.000174035522795035,
        "epoch": 0.7455846188240555,
        "step": 10005
    },
    {
        "loss": 2.3896,
        "grad_norm": 3.5381031036376953,
        "learning_rate": 0.00017398820355161217,
        "epoch": 0.7456591400253372,
        "step": 10006
    },
    {
        "loss": 2.3164,
        "grad_norm": 4.149101257324219,
        "learning_rate": 0.0001739408476742071,
        "epoch": 0.745733661226619,
        "step": 10007
    },
    {
        "loss": 2.7411,
        "grad_norm": 3.099973678588867,
        "learning_rate": 0.00017389345518626728,
        "epoch": 0.7458081824279007,
        "step": 10008
    },
    {
        "loss": 2.2987,
        "grad_norm": 2.9036672115325928,
        "learning_rate": 0.00017384602611125816,
        "epoch": 0.7458827036291825,
        "step": 10009
    },
    {
        "loss": 1.596,
        "grad_norm": 4.141575336456299,
        "learning_rate": 0.00017379856047266366,
        "epoch": 0.7459572248304642,
        "step": 10010
    },
    {
        "loss": 1.8864,
        "grad_norm": 3.215705633163452,
        "learning_rate": 0.00017375105829398537,
        "epoch": 0.746031746031746,
        "step": 10011
    },
    {
        "loss": 3.0119,
        "grad_norm": 1.3559030294418335,
        "learning_rate": 0.00017370351959874317,
        "epoch": 0.7461062672330278,
        "step": 10012
    },
    {
        "loss": 1.3886,
        "grad_norm": 3.3838117122650146,
        "learning_rate": 0.00017365594441047527,
        "epoch": 0.7461807884343096,
        "step": 10013
    },
    {
        "loss": 2.0739,
        "grad_norm": 4.1178460121154785,
        "learning_rate": 0.00017360833275273735,
        "epoch": 0.7462553096355913,
        "step": 10014
    },
    {
        "loss": 2.2445,
        "grad_norm": 2.9857795238494873,
        "learning_rate": 0.00017356068464910375,
        "epoch": 0.7463298308368731,
        "step": 10015
    },
    {
        "loss": 2.5122,
        "grad_norm": 2.878675937652588,
        "learning_rate": 0.00017351300012316654,
        "epoch": 0.7464043520381548,
        "step": 10016
    },
    {
        "loss": 1.5467,
        "grad_norm": 3.8753139972686768,
        "learning_rate": 0.00017346527919853577,
        "epoch": 0.7464788732394366,
        "step": 10017
    },
    {
        "loss": 2.2021,
        "grad_norm": 3.031944990158081,
        "learning_rate": 0.00017341752189883983,
        "epoch": 0.7465533944407183,
        "step": 10018
    },
    {
        "loss": 2.7878,
        "grad_norm": 2.270482301712036,
        "learning_rate": 0.00017336972824772478,
        "epoch": 0.7466279156420002,
        "step": 10019
    },
    {
        "loss": 2.3672,
        "grad_norm": 2.836599826812744,
        "learning_rate": 0.00017332189826885493,
        "epoch": 0.746702436843282,
        "step": 10020
    },
    {
        "loss": 1.6779,
        "grad_norm": 1.6401182413101196,
        "learning_rate": 0.00017327403198591244,
        "epoch": 0.7467769580445637,
        "step": 10021
    },
    {
        "loss": 2.937,
        "grad_norm": 4.3388471603393555,
        "learning_rate": 0.0001732261294225974,
        "epoch": 0.7468514792458455,
        "step": 10022
    },
    {
        "loss": 2.9806,
        "grad_norm": 3.013794422149658,
        "learning_rate": 0.00017317819060262793,
        "epoch": 0.7469260004471272,
        "step": 10023
    },
    {
        "loss": 2.5594,
        "grad_norm": 3.5379488468170166,
        "learning_rate": 0.00017313021554974024,
        "epoch": 0.747000521648409,
        "step": 10024
    },
    {
        "loss": 2.2859,
        "grad_norm": 4.268285274505615,
        "learning_rate": 0.00017308220428768832,
        "epoch": 0.7470750428496907,
        "step": 10025
    },
    {
        "loss": 2.4207,
        "grad_norm": 2.8618087768554688,
        "learning_rate": 0.00017303415684024405,
        "epoch": 0.7471495640509725,
        "step": 10026
    },
    {
        "loss": 2.939,
        "grad_norm": 2.654949188232422,
        "learning_rate": 0.0001729860732311972,
        "epoch": 0.7472240852522543,
        "step": 10027
    },
    {
        "loss": 3.0009,
        "grad_norm": 2.412702798843384,
        "learning_rate": 0.00017293795348435578,
        "epoch": 0.7472986064535361,
        "step": 10028
    },
    {
        "loss": 1.5767,
        "grad_norm": 4.013646602630615,
        "learning_rate": 0.0001728897976235453,
        "epoch": 0.7473731276548178,
        "step": 10029
    },
    {
        "loss": 2.658,
        "grad_norm": 2.3053808212280273,
        "learning_rate": 0.00017284160567260926,
        "epoch": 0.7474476488560996,
        "step": 10030
    },
    {
        "loss": 1.8593,
        "grad_norm": 3.2281174659729004,
        "learning_rate": 0.00017279337765540925,
        "epoch": 0.7475221700573813,
        "step": 10031
    },
    {
        "loss": 2.5133,
        "grad_norm": 2.4033408164978027,
        "learning_rate": 0.00017274511359582432,
        "epoch": 0.7475966912586631,
        "step": 10032
    },
    {
        "loss": 2.4441,
        "grad_norm": 3.170696973800659,
        "learning_rate": 0.00017269681351775188,
        "epoch": 0.7476712124599448,
        "step": 10033
    },
    {
        "loss": 2.501,
        "grad_norm": 3.9005017280578613,
        "learning_rate": 0.00017264847744510653,
        "epoch": 0.7477457336612267,
        "step": 10034
    },
    {
        "loss": 2.2283,
        "grad_norm": 3.0184977054595947,
        "learning_rate": 0.00017260010540182133,
        "epoch": 0.7478202548625084,
        "step": 10035
    },
    {
        "loss": 2.4184,
        "grad_norm": 4.41353178024292,
        "learning_rate": 0.00017255169741184668,
        "epoch": 0.7478947760637902,
        "step": 10036
    },
    {
        "loss": 2.5387,
        "grad_norm": 4.123635768890381,
        "learning_rate": 0.000172503253499151,
        "epoch": 0.7479692972650719,
        "step": 10037
    },
    {
        "loss": 2.8541,
        "grad_norm": 3.8835580348968506,
        "learning_rate": 0.00017245477368772055,
        "epoch": 0.7480438184663537,
        "step": 10038
    },
    {
        "loss": 2.5591,
        "grad_norm": 4.962296485900879,
        "learning_rate": 0.00017240625800155922,
        "epoch": 0.7481183396676354,
        "step": 10039
    },
    {
        "loss": 1.7221,
        "grad_norm": 3.3056037425994873,
        "learning_rate": 0.00017235770646468858,
        "epoch": 0.7481928608689172,
        "step": 10040
    },
    {
        "loss": 1.8236,
        "grad_norm": 5.0353684425354,
        "learning_rate": 0.00017230911910114834,
        "epoch": 0.7482673820701989,
        "step": 10041
    },
    {
        "loss": 2.4241,
        "grad_norm": 2.365617275238037,
        "learning_rate": 0.00017226049593499555,
        "epoch": 0.7483419032714808,
        "step": 10042
    },
    {
        "loss": 2.5411,
        "grad_norm": 2.763261079788208,
        "learning_rate": 0.00017221183699030505,
        "epoch": 0.7484164244727625,
        "step": 10043
    },
    {
        "loss": 2.1623,
        "grad_norm": 5.538515567779541,
        "learning_rate": 0.00017216314229116976,
        "epoch": 0.7484909456740443,
        "step": 10044
    },
    {
        "loss": 2.0772,
        "grad_norm": 3.950821876525879,
        "learning_rate": 0.0001721144118616996,
        "epoch": 0.748565466875326,
        "step": 10045
    },
    {
        "loss": 2.0538,
        "grad_norm": 3.492668867111206,
        "learning_rate": 0.000172065645726023,
        "epoch": 0.7486399880766078,
        "step": 10046
    },
    {
        "loss": 1.6928,
        "grad_norm": 2.527218818664551,
        "learning_rate": 0.00017201684390828536,
        "epoch": 0.7487145092778895,
        "step": 10047
    },
    {
        "loss": 1.7643,
        "grad_norm": 4.183103561401367,
        "learning_rate": 0.00017196800643265033,
        "epoch": 0.7487890304791713,
        "step": 10048
    },
    {
        "loss": 2.4438,
        "grad_norm": 2.010937452316284,
        "learning_rate": 0.00017191913332329878,
        "epoch": 0.748863551680453,
        "step": 10049
    },
    {
        "loss": 2.1871,
        "grad_norm": 3.249039649963379,
        "learning_rate": 0.0001718702246044293,
        "epoch": 0.7489380728817349,
        "step": 10050
    },
    {
        "loss": 2.4264,
        "grad_norm": 3.4725749492645264,
        "learning_rate": 0.0001718212803002585,
        "epoch": 0.7490125940830166,
        "step": 10051
    },
    {
        "loss": 2.5291,
        "grad_norm": 3.110701322555542,
        "learning_rate": 0.00017177230043501996,
        "epoch": 0.7490871152842984,
        "step": 10052
    },
    {
        "loss": 2.9153,
        "grad_norm": 2.2997848987579346,
        "learning_rate": 0.00017172328503296558,
        "epoch": 0.7491616364855801,
        "step": 10053
    },
    {
        "loss": 2.5401,
        "grad_norm": 4.182703495025635,
        "learning_rate": 0.00017167423411836415,
        "epoch": 0.7492361576868619,
        "step": 10054
    },
    {
        "loss": 2.662,
        "grad_norm": 2.8076322078704834,
        "learning_rate": 0.00017162514771550255,
        "epoch": 0.7493106788881436,
        "step": 10055
    },
    {
        "loss": 2.2941,
        "grad_norm": 2.445622444152832,
        "learning_rate": 0.00017157602584868512,
        "epoch": 0.7493852000894254,
        "step": 10056
    },
    {
        "loss": 2.7316,
        "grad_norm": 3.0814778804779053,
        "learning_rate": 0.00017152686854223347,
        "epoch": 0.7494597212907073,
        "step": 10057
    },
    {
        "loss": 2.3813,
        "grad_norm": 5.307817459106445,
        "learning_rate": 0.00017147767582048723,
        "epoch": 0.749534242491989,
        "step": 10058
    },
    {
        "loss": 2.6686,
        "grad_norm": 3.8359601497650146,
        "learning_rate": 0.00017142844770780328,
        "epoch": 0.7496087636932708,
        "step": 10059
    },
    {
        "loss": 1.749,
        "grad_norm": 3.407755136489868,
        "learning_rate": 0.00017137918422855595,
        "epoch": 0.7496832848945525,
        "step": 10060
    },
    {
        "loss": 2.7607,
        "grad_norm": 3.055246591567993,
        "learning_rate": 0.00017132988540713738,
        "epoch": 0.7497578060958343,
        "step": 10061
    },
    {
        "loss": 1.6732,
        "grad_norm": 3.4438514709472656,
        "learning_rate": 0.0001712805512679569,
        "epoch": 0.749832327297116,
        "step": 10062
    },
    {
        "loss": 0.9258,
        "grad_norm": 4.048305988311768,
        "learning_rate": 0.00017123118183544144,
        "epoch": 0.7499068484983978,
        "step": 10063
    },
    {
        "loss": 2.6789,
        "grad_norm": 4.539123058319092,
        "learning_rate": 0.00017118177713403566,
        "epoch": 0.7499813696996795,
        "step": 10064
    },
    {
        "loss": 1.6604,
        "grad_norm": 3.4059269428253174,
        "learning_rate": 0.00017113233718820108,
        "epoch": 0.7500558909009614,
        "step": 10065
    },
    {
        "loss": 2.1388,
        "grad_norm": 4.150710582733154,
        "learning_rate": 0.0001710828620224173,
        "epoch": 0.7501304121022431,
        "step": 10066
    },
    {
        "loss": 2.5571,
        "grad_norm": 2.7925777435302734,
        "learning_rate": 0.000171033351661181,
        "epoch": 0.7502049333035249,
        "step": 10067
    },
    {
        "loss": 2.5014,
        "grad_norm": 2.476303815841675,
        "learning_rate": 0.00017098380612900633,
        "epoch": 0.7502794545048066,
        "step": 10068
    },
    {
        "loss": 2.5023,
        "grad_norm": 2.6359822750091553,
        "learning_rate": 0.00017093422545042507,
        "epoch": 0.7503539757060884,
        "step": 10069
    },
    {
        "loss": 2.773,
        "grad_norm": 2.8388049602508545,
        "learning_rate": 0.000170884609649986,
        "epoch": 0.7504284969073701,
        "step": 10070
    },
    {
        "loss": 2.6432,
        "grad_norm": 3.0081701278686523,
        "learning_rate": 0.00017083495875225578,
        "epoch": 0.7505030181086519,
        "step": 10071
    },
    {
        "loss": 2.2887,
        "grad_norm": 2.721733808517456,
        "learning_rate": 0.00017078527278181811,
        "epoch": 0.7505775393099337,
        "step": 10072
    },
    {
        "loss": 1.8249,
        "grad_norm": 2.438120126724243,
        "learning_rate": 0.0001707355517632741,
        "epoch": 0.7506520605112155,
        "step": 10073
    },
    {
        "loss": 2.6436,
        "grad_norm": 6.000904560089111,
        "learning_rate": 0.00017068579572124214,
        "epoch": 0.7507265817124972,
        "step": 10074
    },
    {
        "loss": 2.8756,
        "grad_norm": 2.8446900844573975,
        "learning_rate": 0.00017063600468035834,
        "epoch": 0.750801102913779,
        "step": 10075
    },
    {
        "loss": 2.2881,
        "grad_norm": 4.009660720825195,
        "learning_rate": 0.00017058617866527569,
        "epoch": 0.7508756241150607,
        "step": 10076
    },
    {
        "loss": 2.5218,
        "grad_norm": 2.714848518371582,
        "learning_rate": 0.00017053631770066476,
        "epoch": 0.7509501453163425,
        "step": 10077
    },
    {
        "loss": 2.6209,
        "grad_norm": 2.4013314247131348,
        "learning_rate": 0.0001704864218112132,
        "epoch": 0.7510246665176242,
        "step": 10078
    },
    {
        "loss": 2.8722,
        "grad_norm": 2.3906683921813965,
        "learning_rate": 0.00017043649102162632,
        "epoch": 0.751099187718906,
        "step": 10079
    },
    {
        "loss": 2.3399,
        "grad_norm": 3.010594129562378,
        "learning_rate": 0.0001703865253566263,
        "epoch": 0.7511737089201878,
        "step": 10080
    },
    {
        "loss": 2.4711,
        "grad_norm": 1.9259687662124634,
        "learning_rate": 0.00017033652484095291,
        "epoch": 0.7512482301214696,
        "step": 10081
    },
    {
        "loss": 1.6244,
        "grad_norm": 4.569585800170898,
        "learning_rate": 0.00017028648949936304,
        "epoch": 0.7513227513227513,
        "step": 10082
    },
    {
        "loss": 2.6169,
        "grad_norm": 3.2581305503845215,
        "learning_rate": 0.00017023641935663063,
        "epoch": 0.7513972725240331,
        "step": 10083
    },
    {
        "loss": 3.0346,
        "grad_norm": 2.501387357711792,
        "learning_rate": 0.00017018631443754744,
        "epoch": 0.7514717937253148,
        "step": 10084
    },
    {
        "loss": 2.6466,
        "grad_norm": 1.9578348398208618,
        "learning_rate": 0.0001701361747669216,
        "epoch": 0.7515463149265966,
        "step": 10085
    },
    {
        "loss": 2.0228,
        "grad_norm": 2.0936062335968018,
        "learning_rate": 0.0001700860003695792,
        "epoch": 0.7516208361278783,
        "step": 10086
    },
    {
        "loss": 2.3933,
        "grad_norm": 2.564650774002075,
        "learning_rate": 0.00017003579127036314,
        "epoch": 0.7516953573291602,
        "step": 10087
    },
    {
        "loss": 2.2152,
        "grad_norm": 3.286600112915039,
        "learning_rate": 0.0001699855474941335,
        "epoch": 0.7517698785304419,
        "step": 10088
    },
    {
        "loss": 2.1826,
        "grad_norm": 3.571136236190796,
        "learning_rate": 0.00016993526906576779,
        "epoch": 0.7518443997317237,
        "step": 10089
    },
    {
        "loss": 2.1332,
        "grad_norm": 2.9028208255767822,
        "learning_rate": 0.00016988495601016044,
        "epoch": 0.7519189209330054,
        "step": 10090
    },
    {
        "loss": 2.3358,
        "grad_norm": 4.665188789367676,
        "learning_rate": 0.00016983460835222295,
        "epoch": 0.7519934421342872,
        "step": 10091
    },
    {
        "loss": 2.534,
        "grad_norm": 2.88157057762146,
        "learning_rate": 0.0001697842261168843,
        "epoch": 0.752067963335569,
        "step": 10092
    },
    {
        "loss": 3.2325,
        "grad_norm": 2.146400213241577,
        "learning_rate": 0.00016973380932909038,
        "epoch": 0.7521424845368507,
        "step": 10093
    },
    {
        "loss": 2.3769,
        "grad_norm": 2.370565891265869,
        "learning_rate": 0.00016968335801380397,
        "epoch": 0.7522170057381325,
        "step": 10094
    },
    {
        "loss": 2.3788,
        "grad_norm": 4.085542678833008,
        "learning_rate": 0.00016963287219600551,
        "epoch": 0.7522915269394143,
        "step": 10095
    },
    {
        "loss": 2.3232,
        "grad_norm": 3.0288095474243164,
        "learning_rate": 0.0001695823519006918,
        "epoch": 0.7523660481406961,
        "step": 10096
    },
    {
        "loss": 2.854,
        "grad_norm": 2.834799289703369,
        "learning_rate": 0.00016953179715287737,
        "epoch": 0.7524405693419778,
        "step": 10097
    },
    {
        "loss": 2.3083,
        "grad_norm": 5.509127140045166,
        "learning_rate": 0.00016948120797759332,
        "epoch": 0.7525150905432596,
        "step": 10098
    },
    {
        "loss": 0.9318,
        "grad_norm": 3.0783069133758545,
        "learning_rate": 0.00016943058439988827,
        "epoch": 0.7525896117445413,
        "step": 10099
    },
    {
        "loss": 1.7469,
        "grad_norm": 1.9464138746261597,
        "learning_rate": 0.00016937992644482742,
        "epoch": 0.7526641329458231,
        "step": 10100
    },
    {
        "loss": 1.7021,
        "grad_norm": 3.0926434993743896,
        "learning_rate": 0.00016932923413749316,
        "epoch": 0.7527386541471048,
        "step": 10101
    },
    {
        "loss": 2.119,
        "grad_norm": 3.6080446243286133,
        "learning_rate": 0.00016927850750298506,
        "epoch": 0.7528131753483867,
        "step": 10102
    },
    {
        "loss": 1.8376,
        "grad_norm": 4.042576789855957,
        "learning_rate": 0.00016922774656641937,
        "epoch": 0.7528876965496684,
        "step": 10103
    },
    {
        "loss": 2.3043,
        "grad_norm": 1.947036623954773,
        "learning_rate": 0.00016917695135292978,
        "epoch": 0.7529622177509502,
        "step": 10104
    },
    {
        "loss": 2.4635,
        "grad_norm": 3.6042819023132324,
        "learning_rate": 0.00016912612188766627,
        "epoch": 0.7530367389522319,
        "step": 10105
    },
    {
        "loss": 2.0096,
        "grad_norm": 3.77394962310791,
        "learning_rate": 0.00016907525819579648,
        "epoch": 0.7531112601535137,
        "step": 10106
    },
    {
        "loss": 2.4141,
        "grad_norm": 3.8233792781829834,
        "learning_rate": 0.00016902436030250456,
        "epoch": 0.7531857813547954,
        "step": 10107
    },
    {
        "loss": 2.1539,
        "grad_norm": 3.8634402751922607,
        "learning_rate": 0.0001689734282329916,
        "epoch": 0.7532603025560772,
        "step": 10108
    },
    {
        "loss": 2.6507,
        "grad_norm": 3.0221755504608154,
        "learning_rate": 0.00016892246201247603,
        "epoch": 0.7533348237573589,
        "step": 10109
    },
    {
        "loss": 2.2103,
        "grad_norm": 3.760528564453125,
        "learning_rate": 0.0001688714616661927,
        "epoch": 0.7534093449586408,
        "step": 10110
    },
    {
        "loss": 1.5952,
        "grad_norm": 3.56372332572937,
        "learning_rate": 0.00016882042721939354,
        "epoch": 0.7534838661599225,
        "step": 10111
    },
    {
        "loss": 2.2887,
        "grad_norm": 4.116607666015625,
        "learning_rate": 0.00016876935869734747,
        "epoch": 0.7535583873612043,
        "step": 10112
    },
    {
        "loss": 3.0205,
        "grad_norm": 3.5032033920288086,
        "learning_rate": 0.00016871825612534017,
        "epoch": 0.753632908562486,
        "step": 10113
    },
    {
        "loss": 3.0052,
        "grad_norm": 1.8408293724060059,
        "learning_rate": 0.00016866711952867407,
        "epoch": 0.7537074297637678,
        "step": 10114
    },
    {
        "loss": 2.5436,
        "grad_norm": 2.172739267349243,
        "learning_rate": 0.00016861594893266885,
        "epoch": 0.7537819509650495,
        "step": 10115
    },
    {
        "loss": 2.1827,
        "grad_norm": 2.826660394668579,
        "learning_rate": 0.0001685647443626604,
        "epoch": 0.7538564721663313,
        "step": 10116
    },
    {
        "loss": 2.6291,
        "grad_norm": 3.095104455947876,
        "learning_rate": 0.00016851350584400204,
        "epoch": 0.753930993367613,
        "step": 10117
    },
    {
        "loss": 1.8361,
        "grad_norm": 2.8579392433166504,
        "learning_rate": 0.00016846223340206344,
        "epoch": 0.7540055145688949,
        "step": 10118
    },
    {
        "loss": 2.2015,
        "grad_norm": 4.157524585723877,
        "learning_rate": 0.0001684109270622315,
        "epoch": 0.7540800357701766,
        "step": 10119
    },
    {
        "loss": 2.6488,
        "grad_norm": 3.899639368057251,
        "learning_rate": 0.00016835958684990955,
        "epoch": 0.7541545569714584,
        "step": 10120
    },
    {
        "loss": 2.5651,
        "grad_norm": 2.293278217315674,
        "learning_rate": 0.00016830821279051764,
        "epoch": 0.7542290781727401,
        "step": 10121
    },
    {
        "loss": 2.7928,
        "grad_norm": 2.7436530590057373,
        "learning_rate": 0.00016825680490949307,
        "epoch": 0.7543035993740219,
        "step": 10122
    },
    {
        "loss": 2.3602,
        "grad_norm": 2.7384095191955566,
        "learning_rate": 0.00016820536323228937,
        "epoch": 0.7543781205753036,
        "step": 10123
    },
    {
        "loss": 2.4515,
        "grad_norm": 3.3474912643432617,
        "learning_rate": 0.00016815388778437703,
        "epoch": 0.7544526417765854,
        "step": 10124
    },
    {
        "loss": 2.4502,
        "grad_norm": 2.4307215213775635,
        "learning_rate": 0.00016810237859124318,
        "epoch": 0.7545271629778671,
        "step": 10125
    },
    {
        "loss": 1.8039,
        "grad_norm": 2.619913101196289,
        "learning_rate": 0.00016805083567839182,
        "epoch": 0.754601684179149,
        "step": 10126
    },
    {
        "loss": 2.7527,
        "grad_norm": 2.2633960247039795,
        "learning_rate": 0.0001679992590713435,
        "epoch": 0.7546762053804308,
        "step": 10127
    },
    {
        "loss": 2.5666,
        "grad_norm": 3.114851713180542,
        "learning_rate": 0.00016794764879563546,
        "epoch": 0.7547507265817125,
        "step": 10128
    },
    {
        "loss": 2.2521,
        "grad_norm": 2.7746341228485107,
        "learning_rate": 0.00016789600487682153,
        "epoch": 0.7548252477829943,
        "step": 10129
    },
    {
        "loss": 2.1426,
        "grad_norm": 3.1387553215026855,
        "learning_rate": 0.00016784432734047252,
        "epoch": 0.754899768984276,
        "step": 10130
    },
    {
        "loss": 1.5886,
        "grad_norm": 3.4025468826293945,
        "learning_rate": 0.0001677926162121755,
        "epoch": 0.7549742901855578,
        "step": 10131
    },
    {
        "loss": 1.8099,
        "grad_norm": 4.002850532531738,
        "learning_rate": 0.00016774087151753448,
        "epoch": 0.7550488113868395,
        "step": 10132
    },
    {
        "loss": 2.8132,
        "grad_norm": 3.32735013961792,
        "learning_rate": 0.00016768909328216988,
        "epoch": 0.7551233325881214,
        "step": 10133
    },
    {
        "loss": 2.3297,
        "grad_norm": 3.6063594818115234,
        "learning_rate": 0.00016763728153171872,
        "epoch": 0.7551978537894031,
        "step": 10134
    },
    {
        "loss": 2.3251,
        "grad_norm": 3.2280309200286865,
        "learning_rate": 0.00016758543629183495,
        "epoch": 0.7552723749906849,
        "step": 10135
    },
    {
        "loss": 1.9789,
        "grad_norm": 2.133024215698242,
        "learning_rate": 0.0001675335575881885,
        "epoch": 0.7553468961919666,
        "step": 10136
    },
    {
        "loss": 2.4784,
        "grad_norm": 3.223268747329712,
        "learning_rate": 0.0001674816454464665,
        "epoch": 0.7554214173932484,
        "step": 10137
    },
    {
        "loss": 1.8841,
        "grad_norm": 3.608333110809326,
        "learning_rate": 0.0001674296998923722,
        "epoch": 0.7554959385945301,
        "step": 10138
    },
    {
        "loss": 1.7542,
        "grad_norm": 3.0654890537261963,
        "learning_rate": 0.0001673777209516255,
        "epoch": 0.7555704597958119,
        "step": 10139
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.819833278656006,
        "learning_rate": 0.00016732570864996305,
        "epoch": 0.7556449809970937,
        "step": 10140
    },
    {
        "loss": 2.3011,
        "grad_norm": 2.920340061187744,
        "learning_rate": 0.00016727366301313765,
        "epoch": 0.7557195021983755,
        "step": 10141
    },
    {
        "loss": 2.1849,
        "grad_norm": 3.6028878688812256,
        "learning_rate": 0.000167221584066919,
        "epoch": 0.7557940233996572,
        "step": 10142
    },
    {
        "loss": 2.6318,
        "grad_norm": 2.9339473247528076,
        "learning_rate": 0.000167169471837093,
        "epoch": 0.755868544600939,
        "step": 10143
    },
    {
        "loss": 3.0161,
        "grad_norm": 2.740786552429199,
        "learning_rate": 0.00016711732634946212,
        "epoch": 0.7559430658022207,
        "step": 10144
    },
    {
        "loss": 2.2356,
        "grad_norm": 3.2263100147247314,
        "learning_rate": 0.00016706514762984518,
        "epoch": 0.7560175870035025,
        "step": 10145
    },
    {
        "loss": 2.3461,
        "grad_norm": 2.96644926071167,
        "learning_rate": 0.0001670129357040778,
        "epoch": 0.7560921082047842,
        "step": 10146
    },
    {
        "loss": 2.4597,
        "grad_norm": 3.068635940551758,
        "learning_rate": 0.0001669606905980117,
        "epoch": 0.756166629406066,
        "step": 10147
    },
    {
        "loss": 2.5228,
        "grad_norm": 3.534949779510498,
        "learning_rate": 0.00016690841233751513,
        "epoch": 0.7562411506073478,
        "step": 10148
    },
    {
        "loss": 1.442,
        "grad_norm": 1.4319761991500854,
        "learning_rate": 0.00016685610094847267,
        "epoch": 0.7563156718086296,
        "step": 10149
    },
    {
        "loss": 2.161,
        "grad_norm": 4.638673305511475,
        "learning_rate": 0.00016680375645678565,
        "epoch": 0.7563901930099113,
        "step": 10150
    },
    {
        "loss": 2.7151,
        "grad_norm": 2.4275476932525635,
        "learning_rate": 0.00016675137888837126,
        "epoch": 0.7564647142111931,
        "step": 10151
    },
    {
        "loss": 2.3819,
        "grad_norm": 3.135528564453125,
        "learning_rate": 0.00016669896826916361,
        "epoch": 0.7565392354124748,
        "step": 10152
    },
    {
        "loss": 2.4794,
        "grad_norm": 3.046166181564331,
        "learning_rate": 0.0001666465246251128,
        "epoch": 0.7566137566137566,
        "step": 10153
    },
    {
        "loss": 1.8937,
        "grad_norm": 3.3053719997406006,
        "learning_rate": 0.0001665940479821853,
        "epoch": 0.7566882778150383,
        "step": 10154
    },
    {
        "loss": 2.0621,
        "grad_norm": 3.425480365753174,
        "learning_rate": 0.0001665415383663643,
        "epoch": 0.7567627990163202,
        "step": 10155
    },
    {
        "loss": 2.1417,
        "grad_norm": 3.9602270126342773,
        "learning_rate": 0.00016648899580364862,
        "epoch": 0.7568373202176019,
        "step": 10156
    },
    {
        "loss": 2.2125,
        "grad_norm": 3.0687360763549805,
        "learning_rate": 0.00016643642032005413,
        "epoch": 0.7569118414188837,
        "step": 10157
    },
    {
        "loss": 1.777,
        "grad_norm": 3.4780113697052,
        "learning_rate": 0.00016638381194161256,
        "epoch": 0.7569863626201654,
        "step": 10158
    },
    {
        "loss": 2.7477,
        "grad_norm": 2.3828015327453613,
        "learning_rate": 0.00016633117069437197,
        "epoch": 0.7570608838214472,
        "step": 10159
    },
    {
        "loss": 2.4293,
        "grad_norm": 3.495671510696411,
        "learning_rate": 0.0001662784966043969,
        "epoch": 0.7571354050227289,
        "step": 10160
    },
    {
        "loss": 1.327,
        "grad_norm": 3.674243927001953,
        "learning_rate": 0.000166225789697768,
        "epoch": 0.7572099262240107,
        "step": 10161
    },
    {
        "loss": 2.0583,
        "grad_norm": 2.8699557781219482,
        "learning_rate": 0.000166173050000582,
        "epoch": 0.7572844474252926,
        "step": 10162
    },
    {
        "loss": 1.8987,
        "grad_norm": 2.795764446258545,
        "learning_rate": 0.00016612027753895228,
        "epoch": 0.7573589686265743,
        "step": 10163
    },
    {
        "loss": 2.5941,
        "grad_norm": 2.2526986598968506,
        "learning_rate": 0.00016606747233900818,
        "epoch": 0.7574334898278561,
        "step": 10164
    },
    {
        "loss": 3.0272,
        "grad_norm": 3.464812755584717,
        "learning_rate": 0.00016601463442689508,
        "epoch": 0.7575080110291378,
        "step": 10165
    },
    {
        "loss": 2.4875,
        "grad_norm": 2.2847023010253906,
        "learning_rate": 0.00016596176382877517,
        "epoch": 0.7575825322304196,
        "step": 10166
    },
    {
        "loss": 2.5505,
        "grad_norm": 3.3856184482574463,
        "learning_rate": 0.00016590886057082587,
        "epoch": 0.7576570534317013,
        "step": 10167
    },
    {
        "loss": 2.0928,
        "grad_norm": 3.6375246047973633,
        "learning_rate": 0.00016585592467924175,
        "epoch": 0.7577315746329831,
        "step": 10168
    },
    {
        "loss": 2.5286,
        "grad_norm": 2.3269152641296387,
        "learning_rate": 0.00016580295618023286,
        "epoch": 0.7578060958342648,
        "step": 10169
    },
    {
        "loss": 1.3377,
        "grad_norm": 3.557827949523926,
        "learning_rate": 0.00016574995510002582,
        "epoch": 0.7578806170355467,
        "step": 10170
    },
    {
        "loss": 2.6156,
        "grad_norm": 2.1384437084198,
        "learning_rate": 0.0001656969214648631,
        "epoch": 0.7579551382368284,
        "step": 10171
    },
    {
        "loss": 2.4409,
        "grad_norm": 3.8630969524383545,
        "learning_rate": 0.0001656438553010033,
        "epoch": 0.7580296594381102,
        "step": 10172
    },
    {
        "loss": 2.1961,
        "grad_norm": 2.8213131427764893,
        "learning_rate": 0.00016559075663472142,
        "epoch": 0.7581041806393919,
        "step": 10173
    },
    {
        "loss": 2.7634,
        "grad_norm": 2.6397199630737305,
        "learning_rate": 0.00016553762549230815,
        "epoch": 0.7581787018406737,
        "step": 10174
    },
    {
        "loss": 2.7662,
        "grad_norm": 2.8874945640563965,
        "learning_rate": 0.00016548446190007077,
        "epoch": 0.7582532230419554,
        "step": 10175
    },
    {
        "loss": 2.9309,
        "grad_norm": 2.388188123703003,
        "learning_rate": 0.00016543126588433187,
        "epoch": 0.7583277442432372,
        "step": 10176
    },
    {
        "loss": 1.9253,
        "grad_norm": 3.0838730335235596,
        "learning_rate": 0.00016537803747143087,
        "epoch": 0.7584022654445189,
        "step": 10177
    },
    {
        "loss": 2.4776,
        "grad_norm": 2.46189546585083,
        "learning_rate": 0.0001653247766877228,
        "epoch": 0.7584767866458008,
        "step": 10178
    },
    {
        "loss": 1.9615,
        "grad_norm": 4.049410820007324,
        "learning_rate": 0.0001652714835595787,
        "epoch": 0.7585513078470825,
        "step": 10179
    },
    {
        "loss": 2.3721,
        "grad_norm": 3.186737298965454,
        "learning_rate": 0.00016521815811338598,
        "epoch": 0.7586258290483643,
        "step": 10180
    },
    {
        "loss": 2.6389,
        "grad_norm": 3.069547176361084,
        "learning_rate": 0.0001651648003755477,
        "epoch": 0.758700350249646,
        "step": 10181
    },
    {
        "loss": 1.8852,
        "grad_norm": 3.190612554550171,
        "learning_rate": 0.0001651114103724829,
        "epoch": 0.7587748714509278,
        "step": 10182
    },
    {
        "loss": 1.6723,
        "grad_norm": 3.85003924369812,
        "learning_rate": 0.0001650579881306269,
        "epoch": 0.7588493926522095,
        "step": 10183
    },
    {
        "loss": 1.8367,
        "grad_norm": 4.07834005355835,
        "learning_rate": 0.0001650045336764308,
        "epoch": 0.7589239138534913,
        "step": 10184
    },
    {
        "loss": 2.2323,
        "grad_norm": 3.8084471225738525,
        "learning_rate": 0.00016495104703636145,
        "epoch": 0.758998435054773,
        "step": 10185
    },
    {
        "loss": 2.3771,
        "grad_norm": 3.5098865032196045,
        "learning_rate": 0.00016489752823690217,
        "epoch": 0.7590729562560549,
        "step": 10186
    },
    {
        "loss": 2.6052,
        "grad_norm": 2.7259862422943115,
        "learning_rate": 0.00016484397730455146,
        "epoch": 0.7591474774573366,
        "step": 10187
    },
    {
        "loss": 1.8263,
        "grad_norm": 3.8649075031280518,
        "learning_rate": 0.00016479039426582447,
        "epoch": 0.7592219986586184,
        "step": 10188
    },
    {
        "loss": 2.3573,
        "grad_norm": 3.6490166187286377,
        "learning_rate": 0.0001647367791472518,
        "epoch": 0.7592965198599001,
        "step": 10189
    },
    {
        "loss": 2.9218,
        "grad_norm": 2.2149181365966797,
        "learning_rate": 0.00016468313197537997,
        "epoch": 0.7593710410611819,
        "step": 10190
    },
    {
        "loss": 2.2463,
        "grad_norm": 3.4300291538238525,
        "learning_rate": 0.0001646294527767716,
        "epoch": 0.7594455622624636,
        "step": 10191
    },
    {
        "loss": 2.4926,
        "grad_norm": 2.9973161220550537,
        "learning_rate": 0.00016457574157800487,
        "epoch": 0.7595200834637454,
        "step": 10192
    },
    {
        "loss": 2.0153,
        "grad_norm": 3.866438150405884,
        "learning_rate": 0.0001645219984056741,
        "epoch": 0.7595946046650272,
        "step": 10193
    },
    {
        "loss": 2.9215,
        "grad_norm": 2.7882606983184814,
        "learning_rate": 0.00016446822328638927,
        "epoch": 0.759669125866309,
        "step": 10194
    },
    {
        "loss": 2.8069,
        "grad_norm": 2.383033514022827,
        "learning_rate": 0.00016441441624677616,
        "epoch": 0.7597436470675907,
        "step": 10195
    },
    {
        "loss": 2.1344,
        "grad_norm": 2.2233519554138184,
        "learning_rate": 0.00016436057731347627,
        "epoch": 0.7598181682688725,
        "step": 10196
    },
    {
        "loss": 2.5131,
        "grad_norm": 2.4331157207489014,
        "learning_rate": 0.00016430670651314727,
        "epoch": 0.7598926894701542,
        "step": 10197
    },
    {
        "loss": 2.906,
        "grad_norm": 2.8368277549743652,
        "learning_rate": 0.00016425280387246223,
        "epoch": 0.759967210671436,
        "step": 10198
    },
    {
        "loss": 2.3522,
        "grad_norm": 3.5775556564331055,
        "learning_rate": 0.0001641988694181101,
        "epoch": 0.7600417318727178,
        "step": 10199
    },
    {
        "loss": 2.8701,
        "grad_norm": 3.3436384201049805,
        "learning_rate": 0.00016414490317679552,
        "epoch": 0.7601162530739995,
        "step": 10200
    },
    {
        "loss": 2.4081,
        "grad_norm": 3.885329008102417,
        "learning_rate": 0.00016409090517523913,
        "epoch": 0.7601907742752814,
        "step": 10201
    },
    {
        "loss": 2.0103,
        "grad_norm": 2.6780285835266113,
        "learning_rate": 0.0001640368754401769,
        "epoch": 0.7602652954765631,
        "step": 10202
    },
    {
        "loss": 2.8048,
        "grad_norm": 3.7427220344543457,
        "learning_rate": 0.00016398281399836092,
        "epoch": 0.7603398166778449,
        "step": 10203
    },
    {
        "loss": 1.2756,
        "grad_norm": 4.390156269073486,
        "learning_rate": 0.00016392872087655876,
        "epoch": 0.7604143378791266,
        "step": 10204
    },
    {
        "loss": 2.0137,
        "grad_norm": 4.458734035491943,
        "learning_rate": 0.00016387459610155348,
        "epoch": 0.7604888590804084,
        "step": 10205
    },
    {
        "loss": 2.1773,
        "grad_norm": 3.9882867336273193,
        "learning_rate": 0.0001638204397001444,
        "epoch": 0.7605633802816901,
        "step": 10206
    },
    {
        "loss": 2.384,
        "grad_norm": 6.397099018096924,
        "learning_rate": 0.0001637662516991457,
        "epoch": 0.760637901482972,
        "step": 10207
    },
    {
        "loss": 1.8714,
        "grad_norm": 4.16632080078125,
        "learning_rate": 0.0001637120321253879,
        "epoch": 0.7607124226842537,
        "step": 10208
    },
    {
        "loss": 1.5055,
        "grad_norm": 2.949477195739746,
        "learning_rate": 0.00016365778100571685,
        "epoch": 0.7607869438855355,
        "step": 10209
    },
    {
        "loss": 2.8966,
        "grad_norm": 3.3725194931030273,
        "learning_rate": 0.00016360349836699393,
        "epoch": 0.7608614650868172,
        "step": 10210
    },
    {
        "loss": 1.6048,
        "grad_norm": 3.389373540878296,
        "learning_rate": 0.00016354918423609646,
        "epoch": 0.760935986288099,
        "step": 10211
    },
    {
        "loss": 2.4115,
        "grad_norm": 2.972905158996582,
        "learning_rate": 0.00016349483863991693,
        "epoch": 0.7610105074893807,
        "step": 10212
    },
    {
        "loss": 2.613,
        "grad_norm": 3.7546470165252686,
        "learning_rate": 0.00016344046160536384,
        "epoch": 0.7610850286906625,
        "step": 10213
    },
    {
        "loss": 2.6775,
        "grad_norm": 3.104612112045288,
        "learning_rate": 0.00016338605315936093,
        "epoch": 0.7611595498919442,
        "step": 10214
    },
    {
        "loss": 2.4615,
        "grad_norm": 2.463421583175659,
        "learning_rate": 0.0001633316133288476,
        "epoch": 0.761234071093226,
        "step": 10215
    },
    {
        "loss": 2.4654,
        "grad_norm": 3.7917556762695312,
        "learning_rate": 0.0001632771421407787,
        "epoch": 0.7613085922945078,
        "step": 10216
    },
    {
        "loss": 2.4415,
        "grad_norm": 3.0668134689331055,
        "learning_rate": 0.00016322263962212502,
        "epoch": 0.7613831134957896,
        "step": 10217
    },
    {
        "loss": 2.4961,
        "grad_norm": 2.1647989749908447,
        "learning_rate": 0.00016316810579987208,
        "epoch": 0.7614576346970713,
        "step": 10218
    },
    {
        "loss": 1.7364,
        "grad_norm": 3.7634823322296143,
        "learning_rate": 0.0001631135407010217,
        "epoch": 0.7615321558983531,
        "step": 10219
    },
    {
        "loss": 2.5123,
        "grad_norm": 4.323792457580566,
        "learning_rate": 0.00016305894435259064,
        "epoch": 0.7616066770996348,
        "step": 10220
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.6158714294433594,
        "learning_rate": 0.0001630043167816116,
        "epoch": 0.7616811983009166,
        "step": 10221
    },
    {
        "loss": 2.2211,
        "grad_norm": 3.2696845531463623,
        "learning_rate": 0.00016294965801513228,
        "epoch": 0.7617557195021983,
        "step": 10222
    },
    {
        "loss": 2.5722,
        "grad_norm": 3.014866828918457,
        "learning_rate": 0.00016289496808021597,
        "epoch": 0.7618302407034802,
        "step": 10223
    },
    {
        "loss": 1.8371,
        "grad_norm": 3.253650665283203,
        "learning_rate": 0.00016284024700394164,
        "epoch": 0.7619047619047619,
        "step": 10224
    },
    {
        "loss": 1.606,
        "grad_norm": 2.5824010372161865,
        "learning_rate": 0.00016278549481340334,
        "epoch": 0.7619792831060437,
        "step": 10225
    },
    {
        "loss": 2.3712,
        "grad_norm": 3.1903958320617676,
        "learning_rate": 0.0001627307115357109,
        "epoch": 0.7620538043073254,
        "step": 10226
    },
    {
        "loss": 2.6457,
        "grad_norm": 3.0821964740753174,
        "learning_rate": 0.0001626758971979889,
        "epoch": 0.7621283255086072,
        "step": 10227
    },
    {
        "loss": 2.6926,
        "grad_norm": 2.5153794288635254,
        "learning_rate": 0.0001626210518273781,
        "epoch": 0.7622028467098889,
        "step": 10228
    },
    {
        "loss": 2.15,
        "grad_norm": 3.689840078353882,
        "learning_rate": 0.0001625661754510341,
        "epoch": 0.7622773679111707,
        "step": 10229
    },
    {
        "loss": 2.1689,
        "grad_norm": 2.932079792022705,
        "learning_rate": 0.00016251126809612785,
        "epoch": 0.7623518891124524,
        "step": 10230
    },
    {
        "loss": 3.2023,
        "grad_norm": 3.653231382369995,
        "learning_rate": 0.00016245632978984602,
        "epoch": 0.7624264103137343,
        "step": 10231
    },
    {
        "loss": 2.5165,
        "grad_norm": 3.3940601348876953,
        "learning_rate": 0.0001624013605593903,
        "epoch": 0.762500931515016,
        "step": 10232
    },
    {
        "loss": 2.5397,
        "grad_norm": 2.64414644241333,
        "learning_rate": 0.00016234636043197754,
        "epoch": 0.7625754527162978,
        "step": 10233
    },
    {
        "loss": 1.9102,
        "grad_norm": 4.129530906677246,
        "learning_rate": 0.00016229132943484045,
        "epoch": 0.7626499739175796,
        "step": 10234
    },
    {
        "loss": 2.2564,
        "grad_norm": 3.3922464847564697,
        "learning_rate": 0.00016223626759522648,
        "epoch": 0.7627244951188613,
        "step": 10235
    },
    {
        "loss": 2.6386,
        "grad_norm": 3.368879556655884,
        "learning_rate": 0.00016218117494039844,
        "epoch": 0.7627990163201431,
        "step": 10236
    },
    {
        "loss": 2.3763,
        "grad_norm": 2.5641095638275146,
        "learning_rate": 0.0001621260514976349,
        "epoch": 0.7628735375214248,
        "step": 10237
    },
    {
        "loss": 2.9618,
        "grad_norm": 1.504715919494629,
        "learning_rate": 0.00016207089729422877,
        "epoch": 0.7629480587227067,
        "step": 10238
    },
    {
        "loss": 2.5586,
        "grad_norm": 2.4784722328186035,
        "learning_rate": 0.0001620157123574891,
        "epoch": 0.7630225799239884,
        "step": 10239
    },
    {
        "loss": 2.5003,
        "grad_norm": 2.523432493209839,
        "learning_rate": 0.0001619604967147395,
        "epoch": 0.7630971011252702,
        "step": 10240
    },
    {
        "loss": 2.8949,
        "grad_norm": 1.9709545373916626,
        "learning_rate": 0.00016190525039331925,
        "epoch": 0.7631716223265519,
        "step": 10241
    },
    {
        "loss": 1.1574,
        "grad_norm": 3.2584564685821533,
        "learning_rate": 0.00016184997342058252,
        "epoch": 0.7632461435278337,
        "step": 10242
    },
    {
        "loss": 2.6951,
        "grad_norm": 2.3318941593170166,
        "learning_rate": 0.00016179466582389866,
        "epoch": 0.7633206647291154,
        "step": 10243
    },
    {
        "loss": 2.7165,
        "grad_norm": 2.244082450866699,
        "learning_rate": 0.00016173932763065247,
        "epoch": 0.7633951859303972,
        "step": 10244
    },
    {
        "loss": 1.9735,
        "grad_norm": 4.078704833984375,
        "learning_rate": 0.00016168395886824346,
        "epoch": 0.7634697071316789,
        "step": 10245
    },
    {
        "loss": 2.8589,
        "grad_norm": 3.2563581466674805,
        "learning_rate": 0.0001616285595640869,
        "epoch": 0.7635442283329608,
        "step": 10246
    },
    {
        "loss": 2.5476,
        "grad_norm": 2.17893648147583,
        "learning_rate": 0.00016157312974561232,
        "epoch": 0.7636187495342425,
        "step": 10247
    },
    {
        "loss": 2.3018,
        "grad_norm": 4.100429534912109,
        "learning_rate": 0.00016151766944026518,
        "epoch": 0.7636932707355243,
        "step": 10248
    },
    {
        "loss": 2.4793,
        "grad_norm": 3.222811222076416,
        "learning_rate": 0.0001614621786755056,
        "epoch": 0.763767791936806,
        "step": 10249
    },
    {
        "loss": 2.2182,
        "grad_norm": 4.139074802398682,
        "learning_rate": 0.00016140665747880882,
        "epoch": 0.7638423131380878,
        "step": 10250
    },
    {
        "loss": 2.2232,
        "grad_norm": 3.5837905406951904,
        "learning_rate": 0.00016135110587766515,
        "epoch": 0.7639168343393695,
        "step": 10251
    },
    {
        "loss": 2.5038,
        "grad_norm": 2.9179134368896484,
        "learning_rate": 0.0001612955238995802,
        "epoch": 0.7639913555406513,
        "step": 10252
    },
    {
        "loss": 2.545,
        "grad_norm": 2.90999174118042,
        "learning_rate": 0.00016123991157207423,
        "epoch": 0.764065876741933,
        "step": 10253
    },
    {
        "loss": 3.3519,
        "grad_norm": 3.574963331222534,
        "learning_rate": 0.00016118426892268288,
        "epoch": 0.7641403979432149,
        "step": 10254
    },
    {
        "loss": 2.631,
        "grad_norm": 2.1460652351379395,
        "learning_rate": 0.0001611285959789566,
        "epoch": 0.7642149191444966,
        "step": 10255
    },
    {
        "loss": 2.4031,
        "grad_norm": 2.2662148475646973,
        "learning_rate": 0.00016107289276846083,
        "epoch": 0.7642894403457784,
        "step": 10256
    },
    {
        "loss": 2.3073,
        "grad_norm": 2.7088723182678223,
        "learning_rate": 0.00016101715931877622,
        "epoch": 0.7643639615470601,
        "step": 10257
    },
    {
        "loss": 2.439,
        "grad_norm": 3.1377758979797363,
        "learning_rate": 0.00016096139565749797,
        "epoch": 0.7644384827483419,
        "step": 10258
    },
    {
        "loss": 2.438,
        "grad_norm": 4.185473918914795,
        "learning_rate": 0.00016090560181223666,
        "epoch": 0.7645130039496236,
        "step": 10259
    },
    {
        "loss": 2.1341,
        "grad_norm": 2.452526330947876,
        "learning_rate": 0.00016084977781061766,
        "epoch": 0.7645875251509054,
        "step": 10260
    },
    {
        "loss": 2.3657,
        "grad_norm": 3.170001983642578,
        "learning_rate": 0.0001607939236802811,
        "epoch": 0.7646620463521872,
        "step": 10261
    },
    {
        "loss": 2.6585,
        "grad_norm": 4.387463092803955,
        "learning_rate": 0.00016073803944888243,
        "epoch": 0.764736567553469,
        "step": 10262
    },
    {
        "loss": 2.4006,
        "grad_norm": 2.8825490474700928,
        "learning_rate": 0.00016068212514409148,
        "epoch": 0.7648110887547507,
        "step": 10263
    },
    {
        "loss": 1.6737,
        "grad_norm": 3.505951166152954,
        "learning_rate": 0.00016062618079359356,
        "epoch": 0.7648856099560325,
        "step": 10264
    },
    {
        "loss": 1.7446,
        "grad_norm": 2.964938163757324,
        "learning_rate": 0.00016057020642508838,
        "epoch": 0.7649601311573142,
        "step": 10265
    },
    {
        "loss": 1.6371,
        "grad_norm": 3.722932815551758,
        "learning_rate": 0.00016051420206629073,
        "epoch": 0.765034652358596,
        "step": 10266
    },
    {
        "loss": 2.3645,
        "grad_norm": 2.946364402770996,
        "learning_rate": 0.00016045816774493005,
        "epoch": 0.7651091735598777,
        "step": 10267
    },
    {
        "loss": 2.1111,
        "grad_norm": 3.535954713821411,
        "learning_rate": 0.00016040210348875102,
        "epoch": 0.7651836947611595,
        "step": 10268
    },
    {
        "loss": 2.4276,
        "grad_norm": 3.238284111022949,
        "learning_rate": 0.00016034600932551273,
        "epoch": 0.7652582159624414,
        "step": 10269
    },
    {
        "loss": 2.1385,
        "grad_norm": 2.0702455043792725,
        "learning_rate": 0.0001602898852829893,
        "epoch": 0.7653327371637231,
        "step": 10270
    },
    {
        "loss": 1.6493,
        "grad_norm": 3.0408010482788086,
        "learning_rate": 0.00016023373138896943,
        "epoch": 0.7654072583650049,
        "step": 10271
    },
    {
        "loss": 2.6789,
        "grad_norm": 3.547548770904541,
        "learning_rate": 0.00016017754767125696,
        "epoch": 0.7654817795662866,
        "step": 10272
    },
    {
        "loss": 2.3002,
        "grad_norm": 2.9210093021392822,
        "learning_rate": 0.00016012133415767012,
        "epoch": 0.7655563007675684,
        "step": 10273
    },
    {
        "loss": 2.5884,
        "grad_norm": 3.6819381713867188,
        "learning_rate": 0.00016006509087604224,
        "epoch": 0.7656308219688501,
        "step": 10274
    },
    {
        "loss": 2.5086,
        "grad_norm": 2.351386070251465,
        "learning_rate": 0.00016000881785422112,
        "epoch": 0.765705343170132,
        "step": 10275
    },
    {
        "loss": 2.0555,
        "grad_norm": 3.601414203643799,
        "learning_rate": 0.00015995251512006923,
        "epoch": 0.7657798643714137,
        "step": 10276
    },
    {
        "loss": 2.6099,
        "grad_norm": 2.23148775100708,
        "learning_rate": 0.00015989618270146428,
        "epoch": 0.7658543855726955,
        "step": 10277
    },
    {
        "loss": 2.3696,
        "grad_norm": 4.169706344604492,
        "learning_rate": 0.00015983982062629784,
        "epoch": 0.7659289067739772,
        "step": 10278
    },
    {
        "loss": 2.5357,
        "grad_norm": 2.989777088165283,
        "learning_rate": 0.00015978342892247692,
        "epoch": 0.766003427975259,
        "step": 10279
    },
    {
        "loss": 2.0343,
        "grad_norm": 3.1159675121307373,
        "learning_rate": 0.00015972700761792284,
        "epoch": 0.7660779491765407,
        "step": 10280
    },
    {
        "loss": 1.8919,
        "grad_norm": 3.3156259059906006,
        "learning_rate": 0.00015967055674057144,
        "epoch": 0.7661524703778225,
        "step": 10281
    },
    {
        "loss": 1.883,
        "grad_norm": 3.6068649291992188,
        "learning_rate": 0.0001596140763183737,
        "epoch": 0.7662269915791042,
        "step": 10282
    },
    {
        "loss": 2.0842,
        "grad_norm": 4.656020164489746,
        "learning_rate": 0.00015955756637929479,
        "epoch": 0.766301512780386,
        "step": 10283
    },
    {
        "loss": 2.1432,
        "grad_norm": 3.728659152984619,
        "learning_rate": 0.00015950102695131451,
        "epoch": 0.7663760339816678,
        "step": 10284
    },
    {
        "loss": 2.4856,
        "grad_norm": 2.170391321182251,
        "learning_rate": 0.0001594444580624276,
        "epoch": 0.7664505551829496,
        "step": 10285
    },
    {
        "loss": 1.3584,
        "grad_norm": 5.21574068069458,
        "learning_rate": 0.00015938785974064308,
        "epoch": 0.7665250763842313,
        "step": 10286
    },
    {
        "loss": 2.875,
        "grad_norm": 3.446455478668213,
        "learning_rate": 0.0001593312320139845,
        "epoch": 0.7665995975855131,
        "step": 10287
    },
    {
        "loss": 1.8304,
        "grad_norm": 4.308642387390137,
        "learning_rate": 0.00015927457491049046,
        "epoch": 0.7666741187867948,
        "step": 10288
    },
    {
        "loss": 2.0057,
        "grad_norm": 2.610992670059204,
        "learning_rate": 0.0001592178884582133,
        "epoch": 0.7667486399880766,
        "step": 10289
    },
    {
        "loss": 2.2903,
        "grad_norm": 2.840575695037842,
        "learning_rate": 0.00015916117268522065,
        "epoch": 0.7668231611893583,
        "step": 10290
    },
    {
        "loss": 2.3003,
        "grad_norm": 3.4428353309631348,
        "learning_rate": 0.00015910442761959414,
        "epoch": 0.7668976823906402,
        "step": 10291
    },
    {
        "loss": 2.4454,
        "grad_norm": 2.5972344875335693,
        "learning_rate": 0.0001590476532894303,
        "epoch": 0.7669722035919219,
        "step": 10292
    },
    {
        "loss": 2.4185,
        "grad_norm": 3.04779314994812,
        "learning_rate": 0.00015899084972283988,
        "epoch": 0.7670467247932037,
        "step": 10293
    },
    {
        "loss": 2.2388,
        "grad_norm": 2.909256935119629,
        "learning_rate": 0.00015893401694794812,
        "epoch": 0.7671212459944854,
        "step": 10294
    },
    {
        "loss": 2.9372,
        "grad_norm": 2.297382354736328,
        "learning_rate": 0.0001588771549928949,
        "epoch": 0.7671957671957672,
        "step": 10295
    },
    {
        "loss": 1.547,
        "grad_norm": 3.3739352226257324,
        "learning_rate": 0.0001588202638858343,
        "epoch": 0.7672702883970489,
        "step": 10296
    },
    {
        "loss": 2.3555,
        "grad_norm": 3.3311259746551514,
        "learning_rate": 0.00015876334365493532,
        "epoch": 0.7673448095983307,
        "step": 10297
    },
    {
        "loss": 2.9027,
        "grad_norm": 2.4570865631103516,
        "learning_rate": 0.00015870639432838046,
        "epoch": 0.7674193307996124,
        "step": 10298
    },
    {
        "loss": 2.5728,
        "grad_norm": 3.16227650642395,
        "learning_rate": 0.00015864941593436767,
        "epoch": 0.7674938520008943,
        "step": 10299
    },
    {
        "loss": 2.6966,
        "grad_norm": 2.488034248352051,
        "learning_rate": 0.0001585924085011086,
        "epoch": 0.767568373202176,
        "step": 10300
    },
    {
        "loss": 1.3006,
        "grad_norm": 4.266717433929443,
        "learning_rate": 0.00015853537205682948,
        "epoch": 0.7676428944034578,
        "step": 10301
    },
    {
        "loss": 2.9508,
        "grad_norm": 2.214515447616577,
        "learning_rate": 0.0001584783066297711,
        "epoch": 0.7677174156047395,
        "step": 10302
    },
    {
        "loss": 1.6127,
        "grad_norm": 5.986395359039307,
        "learning_rate": 0.0001584212122481883,
        "epoch": 0.7677919368060213,
        "step": 10303
    },
    {
        "loss": 1.7485,
        "grad_norm": 2.5165302753448486,
        "learning_rate": 0.00015836408894035026,
        "epoch": 0.7678664580073031,
        "step": 10304
    },
    {
        "loss": 1.4747,
        "grad_norm": 3.253796100616455,
        "learning_rate": 0.00015830693673454083,
        "epoch": 0.7679409792085848,
        "step": 10305
    },
    {
        "loss": 2.4964,
        "grad_norm": 2.35205340385437,
        "learning_rate": 0.00015824975565905785,
        "epoch": 0.7680155004098667,
        "step": 10306
    },
    {
        "loss": 2.3215,
        "grad_norm": 3.864464521408081,
        "learning_rate": 0.00015819254574221344,
        "epoch": 0.7680900216111484,
        "step": 10307
    },
    {
        "loss": 2.0677,
        "grad_norm": 3.133794069290161,
        "learning_rate": 0.00015813530701233442,
        "epoch": 0.7681645428124302,
        "step": 10308
    },
    {
        "loss": 2.753,
        "grad_norm": 4.371171474456787,
        "learning_rate": 0.00015807803949776107,
        "epoch": 0.7682390640137119,
        "step": 10309
    },
    {
        "loss": 2.6203,
        "grad_norm": 2.30037522315979,
        "learning_rate": 0.00015802074322684878,
        "epoch": 0.7683135852149937,
        "step": 10310
    },
    {
        "loss": 2.1651,
        "grad_norm": 3.1251375675201416,
        "learning_rate": 0.00015796341822796675,
        "epoch": 0.7683881064162754,
        "step": 10311
    },
    {
        "loss": 2.429,
        "grad_norm": 3.6812655925750732,
        "learning_rate": 0.00015790606452949828,
        "epoch": 0.7684626276175572,
        "step": 10312
    },
    {
        "loss": 2.6034,
        "grad_norm": 3.221442699432373,
        "learning_rate": 0.00015784868215984132,
        "epoch": 0.7685371488188389,
        "step": 10313
    },
    {
        "loss": 1.6543,
        "grad_norm": 2.2214972972869873,
        "learning_rate": 0.0001577912711474075,
        "epoch": 0.7686116700201208,
        "step": 10314
    },
    {
        "loss": 2.5759,
        "grad_norm": 3.4726171493530273,
        "learning_rate": 0.00015773383152062318,
        "epoch": 0.7686861912214025,
        "step": 10315
    },
    {
        "loss": 2.6789,
        "grad_norm": 3.204357147216797,
        "learning_rate": 0.00015767636330792843,
        "epoch": 0.7687607124226843,
        "step": 10316
    },
    {
        "loss": 2.1367,
        "grad_norm": 3.746211290359497,
        "learning_rate": 0.00015761886653777767,
        "epoch": 0.768835233623966,
        "step": 10317
    },
    {
        "loss": 2.8968,
        "grad_norm": 2.7931230068206787,
        "learning_rate": 0.00015756134123863933,
        "epoch": 0.7689097548252478,
        "step": 10318
    },
    {
        "loss": 2.8032,
        "grad_norm": 2.263178586959839,
        "learning_rate": 0.00015750378743899628,
        "epoch": 0.7689842760265295,
        "step": 10319
    },
    {
        "loss": 2.4264,
        "grad_norm": 2.668149948120117,
        "learning_rate": 0.00015744620516734516,
        "epoch": 0.7690587972278113,
        "step": 10320
    },
    {
        "loss": 2.2254,
        "grad_norm": 2.9983394145965576,
        "learning_rate": 0.00015738859445219686,
        "epoch": 0.769133318429093,
        "step": 10321
    },
    {
        "loss": 2.5432,
        "grad_norm": 3.225085735321045,
        "learning_rate": 0.00015733095532207623,
        "epoch": 0.7692078396303749,
        "step": 10322
    },
    {
        "loss": 2.7441,
        "grad_norm": 2.610682725906372,
        "learning_rate": 0.0001572732878055225,
        "epoch": 0.7692823608316566,
        "step": 10323
    },
    {
        "loss": 2.1387,
        "grad_norm": 2.311292886734009,
        "learning_rate": 0.00015721559193108853,
        "epoch": 0.7693568820329384,
        "step": 10324
    },
    {
        "loss": 2.3539,
        "grad_norm": 4.565723419189453,
        "learning_rate": 0.00015715786772734167,
        "epoch": 0.7694314032342201,
        "step": 10325
    },
    {
        "loss": 2.6683,
        "grad_norm": 3.7087535858154297,
        "learning_rate": 0.00015710011522286293,
        "epoch": 0.7695059244355019,
        "step": 10326
    },
    {
        "loss": 2.1403,
        "grad_norm": 3.4454867839813232,
        "learning_rate": 0.0001570423344462473,
        "epoch": 0.7695804456367836,
        "step": 10327
    },
    {
        "loss": 2.5205,
        "grad_norm": 2.3804166316986084,
        "learning_rate": 0.0001569845254261044,
        "epoch": 0.7696549668380654,
        "step": 10328
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.6735334396362305,
        "learning_rate": 0.00015692668819105682,
        "epoch": 0.7697294880393472,
        "step": 10329
    },
    {
        "loss": 2.3716,
        "grad_norm": 3.3800814151763916,
        "learning_rate": 0.00015686882276974204,
        "epoch": 0.769804009240629,
        "step": 10330
    },
    {
        "loss": 2.6265,
        "grad_norm": 2.569383144378662,
        "learning_rate": 0.00015681092919081103,
        "epoch": 0.7698785304419107,
        "step": 10331
    },
    {
        "loss": 2.2433,
        "grad_norm": 4.026608467102051,
        "learning_rate": 0.00015675300748292865,
        "epoch": 0.7699530516431925,
        "step": 10332
    },
    {
        "loss": 2.371,
        "grad_norm": 1.6965093612670898,
        "learning_rate": 0.00015669505767477407,
        "epoch": 0.7700275728444742,
        "step": 10333
    },
    {
        "loss": 0.5437,
        "grad_norm": 3.1216635704040527,
        "learning_rate": 0.00015663707979503993,
        "epoch": 0.770102094045756,
        "step": 10334
    },
    {
        "loss": 1.7892,
        "grad_norm": 2.15170955657959,
        "learning_rate": 0.00015657907387243313,
        "epoch": 0.7701766152470377,
        "step": 10335
    },
    {
        "loss": 2.101,
        "grad_norm": 3.580108880996704,
        "learning_rate": 0.00015652103993567428,
        "epoch": 0.7702511364483196,
        "step": 10336
    },
    {
        "loss": 2.0574,
        "grad_norm": 3.018467426300049,
        "learning_rate": 0.00015646297801349789,
        "epoch": 0.7703256576496013,
        "step": 10337
    },
    {
        "loss": 2.3115,
        "grad_norm": 3.974277973175049,
        "learning_rate": 0.0001564048881346521,
        "epoch": 0.7704001788508831,
        "step": 10338
    },
    {
        "loss": 2.0973,
        "grad_norm": 3.6116764545440674,
        "learning_rate": 0.00015634677032789945,
        "epoch": 0.7704747000521649,
        "step": 10339
    },
    {
        "loss": 2.6289,
        "grad_norm": 2.2889277935028076,
        "learning_rate": 0.00015628862462201576,
        "epoch": 0.7705492212534466,
        "step": 10340
    },
    {
        "loss": 2.6304,
        "grad_norm": 3.312286376953125,
        "learning_rate": 0.00015623045104579095,
        "epoch": 0.7706237424547284,
        "step": 10341
    },
    {
        "loss": 1.8835,
        "grad_norm": 5.067805290222168,
        "learning_rate": 0.00015617224962802853,
        "epoch": 0.7706982636560101,
        "step": 10342
    },
    {
        "loss": 2.8542,
        "grad_norm": 3.9106295108795166,
        "learning_rate": 0.00015611402039754612,
        "epoch": 0.770772784857292,
        "step": 10343
    },
    {
        "loss": 2.1698,
        "grad_norm": 2.5477523803710938,
        "learning_rate": 0.0001560557633831748,
        "epoch": 0.7708473060585737,
        "step": 10344
    },
    {
        "loss": 2.2906,
        "grad_norm": 2.484661817550659,
        "learning_rate": 0.0001559974786137595,
        "epoch": 0.7709218272598555,
        "step": 10345
    },
    {
        "loss": 2.4344,
        "grad_norm": 2.6041953563690186,
        "learning_rate": 0.00015593916611815906,
        "epoch": 0.7709963484611372,
        "step": 10346
    },
    {
        "loss": 2.0239,
        "grad_norm": 3.498706579208374,
        "learning_rate": 0.0001558808259252457,
        "epoch": 0.771070869662419,
        "step": 10347
    },
    {
        "loss": 2.6256,
        "grad_norm": 3.2855734825134277,
        "learning_rate": 0.0001558224580639059,
        "epoch": 0.7711453908637007,
        "step": 10348
    },
    {
        "loss": 2.7831,
        "grad_norm": 3.2302846908569336,
        "learning_rate": 0.00015576406256303908,
        "epoch": 0.7712199120649825,
        "step": 10349
    },
    {
        "loss": 2.0529,
        "grad_norm": 2.9854140281677246,
        "learning_rate": 0.00015570563945155907,
        "epoch": 0.7712944332662642,
        "step": 10350
    },
    {
        "loss": 2.7234,
        "grad_norm": 3.338240146636963,
        "learning_rate": 0.00015564718875839292,
        "epoch": 0.771368954467546,
        "step": 10351
    },
    {
        "loss": 2.6493,
        "grad_norm": 2.7301747798919678,
        "learning_rate": 0.00015558871051248144,
        "epoch": 0.7714434756688278,
        "step": 10352
    },
    {
        "loss": 2.5849,
        "grad_norm": 3.4560768604278564,
        "learning_rate": 0.00015553020474277931,
        "epoch": 0.7715179968701096,
        "step": 10353
    },
    {
        "loss": 2.6245,
        "grad_norm": 3.449267864227295,
        "learning_rate": 0.00015547167147825453,
        "epoch": 0.7715925180713913,
        "step": 10354
    },
    {
        "loss": 1.5465,
        "grad_norm": 2.5909423828125,
        "learning_rate": 0.00015541311074788875,
        "epoch": 0.7716670392726731,
        "step": 10355
    },
    {
        "loss": 1.9047,
        "grad_norm": 3.3494396209716797,
        "learning_rate": 0.0001553545225806775,
        "epoch": 0.7717415604739548,
        "step": 10356
    },
    {
        "loss": 1.0922,
        "grad_norm": 3.8117051124572754,
        "learning_rate": 0.00015529590700562963,
        "epoch": 0.7718160816752366,
        "step": 10357
    },
    {
        "loss": 2.2298,
        "grad_norm": 3.3488664627075195,
        "learning_rate": 0.0001552372640517675,
        "epoch": 0.7718906028765183,
        "step": 10358
    },
    {
        "loss": 1.8655,
        "grad_norm": 3.915470600128174,
        "learning_rate": 0.00015517859374812747,
        "epoch": 0.7719651240778002,
        "step": 10359
    },
    {
        "loss": 2.4228,
        "grad_norm": 3.64699387550354,
        "learning_rate": 0.00015511989612375872,
        "epoch": 0.7720396452790819,
        "step": 10360
    },
    {
        "loss": 2.144,
        "grad_norm": 4.404036521911621,
        "learning_rate": 0.00015506117120772466,
        "epoch": 0.7721141664803637,
        "step": 10361
    },
    {
        "loss": 2.6222,
        "grad_norm": 4.258534908294678,
        "learning_rate": 0.00015500241902910168,
        "epoch": 0.7721886876816454,
        "step": 10362
    },
    {
        "loss": 2.0786,
        "grad_norm": 3.658499240875244,
        "learning_rate": 0.00015494363961698025,
        "epoch": 0.7722632088829272,
        "step": 10363
    },
    {
        "loss": 2.6711,
        "grad_norm": 2.242990016937256,
        "learning_rate": 0.00015488483300046374,
        "epoch": 0.7723377300842089,
        "step": 10364
    },
    {
        "loss": 2.2647,
        "grad_norm": 3.1491122245788574,
        "learning_rate": 0.00015482599920866913,
        "epoch": 0.7724122512854907,
        "step": 10365
    },
    {
        "loss": 1.6741,
        "grad_norm": 5.158841133117676,
        "learning_rate": 0.00015476713827072727,
        "epoch": 0.7724867724867724,
        "step": 10366
    },
    {
        "loss": 2.5504,
        "grad_norm": 3.3763017654418945,
        "learning_rate": 0.0001547082502157818,
        "epoch": 0.7725612936880543,
        "step": 10367
    },
    {
        "loss": 2.5622,
        "grad_norm": 3.095900535583496,
        "learning_rate": 0.0001546493350729906,
        "epoch": 0.772635814889336,
        "step": 10368
    },
    {
        "loss": 2.0667,
        "grad_norm": 3.647599220275879,
        "learning_rate": 0.0001545903928715239,
        "epoch": 0.7727103360906178,
        "step": 10369
    },
    {
        "loss": 2.2055,
        "grad_norm": 3.5461678504943848,
        "learning_rate": 0.00015453142364056635,
        "epoch": 0.7727848572918995,
        "step": 10370
    },
    {
        "loss": 2.4251,
        "grad_norm": 2.128488779067993,
        "learning_rate": 0.00015447242740931536,
        "epoch": 0.7728593784931813,
        "step": 10371
    },
    {
        "loss": 2.803,
        "grad_norm": 3.3453006744384766,
        "learning_rate": 0.00015441340420698193,
        "epoch": 0.772933899694463,
        "step": 10372
    },
    {
        "loss": 2.2236,
        "grad_norm": 2.777425527572632,
        "learning_rate": 0.00015435435406279026,
        "epoch": 0.7730084208957448,
        "step": 10373
    },
    {
        "loss": 2.7912,
        "grad_norm": 1.8403974771499634,
        "learning_rate": 0.00015429527700597828,
        "epoch": 0.7730829420970265,
        "step": 10374
    },
    {
        "loss": 2.6037,
        "grad_norm": 2.8451180458068848,
        "learning_rate": 0.00015423617306579667,
        "epoch": 0.7731574632983084,
        "step": 10375
    },
    {
        "loss": 2.0278,
        "grad_norm": 2.5835044384002686,
        "learning_rate": 0.00015417704227151002,
        "epoch": 0.7732319844995902,
        "step": 10376
    },
    {
        "loss": 2.2898,
        "grad_norm": 3.060358762741089,
        "learning_rate": 0.00015411788465239577,
        "epoch": 0.7733065057008719,
        "step": 10377
    },
    {
        "loss": 1.725,
        "grad_norm": 2.659213066101074,
        "learning_rate": 0.00015405870023774468,
        "epoch": 0.7733810269021537,
        "step": 10378
    },
    {
        "loss": 2.2798,
        "grad_norm": 2.9702916145324707,
        "learning_rate": 0.00015399948905686127,
        "epoch": 0.7734555481034354,
        "step": 10379
    },
    {
        "loss": 2.1241,
        "grad_norm": 1.9592267274856567,
        "learning_rate": 0.00015394025113906246,
        "epoch": 0.7735300693047172,
        "step": 10380
    },
    {
        "loss": 1.4134,
        "grad_norm": 3.173246383666992,
        "learning_rate": 0.0001538809865136792,
        "epoch": 0.773604590505999,
        "step": 10381
    },
    {
        "loss": 1.6534,
        "grad_norm": 3.1056504249572754,
        "learning_rate": 0.00015382169521005524,
        "epoch": 0.7736791117072808,
        "step": 10382
    },
    {
        "loss": 2.1727,
        "grad_norm": 2.874248504638672,
        "learning_rate": 0.0001537623772575476,
        "epoch": 0.7737536329085625,
        "step": 10383
    },
    {
        "loss": 1.7614,
        "grad_norm": 3.7564611434936523,
        "learning_rate": 0.00015370303268552666,
        "epoch": 0.7738281541098443,
        "step": 10384
    },
    {
        "loss": 2.6251,
        "grad_norm": 2.439387083053589,
        "learning_rate": 0.0001536436615233757,
        "epoch": 0.773902675311126,
        "step": 10385
    },
    {
        "loss": 2.691,
        "grad_norm": 4.426924705505371,
        "learning_rate": 0.00015358426380049153,
        "epoch": 0.7739771965124078,
        "step": 10386
    },
    {
        "loss": 1.57,
        "grad_norm": 2.781224012374878,
        "learning_rate": 0.00015352483954628383,
        "epoch": 0.7740517177136895,
        "step": 10387
    },
    {
        "loss": 1.8077,
        "grad_norm": 3.5619144439697266,
        "learning_rate": 0.00015346538879017544,
        "epoch": 0.7741262389149713,
        "step": 10388
    },
    {
        "loss": 2.7411,
        "grad_norm": 2.6341562271118164,
        "learning_rate": 0.0001534059115616023,
        "epoch": 0.774200760116253,
        "step": 10389
    },
    {
        "loss": 3.126,
        "grad_norm": 4.049465656280518,
        "learning_rate": 0.00015334640789001376,
        "epoch": 0.7742752813175349,
        "step": 10390
    },
    {
        "loss": 2.1423,
        "grad_norm": 4.108145236968994,
        "learning_rate": 0.0001532868778048719,
        "epoch": 0.7743498025188166,
        "step": 10391
    },
    {
        "loss": 2.9092,
        "grad_norm": 3.866156816482544,
        "learning_rate": 0.00015322732133565205,
        "epoch": 0.7744243237200984,
        "step": 10392
    },
    {
        "loss": 2.8466,
        "grad_norm": 2.1724493503570557,
        "learning_rate": 0.0001531677385118424,
        "epoch": 0.7744988449213801,
        "step": 10393
    },
    {
        "loss": 2.1647,
        "grad_norm": 3.2242937088012695,
        "learning_rate": 0.00015310812936294464,
        "epoch": 0.7745733661226619,
        "step": 10394
    },
    {
        "loss": 2.7262,
        "grad_norm": 4.787136077880859,
        "learning_rate": 0.00015304849391847296,
        "epoch": 0.7746478873239436,
        "step": 10395
    },
    {
        "loss": 2.0839,
        "grad_norm": 2.975757122039795,
        "learning_rate": 0.00015298883220795503,
        "epoch": 0.7747224085252254,
        "step": 10396
    },
    {
        "loss": 2.372,
        "grad_norm": 4.120657444000244,
        "learning_rate": 0.00015292914426093124,
        "epoch": 0.7747969297265072,
        "step": 10397
    },
    {
        "loss": 1.7808,
        "grad_norm": 4.01782751083374,
        "learning_rate": 0.0001528694301069549,
        "epoch": 0.774871450927789,
        "step": 10398
    },
    {
        "loss": 2.5774,
        "grad_norm": 2.906630516052246,
        "learning_rate": 0.0001528096897755928,
        "epoch": 0.7749459721290707,
        "step": 10399
    },
    {
        "loss": 1.669,
        "grad_norm": 4.806768894195557,
        "learning_rate": 0.0001527499232964239,
        "epoch": 0.7750204933303525,
        "step": 10400
    },
    {
        "loss": 2.414,
        "grad_norm": 4.011203289031982,
        "learning_rate": 0.00015269013069904084,
        "epoch": 0.7750950145316342,
        "step": 10401
    },
    {
        "loss": 1.8094,
        "grad_norm": 4.646159648895264,
        "learning_rate": 0.0001526303120130488,
        "epoch": 0.775169535732916,
        "step": 10402
    },
    {
        "loss": 2.4215,
        "grad_norm": 3.09804630279541,
        "learning_rate": 0.00015257046726806586,
        "epoch": 0.7752440569341977,
        "step": 10403
    },
    {
        "loss": 2.1081,
        "grad_norm": 3.5693278312683105,
        "learning_rate": 0.00015251059649372333,
        "epoch": 0.7753185781354796,
        "step": 10404
    },
    {
        "loss": 2.9101,
        "grad_norm": 2.8756401538848877,
        "learning_rate": 0.00015245069971966507,
        "epoch": 0.7753930993367613,
        "step": 10405
    },
    {
        "loss": 1.8528,
        "grad_norm": 3.7317824363708496,
        "learning_rate": 0.00015239077697554783,
        "epoch": 0.7754676205380431,
        "step": 10406
    },
    {
        "loss": 1.5674,
        "grad_norm": 1.8595770597457886,
        "learning_rate": 0.0001523308282910416,
        "epoch": 0.7755421417393248,
        "step": 10407
    },
    {
        "loss": 2.0274,
        "grad_norm": 2.4114973545074463,
        "learning_rate": 0.00015227085369582877,
        "epoch": 0.7756166629406066,
        "step": 10408
    },
    {
        "loss": 2.2895,
        "grad_norm": 4.132413864135742,
        "learning_rate": 0.00015221085321960457,
        "epoch": 0.7756911841418883,
        "step": 10409
    },
    {
        "loss": 2.0229,
        "grad_norm": 4.660178184509277,
        "learning_rate": 0.00015215082689207764,
        "epoch": 0.7757657053431701,
        "step": 10410
    },
    {
        "loss": 2.3167,
        "grad_norm": 5.246085166931152,
        "learning_rate": 0.00015209077474296842,
        "epoch": 0.775840226544452,
        "step": 10411
    },
    {
        "loss": 2.8851,
        "grad_norm": 2.1193244457244873,
        "learning_rate": 0.00015203069680201113,
        "epoch": 0.7759147477457337,
        "step": 10412
    },
    {
        "loss": 2.5331,
        "grad_norm": 2.2976794242858887,
        "learning_rate": 0.00015197059309895208,
        "epoch": 0.7759892689470155,
        "step": 10413
    },
    {
        "loss": 2.6279,
        "grad_norm": 3.154064655303955,
        "learning_rate": 0.00015191046366355075,
        "epoch": 0.7760637901482972,
        "step": 10414
    },
    {
        "loss": 1.4862,
        "grad_norm": 4.212141036987305,
        "learning_rate": 0.00015185030852557914,
        "epoch": 0.776138311349579,
        "step": 10415
    },
    {
        "loss": 2.3675,
        "grad_norm": 4.411258220672607,
        "learning_rate": 0.00015179012771482188,
        "epoch": 0.7762128325508607,
        "step": 10416
    },
    {
        "loss": 2.7318,
        "grad_norm": 1.5662935972213745,
        "learning_rate": 0.00015172992126107673,
        "epoch": 0.7762873537521425,
        "step": 10417
    },
    {
        "loss": 2.7056,
        "grad_norm": 2.3847272396087646,
        "learning_rate": 0.00015166968919415357,
        "epoch": 0.7763618749534242,
        "step": 10418
    },
    {
        "loss": 2.1965,
        "grad_norm": 3.304392099380493,
        "learning_rate": 0.00015160943154387567,
        "epoch": 0.776436396154706,
        "step": 10419
    },
    {
        "loss": 2.8358,
        "grad_norm": 2.337547779083252,
        "learning_rate": 0.0001515491483400781,
        "epoch": 0.7765109173559878,
        "step": 10420
    },
    {
        "loss": 2.4322,
        "grad_norm": 3.274024248123169,
        "learning_rate": 0.00015148883961260942,
        "epoch": 0.7765854385572696,
        "step": 10421
    },
    {
        "loss": 1.8312,
        "grad_norm": 3.0768826007843018,
        "learning_rate": 0.0001514285053913303,
        "epoch": 0.7766599597585513,
        "step": 10422
    },
    {
        "loss": 2.465,
        "grad_norm": 3.1801507472991943,
        "learning_rate": 0.00015136814570611406,
        "epoch": 0.7767344809598331,
        "step": 10423
    },
    {
        "loss": 2.3291,
        "grad_norm": 2.9682631492614746,
        "learning_rate": 0.00015130776058684705,
        "epoch": 0.7768090021611148,
        "step": 10424
    },
    {
        "loss": 2.6462,
        "grad_norm": 2.947166681289673,
        "learning_rate": 0.00015124735006342773,
        "epoch": 0.7768835233623966,
        "step": 10425
    },
    {
        "loss": 1.8355,
        "grad_norm": 3.468604326248169,
        "learning_rate": 0.00015118691416576727,
        "epoch": 0.7769580445636783,
        "step": 10426
    },
    {
        "loss": 2.859,
        "grad_norm": 2.213352680206299,
        "learning_rate": 0.00015112645292378964,
        "epoch": 0.7770325657649602,
        "step": 10427
    },
    {
        "loss": 1.7658,
        "grad_norm": 4.1843438148498535,
        "learning_rate": 0.00015106596636743115,
        "epoch": 0.7771070869662419,
        "step": 10428
    },
    {
        "loss": 2.8568,
        "grad_norm": 3.310354232788086,
        "learning_rate": 0.00015100545452664048,
        "epoch": 0.7771816081675237,
        "step": 10429
    },
    {
        "loss": 1.9474,
        "grad_norm": 3.3599398136138916,
        "learning_rate": 0.0001509449174313794,
        "epoch": 0.7772561293688054,
        "step": 10430
    },
    {
        "loss": 2.7622,
        "grad_norm": 2.7126829624176025,
        "learning_rate": 0.00015088435511162134,
        "epoch": 0.7773306505700872,
        "step": 10431
    },
    {
        "loss": 1.3637,
        "grad_norm": 4.566237449645996,
        "learning_rate": 0.000150823767597353,
        "epoch": 0.7774051717713689,
        "step": 10432
    },
    {
        "loss": 2.3439,
        "grad_norm": 1.5639344453811646,
        "learning_rate": 0.00015076315491857308,
        "epoch": 0.7774796929726507,
        "step": 10433
    },
    {
        "loss": 2.5465,
        "grad_norm": 2.2719364166259766,
        "learning_rate": 0.00015070251710529308,
        "epoch": 0.7775542141739324,
        "step": 10434
    },
    {
        "loss": 2.5208,
        "grad_norm": 1.8106541633605957,
        "learning_rate": 0.0001506418541875367,
        "epoch": 0.7776287353752143,
        "step": 10435
    },
    {
        "loss": 2.1729,
        "grad_norm": 2.894517421722412,
        "learning_rate": 0.00015058116619533996,
        "epoch": 0.777703256576496,
        "step": 10436
    },
    {
        "loss": 2.242,
        "grad_norm": 4.481705665588379,
        "learning_rate": 0.00015052045315875175,
        "epoch": 0.7777777777777778,
        "step": 10437
    },
    {
        "loss": 1.9467,
        "grad_norm": 2.3235251903533936,
        "learning_rate": 0.00015045971510783298,
        "epoch": 0.7778522989790595,
        "step": 10438
    },
    {
        "loss": 2.0689,
        "grad_norm": 3.5958499908447266,
        "learning_rate": 0.000150398952072657,
        "epoch": 0.7779268201803413,
        "step": 10439
    },
    {
        "loss": 1.5342,
        "grad_norm": 3.432756185531616,
        "learning_rate": 0.00015033816408330946,
        "epoch": 0.778001341381623,
        "step": 10440
    },
    {
        "loss": 3.0009,
        "grad_norm": 2.275883674621582,
        "learning_rate": 0.00015027735116988878,
        "epoch": 0.7780758625829048,
        "step": 10441
    },
    {
        "loss": 3.1105,
        "grad_norm": 3.648653984069824,
        "learning_rate": 0.00015021651336250526,
        "epoch": 0.7781503837841866,
        "step": 10442
    },
    {
        "loss": 3.3441,
        "grad_norm": 4.300693511962891,
        "learning_rate": 0.0001501556506912817,
        "epoch": 0.7782249049854684,
        "step": 10443
    },
    {
        "loss": 2.0525,
        "grad_norm": 3.163390636444092,
        "learning_rate": 0.00015009476318635304,
        "epoch": 0.7782994261867501,
        "step": 10444
    },
    {
        "loss": 1.702,
        "grad_norm": 3.447709560394287,
        "learning_rate": 0.00015003385087786696,
        "epoch": 0.7783739473880319,
        "step": 10445
    },
    {
        "loss": 2.6153,
        "grad_norm": 2.703369379043579,
        "learning_rate": 0.00014997291379598293,
        "epoch": 0.7784484685893137,
        "step": 10446
    },
    {
        "loss": 2.52,
        "grad_norm": 3.61076021194458,
        "learning_rate": 0.0001499119519708731,
        "epoch": 0.7785229897905954,
        "step": 10447
    },
    {
        "loss": 2.0521,
        "grad_norm": 3.922424793243408,
        "learning_rate": 0.00014985096543272154,
        "epoch": 0.7785975109918772,
        "step": 10448
    },
    {
        "loss": 2.4271,
        "grad_norm": 3.3284800052642822,
        "learning_rate": 0.00014978995421172458,
        "epoch": 0.778672032193159,
        "step": 10449
    },
    {
        "loss": 2.4779,
        "grad_norm": 2.24239444732666,
        "learning_rate": 0.0001497289183380912,
        "epoch": 0.7787465533944408,
        "step": 10450
    },
    {
        "loss": 2.5403,
        "grad_norm": 4.08224630355835,
        "learning_rate": 0.0001496678578420418,
        "epoch": 0.7788210745957225,
        "step": 10451
    },
    {
        "loss": 2.8406,
        "grad_norm": 2.434276819229126,
        "learning_rate": 0.0001496067727538098,
        "epoch": 0.7788955957970043,
        "step": 10452
    },
    {
        "loss": 2.8427,
        "grad_norm": 3.35183048248291,
        "learning_rate": 0.00014954566310364028,
        "epoch": 0.778970116998286,
        "step": 10453
    },
    {
        "loss": 2.1006,
        "grad_norm": 3.726693630218506,
        "learning_rate": 0.00014948452892179056,
        "epoch": 0.7790446381995678,
        "step": 10454
    },
    {
        "loss": 2.5901,
        "grad_norm": 4.037814617156982,
        "learning_rate": 0.00014942337023853033,
        "epoch": 0.7791191594008495,
        "step": 10455
    },
    {
        "loss": 2.7217,
        "grad_norm": 4.988987922668457,
        "learning_rate": 0.00014936218708414112,
        "epoch": 0.7791936806021313,
        "step": 10456
    },
    {
        "loss": 2.547,
        "grad_norm": 3.3542230129241943,
        "learning_rate": 0.00014930097948891687,
        "epoch": 0.779268201803413,
        "step": 10457
    },
    {
        "loss": 2.4283,
        "grad_norm": 2.774000883102417,
        "learning_rate": 0.00014923974748316338,
        "epoch": 0.7793427230046949,
        "step": 10458
    },
    {
        "loss": 2.9348,
        "grad_norm": 2.5608880519866943,
        "learning_rate": 0.00014917849109719863,
        "epoch": 0.7794172442059766,
        "step": 10459
    },
    {
        "loss": 2.8343,
        "grad_norm": 2.532560110092163,
        "learning_rate": 0.00014911721036135254,
        "epoch": 0.7794917654072584,
        "step": 10460
    },
    {
        "loss": 2.5684,
        "grad_norm": 3.1218810081481934,
        "learning_rate": 0.00014905590530596741,
        "epoch": 0.7795662866085401,
        "step": 10461
    },
    {
        "loss": 1.5465,
        "grad_norm": 2.7413477897644043,
        "learning_rate": 0.0001489945759613973,
        "epoch": 0.7796408078098219,
        "step": 10462
    },
    {
        "loss": 2.3835,
        "grad_norm": 2.6397364139556885,
        "learning_rate": 0.0001489332223580084,
        "epoch": 0.7797153290111036,
        "step": 10463
    },
    {
        "loss": 2.7991,
        "grad_norm": 2.7593464851379395,
        "learning_rate": 0.0001488718445261787,
        "epoch": 0.7797898502123854,
        "step": 10464
    },
    {
        "loss": 2.3237,
        "grad_norm": 2.8219408988952637,
        "learning_rate": 0.00014881044249629864,
        "epoch": 0.7798643714136672,
        "step": 10465
    },
    {
        "loss": 3.2277,
        "grad_norm": 2.626603603363037,
        "learning_rate": 0.0001487490162987703,
        "epoch": 0.779938892614949,
        "step": 10466
    },
    {
        "loss": 2.3728,
        "grad_norm": 2.370504140853882,
        "learning_rate": 0.00014868756596400761,
        "epoch": 0.7800134138162307,
        "step": 10467
    },
    {
        "loss": 2.3168,
        "grad_norm": 2.926438093185425,
        "learning_rate": 0.00014862609152243694,
        "epoch": 0.7800879350175125,
        "step": 10468
    },
    {
        "loss": 2.171,
        "grad_norm": 2.548970937728882,
        "learning_rate": 0.00014856459300449602,
        "epoch": 0.7801624562187942,
        "step": 10469
    },
    {
        "loss": 1.8933,
        "grad_norm": 3.2697484493255615,
        "learning_rate": 0.00014850307044063517,
        "epoch": 0.780236977420076,
        "step": 10470
    },
    {
        "loss": 2.6582,
        "grad_norm": 4.5718302726745605,
        "learning_rate": 0.00014844152386131573,
        "epoch": 0.7803114986213577,
        "step": 10471
    },
    {
        "loss": 2.0105,
        "grad_norm": 3.3794944286346436,
        "learning_rate": 0.00014837995329701177,
        "epoch": 0.7803860198226396,
        "step": 10472
    },
    {
        "loss": 2.3695,
        "grad_norm": 3.965104579925537,
        "learning_rate": 0.00014831835877820884,
        "epoch": 0.7804605410239213,
        "step": 10473
    },
    {
        "loss": 2.3969,
        "grad_norm": 1.9825025796890259,
        "learning_rate": 0.0001482567403354042,
        "epoch": 0.7805350622252031,
        "step": 10474
    },
    {
        "loss": 2.328,
        "grad_norm": 3.3734023571014404,
        "learning_rate": 0.00014819509799910745,
        "epoch": 0.7806095834264848,
        "step": 10475
    },
    {
        "loss": 1.6672,
        "grad_norm": 4.051576614379883,
        "learning_rate": 0.00014813343179983962,
        "epoch": 0.7806841046277666,
        "step": 10476
    },
    {
        "loss": 1.7495,
        "grad_norm": 3.269270658493042,
        "learning_rate": 0.00014807174176813346,
        "epoch": 0.7807586258290483,
        "step": 10477
    },
    {
        "loss": 2.0651,
        "grad_norm": 4.1392502784729,
        "learning_rate": 0.00014801002793453404,
        "epoch": 0.7808331470303301,
        "step": 10478
    },
    {
        "loss": 2.2915,
        "grad_norm": 2.0791730880737305,
        "learning_rate": 0.00014794829032959775,
        "epoch": 0.7809076682316118,
        "step": 10479
    },
    {
        "loss": 2.4604,
        "grad_norm": 2.8236961364746094,
        "learning_rate": 0.0001478865289838928,
        "epoch": 0.7809821894328937,
        "step": 10480
    },
    {
        "loss": 2.1464,
        "grad_norm": 3.6030993461608887,
        "learning_rate": 0.00014782474392799962,
        "epoch": 0.7810567106341755,
        "step": 10481
    },
    {
        "loss": 2.1645,
        "grad_norm": 4.204080581665039,
        "learning_rate": 0.00014776293519250952,
        "epoch": 0.7811312318354572,
        "step": 10482
    },
    {
        "loss": 2.3105,
        "grad_norm": 2.3558976650238037,
        "learning_rate": 0.0001477011028080264,
        "epoch": 0.781205753036739,
        "step": 10483
    },
    {
        "loss": 1.8491,
        "grad_norm": 2.2910873889923096,
        "learning_rate": 0.00014763924680516523,
        "epoch": 0.7812802742380207,
        "step": 10484
    },
    {
        "loss": 1.4021,
        "grad_norm": 4.710171222686768,
        "learning_rate": 0.0001475773672145532,
        "epoch": 0.7813547954393025,
        "step": 10485
    },
    {
        "loss": 3.138,
        "grad_norm": 2.555213212966919,
        "learning_rate": 0.0001475154640668288,
        "epoch": 0.7814293166405842,
        "step": 10486
    },
    {
        "loss": 2.1672,
        "grad_norm": 3.625487804412842,
        "learning_rate": 0.00014745353739264219,
        "epoch": 0.7815038378418661,
        "step": 10487
    },
    {
        "loss": 1.9419,
        "grad_norm": 3.9082190990448,
        "learning_rate": 0.00014739158722265554,
        "epoch": 0.7815783590431478,
        "step": 10488
    },
    {
        "loss": 2.8006,
        "grad_norm": 3.624516248703003,
        "learning_rate": 0.00014732961358754213,
        "epoch": 0.7816528802444296,
        "step": 10489
    },
    {
        "loss": 2.5711,
        "grad_norm": 3.2378439903259277,
        "learning_rate": 0.00014726761651798754,
        "epoch": 0.7817274014457113,
        "step": 10490
    },
    {
        "loss": 1.4921,
        "grad_norm": 3.637399435043335,
        "learning_rate": 0.00014720559604468808,
        "epoch": 0.7818019226469931,
        "step": 10491
    },
    {
        "loss": 1.4493,
        "grad_norm": 3.701425075531006,
        "learning_rate": 0.00014714355219835244,
        "epoch": 0.7818764438482748,
        "step": 10492
    },
    {
        "loss": 1.5286,
        "grad_norm": 2.4483225345611572,
        "learning_rate": 0.00014708148500970042,
        "epoch": 0.7819509650495566,
        "step": 10493
    },
    {
        "loss": 2.6019,
        "grad_norm": 2.7903542518615723,
        "learning_rate": 0.00014701939450946345,
        "epoch": 0.7820254862508383,
        "step": 10494
    },
    {
        "loss": 0.6912,
        "grad_norm": 3.7637248039245605,
        "learning_rate": 0.0001469572807283848,
        "epoch": 0.7821000074521202,
        "step": 10495
    },
    {
        "loss": 2.3576,
        "grad_norm": 3.916231870651245,
        "learning_rate": 0.0001468951436972189,
        "epoch": 0.7821745286534019,
        "step": 10496
    },
    {
        "loss": 2.6894,
        "grad_norm": 3.9511594772338867,
        "learning_rate": 0.00014683298344673166,
        "epoch": 0.7822490498546837,
        "step": 10497
    },
    {
        "loss": 2.8172,
        "grad_norm": 1.9277243614196777,
        "learning_rate": 0.00014677080000770101,
        "epoch": 0.7823235710559654,
        "step": 10498
    },
    {
        "loss": 1.1469,
        "grad_norm": 4.320845127105713,
        "learning_rate": 0.00014670859341091582,
        "epoch": 0.7823980922572472,
        "step": 10499
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.8555915355682373,
        "learning_rate": 0.0001466463636871765,
        "epoch": 0.7824726134585289,
        "step": 10500
    },
    {
        "loss": 1.3732,
        "grad_norm": 1.7481441497802734,
        "learning_rate": 0.00014658411086729533,
        "epoch": 0.7825471346598107,
        "step": 10501
    },
    {
        "loss": 2.7096,
        "grad_norm": 3.8139426708221436,
        "learning_rate": 0.00014652183498209534,
        "epoch": 0.7826216558610924,
        "step": 10502
    },
    {
        "loss": 2.0772,
        "grad_norm": 3.3193705081939697,
        "learning_rate": 0.00014645953606241165,
        "epoch": 0.7826961770623743,
        "step": 10503
    },
    {
        "loss": 2.0947,
        "grad_norm": 2.578707695007324,
        "learning_rate": 0.0001463972141390904,
        "epoch": 0.782770698263656,
        "step": 10504
    },
    {
        "loss": 2.3015,
        "grad_norm": 2.8681023120880127,
        "learning_rate": 0.0001463348692429891,
        "epoch": 0.7828452194649378,
        "step": 10505
    },
    {
        "loss": 2.5476,
        "grad_norm": 3.1415388584136963,
        "learning_rate": 0.00014627250140497697,
        "epoch": 0.7829197406662195,
        "step": 10506
    },
    {
        "loss": 1.7134,
        "grad_norm": 4.372885704040527,
        "learning_rate": 0.00014621011065593417,
        "epoch": 0.7829942618675013,
        "step": 10507
    },
    {
        "loss": 2.5773,
        "grad_norm": 2.3334388732910156,
        "learning_rate": 0.0001461476970267526,
        "epoch": 0.783068783068783,
        "step": 10508
    },
    {
        "loss": 2.2094,
        "grad_norm": 1.8495455980300903,
        "learning_rate": 0.00014608526054833517,
        "epoch": 0.7831433042700648,
        "step": 10509
    },
    {
        "loss": 2.1526,
        "grad_norm": 3.8144686222076416,
        "learning_rate": 0.00014602280125159623,
        "epoch": 0.7832178254713466,
        "step": 10510
    },
    {
        "loss": 2.7863,
        "grad_norm": 2.256948471069336,
        "learning_rate": 0.00014596031916746136,
        "epoch": 0.7832923466726284,
        "step": 10511
    },
    {
        "loss": 2.8672,
        "grad_norm": 2.9601190090179443,
        "learning_rate": 0.00014589781432686763,
        "epoch": 0.7833668678739101,
        "step": 10512
    },
    {
        "loss": 2.6763,
        "grad_norm": 3.632688045501709,
        "learning_rate": 0.00014583528676076317,
        "epoch": 0.7834413890751919,
        "step": 10513
    },
    {
        "loss": 2.4979,
        "grad_norm": 3.430872917175293,
        "learning_rate": 0.00014577273650010744,
        "epoch": 0.7835159102764736,
        "step": 10514
    },
    {
        "loss": 2.3345,
        "grad_norm": 3.5703139305114746,
        "learning_rate": 0.00014571016357587096,
        "epoch": 0.7835904314777554,
        "step": 10515
    },
    {
        "loss": 2.0796,
        "grad_norm": 2.0079281330108643,
        "learning_rate": 0.00014564756801903588,
        "epoch": 0.7836649526790372,
        "step": 10516
    },
    {
        "loss": 2.3763,
        "grad_norm": 1.7514911890029907,
        "learning_rate": 0.0001455849498605951,
        "epoch": 0.783739473880319,
        "step": 10517
    },
    {
        "loss": 2.6916,
        "grad_norm": 3.32728910446167,
        "learning_rate": 0.00014552230913155313,
        "epoch": 0.7838139950816008,
        "step": 10518
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.8621091842651367,
        "learning_rate": 0.00014545964586292536,
        "epoch": 0.7838885162828825,
        "step": 10519
    },
    {
        "loss": 2.7738,
        "grad_norm": 4.1270880699157715,
        "learning_rate": 0.00014539696008573823,
        "epoch": 0.7839630374841643,
        "step": 10520
    },
    {
        "loss": 2.4914,
        "grad_norm": 3.7250185012817383,
        "learning_rate": 0.00014533425183103002,
        "epoch": 0.784037558685446,
        "step": 10521
    },
    {
        "loss": 2.6656,
        "grad_norm": 1.765565276145935,
        "learning_rate": 0.00014527152112984903,
        "epoch": 0.7841120798867278,
        "step": 10522
    },
    {
        "loss": 2.5844,
        "grad_norm": 3.1733152866363525,
        "learning_rate": 0.00014520876801325566,
        "epoch": 0.7841866010880095,
        "step": 10523
    },
    {
        "loss": 2.6308,
        "grad_norm": 2.432060718536377,
        "learning_rate": 0.00014514599251232098,
        "epoch": 0.7842611222892913,
        "step": 10524
    },
    {
        "loss": 2.7316,
        "grad_norm": 2.9487409591674805,
        "learning_rate": 0.00014508319465812703,
        "epoch": 0.784335643490573,
        "step": 10525
    },
    {
        "loss": 2.5089,
        "grad_norm": 2.151644706726074,
        "learning_rate": 0.00014502037448176736,
        "epoch": 0.7844101646918549,
        "step": 10526
    },
    {
        "loss": 2.2501,
        "grad_norm": 4.223575115203857,
        "learning_rate": 0.000144957532014346,
        "epoch": 0.7844846858931366,
        "step": 10527
    },
    {
        "loss": 2.7184,
        "grad_norm": 3.3480098247528076,
        "learning_rate": 0.0001448946672869786,
        "epoch": 0.7845592070944184,
        "step": 10528
    },
    {
        "loss": 2.6341,
        "grad_norm": 3.9609506130218506,
        "learning_rate": 0.00014483178033079138,
        "epoch": 0.7846337282957001,
        "step": 10529
    },
    {
        "loss": 2.6592,
        "grad_norm": 2.0666401386260986,
        "learning_rate": 0.00014476887117692176,
        "epoch": 0.7847082494969819,
        "step": 10530
    },
    {
        "loss": 2.6271,
        "grad_norm": 2.0478994846343994,
        "learning_rate": 0.00014470593985651802,
        "epoch": 0.7847827706982636,
        "step": 10531
    },
    {
        "loss": 2.0359,
        "grad_norm": 3.1965577602386475,
        "learning_rate": 0.00014464298640073984,
        "epoch": 0.7848572918995455,
        "step": 10532
    },
    {
        "loss": 2.663,
        "grad_norm": 3.182781457901001,
        "learning_rate": 0.00014458001084075709,
        "epoch": 0.7849318131008272,
        "step": 10533
    },
    {
        "loss": 2.438,
        "grad_norm": 4.242815971374512,
        "learning_rate": 0.00014451701320775137,
        "epoch": 0.785006334302109,
        "step": 10534
    },
    {
        "loss": 1.849,
        "grad_norm": 3.733887195587158,
        "learning_rate": 0.00014445399353291464,
        "epoch": 0.7850808555033907,
        "step": 10535
    },
    {
        "loss": 2.4623,
        "grad_norm": 3.0550196170806885,
        "learning_rate": 0.00014439095184745024,
        "epoch": 0.7851553767046725,
        "step": 10536
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.670545220375061,
        "learning_rate": 0.0001443278881825721,
        "epoch": 0.7852298979059542,
        "step": 10537
    },
    {
        "loss": 3.2736,
        "grad_norm": 3.0620129108428955,
        "learning_rate": 0.00014426480256950485,
        "epoch": 0.785304419107236,
        "step": 10538
    },
    {
        "loss": 1.7117,
        "grad_norm": 3.505371570587158,
        "learning_rate": 0.00014420169503948462,
        "epoch": 0.7853789403085177,
        "step": 10539
    },
    {
        "loss": 2.8223,
        "grad_norm": 1.5811738967895508,
        "learning_rate": 0.00014413856562375772,
        "epoch": 0.7854534615097996,
        "step": 10540
    },
    {
        "loss": 3.0238,
        "grad_norm": 2.6537857055664062,
        "learning_rate": 0.00014407541435358198,
        "epoch": 0.7855279827110813,
        "step": 10541
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.7520663738250732,
        "learning_rate": 0.00014401224126022515,
        "epoch": 0.7856025039123631,
        "step": 10542
    },
    {
        "loss": 2.3362,
        "grad_norm": 2.9862141609191895,
        "learning_rate": 0.00014394904637496665,
        "epoch": 0.7856770251136448,
        "step": 10543
    },
    {
        "loss": 1.9192,
        "grad_norm": 3.837954044342041,
        "learning_rate": 0.00014388582972909626,
        "epoch": 0.7857515463149266,
        "step": 10544
    },
    {
        "loss": 2.5918,
        "grad_norm": 3.2090277671813965,
        "learning_rate": 0.00014382259135391445,
        "epoch": 0.7858260675162083,
        "step": 10545
    },
    {
        "loss": 2.6085,
        "grad_norm": 3.4610989093780518,
        "learning_rate": 0.00014375933128073286,
        "epoch": 0.7859005887174901,
        "step": 10546
    },
    {
        "loss": 2.397,
        "grad_norm": 3.048304796218872,
        "learning_rate": 0.0001436960495408735,
        "epoch": 0.7859751099187718,
        "step": 10547
    },
    {
        "loss": 1.7376,
        "grad_norm": 3.4817168712615967,
        "learning_rate": 0.00014363274616566914,
        "epoch": 0.7860496311200537,
        "step": 10548
    },
    {
        "loss": 2.8466,
        "grad_norm": 3.111476421356201,
        "learning_rate": 0.00014356942118646358,
        "epoch": 0.7861241523213354,
        "step": 10549
    },
    {
        "loss": 2.7296,
        "grad_norm": 2.4402413368225098,
        "learning_rate": 0.00014350607463461094,
        "epoch": 0.7861986735226172,
        "step": 10550
    },
    {
        "loss": 1.9876,
        "grad_norm": 3.765587568283081,
        "learning_rate": 0.00014344270654147607,
        "epoch": 0.7862731947238989,
        "step": 10551
    },
    {
        "loss": 2.1148,
        "grad_norm": 3.0176913738250732,
        "learning_rate": 0.000143379316938435,
        "epoch": 0.7863477159251807,
        "step": 10552
    },
    {
        "loss": 2.2296,
        "grad_norm": 2.5381383895874023,
        "learning_rate": 0.00014331590585687343,
        "epoch": 0.7864222371264625,
        "step": 10553
    },
    {
        "loss": 2.4056,
        "grad_norm": 3.292647361755371,
        "learning_rate": 0.0001432524733281887,
        "epoch": 0.7864967583277442,
        "step": 10554
    },
    {
        "loss": 2.1214,
        "grad_norm": 3.8703086376190186,
        "learning_rate": 0.00014318901938378803,
        "epoch": 0.7865712795290261,
        "step": 10555
    },
    {
        "loss": 2.8817,
        "grad_norm": 2.697214365005493,
        "learning_rate": 0.00014312554405508985,
        "epoch": 0.7866458007303078,
        "step": 10556
    },
    {
        "loss": 1.8745,
        "grad_norm": 3.993577480316162,
        "learning_rate": 0.0001430620473735227,
        "epoch": 0.7867203219315896,
        "step": 10557
    },
    {
        "loss": 2.6522,
        "grad_norm": 2.5347726345062256,
        "learning_rate": 0.00014299852937052574,
        "epoch": 0.7867948431328713,
        "step": 10558
    },
    {
        "loss": 1.8999,
        "grad_norm": 2.300872564315796,
        "learning_rate": 0.00014293499007754908,
        "epoch": 0.7868693643341531,
        "step": 10559
    },
    {
        "loss": 2.3668,
        "grad_norm": 3.8632726669311523,
        "learning_rate": 0.000142871429526053,
        "epoch": 0.7869438855354348,
        "step": 10560
    },
    {
        "loss": 2.282,
        "grad_norm": 2.9584813117980957,
        "learning_rate": 0.00014280784774750837,
        "epoch": 0.7870184067367166,
        "step": 10561
    },
    {
        "loss": 1.6803,
        "grad_norm": 4.640582084655762,
        "learning_rate": 0.00014274424477339653,
        "epoch": 0.7870929279379983,
        "step": 10562
    },
    {
        "loss": 2.5103,
        "grad_norm": 2.463168144226074,
        "learning_rate": 0.00014268062063520963,
        "epoch": 0.7871674491392802,
        "step": 10563
    },
    {
        "loss": 2.6971,
        "grad_norm": 2.9830610752105713,
        "learning_rate": 0.00014261697536444988,
        "epoch": 0.7872419703405619,
        "step": 10564
    },
    {
        "loss": 2.603,
        "grad_norm": 3.9835660457611084,
        "learning_rate": 0.00014255330899263022,
        "epoch": 0.7873164915418437,
        "step": 10565
    },
    {
        "loss": 1.4701,
        "grad_norm": 3.1065475940704346,
        "learning_rate": 0.0001424896215512738,
        "epoch": 0.7873910127431254,
        "step": 10566
    },
    {
        "loss": 1.9673,
        "grad_norm": 2.691862106323242,
        "learning_rate": 0.0001424259130719146,
        "epoch": 0.7874655339444072,
        "step": 10567
    },
    {
        "loss": 2.4879,
        "grad_norm": 4.102880477905273,
        "learning_rate": 0.00014236218358609656,
        "epoch": 0.7875400551456889,
        "step": 10568
    },
    {
        "loss": 1.7837,
        "grad_norm": 3.3151841163635254,
        "learning_rate": 0.00014229843312537442,
        "epoch": 0.7876145763469707,
        "step": 10569
    },
    {
        "loss": 1.7691,
        "grad_norm": 2.434351682662964,
        "learning_rate": 0.00014223466172131303,
        "epoch": 0.7876890975482524,
        "step": 10570
    },
    {
        "loss": 2.5788,
        "grad_norm": 3.803738832473755,
        "learning_rate": 0.0001421708694054876,
        "epoch": 0.7877636187495343,
        "step": 10571
    },
    {
        "loss": 2.085,
        "grad_norm": 3.2599706649780273,
        "learning_rate": 0.00014210705620948413,
        "epoch": 0.787838139950816,
        "step": 10572
    },
    {
        "loss": 1.873,
        "grad_norm": 6.518040180206299,
        "learning_rate": 0.00014204322216489814,
        "epoch": 0.7879126611520978,
        "step": 10573
    },
    {
        "loss": 0.5215,
        "grad_norm": 4.148837566375732,
        "learning_rate": 0.00014197936730333635,
        "epoch": 0.7879871823533795,
        "step": 10574
    },
    {
        "loss": 2.564,
        "grad_norm": 2.493243932723999,
        "learning_rate": 0.00014191549165641523,
        "epoch": 0.7880617035546613,
        "step": 10575
    },
    {
        "loss": 2.4375,
        "grad_norm": 3.5564446449279785,
        "learning_rate": 0.00014185159525576156,
        "epoch": 0.788136224755943,
        "step": 10576
    },
    {
        "loss": 2.193,
        "grad_norm": 3.612900972366333,
        "learning_rate": 0.00014178767813301287,
        "epoch": 0.7882107459572248,
        "step": 10577
    },
    {
        "loss": 2.0112,
        "grad_norm": 4.8398213386535645,
        "learning_rate": 0.0001417237403198163,
        "epoch": 0.7882852671585066,
        "step": 10578
    },
    {
        "loss": 2.1294,
        "grad_norm": 3.7160825729370117,
        "learning_rate": 0.0001416597818478299,
        "epoch": 0.7883597883597884,
        "step": 10579
    },
    {
        "loss": 3.0841,
        "grad_norm": 2.2955150604248047,
        "learning_rate": 0.00014159580274872134,
        "epoch": 0.7884343095610701,
        "step": 10580
    },
    {
        "loss": 2.7375,
        "grad_norm": 3.494225025177002,
        "learning_rate": 0.00014153180305416884,
        "epoch": 0.7885088307623519,
        "step": 10581
    },
    {
        "loss": 1.8201,
        "grad_norm": 3.5367331504821777,
        "learning_rate": 0.00014146778279586065,
        "epoch": 0.7885833519636336,
        "step": 10582
    },
    {
        "loss": 1.9529,
        "grad_norm": 3.4880881309509277,
        "learning_rate": 0.0001414037420054955,
        "epoch": 0.7886578731649154,
        "step": 10583
    },
    {
        "loss": 2.5822,
        "grad_norm": 2.4300825595855713,
        "learning_rate": 0.00014133968071478194,
        "epoch": 0.7887323943661971,
        "step": 10584
    },
    {
        "loss": 2.4184,
        "grad_norm": 2.988994836807251,
        "learning_rate": 0.00014127559895543888,
        "epoch": 0.788806915567479,
        "step": 10585
    },
    {
        "loss": 1.3325,
        "grad_norm": 4.878803253173828,
        "learning_rate": 0.00014121149675919514,
        "epoch": 0.7888814367687607,
        "step": 10586
    },
    {
        "loss": 2.8311,
        "grad_norm": 2.5962140560150146,
        "learning_rate": 0.00014114737415779003,
        "epoch": 0.7889559579700425,
        "step": 10587
    },
    {
        "loss": 2.4406,
        "grad_norm": 3.006223201751709,
        "learning_rate": 0.00014108323118297252,
        "epoch": 0.7890304791713243,
        "step": 10588
    },
    {
        "loss": 2.4463,
        "grad_norm": 2.999863862991333,
        "learning_rate": 0.0001410190678665022,
        "epoch": 0.789105000372606,
        "step": 10589
    },
    {
        "loss": 1.0725,
        "grad_norm": 4.017180919647217,
        "learning_rate": 0.00014095488424014824,
        "epoch": 0.7891795215738878,
        "step": 10590
    },
    {
        "loss": 2.2073,
        "grad_norm": 3.0771312713623047,
        "learning_rate": 0.00014089068033568998,
        "epoch": 0.7892540427751695,
        "step": 10591
    },
    {
        "loss": 2.1525,
        "grad_norm": 2.6707468032836914,
        "learning_rate": 0.00014082645618491716,
        "epoch": 0.7893285639764513,
        "step": 10592
    },
    {
        "loss": 2.5861,
        "grad_norm": 2.4744482040405273,
        "learning_rate": 0.00014076221181962887,
        "epoch": 0.789403085177733,
        "step": 10593
    },
    {
        "loss": 2.8087,
        "grad_norm": 2.5298643112182617,
        "learning_rate": 0.00014069794727163492,
        "epoch": 0.7894776063790149,
        "step": 10594
    },
    {
        "loss": 2.905,
        "grad_norm": 2.641906261444092,
        "learning_rate": 0.00014063366257275467,
        "epoch": 0.7895521275802966,
        "step": 10595
    },
    {
        "loss": 1.3844,
        "grad_norm": 4.680104732513428,
        "learning_rate": 0.00014056935775481744,
        "epoch": 0.7896266487815784,
        "step": 10596
    },
    {
        "loss": 2.1265,
        "grad_norm": 3.5196542739868164,
        "learning_rate": 0.00014050503284966296,
        "epoch": 0.7897011699828601,
        "step": 10597
    },
    {
        "loss": 2.4181,
        "grad_norm": 2.0257186889648438,
        "learning_rate": 0.0001404406878891404,
        "epoch": 0.7897756911841419,
        "step": 10598
    },
    {
        "loss": 1.4442,
        "grad_norm": 5.926018238067627,
        "learning_rate": 0.00014037632290510895,
        "epoch": 0.7898502123854236,
        "step": 10599
    },
    {
        "loss": 2.6372,
        "grad_norm": 2.613308906555176,
        "learning_rate": 0.00014031193792943811,
        "epoch": 0.7899247335867055,
        "step": 10600
    },
    {
        "loss": 2.5946,
        "grad_norm": 2.6920650005340576,
        "learning_rate": 0.00014024753299400684,
        "epoch": 0.7899992547879872,
        "step": 10601
    },
    {
        "loss": 2.4551,
        "grad_norm": 3.274489641189575,
        "learning_rate": 0.00014018310813070399,
        "epoch": 0.790073775989269,
        "step": 10602
    },
    {
        "loss": 2.1504,
        "grad_norm": 2.993046998977661,
        "learning_rate": 0.0001401186633714288,
        "epoch": 0.7901482971905507,
        "step": 10603
    },
    {
        "loss": 2.0148,
        "grad_norm": 4.2671122550964355,
        "learning_rate": 0.0001400541987480895,
        "epoch": 0.7902228183918325,
        "step": 10604
    },
    {
        "loss": 2.3721,
        "grad_norm": 2.8450942039489746,
        "learning_rate": 0.00013998971429260502,
        "epoch": 0.7902973395931142,
        "step": 10605
    },
    {
        "loss": 2.1917,
        "grad_norm": 2.4062893390655518,
        "learning_rate": 0.00013992521003690342,
        "epoch": 0.790371860794396,
        "step": 10606
    },
    {
        "loss": 1.845,
        "grad_norm": 3.8327014446258545,
        "learning_rate": 0.00013986068601292318,
        "epoch": 0.7904463819956777,
        "step": 10607
    },
    {
        "loss": 2.0974,
        "grad_norm": 5.036773681640625,
        "learning_rate": 0.00013979614225261212,
        "epoch": 0.7905209031969596,
        "step": 10608
    },
    {
        "loss": 2.443,
        "grad_norm": 3.186333179473877,
        "learning_rate": 0.0001397315787879278,
        "epoch": 0.7905954243982413,
        "step": 10609
    },
    {
        "loss": 2.9742,
        "grad_norm": 3.040095567703247,
        "learning_rate": 0.000139666995650838,
        "epoch": 0.7906699455995231,
        "step": 10610
    },
    {
        "loss": 2.0931,
        "grad_norm": 3.096595048904419,
        "learning_rate": 0.00013960239287331966,
        "epoch": 0.7907444668008048,
        "step": 10611
    },
    {
        "loss": 2.7864,
        "grad_norm": 3.2578952312469482,
        "learning_rate": 0.00013953777048736015,
        "epoch": 0.7908189880020866,
        "step": 10612
    },
    {
        "loss": 2.8449,
        "grad_norm": 2.875332832336426,
        "learning_rate": 0.0001394731285249556,
        "epoch": 0.7908935092033683,
        "step": 10613
    },
    {
        "loss": 2.4678,
        "grad_norm": 2.0666656494140625,
        "learning_rate": 0.00013940846701811278,
        "epoch": 0.7909680304046501,
        "step": 10614
    },
    {
        "loss": 2.0469,
        "grad_norm": 2.8939268589019775,
        "learning_rate": 0.00013934378599884752,
        "epoch": 0.7910425516059318,
        "step": 10615
    },
    {
        "loss": 2.4425,
        "grad_norm": 2.917961359024048,
        "learning_rate": 0.00013927908549918541,
        "epoch": 0.7911170728072137,
        "step": 10616
    },
    {
        "loss": 1.7875,
        "grad_norm": 3.698930501937866,
        "learning_rate": 0.0001392143655511621,
        "epoch": 0.7911915940084954,
        "step": 10617
    },
    {
        "loss": 2.197,
        "grad_norm": 3.4528560638427734,
        "learning_rate": 0.00013914962618682237,
        "epoch": 0.7912661152097772,
        "step": 10618
    },
    {
        "loss": 2.5368,
        "grad_norm": 2.403954029083252,
        "learning_rate": 0.00013908486743822066,
        "epoch": 0.7913406364110589,
        "step": 10619
    },
    {
        "loss": 2.9379,
        "grad_norm": 1.9714782238006592,
        "learning_rate": 0.00013902008933742142,
        "epoch": 0.7914151576123407,
        "step": 10620
    },
    {
        "loss": 1.4127,
        "grad_norm": 3.417059898376465,
        "learning_rate": 0.0001389552919164983,
        "epoch": 0.7914896788136224,
        "step": 10621
    },
    {
        "loss": 2.332,
        "grad_norm": 3.5579936504364014,
        "learning_rate": 0.00013889047520753448,
        "epoch": 0.7915642000149042,
        "step": 10622
    },
    {
        "loss": 2.6554,
        "grad_norm": 2.702641010284424,
        "learning_rate": 0.00013882563924262327,
        "epoch": 0.7916387212161861,
        "step": 10623
    },
    {
        "loss": 2.1646,
        "grad_norm": 3.87229061126709,
        "learning_rate": 0.00013876078405386646,
        "epoch": 0.7917132424174678,
        "step": 10624
    },
    {
        "loss": 2.1093,
        "grad_norm": 1.9454514980316162,
        "learning_rate": 0.00013869590967337643,
        "epoch": 0.7917877636187496,
        "step": 10625
    },
    {
        "loss": 2.6688,
        "grad_norm": 3.8882389068603516,
        "learning_rate": 0.00013863101613327444,
        "epoch": 0.7918622848200313,
        "step": 10626
    },
    {
        "loss": 2.6725,
        "grad_norm": 3.411525011062622,
        "learning_rate": 0.00013856610346569138,
        "epoch": 0.7919368060213131,
        "step": 10627
    },
    {
        "loss": 2.483,
        "grad_norm": 2.090283155441284,
        "learning_rate": 0.00013850117170276777,
        "epoch": 0.7920113272225948,
        "step": 10628
    },
    {
        "loss": 2.4152,
        "grad_norm": 3.5918233394622803,
        "learning_rate": 0.00013843622087665326,
        "epoch": 0.7920858484238766,
        "step": 10629
    },
    {
        "loss": 2.4532,
        "grad_norm": 4.3268938064575195,
        "learning_rate": 0.00013837125101950738,
        "epoch": 0.7921603696251583,
        "step": 10630
    },
    {
        "loss": 2.8804,
        "grad_norm": 4.050045490264893,
        "learning_rate": 0.00013830626216349874,
        "epoch": 0.7922348908264402,
        "step": 10631
    },
    {
        "loss": 2.0585,
        "grad_norm": 5.12533712387085,
        "learning_rate": 0.00013824125434080538,
        "epoch": 0.7923094120277219,
        "step": 10632
    },
    {
        "loss": 2.5214,
        "grad_norm": 1.668173909187317,
        "learning_rate": 0.0001381762275836147,
        "epoch": 0.7923839332290037,
        "step": 10633
    },
    {
        "loss": 1.9775,
        "grad_norm": 3.467824935913086,
        "learning_rate": 0.00013811118192412383,
        "epoch": 0.7924584544302854,
        "step": 10634
    },
    {
        "loss": 2.4397,
        "grad_norm": 2.860567331314087,
        "learning_rate": 0.00013804611739453893,
        "epoch": 0.7925329756315672,
        "step": 10635
    },
    {
        "loss": 2.0424,
        "grad_norm": 4.1422271728515625,
        "learning_rate": 0.00013798103402707547,
        "epoch": 0.7926074968328489,
        "step": 10636
    },
    {
        "loss": 2.2573,
        "grad_norm": 3.0766713619232178,
        "learning_rate": 0.00013791593185395835,
        "epoch": 0.7926820180341307,
        "step": 10637
    },
    {
        "loss": 2.3934,
        "grad_norm": 4.339736461639404,
        "learning_rate": 0.00013785081090742196,
        "epoch": 0.7927565392354124,
        "step": 10638
    },
    {
        "loss": 2.4925,
        "grad_norm": 3.288623571395874,
        "learning_rate": 0.0001377856712197096,
        "epoch": 0.7928310604366943,
        "step": 10639
    },
    {
        "loss": 2.6494,
        "grad_norm": 3.547410249710083,
        "learning_rate": 0.0001377205128230743,
        "epoch": 0.792905581637976,
        "step": 10640
    },
    {
        "loss": 2.0703,
        "grad_norm": 3.5522282123565674,
        "learning_rate": 0.00013765533574977798,
        "epoch": 0.7929801028392578,
        "step": 10641
    },
    {
        "loss": 2.3152,
        "grad_norm": 2.939476251602173,
        "learning_rate": 0.00013759014003209184,
        "epoch": 0.7930546240405395,
        "step": 10642
    },
    {
        "loss": 2.2808,
        "grad_norm": 4.022143363952637,
        "learning_rate": 0.00013752492570229678,
        "epoch": 0.7931291452418213,
        "step": 10643
    },
    {
        "loss": 2.851,
        "grad_norm": 3.0125138759613037,
        "learning_rate": 0.00013745969279268204,
        "epoch": 0.793203666443103,
        "step": 10644
    },
    {
        "loss": 2.54,
        "grad_norm": 2.292048454284668,
        "learning_rate": 0.00013739444133554696,
        "epoch": 0.7932781876443848,
        "step": 10645
    },
    {
        "loss": 2.6509,
        "grad_norm": 1.6253770589828491,
        "learning_rate": 0.00013732917136319954,
        "epoch": 0.7933527088456666,
        "step": 10646
    },
    {
        "loss": 2.416,
        "grad_norm": 1.7008450031280518,
        "learning_rate": 0.00013726388290795694,
        "epoch": 0.7934272300469484,
        "step": 10647
    },
    {
        "loss": 2.5281,
        "grad_norm": 2.612717628479004,
        "learning_rate": 0.0001371985760021459,
        "epoch": 0.7935017512482301,
        "step": 10648
    },
    {
        "loss": 2.4174,
        "grad_norm": 2.311117172241211,
        "learning_rate": 0.00013713325067810173,
        "epoch": 0.7935762724495119,
        "step": 10649
    },
    {
        "loss": 2.2649,
        "grad_norm": 3.6282565593719482,
        "learning_rate": 0.00013706790696816937,
        "epoch": 0.7936507936507936,
        "step": 10650
    },
    {
        "loss": 2.362,
        "grad_norm": 3.4910671710968018,
        "learning_rate": 0.00013700254490470258,
        "epoch": 0.7937253148520754,
        "step": 10651
    },
    {
        "loss": 2.0583,
        "grad_norm": 2.8436179161071777,
        "learning_rate": 0.00013693716452006416,
        "epoch": 0.7937998360533571,
        "step": 10652
    },
    {
        "loss": 2.7246,
        "grad_norm": 1.3415024280548096,
        "learning_rate": 0.00013687176584662602,
        "epoch": 0.793874357254639,
        "step": 10653
    },
    {
        "loss": 1.7261,
        "grad_norm": 4.153733730316162,
        "learning_rate": 0.00013680634891676956,
        "epoch": 0.7939488784559207,
        "step": 10654
    },
    {
        "loss": 2.1014,
        "grad_norm": 2.569828987121582,
        "learning_rate": 0.00013674091376288428,
        "epoch": 0.7940233996572025,
        "step": 10655
    },
    {
        "loss": 1.952,
        "grad_norm": 3.0222225189208984,
        "learning_rate": 0.0001366754604173697,
        "epoch": 0.7940979208584842,
        "step": 10656
    },
    {
        "loss": 1.8431,
        "grad_norm": 1.9523671865463257,
        "learning_rate": 0.00013660998891263366,
        "epoch": 0.794172442059766,
        "step": 10657
    },
    {
        "loss": 2.0277,
        "grad_norm": 2.0856447219848633,
        "learning_rate": 0.0001365444992810935,
        "epoch": 0.7942469632610478,
        "step": 10658
    },
    {
        "loss": 1.8387,
        "grad_norm": 3.0014212131500244,
        "learning_rate": 0.00013647899155517512,
        "epoch": 0.7943214844623295,
        "step": 10659
    },
    {
        "loss": 2.2101,
        "grad_norm": 3.024172306060791,
        "learning_rate": 0.00013641346576731344,
        "epoch": 0.7943960056636113,
        "step": 10660
    },
    {
        "loss": 2.5753,
        "grad_norm": 2.8534603118896484,
        "learning_rate": 0.00013634792194995268,
        "epoch": 0.7944705268648931,
        "step": 10661
    },
    {
        "loss": 2.8694,
        "grad_norm": 2.035182476043701,
        "learning_rate": 0.00013628236013554546,
        "epoch": 0.7945450480661749,
        "step": 10662
    },
    {
        "loss": 2.4034,
        "grad_norm": 3.528200387954712,
        "learning_rate": 0.000136216780356554,
        "epoch": 0.7946195692674566,
        "step": 10663
    },
    {
        "loss": 2.6757,
        "grad_norm": 2.359375238418579,
        "learning_rate": 0.00013615118264544843,
        "epoch": 0.7946940904687384,
        "step": 10664
    },
    {
        "loss": 2.6516,
        "grad_norm": 2.7976624965667725,
        "learning_rate": 0.00013608556703470875,
        "epoch": 0.7947686116700201,
        "step": 10665
    },
    {
        "loss": 2.0807,
        "grad_norm": 3.681622266769409,
        "learning_rate": 0.0001360199335568232,
        "epoch": 0.7948431328713019,
        "step": 10666
    },
    {
        "loss": 2.0296,
        "grad_norm": 4.039365291595459,
        "learning_rate": 0.00013595428224428906,
        "epoch": 0.7949176540725836,
        "step": 10667
    },
    {
        "loss": 2.3278,
        "grad_norm": 2.4544036388397217,
        "learning_rate": 0.0001358886131296126,
        "epoch": 0.7949921752738655,
        "step": 10668
    },
    {
        "loss": 2.2426,
        "grad_norm": 3.0539097785949707,
        "learning_rate": 0.00013582292624530866,
        "epoch": 0.7950666964751472,
        "step": 10669
    },
    {
        "loss": 1.6014,
        "grad_norm": 1.6853172779083252,
        "learning_rate": 0.00013575722162390084,
        "epoch": 0.795141217676429,
        "step": 10670
    },
    {
        "loss": 2.4697,
        "grad_norm": 2.7268645763397217,
        "learning_rate": 0.00013569149929792187,
        "epoch": 0.7952157388777107,
        "step": 10671
    },
    {
        "loss": 2.1502,
        "grad_norm": 3.107254981994629,
        "learning_rate": 0.00013562575929991293,
        "epoch": 0.7952902600789925,
        "step": 10672
    },
    {
        "loss": 1.2809,
        "grad_norm": 3.937565326690674,
        "learning_rate": 0.00013556000166242392,
        "epoch": 0.7953647812802742,
        "step": 10673
    },
    {
        "loss": 2.6429,
        "grad_norm": 3.230773448944092,
        "learning_rate": 0.000135494226418014,
        "epoch": 0.795439302481556,
        "step": 10674
    },
    {
        "loss": 2.7201,
        "grad_norm": 2.8229787349700928,
        "learning_rate": 0.00013542843359925008,
        "epoch": 0.7955138236828377,
        "step": 10675
    },
    {
        "loss": 2.53,
        "grad_norm": 1.8959037065505981,
        "learning_rate": 0.00013536262323870878,
        "epoch": 0.7955883448841196,
        "step": 10676
    },
    {
        "loss": 2.2036,
        "grad_norm": 2.7977821826934814,
        "learning_rate": 0.00013529679536897465,
        "epoch": 0.7956628660854013,
        "step": 10677
    },
    {
        "loss": 2.3767,
        "grad_norm": 3.7095749378204346,
        "learning_rate": 0.00013523095002264155,
        "epoch": 0.7957373872866831,
        "step": 10678
    },
    {
        "loss": 2.1093,
        "grad_norm": 3.7886903285980225,
        "learning_rate": 0.00013516508723231146,
        "epoch": 0.7958119084879648,
        "step": 10679
    },
    {
        "loss": 2.6522,
        "grad_norm": 2.521500825881958,
        "learning_rate": 0.00013509920703059513,
        "epoch": 0.7958864296892466,
        "step": 10680
    },
    {
        "loss": 2.0861,
        "grad_norm": 3.971031665802002,
        "learning_rate": 0.00013503330945011224,
        "epoch": 0.7959609508905283,
        "step": 10681
    },
    {
        "loss": 2.3614,
        "grad_norm": 2.5868172645568848,
        "learning_rate": 0.00013496739452349064,
        "epoch": 0.7960354720918101,
        "step": 10682
    },
    {
        "loss": 2.6747,
        "grad_norm": 2.368143320083618,
        "learning_rate": 0.00013490146228336725,
        "epoch": 0.7961099932930918,
        "step": 10683
    },
    {
        "loss": 2.0055,
        "grad_norm": 3.243562698364258,
        "learning_rate": 0.00013483551276238684,
        "epoch": 0.7961845144943737,
        "step": 10684
    },
    {
        "loss": 2.2209,
        "grad_norm": 3.4664857387542725,
        "learning_rate": 0.00013476954599320355,
        "epoch": 0.7962590356956554,
        "step": 10685
    },
    {
        "loss": 2.7606,
        "grad_norm": 2.618084192276001,
        "learning_rate": 0.00013470356200847956,
        "epoch": 0.7963335568969372,
        "step": 10686
    },
    {
        "loss": 2.718,
        "grad_norm": 3.351284980773926,
        "learning_rate": 0.0001346375608408857,
        "epoch": 0.7964080780982189,
        "step": 10687
    },
    {
        "loss": 2.5795,
        "grad_norm": 3.5231847763061523,
        "learning_rate": 0.00013457154252310115,
        "epoch": 0.7964825992995007,
        "step": 10688
    },
    {
        "loss": 2.3631,
        "grad_norm": 3.4814040660858154,
        "learning_rate": 0.00013450550708781405,
        "epoch": 0.7965571205007824,
        "step": 10689
    },
    {
        "loss": 2.8434,
        "grad_norm": 2.309288501739502,
        "learning_rate": 0.0001344394545677204,
        "epoch": 0.7966316417020642,
        "step": 10690
    },
    {
        "loss": 2.2776,
        "grad_norm": 2.489431858062744,
        "learning_rate": 0.00013437338499552526,
        "epoch": 0.796706162903346,
        "step": 10691
    },
    {
        "loss": 2.6208,
        "grad_norm": 2.7218236923217773,
        "learning_rate": 0.0001343072984039417,
        "epoch": 0.7967806841046278,
        "step": 10692
    },
    {
        "loss": 2.6082,
        "grad_norm": 2.3002872467041016,
        "learning_rate": 0.0001342411948256912,
        "epoch": 0.7968552053059096,
        "step": 10693
    },
    {
        "loss": 2.1662,
        "grad_norm": 2.59830379486084,
        "learning_rate": 0.00013417507429350423,
        "epoch": 0.7969297265071913,
        "step": 10694
    },
    {
        "loss": 2.2505,
        "grad_norm": 3.8215670585632324,
        "learning_rate": 0.0001341089368401187,
        "epoch": 0.7970042477084731,
        "step": 10695
    },
    {
        "loss": 2.8355,
        "grad_norm": 2.2393412590026855,
        "learning_rate": 0.00013404278249828182,
        "epoch": 0.7970787689097548,
        "step": 10696
    },
    {
        "loss": 2.1839,
        "grad_norm": 3.3360750675201416,
        "learning_rate": 0.0001339766113007486,
        "epoch": 0.7971532901110366,
        "step": 10697
    },
    {
        "loss": 1.9548,
        "grad_norm": 3.3349242210388184,
        "learning_rate": 0.0001339104232802825,
        "epoch": 0.7972278113123183,
        "step": 10698
    },
    {
        "loss": 1.6417,
        "grad_norm": 1.055184245109558,
        "learning_rate": 0.00013384421846965566,
        "epoch": 0.7973023325136002,
        "step": 10699
    },
    {
        "loss": 2.9585,
        "grad_norm": 1.8877406120300293,
        "learning_rate": 0.0001337779969016479,
        "epoch": 0.7973768537148819,
        "step": 10700
    },
    {
        "loss": 2.2954,
        "grad_norm": 3.284928321838379,
        "learning_rate": 0.00013371175860904796,
        "epoch": 0.7974513749161637,
        "step": 10701
    },
    {
        "loss": 2.1976,
        "grad_norm": 3.9137227535247803,
        "learning_rate": 0.00013364550362465248,
        "epoch": 0.7975258961174454,
        "step": 10702
    },
    {
        "loss": 2.2245,
        "grad_norm": 1.7166703939437866,
        "learning_rate": 0.00013357923198126654,
        "epoch": 0.7976004173187272,
        "step": 10703
    },
    {
        "loss": 2.7999,
        "grad_norm": 3.070072650909424,
        "learning_rate": 0.00013351294371170315,
        "epoch": 0.7976749385200089,
        "step": 10704
    },
    {
        "loss": 2.7498,
        "grad_norm": 2.49898099899292,
        "learning_rate": 0.00013344663884878413,
        "epoch": 0.7977494597212907,
        "step": 10705
    },
    {
        "loss": 2.5489,
        "grad_norm": 3.8173134326934814,
        "learning_rate": 0.00013338031742533905,
        "epoch": 0.7978239809225725,
        "step": 10706
    },
    {
        "loss": 1.9388,
        "grad_norm": 3.4316630363464355,
        "learning_rate": 0.00013331397947420585,
        "epoch": 0.7978985021238543,
        "step": 10707
    },
    {
        "loss": 2.4016,
        "grad_norm": 3.0525829792022705,
        "learning_rate": 0.0001332476250282304,
        "epoch": 0.797973023325136,
        "step": 10708
    },
    {
        "loss": 1.5794,
        "grad_norm": 4.887421607971191,
        "learning_rate": 0.00013318125412026732,
        "epoch": 0.7980475445264178,
        "step": 10709
    },
    {
        "loss": 2.488,
        "grad_norm": 2.79982590675354,
        "learning_rate": 0.0001331148667831787,
        "epoch": 0.7981220657276995,
        "step": 10710
    },
    {
        "loss": 2.479,
        "grad_norm": 2.169501304626465,
        "learning_rate": 0.00013304846304983538,
        "epoch": 0.7981965869289813,
        "step": 10711
    },
    {
        "loss": 2.795,
        "grad_norm": 2.3040120601654053,
        "learning_rate": 0.00013298204295311586,
        "epoch": 0.798271108130263,
        "step": 10712
    },
    {
        "loss": 1.9782,
        "grad_norm": 3.150301694869995,
        "learning_rate": 0.00013291560652590678,
        "epoch": 0.7983456293315448,
        "step": 10713
    },
    {
        "loss": 1.02,
        "grad_norm": 3.511286497116089,
        "learning_rate": 0.0001328491538011034,
        "epoch": 0.7984201505328266,
        "step": 10714
    },
    {
        "loss": 2.4653,
        "grad_norm": 2.990159034729004,
        "learning_rate": 0.0001327826848116081,
        "epoch": 0.7984946717341084,
        "step": 10715
    },
    {
        "loss": 2.1962,
        "grad_norm": 2.5923643112182617,
        "learning_rate": 0.00013271619959033225,
        "epoch": 0.7985691929353901,
        "step": 10716
    },
    {
        "loss": 2.4968,
        "grad_norm": 3.6822617053985596,
        "learning_rate": 0.00013264969817019474,
        "epoch": 0.7986437141366719,
        "step": 10717
    },
    {
        "loss": 2.4579,
        "grad_norm": 2.2294838428497314,
        "learning_rate": 0.00013258318058412244,
        "epoch": 0.7987182353379536,
        "step": 10718
    },
    {
        "loss": 2.6943,
        "grad_norm": 3.036529064178467,
        "learning_rate": 0.00013251664686505073,
        "epoch": 0.7987927565392354,
        "step": 10719
    },
    {
        "loss": 2.2983,
        "grad_norm": 5.122285842895508,
        "learning_rate": 0.00013245009704592237,
        "epoch": 0.7988672777405171,
        "step": 10720
    },
    {
        "loss": 2.2691,
        "grad_norm": 4.161751747131348,
        "learning_rate": 0.00013238353115968833,
        "epoch": 0.798941798941799,
        "step": 10721
    },
    {
        "loss": 2.3401,
        "grad_norm": 4.198709487915039,
        "learning_rate": 0.0001323169492393078,
        "epoch": 0.7990163201430807,
        "step": 10722
    },
    {
        "loss": 2.7199,
        "grad_norm": 3.4215760231018066,
        "learning_rate": 0.0001322503513177475,
        "epoch": 0.7990908413443625,
        "step": 10723
    },
    {
        "loss": 2.146,
        "grad_norm": 3.279860019683838,
        "learning_rate": 0.00013218373742798214,
        "epoch": 0.7991653625456442,
        "step": 10724
    },
    {
        "loss": 1.4659,
        "grad_norm": 3.129979133605957,
        "learning_rate": 0.00013211710760299483,
        "epoch": 0.799239883746926,
        "step": 10725
    },
    {
        "loss": 2.1432,
        "grad_norm": 2.8438687324523926,
        "learning_rate": 0.00013205046187577565,
        "epoch": 0.7993144049482077,
        "step": 10726
    },
    {
        "loss": 2.3853,
        "grad_norm": 1.773189663887024,
        "learning_rate": 0.00013198380027932345,
        "epoch": 0.7993889261494895,
        "step": 10727
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.5809319019317627,
        "learning_rate": 0.00013191712284664435,
        "epoch": 0.7994634473507712,
        "step": 10728
    },
    {
        "loss": 2.2529,
        "grad_norm": 3.4457483291625977,
        "learning_rate": 0.00013185042961075277,
        "epoch": 0.7995379685520531,
        "step": 10729
    },
    {
        "loss": 2.3945,
        "grad_norm": 3.045074462890625,
        "learning_rate": 0.00013178372060467062,
        "epoch": 0.7996124897533349,
        "step": 10730
    },
    {
        "loss": 2.1303,
        "grad_norm": 4.166909217834473,
        "learning_rate": 0.0001317169958614275,
        "epoch": 0.7996870109546166,
        "step": 10731
    },
    {
        "loss": 2.351,
        "grad_norm": 2.5645253658294678,
        "learning_rate": 0.00013165025541406135,
        "epoch": 0.7997615321558984,
        "step": 10732
    },
    {
        "loss": 2.5568,
        "grad_norm": 2.406773805618286,
        "learning_rate": 0.00013158349929561725,
        "epoch": 0.7998360533571801,
        "step": 10733
    },
    {
        "loss": 2.4684,
        "grad_norm": 2.1074671745300293,
        "learning_rate": 0.0001315167275391488,
        "epoch": 0.7999105745584619,
        "step": 10734
    },
    {
        "loss": 2.5908,
        "grad_norm": 2.5882744789123535,
        "learning_rate": 0.0001314499401777163,
        "epoch": 0.7999850957597436,
        "step": 10735
    },
    {
        "loss": 2.2971,
        "grad_norm": 3.5721523761749268,
        "learning_rate": 0.00013138313724438868,
        "epoch": 0.8000596169610255,
        "step": 10736
    },
    {
        "loss": 2.7464,
        "grad_norm": 2.100429058074951,
        "learning_rate": 0.00013131631877224227,
        "epoch": 0.8001341381623072,
        "step": 10737
    },
    {
        "loss": 2.5556,
        "grad_norm": 1.7485278844833374,
        "learning_rate": 0.0001312494847943609,
        "epoch": 0.800208659363589,
        "step": 10738
    },
    {
        "loss": 1.7534,
        "grad_norm": 2.714061975479126,
        "learning_rate": 0.0001311826353438365,
        "epoch": 0.8002831805648707,
        "step": 10739
    },
    {
        "loss": 1.4089,
        "grad_norm": 5.345097064971924,
        "learning_rate": 0.00013111577045376828,
        "epoch": 0.8003577017661525,
        "step": 10740
    },
    {
        "loss": 2.2888,
        "grad_norm": 3.0061838626861572,
        "learning_rate": 0.00013104889015726315,
        "epoch": 0.8004322229674342,
        "step": 10741
    },
    {
        "loss": 2.3071,
        "grad_norm": 4.927674770355225,
        "learning_rate": 0.00013098199448743602,
        "epoch": 0.800506744168716,
        "step": 10742
    },
    {
        "loss": 1.4137,
        "grad_norm": 3.9777445793151855,
        "learning_rate": 0.00013091508347740888,
        "epoch": 0.8005812653699977,
        "step": 10743
    },
    {
        "loss": 2.5694,
        "grad_norm": 2.8558404445648193,
        "learning_rate": 0.00013084815716031157,
        "epoch": 0.8006557865712796,
        "step": 10744
    },
    {
        "loss": 2.8136,
        "grad_norm": 3.4999334812164307,
        "learning_rate": 0.0001307812155692818,
        "epoch": 0.8007303077725613,
        "step": 10745
    },
    {
        "loss": 2.2703,
        "grad_norm": 4.21792459487915,
        "learning_rate": 0.00013071425873746403,
        "epoch": 0.8008048289738431,
        "step": 10746
    },
    {
        "loss": 2.1354,
        "grad_norm": 2.59751558303833,
        "learning_rate": 0.0001306472866980112,
        "epoch": 0.8008793501751248,
        "step": 10747
    },
    {
        "loss": 2.4392,
        "grad_norm": 2.904735803604126,
        "learning_rate": 0.00013058029948408324,
        "epoch": 0.8009538713764066,
        "step": 10748
    },
    {
        "loss": 1.5143,
        "grad_norm": 3.607573986053467,
        "learning_rate": 0.00013051329712884756,
        "epoch": 0.8010283925776883,
        "step": 10749
    },
    {
        "loss": 1.9347,
        "grad_norm": 3.8724308013916016,
        "learning_rate": 0.00013044627966547943,
        "epoch": 0.8011029137789701,
        "step": 10750
    },
    {
        "loss": 2.4662,
        "grad_norm": 3.784271717071533,
        "learning_rate": 0.00013037924712716123,
        "epoch": 0.8011774349802518,
        "step": 10751
    },
    {
        "loss": 2.6477,
        "grad_norm": 1.1888259649276733,
        "learning_rate": 0.00013031219954708317,
        "epoch": 0.8012519561815337,
        "step": 10752
    },
    {
        "loss": 2.3828,
        "grad_norm": 3.0820226669311523,
        "learning_rate": 0.00013024513695844258,
        "epoch": 0.8013264773828154,
        "step": 10753
    },
    {
        "loss": 1.794,
        "grad_norm": 2.732785940170288,
        "learning_rate": 0.00013017805939444433,
        "epoch": 0.8014009985840972,
        "step": 10754
    },
    {
        "loss": 2.5512,
        "grad_norm": 1.8374779224395752,
        "learning_rate": 0.0001301109668883006,
        "epoch": 0.8014755197853789,
        "step": 10755
    },
    {
        "loss": 2.4508,
        "grad_norm": 2.9993958473205566,
        "learning_rate": 0.0001300438594732314,
        "epoch": 0.8015500409866607,
        "step": 10756
    },
    {
        "loss": 2.1658,
        "grad_norm": 1.8355128765106201,
        "learning_rate": 0.00012997673718246358,
        "epoch": 0.8016245621879424,
        "step": 10757
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.4182296991348267,
        "learning_rate": 0.00012990960004923162,
        "epoch": 0.8016990833892242,
        "step": 10758
    },
    {
        "loss": 2.5928,
        "grad_norm": 3.099910020828247,
        "learning_rate": 0.0001298424481067772,
        "epoch": 0.801773604590506,
        "step": 10759
    },
    {
        "loss": 2.6039,
        "grad_norm": 1.9263722896575928,
        "learning_rate": 0.00012977528138834977,
        "epoch": 0.8018481257917878,
        "step": 10760
    },
    {
        "loss": 2.226,
        "grad_norm": 3.979280948638916,
        "learning_rate": 0.00012970809992720534,
        "epoch": 0.8019226469930695,
        "step": 10761
    },
    {
        "loss": 2.6923,
        "grad_norm": 3.5569007396698,
        "learning_rate": 0.00012964090375660806,
        "epoch": 0.8019971681943513,
        "step": 10762
    },
    {
        "loss": 2.1456,
        "grad_norm": 3.760057210922241,
        "learning_rate": 0.00012957369290982876,
        "epoch": 0.802071689395633,
        "step": 10763
    },
    {
        "loss": 2.4196,
        "grad_norm": 4.385921478271484,
        "learning_rate": 0.00012950646742014566,
        "epoch": 0.8021462105969148,
        "step": 10764
    },
    {
        "loss": 2.4948,
        "grad_norm": 3.630740165710449,
        "learning_rate": 0.0001294392273208446,
        "epoch": 0.8022207317981966,
        "step": 10765
    },
    {
        "loss": 2.1684,
        "grad_norm": 3.6609201431274414,
        "learning_rate": 0.00012937197264521787,
        "epoch": 0.8022952529994783,
        "step": 10766
    },
    {
        "loss": 1.758,
        "grad_norm": 4.50434684753418,
        "learning_rate": 0.00012930470342656595,
        "epoch": 0.8023697742007602,
        "step": 10767
    },
    {
        "loss": 2.2551,
        "grad_norm": 3.846750497817993,
        "learning_rate": 0.00012923741969819572,
        "epoch": 0.8024442954020419,
        "step": 10768
    },
    {
        "loss": 2.3458,
        "grad_norm": 2.2104032039642334,
        "learning_rate": 0.0001291701214934216,
        "epoch": 0.8025188166033237,
        "step": 10769
    },
    {
        "loss": 2.3041,
        "grad_norm": 4.573181629180908,
        "learning_rate": 0.00012910280884556526,
        "epoch": 0.8025933378046054,
        "step": 10770
    },
    {
        "loss": 2.6546,
        "grad_norm": 2.3078298568725586,
        "learning_rate": 0.0001290354817879552,
        "epoch": 0.8026678590058872,
        "step": 10771
    },
    {
        "loss": 2.1966,
        "grad_norm": 2.6161675453186035,
        "learning_rate": 0.00012896814035392754,
        "epoch": 0.8027423802071689,
        "step": 10772
    },
    {
        "loss": 3.1205,
        "grad_norm": 2.493351459503174,
        "learning_rate": 0.00012890078457682507,
        "epoch": 0.8028169014084507,
        "step": 10773
    },
    {
        "loss": 2.524,
        "grad_norm": 3.5207297801971436,
        "learning_rate": 0.00012883341448999782,
        "epoch": 0.8028914226097325,
        "step": 10774
    },
    {
        "loss": 1.8618,
        "grad_norm": 2.168928861618042,
        "learning_rate": 0.0001287660301268028,
        "epoch": 0.8029659438110143,
        "step": 10775
    },
    {
        "loss": 3.1669,
        "grad_norm": 2.8510570526123047,
        "learning_rate": 0.0001286986315206045,
        "epoch": 0.803040465012296,
        "step": 10776
    },
    {
        "loss": 2.7567,
        "grad_norm": 2.3540902137756348,
        "learning_rate": 0.00012863121870477403,
        "epoch": 0.8031149862135778,
        "step": 10777
    },
    {
        "loss": 2.6186,
        "grad_norm": 2.4748880863189697,
        "learning_rate": 0.0001285637917126897,
        "epoch": 0.8031895074148595,
        "step": 10778
    },
    {
        "loss": 1.8281,
        "grad_norm": 4.3044939041137695,
        "learning_rate": 0.00012849635057773668,
        "epoch": 0.8032640286161413,
        "step": 10779
    },
    {
        "loss": 1.7939,
        "grad_norm": 3.8525948524475098,
        "learning_rate": 0.00012842889533330753,
        "epoch": 0.803338549817423,
        "step": 10780
    },
    {
        "loss": 2.2733,
        "grad_norm": 5.467250823974609,
        "learning_rate": 0.0001283614260128014,
        "epoch": 0.8034130710187048,
        "step": 10781
    },
    {
        "loss": 3.023,
        "grad_norm": 2.971796751022339,
        "learning_rate": 0.0001282939426496245,
        "epoch": 0.8034875922199866,
        "step": 10782
    },
    {
        "loss": 2.1312,
        "grad_norm": 2.1065783500671387,
        "learning_rate": 0.0001282264452771903,
        "epoch": 0.8035621134212684,
        "step": 10783
    },
    {
        "loss": 2.6262,
        "grad_norm": 3.9721829891204834,
        "learning_rate": 0.00012815893392891865,
        "epoch": 0.8036366346225501,
        "step": 10784
    },
    {
        "loss": 2.1296,
        "grad_norm": 4.345099449157715,
        "learning_rate": 0.000128091408638237,
        "epoch": 0.8037111558238319,
        "step": 10785
    },
    {
        "loss": 1.636,
        "grad_norm": 4.953359127044678,
        "learning_rate": 0.0001280238694385789,
        "epoch": 0.8037856770251136,
        "step": 10786
    },
    {
        "loss": 2.308,
        "grad_norm": 2.494504690170288,
        "learning_rate": 0.00012795631636338555,
        "epoch": 0.8038601982263954,
        "step": 10787
    },
    {
        "loss": 2.4038,
        "grad_norm": 2.5685672760009766,
        "learning_rate": 0.00012788874944610461,
        "epoch": 0.8039347194276771,
        "step": 10788
    },
    {
        "loss": 2.2391,
        "grad_norm": 1.9972962141036987,
        "learning_rate": 0.00012782116872019046,
        "epoch": 0.804009240628959,
        "step": 10789
    },
    {
        "loss": 1.7639,
        "grad_norm": 3.999054193496704,
        "learning_rate": 0.00012775357421910486,
        "epoch": 0.8040837618302407,
        "step": 10790
    },
    {
        "loss": 2.1724,
        "grad_norm": 3.1372313499450684,
        "learning_rate": 0.0001276859659763159,
        "epoch": 0.8041582830315225,
        "step": 10791
    },
    {
        "loss": 2.7337,
        "grad_norm": 2.3760218620300293,
        "learning_rate": 0.00012761834402529853,
        "epoch": 0.8042328042328042,
        "step": 10792
    },
    {
        "loss": 1.9766,
        "grad_norm": 3.6645328998565674,
        "learning_rate": 0.0001275507083995348,
        "epoch": 0.804307325434086,
        "step": 10793
    },
    {
        "loss": 2.9667,
        "grad_norm": 3.367629289627075,
        "learning_rate": 0.00012748305913251327,
        "epoch": 0.8043818466353677,
        "step": 10794
    },
    {
        "loss": 1.6638,
        "grad_norm": 4.674901485443115,
        "learning_rate": 0.0001274153962577291,
        "epoch": 0.8044563678366495,
        "step": 10795
    },
    {
        "loss": 2.1176,
        "grad_norm": 3.760183811187744,
        "learning_rate": 0.00012734771980868486,
        "epoch": 0.8045308890379312,
        "step": 10796
    },
    {
        "loss": 2.5571,
        "grad_norm": 3.4138948917388916,
        "learning_rate": 0.00012728002981888875,
        "epoch": 0.8046054102392131,
        "step": 10797
    },
    {
        "loss": 2.7661,
        "grad_norm": 3.6457486152648926,
        "learning_rate": 0.00012721232632185685,
        "epoch": 0.8046799314404948,
        "step": 10798
    },
    {
        "loss": 2.7699,
        "grad_norm": 2.421271562576294,
        "learning_rate": 0.00012714460935111095,
        "epoch": 0.8047544526417766,
        "step": 10799
    },
    {
        "loss": 2.2172,
        "grad_norm": 2.905531644821167,
        "learning_rate": 0.0001270768789401803,
        "epoch": 0.8048289738430584,
        "step": 10800
    },
    {
        "loss": 1.6021,
        "grad_norm": 4.580999374389648,
        "learning_rate": 0.00012700913512260031,
        "epoch": 0.8049034950443401,
        "step": 10801
    },
    {
        "loss": 2.3051,
        "grad_norm": 2.834362506866455,
        "learning_rate": 0.000126941377931913,
        "epoch": 0.8049780162456219,
        "step": 10802
    },
    {
        "loss": 2.346,
        "grad_norm": 1.9728139638900757,
        "learning_rate": 0.0001268736074016674,
        "epoch": 0.8050525374469036,
        "step": 10803
    },
    {
        "loss": 2.9733,
        "grad_norm": 1.763575792312622,
        "learning_rate": 0.00012680582356541873,
        "epoch": 0.8051270586481855,
        "step": 10804
    },
    {
        "loss": 2.4084,
        "grad_norm": 2.769484281539917,
        "learning_rate": 0.00012673802645672938,
        "epoch": 0.8052015798494672,
        "step": 10805
    },
    {
        "loss": 2.6163,
        "grad_norm": 3.056212902069092,
        "learning_rate": 0.00012667021610916735,
        "epoch": 0.805276101050749,
        "step": 10806
    },
    {
        "loss": 1.8525,
        "grad_norm": 4.417164325714111,
        "learning_rate": 0.00012660239255630814,
        "epoch": 0.8053506222520307,
        "step": 10807
    },
    {
        "loss": 2.4687,
        "grad_norm": 3.247718095779419,
        "learning_rate": 0.00012653455583173332,
        "epoch": 0.8054251434533125,
        "step": 10808
    },
    {
        "loss": 2.2288,
        "grad_norm": 1.6999962329864502,
        "learning_rate": 0.0001264667059690311,
        "epoch": 0.8054996646545942,
        "step": 10809
    },
    {
        "loss": 2.0943,
        "grad_norm": 2.7214043140411377,
        "learning_rate": 0.000126398843001796,
        "epoch": 0.805574185855876,
        "step": 10810
    },
    {
        "loss": 2.1524,
        "grad_norm": 2.587244987487793,
        "learning_rate": 0.00012633096696362944,
        "epoch": 0.8056487070571577,
        "step": 10811
    },
    {
        "loss": 3.0826,
        "grad_norm": 4.693606853485107,
        "learning_rate": 0.00012626307788813886,
        "epoch": 0.8057232282584396,
        "step": 10812
    },
    {
        "loss": 1.9395,
        "grad_norm": 3.9841182231903076,
        "learning_rate": 0.0001261951758089386,
        "epoch": 0.8057977494597213,
        "step": 10813
    },
    {
        "loss": 2.3192,
        "grad_norm": 2.1326122283935547,
        "learning_rate": 0.00012612726075964906,
        "epoch": 0.8058722706610031,
        "step": 10814
    },
    {
        "loss": 2.394,
        "grad_norm": 2.1718130111694336,
        "learning_rate": 0.00012605933277389704,
        "epoch": 0.8059467918622848,
        "step": 10815
    },
    {
        "loss": 2.4458,
        "grad_norm": 3.975414991378784,
        "learning_rate": 0.0001259913918853164,
        "epoch": 0.8060213130635666,
        "step": 10816
    },
    {
        "loss": 2.5801,
        "grad_norm": 2.804409980773926,
        "learning_rate": 0.0001259234381275463,
        "epoch": 0.8060958342648483,
        "step": 10817
    },
    {
        "loss": 3.1125,
        "grad_norm": 1.8618208169937134,
        "learning_rate": 0.00012585547153423328,
        "epoch": 0.8061703554661301,
        "step": 10818
    },
    {
        "loss": 1.6632,
        "grad_norm": 3.2903084754943848,
        "learning_rate": 0.0001257874921390297,
        "epoch": 0.8062448766674118,
        "step": 10819
    },
    {
        "loss": 1.9418,
        "grad_norm": 2.9851624965667725,
        "learning_rate": 0.00012571949997559426,
        "epoch": 0.8063193978686937,
        "step": 10820
    },
    {
        "loss": 2.3802,
        "grad_norm": 2.7593400478363037,
        "learning_rate": 0.00012565149507759237,
        "epoch": 0.8063939190699754,
        "step": 10821
    },
    {
        "loss": 2.588,
        "grad_norm": 2.0803818702697754,
        "learning_rate": 0.00012558347747869522,
        "epoch": 0.8064684402712572,
        "step": 10822
    },
    {
        "loss": 2.6169,
        "grad_norm": 1.8779585361480713,
        "learning_rate": 0.00012551544721258075,
        "epoch": 0.8065429614725389,
        "step": 10823
    },
    {
        "loss": 2.4596,
        "grad_norm": 3.627004384994507,
        "learning_rate": 0.00012544740431293295,
        "epoch": 0.8066174826738207,
        "step": 10824
    },
    {
        "loss": 2.5752,
        "grad_norm": 3.369997978210449,
        "learning_rate": 0.000125379348813442,
        "epoch": 0.8066920038751024,
        "step": 10825
    },
    {
        "loss": 2.0878,
        "grad_norm": 3.7333731651306152,
        "learning_rate": 0.00012531128074780428,
        "epoch": 0.8067665250763842,
        "step": 10826
    },
    {
        "loss": 1.9526,
        "grad_norm": 3.905308961868286,
        "learning_rate": 0.00012524320014972287,
        "epoch": 0.806841046277666,
        "step": 10827
    },
    {
        "loss": 2.79,
        "grad_norm": 2.2981550693511963,
        "learning_rate": 0.0001251751070529065,
        "epoch": 0.8069155674789478,
        "step": 10828
    },
    {
        "loss": 1.752,
        "grad_norm": 3.937548875808716,
        "learning_rate": 0.00012510700149107027,
        "epoch": 0.8069900886802295,
        "step": 10829
    },
    {
        "loss": 2.8655,
        "grad_norm": 2.2563531398773193,
        "learning_rate": 0.00012503888349793538,
        "epoch": 0.8070646098815113,
        "step": 10830
    },
    {
        "loss": 1.6696,
        "grad_norm": 4.725431442260742,
        "learning_rate": 0.0001249707531072295,
        "epoch": 0.807139131082793,
        "step": 10831
    },
    {
        "loss": 2.1511,
        "grad_norm": 2.6104090213775635,
        "learning_rate": 0.00012490261035268607,
        "epoch": 0.8072136522840748,
        "step": 10832
    },
    {
        "loss": 1.6193,
        "grad_norm": 2.665048360824585,
        "learning_rate": 0.00012483445526804493,
        "epoch": 0.8072881734853565,
        "step": 10833
    },
    {
        "loss": 2.3863,
        "grad_norm": 3.627377986907959,
        "learning_rate": 0.00012476628788705182,
        "epoch": 0.8073626946866383,
        "step": 10834
    },
    {
        "loss": 0.8444,
        "grad_norm": 3.8017635345458984,
        "learning_rate": 0.00012469810824345849,
        "epoch": 0.8074372158879202,
        "step": 10835
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.4335899353027344,
        "learning_rate": 0.00012462991637102338,
        "epoch": 0.8075117370892019,
        "step": 10836
    },
    {
        "loss": 2.7457,
        "grad_norm": 3.009159803390503,
        "learning_rate": 0.00012456171230350988,
        "epoch": 0.8075862582904837,
        "step": 10837
    },
    {
        "loss": 2.5601,
        "grad_norm": 1.9236454963684082,
        "learning_rate": 0.00012449349607468853,
        "epoch": 0.8076607794917654,
        "step": 10838
    },
    {
        "loss": 2.8206,
        "grad_norm": 2.5938103199005127,
        "learning_rate": 0.00012442526771833527,
        "epoch": 0.8077353006930472,
        "step": 10839
    },
    {
        "loss": 2.4144,
        "grad_norm": 3.454608678817749,
        "learning_rate": 0.00012435702726823204,
        "epoch": 0.8078098218943289,
        "step": 10840
    },
    {
        "loss": 2.6723,
        "grad_norm": 2.0133659839630127,
        "learning_rate": 0.0001242887747581672,
        "epoch": 0.8078843430956107,
        "step": 10841
    },
    {
        "loss": 2.0073,
        "grad_norm": 3.0050156116485596,
        "learning_rate": 0.0001242205102219347,
        "epoch": 0.8079588642968925,
        "step": 10842
    },
    {
        "loss": 2.3783,
        "grad_norm": 3.4364075660705566,
        "learning_rate": 0.00012415223369333436,
        "epoch": 0.8080333854981743,
        "step": 10843
    },
    {
        "loss": 2.126,
        "grad_norm": 2.354573965072632,
        "learning_rate": 0.0001240839452061724,
        "epoch": 0.808107906699456,
        "step": 10844
    },
    {
        "loss": 2.1066,
        "grad_norm": 2.778446912765503,
        "learning_rate": 0.0001240156447942606,
        "epoch": 0.8081824279007378,
        "step": 10845
    },
    {
        "loss": 2.6235,
        "grad_norm": 2.3901920318603516,
        "learning_rate": 0.00012394733249141657,
        "epoch": 0.8082569491020195,
        "step": 10846
    },
    {
        "loss": 2.3452,
        "grad_norm": 2.4576759338378906,
        "learning_rate": 0.00012387900833146434,
        "epoch": 0.8083314703033013,
        "step": 10847
    },
    {
        "loss": 2.3704,
        "grad_norm": 2.4024155139923096,
        "learning_rate": 0.00012381067234823296,
        "epoch": 0.808405991504583,
        "step": 10848
    },
    {
        "loss": 1.7056,
        "grad_norm": 3.8753724098205566,
        "learning_rate": 0.00012374232457555817,
        "epoch": 0.8084805127058649,
        "step": 10849
    },
    {
        "loss": 2.6881,
        "grad_norm": 3.7188708782196045,
        "learning_rate": 0.00012367396504728092,
        "epoch": 0.8085550339071466,
        "step": 10850
    },
    {
        "loss": 1.5148,
        "grad_norm": 4.641262531280518,
        "learning_rate": 0.0001236055937972486,
        "epoch": 0.8086295551084284,
        "step": 10851
    },
    {
        "loss": 2.4072,
        "grad_norm": 2.380263090133667,
        "learning_rate": 0.00012353721085931377,
        "epoch": 0.8087040763097101,
        "step": 10852
    },
    {
        "loss": 1.4022,
        "grad_norm": 3.0753324031829834,
        "learning_rate": 0.00012346881626733506,
        "epoch": 0.8087785975109919,
        "step": 10853
    },
    {
        "loss": 2.1524,
        "grad_norm": 2.049583673477173,
        "learning_rate": 0.00012340041005517706,
        "epoch": 0.8088531187122736,
        "step": 10854
    },
    {
        "loss": 2.308,
        "grad_norm": 3.9013233184814453,
        "learning_rate": 0.00012333199225670976,
        "epoch": 0.8089276399135554,
        "step": 10855
    },
    {
        "loss": 2.2998,
        "grad_norm": 3.395632028579712,
        "learning_rate": 0.00012326356290580932,
        "epoch": 0.8090021611148371,
        "step": 10856
    },
    {
        "loss": 2.1611,
        "grad_norm": 4.683459281921387,
        "learning_rate": 0.0001231951220363569,
        "epoch": 0.809076682316119,
        "step": 10857
    },
    {
        "loss": 2.1979,
        "grad_norm": 2.442070960998535,
        "learning_rate": 0.0001231266696822402,
        "epoch": 0.8091512035174007,
        "step": 10858
    },
    {
        "loss": 2.312,
        "grad_norm": 3.9703757762908936,
        "learning_rate": 0.00012305820587735209,
        "epoch": 0.8092257247186825,
        "step": 10859
    },
    {
        "loss": 1.8969,
        "grad_norm": 4.6163835525512695,
        "learning_rate": 0.0001229897306555911,
        "epoch": 0.8093002459199642,
        "step": 10860
    },
    {
        "loss": 2.5571,
        "grad_norm": 2.501211643218994,
        "learning_rate": 0.0001229212440508618,
        "epoch": 0.809374767121246,
        "step": 10861
    },
    {
        "loss": 1.9417,
        "grad_norm": 3.4062981605529785,
        "learning_rate": 0.0001228527460970741,
        "epoch": 0.8094492883225277,
        "step": 10862
    },
    {
        "loss": 2.5166,
        "grad_norm": 2.1770896911621094,
        "learning_rate": 0.00012278423682814338,
        "epoch": 0.8095238095238095,
        "step": 10863
    },
    {
        "loss": 2.3163,
        "grad_norm": 3.6661534309387207,
        "learning_rate": 0.00012271571627799107,
        "epoch": 0.8095983307250912,
        "step": 10864
    },
    {
        "loss": 1.6874,
        "grad_norm": 4.140713214874268,
        "learning_rate": 0.00012264718448054383,
        "epoch": 0.8096728519263731,
        "step": 10865
    },
    {
        "loss": 0.6788,
        "grad_norm": 3.8586432933807373,
        "learning_rate": 0.0001225786414697339,
        "epoch": 0.8097473731276548,
        "step": 10866
    },
    {
        "loss": 2.8891,
        "grad_norm": 3.5436654090881348,
        "learning_rate": 0.00012251008727949948,
        "epoch": 0.8098218943289366,
        "step": 10867
    },
    {
        "loss": 2.1913,
        "grad_norm": 3.457763433456421,
        "learning_rate": 0.00012244152194378357,
        "epoch": 0.8098964155302183,
        "step": 10868
    },
    {
        "loss": 2.4153,
        "grad_norm": 3.1947903633117676,
        "learning_rate": 0.00012237294549653544,
        "epoch": 0.8099709367315001,
        "step": 10869
    },
    {
        "loss": 2.6575,
        "grad_norm": 2.8150737285614014,
        "learning_rate": 0.0001223043579717093,
        "epoch": 0.8100454579327819,
        "step": 10870
    },
    {
        "loss": 1.5721,
        "grad_norm": 2.1542320251464844,
        "learning_rate": 0.0001222357594032653,
        "epoch": 0.8101199791340636,
        "step": 10871
    },
    {
        "loss": 2.1831,
        "grad_norm": 3.048518180847168,
        "learning_rate": 0.00012216714982516878,
        "epoch": 0.8101945003353455,
        "step": 10872
    },
    {
        "loss": 2.8299,
        "grad_norm": 2.729962110519409,
        "learning_rate": 0.0001220985292713904,
        "epoch": 0.8102690215366272,
        "step": 10873
    },
    {
        "loss": 2.5594,
        "grad_norm": 3.840768337249756,
        "learning_rate": 0.00012202989777590672,
        "epoch": 0.810343542737909,
        "step": 10874
    },
    {
        "loss": 2.3327,
        "grad_norm": 4.152149677276611,
        "learning_rate": 0.00012196125537269929,
        "epoch": 0.8104180639391907,
        "step": 10875
    },
    {
        "loss": 2.2676,
        "grad_norm": 3.175246477127075,
        "learning_rate": 0.00012189260209575524,
        "epoch": 0.8104925851404725,
        "step": 10876
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.485205888748169,
        "learning_rate": 0.00012182393797906695,
        "epoch": 0.8105671063417542,
        "step": 10877
    },
    {
        "loss": 2.8144,
        "grad_norm": 2.689035654067993,
        "learning_rate": 0.00012175526305663246,
        "epoch": 0.810641627543036,
        "step": 10878
    },
    {
        "loss": 1.8571,
        "grad_norm": 4.725839138031006,
        "learning_rate": 0.00012168657736245488,
        "epoch": 0.8107161487443177,
        "step": 10879
    },
    {
        "loss": 2.4612,
        "grad_norm": 2.5657002925872803,
        "learning_rate": 0.00012161788093054278,
        "epoch": 0.8107906699455996,
        "step": 10880
    },
    {
        "loss": 1.7855,
        "grad_norm": 4.4277567863464355,
        "learning_rate": 0.00012154917379490984,
        "epoch": 0.8108651911468813,
        "step": 10881
    },
    {
        "loss": 1.7263,
        "grad_norm": 2.855680227279663,
        "learning_rate": 0.00012148045598957547,
        "epoch": 0.8109397123481631,
        "step": 10882
    },
    {
        "loss": 2.3334,
        "grad_norm": 2.98341703414917,
        "learning_rate": 0.00012141172754856387,
        "epoch": 0.8110142335494448,
        "step": 10883
    },
    {
        "loss": 2.0986,
        "grad_norm": 3.2226381301879883,
        "learning_rate": 0.00012134298850590497,
        "epoch": 0.8110887547507266,
        "step": 10884
    },
    {
        "loss": 2.7895,
        "grad_norm": 2.4924416542053223,
        "learning_rate": 0.00012127423889563365,
        "epoch": 0.8111632759520083,
        "step": 10885
    },
    {
        "loss": 1.6154,
        "grad_norm": 4.333071708679199,
        "learning_rate": 0.0001212054787517899,
        "epoch": 0.8112377971532901,
        "step": 10886
    },
    {
        "loss": 2.2062,
        "grad_norm": 5.025325775146484,
        "learning_rate": 0.00012113670810841953,
        "epoch": 0.8113123183545718,
        "step": 10887
    },
    {
        "loss": 3.1203,
        "grad_norm": 3.8940556049346924,
        "learning_rate": 0.00012106792699957263,
        "epoch": 0.8113868395558537,
        "step": 10888
    },
    {
        "loss": 2.4022,
        "grad_norm": 2.975050449371338,
        "learning_rate": 0.00012099913545930536,
        "epoch": 0.8114613607571354,
        "step": 10889
    },
    {
        "loss": 2.3045,
        "grad_norm": 2.922212839126587,
        "learning_rate": 0.00012093033352167853,
        "epoch": 0.8115358819584172,
        "step": 10890
    },
    {
        "loss": 2.7095,
        "grad_norm": 3.7215054035186768,
        "learning_rate": 0.00012086152122075812,
        "epoch": 0.8116104031596989,
        "step": 10891
    },
    {
        "loss": 1.6926,
        "grad_norm": 3.2225241661071777,
        "learning_rate": 0.00012079269859061559,
        "epoch": 0.8116849243609807,
        "step": 10892
    },
    {
        "loss": 2.3735,
        "grad_norm": 3.5559287071228027,
        "learning_rate": 0.00012072386566532704,
        "epoch": 0.8117594455622624,
        "step": 10893
    },
    {
        "loss": 2.1149,
        "grad_norm": 3.9684853553771973,
        "learning_rate": 0.00012065502247897412,
        "epoch": 0.8118339667635442,
        "step": 10894
    },
    {
        "loss": 2.0086,
        "grad_norm": 2.9061551094055176,
        "learning_rate": 0.00012058616906564327,
        "epoch": 0.811908487964826,
        "step": 10895
    },
    {
        "loss": 2.7026,
        "grad_norm": 2.042302131652832,
        "learning_rate": 0.00012051730545942606,
        "epoch": 0.8119830091661078,
        "step": 10896
    },
    {
        "loss": 2.8706,
        "grad_norm": 2.266719102859497,
        "learning_rate": 0.00012044843169441892,
        "epoch": 0.8120575303673895,
        "step": 10897
    },
    {
        "loss": 2.3642,
        "grad_norm": 3.7751238346099854,
        "learning_rate": 0.00012037954780472387,
        "epoch": 0.8121320515686713,
        "step": 10898
    },
    {
        "loss": 2.0986,
        "grad_norm": 3.0270655155181885,
        "learning_rate": 0.00012031065382444737,
        "epoch": 0.812206572769953,
        "step": 10899
    },
    {
        "loss": 2.8799,
        "grad_norm": 2.4553115367889404,
        "learning_rate": 0.00012024174978770115,
        "epoch": 0.8122810939712348,
        "step": 10900
    },
    {
        "loss": 1.9839,
        "grad_norm": 2.031541109085083,
        "learning_rate": 0.00012017283572860168,
        "epoch": 0.8123556151725165,
        "step": 10901
    },
    {
        "loss": 2.6098,
        "grad_norm": 3.0439774990081787,
        "learning_rate": 0.00012010391168127083,
        "epoch": 0.8124301363737984,
        "step": 10902
    },
    {
        "loss": 2.7714,
        "grad_norm": 2.5136373043060303,
        "learning_rate": 0.00012003497767983502,
        "epoch": 0.8125046575750801,
        "step": 10903
    },
    {
        "loss": 2.1461,
        "grad_norm": 3.675018310546875,
        "learning_rate": 0.00011996603375842562,
        "epoch": 0.8125791787763619,
        "step": 10904
    },
    {
        "loss": 1.8336,
        "grad_norm": 3.837117910385132,
        "learning_rate": 0.00011989707995117927,
        "epoch": 0.8126536999776436,
        "step": 10905
    },
    {
        "loss": 2.6418,
        "grad_norm": 2.9986469745635986,
        "learning_rate": 0.00011982811629223701,
        "epoch": 0.8127282211789254,
        "step": 10906
    },
    {
        "loss": 2.098,
        "grad_norm": 3.003957748413086,
        "learning_rate": 0.00011975914281574538,
        "epoch": 0.8128027423802072,
        "step": 10907
    },
    {
        "loss": 2.3476,
        "grad_norm": 2.1022331714630127,
        "learning_rate": 0.00011969015955585487,
        "epoch": 0.8128772635814889,
        "step": 10908
    },
    {
        "loss": 2.4112,
        "grad_norm": 1.9780603647232056,
        "learning_rate": 0.00011962116654672179,
        "epoch": 0.8129517847827707,
        "step": 10909
    },
    {
        "loss": 1.9313,
        "grad_norm": 3.249993324279785,
        "learning_rate": 0.00011955216382250669,
        "epoch": 0.8130263059840525,
        "step": 10910
    },
    {
        "loss": 2.3959,
        "grad_norm": 3.6258721351623535,
        "learning_rate": 0.00011948315141737494,
        "epoch": 0.8131008271853343,
        "step": 10911
    },
    {
        "loss": 2.3074,
        "grad_norm": 3.7442054748535156,
        "learning_rate": 0.0001194141293654971,
        "epoch": 0.813175348386616,
        "step": 10912
    },
    {
        "loss": 2.3513,
        "grad_norm": 3.4249510765075684,
        "learning_rate": 0.00011934509770104809,
        "epoch": 0.8132498695878978,
        "step": 10913
    },
    {
        "loss": 1.9953,
        "grad_norm": 2.0332088470458984,
        "learning_rate": 0.00011927605645820767,
        "epoch": 0.8133243907891795,
        "step": 10914
    },
    {
        "loss": 2.6764,
        "grad_norm": 4.079377174377441,
        "learning_rate": 0.00011920700567116067,
        "epoch": 0.8133989119904613,
        "step": 10915
    },
    {
        "loss": 2.439,
        "grad_norm": 2.3613994121551514,
        "learning_rate": 0.00011913794537409623,
        "epoch": 0.813473433191743,
        "step": 10916
    },
    {
        "loss": 1.9288,
        "grad_norm": 3.3859426975250244,
        "learning_rate": 0.00011906887560120827,
        "epoch": 0.8135479543930249,
        "step": 10917
    },
    {
        "loss": 2.8454,
        "grad_norm": 2.5838096141815186,
        "learning_rate": 0.00011899979638669589,
        "epoch": 0.8136224755943066,
        "step": 10918
    },
    {
        "loss": 2.9403,
        "grad_norm": 1.6045199632644653,
        "learning_rate": 0.00011893070776476192,
        "epoch": 0.8136969967955884,
        "step": 10919
    },
    {
        "loss": 2.5207,
        "grad_norm": 2.2456681728363037,
        "learning_rate": 0.0001188616097696148,
        "epoch": 0.8137715179968701,
        "step": 10920
    },
    {
        "loss": 2.326,
        "grad_norm": 2.613194704055786,
        "learning_rate": 0.00011879250243546694,
        "epoch": 0.8138460391981519,
        "step": 10921
    },
    {
        "loss": 2.3674,
        "grad_norm": 3.248457670211792,
        "learning_rate": 0.00011872338579653595,
        "epoch": 0.8139205603994336,
        "step": 10922
    },
    {
        "loss": 2.3612,
        "grad_norm": 3.631873846054077,
        "learning_rate": 0.00011865425988704355,
        "epoch": 0.8139950816007154,
        "step": 10923
    },
    {
        "loss": 2.7628,
        "grad_norm": 3.186680316925049,
        "learning_rate": 0.00011858512474121615,
        "epoch": 0.8140696028019971,
        "step": 10924
    },
    {
        "loss": 1.8548,
        "grad_norm": 3.4941649436950684,
        "learning_rate": 0.00011851598039328507,
        "epoch": 0.814144124003279,
        "step": 10925
    },
    {
        "loss": 2.1367,
        "grad_norm": 4.2391533851623535,
        "learning_rate": 0.00011844682687748567,
        "epoch": 0.8142186452045607,
        "step": 10926
    },
    {
        "loss": 2.6496,
        "grad_norm": 2.7151551246643066,
        "learning_rate": 0.00011837766422805856,
        "epoch": 0.8142931664058425,
        "step": 10927
    },
    {
        "loss": 2.1543,
        "grad_norm": 2.988107204437256,
        "learning_rate": 0.00011830849247924787,
        "epoch": 0.8143676876071242,
        "step": 10928
    },
    {
        "loss": 2.6216,
        "grad_norm": 3.643054246902466,
        "learning_rate": 0.00011823931166530318,
        "epoch": 0.814442208808406,
        "step": 10929
    },
    {
        "loss": 1.8811,
        "grad_norm": 3.8744912147521973,
        "learning_rate": 0.00011817012182047809,
        "epoch": 0.8145167300096877,
        "step": 10930
    },
    {
        "loss": 2.1019,
        "grad_norm": 2.6649651527404785,
        "learning_rate": 0.00011810092297903058,
        "epoch": 0.8145912512109695,
        "step": 10931
    },
    {
        "loss": 2.1621,
        "grad_norm": 3.74690580368042,
        "learning_rate": 0.00011803171517522351,
        "epoch": 0.8146657724122512,
        "step": 10932
    },
    {
        "loss": 2.5355,
        "grad_norm": 1.6170926094055176,
        "learning_rate": 0.00011796249844332385,
        "epoch": 0.8147402936135331,
        "step": 10933
    },
    {
        "loss": 2.1169,
        "grad_norm": 4.168224334716797,
        "learning_rate": 0.00011789327281760296,
        "epoch": 0.8148148148148148,
        "step": 10934
    },
    {
        "loss": 1.8989,
        "grad_norm": 4.9320173263549805,
        "learning_rate": 0.00011782403833233689,
        "epoch": 0.8148893360160966,
        "step": 10935
    },
    {
        "loss": 2.413,
        "grad_norm": 1.486775279045105,
        "learning_rate": 0.00011775479502180582,
        "epoch": 0.8149638572173783,
        "step": 10936
    },
    {
        "loss": 2.5582,
        "grad_norm": 2.83909273147583,
        "learning_rate": 0.00011768554292029426,
        "epoch": 0.8150383784186601,
        "step": 10937
    },
    {
        "loss": 2.4739,
        "grad_norm": 1.7569528818130493,
        "learning_rate": 0.00011761628206209159,
        "epoch": 0.8151128996199418,
        "step": 10938
    },
    {
        "loss": 1.7959,
        "grad_norm": 5.158870220184326,
        "learning_rate": 0.00011754701248149056,
        "epoch": 0.8151874208212236,
        "step": 10939
    },
    {
        "loss": 2.4915,
        "grad_norm": 2.5156660079956055,
        "learning_rate": 0.00011747773421278923,
        "epoch": 0.8152619420225053,
        "step": 10940
    },
    {
        "loss": 2.2738,
        "grad_norm": 2.697791337966919,
        "learning_rate": 0.00011740844729028941,
        "epoch": 0.8153364632237872,
        "step": 10941
    },
    {
        "loss": 2.2358,
        "grad_norm": 2.695357084274292,
        "learning_rate": 0.00011733915174829719,
        "epoch": 0.815410984425069,
        "step": 10942
    },
    {
        "loss": 2.0625,
        "grad_norm": 5.072221279144287,
        "learning_rate": 0.00011726984762112332,
        "epoch": 0.8154855056263507,
        "step": 10943
    },
    {
        "loss": 1.0609,
        "grad_norm": 2.3002169132232666,
        "learning_rate": 0.00011720053494308225,
        "epoch": 0.8155600268276325,
        "step": 10944
    },
    {
        "loss": 2.4345,
        "grad_norm": 2.6327853202819824,
        "learning_rate": 0.00011713121374849331,
        "epoch": 0.8156345480289142,
        "step": 10945
    },
    {
        "loss": 2.5342,
        "grad_norm": 3.4449405670166016,
        "learning_rate": 0.00011706188407167947,
        "epoch": 0.815709069230196,
        "step": 10946
    },
    {
        "loss": 2.4016,
        "grad_norm": 2.076845407485962,
        "learning_rate": 0.00011699254594696816,
        "epoch": 0.8157835904314777,
        "step": 10947
    },
    {
        "loss": 1.9973,
        "grad_norm": 2.8098082542419434,
        "learning_rate": 0.0001169231994086908,
        "epoch": 0.8158581116327596,
        "step": 10948
    },
    {
        "loss": 2.3544,
        "grad_norm": 2.3923609256744385,
        "learning_rate": 0.00011685384449118346,
        "epoch": 0.8159326328340413,
        "step": 10949
    },
    {
        "loss": 3.3169,
        "grad_norm": 6.50550651550293,
        "learning_rate": 0.00011678448122878582,
        "epoch": 0.8160071540353231,
        "step": 10950
    },
    {
        "loss": 2.3738,
        "grad_norm": 3.3000802993774414,
        "learning_rate": 0.00011671510965584194,
        "epoch": 0.8160816752366048,
        "step": 10951
    },
    {
        "loss": 2.1298,
        "grad_norm": 4.515478134155273,
        "learning_rate": 0.00011664572980669983,
        "epoch": 0.8161561964378866,
        "step": 10952
    },
    {
        "loss": 1.5045,
        "grad_norm": 3.5852909088134766,
        "learning_rate": 0.00011657634171571199,
        "epoch": 0.8162307176391683,
        "step": 10953
    },
    {
        "loss": 2.6351,
        "grad_norm": 2.7250125408172607,
        "learning_rate": 0.00011650694541723446,
        "epoch": 0.8163052388404501,
        "step": 10954
    },
    {
        "loss": 3.143,
        "grad_norm": 3.233990430831909,
        "learning_rate": 0.00011643754094562786,
        "epoch": 0.8163797600417319,
        "step": 10955
    },
    {
        "loss": 2.5699,
        "grad_norm": 2.1120269298553467,
        "learning_rate": 0.00011636812833525656,
        "epoch": 0.8164542812430137,
        "step": 10956
    },
    {
        "loss": 1.2914,
        "grad_norm": 1.5625004768371582,
        "learning_rate": 0.0001162987076204888,
        "epoch": 0.8165288024442954,
        "step": 10957
    },
    {
        "loss": 1.8464,
        "grad_norm": 3.986060857772827,
        "learning_rate": 0.00011622927883569748,
        "epoch": 0.8166033236455772,
        "step": 10958
    },
    {
        "loss": 1.6813,
        "grad_norm": 2.746933698654175,
        "learning_rate": 0.00011615984201525854,
        "epoch": 0.8166778448468589,
        "step": 10959
    },
    {
        "loss": 2.4339,
        "grad_norm": 1.645805835723877,
        "learning_rate": 0.00011609039719355284,
        "epoch": 0.8167523660481407,
        "step": 10960
    },
    {
        "loss": 2.627,
        "grad_norm": 3.696424722671509,
        "learning_rate": 0.00011602094440496461,
        "epoch": 0.8168268872494224,
        "step": 10961
    },
    {
        "loss": 1.8041,
        "grad_norm": 3.698664426803589,
        "learning_rate": 0.00011595148368388209,
        "epoch": 0.8169014084507042,
        "step": 10962
    },
    {
        "loss": 2.7233,
        "grad_norm": 2.46882963180542,
        "learning_rate": 0.00011588201506469781,
        "epoch": 0.816975929651986,
        "step": 10963
    },
    {
        "loss": 2.2792,
        "grad_norm": 2.9896273612976074,
        "learning_rate": 0.00011581253858180771,
        "epoch": 0.8170504508532678,
        "step": 10964
    },
    {
        "loss": 2.5166,
        "grad_norm": 2.5755927562713623,
        "learning_rate": 0.00011574305426961214,
        "epoch": 0.8171249720545495,
        "step": 10965
    },
    {
        "loss": 1.9666,
        "grad_norm": 3.1992385387420654,
        "learning_rate": 0.0001156735621625149,
        "epoch": 0.8171994932558313,
        "step": 10966
    },
    {
        "loss": 1.7957,
        "grad_norm": 3.497326135635376,
        "learning_rate": 0.00011560406229492382,
        "epoch": 0.817274014457113,
        "step": 10967
    },
    {
        "loss": 2.5925,
        "grad_norm": 1.9224326610565186,
        "learning_rate": 0.00011553455470125045,
        "epoch": 0.8173485356583948,
        "step": 10968
    },
    {
        "loss": 2.6383,
        "grad_norm": 3.945117950439453,
        "learning_rate": 0.00011546503941591065,
        "epoch": 0.8174230568596765,
        "step": 10969
    },
    {
        "loss": 2.2934,
        "grad_norm": 4.239328861236572,
        "learning_rate": 0.00011539551647332316,
        "epoch": 0.8174975780609584,
        "step": 10970
    },
    {
        "loss": 2.0059,
        "grad_norm": 2.0602078437805176,
        "learning_rate": 0.00011532598590791153,
        "epoch": 0.8175720992622401,
        "step": 10971
    },
    {
        "loss": 2.0382,
        "grad_norm": 3.0797030925750732,
        "learning_rate": 0.00011525644775410229,
        "epoch": 0.8176466204635219,
        "step": 10972
    },
    {
        "loss": 1.9251,
        "grad_norm": 2.7326204776763916,
        "learning_rate": 0.00011518690204632634,
        "epoch": 0.8177211416648036,
        "step": 10973
    },
    {
        "loss": 2.7614,
        "grad_norm": 2.5992953777313232,
        "learning_rate": 0.0001151173488190179,
        "epoch": 0.8177956628660854,
        "step": 10974
    },
    {
        "loss": 1.3503,
        "grad_norm": 2.08038592338562,
        "learning_rate": 0.00011504778810661496,
        "epoch": 0.8178701840673671,
        "step": 10975
    },
    {
        "loss": 2.604,
        "grad_norm": 3.684471845626831,
        "learning_rate": 0.00011497821994355952,
        "epoch": 0.8179447052686489,
        "step": 10976
    },
    {
        "loss": 1.3348,
        "grad_norm": 3.82490611076355,
        "learning_rate": 0.00011490864436429683,
        "epoch": 0.8180192264699307,
        "step": 10977
    },
    {
        "loss": 2.5642,
        "grad_norm": 2.816255569458008,
        "learning_rate": 0.0001148390614032764,
        "epoch": 0.8180937476712125,
        "step": 10978
    },
    {
        "loss": 2.4435,
        "grad_norm": 3.375441551208496,
        "learning_rate": 0.00011476947109495054,
        "epoch": 0.8181682688724943,
        "step": 10979
    },
    {
        "loss": 1.5255,
        "grad_norm": Infinity,
        "learning_rate": 0.00011476947109495054,
        "epoch": 0.818242790073776,
        "step": 10980
    },
    {
        "loss": 1.672,
        "grad_norm": 2.5692389011383057,
        "learning_rate": 0.00011469987347377604,
        "epoch": 0.8183173112750578,
        "step": 10981
    },
    {
        "loss": 2.7811,
        "grad_norm": 2.5170652866363525,
        "learning_rate": 0.00011463026857421286,
        "epoch": 0.8183918324763395,
        "step": 10982
    },
    {
        "loss": 2.286,
        "grad_norm": 2.7967636585235596,
        "learning_rate": 0.00011456065643072453,
        "epoch": 0.8184663536776213,
        "step": 10983
    },
    {
        "loss": 2.6882,
        "grad_norm": 2.360013484954834,
        "learning_rate": 0.00011449103707777858,
        "epoch": 0.818540874878903,
        "step": 10984
    },
    {
        "loss": 1.9096,
        "grad_norm": 3.61824631690979,
        "learning_rate": 0.00011442141054984566,
        "epoch": 0.8186153960801849,
        "step": 10985
    },
    {
        "loss": 2.3502,
        "grad_norm": 3.609649896621704,
        "learning_rate": 0.00011435177688140002,
        "epoch": 0.8186899172814666,
        "step": 10986
    },
    {
        "loss": 2.8754,
        "grad_norm": 4.258726119995117,
        "learning_rate": 0.00011428213610691987,
        "epoch": 0.8187644384827484,
        "step": 10987
    },
    {
        "loss": 2.3415,
        "grad_norm": 2.7845852375030518,
        "learning_rate": 0.00011421248826088648,
        "epoch": 0.8188389596840301,
        "step": 10988
    },
    {
        "loss": 1.999,
        "grad_norm": 2.6545517444610596,
        "learning_rate": 0.00011414283337778465,
        "epoch": 0.8189134808853119,
        "step": 10989
    },
    {
        "loss": 2.5984,
        "grad_norm": 4.930144309997559,
        "learning_rate": 0.0001140731714921032,
        "epoch": 0.8189880020865936,
        "step": 10990
    },
    {
        "loss": 2.4779,
        "grad_norm": 3.806840181350708,
        "learning_rate": 0.00011400350263833346,
        "epoch": 0.8190625232878754,
        "step": 10991
    },
    {
        "loss": 2.137,
        "grad_norm": 2.4090921878814697,
        "learning_rate": 0.00011393382685097116,
        "epoch": 0.8191370444891571,
        "step": 10992
    },
    {
        "loss": 2.8275,
        "grad_norm": 2.4969310760498047,
        "learning_rate": 0.0001138641441645148,
        "epoch": 0.819211565690439,
        "step": 10993
    },
    {
        "loss": 2.121,
        "grad_norm": 3.2997677326202393,
        "learning_rate": 0.00011379445461346685,
        "epoch": 0.8192860868917207,
        "step": 10994
    },
    {
        "loss": 2.5825,
        "grad_norm": 2.661247491836548,
        "learning_rate": 0.00011372475823233278,
        "epoch": 0.8193606080930025,
        "step": 10995
    },
    {
        "loss": 2.131,
        "grad_norm": 3.22672176361084,
        "learning_rate": 0.00011365505505562131,
        "epoch": 0.8194351292942842,
        "step": 10996
    },
    {
        "loss": 2.2456,
        "grad_norm": 3.364680051803589,
        "learning_rate": 0.00011358534511784511,
        "epoch": 0.819509650495566,
        "step": 10997
    },
    {
        "loss": 1.8125,
        "grad_norm": 3.6543772220611572,
        "learning_rate": 0.00011351562845351973,
        "epoch": 0.8195841716968477,
        "step": 10998
    },
    {
        "loss": 2.4322,
        "grad_norm": 4.974654197692871,
        "learning_rate": 0.00011344590509716415,
        "epoch": 0.8196586928981295,
        "step": 10999
    },
    {
        "loss": 2.6534,
        "grad_norm": 2.147428035736084,
        "learning_rate": 0.00011337617508330054,
        "epoch": 0.8197332140994112,
        "step": 11000
    },
    {
        "loss": 2.1659,
        "grad_norm": 3.9527714252471924,
        "learning_rate": 0.00011330643844645479,
        "epoch": 0.8198077353006931,
        "step": 11001
    },
    {
        "loss": 2.5294,
        "grad_norm": 3.437878370285034,
        "learning_rate": 0.00011323669522115568,
        "epoch": 0.8198822565019748,
        "step": 11002
    },
    {
        "loss": 1.9366,
        "grad_norm": 3.857726812362671,
        "learning_rate": 0.00011316694544193533,
        "epoch": 0.8199567777032566,
        "step": 11003
    },
    {
        "loss": 2.1245,
        "grad_norm": 3.6778759956359863,
        "learning_rate": 0.00011309718914332903,
        "epoch": 0.8200312989045383,
        "step": 11004
    },
    {
        "loss": 2.1501,
        "grad_norm": 4.096264839172363,
        "learning_rate": 0.00011302742635987567,
        "epoch": 0.8201058201058201,
        "step": 11005
    },
    {
        "loss": 1.7585,
        "grad_norm": 7.579664707183838,
        "learning_rate": 0.00011295765712611681,
        "epoch": 0.8201803413071018,
        "step": 11006
    },
    {
        "loss": 0.9604,
        "grad_norm": 2.884460210800171,
        "learning_rate": 0.00011288788147659779,
        "epoch": 0.8202548625083836,
        "step": 11007
    },
    {
        "loss": 2.6325,
        "grad_norm": 2.9178435802459717,
        "learning_rate": 0.00011281809944586661,
        "epoch": 0.8203293837096653,
        "step": 11008
    },
    {
        "loss": 2.7754,
        "grad_norm": 3.113734483718872,
        "learning_rate": 0.00011274831106847457,
        "epoch": 0.8204039049109472,
        "step": 11009
    },
    {
        "loss": 2.1956,
        "grad_norm": 3.552265167236328,
        "learning_rate": 0.00011267851637897657,
        "epoch": 0.8204784261122289,
        "step": 11010
    },
    {
        "loss": 4.0439,
        "grad_norm": 6.72565221786499,
        "learning_rate": 0.0001126087154119297,
        "epoch": 0.8205529473135107,
        "step": 11011
    },
    {
        "loss": 2.591,
        "grad_norm": 2.392529010772705,
        "learning_rate": 0.00011253890820189508,
        "epoch": 0.8206274685147925,
        "step": 11012
    },
    {
        "loss": 1.8328,
        "grad_norm": 3.3699545860290527,
        "learning_rate": 0.00011246909478343645,
        "epoch": 0.8207019897160742,
        "step": 11013
    },
    {
        "loss": 2.4262,
        "grad_norm": 4.177713394165039,
        "learning_rate": 0.00011239927519112059,
        "epoch": 0.820776510917356,
        "step": 11014
    },
    {
        "loss": 2.8524,
        "grad_norm": 2.4694342613220215,
        "learning_rate": 0.00011232944945951771,
        "epoch": 0.8208510321186377,
        "step": 11015
    },
    {
        "loss": 2.3828,
        "grad_norm": 3.544076919555664,
        "learning_rate": 0.00011225961762320055,
        "epoch": 0.8209255533199196,
        "step": 11016
    },
    {
        "loss": 2.6192,
        "grad_norm": 2.610419988632202,
        "learning_rate": 0.0001121897797167454,
        "epoch": 0.8210000745212013,
        "step": 11017
    },
    {
        "loss": 2.3098,
        "grad_norm": 2.2187304496765137,
        "learning_rate": 0.00011211993577473121,
        "epoch": 0.8210745957224831,
        "step": 11018
    },
    {
        "loss": 1.5262,
        "grad_norm": 3.3231749534606934,
        "learning_rate": 0.00011205008583173995,
        "epoch": 0.8211491169237648,
        "step": 11019
    },
    {
        "loss": 2.6223,
        "grad_norm": 4.816311359405518,
        "learning_rate": 0.00011198022992235649,
        "epoch": 0.8212236381250466,
        "step": 11020
    },
    {
        "loss": 1.314,
        "grad_norm": 3.783905029296875,
        "learning_rate": 0.00011191036808116904,
        "epoch": 0.8212981593263283,
        "step": 11021
    },
    {
        "loss": 2.5818,
        "grad_norm": 3.57645583152771,
        "learning_rate": 0.00011184050034276838,
        "epoch": 0.8213726805276101,
        "step": 11022
    },
    {
        "loss": 2.772,
        "grad_norm": 2.259113073348999,
        "learning_rate": 0.0001117706267417483,
        "epoch": 0.8214472017288919,
        "step": 11023
    },
    {
        "loss": 2.1811,
        "grad_norm": 3.0505025386810303,
        "learning_rate": 0.00011170074731270536,
        "epoch": 0.8215217229301737,
        "step": 11024
    },
    {
        "loss": 2.755,
        "grad_norm": 2.0550646781921387,
        "learning_rate": 0.00011163086209023945,
        "epoch": 0.8215962441314554,
        "step": 11025
    },
    {
        "loss": 2.3603,
        "grad_norm": 1.9315131902694702,
        "learning_rate": 0.00011156097110895274,
        "epoch": 0.8216707653327372,
        "step": 11026
    },
    {
        "loss": 2.7181,
        "grad_norm": 2.807720899581909,
        "learning_rate": 0.00011149107440345081,
        "epoch": 0.8217452865340189,
        "step": 11027
    },
    {
        "loss": 2.3272,
        "grad_norm": 2.806812047958374,
        "learning_rate": 0.00011142117200834168,
        "epoch": 0.8218198077353007,
        "step": 11028
    },
    {
        "loss": 2.7027,
        "grad_norm": 2.516571283340454,
        "learning_rate": 0.00011135126395823618,
        "epoch": 0.8218943289365824,
        "step": 11029
    },
    {
        "loss": 2.6996,
        "grad_norm": 2.1310064792633057,
        "learning_rate": 0.00011128135028774851,
        "epoch": 0.8219688501378642,
        "step": 11030
    },
    {
        "loss": 2.014,
        "grad_norm": 3.163923501968384,
        "learning_rate": 0.00011121143103149462,
        "epoch": 0.822043371339146,
        "step": 11031
    },
    {
        "loss": 2.0847,
        "grad_norm": 3.7125539779663086,
        "learning_rate": 0.00011114150622409427,
        "epoch": 0.8221178925404278,
        "step": 11032
    },
    {
        "loss": 1.7486,
        "grad_norm": 3.2633025646209717,
        "learning_rate": 0.00011107157590016938,
        "epoch": 0.8221924137417095,
        "step": 11033
    },
    {
        "loss": 1.912,
        "grad_norm": 3.754207134246826,
        "learning_rate": 0.0001110016400943446,
        "epoch": 0.8222669349429913,
        "step": 11034
    },
    {
        "loss": 2.5797,
        "grad_norm": 2.6452550888061523,
        "learning_rate": 0.0001109316988412477,
        "epoch": 0.822341456144273,
        "step": 11035
    },
    {
        "loss": 2.4831,
        "grad_norm": 1.6718660593032837,
        "learning_rate": 0.00011086175217550878,
        "epoch": 0.8224159773455548,
        "step": 11036
    },
    {
        "loss": 1.9423,
        "grad_norm": 3.1941826343536377,
        "learning_rate": 0.00011079180013176053,
        "epoch": 0.8224904985468365,
        "step": 11037
    },
    {
        "loss": 2.5225,
        "grad_norm": 2.757225513458252,
        "learning_rate": 0.00011072184274463884,
        "epoch": 0.8225650197481184,
        "step": 11038
    },
    {
        "loss": 2.1859,
        "grad_norm": 4.494661331176758,
        "learning_rate": 0.00011065188004878171,
        "epoch": 0.8226395409494001,
        "step": 11039
    },
    {
        "loss": 2.6411,
        "grad_norm": 3.554039716720581,
        "learning_rate": 0.00011058191207882987,
        "epoch": 0.8227140621506819,
        "step": 11040
    },
    {
        "loss": 2.3746,
        "grad_norm": 2.5684115886688232,
        "learning_rate": 0.00011051193886942712,
        "epoch": 0.8227885833519636,
        "step": 11041
    },
    {
        "loss": 2.222,
        "grad_norm": 3.3900794982910156,
        "learning_rate": 0.00011044196045521897,
        "epoch": 0.8228631045532454,
        "step": 11042
    },
    {
        "loss": 1.2741,
        "grad_norm": 2.9790687561035156,
        "learning_rate": 0.00011037197687085444,
        "epoch": 0.8229376257545271,
        "step": 11043
    },
    {
        "loss": 1.4418,
        "grad_norm": 3.8328280448913574,
        "learning_rate": 0.00011030198815098438,
        "epoch": 0.8230121469558089,
        "step": 11044
    },
    {
        "loss": 2.3201,
        "grad_norm": 3.309936761856079,
        "learning_rate": 0.00011023199433026283,
        "epoch": 0.8230866681570906,
        "step": 11045
    },
    {
        "loss": 1.9833,
        "grad_norm": 4.4409918785095215,
        "learning_rate": 0.00011016199544334584,
        "epoch": 0.8231611893583725,
        "step": 11046
    },
    {
        "loss": 2.8354,
        "grad_norm": 4.0957255363464355,
        "learning_rate": 0.00011009199152489203,
        "epoch": 0.8232357105596543,
        "step": 11047
    },
    {
        "loss": 2.1881,
        "grad_norm": 2.4251580238342285,
        "learning_rate": 0.00011002198260956291,
        "epoch": 0.823310231760936,
        "step": 11048
    },
    {
        "loss": 2.4574,
        "grad_norm": 3.125504970550537,
        "learning_rate": 0.00010995196873202189,
        "epoch": 0.8233847529622178,
        "step": 11049
    },
    {
        "loss": 2.8801,
        "grad_norm": 4.20826530456543,
        "learning_rate": 0.00010988194992693555,
        "epoch": 0.8234592741634995,
        "step": 11050
    },
    {
        "loss": 2.8364,
        "grad_norm": 3.184447765350342,
        "learning_rate": 0.00010981192622897199,
        "epoch": 0.8235337953647813,
        "step": 11051
    },
    {
        "loss": 2.2307,
        "grad_norm": 2.8729636669158936,
        "learning_rate": 0.00010974189767280258,
        "epoch": 0.823608316566063,
        "step": 11052
    },
    {
        "loss": 2.0413,
        "grad_norm": 4.112988471984863,
        "learning_rate": 0.00010967186429310066,
        "epoch": 0.8236828377673449,
        "step": 11053
    },
    {
        "loss": 1.7975,
        "grad_norm": 2.8343231678009033,
        "learning_rate": 0.00010960182612454191,
        "epoch": 0.8237573589686266,
        "step": 11054
    },
    {
        "loss": 2.1148,
        "grad_norm": 2.678537368774414,
        "learning_rate": 0.00010953178320180475,
        "epoch": 0.8238318801699084,
        "step": 11055
    },
    {
        "loss": 1.537,
        "grad_norm": 1.7889554500579834,
        "learning_rate": 0.00010946173555956966,
        "epoch": 0.8239064013711901,
        "step": 11056
    },
    {
        "loss": 2.744,
        "grad_norm": 3.4128708839416504,
        "learning_rate": 0.00010939168323251932,
        "epoch": 0.8239809225724719,
        "step": 11057
    },
    {
        "loss": 1.5256,
        "grad_norm": 3.031508207321167,
        "learning_rate": 0.00010932162625533923,
        "epoch": 0.8240554437737536,
        "step": 11058
    },
    {
        "loss": 1.8815,
        "grad_norm": 3.133028984069824,
        "learning_rate": 0.00010925156466271677,
        "epoch": 0.8241299649750354,
        "step": 11059
    },
    {
        "loss": 1.7551,
        "grad_norm": 1.983414649963379,
        "learning_rate": 0.00010918149848934159,
        "epoch": 0.8242044861763171,
        "step": 11060
    },
    {
        "loss": 1.9626,
        "grad_norm": 3.3127427101135254,
        "learning_rate": 0.0001091114277699061,
        "epoch": 0.824279007377599,
        "step": 11061
    },
    {
        "loss": 2.3145,
        "grad_norm": 3.1375253200531006,
        "learning_rate": 0.00010904135253910413,
        "epoch": 0.8243535285788807,
        "step": 11062
    },
    {
        "loss": 2.5399,
        "grad_norm": 2.5069727897644043,
        "learning_rate": 0.00010897127283163262,
        "epoch": 0.8244280497801625,
        "step": 11063
    },
    {
        "loss": 2.9588,
        "grad_norm": 2.7899906635284424,
        "learning_rate": 0.00010890118868219017,
        "epoch": 0.8245025709814442,
        "step": 11064
    },
    {
        "loss": 2.811,
        "grad_norm": 4.0266432762146,
        "learning_rate": 0.00010883110012547759,
        "epoch": 0.824577092182726,
        "step": 11065
    },
    {
        "loss": 1.6566,
        "grad_norm": 3.4124414920806885,
        "learning_rate": 0.00010876100719619833,
        "epoch": 0.8246516133840077,
        "step": 11066
    },
    {
        "loss": 2.1364,
        "grad_norm": 2.4903767108917236,
        "learning_rate": 0.0001086909099290574,
        "epoch": 0.8247261345852895,
        "step": 11067
    },
    {
        "loss": 1.2762,
        "grad_norm": 2.854067087173462,
        "learning_rate": 0.00010862080835876251,
        "epoch": 0.8248006557865712,
        "step": 11068
    },
    {
        "loss": 2.6992,
        "grad_norm": 2.885803461074829,
        "learning_rate": 0.00010855070252002315,
        "epoch": 0.8248751769878531,
        "step": 11069
    },
    {
        "loss": 1.8752,
        "grad_norm": 3.8495004177093506,
        "learning_rate": 0.000108480592447551,
        "epoch": 0.8249496981891348,
        "step": 11070
    },
    {
        "loss": 2.3673,
        "grad_norm": 3.406888008117676,
        "learning_rate": 0.00010841047817605968,
        "epoch": 0.8250242193904166,
        "step": 11071
    },
    {
        "loss": 2.4611,
        "grad_norm": 3.6187667846679688,
        "learning_rate": 0.00010834035974026535,
        "epoch": 0.8250987405916983,
        "step": 11072
    },
    {
        "loss": 1.6533,
        "grad_norm": 3.6201765537261963,
        "learning_rate": 0.00010827023717488584,
        "epoch": 0.8251732617929801,
        "step": 11073
    },
    {
        "loss": 1.8147,
        "grad_norm": 4.903271198272705,
        "learning_rate": 0.00010820011051464111,
        "epoch": 0.8252477829942618,
        "step": 11074
    },
    {
        "loss": 2.1988,
        "grad_norm": 2.9681506156921387,
        "learning_rate": 0.00010812997979425301,
        "epoch": 0.8253223041955436,
        "step": 11075
    },
    {
        "loss": 2.1689,
        "grad_norm": 4.075324058532715,
        "learning_rate": 0.00010805984504844587,
        "epoch": 0.8253968253968254,
        "step": 11076
    },
    {
        "loss": 1.6037,
        "grad_norm": 3.8729708194732666,
        "learning_rate": 0.00010798970631194536,
        "epoch": 0.8254713465981072,
        "step": 11077
    },
    {
        "loss": 1.9996,
        "grad_norm": 3.5257701873779297,
        "learning_rate": 0.00010791956361947979,
        "epoch": 0.8255458677993889,
        "step": 11078
    },
    {
        "loss": 2.2703,
        "grad_norm": 4.218458652496338,
        "learning_rate": 0.00010784941700577899,
        "epoch": 0.8256203890006707,
        "step": 11079
    },
    {
        "loss": 2.7452,
        "grad_norm": 3.055137872695923,
        "learning_rate": 0.00010777926650557463,
        "epoch": 0.8256949102019524,
        "step": 11080
    },
    {
        "loss": 1.3599,
        "grad_norm": 3.0064022541046143,
        "learning_rate": 0.000107709112153601,
        "epoch": 0.8257694314032342,
        "step": 11081
    },
    {
        "loss": 2.3158,
        "grad_norm": 4.248334884643555,
        "learning_rate": 0.00010763895398459327,
        "epoch": 0.8258439526045159,
        "step": 11082
    },
    {
        "loss": 1.9539,
        "grad_norm": 4.0529890060424805,
        "learning_rate": 0.00010756879203328937,
        "epoch": 0.8259184738057977,
        "step": 11083
    },
    {
        "loss": 1.9065,
        "grad_norm": 4.43819522857666,
        "learning_rate": 0.00010749862633442876,
        "epoch": 0.8259929950070796,
        "step": 11084
    },
    {
        "loss": 2.1366,
        "grad_norm": 2.727982997894287,
        "learning_rate": 0.00010742845692275254,
        "epoch": 0.8260675162083613,
        "step": 11085
    },
    {
        "loss": 2.0739,
        "grad_norm": 3.7979280948638916,
        "learning_rate": 0.00010735828383300423,
        "epoch": 0.8261420374096431,
        "step": 11086
    },
    {
        "loss": 2.1378,
        "grad_norm": 2.7621214389801025,
        "learning_rate": 0.00010728810709992849,
        "epoch": 0.8262165586109248,
        "step": 11087
    },
    {
        "loss": 2.5073,
        "grad_norm": 3.3397254943847656,
        "learning_rate": 0.00010721792675827245,
        "epoch": 0.8262910798122066,
        "step": 11088
    },
    {
        "loss": 1.6403,
        "grad_norm": 1.7704421281814575,
        "learning_rate": 0.0001071477428427845,
        "epoch": 0.8263656010134883,
        "step": 11089
    },
    {
        "loss": 1.8956,
        "grad_norm": 2.7945048809051514,
        "learning_rate": 0.00010707755538821506,
        "epoch": 0.8264401222147701,
        "step": 11090
    },
    {
        "loss": 2.4841,
        "grad_norm": 4.141993999481201,
        "learning_rate": 0.00010700736442931604,
        "epoch": 0.8265146434160519,
        "step": 11091
    },
    {
        "loss": 1.8641,
        "grad_norm": 3.2974584102630615,
        "learning_rate": 0.00010693717000084171,
        "epoch": 0.8265891646173337,
        "step": 11092
    },
    {
        "loss": 2.6451,
        "grad_norm": 3.0087532997131348,
        "learning_rate": 0.00010686697213754713,
        "epoch": 0.8266636858186154,
        "step": 11093
    },
    {
        "loss": 1.7513,
        "grad_norm": 3.8157989978790283,
        "learning_rate": 0.00010679677087418989,
        "epoch": 0.8267382070198972,
        "step": 11094
    },
    {
        "loss": 1.8666,
        "grad_norm": 3.3781630992889404,
        "learning_rate": 0.00010672656624552873,
        "epoch": 0.8268127282211789,
        "step": 11095
    },
    {
        "loss": 2.328,
        "grad_norm": 2.2152278423309326,
        "learning_rate": 0.00010665635828632448,
        "epoch": 0.8268872494224607,
        "step": 11096
    },
    {
        "loss": 2.501,
        "grad_norm": 2.8923604488372803,
        "learning_rate": 0.0001065861470313393,
        "epoch": 0.8269617706237424,
        "step": 11097
    },
    {
        "loss": 1.6599,
        "grad_norm": 2.064011573791504,
        "learning_rate": 0.00010651593251533695,
        "epoch": 0.8270362918250243,
        "step": 11098
    },
    {
        "loss": 1.7501,
        "grad_norm": 4.748624801635742,
        "learning_rate": 0.00010644571477308316,
        "epoch": 0.827110813026306,
        "step": 11099
    },
    {
        "loss": 2.5677,
        "grad_norm": 3.1874778270721436,
        "learning_rate": 0.00010637549383934482,
        "epoch": 0.8271853342275878,
        "step": 11100
    },
    {
        "loss": 2.2113,
        "grad_norm": 3.6493117809295654,
        "learning_rate": 0.000106305269748891,
        "epoch": 0.8272598554288695,
        "step": 11101
    },
    {
        "loss": 2.1363,
        "grad_norm": 3.2420482635498047,
        "learning_rate": 0.00010623504253649137,
        "epoch": 0.8273343766301513,
        "step": 11102
    },
    {
        "loss": 2.5107,
        "grad_norm": 2.4161338806152344,
        "learning_rate": 0.00010616481223691817,
        "epoch": 0.827408897831433,
        "step": 11103
    },
    {
        "loss": 2.1431,
        "grad_norm": 2.842869520187378,
        "learning_rate": 0.00010609457888494458,
        "epoch": 0.8274834190327148,
        "step": 11104
    },
    {
        "loss": 2.0637,
        "grad_norm": 3.0458242893218994,
        "learning_rate": 0.00010602434251534528,
        "epoch": 0.8275579402339965,
        "step": 11105
    },
    {
        "loss": 1.4869,
        "grad_norm": 2.7769579887390137,
        "learning_rate": 0.00010595410316289687,
        "epoch": 0.8276324614352784,
        "step": 11106
    },
    {
        "loss": 2.4161,
        "grad_norm": 2.3522162437438965,
        "learning_rate": 0.00010588386086237703,
        "epoch": 0.8277069826365601,
        "step": 11107
    },
    {
        "loss": 1.7882,
        "grad_norm": 3.3460378646850586,
        "learning_rate": 0.00010581361564856487,
        "epoch": 0.8277815038378419,
        "step": 11108
    },
    {
        "loss": 1.7333,
        "grad_norm": 3.7002205848693848,
        "learning_rate": 0.00010574336755624142,
        "epoch": 0.8278560250391236,
        "step": 11109
    },
    {
        "loss": 2.6549,
        "grad_norm": 2.397240400314331,
        "learning_rate": 0.00010567311662018859,
        "epoch": 0.8279305462404054,
        "step": 11110
    },
    {
        "loss": 0.8538,
        "grad_norm": 3.786452054977417,
        "learning_rate": 0.00010560286287518983,
        "epoch": 0.8280050674416871,
        "step": 11111
    },
    {
        "loss": 1.426,
        "grad_norm": 3.290647029876709,
        "learning_rate": 0.00010553260635603051,
        "epoch": 0.8280795886429689,
        "step": 11112
    },
    {
        "loss": 2.4121,
        "grad_norm": 2.10337495803833,
        "learning_rate": 0.00010546234709749632,
        "epoch": 0.8281541098442506,
        "step": 11113
    },
    {
        "loss": 1.2827,
        "grad_norm": 5.187317371368408,
        "learning_rate": 0.00010539208513437538,
        "epoch": 0.8282286310455325,
        "step": 11114
    },
    {
        "loss": 2.026,
        "grad_norm": 1.7207345962524414,
        "learning_rate": 0.00010532182050145639,
        "epoch": 0.8283031522468142,
        "step": 11115
    },
    {
        "loss": 2.6694,
        "grad_norm": 2.8204238414764404,
        "learning_rate": 0.00010525155323352994,
        "epoch": 0.828377673448096,
        "step": 11116
    },
    {
        "loss": 2.5881,
        "grad_norm": 2.8601722717285156,
        "learning_rate": 0.00010518128336538753,
        "epoch": 0.8284521946493777,
        "step": 11117
    },
    {
        "loss": 1.6966,
        "grad_norm": 3.010718822479248,
        "learning_rate": 0.00010511101093182194,
        "epoch": 0.8285267158506595,
        "step": 11118
    },
    {
        "loss": 2.1021,
        "grad_norm": 3.5292084217071533,
        "learning_rate": 0.0001050407359676276,
        "epoch": 0.8286012370519413,
        "step": 11119
    },
    {
        "loss": 1.9519,
        "grad_norm": 3.8193185329437256,
        "learning_rate": 0.00010497045850759972,
        "epoch": 0.828675758253223,
        "step": 11120
    },
    {
        "loss": 2.1172,
        "grad_norm": 3.503810405731201,
        "learning_rate": 0.00010490017858653536,
        "epoch": 0.8287502794545049,
        "step": 11121
    },
    {
        "loss": 2.305,
        "grad_norm": 9.326519966125488,
        "learning_rate": 0.00010482989623923185,
        "epoch": 0.8288248006557866,
        "step": 11122
    },
    {
        "loss": 2.4926,
        "grad_norm": 2.0272061824798584,
        "learning_rate": 0.00010475961150048868,
        "epoch": 0.8288993218570684,
        "step": 11123
    },
    {
        "loss": 2.5449,
        "grad_norm": 2.607785940170288,
        "learning_rate": 0.00010468932440510604,
        "epoch": 0.8289738430583501,
        "step": 11124
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.405869722366333,
        "learning_rate": 0.00010461903498788532,
        "epoch": 0.8290483642596319,
        "step": 11125
    },
    {
        "loss": 2.5777,
        "grad_norm": 3.487295627593994,
        "learning_rate": 0.000104548743283629,
        "epoch": 0.8291228854609136,
        "step": 11126
    },
    {
        "loss": 2.7401,
        "grad_norm": 2.498789072036743,
        "learning_rate": 0.00010447844932714108,
        "epoch": 0.8291974066621954,
        "step": 11127
    },
    {
        "loss": 2.8115,
        "grad_norm": 3.079017162322998,
        "learning_rate": 0.00010440815315322612,
        "epoch": 0.8292719278634771,
        "step": 11128
    },
    {
        "loss": 2.64,
        "grad_norm": 2.641883134841919,
        "learning_rate": 0.00010433785479669034,
        "epoch": 0.829346449064759,
        "step": 11129
    },
    {
        "loss": 2.4327,
        "grad_norm": 3.1746273040771484,
        "learning_rate": 0.00010426755429234058,
        "epoch": 0.8294209702660407,
        "step": 11130
    },
    {
        "loss": 1.75,
        "grad_norm": 2.53705096244812,
        "learning_rate": 0.00010419725167498484,
        "epoch": 0.8294954914673225,
        "step": 11131
    },
    {
        "loss": 2.7272,
        "grad_norm": 3.364053964614868,
        "learning_rate": 0.00010412694697943264,
        "epoch": 0.8295700126686042,
        "step": 11132
    },
    {
        "loss": 2.3906,
        "grad_norm": 3.0407726764678955,
        "learning_rate": 0.00010405664024049358,
        "epoch": 0.829644533869886,
        "step": 11133
    },
    {
        "loss": 1.7637,
        "grad_norm": 4.074906349182129,
        "learning_rate": 0.00010398633149297923,
        "epoch": 0.8297190550711677,
        "step": 11134
    },
    {
        "loss": 3.1566,
        "grad_norm": 2.328997850418091,
        "learning_rate": 0.00010391602077170166,
        "epoch": 0.8297935762724495,
        "step": 11135
    },
    {
        "loss": 2.7372,
        "grad_norm": 3.285005807876587,
        "learning_rate": 0.00010384570811147383,
        "epoch": 0.8298680974737312,
        "step": 11136
    },
    {
        "loss": 2.3404,
        "grad_norm": 3.244783878326416,
        "learning_rate": 0.00010377539354711012,
        "epoch": 0.8299426186750131,
        "step": 11137
    },
    {
        "loss": 2.0587,
        "grad_norm": 3.2595906257629395,
        "learning_rate": 0.0001037050771134253,
        "epoch": 0.8300171398762948,
        "step": 11138
    },
    {
        "loss": 2.4948,
        "grad_norm": 3.3073575496673584,
        "learning_rate": 0.00010363475884523566,
        "epoch": 0.8300916610775766,
        "step": 11139
    },
    {
        "loss": 2.8296,
        "grad_norm": 1.3066625595092773,
        "learning_rate": 0.00010356443877735786,
        "epoch": 0.8301661822788583,
        "step": 11140
    },
    {
        "loss": 2.0187,
        "grad_norm": 3.434699058532715,
        "learning_rate": 0.00010349411694460971,
        "epoch": 0.8302407034801401,
        "step": 11141
    },
    {
        "loss": 2.4875,
        "grad_norm": 3.178333044052124,
        "learning_rate": 0.00010342379338180971,
        "epoch": 0.8303152246814218,
        "step": 11142
    },
    {
        "loss": 2.0743,
        "grad_norm": 3.261385917663574,
        "learning_rate": 0.00010335346812377768,
        "epoch": 0.8303897458827036,
        "step": 11143
    },
    {
        "loss": 2.3694,
        "grad_norm": 5.9353837966918945,
        "learning_rate": 0.00010328314120533373,
        "epoch": 0.8304642670839854,
        "step": 11144
    },
    {
        "loss": 2.3556,
        "grad_norm": 3.4243528842926025,
        "learning_rate": 0.0001032128126612991,
        "epoch": 0.8305387882852672,
        "step": 11145
    },
    {
        "loss": 2.6888,
        "grad_norm": 3.568056583404541,
        "learning_rate": 0.00010314248252649555,
        "epoch": 0.8306133094865489,
        "step": 11146
    },
    {
        "loss": 2.718,
        "grad_norm": 2.3847973346710205,
        "learning_rate": 0.00010307215083574617,
        "epoch": 0.8306878306878307,
        "step": 11147
    },
    {
        "loss": 2.3834,
        "grad_norm": 3.329617500305176,
        "learning_rate": 0.00010300181762387418,
        "epoch": 0.8307623518891124,
        "step": 11148
    },
    {
        "loss": 2.479,
        "grad_norm": 3.2171425819396973,
        "learning_rate": 0.00010293148292570411,
        "epoch": 0.8308368730903942,
        "step": 11149
    },
    {
        "loss": 2.484,
        "grad_norm": 3.2694034576416016,
        "learning_rate": 0.00010286114677606089,
        "epoch": 0.8309113942916759,
        "step": 11150
    },
    {
        "loss": 2.6945,
        "grad_norm": 2.241577386856079,
        "learning_rate": 0.00010279080920977004,
        "epoch": 0.8309859154929577,
        "step": 11151
    },
    {
        "loss": 2.7658,
        "grad_norm": 3.0032379627227783,
        "learning_rate": 0.00010272047026165845,
        "epoch": 0.8310604366942395,
        "step": 11152
    },
    {
        "loss": 2.6028,
        "grad_norm": 3.370351552963257,
        "learning_rate": 0.0001026501299665527,
        "epoch": 0.8311349578955213,
        "step": 11153
    },
    {
        "loss": 2.7524,
        "grad_norm": 3.013385057449341,
        "learning_rate": 0.00010257978835928097,
        "epoch": 0.8312094790968031,
        "step": 11154
    },
    {
        "loss": 1.6765,
        "grad_norm": 3.9081060886383057,
        "learning_rate": 0.00010250944547467152,
        "epoch": 0.8312840002980848,
        "step": 11155
    },
    {
        "loss": 2.3832,
        "grad_norm": 1.9376897811889648,
        "learning_rate": 0.00010243910134755333,
        "epoch": 0.8313585214993666,
        "step": 11156
    },
    {
        "loss": 1.8082,
        "grad_norm": 3.0868163108825684,
        "learning_rate": 0.00010236875601275632,
        "epoch": 0.8314330427006483,
        "step": 11157
    },
    {
        "loss": 1.94,
        "grad_norm": 3.0102598667144775,
        "learning_rate": 0.00010229840950511068,
        "epoch": 0.8315075639019301,
        "step": 11158
    },
    {
        "loss": 2.1529,
        "grad_norm": 3.0705389976501465,
        "learning_rate": 0.00010222806185944706,
        "epoch": 0.8315820851032119,
        "step": 11159
    },
    {
        "loss": 2.5548,
        "grad_norm": 2.182065725326538,
        "learning_rate": 0.00010215771311059723,
        "epoch": 0.8316566063044937,
        "step": 11160
    },
    {
        "loss": 1.9511,
        "grad_norm": 3.174936056137085,
        "learning_rate": 0.00010208736329339297,
        "epoch": 0.8317311275057754,
        "step": 11161
    },
    {
        "loss": 1.6767,
        "grad_norm": 2.2692599296569824,
        "learning_rate": 0.00010201701244266666,
        "epoch": 0.8318056487070572,
        "step": 11162
    },
    {
        "loss": 2.7889,
        "grad_norm": 3.2877132892608643,
        "learning_rate": 0.0001019466605932517,
        "epoch": 0.8318801699083389,
        "step": 11163
    },
    {
        "loss": 1.8431,
        "grad_norm": 3.6365838050842285,
        "learning_rate": 0.00010187630777998109,
        "epoch": 0.8319546911096207,
        "step": 11164
    },
    {
        "loss": 1.6265,
        "grad_norm": 2.277371883392334,
        "learning_rate": 0.0001018059540376892,
        "epoch": 0.8320292123109024,
        "step": 11165
    },
    {
        "loss": 2.4427,
        "grad_norm": 3.2459754943847656,
        "learning_rate": 0.00010173559940121019,
        "epoch": 0.8321037335121843,
        "step": 11166
    },
    {
        "loss": 2.3121,
        "grad_norm": 3.4886229038238525,
        "learning_rate": 0.00010166524390537922,
        "epoch": 0.832178254713466,
        "step": 11167
    },
    {
        "loss": 1.7863,
        "grad_norm": 3.480842113494873,
        "learning_rate": 0.00010159488758503144,
        "epoch": 0.8322527759147478,
        "step": 11168
    },
    {
        "loss": 2.6225,
        "grad_norm": 3.6363258361816406,
        "learning_rate": 0.00010152453047500248,
        "epoch": 0.8323272971160295,
        "step": 11169
    },
    {
        "loss": 1.9692,
        "grad_norm": 3.491551399230957,
        "learning_rate": 0.00010145417261012873,
        "epoch": 0.8324018183173113,
        "step": 11170
    },
    {
        "loss": 2.0929,
        "grad_norm": 1.8299622535705566,
        "learning_rate": 0.00010138381402524633,
        "epoch": 0.832476339518593,
        "step": 11171
    },
    {
        "loss": 1.9658,
        "grad_norm": 3.222933530807495,
        "learning_rate": 0.00010131345475519258,
        "epoch": 0.8325508607198748,
        "step": 11172
    },
    {
        "loss": 1.8697,
        "grad_norm": 2.431729555130005,
        "learning_rate": 0.00010124309483480409,
        "epoch": 0.8326253819211565,
        "step": 11173
    },
    {
        "loss": 2.4851,
        "grad_norm": 2.2663025856018066,
        "learning_rate": 0.0001011727342989188,
        "epoch": 0.8326999031224384,
        "step": 11174
    },
    {
        "loss": 2.1924,
        "grad_norm": 2.582658052444458,
        "learning_rate": 0.00010110237318237435,
        "epoch": 0.8327744243237201,
        "step": 11175
    },
    {
        "loss": 2.5374,
        "grad_norm": 2.3574674129486084,
        "learning_rate": 0.00010103201152000869,
        "epoch": 0.8328489455250019,
        "step": 11176
    },
    {
        "loss": 2.7848,
        "grad_norm": 3.147085428237915,
        "learning_rate": 0.0001009616493466605,
        "epoch": 0.8329234667262836,
        "step": 11177
    },
    {
        "loss": 2.4179,
        "grad_norm": 2.2984135150909424,
        "learning_rate": 0.00010089128669716821,
        "epoch": 0.8329979879275654,
        "step": 11178
    },
    {
        "loss": 2.4425,
        "grad_norm": 2.586886405944824,
        "learning_rate": 0.00010082092360637057,
        "epoch": 0.8330725091288471,
        "step": 11179
    },
    {
        "loss": 2.3019,
        "grad_norm": 3.384538173675537,
        "learning_rate": 0.00010075056010910688,
        "epoch": 0.8331470303301289,
        "step": 11180
    },
    {
        "loss": 1.9554,
        "grad_norm": 3.556103229522705,
        "learning_rate": 0.00010068019624021634,
        "epoch": 0.8332215515314106,
        "step": 11181
    },
    {
        "loss": 1.8345,
        "grad_norm": 3.1867001056671143,
        "learning_rate": 0.0001006098320345382,
        "epoch": 0.8332960727326925,
        "step": 11182
    },
    {
        "loss": 2.4307,
        "grad_norm": 2.8313913345336914,
        "learning_rate": 0.00010053946752691256,
        "epoch": 0.8333705939339742,
        "step": 11183
    },
    {
        "loss": 1.9174,
        "grad_norm": 3.724783182144165,
        "learning_rate": 0.00010046910275217858,
        "epoch": 0.833445115135256,
        "step": 11184
    },
    {
        "loss": 2.6063,
        "grad_norm": 2.912503719329834,
        "learning_rate": 0.0001003987377451766,
        "epoch": 0.8335196363365377,
        "step": 11185
    },
    {
        "loss": 2.7527,
        "grad_norm": 2.1310646533966064,
        "learning_rate": 0.00010032837254074653,
        "epoch": 0.8335941575378195,
        "step": 11186
    },
    {
        "loss": 2.2964,
        "grad_norm": 3.3179564476013184,
        "learning_rate": 0.0001002580071737283,
        "epoch": 0.8336686787391012,
        "step": 11187
    },
    {
        "loss": 2.1988,
        "grad_norm": 1.8532116413116455,
        "learning_rate": 0.00010018764167896247,
        "epoch": 0.833743199940383,
        "step": 11188
    },
    {
        "loss": 2.6765,
        "grad_norm": 2.773098945617676,
        "learning_rate": 0.00010011727609128889,
        "epoch": 0.8338177211416649,
        "step": 11189
    },
    {
        "loss": 2.2223,
        "grad_norm": 2.4646923542022705,
        "learning_rate": 0.00010004691044554824,
        "epoch": 0.8338922423429466,
        "step": 11190
    },
    {
        "loss": 2.5781,
        "grad_norm": 2.979412794113159,
        "learning_rate": 9.997654477658069e-05,
        "epoch": 0.8339667635442284,
        "step": 11191
    },
    {
        "loss": 1.6332,
        "grad_norm": 2.445963144302368,
        "learning_rate": 9.99061791192266e-05,
        "epoch": 0.8340412847455101,
        "step": 11192
    },
    {
        "loss": 2.8532,
        "grad_norm": 2.4255213737487793,
        "learning_rate": 9.983581350832616e-05,
        "epoch": 0.8341158059467919,
        "step": 11193
    },
    {
        "loss": 2.9911,
        "grad_norm": 2.5696182250976562,
        "learning_rate": 9.976544797871996e-05,
        "epoch": 0.8341903271480736,
        "step": 11194
    },
    {
        "loss": 2.7415,
        "grad_norm": 3.935379981994629,
        "learning_rate": 9.969508256524816e-05,
        "epoch": 0.8342648483493554,
        "step": 11195
    },
    {
        "loss": 2.7986,
        "grad_norm": 2.7235043048858643,
        "learning_rate": 9.962471730275095e-05,
        "epoch": 0.8343393695506371,
        "step": 11196
    },
    {
        "loss": 1.3915,
        "grad_norm": 3.368384838104248,
        "learning_rate": 9.955435222606834e-05,
        "epoch": 0.834413890751919,
        "step": 11197
    },
    {
        "loss": 2.7626,
        "grad_norm": 1.8566142320632935,
        "learning_rate": 9.94839873700407e-05,
        "epoch": 0.8344884119532007,
        "step": 11198
    },
    {
        "loss": 2.464,
        "grad_norm": 2.34834623336792,
        "learning_rate": 9.941362276950765e-05,
        "epoch": 0.8345629331544825,
        "step": 11199
    },
    {
        "loss": 2.6861,
        "grad_norm": 4.690017223358154,
        "learning_rate": 9.934325845930933e-05,
        "epoch": 0.8346374543557642,
        "step": 11200
    },
    {
        "loss": 2.5105,
        "grad_norm": 3.7649424076080322,
        "learning_rate": 9.927289447428524e-05,
        "epoch": 0.834711975557046,
        "step": 11201
    },
    {
        "loss": 1.598,
        "grad_norm": 3.789685010910034,
        "learning_rate": 9.920253084927475e-05,
        "epoch": 0.8347864967583277,
        "step": 11202
    },
    {
        "loss": 2.7579,
        "grad_norm": 3.2359607219696045,
        "learning_rate": 9.913216761911765e-05,
        "epoch": 0.8348610179596095,
        "step": 11203
    },
    {
        "loss": 2.4187,
        "grad_norm": 3.203446865081787,
        "learning_rate": 9.90618048186525e-05,
        "epoch": 0.8349355391608912,
        "step": 11204
    },
    {
        "loss": 2.8191,
        "grad_norm": 2.2661397457122803,
        "learning_rate": 9.899144248271863e-05,
        "epoch": 0.8350100603621731,
        "step": 11205
    },
    {
        "loss": 1.9275,
        "grad_norm": 2.8457224369049072,
        "learning_rate": 9.892108064615461e-05,
        "epoch": 0.8350845815634548,
        "step": 11206
    },
    {
        "loss": 1.7609,
        "grad_norm": 3.770758867263794,
        "learning_rate": 9.885071934379876e-05,
        "epoch": 0.8351591027647366,
        "step": 11207
    },
    {
        "loss": 1.3692,
        "grad_norm": 2.551490306854248,
        "learning_rate": 9.878035861048952e-05,
        "epoch": 0.8352336239660183,
        "step": 11208
    },
    {
        "loss": 2.6253,
        "grad_norm": 2.342219114303589,
        "learning_rate": 9.870999848106453e-05,
        "epoch": 0.8353081451673001,
        "step": 11209
    },
    {
        "loss": 1.924,
        "grad_norm": 2.787240982055664,
        "learning_rate": 9.863963899036168e-05,
        "epoch": 0.8353826663685818,
        "step": 11210
    },
    {
        "loss": 2.3597,
        "grad_norm": 1.8406392335891724,
        "learning_rate": 9.856928017321812e-05,
        "epoch": 0.8354571875698636,
        "step": 11211
    },
    {
        "loss": 2.694,
        "grad_norm": 4.375821113586426,
        "learning_rate": 9.849892206447076e-05,
        "epoch": 0.8355317087711454,
        "step": 11212
    },
    {
        "loss": 3.0403,
        "grad_norm": 3.1700339317321777,
        "learning_rate": 9.842856469895612e-05,
        "epoch": 0.8356062299724272,
        "step": 11213
    },
    {
        "loss": 2.2823,
        "grad_norm": 3.2819759845733643,
        "learning_rate": 9.835820811151069e-05,
        "epoch": 0.8356807511737089,
        "step": 11214
    },
    {
        "loss": 2.5973,
        "grad_norm": 2.734250783920288,
        "learning_rate": 9.828785233697018e-05,
        "epoch": 0.8357552723749907,
        "step": 11215
    },
    {
        "loss": 2.45,
        "grad_norm": 4.2826409339904785,
        "learning_rate": 9.821749741017004e-05,
        "epoch": 0.8358297935762724,
        "step": 11216
    },
    {
        "loss": 2.9851,
        "grad_norm": 2.45595383644104,
        "learning_rate": 9.814714336594515e-05,
        "epoch": 0.8359043147775542,
        "step": 11217
    },
    {
        "loss": 2.6959,
        "grad_norm": 2.7170917987823486,
        "learning_rate": 9.807679023913043e-05,
        "epoch": 0.8359788359788359,
        "step": 11218
    },
    {
        "loss": 2.5131,
        "grad_norm": 2.8530328273773193,
        "learning_rate": 9.800643806455979e-05,
        "epoch": 0.8360533571801178,
        "step": 11219
    },
    {
        "loss": 2.5015,
        "grad_norm": 3.3982903957366943,
        "learning_rate": 9.793608687706681e-05,
        "epoch": 0.8361278783813995,
        "step": 11220
    },
    {
        "loss": 2.2715,
        "grad_norm": 2.890869379043579,
        "learning_rate": 9.786573671148495e-05,
        "epoch": 0.8362023995826813,
        "step": 11221
    },
    {
        "loss": 2.3487,
        "grad_norm": 4.512421607971191,
        "learning_rate": 9.779538760264657e-05,
        "epoch": 0.836276920783963,
        "step": 11222
    },
    {
        "loss": 2.4679,
        "grad_norm": 2.3334484100341797,
        "learning_rate": 9.772503958538428e-05,
        "epoch": 0.8363514419852448,
        "step": 11223
    },
    {
        "loss": 2.9335,
        "grad_norm": 2.929837465286255,
        "learning_rate": 9.765469269452908e-05,
        "epoch": 0.8364259631865266,
        "step": 11224
    },
    {
        "loss": 2.6498,
        "grad_norm": 3.755723714828491,
        "learning_rate": 9.758434696491247e-05,
        "epoch": 0.8365004843878083,
        "step": 11225
    },
    {
        "loss": 2.6771,
        "grad_norm": 3.458188772201538,
        "learning_rate": 9.751400243136478e-05,
        "epoch": 0.8365750055890901,
        "step": 11226
    },
    {
        "loss": 2.7223,
        "grad_norm": 3.2359583377838135,
        "learning_rate": 9.744365912871574e-05,
        "epoch": 0.8366495267903719,
        "step": 11227
    },
    {
        "loss": 2.5132,
        "grad_norm": 3.060636043548584,
        "learning_rate": 9.73733170917949e-05,
        "epoch": 0.8367240479916537,
        "step": 11228
    },
    {
        "loss": 2.7119,
        "grad_norm": 2.8102662563323975,
        "learning_rate": 9.73029763554308e-05,
        "epoch": 0.8367985691929354,
        "step": 11229
    },
    {
        "loss": 2.5761,
        "grad_norm": 4.028194427490234,
        "learning_rate": 9.723263695445127e-05,
        "epoch": 0.8368730903942172,
        "step": 11230
    },
    {
        "loss": 2.2647,
        "grad_norm": 3.1001338958740234,
        "learning_rate": 9.716229892368392e-05,
        "epoch": 0.8369476115954989,
        "step": 11231
    },
    {
        "loss": 2.1857,
        "grad_norm": 3.703247547149658,
        "learning_rate": 9.709196229795529e-05,
        "epoch": 0.8370221327967807,
        "step": 11232
    },
    {
        "loss": 2.0535,
        "grad_norm": 2.9663069248199463,
        "learning_rate": 9.702162711209122e-05,
        "epoch": 0.8370966539980624,
        "step": 11233
    },
    {
        "loss": 2.3127,
        "grad_norm": 3.9888947010040283,
        "learning_rate": 9.695129340091736e-05,
        "epoch": 0.8371711751993443,
        "step": 11234
    },
    {
        "loss": 2.6549,
        "grad_norm": 2.35076904296875,
        "learning_rate": 9.688096119925769e-05,
        "epoch": 0.837245696400626,
        "step": 11235
    },
    {
        "loss": 2.4625,
        "grad_norm": 2.8874495029449463,
        "learning_rate": 9.68106305419364e-05,
        "epoch": 0.8373202176019078,
        "step": 11236
    },
    {
        "loss": 2.3515,
        "grad_norm": 3.0551509857177734,
        "learning_rate": 9.674030146377624e-05,
        "epoch": 0.8373947388031895,
        "step": 11237
    },
    {
        "loss": 3.1346,
        "grad_norm": 2.37276291847229,
        "learning_rate": 9.666997399959968e-05,
        "epoch": 0.8374692600044713,
        "step": 11238
    },
    {
        "loss": 2.5572,
        "grad_norm": 3.57974910736084,
        "learning_rate": 9.659964818422801e-05,
        "epoch": 0.837543781205753,
        "step": 11239
    },
    {
        "loss": 2.2079,
        "grad_norm": 3.6156044006347656,
        "learning_rate": 9.652932405248174e-05,
        "epoch": 0.8376183024070348,
        "step": 11240
    },
    {
        "loss": 2.1092,
        "grad_norm": 2.478445291519165,
        "learning_rate": 9.645900163918093e-05,
        "epoch": 0.8376928236083165,
        "step": 11241
    },
    {
        "loss": 1.955,
        "grad_norm": 1.8239247798919678,
        "learning_rate": 9.638868097914421e-05,
        "epoch": 0.8377673448095984,
        "step": 11242
    },
    {
        "loss": 2.142,
        "grad_norm": 3.666558027267456,
        "learning_rate": 9.631836210719004e-05,
        "epoch": 0.8378418660108801,
        "step": 11243
    },
    {
        "loss": 2.3686,
        "grad_norm": 2.1879329681396484,
        "learning_rate": 9.624804505813507e-05,
        "epoch": 0.8379163872121619,
        "step": 11244
    },
    {
        "loss": 2.4969,
        "grad_norm": 4.083766460418701,
        "learning_rate": 9.617772986679601e-05,
        "epoch": 0.8379909084134436,
        "step": 11245
    },
    {
        "loss": 1.7922,
        "grad_norm": 5.053025245666504,
        "learning_rate": 9.610741656798802e-05,
        "epoch": 0.8380654296147254,
        "step": 11246
    },
    {
        "loss": 2.2003,
        "grad_norm": 2.53996205329895,
        "learning_rate": 9.603710519652543e-05,
        "epoch": 0.8381399508160071,
        "step": 11247
    },
    {
        "loss": 2.7802,
        "grad_norm": 2.606306314468384,
        "learning_rate": 9.596679578722194e-05,
        "epoch": 0.8382144720172889,
        "step": 11248
    },
    {
        "loss": 1.7052,
        "grad_norm": 2.5862951278686523,
        "learning_rate": 9.589648837488993e-05,
        "epoch": 0.8382889932185706,
        "step": 11249
    },
    {
        "loss": 2.151,
        "grad_norm": 3.7126173973083496,
        "learning_rate": 9.582618299434074e-05,
        "epoch": 0.8383635144198525,
        "step": 11250
    },
    {
        "loss": 2.6937,
        "grad_norm": 2.2169549465179443,
        "learning_rate": 9.575587968038516e-05,
        "epoch": 0.8384380356211342,
        "step": 11251
    },
    {
        "loss": 2.3859,
        "grad_norm": 1.6431190967559814,
        "learning_rate": 9.568557846783252e-05,
        "epoch": 0.838512556822416,
        "step": 11252
    },
    {
        "loss": 2.6333,
        "grad_norm": 2.37040376663208,
        "learning_rate": 9.56152793914911e-05,
        "epoch": 0.8385870780236977,
        "step": 11253
    },
    {
        "loss": 1.4596,
        "grad_norm": 3.6594302654266357,
        "learning_rate": 9.55449824861687e-05,
        "epoch": 0.8386615992249795,
        "step": 11254
    },
    {
        "loss": 2.522,
        "grad_norm": 2.5987696647644043,
        "learning_rate": 9.547468778667108e-05,
        "epoch": 0.8387361204262612,
        "step": 11255
    },
    {
        "loss": 1.7798,
        "grad_norm": 3.6763999462127686,
        "learning_rate": 9.540439532780387e-05,
        "epoch": 0.838810641627543,
        "step": 11256
    },
    {
        "loss": 2.1777,
        "grad_norm": 2.706693649291992,
        "learning_rate": 9.533410514437097e-05,
        "epoch": 0.8388851628288247,
        "step": 11257
    },
    {
        "loss": 2.6385,
        "grad_norm": 4.114736557006836,
        "learning_rate": 9.526381727117528e-05,
        "epoch": 0.8389596840301066,
        "step": 11258
    },
    {
        "loss": 2.7415,
        "grad_norm": 2.60016131401062,
        "learning_rate": 9.519353174301886e-05,
        "epoch": 0.8390342052313883,
        "step": 11259
    },
    {
        "loss": 2.7992,
        "grad_norm": 1.8329932689666748,
        "learning_rate": 9.512324859470215e-05,
        "epoch": 0.8391087264326701,
        "step": 11260
    },
    {
        "loss": 2.1841,
        "grad_norm": 3.3934035301208496,
        "learning_rate": 9.505296786102486e-05,
        "epoch": 0.8391832476339519,
        "step": 11261
    },
    {
        "loss": 3.1394,
        "grad_norm": 3.9383556842803955,
        "learning_rate": 9.498268957678515e-05,
        "epoch": 0.8392577688352336,
        "step": 11262
    },
    {
        "loss": 2.3048,
        "grad_norm": 3.9851396083831787,
        "learning_rate": 9.491241377678015e-05,
        "epoch": 0.8393322900365154,
        "step": 11263
    },
    {
        "loss": 1.5896,
        "grad_norm": 3.2970566749572754,
        "learning_rate": 9.484214049580553e-05,
        "epoch": 0.8394068112377971,
        "step": 11264
    },
    {
        "loss": 2.4317,
        "grad_norm": 2.2514188289642334,
        "learning_rate": 9.477186976865615e-05,
        "epoch": 0.839481332439079,
        "step": 11265
    },
    {
        "loss": 2.3197,
        "grad_norm": 5.803617477416992,
        "learning_rate": 9.470160163012525e-05,
        "epoch": 0.8395558536403607,
        "step": 11266
    },
    {
        "loss": 2.5075,
        "grad_norm": 2.655961275100708,
        "learning_rate": 9.463133611500488e-05,
        "epoch": 0.8396303748416425,
        "step": 11267
    },
    {
        "loss": 1.3846,
        "grad_norm": 2.750093698501587,
        "learning_rate": 9.456107325808565e-05,
        "epoch": 0.8397048960429242,
        "step": 11268
    },
    {
        "loss": 1.3306,
        "grad_norm": 3.5060670375823975,
        "learning_rate": 9.449081309415727e-05,
        "epoch": 0.839779417244206,
        "step": 11269
    },
    {
        "loss": 2.7965,
        "grad_norm": 4.102145671844482,
        "learning_rate": 9.442055565800758e-05,
        "epoch": 0.8398539384454877,
        "step": 11270
    },
    {
        "loss": 2.2885,
        "grad_norm": 3.3339314460754395,
        "learning_rate": 9.435030098442364e-05,
        "epoch": 0.8399284596467695,
        "step": 11271
    },
    {
        "loss": 2.4186,
        "grad_norm": 2.742058038711548,
        "learning_rate": 9.428004910819064e-05,
        "epoch": 0.8400029808480513,
        "step": 11272
    },
    {
        "loss": 2.2603,
        "grad_norm": 2.5995700359344482,
        "learning_rate": 9.42098000640925e-05,
        "epoch": 0.8400775020493331,
        "step": 11273
    },
    {
        "loss": 2.0442,
        "grad_norm": 3.498624086380005,
        "learning_rate": 9.413955388691221e-05,
        "epoch": 0.8401520232506148,
        "step": 11274
    },
    {
        "loss": 2.554,
        "grad_norm": 2.7247488498687744,
        "learning_rate": 9.406931061143041e-05,
        "epoch": 0.8402265444518966,
        "step": 11275
    },
    {
        "loss": 2.4892,
        "grad_norm": 2.2444088459014893,
        "learning_rate": 9.399907027242726e-05,
        "epoch": 0.8403010656531783,
        "step": 11276
    },
    {
        "loss": 1.9358,
        "grad_norm": 3.1919050216674805,
        "learning_rate": 9.39288329046809e-05,
        "epoch": 0.8403755868544601,
        "step": 11277
    },
    {
        "loss": 2.2595,
        "grad_norm": 3.2280900478363037,
        "learning_rate": 9.385859854296802e-05,
        "epoch": 0.8404501080557418,
        "step": 11278
    },
    {
        "loss": 2.3759,
        "grad_norm": 1.9688972234725952,
        "learning_rate": 9.378836722206427e-05,
        "epoch": 0.8405246292570236,
        "step": 11279
    },
    {
        "loss": 2.1791,
        "grad_norm": 4.691969394683838,
        "learning_rate": 9.371813897674327e-05,
        "epoch": 0.8405991504583054,
        "step": 11280
    },
    {
        "loss": 2.5154,
        "grad_norm": 2.236893653869629,
        "learning_rate": 9.36479138417772e-05,
        "epoch": 0.8406736716595872,
        "step": 11281
    },
    {
        "loss": 2.4303,
        "grad_norm": 3.969306468963623,
        "learning_rate": 9.357769185193714e-05,
        "epoch": 0.8407481928608689,
        "step": 11282
    },
    {
        "loss": 2.8867,
        "grad_norm": 4.1609039306640625,
        "learning_rate": 9.350747304199214e-05,
        "epoch": 0.8408227140621507,
        "step": 11283
    },
    {
        "loss": 2.472,
        "grad_norm": 2.9370663166046143,
        "learning_rate": 9.343725744670968e-05,
        "epoch": 0.8408972352634324,
        "step": 11284
    },
    {
        "loss": 1.802,
        "grad_norm": 3.7240257263183594,
        "learning_rate": 9.336704510085623e-05,
        "epoch": 0.8409717564647142,
        "step": 11285
    },
    {
        "loss": 2.0087,
        "grad_norm": 3.709407329559326,
        "learning_rate": 9.329683603919566e-05,
        "epoch": 0.8410462776659959,
        "step": 11286
    },
    {
        "loss": 1.9129,
        "grad_norm": 4.593632221221924,
        "learning_rate": 9.322663029649117e-05,
        "epoch": 0.8411207988672778,
        "step": 11287
    },
    {
        "loss": 1.2771,
        "grad_norm": 4.49177360534668,
        "learning_rate": 9.315642790750369e-05,
        "epoch": 0.8411953200685595,
        "step": 11288
    },
    {
        "loss": 2.13,
        "grad_norm": 3.643859386444092,
        "learning_rate": 9.308622890699295e-05,
        "epoch": 0.8412698412698413,
        "step": 11289
    },
    {
        "loss": 2.2232,
        "grad_norm": 3.376739501953125,
        "learning_rate": 9.30160333297167e-05,
        "epoch": 0.841344362471123,
        "step": 11290
    },
    {
        "loss": 2.5775,
        "grad_norm": 3.319981336593628,
        "learning_rate": 9.294584121043086e-05,
        "epoch": 0.8414188836724048,
        "step": 11291
    },
    {
        "loss": 2.1796,
        "grad_norm": 2.6304636001586914,
        "learning_rate": 9.287565258389016e-05,
        "epoch": 0.8414934048736865,
        "step": 11292
    },
    {
        "loss": 2.6001,
        "grad_norm": 3.3924636840820312,
        "learning_rate": 9.2805467484847e-05,
        "epoch": 0.8415679260749683,
        "step": 11293
    },
    {
        "loss": 2.5723,
        "grad_norm": 3.4782440662384033,
        "learning_rate": 9.273528594805278e-05,
        "epoch": 0.84164244727625,
        "step": 11294
    },
    {
        "loss": 2.4906,
        "grad_norm": 1.8763319253921509,
        "learning_rate": 9.266510800825605e-05,
        "epoch": 0.8417169684775319,
        "step": 11295
    },
    {
        "loss": 2.9259,
        "grad_norm": 3.1491196155548096,
        "learning_rate": 9.259493370020464e-05,
        "epoch": 0.8417914896788137,
        "step": 11296
    },
    {
        "loss": 2.0275,
        "grad_norm": 3.0015344619750977,
        "learning_rate": 9.252476305864407e-05,
        "epoch": 0.8418660108800954,
        "step": 11297
    },
    {
        "loss": 1.6442,
        "grad_norm": 3.7866835594177246,
        "learning_rate": 9.245459611831791e-05,
        "epoch": 0.8419405320813772,
        "step": 11298
    },
    {
        "loss": 2.4269,
        "grad_norm": 3.221682548522949,
        "learning_rate": 9.238443291396841e-05,
        "epoch": 0.8420150532826589,
        "step": 11299
    },
    {
        "loss": 1.8414,
        "grad_norm": 3.088792085647583,
        "learning_rate": 9.231427348033554e-05,
        "epoch": 0.8420895744839407,
        "step": 11300
    },
    {
        "loss": 2.1662,
        "grad_norm": 3.026228904724121,
        "learning_rate": 9.22441178521574e-05,
        "epoch": 0.8421640956852224,
        "step": 11301
    },
    {
        "loss": 1.8819,
        "grad_norm": 3.765460968017578,
        "learning_rate": 9.217396606417061e-05,
        "epoch": 0.8422386168865043,
        "step": 11302
    },
    {
        "loss": 2.8008,
        "grad_norm": 4.0980401039123535,
        "learning_rate": 9.210381815110946e-05,
        "epoch": 0.842313138087786,
        "step": 11303
    },
    {
        "loss": 2.3503,
        "grad_norm": 3.5744757652282715,
        "learning_rate": 9.203367414770636e-05,
        "epoch": 0.8423876592890678,
        "step": 11304
    },
    {
        "loss": 2.2892,
        "grad_norm": 4.560667037963867,
        "learning_rate": 9.196353408869233e-05,
        "epoch": 0.8424621804903495,
        "step": 11305
    },
    {
        "loss": 1.8826,
        "grad_norm": 3.3268795013427734,
        "learning_rate": 9.189339800879542e-05,
        "epoch": 0.8425367016916313,
        "step": 11306
    },
    {
        "loss": 2.0172,
        "grad_norm": 3.698977470397949,
        "learning_rate": 9.182326594274273e-05,
        "epoch": 0.842611222892913,
        "step": 11307
    },
    {
        "loss": 2.53,
        "grad_norm": 1.5515652894973755,
        "learning_rate": 9.175313792525868e-05,
        "epoch": 0.8426857440941948,
        "step": 11308
    },
    {
        "loss": 1.85,
        "grad_norm": 3.878537178039551,
        "learning_rate": 9.168301399106624e-05,
        "epoch": 0.8427602652954765,
        "step": 11309
    },
    {
        "loss": 2.7582,
        "grad_norm": 1.5153967142105103,
        "learning_rate": 9.161289417488587e-05,
        "epoch": 0.8428347864967584,
        "step": 11310
    },
    {
        "loss": 2.3503,
        "grad_norm": 2.2697179317474365,
        "learning_rate": 9.154277851143609e-05,
        "epoch": 0.8429093076980401,
        "step": 11311
    },
    {
        "loss": 1.0885,
        "grad_norm": 3.8168535232543945,
        "learning_rate": 9.147266703543372e-05,
        "epoch": 0.8429838288993219,
        "step": 11312
    },
    {
        "loss": 2.1313,
        "grad_norm": 2.914870023727417,
        "learning_rate": 9.140255978159314e-05,
        "epoch": 0.8430583501006036,
        "step": 11313
    },
    {
        "loss": 2.6355,
        "grad_norm": 3.648822546005249,
        "learning_rate": 9.13324567846267e-05,
        "epoch": 0.8431328713018854,
        "step": 11314
    },
    {
        "loss": 2.2722,
        "grad_norm": 1.9139485359191895,
        "learning_rate": 9.126235807924458e-05,
        "epoch": 0.8432073925031671,
        "step": 11315
    },
    {
        "loss": 2.023,
        "grad_norm": 2.3587372303009033,
        "learning_rate": 9.119226370015521e-05,
        "epoch": 0.8432819137044489,
        "step": 11316
    },
    {
        "loss": 1.7928,
        "grad_norm": 2.7737014293670654,
        "learning_rate": 9.11221736820645e-05,
        "epoch": 0.8433564349057306,
        "step": 11317
    },
    {
        "loss": 1.384,
        "grad_norm": 4.929811477661133,
        "learning_rate": 9.105208805967629e-05,
        "epoch": 0.8434309561070125,
        "step": 11318
    },
    {
        "loss": 0.8102,
        "grad_norm": 2.9238016605377197,
        "learning_rate": 9.098200686769212e-05,
        "epoch": 0.8435054773082942,
        "step": 11319
    },
    {
        "loss": 2.8594,
        "grad_norm": 2.801821708679199,
        "learning_rate": 9.091193014081178e-05,
        "epoch": 0.843579998509576,
        "step": 11320
    },
    {
        "loss": 2.2028,
        "grad_norm": 3.6194608211517334,
        "learning_rate": 9.084185791373228e-05,
        "epoch": 0.8436545197108577,
        "step": 11321
    },
    {
        "loss": 2.8632,
        "grad_norm": 3.7954728603363037,
        "learning_rate": 9.07717902211489e-05,
        "epoch": 0.8437290409121395,
        "step": 11322
    },
    {
        "loss": 2.7625,
        "grad_norm": 3.9259443283081055,
        "learning_rate": 9.070172709775436e-05,
        "epoch": 0.8438035621134212,
        "step": 11323
    },
    {
        "loss": 2.5989,
        "grad_norm": 3.081096649169922,
        "learning_rate": 9.063166857823906e-05,
        "epoch": 0.843878083314703,
        "step": 11324
    },
    {
        "loss": 2.0227,
        "grad_norm": 2.963322162628174,
        "learning_rate": 9.05616146972917e-05,
        "epoch": 0.8439526045159848,
        "step": 11325
    },
    {
        "loss": 1.9612,
        "grad_norm": 3.469405174255371,
        "learning_rate": 9.049156548959765e-05,
        "epoch": 0.8440271257172666,
        "step": 11326
    },
    {
        "loss": 2.5253,
        "grad_norm": 3.3225860595703125,
        "learning_rate": 9.0421520989841e-05,
        "epoch": 0.8441016469185483,
        "step": 11327
    },
    {
        "loss": 1.4762,
        "grad_norm": 2.717386245727539,
        "learning_rate": 9.035148123270297e-05,
        "epoch": 0.8441761681198301,
        "step": 11328
    },
    {
        "loss": 2.4909,
        "grad_norm": 1.8750823736190796,
        "learning_rate": 9.028144625286235e-05,
        "epoch": 0.8442506893211118,
        "step": 11329
    },
    {
        "loss": 2.8092,
        "grad_norm": 2.640043020248413,
        "learning_rate": 9.021141608499606e-05,
        "epoch": 0.8443252105223936,
        "step": 11330
    },
    {
        "loss": 2.4235,
        "grad_norm": 3.7157199382781982,
        "learning_rate": 9.014139076377803e-05,
        "epoch": 0.8443997317236754,
        "step": 11331
    },
    {
        "loss": 1.7658,
        "grad_norm": 7.933465957641602,
        "learning_rate": 9.00713703238804e-05,
        "epoch": 0.8444742529249571,
        "step": 11332
    },
    {
        "loss": 2.8335,
        "grad_norm": 3.406132936477661,
        "learning_rate": 9.000135479997241e-05,
        "epoch": 0.844548774126239,
        "step": 11333
    },
    {
        "loss": 2.26,
        "grad_norm": 3.506120443344116,
        "learning_rate": 8.993134422672105e-05,
        "epoch": 0.8446232953275207,
        "step": 11334
    },
    {
        "loss": 2.1272,
        "grad_norm": 3.1915225982666016,
        "learning_rate": 8.986133863879071e-05,
        "epoch": 0.8446978165288025,
        "step": 11335
    },
    {
        "loss": 2.4242,
        "grad_norm": 3.6120147705078125,
        "learning_rate": 8.979133807084375e-05,
        "epoch": 0.8447723377300842,
        "step": 11336
    },
    {
        "loss": 2.465,
        "grad_norm": 3.243187427520752,
        "learning_rate": 8.972134255753955e-05,
        "epoch": 0.844846858931366,
        "step": 11337
    },
    {
        "loss": 2.0855,
        "grad_norm": 2.6175332069396973,
        "learning_rate": 8.965135213353522e-05,
        "epoch": 0.8449213801326477,
        "step": 11338
    },
    {
        "loss": 2.4021,
        "grad_norm": 2.209017515182495,
        "learning_rate": 8.958136683348519e-05,
        "epoch": 0.8449959013339295,
        "step": 11339
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.9816384315490723,
        "learning_rate": 8.951138669204167e-05,
        "epoch": 0.8450704225352113,
        "step": 11340
    },
    {
        "loss": 2.0016,
        "grad_norm": 2.7755119800567627,
        "learning_rate": 8.944141174385394e-05,
        "epoch": 0.8451449437364931,
        "step": 11341
    },
    {
        "loss": 2.7909,
        "grad_norm": 2.7844834327697754,
        "learning_rate": 8.93714420235691e-05,
        "epoch": 0.8452194649377748,
        "step": 11342
    },
    {
        "loss": 1.8449,
        "grad_norm": 3.470254421234131,
        "learning_rate": 8.930147756583135e-05,
        "epoch": 0.8452939861390566,
        "step": 11343
    },
    {
        "loss": 2.2193,
        "grad_norm": 4.173576831817627,
        "learning_rate": 8.923151840528222e-05,
        "epoch": 0.8453685073403383,
        "step": 11344
    },
    {
        "loss": 2.8246,
        "grad_norm": 2.9775476455688477,
        "learning_rate": 8.91615645765612e-05,
        "epoch": 0.8454430285416201,
        "step": 11345
    },
    {
        "loss": 2.4976,
        "grad_norm": 3.3718767166137695,
        "learning_rate": 8.909161611430417e-05,
        "epoch": 0.8455175497429018,
        "step": 11346
    },
    {
        "loss": 2.2185,
        "grad_norm": 3.0859382152557373,
        "learning_rate": 8.902167305314534e-05,
        "epoch": 0.8455920709441836,
        "step": 11347
    },
    {
        "loss": 2.8129,
        "grad_norm": 2.0455548763275146,
        "learning_rate": 8.895173542771563e-05,
        "epoch": 0.8456665921454654,
        "step": 11348
    },
    {
        "loss": 1.9428,
        "grad_norm": 4.83421516418457,
        "learning_rate": 8.888180327264334e-05,
        "epoch": 0.8457411133467472,
        "step": 11349
    },
    {
        "loss": 2.2735,
        "grad_norm": 2.5121893882751465,
        "learning_rate": 8.881187662255445e-05,
        "epoch": 0.8458156345480289,
        "step": 11350
    },
    {
        "loss": 1.7747,
        "grad_norm": 2.7432708740234375,
        "learning_rate": 8.874195551207178e-05,
        "epoch": 0.8458901557493107,
        "step": 11351
    },
    {
        "loss": 1.9508,
        "grad_norm": 2.907740831375122,
        "learning_rate": 8.867203997581543e-05,
        "epoch": 0.8459646769505924,
        "step": 11352
    },
    {
        "loss": 2.3967,
        "grad_norm": 1.7560232877731323,
        "learning_rate": 8.860213004840313e-05,
        "epoch": 0.8460391981518742,
        "step": 11353
    },
    {
        "loss": 2.9175,
        "grad_norm": 3.0499908924102783,
        "learning_rate": 8.853222576444948e-05,
        "epoch": 0.8461137193531559,
        "step": 11354
    },
    {
        "loss": 2.4054,
        "grad_norm": 3.4342751502990723,
        "learning_rate": 8.84623271585662e-05,
        "epoch": 0.8461882405544378,
        "step": 11355
    },
    {
        "loss": 2.1775,
        "grad_norm": 3.3102967739105225,
        "learning_rate": 8.83924342653628e-05,
        "epoch": 0.8462627617557195,
        "step": 11356
    },
    {
        "loss": 2.3578,
        "grad_norm": 1.924659252166748,
        "learning_rate": 8.832254711944498e-05,
        "epoch": 0.8463372829570013,
        "step": 11357
    },
    {
        "loss": 2.6279,
        "grad_norm": 3.0128612518310547,
        "learning_rate": 8.825266575541662e-05,
        "epoch": 0.846411804158283,
        "step": 11358
    },
    {
        "loss": 1.5511,
        "grad_norm": 3.3070297241210938,
        "learning_rate": 8.818279020787794e-05,
        "epoch": 0.8464863253595648,
        "step": 11359
    },
    {
        "loss": 2.1586,
        "grad_norm": 3.9014790058135986,
        "learning_rate": 8.811292051142689e-05,
        "epoch": 0.8465608465608465,
        "step": 11360
    },
    {
        "loss": 1.4251,
        "grad_norm": 1.857998013496399,
        "learning_rate": 8.804305670065814e-05,
        "epoch": 0.8466353677621283,
        "step": 11361
    },
    {
        "loss": 3.2156,
        "grad_norm": 2.4112296104431152,
        "learning_rate": 8.797319881016339e-05,
        "epoch": 0.84670988896341,
        "step": 11362
    },
    {
        "loss": 2.7916,
        "grad_norm": 2.507565498352051,
        "learning_rate": 8.790334687453185e-05,
        "epoch": 0.8467844101646919,
        "step": 11363
    },
    {
        "loss": 1.753,
        "grad_norm": 3.9692749977111816,
        "learning_rate": 8.783350092834923e-05,
        "epoch": 0.8468589313659736,
        "step": 11364
    },
    {
        "loss": 2.6549,
        "grad_norm": 3.475053548812866,
        "learning_rate": 8.776366100619895e-05,
        "epoch": 0.8469334525672554,
        "step": 11365
    },
    {
        "loss": 1.9198,
        "grad_norm": 3.0821306705474854,
        "learning_rate": 8.769382714266049e-05,
        "epoch": 0.8470079737685372,
        "step": 11366
    },
    {
        "loss": 2.2632,
        "grad_norm": 3.5416243076324463,
        "learning_rate": 8.762399937231129e-05,
        "epoch": 0.8470824949698189,
        "step": 11367
    },
    {
        "loss": 2.3965,
        "grad_norm": 2.713444232940674,
        "learning_rate": 8.755417772972523e-05,
        "epoch": 0.8471570161711007,
        "step": 11368
    },
    {
        "loss": 2.359,
        "grad_norm": 1.697351336479187,
        "learning_rate": 8.748436224947318e-05,
        "epoch": 0.8472315373723824,
        "step": 11369
    },
    {
        "loss": 2.5311,
        "grad_norm": 2.844998598098755,
        "learning_rate": 8.741455296612336e-05,
        "epoch": 0.8473060585736643,
        "step": 11370
    },
    {
        "loss": 2.7843,
        "grad_norm": 2.593696355819702,
        "learning_rate": 8.734474991424042e-05,
        "epoch": 0.847380579774946,
        "step": 11371
    },
    {
        "loss": 2.3699,
        "grad_norm": 2.2560272216796875,
        "learning_rate": 8.727495312838602e-05,
        "epoch": 0.8474551009762278,
        "step": 11372
    },
    {
        "loss": 3.5734,
        "grad_norm": 3.535249710083008,
        "learning_rate": 8.720516264311919e-05,
        "epoch": 0.8475296221775095,
        "step": 11373
    },
    {
        "loss": 2.7445,
        "grad_norm": 3.2155821323394775,
        "learning_rate": 8.713537849299524e-05,
        "epoch": 0.8476041433787913,
        "step": 11374
    },
    {
        "loss": 1.6031,
        "grad_norm": 3.7259063720703125,
        "learning_rate": 8.70656007125665e-05,
        "epoch": 0.847678664580073,
        "step": 11375
    },
    {
        "loss": 1.5674,
        "grad_norm": 4.220101833343506,
        "learning_rate": 8.699582933638267e-05,
        "epoch": 0.8477531857813548,
        "step": 11376
    },
    {
        "loss": 2.1255,
        "grad_norm": 3.6137824058532715,
        "learning_rate": 8.692606439898923e-05,
        "epoch": 0.8478277069826365,
        "step": 11377
    },
    {
        "loss": 2.4933,
        "grad_norm": 3.009286880493164,
        "learning_rate": 8.685630593492955e-05,
        "epoch": 0.8479022281839184,
        "step": 11378
    },
    {
        "loss": 2.3407,
        "grad_norm": 2.011469602584839,
        "learning_rate": 8.678655397874317e-05,
        "epoch": 0.8479767493852001,
        "step": 11379
    },
    {
        "loss": 3.6421,
        "grad_norm": 2.557685375213623,
        "learning_rate": 8.671680856496645e-05,
        "epoch": 0.8480512705864819,
        "step": 11380
    },
    {
        "loss": 2.056,
        "grad_norm": 2.8983919620513916,
        "learning_rate": 8.664706972813288e-05,
        "epoch": 0.8481257917877636,
        "step": 11381
    },
    {
        "loss": 2.5159,
        "grad_norm": 2.8359315395355225,
        "learning_rate": 8.657733750277217e-05,
        "epoch": 0.8482003129890454,
        "step": 11382
    },
    {
        "loss": 1.5503,
        "grad_norm": 3.5090856552124023,
        "learning_rate": 8.65076119234113e-05,
        "epoch": 0.8482748341903271,
        "step": 11383
    },
    {
        "loss": 2.7261,
        "grad_norm": 3.222994327545166,
        "learning_rate": 8.643789302457359e-05,
        "epoch": 0.8483493553916089,
        "step": 11384
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.7284951210021973,
        "learning_rate": 8.636818084077909e-05,
        "epoch": 0.8484238765928906,
        "step": 11385
    },
    {
        "loss": 2.5186,
        "grad_norm": 2.5372109413146973,
        "learning_rate": 8.629847540654447e-05,
        "epoch": 0.8484983977941725,
        "step": 11386
    },
    {
        "loss": 1.7973,
        "grad_norm": 1.7280491590499878,
        "learning_rate": 8.622877675638347e-05,
        "epoch": 0.8485729189954542,
        "step": 11387
    },
    {
        "loss": 2.8668,
        "grad_norm": 3.7953004837036133,
        "learning_rate": 8.615908492480602e-05,
        "epoch": 0.848647440196736,
        "step": 11388
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.702329158782959,
        "learning_rate": 8.608939994631882e-05,
        "epoch": 0.8487219613980177,
        "step": 11389
    },
    {
        "loss": 2.4781,
        "grad_norm": 2.1411709785461426,
        "learning_rate": 8.601972185542506e-05,
        "epoch": 0.8487964825992995,
        "step": 11390
    },
    {
        "loss": 2.1253,
        "grad_norm": 3.3599278926849365,
        "learning_rate": 8.595005068662489e-05,
        "epoch": 0.8488710038005812,
        "step": 11391
    },
    {
        "loss": 2.3175,
        "grad_norm": 4.010967254638672,
        "learning_rate": 8.588038647441456e-05,
        "epoch": 0.848945525001863,
        "step": 11392
    },
    {
        "loss": 2.3689,
        "grad_norm": 3.591787576675415,
        "learning_rate": 8.581072925328736e-05,
        "epoch": 0.8490200462031448,
        "step": 11393
    },
    {
        "loss": 1.8508,
        "grad_norm": 3.222522735595703,
        "learning_rate": 8.57410790577327e-05,
        "epoch": 0.8490945674044266,
        "step": 11394
    },
    {
        "loss": 2.3868,
        "grad_norm": 2.2565994262695312,
        "learning_rate": 8.567143592223654e-05,
        "epoch": 0.8491690886057083,
        "step": 11395
    },
    {
        "loss": 2.4894,
        "grad_norm": 2.466888666152954,
        "learning_rate": 8.560179988128193e-05,
        "epoch": 0.8492436098069901,
        "step": 11396
    },
    {
        "loss": 2.6909,
        "grad_norm": 2.5634536743164062,
        "learning_rate": 8.553217096934736e-05,
        "epoch": 0.8493181310082718,
        "step": 11397
    },
    {
        "loss": 1.7555,
        "grad_norm": 2.285994291305542,
        "learning_rate": 8.54625492209088e-05,
        "epoch": 0.8493926522095536,
        "step": 11398
    },
    {
        "loss": 2.4911,
        "grad_norm": 3.742542266845703,
        "learning_rate": 8.539293467043816e-05,
        "epoch": 0.8494671734108353,
        "step": 11399
    },
    {
        "loss": 2.395,
        "grad_norm": 2.2152762413024902,
        "learning_rate": 8.53233273524037e-05,
        "epoch": 0.8495416946121171,
        "step": 11400
    },
    {
        "loss": 2.9067,
        "grad_norm": 3.1638693809509277,
        "learning_rate": 8.525372730127056e-05,
        "epoch": 0.849616215813399,
        "step": 11401
    },
    {
        "loss": 2.6089,
        "grad_norm": 2.4277729988098145,
        "learning_rate": 8.518413455149976e-05,
        "epoch": 0.8496907370146807,
        "step": 11402
    },
    {
        "loss": 1.8832,
        "grad_norm": 2.048551321029663,
        "learning_rate": 8.511454913754919e-05,
        "epoch": 0.8497652582159625,
        "step": 11403
    },
    {
        "loss": 2.182,
        "grad_norm": 2.3126163482666016,
        "learning_rate": 8.504497109387279e-05,
        "epoch": 0.8498397794172442,
        "step": 11404
    },
    {
        "loss": 2.7599,
        "grad_norm": 3.141920328140259,
        "learning_rate": 8.497540045492086e-05,
        "epoch": 0.849914300618526,
        "step": 11405
    },
    {
        "loss": 1.886,
        "grad_norm": 3.556938409805298,
        "learning_rate": 8.490583725513996e-05,
        "epoch": 0.8499888218198077,
        "step": 11406
    },
    {
        "loss": 1.6923,
        "grad_norm": 3.9913346767425537,
        "learning_rate": 8.48362815289736e-05,
        "epoch": 0.8500633430210895,
        "step": 11407
    },
    {
        "loss": 1.2859,
        "grad_norm": 5.571232318878174,
        "learning_rate": 8.476673331086047e-05,
        "epoch": 0.8501378642223713,
        "step": 11408
    },
    {
        "loss": 2.4139,
        "grad_norm": 3.13581919670105,
        "learning_rate": 8.46971926352366e-05,
        "epoch": 0.8502123854236531,
        "step": 11409
    },
    {
        "loss": 2.4596,
        "grad_norm": 2.0915145874023438,
        "learning_rate": 8.46276595365336e-05,
        "epoch": 0.8502869066249348,
        "step": 11410
    },
    {
        "loss": 2.5881,
        "grad_norm": 2.4100897312164307,
        "learning_rate": 8.455813404917986e-05,
        "epoch": 0.8503614278262166,
        "step": 11411
    },
    {
        "loss": 2.5984,
        "grad_norm": 3.053508758544922,
        "learning_rate": 8.448861620759956e-05,
        "epoch": 0.8504359490274983,
        "step": 11412
    },
    {
        "loss": 2.5125,
        "grad_norm": 4.344322204589844,
        "learning_rate": 8.441910604621314e-05,
        "epoch": 0.8505104702287801,
        "step": 11413
    },
    {
        "loss": 2.3994,
        "grad_norm": 2.7458128929138184,
        "learning_rate": 8.434960359943763e-05,
        "epoch": 0.8505849914300618,
        "step": 11414
    },
    {
        "loss": 2.0105,
        "grad_norm": 3.3402514457702637,
        "learning_rate": 8.428010890168571e-05,
        "epoch": 0.8506595126313437,
        "step": 11415
    },
    {
        "loss": 2.4649,
        "grad_norm": 2.0933268070220947,
        "learning_rate": 8.421062198736687e-05,
        "epoch": 0.8507340338326254,
        "step": 11416
    },
    {
        "loss": 1.4975,
        "grad_norm": 4.695122241973877,
        "learning_rate": 8.41411428908858e-05,
        "epoch": 0.8508085550339072,
        "step": 11417
    },
    {
        "loss": 2.5042,
        "grad_norm": 2.6275222301483154,
        "learning_rate": 8.407167164664432e-05,
        "epoch": 0.8508830762351889,
        "step": 11418
    },
    {
        "loss": 2.9441,
        "grad_norm": 2.058835983276367,
        "learning_rate": 8.400220828903974e-05,
        "epoch": 0.8509575974364707,
        "step": 11419
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.9437057971954346,
        "learning_rate": 8.393275285246553e-05,
        "epoch": 0.8510321186377524,
        "step": 11420
    },
    {
        "loss": 2.1903,
        "grad_norm": 4.180764675140381,
        "learning_rate": 8.386330537131163e-05,
        "epoch": 0.8511066398390342,
        "step": 11421
    },
    {
        "loss": 1.9135,
        "grad_norm": 4.130759239196777,
        "learning_rate": 8.379386587996362e-05,
        "epoch": 0.8511811610403159,
        "step": 11422
    },
    {
        "loss": 2.8304,
        "grad_norm": 2.239300012588501,
        "learning_rate": 8.372443441280312e-05,
        "epoch": 0.8512556822415978,
        "step": 11423
    },
    {
        "loss": 2.2759,
        "grad_norm": 1.9621450901031494,
        "learning_rate": 8.365501100420819e-05,
        "epoch": 0.8513302034428795,
        "step": 11424
    },
    {
        "loss": 2.01,
        "grad_norm": 3.738018274307251,
        "learning_rate": 8.358559568855251e-05,
        "epoch": 0.8514047246441613,
        "step": 11425
    },
    {
        "loss": 2.6118,
        "grad_norm": 2.3629236221313477,
        "learning_rate": 8.351618850020575e-05,
        "epoch": 0.851479245845443,
        "step": 11426
    },
    {
        "loss": 2.6525,
        "grad_norm": 4.016454696655273,
        "learning_rate": 8.344678947353409e-05,
        "epoch": 0.8515537670467248,
        "step": 11427
    },
    {
        "loss": 2.6964,
        "grad_norm": 5.453847885131836,
        "learning_rate": 8.337739864289866e-05,
        "epoch": 0.8516282882480065,
        "step": 11428
    },
    {
        "loss": 2.4339,
        "grad_norm": 3.9146225452423096,
        "learning_rate": 8.330801604265761e-05,
        "epoch": 0.8517028094492883,
        "step": 11429
    },
    {
        "loss": 2.2829,
        "grad_norm": 3.229465961456299,
        "learning_rate": 8.323864170716426e-05,
        "epoch": 0.85177733065057,
        "step": 11430
    },
    {
        "loss": 2.7659,
        "grad_norm": 3.4813761711120605,
        "learning_rate": 8.316927567076833e-05,
        "epoch": 0.8518518518518519,
        "step": 11431
    },
    {
        "loss": 2.1268,
        "grad_norm": 4.206568241119385,
        "learning_rate": 8.309991796781514e-05,
        "epoch": 0.8519263730531336,
        "step": 11432
    },
    {
        "loss": 2.1595,
        "grad_norm": 2.5092966556549072,
        "learning_rate": 8.303056863264578e-05,
        "epoch": 0.8520008942544154,
        "step": 11433
    },
    {
        "loss": 1.8607,
        "grad_norm": 4.336984157562256,
        "learning_rate": 8.296122769959768e-05,
        "epoch": 0.8520754154556971,
        "step": 11434
    },
    {
        "loss": 2.6857,
        "grad_norm": 2.4451842308044434,
        "learning_rate": 8.289189520300354e-05,
        "epoch": 0.8521499366569789,
        "step": 11435
    },
    {
        "loss": 2.4576,
        "grad_norm": 2.9207465648651123,
        "learning_rate": 8.28225711771925e-05,
        "epoch": 0.8522244578582606,
        "step": 11436
    },
    {
        "loss": 2.491,
        "grad_norm": 2.4090588092803955,
        "learning_rate": 8.275325565648868e-05,
        "epoch": 0.8522989790595424,
        "step": 11437
    },
    {
        "loss": 2.0855,
        "grad_norm": 3.407087564468384,
        "learning_rate": 8.268394867521286e-05,
        "epoch": 0.8523735002608243,
        "step": 11438
    },
    {
        "loss": 2.6221,
        "grad_norm": 2.2976114749908447,
        "learning_rate": 8.2614650267681e-05,
        "epoch": 0.852448021462106,
        "step": 11439
    },
    {
        "loss": 2.5371,
        "grad_norm": 2.7258107662200928,
        "learning_rate": 8.254536046820507e-05,
        "epoch": 0.8525225426633878,
        "step": 11440
    },
    {
        "loss": 2.646,
        "grad_norm": 1.996368169784546,
        "learning_rate": 8.247607931109259e-05,
        "epoch": 0.8525970638646695,
        "step": 11441
    },
    {
        "loss": 2.1791,
        "grad_norm": 3.044743299484253,
        "learning_rate": 8.240680683064716e-05,
        "epoch": 0.8526715850659513,
        "step": 11442
    },
    {
        "loss": 2.428,
        "grad_norm": 4.642334461212158,
        "learning_rate": 8.233754306116759e-05,
        "epoch": 0.852746106267233,
        "step": 11443
    },
    {
        "loss": 1.3822,
        "grad_norm": 3.4305875301361084,
        "learning_rate": 8.226828803694891e-05,
        "epoch": 0.8528206274685148,
        "step": 11444
    },
    {
        "loss": 1.7533,
        "grad_norm": 3.63199782371521,
        "learning_rate": 8.219904179228146e-05,
        "epoch": 0.8528951486697965,
        "step": 11445
    },
    {
        "loss": 2.3296,
        "grad_norm": 2.8588106632232666,
        "learning_rate": 8.212980436145114e-05,
        "epoch": 0.8529696698710784,
        "step": 11446
    },
    {
        "loss": 1.9629,
        "grad_norm": 3.8240041732788086,
        "learning_rate": 8.206057577874007e-05,
        "epoch": 0.8530441910723601,
        "step": 11447
    },
    {
        "loss": 2.2083,
        "grad_norm": 3.3715367317199707,
        "learning_rate": 8.199135607842508e-05,
        "epoch": 0.8531187122736419,
        "step": 11448
    },
    {
        "loss": 2.2654,
        "grad_norm": 3.3306097984313965,
        "learning_rate": 8.192214529477952e-05,
        "epoch": 0.8531932334749236,
        "step": 11449
    },
    {
        "loss": 1.9333,
        "grad_norm": 3.2998883724212646,
        "learning_rate": 8.185294346207173e-05,
        "epoch": 0.8532677546762054,
        "step": 11450
    },
    {
        "loss": 2.3439,
        "grad_norm": 3.054717540740967,
        "learning_rate": 8.178375061456571e-05,
        "epoch": 0.8533422758774871,
        "step": 11451
    },
    {
        "loss": 2.0742,
        "grad_norm": 2.659980058670044,
        "learning_rate": 8.171456678652139e-05,
        "epoch": 0.8534167970787689,
        "step": 11452
    },
    {
        "loss": 2.4693,
        "grad_norm": 2.8817977905273438,
        "learning_rate": 8.164539201219362e-05,
        "epoch": 0.8534913182800506,
        "step": 11453
    },
    {
        "loss": 1.8669,
        "grad_norm": 3.7783966064453125,
        "learning_rate": 8.157622632583339e-05,
        "epoch": 0.8535658394813325,
        "step": 11454
    },
    {
        "loss": 1.6933,
        "grad_norm": 4.06667947769165,
        "learning_rate": 8.150706976168679e-05,
        "epoch": 0.8536403606826142,
        "step": 11455
    },
    {
        "loss": 2.9518,
        "grad_norm": 2.994582414627075,
        "learning_rate": 8.14379223539955e-05,
        "epoch": 0.853714881883896,
        "step": 11456
    },
    {
        "loss": 2.8328,
        "grad_norm": 2.416041851043701,
        "learning_rate": 8.136878413699646e-05,
        "epoch": 0.8537894030851777,
        "step": 11457
    },
    {
        "loss": 2.368,
        "grad_norm": 3.373483896255493,
        "learning_rate": 8.12996551449226e-05,
        "epoch": 0.8538639242864595,
        "step": 11458
    },
    {
        "loss": 2.3075,
        "grad_norm": 2.1749985218048096,
        "learning_rate": 8.123053541200181e-05,
        "epoch": 0.8539384454877412,
        "step": 11459
    },
    {
        "loss": 2.1714,
        "grad_norm": 3.733182907104492,
        "learning_rate": 8.116142497245752e-05,
        "epoch": 0.854012966689023,
        "step": 11460
    },
    {
        "loss": 2.6339,
        "grad_norm": 2.715919256210327,
        "learning_rate": 8.109232386050844e-05,
        "epoch": 0.8540874878903048,
        "step": 11461
    },
    {
        "loss": 1.264,
        "grad_norm": 3.3589189052581787,
        "learning_rate": 8.102323211036904e-05,
        "epoch": 0.8541620090915866,
        "step": 11462
    },
    {
        "loss": 2.8736,
        "grad_norm": 2.1510024070739746,
        "learning_rate": 8.095414975624866e-05,
        "epoch": 0.8542365302928683,
        "step": 11463
    },
    {
        "loss": 1.5972,
        "grad_norm": 4.230395793914795,
        "learning_rate": 8.088507683235253e-05,
        "epoch": 0.8543110514941501,
        "step": 11464
    },
    {
        "loss": 2.5921,
        "grad_norm": 2.7549936771392822,
        "learning_rate": 8.081601337288073e-05,
        "epoch": 0.8543855726954318,
        "step": 11465
    },
    {
        "loss": 2.664,
        "grad_norm": 3.159532070159912,
        "learning_rate": 8.074695941202872e-05,
        "epoch": 0.8544600938967136,
        "step": 11466
    },
    {
        "loss": 2.3466,
        "grad_norm": 3.7226128578186035,
        "learning_rate": 8.067791498398783e-05,
        "epoch": 0.8545346150979953,
        "step": 11467
    },
    {
        "loss": 2.1953,
        "grad_norm": 3.1441245079040527,
        "learning_rate": 8.060888012294361e-05,
        "epoch": 0.8546091362992772,
        "step": 11468
    },
    {
        "loss": 1.3606,
        "grad_norm": 3.250877618789673,
        "learning_rate": 8.053985486307792e-05,
        "epoch": 0.8546836575005589,
        "step": 11469
    },
    {
        "loss": 1.7076,
        "grad_norm": 4.309354305267334,
        "learning_rate": 8.047083923856727e-05,
        "epoch": 0.8547581787018407,
        "step": 11470
    },
    {
        "loss": 2.7496,
        "grad_norm": 2.7558975219726562,
        "learning_rate": 8.040183328358344e-05,
        "epoch": 0.8548326999031224,
        "step": 11471
    },
    {
        "loss": 2.0691,
        "grad_norm": 3.9973134994506836,
        "learning_rate": 8.033283703229383e-05,
        "epoch": 0.8549072211044042,
        "step": 11472
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.8560452461242676,
        "learning_rate": 8.026385051886057e-05,
        "epoch": 0.854981742305686,
        "step": 11473
    },
    {
        "loss": 1.2656,
        "grad_norm": 2.4806408882141113,
        "learning_rate": 8.0194873777441e-05,
        "epoch": 0.8550562635069677,
        "step": 11474
    },
    {
        "loss": 2.9281,
        "grad_norm": 3.5111277103424072,
        "learning_rate": 8.012590684218808e-05,
        "epoch": 0.8551307847082495,
        "step": 11475
    },
    {
        "loss": 2.7988,
        "grad_norm": 2.817667245864868,
        "learning_rate": 8.005694974724941e-05,
        "epoch": 0.8552053059095313,
        "step": 11476
    },
    {
        "loss": 2.2243,
        "grad_norm": 3.3167381286621094,
        "learning_rate": 7.99880025267678e-05,
        "epoch": 0.8552798271108131,
        "step": 11477
    },
    {
        "loss": 2.6922,
        "grad_norm": 3.1354026794433594,
        "learning_rate": 7.991906521488169e-05,
        "epoch": 0.8553543483120948,
        "step": 11478
    },
    {
        "loss": 2.7981,
        "grad_norm": 2.081120491027832,
        "learning_rate": 7.985013784572364e-05,
        "epoch": 0.8554288695133766,
        "step": 11479
    },
    {
        "loss": 2.4316,
        "grad_norm": 1.9737895727157593,
        "learning_rate": 7.978122045342224e-05,
        "epoch": 0.8555033907146583,
        "step": 11480
    },
    {
        "loss": 2.2827,
        "grad_norm": 3.4607696533203125,
        "learning_rate": 7.971231307210052e-05,
        "epoch": 0.8555779119159401,
        "step": 11481
    },
    {
        "loss": 2.4782,
        "grad_norm": 2.6248619556427,
        "learning_rate": 7.964341573587701e-05,
        "epoch": 0.8556524331172218,
        "step": 11482
    },
    {
        "loss": 2.1178,
        "grad_norm": 4.106978893280029,
        "learning_rate": 7.957452847886495e-05,
        "epoch": 0.8557269543185037,
        "step": 11483
    },
    {
        "loss": 2.1295,
        "grad_norm": 3.385047435760498,
        "learning_rate": 7.950565133517249e-05,
        "epoch": 0.8558014755197854,
        "step": 11484
    },
    {
        "loss": 2.3103,
        "grad_norm": 3.6350324153900146,
        "learning_rate": 7.943678433890325e-05,
        "epoch": 0.8558759967210672,
        "step": 11485
    },
    {
        "loss": 2.9345,
        "grad_norm": 2.4678637981414795,
        "learning_rate": 7.936792752415526e-05,
        "epoch": 0.8559505179223489,
        "step": 11486
    },
    {
        "loss": 2.5712,
        "grad_norm": 2.381247043609619,
        "learning_rate": 7.929908092502222e-05,
        "epoch": 0.8560250391236307,
        "step": 11487
    },
    {
        "loss": 2.5983,
        "grad_norm": 2.7008230686187744,
        "learning_rate": 7.923024457559174e-05,
        "epoch": 0.8560995603249124,
        "step": 11488
    },
    {
        "loss": 2.7956,
        "grad_norm": 3.4361109733581543,
        "learning_rate": 7.91614185099474e-05,
        "epoch": 0.8561740815261942,
        "step": 11489
    },
    {
        "loss": 2.089,
        "grad_norm": 3.1753334999084473,
        "learning_rate": 7.90926027621671e-05,
        "epoch": 0.8562486027274759,
        "step": 11490
    },
    {
        "loss": 1.9114,
        "grad_norm": 4.821897029876709,
        "learning_rate": 7.902379736632361e-05,
        "epoch": 0.8563231239287578,
        "step": 11491
    },
    {
        "loss": 2.4814,
        "grad_norm": 6.021666049957275,
        "learning_rate": 7.895500235648501e-05,
        "epoch": 0.8563976451300395,
        "step": 11492
    },
    {
        "loss": 2.4076,
        "grad_norm": 3.7679197788238525,
        "learning_rate": 7.888621776671383e-05,
        "epoch": 0.8564721663313213,
        "step": 11493
    },
    {
        "loss": 2.4122,
        "grad_norm": 3.0492067337036133,
        "learning_rate": 7.881744363106744e-05,
        "epoch": 0.856546687532603,
        "step": 11494
    },
    {
        "loss": 2.4189,
        "grad_norm": 3.940117120742798,
        "learning_rate": 7.874867998359848e-05,
        "epoch": 0.8566212087338848,
        "step": 11495
    },
    {
        "loss": 1.4558,
        "grad_norm": 2.4531548023223877,
        "learning_rate": 7.867992685835388e-05,
        "epoch": 0.8566957299351665,
        "step": 11496
    },
    {
        "loss": 2.7933,
        "grad_norm": 3.606218099594116,
        "learning_rate": 7.861118428937545e-05,
        "epoch": 0.8567702511364483,
        "step": 11497
    },
    {
        "loss": 2.505,
        "grad_norm": 2.839953899383545,
        "learning_rate": 7.854245231070032e-05,
        "epoch": 0.85684477233773,
        "step": 11498
    },
    {
        "loss": 2.9047,
        "grad_norm": 1.419942855834961,
        "learning_rate": 7.847373095635937e-05,
        "epoch": 0.8569192935390119,
        "step": 11499
    },
    {
        "loss": 2.1351,
        "grad_norm": 2.906778335571289,
        "learning_rate": 7.84050202603792e-05,
        "epoch": 0.8569938147402936,
        "step": 11500
    },
    {
        "loss": 2.5655,
        "grad_norm": 2.582198143005371,
        "learning_rate": 7.833632025678061e-05,
        "epoch": 0.8570683359415754,
        "step": 11501
    },
    {
        "loss": 2.1773,
        "grad_norm": 3.8142125606536865,
        "learning_rate": 7.826763097957908e-05,
        "epoch": 0.8571428571428571,
        "step": 11502
    },
    {
        "loss": 2.0121,
        "grad_norm": 4.34206485748291,
        "learning_rate": 7.819895246278517e-05,
        "epoch": 0.8572173783441389,
        "step": 11503
    },
    {
        "loss": 2.0384,
        "grad_norm": 3.954803705215454,
        "learning_rate": 7.81302847404036e-05,
        "epoch": 0.8572918995454206,
        "step": 11504
    },
    {
        "loss": 2.5559,
        "grad_norm": 2.229123115539551,
        "learning_rate": 7.80616278464343e-05,
        "epoch": 0.8573664207467024,
        "step": 11505
    },
    {
        "loss": 2.1102,
        "grad_norm": 2.9233951568603516,
        "learning_rate": 7.799298181487142e-05,
        "epoch": 0.8574409419479841,
        "step": 11506
    },
    {
        "loss": 1.8601,
        "grad_norm": 3.4757742881774902,
        "learning_rate": 7.79243466797038e-05,
        "epoch": 0.857515463149266,
        "step": 11507
    },
    {
        "loss": 2.3459,
        "grad_norm": 2.5432655811309814,
        "learning_rate": 7.785572247491486e-05,
        "epoch": 0.8575899843505478,
        "step": 11508
    },
    {
        "loss": 2.6989,
        "grad_norm": 3.4063684940338135,
        "learning_rate": 7.778710923448296e-05,
        "epoch": 0.8576645055518295,
        "step": 11509
    },
    {
        "loss": 2.2761,
        "grad_norm": 2.40344500541687,
        "learning_rate": 7.771850699238059e-05,
        "epoch": 0.8577390267531113,
        "step": 11510
    },
    {
        "loss": 1.6264,
        "grad_norm": 5.01608943939209,
        "learning_rate": 7.764991578257499e-05,
        "epoch": 0.857813547954393,
        "step": 11511
    },
    {
        "loss": 2.5926,
        "grad_norm": 2.4688124656677246,
        "learning_rate": 7.758133563902776e-05,
        "epoch": 0.8578880691556748,
        "step": 11512
    },
    {
        "loss": 2.1297,
        "grad_norm": 2.7757105827331543,
        "learning_rate": 7.75127665956955e-05,
        "epoch": 0.8579625903569565,
        "step": 11513
    },
    {
        "loss": 2.4642,
        "grad_norm": 3.436962127685547,
        "learning_rate": 7.744420868652867e-05,
        "epoch": 0.8580371115582384,
        "step": 11514
    },
    {
        "loss": 2.7093,
        "grad_norm": 2.0772995948791504,
        "learning_rate": 7.737566194547278e-05,
        "epoch": 0.8581116327595201,
        "step": 11515
    },
    {
        "loss": 1.7768,
        "grad_norm": 3.4221999645233154,
        "learning_rate": 7.730712640646748e-05,
        "epoch": 0.8581861539608019,
        "step": 11516
    },
    {
        "loss": 3.0902,
        "grad_norm": 3.4367661476135254,
        "learning_rate": 7.723860210344682e-05,
        "epoch": 0.8582606751620836,
        "step": 11517
    },
    {
        "loss": 2.7579,
        "grad_norm": 2.3596479892730713,
        "learning_rate": 7.717008907033983e-05,
        "epoch": 0.8583351963633654,
        "step": 11518
    },
    {
        "loss": 3.0307,
        "grad_norm": 2.2360973358154297,
        "learning_rate": 7.7101587341069e-05,
        "epoch": 0.8584097175646471,
        "step": 11519
    },
    {
        "loss": 2.6764,
        "grad_norm": 1.7543636560440063,
        "learning_rate": 7.70330969495522e-05,
        "epoch": 0.8584842387659289,
        "step": 11520
    },
    {
        "loss": 1.7814,
        "grad_norm": 3.7955429553985596,
        "learning_rate": 7.69646179297011e-05,
        "epoch": 0.8585587599672106,
        "step": 11521
    },
    {
        "loss": 2.6354,
        "grad_norm": 1.3673973083496094,
        "learning_rate": 7.689615031542182e-05,
        "epoch": 0.8586332811684925,
        "step": 11522
    },
    {
        "loss": 1.8345,
        "grad_norm": 2.57234263420105,
        "learning_rate": 7.682769414061513e-05,
        "epoch": 0.8587078023697742,
        "step": 11523
    },
    {
        "loss": 2.3371,
        "grad_norm": 4.366091728210449,
        "learning_rate": 7.675924943917571e-05,
        "epoch": 0.858782323571056,
        "step": 11524
    },
    {
        "loss": 1.9167,
        "grad_norm": 3.8938562870025635,
        "learning_rate": 7.669081624499301e-05,
        "epoch": 0.8588568447723377,
        "step": 11525
    },
    {
        "loss": 2.2363,
        "grad_norm": 3.3247063159942627,
        "learning_rate": 7.662239459195042e-05,
        "epoch": 0.8589313659736195,
        "step": 11526
    },
    {
        "loss": 2.465,
        "grad_norm": 2.5189266204833984,
        "learning_rate": 7.655398451392573e-05,
        "epoch": 0.8590058871749012,
        "step": 11527
    },
    {
        "loss": 2.3635,
        "grad_norm": 2.2319512367248535,
        "learning_rate": 7.648558604479087e-05,
        "epoch": 0.859080408376183,
        "step": 11528
    },
    {
        "loss": 1.8871,
        "grad_norm": 2.6484813690185547,
        "learning_rate": 7.641719921841247e-05,
        "epoch": 0.8591549295774648,
        "step": 11529
    },
    {
        "loss": 2.7129,
        "grad_norm": 7.048717498779297,
        "learning_rate": 7.634882406865092e-05,
        "epoch": 0.8592294507787466,
        "step": 11530
    },
    {
        "loss": 3.2671,
        "grad_norm": 3.9288864135742188,
        "learning_rate": 7.628046062936101e-05,
        "epoch": 0.8593039719800283,
        "step": 11531
    },
    {
        "loss": 2.5875,
        "grad_norm": 2.117358446121216,
        "learning_rate": 7.621210893439154e-05,
        "epoch": 0.8593784931813101,
        "step": 11532
    },
    {
        "loss": 2.397,
        "grad_norm": 2.300520896911621,
        "learning_rate": 7.6143769017586e-05,
        "epoch": 0.8594530143825918,
        "step": 11533
    },
    {
        "loss": 2.5312,
        "grad_norm": 2.1662404537200928,
        "learning_rate": 7.607544091278154e-05,
        "epoch": 0.8595275355838736,
        "step": 11534
    },
    {
        "loss": 1.8636,
        "grad_norm": 3.471998929977417,
        "learning_rate": 7.600712465380955e-05,
        "epoch": 0.8596020567851553,
        "step": 11535
    },
    {
        "loss": 2.9231,
        "grad_norm": 2.0501537322998047,
        "learning_rate": 7.59388202744959e-05,
        "epoch": 0.8596765779864372,
        "step": 11536
    },
    {
        "loss": 2.4526,
        "grad_norm": 3.1706295013427734,
        "learning_rate": 7.587052780866004e-05,
        "epoch": 0.8597510991877189,
        "step": 11537
    },
    {
        "loss": 2.6367,
        "grad_norm": 1.744177222251892,
        "learning_rate": 7.58022472901162e-05,
        "epoch": 0.8598256203890007,
        "step": 11538
    },
    {
        "loss": 2.5766,
        "grad_norm": 2.588904857635498,
        "learning_rate": 7.57339787526718e-05,
        "epoch": 0.8599001415902824,
        "step": 11539
    },
    {
        "loss": 2.7188,
        "grad_norm": 2.243234157562256,
        "learning_rate": 7.566572223012925e-05,
        "epoch": 0.8599746627915642,
        "step": 11540
    },
    {
        "loss": 1.602,
        "grad_norm": 3.9959561824798584,
        "learning_rate": 7.559747775628436e-05,
        "epoch": 0.8600491839928459,
        "step": 11541
    },
    {
        "loss": 2.6352,
        "grad_norm": 1.7521916627883911,
        "learning_rate": 7.552924536492716e-05,
        "epoch": 0.8601237051941277,
        "step": 11542
    },
    {
        "loss": 2.2036,
        "grad_norm": 1.4011880159378052,
        "learning_rate": 7.546102508984199e-05,
        "epoch": 0.8601982263954095,
        "step": 11543
    },
    {
        "loss": 2.9002,
        "grad_norm": 2.528665542602539,
        "learning_rate": 7.539281696480677e-05,
        "epoch": 0.8602727475966913,
        "step": 11544
    },
    {
        "loss": 2.4819,
        "grad_norm": 3.579244613647461,
        "learning_rate": 7.532462102359348e-05,
        "epoch": 0.8603472687979731,
        "step": 11545
    },
    {
        "loss": 2.5524,
        "grad_norm": 3.525496482849121,
        "learning_rate": 7.525643729996841e-05,
        "epoch": 0.8604217899992548,
        "step": 11546
    },
    {
        "loss": 2.5169,
        "grad_norm": 2.8162295818328857,
        "learning_rate": 7.518826582769144e-05,
        "epoch": 0.8604963112005366,
        "step": 11547
    },
    {
        "loss": 2.4268,
        "grad_norm": 3.2620983123779297,
        "learning_rate": 7.512010664051634e-05,
        "epoch": 0.8605708324018183,
        "step": 11548
    },
    {
        "loss": 2.2112,
        "grad_norm": 3.015631914138794,
        "learning_rate": 7.505195977219137e-05,
        "epoch": 0.8606453536031001,
        "step": 11549
    },
    {
        "loss": 2.4647,
        "grad_norm": 2.285616636276245,
        "learning_rate": 7.498382525645776e-05,
        "epoch": 0.8607198748043818,
        "step": 11550
    },
    {
        "loss": 2.4704,
        "grad_norm": 2.896498203277588,
        "learning_rate": 7.491570312705152e-05,
        "epoch": 0.8607943960056637,
        "step": 11551
    },
    {
        "loss": 2.8296,
        "grad_norm": 2.819831609725952,
        "learning_rate": 7.484759341770187e-05,
        "epoch": 0.8608689172069454,
        "step": 11552
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.7370219230651855,
        "learning_rate": 7.477949616213244e-05,
        "epoch": 0.8609434384082272,
        "step": 11553
    },
    {
        "loss": 1.4228,
        "grad_norm": 4.3813276290893555,
        "learning_rate": 7.471141139406028e-05,
        "epoch": 0.8610179596095089,
        "step": 11554
    },
    {
        "loss": 2.1573,
        "grad_norm": 3.3792495727539062,
        "learning_rate": 7.464333914719624e-05,
        "epoch": 0.8610924808107907,
        "step": 11555
    },
    {
        "loss": 2.0257,
        "grad_norm": 3.3020942211151123,
        "learning_rate": 7.45752794552454e-05,
        "epoch": 0.8611670020120724,
        "step": 11556
    },
    {
        "loss": 1.4892,
        "grad_norm": 4.062227249145508,
        "learning_rate": 7.450723235190605e-05,
        "epoch": 0.8612415232133542,
        "step": 11557
    },
    {
        "loss": 2.2072,
        "grad_norm": 4.048569679260254,
        "learning_rate": 7.4439197870871e-05,
        "epoch": 0.8613160444146359,
        "step": 11558
    },
    {
        "loss": 2.6367,
        "grad_norm": 1.6626938581466675,
        "learning_rate": 7.437117604582573e-05,
        "epoch": 0.8613905656159178,
        "step": 11559
    },
    {
        "loss": 1.9974,
        "grad_norm": 3.783688545227051,
        "learning_rate": 7.430316691045053e-05,
        "epoch": 0.8614650868171995,
        "step": 11560
    },
    {
        "loss": 2.119,
        "grad_norm": 2.8436880111694336,
        "learning_rate": 7.423517049841879e-05,
        "epoch": 0.8615396080184813,
        "step": 11561
    },
    {
        "loss": 2.2178,
        "grad_norm": 2.806828737258911,
        "learning_rate": 7.41671868433978e-05,
        "epoch": 0.861614129219763,
        "step": 11562
    },
    {
        "loss": 2.2326,
        "grad_norm": 1.6034929752349854,
        "learning_rate": 7.409921597904832e-05,
        "epoch": 0.8616886504210448,
        "step": 11563
    },
    {
        "loss": 2.2811,
        "grad_norm": 3.9766769409179688,
        "learning_rate": 7.403125793902524e-05,
        "epoch": 0.8617631716223265,
        "step": 11564
    },
    {
        "loss": 2.1498,
        "grad_norm": 3.2049942016601562,
        "learning_rate": 7.396331275697652e-05,
        "epoch": 0.8618376928236083,
        "step": 11565
    },
    {
        "loss": 2.6139,
        "grad_norm": 2.8845372200012207,
        "learning_rate": 7.389538046654433e-05,
        "epoch": 0.86191221402489,
        "step": 11566
    },
    {
        "loss": 2.5738,
        "grad_norm": 3.2949113845825195,
        "learning_rate": 7.382746110136405e-05,
        "epoch": 0.8619867352261719,
        "step": 11567
    },
    {
        "loss": 2.3176,
        "grad_norm": 3.3875865936279297,
        "learning_rate": 7.375955469506462e-05,
        "epoch": 0.8620612564274536,
        "step": 11568
    },
    {
        "loss": 2.7025,
        "grad_norm": 2.7678284645080566,
        "learning_rate": 7.369166128126913e-05,
        "epoch": 0.8621357776287354,
        "step": 11569
    },
    {
        "loss": 2.6027,
        "grad_norm": 5.599813461303711,
        "learning_rate": 7.362378089359332e-05,
        "epoch": 0.8622102988300171,
        "step": 11570
    },
    {
        "loss": 1.8493,
        "grad_norm": 4.065565586090088,
        "learning_rate": 7.355591356564735e-05,
        "epoch": 0.8622848200312989,
        "step": 11571
    },
    {
        "loss": 2.5489,
        "grad_norm": 1.9185823202133179,
        "learning_rate": 7.348805933103447e-05,
        "epoch": 0.8623593412325806,
        "step": 11572
    },
    {
        "loss": 2.5596,
        "grad_norm": 2.6966567039489746,
        "learning_rate": 7.342021822335139e-05,
        "epoch": 0.8624338624338624,
        "step": 11573
    },
    {
        "loss": 2.7877,
        "grad_norm": 2.179560422897339,
        "learning_rate": 7.335239027618872e-05,
        "epoch": 0.8625083836351441,
        "step": 11574
    },
    {
        "loss": 2.739,
        "grad_norm": 3.283159017562866,
        "learning_rate": 7.328457552313003e-05,
        "epoch": 0.862582904836426,
        "step": 11575
    },
    {
        "loss": 2.4942,
        "grad_norm": 3.330127716064453,
        "learning_rate": 7.321677399775289e-05,
        "epoch": 0.8626574260377077,
        "step": 11576
    },
    {
        "loss": 1.6392,
        "grad_norm": 4.028313636779785,
        "learning_rate": 7.314898573362794e-05,
        "epoch": 0.8627319472389895,
        "step": 11577
    },
    {
        "loss": 2.1302,
        "grad_norm": 3.9653189182281494,
        "learning_rate": 7.308121076431935e-05,
        "epoch": 0.8628064684402713,
        "step": 11578
    },
    {
        "loss": 2.8117,
        "grad_norm": 3.5190155506134033,
        "learning_rate": 7.301344912338459e-05,
        "epoch": 0.862880989641553,
        "step": 11579
    },
    {
        "loss": 2.1744,
        "grad_norm": 3.3744702339172363,
        "learning_rate": 7.294570084437494e-05,
        "epoch": 0.8629555108428348,
        "step": 11580
    },
    {
        "loss": 2.5545,
        "grad_norm": 3.4877896308898926,
        "learning_rate": 7.287796596083469e-05,
        "epoch": 0.8630300320441165,
        "step": 11581
    },
    {
        "loss": 2.8843,
        "grad_norm": 3.262629508972168,
        "learning_rate": 7.281024450630153e-05,
        "epoch": 0.8631045532453984,
        "step": 11582
    },
    {
        "loss": 2.5007,
        "grad_norm": 2.0939786434173584,
        "learning_rate": 7.274253651430651e-05,
        "epoch": 0.8631790744466801,
        "step": 11583
    },
    {
        "loss": 1.8265,
        "grad_norm": 4.010287284851074,
        "learning_rate": 7.267484201837427e-05,
        "epoch": 0.8632535956479619,
        "step": 11584
    },
    {
        "loss": 2.5329,
        "grad_norm": 2.5088393688201904,
        "learning_rate": 7.260716105202239e-05,
        "epoch": 0.8633281168492436,
        "step": 11585
    },
    {
        "loss": 1.9502,
        "grad_norm": 3.3204362392425537,
        "learning_rate": 7.253949364876212e-05,
        "epoch": 0.8634026380505254,
        "step": 11586
    },
    {
        "loss": 2.9183,
        "grad_norm": 3.34525990486145,
        "learning_rate": 7.247183984209769e-05,
        "epoch": 0.8634771592518071,
        "step": 11587
    },
    {
        "loss": 2.5411,
        "grad_norm": 2.2899882793426514,
        "learning_rate": 7.24041996655266e-05,
        "epoch": 0.8635516804530889,
        "step": 11588
    },
    {
        "loss": 2.4009,
        "grad_norm": 2.4109551906585693,
        "learning_rate": 7.233657315254014e-05,
        "epoch": 0.8636262016543707,
        "step": 11589
    },
    {
        "loss": 1.5811,
        "grad_norm": 4.265384197235107,
        "learning_rate": 7.226896033662184e-05,
        "epoch": 0.8637007228556525,
        "step": 11590
    },
    {
        "loss": 2.4296,
        "grad_norm": 2.588559627532959,
        "learning_rate": 7.220136125124941e-05,
        "epoch": 0.8637752440569342,
        "step": 11591
    },
    {
        "loss": 2.7355,
        "grad_norm": 2.5469441413879395,
        "learning_rate": 7.213377592989328e-05,
        "epoch": 0.863849765258216,
        "step": 11592
    },
    {
        "loss": 2.5609,
        "grad_norm": 1.8293637037277222,
        "learning_rate": 7.206620440601699e-05,
        "epoch": 0.8639242864594977,
        "step": 11593
    },
    {
        "loss": 2.3009,
        "grad_norm": 2.0864593982696533,
        "learning_rate": 7.199864671307767e-05,
        "epoch": 0.8639988076607795,
        "step": 11594
    },
    {
        "loss": 2.146,
        "grad_norm": 3.49456787109375,
        "learning_rate": 7.193110288452525e-05,
        "epoch": 0.8640733288620612,
        "step": 11595
    },
    {
        "loss": 2.0444,
        "grad_norm": 3.707531213760376,
        "learning_rate": 7.186357295380273e-05,
        "epoch": 0.864147850063343,
        "step": 11596
    },
    {
        "loss": 3.1742,
        "grad_norm": 3.1941404342651367,
        "learning_rate": 7.179605695434668e-05,
        "epoch": 0.8642223712646248,
        "step": 11597
    },
    {
        "loss": 2.6113,
        "grad_norm": 3.18169903755188,
        "learning_rate": 7.172855491958632e-05,
        "epoch": 0.8642968924659066,
        "step": 11598
    },
    {
        "loss": 2.8896,
        "grad_norm": 2.5310280323028564,
        "learning_rate": 7.166106688294401e-05,
        "epoch": 0.8643714136671883,
        "step": 11599
    },
    {
        "loss": 2.1582,
        "grad_norm": 4.148640155792236,
        "learning_rate": 7.159359287783567e-05,
        "epoch": 0.8644459348684701,
        "step": 11600
    },
    {
        "loss": 1.834,
        "grad_norm": 1.889324426651001,
        "learning_rate": 7.152613293766936e-05,
        "epoch": 0.8645204560697518,
        "step": 11601
    },
    {
        "loss": 2.62,
        "grad_norm": 2.4314136505126953,
        "learning_rate": 7.14586870958471e-05,
        "epoch": 0.8645949772710336,
        "step": 11602
    },
    {
        "loss": 2.2138,
        "grad_norm": 3.240511178970337,
        "learning_rate": 7.139125538576327e-05,
        "epoch": 0.8646694984723153,
        "step": 11603
    },
    {
        "loss": 1.9724,
        "grad_norm": 3.717740535736084,
        "learning_rate": 7.132383784080583e-05,
        "epoch": 0.8647440196735972,
        "step": 11604
    },
    {
        "loss": 1.9515,
        "grad_norm": 3.5874552726745605,
        "learning_rate": 7.125643449435519e-05,
        "epoch": 0.8648185408748789,
        "step": 11605
    },
    {
        "loss": 2.4312,
        "grad_norm": 2.2984619140625,
        "learning_rate": 7.118904537978489e-05,
        "epoch": 0.8648930620761607,
        "step": 11606
    },
    {
        "loss": 2.4555,
        "grad_norm": 2.2932028770446777,
        "learning_rate": 7.112167053046172e-05,
        "epoch": 0.8649675832774424,
        "step": 11607
    },
    {
        "loss": 2.7566,
        "grad_norm": 2.942676544189453,
        "learning_rate": 7.105430997974496e-05,
        "epoch": 0.8650421044787242,
        "step": 11608
    },
    {
        "loss": 2.4622,
        "grad_norm": 2.8068997859954834,
        "learning_rate": 7.098696376098736e-05,
        "epoch": 0.8651166256800059,
        "step": 11609
    },
    {
        "loss": 1.9318,
        "grad_norm": 3.2551445960998535,
        "learning_rate": 7.091963190753372e-05,
        "epoch": 0.8651911468812877,
        "step": 11610
    },
    {
        "loss": 2.3948,
        "grad_norm": 3.778256893157959,
        "learning_rate": 7.08523144527227e-05,
        "epoch": 0.8652656680825694,
        "step": 11611
    },
    {
        "loss": 1.9858,
        "grad_norm": 2.1453850269317627,
        "learning_rate": 7.078501142988515e-05,
        "epoch": 0.8653401892838513,
        "step": 11612
    },
    {
        "loss": 2.2657,
        "grad_norm": 2.589951276779175,
        "learning_rate": 7.071772287234493e-05,
        "epoch": 0.865414710485133,
        "step": 11613
    },
    {
        "loss": 1.7743,
        "grad_norm": 3.433375358581543,
        "learning_rate": 7.065044881341906e-05,
        "epoch": 0.8654892316864148,
        "step": 11614
    },
    {
        "loss": 2.106,
        "grad_norm": 3.26623272895813,
        "learning_rate": 7.058318928641701e-05,
        "epoch": 0.8655637528876966,
        "step": 11615
    },
    {
        "loss": 1.9861,
        "grad_norm": 3.242947816848755,
        "learning_rate": 7.051594432464101e-05,
        "epoch": 0.8656382740889783,
        "step": 11616
    },
    {
        "loss": 2.61,
        "grad_norm": 3.0818891525268555,
        "learning_rate": 7.044871396138653e-05,
        "epoch": 0.8657127952902601,
        "step": 11617
    },
    {
        "loss": 2.0062,
        "grad_norm": 3.5373482704162598,
        "learning_rate": 7.038149822994138e-05,
        "epoch": 0.8657873164915418,
        "step": 11618
    },
    {
        "loss": 2.9064,
        "grad_norm": 2.543517827987671,
        "learning_rate": 7.031429716358617e-05,
        "epoch": 0.8658618376928237,
        "step": 11619
    },
    {
        "loss": 2.7399,
        "grad_norm": 2.673229932785034,
        "learning_rate": 7.02471107955947e-05,
        "epoch": 0.8659363588941054,
        "step": 11620
    },
    {
        "loss": 2.4984,
        "grad_norm": 4.782303810119629,
        "learning_rate": 7.017993915923262e-05,
        "epoch": 0.8660108800953872,
        "step": 11621
    },
    {
        "loss": 2.0822,
        "grad_norm": 2.637590169906616,
        "learning_rate": 7.011278228775927e-05,
        "epoch": 0.8660854012966689,
        "step": 11622
    },
    {
        "loss": 2.137,
        "grad_norm": 3.0788962841033936,
        "learning_rate": 7.004564021442584e-05,
        "epoch": 0.8661599224979507,
        "step": 11623
    },
    {
        "loss": 2.3582,
        "grad_norm": 3.5002658367156982,
        "learning_rate": 6.99785129724769e-05,
        "epoch": 0.8662344436992324,
        "step": 11624
    },
    {
        "loss": 2.2285,
        "grad_norm": 2.1763217449188232,
        "learning_rate": 6.99114005951492e-05,
        "epoch": 0.8663089649005142,
        "step": 11625
    },
    {
        "loss": 1.81,
        "grad_norm": 3.725348472595215,
        "learning_rate": 6.984430311567215e-05,
        "epoch": 0.8663834861017959,
        "step": 11626
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.9898343086242676,
        "learning_rate": 6.977722056726819e-05,
        "epoch": 0.8664580073030778,
        "step": 11627
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.0337061882019043,
        "learning_rate": 6.971015298315191e-05,
        "epoch": 0.8665325285043595,
        "step": 11628
    },
    {
        "loss": 1.6968,
        "grad_norm": 3.13604998588562,
        "learning_rate": 6.964310039653069e-05,
        "epoch": 0.8666070497056413,
        "step": 11629
    },
    {
        "loss": 1.858,
        "grad_norm": 2.8550994396209717,
        "learning_rate": 6.957606284060434e-05,
        "epoch": 0.866681570906923,
        "step": 11630
    },
    {
        "loss": 2.7096,
        "grad_norm": 2.636371612548828,
        "learning_rate": 6.950904034856561e-05,
        "epoch": 0.8667560921082048,
        "step": 11631
    },
    {
        "loss": 2.8579,
        "grad_norm": 1.9828827381134033,
        "learning_rate": 6.94420329535994e-05,
        "epoch": 0.8668306133094865,
        "step": 11632
    },
    {
        "loss": 2.0769,
        "grad_norm": 3.0407960414886475,
        "learning_rate": 6.937504068888323e-05,
        "epoch": 0.8669051345107683,
        "step": 11633
    },
    {
        "loss": 2.6056,
        "grad_norm": 1.861818552017212,
        "learning_rate": 6.930806358758708e-05,
        "epoch": 0.86697965571205,
        "step": 11634
    },
    {
        "loss": 2.1743,
        "grad_norm": 2.834414482116699,
        "learning_rate": 6.924110168287372e-05,
        "epoch": 0.8670541769133319,
        "step": 11635
    },
    {
        "loss": 2.3292,
        "grad_norm": 3.72356915473938,
        "learning_rate": 6.917415500789797e-05,
        "epoch": 0.8671286981146136,
        "step": 11636
    },
    {
        "loss": 2.5996,
        "grad_norm": 3.4774937629699707,
        "learning_rate": 6.910722359580755e-05,
        "epoch": 0.8672032193158954,
        "step": 11637
    },
    {
        "loss": 2.6473,
        "grad_norm": 2.397305965423584,
        "learning_rate": 6.904030747974229e-05,
        "epoch": 0.8672777405171771,
        "step": 11638
    },
    {
        "loss": 1.6238,
        "grad_norm": 3.662886142730713,
        "learning_rate": 6.897340669283438e-05,
        "epoch": 0.8673522617184589,
        "step": 11639
    },
    {
        "loss": 2.808,
        "grad_norm": 3.623764991760254,
        "learning_rate": 6.8906521268209e-05,
        "epoch": 0.8674267829197406,
        "step": 11640
    },
    {
        "loss": 2.3541,
        "grad_norm": 2.540679693222046,
        "learning_rate": 6.883965123898281e-05,
        "epoch": 0.8675013041210224,
        "step": 11641
    },
    {
        "loss": 1.7029,
        "grad_norm": 3.113234281539917,
        "learning_rate": 6.877279663826573e-05,
        "epoch": 0.8675758253223042,
        "step": 11642
    },
    {
        "loss": 2.6153,
        "grad_norm": 2.872720241546631,
        "learning_rate": 6.870595749915952e-05,
        "epoch": 0.867650346523586,
        "step": 11643
    },
    {
        "loss": 1.7849,
        "grad_norm": 3.2472705841064453,
        "learning_rate": 6.863913385475831e-05,
        "epoch": 0.8677248677248677,
        "step": 11644
    },
    {
        "loss": 2.1346,
        "grad_norm": 3.2723984718322754,
        "learning_rate": 6.857232573814896e-05,
        "epoch": 0.8677993889261495,
        "step": 11645
    },
    {
        "loss": 2.4135,
        "grad_norm": 3.5908327102661133,
        "learning_rate": 6.850553318241002e-05,
        "epoch": 0.8678739101274312,
        "step": 11646
    },
    {
        "loss": 2.3854,
        "grad_norm": 3.4327690601348877,
        "learning_rate": 6.8438756220613e-05,
        "epoch": 0.867948431328713,
        "step": 11647
    },
    {
        "loss": 2.357,
        "grad_norm": 2.030113458633423,
        "learning_rate": 6.83719948858212e-05,
        "epoch": 0.8680229525299947,
        "step": 11648
    },
    {
        "loss": 2.5768,
        "grad_norm": 2.979386806488037,
        "learning_rate": 6.830524921109037e-05,
        "epoch": 0.8680974737312765,
        "step": 11649
    },
    {
        "loss": 2.5462,
        "grad_norm": 1.9211889505386353,
        "learning_rate": 6.82385192294683e-05,
        "epoch": 0.8681719949325584,
        "step": 11650
    },
    {
        "loss": 2.0103,
        "grad_norm": 2.882096529006958,
        "learning_rate": 6.81718049739955e-05,
        "epoch": 0.8682465161338401,
        "step": 11651
    },
    {
        "loss": 1.7116,
        "grad_norm": 4.109109401702881,
        "learning_rate": 6.810510647770422e-05,
        "epoch": 0.8683210373351219,
        "step": 11652
    },
    {
        "loss": 2.7631,
        "grad_norm": 2.134925603866577,
        "learning_rate": 6.80384237736191e-05,
        "epoch": 0.8683955585364036,
        "step": 11653
    },
    {
        "loss": 2.4492,
        "grad_norm": 3.0478694438934326,
        "learning_rate": 6.797175689475677e-05,
        "epoch": 0.8684700797376854,
        "step": 11654
    },
    {
        "loss": 2.4887,
        "grad_norm": 3.3134913444519043,
        "learning_rate": 6.790510587412644e-05,
        "epoch": 0.8685446009389671,
        "step": 11655
    },
    {
        "loss": 2.9647,
        "grad_norm": 3.1941254138946533,
        "learning_rate": 6.783847074472914e-05,
        "epoch": 0.868619122140249,
        "step": 11656
    },
    {
        "loss": 2.8709,
        "grad_norm": 3.845224618911743,
        "learning_rate": 6.77718515395579e-05,
        "epoch": 0.8686936433415307,
        "step": 11657
    },
    {
        "loss": 2.836,
        "grad_norm": 4.014120578765869,
        "learning_rate": 6.770524829159836e-05,
        "epoch": 0.8687681645428125,
        "step": 11658
    },
    {
        "loss": 2.2602,
        "grad_norm": 5.724945545196533,
        "learning_rate": 6.763866103382771e-05,
        "epoch": 0.8688426857440942,
        "step": 11659
    },
    {
        "loss": 2.7705,
        "grad_norm": 2.78594970703125,
        "learning_rate": 6.75720897992159e-05,
        "epoch": 0.868917206945376,
        "step": 11660
    },
    {
        "loss": 1.7491,
        "grad_norm": 2.579533100128174,
        "learning_rate": 6.750553462072395e-05,
        "epoch": 0.8689917281466577,
        "step": 11661
    },
    {
        "loss": 2.3375,
        "grad_norm": 2.3857293128967285,
        "learning_rate": 6.743899553130595e-05,
        "epoch": 0.8690662493479395,
        "step": 11662
    },
    {
        "loss": 2.1727,
        "grad_norm": 3.5047240257263184,
        "learning_rate": 6.737247256390742e-05,
        "epoch": 0.8691407705492212,
        "step": 11663
    },
    {
        "loss": 2.2726,
        "grad_norm": 3.4737050533294678,
        "learning_rate": 6.730596575146593e-05,
        "epoch": 0.869215291750503,
        "step": 11664
    },
    {
        "loss": 2.1283,
        "grad_norm": 4.307279586791992,
        "learning_rate": 6.723947512691149e-05,
        "epoch": 0.8692898129517848,
        "step": 11665
    },
    {
        "loss": 2.5671,
        "grad_norm": 3.3833260536193848,
        "learning_rate": 6.717300072316559e-05,
        "epoch": 0.8693643341530666,
        "step": 11666
    },
    {
        "loss": 2.5448,
        "grad_norm": 2.2837250232696533,
        "learning_rate": 6.710654257314177e-05,
        "epoch": 0.8694388553543483,
        "step": 11667
    },
    {
        "loss": 3.0297,
        "grad_norm": 2.4166884422302246,
        "learning_rate": 6.704010070974594e-05,
        "epoch": 0.8695133765556301,
        "step": 11668
    },
    {
        "loss": 2.0053,
        "grad_norm": 3.4435696601867676,
        "learning_rate": 6.697367516587545e-05,
        "epoch": 0.8695878977569118,
        "step": 11669
    },
    {
        "loss": 3.3915,
        "grad_norm": 4.381388187408447,
        "learning_rate": 6.690726597441967e-05,
        "epoch": 0.8696624189581936,
        "step": 11670
    },
    {
        "loss": 1.7482,
        "grad_norm": 4.499790668487549,
        "learning_rate": 6.684087316826036e-05,
        "epoch": 0.8697369401594753,
        "step": 11671
    },
    {
        "loss": 2.3727,
        "grad_norm": 1.9825005531311035,
        "learning_rate": 6.677449678027022e-05,
        "epoch": 0.8698114613607572,
        "step": 11672
    },
    {
        "loss": 1.1921,
        "grad_norm": 5.773055076599121,
        "learning_rate": 6.670813684331478e-05,
        "epoch": 0.8698859825620389,
        "step": 11673
    },
    {
        "loss": 1.998,
        "grad_norm": 3.021199941635132,
        "learning_rate": 6.664179339025076e-05,
        "epoch": 0.8699605037633207,
        "step": 11674
    },
    {
        "loss": 2.9696,
        "grad_norm": 2.6391658782958984,
        "learning_rate": 6.657546645392723e-05,
        "epoch": 0.8700350249646024,
        "step": 11675
    },
    {
        "loss": 1.8658,
        "grad_norm": 3.9792959690093994,
        "learning_rate": 6.650915606718473e-05,
        "epoch": 0.8701095461658842,
        "step": 11676
    },
    {
        "loss": 2.1899,
        "grad_norm": 3.1372742652893066,
        "learning_rate": 6.644286226285552e-05,
        "epoch": 0.8701840673671659,
        "step": 11677
    },
    {
        "loss": 2.2421,
        "grad_norm": 2.6909000873565674,
        "learning_rate": 6.637658507376412e-05,
        "epoch": 0.8702585885684477,
        "step": 11678
    },
    {
        "loss": 2.5084,
        "grad_norm": 3.373994827270508,
        "learning_rate": 6.63103245327263e-05,
        "epoch": 0.8703331097697294,
        "step": 11679
    },
    {
        "loss": 1.9199,
        "grad_norm": 3.850921869277954,
        "learning_rate": 6.624408067255017e-05,
        "epoch": 0.8704076309710113,
        "step": 11680
    },
    {
        "loss": 1.5047,
        "grad_norm": 2.981980562210083,
        "learning_rate": 6.617785352603473e-05,
        "epoch": 0.870482152172293,
        "step": 11681
    },
    {
        "loss": 1.8213,
        "grad_norm": 3.631535768508911,
        "learning_rate": 6.611164312597162e-05,
        "epoch": 0.8705566733735748,
        "step": 11682
    },
    {
        "loss": 2.6194,
        "grad_norm": 2.1214282512664795,
        "learning_rate": 6.604544950514363e-05,
        "epoch": 0.8706311945748565,
        "step": 11683
    },
    {
        "loss": 2.561,
        "grad_norm": 2.2459986209869385,
        "learning_rate": 6.597927269632525e-05,
        "epoch": 0.8707057157761383,
        "step": 11684
    },
    {
        "loss": 2.4985,
        "grad_norm": 2.8586130142211914,
        "learning_rate": 6.591311273228304e-05,
        "epoch": 0.8707802369774201,
        "step": 11685
    },
    {
        "loss": 2.3555,
        "grad_norm": 2.4597034454345703,
        "learning_rate": 6.584696964577485e-05,
        "epoch": 0.8708547581787018,
        "step": 11686
    },
    {
        "loss": 2.4153,
        "grad_norm": 2.3565502166748047,
        "learning_rate": 6.57808434695502e-05,
        "epoch": 0.8709292793799837,
        "step": 11687
    },
    {
        "loss": 2.434,
        "grad_norm": 2.184110641479492,
        "learning_rate": 6.57147342363505e-05,
        "epoch": 0.8710038005812654,
        "step": 11688
    },
    {
        "loss": 2.0212,
        "grad_norm": 3.156688690185547,
        "learning_rate": 6.56486419789085e-05,
        "epoch": 0.8710783217825472,
        "step": 11689
    },
    {
        "loss": 1.4832,
        "grad_norm": 3.212007999420166,
        "learning_rate": 6.558256672994853e-05,
        "epoch": 0.8711528429838289,
        "step": 11690
    },
    {
        "loss": 2.4275,
        "grad_norm": 2.654477119445801,
        "learning_rate": 6.5516508522187e-05,
        "epoch": 0.8712273641851107,
        "step": 11691
    },
    {
        "loss": 2.3021,
        "grad_norm": 2.100206136703491,
        "learning_rate": 6.545046738833096e-05,
        "epoch": 0.8713018853863924,
        "step": 11692
    },
    {
        "loss": 2.7776,
        "grad_norm": 1.6430301666259766,
        "learning_rate": 6.53844433610799e-05,
        "epoch": 0.8713764065876742,
        "step": 11693
    },
    {
        "loss": 2.3136,
        "grad_norm": 2.2654569149017334,
        "learning_rate": 6.531843647312441e-05,
        "epoch": 0.8714509277889559,
        "step": 11694
    },
    {
        "loss": 2.0647,
        "grad_norm": 2.007319688796997,
        "learning_rate": 6.525244675714647e-05,
        "epoch": 0.8715254489902378,
        "step": 11695
    },
    {
        "loss": 2.3005,
        "grad_norm": 3.733464479446411,
        "learning_rate": 6.518647424582005e-05,
        "epoch": 0.8715999701915195,
        "step": 11696
    },
    {
        "loss": 1.8922,
        "grad_norm": 4.998372554779053,
        "learning_rate": 6.512051897181001e-05,
        "epoch": 0.8716744913928013,
        "step": 11697
    },
    {
        "loss": 2.1679,
        "grad_norm": 3.488298177719116,
        "learning_rate": 6.505458096777326e-05,
        "epoch": 0.871749012594083,
        "step": 11698
    },
    {
        "loss": 2.3607,
        "grad_norm": 3.1019089221954346,
        "learning_rate": 6.498866026635771e-05,
        "epoch": 0.8718235337953648,
        "step": 11699
    },
    {
        "loss": 2.3617,
        "grad_norm": 3.9560701847076416,
        "learning_rate": 6.492275690020285e-05,
        "epoch": 0.8718980549966465,
        "step": 11700
    },
    {
        "loss": 2.3075,
        "grad_norm": 2.774402379989624,
        "learning_rate": 6.485687090193948e-05,
        "epoch": 0.8719725761979283,
        "step": 11701
    },
    {
        "loss": 2.3272,
        "grad_norm": 2.8187873363494873,
        "learning_rate": 6.47910023041902e-05,
        "epoch": 0.87204709739921,
        "step": 11702
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.313537836074829,
        "learning_rate": 6.472515113956854e-05,
        "epoch": 0.8721216186004919,
        "step": 11703
    },
    {
        "loss": 2.3337,
        "grad_norm": 3.7625157833099365,
        "learning_rate": 6.465931744067957e-05,
        "epoch": 0.8721961398017736,
        "step": 11704
    },
    {
        "loss": 2.2191,
        "grad_norm": 3.496523857116699,
        "learning_rate": 6.45935012401196e-05,
        "epoch": 0.8722706610030554,
        "step": 11705
    },
    {
        "loss": 2.6296,
        "grad_norm": 3.94012188911438,
        "learning_rate": 6.452770257047665e-05,
        "epoch": 0.8723451822043371,
        "step": 11706
    },
    {
        "loss": 1.3909,
        "grad_norm": 2.9727673530578613,
        "learning_rate": 6.44619214643295e-05,
        "epoch": 0.8724197034056189,
        "step": 11707
    },
    {
        "loss": 2.3191,
        "grad_norm": 2.4710214138031006,
        "learning_rate": 6.43961579542488e-05,
        "epoch": 0.8724942246069006,
        "step": 11708
    },
    {
        "loss": 2.2302,
        "grad_norm": 2.6877264976501465,
        "learning_rate": 6.433041207279614e-05,
        "epoch": 0.8725687458081824,
        "step": 11709
    },
    {
        "loss": 2.2479,
        "grad_norm": 3.857346534729004,
        "learning_rate": 6.426468385252427e-05,
        "epoch": 0.8726432670094642,
        "step": 11710
    },
    {
        "loss": 1.2926,
        "grad_norm": 2.837026596069336,
        "learning_rate": 6.419897332597778e-05,
        "epoch": 0.872717788210746,
        "step": 11711
    },
    {
        "loss": 2.6907,
        "grad_norm": 2.488597869873047,
        "learning_rate": 6.41332805256916e-05,
        "epoch": 0.8727923094120277,
        "step": 11712
    },
    {
        "loss": 2.5206,
        "grad_norm": 2.1295993328094482,
        "learning_rate": 6.40676054841928e-05,
        "epoch": 0.8728668306133095,
        "step": 11713
    },
    {
        "loss": 2.6906,
        "grad_norm": 3.2594594955444336,
        "learning_rate": 6.400194823399905e-05,
        "epoch": 0.8729413518145912,
        "step": 11714
    },
    {
        "loss": 2.5597,
        "grad_norm": 3.3828632831573486,
        "learning_rate": 6.393630880761933e-05,
        "epoch": 0.873015873015873,
        "step": 11715
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.9745956659317017,
        "learning_rate": 6.387068723755409e-05,
        "epoch": 0.8730903942171547,
        "step": 11716
    },
    {
        "loss": 2.4484,
        "grad_norm": 3.2170968055725098,
        "learning_rate": 6.38050835562945e-05,
        "epoch": 0.8731649154184365,
        "step": 11717
    },
    {
        "loss": 2.7053,
        "grad_norm": 3.121913433074951,
        "learning_rate": 6.373949779632333e-05,
        "epoch": 0.8732394366197183,
        "step": 11718
    },
    {
        "loss": 2.9904,
        "grad_norm": 2.4315152168273926,
        "learning_rate": 6.367392999011409e-05,
        "epoch": 0.8733139578210001,
        "step": 11719
    },
    {
        "loss": 2.0203,
        "grad_norm": 3.2827863693237305,
        "learning_rate": 6.360838017013156e-05,
        "epoch": 0.8733884790222819,
        "step": 11720
    },
    {
        "loss": 1.1695,
        "grad_norm": 2.5901455879211426,
        "learning_rate": 6.354284836883151e-05,
        "epoch": 0.8734630002235636,
        "step": 11721
    },
    {
        "loss": 2.0699,
        "grad_norm": 2.8437340259552,
        "learning_rate": 6.347733461866119e-05,
        "epoch": 0.8735375214248454,
        "step": 11722
    },
    {
        "loss": 1.981,
        "grad_norm": 3.480356216430664,
        "learning_rate": 6.341183895205815e-05,
        "epoch": 0.8736120426261271,
        "step": 11723
    },
    {
        "loss": 2.3473,
        "grad_norm": 2.3692524433135986,
        "learning_rate": 6.334636140145181e-05,
        "epoch": 0.873686563827409,
        "step": 11724
    },
    {
        "loss": 1.9079,
        "grad_norm": 1.8972445726394653,
        "learning_rate": 6.328090199926199e-05,
        "epoch": 0.8737610850286907,
        "step": 11725
    },
    {
        "loss": 2.5211,
        "grad_norm": 2.82075572013855,
        "learning_rate": 6.321546077790003e-05,
        "epoch": 0.8738356062299725,
        "step": 11726
    },
    {
        "loss": 2.4392,
        "grad_norm": 3.2262187004089355,
        "learning_rate": 6.31500377697679e-05,
        "epoch": 0.8739101274312542,
        "step": 11727
    },
    {
        "loss": 2.524,
        "grad_norm": 2.9264323711395264,
        "learning_rate": 6.308463300725854e-05,
        "epoch": 0.873984648632536,
        "step": 11728
    },
    {
        "loss": 2.6571,
        "grad_norm": 2.648012638092041,
        "learning_rate": 6.301924652275628e-05,
        "epoch": 0.8740591698338177,
        "step": 11729
    },
    {
        "loss": 2.3002,
        "grad_norm": 3.0579617023468018,
        "learning_rate": 6.295387834863581e-05,
        "epoch": 0.8741336910350995,
        "step": 11730
    },
    {
        "loss": 2.6184,
        "grad_norm": 3.341796398162842,
        "learning_rate": 6.288852851726345e-05,
        "epoch": 0.8742082122363812,
        "step": 11731
    },
    {
        "loss": 2.1213,
        "grad_norm": 3.145171642303467,
        "learning_rate": 6.282319706099555e-05,
        "epoch": 0.874282733437663,
        "step": 11732
    },
    {
        "loss": 2.6436,
        "grad_norm": 2.5064570903778076,
        "learning_rate": 6.275788401218021e-05,
        "epoch": 0.8743572546389448,
        "step": 11733
    },
    {
        "loss": 2.1399,
        "grad_norm": 3.019327163696289,
        "learning_rate": 6.269258940315595e-05,
        "epoch": 0.8744317758402266,
        "step": 11734
    },
    {
        "loss": 2.3009,
        "grad_norm": 3.0673599243164062,
        "learning_rate": 6.262731326625211e-05,
        "epoch": 0.8745062970415083,
        "step": 11735
    },
    {
        "loss": 1.6616,
        "grad_norm": 2.691183090209961,
        "learning_rate": 6.256205563378935e-05,
        "epoch": 0.8745808182427901,
        "step": 11736
    },
    {
        "loss": 2.3996,
        "grad_norm": 2.2011947631835938,
        "learning_rate": 6.249681653807868e-05,
        "epoch": 0.8746553394440718,
        "step": 11737
    },
    {
        "loss": 1.5163,
        "grad_norm": 1.9193673133850098,
        "learning_rate": 6.2431596011422e-05,
        "epoch": 0.8747298606453536,
        "step": 11738
    },
    {
        "loss": 2.8028,
        "grad_norm": 2.43525767326355,
        "learning_rate": 6.236639408611242e-05,
        "epoch": 0.8748043818466353,
        "step": 11739
    },
    {
        "loss": 2.573,
        "grad_norm": 2.772366523742676,
        "learning_rate": 6.230121079443341e-05,
        "epoch": 0.8748789030479172,
        "step": 11740
    },
    {
        "loss": 1.6726,
        "grad_norm": 3.5982885360717773,
        "learning_rate": 6.223604616865917e-05,
        "epoch": 0.8749534242491989,
        "step": 11741
    },
    {
        "loss": 1.8788,
        "grad_norm": 2.7176296710968018,
        "learning_rate": 6.217090024105525e-05,
        "epoch": 0.8750279454504807,
        "step": 11742
    },
    {
        "loss": 2.3747,
        "grad_norm": 4.60437536239624,
        "learning_rate": 6.210577304387706e-05,
        "epoch": 0.8751024666517624,
        "step": 11743
    },
    {
        "loss": 2.7389,
        "grad_norm": 3.4851982593536377,
        "learning_rate": 6.204066460937157e-05,
        "epoch": 0.8751769878530442,
        "step": 11744
    },
    {
        "loss": 1.9483,
        "grad_norm": 2.9589552879333496,
        "learning_rate": 6.197557496977581e-05,
        "epoch": 0.8752515090543259,
        "step": 11745
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.8880887031555176,
        "learning_rate": 6.191050415731807e-05,
        "epoch": 0.8753260302556077,
        "step": 11746
    },
    {
        "loss": 2.0174,
        "grad_norm": 2.5527284145355225,
        "learning_rate": 6.184545220421693e-05,
        "epoch": 0.8754005514568894,
        "step": 11747
    },
    {
        "loss": 2.3811,
        "grad_norm": 2.5307037830352783,
        "learning_rate": 6.17804191426816e-05,
        "epoch": 0.8754750726581713,
        "step": 11748
    },
    {
        "loss": 2.8014,
        "grad_norm": 2.471468210220337,
        "learning_rate": 6.171540500491233e-05,
        "epoch": 0.875549593859453,
        "step": 11749
    },
    {
        "loss": 1.8616,
        "grad_norm": 2.340390920639038,
        "learning_rate": 6.16504098230996e-05,
        "epoch": 0.8756241150607348,
        "step": 11750
    },
    {
        "loss": 2.1948,
        "grad_norm": 2.8074002265930176,
        "learning_rate": 6.15854336294247e-05,
        "epoch": 0.8756986362620165,
        "step": 11751
    },
    {
        "loss": 2.5979,
        "grad_norm": 2.2960386276245117,
        "learning_rate": 6.152047645605932e-05,
        "epoch": 0.8757731574632983,
        "step": 11752
    },
    {
        "loss": 2.7035,
        "grad_norm": 2.8134400844573975,
        "learning_rate": 6.145553833516613e-05,
        "epoch": 0.87584767866458,
        "step": 11753
    },
    {
        "loss": 1.1506,
        "grad_norm": 3.3278727531433105,
        "learning_rate": 6.139061929889799e-05,
        "epoch": 0.8759221998658618,
        "step": 11754
    },
    {
        "loss": 2.3818,
        "grad_norm": 2.7754199504852295,
        "learning_rate": 6.132571937939847e-05,
        "epoch": 0.8759967210671437,
        "step": 11755
    },
    {
        "loss": 2.2231,
        "grad_norm": 2.847966432571411,
        "learning_rate": 6.126083860880148e-05,
        "epoch": 0.8760712422684254,
        "step": 11756
    },
    {
        "loss": 2.9292,
        "grad_norm": 4.049690246582031,
        "learning_rate": 6.11959770192319e-05,
        "epoch": 0.8761457634697072,
        "step": 11757
    },
    {
        "loss": 1.9454,
        "grad_norm": 2.366787910461426,
        "learning_rate": 6.113113464280455e-05,
        "epoch": 0.8762202846709889,
        "step": 11758
    },
    {
        "loss": 1.6927,
        "grad_norm": 3.17512583732605,
        "learning_rate": 6.10663115116253e-05,
        "epoch": 0.8762948058722707,
        "step": 11759
    },
    {
        "loss": 2.1806,
        "grad_norm": 1.491823673248291,
        "learning_rate": 6.1001507657790026e-05,
        "epoch": 0.8763693270735524,
        "step": 11760
    },
    {
        "loss": 1.7792,
        "grad_norm": 3.4927375316619873,
        "learning_rate": 6.093672311338514e-05,
        "epoch": 0.8764438482748342,
        "step": 11761
    },
    {
        "loss": 1.9062,
        "grad_norm": 2.6338233947753906,
        "learning_rate": 6.0871957910488e-05,
        "epoch": 0.8765183694761159,
        "step": 11762
    },
    {
        "loss": 1.9772,
        "grad_norm": 3.4326114654541016,
        "learning_rate": 6.0807212081165423e-05,
        "epoch": 0.8765928906773978,
        "step": 11763
    },
    {
        "loss": 2.4007,
        "grad_norm": 3.3297958374023438,
        "learning_rate": 6.074248565747558e-05,
        "epoch": 0.8766674118786795,
        "step": 11764
    },
    {
        "loss": 2.9915,
        "grad_norm": 2.5700364112854004,
        "learning_rate": 6.0677778671466534e-05,
        "epoch": 0.8767419330799613,
        "step": 11765
    },
    {
        "loss": 1.9882,
        "grad_norm": 1.954348087310791,
        "learning_rate": 6.061309115517673e-05,
        "epoch": 0.876816454281243,
        "step": 11766
    },
    {
        "loss": 2.4957,
        "grad_norm": 3.4506382942199707,
        "learning_rate": 6.0548423140635246e-05,
        "epoch": 0.8768909754825248,
        "step": 11767
    },
    {
        "loss": 2.6372,
        "grad_norm": 4.076754093170166,
        "learning_rate": 6.048377465986116e-05,
        "epoch": 0.8769654966838065,
        "step": 11768
    },
    {
        "loss": 2.1989,
        "grad_norm": 3.959635019302368,
        "learning_rate": 6.0419145744864255e-05,
        "epoch": 0.8770400178850883,
        "step": 11769
    },
    {
        "loss": 1.9351,
        "grad_norm": 3.039198637008667,
        "learning_rate": 6.035453642764435e-05,
        "epoch": 0.87711453908637,
        "step": 11770
    },
    {
        "loss": 2.4503,
        "grad_norm": 3.143125295639038,
        "learning_rate": 6.028994674019157e-05,
        "epoch": 0.8771890602876519,
        "step": 11771
    },
    {
        "loss": 2.8271,
        "grad_norm": 2.087423324584961,
        "learning_rate": 6.022537671448629e-05,
        "epoch": 0.8772635814889336,
        "step": 11772
    },
    {
        "loss": 2.5562,
        "grad_norm": 2.3384881019592285,
        "learning_rate": 6.01608263824995e-05,
        "epoch": 0.8773381026902154,
        "step": 11773
    },
    {
        "loss": 1.8669,
        "grad_norm": 3.745964527130127,
        "learning_rate": 6.009629577619207e-05,
        "epoch": 0.8774126238914971,
        "step": 11774
    },
    {
        "loss": 2.1759,
        "grad_norm": 3.906846523284912,
        "learning_rate": 6.003178492751518e-05,
        "epoch": 0.8774871450927789,
        "step": 11775
    },
    {
        "loss": 2.4245,
        "grad_norm": 2.3154444694519043,
        "learning_rate": 5.9967293868410146e-05,
        "epoch": 0.8775616662940606,
        "step": 11776
    },
    {
        "loss": 2.3556,
        "grad_norm": 2.871556043624878,
        "learning_rate": 5.990282263080889e-05,
        "epoch": 0.8776361874953424,
        "step": 11777
    },
    {
        "loss": 2.498,
        "grad_norm": 4.584130764007568,
        "learning_rate": 5.983837124663294e-05,
        "epoch": 0.8777107086966242,
        "step": 11778
    },
    {
        "loss": 1.9198,
        "grad_norm": 2.6151785850524902,
        "learning_rate": 5.977393974779455e-05,
        "epoch": 0.877785229897906,
        "step": 11779
    },
    {
        "loss": 2.3378,
        "grad_norm": 4.138895511627197,
        "learning_rate": 5.970952816619573e-05,
        "epoch": 0.8778597510991877,
        "step": 11780
    },
    {
        "loss": 2.0092,
        "grad_norm": 2.9219839572906494,
        "learning_rate": 5.964513653372866e-05,
        "epoch": 0.8779342723004695,
        "step": 11781
    },
    {
        "loss": 2.3972,
        "grad_norm": 2.7907989025115967,
        "learning_rate": 5.958076488227613e-05,
        "epoch": 0.8780087935017512,
        "step": 11782
    },
    {
        "loss": 3.0053,
        "grad_norm": 3.354573965072632,
        "learning_rate": 5.951641324371011e-05,
        "epoch": 0.878083314703033,
        "step": 11783
    },
    {
        "loss": 2.4939,
        "grad_norm": 2.4220049381256104,
        "learning_rate": 5.945208164989362e-05,
        "epoch": 0.8781578359043147,
        "step": 11784
    },
    {
        "loss": 2.3565,
        "grad_norm": 2.990652084350586,
        "learning_rate": 5.93877701326792e-05,
        "epoch": 0.8782323571055966,
        "step": 11785
    },
    {
        "loss": 2.6025,
        "grad_norm": 2.933609962463379,
        "learning_rate": 5.932347872390944e-05,
        "epoch": 0.8783068783068783,
        "step": 11786
    },
    {
        "loss": 2.6202,
        "grad_norm": 3.5615346431732178,
        "learning_rate": 5.925920745541739e-05,
        "epoch": 0.8783813995081601,
        "step": 11787
    },
    {
        "loss": 1.058,
        "grad_norm": 5.228195667266846,
        "learning_rate": 5.9194956359025734e-05,
        "epoch": 0.8784559207094418,
        "step": 11788
    },
    {
        "loss": 3.0415,
        "grad_norm": 2.862022876739502,
        "learning_rate": 5.9130725466547185e-05,
        "epoch": 0.8785304419107236,
        "step": 11789
    },
    {
        "loss": 1.9444,
        "grad_norm": 2.737257957458496,
        "learning_rate": 5.906651480978478e-05,
        "epoch": 0.8786049631120053,
        "step": 11790
    },
    {
        "loss": 2.1076,
        "grad_norm": 3.3243560791015625,
        "learning_rate": 5.900232442053122e-05,
        "epoch": 0.8786794843132871,
        "step": 11791
    },
    {
        "loss": 2.141,
        "grad_norm": 2.015195608139038,
        "learning_rate": 5.893815433056911e-05,
        "epoch": 0.878754005514569,
        "step": 11792
    },
    {
        "loss": 1.9779,
        "grad_norm": 3.466989517211914,
        "learning_rate": 5.887400457167158e-05,
        "epoch": 0.8788285267158507,
        "step": 11793
    },
    {
        "loss": 2.3299,
        "grad_norm": 4.686579704284668,
        "learning_rate": 5.880987517560075e-05,
        "epoch": 0.8789030479171325,
        "step": 11794
    },
    {
        "loss": 1.5248,
        "grad_norm": 3.8713600635528564,
        "learning_rate": 5.874576617410956e-05,
        "epoch": 0.8789775691184142,
        "step": 11795
    },
    {
        "loss": 2.1503,
        "grad_norm": 3.3471696376800537,
        "learning_rate": 5.868167759894023e-05,
        "epoch": 0.879052090319696,
        "step": 11796
    },
    {
        "loss": 2.4783,
        "grad_norm": 2.6968886852264404,
        "learning_rate": 5.8617609481825385e-05,
        "epoch": 0.8791266115209777,
        "step": 11797
    },
    {
        "loss": 2.4907,
        "grad_norm": 3.511404514312744,
        "learning_rate": 5.855356185448707e-05,
        "epoch": 0.8792011327222595,
        "step": 11798
    },
    {
        "loss": 2.5775,
        "grad_norm": 3.6642186641693115,
        "learning_rate": 5.848953474863727e-05,
        "epoch": 0.8792756539235412,
        "step": 11799
    },
    {
        "loss": 1.5458,
        "grad_norm": 2.9730613231658936,
        "learning_rate": 5.842552819597818e-05,
        "epoch": 0.879350175124823,
        "step": 11800
    },
    {
        "loss": 2.0212,
        "grad_norm": 2.762395143508911,
        "learning_rate": 5.836154222820121e-05,
        "epoch": 0.8794246963261048,
        "step": 11801
    },
    {
        "loss": 2.5434,
        "grad_norm": 2.641413927078247,
        "learning_rate": 5.829757687698838e-05,
        "epoch": 0.8794992175273866,
        "step": 11802
    },
    {
        "loss": 1.6456,
        "grad_norm": 3.7068870067596436,
        "learning_rate": 5.8233632174010456e-05,
        "epoch": 0.8795737387286683,
        "step": 11803
    },
    {
        "loss": 2.1549,
        "grad_norm": 2.9431138038635254,
        "learning_rate": 5.816970815092895e-05,
        "epoch": 0.8796482599299501,
        "step": 11804
    },
    {
        "loss": 2.6695,
        "grad_norm": 2.119344472885132,
        "learning_rate": 5.810580483939463e-05,
        "epoch": 0.8797227811312318,
        "step": 11805
    },
    {
        "loss": 1.876,
        "grad_norm": 2.95125150680542,
        "learning_rate": 5.8041922271047966e-05,
        "epoch": 0.8797973023325136,
        "step": 11806
    },
    {
        "loss": 2.5625,
        "grad_norm": 2.160885810852051,
        "learning_rate": 5.797806047751957e-05,
        "epoch": 0.8798718235337953,
        "step": 11807
    },
    {
        "loss": 2.4378,
        "grad_norm": 2.730891704559326,
        "learning_rate": 5.79142194904294e-05,
        "epoch": 0.8799463447350772,
        "step": 11808
    },
    {
        "loss": 2.0755,
        "grad_norm": 3.2608444690704346,
        "learning_rate": 5.7850399341387076e-05,
        "epoch": 0.8800208659363589,
        "step": 11809
    },
    {
        "loss": 1.3426,
        "grad_norm": 4.2224202156066895,
        "learning_rate": 5.778660006199235e-05,
        "epoch": 0.8800953871376407,
        "step": 11810
    },
    {
        "loss": 2.3665,
        "grad_norm": 3.4253323078155518,
        "learning_rate": 5.772282168383409e-05,
        "epoch": 0.8801699083389224,
        "step": 11811
    },
    {
        "loss": 1.9612,
        "grad_norm": 3.7970428466796875,
        "learning_rate": 5.765906423849104e-05,
        "epoch": 0.8802444295402042,
        "step": 11812
    },
    {
        "loss": 2.5057,
        "grad_norm": 1.914864182472229,
        "learning_rate": 5.75953277575319e-05,
        "epoch": 0.8803189507414859,
        "step": 11813
    },
    {
        "loss": 2.97,
        "grad_norm": 3.4182496070861816,
        "learning_rate": 5.7531612272514246e-05,
        "epoch": 0.8803934719427677,
        "step": 11814
    },
    {
        "loss": 2.4198,
        "grad_norm": 2.2947633266448975,
        "learning_rate": 5.746791781498604e-05,
        "epoch": 0.8804679931440494,
        "step": 11815
    },
    {
        "loss": 1.9465,
        "grad_norm": 3.1004257202148438,
        "learning_rate": 5.74042444164844e-05,
        "epoch": 0.8805425143453313,
        "step": 11816
    },
    {
        "loss": 2.3713,
        "grad_norm": 3.1425578594207764,
        "learning_rate": 5.734059210853599e-05,
        "epoch": 0.880617035546613,
        "step": 11817
    },
    {
        "loss": 2.1144,
        "grad_norm": 1.9725099802017212,
        "learning_rate": 5.7276960922657354e-05,
        "epoch": 0.8806915567478948,
        "step": 11818
    },
    {
        "loss": 2.7048,
        "grad_norm": 2.212508201599121,
        "learning_rate": 5.7213350890354156e-05,
        "epoch": 0.8807660779491765,
        "step": 11819
    },
    {
        "loss": 2.8425,
        "grad_norm": 2.727385997772217,
        "learning_rate": 5.714976204312204e-05,
        "epoch": 0.8808405991504583,
        "step": 11820
    },
    {
        "loss": 2.3486,
        "grad_norm": 4.759124279022217,
        "learning_rate": 5.708619441244583e-05,
        "epoch": 0.88091512035174,
        "step": 11821
    },
    {
        "loss": 2.7322,
        "grad_norm": 2.511274576187134,
        "learning_rate": 5.702264802979991e-05,
        "epoch": 0.8809896415530218,
        "step": 11822
    },
    {
        "loss": 2.4573,
        "grad_norm": 2.300079107284546,
        "learning_rate": 5.695912292664807e-05,
        "epoch": 0.8810641627543035,
        "step": 11823
    },
    {
        "loss": 2.4444,
        "grad_norm": 2.8206961154937744,
        "learning_rate": 5.68956191344439e-05,
        "epoch": 0.8811386839555854,
        "step": 11824
    },
    {
        "loss": 2.685,
        "grad_norm": 3.9544026851654053,
        "learning_rate": 5.683213668463015e-05,
        "epoch": 0.8812132051568671,
        "step": 11825
    },
    {
        "loss": 2.0606,
        "grad_norm": 3.153140068054199,
        "learning_rate": 5.6768675608638955e-05,
        "epoch": 0.8812877263581489,
        "step": 11826
    },
    {
        "loss": 2.1324,
        "grad_norm": 3.236149311065674,
        "learning_rate": 5.67052359378919e-05,
        "epoch": 0.8813622475594307,
        "step": 11827
    },
    {
        "loss": 2.4649,
        "grad_norm": 2.2737886905670166,
        "learning_rate": 5.6641817703800296e-05,
        "epoch": 0.8814367687607124,
        "step": 11828
    },
    {
        "loss": 2.4968,
        "grad_norm": 2.8500208854675293,
        "learning_rate": 5.657842093776432e-05,
        "epoch": 0.8815112899619942,
        "step": 11829
    },
    {
        "loss": 1.2351,
        "grad_norm": 3.3653054237365723,
        "learning_rate": 5.651504567117405e-05,
        "epoch": 0.881585811163276,
        "step": 11830
    },
    {
        "loss": 1.8508,
        "grad_norm": 2.99405837059021,
        "learning_rate": 5.645169193540853e-05,
        "epoch": 0.8816603323645578,
        "step": 11831
    },
    {
        "loss": 2.438,
        "grad_norm": 3.3589537143707275,
        "learning_rate": 5.638835976183615e-05,
        "epoch": 0.8817348535658395,
        "step": 11832
    },
    {
        "loss": 2.4728,
        "grad_norm": 2.3023884296417236,
        "learning_rate": 5.6325049181815135e-05,
        "epoch": 0.8818093747671213,
        "step": 11833
    },
    {
        "loss": 2.8947,
        "grad_norm": 3.1097664833068848,
        "learning_rate": 5.626176022669207e-05,
        "epoch": 0.881883895968403,
        "step": 11834
    },
    {
        "loss": 2.2555,
        "grad_norm": 5.855809688568115,
        "learning_rate": 5.619849292780379e-05,
        "epoch": 0.8819584171696848,
        "step": 11835
    },
    {
        "loss": 2.3987,
        "grad_norm": 2.564774513244629,
        "learning_rate": 5.613524731647589e-05,
        "epoch": 0.8820329383709665,
        "step": 11836
    },
    {
        "loss": 1.5823,
        "grad_norm": 3.9007177352905273,
        "learning_rate": 5.607202342402322e-05,
        "epoch": 0.8821074595722483,
        "step": 11837
    },
    {
        "loss": 2.4833,
        "grad_norm": 2.976475715637207,
        "learning_rate": 5.6008821281750245e-05,
        "epoch": 0.88218198077353,
        "step": 11838
    },
    {
        "loss": 2.6236,
        "grad_norm": 3.4226338863372803,
        "learning_rate": 5.594564092095016e-05,
        "epoch": 0.8822565019748119,
        "step": 11839
    },
    {
        "loss": 1.3674,
        "grad_norm": 3.1696932315826416,
        "learning_rate": 5.588248237290592e-05,
        "epoch": 0.8823310231760936,
        "step": 11840
    },
    {
        "loss": 2.2924,
        "grad_norm": 3.208787441253662,
        "learning_rate": 5.581934566888918e-05,
        "epoch": 0.8824055443773754,
        "step": 11841
    },
    {
        "loss": 2.6138,
        "grad_norm": 2.1637609004974365,
        "learning_rate": 5.575623084016105e-05,
        "epoch": 0.8824800655786571,
        "step": 11842
    },
    {
        "loss": 2.6579,
        "grad_norm": 3.0027029514312744,
        "learning_rate": 5.5693137917971616e-05,
        "epoch": 0.8825545867799389,
        "step": 11843
    },
    {
        "loss": 2.7058,
        "grad_norm": 3.1352362632751465,
        "learning_rate": 5.563006693356061e-05,
        "epoch": 0.8826291079812206,
        "step": 11844
    },
    {
        "loss": 2.1967,
        "grad_norm": 2.267712354660034,
        "learning_rate": 5.556701791815606e-05,
        "epoch": 0.8827036291825024,
        "step": 11845
    },
    {
        "loss": 2.0191,
        "grad_norm": 2.606327772140503,
        "learning_rate": 5.550399090297598e-05,
        "epoch": 0.8827781503837842,
        "step": 11846
    },
    {
        "loss": 2.4112,
        "grad_norm": 4.476079940795898,
        "learning_rate": 5.544098591922682e-05,
        "epoch": 0.882852671585066,
        "step": 11847
    },
    {
        "loss": 2.4639,
        "grad_norm": 2.2966437339782715,
        "learning_rate": 5.537800299810474e-05,
        "epoch": 0.8829271927863477,
        "step": 11848
    },
    {
        "loss": 2.4155,
        "grad_norm": 2.428345203399658,
        "learning_rate": 5.531504217079442e-05,
        "epoch": 0.8830017139876295,
        "step": 11849
    },
    {
        "loss": 2.3329,
        "grad_norm": 2.575430154800415,
        "learning_rate": 5.525210346846975e-05,
        "epoch": 0.8830762351889112,
        "step": 11850
    },
    {
        "loss": 2.3013,
        "grad_norm": 3.9155149459838867,
        "learning_rate": 5.518918692229399e-05,
        "epoch": 0.883150756390193,
        "step": 11851
    },
    {
        "loss": 2.1533,
        "grad_norm": 4.647481918334961,
        "learning_rate": 5.5126292563418946e-05,
        "epoch": 0.8832252775914747,
        "step": 11852
    },
    {
        "loss": 2.739,
        "grad_norm": 2.3677337169647217,
        "learning_rate": 5.5063420422986014e-05,
        "epoch": 0.8832997987927566,
        "step": 11853
    },
    {
        "loss": 2.7812,
        "grad_norm": 2.172714948654175,
        "learning_rate": 5.500057053212479e-05,
        "epoch": 0.8833743199940383,
        "step": 11854
    },
    {
        "loss": 1.2523,
        "grad_norm": 4.66192626953125,
        "learning_rate": 5.4937742921954694e-05,
        "epoch": 0.8834488411953201,
        "step": 11855
    },
    {
        "loss": 2.4919,
        "grad_norm": 2.8017609119415283,
        "learning_rate": 5.4874937623583555e-05,
        "epoch": 0.8835233623966018,
        "step": 11856
    },
    {
        "loss": 2.6076,
        "grad_norm": 2.4710662364959717,
        "learning_rate": 5.4812154668108226e-05,
        "epoch": 0.8835978835978836,
        "step": 11857
    },
    {
        "loss": 2.1457,
        "grad_norm": 3.7922797203063965,
        "learning_rate": 5.474939408661488e-05,
        "epoch": 0.8836724047991653,
        "step": 11858
    },
    {
        "loss": 1.99,
        "grad_norm": 3.4032931327819824,
        "learning_rate": 5.4686655910178186e-05,
        "epoch": 0.8837469260004471,
        "step": 11859
    },
    {
        "loss": 1.7078,
        "grad_norm": 2.899047374725342,
        "learning_rate": 5.4623940169861765e-05,
        "epoch": 0.8838214472017288,
        "step": 11860
    },
    {
        "loss": 2.7848,
        "grad_norm": 2.723118543624878,
        "learning_rate": 5.4561246896718465e-05,
        "epoch": 0.8838959684030107,
        "step": 11861
    },
    {
        "loss": 2.9381,
        "grad_norm": 3.1496410369873047,
        "learning_rate": 5.44985761217897e-05,
        "epoch": 0.8839704896042925,
        "step": 11862
    },
    {
        "loss": 2.3595,
        "grad_norm": 2.723289728164673,
        "learning_rate": 5.443592787610562e-05,
        "epoch": 0.8840450108055742,
        "step": 11863
    },
    {
        "loss": 2.7991,
        "grad_norm": 3.8279757499694824,
        "learning_rate": 5.437330219068583e-05,
        "epoch": 0.884119532006856,
        "step": 11864
    },
    {
        "loss": 2.1893,
        "grad_norm": 3.684037685394287,
        "learning_rate": 5.4310699096537875e-05,
        "epoch": 0.8841940532081377,
        "step": 11865
    },
    {
        "loss": 2.1678,
        "grad_norm": 2.546795129776001,
        "learning_rate": 5.424811862465895e-05,
        "epoch": 0.8842685744094195,
        "step": 11866
    },
    {
        "loss": 2.4929,
        "grad_norm": 3.2129886150360107,
        "learning_rate": 5.4185560806034475e-05,
        "epoch": 0.8843430956107012,
        "step": 11867
    },
    {
        "loss": 1.9162,
        "grad_norm": 2.7920033931732178,
        "learning_rate": 5.412302567163908e-05,
        "epoch": 0.884417616811983,
        "step": 11868
    },
    {
        "loss": 2.8552,
        "grad_norm": 3.5220725536346436,
        "learning_rate": 5.406051325243586e-05,
        "epoch": 0.8844921380132648,
        "step": 11869
    },
    {
        "loss": 1.9592,
        "grad_norm": 2.2510790824890137,
        "learning_rate": 5.3998023579376665e-05,
        "epoch": 0.8845666592145466,
        "step": 11870
    },
    {
        "loss": 2.6369,
        "grad_norm": 4.168890476226807,
        "learning_rate": 5.393555668340241e-05,
        "epoch": 0.8846411804158283,
        "step": 11871
    },
    {
        "loss": 2.9436,
        "grad_norm": 2.894824504852295,
        "learning_rate": 5.387311259544221e-05,
        "epoch": 0.8847157016171101,
        "step": 11872
    },
    {
        "loss": 2.2855,
        "grad_norm": 2.4867942333221436,
        "learning_rate": 5.3810691346414566e-05,
        "epoch": 0.8847902228183918,
        "step": 11873
    },
    {
        "loss": 2.0499,
        "grad_norm": 2.4745054244995117,
        "learning_rate": 5.374829296722581e-05,
        "epoch": 0.8848647440196736,
        "step": 11874
    },
    {
        "loss": 2.2186,
        "grad_norm": 2.8299474716186523,
        "learning_rate": 5.3685917488771784e-05,
        "epoch": 0.8849392652209553,
        "step": 11875
    },
    {
        "loss": 1.8335,
        "grad_norm": 1.5139074325561523,
        "learning_rate": 5.36235649419365e-05,
        "epoch": 0.8850137864222372,
        "step": 11876
    },
    {
        "loss": 2.7911,
        "grad_norm": 2.895831346511841,
        "learning_rate": 5.356123535759279e-05,
        "epoch": 0.8850883076235189,
        "step": 11877
    },
    {
        "loss": 2.2871,
        "grad_norm": 4.8768815994262695,
        "learning_rate": 5.349892876660188e-05,
        "epoch": 0.8851628288248007,
        "step": 11878
    },
    {
        "loss": 2.1209,
        "grad_norm": 3.072619676589966,
        "learning_rate": 5.3436645199814075e-05,
        "epoch": 0.8852373500260824,
        "step": 11879
    },
    {
        "loss": 2.7778,
        "grad_norm": 2.4996979236602783,
        "learning_rate": 5.337438468806777e-05,
        "epoch": 0.8853118712273642,
        "step": 11880
    },
    {
        "loss": 2.672,
        "grad_norm": 2.6305911540985107,
        "learning_rate": 5.331214726219041e-05,
        "epoch": 0.8853863924286459,
        "step": 11881
    },
    {
        "loss": 2.5262,
        "grad_norm": 2.5533857345581055,
        "learning_rate": 5.324993295299769e-05,
        "epoch": 0.8854609136299277,
        "step": 11882
    },
    {
        "loss": 2.1814,
        "grad_norm": 3.9384539127349854,
        "learning_rate": 5.318774179129381e-05,
        "epoch": 0.8855354348312094,
        "step": 11883
    },
    {
        "loss": 1.6181,
        "grad_norm": 2.7795517444610596,
        "learning_rate": 5.312557380787202e-05,
        "epoch": 0.8856099560324913,
        "step": 11884
    },
    {
        "loss": 2.6025,
        "grad_norm": 3.8908579349517822,
        "learning_rate": 5.3063429033513275e-05,
        "epoch": 0.885684477233773,
        "step": 11885
    },
    {
        "loss": 2.4798,
        "grad_norm": 2.858874559402466,
        "learning_rate": 5.3001307498987776e-05,
        "epoch": 0.8857589984350548,
        "step": 11886
    },
    {
        "loss": 2.7395,
        "grad_norm": 2.0576751232147217,
        "learning_rate": 5.293920923505385e-05,
        "epoch": 0.8858335196363365,
        "step": 11887
    },
    {
        "loss": 2.896,
        "grad_norm": 3.224768877029419,
        "learning_rate": 5.287713427245826e-05,
        "epoch": 0.8859080408376183,
        "step": 11888
    },
    {
        "loss": 2.9544,
        "grad_norm": 3.1418607234954834,
        "learning_rate": 5.28150826419366e-05,
        "epoch": 0.8859825620389,
        "step": 11889
    },
    {
        "loss": 2.7858,
        "grad_norm": 2.4088997840881348,
        "learning_rate": 5.2753054374212396e-05,
        "epoch": 0.8860570832401818,
        "step": 11890
    },
    {
        "loss": 2.6674,
        "grad_norm": 2.9713666439056396,
        "learning_rate": 5.269104949999815e-05,
        "epoch": 0.8861316044414635,
        "step": 11891
    },
    {
        "loss": 1.8449,
        "grad_norm": 3.3335344791412354,
        "learning_rate": 5.2629068049994344e-05,
        "epoch": 0.8862061256427454,
        "step": 11892
    },
    {
        "loss": 1.4166,
        "grad_norm": 3.882086753845215,
        "learning_rate": 5.25671100548901e-05,
        "epoch": 0.8862806468440271,
        "step": 11893
    },
    {
        "loss": 1.8425,
        "grad_norm": 2.274059295654297,
        "learning_rate": 5.250517554536265e-05,
        "epoch": 0.8863551680453089,
        "step": 11894
    },
    {
        "loss": 2.2029,
        "grad_norm": 2.509880304336548,
        "learning_rate": 5.244326455207805e-05,
        "epoch": 0.8864296892465906,
        "step": 11895
    },
    {
        "loss": 2.3668,
        "grad_norm": 2.5021767616271973,
        "learning_rate": 5.2381377105690355e-05,
        "epoch": 0.8865042104478724,
        "step": 11896
    },
    {
        "loss": 2.0917,
        "grad_norm": 3.162436008453369,
        "learning_rate": 5.2319513236842076e-05,
        "epoch": 0.8865787316491542,
        "step": 11897
    },
    {
        "loss": 1.5338,
        "grad_norm": 4.340474605560303,
        "learning_rate": 5.225767297616391e-05,
        "epoch": 0.886653252850436,
        "step": 11898
    },
    {
        "loss": 1.9335,
        "grad_norm": 3.2312498092651367,
        "learning_rate": 5.219585635427524e-05,
        "epoch": 0.8867277740517178,
        "step": 11899
    },
    {
        "loss": 2.1718,
        "grad_norm": 2.455399513244629,
        "learning_rate": 5.213406340178324e-05,
        "epoch": 0.8868022952529995,
        "step": 11900
    },
    {
        "loss": 2.5762,
        "grad_norm": 2.3007869720458984,
        "learning_rate": 5.207229414928393e-05,
        "epoch": 0.8868768164542813,
        "step": 11901
    },
    {
        "loss": 2.5323,
        "grad_norm": 3.3664534091949463,
        "learning_rate": 5.2010548627361076e-05,
        "epoch": 0.886951337655563,
        "step": 11902
    },
    {
        "loss": 2.1929,
        "grad_norm": 3.9163765907287598,
        "learning_rate": 5.194882686658683e-05,
        "epoch": 0.8870258588568448,
        "step": 11903
    },
    {
        "loss": 2.074,
        "grad_norm": 3.579958200454712,
        "learning_rate": 5.188712889752201e-05,
        "epoch": 0.8871003800581265,
        "step": 11904
    },
    {
        "loss": 1.6372,
        "grad_norm": 2.88462233543396,
        "learning_rate": 5.182545475071484e-05,
        "epoch": 0.8871749012594083,
        "step": 11905
    },
    {
        "loss": 2.9283,
        "grad_norm": 4.159182548522949,
        "learning_rate": 5.1763804456702545e-05,
        "epoch": 0.88724942246069,
        "step": 11906
    },
    {
        "loss": 2.0476,
        "grad_norm": 3.3420357704162598,
        "learning_rate": 5.170217804601015e-05,
        "epoch": 0.8873239436619719,
        "step": 11907
    },
    {
        "loss": 2.3508,
        "grad_norm": 2.4719221591949463,
        "learning_rate": 5.164057554915069e-05,
        "epoch": 0.8873984648632536,
        "step": 11908
    },
    {
        "loss": 1.5739,
        "grad_norm": 3.1571080684661865,
        "learning_rate": 5.1578996996625875e-05,
        "epoch": 0.8874729860645354,
        "step": 11909
    },
    {
        "loss": 2.1257,
        "grad_norm": 2.736588954925537,
        "learning_rate": 5.151744241892511e-05,
        "epoch": 0.8875475072658171,
        "step": 11910
    },
    {
        "loss": 1.7471,
        "grad_norm": 4.2067365646362305,
        "learning_rate": 5.145591184652601e-05,
        "epoch": 0.8876220284670989,
        "step": 11911
    },
    {
        "loss": 1.9696,
        "grad_norm": 2.699514865875244,
        "learning_rate": 5.1394405309894563e-05,
        "epoch": 0.8876965496683806,
        "step": 11912
    },
    {
        "loss": 2.4787,
        "grad_norm": 3.6000559329986572,
        "learning_rate": 5.133292283948459e-05,
        "epoch": 0.8877710708696624,
        "step": 11913
    },
    {
        "loss": 2.0352,
        "grad_norm": 3.5042078495025635,
        "learning_rate": 5.127146446573794e-05,
        "epoch": 0.8878455920709442,
        "step": 11914
    },
    {
        "loss": 2.1172,
        "grad_norm": 2.254999876022339,
        "learning_rate": 5.121003021908506e-05,
        "epoch": 0.887920113272226,
        "step": 11915
    },
    {
        "loss": 2.0632,
        "grad_norm": 3.0136663913726807,
        "learning_rate": 5.11486201299435e-05,
        "epoch": 0.8879946344735077,
        "step": 11916
    },
    {
        "loss": 1.896,
        "grad_norm": 3.083704710006714,
        "learning_rate": 5.108723422871983e-05,
        "epoch": 0.8880691556747895,
        "step": 11917
    },
    {
        "loss": 2.0839,
        "grad_norm": 2.5890190601348877,
        "learning_rate": 5.102587254580795e-05,
        "epoch": 0.8881436768760712,
        "step": 11918
    },
    {
        "loss": 1.4737,
        "grad_norm": 3.459350347518921,
        "learning_rate": 5.096453511159028e-05,
        "epoch": 0.888218198077353,
        "step": 11919
    },
    {
        "loss": 1.9864,
        "grad_norm": 3.2693769931793213,
        "learning_rate": 5.090322195643688e-05,
        "epoch": 0.8882927192786347,
        "step": 11920
    },
    {
        "loss": 2.6341,
        "grad_norm": 2.362567186355591,
        "learning_rate": 5.0841933110705806e-05,
        "epoch": 0.8883672404799166,
        "step": 11921
    },
    {
        "loss": 2.7917,
        "grad_norm": 2.6175405979156494,
        "learning_rate": 5.078066860474337e-05,
        "epoch": 0.8884417616811983,
        "step": 11922
    },
    {
        "loss": 2.4956,
        "grad_norm": 2.568319320678711,
        "learning_rate": 5.071942846888342e-05,
        "epoch": 0.8885162828824801,
        "step": 11923
    },
    {
        "loss": 2.8784,
        "grad_norm": 2.600341796875,
        "learning_rate": 5.065821273344833e-05,
        "epoch": 0.8885908040837618,
        "step": 11924
    },
    {
        "loss": 2.3159,
        "grad_norm": 3.0736515522003174,
        "learning_rate": 5.059702142874748e-05,
        "epoch": 0.8886653252850436,
        "step": 11925
    },
    {
        "loss": 1.672,
        "grad_norm": 3.7857909202575684,
        "learning_rate": 5.0535854585079076e-05,
        "epoch": 0.8887398464863253,
        "step": 11926
    },
    {
        "loss": 2.4687,
        "grad_norm": 2.9796500205993652,
        "learning_rate": 5.0474712232728705e-05,
        "epoch": 0.8888143676876071,
        "step": 11927
    },
    {
        "loss": 2.5383,
        "grad_norm": 4.411137580871582,
        "learning_rate": 5.0413594401969844e-05,
        "epoch": 0.8888888888888888,
        "step": 11928
    },
    {
        "loss": 2.7923,
        "grad_norm": 2.462425708770752,
        "learning_rate": 5.0352501123064155e-05,
        "epoch": 0.8889634100901707,
        "step": 11929
    },
    {
        "loss": 2.0855,
        "grad_norm": 3.233267307281494,
        "learning_rate": 5.029143242626083e-05,
        "epoch": 0.8890379312914524,
        "step": 11930
    },
    {
        "loss": 1.153,
        "grad_norm": 4.058870315551758,
        "learning_rate": 5.023038834179681e-05,
        "epoch": 0.8891124524927342,
        "step": 11931
    },
    {
        "loss": 1.2261,
        "grad_norm": 2.802278518676758,
        "learning_rate": 5.016936889989728e-05,
        "epoch": 0.889186973694016,
        "step": 11932
    },
    {
        "loss": 1.2175,
        "grad_norm": 1.643447756767273,
        "learning_rate": 5.010837413077485e-05,
        "epoch": 0.8892614948952977,
        "step": 11933
    },
    {
        "loss": 1.7224,
        "grad_norm": 3.369337320327759,
        "learning_rate": 5.004740406462991e-05,
        "epoch": 0.8893360160965795,
        "step": 11934
    },
    {
        "loss": 2.5256,
        "grad_norm": 3.5258007049560547,
        "learning_rate": 4.998645873165107e-05,
        "epoch": 0.8894105372978612,
        "step": 11935
    },
    {
        "loss": 2.8727,
        "grad_norm": 3.3922982215881348,
        "learning_rate": 4.992553816201393e-05,
        "epoch": 0.889485058499143,
        "step": 11936
    },
    {
        "loss": 2.2832,
        "grad_norm": 3.129760980606079,
        "learning_rate": 4.9864642385882575e-05,
        "epoch": 0.8895595797004248,
        "step": 11937
    },
    {
        "loss": 2.4773,
        "grad_norm": 2.993276596069336,
        "learning_rate": 4.980377143340843e-05,
        "epoch": 0.8896341009017066,
        "step": 11938
    },
    {
        "loss": 2.7003,
        "grad_norm": 2.245605230331421,
        "learning_rate": 4.974292533473052e-05,
        "epoch": 0.8897086221029883,
        "step": 11939
    },
    {
        "loss": 2.7284,
        "grad_norm": 2.960151433944702,
        "learning_rate": 4.9682104119975984e-05,
        "epoch": 0.8897831433042701,
        "step": 11940
    },
    {
        "loss": 2.2304,
        "grad_norm": 2.6939260959625244,
        "learning_rate": 4.962130781925919e-05,
        "epoch": 0.8898576645055518,
        "step": 11941
    },
    {
        "loss": 1.7905,
        "grad_norm": 2.63714599609375,
        "learning_rate": 4.9560536462682585e-05,
        "epoch": 0.8899321857068336,
        "step": 11942
    },
    {
        "loss": 1.9264,
        "grad_norm": 3.052520751953125,
        "learning_rate": 4.949979008033596e-05,
        "epoch": 0.8900067069081153,
        "step": 11943
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.6085262298583984,
        "learning_rate": 4.9439068702296834e-05,
        "epoch": 0.8900812281093972,
        "step": 11944
    },
    {
        "loss": 2.5486,
        "grad_norm": 3.7318897247314453,
        "learning_rate": 4.937837235863022e-05,
        "epoch": 0.8901557493106789,
        "step": 11945
    },
    {
        "loss": 2.4836,
        "grad_norm": 2.5702497959136963,
        "learning_rate": 4.931770107938916e-05,
        "epoch": 0.8902302705119607,
        "step": 11946
    },
    {
        "loss": 2.1214,
        "grad_norm": 3.836883306503296,
        "learning_rate": 4.925705489461378e-05,
        "epoch": 0.8903047917132424,
        "step": 11947
    },
    {
        "loss": 2.1125,
        "grad_norm": 3.067667245864868,
        "learning_rate": 4.919643383433203e-05,
        "epoch": 0.8903793129145242,
        "step": 11948
    },
    {
        "loss": 2.4603,
        "grad_norm": 4.426876068115234,
        "learning_rate": 4.913583792855928e-05,
        "epoch": 0.8904538341158059,
        "step": 11949
    },
    {
        "loss": 2.7115,
        "grad_norm": 2.3251733779907227,
        "learning_rate": 4.907526720729876e-05,
        "epoch": 0.8905283553170877,
        "step": 11950
    },
    {
        "loss": 2.5509,
        "grad_norm": 3.1025946140289307,
        "learning_rate": 4.90147217005408e-05,
        "epoch": 0.8906028765183694,
        "step": 11951
    },
    {
        "loss": 2.7947,
        "grad_norm": 2.97255539894104,
        "learning_rate": 4.895420143826369e-05,
        "epoch": 0.8906773977196513,
        "step": 11952
    },
    {
        "loss": 2.0893,
        "grad_norm": 3.782045602798462,
        "learning_rate": 4.8893706450432916e-05,
        "epoch": 0.890751918920933,
        "step": 11953
    },
    {
        "loss": 2.5231,
        "grad_norm": 2.154463768005371,
        "learning_rate": 4.883323676700135e-05,
        "epoch": 0.8908264401222148,
        "step": 11954
    },
    {
        "loss": 1.7383,
        "grad_norm": 2.995354175567627,
        "learning_rate": 4.877279241790986e-05,
        "epoch": 0.8909009613234965,
        "step": 11955
    },
    {
        "loss": 1.9773,
        "grad_norm": 3.925245523452759,
        "learning_rate": 4.8712373433086e-05,
        "epoch": 0.8909754825247783,
        "step": 11956
    },
    {
        "loss": 2.5398,
        "grad_norm": 2.638587474822998,
        "learning_rate": 4.8651979842445515e-05,
        "epoch": 0.89105000372606,
        "step": 11957
    },
    {
        "loss": 2.4606,
        "grad_norm": 1.6368931531906128,
        "learning_rate": 4.859161167589113e-05,
        "epoch": 0.8911245249273418,
        "step": 11958
    },
    {
        "loss": 2.0645,
        "grad_norm": 3.785076141357422,
        "learning_rate": 4.853126896331299e-05,
        "epoch": 0.8911990461286236,
        "step": 11959
    },
    {
        "loss": 2.1678,
        "grad_norm": 3.673603057861328,
        "learning_rate": 4.847095173458899e-05,
        "epoch": 0.8912735673299054,
        "step": 11960
    },
    {
        "loss": 2.2622,
        "grad_norm": 2.7708563804626465,
        "learning_rate": 4.841066001958391e-05,
        "epoch": 0.8913480885311871,
        "step": 11961
    },
    {
        "loss": 2.4781,
        "grad_norm": 2.665830373764038,
        "learning_rate": 4.8350393848150446e-05,
        "epoch": 0.8914226097324689,
        "step": 11962
    },
    {
        "loss": 2.0869,
        "grad_norm": 2.228153705596924,
        "learning_rate": 4.8290153250128144e-05,
        "epoch": 0.8914971309337506,
        "step": 11963
    },
    {
        "loss": 2.4123,
        "grad_norm": 2.6688826084136963,
        "learning_rate": 4.8229938255344155e-05,
        "epoch": 0.8915716521350324,
        "step": 11964
    },
    {
        "loss": 2.9974,
        "grad_norm": 2.495695114135742,
        "learning_rate": 4.816974889361275e-05,
        "epoch": 0.8916461733363141,
        "step": 11965
    },
    {
        "loss": 2.295,
        "grad_norm": 2.7370564937591553,
        "learning_rate": 4.810958519473591e-05,
        "epoch": 0.891720694537596,
        "step": 11966
    },
    {
        "loss": 1.7954,
        "grad_norm": 2.5091824531555176,
        "learning_rate": 4.804944718850252e-05,
        "epoch": 0.8917952157388777,
        "step": 11967
    },
    {
        "loss": 2.6625,
        "grad_norm": 2.5716283321380615,
        "learning_rate": 4.798933490468892e-05,
        "epoch": 0.8918697369401595,
        "step": 11968
    },
    {
        "loss": 1.5651,
        "grad_norm": 4.0737175941467285,
        "learning_rate": 4.792924837305848e-05,
        "epoch": 0.8919442581414413,
        "step": 11969
    },
    {
        "loss": 2.2384,
        "grad_norm": 2.3523244857788086,
        "learning_rate": 4.786918762336228e-05,
        "epoch": 0.892018779342723,
        "step": 11970
    },
    {
        "loss": 1.7072,
        "grad_norm": 2.4774343967437744,
        "learning_rate": 4.7809152685338245e-05,
        "epoch": 0.8920933005440048,
        "step": 11971
    },
    {
        "loss": 2.3677,
        "grad_norm": 2.4968490600585938,
        "learning_rate": 4.774914358871152e-05,
        "epoch": 0.8921678217452865,
        "step": 11972
    },
    {
        "loss": 2.1906,
        "grad_norm": 3.082136869430542,
        "learning_rate": 4.768916036319481e-05,
        "epoch": 0.8922423429465683,
        "step": 11973
    },
    {
        "loss": 2.574,
        "grad_norm": 1.9862960577011108,
        "learning_rate": 4.762920303848753e-05,
        "epoch": 0.89231686414785,
        "step": 11974
    },
    {
        "loss": 1.8682,
        "grad_norm": 4.552300930023193,
        "learning_rate": 4.756927164427692e-05,
        "epoch": 0.8923913853491319,
        "step": 11975
    },
    {
        "loss": 2.44,
        "grad_norm": 2.8914687633514404,
        "learning_rate": 4.750936621023643e-05,
        "epoch": 0.8924659065504136,
        "step": 11976
    },
    {
        "loss": 2.482,
        "grad_norm": 2.5927178859710693,
        "learning_rate": 4.7449486766027584e-05,
        "epoch": 0.8925404277516954,
        "step": 11977
    },
    {
        "loss": 2.7859,
        "grad_norm": 1.9624484777450562,
        "learning_rate": 4.738963334129853e-05,
        "epoch": 0.8926149489529771,
        "step": 11978
    },
    {
        "loss": 2.5652,
        "grad_norm": 2.0628559589385986,
        "learning_rate": 4.732980596568457e-05,
        "epoch": 0.8926894701542589,
        "step": 11979
    },
    {
        "loss": 2.7424,
        "grad_norm": 2.8701555728912354,
        "learning_rate": 4.7270004668808397e-05,
        "epoch": 0.8927639913555406,
        "step": 11980
    },
    {
        "loss": 2.0752,
        "grad_norm": 4.348239421844482,
        "learning_rate": 4.72102294802795e-05,
        "epoch": 0.8928385125568225,
        "step": 11981
    },
    {
        "loss": 2.8536,
        "grad_norm": 2.225210428237915,
        "learning_rate": 4.71504804296944e-05,
        "epoch": 0.8929130337581042,
        "step": 11982
    },
    {
        "loss": 2.8062,
        "grad_norm": 1.4102545976638794,
        "learning_rate": 4.7090757546637045e-05,
        "epoch": 0.892987554959386,
        "step": 11983
    },
    {
        "loss": 1.4362,
        "grad_norm": 3.849092721939087,
        "learning_rate": 4.7031060860678114e-05,
        "epoch": 0.8930620761606677,
        "step": 11984
    },
    {
        "loss": 1.932,
        "grad_norm": 2.1082301139831543,
        "learning_rate": 4.69713904013752e-05,
        "epoch": 0.8931365973619495,
        "step": 11985
    },
    {
        "loss": 1.8718,
        "grad_norm": 1.4858989715576172,
        "learning_rate": 4.691174619827348e-05,
        "epoch": 0.8932111185632312,
        "step": 11986
    },
    {
        "loss": 2.4984,
        "grad_norm": 1.8862038850784302,
        "learning_rate": 4.6852128280904296e-05,
        "epoch": 0.893285639764513,
        "step": 11987
    },
    {
        "loss": 1.9439,
        "grad_norm": 3.4752228260040283,
        "learning_rate": 4.679253667878678e-05,
        "epoch": 0.8933601609657947,
        "step": 11988
    },
    {
        "loss": 2.706,
        "grad_norm": 2.5747416019439697,
        "learning_rate": 4.673297142142644e-05,
        "epoch": 0.8934346821670766,
        "step": 11989
    },
    {
        "loss": 1.7487,
        "grad_norm": 3.7775888442993164,
        "learning_rate": 4.667343253831623e-05,
        "epoch": 0.8935092033683583,
        "step": 11990
    },
    {
        "loss": 2.1598,
        "grad_norm": 3.034390687942505,
        "learning_rate": 4.6613920058935725e-05,
        "epoch": 0.8935837245696401,
        "step": 11991
    },
    {
        "loss": 2.0039,
        "grad_norm": 3.307478427886963,
        "learning_rate": 4.6554434012751304e-05,
        "epoch": 0.8936582457709218,
        "step": 11992
    },
    {
        "loss": 2.6816,
        "grad_norm": 2.972811698913574,
        "learning_rate": 4.649497442921672e-05,
        "epoch": 0.8937327669722036,
        "step": 11993
    },
    {
        "loss": 2.1154,
        "grad_norm": 3.0468649864196777,
        "learning_rate": 4.643554133777215e-05,
        "epoch": 0.8938072881734853,
        "step": 11994
    },
    {
        "loss": 2.3786,
        "grad_norm": 2.4149703979492188,
        "learning_rate": 4.637613476784517e-05,
        "epoch": 0.8938818093747671,
        "step": 11995
    },
    {
        "loss": 1.4111,
        "grad_norm": 3.7699320316314697,
        "learning_rate": 4.63167547488495e-05,
        "epoch": 0.8939563305760488,
        "step": 11996
    },
    {
        "loss": 2.7642,
        "grad_norm": 3.30125093460083,
        "learning_rate": 4.625740131018645e-05,
        "epoch": 0.8940308517773307,
        "step": 11997
    },
    {
        "loss": 1.8357,
        "grad_norm": 4.263961315155029,
        "learning_rate": 4.619807448124379e-05,
        "epoch": 0.8941053729786124,
        "step": 11998
    },
    {
        "loss": 2.2979,
        "grad_norm": 3.4050071239471436,
        "learning_rate": 4.613877429139598e-05,
        "epoch": 0.8941798941798942,
        "step": 11999
    },
    {
        "loss": 2.6683,
        "grad_norm": 2.54355525970459,
        "learning_rate": 4.607950077000474e-05,
        "epoch": 0.8942544153811759,
        "step": 12000
    },
    {
        "loss": 2.3425,
        "grad_norm": 1.914484977722168,
        "learning_rate": 4.602025394641823e-05,
        "epoch": 0.8943289365824577,
        "step": 12001
    },
    {
        "loss": 2.3815,
        "grad_norm": 3.3453903198242188,
        "learning_rate": 4.596103384997138e-05,
        "epoch": 0.8944034577837394,
        "step": 12002
    },
    {
        "loss": 2.1827,
        "grad_norm": 3.045039653778076,
        "learning_rate": 4.5901840509986216e-05,
        "epoch": 0.8944779789850212,
        "step": 12003
    },
    {
        "loss": 2.6545,
        "grad_norm": 2.7401671409606934,
        "learning_rate": 4.5842673955771195e-05,
        "epoch": 0.8945525001863031,
        "step": 12004
    },
    {
        "loss": 2.3791,
        "grad_norm": 2.5478944778442383,
        "learning_rate": 4.5783534216621506e-05,
        "epoch": 0.8946270213875848,
        "step": 12005
    },
    {
        "loss": 2.4353,
        "grad_norm": 2.0867514610290527,
        "learning_rate": 4.572442132181953e-05,
        "epoch": 0.8947015425888666,
        "step": 12006
    },
    {
        "loss": 2.0989,
        "grad_norm": 2.7071633338928223,
        "learning_rate": 4.566533530063348e-05,
        "epoch": 0.8947760637901483,
        "step": 12007
    },
    {
        "loss": 2.6353,
        "grad_norm": 2.2822651863098145,
        "learning_rate": 4.5606276182319165e-05,
        "epoch": 0.8948505849914301,
        "step": 12008
    },
    {
        "loss": 2.8843,
        "grad_norm": 3.4778378009796143,
        "learning_rate": 4.5547243996118546e-05,
        "epoch": 0.8949251061927118,
        "step": 12009
    },
    {
        "loss": 1.771,
        "grad_norm": 4.41768217086792,
        "learning_rate": 4.5488238771260314e-05,
        "epoch": 0.8949996273939936,
        "step": 12010
    },
    {
        "loss": 2.2622,
        "grad_norm": 3.1682331562042236,
        "learning_rate": 4.54292605369601e-05,
        "epoch": 0.8950741485952753,
        "step": 12011
    },
    {
        "loss": 1.9444,
        "grad_norm": 3.4082417488098145,
        "learning_rate": 4.537030932241972e-05,
        "epoch": 0.8951486697965572,
        "step": 12012
    },
    {
        "loss": 1.8146,
        "grad_norm": 5.150546550750732,
        "learning_rate": 4.53113851568281e-05,
        "epoch": 0.8952231909978389,
        "step": 12013
    },
    {
        "loss": 2.3214,
        "grad_norm": 3.412484645843506,
        "learning_rate": 4.525248806936042e-05,
        "epoch": 0.8952977121991207,
        "step": 12014
    },
    {
        "loss": 2.2453,
        "grad_norm": 2.7518248558044434,
        "learning_rate": 4.519361808917851e-05,
        "epoch": 0.8953722334004024,
        "step": 12015
    },
    {
        "loss": 2.9404,
        "grad_norm": 2.2280731201171875,
        "learning_rate": 4.5134775245430736e-05,
        "epoch": 0.8954467546016842,
        "step": 12016
    },
    {
        "loss": 2.9281,
        "grad_norm": 2.6118149757385254,
        "learning_rate": 4.5075959567252335e-05,
        "epoch": 0.8955212758029659,
        "step": 12017
    },
    {
        "loss": 2.4106,
        "grad_norm": 2.961766481399536,
        "learning_rate": 4.5017171083764776e-05,
        "epoch": 0.8955957970042477,
        "step": 12018
    },
    {
        "loss": 2.173,
        "grad_norm": 2.787904977798462,
        "learning_rate": 4.495840982407616e-05,
        "epoch": 0.8956703182055294,
        "step": 12019
    },
    {
        "loss": 2.7296,
        "grad_norm": 2.5719540119171143,
        "learning_rate": 4.4899675817280994e-05,
        "epoch": 0.8957448394068113,
        "step": 12020
    },
    {
        "loss": 1.9055,
        "grad_norm": 3.883833646774292,
        "learning_rate": 4.4840969092460675e-05,
        "epoch": 0.895819360608093,
        "step": 12021
    },
    {
        "loss": 2.9712,
        "grad_norm": 2.841294765472412,
        "learning_rate": 4.478228967868251e-05,
        "epoch": 0.8958938818093748,
        "step": 12022
    },
    {
        "loss": 2.5859,
        "grad_norm": 2.1852684020996094,
        "learning_rate": 4.4723637605000866e-05,
        "epoch": 0.8959684030106565,
        "step": 12023
    },
    {
        "loss": 1.6305,
        "grad_norm": 3.980841875076294,
        "learning_rate": 4.4665012900456216e-05,
        "epoch": 0.8960429242119383,
        "step": 12024
    },
    {
        "loss": 2.8338,
        "grad_norm": 2.578447103500366,
        "learning_rate": 4.4606415594075436e-05,
        "epoch": 0.89611744541322,
        "step": 12025
    },
    {
        "loss": 2.4945,
        "grad_norm": 2.5051910877227783,
        "learning_rate": 4.4547845714872304e-05,
        "epoch": 0.8961919666145018,
        "step": 12026
    },
    {
        "loss": 2.8109,
        "grad_norm": 2.849076271057129,
        "learning_rate": 4.448930329184625e-05,
        "epoch": 0.8962664878157836,
        "step": 12027
    },
    {
        "loss": 1.8703,
        "grad_norm": 4.084702491760254,
        "learning_rate": 4.443078835398388e-05,
        "epoch": 0.8963410090170654,
        "step": 12028
    },
    {
        "loss": 2.7411,
        "grad_norm": 3.249082565307617,
        "learning_rate": 4.437230093025777e-05,
        "epoch": 0.8964155302183471,
        "step": 12029
    },
    {
        "loss": 2.1457,
        "grad_norm": 2.165633201599121,
        "learning_rate": 4.4313841049626794e-05,
        "epoch": 0.8964900514196289,
        "step": 12030
    },
    {
        "loss": 1.5821,
        "grad_norm": 2.2901861667633057,
        "learning_rate": 4.4255408741036584e-05,
        "epoch": 0.8965645726209106,
        "step": 12031
    },
    {
        "loss": 2.823,
        "grad_norm": 2.762568712234497,
        "learning_rate": 4.41970040334188e-05,
        "epoch": 0.8966390938221924,
        "step": 12032
    },
    {
        "loss": 1.5732,
        "grad_norm": 2.9722304344177246,
        "learning_rate": 4.41386269556914e-05,
        "epoch": 0.8967136150234741,
        "step": 12033
    },
    {
        "loss": 2.0301,
        "grad_norm": 2.8770759105682373,
        "learning_rate": 4.408027753675902e-05,
        "epoch": 0.896788136224756,
        "step": 12034
    },
    {
        "loss": 2.7435,
        "grad_norm": 2.180558443069458,
        "learning_rate": 4.402195580551225e-05,
        "epoch": 0.8968626574260377,
        "step": 12035
    },
    {
        "loss": 2.2955,
        "grad_norm": 3.0234286785125732,
        "learning_rate": 4.396366179082797e-05,
        "epoch": 0.8969371786273195,
        "step": 12036
    },
    {
        "loss": 1.8759,
        "grad_norm": 4.322848320007324,
        "learning_rate": 4.390539552156985e-05,
        "epoch": 0.8970116998286012,
        "step": 12037
    },
    {
        "loss": 2.5168,
        "grad_norm": 3.7030856609344482,
        "learning_rate": 4.384715702658689e-05,
        "epoch": 0.897086221029883,
        "step": 12038
    },
    {
        "loss": 1.5815,
        "grad_norm": 3.6224491596221924,
        "learning_rate": 4.378894633471525e-05,
        "epoch": 0.8971607422311648,
        "step": 12039
    },
    {
        "loss": 2.4386,
        "grad_norm": 2.9362103939056396,
        "learning_rate": 4.3730763474776736e-05,
        "epoch": 0.8972352634324465,
        "step": 12040
    },
    {
        "loss": 1.956,
        "grad_norm": 3.8222758769989014,
        "learning_rate": 4.367260847557979e-05,
        "epoch": 0.8973097846337283,
        "step": 12041
    },
    {
        "loss": 1.7551,
        "grad_norm": 2.7979273796081543,
        "learning_rate": 4.361448136591877e-05,
        "epoch": 0.89738430583501,
        "step": 12042
    },
    {
        "loss": 2.5046,
        "grad_norm": 3.3754959106445312,
        "learning_rate": 4.3556382174574174e-05,
        "epoch": 0.8974588270362919,
        "step": 12043
    },
    {
        "loss": 2.4332,
        "grad_norm": 3.2623932361602783,
        "learning_rate": 4.349831093031309e-05,
        "epoch": 0.8975333482375736,
        "step": 12044
    },
    {
        "loss": 2.2432,
        "grad_norm": 6.05411958694458,
        "learning_rate": 4.34402676618882e-05,
        "epoch": 0.8976078694388554,
        "step": 12045
    },
    {
        "loss": 2.1776,
        "grad_norm": 2.5368144512176514,
        "learning_rate": 4.338225239803898e-05,
        "epoch": 0.8976823906401371,
        "step": 12046
    },
    {
        "loss": 2.1924,
        "grad_norm": 3.7406983375549316,
        "learning_rate": 4.332426516749026e-05,
        "epoch": 0.8977569118414189,
        "step": 12047
    },
    {
        "loss": 2.4548,
        "grad_norm": 2.4993784427642822,
        "learning_rate": 4.326630599895374e-05,
        "epoch": 0.8978314330427006,
        "step": 12048
    },
    {
        "loss": 2.2309,
        "grad_norm": 3.3299336433410645,
        "learning_rate": 4.3208374921126795e-05,
        "epoch": 0.8979059542439825,
        "step": 12049
    },
    {
        "loss": 2.4998,
        "grad_norm": 3.642840623855591,
        "learning_rate": 4.315047196269287e-05,
        "epoch": 0.8979804754452642,
        "step": 12050
    },
    {
        "loss": 2.0037,
        "grad_norm": 3.0927443504333496,
        "learning_rate": 4.309259715232187e-05,
        "epoch": 0.898054996646546,
        "step": 12051
    },
    {
        "loss": 2.6405,
        "grad_norm": 2.7289974689483643,
        "learning_rate": 4.303475051866941e-05,
        "epoch": 0.8981295178478277,
        "step": 12052
    },
    {
        "loss": 1.6272,
        "grad_norm": 3.858332395553589,
        "learning_rate": 4.297693209037709e-05,
        "epoch": 0.8982040390491095,
        "step": 12053
    },
    {
        "loss": 2.7383,
        "grad_norm": 2.450127601623535,
        "learning_rate": 4.2919141896072935e-05,
        "epoch": 0.8982785602503912,
        "step": 12054
    },
    {
        "loss": 2.3959,
        "grad_norm": 3.5224006175994873,
        "learning_rate": 4.286137996437067e-05,
        "epoch": 0.898353081451673,
        "step": 12055
    },
    {
        "loss": 1.905,
        "grad_norm": 3.775038242340088,
        "learning_rate": 4.2803646323870004e-05,
        "epoch": 0.8984276026529547,
        "step": 12056
    },
    {
        "loss": 2.0421,
        "grad_norm": 3.0677945613861084,
        "learning_rate": 4.2745941003157094e-05,
        "epoch": 0.8985021238542366,
        "step": 12057
    },
    {
        "loss": 3.3516,
        "grad_norm": 3.8448214530944824,
        "learning_rate": 4.268826403080328e-05,
        "epoch": 0.8985766450555183,
        "step": 12058
    },
    {
        "loss": 1.7365,
        "grad_norm": 4.145916938781738,
        "learning_rate": 4.2630615435366694e-05,
        "epoch": 0.8986511662568001,
        "step": 12059
    },
    {
        "loss": 2.2181,
        "grad_norm": 4.555772304534912,
        "learning_rate": 4.2572995245390744e-05,
        "epoch": 0.8987256874580818,
        "step": 12060
    },
    {
        "loss": 2.7297,
        "grad_norm": 4.193958759307861,
        "learning_rate": 4.2515403489405335e-05,
        "epoch": 0.8988002086593636,
        "step": 12061
    },
    {
        "loss": 2.3547,
        "grad_norm": 3.18105411529541,
        "learning_rate": 4.245784019592591e-05,
        "epoch": 0.8988747298606453,
        "step": 12062
    },
    {
        "loss": 2.753,
        "grad_norm": 1.8469154834747314,
        "learning_rate": 4.240030539345385e-05,
        "epoch": 0.8989492510619271,
        "step": 12063
    },
    {
        "loss": 2.776,
        "grad_norm": 2.5756161212921143,
        "learning_rate": 4.2342799110476726e-05,
        "epoch": 0.8990237722632088,
        "step": 12064
    },
    {
        "loss": 2.2542,
        "grad_norm": 4.107778549194336,
        "learning_rate": 4.228532137546768e-05,
        "epoch": 0.8990982934644907,
        "step": 12065
    },
    {
        "loss": 1.5991,
        "grad_norm": 4.6261749267578125,
        "learning_rate": 4.222787221688584e-05,
        "epoch": 0.8991728146657724,
        "step": 12066
    },
    {
        "loss": 2.4035,
        "grad_norm": 3.694307804107666,
        "learning_rate": 4.2170451663176e-05,
        "epoch": 0.8992473358670542,
        "step": 12067
    },
    {
        "loss": 2.1803,
        "grad_norm": 2.313764810562134,
        "learning_rate": 4.211305974276921e-05,
        "epoch": 0.8993218570683359,
        "step": 12068
    },
    {
        "loss": 2.3197,
        "grad_norm": 3.5582823753356934,
        "learning_rate": 4.205569648408197e-05,
        "epoch": 0.8993963782696177,
        "step": 12069
    },
    {
        "loss": 2.2741,
        "grad_norm": 2.8168697357177734,
        "learning_rate": 4.199836191551673e-05,
        "epoch": 0.8994708994708994,
        "step": 12070
    },
    {
        "loss": 2.3297,
        "grad_norm": 3.1265711784362793,
        "learning_rate": 4.194105606546157e-05,
        "epoch": 0.8995454206721812,
        "step": 12071
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.275233507156372,
        "learning_rate": 4.188377896229073e-05,
        "epoch": 0.899619941873463,
        "step": 12072
    },
    {
        "loss": 2.4576,
        "grad_norm": 2.810652494430542,
        "learning_rate": 4.1826530634363756e-05,
        "epoch": 0.8996944630747448,
        "step": 12073
    },
    {
        "loss": 2.8386,
        "grad_norm": 4.086860656738281,
        "learning_rate": 4.17693111100264e-05,
        "epoch": 0.8997689842760266,
        "step": 12074
    },
    {
        "loss": 1.6278,
        "grad_norm": 3.3332107067108154,
        "learning_rate": 4.171212041760983e-05,
        "epoch": 0.8998435054773083,
        "step": 12075
    },
    {
        "loss": 2.7213,
        "grad_norm": 3.1707775592803955,
        "learning_rate": 4.165495858543087e-05,
        "epoch": 0.8999180266785901,
        "step": 12076
    },
    {
        "loss": 1.794,
        "grad_norm": 4.2514567375183105,
        "learning_rate": 4.1597825641792566e-05,
        "epoch": 0.8999925478798718,
        "step": 12077
    },
    {
        "loss": 2.0286,
        "grad_norm": 3.2950448989868164,
        "learning_rate": 4.154072161498287e-05,
        "epoch": 0.9000670690811536,
        "step": 12078
    },
    {
        "loss": 1.4612,
        "grad_norm": 1.379522442817688,
        "learning_rate": 4.148364653327618e-05,
        "epoch": 0.9001415902824353,
        "step": 12079
    },
    {
        "loss": 1.9229,
        "grad_norm": 3.1847875118255615,
        "learning_rate": 4.142660042493214e-05,
        "epoch": 0.9002161114837172,
        "step": 12080
    },
    {
        "loss": 1.493,
        "grad_norm": 4.046321392059326,
        "learning_rate": 4.1369583318196036e-05,
        "epoch": 0.9002906326849989,
        "step": 12081
    },
    {
        "loss": 2.3639,
        "grad_norm": 2.4796576499938965,
        "learning_rate": 4.1312595241299156e-05,
        "epoch": 0.9003651538862807,
        "step": 12082
    },
    {
        "loss": 1.9883,
        "grad_norm": 3.1993143558502197,
        "learning_rate": 4.125563622245786e-05,
        "epoch": 0.9004396750875624,
        "step": 12083
    },
    {
        "loss": 2.3713,
        "grad_norm": 3.3639440536499023,
        "learning_rate": 4.11987062898747e-05,
        "epoch": 0.9005141962888442,
        "step": 12084
    },
    {
        "loss": 1.808,
        "grad_norm": 3.6861624717712402,
        "learning_rate": 4.114180547173743e-05,
        "epoch": 0.9005887174901259,
        "step": 12085
    },
    {
        "loss": 2.4558,
        "grad_norm": 3.006439685821533,
        "learning_rate": 4.10849337962195e-05,
        "epoch": 0.9006632386914077,
        "step": 12086
    },
    {
        "loss": 2.8307,
        "grad_norm": 2.7881054878234863,
        "learning_rate": 4.1028091291479856e-05,
        "epoch": 0.9007377598926894,
        "step": 12087
    },
    {
        "loss": 2.3255,
        "grad_norm": 3.1289119720458984,
        "learning_rate": 4.097127798566327e-05,
        "epoch": 0.9008122810939713,
        "step": 12088
    },
    {
        "loss": 2.2133,
        "grad_norm": 4.223574161529541,
        "learning_rate": 4.091449390689981e-05,
        "epoch": 0.900886802295253,
        "step": 12089
    },
    {
        "loss": 2.7966,
        "grad_norm": 1.7949963808059692,
        "learning_rate": 4.085773908330506e-05,
        "epoch": 0.9009613234965348,
        "step": 12090
    },
    {
        "loss": 1.9369,
        "grad_norm": 4.055802822113037,
        "learning_rate": 4.080101354298012e-05,
        "epoch": 0.9010358446978165,
        "step": 12091
    },
    {
        "loss": 1.8889,
        "grad_norm": 4.492603302001953,
        "learning_rate": 4.074431731401188e-05,
        "epoch": 0.9011103658990983,
        "step": 12092
    },
    {
        "loss": 2.0014,
        "grad_norm": 2.9008162021636963,
        "learning_rate": 4.068765042447239e-05,
        "epoch": 0.90118488710038,
        "step": 12093
    },
    {
        "loss": 2.0082,
        "grad_norm": 3.4732251167297363,
        "learning_rate": 4.0631012902419175e-05,
        "epoch": 0.9012594083016618,
        "step": 12094
    },
    {
        "loss": 2.401,
        "grad_norm": 3.4799530506134033,
        "learning_rate": 4.057440477589555e-05,
        "epoch": 0.9013339295029436,
        "step": 12095
    },
    {
        "loss": 1.8064,
        "grad_norm": 2.4511501789093018,
        "learning_rate": 4.051782607292982e-05,
        "epoch": 0.9014084507042254,
        "step": 12096
    },
    {
        "loss": 2.3577,
        "grad_norm": 3.4613208770751953,
        "learning_rate": 4.046127682153632e-05,
        "epoch": 0.9014829719055071,
        "step": 12097
    },
    {
        "loss": 2.7246,
        "grad_norm": 2.3720130920410156,
        "learning_rate": 4.0404757049713935e-05,
        "epoch": 0.9015574931067889,
        "step": 12098
    },
    {
        "loss": 2.6672,
        "grad_norm": 3.061535358428955,
        "learning_rate": 4.034826678544782e-05,
        "epoch": 0.9016320143080706,
        "step": 12099
    },
    {
        "loss": 1.6084,
        "grad_norm": 3.409778356552124,
        "learning_rate": 4.029180605670804e-05,
        "epoch": 0.9017065355093524,
        "step": 12100
    },
    {
        "loss": 2.9757,
        "grad_norm": 2.552839517593384,
        "learning_rate": 4.023537489145003e-05,
        "epoch": 0.9017810567106341,
        "step": 12101
    },
    {
        "loss": 2.3955,
        "grad_norm": 2.5163896083831787,
        "learning_rate": 4.017897331761494e-05,
        "epoch": 0.901855577911916,
        "step": 12102
    },
    {
        "loss": 2.2154,
        "grad_norm": 4.099248886108398,
        "learning_rate": 4.012260136312893e-05,
        "epoch": 0.9019300991131977,
        "step": 12103
    },
    {
        "loss": 2.3639,
        "grad_norm": 3.530533790588379,
        "learning_rate": 4.006625905590349e-05,
        "epoch": 0.9020046203144795,
        "step": 12104
    },
    {
        "loss": 2.661,
        "grad_norm": 3.6703264713287354,
        "learning_rate": 4.000994642383579e-05,
        "epoch": 0.9020791415157612,
        "step": 12105
    },
    {
        "loss": 1.9344,
        "grad_norm": 3.925124168395996,
        "learning_rate": 3.995366349480787e-05,
        "epoch": 0.902153662717043,
        "step": 12106
    },
    {
        "loss": 2.234,
        "grad_norm": 3.4469475746154785,
        "learning_rate": 3.98974102966872e-05,
        "epoch": 0.9022281839183247,
        "step": 12107
    },
    {
        "loss": 2.8147,
        "grad_norm": 2.7523484230041504,
        "learning_rate": 3.984118685732687e-05,
        "epoch": 0.9023027051196065,
        "step": 12108
    },
    {
        "loss": 2.6892,
        "grad_norm": 3.5592288970947266,
        "learning_rate": 3.9784993204564556e-05,
        "epoch": 0.9023772263208883,
        "step": 12109
    },
    {
        "loss": 1.6799,
        "grad_norm": 3.8396527767181396,
        "learning_rate": 3.972882936622386e-05,
        "epoch": 0.9024517475221701,
        "step": 12110
    },
    {
        "loss": 2.0991,
        "grad_norm": 2.8540236949920654,
        "learning_rate": 3.967269537011316e-05,
        "epoch": 0.9025262687234519,
        "step": 12111
    },
    {
        "loss": 2.6634,
        "grad_norm": 2.5336012840270996,
        "learning_rate": 3.9616591244026457e-05,
        "epoch": 0.9026007899247336,
        "step": 12112
    },
    {
        "loss": 2.5848,
        "grad_norm": 3.519939661026001,
        "learning_rate": 3.956051701574257e-05,
        "epoch": 0.9026753111260154,
        "step": 12113
    },
    {
        "loss": 2.7642,
        "grad_norm": 2.452265501022339,
        "learning_rate": 3.950447271302562e-05,
        "epoch": 0.9027498323272971,
        "step": 12114
    },
    {
        "loss": 2.1932,
        "grad_norm": 3.1302201747894287,
        "learning_rate": 3.9448458363625185e-05,
        "epoch": 0.9028243535285789,
        "step": 12115
    },
    {
        "loss": 2.9399,
        "grad_norm": 2.867079734802246,
        "learning_rate": 3.939247399527559e-05,
        "epoch": 0.9028988747298606,
        "step": 12116
    },
    {
        "loss": 2.2749,
        "grad_norm": 3.2960572242736816,
        "learning_rate": 3.9336519635696855e-05,
        "epoch": 0.9029733959311425,
        "step": 12117
    },
    {
        "loss": 3.0984,
        "grad_norm": 2.9043848514556885,
        "learning_rate": 3.928059531259337e-05,
        "epoch": 0.9030479171324242,
        "step": 12118
    },
    {
        "loss": 2.8574,
        "grad_norm": 3.045414924621582,
        "learning_rate": 3.9224701053655444e-05,
        "epoch": 0.903122438333706,
        "step": 12119
    },
    {
        "loss": 2.029,
        "grad_norm": 4.778526306152344,
        "learning_rate": 3.916883688655807e-05,
        "epoch": 0.9031969595349877,
        "step": 12120
    },
    {
        "loss": 2.3844,
        "grad_norm": 3.1132946014404297,
        "learning_rate": 3.9113002838961254e-05,
        "epoch": 0.9032714807362695,
        "step": 12121
    },
    {
        "loss": 2.1064,
        "grad_norm": 2.933542490005493,
        "learning_rate": 3.905719893851051e-05,
        "epoch": 0.9033460019375512,
        "step": 12122
    },
    {
        "loss": 1.9276,
        "grad_norm": 2.730982542037964,
        "learning_rate": 3.900142521283605e-05,
        "epoch": 0.903420523138833,
        "step": 12123
    },
    {
        "loss": 2.1334,
        "grad_norm": 3.3610522747039795,
        "learning_rate": 3.8945681689553195e-05,
        "epoch": 0.9034950443401147,
        "step": 12124
    },
    {
        "loss": 2.7557,
        "grad_norm": 4.038014888763428,
        "learning_rate": 3.888996839626259e-05,
        "epoch": 0.9035695655413966,
        "step": 12125
    },
    {
        "loss": 2.2845,
        "grad_norm": 1.7973636388778687,
        "learning_rate": 3.88342853605496e-05,
        "epoch": 0.9036440867426783,
        "step": 12126
    },
    {
        "loss": 2.1931,
        "grad_norm": 4.189895153045654,
        "learning_rate": 3.877863260998461e-05,
        "epoch": 0.9037186079439601,
        "step": 12127
    },
    {
        "loss": 2.8938,
        "grad_norm": 3.0654103755950928,
        "learning_rate": 3.872301017212349e-05,
        "epoch": 0.9037931291452418,
        "step": 12128
    },
    {
        "loss": 2.1848,
        "grad_norm": 3.0298023223876953,
        "learning_rate": 3.8667418074506215e-05,
        "epoch": 0.9038676503465236,
        "step": 12129
    },
    {
        "loss": 2.9125,
        "grad_norm": 2.474172592163086,
        "learning_rate": 3.8611856344658616e-05,
        "epoch": 0.9039421715478053,
        "step": 12130
    },
    {
        "loss": 2.5013,
        "grad_norm": 2.0727431774139404,
        "learning_rate": 3.855632501009101e-05,
        "epoch": 0.9040166927490871,
        "step": 12131
    },
    {
        "loss": 2.3778,
        "grad_norm": 2.7539708614349365,
        "learning_rate": 3.85008240982987e-05,
        "epoch": 0.9040912139503688,
        "step": 12132
    },
    {
        "loss": 1.8089,
        "grad_norm": 5.139599323272705,
        "learning_rate": 3.844535363676218e-05,
        "epoch": 0.9041657351516507,
        "step": 12133
    },
    {
        "loss": 2.3772,
        "grad_norm": 3.1133759021759033,
        "learning_rate": 3.8389913652946507e-05,
        "epoch": 0.9042402563529324,
        "step": 12134
    },
    {
        "loss": 1.8542,
        "grad_norm": 1.550534725189209,
        "learning_rate": 3.833450417430207e-05,
        "epoch": 0.9043147775542142,
        "step": 12135
    },
    {
        "loss": 2.1744,
        "grad_norm": 4.29428243637085,
        "learning_rate": 3.827912522826374e-05,
        "epoch": 0.9043892987554959,
        "step": 12136
    },
    {
        "loss": 1.2681,
        "grad_norm": 3.8963961601257324,
        "learning_rate": 3.8223776842251466e-05,
        "epoch": 0.9044638199567777,
        "step": 12137
    },
    {
        "loss": 2.8548,
        "grad_norm": 2.4839236736297607,
        "learning_rate": 3.8168459043669976e-05,
        "epoch": 0.9045383411580594,
        "step": 12138
    },
    {
        "loss": 2.5328,
        "grad_norm": 2.6931777000427246,
        "learning_rate": 3.81131718599091e-05,
        "epoch": 0.9046128623593412,
        "step": 12139
    },
    {
        "loss": 2.1844,
        "grad_norm": 2.745405673980713,
        "learning_rate": 3.805791531834326e-05,
        "epoch": 0.904687383560623,
        "step": 12140
    },
    {
        "loss": 2.8961,
        "grad_norm": 2.2703399658203125,
        "learning_rate": 3.800268944633175e-05,
        "epoch": 0.9047619047619048,
        "step": 12141
    },
    {
        "loss": 2.2234,
        "grad_norm": 2.9678542613983154,
        "learning_rate": 3.7947494271218654e-05,
        "epoch": 0.9048364259631865,
        "step": 12142
    },
    {
        "loss": 2.4114,
        "grad_norm": 3.2460529804229736,
        "learning_rate": 3.789232982033304e-05,
        "epoch": 0.9049109471644683,
        "step": 12143
    },
    {
        "loss": 2.1989,
        "grad_norm": 3.122537136077881,
        "learning_rate": 3.783719612098846e-05,
        "epoch": 0.90498546836575,
        "step": 12144
    },
    {
        "loss": 2.0263,
        "grad_norm": 2.4640161991119385,
        "learning_rate": 3.778209320048364e-05,
        "epoch": 0.9050599895670318,
        "step": 12145
    },
    {
        "loss": 2.3537,
        "grad_norm": 3.2735955715179443,
        "learning_rate": 3.772702108610171e-05,
        "epoch": 0.9051345107683136,
        "step": 12146
    },
    {
        "loss": 1.869,
        "grad_norm": 2.489983320236206,
        "learning_rate": 3.7671979805110594e-05,
        "epoch": 0.9052090319695953,
        "step": 12147
    },
    {
        "loss": 2.2119,
        "grad_norm": 1.9362766742706299,
        "learning_rate": 3.761696938476333e-05,
        "epoch": 0.9052835531708772,
        "step": 12148
    },
    {
        "loss": 2.5294,
        "grad_norm": 1.433432698249817,
        "learning_rate": 3.756198985229697e-05,
        "epoch": 0.9053580743721589,
        "step": 12149
    },
    {
        "loss": 2.4866,
        "grad_norm": 2.797398805618286,
        "learning_rate": 3.750704123493404e-05,
        "epoch": 0.9054325955734407,
        "step": 12150
    },
    {
        "loss": 1.8698,
        "grad_norm": 3.953702211380005,
        "learning_rate": 3.745212355988119e-05,
        "epoch": 0.9055071167747224,
        "step": 12151
    },
    {
        "loss": 1.4285,
        "grad_norm": 4.061319351196289,
        "learning_rate": 3.739723685432993e-05,
        "epoch": 0.9055816379760042,
        "step": 12152
    },
    {
        "loss": 2.3061,
        "grad_norm": 2.3095855712890625,
        "learning_rate": 3.734238114545665e-05,
        "epoch": 0.9056561591772859,
        "step": 12153
    },
    {
        "loss": 1.8162,
        "grad_norm": 3.938028573989868,
        "learning_rate": 3.7287556460422016e-05,
        "epoch": 0.9057306803785677,
        "step": 12154
    },
    {
        "loss": 0.9912,
        "grad_norm": 4.589860439300537,
        "learning_rate": 3.72327628263717e-05,
        "epoch": 0.9058052015798495,
        "step": 12155
    },
    {
        "loss": 1.4136,
        "grad_norm": 3.8013508319854736,
        "learning_rate": 3.717800027043576e-05,
        "epoch": 0.9058797227811313,
        "step": 12156
    },
    {
        "loss": 2.4173,
        "grad_norm": 2.818495750427246,
        "learning_rate": 3.712326881972894e-05,
        "epoch": 0.905954243982413,
        "step": 12157
    },
    {
        "loss": 2.4859,
        "grad_norm": 2.6121978759765625,
        "learning_rate": 3.706856850135041e-05,
        "epoch": 0.9060287651836948,
        "step": 12158
    },
    {
        "loss": 1.6944,
        "grad_norm": 4.160549163818359,
        "learning_rate": 3.701389934238444e-05,
        "epoch": 0.9061032863849765,
        "step": 12159
    },
    {
        "loss": 2.3166,
        "grad_norm": 3.03643536567688,
        "learning_rate": 3.695926136989909e-05,
        "epoch": 0.9061778075862583,
        "step": 12160
    },
    {
        "loss": 2.4519,
        "grad_norm": 3.525151252746582,
        "learning_rate": 3.690465461094773e-05,
        "epoch": 0.90625232878754,
        "step": 12161
    },
    {
        "loss": 2.8478,
        "grad_norm": 3.106431245803833,
        "learning_rate": 3.685007909256775e-05,
        "epoch": 0.9063268499888218,
        "step": 12162
    },
    {
        "loss": 1.2429,
        "grad_norm": 2.5094995498657227,
        "learning_rate": 3.6795534841781475e-05,
        "epoch": 0.9064013711901036,
        "step": 12163
    },
    {
        "loss": 1.3467,
        "grad_norm": 3.840876340866089,
        "learning_rate": 3.674102188559545e-05,
        "epoch": 0.9064758923913854,
        "step": 12164
    },
    {
        "loss": 2.089,
        "grad_norm": 2.6505675315856934,
        "learning_rate": 3.6686540251000756e-05,
        "epoch": 0.9065504135926671,
        "step": 12165
    },
    {
        "loss": 2.311,
        "grad_norm": 2.8908281326293945,
        "learning_rate": 3.663208996497317e-05,
        "epoch": 0.9066249347939489,
        "step": 12166
    },
    {
        "loss": 2.6904,
        "grad_norm": 2.6662113666534424,
        "learning_rate": 3.657767105447264e-05,
        "epoch": 0.9066994559952306,
        "step": 12167
    },
    {
        "loss": 2.3118,
        "grad_norm": 3.247849941253662,
        "learning_rate": 3.6523283546444065e-05,
        "epoch": 0.9067739771965124,
        "step": 12168
    },
    {
        "loss": 2.6444,
        "grad_norm": 2.741633415222168,
        "learning_rate": 3.646892746781607e-05,
        "epoch": 0.9068484983977941,
        "step": 12169
    },
    {
        "loss": 2.0733,
        "grad_norm": 4.228607654571533,
        "learning_rate": 3.641460284550242e-05,
        "epoch": 0.906923019599076,
        "step": 12170
    },
    {
        "loss": 2.3984,
        "grad_norm": 2.167109489440918,
        "learning_rate": 3.6360309706400944e-05,
        "epoch": 0.9069975408003577,
        "step": 12171
    },
    {
        "loss": 2.6358,
        "grad_norm": 1.2818573713302612,
        "learning_rate": 3.6306048077393806e-05,
        "epoch": 0.9070720620016395,
        "step": 12172
    },
    {
        "loss": 2.0261,
        "grad_norm": 3.677122116088867,
        "learning_rate": 3.6251817985348004e-05,
        "epoch": 0.9071465832029212,
        "step": 12173
    },
    {
        "loss": 1.9291,
        "grad_norm": 1.7946512699127197,
        "learning_rate": 3.619761945711442e-05,
        "epoch": 0.907221104404203,
        "step": 12174
    },
    {
        "loss": 2.4127,
        "grad_norm": 3.4496021270751953,
        "learning_rate": 3.614345251952848e-05,
        "epoch": 0.9072956256054847,
        "step": 12175
    },
    {
        "loss": 0.9985,
        "grad_norm": 4.1629743576049805,
        "learning_rate": 3.608931719941019e-05,
        "epoch": 0.9073701468067665,
        "step": 12176
    },
    {
        "loss": 2.2547,
        "grad_norm": 3.273746967315674,
        "learning_rate": 3.603521352356364e-05,
        "epoch": 0.9074446680080482,
        "step": 12177
    },
    {
        "loss": 2.0144,
        "grad_norm": 4.745067596435547,
        "learning_rate": 3.5981141518777206e-05,
        "epoch": 0.9075191892093301,
        "step": 12178
    },
    {
        "loss": 1.9787,
        "grad_norm": 2.835549831390381,
        "learning_rate": 3.592710121182407e-05,
        "epoch": 0.9075937104106118,
        "step": 12179
    },
    {
        "loss": 2.4165,
        "grad_norm": 1.8874887228012085,
        "learning_rate": 3.587309262946087e-05,
        "epoch": 0.9076682316118936,
        "step": 12180
    },
    {
        "loss": 2.4549,
        "grad_norm": 2.1361544132232666,
        "learning_rate": 3.581911579842937e-05,
        "epoch": 0.9077427528131754,
        "step": 12181
    },
    {
        "loss": 2.4586,
        "grad_norm": 2.3404109477996826,
        "learning_rate": 3.576517074545505e-05,
        "epoch": 0.9078172740144571,
        "step": 12182
    },
    {
        "loss": 2.4605,
        "grad_norm": 3.0638153553009033,
        "learning_rate": 3.571125749724808e-05,
        "epoch": 0.9078917952157389,
        "step": 12183
    },
    {
        "loss": 2.6109,
        "grad_norm": 2.3403995037078857,
        "learning_rate": 3.565737608050259e-05,
        "epoch": 0.9079663164170206,
        "step": 12184
    },
    {
        "loss": 1.8286,
        "grad_norm": 4.104869365692139,
        "learning_rate": 3.5603526521896914e-05,
        "epoch": 0.9080408376183025,
        "step": 12185
    },
    {
        "loss": 2.7866,
        "grad_norm": 3.026491403579712,
        "learning_rate": 3.5549708848093946e-05,
        "epoch": 0.9081153588195842,
        "step": 12186
    },
    {
        "loss": 2.4309,
        "grad_norm": 3.5139589309692383,
        "learning_rate": 3.549592308574046e-05,
        "epoch": 0.908189880020866,
        "step": 12187
    },
    {
        "loss": 2.1572,
        "grad_norm": 3.191984176635742,
        "learning_rate": 3.54421692614676e-05,
        "epoch": 0.9082644012221477,
        "step": 12188
    },
    {
        "loss": 2.4581,
        "grad_norm": 4.550690174102783,
        "learning_rate": 3.538844740189046e-05,
        "epoch": 0.9083389224234295,
        "step": 12189
    },
    {
        "loss": 2.0161,
        "grad_norm": 2.1404502391815186,
        "learning_rate": 3.533475753360874e-05,
        "epoch": 0.9084134436247112,
        "step": 12190
    },
    {
        "loss": 1.797,
        "grad_norm": 2.092164993286133,
        "learning_rate": 3.5281099683205954e-05,
        "epoch": 0.908487964825993,
        "step": 12191
    },
    {
        "loss": 1.9213,
        "grad_norm": 2.8780527114868164,
        "learning_rate": 3.522747387724984e-05,
        "epoch": 0.9085624860272747,
        "step": 12192
    },
    {
        "loss": 2.59,
        "grad_norm": 2.103870391845703,
        "learning_rate": 3.5173880142292215e-05,
        "epoch": 0.9086370072285566,
        "step": 12193
    },
    {
        "loss": 2.6957,
        "grad_norm": 2.2042620182037354,
        "learning_rate": 3.512031850486928e-05,
        "epoch": 0.9087115284298383,
        "step": 12194
    },
    {
        "loss": 1.8332,
        "grad_norm": 2.9247658252716064,
        "learning_rate": 3.5066788991500944e-05,
        "epoch": 0.9087860496311201,
        "step": 12195
    },
    {
        "loss": 2.8294,
        "grad_norm": 3.706462860107422,
        "learning_rate": 3.501329162869168e-05,
        "epoch": 0.9088605708324018,
        "step": 12196
    },
    {
        "loss": 2.699,
        "grad_norm": 3.0409128665924072,
        "learning_rate": 3.49598264429296e-05,
        "epoch": 0.9089350920336836,
        "step": 12197
    },
    {
        "loss": 1.8713,
        "grad_norm": 3.0150580406188965,
        "learning_rate": 3.4906393460687006e-05,
        "epoch": 0.9090096132349653,
        "step": 12198
    },
    {
        "loss": 2.842,
        "grad_norm": 3.7509102821350098,
        "learning_rate": 3.485299270842062e-05,
        "epoch": 0.9090841344362471,
        "step": 12199
    },
    {
        "loss": 2.9737,
        "grad_norm": 2.3848836421966553,
        "learning_rate": 3.479962421257049e-05,
        "epoch": 0.9091586556375288,
        "step": 12200
    },
    {
        "loss": 1.7765,
        "grad_norm": 3.1135928630828857,
        "learning_rate": 3.474628799956143e-05,
        "epoch": 0.9092331768388107,
        "step": 12201
    },
    {
        "loss": 2.4103,
        "grad_norm": 2.163323163986206,
        "learning_rate": 3.469298409580186e-05,
        "epoch": 0.9093076980400924,
        "step": 12202
    },
    {
        "loss": 1.9529,
        "grad_norm": 4.284180641174316,
        "learning_rate": 3.463971252768413e-05,
        "epoch": 0.9093822192413742,
        "step": 12203
    },
    {
        "loss": 2.3214,
        "grad_norm": 3.0058183670043945,
        "learning_rate": 3.458647332158497e-05,
        "epoch": 0.9094567404426559,
        "step": 12204
    },
    {
        "loss": 2.4902,
        "grad_norm": 2.8016579151153564,
        "learning_rate": 3.4533266503864634e-05,
        "epoch": 0.9095312616439377,
        "step": 12205
    },
    {
        "loss": 2.2091,
        "grad_norm": 3.4556872844696045,
        "learning_rate": 3.448009210086779e-05,
        "epoch": 0.9096057828452194,
        "step": 12206
    },
    {
        "loss": 2.7549,
        "grad_norm": 3.619072914123535,
        "learning_rate": 3.442695013892272e-05,
        "epoch": 0.9096803040465012,
        "step": 12207
    },
    {
        "loss": 2.6417,
        "grad_norm": 3.434779167175293,
        "learning_rate": 3.4373840644341726e-05,
        "epoch": 0.909754825247783,
        "step": 12208
    },
    {
        "loss": 2.6079,
        "grad_norm": 1.9178781509399414,
        "learning_rate": 3.4320763643420996e-05,
        "epoch": 0.9098293464490648,
        "step": 12209
    },
    {
        "loss": 2.0413,
        "grad_norm": 3.7169880867004395,
        "learning_rate": 3.4267719162440884e-05,
        "epoch": 0.9099038676503465,
        "step": 12210
    },
    {
        "loss": 0.987,
        "grad_norm": 2.474951982498169,
        "learning_rate": 3.421470722766531e-05,
        "epoch": 0.9099783888516283,
        "step": 12211
    },
    {
        "loss": 2.4726,
        "grad_norm": 2.2258405685424805,
        "learning_rate": 3.4161727865342244e-05,
        "epoch": 0.91005291005291,
        "step": 12212
    },
    {
        "loss": 1.8641,
        "grad_norm": 2.7519402503967285,
        "learning_rate": 3.4108781101703404e-05,
        "epoch": 0.9101274312541918,
        "step": 12213
    },
    {
        "loss": 2.6099,
        "grad_norm": 2.3012866973876953,
        "learning_rate": 3.405586696296465e-05,
        "epoch": 0.9102019524554735,
        "step": 12214
    },
    {
        "loss": 2.4417,
        "grad_norm": 2.621854305267334,
        "learning_rate": 3.4002985475325344e-05,
        "epoch": 0.9102764736567553,
        "step": 12215
    },
    {
        "loss": 0.9995,
        "grad_norm": 5.040997505187988,
        "learning_rate": 3.3950136664969e-05,
        "epoch": 0.9103509948580372,
        "step": 12216
    },
    {
        "loss": 1.7364,
        "grad_norm": 4.5996599197387695,
        "learning_rate": 3.389732055806272e-05,
        "epoch": 0.9104255160593189,
        "step": 12217
    },
    {
        "loss": 2.3778,
        "grad_norm": 1.928395390510559,
        "learning_rate": 3.3844537180757385e-05,
        "epoch": 0.9105000372606007,
        "step": 12218
    },
    {
        "loss": 2.1905,
        "grad_norm": 3.165963888168335,
        "learning_rate": 3.37917865591881e-05,
        "epoch": 0.9105745584618824,
        "step": 12219
    },
    {
        "loss": 2.2043,
        "grad_norm": 4.802699565887451,
        "learning_rate": 3.373906871947295e-05,
        "epoch": 0.9106490796631642,
        "step": 12220
    },
    {
        "loss": 2.2385,
        "grad_norm": 2.4834842681884766,
        "learning_rate": 3.368638368771462e-05,
        "epoch": 0.9107236008644459,
        "step": 12221
    },
    {
        "loss": 2.0619,
        "grad_norm": 5.124154567718506,
        "learning_rate": 3.363373148999907e-05,
        "epoch": 0.9107981220657277,
        "step": 12222
    },
    {
        "loss": 2.1897,
        "grad_norm": 2.0468897819519043,
        "learning_rate": 3.358111215239604e-05,
        "epoch": 0.9108726432670095,
        "step": 12223
    },
    {
        "loss": 2.3532,
        "grad_norm": 2.943650960922241,
        "learning_rate": 3.35285257009593e-05,
        "epoch": 0.9109471644682913,
        "step": 12224
    },
    {
        "loss": 2.1282,
        "grad_norm": 3.2720353603363037,
        "learning_rate": 3.347597216172603e-05,
        "epoch": 0.911021685669573,
        "step": 12225
    },
    {
        "loss": 1.9377,
        "grad_norm": 4.656104564666748,
        "learning_rate": 3.3423451560717056e-05,
        "epoch": 0.9110962068708548,
        "step": 12226
    },
    {
        "loss": 2.1411,
        "grad_norm": 3.0133216381073,
        "learning_rate": 3.337096392393728e-05,
        "epoch": 0.9111707280721365,
        "step": 12227
    },
    {
        "loss": 2.1113,
        "grad_norm": 4.0947675704956055,
        "learning_rate": 3.331850927737495e-05,
        "epoch": 0.9112452492734183,
        "step": 12228
    },
    {
        "loss": 2.4835,
        "grad_norm": 2.4960224628448486,
        "learning_rate": 3.3266087647001984e-05,
        "epoch": 0.9113197704747,
        "step": 12229
    },
    {
        "loss": 2.6138,
        "grad_norm": 4.0810322761535645,
        "learning_rate": 3.3213699058774336e-05,
        "epoch": 0.9113942916759818,
        "step": 12230
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.6856727600097656,
        "learning_rate": 3.3161343538630916e-05,
        "epoch": 0.9114688128772636,
        "step": 12231
    },
    {
        "loss": 2.3133,
        "grad_norm": 2.3318679332733154,
        "learning_rate": 3.3109021112494956e-05,
        "epoch": 0.9115433340785454,
        "step": 12232
    },
    {
        "loss": 2.6939,
        "grad_norm": 2.428895950317383,
        "learning_rate": 3.3056731806272844e-05,
        "epoch": 0.9116178552798271,
        "step": 12233
    },
    {
        "loss": 2.6899,
        "grad_norm": 3.6006531715393066,
        "learning_rate": 3.300447564585485e-05,
        "epoch": 0.9116923764811089,
        "step": 12234
    },
    {
        "loss": 2.2234,
        "grad_norm": 2.299567222595215,
        "learning_rate": 3.2952252657114643e-05,
        "epoch": 0.9117668976823906,
        "step": 12235
    },
    {
        "loss": 2.71,
        "grad_norm": 2.5071558952331543,
        "learning_rate": 3.2900062865909445e-05,
        "epoch": 0.9118414188836724,
        "step": 12236
    },
    {
        "loss": 2.5143,
        "grad_norm": 2.6546390056610107,
        "learning_rate": 3.2847906298080324e-05,
        "epoch": 0.9119159400849541,
        "step": 12237
    },
    {
        "loss": 1.4904,
        "grad_norm": 2.462501049041748,
        "learning_rate": 3.2795782979451505e-05,
        "epoch": 0.911990461286236,
        "step": 12238
    },
    {
        "loss": 2.4921,
        "grad_norm": 3.8138427734375,
        "learning_rate": 3.274369293583124e-05,
        "epoch": 0.9120649824875177,
        "step": 12239
    },
    {
        "loss": 1.4104,
        "grad_norm": 4.698629379272461,
        "learning_rate": 3.2691636193010635e-05,
        "epoch": 0.9121395036887995,
        "step": 12240
    },
    {
        "loss": 2.5073,
        "grad_norm": 1.3043408393859863,
        "learning_rate": 3.2639612776765016e-05,
        "epoch": 0.9122140248900812,
        "step": 12241
    },
    {
        "loss": 1.781,
        "grad_norm": 3.9580612182617188,
        "learning_rate": 3.2587622712852696e-05,
        "epoch": 0.912288546091363,
        "step": 12242
    },
    {
        "loss": 2.7597,
        "grad_norm": 2.474247694015503,
        "learning_rate": 3.253566602701561e-05,
        "epoch": 0.9123630672926447,
        "step": 12243
    },
    {
        "loss": 3.1891,
        "grad_norm": 3.720209836959839,
        "learning_rate": 3.248374274497942e-05,
        "epoch": 0.9124375884939265,
        "step": 12244
    },
    {
        "loss": 1.879,
        "grad_norm": 2.84035587310791,
        "learning_rate": 3.243185289245292e-05,
        "epoch": 0.9125121096952082,
        "step": 12245
    },
    {
        "loss": 2.5441,
        "grad_norm": 2.3141212463378906,
        "learning_rate": 3.237999649512839e-05,
        "epoch": 0.9125866308964901,
        "step": 12246
    },
    {
        "loss": 2.3856,
        "grad_norm": 2.8842413425445557,
        "learning_rate": 3.232817357868182e-05,
        "epoch": 0.9126611520977718,
        "step": 12247
    },
    {
        "loss": 2.6055,
        "grad_norm": 2.5930628776550293,
        "learning_rate": 3.227638416877237e-05,
        "epoch": 0.9127356732990536,
        "step": 12248
    },
    {
        "loss": 2.2118,
        "grad_norm": 2.025789737701416,
        "learning_rate": 3.2224628291042526e-05,
        "epoch": 0.9128101945003353,
        "step": 12249
    },
    {
        "loss": 2.5884,
        "grad_norm": 2.9797213077545166,
        "learning_rate": 3.217290597111862e-05,
        "epoch": 0.9128847157016171,
        "step": 12250
    },
    {
        "loss": 2.5176,
        "grad_norm": 2.4225330352783203,
        "learning_rate": 3.212121723460968e-05,
        "epoch": 0.9129592369028989,
        "step": 12251
    },
    {
        "loss": 2.0306,
        "grad_norm": 2.477677583694458,
        "learning_rate": 3.20695621071088e-05,
        "epoch": 0.9130337581041806,
        "step": 12252
    },
    {
        "loss": 2.7756,
        "grad_norm": 2.3872432708740234,
        "learning_rate": 3.201794061419201e-05,
        "epoch": 0.9131082793054625,
        "step": 12253
    },
    {
        "loss": 2.5344,
        "grad_norm": 2.9173922538757324,
        "learning_rate": 3.196635278141871e-05,
        "epoch": 0.9131828005067442,
        "step": 12254
    },
    {
        "loss": 2.5475,
        "grad_norm": 3.2727878093719482,
        "learning_rate": 3.191479863433195e-05,
        "epoch": 0.913257321708026,
        "step": 12255
    },
    {
        "loss": 1.8808,
        "grad_norm": 3.298583745956421,
        "learning_rate": 3.1863278198457634e-05,
        "epoch": 0.9133318429093077,
        "step": 12256
    },
    {
        "loss": 2.4846,
        "grad_norm": 3.4737110137939453,
        "learning_rate": 3.181179149930542e-05,
        "epoch": 0.9134063641105895,
        "step": 12257
    },
    {
        "loss": 2.8312,
        "grad_norm": 2.6532223224639893,
        "learning_rate": 3.1760338562367975e-05,
        "epoch": 0.9134808853118712,
        "step": 12258
    },
    {
        "loss": 2.8083,
        "grad_norm": 4.325117588043213,
        "learning_rate": 3.170891941312133e-05,
        "epoch": 0.913555406513153,
        "step": 12259
    },
    {
        "loss": 2.4296,
        "grad_norm": 1.3389441967010498,
        "learning_rate": 3.165753407702469e-05,
        "epoch": 0.9136299277144347,
        "step": 12260
    },
    {
        "loss": 2.0995,
        "grad_norm": 3.933741331100464,
        "learning_rate": 3.160618257952081e-05,
        "epoch": 0.9137044489157166,
        "step": 12261
    },
    {
        "loss": 1.8387,
        "grad_norm": 4.025758266448975,
        "learning_rate": 3.155486494603541e-05,
        "epoch": 0.9137789701169983,
        "step": 12262
    },
    {
        "loss": 2.8767,
        "grad_norm": 3.334152936935425,
        "learning_rate": 3.150358120197756e-05,
        "epoch": 0.9138534913182801,
        "step": 12263
    },
    {
        "loss": 2.3695,
        "grad_norm": 3.356077194213867,
        "learning_rate": 3.1452331372739344e-05,
        "epoch": 0.9139280125195618,
        "step": 12264
    },
    {
        "loss": 2.1325,
        "grad_norm": 3.029435634613037,
        "learning_rate": 3.140111548369648e-05,
        "epoch": 0.9140025337208436,
        "step": 12265
    },
    {
        "loss": 2.7388,
        "grad_norm": 3.884493112564087,
        "learning_rate": 3.1349933560207425e-05,
        "epoch": 0.9140770549221253,
        "step": 12266
    },
    {
        "loss": 2.1487,
        "grad_norm": 2.989722490310669,
        "learning_rate": 3.129878562761421e-05,
        "epoch": 0.9141515761234071,
        "step": 12267
    },
    {
        "loss": 2.0049,
        "grad_norm": 3.1641814708709717,
        "learning_rate": 3.124767171124178e-05,
        "epoch": 0.9142260973246888,
        "step": 12268
    },
    {
        "loss": 2.1869,
        "grad_norm": 3.025182008743286,
        "learning_rate": 3.119659183639818e-05,
        "epoch": 0.9143006185259707,
        "step": 12269
    },
    {
        "loss": 1.2107,
        "grad_norm": 3.9827044010162354,
        "learning_rate": 3.114554602837508e-05,
        "epoch": 0.9143751397272524,
        "step": 12270
    },
    {
        "loss": 1.8867,
        "grad_norm": 2.783282995223999,
        "learning_rate": 3.1094534312446464e-05,
        "epoch": 0.9144496609285342,
        "step": 12271
    },
    {
        "loss": 1.841,
        "grad_norm": 2.5670254230499268,
        "learning_rate": 3.1043556713870235e-05,
        "epoch": 0.9145241821298159,
        "step": 12272
    },
    {
        "loss": 1.4837,
        "grad_norm": 4.304375648498535,
        "learning_rate": 3.099261325788697e-05,
        "epoch": 0.9145987033310977,
        "step": 12273
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.988753318786621,
        "learning_rate": 3.0941703969720334e-05,
        "epoch": 0.9146732245323794,
        "step": 12274
    },
    {
        "loss": 1.4664,
        "grad_norm": 3.36531662940979,
        "learning_rate": 3.089082887457739e-05,
        "epoch": 0.9147477457336612,
        "step": 12275
    },
    {
        "loss": 2.3113,
        "grad_norm": 2.859527349472046,
        "learning_rate": 3.083998799764787e-05,
        "epoch": 0.914822266934943,
        "step": 12276
    },
    {
        "loss": 2.3109,
        "grad_norm": 2.409881591796875,
        "learning_rate": 3.078918136410495e-05,
        "epoch": 0.9148967881362248,
        "step": 12277
    },
    {
        "loss": 1.3764,
        "grad_norm": 4.320705413818359,
        "learning_rate": 3.073840899910457e-05,
        "epoch": 0.9149713093375065,
        "step": 12278
    },
    {
        "loss": 2.4224,
        "grad_norm": 2.0656683444976807,
        "learning_rate": 3.068767092778576e-05,
        "epoch": 0.9150458305387883,
        "step": 12279
    },
    {
        "loss": 2.6815,
        "grad_norm": 2.160160779953003,
        "learning_rate": 3.0636967175270525e-05,
        "epoch": 0.91512035174007,
        "step": 12280
    },
    {
        "loss": 2.1453,
        "grad_norm": 3.3184261322021484,
        "learning_rate": 3.058629776666423e-05,
        "epoch": 0.9151948729413518,
        "step": 12281
    },
    {
        "loss": 2.2515,
        "grad_norm": 4.085546493530273,
        "learning_rate": 3.053566272705458e-05,
        "epoch": 0.9152693941426335,
        "step": 12282
    },
    {
        "loss": 2.5466,
        "grad_norm": 3.64665150642395,
        "learning_rate": 3.0485062081512938e-05,
        "epoch": 0.9153439153439153,
        "step": 12283
    },
    {
        "loss": 2.0884,
        "grad_norm": 3.7355847358703613,
        "learning_rate": 3.0434495855093148e-05,
        "epoch": 0.9154184365451971,
        "step": 12284
    },
    {
        "loss": 2.456,
        "grad_norm": 2.619082450866699,
        "learning_rate": 3.0383964072832383e-05,
        "epoch": 0.9154929577464789,
        "step": 12285
    },
    {
        "loss": 1.7716,
        "grad_norm": 4.154892444610596,
        "learning_rate": 3.033346675975053e-05,
        "epoch": 0.9155674789477607,
        "step": 12286
    },
    {
        "loss": 1.9347,
        "grad_norm": 4.1221394538879395,
        "learning_rate": 3.0283003940850296e-05,
        "epoch": 0.9156420001490424,
        "step": 12287
    },
    {
        "loss": 2.5001,
        "grad_norm": 3.0130650997161865,
        "learning_rate": 3.02325756411177e-05,
        "epoch": 0.9157165213503242,
        "step": 12288
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.811985969543457,
        "learning_rate": 3.0182181885521255e-05,
        "epoch": 0.9157910425516059,
        "step": 12289
    },
    {
        "loss": 2.0948,
        "grad_norm": 3.107998847961426,
        "learning_rate": 3.0131822699012847e-05,
        "epoch": 0.9158655637528877,
        "step": 12290
    },
    {
        "loss": 2.3838,
        "grad_norm": 5.1949262619018555,
        "learning_rate": 3.0081498106526575e-05,
        "epoch": 0.9159400849541695,
        "step": 12291
    },
    {
        "loss": 2.4737,
        "grad_norm": 2.7881383895874023,
        "learning_rate": 3.0031208132980083e-05,
        "epoch": 0.9160146061554513,
        "step": 12292
    },
    {
        "loss": 2.5446,
        "grad_norm": 1.981387972831726,
        "learning_rate": 2.998095280327351e-05,
        "epoch": 0.916089127356733,
        "step": 12293
    },
    {
        "loss": 2.7028,
        "grad_norm": 3.6830601692199707,
        "learning_rate": 2.9930732142289764e-05,
        "epoch": 0.9161636485580148,
        "step": 12294
    },
    {
        "loss": 1.7384,
        "grad_norm": 2.2837445735931396,
        "learning_rate": 2.9880546174894962e-05,
        "epoch": 0.9162381697592965,
        "step": 12295
    },
    {
        "loss": 2.6181,
        "grad_norm": 2.280734062194824,
        "learning_rate": 2.983039492593771e-05,
        "epoch": 0.9163126909605783,
        "step": 12296
    },
    {
        "loss": 2.0529,
        "grad_norm": 4.425714492797852,
        "learning_rate": 2.9780278420249464e-05,
        "epoch": 0.91638721216186,
        "step": 12297
    },
    {
        "loss": 1.0135,
        "grad_norm": 5.270197868347168,
        "learning_rate": 2.9730196682644717e-05,
        "epoch": 0.9164617333631419,
        "step": 12298
    },
    {
        "loss": 2.4577,
        "grad_norm": 3.222412347793579,
        "learning_rate": 2.9680149737920492e-05,
        "epoch": 0.9165362545644236,
        "step": 12299
    },
    {
        "loss": 1.6414,
        "grad_norm": 3.628770589828491,
        "learning_rate": 2.9630137610856567e-05,
        "epoch": 0.9166107757657054,
        "step": 12300
    },
    {
        "loss": 1.684,
        "grad_norm": 3.211225986480713,
        "learning_rate": 2.958016032621591e-05,
        "epoch": 0.9166852969669871,
        "step": 12301
    },
    {
        "loss": 2.1482,
        "grad_norm": 2.9905149936676025,
        "learning_rate": 2.9530217908743452e-05,
        "epoch": 0.9167598181682689,
        "step": 12302
    },
    {
        "loss": 2.5852,
        "grad_norm": 2.393571376800537,
        "learning_rate": 2.9480310383167653e-05,
        "epoch": 0.9168343393695506,
        "step": 12303
    },
    {
        "loss": 2.139,
        "grad_norm": 1.9601699113845825,
        "learning_rate": 2.9430437774199173e-05,
        "epoch": 0.9169088605708324,
        "step": 12304
    },
    {
        "loss": 2.2475,
        "grad_norm": 2.820892333984375,
        "learning_rate": 2.9380600106531752e-05,
        "epoch": 0.9169833817721141,
        "step": 12305
    },
    {
        "loss": 2.5663,
        "grad_norm": 3.6672043800354004,
        "learning_rate": 2.9330797404841558e-05,
        "epoch": 0.917057902973396,
        "step": 12306
    },
    {
        "loss": 2.6503,
        "grad_norm": 1.8814364671707153,
        "learning_rate": 2.9281029693787444e-05,
        "epoch": 0.9171324241746777,
        "step": 12307
    },
    {
        "loss": 1.8115,
        "grad_norm": 2.0273795127868652,
        "learning_rate": 2.923129699801124e-05,
        "epoch": 0.9172069453759595,
        "step": 12308
    },
    {
        "loss": 1.9966,
        "grad_norm": 3.567039728164673,
        "learning_rate": 2.9181599342137023e-05,
        "epoch": 0.9172814665772412,
        "step": 12309
    },
    {
        "loss": 1.7355,
        "grad_norm": 2.5768561363220215,
        "learning_rate": 2.9131936750772047e-05,
        "epoch": 0.917355987778523,
        "step": 12310
    },
    {
        "loss": 2.1201,
        "grad_norm": 2.6563808917999268,
        "learning_rate": 2.9082309248505446e-05,
        "epoch": 0.9174305089798047,
        "step": 12311
    },
    {
        "loss": 1.8579,
        "grad_norm": 4.202940940856934,
        "learning_rate": 2.9032716859909747e-05,
        "epoch": 0.9175050301810865,
        "step": 12312
    },
    {
        "loss": 2.915,
        "grad_norm": 3.2913436889648438,
        "learning_rate": 2.8983159609539666e-05,
        "epoch": 0.9175795513823682,
        "step": 12313
    },
    {
        "loss": 2.4131,
        "grad_norm": 3.334402561187744,
        "learning_rate": 2.893363752193262e-05,
        "epoch": 0.9176540725836501,
        "step": 12314
    },
    {
        "loss": 2.2888,
        "grad_norm": 3.782616138458252,
        "learning_rate": 2.888415062160852e-05,
        "epoch": 0.9177285937849318,
        "step": 12315
    },
    {
        "loss": 2.0982,
        "grad_norm": 3.5230042934417725,
        "learning_rate": 2.883469893307017e-05,
        "epoch": 0.9178031149862136,
        "step": 12316
    },
    {
        "loss": 2.1059,
        "grad_norm": 3.43207049369812,
        "learning_rate": 2.878528248080248e-05,
        "epoch": 0.9178776361874953,
        "step": 12317
    },
    {
        "loss": 2.6404,
        "grad_norm": 1.9804843664169312,
        "learning_rate": 2.873590128927335e-05,
        "epoch": 0.9179521573887771,
        "step": 12318
    },
    {
        "loss": 1.8959,
        "grad_norm": 2.4793524742126465,
        "learning_rate": 2.8686555382932944e-05,
        "epoch": 0.9180266785900588,
        "step": 12319
    },
    {
        "loss": 2.3169,
        "grad_norm": 3.9351274967193604,
        "learning_rate": 2.8637244786213957e-05,
        "epoch": 0.9181011997913406,
        "step": 12320
    },
    {
        "loss": 2.0343,
        "grad_norm": 3.548652410507202,
        "learning_rate": 2.8587969523531932e-05,
        "epoch": 0.9181757209926223,
        "step": 12321
    },
    {
        "loss": 2.9257,
        "grad_norm": 2.3053393363952637,
        "learning_rate": 2.8538729619284312e-05,
        "epoch": 0.9182502421939042,
        "step": 12322
    },
    {
        "loss": 2.7205,
        "grad_norm": 3.1566717624664307,
        "learning_rate": 2.848952509785169e-05,
        "epoch": 0.918324763395186,
        "step": 12323
    },
    {
        "loss": 2.6602,
        "grad_norm": 1.934923529624939,
        "learning_rate": 2.8440355983596678e-05,
        "epoch": 0.9183992845964677,
        "step": 12324
    },
    {
        "loss": 1.9417,
        "grad_norm": 4.341338634490967,
        "learning_rate": 2.8391222300864463e-05,
        "epoch": 0.9184738057977495,
        "step": 12325
    },
    {
        "loss": 2.4477,
        "grad_norm": 3.375220775604248,
        "learning_rate": 2.8342124073982914e-05,
        "epoch": 0.9185483269990312,
        "step": 12326
    },
    {
        "loss": 1.9924,
        "grad_norm": 3.8869729042053223,
        "learning_rate": 2.8293061327261982e-05,
        "epoch": 0.918622848200313,
        "step": 12327
    },
    {
        "loss": 2.7424,
        "grad_norm": 2.391246795654297,
        "learning_rate": 2.8244034084994443e-05,
        "epoch": 0.9186973694015947,
        "step": 12328
    },
    {
        "loss": 1.9415,
        "grad_norm": 3.308788776397705,
        "learning_rate": 2.8195042371455195e-05,
        "epoch": 0.9187718906028766,
        "step": 12329
    },
    {
        "loss": 2.8358,
        "grad_norm": 2.917372226715088,
        "learning_rate": 2.8146086210901612e-05,
        "epoch": 0.9188464118041583,
        "step": 12330
    },
    {
        "loss": 1.8975,
        "grad_norm": 4.413589954376221,
        "learning_rate": 2.809716562757343e-05,
        "epoch": 0.9189209330054401,
        "step": 12331
    },
    {
        "loss": 2.0716,
        "grad_norm": 2.2704505920410156,
        "learning_rate": 2.804828064569304e-05,
        "epoch": 0.9189954542067218,
        "step": 12332
    },
    {
        "loss": 1.9777,
        "grad_norm": 3.749237537384033,
        "learning_rate": 2.799943128946483e-05,
        "epoch": 0.9190699754080036,
        "step": 12333
    },
    {
        "loss": 2.8229,
        "grad_norm": 2.9135680198669434,
        "learning_rate": 2.7950617583075756e-05,
        "epoch": 0.9191444966092853,
        "step": 12334
    },
    {
        "loss": 1.8092,
        "grad_norm": 4.540698051452637,
        "learning_rate": 2.7901839550694996e-05,
        "epoch": 0.9192190178105671,
        "step": 12335
    },
    {
        "loss": 1.4707,
        "grad_norm": 4.849955081939697,
        "learning_rate": 2.7853097216474334e-05,
        "epoch": 0.9192935390118488,
        "step": 12336
    },
    {
        "loss": 2.0049,
        "grad_norm": 3.5391688346862793,
        "learning_rate": 2.7804390604547493e-05,
        "epoch": 0.9193680602131307,
        "step": 12337
    },
    {
        "loss": 2.7984,
        "grad_norm": 4.014601230621338,
        "learning_rate": 2.7755719739030918e-05,
        "epoch": 0.9194425814144124,
        "step": 12338
    },
    {
        "loss": 2.9536,
        "grad_norm": 2.4677798748016357,
        "learning_rate": 2.7707084644023084e-05,
        "epoch": 0.9195171026156942,
        "step": 12339
    },
    {
        "loss": 2.5081,
        "grad_norm": 2.567359685897827,
        "learning_rate": 2.7658485343604656e-05,
        "epoch": 0.9195916238169759,
        "step": 12340
    },
    {
        "loss": 2.0429,
        "grad_norm": 5.229269027709961,
        "learning_rate": 2.760992186183905e-05,
        "epoch": 0.9196661450182577,
        "step": 12341
    },
    {
        "loss": 2.282,
        "grad_norm": 2.443715810775757,
        "learning_rate": 2.7561394222771265e-05,
        "epoch": 0.9197406662195394,
        "step": 12342
    },
    {
        "loss": 2.7073,
        "grad_norm": 2.916285514831543,
        "learning_rate": 2.75129024504292e-05,
        "epoch": 0.9198151874208212,
        "step": 12343
    },
    {
        "loss": 2.9165,
        "grad_norm": 2.1935458183288574,
        "learning_rate": 2.746444656882263e-05,
        "epoch": 0.919889708622103,
        "step": 12344
    },
    {
        "loss": 2.0382,
        "grad_norm": 3.503901720046997,
        "learning_rate": 2.741602660194357e-05,
        "epoch": 0.9199642298233848,
        "step": 12345
    },
    {
        "loss": 1.9997,
        "grad_norm": 4.160645961761475,
        "learning_rate": 2.7367642573766515e-05,
        "epoch": 0.9200387510246665,
        "step": 12346
    },
    {
        "loss": 2.506,
        "grad_norm": 1.7622191905975342,
        "learning_rate": 2.731929450824786e-05,
        "epoch": 0.9201132722259483,
        "step": 12347
    },
    {
        "loss": 1.7943,
        "grad_norm": 3.441526412963867,
        "learning_rate": 2.727098242932623e-05,
        "epoch": 0.92018779342723,
        "step": 12348
    },
    {
        "loss": 2.5666,
        "grad_norm": 2.7117905616760254,
        "learning_rate": 2.7222706360922723e-05,
        "epoch": 0.9202623146285118,
        "step": 12349
    },
    {
        "loss": 1.4595,
        "grad_norm": 1.6323481798171997,
        "learning_rate": 2.7174466326940283e-05,
        "epoch": 0.9203368358297935,
        "step": 12350
    },
    {
        "loss": 2.1542,
        "grad_norm": 3.521000862121582,
        "learning_rate": 2.7126262351264066e-05,
        "epoch": 0.9204113570310754,
        "step": 12351
    },
    {
        "loss": 1.8807,
        "grad_norm": 3.830016851425171,
        "learning_rate": 2.707809445776168e-05,
        "epoch": 0.9204858782323571,
        "step": 12352
    },
    {
        "loss": 2.2065,
        "grad_norm": 3.2768778800964355,
        "learning_rate": 2.7029962670282304e-05,
        "epoch": 0.9205603994336389,
        "step": 12353
    },
    {
        "loss": 2.2793,
        "grad_norm": 3.69480562210083,
        "learning_rate": 2.6981867012657823e-05,
        "epoch": 0.9206349206349206,
        "step": 12354
    },
    {
        "loss": 2.2455,
        "grad_norm": 3.8945271968841553,
        "learning_rate": 2.693380750870177e-05,
        "epoch": 0.9207094418362024,
        "step": 12355
    },
    {
        "loss": 2.1902,
        "grad_norm": 3.176384687423706,
        "learning_rate": 2.6885784182210162e-05,
        "epoch": 0.9207839630374841,
        "step": 12356
    },
    {
        "loss": 2.3119,
        "grad_norm": 2.5533335208892822,
        "learning_rate": 2.6837797056960835e-05,
        "epoch": 0.9208584842387659,
        "step": 12357
    },
    {
        "loss": 2.4125,
        "grad_norm": 4.126401901245117,
        "learning_rate": 2.6789846156713694e-05,
        "epoch": 0.9209330054400477,
        "step": 12358
    },
    {
        "loss": 1.2707,
        "grad_norm": 2.4086806774139404,
        "learning_rate": 2.6741931505210992e-05,
        "epoch": 0.9210075266413295,
        "step": 12359
    },
    {
        "loss": 2.713,
        "grad_norm": 1.9451191425323486,
        "learning_rate": 2.6694053126176654e-05,
        "epoch": 0.9210820478426113,
        "step": 12360
    },
    {
        "loss": 1.5108,
        "grad_norm": 2.350585699081421,
        "learning_rate": 2.6646211043317125e-05,
        "epoch": 0.921156569043893,
        "step": 12361
    },
    {
        "loss": 2.7241,
        "grad_norm": 3.3926546573638916,
        "learning_rate": 2.6598405280320183e-05,
        "epoch": 0.9212310902451748,
        "step": 12362
    },
    {
        "loss": 2.8396,
        "grad_norm": 2.288604259490967,
        "learning_rate": 2.655063586085632e-05,
        "epoch": 0.9213056114464565,
        "step": 12363
    },
    {
        "loss": 2.5685,
        "grad_norm": 2.757101535797119,
        "learning_rate": 2.6502902808577647e-05,
        "epoch": 0.9213801326477383,
        "step": 12364
    },
    {
        "loss": 2.1589,
        "grad_norm": 2.52062726020813,
        "learning_rate": 2.6455206147118273e-05,
        "epoch": 0.92145465384902,
        "step": 12365
    },
    {
        "loss": 2.2926,
        "grad_norm": 2.8980655670166016,
        "learning_rate": 2.6407545900094565e-05,
        "epoch": 0.9215291750503019,
        "step": 12366
    },
    {
        "loss": 1.4789,
        "grad_norm": 2.9627156257629395,
        "learning_rate": 2.635992209110456e-05,
        "epoch": 0.9216036962515836,
        "step": 12367
    },
    {
        "loss": 2.5203,
        "grad_norm": 3.708737850189209,
        "learning_rate": 2.6312334743728296e-05,
        "epoch": 0.9216782174528654,
        "step": 12368
    },
    {
        "loss": 2.5056,
        "grad_norm": 2.0261309146881104,
        "learning_rate": 2.6264783881528032e-05,
        "epoch": 0.9217527386541471,
        "step": 12369
    },
    {
        "loss": 2.2886,
        "grad_norm": 6.420131206512451,
        "learning_rate": 2.6217269528047606e-05,
        "epoch": 0.9218272598554289,
        "step": 12370
    },
    {
        "loss": 2.5749,
        "grad_norm": 3.4285366535186768,
        "learning_rate": 2.6169791706812864e-05,
        "epoch": 0.9219017810567106,
        "step": 12371
    },
    {
        "loss": 1.9626,
        "grad_norm": 3.345442056655884,
        "learning_rate": 2.61223504413319e-05,
        "epoch": 0.9219763022579924,
        "step": 12372
    },
    {
        "loss": 2.0229,
        "grad_norm": 3.175288200378418,
        "learning_rate": 2.6074945755094094e-05,
        "epoch": 0.9220508234592741,
        "step": 12373
    },
    {
        "loss": 2.9587,
        "grad_norm": 2.356863021850586,
        "learning_rate": 2.6027577671571314e-05,
        "epoch": 0.922125344660556,
        "step": 12374
    },
    {
        "loss": 2.6699,
        "grad_norm": 2.2630178928375244,
        "learning_rate": 2.598024621421695e-05,
        "epoch": 0.9221998658618377,
        "step": 12375
    },
    {
        "loss": 2.5158,
        "grad_norm": 2.150968074798584,
        "learning_rate": 2.593295140646629e-05,
        "epoch": 0.9222743870631195,
        "step": 12376
    },
    {
        "loss": 2.9646,
        "grad_norm": 2.319819450378418,
        "learning_rate": 2.5885693271736744e-05,
        "epoch": 0.9223489082644012,
        "step": 12377
    },
    {
        "loss": 2.6242,
        "grad_norm": 2.3828399181365967,
        "learning_rate": 2.5838471833427112e-05,
        "epoch": 0.922423429465683,
        "step": 12378
    },
    {
        "loss": 2.3736,
        "grad_norm": 3.9249749183654785,
        "learning_rate": 2.5791287114918484e-05,
        "epoch": 0.9224979506669647,
        "step": 12379
    },
    {
        "loss": 2.2878,
        "grad_norm": 3.1587491035461426,
        "learning_rate": 2.5744139139573486e-05,
        "epoch": 0.9225724718682465,
        "step": 12380
    },
    {
        "loss": 2.519,
        "grad_norm": 2.4717423915863037,
        "learning_rate": 2.5697027930736616e-05,
        "epoch": 0.9226469930695282,
        "step": 12381
    },
    {
        "loss": 1.8142,
        "grad_norm": 1.7066680192947388,
        "learning_rate": 2.5649953511734082e-05,
        "epoch": 0.9227215142708101,
        "step": 12382
    },
    {
        "loss": 1.8618,
        "grad_norm": 3.5571224689483643,
        "learning_rate": 2.5602915905874147e-05,
        "epoch": 0.9227960354720918,
        "step": 12383
    },
    {
        "loss": 2.8677,
        "grad_norm": 2.3845207691192627,
        "learning_rate": 2.5555915136446618e-05,
        "epoch": 0.9228705566733736,
        "step": 12384
    },
    {
        "loss": 2.4985,
        "grad_norm": 2.7018871307373047,
        "learning_rate": 2.5508951226723054e-05,
        "epoch": 0.9229450778746553,
        "step": 12385
    },
    {
        "loss": 1.9736,
        "grad_norm": 3.270745277404785,
        "learning_rate": 2.5462024199956757e-05,
        "epoch": 0.9230195990759371,
        "step": 12386
    },
    {
        "loss": 2.2854,
        "grad_norm": 3.3561370372772217,
        "learning_rate": 2.541513407938303e-05,
        "epoch": 0.9230941202772188,
        "step": 12387
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.5831470489501953,
        "learning_rate": 2.536828088821852e-05,
        "epoch": 0.9231686414785006,
        "step": 12388
    },
    {
        "loss": 2.6082,
        "grad_norm": 3.1349141597747803,
        "learning_rate": 2.532146464966194e-05,
        "epoch": 0.9232431626797823,
        "step": 12389
    },
    {
        "loss": 2.4202,
        "grad_norm": 2.525625705718994,
        "learning_rate": 2.5274685386893472e-05,
        "epoch": 0.9233176838810642,
        "step": 12390
    },
    {
        "loss": 2.3988,
        "grad_norm": 3.8752503395080566,
        "learning_rate": 2.5227943123074972e-05,
        "epoch": 0.9233922050823459,
        "step": 12391
    },
    {
        "loss": 2.9255,
        "grad_norm": 2.6673426628112793,
        "learning_rate": 2.518123788135035e-05,
        "epoch": 0.9234667262836277,
        "step": 12392
    },
    {
        "loss": 2.3391,
        "grad_norm": 1.9219684600830078,
        "learning_rate": 2.5134569684844488e-05,
        "epoch": 0.9235412474849095,
        "step": 12393
    },
    {
        "loss": 1.4858,
        "grad_norm": 3.78348970413208,
        "learning_rate": 2.5087938556664647e-05,
        "epoch": 0.9236157686861912,
        "step": 12394
    },
    {
        "loss": 2.7133,
        "grad_norm": 2.332477331161499,
        "learning_rate": 2.5041344519899313e-05,
        "epoch": 0.923690289887473,
        "step": 12395
    },
    {
        "loss": 2.319,
        "grad_norm": 2.6597375869750977,
        "learning_rate": 2.4994787597618662e-05,
        "epoch": 0.9237648110887547,
        "step": 12396
    },
    {
        "loss": 2.524,
        "grad_norm": 2.378038167953491,
        "learning_rate": 2.4948267812874703e-05,
        "epoch": 0.9238393322900366,
        "step": 12397
    },
    {
        "loss": 2.4122,
        "grad_norm": 2.3077797889709473,
        "learning_rate": 2.490178518870073e-05,
        "epoch": 0.9239138534913183,
        "step": 12398
    },
    {
        "loss": 2.302,
        "grad_norm": 2.523794651031494,
        "learning_rate": 2.4855339748112007e-05,
        "epoch": 0.9239883746926001,
        "step": 12399
    },
    {
        "loss": 2.5393,
        "grad_norm": 2.8689334392547607,
        "learning_rate": 2.4808931514105106e-05,
        "epoch": 0.9240628958938818,
        "step": 12400
    },
    {
        "loss": 2.1688,
        "grad_norm": 3.182227373123169,
        "learning_rate": 2.4762560509658226e-05,
        "epoch": 0.9241374170951636,
        "step": 12401
    },
    {
        "loss": 2.1737,
        "grad_norm": 2.026750326156616,
        "learning_rate": 2.471622675773111e-05,
        "epoch": 0.9242119382964453,
        "step": 12402
    },
    {
        "loss": 1.6408,
        "grad_norm": 3.2573580741882324,
        "learning_rate": 2.4669930281265274e-05,
        "epoch": 0.9242864594977271,
        "step": 12403
    },
    {
        "loss": 2.4655,
        "grad_norm": 3.574835777282715,
        "learning_rate": 2.4623671103183555e-05,
        "epoch": 0.9243609806990088,
        "step": 12404
    },
    {
        "loss": 2.5133,
        "grad_norm": 2.6437485218048096,
        "learning_rate": 2.4577449246390372e-05,
        "epoch": 0.9244355019002907,
        "step": 12405
    },
    {
        "loss": 2.4843,
        "grad_norm": 2.284052610397339,
        "learning_rate": 2.4531264733771596e-05,
        "epoch": 0.9245100231015724,
        "step": 12406
    },
    {
        "loss": 2.8684,
        "grad_norm": 2.028869152069092,
        "learning_rate": 2.4485117588194895e-05,
        "epoch": 0.9245845443028542,
        "step": 12407
    },
    {
        "loss": 2.4201,
        "grad_norm": 2.4093945026397705,
        "learning_rate": 2.4439007832509087e-05,
        "epoch": 0.9246590655041359,
        "step": 12408
    },
    {
        "loss": 2.4477,
        "grad_norm": 1.8980140686035156,
        "learning_rate": 2.4392935489544556e-05,
        "epoch": 0.9247335867054177,
        "step": 12409
    },
    {
        "loss": 2.3245,
        "grad_norm": 2.4272005558013916,
        "learning_rate": 2.4346900582113417e-05,
        "epoch": 0.9248081079066994,
        "step": 12410
    },
    {
        "loss": 0.712,
        "grad_norm": 2.5815255641937256,
        "learning_rate": 2.430090313300888e-05,
        "epoch": 0.9248826291079812,
        "step": 12411
    },
    {
        "loss": 2.805,
        "grad_norm": 2.7689716815948486,
        "learning_rate": 2.4254943165006038e-05,
        "epoch": 0.924957150309263,
        "step": 12412
    },
    {
        "loss": 1.6315,
        "grad_norm": 2.450263023376465,
        "learning_rate": 2.420902070086084e-05,
        "epoch": 0.9250316715105448,
        "step": 12413
    },
    {
        "loss": 2.3156,
        "grad_norm": 2.9239253997802734,
        "learning_rate": 2.4163135763311294e-05,
        "epoch": 0.9251061927118265,
        "step": 12414
    },
    {
        "loss": 2.3552,
        "grad_norm": 2.612651824951172,
        "learning_rate": 2.4117288375076375e-05,
        "epoch": 0.9251807139131083,
        "step": 12415
    },
    {
        "loss": 2.3092,
        "grad_norm": 2.5911426544189453,
        "learning_rate": 2.4071478558856585e-05,
        "epoch": 0.92525523511439,
        "step": 12416
    },
    {
        "loss": 2.5552,
        "grad_norm": 3.0935049057006836,
        "learning_rate": 2.402570633733403e-05,
        "epoch": 0.9253297563156718,
        "step": 12417
    },
    {
        "loss": 2.7571,
        "grad_norm": 2.546412467956543,
        "learning_rate": 2.3979971733171947e-05,
        "epoch": 0.9254042775169535,
        "step": 12418
    },
    {
        "loss": 2.5227,
        "grad_norm": 2.285135507583618,
        "learning_rate": 2.3934274769014963e-05,
        "epoch": 0.9254787987182354,
        "step": 12419
    },
    {
        "loss": 1.8276,
        "grad_norm": 3.6596715450286865,
        "learning_rate": 2.3888615467489296e-05,
        "epoch": 0.9255533199195171,
        "step": 12420
    },
    {
        "loss": 2.303,
        "grad_norm": 2.070286989212036,
        "learning_rate": 2.3842993851202277e-05,
        "epoch": 0.9256278411207989,
        "step": 12421
    },
    {
        "loss": 2.6239,
        "grad_norm": 2.565255880355835,
        "learning_rate": 2.3797409942742586e-05,
        "epoch": 0.9257023623220806,
        "step": 12422
    },
    {
        "loss": 2.2644,
        "grad_norm": 2.1155576705932617,
        "learning_rate": 2.3751863764680583e-05,
        "epoch": 0.9257768835233624,
        "step": 12423
    },
    {
        "loss": 2.6761,
        "grad_norm": 4.208157539367676,
        "learning_rate": 2.3706355339567286e-05,
        "epoch": 0.9258514047246441,
        "step": 12424
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.0851094722747803,
        "learning_rate": 2.3660884689935693e-05,
        "epoch": 0.9259259259259259,
        "step": 12425
    },
    {
        "loss": 2.7425,
        "grad_norm": 2.136528968811035,
        "learning_rate": 2.3615451838299618e-05,
        "epoch": 0.9260004471272076,
        "step": 12426
    },
    {
        "loss": 2.33,
        "grad_norm": 3.4284467697143555,
        "learning_rate": 2.357005680715454e-05,
        "epoch": 0.9260749683284895,
        "step": 12427
    },
    {
        "loss": 2.4021,
        "grad_norm": 2.087587833404541,
        "learning_rate": 2.3524699618976942e-05,
        "epoch": 0.9261494895297713,
        "step": 12428
    },
    {
        "loss": 2.5494,
        "grad_norm": 2.7106082439422607,
        "learning_rate": 2.3479380296224562e-05,
        "epoch": 0.926224010731053,
        "step": 12429
    },
    {
        "loss": 2.7903,
        "grad_norm": 3.250476121902466,
        "learning_rate": 2.3434098861336695e-05,
        "epoch": 0.9262985319323348,
        "step": 12430
    },
    {
        "loss": 1.9068,
        "grad_norm": 3.5749244689941406,
        "learning_rate": 2.3388855336733407e-05,
        "epoch": 0.9263730531336165,
        "step": 12431
    },
    {
        "loss": 2.1644,
        "grad_norm": 5.915236473083496,
        "learning_rate": 2.3343649744816554e-05,
        "epoch": 0.9264475743348983,
        "step": 12432
    },
    {
        "loss": 1.3953,
        "grad_norm": 5.4825639724731445,
        "learning_rate": 2.3298482107968557e-05,
        "epoch": 0.92652209553618,
        "step": 12433
    },
    {
        "loss": 2.5344,
        "grad_norm": 2.3163108825683594,
        "learning_rate": 2.3253352448553645e-05,
        "epoch": 0.9265966167374619,
        "step": 12434
    },
    {
        "loss": 1.6697,
        "grad_norm": 2.244446039199829,
        "learning_rate": 2.3208260788916902e-05,
        "epoch": 0.9266711379387436,
        "step": 12435
    },
    {
        "loss": 2.7409,
        "grad_norm": 3.18560528755188,
        "learning_rate": 2.3163207151384613e-05,
        "epoch": 0.9267456591400254,
        "step": 12436
    },
    {
        "loss": 3.1115,
        "grad_norm": 2.1517014503479004,
        "learning_rate": 2.31181915582645e-05,
        "epoch": 0.9268201803413071,
        "step": 12437
    },
    {
        "loss": 2.5043,
        "grad_norm": 1.7570592164993286,
        "learning_rate": 2.3073214031845115e-05,
        "epoch": 0.9268947015425889,
        "step": 12438
    },
    {
        "loss": 2.6436,
        "grad_norm": 2.8544304370880127,
        "learning_rate": 2.302827459439625e-05,
        "epoch": 0.9269692227438706,
        "step": 12439
    },
    {
        "loss": 2.3754,
        "grad_norm": 3.014726400375366,
        "learning_rate": 2.2983373268169063e-05,
        "epoch": 0.9270437439451524,
        "step": 12440
    },
    {
        "loss": 2.0053,
        "grad_norm": 3.0768892765045166,
        "learning_rate": 2.293851007539558e-05,
        "epoch": 0.9271182651464341,
        "step": 12441
    },
    {
        "loss": 2.4349,
        "grad_norm": 2.638500690460205,
        "learning_rate": 2.289368503828895e-05,
        "epoch": 0.927192786347716,
        "step": 12442
    },
    {
        "loss": 2.7375,
        "grad_norm": 2.5221316814422607,
        "learning_rate": 2.2848898179043777e-05,
        "epoch": 0.9272673075489977,
        "step": 12443
    },
    {
        "loss": 2.302,
        "grad_norm": 2.9271764755249023,
        "learning_rate": 2.280414951983516e-05,
        "epoch": 0.9273418287502795,
        "step": 12444
    },
    {
        "loss": 2.5311,
        "grad_norm": 2.5040152072906494,
        "learning_rate": 2.275943908281992e-05,
        "epoch": 0.9274163499515612,
        "step": 12445
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.8276777267456055,
        "learning_rate": 2.2714766890135498e-05,
        "epoch": 0.927490871152843,
        "step": 12446
    },
    {
        "loss": 2.6407,
        "grad_norm": 2.943279981613159,
        "learning_rate": 2.2670132963900515e-05,
        "epoch": 0.9275653923541247,
        "step": 12447
    },
    {
        "loss": 1.5492,
        "grad_norm": 1.4889618158340454,
        "learning_rate": 2.262553732621485e-05,
        "epoch": 0.9276399135554065,
        "step": 12448
    },
    {
        "loss": 2.8297,
        "grad_norm": 1.6973768472671509,
        "learning_rate": 2.25809799991591e-05,
        "epoch": 0.9277144347566882,
        "step": 12449
    },
    {
        "loss": 2.1468,
        "grad_norm": 2.9431493282318115,
        "learning_rate": 2.253646100479523e-05,
        "epoch": 0.9277889559579701,
        "step": 12450
    },
    {
        "loss": 2.0625,
        "grad_norm": 2.9374547004699707,
        "learning_rate": 2.2491980365165956e-05,
        "epoch": 0.9278634771592518,
        "step": 12451
    },
    {
        "loss": 2.1092,
        "grad_norm": 4.284953594207764,
        "learning_rate": 2.244753810229514e-05,
        "epoch": 0.9279379983605336,
        "step": 12452
    },
    {
        "loss": 2.2047,
        "grad_norm": 3.274488687515259,
        "learning_rate": 2.2403134238187452e-05,
        "epoch": 0.9280125195618153,
        "step": 12453
    },
    {
        "loss": 2.2436,
        "grad_norm": 3.449016571044922,
        "learning_rate": 2.235876879482889e-05,
        "epoch": 0.9280870407630971,
        "step": 12454
    },
    {
        "loss": 3.0259,
        "grad_norm": 3.6655571460723877,
        "learning_rate": 2.231444179418617e-05,
        "epoch": 0.9281615619643788,
        "step": 12455
    },
    {
        "loss": 2.2214,
        "grad_norm": 3.757469892501831,
        "learning_rate": 2.2270153258207027e-05,
        "epoch": 0.9282360831656606,
        "step": 12456
    },
    {
        "loss": 2.2878,
        "grad_norm": 2.6315290927886963,
        "learning_rate": 2.2225903208820098e-05,
        "epoch": 0.9283106043669423,
        "step": 12457
    },
    {
        "loss": 2.2226,
        "grad_norm": 2.3034069538116455,
        "learning_rate": 2.218169166793518e-05,
        "epoch": 0.9283851255682242,
        "step": 12458
    },
    {
        "loss": 2.0864,
        "grad_norm": 2.852290630340576,
        "learning_rate": 2.2137518657442703e-05,
        "epoch": 0.9284596467695059,
        "step": 12459
    },
    {
        "loss": 1.9811,
        "grad_norm": 2.8678085803985596,
        "learning_rate": 2.209338419921435e-05,
        "epoch": 0.9285341679707877,
        "step": 12460
    },
    {
        "loss": 2.1911,
        "grad_norm": 3.252851963043213,
        "learning_rate": 2.2049288315102412e-05,
        "epoch": 0.9286086891720694,
        "step": 12461
    },
    {
        "loss": 3.7026,
        "grad_norm": 5.6165971755981445,
        "learning_rate": 2.2005231026940132e-05,
        "epoch": 0.9286832103733512,
        "step": 12462
    },
    {
        "loss": 2.2598,
        "grad_norm": 2.219170570373535,
        "learning_rate": 2.1961212356541994e-05,
        "epoch": 0.9287577315746329,
        "step": 12463
    },
    {
        "loss": 1.841,
        "grad_norm": 3.4896039962768555,
        "learning_rate": 2.191723232570272e-05,
        "epoch": 0.9288322527759147,
        "step": 12464
    },
    {
        "loss": 3.1789,
        "grad_norm": 3.157763719558716,
        "learning_rate": 2.1873290956198532e-05,
        "epoch": 0.9289067739771966,
        "step": 12465
    },
    {
        "loss": 1.1852,
        "grad_norm": 3.0803701877593994,
        "learning_rate": 2.1829388269786155e-05,
        "epoch": 0.9289812951784783,
        "step": 12466
    },
    {
        "loss": 2.431,
        "grad_norm": 2.6127560138702393,
        "learning_rate": 2.1785524288203152e-05,
        "epoch": 0.9290558163797601,
        "step": 12467
    },
    {
        "loss": 1.8528,
        "grad_norm": 2.8188483715057373,
        "learning_rate": 2.1741699033168227e-05,
        "epoch": 0.9291303375810418,
        "step": 12468
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.8699495792388916,
        "learning_rate": 2.169791252638055e-05,
        "epoch": 0.9292048587823236,
        "step": 12469
    },
    {
        "loss": 1.9031,
        "grad_norm": 3.652134418487549,
        "learning_rate": 2.165416478952019e-05,
        "epoch": 0.9292793799836053,
        "step": 12470
    },
    {
        "loss": 2.2613,
        "grad_norm": 3.193225383758545,
        "learning_rate": 2.161045584424828e-05,
        "epoch": 0.9293539011848871,
        "step": 12471
    },
    {
        "loss": 1.9096,
        "grad_norm": 3.78359317779541,
        "learning_rate": 2.1566785712206438e-05,
        "epoch": 0.9294284223861689,
        "step": 12472
    },
    {
        "loss": 2.1026,
        "grad_norm": 3.079216718673706,
        "learning_rate": 2.15231544150171e-05,
        "epoch": 0.9295029435874507,
        "step": 12473
    },
    {
        "loss": 1.9301,
        "grad_norm": 3.1155736446380615,
        "learning_rate": 2.1479561974283835e-05,
        "epoch": 0.9295774647887324,
        "step": 12474
    },
    {
        "loss": 1.5529,
        "grad_norm": 2.281816244125366,
        "learning_rate": 2.1436008411590292e-05,
        "epoch": 0.9296519859900142,
        "step": 12475
    },
    {
        "loss": 2.3805,
        "grad_norm": 4.859673023223877,
        "learning_rate": 2.139249374850155e-05,
        "epoch": 0.9297265071912959,
        "step": 12476
    },
    {
        "loss": 1.8136,
        "grad_norm": 5.803032398223877,
        "learning_rate": 2.134901800656297e-05,
        "epoch": 0.9298010283925777,
        "step": 12477
    },
    {
        "loss": 1.906,
        "grad_norm": 3.3625919818878174,
        "learning_rate": 2.1305581207300983e-05,
        "epoch": 0.9298755495938594,
        "step": 12478
    },
    {
        "loss": 2.0933,
        "grad_norm": 3.467081069946289,
        "learning_rate": 2.12621833722225e-05,
        "epoch": 0.9299500707951412,
        "step": 12479
    },
    {
        "loss": 2.2331,
        "grad_norm": 2.730212926864624,
        "learning_rate": 2.1218824522815105e-05,
        "epoch": 0.930024591996423,
        "step": 12480
    },
    {
        "loss": 1.9165,
        "grad_norm": 2.598554849624634,
        "learning_rate": 2.1175504680547363e-05,
        "epoch": 0.9300991131977048,
        "step": 12481
    },
    {
        "loss": 2.4976,
        "grad_norm": 3.3516738414764404,
        "learning_rate": 2.1132223866868182e-05,
        "epoch": 0.9301736343989865,
        "step": 12482
    },
    {
        "loss": 2.6406,
        "grad_norm": 2.228929042816162,
        "learning_rate": 2.1088982103207588e-05,
        "epoch": 0.9302481556002683,
        "step": 12483
    },
    {
        "loss": 2.038,
        "grad_norm": 2.504552125930786,
        "learning_rate": 2.1045779410975595e-05,
        "epoch": 0.93032267680155,
        "step": 12484
    },
    {
        "loss": 2.6717,
        "grad_norm": 1.8589754104614258,
        "learning_rate": 2.1002615811563563e-05,
        "epoch": 0.9303971980028318,
        "step": 12485
    },
    {
        "loss": 2.6932,
        "grad_norm": 3.230755090713501,
        "learning_rate": 2.0959491326343106e-05,
        "epoch": 0.9304717192041135,
        "step": 12486
    },
    {
        "loss": 1.5095,
        "grad_norm": 3.7970521450042725,
        "learning_rate": 2.0916405976666497e-05,
        "epoch": 0.9305462404053954,
        "step": 12487
    },
    {
        "loss": 2.5812,
        "grad_norm": 4.406254768371582,
        "learning_rate": 2.087335978386685e-05,
        "epoch": 0.9306207616066771,
        "step": 12488
    },
    {
        "loss": 1.5661,
        "grad_norm": 3.6136529445648193,
        "learning_rate": 2.08303527692577e-05,
        "epoch": 0.9306952828079589,
        "step": 12489
    },
    {
        "loss": 2.5316,
        "grad_norm": 2.3250460624694824,
        "learning_rate": 2.0787384954133128e-05,
        "epoch": 0.9307698040092406,
        "step": 12490
    },
    {
        "loss": 2.5389,
        "grad_norm": 2.514836549758911,
        "learning_rate": 2.074445635976805e-05,
        "epoch": 0.9308443252105224,
        "step": 12491
    },
    {
        "loss": 2.2213,
        "grad_norm": 2.5218405723571777,
        "learning_rate": 2.0701567007417744e-05,
        "epoch": 0.9309188464118041,
        "step": 12492
    },
    {
        "loss": 1.6165,
        "grad_norm": 3.669985771179199,
        "learning_rate": 2.0658716918318054e-05,
        "epoch": 0.9309933676130859,
        "step": 12493
    },
    {
        "loss": 2.3292,
        "grad_norm": 2.355856418609619,
        "learning_rate": 2.0615906113685713e-05,
        "epoch": 0.9310678888143676,
        "step": 12494
    },
    {
        "loss": 2.1176,
        "grad_norm": 3.5679543018341064,
        "learning_rate": 2.0573134614717425e-05,
        "epoch": 0.9311424100156495,
        "step": 12495
    },
    {
        "loss": 2.3887,
        "grad_norm": 2.172901153564453,
        "learning_rate": 2.0530402442591013e-05,
        "epoch": 0.9312169312169312,
        "step": 12496
    },
    {
        "loss": 2.5241,
        "grad_norm": 2.900543212890625,
        "learning_rate": 2.048770961846439e-05,
        "epoch": 0.931291452418213,
        "step": 12497
    },
    {
        "loss": 1.914,
        "grad_norm": 3.598966121673584,
        "learning_rate": 2.0445056163476374e-05,
        "epoch": 0.9313659736194947,
        "step": 12498
    },
    {
        "loss": 2.6823,
        "grad_norm": 5.547609329223633,
        "learning_rate": 2.0402442098745954e-05,
        "epoch": 0.9314404948207765,
        "step": 12499
    },
    {
        "loss": 2.6345,
        "grad_norm": 2.948887825012207,
        "learning_rate": 2.0359867445372693e-05,
        "epoch": 0.9315150160220583,
        "step": 12500
    },
    {
        "loss": 3.0599,
        "grad_norm": 2.364170789718628,
        "learning_rate": 2.0317332224436847e-05,
        "epoch": 0.93158953722334,
        "step": 12501
    },
    {
        "loss": 2.4521,
        "grad_norm": 3.0340566635131836,
        "learning_rate": 2.0274836456998937e-05,
        "epoch": 0.9316640584246219,
        "step": 12502
    },
    {
        "loss": 1.8423,
        "grad_norm": 3.007580041885376,
        "learning_rate": 2.0232380164099984e-05,
        "epoch": 0.9317385796259036,
        "step": 12503
    },
    {
        "loss": 1.6327,
        "grad_norm": 3.4780592918395996,
        "learning_rate": 2.0189963366761434e-05,
        "epoch": 0.9318131008271854,
        "step": 12504
    },
    {
        "loss": 2.3635,
        "grad_norm": 3.2422947883605957,
        "learning_rate": 2.0147586085985403e-05,
        "epoch": 0.9318876220284671,
        "step": 12505
    },
    {
        "loss": 1.5718,
        "grad_norm": 2.8500990867614746,
        "learning_rate": 2.0105248342754135e-05,
        "epoch": 0.9319621432297489,
        "step": 12506
    },
    {
        "loss": 1.0,
        "grad_norm": 3.7835605144500732,
        "learning_rate": 2.006295015803047e-05,
        "epoch": 0.9320366644310306,
        "step": 12507
    },
    {
        "loss": 2.4878,
        "grad_norm": 1.9633171558380127,
        "learning_rate": 2.002069155275753e-05,
        "epoch": 0.9321111856323124,
        "step": 12508
    },
    {
        "loss": 2.3786,
        "grad_norm": 2.7028143405914307,
        "learning_rate": 1.9978472547859117e-05,
        "epoch": 0.9321857068335941,
        "step": 12509
    },
    {
        "loss": 2.419,
        "grad_norm": 2.2431282997131348,
        "learning_rate": 1.993629316423906e-05,
        "epoch": 0.932260228034876,
        "step": 12510
    },
    {
        "loss": 2.2509,
        "grad_norm": 2.774050712585449,
        "learning_rate": 1.9894153422781925e-05,
        "epoch": 0.9323347492361577,
        "step": 12511
    },
    {
        "loss": 2.5509,
        "grad_norm": 1.4592278003692627,
        "learning_rate": 1.9852053344352396e-05,
        "epoch": 0.9324092704374395,
        "step": 12512
    },
    {
        "loss": 2.012,
        "grad_norm": 3.9625728130340576,
        "learning_rate": 1.9809992949795543e-05,
        "epoch": 0.9324837916387212,
        "step": 12513
    },
    {
        "loss": 2.818,
        "grad_norm": 4.366984844207764,
        "learning_rate": 1.976797225993704e-05,
        "epoch": 0.932558312840003,
        "step": 12514
    },
    {
        "loss": 3.102,
        "grad_norm": 3.594510793685913,
        "learning_rate": 1.9725991295582402e-05,
        "epoch": 0.9326328340412847,
        "step": 12515
    },
    {
        "loss": 2.5649,
        "grad_norm": 1.8583370447158813,
        "learning_rate": 1.9684050077518036e-05,
        "epoch": 0.9327073552425665,
        "step": 12516
    },
    {
        "loss": 2.9887,
        "grad_norm": 3.398029088973999,
        "learning_rate": 1.9642148626510306e-05,
        "epoch": 0.9327818764438482,
        "step": 12517
    },
    {
        "loss": 2.6972,
        "grad_norm": 2.0717973709106445,
        "learning_rate": 1.9600286963305937e-05,
        "epoch": 0.9328563976451301,
        "step": 12518
    },
    {
        "loss": 2.19,
        "grad_norm": 2.8088877201080322,
        "learning_rate": 1.9558465108632152e-05,
        "epoch": 0.9329309188464118,
        "step": 12519
    },
    {
        "loss": 2.4251,
        "grad_norm": 3.1004276275634766,
        "learning_rate": 1.9516683083196175e-05,
        "epoch": 0.9330054400476936,
        "step": 12520
    },
    {
        "loss": 2.2658,
        "grad_norm": 4.3304266929626465,
        "learning_rate": 1.9474940907685824e-05,
        "epoch": 0.9330799612489753,
        "step": 12521
    },
    {
        "loss": 1.6869,
        "grad_norm": 3.0788307189941406,
        "learning_rate": 1.94332386027689e-05,
        "epoch": 0.9331544824502571,
        "step": 12522
    },
    {
        "loss": 1.4147,
        "grad_norm": 1.2824437618255615,
        "learning_rate": 1.9391576189093607e-05,
        "epoch": 0.9332290036515388,
        "step": 12523
    },
    {
        "loss": 2.7756,
        "grad_norm": 5.583252429962158,
        "learning_rate": 1.934995368728828e-05,
        "epoch": 0.9333035248528206,
        "step": 12524
    },
    {
        "loss": 1.9459,
        "grad_norm": 2.246619701385498,
        "learning_rate": 1.9308371117961742e-05,
        "epoch": 0.9333780460541024,
        "step": 12525
    },
    {
        "loss": 2.852,
        "grad_norm": 2.322542428970337,
        "learning_rate": 1.9266828501702805e-05,
        "epoch": 0.9334525672553842,
        "step": 12526
    },
    {
        "loss": 2.0646,
        "grad_norm": 3.1688902378082275,
        "learning_rate": 1.9225325859080624e-05,
        "epoch": 0.9335270884566659,
        "step": 12527
    },
    {
        "loss": 2.1114,
        "grad_norm": 3.3253118991851807,
        "learning_rate": 1.9183863210644358e-05,
        "epoch": 0.9336016096579477,
        "step": 12528
    },
    {
        "loss": 1.9117,
        "grad_norm": 3.783342123031616,
        "learning_rate": 1.9142440576923716e-05,
        "epoch": 0.9336761308592294,
        "step": 12529
    },
    {
        "loss": 1.8099,
        "grad_norm": 4.12213659286499,
        "learning_rate": 1.910105797842825e-05,
        "epoch": 0.9337506520605112,
        "step": 12530
    },
    {
        "loss": 2.6268,
        "grad_norm": 1.9840339422225952,
        "learning_rate": 1.9059715435647985e-05,
        "epoch": 0.9338251732617929,
        "step": 12531
    },
    {
        "loss": 3.1422,
        "grad_norm": 3.082418203353882,
        "learning_rate": 1.9018412969052902e-05,
        "epoch": 0.9338996944630747,
        "step": 12532
    },
    {
        "loss": 2.6554,
        "grad_norm": 3.539644241333008,
        "learning_rate": 1.8977150599093117e-05,
        "epoch": 0.9339742156643565,
        "step": 12533
    },
    {
        "loss": 2.9089,
        "grad_norm": 2.4936840534210205,
        "learning_rate": 1.8935928346199217e-05,
        "epoch": 0.9340487368656383,
        "step": 12534
    },
    {
        "loss": 2.8627,
        "grad_norm": 2.0775232315063477,
        "learning_rate": 1.889474623078138e-05,
        "epoch": 0.9341232580669201,
        "step": 12535
    },
    {
        "loss": 2.538,
        "grad_norm": 2.535839319229126,
        "learning_rate": 1.8853604273230473e-05,
        "epoch": 0.9341977792682018,
        "step": 12536
    },
    {
        "loss": 2.4186,
        "grad_norm": 1.9959402084350586,
        "learning_rate": 1.8812502493917094e-05,
        "epoch": 0.9342723004694836,
        "step": 12537
    },
    {
        "loss": 2.1947,
        "grad_norm": 2.1305463314056396,
        "learning_rate": 1.8771440913192052e-05,
        "epoch": 0.9343468216707653,
        "step": 12538
    },
    {
        "loss": 1.8014,
        "grad_norm": 3.3069820404052734,
        "learning_rate": 1.8730419551386402e-05,
        "epoch": 0.9344213428720471,
        "step": 12539
    },
    {
        "loss": 2.5589,
        "grad_norm": 2.2092459201812744,
        "learning_rate": 1.8689438428811112e-05,
        "epoch": 0.9344958640733289,
        "step": 12540
    },
    {
        "loss": 2.3266,
        "grad_norm": 2.2914226055145264,
        "learning_rate": 1.8648497565757183e-05,
        "epoch": 0.9345703852746107,
        "step": 12541
    },
    {
        "loss": 2.5371,
        "grad_norm": 2.4860494136810303,
        "learning_rate": 1.8607596982495945e-05,
        "epoch": 0.9346449064758924,
        "step": 12542
    },
    {
        "loss": 2.425,
        "grad_norm": 2.122764825820923,
        "learning_rate": 1.8566736699278554e-05,
        "epoch": 0.9347194276771742,
        "step": 12543
    },
    {
        "loss": 2.106,
        "grad_norm": 3.152942180633545,
        "learning_rate": 1.8525916736336145e-05,
        "epoch": 0.9347939488784559,
        "step": 12544
    },
    {
        "loss": 2.6702,
        "grad_norm": 1.6644935607910156,
        "learning_rate": 1.848513711388029e-05,
        "epoch": 0.9348684700797377,
        "step": 12545
    },
    {
        "loss": 2.1973,
        "grad_norm": 3.427125930786133,
        "learning_rate": 1.844439785210199e-05,
        "epoch": 0.9349429912810194,
        "step": 12546
    },
    {
        "loss": 2.7498,
        "grad_norm": 3.071194887161255,
        "learning_rate": 1.840369897117282e-05,
        "epoch": 0.9350175124823012,
        "step": 12547
    },
    {
        "loss": 2.7498,
        "grad_norm": 1.8881958723068237,
        "learning_rate": 1.8363040491243998e-05,
        "epoch": 0.935092033683583,
        "step": 12548
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.249462366104126,
        "learning_rate": 1.8322422432446996e-05,
        "epoch": 0.9351665548848648,
        "step": 12549
    },
    {
        "loss": 1.8823,
        "grad_norm": 2.7268247604370117,
        "learning_rate": 1.828184481489309e-05,
        "epoch": 0.9352410760861465,
        "step": 12550
    },
    {
        "loss": 3.0011,
        "grad_norm": 2.6433444023132324,
        "learning_rate": 1.824130765867349e-05,
        "epoch": 0.9353155972874283,
        "step": 12551
    },
    {
        "loss": 2.069,
        "grad_norm": 2.979804515838623,
        "learning_rate": 1.8200810983859618e-05,
        "epoch": 0.93539011848871,
        "step": 12552
    },
    {
        "loss": 2.2553,
        "grad_norm": 2.011993646621704,
        "learning_rate": 1.816035481050257e-05,
        "epoch": 0.9354646396899918,
        "step": 12553
    },
    {
        "loss": 2.6328,
        "grad_norm": 2.9398410320281982,
        "learning_rate": 1.8119939158633736e-05,
        "epoch": 0.9355391608912735,
        "step": 12554
    },
    {
        "loss": 2.4657,
        "grad_norm": 3.5534918308258057,
        "learning_rate": 1.807956404826393e-05,
        "epoch": 0.9356136820925554,
        "step": 12555
    },
    {
        "loss": 2.1914,
        "grad_norm": 4.789982795715332,
        "learning_rate": 1.8039229499384436e-05,
        "epoch": 0.9356882032938371,
        "step": 12556
    },
    {
        "loss": 2.8495,
        "grad_norm": 4.187112808227539,
        "learning_rate": 1.79989355319661e-05,
        "epoch": 0.9357627244951189,
        "step": 12557
    },
    {
        "loss": 2.26,
        "grad_norm": 3.269245147705078,
        "learning_rate": 1.795868216595976e-05,
        "epoch": 0.9358372456964006,
        "step": 12558
    },
    {
        "loss": 2.0866,
        "grad_norm": 1.7444686889648438,
        "learning_rate": 1.791846942129628e-05,
        "epoch": 0.9359117668976824,
        "step": 12559
    },
    {
        "loss": 2.312,
        "grad_norm": 3.229321241378784,
        "learning_rate": 1.7878297317886227e-05,
        "epoch": 0.9359862880989641,
        "step": 12560
    },
    {
        "loss": 1.6029,
        "grad_norm": 3.5965888500213623,
        "learning_rate": 1.7838165875620085e-05,
        "epoch": 0.9360608093002459,
        "step": 12561
    },
    {
        "loss": 2.6661,
        "grad_norm": 1.929061770439148,
        "learning_rate": 1.7798075114368364e-05,
        "epoch": 0.9361353305015276,
        "step": 12562
    },
    {
        "loss": 2.7467,
        "grad_norm": 2.5052742958068848,
        "learning_rate": 1.7758025053981264e-05,
        "epoch": 0.9362098517028095,
        "step": 12563
    },
    {
        "loss": 2.0303,
        "grad_norm": 2.389279365539551,
        "learning_rate": 1.7718015714288783e-05,
        "epoch": 0.9362843729040912,
        "step": 12564
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.741590976715088,
        "learning_rate": 1.7678047115101126e-05,
        "epoch": 0.936358894105373,
        "step": 12565
    },
    {
        "loss": 2.062,
        "grad_norm": 2.4999003410339355,
        "learning_rate": 1.7638119276207688e-05,
        "epoch": 0.9364334153066547,
        "step": 12566
    },
    {
        "loss": 1.6326,
        "grad_norm": 4.422389507293701,
        "learning_rate": 1.759823221737832e-05,
        "epoch": 0.9365079365079365,
        "step": 12567
    },
    {
        "loss": 2.438,
        "grad_norm": 2.91192364692688,
        "learning_rate": 1.7558385958362313e-05,
        "epoch": 0.9365824577092182,
        "step": 12568
    },
    {
        "loss": 2.331,
        "grad_norm": 2.766309976577759,
        "learning_rate": 1.7518580518888782e-05,
        "epoch": 0.9366569789105,
        "step": 12569
    },
    {
        "loss": 2.9308,
        "grad_norm": 3.551177978515625,
        "learning_rate": 1.747881591866687e-05,
        "epoch": 0.9367315001117819,
        "step": 12570
    },
    {
        "loss": 1.7469,
        "grad_norm": 3.4659903049468994,
        "learning_rate": 1.7439092177385174e-05,
        "epoch": 0.9368060213130636,
        "step": 12571
    },
    {
        "loss": 2.237,
        "grad_norm": 2.342210054397583,
        "learning_rate": 1.7399409314712367e-05,
        "epoch": 0.9368805425143454,
        "step": 12572
    },
    {
        "loss": 1.731,
        "grad_norm": 4.008185386657715,
        "learning_rate": 1.7359767350296685e-05,
        "epoch": 0.9369550637156271,
        "step": 12573
    },
    {
        "loss": 2.7255,
        "grad_norm": 2.0302975177764893,
        "learning_rate": 1.7320166303766118e-05,
        "epoch": 0.9370295849169089,
        "step": 12574
    },
    {
        "loss": 2.3444,
        "grad_norm": 4.956419944763184,
        "learning_rate": 1.7280606194728377e-05,
        "epoch": 0.9371041061181906,
        "step": 12575
    },
    {
        "loss": 2.2879,
        "grad_norm": 3.243476152420044,
        "learning_rate": 1.724108704277113e-05,
        "epoch": 0.9371786273194724,
        "step": 12576
    },
    {
        "loss": 2.8101,
        "grad_norm": 2.34213924407959,
        "learning_rate": 1.7201608867461515e-05,
        "epoch": 0.9372531485207541,
        "step": 12577
    },
    {
        "loss": 2.6973,
        "grad_norm": 2.683863401412964,
        "learning_rate": 1.7162171688346495e-05,
        "epoch": 0.937327669722036,
        "step": 12578
    },
    {
        "loss": 1.863,
        "grad_norm": 3.349081516265869,
        "learning_rate": 1.7122775524952606e-05,
        "epoch": 0.9374021909233177,
        "step": 12579
    },
    {
        "loss": 2.5394,
        "grad_norm": 3.770134687423706,
        "learning_rate": 1.708342039678634e-05,
        "epoch": 0.9374767121245995,
        "step": 12580
    },
    {
        "loss": 3.019,
        "grad_norm": 3.145740509033203,
        "learning_rate": 1.7044106323333585e-05,
        "epoch": 0.9375512333258812,
        "step": 12581
    },
    {
        "loss": 2.3005,
        "grad_norm": 2.4899344444274902,
        "learning_rate": 1.7004833324060132e-05,
        "epoch": 0.937625754527163,
        "step": 12582
    },
    {
        "loss": 2.7455,
        "grad_norm": 2.1753785610198975,
        "learning_rate": 1.696560141841126e-05,
        "epoch": 0.9377002757284447,
        "step": 12583
    },
    {
        "loss": 2.3781,
        "grad_norm": 3.1316983699798584,
        "learning_rate": 1.6926410625811918e-05,
        "epoch": 0.9377747969297265,
        "step": 12584
    },
    {
        "loss": 1.7999,
        "grad_norm": 2.750936269760132,
        "learning_rate": 1.6887260965666963e-05,
        "epoch": 0.9378493181310082,
        "step": 12585
    },
    {
        "loss": 1.743,
        "grad_norm": 2.49039626121521,
        "learning_rate": 1.6848152457360367e-05,
        "epoch": 0.9379238393322901,
        "step": 12586
    },
    {
        "loss": 2.4623,
        "grad_norm": 3.010563611984253,
        "learning_rate": 1.6809085120256273e-05,
        "epoch": 0.9379983605335718,
        "step": 12587
    },
    {
        "loss": 2.1582,
        "grad_norm": 3.762089967727661,
        "learning_rate": 1.6770058973698145e-05,
        "epoch": 0.9380728817348536,
        "step": 12588
    },
    {
        "loss": 1.9897,
        "grad_norm": 3.9179811477661133,
        "learning_rate": 1.6731074037008976e-05,
        "epoch": 0.9381474029361353,
        "step": 12589
    },
    {
        "loss": 2.9375,
        "grad_norm": 2.5951650142669678,
        "learning_rate": 1.669213032949166e-05,
        "epoch": 0.9382219241374171,
        "step": 12590
    },
    {
        "loss": 2.2119,
        "grad_norm": 3.5433084964752197,
        "learning_rate": 1.6653227870428355e-05,
        "epoch": 0.9382964453386988,
        "step": 12591
    },
    {
        "loss": 2.0466,
        "grad_norm": 4.102902412414551,
        "learning_rate": 1.6614366679081095e-05,
        "epoch": 0.9383709665399806,
        "step": 12592
    },
    {
        "loss": 3.0103,
        "grad_norm": 2.966909646987915,
        "learning_rate": 1.657554677469124e-05,
        "epoch": 0.9384454877412624,
        "step": 12593
    },
    {
        "loss": 3.016,
        "grad_norm": 2.3723645210266113,
        "learning_rate": 1.6536768176479812e-05,
        "epoch": 0.9385200089425442,
        "step": 12594
    },
    {
        "loss": 2.1247,
        "grad_norm": 2.0706350803375244,
        "learning_rate": 1.6498030903647277e-05,
        "epoch": 0.9385945301438259,
        "step": 12595
    },
    {
        "loss": 2.4602,
        "grad_norm": 2.05875825881958,
        "learning_rate": 1.6459334975373987e-05,
        "epoch": 0.9386690513451077,
        "step": 12596
    },
    {
        "loss": 2.9911,
        "grad_norm": 3.333815574645996,
        "learning_rate": 1.642068041081921e-05,
        "epoch": 0.9387435725463894,
        "step": 12597
    },
    {
        "loss": 2.4725,
        "grad_norm": 3.1354501247406006,
        "learning_rate": 1.6382067229122334e-05,
        "epoch": 0.9388180937476712,
        "step": 12598
    },
    {
        "loss": 2.2533,
        "grad_norm": 3.6489593982696533,
        "learning_rate": 1.6343495449401858e-05,
        "epoch": 0.9388926149489529,
        "step": 12599
    },
    {
        "loss": 2.9758,
        "grad_norm": 2.3589975833892822,
        "learning_rate": 1.6304965090756095e-05,
        "epoch": 0.9389671361502347,
        "step": 12600
    },
    {
        "loss": 1.8667,
        "grad_norm": 3.6900243759155273,
        "learning_rate": 1.62664761722626e-05,
        "epoch": 0.9390416573515165,
        "step": 12601
    },
    {
        "loss": 2.4154,
        "grad_norm": 2.6508872509002686,
        "learning_rate": 1.6228028712978428e-05,
        "epoch": 0.9391161785527983,
        "step": 12602
    },
    {
        "loss": 2.2644,
        "grad_norm": 4.129291534423828,
        "learning_rate": 1.6189622731940335e-05,
        "epoch": 0.93919069975408,
        "step": 12603
    },
    {
        "loss": 2.4051,
        "grad_norm": 3.8069214820861816,
        "learning_rate": 1.61512582481642e-05,
        "epoch": 0.9392652209553618,
        "step": 12604
    },
    {
        "loss": 2.7325,
        "grad_norm": 4.203175067901611,
        "learning_rate": 1.6112935280645782e-05,
        "epoch": 0.9393397421566436,
        "step": 12605
    },
    {
        "loss": 1.8435,
        "grad_norm": 3.069742202758789,
        "learning_rate": 1.6074653848359735e-05,
        "epoch": 0.9394142633579253,
        "step": 12606
    },
    {
        "loss": 2.4052,
        "grad_norm": 1.5938739776611328,
        "learning_rate": 1.6036413970260666e-05,
        "epoch": 0.9394887845592071,
        "step": 12607
    },
    {
        "loss": 2.552,
        "grad_norm": 3.252581834793091,
        "learning_rate": 1.5998215665282313e-05,
        "epoch": 0.9395633057604889,
        "step": 12608
    },
    {
        "loss": 2.4276,
        "grad_norm": 2.77632212638855,
        "learning_rate": 1.5960058952337854e-05,
        "epoch": 0.9396378269617707,
        "step": 12609
    },
    {
        "loss": 1.4696,
        "grad_norm": 3.3853249549865723,
        "learning_rate": 1.5921943850320064e-05,
        "epoch": 0.9397123481630524,
        "step": 12610
    },
    {
        "loss": 2.7106,
        "grad_norm": 2.649576187133789,
        "learning_rate": 1.588387037810092e-05,
        "epoch": 0.9397868693643342,
        "step": 12611
    },
    {
        "loss": 2.2744,
        "grad_norm": 2.707850694656372,
        "learning_rate": 1.5845838554531732e-05,
        "epoch": 0.9398613905656159,
        "step": 12612
    },
    {
        "loss": 2.1456,
        "grad_norm": 2.942939519882202,
        "learning_rate": 1.580784839844348e-05,
        "epoch": 0.9399359117668977,
        "step": 12613
    },
    {
        "loss": 3.0603,
        "grad_norm": 2.90447735786438,
        "learning_rate": 1.5769899928646235e-05,
        "epoch": 0.9400104329681794,
        "step": 12614
    },
    {
        "loss": 1.7899,
        "grad_norm": 3.3398871421813965,
        "learning_rate": 1.573199316392948e-05,
        "epoch": 0.9400849541694613,
        "step": 12615
    },
    {
        "loss": 2.5223,
        "grad_norm": 3.7190632820129395,
        "learning_rate": 1.5694128123062313e-05,
        "epoch": 0.940159475370743,
        "step": 12616
    },
    {
        "loss": 2.6253,
        "grad_norm": 2.036522150039673,
        "learning_rate": 1.5656304824792646e-05,
        "epoch": 0.9402339965720248,
        "step": 12617
    },
    {
        "loss": 1.8042,
        "grad_norm": 3.736748695373535,
        "learning_rate": 1.5618523287848286e-05,
        "epoch": 0.9403085177733065,
        "step": 12618
    },
    {
        "loss": 2.4127,
        "grad_norm": 2.27340030670166,
        "learning_rate": 1.5580783530935917e-05,
        "epoch": 0.9403830389745883,
        "step": 12619
    },
    {
        "loss": 2.8013,
        "grad_norm": 1.8911628723144531,
        "learning_rate": 1.554308557274189e-05,
        "epoch": 0.94045756017587,
        "step": 12620
    },
    {
        "loss": 2.7951,
        "grad_norm": 3.697906970977783,
        "learning_rate": 1.550542943193163e-05,
        "epoch": 0.9405320813771518,
        "step": 12621
    },
    {
        "loss": 1.8105,
        "grad_norm": 2.8789894580841064,
        "learning_rate": 1.5467815127149853e-05,
        "epoch": 0.9406066025784335,
        "step": 12622
    },
    {
        "loss": 2.2785,
        "grad_norm": 4.788883686065674,
        "learning_rate": 1.543024267702079e-05,
        "epoch": 0.9406811237797154,
        "step": 12623
    },
    {
        "loss": 2.6347,
        "grad_norm": 1.9530450105667114,
        "learning_rate": 1.539271210014762e-05,
        "epoch": 0.9407556449809971,
        "step": 12624
    },
    {
        "loss": 2.1528,
        "grad_norm": 2.1150169372558594,
        "learning_rate": 1.5355223415113207e-05,
        "epoch": 0.9408301661822789,
        "step": 12625
    },
    {
        "loss": 2.5495,
        "grad_norm": 2.2992453575134277,
        "learning_rate": 1.531777664047913e-05,
        "epoch": 0.9409046873835606,
        "step": 12626
    },
    {
        "loss": 1.6763,
        "grad_norm": 3.559116840362549,
        "learning_rate": 1.528037179478671e-05,
        "epoch": 0.9409792085848424,
        "step": 12627
    },
    {
        "loss": 2.663,
        "grad_norm": 2.319146156311035,
        "learning_rate": 1.5243008896556255e-05,
        "epoch": 0.9410537297861241,
        "step": 12628
    },
    {
        "loss": 2.7302,
        "grad_norm": 2.250314950942993,
        "learning_rate": 1.5205687964287352e-05,
        "epoch": 0.9411282509874059,
        "step": 12629
    },
    {
        "loss": 3.0637,
        "grad_norm": 3.6533548831939697,
        "learning_rate": 1.516840901645874e-05,
        "epoch": 0.9412027721886876,
        "step": 12630
    },
    {
        "loss": 2.4313,
        "grad_norm": 3.6722004413604736,
        "learning_rate": 1.513117207152861e-05,
        "epoch": 0.9412772933899695,
        "step": 12631
    },
    {
        "loss": 2.24,
        "grad_norm": 3.460705041885376,
        "learning_rate": 1.5093977147934035e-05,
        "epoch": 0.9413518145912512,
        "step": 12632
    },
    {
        "loss": 2.5764,
        "grad_norm": 3.9710490703582764,
        "learning_rate": 1.5056824264091585e-05,
        "epoch": 0.941426335792533,
        "step": 12633
    },
    {
        "loss": 2.0472,
        "grad_norm": 3.0564661026000977,
        "learning_rate": 1.5019713438396822e-05,
        "epoch": 0.9415008569938147,
        "step": 12634
    },
    {
        "loss": 2.2942,
        "grad_norm": 2.304924488067627,
        "learning_rate": 1.4982644689224412e-05,
        "epoch": 0.9415753781950965,
        "step": 12635
    },
    {
        "loss": 2.4896,
        "grad_norm": 3.191016912460327,
        "learning_rate": 1.4945618034928544e-05,
        "epoch": 0.9416498993963782,
        "step": 12636
    },
    {
        "loss": 2.4436,
        "grad_norm": 2.6597230434417725,
        "learning_rate": 1.490863349384205e-05,
        "epoch": 0.94172442059766,
        "step": 12637
    },
    {
        "loss": 2.1061,
        "grad_norm": 4.065380096435547,
        "learning_rate": 1.4871691084277383e-05,
        "epoch": 0.9417989417989417,
        "step": 12638
    },
    {
        "loss": 2.5133,
        "grad_norm": 2.621467113494873,
        "learning_rate": 1.4834790824525879e-05,
        "epoch": 0.9418734630002236,
        "step": 12639
    },
    {
        "loss": 1.6879,
        "grad_norm": 3.3155179023742676,
        "learning_rate": 1.4797932732858e-05,
        "epoch": 0.9419479842015053,
        "step": 12640
    },
    {
        "loss": 2.7304,
        "grad_norm": 1.8009146451950073,
        "learning_rate": 1.4761116827523558e-05,
        "epoch": 0.9420225054027871,
        "step": 12641
    },
    {
        "loss": 1.9793,
        "grad_norm": 2.687300682067871,
        "learning_rate": 1.4724343126751116e-05,
        "epoch": 0.9420970266040689,
        "step": 12642
    },
    {
        "loss": 1.7864,
        "grad_norm": 3.229871988296509,
        "learning_rate": 1.46876116487487e-05,
        "epoch": 0.9421715478053506,
        "step": 12643
    },
    {
        "loss": 2.6544,
        "grad_norm": 3.3334784507751465,
        "learning_rate": 1.4650922411703206e-05,
        "epoch": 0.9422460690066324,
        "step": 12644
    },
    {
        "loss": 3.09,
        "grad_norm": 2.280437469482422,
        "learning_rate": 1.4614275433780678e-05,
        "epoch": 0.9423205902079141,
        "step": 12645
    },
    {
        "loss": 2.5179,
        "grad_norm": 2.6387596130371094,
        "learning_rate": 1.457767073312616e-05,
        "epoch": 0.942395111409196,
        "step": 12646
    },
    {
        "loss": 2.7451,
        "grad_norm": 2.4228429794311523,
        "learning_rate": 1.4541108327863973e-05,
        "epoch": 0.9424696326104777,
        "step": 12647
    },
    {
        "loss": 2.2372,
        "grad_norm": 2.933037757873535,
        "learning_rate": 1.4504588236097294e-05,
        "epoch": 0.9425441538117595,
        "step": 12648
    },
    {
        "loss": 2.2027,
        "grad_norm": 4.4682183265686035,
        "learning_rate": 1.4468110475908447e-05,
        "epoch": 0.9426186750130412,
        "step": 12649
    },
    {
        "loss": 2.651,
        "grad_norm": 2.475491523742676,
        "learning_rate": 1.443167506535864e-05,
        "epoch": 0.942693196214323,
        "step": 12650
    },
    {
        "loss": 1.3345,
        "grad_norm": 2.7276809215545654,
        "learning_rate": 1.439528202248841e-05,
        "epoch": 0.9427677174156047,
        "step": 12651
    },
    {
        "loss": 2.7743,
        "grad_norm": 2.914044141769409,
        "learning_rate": 1.4358931365316996e-05,
        "epoch": 0.9428422386168865,
        "step": 12652
    },
    {
        "loss": 1.3746,
        "grad_norm": 3.606994867324829,
        "learning_rate": 1.4322623111842947e-05,
        "epoch": 0.9429167598181682,
        "step": 12653
    },
    {
        "loss": 2.7952,
        "grad_norm": 2.384227991104126,
        "learning_rate": 1.4286357280043583e-05,
        "epoch": 0.9429912810194501,
        "step": 12654
    },
    {
        "loss": 2.0994,
        "grad_norm": 3.4231104850769043,
        "learning_rate": 1.425013388787525e-05,
        "epoch": 0.9430658022207318,
        "step": 12655
    },
    {
        "loss": 2.1432,
        "grad_norm": 3.5444400310516357,
        "learning_rate": 1.4213952953273557e-05,
        "epoch": 0.9431403234220136,
        "step": 12656
    },
    {
        "loss": 2.7949,
        "grad_norm": 2.3613617420196533,
        "learning_rate": 1.4177814494152564e-05,
        "epoch": 0.9432148446232953,
        "step": 12657
    },
    {
        "loss": 2.7363,
        "grad_norm": 1.6265629529953003,
        "learning_rate": 1.4141718528405812e-05,
        "epoch": 0.9432893658245771,
        "step": 12658
    },
    {
        "loss": 1.9457,
        "grad_norm": 3.139838933944702,
        "learning_rate": 1.4105665073905538e-05,
        "epoch": 0.9433638870258588,
        "step": 12659
    },
    {
        "loss": 2.6142,
        "grad_norm": 2.4359707832336426,
        "learning_rate": 1.406965414850292e-05,
        "epoch": 0.9434384082271406,
        "step": 12660
    },
    {
        "loss": 2.5095,
        "grad_norm": 1.8472899198532104,
        "learning_rate": 1.4033685770028282e-05,
        "epoch": 0.9435129294284224,
        "step": 12661
    },
    {
        "loss": 1.7715,
        "grad_norm": 3.896012783050537,
        "learning_rate": 1.399775995629068e-05,
        "epoch": 0.9435874506297042,
        "step": 12662
    },
    {
        "loss": 2.5547,
        "grad_norm": 2.5067131519317627,
        "learning_rate": 1.3961876725078082e-05,
        "epoch": 0.9436619718309859,
        "step": 12663
    },
    {
        "loss": 2.3418,
        "grad_norm": 2.1069679260253906,
        "learning_rate": 1.3926036094157624e-05,
        "epoch": 0.9437364930322677,
        "step": 12664
    },
    {
        "loss": 1.868,
        "grad_norm": 3.6866800785064697,
        "learning_rate": 1.3890238081275065e-05,
        "epoch": 0.9438110142335494,
        "step": 12665
    },
    {
        "loss": 0.8269,
        "grad_norm": 2.4719491004943848,
        "learning_rate": 1.3854482704155103e-05,
        "epoch": 0.9438855354348312,
        "step": 12666
    },
    {
        "loss": 2.4365,
        "grad_norm": 3.401688575744629,
        "learning_rate": 1.3818769980501622e-05,
        "epoch": 0.9439600566361129,
        "step": 12667
    },
    {
        "loss": 2.2216,
        "grad_norm": 3.1798434257507324,
        "learning_rate": 1.378309992799689e-05,
        "epoch": 0.9440345778373948,
        "step": 12668
    },
    {
        "loss": 2.7711,
        "grad_norm": 1.960971713066101,
        "learning_rate": 1.3747472564302511e-05,
        "epoch": 0.9441090990386765,
        "step": 12669
    },
    {
        "loss": 2.5018,
        "grad_norm": 2.6939756870269775,
        "learning_rate": 1.3711887907058617e-05,
        "epoch": 0.9441836202399583,
        "step": 12670
    },
    {
        "loss": 1.7153,
        "grad_norm": 2.824718952178955,
        "learning_rate": 1.367634597388452e-05,
        "epoch": 0.94425814144124,
        "step": 12671
    },
    {
        "loss": 2.57,
        "grad_norm": 3.9534826278686523,
        "learning_rate": 1.364084678237806e-05,
        "epoch": 0.9443326626425218,
        "step": 12672
    },
    {
        "loss": 2.5163,
        "grad_norm": 2.3677964210510254,
        "learning_rate": 1.360539035011602e-05,
        "epoch": 0.9444071838438035,
        "step": 12673
    },
    {
        "loss": 2.0152,
        "grad_norm": 2.098846912384033,
        "learning_rate": 1.3569976694654163e-05,
        "epoch": 0.9444817050450853,
        "step": 12674
    },
    {
        "loss": 2.495,
        "grad_norm": 2.839444160461426,
        "learning_rate": 1.3534605833526815e-05,
        "epoch": 0.944556226246367,
        "step": 12675
    },
    {
        "loss": 2.0134,
        "grad_norm": 3.672292709350586,
        "learning_rate": 1.3499277784247455e-05,
        "epoch": 0.9446307474476489,
        "step": 12676
    },
    {
        "loss": 1.589,
        "grad_norm": 2.397491455078125,
        "learning_rate": 1.3463992564307881e-05,
        "epoch": 0.9447052686489307,
        "step": 12677
    },
    {
        "loss": 2.0707,
        "grad_norm": 2.5308663845062256,
        "learning_rate": 1.3428750191179174e-05,
        "epoch": 0.9447797898502124,
        "step": 12678
    },
    {
        "loss": 2.6612,
        "grad_norm": 2.046902894973755,
        "learning_rate": 1.339355068231094e-05,
        "epoch": 0.9448543110514942,
        "step": 12679
    },
    {
        "loss": 1.9415,
        "grad_norm": 2.2775778770446777,
        "learning_rate": 1.3358394055131485e-05,
        "epoch": 0.9449288322527759,
        "step": 12680
    },
    {
        "loss": 1.9947,
        "grad_norm": 2.400972366333008,
        "learning_rate": 1.3323280327048171e-05,
        "epoch": 0.9450033534540577,
        "step": 12681
    },
    {
        "loss": 2.2413,
        "grad_norm": 4.371061325073242,
        "learning_rate": 1.3288209515446893e-05,
        "epoch": 0.9450778746553394,
        "step": 12682
    },
    {
        "loss": 2.6844,
        "grad_norm": 2.621079921722412,
        "learning_rate": 1.325318163769228e-05,
        "epoch": 0.9451523958566213,
        "step": 12683
    },
    {
        "loss": 2.6071,
        "grad_norm": 1.8818018436431885,
        "learning_rate": 1.3218196711127928e-05,
        "epoch": 0.945226917057903,
        "step": 12684
    },
    {
        "loss": 2.2272,
        "grad_norm": 4.432336330413818,
        "learning_rate": 1.3183254753075935e-05,
        "epoch": 0.9453014382591848,
        "step": 12685
    },
    {
        "loss": 1.8787,
        "grad_norm": 2.1984879970550537,
        "learning_rate": 1.3148355780837151e-05,
        "epoch": 0.9453759594604665,
        "step": 12686
    },
    {
        "loss": 1.7992,
        "grad_norm": 5.41114616394043,
        "learning_rate": 1.311349981169142e-05,
        "epoch": 0.9454504806617483,
        "step": 12687
    },
    {
        "loss": 2.6989,
        "grad_norm": 3.0997848510742188,
        "learning_rate": 1.3078686862896772e-05,
        "epoch": 0.94552500186303,
        "step": 12688
    },
    {
        "loss": 2.1402,
        "grad_norm": 3.5780045986175537,
        "learning_rate": 1.304391695169046e-05,
        "epoch": 0.9455995230643118,
        "step": 12689
    },
    {
        "loss": 2.2622,
        "grad_norm": 3.674448251724243,
        "learning_rate": 1.3009190095288127e-05,
        "epoch": 0.9456740442655935,
        "step": 12690
    },
    {
        "loss": 2.2995,
        "grad_norm": 3.040091037750244,
        "learning_rate": 1.2974506310884104e-05,
        "epoch": 0.9457485654668754,
        "step": 12691
    },
    {
        "loss": 1.8878,
        "grad_norm": 2.3173720836639404,
        "learning_rate": 1.2939865615651626e-05,
        "epoch": 0.9458230866681571,
        "step": 12692
    },
    {
        "loss": 2.8036,
        "grad_norm": 2.652681589126587,
        "learning_rate": 1.290526802674229e-05,
        "epoch": 0.9458976078694389,
        "step": 12693
    },
    {
        "loss": 2.5617,
        "grad_norm": 2.0065577030181885,
        "learning_rate": 1.2870713561286641e-05,
        "epoch": 0.9459721290707206,
        "step": 12694
    },
    {
        "loss": 2.7022,
        "grad_norm": 3.022284507751465,
        "learning_rate": 1.2836202236393624e-05,
        "epoch": 0.9460466502720024,
        "step": 12695
    },
    {
        "loss": 2.5925,
        "grad_norm": 4.018276214599609,
        "learning_rate": 1.2801734069150951e-05,
        "epoch": 0.9461211714732841,
        "step": 12696
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.339787483215332,
        "learning_rate": 1.2767309076624877e-05,
        "epoch": 0.9461956926745659,
        "step": 12697
    },
    {
        "loss": 2.2207,
        "grad_norm": 2.402951240539551,
        "learning_rate": 1.2732927275860462e-05,
        "epoch": 0.9462702138758476,
        "step": 12698
    },
    {
        "loss": 2.2804,
        "grad_norm": 2.875001907348633,
        "learning_rate": 1.2698588683881208e-05,
        "epoch": 0.9463447350771295,
        "step": 12699
    },
    {
        "loss": 2.4894,
        "grad_norm": 3.358349561691284,
        "learning_rate": 1.2664293317689257e-05,
        "epoch": 0.9464192562784112,
        "step": 12700
    },
    {
        "loss": 2.8085,
        "grad_norm": 2.8667263984680176,
        "learning_rate": 1.263004119426533e-05,
        "epoch": 0.946493777479693,
        "step": 12701
    },
    {
        "loss": 2.1982,
        "grad_norm": 3.2244555950164795,
        "learning_rate": 1.2595832330568912e-05,
        "epoch": 0.9465682986809747,
        "step": 12702
    },
    {
        "loss": 2.339,
        "grad_norm": 2.0095958709716797,
        "learning_rate": 1.2561666743537758e-05,
        "epoch": 0.9466428198822565,
        "step": 12703
    },
    {
        "loss": 1.4253,
        "grad_norm": 3.2305519580841064,
        "learning_rate": 1.2527544450088524e-05,
        "epoch": 0.9467173410835382,
        "step": 12704
    },
    {
        "loss": 1.9753,
        "grad_norm": 2.853785991668701,
        "learning_rate": 1.2493465467116206e-05,
        "epoch": 0.94679186228482,
        "step": 12705
    },
    {
        "loss": 2.3472,
        "grad_norm": 3.2386579513549805,
        "learning_rate": 1.2459429811494361e-05,
        "epoch": 0.9468663834861017,
        "step": 12706
    },
    {
        "loss": 2.2535,
        "grad_norm": 3.200502872467041,
        "learning_rate": 1.2425437500075342e-05,
        "epoch": 0.9469409046873836,
        "step": 12707
    },
    {
        "loss": 2.4268,
        "grad_norm": 2.895486831665039,
        "learning_rate": 1.2391488549689623e-05,
        "epoch": 0.9470154258886653,
        "step": 12708
    },
    {
        "loss": 2.0494,
        "grad_norm": 3.289012908935547,
        "learning_rate": 1.2357582977146631e-05,
        "epoch": 0.9470899470899471,
        "step": 12709
    },
    {
        "loss": 1.9471,
        "grad_norm": 2.8202545642852783,
        "learning_rate": 1.2323720799234017e-05,
        "epoch": 0.9471644682912288,
        "step": 12710
    },
    {
        "loss": 1.9883,
        "grad_norm": 1.3183916807174683,
        "learning_rate": 1.2289902032718026e-05,
        "epoch": 0.9472389894925106,
        "step": 12711
    },
    {
        "loss": 1.7996,
        "grad_norm": 4.142179012298584,
        "learning_rate": 1.2256126694343562e-05,
        "epoch": 0.9473135106937924,
        "step": 12712
    },
    {
        "loss": 2.2411,
        "grad_norm": 2.921948194503784,
        "learning_rate": 1.222239480083377e-05,
        "epoch": 0.9473880318950741,
        "step": 12713
    },
    {
        "loss": 1.8788,
        "grad_norm": 2.7429425716400146,
        "learning_rate": 1.2188706368890545e-05,
        "epoch": 0.947462553096356,
        "step": 12714
    },
    {
        "loss": 2.5374,
        "grad_norm": 2.1885383129119873,
        "learning_rate": 1.2155061415194058e-05,
        "epoch": 0.9475370742976377,
        "step": 12715
    },
    {
        "loss": 2.2988,
        "grad_norm": 3.6976022720336914,
        "learning_rate": 1.212145995640307e-05,
        "epoch": 0.9476115954989195,
        "step": 12716
    },
    {
        "loss": 2.0992,
        "grad_norm": 3.5197677612304688,
        "learning_rate": 1.2087902009154627e-05,
        "epoch": 0.9476861167002012,
        "step": 12717
    },
    {
        "loss": 2.7212,
        "grad_norm": 2.838008165359497,
        "learning_rate": 1.2054387590064542e-05,
        "epoch": 0.947760637901483,
        "step": 12718
    },
    {
        "loss": 1.3353,
        "grad_norm": 4.180452823638916,
        "learning_rate": 1.2020916715726837e-05,
        "epoch": 0.9478351591027647,
        "step": 12719
    },
    {
        "loss": 2.2796,
        "grad_norm": 3.128849506378174,
        "learning_rate": 1.1987489402714025e-05,
        "epoch": 0.9479096803040465,
        "step": 12720
    },
    {
        "loss": 2.2849,
        "grad_norm": 3.3863608837127686,
        "learning_rate": 1.1954105667576998e-05,
        "epoch": 0.9479842015053283,
        "step": 12721
    },
    {
        "loss": 2.4019,
        "grad_norm": 1.782086730003357,
        "learning_rate": 1.1920765526845268e-05,
        "epoch": 0.9480587227066101,
        "step": 12722
    },
    {
        "loss": 2.8202,
        "grad_norm": 3.5938608646392822,
        "learning_rate": 1.1887468997026585e-05,
        "epoch": 0.9481332439078918,
        "step": 12723
    },
    {
        "loss": 2.9574,
        "grad_norm": 1.4477894306182861,
        "learning_rate": 1.185421609460705e-05,
        "epoch": 0.9482077651091736,
        "step": 12724
    },
    {
        "loss": 2.1507,
        "grad_norm": 3.5081593990325928,
        "learning_rate": 1.1821006836051452e-05,
        "epoch": 0.9482822863104553,
        "step": 12725
    },
    {
        "loss": 2.0602,
        "grad_norm": 3.5273702144622803,
        "learning_rate": 1.1787841237802589e-05,
        "epoch": 0.9483568075117371,
        "step": 12726
    },
    {
        "loss": 2.2357,
        "grad_norm": 1.7320343255996704,
        "learning_rate": 1.1754719316282048e-05,
        "epoch": 0.9484313287130188,
        "step": 12727
    },
    {
        "loss": 2.4589,
        "grad_norm": 3.259885787963867,
        "learning_rate": 1.1721641087889323e-05,
        "epoch": 0.9485058499143006,
        "step": 12728
    },
    {
        "loss": 2.2861,
        "grad_norm": 3.1056346893310547,
        "learning_rate": 1.1688606569002736e-05,
        "epoch": 0.9485803711155824,
        "step": 12729
    },
    {
        "loss": 2.5203,
        "grad_norm": 2.4032204151153564,
        "learning_rate": 1.1655615775978667e-05,
        "epoch": 0.9486548923168642,
        "step": 12730
    },
    {
        "loss": 2.4444,
        "grad_norm": 2.040740966796875,
        "learning_rate": 1.1622668725151898e-05,
        "epoch": 0.9487294135181459,
        "step": 12731
    },
    {
        "loss": 2.5612,
        "grad_norm": 2.280904531478882,
        "learning_rate": 1.1589765432835731e-05,
        "epoch": 0.9488039347194277,
        "step": 12732
    },
    {
        "loss": 1.8876,
        "grad_norm": 3.2121164798736572,
        "learning_rate": 1.1556905915321558e-05,
        "epoch": 0.9488784559207094,
        "step": 12733
    },
    {
        "loss": 1.2541,
        "grad_norm": 3.3392157554626465,
        "learning_rate": 1.1524090188879155e-05,
        "epoch": 0.9489529771219912,
        "step": 12734
    },
    {
        "loss": 1.8054,
        "grad_norm": 3.6958436965942383,
        "learning_rate": 1.14913182697568e-05,
        "epoch": 0.9490274983232729,
        "step": 12735
    },
    {
        "loss": 2.2947,
        "grad_norm": 3.8284385204315186,
        "learning_rate": 1.1458590174180872e-05,
        "epoch": 0.9491020195245548,
        "step": 12736
    },
    {
        "loss": 2.0968,
        "grad_norm": 3.563779830932617,
        "learning_rate": 1.1425905918356061e-05,
        "epoch": 0.9491765407258365,
        "step": 12737
    },
    {
        "loss": 2.5034,
        "grad_norm": 2.3788483142852783,
        "learning_rate": 1.1393265518465602e-05,
        "epoch": 0.9492510619271183,
        "step": 12738
    },
    {
        "loss": 2.6342,
        "grad_norm": 3.471757650375366,
        "learning_rate": 1.1360668990670598e-05,
        "epoch": 0.9493255831284,
        "step": 12739
    },
    {
        "loss": 2.2991,
        "grad_norm": 3.8271751403808594,
        "learning_rate": 1.1328116351110806e-05,
        "epoch": 0.9494001043296818,
        "step": 12740
    },
    {
        "loss": 2.0277,
        "grad_norm": 3.208688735961914,
        "learning_rate": 1.1295607615903992e-05,
        "epoch": 0.9494746255309635,
        "step": 12741
    },
    {
        "loss": 2.22,
        "grad_norm": 4.495691299438477,
        "learning_rate": 1.126314280114642e-05,
        "epoch": 0.9495491467322453,
        "step": 12742
    },
    {
        "loss": 2.6893,
        "grad_norm": 2.0052409172058105,
        "learning_rate": 1.1230721922912423e-05,
        "epoch": 0.949623667933527,
        "step": 12743
    },
    {
        "loss": 1.519,
        "grad_norm": 2.6915740966796875,
        "learning_rate": 1.1198344997254584e-05,
        "epoch": 0.9496981891348089,
        "step": 12744
    },
    {
        "loss": 2.3974,
        "grad_norm": 3.2410531044006348,
        "learning_rate": 1.1166012040203888e-05,
        "epoch": 0.9497727103360906,
        "step": 12745
    },
    {
        "loss": 2.7623,
        "grad_norm": 2.183062791824341,
        "learning_rate": 1.113372306776933e-05,
        "epoch": 0.9498472315373724,
        "step": 12746
    },
    {
        "loss": 2.7446,
        "grad_norm": 2.71531081199646,
        "learning_rate": 1.1101478095938433e-05,
        "epoch": 0.9499217527386542,
        "step": 12747
    },
    {
        "loss": 1.9875,
        "grad_norm": 2.837137460708618,
        "learning_rate": 1.106927714067646e-05,
        "epoch": 0.9499962739399359,
        "step": 12748
    },
    {
        "loss": 2.1649,
        "grad_norm": 2.690675973892212,
        "learning_rate": 1.1037120217927365e-05,
        "epoch": 0.9500707951412177,
        "step": 12749
    },
    {
        "loss": 1.7519,
        "grad_norm": 2.899071216583252,
        "learning_rate": 1.1005007343613028e-05,
        "epoch": 0.9501453163424994,
        "step": 12750
    },
    {
        "loss": 1.1284,
        "grad_norm": 3.526017665863037,
        "learning_rate": 1.0972938533633581e-05,
        "epoch": 0.9502198375437813,
        "step": 12751
    },
    {
        "loss": 2.396,
        "grad_norm": 4.783058166503906,
        "learning_rate": 1.094091380386728e-05,
        "epoch": 0.950294358745063,
        "step": 12752
    },
    {
        "loss": 2.192,
        "grad_norm": 2.718676805496216,
        "learning_rate": 1.0908933170170744e-05,
        "epoch": 0.9503688799463448,
        "step": 12753
    },
    {
        "loss": 2.5658,
        "grad_norm": 2.5558454990386963,
        "learning_rate": 1.0876996648378512e-05,
        "epoch": 0.9504434011476265,
        "step": 12754
    },
    {
        "loss": 1.5384,
        "grad_norm": 4.274173259735107,
        "learning_rate": 1.084510425430354e-05,
        "epoch": 0.9505179223489083,
        "step": 12755
    },
    {
        "loss": 1.7698,
        "grad_norm": 3.1427581310272217,
        "learning_rate": 1.0813256003736694e-05,
        "epoch": 0.95059244355019,
        "step": 12756
    },
    {
        "loss": 2.6523,
        "grad_norm": 2.787508964538574,
        "learning_rate": 1.078145191244705e-05,
        "epoch": 0.9506669647514718,
        "step": 12757
    },
    {
        "loss": 2.5767,
        "grad_norm": 1.849880337715149,
        "learning_rate": 1.0749691996182032e-05,
        "epoch": 0.9507414859527535,
        "step": 12758
    },
    {
        "loss": 2.0057,
        "grad_norm": 3.7773451805114746,
        "learning_rate": 1.0717976270666763e-05,
        "epoch": 0.9508160071540354,
        "step": 12759
    },
    {
        "loss": 2.117,
        "grad_norm": 5.506278038024902,
        "learning_rate": 1.0686304751604947e-05,
        "epoch": 0.9508905283553171,
        "step": 12760
    },
    {
        "loss": 1.6992,
        "grad_norm": 3.5467617511749268,
        "learning_rate": 1.0654677454678108e-05,
        "epoch": 0.9509650495565989,
        "step": 12761
    },
    {
        "loss": 2.1927,
        "grad_norm": 4.7444868087768555,
        "learning_rate": 1.0623094395545918e-05,
        "epoch": 0.9510395707578806,
        "step": 12762
    },
    {
        "loss": 2.1648,
        "grad_norm": 3.5650322437286377,
        "learning_rate": 1.0591555589846259e-05,
        "epoch": 0.9511140919591624,
        "step": 12763
    },
    {
        "loss": 2.6591,
        "grad_norm": 3.32106614112854,
        "learning_rate": 1.0560061053194947e-05,
        "epoch": 0.9511886131604441,
        "step": 12764
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.1688497066497803,
        "learning_rate": 1.0528610801186056e-05,
        "epoch": 0.9512631343617259,
        "step": 12765
    },
    {
        "loss": 2.5974,
        "grad_norm": 1.7936809062957764,
        "learning_rate": 1.0497204849391595e-05,
        "epoch": 0.9513376555630076,
        "step": 12766
    },
    {
        "loss": 2.2111,
        "grad_norm": 3.1619718074798584,
        "learning_rate": 1.0465843213361658e-05,
        "epoch": 0.9514121767642895,
        "step": 12767
    },
    {
        "loss": 2.2199,
        "grad_norm": 4.2652692794799805,
        "learning_rate": 1.0434525908624382e-05,
        "epoch": 0.9514866979655712,
        "step": 12768
    },
    {
        "loss": 2.4293,
        "grad_norm": 3.3253211975097656,
        "learning_rate": 1.0403252950686083e-05,
        "epoch": 0.951561219166853,
        "step": 12769
    },
    {
        "loss": 2.5882,
        "grad_norm": 2.395568609237671,
        "learning_rate": 1.0372024355031008e-05,
        "epoch": 0.9516357403681347,
        "step": 12770
    },
    {
        "loss": 2.5726,
        "grad_norm": 4.823665618896484,
        "learning_rate": 1.0340840137121399e-05,
        "epoch": 0.9517102615694165,
        "step": 12771
    },
    {
        "loss": 2.3833,
        "grad_norm": 3.2615647315979004,
        "learning_rate": 1.0309700312397563e-05,
        "epoch": 0.9517847827706982,
        "step": 12772
    },
    {
        "loss": 2.3253,
        "grad_norm": 1.8421281576156616,
        "learning_rate": 1.0278604896277965e-05,
        "epoch": 0.95185930397198,
        "step": 12773
    },
    {
        "loss": 1.9225,
        "grad_norm": 3.090988874435425,
        "learning_rate": 1.0247553904158824e-05,
        "epoch": 0.9519338251732617,
        "step": 12774
    },
    {
        "loss": 2.6722,
        "grad_norm": 3.3952226638793945,
        "learning_rate": 1.021654735141464e-05,
        "epoch": 0.9520083463745436,
        "step": 12775
    },
    {
        "loss": 2.3187,
        "grad_norm": 3.0328755378723145,
        "learning_rate": 1.0185585253397711e-05,
        "epoch": 0.9520828675758253,
        "step": 12776
    },
    {
        "loss": 2.9016,
        "grad_norm": 3.6730692386627197,
        "learning_rate": 1.015466762543832e-05,
        "epoch": 0.9521573887771071,
        "step": 12777
    },
    {
        "loss": 2.2007,
        "grad_norm": 2.2360382080078125,
        "learning_rate": 1.0123794482844995e-05,
        "epoch": 0.9522319099783888,
        "step": 12778
    },
    {
        "loss": 2.7573,
        "grad_norm": 3.1744987964630127,
        "learning_rate": 1.0092965840903767e-05,
        "epoch": 0.9523064311796706,
        "step": 12779
    },
    {
        "loss": 2.1813,
        "grad_norm": 3.496534824371338,
        "learning_rate": 1.0062181714879104e-05,
        "epoch": 0.9523809523809523,
        "step": 12780
    },
    {
        "loss": 1.1587,
        "grad_norm": 3.8754634857177734,
        "learning_rate": 1.003144212001318e-05,
        "epoch": 0.9524554735822341,
        "step": 12781
    },
    {
        "loss": 2.4073,
        "grad_norm": 3.3545403480529785,
        "learning_rate": 1.0000747071526106e-05,
        "epoch": 0.952529994783516,
        "step": 12782
    },
    {
        "loss": 2.4877,
        "grad_norm": 2.225834369659424,
        "learning_rate": 9.970096584616118e-06,
        "epoch": 0.9526045159847977,
        "step": 12783
    },
    {
        "loss": 2.0565,
        "grad_norm": 3.3319473266601562,
        "learning_rate": 9.939490674459262e-06,
        "epoch": 0.9526790371860795,
        "step": 12784
    },
    {
        "loss": 2.2461,
        "grad_norm": 2.9205851554870605,
        "learning_rate": 9.908929356209396e-06,
        "epoch": 0.9527535583873612,
        "step": 12785
    },
    {
        "loss": 2.7122,
        "grad_norm": 1.9729697704315186,
        "learning_rate": 9.87841264499859e-06,
        "epoch": 0.952828079588643,
        "step": 12786
    },
    {
        "loss": 2.5023,
        "grad_norm": 3.594127655029297,
        "learning_rate": 9.847940555936608e-06,
        "epoch": 0.9529026007899247,
        "step": 12787
    },
    {
        "loss": 1.6902,
        "grad_norm": 3.770099639892578,
        "learning_rate": 9.81751310411111e-06,
        "epoch": 0.9529771219912065,
        "step": 12788
    },
    {
        "loss": 2.4441,
        "grad_norm": 3.8023602962493896,
        "learning_rate": 9.7871303045879e-06,
        "epoch": 0.9530516431924883,
        "step": 12789
    },
    {
        "loss": 2.887,
        "grad_norm": 2.6728787422180176,
        "learning_rate": 9.756792172410256e-06,
        "epoch": 0.9531261643937701,
        "step": 12790
    },
    {
        "loss": 2.043,
        "grad_norm": 4.534750461578369,
        "learning_rate": 9.726498722599776e-06,
        "epoch": 0.9532006855950518,
        "step": 12791
    },
    {
        "loss": 2.157,
        "grad_norm": 2.483795404434204,
        "learning_rate": 9.696249970155613e-06,
        "epoch": 0.9532752067963336,
        "step": 12792
    },
    {
        "loss": 2.9184,
        "grad_norm": 4.287753105163574,
        "learning_rate": 9.666045930055046e-06,
        "epoch": 0.9533497279976153,
        "step": 12793
    },
    {
        "loss": 2.7841,
        "grad_norm": 3.1229658126831055,
        "learning_rate": 9.635886617252999e-06,
        "epoch": 0.9534242491988971,
        "step": 12794
    },
    {
        "loss": 2.5384,
        "grad_norm": 2.9295382499694824,
        "learning_rate": 9.605772046682272e-06,
        "epoch": 0.9534987704001788,
        "step": 12795
    },
    {
        "loss": 2.541,
        "grad_norm": 2.701491355895996,
        "learning_rate": 9.575702233253725e-06,
        "epoch": 0.9535732916014606,
        "step": 12796
    },
    {
        "loss": 1.9697,
        "grad_norm": 2.630736827850342,
        "learning_rate": 9.545677191855762e-06,
        "epoch": 0.9536478128027424,
        "step": 12797
    },
    {
        "loss": 2.6066,
        "grad_norm": 2.4377028942108154,
        "learning_rate": 9.515696937354945e-06,
        "epoch": 0.9537223340040242,
        "step": 12798
    },
    {
        "loss": 2.7295,
        "grad_norm": 4.419741630554199,
        "learning_rate": 9.485761484595269e-06,
        "epoch": 0.9537968552053059,
        "step": 12799
    },
    {
        "loss": 2.7318,
        "grad_norm": 4.0179290771484375,
        "learning_rate": 9.455870848398918e-06,
        "epoch": 0.9538713764065877,
        "step": 12800
    },
    {
        "loss": 2.0721,
        "grad_norm": 2.888965129852295,
        "learning_rate": 9.426025043565657e-06,
        "epoch": 0.9539458976078694,
        "step": 12801
    },
    {
        "loss": 1.2923,
        "grad_norm": 4.37228536605835,
        "learning_rate": 9.396224084873063e-06,
        "epoch": 0.9540204188091512,
        "step": 12802
    },
    {
        "loss": 2.9344,
        "grad_norm": 1.9174145460128784,
        "learning_rate": 9.366467987076688e-06,
        "epoch": 0.9540949400104329,
        "step": 12803
    },
    {
        "loss": 2.159,
        "grad_norm": 2.4243831634521484,
        "learning_rate": 9.336756764909693e-06,
        "epoch": 0.9541694612117148,
        "step": 12804
    },
    {
        "loss": 2.3063,
        "grad_norm": 3.9320335388183594,
        "learning_rate": 9.307090433083022e-06,
        "epoch": 0.9542439824129965,
        "step": 12805
    },
    {
        "loss": 2.3826,
        "grad_norm": 4.4911065101623535,
        "learning_rate": 9.277469006285554e-06,
        "epoch": 0.9543185036142783,
        "step": 12806
    },
    {
        "loss": 2.4234,
        "grad_norm": 2.577669858932495,
        "learning_rate": 9.247892499183797e-06,
        "epoch": 0.95439302481556,
        "step": 12807
    },
    {
        "loss": 2.1197,
        "grad_norm": 3.279597043991089,
        "learning_rate": 9.218360926421954e-06,
        "epoch": 0.9544675460168418,
        "step": 12808
    },
    {
        "loss": 2.3313,
        "grad_norm": 3.231691837310791,
        "learning_rate": 9.188874302622253e-06,
        "epoch": 0.9545420672181235,
        "step": 12809
    },
    {
        "loss": 2.5691,
        "grad_norm": 1.932288646697998,
        "learning_rate": 9.15943264238427e-06,
        "epoch": 0.9546165884194053,
        "step": 12810
    },
    {
        "loss": 2.4023,
        "grad_norm": 2.3104379177093506,
        "learning_rate": 9.130035960285709e-06,
        "epoch": 0.954691109620687,
        "step": 12811
    },
    {
        "loss": 1.8207,
        "grad_norm": 2.686091661453247,
        "learning_rate": 9.1006842708817e-06,
        "epoch": 0.9547656308219689,
        "step": 12812
    },
    {
        "loss": 2.0141,
        "grad_norm": 2.9229886531829834,
        "learning_rate": 9.071377588705354e-06,
        "epoch": 0.9548401520232506,
        "step": 12813
    },
    {
        "loss": 1.5106,
        "grad_norm": 3.8903231620788574,
        "learning_rate": 9.042115928267304e-06,
        "epoch": 0.9549146732245324,
        "step": 12814
    },
    {
        "loss": 2.0964,
        "grad_norm": 3.01839017868042,
        "learning_rate": 9.012899304055899e-06,
        "epoch": 0.9549891944258141,
        "step": 12815
    },
    {
        "loss": 2.0414,
        "grad_norm": 2.676703929901123,
        "learning_rate": 8.983727730537395e-06,
        "epoch": 0.9550637156270959,
        "step": 12816
    },
    {
        "loss": 2.025,
        "grad_norm": 3.9185314178466797,
        "learning_rate": 8.954601222155468e-06,
        "epoch": 0.9551382368283776,
        "step": 12817
    },
    {
        "loss": 2.2809,
        "grad_norm": 2.129931926727295,
        "learning_rate": 8.92551979333165e-06,
        "epoch": 0.9552127580296594,
        "step": 12818
    },
    {
        "loss": 1.6441,
        "grad_norm": 2.5439441204071045,
        "learning_rate": 8.896483458465043e-06,
        "epoch": 0.9552872792309413,
        "step": 12819
    },
    {
        "loss": 2.1022,
        "grad_norm": 3.2225921154022217,
        "learning_rate": 8.867492231932606e-06,
        "epoch": 0.955361800432223,
        "step": 12820
    },
    {
        "loss": 2.5056,
        "grad_norm": 7.4448394775390625,
        "learning_rate": 8.838546128088787e-06,
        "epoch": 0.9554363216335048,
        "step": 12821
    },
    {
        "loss": 2.3959,
        "grad_norm": 3.8667280673980713,
        "learning_rate": 8.809645161265757e-06,
        "epoch": 0.9555108428347865,
        "step": 12822
    },
    {
        "loss": 2.7714,
        "grad_norm": 2.288295269012451,
        "learning_rate": 8.780789345773244e-06,
        "epoch": 0.9555853640360683,
        "step": 12823
    },
    {
        "loss": 2.6396,
        "grad_norm": 2.954301357269287,
        "learning_rate": 8.751978695898844e-06,
        "epoch": 0.95565988523735,
        "step": 12824
    },
    {
        "loss": 1.8667,
        "grad_norm": 5.058019638061523,
        "learning_rate": 8.723213225907545e-06,
        "epoch": 0.9557344064386318,
        "step": 12825
    },
    {
        "loss": 2.6644,
        "grad_norm": 2.3386945724487305,
        "learning_rate": 8.694492950042165e-06,
        "epoch": 0.9558089276399135,
        "step": 12826
    },
    {
        "loss": 2.4761,
        "grad_norm": 1.7132294178009033,
        "learning_rate": 8.665817882523009e-06,
        "epoch": 0.9558834488411954,
        "step": 12827
    },
    {
        "loss": 2.6217,
        "grad_norm": 2.19557785987854,
        "learning_rate": 8.637188037547995e-06,
        "epoch": 0.9559579700424771,
        "step": 12828
    },
    {
        "loss": 2.4842,
        "grad_norm": 2.1694135665893555,
        "learning_rate": 8.608603429292861e-06,
        "epoch": 0.9560324912437589,
        "step": 12829
    },
    {
        "loss": 2.2777,
        "grad_norm": 2.7414443492889404,
        "learning_rate": 8.580064071910544e-06,
        "epoch": 0.9561070124450406,
        "step": 12830
    },
    {
        "loss": 2.576,
        "grad_norm": 2.3177478313446045,
        "learning_rate": 8.551569979531982e-06,
        "epoch": 0.9561815336463224,
        "step": 12831
    },
    {
        "loss": 2.514,
        "grad_norm": 1.8977679014205933,
        "learning_rate": 8.52312116626549e-06,
        "epoch": 0.9562560548476041,
        "step": 12832
    },
    {
        "loss": 2.2532,
        "grad_norm": 2.471731185913086,
        "learning_rate": 8.49471764619696e-06,
        "epoch": 0.9563305760488859,
        "step": 12833
    },
    {
        "loss": 1.7535,
        "grad_norm": 2.93350887298584,
        "learning_rate": 8.466359433389991e-06,
        "epoch": 0.9564050972501676,
        "step": 12834
    },
    {
        "loss": 2.6438,
        "grad_norm": 1.9628779888153076,
        "learning_rate": 8.43804654188557e-06,
        "epoch": 0.9564796184514495,
        "step": 12835
    },
    {
        "loss": 1.9078,
        "grad_norm": 3.4390034675598145,
        "learning_rate": 8.409778985702432e-06,
        "epoch": 0.9565541396527312,
        "step": 12836
    },
    {
        "loss": 2.4869,
        "grad_norm": 1.7459067106246948,
        "learning_rate": 8.38155677883673e-06,
        "epoch": 0.956628660854013,
        "step": 12837
    },
    {
        "loss": 2.4079,
        "grad_norm": 3.3171584606170654,
        "learning_rate": 8.353379935262218e-06,
        "epoch": 0.9567031820552947,
        "step": 12838
    },
    {
        "loss": 2.2752,
        "grad_norm": 2.92922306060791,
        "learning_rate": 8.32524846893008e-06,
        "epoch": 0.9567777032565765,
        "step": 12839
    },
    {
        "loss": 1.6845,
        "grad_norm": 3.6442549228668213,
        "learning_rate": 8.297162393769253e-06,
        "epoch": 0.9568522244578582,
        "step": 12840
    },
    {
        "loss": 2.4169,
        "grad_norm": 1.9919332265853882,
        "learning_rate": 8.269121723686013e-06,
        "epoch": 0.95692674565914,
        "step": 12841
    },
    {
        "loss": 2.1684,
        "grad_norm": 3.397352457046509,
        "learning_rate": 8.24112647256422e-06,
        "epoch": 0.9570012668604218,
        "step": 12842
    },
    {
        "loss": 2.8778,
        "grad_norm": 1.985986351966858,
        "learning_rate": 8.213176654265198e-06,
        "epoch": 0.9570757880617036,
        "step": 12843
    },
    {
        "loss": 2.6099,
        "grad_norm": 2.2488789558410645,
        "learning_rate": 8.185272282627899e-06,
        "epoch": 0.9571503092629853,
        "step": 12844
    },
    {
        "loss": 2.4408,
        "grad_norm": 2.3359215259552,
        "learning_rate": 8.157413371468669e-06,
        "epoch": 0.9572248304642671,
        "step": 12845
    },
    {
        "loss": 2.572,
        "grad_norm": 2.45326828956604,
        "learning_rate": 8.129599934581278e-06,
        "epoch": 0.9572993516655488,
        "step": 12846
    },
    {
        "loss": 2.3024,
        "grad_norm": 2.7243688106536865,
        "learning_rate": 8.101831985737197e-06,
        "epoch": 0.9573738728668306,
        "step": 12847
    },
    {
        "loss": 2.5139,
        "grad_norm": 1.6756664514541626,
        "learning_rate": 8.074109538685127e-06,
        "epoch": 0.9574483940681123,
        "step": 12848
    },
    {
        "loss": 2.4746,
        "grad_norm": 4.880563735961914,
        "learning_rate": 8.046432607151533e-06,
        "epoch": 0.9575229152693941,
        "step": 12849
    },
    {
        "loss": 2.3548,
        "grad_norm": 3.5087480545043945,
        "learning_rate": 8.018801204839954e-06,
        "epoch": 0.9575974364706759,
        "step": 12850
    },
    {
        "loss": 2.6971,
        "grad_norm": 3.0294859409332275,
        "learning_rate": 7.991215345431757e-06,
        "epoch": 0.9576719576719577,
        "step": 12851
    },
    {
        "loss": 2.0314,
        "grad_norm": 4.963319301605225,
        "learning_rate": 7.963675042585572e-06,
        "epoch": 0.9577464788732394,
        "step": 12852
    },
    {
        "loss": 2.5508,
        "grad_norm": 4.875725269317627,
        "learning_rate": 7.936180309937413e-06,
        "epoch": 0.9578210000745212,
        "step": 12853
    },
    {
        "loss": 2.2779,
        "grad_norm": 2.7320220470428467,
        "learning_rate": 7.908731161100946e-06,
        "epoch": 0.957895521275803,
        "step": 12854
    },
    {
        "loss": 2.4732,
        "grad_norm": 2.603048086166382,
        "learning_rate": 7.8813276096671e-06,
        "epoch": 0.9579700424770847,
        "step": 12855
    },
    {
        "loss": 2.3333,
        "grad_norm": 2.805131673812866,
        "learning_rate": 7.85396966920422e-06,
        "epoch": 0.9580445636783665,
        "step": 12856
    },
    {
        "loss": 2.2967,
        "grad_norm": 2.4851131439208984,
        "learning_rate": 7.826657353258204e-06,
        "epoch": 0.9581190848796483,
        "step": 12857
    },
    {
        "loss": 2.8385,
        "grad_norm": 1.6751223802566528,
        "learning_rate": 7.799390675352225e-06,
        "epoch": 0.9581936060809301,
        "step": 12858
    },
    {
        "loss": 3.1959,
        "grad_norm": 2.7132792472839355,
        "learning_rate": 7.772169648986871e-06,
        "epoch": 0.9582681272822118,
        "step": 12859
    },
    {
        "loss": 2.1958,
        "grad_norm": 3.1942970752716064,
        "learning_rate": 7.744994287640329e-06,
        "epoch": 0.9583426484834936,
        "step": 12860
    },
    {
        "loss": 1.8034,
        "grad_norm": 3.2112836837768555,
        "learning_rate": 7.717864604767766e-06,
        "epoch": 0.9584171696847753,
        "step": 12861
    },
    {
        "loss": 1.7746,
        "grad_norm": 4.108814239501953,
        "learning_rate": 7.69078061380215e-06,
        "epoch": 0.9584916908860571,
        "step": 12862
    },
    {
        "loss": 2.5086,
        "grad_norm": 3.263143301010132,
        "learning_rate": 7.663742328153556e-06,
        "epoch": 0.9585662120873388,
        "step": 12863
    },
    {
        "loss": 2.4927,
        "grad_norm": 2.905508518218994,
        "learning_rate": 7.636749761209616e-06,
        "epoch": 0.9586407332886207,
        "step": 12864
    },
    {
        "loss": 1.2426,
        "grad_norm": 2.2386820316314697,
        "learning_rate": 7.6098029263351855e-06,
        "epoch": 0.9587152544899024,
        "step": 12865
    },
    {
        "loss": 2.2736,
        "grad_norm": 2.1349802017211914,
        "learning_rate": 7.5829018368724805e-06,
        "epoch": 0.9587897756911842,
        "step": 12866
    },
    {
        "loss": 2.0288,
        "grad_norm": 4.620401859283447,
        "learning_rate": 7.556046506141201e-06,
        "epoch": 0.9588642968924659,
        "step": 12867
    },
    {
        "loss": 2.2915,
        "grad_norm": 2.8570706844329834,
        "learning_rate": 7.529236947438212e-06,
        "epoch": 0.9589388180937477,
        "step": 12868
    },
    {
        "loss": 2.821,
        "grad_norm": 2.342607259750366,
        "learning_rate": 7.502473174037961e-06,
        "epoch": 0.9590133392950294,
        "step": 12869
    },
    {
        "loss": 1.6297,
        "grad_norm": 2.990748167037964,
        "learning_rate": 7.475755199191836e-06,
        "epoch": 0.9590878604963112,
        "step": 12870
    },
    {
        "loss": 2.3084,
        "grad_norm": 3.6768743991851807,
        "learning_rate": 7.449083036128957e-06,
        "epoch": 0.9591623816975929,
        "step": 12871
    },
    {
        "loss": 2.5408,
        "grad_norm": 2.8778975009918213,
        "learning_rate": 7.422456698055536e-06,
        "epoch": 0.9592369028988748,
        "step": 12872
    },
    {
        "loss": 2.4876,
        "grad_norm": 4.237392425537109,
        "learning_rate": 7.3958761981550825e-06,
        "epoch": 0.9593114241001565,
        "step": 12873
    },
    {
        "loss": 2.6877,
        "grad_norm": 2.567755699157715,
        "learning_rate": 7.369341549588571e-06,
        "epoch": 0.9593859453014383,
        "step": 12874
    },
    {
        "loss": 1.6174,
        "grad_norm": 2.7727036476135254,
        "learning_rate": 7.3428527654941574e-06,
        "epoch": 0.95946046650272,
        "step": 12875
    },
    {
        "loss": 2.1042,
        "grad_norm": 2.192681312561035,
        "learning_rate": 7.316409858987206e-06,
        "epoch": 0.9595349877040018,
        "step": 12876
    },
    {
        "loss": 2.6625,
        "grad_norm": 2.326472759246826,
        "learning_rate": 7.290012843160599e-06,
        "epoch": 0.9596095089052835,
        "step": 12877
    },
    {
        "loss": 2.5927,
        "grad_norm": 2.590939521789551,
        "learning_rate": 7.263661731084303e-06,
        "epoch": 0.9596840301065653,
        "step": 12878
    },
    {
        "loss": 2.526,
        "grad_norm": 1.8180612325668335,
        "learning_rate": 7.237356535805562e-06,
        "epoch": 0.959758551307847,
        "step": 12879
    },
    {
        "loss": 2.6646,
        "grad_norm": 2.0312037467956543,
        "learning_rate": 7.211097270349099e-06,
        "epoch": 0.9598330725091289,
        "step": 12880
    },
    {
        "loss": 1.8602,
        "grad_norm": 3.1948580741882324,
        "learning_rate": 7.1848839477165364e-06,
        "epoch": 0.9599075937104106,
        "step": 12881
    },
    {
        "loss": 1.9078,
        "grad_norm": 2.6442079544067383,
        "learning_rate": 7.158716580887093e-06,
        "epoch": 0.9599821149116924,
        "step": 12882
    },
    {
        "loss": 2.1587,
        "grad_norm": 1.5122052431106567,
        "learning_rate": 7.132595182817059e-06,
        "epoch": 0.9600566361129741,
        "step": 12883
    },
    {
        "loss": 2.0643,
        "grad_norm": 3.6061317920684814,
        "learning_rate": 7.106519766439912e-06,
        "epoch": 0.9601311573142559,
        "step": 12884
    },
    {
        "loss": 2.3883,
        "grad_norm": 3.7990753650665283,
        "learning_rate": 7.080490344666546e-06,
        "epoch": 0.9602056785155376,
        "step": 12885
    },
    {
        "loss": 2.1569,
        "grad_norm": 3.226208448410034,
        "learning_rate": 7.054506930384896e-06,
        "epoch": 0.9602801997168194,
        "step": 12886
    },
    {
        "loss": 2.2753,
        "grad_norm": 2.4432082176208496,
        "learning_rate": 7.028569536460305e-06,
        "epoch": 0.9603547209181011,
        "step": 12887
    },
    {
        "loss": 1.8438,
        "grad_norm": 3.2101833820343018,
        "learning_rate": 7.002678175735167e-06,
        "epoch": 0.960429242119383,
        "step": 12888
    },
    {
        "loss": 2.5906,
        "grad_norm": 2.4450581073760986,
        "learning_rate": 6.976832861029137e-06,
        "epoch": 0.9605037633206648,
        "step": 12889
    },
    {
        "loss": 2.3496,
        "grad_norm": 3.2827978134155273,
        "learning_rate": 6.951033605139045e-06,
        "epoch": 0.9605782845219465,
        "step": 12890
    },
    {
        "loss": 2.6646,
        "grad_norm": 2.711353063583374,
        "learning_rate": 6.925280420839042e-06,
        "epoch": 0.9606528057232283,
        "step": 12891
    },
    {
        "loss": 1.6192,
        "grad_norm": 4.535824775695801,
        "learning_rate": 6.899573320880303e-06,
        "epoch": 0.96072732692451,
        "step": 12892
    },
    {
        "loss": 2.4952,
        "grad_norm": 3.341484308242798,
        "learning_rate": 6.87391231799126e-06,
        "epoch": 0.9608018481257918,
        "step": 12893
    },
    {
        "loss": 2.7026,
        "grad_norm": 2.322482109069824,
        "learning_rate": 6.848297424877492e-06,
        "epoch": 0.9608763693270735,
        "step": 12894
    },
    {
        "loss": 2.2004,
        "grad_norm": 3.127680778503418,
        "learning_rate": 6.822728654221844e-06,
        "epoch": 0.9609508905283554,
        "step": 12895
    },
    {
        "loss": 2.5136,
        "grad_norm": 3.778447389602661,
        "learning_rate": 6.797206018684166e-06,
        "epoch": 0.9610254117296371,
        "step": 12896
    },
    {
        "loss": 2.4765,
        "grad_norm": 3.9066247940063477,
        "learning_rate": 6.771729530901638e-06,
        "epoch": 0.9610999329309189,
        "step": 12897
    },
    {
        "loss": 2.3509,
        "grad_norm": 2.677839756011963,
        "learning_rate": 6.746299203488482e-06,
        "epoch": 0.9611744541322006,
        "step": 12898
    },
    {
        "loss": 2.2258,
        "grad_norm": 2.4575510025024414,
        "learning_rate": 6.7209150490359914e-06,
        "epoch": 0.9612489753334824,
        "step": 12899
    },
    {
        "loss": 2.6343,
        "grad_norm": 3.3313076496124268,
        "learning_rate": 6.6955770801128694e-06,
        "epoch": 0.9613234965347641,
        "step": 12900
    },
    {
        "loss": 2.2652,
        "grad_norm": 3.2765438556671143,
        "learning_rate": 6.670285309264579e-06,
        "epoch": 0.9613980177360459,
        "step": 12901
    },
    {
        "loss": 2.3075,
        "grad_norm": 2.284245729446411,
        "learning_rate": 6.645039749014037e-06,
        "epoch": 0.9614725389373276,
        "step": 12902
    },
    {
        "loss": 3.0293,
        "grad_norm": 3.8789730072021484,
        "learning_rate": 6.619840411861111e-06,
        "epoch": 0.9615470601386095,
        "step": 12903
    },
    {
        "loss": 2.0896,
        "grad_norm": 2.0267350673675537,
        "learning_rate": 6.5946873102827545e-06,
        "epoch": 0.9616215813398912,
        "step": 12904
    },
    {
        "loss": 2.0495,
        "grad_norm": 2.6879489421844482,
        "learning_rate": 6.5695804567332044e-06,
        "epoch": 0.961696102541173,
        "step": 12905
    },
    {
        "loss": 1.4904,
        "grad_norm": 2.6070568561553955,
        "learning_rate": 6.544519863643606e-06,
        "epoch": 0.9617706237424547,
        "step": 12906
    },
    {
        "loss": 2.3618,
        "grad_norm": 2.6801583766937256,
        "learning_rate": 6.519505543422322e-06,
        "epoch": 0.9618451449437365,
        "step": 12907
    },
    {
        "loss": 1.967,
        "grad_norm": 3.83183217048645,
        "learning_rate": 6.494537508454756e-06,
        "epoch": 0.9619196661450182,
        "step": 12908
    },
    {
        "loss": 2.2997,
        "grad_norm": 3.3031554222106934,
        "learning_rate": 6.4696157711033766e-06,
        "epoch": 0.9619941873463,
        "step": 12909
    },
    {
        "loss": 1.3242,
        "grad_norm": 3.810046911239624,
        "learning_rate": 6.444740343707722e-06,
        "epoch": 0.9620687085475818,
        "step": 12910
    },
    {
        "loss": 0.7987,
        "grad_norm": 3.1881837844848633,
        "learning_rate": 6.419911238584564e-06,
        "epoch": 0.9621432297488636,
        "step": 12911
    },
    {
        "loss": 2.7472,
        "grad_norm": 3.746424913406372,
        "learning_rate": 6.395128468027423e-06,
        "epoch": 0.9622177509501453,
        "step": 12912
    },
    {
        "loss": 2.7093,
        "grad_norm": 3.523696184158325,
        "learning_rate": 6.370392044307216e-06,
        "epoch": 0.9622922721514271,
        "step": 12913
    },
    {
        "loss": 2.8819,
        "grad_norm": 2.894786834716797,
        "learning_rate": 6.345701979671626e-06,
        "epoch": 0.9623667933527088,
        "step": 12914
    },
    {
        "loss": 1.8994,
        "grad_norm": 3.601773500442505,
        "learning_rate": 6.321058286345616e-06,
        "epoch": 0.9624413145539906,
        "step": 12915
    },
    {
        "loss": 2.309,
        "grad_norm": 2.696418523788452,
        "learning_rate": 6.29646097653106e-06,
        "epoch": 0.9625158357552723,
        "step": 12916
    },
    {
        "loss": 2.2631,
        "grad_norm": 2.97947096824646,
        "learning_rate": 6.271910062406816e-06,
        "epoch": 0.9625903569565541,
        "step": 12917
    },
    {
        "loss": 2.7819,
        "grad_norm": 2.5072505474090576,
        "learning_rate": 6.247405556128971e-06,
        "epoch": 0.9626648781578359,
        "step": 12918
    },
    {
        "loss": 2.4108,
        "grad_norm": 2.7825865745544434,
        "learning_rate": 6.2229474698303865e-06,
        "epoch": 0.9627393993591177,
        "step": 12919
    },
    {
        "loss": 2.3479,
        "grad_norm": 2.0862200260162354,
        "learning_rate": 6.198535815621232e-06,
        "epoch": 0.9628139205603994,
        "step": 12920
    },
    {
        "loss": 2.5658,
        "grad_norm": 2.3003127574920654,
        "learning_rate": 6.1741706055883034e-06,
        "epoch": 0.9628884417616812,
        "step": 12921
    },
    {
        "loss": 1.9545,
        "grad_norm": 3.3712873458862305,
        "learning_rate": 6.149851851795751e-06,
        "epoch": 0.9629629629629629,
        "step": 12922
    },
    {
        "loss": 1.6697,
        "grad_norm": 3.2409777641296387,
        "learning_rate": 6.1255795662845675e-06,
        "epoch": 0.9630374841642447,
        "step": 12923
    },
    {
        "loss": 2.1472,
        "grad_norm": 2.968290090560913,
        "learning_rate": 6.101353761072681e-06,
        "epoch": 0.9631120053655265,
        "step": 12924
    },
    {
        "loss": 1.9942,
        "grad_norm": 2.2618424892425537,
        "learning_rate": 6.077174448155187e-06,
        "epoch": 0.9631865265668083,
        "step": 12925
    },
    {
        "loss": 2.5832,
        "grad_norm": 3.4422447681427,
        "learning_rate": 6.0530416395040094e-06,
        "epoch": 0.9632610477680901,
        "step": 12926
    },
    {
        "loss": 2.4942,
        "grad_norm": 1.9113301038742065,
        "learning_rate": 6.028955347068044e-06,
        "epoch": 0.9633355689693718,
        "step": 12927
    },
    {
        "loss": 2.5282,
        "grad_norm": 2.9640800952911377,
        "learning_rate": 6.004915582773296e-06,
        "epoch": 0.9634100901706536,
        "step": 12928
    },
    {
        "loss": 1.8535,
        "grad_norm": 4.602691650390625,
        "learning_rate": 5.9809223585226115e-06,
        "epoch": 0.9634846113719353,
        "step": 12929
    },
    {
        "loss": 2.8114,
        "grad_norm": 3.0855460166931152,
        "learning_rate": 5.9569756861957425e-06,
        "epoch": 0.9635591325732171,
        "step": 12930
    },
    {
        "loss": 2.6963,
        "grad_norm": 3.1961638927459717,
        "learning_rate": 5.9330755776496165e-06,
        "epoch": 0.9636336537744988,
        "step": 12931
    },
    {
        "loss": 2.6752,
        "grad_norm": 3.7534544467926025,
        "learning_rate": 5.9092220447177885e-06,
        "epoch": 0.9637081749757807,
        "step": 12932
    },
    {
        "loss": 2.4072,
        "grad_norm": 3.277662992477417,
        "learning_rate": 5.8854150992110555e-06,
        "epoch": 0.9637826961770624,
        "step": 12933
    },
    {
        "loss": 2.345,
        "grad_norm": 2.7378854751586914,
        "learning_rate": 5.861654752916923e-06,
        "epoch": 0.9638572173783442,
        "step": 12934
    },
    {
        "loss": 2.724,
        "grad_norm": 2.3524715900421143,
        "learning_rate": 5.837941017600001e-06,
        "epoch": 0.9639317385796259,
        "step": 12935
    },
    {
        "loss": 2.6189,
        "grad_norm": 2.409990072250366,
        "learning_rate": 5.814273905001721e-06,
        "epoch": 0.9640062597809077,
        "step": 12936
    },
    {
        "loss": 1.6258,
        "grad_norm": 3.9060444831848145,
        "learning_rate": 5.790653426840365e-06,
        "epoch": 0.9640807809821894,
        "step": 12937
    },
    {
        "loss": 2.6438,
        "grad_norm": 3.2441141605377197,
        "learning_rate": 5.767079594811309e-06,
        "epoch": 0.9641553021834712,
        "step": 12938
    },
    {
        "loss": 2.8405,
        "grad_norm": 2.1106815338134766,
        "learning_rate": 5.743552420586684e-06,
        "epoch": 0.9642298233847529,
        "step": 12939
    },
    {
        "loss": 1.8572,
        "grad_norm": 1.8007261753082275,
        "learning_rate": 5.72007191581555e-06,
        "epoch": 0.9643043445860348,
        "step": 12940
    },
    {
        "loss": 2.9624,
        "grad_norm": 2.8279941082000732,
        "learning_rate": 5.696638092123863e-06,
        "epoch": 0.9643788657873165,
        "step": 12941
    },
    {
        "loss": 2.4372,
        "grad_norm": 2.0524139404296875,
        "learning_rate": 5.673250961114529e-06,
        "epoch": 0.9644533869885983,
        "step": 12942
    },
    {
        "loss": 2.4653,
        "grad_norm": 1.8535622358322144,
        "learning_rate": 5.649910534367275e-06,
        "epoch": 0.96452790818988,
        "step": 12943
    },
    {
        "loss": 2.044,
        "grad_norm": 2.696631669998169,
        "learning_rate": 5.626616823438679e-06,
        "epoch": 0.9646024293911618,
        "step": 12944
    },
    {
        "loss": 2.7065,
        "grad_norm": 2.1846306324005127,
        "learning_rate": 5.603369839862183e-06,
        "epoch": 0.9646769505924435,
        "step": 12945
    },
    {
        "loss": 2.3701,
        "grad_norm": 3.2248852252960205,
        "learning_rate": 5.580169595148222e-06,
        "epoch": 0.9647514717937253,
        "step": 12946
    },
    {
        "loss": 2.0858,
        "grad_norm": 3.1757419109344482,
        "learning_rate": 5.557016100783907e-06,
        "epoch": 0.964825992995007,
        "step": 12947
    },
    {
        "loss": 1.9943,
        "grad_norm": 2.4199633598327637,
        "learning_rate": 5.533909368233381e-06,
        "epoch": 0.9649005141962889,
        "step": 12948
    },
    {
        "loss": 2.0633,
        "grad_norm": 3.209527015686035,
        "learning_rate": 5.5108494089375015e-06,
        "epoch": 0.9649750353975706,
        "step": 12949
    },
    {
        "loss": 2.53,
        "grad_norm": 2.301478385925293,
        "learning_rate": 5.487836234313959e-06,
        "epoch": 0.9650495565988524,
        "step": 12950
    },
    {
        "loss": 2.4286,
        "grad_norm": 2.826022148132324,
        "learning_rate": 5.464869855757493e-06,
        "epoch": 0.9651240778001341,
        "step": 12951
    },
    {
        "loss": 2.6518,
        "grad_norm": 2.334061861038208,
        "learning_rate": 5.441950284639275e-06,
        "epoch": 0.9651985990014159,
        "step": 12952
    },
    {
        "loss": 1.6087,
        "grad_norm": 2.5772242546081543,
        "learning_rate": 5.419077532307715e-06,
        "epoch": 0.9652731202026976,
        "step": 12953
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.4376764297485352,
        "learning_rate": 5.396251610087788e-06,
        "epoch": 0.9653476414039794,
        "step": 12954
    },
    {
        "loss": 2.243,
        "grad_norm": 2.980962038040161,
        "learning_rate": 5.373472529281332e-06,
        "epoch": 0.9654221626052611,
        "step": 12955
    },
    {
        "loss": 2.2747,
        "grad_norm": 3.2522621154785156,
        "learning_rate": 5.35074030116709e-06,
        "epoch": 0.965496683806543,
        "step": 12956
    },
    {
        "loss": 2.5835,
        "grad_norm": 2.935657262802124,
        "learning_rate": 5.328054937000459e-06,
        "epoch": 0.9655712050078247,
        "step": 12957
    },
    {
        "loss": 2.437,
        "grad_norm": 1.5847598314285278,
        "learning_rate": 5.305416448013789e-06,
        "epoch": 0.9656457262091065,
        "step": 12958
    },
    {
        "loss": 2.7631,
        "grad_norm": 2.0855605602264404,
        "learning_rate": 5.28282484541609e-06,
        "epoch": 0.9657202474103883,
        "step": 12959
    },
    {
        "loss": 2.4629,
        "grad_norm": 3.7178900241851807,
        "learning_rate": 5.2602801403932035e-06,
        "epoch": 0.96579476861167,
        "step": 12960
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.8956862688064575,
        "learning_rate": 5.2377823441077e-06,
        "epoch": 0.9658692898129518,
        "step": 12961
    },
    {
        "loss": 2.1987,
        "grad_norm": 3.030892848968506,
        "learning_rate": 5.215331467699081e-06,
        "epoch": 0.9659438110142335,
        "step": 12962
    },
    {
        "loss": 2.089,
        "grad_norm": 2.638127565383911,
        "learning_rate": 5.192927522283441e-06,
        "epoch": 0.9660183322155154,
        "step": 12963
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.789886474609375,
        "learning_rate": 5.170570518953732e-06,
        "epoch": 0.9660928534167971,
        "step": 12964
    },
    {
        "loss": 2.3046,
        "grad_norm": 3.0919461250305176,
        "learning_rate": 5.148260468779587e-06,
        "epoch": 0.9661673746180789,
        "step": 12965
    },
    {
        "loss": 2.5946,
        "grad_norm": 2.638684034347534,
        "learning_rate": 5.125997382807535e-06,
        "epoch": 0.9662418958193606,
        "step": 12966
    },
    {
        "loss": 2.5144,
        "grad_norm": 2.518240451812744,
        "learning_rate": 5.103781272060659e-06,
        "epoch": 0.9663164170206424,
        "step": 12967
    },
    {
        "loss": 2.5572,
        "grad_norm": 2.6103386878967285,
        "learning_rate": 5.081612147538961e-06,
        "epoch": 0.9663909382219241,
        "step": 12968
    },
    {
        "loss": 2.2533,
        "grad_norm": 2.3168327808380127,
        "learning_rate": 5.059490020219082e-06,
        "epoch": 0.9664654594232059,
        "step": 12969
    },
    {
        "loss": 2.5428,
        "grad_norm": 2.2425129413604736,
        "learning_rate": 5.03741490105436e-06,
        "epoch": 0.9665399806244876,
        "step": 12970
    },
    {
        "loss": 2.4738,
        "grad_norm": 2.4931745529174805,
        "learning_rate": 5.0153868009750305e-06,
        "epoch": 0.9666145018257695,
        "step": 12971
    },
    {
        "loss": 2.6229,
        "grad_norm": 1.8949073553085327,
        "learning_rate": 4.993405730887768e-06,
        "epoch": 0.9666890230270512,
        "step": 12972
    },
    {
        "loss": 2.7515,
        "grad_norm": 2.0071089267730713,
        "learning_rate": 4.971471701676233e-06,
        "epoch": 0.966763544228333,
        "step": 12973
    },
    {
        "loss": 2.3154,
        "grad_norm": 3.1184635162353516,
        "learning_rate": 4.949584724200662e-06,
        "epoch": 0.9668380654296147,
        "step": 12974
    },
    {
        "loss": 2.0964,
        "grad_norm": 4.135318756103516,
        "learning_rate": 4.927744809297952e-06,
        "epoch": 0.9669125866308965,
        "step": 12975
    },
    {
        "loss": 2.4883,
        "grad_norm": 2.1485424041748047,
        "learning_rate": 4.905951967781841e-06,
        "epoch": 0.9669871078321782,
        "step": 12976
    },
    {
        "loss": 1.9158,
        "grad_norm": 3.870434522628784,
        "learning_rate": 4.884206210442654e-06,
        "epoch": 0.96706162903346,
        "step": 12977
    },
    {
        "loss": 2.3716,
        "grad_norm": 3.205065965652466,
        "learning_rate": 4.862507548047368e-06,
        "epoch": 0.9671361502347418,
        "step": 12978
    },
    {
        "loss": 2.4288,
        "grad_norm": 2.724778890609741,
        "learning_rate": 4.840855991339799e-06,
        "epoch": 0.9672106714360236,
        "step": 12979
    },
    {
        "loss": 2.0423,
        "grad_norm": 3.625256299972534,
        "learning_rate": 4.819251551040293e-06,
        "epoch": 0.9672851926373053,
        "step": 12980
    },
    {
        "loss": 2.3624,
        "grad_norm": 2.7118682861328125,
        "learning_rate": 4.797694237845873e-06,
        "epoch": 0.9673597138385871,
        "step": 12981
    },
    {
        "loss": 2.656,
        "grad_norm": 2.0694730281829834,
        "learning_rate": 4.7761840624304114e-06,
        "epoch": 0.9674342350398688,
        "step": 12982
    },
    {
        "loss": 2.6729,
        "grad_norm": 2.745408296585083,
        "learning_rate": 4.754721035444109e-06,
        "epoch": 0.9675087562411506,
        "step": 12983
    },
    {
        "loss": 2.2146,
        "grad_norm": 3.1416361331939697,
        "learning_rate": 4.733305167514157e-06,
        "epoch": 0.9675832774424323,
        "step": 12984
    },
    {
        "loss": 2.1408,
        "grad_norm": 3.169968843460083,
        "learning_rate": 4.711936469244149e-06,
        "epoch": 0.9676577986437142,
        "step": 12985
    },
    {
        "loss": 2.2121,
        "grad_norm": 2.9390780925750732,
        "learning_rate": 4.690614951214533e-06,
        "epoch": 0.9677323198449959,
        "step": 12986
    },
    {
        "loss": 1.2772,
        "grad_norm": 1.516659140586853,
        "learning_rate": 4.669340623982243e-06,
        "epoch": 0.9678068410462777,
        "step": 12987
    },
    {
        "loss": 2.4227,
        "grad_norm": 2.5783050060272217,
        "learning_rate": 4.648113498080842e-06,
        "epoch": 0.9678813622475594,
        "step": 12988
    },
    {
        "loss": 2.3916,
        "grad_norm": 2.2932844161987305,
        "learning_rate": 4.6269335840206875e-06,
        "epoch": 0.9679558834488412,
        "step": 12989
    },
    {
        "loss": 2.2506,
        "grad_norm": 3.3715550899505615,
        "learning_rate": 4.605800892288525e-06,
        "epoch": 0.9680304046501229,
        "step": 12990
    },
    {
        "loss": 2.3081,
        "grad_norm": Infinity,
        "learning_rate": 4.605800892288525e-06,
        "epoch": 0.9681049258514047,
        "step": 12991
    },
    {
        "loss": 1.9199,
        "grad_norm": 2.8975305557250977,
        "learning_rate": 4.584715433347986e-06,
        "epoch": 0.9681794470526864,
        "step": 12992
    },
    {
        "loss": 2.5139,
        "grad_norm": 3.9006917476654053,
        "learning_rate": 4.563677217639007e-06,
        "epoch": 0.9682539682539683,
        "step": 12993
    },
    {
        "loss": 1.4051,
        "grad_norm": 3.512695789337158,
        "learning_rate": 4.542686255578421e-06,
        "epoch": 0.96832848945525,
        "step": 12994
    },
    {
        "loss": 2.7591,
        "grad_norm": 3.10030198097229,
        "learning_rate": 4.5217425575595054e-06,
        "epoch": 0.9684030106565318,
        "step": 12995
    },
    {
        "loss": 2.5827,
        "grad_norm": 2.9342291355133057,
        "learning_rate": 4.500846133952108e-06,
        "epoch": 0.9684775318578136,
        "step": 12996
    },
    {
        "loss": 2.5528,
        "grad_norm": 3.2370898723602295,
        "learning_rate": 4.479996995102853e-06,
        "epoch": 0.9685520530590953,
        "step": 12997
    },
    {
        "loss": 2.081,
        "grad_norm": 2.962541341781616,
        "learning_rate": 4.4591951513347604e-06,
        "epoch": 0.9686265742603771,
        "step": 12998
    },
    {
        "loss": 2.619,
        "grad_norm": 2.8339858055114746,
        "learning_rate": 4.438440612947459e-06,
        "epoch": 0.9687010954616588,
        "step": 12999
    },
    {
        "loss": 1.8434,
        "grad_norm": 3.3039236068725586,
        "learning_rate": 4.417733390217305e-06,
        "epoch": 0.9687756166629407,
        "step": 13000
    },
    {
        "loss": 2.2906,
        "grad_norm": 2.7718913555145264,
        "learning_rate": 4.397073493397053e-06,
        "epoch": 0.9688501378642224,
        "step": 13001
    },
    {
        "loss": 1.601,
        "grad_norm": 3.0062942504882812,
        "learning_rate": 4.376460932716086e-06,
        "epoch": 0.9689246590655042,
        "step": 13002
    },
    {
        "loss": 2.3814,
        "grad_norm": 3.3597121238708496,
        "learning_rate": 4.355895718380454e-06,
        "epoch": 0.9689991802667859,
        "step": 13003
    },
    {
        "loss": 2.1415,
        "grad_norm": 3.626253366470337,
        "learning_rate": 4.33537786057252e-06,
        "epoch": 0.9690737014680677,
        "step": 13004
    },
    {
        "loss": 2.7475,
        "grad_norm": 1.9893444776535034,
        "learning_rate": 4.314907369451482e-06,
        "epoch": 0.9691482226693494,
        "step": 13005
    },
    {
        "loss": 2.2566,
        "grad_norm": 3.0708200931549072,
        "learning_rate": 4.294484255152875e-06,
        "epoch": 0.9692227438706312,
        "step": 13006
    },
    {
        "loss": 2.59,
        "grad_norm": 2.3843960762023926,
        "learning_rate": 4.274108527788834e-06,
        "epoch": 0.9692972650719129,
        "step": 13007
    },
    {
        "loss": 2.5363,
        "grad_norm": 2.2928757667541504,
        "learning_rate": 4.253780197448121e-06,
        "epoch": 0.9693717862731948,
        "step": 13008
    },
    {
        "loss": 2.4891,
        "grad_norm": 3.798696517944336,
        "learning_rate": 4.233499274195874e-06,
        "epoch": 0.9694463074744765,
        "step": 13009
    },
    {
        "loss": 1.7157,
        "grad_norm": 3.5934908390045166,
        "learning_rate": 4.213265768073937e-06,
        "epoch": 0.9695208286757583,
        "step": 13010
    },
    {
        "loss": 2.3464,
        "grad_norm": 3.3539977073669434,
        "learning_rate": 4.193079689100532e-06,
        "epoch": 0.96959534987704,
        "step": 13011
    },
    {
        "loss": 2.3659,
        "grad_norm": 2.876028537750244,
        "learning_rate": 4.1729410472704625e-06,
        "epoch": 0.9696698710783218,
        "step": 13012
    },
    {
        "loss": 2.4444,
        "grad_norm": 1.8441216945648193,
        "learning_rate": 4.152849852554963e-06,
        "epoch": 0.9697443922796035,
        "step": 13013
    },
    {
        "loss": 2.4652,
        "grad_norm": 3.292330503463745,
        "learning_rate": 4.132806114901944e-06,
        "epoch": 0.9698189134808853,
        "step": 13014
    },
    {
        "loss": 1.9608,
        "grad_norm": 4.08621883392334,
        "learning_rate": 4.112809844235654e-06,
        "epoch": 0.969893434682167,
        "step": 13015
    },
    {
        "loss": 1.9737,
        "grad_norm": 2.90791654586792,
        "learning_rate": 4.092861050456919e-06,
        "epoch": 0.9699679558834489,
        "step": 13016
    },
    {
        "loss": 2.4882,
        "grad_norm": 3.2853620052337646,
        "learning_rate": 4.072959743443006e-06,
        "epoch": 0.9700424770847306,
        "step": 13017
    },
    {
        "loss": 1.5564,
        "grad_norm": 3.864858388900757,
        "learning_rate": 4.053105933047763e-06,
        "epoch": 0.9701169982860124,
        "step": 13018
    },
    {
        "loss": 2.2378,
        "grad_norm": 2.151397228240967,
        "learning_rate": 4.033299629101394e-06,
        "epoch": 0.9701915194872941,
        "step": 13019
    },
    {
        "loss": 2.5833,
        "grad_norm": 2.846522808074951,
        "learning_rate": 4.013540841410735e-06,
        "epoch": 0.9702660406885759,
        "step": 13020
    },
    {
        "loss": 2.2133,
        "grad_norm": 1.9508740901947021,
        "learning_rate": 3.993829579758956e-06,
        "epoch": 0.9703405618898576,
        "step": 13021
    },
    {
        "loss": 1.9164,
        "grad_norm": 3.401414632797241,
        "learning_rate": 3.974165853905709e-06,
        "epoch": 0.9704150830911394,
        "step": 13022
    },
    {
        "loss": 2.6171,
        "grad_norm": 2.513153553009033,
        "learning_rate": 3.954549673587271e-06,
        "epoch": 0.9704896042924211,
        "step": 13023
    },
    {
        "loss": 1.9927,
        "grad_norm": 2.5214383602142334,
        "learning_rate": 3.934981048516107e-06,
        "epoch": 0.970564125493703,
        "step": 13024
    },
    {
        "loss": 2.1097,
        "grad_norm": 3.2611992359161377,
        "learning_rate": 3.915459988381376e-06,
        "epoch": 0.9706386466949847,
        "step": 13025
    },
    {
        "loss": 2.7016,
        "grad_norm": 3.2221014499664307,
        "learning_rate": 3.895986502848581e-06,
        "epoch": 0.9707131678962665,
        "step": 13026
    },
    {
        "loss": 2.8477,
        "grad_norm": 3.17677903175354,
        "learning_rate": 3.876560601559642e-06,
        "epoch": 0.9707876890975482,
        "step": 13027
    },
    {
        "loss": 1.7975,
        "grad_norm": 3.9575934410095215,
        "learning_rate": 3.857182294133022e-06,
        "epoch": 0.97086221029883,
        "step": 13028
    },
    {
        "loss": 2.0856,
        "grad_norm": 3.619804859161377,
        "learning_rate": 3.8378515901634884e-06,
        "epoch": 0.9709367315001117,
        "step": 13029
    },
    {
        "loss": 2.6295,
        "grad_norm": 2.64963698387146,
        "learning_rate": 3.8185684992223745e-06,
        "epoch": 0.9710112527013935,
        "step": 13030
    },
    {
        "loss": 1.8402,
        "grad_norm": 3.3685247898101807,
        "learning_rate": 3.799333030857333e-06,
        "epoch": 0.9710857739026754,
        "step": 13031
    },
    {
        "loss": 1.9082,
        "grad_norm": 2.8357603549957275,
        "learning_rate": 3.780145194592477e-06,
        "epoch": 0.9711602951039571,
        "step": 13032
    },
    {
        "loss": 2.267,
        "grad_norm": 2.4700963497161865,
        "learning_rate": 3.761004999928297e-06,
        "epoch": 0.9712348163052389,
        "step": 13033
    },
    {
        "loss": 2.1934,
        "grad_norm": 2.3821184635162354,
        "learning_rate": 3.741912456341845e-06,
        "epoch": 0.9713093375065206,
        "step": 13034
    },
    {
        "loss": 2.17,
        "grad_norm": 3.4189562797546387,
        "learning_rate": 3.722867573286315e-06,
        "epoch": 0.9713838587078024,
        "step": 13035
    },
    {
        "loss": 2.6046,
        "grad_norm": 3.0134050846099854,
        "learning_rate": 3.7038703601915415e-06,
        "epoch": 0.9714583799090841,
        "step": 13036
    },
    {
        "loss": 2.4656,
        "grad_norm": 2.8355910778045654,
        "learning_rate": 3.6849208264636227e-06,
        "epoch": 0.9715329011103659,
        "step": 13037
    },
    {
        "loss": 2.108,
        "grad_norm": 3.7076432704925537,
        "learning_rate": 3.6660189814851532e-06,
        "epoch": 0.9716074223116477,
        "step": 13038
    },
    {
        "loss": 2.6364,
        "grad_norm": 2.9080758094787598,
        "learning_rate": 3.6471648346150134e-06,
        "epoch": 0.9716819435129295,
        "step": 13039
    },
    {
        "loss": 2.2681,
        "grad_norm": 3.2969985008239746,
        "learning_rate": 3.6283583951884913e-06,
        "epoch": 0.9717564647142112,
        "step": 13040
    },
    {
        "loss": 2.3319,
        "grad_norm": 2.8105270862579346,
        "learning_rate": 3.6095996725173163e-06,
        "epoch": 0.971830985915493,
        "step": 13041
    },
    {
        "loss": 2.1758,
        "grad_norm": 3.5606000423431396,
        "learning_rate": 3.590888675889503e-06,
        "epoch": 0.9719055071167747,
        "step": 13042
    },
    {
        "loss": 2.9486,
        "grad_norm": 2.13124680519104,
        "learning_rate": 3.5722254145695745e-06,
        "epoch": 0.9719800283180565,
        "step": 13043
    },
    {
        "loss": 2.7141,
        "grad_norm": 4.0955424308776855,
        "learning_rate": 3.5536098977981714e-06,
        "epoch": 0.9720545495193382,
        "step": 13044
    },
    {
        "loss": 2.0548,
        "grad_norm": 2.9409515857696533,
        "learning_rate": 3.5350421347925655e-06,
        "epoch": 0.97212907072062,
        "step": 13045
    },
    {
        "loss": 2.3109,
        "grad_norm": 3.108956813812256,
        "learning_rate": 3.516522134746203e-06,
        "epoch": 0.9722035919219018,
        "step": 13046
    },
    {
        "loss": 1.9179,
        "grad_norm": 3.173549175262451,
        "learning_rate": 3.4980499068289376e-06,
        "epoch": 0.9722781131231836,
        "step": 13047
    },
    {
        "loss": 2.5651,
        "grad_norm": 4.415982723236084,
        "learning_rate": 3.4796254601870305e-06,
        "epoch": 0.9723526343244653,
        "step": 13048
    },
    {
        "loss": 2.4975,
        "grad_norm": 3.253286838531494,
        "learning_rate": 3.461248803942996e-06,
        "epoch": 0.9724271555257471,
        "step": 13049
    },
    {
        "loss": 2.3817,
        "grad_norm": 2.1450133323669434,
        "learning_rate": 3.4429199471956885e-06,
        "epoch": 0.9725016767270288,
        "step": 13050
    },
    {
        "loss": 2.2544,
        "grad_norm": 3.8795135021209717,
        "learning_rate": 3.4246388990203936e-06,
        "epoch": 0.9725761979283106,
        "step": 13051
    },
    {
        "loss": 1.084,
        "grad_norm": 3.8418140411376953,
        "learning_rate": 3.4064056684686154e-06,
        "epoch": 0.9726507191295923,
        "step": 13052
    },
    {
        "loss": 2.2049,
        "grad_norm": 2.521726131439209,
        "learning_rate": 3.388220264568176e-06,
        "epoch": 0.9727252403308742,
        "step": 13053
    },
    {
        "loss": 1.869,
        "grad_norm": 3.3289260864257812,
        "learning_rate": 3.3700826963233955e-06,
        "epoch": 0.9727997615321559,
        "step": 13054
    },
    {
        "loss": 2.2692,
        "grad_norm": 2.118837833404541,
        "learning_rate": 3.3519929727146237e-06,
        "epoch": 0.9728742827334377,
        "step": 13055
    },
    {
        "loss": 2.5111,
        "grad_norm": 2.4893887042999268,
        "learning_rate": 3.3339511026987844e-06,
        "epoch": 0.9729488039347194,
        "step": 13056
    },
    {
        "loss": 2.5106,
        "grad_norm": 1.84882390499115,
        "learning_rate": 3.3159570952089215e-06,
        "epoch": 0.9730233251360012,
        "step": 13057
    },
    {
        "loss": 2.4207,
        "grad_norm": 3.073132276535034,
        "learning_rate": 3.298010959154552e-06,
        "epoch": 0.9730978463372829,
        "step": 13058
    },
    {
        "loss": 2.3912,
        "grad_norm": 3.481874465942383,
        "learning_rate": 3.280112703421323e-06,
        "epoch": 0.9731723675385647,
        "step": 13059
    },
    {
        "loss": 2.5389,
        "grad_norm": 3.6164517402648926,
        "learning_rate": 3.262262336871236e-06,
        "epoch": 0.9732468887398464,
        "step": 13060
    },
    {
        "loss": 2.267,
        "grad_norm": 2.735783338546753,
        "learning_rate": 3.244459868342653e-06,
        "epoch": 0.9733214099411283,
        "step": 13061
    },
    {
        "loss": 1.4554,
        "grad_norm": 3.7747912406921387,
        "learning_rate": 3.226705306650091e-06,
        "epoch": 0.97339593114241,
        "step": 13062
    },
    {
        "loss": 1.9501,
        "grad_norm": 3.305955648422241,
        "learning_rate": 3.208998660584517e-06,
        "epoch": 0.9734704523436918,
        "step": 13063
    },
    {
        "loss": 2.3172,
        "grad_norm": 2.6214165687561035,
        "learning_rate": 3.1913399389129407e-06,
        "epoch": 0.9735449735449735,
        "step": 13064
    },
    {
        "loss": 2.8247,
        "grad_norm": 2.7114012241363525,
        "learning_rate": 3.173729150378879e-06,
        "epoch": 0.9736194947462553,
        "step": 13065
    },
    {
        "loss": 2.7291,
        "grad_norm": 2.346172332763672,
        "learning_rate": 3.156166303701957e-06,
        "epoch": 0.9736940159475371,
        "step": 13066
    },
    {
        "loss": 1.8775,
        "grad_norm": 4.894349575042725,
        "learning_rate": 3.13865140757813e-06,
        "epoch": 0.9737685371488188,
        "step": 13067
    },
    {
        "loss": 2.8221,
        "grad_norm": 1.9396967887878418,
        "learning_rate": 3.121184470679561e-06,
        "epoch": 0.9738430583501007,
        "step": 13068
    },
    {
        "loss": 2.812,
        "grad_norm": 3.0723698139190674,
        "learning_rate": 3.1037655016547763e-06,
        "epoch": 0.9739175795513824,
        "step": 13069
    },
    {
        "loss": 2.4506,
        "grad_norm": 2.7520174980163574,
        "learning_rate": 3.0863945091283874e-06,
        "epoch": 0.9739921007526642,
        "step": 13070
    },
    {
        "loss": 2.3041,
        "grad_norm": 2.8295507431030273,
        "learning_rate": 3.069071501701437e-06,
        "epoch": 0.9740666219539459,
        "step": 13071
    },
    {
        "loss": 2.2788,
        "grad_norm": 3.9457168579101562,
        "learning_rate": 3.051796487951053e-06,
        "epoch": 0.9741411431552277,
        "step": 13072
    },
    {
        "loss": 2.5533,
        "grad_norm": 2.1502692699432373,
        "learning_rate": 3.034569476430649e-06,
        "epoch": 0.9742156643565094,
        "step": 13073
    },
    {
        "loss": 2.3082,
        "grad_norm": 3.3097634315490723,
        "learning_rate": 3.01739047566999e-06,
        "epoch": 0.9742901855577912,
        "step": 13074
    },
    {
        "loss": 2.6718,
        "grad_norm": 2.8132829666137695,
        "learning_rate": 3.000259494174795e-06,
        "epoch": 0.9743647067590729,
        "step": 13075
    },
    {
        "loss": 1.8174,
        "grad_norm": 3.457486867904663,
        "learning_rate": 2.9831765404272905e-06,
        "epoch": 0.9744392279603548,
        "step": 13076
    },
    {
        "loss": 2.4883,
        "grad_norm": 2.3217527866363525,
        "learning_rate": 2.9661416228857765e-06,
        "epoch": 0.9745137491616365,
        "step": 13077
    },
    {
        "loss": 2.8151,
        "grad_norm": 2.1651976108551025,
        "learning_rate": 2.9491547499847615e-06,
        "epoch": 0.9745882703629183,
        "step": 13078
    },
    {
        "loss": 2.5283,
        "grad_norm": 2.902740955352783,
        "learning_rate": 2.932215930135074e-06,
        "epoch": 0.9746627915642,
        "step": 13079
    },
    {
        "loss": 2.6203,
        "grad_norm": 2.398627758026123,
        "learning_rate": 2.915325171723604e-06,
        "epoch": 0.9747373127654818,
        "step": 13080
    },
    {
        "loss": 2.5233,
        "grad_norm": 2.3719639778137207,
        "learning_rate": 2.898482483113607e-06,
        "epoch": 0.9748118339667635,
        "step": 13081
    },
    {
        "loss": 1.8163,
        "grad_norm": 3.2391066551208496,
        "learning_rate": 2.8816878726443785e-06,
        "epoch": 0.9748863551680453,
        "step": 13082
    },
    {
        "loss": 1.9323,
        "grad_norm": 2.8495097160339355,
        "learning_rate": 2.8649413486315113e-06,
        "epoch": 0.974960876369327,
        "step": 13083
    },
    {
        "loss": 2.6627,
        "grad_norm": 2.347623348236084,
        "learning_rate": 2.848242919366706e-06,
        "epoch": 0.9750353975706089,
        "step": 13084
    },
    {
        "loss": 1.7568,
        "grad_norm": 3.706993818283081,
        "learning_rate": 2.8315925931179708e-06,
        "epoch": 0.9751099187718906,
        "step": 13085
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.5465774536132812,
        "learning_rate": 2.814990378129412e-06,
        "epoch": 0.9751844399731724,
        "step": 13086
    },
    {
        "loss": 2.3944,
        "grad_norm": 4.620626449584961,
        "learning_rate": 2.7984362826213083e-06,
        "epoch": 0.9752589611744541,
        "step": 13087
    },
    {
        "loss": 2.4788,
        "grad_norm": 1.9080383777618408,
        "learning_rate": 2.7819303147901264e-06,
        "epoch": 0.9753334823757359,
        "step": 13088
    },
    {
        "loss": 2.4446,
        "grad_norm": 2.80546236038208,
        "learning_rate": 2.765472482808551e-06,
        "epoch": 0.9754080035770176,
        "step": 13089
    },
    {
        "loss": 2.4182,
        "grad_norm": 2.4920146465301514,
        "learning_rate": 2.74906279482533e-06,
        "epoch": 0.9754825247782994,
        "step": 13090
    },
    {
        "loss": 2.2868,
        "grad_norm": 3.0084035396575928,
        "learning_rate": 2.73270125896552e-06,
        "epoch": 0.9755570459795811,
        "step": 13091
    },
    {
        "loss": 1.6867,
        "grad_norm": 3.1028661727905273,
        "learning_rate": 2.716387883330207e-06,
        "epoch": 0.975631567180863,
        "step": 13092
    },
    {
        "loss": 2.233,
        "grad_norm": 2.9112026691436768,
        "learning_rate": 2.7001226759966524e-06,
        "epoch": 0.9757060883821447,
        "step": 13093
    },
    {
        "loss": 1.7282,
        "grad_norm": 3.4470996856689453,
        "learning_rate": 2.683905645018381e-06,
        "epoch": 0.9757806095834265,
        "step": 13094
    },
    {
        "loss": 2.4255,
        "grad_norm": 2.1648244857788086,
        "learning_rate": 2.667736798424858e-06,
        "epoch": 0.9758551307847082,
        "step": 13095
    },
    {
        "loss": 2.5422,
        "grad_norm": 3.092379093170166,
        "learning_rate": 2.6516161442219136e-06,
        "epoch": 0.97592965198599,
        "step": 13096
    },
    {
        "loss": 2.4651,
        "grad_norm": 2.685703992843628,
        "learning_rate": 2.635543690391351e-06,
        "epoch": 0.9760041731872717,
        "step": 13097
    },
    {
        "loss": 2.7735,
        "grad_norm": 2.534489154815674,
        "learning_rate": 2.6195194448911497e-06,
        "epoch": 0.9760786943885535,
        "step": 13098
    },
    {
        "loss": 2.1379,
        "grad_norm": 3.7473011016845703,
        "learning_rate": 2.603543415655507e-06,
        "epoch": 0.9761532155898353,
        "step": 13099
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.9598134756088257,
        "learning_rate": 2.58761561059464e-06,
        "epoch": 0.9762277367911171,
        "step": 13100
    },
    {
        "loss": 2.8654,
        "grad_norm": 2.390831708908081,
        "learning_rate": 2.5717360375949073e-06,
        "epoch": 0.9763022579923989,
        "step": 13101
    },
    {
        "loss": 2.529,
        "grad_norm": 2.3265819549560547,
        "learning_rate": 2.5559047045188522e-06,
        "epoch": 0.9763767791936806,
        "step": 13102
    },
    {
        "loss": 2.5458,
        "grad_norm": 3.9740140438079834,
        "learning_rate": 2.54012161920506e-06,
        "epoch": 0.9764513003949624,
        "step": 13103
    },
    {
        "loss": 2.1502,
        "grad_norm": 4.828082084655762,
        "learning_rate": 2.524386789468236e-06,
        "epoch": 0.9765258215962441,
        "step": 13104
    },
    {
        "loss": 2.3366,
        "grad_norm": 3.335832357406616,
        "learning_rate": 2.5087002230992696e-06,
        "epoch": 0.9766003427975259,
        "step": 13105
    },
    {
        "loss": 1.7732,
        "grad_norm": 4.639264106750488,
        "learning_rate": 2.4930619278650036e-06,
        "epoch": 0.9766748639988077,
        "step": 13106
    },
    {
        "loss": 2.5406,
        "grad_norm": 2.5077874660491943,
        "learning_rate": 2.4774719115085445e-06,
        "epoch": 0.9767493852000895,
        "step": 13107
    },
    {
        "loss": 2.788,
        "grad_norm": 3.055927276611328,
        "learning_rate": 2.4619301817489616e-06,
        "epoch": 0.9768239064013712,
        "step": 13108
    },
    {
        "loss": 2.2983,
        "grad_norm": 2.207653045654297,
        "learning_rate": 2.446436746281544e-06,
        "epoch": 0.976898427602653,
        "step": 13109
    },
    {
        "loss": 1.5724,
        "grad_norm": 3.3082492351531982,
        "learning_rate": 2.4309916127775667e-06,
        "epoch": 0.9769729488039347,
        "step": 13110
    },
    {
        "loss": 1.0711,
        "grad_norm": 2.31657338142395,
        "learning_rate": 2.4155947888843676e-06,
        "epoch": 0.9770474700052165,
        "step": 13111
    },
    {
        "loss": 1.3925,
        "grad_norm": 4.488020896911621,
        "learning_rate": 2.4002462822255046e-06,
        "epoch": 0.9771219912064982,
        "step": 13112
    },
    {
        "loss": 2.5521,
        "grad_norm": 2.3432655334472656,
        "learning_rate": 2.3849461004004646e-06,
        "epoch": 0.97719651240778,
        "step": 13113
    },
    {
        "loss": 1.9433,
        "grad_norm": 3.2233352661132812,
        "learning_rate": 2.369694250984944e-06,
        "epoch": 0.9772710336090618,
        "step": 13114
    },
    {
        "loss": 2.5257,
        "grad_norm": 2.270622730255127,
        "learning_rate": 2.354490741530524e-06,
        "epoch": 0.9773455548103436,
        "step": 13115
    },
    {
        "loss": 1.7613,
        "grad_norm": 2.570082426071167,
        "learning_rate": 2.3393355795650385e-06,
        "epoch": 0.9774200760116253,
        "step": 13116
    },
    {
        "loss": 2.8371,
        "grad_norm": 3.146000385284424,
        "learning_rate": 2.324228772592285e-06,
        "epoch": 0.9774945972129071,
        "step": 13117
    },
    {
        "loss": 2.127,
        "grad_norm": 2.175593376159668,
        "learning_rate": 2.3091703280921138e-06,
        "epoch": 0.9775691184141888,
        "step": 13118
    },
    {
        "loss": 1.7393,
        "grad_norm": 4.0081071853637695,
        "learning_rate": 2.2941602535205163e-06,
        "epoch": 0.9776436396154706,
        "step": 13119
    },
    {
        "loss": 2.2388,
        "grad_norm": 4.245666980743408,
        "learning_rate": 2.2791985563094143e-06,
        "epoch": 0.9777181608167523,
        "step": 13120
    },
    {
        "loss": 2.2801,
        "grad_norm": 1.7526761293411255,
        "learning_rate": 2.264285243866815e-06,
        "epoch": 0.9777926820180342,
        "step": 13121
    },
    {
        "loss": 2.2391,
        "grad_norm": 2.56154203414917,
        "learning_rate": 2.2494203235768673e-06,
        "epoch": 0.9778672032193159,
        "step": 13122
    },
    {
        "loss": 1.9801,
        "grad_norm": 2.4812991619110107,
        "learning_rate": 2.2346038027996285e-06,
        "epoch": 0.9779417244205977,
        "step": 13123
    },
    {
        "loss": 2.1755,
        "grad_norm": 3.3396358489990234,
        "learning_rate": 2.2198356888712303e-06,
        "epoch": 0.9780162456218794,
        "step": 13124
    },
    {
        "loss": 2.3591,
        "grad_norm": 2.2357146739959717,
        "learning_rate": 2.205115989103923e-06,
        "epoch": 0.9780907668231612,
        "step": 13125
    },
    {
        "loss": 2.4003,
        "grad_norm": 4.29547643661499,
        "learning_rate": 2.1904447107858107e-06,
        "epoch": 0.9781652880244429,
        "step": 13126
    },
    {
        "loss": 2.177,
        "grad_norm": 3.4715065956115723,
        "learning_rate": 2.175821861181182e-06,
        "epoch": 0.9782398092257247,
        "step": 13127
    },
    {
        "loss": 2.5887,
        "grad_norm": 2.3572580814361572,
        "learning_rate": 2.16124744753029e-06,
        "epoch": 0.9783143304270064,
        "step": 13128
    },
    {
        "loss": 1.6965,
        "grad_norm": 2.8968777656555176,
        "learning_rate": 2.1467214770493625e-06,
        "epoch": 0.9783888516282883,
        "step": 13129
    },
    {
        "loss": 1.4109,
        "grad_norm": 2.1107940673828125,
        "learning_rate": 2.1322439569307464e-06,
        "epoch": 0.97846337282957,
        "step": 13130
    },
    {
        "loss": 2.5541,
        "grad_norm": 2.693183422088623,
        "learning_rate": 2.1178148943426734e-06,
        "epoch": 0.9785378940308518,
        "step": 13131
    },
    {
        "loss": 2.0475,
        "grad_norm": 2.6865427494049072,
        "learning_rate": 2.103434296429496e-06,
        "epoch": 0.9786124152321335,
        "step": 13132
    },
    {
        "loss": 2.8005,
        "grad_norm": 3.4254844188690186,
        "learning_rate": 2.089102170311508e-06,
        "epoch": 0.9786869364334153,
        "step": 13133
    },
    {
        "loss": 1.8977,
        "grad_norm": 3.036661386489868,
        "learning_rate": 2.074818523084998e-06,
        "epoch": 0.978761457634697,
        "step": 13134
    },
    {
        "loss": 2.0357,
        "grad_norm": 3.7881996631622314,
        "learning_rate": 2.0605833618222436e-06,
        "epoch": 0.9788359788359788,
        "step": 13135
    },
    {
        "loss": 2.5313,
        "grad_norm": 2.9583449363708496,
        "learning_rate": 2.046396693571606e-06,
        "epoch": 0.9789105000372607,
        "step": 13136
    },
    {
        "loss": 1.9423,
        "grad_norm": 2.749329090118408,
        "learning_rate": 2.0322585253573223e-06,
        "epoch": 0.9789850212385424,
        "step": 13137
    },
    {
        "loss": 1.8099,
        "grad_norm": 2.850419044494629,
        "learning_rate": 2.0181688641796702e-06,
        "epoch": 0.9790595424398242,
        "step": 13138
    },
    {
        "loss": 2.1482,
        "grad_norm": 4.027135372161865,
        "learning_rate": 2.004127717014892e-06,
        "epoch": 0.9791340636411059,
        "step": 13139
    },
    {
        "loss": 2.5656,
        "grad_norm": 2.266761541366577,
        "learning_rate": 1.9901350908152483e-06,
        "epoch": 0.9792085848423877,
        "step": 13140
    },
    {
        "loss": 3.3964,
        "grad_norm": 3.2807154655456543,
        "learning_rate": 1.976190992508897e-06,
        "epoch": 0.9792831060436694,
        "step": 13141
    },
    {
        "loss": 2.3631,
        "grad_norm": 4.383461952209473,
        "learning_rate": 1.9622954290000937e-06,
        "epoch": 0.9793576272449512,
        "step": 13142
    },
    {
        "loss": 2.8948,
        "grad_norm": 3.3543970584869385,
        "learning_rate": 1.9484484071689346e-06,
        "epoch": 0.9794321484462329,
        "step": 13143
    },
    {
        "loss": 2.2639,
        "grad_norm": 2.0215208530426025,
        "learning_rate": 1.934649933871524e-06,
        "epoch": 0.9795066696475148,
        "step": 13144
    },
    {
        "loss": 2.5304,
        "grad_norm": 3.165402889251709,
        "learning_rate": 1.920900015939997e-06,
        "epoch": 0.9795811908487965,
        "step": 13145
    },
    {
        "loss": 2.5905,
        "grad_norm": 2.5164499282836914,
        "learning_rate": 1.9071986601823077e-06,
        "epoch": 0.9796557120500783,
        "step": 13146
    },
    {
        "loss": 2.2869,
        "grad_norm": 5.042438983917236,
        "learning_rate": 1.8935458733825074e-06,
        "epoch": 0.97973023325136,
        "step": 13147
    },
    {
        "loss": 2.7767,
        "grad_norm": 2.9466025829315186,
        "learning_rate": 1.8799416623005217e-06,
        "epoch": 0.9798047544526418,
        "step": 13148
    },
    {
        "loss": 1.7,
        "grad_norm": 3.5494179725646973,
        "learning_rate": 1.8663860336722071e-06,
        "epoch": 0.9798792756539235,
        "step": 13149
    },
    {
        "loss": 1.3726,
        "grad_norm": 3.5647668838500977,
        "learning_rate": 1.8528789942094504e-06,
        "epoch": 0.9799537968552053,
        "step": 13150
    },
    {
        "loss": 2.2092,
        "grad_norm": 3.475679397583008,
        "learning_rate": 1.8394205505999906e-06,
        "epoch": 0.980028318056487,
        "step": 13151
    },
    {
        "loss": 2.4391,
        "grad_norm": 2.733807325363159,
        "learning_rate": 1.8260107095075751e-06,
        "epoch": 0.9801028392577689,
        "step": 13152
    },
    {
        "loss": 2.7534,
        "grad_norm": 2.476109027862549,
        "learning_rate": 1.8126494775718483e-06,
        "epoch": 0.9801773604590506,
        "step": 13153
    },
    {
        "loss": 2.7579,
        "grad_norm": 2.776695966720581,
        "learning_rate": 1.799336861408374e-06,
        "epoch": 0.9802518816603324,
        "step": 13154
    },
    {
        "loss": 2.0296,
        "grad_norm": 3.1340479850769043,
        "learning_rate": 1.7860728676086568e-06,
        "epoch": 0.9803264028616141,
        "step": 13155
    },
    {
        "loss": 2.5407,
        "grad_norm": 2.1076903343200684,
        "learning_rate": 1.7728575027401884e-06,
        "epoch": 0.9804009240628959,
        "step": 13156
    },
    {
        "loss": 1.4088,
        "grad_norm": 4.583400249481201,
        "learning_rate": 1.7596907733462897e-06,
        "epoch": 0.9804754452641776,
        "step": 13157
    },
    {
        "loss": 2.4863,
        "grad_norm": 2.4241089820861816,
        "learning_rate": 1.746572685946235e-06,
        "epoch": 0.9805499664654594,
        "step": 13158
    },
    {
        "loss": 1.8125,
        "grad_norm": 3.947709560394287,
        "learning_rate": 1.7335032470352175e-06,
        "epoch": 0.9806244876667412,
        "step": 13159
    },
    {
        "loss": 2.1386,
        "grad_norm": 2.9099652767181396,
        "learning_rate": 1.7204824630843608e-06,
        "epoch": 0.980699008868023,
        "step": 13160
    },
    {
        "loss": 2.4491,
        "grad_norm": 1.8653852939605713,
        "learning_rate": 1.7075103405406857e-06,
        "epoch": 0.9807735300693047,
        "step": 13161
    },
    {
        "loss": 2.8214,
        "grad_norm": 2.133664131164551,
        "learning_rate": 1.6945868858270764e-06,
        "epoch": 0.9808480512705865,
        "step": 13162
    },
    {
        "loss": 1.8528,
        "grad_norm": 2.280674695968628,
        "learning_rate": 1.6817121053424144e-06,
        "epoch": 0.9809225724718682,
        "step": 13163
    },
    {
        "loss": 2.6369,
        "grad_norm": 3.3664212226867676,
        "learning_rate": 1.6688860054613786e-06,
        "epoch": 0.98099709367315,
        "step": 13164
    },
    {
        "loss": 2.8639,
        "grad_norm": 3.4776976108551025,
        "learning_rate": 1.6561085925346554e-06,
        "epoch": 0.9810716148744317,
        "step": 13165
    },
    {
        "loss": 1.7079,
        "grad_norm": 3.1860430240631104,
        "learning_rate": 1.6433798728886617e-06,
        "epoch": 0.9811461360757135,
        "step": 13166
    },
    {
        "loss": 1.9367,
        "grad_norm": 3.6094610691070557,
        "learning_rate": 1.6306998528258787e-06,
        "epoch": 0.9812206572769953,
        "step": 13167
    },
    {
        "loss": 2.1976,
        "grad_norm": 3.7082302570343018,
        "learning_rate": 1.6180685386245731e-06,
        "epoch": 0.9812951784782771,
        "step": 13168
    },
    {
        "loss": 1.5183,
        "grad_norm": 2.1320366859436035,
        "learning_rate": 1.6054859365389086e-06,
        "epoch": 0.9813696996795588,
        "step": 13169
    },
    {
        "loss": 2.182,
        "grad_norm": 1.6500309705734253,
        "learning_rate": 1.5929520527989795e-06,
        "epoch": 0.9814442208808406,
        "step": 13170
    },
    {
        "loss": 2.1551,
        "grad_norm": 3.5314600467681885,
        "learning_rate": 1.5804668936107214e-06,
        "epoch": 0.9815187420821223,
        "step": 13171
    },
    {
        "loss": 2.8262,
        "grad_norm": 2.121971845626831,
        "learning_rate": 1.5680304651558898e-06,
        "epoch": 0.9815932632834041,
        "step": 13172
    },
    {
        "loss": 2.4699,
        "grad_norm": 3.727663040161133,
        "learning_rate": 1.5556427735922251e-06,
        "epoch": 0.981667784484686,
        "step": 13173
    },
    {
        "loss": 1.6922,
        "grad_norm": 3.3992698192596436,
        "learning_rate": 1.543303825053266e-06,
        "epoch": 0.9817423056859677,
        "step": 13174
    },
    {
        "loss": 1.715,
        "grad_norm": 4.589117527008057,
        "learning_rate": 1.531013625648392e-06,
        "epoch": 0.9818168268872495,
        "step": 13175
    },
    {
        "loss": 2.1957,
        "grad_norm": 2.7697267532348633,
        "learning_rate": 1.5187721814629685e-06,
        "epoch": 0.9818913480885312,
        "step": 13176
    },
    {
        "loss": 2.4473,
        "grad_norm": 2.0829050540924072,
        "learning_rate": 1.506579498558025e-06,
        "epoch": 0.981965869289813,
        "step": 13177
    },
    {
        "loss": 2.4745,
        "grad_norm": 2.4799892902374268,
        "learning_rate": 1.4944355829706546e-06,
        "epoch": 0.9820403904910947,
        "step": 13178
    },
    {
        "loss": 2.2978,
        "grad_norm": 2.0513126850128174,
        "learning_rate": 1.4823404407136366e-06,
        "epoch": 0.9821149116923765,
        "step": 13179
    },
    {
        "loss": 2.495,
        "grad_norm": 3.641274929046631,
        "learning_rate": 1.4702940777757136e-06,
        "epoch": 0.9821894328936582,
        "step": 13180
    },
    {
        "loss": 2.45,
        "grad_norm": 2.2753186225891113,
        "learning_rate": 1.4582965001214367e-06,
        "epoch": 0.98226395409494,
        "step": 13181
    },
    {
        "loss": 2.3343,
        "grad_norm": 2.7164688110351562,
        "learning_rate": 1.446347713691154e-06,
        "epoch": 0.9823384752962218,
        "step": 13182
    },
    {
        "loss": 2.5173,
        "grad_norm": 2.3116273880004883,
        "learning_rate": 1.4344477244011668e-06,
        "epoch": 0.9824129964975036,
        "step": 13183
    },
    {
        "loss": 2.7417,
        "grad_norm": 3.253058910369873,
        "learning_rate": 1.4225965381434837e-06,
        "epoch": 0.9824875176987853,
        "step": 13184
    },
    {
        "loss": 1.8821,
        "grad_norm": 4.297513484954834,
        "learning_rate": 1.4107941607860996e-06,
        "epoch": 0.9825620389000671,
        "step": 13185
    },
    {
        "loss": 1.8943,
        "grad_norm": 2.032233476638794,
        "learning_rate": 1.3990405981726517e-06,
        "epoch": 0.9826365601013488,
        "step": 13186
    },
    {
        "loss": 1.6031,
        "grad_norm": 3.2625370025634766,
        "learning_rate": 1.3873358561227955e-06,
        "epoch": 0.9827110813026306,
        "step": 13187
    },
    {
        "loss": 2.5066,
        "grad_norm": 1.8301657438278198,
        "learning_rate": 1.3756799404319064e-06,
        "epoch": 0.9827856025039123,
        "step": 13188
    },
    {
        "loss": 2.3331,
        "grad_norm": 2.8651645183563232,
        "learning_rate": 1.364072856871179e-06,
        "epoch": 0.9828601237051942,
        "step": 13189
    },
    {
        "loss": 2.2704,
        "grad_norm": 3.2481112480163574,
        "learning_rate": 1.3525146111877162e-06,
        "epoch": 0.9829346449064759,
        "step": 13190
    },
    {
        "loss": 2.7524,
        "grad_norm": 3.15159010887146,
        "learning_rate": 1.3410052091043512e-06,
        "epoch": 0.9830091661077577,
        "step": 13191
    },
    {
        "loss": 3.075,
        "grad_norm": 2.108948230743408,
        "learning_rate": 1.3295446563197479e-06,
        "epoch": 0.9830836873090394,
        "step": 13192
    },
    {
        "loss": 2.1345,
        "grad_norm": 2.5704755783081055,
        "learning_rate": 1.3181329585084557e-06,
        "epoch": 0.9831582085103212,
        "step": 13193
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.7709217071533203,
        "learning_rate": 1.306770121320744e-06,
        "epoch": 0.9832327297116029,
        "step": 13194
    },
    {
        "loss": 1.8549,
        "grad_norm": 3.6793997287750244,
        "learning_rate": 1.2954561503827122e-06,
        "epoch": 0.9833072509128847,
        "step": 13195
    },
    {
        "loss": 1.8393,
        "grad_norm": 4.784993648529053,
        "learning_rate": 1.284191051296335e-06,
        "epoch": 0.9833817721141664,
        "step": 13196
    },
    {
        "loss": 1.3059,
        "grad_norm": 1.7069674730300903,
        "learning_rate": 1.2729748296392507e-06,
        "epoch": 0.9834562933154483,
        "step": 13197
    },
    {
        "loss": 1.8598,
        "grad_norm": 4.793339252471924,
        "learning_rate": 1.261807490965039e-06,
        "epoch": 0.98353081451673,
        "step": 13198
    },
    {
        "loss": 2.5501,
        "grad_norm": 2.4145286083221436,
        "learning_rate": 1.250689040802988e-06,
        "epoch": 0.9836053357180118,
        "step": 13199
    },
    {
        "loss": 2.5143,
        "grad_norm": 2.892116069793701,
        "learning_rate": 1.2396194846582055e-06,
        "epoch": 0.9836798569192935,
        "step": 13200
    },
    {
        "loss": 2.469,
        "grad_norm": 2.048252582550049,
        "learning_rate": 1.2285988280116289e-06,
        "epoch": 0.9837543781205753,
        "step": 13201
    },
    {
        "loss": 2.5726,
        "grad_norm": 2.68552565574646,
        "learning_rate": 1.2176270763198828e-06,
        "epoch": 0.983828899321857,
        "step": 13202
    },
    {
        "loss": 2.0887,
        "grad_norm": 2.803985834121704,
        "learning_rate": 1.2067042350154988e-06,
        "epoch": 0.9839034205231388,
        "step": 13203
    },
    {
        "loss": 1.7758,
        "grad_norm": 2.3945059776306152,
        "learning_rate": 1.195830309506707e-06,
        "epoch": 0.9839779417244205,
        "step": 13204
    },
    {
        "loss": 2.4449,
        "grad_norm": 2.583627700805664,
        "learning_rate": 1.1850053051775444e-06,
        "epoch": 0.9840524629257024,
        "step": 13205
    },
    {
        "loss": 1.7333,
        "grad_norm": 2.5370006561279297,
        "learning_rate": 1.1742292273878131e-06,
        "epoch": 0.9841269841269841,
        "step": 13206
    },
    {
        "loss": 2.1409,
        "grad_norm": 3.1483635902404785,
        "learning_rate": 1.1635020814731335e-06,
        "epoch": 0.9842015053282659,
        "step": 13207
    },
    {
        "loss": 2.0093,
        "grad_norm": 3.00076961517334,
        "learning_rate": 1.152823872744846e-06,
        "epoch": 0.9842760265295477,
        "step": 13208
    },
    {
        "loss": 1.7576,
        "grad_norm": 1.9715778827667236,
        "learning_rate": 1.142194606490099e-06,
        "epoch": 0.9843505477308294,
        "step": 13209
    },
    {
        "loss": 2.4217,
        "grad_norm": 2.602484941482544,
        "learning_rate": 1.1316142879717384e-06,
        "epoch": 0.9844250689321112,
        "step": 13210
    },
    {
        "loss": 2.2821,
        "grad_norm": 2.1701817512512207,
        "learning_rate": 1.1210829224284958e-06,
        "epoch": 0.9844995901333929,
        "step": 13211
    },
    {
        "loss": 1.8585,
        "grad_norm": 3.1840460300445557,
        "learning_rate": 1.1106005150747334e-06,
        "epoch": 0.9845741113346748,
        "step": 13212
    },
    {
        "loss": 2.5095,
        "grad_norm": 2.5916168689727783,
        "learning_rate": 1.1001670711006884e-06,
        "epoch": 0.9846486325359565,
        "step": 13213
    },
    {
        "loss": 2.1785,
        "grad_norm": 2.594698429107666,
        "learning_rate": 1.0897825956722841e-06,
        "epoch": 0.9847231537372383,
        "step": 13214
    },
    {
        "loss": 2.5751,
        "grad_norm": 2.2206075191497803,
        "learning_rate": 1.079447093931174e-06,
        "epoch": 0.98479767493852,
        "step": 13215
    },
    {
        "loss": 2.661,
        "grad_norm": 2.5688798427581787,
        "learning_rate": 1.0691605709948982e-06,
        "epoch": 0.9848721961398018,
        "step": 13216
    },
    {
        "loss": 1.6105,
        "grad_norm": 2.3821160793304443,
        "learning_rate": 1.058923031956527e-06,
        "epoch": 0.9849467173410835,
        "step": 13217
    },
    {
        "loss": 2.4554,
        "grad_norm": 2.4974772930145264,
        "learning_rate": 1.0487344818850942e-06,
        "epoch": 0.9850212385423653,
        "step": 13218
    },
    {
        "loss": 2.4418,
        "grad_norm": 2.408719778060913,
        "learning_rate": 1.0385949258252426e-06,
        "epoch": 0.985095759743647,
        "step": 13219
    },
    {
        "loss": 2.2283,
        "grad_norm": 3.1396002769470215,
        "learning_rate": 1.0285043687974006e-06,
        "epoch": 0.9851702809449289,
        "step": 13220
    },
    {
        "loss": 2.4512,
        "grad_norm": 2.017988681793213,
        "learning_rate": 1.0184628157977494e-06,
        "epoch": 0.9852448021462106,
        "step": 13221
    },
    {
        "loss": 2.5828,
        "grad_norm": 3.2789371013641357,
        "learning_rate": 1.0084702717982009e-06,
        "epoch": 0.9853193233474924,
        "step": 13222
    },
    {
        "loss": 1.9557,
        "grad_norm": 2.924926996231079,
        "learning_rate": 9.985267417463417e-07,
        "epoch": 0.9853938445487741,
        "step": 13223
    },
    {
        "loss": 2.7865,
        "grad_norm": 2.1093590259552,
        "learning_rate": 9.886322305656003e-07,
        "epoch": 0.9854683657500559,
        "step": 13224
    },
    {
        "loss": 1.9344,
        "grad_norm": 1.399458646774292,
        "learning_rate": 9.787867431550245e-07,
        "epoch": 0.9855428869513376,
        "step": 13225
    },
    {
        "loss": 1.9158,
        "grad_norm": 4.014880180358887,
        "learning_rate": 9.689902843894595e-07,
        "epoch": 0.9856174081526194,
        "step": 13226
    },
    {
        "loss": 2.4042,
        "grad_norm": 2.3955321311950684,
        "learning_rate": 9.592428591194691e-07,
        "epoch": 0.9856919293539012,
        "step": 13227
    },
    {
        "loss": 1.6369,
        "grad_norm": 2.036484718322754,
        "learning_rate": 9.49544472171271e-07,
        "epoch": 0.985766450555183,
        "step": 13228
    },
    {
        "loss": 2.4937,
        "grad_norm": 2.8704724311828613,
        "learning_rate": 9.398951283469126e-07,
        "epoch": 0.9858409717564647,
        "step": 13229
    },
    {
        "loss": 2.2633,
        "grad_norm": 2.358386993408203,
        "learning_rate": 9.302948324240501e-07,
        "epoch": 0.9859154929577465,
        "step": 13230
    },
    {
        "loss": 2.4216,
        "grad_norm": 3.8351902961730957,
        "learning_rate": 9.207435891561477e-07,
        "epoch": 0.9859900141590282,
        "step": 13231
    },
    {
        "loss": 1.5816,
        "grad_norm": 3.24027943611145,
        "learning_rate": 9.112414032723227e-07,
        "epoch": 0.98606453536031,
        "step": 13232
    },
    {
        "loss": 2.1002,
        "grad_norm": 1.8910537958145142,
        "learning_rate": 9.017882794773891e-07,
        "epoch": 0.9861390565615917,
        "step": 13233
    },
    {
        "loss": 1.8634,
        "grad_norm": 4.615991592407227,
        "learning_rate": 8.923842224519474e-07,
        "epoch": 0.9862135777628736,
        "step": 13234
    },
    {
        "loss": 1.7798,
        "grad_norm": 3.0438668727874756,
        "learning_rate": 8.830292368522176e-07,
        "epoch": 0.9862880989641553,
        "step": 13235
    },
    {
        "loss": 2.1252,
        "grad_norm": 2.7196710109710693,
        "learning_rate": 8.737233273101942e-07,
        "epoch": 0.9863626201654371,
        "step": 13236
    },
    {
        "loss": 2.666,
        "grad_norm": 2.318387031555176,
        "learning_rate": 8.644664984334916e-07,
        "epoch": 0.9864371413667188,
        "step": 13237
    },
    {
        "loss": 1.7669,
        "grad_norm": 3.8585777282714844,
        "learning_rate": 8.552587548055102e-07,
        "epoch": 0.9865116625680006,
        "step": 13238
    },
    {
        "loss": 2.3006,
        "grad_norm": 2.3048770427703857,
        "learning_rate": 8.46100100985292e-07,
        "epoch": 0.9865861837692823,
        "step": 13239
    },
    {
        "loss": 2.7591,
        "grad_norm": 2.0421860218048096,
        "learning_rate": 8.369905415075541e-07,
        "epoch": 0.9866607049705641,
        "step": 13240
    },
    {
        "loss": 2.0942,
        "grad_norm": 3.496103048324585,
        "learning_rate": 8.279300808827883e-07,
        "epoch": 0.9867352261718458,
        "step": 13241
    },
    {
        "loss": 2.1735,
        "grad_norm": 3.834760904312134,
        "learning_rate": 8.189187235971063e-07,
        "epoch": 0.9868097473731277,
        "step": 13242
    },
    {
        "loss": 2.3109,
        "grad_norm": 2.7926342487335205,
        "learning_rate": 8.099564741123056e-07,
        "epoch": 0.9868842685744095,
        "step": 13243
    },
    {
        "loss": 2.7796,
        "grad_norm": 2.3918161392211914,
        "learning_rate": 8.010433368659254e-07,
        "epoch": 0.9869587897756912,
        "step": 13244
    },
    {
        "loss": 2.7607,
        "grad_norm": 2.844298839569092,
        "learning_rate": 7.921793162711244e-07,
        "epoch": 0.987033310976973,
        "step": 13245
    },
    {
        "loss": 2.2399,
        "grad_norm": 3.4838173389434814,
        "learning_rate": 7.833644167167697e-07,
        "epoch": 0.9871078321782547,
        "step": 13246
    },
    {
        "loss": 2.3202,
        "grad_norm": 2.8193790912628174,
        "learning_rate": 7.745986425674589e-07,
        "epoch": 0.9871823533795365,
        "step": 13247
    },
    {
        "loss": 2.4721,
        "grad_norm": 2.172074556350708,
        "learning_rate": 7.658819981633203e-07,
        "epoch": 0.9872568745808182,
        "step": 13248
    },
    {
        "loss": 0.5452,
        "grad_norm": 1.8924404382705688,
        "learning_rate": 7.572144878203235e-07,
        "epoch": 0.9873313957821,
        "step": 13249
    },
    {
        "loss": 1.6636,
        "grad_norm": 1.5894700288772583,
        "learning_rate": 7.485961158299915e-07,
        "epoch": 0.9874059169833818,
        "step": 13250
    },
    {
        "loss": 1.9216,
        "grad_norm": 4.172618865966797,
        "learning_rate": 7.400268864596105e-07,
        "epoch": 0.9874804381846636,
        "step": 13251
    },
    {
        "loss": 1.5317,
        "grad_norm": 3.9484479427337646,
        "learning_rate": 7.315068039520534e-07,
        "epoch": 0.9875549593859453,
        "step": 13252
    },
    {
        "loss": 2.2246,
        "grad_norm": 3.032228469848633,
        "learning_rate": 7.23035872525879e-07,
        "epoch": 0.9876294805872271,
        "step": 13253
    },
    {
        "loss": 2.465,
        "grad_norm": 2.523606538772583,
        "learning_rate": 7.146140963753656e-07,
        "epoch": 0.9877040017885088,
        "step": 13254
    },
    {
        "loss": 3.1253,
        "grad_norm": 3.033487558364868,
        "learning_rate": 7.062414796703887e-07,
        "epoch": 0.9877785229897906,
        "step": 13255
    },
    {
        "loss": 2.3928,
        "grad_norm": 2.2016654014587402,
        "learning_rate": 6.979180265564988e-07,
        "epoch": 0.9878530441910723,
        "step": 13256
    },
    {
        "loss": 2.3404,
        "grad_norm": 3.2381622791290283,
        "learning_rate": 6.896437411549106e-07,
        "epoch": 0.9879275653923542,
        "step": 13257
    },
    {
        "loss": 2.0632,
        "grad_norm": 3.799459934234619,
        "learning_rate": 6.814186275625023e-07,
        "epoch": 0.9880020865936359,
        "step": 13258
    },
    {
        "loss": 2.5661,
        "grad_norm": 2.9935436248779297,
        "learning_rate": 6.73242689851794e-07,
        "epoch": 0.9880766077949177,
        "step": 13259
    },
    {
        "loss": 2.0222,
        "grad_norm": 3.1557528972625732,
        "learning_rate": 6.651159320709588e-07,
        "epoch": 0.9881511289961994,
        "step": 13260
    },
    {
        "loss": 3.011,
        "grad_norm": 4.268530368804932,
        "learning_rate": 6.570383582438e-07,
        "epoch": 0.9882256501974812,
        "step": 13261
    },
    {
        "loss": 2.0962,
        "grad_norm": 2.6961863040924072,
        "learning_rate": 6.490099723698406e-07,
        "epoch": 0.9883001713987629,
        "step": 13262
    },
    {
        "loss": 1.3475,
        "grad_norm": 3.751626968383789,
        "learning_rate": 6.410307784241454e-07,
        "epoch": 0.9883746926000447,
        "step": 13263
    },
    {
        "loss": 2.239,
        "grad_norm": 3.6781117916107178,
        "learning_rate": 6.331007803575095e-07,
        "epoch": 0.9884492138013264,
        "step": 13264
    },
    {
        "loss": 2.5624,
        "grad_norm": 1.7571882009506226,
        "learning_rate": 6.252199820963144e-07,
        "epoch": 0.9885237350026083,
        "step": 13265
    },
    {
        "loss": 2.4139,
        "grad_norm": 2.9314775466918945,
        "learning_rate": 6.173883875425945e-07,
        "epoch": 0.98859825620389,
        "step": 13266
    },
    {
        "loss": 2.2795,
        "grad_norm": 1.9358640909194946,
        "learning_rate": 6.096060005740811e-07,
        "epoch": 0.9886727774051718,
        "step": 13267
    },
    {
        "loss": 2.2947,
        "grad_norm": 5.045874118804932,
        "learning_rate": 6.018728250440032e-07,
        "epoch": 0.9887472986064535,
        "step": 13268
    },
    {
        "loss": 1.4917,
        "grad_norm": 5.822444915771484,
        "learning_rate": 5.941888647813864e-07,
        "epoch": 0.9888218198077353,
        "step": 13269
    },
    {
        "loss": 2.1919,
        "grad_norm": 3.8842504024505615,
        "learning_rate": 5.865541235907767e-07,
        "epoch": 0.988896341009017,
        "step": 13270
    },
    {
        "loss": 2.4346,
        "grad_norm": 3.3138487339019775,
        "learning_rate": 5.789686052523724e-07,
        "epoch": 0.9889708622102988,
        "step": 13271
    },
    {
        "loss": 1.918,
        "grad_norm": 2.7813851833343506,
        "learning_rate": 5.714323135220357e-07,
        "epoch": 0.9890453834115805,
        "step": 13272
    },
    {
        "loss": 2.8277,
        "grad_norm": 2.089782238006592,
        "learning_rate": 5.63945252131215e-07,
        "epoch": 0.9891199046128624,
        "step": 13273
    },
    {
        "loss": 2.1672,
        "grad_norm": 2.8586864471435547,
        "learning_rate": 5.565074247870228e-07,
        "epoch": 0.9891944258141441,
        "step": 13274
    },
    {
        "loss": 2.6332,
        "grad_norm": 2.494709014892578,
        "learning_rate": 5.491188351721576e-07,
        "epoch": 0.9892689470154259,
        "step": 13275
    },
    {
        "loss": 2.2225,
        "grad_norm": 3.0318710803985596,
        "learning_rate": 5.417794869449377e-07,
        "epoch": 0.9893434682167076,
        "step": 13276
    },
    {
        "loss": 2.5304,
        "grad_norm": 2.9085001945495605,
        "learning_rate": 5.344893837393228e-07,
        "epoch": 0.9894179894179894,
        "step": 13277
    },
    {
        "loss": 1.9499,
        "grad_norm": 4.980042457580566,
        "learning_rate": 5.272485291649143e-07,
        "epoch": 0.9894925106192712,
        "step": 13278
    },
    {
        "loss": 2.2418,
        "grad_norm": 3.2445807456970215,
        "learning_rate": 5.200569268068444e-07,
        "epoch": 0.989567031820553,
        "step": 13279
    },
    {
        "loss": 1.8494,
        "grad_norm": 3.298133134841919,
        "learning_rate": 5.129145802259538e-07,
        "epoch": 0.9896415530218348,
        "step": 13280
    },
    {
        "loss": 2.2992,
        "grad_norm": 3.7906088829040527,
        "learning_rate": 5.058214929586025e-07,
        "epoch": 0.9897160742231165,
        "step": 13281
    },
    {
        "loss": 2.5519,
        "grad_norm": 3.370270252227783,
        "learning_rate": 4.987776685168699e-07,
        "epoch": 0.9897905954243983,
        "step": 13282
    },
    {
        "loss": 2.4954,
        "grad_norm": 2.076996326446533,
        "learning_rate": 4.91783110388333e-07,
        "epoch": 0.98986511662568,
        "step": 13283
    },
    {
        "loss": 1.896,
        "grad_norm": 4.567753314971924,
        "learning_rate": 4.848378220362659e-07,
        "epoch": 0.9899396378269618,
        "step": 13284
    },
    {
        "loss": 2.9189,
        "grad_norm": 2.7031071186065674,
        "learning_rate": 4.779418068994957e-07,
        "epoch": 0.9900141590282435,
        "step": 13285
    },
    {
        "loss": 2.3434,
        "grad_norm": 2.5610203742980957,
        "learning_rate": 4.710950683924353e-07,
        "epoch": 0.9900886802295253,
        "step": 13286
    },
    {
        "loss": 1.7865,
        "grad_norm": 3.931199789047241,
        "learning_rate": 4.6429760990518436e-07,
        "epoch": 0.990163201430807,
        "step": 13287
    },
    {
        "loss": 2.3525,
        "grad_norm": 2.1668803691864014,
        "learning_rate": 4.5754943480333936e-07,
        "epoch": 0.9902377226320889,
        "step": 13288
    },
    {
        "loss": 2.7507,
        "grad_norm": 3.259700059890747,
        "learning_rate": 4.5085054642817204e-07,
        "epoch": 0.9903122438333706,
        "step": 13289
    },
    {
        "loss": 2.4801,
        "grad_norm": 2.4773077964782715,
        "learning_rate": 4.4420094809650703e-07,
        "epoch": 0.9903867650346524,
        "step": 13290
    },
    {
        "loss": 3.4941,
        "grad_norm": 3.9709789752960205,
        "learning_rate": 4.376006431007662e-07,
        "epoch": 0.9904612862359341,
        "step": 13291
    },
    {
        "loss": 2.5804,
        "grad_norm": 2.1323559284210205,
        "learning_rate": 4.3104963470901314e-07,
        "epoch": 0.9905358074372159,
        "step": 13292
    },
    {
        "loss": 2.7029,
        "grad_norm": 3.0800023078918457,
        "learning_rate": 4.245479261648311e-07,
        "epoch": 0.9906103286384976,
        "step": 13293
    },
    {
        "loss": 2.672,
        "grad_norm": 3.1202635765075684,
        "learning_rate": 4.1809552068744487e-07,
        "epoch": 0.9906848498397794,
        "step": 13294
    },
    {
        "loss": 2.7757,
        "grad_norm": 2.0836920738220215,
        "learning_rate": 4.116924214716544e-07,
        "epoch": 0.9907593710410612,
        "step": 13295
    },
    {
        "loss": 2.4419,
        "grad_norm": 1.8220586776733398,
        "learning_rate": 4.053386316878349e-07,
        "epoch": 0.990833892242343,
        "step": 13296
    },
    {
        "loss": 2.2412,
        "grad_norm": 3.212878942489624,
        "learning_rate": 3.990341544819587e-07,
        "epoch": 0.9909084134436247,
        "step": 13297
    },
    {
        "loss": 1.6187,
        "grad_norm": 5.195916175842285,
        "learning_rate": 3.927789929755954e-07,
        "epoch": 0.9909829346449065,
        "step": 13298
    },
    {
        "loss": 2.4568,
        "grad_norm": 2.712521553039551,
        "learning_rate": 3.865731502658454e-07,
        "epoch": 0.9910574558461882,
        "step": 13299
    },
    {
        "loss": 2.3513,
        "grad_norm": 2.9996392726898193,
        "learning_rate": 3.8041662942546186e-07,
        "epoch": 0.99113197704747,
        "step": 13300
    },
    {
        "loss": 2.2458,
        "grad_norm": 3.3949434757232666,
        "learning_rate": 3.743094335027064e-07,
        "epoch": 0.9912064982487517,
        "step": 13301
    },
    {
        "loss": 1.6474,
        "grad_norm": 3.2110302448272705,
        "learning_rate": 3.682515655214713e-07,
        "epoch": 0.9912810194500336,
        "step": 13302
    },
    {
        "loss": 2.2006,
        "grad_norm": 3.431187152862549,
        "learning_rate": 3.6224302848121283e-07,
        "epoch": 0.9913555406513153,
        "step": 13303
    },
    {
        "loss": 1.9508,
        "grad_norm": 2.430065155029297,
        "learning_rate": 3.5628382535692895e-07,
        "epoch": 0.9914300618525971,
        "step": 13304
    },
    {
        "loss": 2.3842,
        "grad_norm": 1.77530038356781,
        "learning_rate": 3.503739590992372e-07,
        "epoch": 0.9915045830538788,
        "step": 13305
    },
    {
        "loss": 2.5227,
        "grad_norm": 2.3601925373077393,
        "learning_rate": 3.4451343263429693e-07,
        "epoch": 0.9915791042551606,
        "step": 13306
    },
    {
        "loss": 1.7734,
        "grad_norm": 1.9903355836868286,
        "learning_rate": 3.387022488638647e-07,
        "epoch": 0.9916536254564423,
        "step": 13307
    },
    {
        "loss": 3.3639,
        "grad_norm": 2.3282434940338135,
        "learning_rate": 3.3294041066520565e-07,
        "epoch": 0.9917281466577241,
        "step": 13308
    },
    {
        "loss": 2.4174,
        "grad_norm": 2.445739984512329,
        "learning_rate": 3.2722792089123766e-07,
        "epoch": 0.9918026678590058,
        "step": 13309
    },
    {
        "loss": 2.071,
        "grad_norm": 3.0107851028442383,
        "learning_rate": 3.2156478237038714e-07,
        "epoch": 0.9918771890602877,
        "step": 13310
    },
    {
        "loss": 1.4277,
        "grad_norm": 5.6220574378967285,
        "learning_rate": 3.159509979066444e-07,
        "epoch": 0.9919517102615694,
        "step": 13311
    },
    {
        "loss": 2.6729,
        "grad_norm": 3.6672921180725098,
        "learning_rate": 3.1038657027959717e-07,
        "epoch": 0.9920262314628512,
        "step": 13312
    },
    {
        "loss": 2.3212,
        "grad_norm": 2.7712161540985107,
        "learning_rate": 3.048715022443749e-07,
        "epoch": 0.992100752664133,
        "step": 13313
    },
    {
        "loss": 2.6252,
        "grad_norm": 1.8092819452285767,
        "learning_rate": 2.994057965316488e-07,
        "epoch": 0.9921752738654147,
        "step": 13314
    },
    {
        "loss": 2.6902,
        "grad_norm": 2.4154891967773438,
        "learning_rate": 2.9398945584769854e-07,
        "epoch": 0.9922497950666965,
        "step": 13315
    },
    {
        "loss": 2.564,
        "grad_norm": 2.8331105709075928,
        "learning_rate": 2.886224828743123e-07,
        "epoch": 0.9923243162679782,
        "step": 13316
    },
    {
        "loss": 2.3705,
        "grad_norm": 3.036074638366699,
        "learning_rate": 2.8330488026885315e-07,
        "epoch": 0.99239883746926,
        "step": 13317
    },
    {
        "loss": 2.139,
        "grad_norm": 3.0753121376037598,
        "learning_rate": 2.7803665066425953e-07,
        "epoch": 0.9924733586705418,
        "step": 13318
    },
    {
        "loss": 2.7577,
        "grad_norm": 2.169034004211426,
        "learning_rate": 2.728177966689671e-07,
        "epoch": 0.9925478798718236,
        "step": 13319
    },
    {
        "loss": 2.8725,
        "grad_norm": 2.272709369659424,
        "learning_rate": 2.676483208670422e-07,
        "epoch": 0.9926224010731053,
        "step": 13320
    },
    {
        "loss": 1.9496,
        "grad_norm": 4.264737606048584,
        "learning_rate": 2.625282258180373e-07,
        "epoch": 0.9926969222743871,
        "step": 13321
    },
    {
        "loss": 2.748,
        "grad_norm": 2.377406597137451,
        "learning_rate": 2.5745751405706896e-07,
        "epoch": 0.9927714434756688,
        "step": 13322
    },
    {
        "loss": 2.5644,
        "grad_norm": 3.3892927169799805,
        "learning_rate": 2.5243618809485116e-07,
        "epoch": 0.9928459646769506,
        "step": 13323
    },
    {
        "loss": 2.5765,
        "grad_norm": 2.8253610134124756,
        "learning_rate": 2.474642504175728e-07,
        "epoch": 0.9929204858782323,
        "step": 13324
    },
    {
        "loss": 2.6115,
        "grad_norm": 2.9712955951690674,
        "learning_rate": 2.425417034870203e-07,
        "epoch": 0.9929950070795142,
        "step": 13325
    },
    {
        "loss": 2.1511,
        "grad_norm": 11.308663368225098,
        "learning_rate": 2.3766854974049956e-07,
        "epoch": 0.9930695282807959,
        "step": 13326
    },
    {
        "loss": 1.8439,
        "grad_norm": 3.7947587966918945,
        "learning_rate": 2.3284479159086935e-07,
        "epoch": 0.9931440494820777,
        "step": 13327
    },
    {
        "loss": 2.3442,
        "grad_norm": 1.94786536693573,
        "learning_rate": 2.2807043142653027e-07,
        "epoch": 0.9932185706833594,
        "step": 13328
    },
    {
        "loss": 1.3688,
        "grad_norm": 3.134269952774048,
        "learning_rate": 2.2334547161143582e-07,
        "epoch": 0.9932930918846412,
        "step": 13329
    },
    {
        "loss": 3.0133,
        "grad_norm": 1.8612167835235596,
        "learning_rate": 2.1866991448505902e-07,
        "epoch": 0.9933676130859229,
        "step": 13330
    },
    {
        "loss": 2.3969,
        "grad_norm": 3.055931806564331,
        "learning_rate": 2.1404376236241474e-07,
        "epoch": 0.9934421342872047,
        "step": 13331
    },
    {
        "loss": 1.772,
        "grad_norm": 2.856539726257324,
        "learning_rate": 2.094670175340596e-07,
        "epoch": 0.9935166554884864,
        "step": 13332
    },
    {
        "loss": 2.1263,
        "grad_norm": 2.9557645320892334,
        "learning_rate": 2.0493968226611427e-07,
        "epoch": 0.9935911766897683,
        "step": 13333
    },
    {
        "loss": 1.7835,
        "grad_norm": 3.5220835208892822,
        "learning_rate": 2.0046175880018557e-07,
        "epoch": 0.99366569789105,
        "step": 13334
    },
    {
        "loss": 2.5571,
        "grad_norm": 2.0219955444335938,
        "learning_rate": 1.9603324935345557e-07,
        "epoch": 0.9937402190923318,
        "step": 13335
    },
    {
        "loss": 3.0155,
        "grad_norm": 2.357017993927002,
        "learning_rate": 1.916541561186147e-07,
        "epoch": 0.9938147402936135,
        "step": 13336
    },
    {
        "loss": 2.0451,
        "grad_norm": 2.0981459617614746,
        "learning_rate": 1.8732448126389525e-07,
        "epoch": 0.9938892614948953,
        "step": 13337
    },
    {
        "loss": 2.2255,
        "grad_norm": 3.857579469680786,
        "learning_rate": 1.8304422693308232e-07,
        "epoch": 0.993963782696177,
        "step": 13338
    },
    {
        "loss": 2.0578,
        "grad_norm": 3.512763500213623,
        "learning_rate": 1.7881339524543627e-07,
        "epoch": 0.9940383038974588,
        "step": 13339
    },
    {
        "loss": 2.0174,
        "grad_norm": 3.9653334617614746,
        "learning_rate": 1.7463198829580363e-07,
        "epoch": 0.9941128250987405,
        "step": 13340
    },
    {
        "loss": 2.2462,
        "grad_norm": 2.2435755729675293,
        "learning_rate": 1.7050000815452827e-07,
        "epoch": 0.9941873463000224,
        "step": 13341
    },
    {
        "loss": 2.7699,
        "grad_norm": 2.735445499420166,
        "learning_rate": 1.6641745686748478e-07,
        "epoch": 0.9942618675013041,
        "step": 13342
    },
    {
        "loss": 2.255,
        "grad_norm": 3.8195996284484863,
        "learning_rate": 1.6238433645608953e-07,
        "epoch": 0.9943363887025859,
        "step": 13343
    },
    {
        "loss": 2.4305,
        "grad_norm": 3.384976387023926,
        "learning_rate": 1.5840064891725625e-07,
        "epoch": 0.9944109099038676,
        "step": 13344
    },
    {
        "loss": 2.228,
        "grad_norm": 3.3003811836242676,
        "learning_rate": 1.544663962234627e-07,
        "epoch": 0.9944854311051494,
        "step": 13345
    },
    {
        "loss": 2.2816,
        "grad_norm": 2.4275152683258057,
        "learning_rate": 1.505815803226618e-07,
        "epoch": 0.9945599523064311,
        "step": 13346
    },
    {
        "loss": 1.808,
        "grad_norm": 2.6096529960632324,
        "learning_rate": 1.467462031383704e-07,
        "epoch": 0.994634473507713,
        "step": 13347
    },
    {
        "loss": 2.1714,
        "grad_norm": 2.968156576156616,
        "learning_rate": 1.4296026656959172e-07,
        "epoch": 0.9947089947089947,
        "step": 13348
    },
    {
        "loss": 2.3029,
        "grad_norm": 2.9982099533081055,
        "learning_rate": 1.392237724908929e-07,
        "epoch": 0.9947835159102765,
        "step": 13349
    },
    {
        "loss": 1.6856,
        "grad_norm": 3.4396135807037354,
        "learning_rate": 1.3553672275230523e-07,
        "epoch": 0.9948580371115583,
        "step": 13350
    },
    {
        "loss": 2.0281,
        "grad_norm": 6.092769145965576,
        "learning_rate": 1.3189911917943497e-07,
        "epoch": 0.99493255831284,
        "step": 13351
    },
    {
        "loss": 1.9319,
        "grad_norm": 5.2141852378845215,
        "learning_rate": 1.283109635733526e-07,
        "epoch": 0.9950070795141218,
        "step": 13352
    },
    {
        "loss": 2.0539,
        "grad_norm": 2.5459771156311035,
        "learning_rate": 1.2477225771069247e-07,
        "epoch": 0.9950816007154035,
        "step": 13353
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.3295845985412598,
        "learning_rate": 1.2128300334357522e-07,
        "epoch": 0.9951561219166853,
        "step": 13354
    },
    {
        "loss": 2.3371,
        "grad_norm": 3.313150644302368,
        "learning_rate": 1.1784320219963007e-07,
        "epoch": 0.995230643117967,
        "step": 13355
    },
    {
        "loss": 2.3963,
        "grad_norm": 3.029043197631836,
        "learning_rate": 1.1445285598205013e-07,
        "epoch": 0.9953051643192489,
        "step": 13356
    },
    {
        "loss": 1.8762,
        "grad_norm": 1.6482539176940918,
        "learning_rate": 1.1111196636947042e-07,
        "epoch": 0.9953796855205306,
        "step": 13357
    },
    {
        "loss": 2.5548,
        "grad_norm": 6.13157844543457,
        "learning_rate": 1.078205350161121e-07,
        "epoch": 0.9954542067218124,
        "step": 13358
    },
    {
        "loss": 2.0086,
        "grad_norm": 3.0315282344818115,
        "learning_rate": 1.045785635516272e-07,
        "epoch": 0.9955287279230941,
        "step": 13359
    },
    {
        "loss": 1.7114,
        "grad_norm": 2.970703363418579,
        "learning_rate": 1.0138605358126496e-07,
        "epoch": 0.9956032491243759,
        "step": 13360
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.7303186655044556,
        "learning_rate": 9.824300668571651e-08,
        "epoch": 0.9956777703256576,
        "step": 13361
    },
    {
        "loss": 2.3343,
        "grad_norm": 2.327800989151001,
        "learning_rate": 9.514942442120367e-08,
        "epoch": 0.9957522915269394,
        "step": 13362
    },
    {
        "loss": 2.4805,
        "grad_norm": 1.9454963207244873,
        "learning_rate": 9.210530831946784e-08,
        "epoch": 0.9958268127282212,
        "step": 13363
    },
    {
        "loss": 2.5332,
        "grad_norm": 2.1883816719055176,
        "learning_rate": 8.91106598877589e-08,
        "epoch": 0.995901333929503,
        "step": 13364
    },
    {
        "loss": 2.6984,
        "grad_norm": 2.2380878925323486,
        "learning_rate": 8.616548060881302e-08,
        "epoch": 0.9959758551307847,
        "step": 13365
    },
    {
        "loss": 2.5648,
        "grad_norm": 3.8846547603607178,
        "learning_rate": 8.326977194089703e-08,
        "epoch": 0.9960503763320665,
        "step": 13366
    },
    {
        "loss": 2.8374,
        "grad_norm": 3.0605664253234863,
        "learning_rate": 8.042353531776403e-08,
        "epoch": 0.9961248975333482,
        "step": 13367
    },
    {
        "loss": 2.0493,
        "grad_norm": 1.8991118669509888,
        "learning_rate": 7.762677214866454e-08,
        "epoch": 0.99619941873463,
        "step": 13368
    },
    {
        "loss": 2.1777,
        "grad_norm": 3.155290126800537,
        "learning_rate": 7.487948381840193e-08,
        "epoch": 0.9962739399359117,
        "step": 13369
    },
    {
        "loss": 2.4691,
        "grad_norm": 3.283010959625244,
        "learning_rate": 7.218167168722146e-08,
        "epoch": 0.9963484611371936,
        "step": 13370
    },
    {
        "loss": 2.4097,
        "grad_norm": 2.9287211894989014,
        "learning_rate": 6.953333709089905e-08,
        "epoch": 0.9964229823384753,
        "step": 13371
    },
    {
        "loss": 2.6651,
        "grad_norm": 2.2088520526885986,
        "learning_rate": 6.693448134071912e-08,
        "epoch": 0.9964975035397571,
        "step": 13372
    },
    {
        "loss": 2.2537,
        "grad_norm": 2.214060068130493,
        "learning_rate": 6.438510572346345e-08,
        "epoch": 0.9965720247410388,
        "step": 13373
    },
    {
        "loss": 2.5051,
        "grad_norm": 3.213627815246582,
        "learning_rate": 6.188521150141125e-08,
        "epoch": 0.9966465459423206,
        "step": 13374
    },
    {
        "loss": 1.7382,
        "grad_norm": 2.562682867050171,
        "learning_rate": 5.943479991232792e-08,
        "epoch": 0.9967210671436023,
        "step": 13375
    },
    {
        "loss": 1.4748,
        "grad_norm": 3.751148223876953,
        "learning_rate": 5.7033872169509615e-08,
        "epoch": 0.9967955883448841,
        "step": 13376
    },
    {
        "loss": 1.0304,
        "grad_norm": 2.298786163330078,
        "learning_rate": 5.4682429461716535e-08,
        "epoch": 0.9968701095461658,
        "step": 13377
    },
    {
        "loss": 2.2716,
        "grad_norm": 3.998670816421509,
        "learning_rate": 5.238047295325066e-08,
        "epoch": 0.9969446307474477,
        "step": 13378
    },
    {
        "loss": 2.2357,
        "grad_norm": 3.6272149085998535,
        "learning_rate": 5.012800378386695e-08,
        "epoch": 0.9970191519487294,
        "step": 13379
    },
    {
        "loss": 1.5988,
        "grad_norm": 3.1848840713500977,
        "learning_rate": 4.792502306883995e-08,
        "epoch": 0.9970936731500112,
        "step": 13380
    },
    {
        "loss": 2.4345,
        "grad_norm": 1.976440191268921,
        "learning_rate": 4.5771531898930465e-08,
        "epoch": 0.9971681943512929,
        "step": 13381
    },
    {
        "loss": 1.2893,
        "grad_norm": 3.9406633377075195,
        "learning_rate": 4.36675313404189e-08,
        "epoch": 0.9972427155525747,
        "step": 13382
    },
    {
        "loss": 2.6591,
        "grad_norm": 3.0635440349578857,
        "learning_rate": 4.161302243504972e-08,
        "epoch": 0.9973172367538564,
        "step": 13383
    },
    {
        "loss": 2.6192,
        "grad_norm": 1.726000189781189,
        "learning_rate": 3.960800620009808e-08,
        "epoch": 0.9973917579551382,
        "step": 13384
    },
    {
        "loss": 2.8737,
        "grad_norm": 2.744094133377075,
        "learning_rate": 3.7652483628281e-08,
        "epoch": 0.99746627915642,
        "step": 13385
    },
    {
        "loss": 2.797,
        "grad_norm": 2.6055397987365723,
        "learning_rate": 3.5746455687868386e-08,
        "epoch": 0.9975408003577018,
        "step": 13386
    },
    {
        "loss": 2.508,
        "grad_norm": 2.0753278732299805,
        "learning_rate": 3.388992332259422e-08,
        "epoch": 0.9976153215589836,
        "step": 13387
    },
    {
        "loss": 2.7844,
        "grad_norm": 3.0987329483032227,
        "learning_rate": 3.208288745166765e-08,
        "epoch": 0.9976898427602653,
        "step": 13388
    },
    {
        "loss": 2.6528,
        "grad_norm": 3.2519636154174805,
        "learning_rate": 3.032534896983963e-08,
        "epoch": 0.9977643639615471,
        "step": 13389
    },
    {
        "loss": 2.255,
        "grad_norm": 3.395836591720581,
        "learning_rate": 2.8617308747314054e-08,
        "epoch": 0.9978388851628288,
        "step": 13390
    },
    {
        "loss": 2.1872,
        "grad_norm": 2.9523534774780273,
        "learning_rate": 2.6958767629792215e-08,
        "epoch": 0.9979134063641106,
        "step": 13391
    },
    {
        "loss": 2.1353,
        "grad_norm": 3.329028367996216,
        "learning_rate": 2.5349726438472777e-08,
        "epoch": 0.9979879275653923,
        "step": 13392
    },
    {
        "loss": 1.0125,
        "grad_norm": 3.0308234691619873,
        "learning_rate": 2.379018597004068e-08,
        "epoch": 0.9980624487666742,
        "step": 13393
    },
    {
        "loss": 3.1129,
        "grad_norm": 3.2367448806762695,
        "learning_rate": 2.2280146996689344e-08,
        "epoch": 0.9981369699679559,
        "step": 13394
    },
    {
        "loss": 1.4609,
        "grad_norm": 3.678208827972412,
        "learning_rate": 2.081961026607626e-08,
        "epoch": 0.9982114911692377,
        "step": 13395
    },
    {
        "loss": 2.3529,
        "grad_norm": 2.3180110454559326,
        "learning_rate": 1.9408576501378507e-08,
        "epoch": 0.9982860123705194,
        "step": 13396
    },
    {
        "loss": 2.3384,
        "grad_norm": 4.915504455566406,
        "learning_rate": 1.804704640122612e-08,
        "epoch": 0.9983605335718012,
        "step": 13397
    },
    {
        "loss": 1.6171,
        "grad_norm": 3.231203556060791,
        "learning_rate": 1.6735020639757626e-08,
        "epoch": 0.9984350547730829,
        "step": 13398
    },
    {
        "loss": 3.0836,
        "grad_norm": 1.958600401878357,
        "learning_rate": 1.5472499866608926e-08,
        "epoch": 0.9985095759743647,
        "step": 13399
    },
    {
        "loss": 2.271,
        "grad_norm": 2.072740316390991,
        "learning_rate": 1.4259484706902193e-08,
        "epoch": 0.9985840971756464,
        "step": 13400
    },
    {
        "loss": 1.994,
        "grad_norm": 3.1084682941436768,
        "learning_rate": 1.3095975761223678e-08,
        "epoch": 0.9986586183769283,
        "step": 13401
    },
    {
        "loss": 2.1757,
        "grad_norm": 5.606716632843018,
        "learning_rate": 1.1981973605668107e-08,
        "epoch": 0.99873313957821,
        "step": 13402
    },
    {
        "loss": 2.2336,
        "grad_norm": 2.5001285076141357,
        "learning_rate": 1.0917478791816482e-08,
        "epoch": 0.9988076607794918,
        "step": 13403
    },
    {
        "loss": 1.6051,
        "grad_norm": 3.6450066566467285,
        "learning_rate": 9.902491846747185e-09,
        "epoch": 0.9988821819807735,
        "step": 13404
    },
    {
        "loss": 2.4418,
        "grad_norm": 3.184798240661621,
        "learning_rate": 8.93701327300267e-09,
        "epoch": 0.9989567031820553,
        "step": 13405
    },
    {
        "loss": 1.803,
        "grad_norm": 8.1272554397583,
        "learning_rate": 8.021043548611663e-09,
        "epoch": 0.999031224383337,
        "step": 13406
    },
    {
        "loss": 2.7162,
        "grad_norm": 2.4705910682678223,
        "learning_rate": 7.154583127122472e-09,
        "epoch": 0.9991057455846188,
        "step": 13407
    },
    {
        "loss": 3.0137,
        "grad_norm": 2.266202449798584,
        "learning_rate": 6.337632437536378e-09,
        "epoch": 0.9991802667859006,
        "step": 13408
    },
    {
        "loss": 1.827,
        "grad_norm": 2.612022638320923,
        "learning_rate": 5.570191884363141e-09,
        "epoch": 0.9992547879871824,
        "step": 13409
    },
    {
        "loss": 2.0651,
        "grad_norm": 4.0675740242004395,
        "learning_rate": 4.852261847565487e-09,
        "epoch": 0.9993293091884641,
        "step": 13410
    },
    {
        "loss": 2.0831,
        "grad_norm": 2.9713246822357178,
        "learning_rate": 4.1838426826368295e-09,
        "epoch": 0.9994038303897459,
        "step": 13411
    },
    {
        "loss": 2.0593,
        "grad_norm": 3.253587007522583,
        "learning_rate": 3.5649347205235494e-09,
        "epoch": 0.9994783515910276,
        "step": 13412
    },
    {
        "loss": 2.487,
        "grad_norm": 3.3146748542785645,
        "learning_rate": 2.9955382676694068e-09,
        "epoch": 0.9995528727923094,
        "step": 13413
    },
    {
        "loss": 1.9163,
        "grad_norm": 3.5633575916290283,
        "learning_rate": 2.475653605993333e-09,
        "epoch": 0.9996273939935911,
        "step": 13414
    },
    {
        "loss": 2.3008,
        "grad_norm": 2.2233059406280518,
        "learning_rate": 2.00528099291164e-09,
        "epoch": 0.999701915194873,
        "step": 13415
    },
    {
        "loss": 2.3049,
        "grad_norm": 4.516252517700195,
        "learning_rate": 1.584420661326913e-09,
        "epoch": 0.9997764363961547,
        "step": 13416
    },
    {
        "loss": 2.4456,
        "grad_norm": 3.1336774826049805,
        "learning_rate": 1.2130728196169118e-09,
        "epoch": 0.9998509575974365,
        "step": 13417
    },
    {
        "loss": 1.6832,
        "grad_norm": 3.9133529663085938,
        "learning_rate": 8.912376516567733e-10,
        "epoch": 0.9999254787987182,
        "step": 13418
    },
    {
        "loss": 3.1466,
        "grad_norm": 4.129668712615967,
        "learning_rate": 6.189153167746042e-10,
        "epoch": 1.0,
        "step": 13419
    },
    {
        "train_runtime": 12855.4864,
        "train_samples_per_second": 2.088,
        "train_steps_per_second": 1.044,
        "total_flos": 1.43393022033408e+17,
        "train_loss": 2.334042734876458,
        "epoch": 1.0,
        "step": 13419
    }
]