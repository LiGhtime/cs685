[
    {
        "loss": 3.4823,
        "grad_norm": 1.9617375135421753,
        "learning_rate": 6.666666666666667e-06,
        "epoch": 0.0001288992008249549,
        "step": 1
    },
    {
        "loss": 4.0313,
        "grad_norm": 3.204146146774292,
        "learning_rate": 1.3333333333333333e-05,
        "epoch": 0.0002577984016499098,
        "step": 2
    },
    {
        "loss": 3.8915,
        "grad_norm": 2.5568368434906006,
        "learning_rate": 2e-05,
        "epoch": 0.0003866976024748647,
        "step": 3
    },
    {
        "loss": 4.3504,
        "grad_norm": 3.7487893104553223,
        "learning_rate": 2.6666666666666667e-05,
        "epoch": 0.0005155968032998196,
        "step": 4
    },
    {
        "loss": 4.0402,
        "grad_norm": 2.4379467964172363,
        "learning_rate": 3.3333333333333335e-05,
        "epoch": 0.0006444960041247745,
        "step": 5
    },
    {
        "loss": 3.6329,
        "grad_norm": 2.207249879837036,
        "learning_rate": 4e-05,
        "epoch": 0.0007733952049497294,
        "step": 6
    },
    {
        "loss": 4.2022,
        "grad_norm": 3.318112850189209,
        "learning_rate": 4.666666666666667e-05,
        "epoch": 0.0009022944057746842,
        "step": 7
    },
    {
        "loss": 3.8903,
        "grad_norm": 3.6431221961975098,
        "learning_rate": 5.333333333333333e-05,
        "epoch": 0.0010311936065996391,
        "step": 8
    },
    {
        "loss": 3.4092,
        "grad_norm": 2.343822717666626,
        "learning_rate": 6e-05,
        "epoch": 0.001160092807424594,
        "step": 9
    },
    {
        "loss": 3.8173,
        "grad_norm": 2.804757595062256,
        "learning_rate": 6.666666666666667e-05,
        "epoch": 0.001288992008249549,
        "step": 10
    },
    {
        "loss": 3.1503,
        "grad_norm": 1.934587836265564,
        "learning_rate": 7.333333333333333e-05,
        "epoch": 0.0014178912090745037,
        "step": 11
    },
    {
        "loss": 3.6041,
        "grad_norm": 2.400517463684082,
        "learning_rate": 8e-05,
        "epoch": 0.0015467904098994587,
        "step": 12
    },
    {
        "loss": 3.5708,
        "grad_norm": 1.555566430091858,
        "learning_rate": 8.666666666666667e-05,
        "epoch": 0.0016756896107244135,
        "step": 13
    },
    {
        "loss": 3.227,
        "grad_norm": 1.988276481628418,
        "learning_rate": 9.333333333333334e-05,
        "epoch": 0.0018045888115493685,
        "step": 14
    },
    {
        "loss": 3.3499,
        "grad_norm": 1.8052036762237549,
        "learning_rate": 0.0001,
        "epoch": 0.0019334880123743233,
        "step": 15
    },
    {
        "loss": 3.3435,
        "grad_norm": 2.4414541721343994,
        "learning_rate": 9.951340343707852e-05,
        "epoch": 0.0020623872131992783,
        "step": 16
    },
    {
        "loss": 2.918,
        "grad_norm": 2.215853452682495,
        "learning_rate": 9.806308479691595e-05,
        "epoch": 0.002191286414024233,
        "step": 17
    },
    {
        "loss": 3.2352,
        "grad_norm": 4.731757640838623,
        "learning_rate": 9.567727288213005e-05,
        "epoch": 0.002320185614849188,
        "step": 18
    },
    {
        "loss": 3.6015,
        "grad_norm": 2.9749162197113037,
        "learning_rate": 9.24024048078213e-05,
        "epoch": 0.0024490848156741426,
        "step": 19
    },
    {
        "loss": 2.7969,
        "grad_norm": 2.3672657012939453,
        "learning_rate": 8.83022221559489e-05,
        "epoch": 0.002577984016499098,
        "step": 20
    },
    {
        "loss": 3.0607,
        "grad_norm": 2.0784413814544678,
        "learning_rate": 8.345653031794292e-05,
        "epoch": 0.0027068832173240526,
        "step": 21
    },
    {
        "loss": 3.2945,
        "grad_norm": 1.3625478744506836,
        "learning_rate": 7.795964517353735e-05,
        "epoch": 0.0028357824181490074,
        "step": 22
    },
    {
        "loss": 2.767,
        "grad_norm": 1.7837673425674438,
        "learning_rate": 7.191855733945387e-05,
        "epoch": 0.002964681618973962,
        "step": 23
    },
    {
        "loss": 2.9237,
        "grad_norm": 1.5471205711364746,
        "learning_rate": 6.545084971874738e-05,
        "epoch": 0.0030935808197989174,
        "step": 24
    },
    {
        "loss": 2.5285,
        "grad_norm": 1.7228271961212158,
        "learning_rate": 5.868240888334653e-05,
        "epoch": 0.003222480020623872,
        "step": 25
    },
    {
        "loss": 2.4364,
        "grad_norm": 1.3180358409881592,
        "learning_rate": 5.174497483512506e-05,
        "epoch": 0.003351379221448827,
        "step": 26
    },
    {
        "loss": 3.0843,
        "grad_norm": 1.491814374923706,
        "learning_rate": 4.477357683661734e-05,
        "epoch": 0.0034802784222737818,
        "step": 27
    },
    {
        "loss": 2.6466,
        "grad_norm": 2.6938047409057617,
        "learning_rate": 3.790390522001662e-05,
        "epoch": 0.003609177623098737,
        "step": 28
    },
    {
        "loss": 2.6731,
        "grad_norm": 1.148079514503479,
        "learning_rate": 3.12696703292044e-05,
        "epoch": 0.0037380768239236918,
        "step": 29
    },
    {
        "loss": 2.8201,
        "grad_norm": 1.3087565898895264,
        "learning_rate": 2.500000000000001e-05,
        "epoch": 0.0038669760247486465,
        "step": 30
    },
    {
        "loss": 2.7079,
        "grad_norm": 1.3508853912353516,
        "learning_rate": 1.9216926233717085e-05,
        "epoch": 0.003995875225573602,
        "step": 31
    },
    {
        "loss": 2.6323,
        "grad_norm": 1.8391640186309814,
        "learning_rate": 1.4033009983067452e-05,
        "epoch": 0.0041247744263985565,
        "step": 32
    },
    {
        "loss": 2.9381,
        "grad_norm": 1.3376376628875732,
        "learning_rate": 9.549150281252633e-06,
        "epoch": 0.004253673627223511,
        "step": 33
    },
    {
        "loss": 3.0204,
        "grad_norm": 1.1757529973983765,
        "learning_rate": 5.852620357053651e-06,
        "epoch": 0.004382572828048466,
        "step": 34
    },
    {
        "loss": 2.9714,
        "grad_norm": 1.5370445251464844,
        "learning_rate": 3.0153689607045845e-06,
        "epoch": 0.004511472028873421,
        "step": 35
    },
    {
        "loss": 2.1034,
        "grad_norm": 3.8314452171325684,
        "learning_rate": 1.0926199633097157e-06,
        "epoch": 0.004640371229698376,
        "step": 36
    },
    {
        "loss": 2.4093,
        "grad_norm": 1.3809324502944946,
        "learning_rate": 1.2179748700879012e-07,
        "epoch": 0.0047692704305233305,
        "step": 37
    },
    {
        "loss": 3.0526,
        "grad_norm": 1.5328214168548584,
        "learning_rate": 9.987820251299122e-05,
        "epoch": 0.004898169631348285,
        "step": 38
    },
    {
        "loss": 2.6022,
        "grad_norm": 1.4064735174179077,
        "learning_rate": 9.890738003669029e-05,
        "epoch": 0.005027068832173241,
        "step": 39
    },
    {
        "loss": 2.6346,
        "grad_norm": 2.0686862468719482,
        "learning_rate": 9.698463103929542e-05,
        "epoch": 0.005155968032998196,
        "step": 40
    },
    {
        "loss": 2.6925,
        "grad_norm": 1.4972493648529053,
        "learning_rate": 9.414737964294636e-05,
        "epoch": 0.0052848672338231505,
        "step": 41
    },
    {
        "loss": 2.8972,
        "grad_norm": 1.4399713277816772,
        "learning_rate": 9.045084971874738e-05,
        "epoch": 0.005413766434648105,
        "step": 42
    },
    {
        "loss": 2.7629,
        "grad_norm": 1.586390495300293,
        "learning_rate": 8.596699001693255e-05,
        "epoch": 0.00554266563547306,
        "step": 43
    },
    {
        "loss": 2.5886,
        "grad_norm": 2.8944132328033447,
        "learning_rate": 8.07830737662829e-05,
        "epoch": 0.005671564836298015,
        "step": 44
    },
    {
        "loss": 2.8496,
        "grad_norm": 1.199223518371582,
        "learning_rate": 7.500000000000002e-05,
        "epoch": 0.00580046403712297,
        "step": 45
    },
    {
        "loss": 2.6413,
        "grad_norm": 1.985006332397461,
        "learning_rate": 6.873032967079561e-05,
        "epoch": 0.005929363237947924,
        "step": 46
    },
    {
        "loss": 2.9153,
        "grad_norm": 1.3378537893295288,
        "learning_rate": 6.209609477998338e-05,
        "epoch": 0.00605826243877288,
        "step": 47
    },
    {
        "loss": 2.4392,
        "grad_norm": 2.277683734893799,
        "learning_rate": 5.52264231633827e-05,
        "epoch": 0.006187161639597835,
        "step": 48
    },
    {
        "loss": 2.7161,
        "grad_norm": 1.2837814092636108,
        "learning_rate": 4.825502516487497e-05,
        "epoch": 0.00631606084042279,
        "step": 49
    },
    {
        "loss": 1.9557,
        "grad_norm": 1.809348464012146,
        "learning_rate": 4.131759111665349e-05,
        "epoch": 0.006444960041247744,
        "step": 50
    },
    {
        "loss": 1.9937,
        "grad_norm": 2.155961513519287,
        "learning_rate": 3.454915028125262e-05,
        "epoch": 0.006573859242072699,
        "step": 51
    },
    {
        "loss": 2.779,
        "grad_norm": 1.509371280670166,
        "learning_rate": 2.8081442660546142e-05,
        "epoch": 0.006702758442897654,
        "step": 52
    },
    {
        "loss": 2.6764,
        "grad_norm": 1.9625684022903442,
        "learning_rate": 2.2040354826462668e-05,
        "epoch": 0.006831657643722609,
        "step": 53
    },
    {
        "loss": 2.3491,
        "grad_norm": 2.419404983520508,
        "learning_rate": 1.654346968205709e-05,
        "epoch": 0.0069605568445475635,
        "step": 54
    },
    {
        "loss": 2.7259,
        "grad_norm": 1.604870319366455,
        "learning_rate": 1.1697777844051105e-05,
        "epoch": 0.007089456045372518,
        "step": 55
    },
    {
        "loss": 2.8492,
        "grad_norm": 1.255843997001648,
        "learning_rate": 7.597595192178702e-06,
        "epoch": 0.007218355246197474,
        "step": 56
    },
    {
        "loss": 2.1273,
        "grad_norm": 2.379136323928833,
        "learning_rate": 4.322727117869951e-06,
        "epoch": 0.007347254447022429,
        "step": 57
    },
    {
        "loss": 1.875,
        "grad_norm": 2.6899354457855225,
        "learning_rate": 1.93691520308405e-06,
        "epoch": 0.0074761536478473835,
        "step": 58
    },
    {
        "loss": 2.1459,
        "grad_norm": 2.6425693035125732,
        "learning_rate": 4.865965629214875e-07,
        "epoch": 0.007605052848672338,
        "step": 59
    },
    {
        "loss": 2.4519,
        "grad_norm": 1.5762133598327637,
        "learning_rate": 0.0,
        "epoch": 0.007733952049497293,
        "step": 60
    },
    {
        "train_runtime": 148.9093,
        "train_samples_per_second": 0.806,
        "train_steps_per_second": 0.403,
        "total_flos": 2210397586882560.0,
        "train_loss": 2.9547263105710346,
        "epoch": 0.007733952049497293,
        "step": 60
    }
]