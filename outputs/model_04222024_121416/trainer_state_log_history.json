[
    {
        "loss": 3.3952,
        "grad_norm": 2.012531280517578,
        "learning_rate": 1.3333333333333333e-05,
        "epoch": 0.00012858428700012858,
        "step": 1
    },
    {
        "loss": 4.0747,
        "grad_norm": 3.3594727516174316,
        "learning_rate": 2.6666666666666667e-05,
        "epoch": 0.00025716857400025716,
        "step": 2
    },
    {
        "loss": 4.3265,
        "grad_norm": 3.8194384574890137,
        "learning_rate": 4e-05,
        "epoch": 0.00038575286100038574,
        "step": 3
    },
    {
        "loss": 4.0461,
        "grad_norm": 2.7639272212982178,
        "learning_rate": 5.333333333333333e-05,
        "epoch": 0.0005143371480005143,
        "step": 4
    },
    {
        "loss": 3.4693,
        "grad_norm": 2.5845935344696045,
        "learning_rate": 6.666666666666667e-05,
        "epoch": 0.0006429214350006429,
        "step": 5
    },
    {
        "loss": 3.7175,
        "grad_norm": 2.903730869293213,
        "learning_rate": 8e-05,
        "epoch": 0.0007715057220007715,
        "step": 6
    },
    {
        "loss": 3.8823,
        "grad_norm": 2.47450852394104,
        "learning_rate": 9.333333333333334e-05,
        "epoch": 0.0009000900090009,
        "step": 7
    },
    {
        "loss": 3.2823,
        "grad_norm": 2.9983608722686768,
        "learning_rate": 0.00010666666666666667,
        "epoch": 0.0010286742960010286,
        "step": 8
    },
    {
        "loss": 3.1383,
        "grad_norm": 2.467730760574341,
        "learning_rate": 0.00012,
        "epoch": 0.0011572585830011573,
        "step": 9
    },
    {
        "loss": 3.2341,
        "grad_norm": 1.6206737756729126,
        "learning_rate": 0.00013333333333333334,
        "epoch": 0.0012858428700012858,
        "step": 10
    },
    {
        "loss": 3.0702,
        "grad_norm": 2.3068008422851562,
        "learning_rate": 0.00014666666666666666,
        "epoch": 0.0014144271570014145,
        "step": 11
    },
    {
        "loss": 3.2083,
        "grad_norm": 1.98676335811615,
        "learning_rate": 0.00016,
        "epoch": 0.001543011444001543,
        "step": 12
    },
    {
        "loss": 2.8357,
        "grad_norm": 2.9425582885742188,
        "learning_rate": 0.00017333333333333334,
        "epoch": 0.0016715957310016716,
        "step": 13
    },
    {
        "loss": 2.8976,
        "grad_norm": 4.4145965576171875,
        "learning_rate": 0.0001866666666666667,
        "epoch": 0.0018001800180018,
        "step": 14
    },
    {
        "loss": 3.0489,
        "grad_norm": 2.811729907989502,
        "learning_rate": 0.0002,
        "epoch": 0.0019287643050019288,
        "step": 15
    },
    {
        "loss": 3.0126,
        "grad_norm": 2.3464739322662354,
        "learning_rate": 0.00019999999180927272,
        "epoch": 0.0020573485920020573,
        "step": 16
    },
    {
        "loss": 2.8929,
        "grad_norm": 2.7930073738098145,
        "learning_rate": 0.00019999996723709214,
        "epoch": 0.002185932879002186,
        "step": 17
    },
    {
        "loss": 2.5119,
        "grad_norm": 1.4490352869033813,
        "learning_rate": 0.00019999992628346233,
        "epoch": 0.0023145171660023146,
        "step": 18
    },
    {
        "loss": 2.4774,
        "grad_norm": 1.870928406715393,
        "learning_rate": 0.00019999986894839,
        "epoch": 0.002443101453002443,
        "step": 19
    },
    {
        "loss": 2.8091,
        "grad_norm": 2.1993885040283203,
        "learning_rate": 0.00019999979523188454,
        "epoch": 0.0025716857400025716,
        "step": 20
    },
    {
        "loss": 2.9279,
        "grad_norm": 1.9664274454116821,
        "learning_rate": 0.00019999970513395803,
        "epoch": 0.0027002700270027003,
        "step": 21
    },
    {
        "loss": 2.5358,
        "grad_norm": 1.692348599433899,
        "learning_rate": 0.00019999959865462522,
        "epoch": 0.002828854314002829,
        "step": 22
    },
    {
        "loss": 2.2506,
        "grad_norm": 1.9914472103118896,
        "learning_rate": 0.0001999994757939035,
        "epoch": 0.0029574386010029576,
        "step": 23
    },
    {
        "loss": 2.7939,
        "grad_norm": 1.2604303359985352,
        "learning_rate": 0.00019999933655181308,
        "epoch": 0.003086022888003086,
        "step": 24
    },
    {
        "loss": 2.3093,
        "grad_norm": 1.9733800888061523,
        "learning_rate": 0.00019999918092837677,
        "epoch": 0.0032146071750032146,
        "step": 25
    },
    {
        "loss": 2.6249,
        "grad_norm": 1.8180614709854126,
        "learning_rate": 0.00019999900892362,
        "epoch": 0.0033431914620033433,
        "step": 26
    },
    {
        "loss": 2.4171,
        "grad_norm": 1.4196295738220215,
        "learning_rate": 0.000199998820537571,
        "epoch": 0.003471775749003472,
        "step": 27
    },
    {
        "loss": 1.8976,
        "grad_norm": 2.442941665649414,
        "learning_rate": 0.00019999861577026058,
        "epoch": 0.0036003600360036,
        "step": 28
    },
    {
        "loss": 2.793,
        "grad_norm": 2.0285239219665527,
        "learning_rate": 0.00019999839462172232,
        "epoch": 0.003728944323003729,
        "step": 29
    },
    {
        "loss": 1.8636,
        "grad_norm": 2.0691676139831543,
        "learning_rate": 0.00019999815709199247,
        "epoch": 0.0038575286100038576,
        "step": 30
    },
    {
        "loss": 2.0874,
        "grad_norm": 2.2312052249908447,
        "learning_rate": 0.00019999790318110986,
        "epoch": 0.003986112897003986,
        "step": 31
    },
    {
        "loss": 2.8415,
        "grad_norm": 1.6545689105987549,
        "learning_rate": 0.00019999763288911616,
        "epoch": 0.0041146971840041145,
        "step": 32
    },
    {
        "loss": 2.772,
        "grad_norm": 1.1088366508483887,
        "learning_rate": 0.00019999734621605562,
        "epoch": 0.004243281471004243,
        "step": 33
    },
    {
        "loss": 1.9798,
        "grad_norm": 2.1321489810943604,
        "learning_rate": 0.00019999704316197523,
        "epoch": 0.004371865758004372,
        "step": 34
    },
    {
        "loss": 2.373,
        "grad_norm": 1.7379647493362427,
        "learning_rate": 0.00019999672372692458,
        "epoch": 0.004500450045004501,
        "step": 35
    },
    {
        "loss": 2.4789,
        "grad_norm": 1.9320133924484253,
        "learning_rate": 0.00019999638791095604,
        "epoch": 0.004629034332004629,
        "step": 36
    },
    {
        "loss": 2.0434,
        "grad_norm": 2.139540910720825,
        "learning_rate": 0.0001999960357141246,
        "epoch": 0.004757618619004758,
        "step": 37
    },
    {
        "loss": 2.6323,
        "grad_norm": 1.9493412971496582,
        "learning_rate": 0.000199995667136488,
        "epoch": 0.004886202906004886,
        "step": 38
    },
    {
        "loss": 2.0299,
        "grad_norm": 2.7368714809417725,
        "learning_rate": 0.00019999528217810655,
        "epoch": 0.0050147871930050145,
        "step": 39
    },
    {
        "loss": 2.4348,
        "grad_norm": 1.7025331258773804,
        "learning_rate": 0.00019999488083904333,
        "epoch": 0.005143371480005143,
        "step": 40
    },
    {
        "loss": 2.2834,
        "grad_norm": 1.8736222982406616,
        "learning_rate": 0.00019999446311936412,
        "epoch": 0.005271955767005272,
        "step": 41
    },
    {
        "loss": 2.8508,
        "grad_norm": 1.4527090787887573,
        "learning_rate": 0.00019999402901913732,
        "epoch": 0.0054005400540054005,
        "step": 42
    },
    {
        "loss": 2.0406,
        "grad_norm": 1.7838655710220337,
        "learning_rate": 0.00019999357853843405,
        "epoch": 0.005529124341005529,
        "step": 43
    },
    {
        "loss": 2.2959,
        "grad_norm": 2.029550313949585,
        "learning_rate": 0.0001999931116773281,
        "epoch": 0.005657708628005658,
        "step": 44
    },
    {
        "loss": 2.6594,
        "grad_norm": 1.9194692373275757,
        "learning_rate": 0.00019999262843589598,
        "epoch": 0.005786292915005787,
        "step": 45
    },
    {
        "loss": 2.3341,
        "grad_norm": 1.3478631973266602,
        "learning_rate": 0.0001999921288142168,
        "epoch": 0.005914877202005915,
        "step": 46
    },
    {
        "loss": 2.4945,
        "grad_norm": 1.9622894525527954,
        "learning_rate": 0.00019999161281237244,
        "epoch": 0.006043461489006043,
        "step": 47
    },
    {
        "loss": 2.4961,
        "grad_norm": 1.3867318630218506,
        "learning_rate": 0.0001999910804304474,
        "epoch": 0.006172045776006172,
        "step": 48
    },
    {
        "loss": 2.3051,
        "grad_norm": 1.6765519380569458,
        "learning_rate": 0.00019999053166852893,
        "epoch": 0.0063006300630063005,
        "step": 49
    },
    {
        "loss": 2.5517,
        "grad_norm": 1.7718209028244019,
        "learning_rate": 0.00019998996652670692,
        "epoch": 0.006429214350006429,
        "step": 50
    },
    {
        "loss": 1.4482,
        "grad_norm": 3.1815543174743652,
        "learning_rate": 0.0001999893850050739,
        "epoch": 0.006557798637006558,
        "step": 51
    },
    {
        "loss": 2.7253,
        "grad_norm": 1.2103559970855713,
        "learning_rate": 0.0001999887871037252,
        "epoch": 0.0066863829240066865,
        "step": 52
    },
    {
        "loss": 2.1914,
        "grad_norm": 1.6743096113204956,
        "learning_rate": 0.00019998817282275872,
        "epoch": 0.006814967211006815,
        "step": 53
    },
    {
        "loss": 2.775,
        "grad_norm": 1.2055448293685913,
        "learning_rate": 0.00019998754216227508,
        "epoch": 0.006943551498006944,
        "step": 54
    },
    {
        "loss": 2.1519,
        "grad_norm": 1.5713063478469849,
        "learning_rate": 0.00019998689512237765,
        "epoch": 0.007072135785007072,
        "step": 55
    },
    {
        "loss": 2.488,
        "grad_norm": 1.764804482460022,
        "learning_rate": 0.00019998623170317233,
        "epoch": 0.0072007200720072,
        "step": 56
    },
    {
        "loss": 2.0727,
        "grad_norm": 1.8596785068511963,
        "learning_rate": 0.00019998555190476794,
        "epoch": 0.007329304359007329,
        "step": 57
    },
    {
        "loss": 2.5418,
        "grad_norm": 1.3348357677459717,
        "learning_rate": 0.0001999848557272757,
        "epoch": 0.007457888646007458,
        "step": 58
    },
    {
        "loss": 2.123,
        "grad_norm": 2.6458513736724854,
        "learning_rate": 0.00019998414317080968,
        "epoch": 0.0075864729330075865,
        "step": 59
    },
    {
        "loss": 2.3257,
        "grad_norm": 1.5841151475906372,
        "learning_rate": 0.00019998341423548666,
        "epoch": 0.007715057220007715,
        "step": 60
    },
    {
        "loss": 2.4057,
        "grad_norm": 1.9497259855270386,
        "learning_rate": 0.00019998266892142606,
        "epoch": 0.007843641507007844,
        "step": 61
    },
    {
        "loss": 2.3238,
        "grad_norm": 1.7763503789901733,
        "learning_rate": 0.00019998190722874987,
        "epoch": 0.007972225794007972,
        "step": 62
    },
    {
        "loss": 2.5267,
        "grad_norm": 1.4813320636749268,
        "learning_rate": 0.000199981129157583,
        "epoch": 0.008100810081008101,
        "step": 63
    },
    {
        "loss": 2.555,
        "grad_norm": 1.0904991626739502,
        "learning_rate": 0.00019998033470805282,
        "epoch": 0.008229394368008229,
        "step": 64
    },
    {
        "loss": 2.5618,
        "grad_norm": 1.2564836740493774,
        "learning_rate": 0.0001999795238802895,
        "epoch": 0.008357978655008359,
        "step": 65
    },
    {
        "loss": 2.5925,
        "grad_norm": 1.4494476318359375,
        "learning_rate": 0.00019997869667442585,
        "epoch": 0.008486562942008486,
        "step": 66
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.874958872795105,
        "learning_rate": 0.0001999778530905974,
        "epoch": 0.008615147229008616,
        "step": 67
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.3480972051620483,
        "learning_rate": 0.00019997699312894232,
        "epoch": 0.008743731516008744,
        "step": 68
    },
    {
        "loss": 2.4984,
        "grad_norm": 1.3522977828979492,
        "learning_rate": 0.00019997611678960152,
        "epoch": 0.008872315803008872,
        "step": 69
    },
    {
        "loss": 2.2592,
        "grad_norm": 1.4682748317718506,
        "learning_rate": 0.00019997522407271852,
        "epoch": 0.009000900090009001,
        "step": 70
    },
    {
        "loss": 1.9032,
        "grad_norm": 2.20346999168396,
        "learning_rate": 0.00019997431497843958,
        "epoch": 0.009129484377009129,
        "step": 71
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.9514055252075195,
        "learning_rate": 0.0001999733895069136,
        "epoch": 0.009258068664009259,
        "step": 72
    },
    {
        "loss": 2.1017,
        "grad_norm": 2.2662127017974854,
        "learning_rate": 0.00019997244765829222,
        "epoch": 0.009386652951009386,
        "step": 73
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.6418730020523071,
        "learning_rate": 0.0001999714894327297,
        "epoch": 0.009515237238009516,
        "step": 74
    },
    {
        "loss": 2.3008,
        "grad_norm": 1.656128168106079,
        "learning_rate": 0.00019997051483038302,
        "epoch": 0.009643821525009644,
        "step": 75
    },
    {
        "loss": 2.6565,
        "grad_norm": 1.8365764617919922,
        "learning_rate": 0.00019996952385141183,
        "epoch": 0.009772405812009772,
        "step": 76
    },
    {
        "loss": 2.1603,
        "grad_norm": 2.090331792831421,
        "learning_rate": 0.0001999685164959785,
        "epoch": 0.009900990099009901,
        "step": 77
    },
    {
        "loss": 1.4608,
        "grad_norm": 2.410916566848755,
        "learning_rate": 0.000199967492764248,
        "epoch": 0.010029574386010029,
        "step": 78
    },
    {
        "loss": 1.9006,
        "grad_norm": 1.8424230813980103,
        "learning_rate": 0.00019996645265638806,
        "epoch": 0.010158158673010158,
        "step": 79
    },
    {
        "loss": 2.3409,
        "grad_norm": 1.539727807044983,
        "learning_rate": 0.00019996539617256903,
        "epoch": 0.010286742960010286,
        "step": 80
    },
    {
        "loss": 2.7768,
        "grad_norm": 1.7084951400756836,
        "learning_rate": 0.00019996432331296403,
        "epoch": 0.010415327247010416,
        "step": 81
    },
    {
        "loss": 2.8264,
        "grad_norm": 1.3067466020584106,
        "learning_rate": 0.00019996323407774883,
        "epoch": 0.010543911534010544,
        "step": 82
    },
    {
        "loss": 2.3723,
        "grad_norm": 1.6838494539260864,
        "learning_rate": 0.00019996212846710176,
        "epoch": 0.010672495821010673,
        "step": 83
    },
    {
        "loss": 2.6364,
        "grad_norm": 1.677919864654541,
        "learning_rate": 0.00019996100648120402,
        "epoch": 0.010801080108010801,
        "step": 84
    },
    {
        "loss": 2.1942,
        "grad_norm": 1.9070600271224976,
        "learning_rate": 0.00019995986812023936,
        "epoch": 0.010929664395010929,
        "step": 85
    },
    {
        "loss": 1.3073,
        "grad_norm": 2.1544547080993652,
        "learning_rate": 0.0001999587133843943,
        "epoch": 0.011058248682011058,
        "step": 86
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.6573196649551392,
        "learning_rate": 0.000199957542273858,
        "epoch": 0.011186832969011186,
        "step": 87
    },
    {
        "loss": 2.2494,
        "grad_norm": 1.6135841608047485,
        "learning_rate": 0.00019995635478882226,
        "epoch": 0.011315417256011316,
        "step": 88
    },
    {
        "loss": 2.6328,
        "grad_norm": 1.1740615367889404,
        "learning_rate": 0.00019995515092948165,
        "epoch": 0.011444001543011444,
        "step": 89
    },
    {
        "loss": 2.0672,
        "grad_norm": 1.8292564153671265,
        "learning_rate": 0.00019995393069603337,
        "epoch": 0.011572585830011573,
        "step": 90
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.5512869358062744,
        "learning_rate": 0.0001999526940886773,
        "epoch": 0.011701170117011701,
        "step": 91
    },
    {
        "loss": 2.112,
        "grad_norm": 1.7951157093048096,
        "learning_rate": 0.00019995144110761598,
        "epoch": 0.01182975440401183,
        "step": 92
    },
    {
        "loss": 2.1987,
        "grad_norm": 1.2448079586029053,
        "learning_rate": 0.00019995017175305477,
        "epoch": 0.011958338691011958,
        "step": 93
    },
    {
        "loss": 1.9031,
        "grad_norm": 2.028672456741333,
        "learning_rate": 0.00019994888602520154,
        "epoch": 0.012086922978012086,
        "step": 94
    },
    {
        "loss": 2.7214,
        "grad_norm": 1.5048408508300781,
        "learning_rate": 0.00019994758392426692,
        "epoch": 0.012215507265012216,
        "step": 95
    },
    {
        "loss": 2.2853,
        "grad_norm": 1.787890911102295,
        "learning_rate": 0.00019994626545046418,
        "epoch": 0.012344091552012344,
        "step": 96
    },
    {
        "loss": 2.0227,
        "grad_norm": 2.342465400695801,
        "learning_rate": 0.00019994493060400938,
        "epoch": 0.012472675839012473,
        "step": 97
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.2828863859176636,
        "learning_rate": 0.0001999435793851211,
        "epoch": 0.012601260126012601,
        "step": 98
    },
    {
        "loss": 2.5048,
        "grad_norm": 2.0062479972839355,
        "learning_rate": 0.00019994221179402076,
        "epoch": 0.01272984441301273,
        "step": 99
    },
    {
        "loss": 2.0867,
        "grad_norm": 1.304421067237854,
        "learning_rate": 0.00019994082783093234,
        "epoch": 0.012858428700012858,
        "step": 100
    },
    {
        "eval_loss": 2.238044023513794,
        "eval_runtime": 28.2537,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.012858428700012858,
        "step": 100
    },
    {
        "loss": 2.305,
        "grad_norm": 1.6229183673858643,
        "learning_rate": 0.0001999394274960826,
        "epoch": 0.012987012987012988,
        "step": 101
    },
    {
        "loss": 2.575,
        "grad_norm": 1.742984652519226,
        "learning_rate": 0.0001999380107897009,
        "epoch": 0.013115597274013116,
        "step": 102
    },
    {
        "loss": 2.182,
        "grad_norm": 2.551525115966797,
        "learning_rate": 0.00019993657771201937,
        "epoch": 0.013244181561013244,
        "step": 103
    },
    {
        "loss": 2.2287,
        "grad_norm": 1.5539696216583252,
        "learning_rate": 0.00019993512826327269,
        "epoch": 0.013372765848013373,
        "step": 104
    },
    {
        "loss": 2.1102,
        "grad_norm": 2.1099305152893066,
        "learning_rate": 0.00019993366244369835,
        "epoch": 0.013501350135013501,
        "step": 105
    },
    {
        "loss": 2.4696,
        "grad_norm": 1.2369389533996582,
        "learning_rate": 0.00019993218025353646,
        "epoch": 0.01362993442201363,
        "step": 106
    },
    {
        "loss": 2.4777,
        "grad_norm": 1.3099441528320312,
        "learning_rate": 0.00019993068169302984,
        "epoch": 0.013758518709013758,
        "step": 107
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.5890085697174072,
        "learning_rate": 0.00019992916676242394,
        "epoch": 0.013887102996013888,
        "step": 108
    },
    {
        "loss": 1.6749,
        "grad_norm": 1.7969534397125244,
        "learning_rate": 0.00019992763546196698,
        "epoch": 0.014015687283014016,
        "step": 109
    },
    {
        "loss": 2.0162,
        "grad_norm": 1.6836079359054565,
        "learning_rate": 0.00019992608779190977,
        "epoch": 0.014144271570014143,
        "step": 110
    },
    {
        "loss": 1.3921,
        "grad_norm": 1.9170575141906738,
        "learning_rate": 0.0001999245237525058,
        "epoch": 0.014272855857014273,
        "step": 111
    },
    {
        "loss": 2.1523,
        "grad_norm": 1.8052732944488525,
        "learning_rate": 0.00019992294334401141,
        "epoch": 0.0144014401440144,
        "step": 112
    },
    {
        "loss": 2.5102,
        "grad_norm": 1.3715606927871704,
        "learning_rate": 0.00019992134656668538,
        "epoch": 0.01453002443101453,
        "step": 113
    },
    {
        "loss": 2.0209,
        "grad_norm": 1.8173145055770874,
        "learning_rate": 0.0001999197334207893,
        "epoch": 0.014658608718014658,
        "step": 114
    },
    {
        "loss": 2.3939,
        "grad_norm": 1.4175951480865479,
        "learning_rate": 0.0001999181039065875,
        "epoch": 0.014787193005014788,
        "step": 115
    },
    {
        "loss": 1.5535,
        "grad_norm": 2.1319870948791504,
        "learning_rate": 0.00019991645802434685,
        "epoch": 0.014915777292014916,
        "step": 116
    },
    {
        "loss": 1.8965,
        "grad_norm": 1.8267481327056885,
        "learning_rate": 0.00019991479577433698,
        "epoch": 0.015044361579015045,
        "step": 117
    },
    {
        "loss": 2.061,
        "grad_norm": 1.6490148305892944,
        "learning_rate": 0.0001999131171568302,
        "epoch": 0.015172945866015173,
        "step": 118
    },
    {
        "loss": 2.2656,
        "grad_norm": 1.6976292133331299,
        "learning_rate": 0.0001999114221721015,
        "epoch": 0.0153015301530153,
        "step": 119
    },
    {
        "loss": 2.1045,
        "grad_norm": 2.125391960144043,
        "learning_rate": 0.00019990971082042852,
        "epoch": 0.01543011444001543,
        "step": 120
    },
    {
        "loss": 2.0678,
        "grad_norm": 2.1051957607269287,
        "learning_rate": 0.0001999079831020916,
        "epoch": 0.015558698727015558,
        "step": 121
    },
    {
        "loss": 2.2194,
        "grad_norm": 1.7219613790512085,
        "learning_rate": 0.00019990623901737378,
        "epoch": 0.015687283014015688,
        "step": 122
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.3457845449447632,
        "learning_rate": 0.00019990447856656083,
        "epoch": 0.015815867301015817,
        "step": 123
    },
    {
        "loss": 1.9729,
        "grad_norm": 1.9368445873260498,
        "learning_rate": 0.00019990270174994102,
        "epoch": 0.015944451588015943,
        "step": 124
    },
    {
        "loss": 2.597,
        "grad_norm": 1.4758708477020264,
        "learning_rate": 0.0001999009085678055,
        "epoch": 0.016073035875016073,
        "step": 125
    },
    {
        "loss": 2.1273,
        "grad_norm": 1.4250813722610474,
        "learning_rate": 0.00019989909902044797,
        "epoch": 0.016201620162016202,
        "step": 126
    },
    {
        "loss": 2.2445,
        "grad_norm": 1.9757497310638428,
        "learning_rate": 0.0001998972731081649,
        "epoch": 0.01633020444901633,
        "step": 127
    },
    {
        "loss": 2.0677,
        "grad_norm": 1.6264728307724,
        "learning_rate": 0.00019989543083125537,
        "epoch": 0.016458788736016458,
        "step": 128
    },
    {
        "loss": 2.5477,
        "grad_norm": 1.3020594120025635,
        "learning_rate": 0.0001998935721900212,
        "epoch": 0.016587373023016588,
        "step": 129
    },
    {
        "loss": 1.976,
        "grad_norm": 1.5786411762237549,
        "learning_rate": 0.00019989169718476687,
        "epoch": 0.016715957310016717,
        "step": 130
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.2894964218139648,
        "learning_rate": 0.00019988980581579947,
        "epoch": 0.016844541597016843,
        "step": 131
    },
    {
        "loss": 2.8226,
        "grad_norm": 1.1173386573791504,
        "learning_rate": 0.0001998878980834289,
        "epoch": 0.016973125884016973,
        "step": 132
    },
    {
        "loss": 2.707,
        "grad_norm": 1.261536717414856,
        "learning_rate": 0.00019988597398796766,
        "epoch": 0.017101710171017102,
        "step": 133
    },
    {
        "loss": 2.5835,
        "grad_norm": 0.8538655638694763,
        "learning_rate": 0.00019988403352973094,
        "epoch": 0.017230294458017232,
        "step": 134
    },
    {
        "loss": 2.4325,
        "grad_norm": 1.063016414642334,
        "learning_rate": 0.00019988207670903658,
        "epoch": 0.017358878745017358,
        "step": 135
    },
    {
        "loss": 2.4732,
        "grad_norm": 1.712859869003296,
        "learning_rate": 0.00019988010352620518,
        "epoch": 0.017487463032017488,
        "step": 136
    },
    {
        "loss": 1.924,
        "grad_norm": 1.699852705001831,
        "learning_rate": 0.00019987811398156,
        "epoch": 0.017616047319017617,
        "step": 137
    },
    {
        "loss": 2.1083,
        "grad_norm": 1.661536455154419,
        "learning_rate": 0.00019987610807542684,
        "epoch": 0.017744631606017743,
        "step": 138
    },
    {
        "loss": 2.0117,
        "grad_norm": 1.6101436614990234,
        "learning_rate": 0.00019987408580813445,
        "epoch": 0.017873215893017873,
        "step": 139
    },
    {
        "loss": 1.5938,
        "grad_norm": 2.2395007610321045,
        "learning_rate": 0.000199872047180014,
        "epoch": 0.018001800180018002,
        "step": 140
    },
    {
        "loss": 1.9423,
        "grad_norm": 2.381032943725586,
        "learning_rate": 0.00019986999219139947,
        "epoch": 0.018130384467018132,
        "step": 141
    },
    {
        "loss": 2.0307,
        "grad_norm": 1.5172159671783447,
        "learning_rate": 0.00019986792084262752,
        "epoch": 0.018258968754018258,
        "step": 142
    },
    {
        "loss": 2.2909,
        "grad_norm": 1.3426662683486938,
        "learning_rate": 0.00019986583313403745,
        "epoch": 0.018387553041018388,
        "step": 143
    },
    {
        "loss": 2.655,
        "grad_norm": 1.6184269189834595,
        "learning_rate": 0.00019986372906597124,
        "epoch": 0.018516137328018517,
        "step": 144
    },
    {
        "loss": 1.9644,
        "grad_norm": 2.0213522911071777,
        "learning_rate": 0.00019986160863877362,
        "epoch": 0.018644721615018643,
        "step": 145
    },
    {
        "loss": 2.4864,
        "grad_norm": 1.5661835670471191,
        "learning_rate": 0.0001998594718527919,
        "epoch": 0.018773305902018773,
        "step": 146
    },
    {
        "loss": 1.6099,
        "grad_norm": 1.8727107048034668,
        "learning_rate": 0.00019985731870837613,
        "epoch": 0.018901890189018902,
        "step": 147
    },
    {
        "loss": 2.2239,
        "grad_norm": 1.380500078201294,
        "learning_rate": 0.00019985514920587902,
        "epoch": 0.019030474476019032,
        "step": 148
    },
    {
        "loss": 1.866,
        "grad_norm": 2.014183521270752,
        "learning_rate": 0.00019985296334565597,
        "epoch": 0.019159058763019158,
        "step": 149
    },
    {
        "loss": 1.6475,
        "grad_norm": 1.7348607778549194,
        "learning_rate": 0.0001998507611280651,
        "epoch": 0.019287643050019287,
        "step": 150
    },
    {
        "loss": 2.1745,
        "grad_norm": 1.5753304958343506,
        "learning_rate": 0.00019984854255346708,
        "epoch": 0.019416227337019417,
        "step": 151
    },
    {
        "loss": 2.4363,
        "grad_norm": 0.9170646667480469,
        "learning_rate": 0.0001998463076222254,
        "epoch": 0.019544811624019543,
        "step": 152
    },
    {
        "loss": 2.3477,
        "grad_norm": 1.415629267692566,
        "learning_rate": 0.00019984405633470618,
        "epoch": 0.019673395911019673,
        "step": 153
    },
    {
        "loss": 2.0931,
        "grad_norm": 1.8133246898651123,
        "learning_rate": 0.0001998417886912782,
        "epoch": 0.019801980198019802,
        "step": 154
    },
    {
        "loss": 1.4773,
        "grad_norm": 1.8239887952804565,
        "learning_rate": 0.0001998395046923129,
        "epoch": 0.019930564485019932,
        "step": 155
    },
    {
        "loss": 2.5509,
        "grad_norm": 1.3424469232559204,
        "learning_rate": 0.00019983720433818447,
        "epoch": 0.020059148772020058,
        "step": 156
    },
    {
        "loss": 2.2013,
        "grad_norm": 1.1210383176803589,
        "learning_rate": 0.00019983488762926975,
        "epoch": 0.020187733059020187,
        "step": 157
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.346388816833496,
        "learning_rate": 0.0001998325545659482,
        "epoch": 0.020316317346020317,
        "step": 158
    },
    {
        "loss": 2.0371,
        "grad_norm": 1.9630553722381592,
        "learning_rate": 0.00019983020514860206,
        "epoch": 0.020444901633020447,
        "step": 159
    },
    {
        "loss": 1.9909,
        "grad_norm": 1.372331976890564,
        "learning_rate": 0.0001998278393776162,
        "epoch": 0.020573485920020573,
        "step": 160
    },
    {
        "loss": 2.4835,
        "grad_norm": 1.2927299737930298,
        "learning_rate": 0.00019982545725337816,
        "epoch": 0.020702070207020702,
        "step": 161
    },
    {
        "loss": 1.3631,
        "grad_norm": 2.037105083465576,
        "learning_rate": 0.0001998230587762781,
        "epoch": 0.02083065449402083,
        "step": 162
    },
    {
        "loss": 1.9561,
        "grad_norm": 2.2112228870391846,
        "learning_rate": 0.00019982064394670905,
        "epoch": 0.020959238781020958,
        "step": 163
    },
    {
        "loss": 1.6141,
        "grad_norm": 2.1693830490112305,
        "learning_rate": 0.0001998182127650665,
        "epoch": 0.021087823068021087,
        "step": 164
    },
    {
        "loss": 2.674,
        "grad_norm": 1.462295413017273,
        "learning_rate": 0.00019981576523174873,
        "epoch": 0.021216407355021217,
        "step": 165
    },
    {
        "loss": 1.9631,
        "grad_norm": 2.1581690311431885,
        "learning_rate": 0.00019981330134715667,
        "epoch": 0.021344991642021346,
        "step": 166
    },
    {
        "loss": 1.825,
        "grad_norm": 2.218177080154419,
        "learning_rate": 0.000199810821111694,
        "epoch": 0.021473575929021473,
        "step": 167
    },
    {
        "loss": 2.1433,
        "grad_norm": 2.2627506256103516,
        "learning_rate": 0.00019980832452576696,
        "epoch": 0.021602160216021602,
        "step": 168
    },
    {
        "loss": 1.5814,
        "grad_norm": 2.1391918659210205,
        "learning_rate": 0.00019980581158978456,
        "epoch": 0.02173074450302173,
        "step": 169
    },
    {
        "loss": 2.552,
        "grad_norm": 1.3439804315567017,
        "learning_rate": 0.00019980328230415845,
        "epoch": 0.021859328790021858,
        "step": 170
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.8722479343414307,
        "learning_rate": 0.00019980073666930293,
        "epoch": 0.021987913077021987,
        "step": 171
    },
    {
        "loss": 2.6583,
        "grad_norm": 1.8278143405914307,
        "learning_rate": 0.00019979817468563504,
        "epoch": 0.022116497364022117,
        "step": 172
    },
    {
        "loss": 2.142,
        "grad_norm": 1.612018346786499,
        "learning_rate": 0.00019979559635357447,
        "epoch": 0.022245081651022246,
        "step": 173
    },
    {
        "loss": 1.7332,
        "grad_norm": 1.5523662567138672,
        "learning_rate": 0.00019979300167354363,
        "epoch": 0.022373665938022372,
        "step": 174
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.6695754528045654,
        "learning_rate": 0.00019979039064596746,
        "epoch": 0.022502250225022502,
        "step": 175
    },
    {
        "loss": 2.7452,
        "grad_norm": 1.373579978942871,
        "learning_rate": 0.00019978776327127378,
        "epoch": 0.02263083451202263,
        "step": 176
    },
    {
        "loss": 2.5763,
        "grad_norm": 1.0794419050216675,
        "learning_rate": 0.00019978511954989297,
        "epoch": 0.022759418799022758,
        "step": 177
    },
    {
        "loss": 2.4813,
        "grad_norm": 1.3597029447555542,
        "learning_rate": 0.00019978245948225806,
        "epoch": 0.022888003086022887,
        "step": 178
    },
    {
        "loss": 2.179,
        "grad_norm": 1.6822173595428467,
        "learning_rate": 0.0001997797830688049,
        "epoch": 0.023016587373023017,
        "step": 179
    },
    {
        "loss": 2.0156,
        "grad_norm": 2.0252389907836914,
        "learning_rate": 0.00019977709030997182,
        "epoch": 0.023145171660023146,
        "step": 180
    },
    {
        "loss": 2.5816,
        "grad_norm": 1.057065725326538,
        "learning_rate": 0.00019977438120620002,
        "epoch": 0.023273755947023272,
        "step": 181
    },
    {
        "loss": 2.4453,
        "grad_norm": 1.4033701419830322,
        "learning_rate": 0.00019977165575793326,
        "epoch": 0.023402340234023402,
        "step": 182
    },
    {
        "loss": 1.969,
        "grad_norm": 1.885599136352539,
        "learning_rate": 0.000199768913965618,
        "epoch": 0.02353092452102353,
        "step": 183
    },
    {
        "loss": 2.695,
        "grad_norm": 1.253196358680725,
        "learning_rate": 0.00019976615582970335,
        "epoch": 0.02365950880802366,
        "step": 184
    },
    {
        "loss": 2.1392,
        "grad_norm": 1.409885048866272,
        "learning_rate": 0.00019976338135064122,
        "epoch": 0.023788093095023787,
        "step": 185
    },
    {
        "loss": 1.9508,
        "grad_norm": 2.174980640411377,
        "learning_rate": 0.00019976059052888604,
        "epoch": 0.023916677382023917,
        "step": 186
    },
    {
        "loss": 2.4164,
        "grad_norm": 1.5575016736984253,
        "learning_rate": 0.000199757783364895,
        "epoch": 0.024045261669024046,
        "step": 187
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.4173219203948975,
        "learning_rate": 0.000199754959859128,
        "epoch": 0.024173845956024172,
        "step": 188
    },
    {
        "loss": 1.4128,
        "grad_norm": 2.2168047428131104,
        "learning_rate": 0.0001997521200120475,
        "epoch": 0.024302430243024302,
        "step": 189
    },
    {
        "loss": 2.0513,
        "grad_norm": 1.3894612789154053,
        "learning_rate": 0.0001997492638241188,
        "epoch": 0.02443101453002443,
        "step": 190
    },
    {
        "loss": 2.2693,
        "grad_norm": 1.6412835121154785,
        "learning_rate": 0.00019974639129580966,
        "epoch": 0.02455959881702456,
        "step": 191
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.9239273071289062,
        "learning_rate": 0.00019974350242759074,
        "epoch": 0.024688183104024687,
        "step": 192
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.6757783889770508,
        "learning_rate": 0.00019974059721993524,
        "epoch": 0.024816767391024817,
        "step": 193
    },
    {
        "loss": 1.7582,
        "grad_norm": 1.4251726865768433,
        "learning_rate": 0.0001997376756733191,
        "epoch": 0.024945351678024946,
        "step": 194
    },
    {
        "loss": 2.0513,
        "grad_norm": 1.697187900543213,
        "learning_rate": 0.00019973473778822087,
        "epoch": 0.025073935965025072,
        "step": 195
    },
    {
        "loss": 2.3136,
        "grad_norm": 1.5185929536819458,
        "learning_rate": 0.0001997317835651219,
        "epoch": 0.025202520252025202,
        "step": 196
    },
    {
        "loss": 2.5949,
        "grad_norm": 1.4605658054351807,
        "learning_rate": 0.00019972881300450604,
        "epoch": 0.02533110453902533,
        "step": 197
    },
    {
        "loss": 2.4729,
        "grad_norm": 1.1128497123718262,
        "learning_rate": 0.00019972582610685994,
        "epoch": 0.02545968882602546,
        "step": 198
    },
    {
        "loss": 2.4894,
        "grad_norm": 1.4492555856704712,
        "learning_rate": 0.0001997228228726729,
        "epoch": 0.025588273113025587,
        "step": 199
    },
    {
        "loss": 2.5439,
        "grad_norm": 1.7232681512832642,
        "learning_rate": 0.00019971980330243694,
        "epoch": 0.025716857400025717,
        "step": 200
    },
    {
        "eval_loss": 2.1924009323120117,
        "eval_runtime": 28.259,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.025716857400025717,
        "step": 200
    },
    {
        "loss": 1.9782,
        "grad_norm": 1.3303239345550537,
        "learning_rate": 0.00019971676739664665,
        "epoch": 0.025845441687025846,
        "step": 201
    },
    {
        "loss": 1.7646,
        "grad_norm": 1.9500255584716797,
        "learning_rate": 0.00019971371515579937,
        "epoch": 0.025974025974025976,
        "step": 202
    },
    {
        "loss": 2.1411,
        "grad_norm": 1.5833436250686646,
        "learning_rate": 0.00019971064658039512,
        "epoch": 0.026102610261026102,
        "step": 203
    },
    {
        "loss": 2.5517,
        "grad_norm": 1.107698917388916,
        "learning_rate": 0.00019970756167093657,
        "epoch": 0.02623119454802623,
        "step": 204
    },
    {
        "loss": 1.9237,
        "grad_norm": 3.185825824737549,
        "learning_rate": 0.00019970446042792906,
        "epoch": 0.02635977883502636,
        "step": 205
    },
    {
        "loss": 2.8105,
        "grad_norm": 1.0460760593414307,
        "learning_rate": 0.0001997013428518806,
        "epoch": 0.026488363122026487,
        "step": 206
    },
    {
        "loss": 2.608,
        "grad_norm": 0.8931776285171509,
        "learning_rate": 0.00019969820894330198,
        "epoch": 0.026616947409026617,
        "step": 207
    },
    {
        "loss": 2.038,
        "grad_norm": 1.638193964958191,
        "learning_rate": 0.0001996950587027065,
        "epoch": 0.026745531696026746,
        "step": 208
    },
    {
        "loss": 1.9542,
        "grad_norm": 1.6501176357269287,
        "learning_rate": 0.0001996918921306102,
        "epoch": 0.026874115983026876,
        "step": 209
    },
    {
        "loss": 2.2181,
        "grad_norm": 2.640367031097412,
        "learning_rate": 0.0001996887092275319,
        "epoch": 0.027002700270027002,
        "step": 210
    },
    {
        "loss": 1.9745,
        "grad_norm": 1.8670960664749146,
        "learning_rate": 0.00019968550999399293,
        "epoch": 0.02713128455702713,
        "step": 211
    },
    {
        "loss": 2.0262,
        "grad_norm": 1.8636561632156372,
        "learning_rate": 0.0001996822944305174,
        "epoch": 0.02725986884402726,
        "step": 212
    },
    {
        "loss": 1.5561,
        "grad_norm": 1.786086082458496,
        "learning_rate": 0.00019967906253763205,
        "epoch": 0.027388453131027387,
        "step": 213
    },
    {
        "loss": 2.2568,
        "grad_norm": 1.4404301643371582,
        "learning_rate": 0.0001996758143158663,
        "epoch": 0.027517037418027517,
        "step": 214
    },
    {
        "loss": 2.2891,
        "grad_norm": 1.7788045406341553,
        "learning_rate": 0.00019967254976575229,
        "epoch": 0.027645621705027646,
        "step": 215
    },
    {
        "loss": 2.4192,
        "grad_norm": 1.6761780977249146,
        "learning_rate": 0.0001996692688878248,
        "epoch": 0.027774205992027776,
        "step": 216
    },
    {
        "loss": 2.045,
        "grad_norm": 1.693355679512024,
        "learning_rate": 0.0001996659716826213,
        "epoch": 0.0279027902790279,
        "step": 217
    },
    {
        "loss": 2.649,
        "grad_norm": 1.6340537071228027,
        "learning_rate": 0.00019966265815068185,
        "epoch": 0.02803137456602803,
        "step": 218
    },
    {
        "loss": 2.5697,
        "grad_norm": 1.5677096843719482,
        "learning_rate": 0.0001996593282925493,
        "epoch": 0.02815995885302816,
        "step": 219
    },
    {
        "loss": 2.1558,
        "grad_norm": 1.8901463747024536,
        "learning_rate": 0.00019965598210876912,
        "epoch": 0.028288543140028287,
        "step": 220
    },
    {
        "loss": 2.2047,
        "grad_norm": 2.172618865966797,
        "learning_rate": 0.0001996526195998895,
        "epoch": 0.028417127427028416,
        "step": 221
    },
    {
        "loss": 2.5455,
        "grad_norm": 1.4178154468536377,
        "learning_rate": 0.00019964924076646122,
        "epoch": 0.028545711714028546,
        "step": 222
    },
    {
        "loss": 1.4882,
        "grad_norm": 1.8777145147323608,
        "learning_rate": 0.00019964584560903782,
        "epoch": 0.028674296001028676,
        "step": 223
    },
    {
        "loss": 2.3439,
        "grad_norm": 1.8563652038574219,
        "learning_rate": 0.00019964243412817542,
        "epoch": 0.0288028802880288,
        "step": 224
    },
    {
        "loss": 1.721,
        "grad_norm": 1.968605637550354,
        "learning_rate": 0.00019963900632443294,
        "epoch": 0.02893146457502893,
        "step": 225
    },
    {
        "loss": 2.2041,
        "grad_norm": 1.0355594158172607,
        "learning_rate": 0.00019963556219837188,
        "epoch": 0.02906004886202906,
        "step": 226
    },
    {
        "loss": 1.727,
        "grad_norm": 2.115198850631714,
        "learning_rate": 0.00019963210175055637,
        "epoch": 0.02918863314902919,
        "step": 227
    },
    {
        "loss": 1.2505,
        "grad_norm": 0.9531246423721313,
        "learning_rate": 0.00019962862498155341,
        "epoch": 0.029317217436029316,
        "step": 228
    },
    {
        "loss": 2.15,
        "grad_norm": 1.6266553401947021,
        "learning_rate": 0.00019962513189193245,
        "epoch": 0.029445801723029446,
        "step": 229
    },
    {
        "loss": 2.823,
        "grad_norm": 1.805993914604187,
        "learning_rate": 0.00019962162248226572,
        "epoch": 0.029574386010029576,
        "step": 230
    },
    {
        "loss": 1.6748,
        "grad_norm": 1.7040899991989136,
        "learning_rate": 0.00019961809675312812,
        "epoch": 0.0297029702970297,
        "step": 231
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.3779171705245972,
        "learning_rate": 0.00019961455470509726,
        "epoch": 0.02983155458402983,
        "step": 232
    },
    {
        "loss": 2.2381,
        "grad_norm": 1.4533026218414307,
        "learning_rate": 0.00019961099633875333,
        "epoch": 0.02996013887102996,
        "step": 233
    },
    {
        "loss": 2.2242,
        "grad_norm": 1.4056851863861084,
        "learning_rate": 0.00019960742165467925,
        "epoch": 0.03008872315803009,
        "step": 234
    },
    {
        "loss": 2.385,
        "grad_norm": 0.8258779048919678,
        "learning_rate": 0.0001996038306534606,
        "epoch": 0.030217307445030216,
        "step": 235
    },
    {
        "loss": 1.8191,
        "grad_norm": 1.5751744508743286,
        "learning_rate": 0.00019960022333568567,
        "epoch": 0.030345891732030346,
        "step": 236
    },
    {
        "loss": 2.4517,
        "grad_norm": 1.0223841667175293,
        "learning_rate": 0.00019959659970194532,
        "epoch": 0.030474476019030475,
        "step": 237
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.5117684602737427,
        "learning_rate": 0.00019959295975283326,
        "epoch": 0.0306030603060306,
        "step": 238
    },
    {
        "loss": 2.182,
        "grad_norm": 1.5196166038513184,
        "learning_rate": 0.0001995893034889457,
        "epoch": 0.03073164459303073,
        "step": 239
    },
    {
        "loss": 1.8284,
        "grad_norm": 1.4731076955795288,
        "learning_rate": 0.00019958563091088157,
        "epoch": 0.03086022888003086,
        "step": 240
    },
    {
        "loss": 1.8997,
        "grad_norm": 1.7506463527679443,
        "learning_rate": 0.00019958194201924253,
        "epoch": 0.03098881316703099,
        "step": 241
    },
    {
        "loss": 2.1022,
        "grad_norm": 1.8402801752090454,
        "learning_rate": 0.00019957823681463288,
        "epoch": 0.031117397454031116,
        "step": 242
    },
    {
        "loss": 2.2004,
        "grad_norm": 1.7730801105499268,
        "learning_rate": 0.00019957451529765955,
        "epoch": 0.031245981741031246,
        "step": 243
    },
    {
        "loss": 2.061,
        "grad_norm": 1.5457736253738403,
        "learning_rate": 0.0001995707774689322,
        "epoch": 0.031374566028031375,
        "step": 244
    },
    {
        "loss": 2.307,
        "grad_norm": 1.4169480800628662,
        "learning_rate": 0.00019956702332906318,
        "epoch": 0.0315031503150315,
        "step": 245
    },
    {
        "loss": 2.1788,
        "grad_norm": 1.5054931640625,
        "learning_rate": 0.0001995632528786674,
        "epoch": 0.031631734602031635,
        "step": 246
    },
    {
        "loss": 1.9973,
        "grad_norm": 1.946098804473877,
        "learning_rate": 0.00019955946611836253,
        "epoch": 0.03176031888903176,
        "step": 247
    },
    {
        "loss": 2.3358,
        "grad_norm": 1.4996309280395508,
        "learning_rate": 0.00019955566304876893,
        "epoch": 0.03188890317603189,
        "step": 248
    },
    {
        "loss": 1.8957,
        "grad_norm": 1.7231782674789429,
        "learning_rate": 0.0001995518436705096,
        "epoch": 0.03201748746303202,
        "step": 249
    },
    {
        "loss": 1.9152,
        "grad_norm": 1.7270965576171875,
        "learning_rate": 0.00019954800798421018,
        "epoch": 0.032146071750032146,
        "step": 250
    },
    {
        "loss": 2.2666,
        "grad_norm": 1.7413864135742188,
        "learning_rate": 0.000199544155990499,
        "epoch": 0.03227465603703227,
        "step": 251
    },
    {
        "loss": 2.3971,
        "grad_norm": 0.9567316770553589,
        "learning_rate": 0.00019954028769000716,
        "epoch": 0.032403240324032405,
        "step": 252
    },
    {
        "loss": 2.3782,
        "grad_norm": 1.1516176462173462,
        "learning_rate": 0.00019953640308336822,
        "epoch": 0.03253182461103253,
        "step": 253
    },
    {
        "loss": 2.0318,
        "grad_norm": 1.6274198293685913,
        "learning_rate": 0.0001995325021712186,
        "epoch": 0.03266040889803266,
        "step": 254
    },
    {
        "loss": 2.5109,
        "grad_norm": 1.504591464996338,
        "learning_rate": 0.00019952858495419734,
        "epoch": 0.03278899318503279,
        "step": 255
    },
    {
        "loss": 2.1735,
        "grad_norm": 1.7166368961334229,
        "learning_rate": 0.00019952465143294613,
        "epoch": 0.032917577472032916,
        "step": 256
    },
    {
        "loss": 1.4106,
        "grad_norm": 2.4081265926361084,
        "learning_rate": 0.00019952070160810927,
        "epoch": 0.03304616175903305,
        "step": 257
    },
    {
        "loss": 2.2899,
        "grad_norm": 1.226463794708252,
        "learning_rate": 0.00019951673548033392,
        "epoch": 0.033174746046033175,
        "step": 258
    },
    {
        "loss": 1.6276,
        "grad_norm": 2.370713233947754,
        "learning_rate": 0.00019951275305026965,
        "epoch": 0.0333033303330333,
        "step": 259
    },
    {
        "loss": 2.028,
        "grad_norm": 1.313671588897705,
        "learning_rate": 0.00019950875431856897,
        "epoch": 0.033431914620033434,
        "step": 260
    },
    {
        "loss": 2.4246,
        "grad_norm": 1.185739517211914,
        "learning_rate": 0.00019950473928588685,
        "epoch": 0.03356049890703356,
        "step": 261
    },
    {
        "loss": 1.8508,
        "grad_norm": 2.025580883026123,
        "learning_rate": 0.00019950070795288108,
        "epoch": 0.03368908319403369,
        "step": 262
    },
    {
        "loss": 1.6148,
        "grad_norm": 1.9424008131027222,
        "learning_rate": 0.00019949666032021196,
        "epoch": 0.03381766748103382,
        "step": 263
    },
    {
        "loss": 2.124,
        "grad_norm": 1.657894492149353,
        "learning_rate": 0.00019949259638854262,
        "epoch": 0.033946251768033946,
        "step": 264
    },
    {
        "loss": 1.8969,
        "grad_norm": 1.6199208498001099,
        "learning_rate": 0.00019948851615853873,
        "epoch": 0.03407483605503407,
        "step": 265
    },
    {
        "loss": 2.5815,
        "grad_norm": 1.0859858989715576,
        "learning_rate": 0.00019948441963086878,
        "epoch": 0.034203420342034205,
        "step": 266
    },
    {
        "loss": 1.796,
        "grad_norm": 1.9587594270706177,
        "learning_rate": 0.00019948030680620374,
        "epoch": 0.03433200462903433,
        "step": 267
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.6899642944335938,
        "learning_rate": 0.00019947617768521748,
        "epoch": 0.034460588916034464,
        "step": 268
    },
    {
        "loss": 2.1204,
        "grad_norm": 1.9079636335372925,
        "learning_rate": 0.00019947203226858626,
        "epoch": 0.03458917320303459,
        "step": 269
    },
    {
        "loss": 1.7083,
        "grad_norm": 1.8004965782165527,
        "learning_rate": 0.00019946787055698923,
        "epoch": 0.034717757490034716,
        "step": 270
    },
    {
        "loss": 1.6414,
        "grad_norm": 2.1954433917999268,
        "learning_rate": 0.00019946369255110816,
        "epoch": 0.03484634177703485,
        "step": 271
    },
    {
        "loss": 1.5116,
        "grad_norm": 2.4268486499786377,
        "learning_rate": 0.00019945949825162746,
        "epoch": 0.034974926064034975,
        "step": 272
    },
    {
        "loss": 1.3717,
        "grad_norm": 2.0486507415771484,
        "learning_rate": 0.0001994552876592342,
        "epoch": 0.0351035103510351,
        "step": 273
    },
    {
        "loss": 1.5078,
        "grad_norm": 2.310330390930176,
        "learning_rate": 0.00019945106077461812,
        "epoch": 0.035232094638035234,
        "step": 274
    },
    {
        "loss": 1.9925,
        "grad_norm": 2.007908344268799,
        "learning_rate": 0.0001994468175984717,
        "epoch": 0.03536067892503536,
        "step": 275
    },
    {
        "loss": 1.9948,
        "grad_norm": 1.845023274421692,
        "learning_rate": 0.00019944255813148995,
        "epoch": 0.035489263212035486,
        "step": 276
    },
    {
        "loss": 2.5693,
        "grad_norm": 2.539674758911133,
        "learning_rate": 0.0001994382823743707,
        "epoch": 0.03561784749903562,
        "step": 277
    },
    {
        "loss": 2.5533,
        "grad_norm": 1.318920373916626,
        "learning_rate": 0.0001994339903278144,
        "epoch": 0.035746431786035746,
        "step": 278
    },
    {
        "loss": 2.2682,
        "grad_norm": 1.7508833408355713,
        "learning_rate": 0.00019942968199252408,
        "epoch": 0.03587501607303587,
        "step": 279
    },
    {
        "loss": 2.1205,
        "grad_norm": 1.7296321392059326,
        "learning_rate": 0.00019942535736920553,
        "epoch": 0.036003600360036005,
        "step": 280
    },
    {
        "loss": 2.637,
        "grad_norm": 1.4948827028274536,
        "learning_rate": 0.00019942101645856723,
        "epoch": 0.03613218464703613,
        "step": 281
    },
    {
        "loss": 1.9787,
        "grad_norm": 1.9935147762298584,
        "learning_rate": 0.00019941665926132026,
        "epoch": 0.036260768934036264,
        "step": 282
    },
    {
        "loss": 1.9353,
        "grad_norm": 1.7259979248046875,
        "learning_rate": 0.00019941228577817835,
        "epoch": 0.03638935322103639,
        "step": 283
    },
    {
        "loss": 1.8022,
        "grad_norm": 1.6258900165557861,
        "learning_rate": 0.000199407896009858,
        "epoch": 0.036517937508036516,
        "step": 284
    },
    {
        "loss": 2.1673,
        "grad_norm": 1.4289274215698242,
        "learning_rate": 0.00019940348995707826,
        "epoch": 0.03664652179503665,
        "step": 285
    },
    {
        "loss": 1.5535,
        "grad_norm": 2.3770973682403564,
        "learning_rate": 0.00019939906762056096,
        "epoch": 0.036775106082036775,
        "step": 286
    },
    {
        "loss": 2.2808,
        "grad_norm": 1.2857615947723389,
        "learning_rate": 0.00019939462900103052,
        "epoch": 0.0369036903690369,
        "step": 287
    },
    {
        "loss": 2.203,
        "grad_norm": 1.9992915391921997,
        "learning_rate": 0.00019939017409921405,
        "epoch": 0.037032274656037034,
        "step": 288
    },
    {
        "loss": 2.0052,
        "grad_norm": 1.4200568199157715,
        "learning_rate": 0.00019938570291584134,
        "epoch": 0.03716085894303716,
        "step": 289
    },
    {
        "loss": 2.7483,
        "grad_norm": 1.6663106679916382,
        "learning_rate": 0.0001993812154516448,
        "epoch": 0.037289443230037286,
        "step": 290
    },
    {
        "loss": 2.2895,
        "grad_norm": 1.6223516464233398,
        "learning_rate": 0.00019937671170735957,
        "epoch": 0.03741802751703742,
        "step": 291
    },
    {
        "loss": 2.7502,
        "grad_norm": 1.53706955909729,
        "learning_rate": 0.00019937219168372346,
        "epoch": 0.037546611804037545,
        "step": 292
    },
    {
        "loss": 2.1507,
        "grad_norm": 0.957810640335083,
        "learning_rate": 0.00019936765538147686,
        "epoch": 0.03767519609103768,
        "step": 293
    },
    {
        "loss": 2.3382,
        "grad_norm": 1.5099798440933228,
        "learning_rate": 0.0001993631028013629,
        "epoch": 0.037803780378037805,
        "step": 294
    },
    {
        "loss": 2.6814,
        "grad_norm": 1.1265565156936646,
        "learning_rate": 0.00019935853394412734,
        "epoch": 0.03793236466503793,
        "step": 295
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.6673537492752075,
        "learning_rate": 0.00019935394881051866,
        "epoch": 0.038060948952038064,
        "step": 296
    },
    {
        "loss": 1.7631,
        "grad_norm": 2.0753743648529053,
        "learning_rate": 0.00019934934740128797,
        "epoch": 0.03818953323903819,
        "step": 297
    },
    {
        "loss": 1.7702,
        "grad_norm": 1.305364966392517,
        "learning_rate": 0.00019934472971718903,
        "epoch": 0.038318117526038316,
        "step": 298
    },
    {
        "loss": 2.3249,
        "grad_norm": 1.2747986316680908,
        "learning_rate": 0.0001993400957589783,
        "epoch": 0.03844670181303845,
        "step": 299
    },
    {
        "loss": 1.8081,
        "grad_norm": 2.205625057220459,
        "learning_rate": 0.00019933544552741488,
        "epoch": 0.038575286100038575,
        "step": 300
    },
    {
        "eval_loss": 2.16398024559021,
        "eval_runtime": 28.3092,
        "eval_samples_per_second": 2.791,
        "eval_steps_per_second": 2.791,
        "epoch": 0.038575286100038575,
        "step": 300
    },
    {
        "loss": 2.3619,
        "grad_norm": 1.6469776630401611,
        "learning_rate": 0.0001993307790232605,
        "epoch": 0.0387038703870387,
        "step": 301
    },
    {
        "loss": 2.2448,
        "grad_norm": 1.553846836090088,
        "learning_rate": 0.0001993260962472797,
        "epoch": 0.038832454674038834,
        "step": 302
    },
    {
        "loss": 2.3481,
        "grad_norm": 1.448089361190796,
        "learning_rate": 0.0001993213972002395,
        "epoch": 0.03896103896103896,
        "step": 303
    },
    {
        "loss": 2.0124,
        "grad_norm": 1.2952134609222412,
        "learning_rate": 0.00019931668188290976,
        "epoch": 0.039089623248039086,
        "step": 304
    },
    {
        "loss": 2.5,
        "grad_norm": 1.6853028535842896,
        "learning_rate": 0.0001993119502960628,
        "epoch": 0.03921820753503922,
        "step": 305
    },
    {
        "loss": 1.753,
        "grad_norm": 2.1124792098999023,
        "learning_rate": 0.0001993072024404738,
        "epoch": 0.039346791822039345,
        "step": 306
    },
    {
        "loss": 2.316,
        "grad_norm": 1.5051329135894775,
        "learning_rate": 0.00019930243831692054,
        "epoch": 0.03947537610903948,
        "step": 307
    },
    {
        "loss": 2.5576,
        "grad_norm": 1.3459222316741943,
        "learning_rate": 0.0001992976579261834,
        "epoch": 0.039603960396039604,
        "step": 308
    },
    {
        "loss": 2.3524,
        "grad_norm": 2.3785107135772705,
        "learning_rate": 0.0001992928612690455,
        "epoch": 0.03973254468303973,
        "step": 309
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.022869348526001,
        "learning_rate": 0.00019928804834629262,
        "epoch": 0.039861128970039864,
        "step": 310
    },
    {
        "loss": 2.3944,
        "grad_norm": 1.7390185594558716,
        "learning_rate": 0.0001992832191587132,
        "epoch": 0.03998971325703999,
        "step": 311
    },
    {
        "loss": 2.5116,
        "grad_norm": 1.7541158199310303,
        "learning_rate": 0.00019927837370709828,
        "epoch": 0.040118297544040116,
        "step": 312
    },
    {
        "loss": 1.9348,
        "grad_norm": 1.837210774421692,
        "learning_rate": 0.00019927351199224163,
        "epoch": 0.04024688183104025,
        "step": 313
    },
    {
        "loss": 2.3963,
        "grad_norm": 1.7814514636993408,
        "learning_rate": 0.00019926863401493967,
        "epoch": 0.040375466118040375,
        "step": 314
    },
    {
        "loss": 1.964,
        "grad_norm": 1.5270428657531738,
        "learning_rate": 0.0001992637397759915,
        "epoch": 0.0405040504050405,
        "step": 315
    },
    {
        "loss": 1.7585,
        "grad_norm": 1.8516186475753784,
        "learning_rate": 0.0001992588292761989,
        "epoch": 0.040632634692040634,
        "step": 316
    },
    {
        "loss": 2.6575,
        "grad_norm": 1.1776756048202515,
        "learning_rate": 0.0001992539025163662,
        "epoch": 0.04076121897904076,
        "step": 317
    },
    {
        "loss": 2.2586,
        "grad_norm": 1.332275390625,
        "learning_rate": 0.00019924895949730053,
        "epoch": 0.04088980326604089,
        "step": 318
    },
    {
        "loss": 1.4408,
        "grad_norm": 2.5564491748809814,
        "learning_rate": 0.00019924400021981161,
        "epoch": 0.04101838755304102,
        "step": 319
    },
    {
        "loss": 2.0211,
        "grad_norm": 1.7735317945480347,
        "learning_rate": 0.00019923902468471186,
        "epoch": 0.041146971840041145,
        "step": 320
    },
    {
        "loss": 1.4695,
        "grad_norm": 2.436461925506592,
        "learning_rate": 0.0001992340328928163,
        "epoch": 0.04127555612704128,
        "step": 321
    },
    {
        "loss": 2.3866,
        "grad_norm": 1.4009257555007935,
        "learning_rate": 0.0001992290248449427,
        "epoch": 0.041404140414041404,
        "step": 322
    },
    {
        "loss": 1.6875,
        "grad_norm": 1.7498561143875122,
        "learning_rate": 0.00019922400054191147,
        "epoch": 0.04153272470104153,
        "step": 323
    },
    {
        "loss": 1.5485,
        "grad_norm": 1.4219326972961426,
        "learning_rate": 0.00019921895998454564,
        "epoch": 0.04166130898804166,
        "step": 324
    },
    {
        "loss": 2.4107,
        "grad_norm": 2.385899066925049,
        "learning_rate": 0.00019921390317367087,
        "epoch": 0.04178989327504179,
        "step": 325
    },
    {
        "loss": 2.1451,
        "grad_norm": 2.1085829734802246,
        "learning_rate": 0.00019920883011011564,
        "epoch": 0.041918477562041916,
        "step": 326
    },
    {
        "loss": 2.0266,
        "grad_norm": 2.07908296585083,
        "learning_rate": 0.00019920374079471093,
        "epoch": 0.04204706184904205,
        "step": 327
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.5698132514953613,
        "learning_rate": 0.00019919863522829045,
        "epoch": 0.042175646136042175,
        "step": 328
    },
    {
        "loss": 2.231,
        "grad_norm": 1.3530625104904175,
        "learning_rate": 0.00019919351341169054,
        "epoch": 0.0423042304230423,
        "step": 329
    },
    {
        "loss": 2.304,
        "grad_norm": 1.6678695678710938,
        "learning_rate": 0.00019918837534575032,
        "epoch": 0.042432814710042434,
        "step": 330
    },
    {
        "loss": 2.4293,
        "grad_norm": 1.2735637426376343,
        "learning_rate": 0.00019918322103131138,
        "epoch": 0.04256139899704256,
        "step": 331
    },
    {
        "loss": 2.4149,
        "grad_norm": 1.4880462884902954,
        "learning_rate": 0.00019917805046921813,
        "epoch": 0.04268998328404269,
        "step": 332
    },
    {
        "loss": 2.4899,
        "grad_norm": 0.8162685632705688,
        "learning_rate": 0.00019917286366031756,
        "epoch": 0.04281856757104282,
        "step": 333
    },
    {
        "loss": 1.3739,
        "grad_norm": 2.002338171005249,
        "learning_rate": 0.00019916766060545935,
        "epoch": 0.042947151858042945,
        "step": 334
    },
    {
        "loss": 1.8653,
        "grad_norm": 1.37831449508667,
        "learning_rate": 0.00019916244130549585,
        "epoch": 0.04307573614504308,
        "step": 335
    },
    {
        "loss": 2.3543,
        "grad_norm": 1.154828667640686,
        "learning_rate": 0.000199157205761282,
        "epoch": 0.043204320432043204,
        "step": 336
    },
    {
        "loss": 2.0174,
        "grad_norm": 1.9363560676574707,
        "learning_rate": 0.00019915195397367553,
        "epoch": 0.04333290471904333,
        "step": 337
    },
    {
        "loss": 1.6158,
        "grad_norm": 1.958778977394104,
        "learning_rate": 0.00019914668594353673,
        "epoch": 0.04346148900604346,
        "step": 338
    },
    {
        "loss": 2.2151,
        "grad_norm": 1.1042317152023315,
        "learning_rate": 0.0001991414016717286,
        "epoch": 0.04359007329304359,
        "step": 339
    },
    {
        "loss": 2.1911,
        "grad_norm": 1.746565341949463,
        "learning_rate": 0.00019913610115911674,
        "epoch": 0.043718657580043715,
        "step": 340
    },
    {
        "loss": 1.6903,
        "grad_norm": 1.7233530282974243,
        "learning_rate": 0.00019913078440656948,
        "epoch": 0.04384724186704385,
        "step": 341
    },
    {
        "loss": 2.0014,
        "grad_norm": 1.6766058206558228,
        "learning_rate": 0.00019912545141495777,
        "epoch": 0.043975826154043975,
        "step": 342
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.2766062021255493,
        "learning_rate": 0.00019912010218515525,
        "epoch": 0.04410441044104411,
        "step": 343
    },
    {
        "loss": 2.3532,
        "grad_norm": 1.2449607849121094,
        "learning_rate": 0.00019911473671803816,
        "epoch": 0.044232994728044234,
        "step": 344
    },
    {
        "loss": 2.2011,
        "grad_norm": 1.2364581823349,
        "learning_rate": 0.0001991093550144855,
        "epoch": 0.04436157901504436,
        "step": 345
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.4915518760681152,
        "learning_rate": 0.0001991039570753788,
        "epoch": 0.04449016330204449,
        "step": 346
    },
    {
        "loss": 2.7285,
        "grad_norm": 1.4732755422592163,
        "learning_rate": 0.0001990985429016024,
        "epoch": 0.04461874758904462,
        "step": 347
    },
    {
        "loss": 1.6393,
        "grad_norm": 1.7791680097579956,
        "learning_rate": 0.00019909311249404315,
        "epoch": 0.044747331876044745,
        "step": 348
    },
    {
        "loss": 2.4352,
        "grad_norm": 1.2856205701828003,
        "learning_rate": 0.0001990876658535907,
        "epoch": 0.04487591616304488,
        "step": 349
    },
    {
        "loss": 2.3069,
        "grad_norm": 1.1870380640029907,
        "learning_rate": 0.0001990822029811372,
        "epoch": 0.045004500450045004,
        "step": 350
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.8723279237747192,
        "learning_rate": 0.0001990767238775776,
        "epoch": 0.04513308473704513,
        "step": 351
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.0546149015426636,
        "learning_rate": 0.0001990712285438095,
        "epoch": 0.04526166902404526,
        "step": 352
    },
    {
        "loss": 1.6889,
        "grad_norm": 2.1949079036712646,
        "learning_rate": 0.00019906571698073304,
        "epoch": 0.04539025331104539,
        "step": 353
    },
    {
        "loss": 2.0465,
        "grad_norm": 1.9590340852737427,
        "learning_rate": 0.00019906018918925113,
        "epoch": 0.045518837598045515,
        "step": 354
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.7057615518569946,
        "learning_rate": 0.00019905464517026934,
        "epoch": 0.04564742188504565,
        "step": 355
    },
    {
        "loss": 1.5026,
        "grad_norm": 1.9632883071899414,
        "learning_rate": 0.00019904908492469577,
        "epoch": 0.045776006172045774,
        "step": 356
    },
    {
        "loss": 2.3201,
        "grad_norm": 1.1458780765533447,
        "learning_rate": 0.0001990435084534413,
        "epoch": 0.04590459045904591,
        "step": 357
    },
    {
        "loss": 2.029,
        "grad_norm": 2.2159247398376465,
        "learning_rate": 0.0001990379157574195,
        "epoch": 0.046033174746046034,
        "step": 358
    },
    {
        "loss": 1.6912,
        "grad_norm": 1.622943639755249,
        "learning_rate": 0.00019903230683754645,
        "epoch": 0.04616175903304616,
        "step": 359
    },
    {
        "loss": 2.1797,
        "grad_norm": 1.5450319051742554,
        "learning_rate": 0.00019902668169474104,
        "epoch": 0.04629034332004629,
        "step": 360
    },
    {
        "loss": 1.6112,
        "grad_norm": 2.013462781906128,
        "learning_rate": 0.00019902104032992472,
        "epoch": 0.04641892760704642,
        "step": 361
    },
    {
        "loss": 2.3627,
        "grad_norm": 1.7587852478027344,
        "learning_rate": 0.00019901538274402162,
        "epoch": 0.046547511894046545,
        "step": 362
    },
    {
        "loss": 2.3657,
        "grad_norm": 1.6338627338409424,
        "learning_rate": 0.00019900970893795857,
        "epoch": 0.04667609618104668,
        "step": 363
    },
    {
        "loss": 2.5034,
        "grad_norm": 2.1304712295532227,
        "learning_rate": 0.00019900401891266497,
        "epoch": 0.046804680468046804,
        "step": 364
    },
    {
        "loss": 1.9706,
        "grad_norm": 1.7496753931045532,
        "learning_rate": 0.00019899831266907296,
        "epoch": 0.04693326475504693,
        "step": 365
    },
    {
        "loss": 2.4695,
        "grad_norm": 1.8564131259918213,
        "learning_rate": 0.0001989925902081173,
        "epoch": 0.04706184904204706,
        "step": 366
    },
    {
        "loss": 2.5594,
        "grad_norm": 0.9838570356369019,
        "learning_rate": 0.00019898685153073542,
        "epoch": 0.04719043332904719,
        "step": 367
    },
    {
        "loss": 2.2876,
        "grad_norm": 1.3657070398330688,
        "learning_rate": 0.0001989810966378674,
        "epoch": 0.04731901761604732,
        "step": 368
    },
    {
        "loss": 2.3234,
        "grad_norm": 1.3577436208724976,
        "learning_rate": 0.00019897532553045592,
        "epoch": 0.04744760190304745,
        "step": 369
    },
    {
        "loss": 1.465,
        "grad_norm": 1.700783371925354,
        "learning_rate": 0.00019896953820944648,
        "epoch": 0.047576186190047574,
        "step": 370
    },
    {
        "loss": 2.1474,
        "grad_norm": 1.236993432044983,
        "learning_rate": 0.00019896373467578703,
        "epoch": 0.04770477047704771,
        "step": 371
    },
    {
        "loss": 1.8945,
        "grad_norm": 1.6517945528030396,
        "learning_rate": 0.00019895791493042833,
        "epoch": 0.047833354764047833,
        "step": 372
    },
    {
        "loss": 2.4185,
        "grad_norm": 1.2082810401916504,
        "learning_rate": 0.00019895207897432373,
        "epoch": 0.04796193905104796,
        "step": 373
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.1501790285110474,
        "learning_rate": 0.0001989462268084292,
        "epoch": 0.04809052333804809,
        "step": 374
    },
    {
        "loss": 2.0498,
        "grad_norm": 1.7234500646591187,
        "learning_rate": 0.00019894035843370347,
        "epoch": 0.04821910762504822,
        "step": 375
    },
    {
        "loss": 2.0308,
        "grad_norm": 2.285212755203247,
        "learning_rate": 0.00019893447385110782,
        "epoch": 0.048347691912048345,
        "step": 376
    },
    {
        "loss": 2.412,
        "grad_norm": 1.3410743474960327,
        "learning_rate": 0.00019892857306160627,
        "epoch": 0.04847627619904848,
        "step": 377
    },
    {
        "loss": 2.501,
        "grad_norm": 0.9898822903633118,
        "learning_rate": 0.0001989226560661654,
        "epoch": 0.048604860486048604,
        "step": 378
    },
    {
        "loss": 1.9303,
        "grad_norm": 2.0033223628997803,
        "learning_rate": 0.00019891672286575455,
        "epoch": 0.04873344477304874,
        "step": 379
    },
    {
        "loss": 2.2513,
        "grad_norm": 1.5336211919784546,
        "learning_rate": 0.00019891077346134565,
        "epoch": 0.04886202906004886,
        "step": 380
    },
    {
        "loss": 1.8037,
        "grad_norm": 1.7934339046478271,
        "learning_rate": 0.0001989048078539133,
        "epoch": 0.04899061334704899,
        "step": 381
    },
    {
        "loss": 1.9561,
        "grad_norm": 1.4599123001098633,
        "learning_rate": 0.00019889882604443474,
        "epoch": 0.04911919763404912,
        "step": 382
    },
    {
        "loss": 2.2443,
        "grad_norm": 1.070094347000122,
        "learning_rate": 0.0001988928280338899,
        "epoch": 0.04924778192104925,
        "step": 383
    },
    {
        "loss": 2.1936,
        "grad_norm": 1.0084800720214844,
        "learning_rate": 0.0001988868138232613,
        "epoch": 0.049376366208049374,
        "step": 384
    },
    {
        "loss": 1.8082,
        "grad_norm": 1.7645341157913208,
        "learning_rate": 0.00019888078341353423,
        "epoch": 0.04950495049504951,
        "step": 385
    },
    {
        "loss": 2.1973,
        "grad_norm": 1.318161129951477,
        "learning_rate": 0.0001988747368056965,
        "epoch": 0.04963353478204963,
        "step": 386
    },
    {
        "loss": 1.8432,
        "grad_norm": 1.6285762786865234,
        "learning_rate": 0.00019886867400073862,
        "epoch": 0.04976211906904976,
        "step": 387
    },
    {
        "loss": 2.5719,
        "grad_norm": 2.1590397357940674,
        "learning_rate": 0.00019886259499965383,
        "epoch": 0.04989070335604989,
        "step": 388
    },
    {
        "loss": 1.2054,
        "grad_norm": 1.7795963287353516,
        "learning_rate": 0.0001988564998034379,
        "epoch": 0.05001928764305002,
        "step": 389
    },
    {
        "loss": 2.1066,
        "grad_norm": 1.424369215965271,
        "learning_rate": 0.00019885038841308933,
        "epoch": 0.050147871930050145,
        "step": 390
    },
    {
        "loss": 1.9207,
        "grad_norm": 1.521499752998352,
        "learning_rate": 0.00019884426082960924,
        "epoch": 0.05027645621705028,
        "step": 391
    },
    {
        "loss": 1.7772,
        "grad_norm": 2.020642042160034,
        "learning_rate": 0.00019883811705400146,
        "epoch": 0.050405040504050404,
        "step": 392
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.0571836233139038,
        "learning_rate": 0.00019883195708727237,
        "epoch": 0.05053362479105054,
        "step": 393
    },
    {
        "loss": 2.5489,
        "grad_norm": 1.475521445274353,
        "learning_rate": 0.00019882578093043113,
        "epoch": 0.05066220907805066,
        "step": 394
    },
    {
        "loss": 2.6885,
        "grad_norm": 1.306524634361267,
        "learning_rate": 0.00019881958858448942,
        "epoch": 0.05079079336505079,
        "step": 395
    },
    {
        "loss": 2.5247,
        "grad_norm": 1.392160415649414,
        "learning_rate": 0.00019881338005046172,
        "epoch": 0.05091937765205092,
        "step": 396
    },
    {
        "loss": 2.5806,
        "grad_norm": 1.3158833980560303,
        "learning_rate": 0.000198807155329365,
        "epoch": 0.05104796193905105,
        "step": 397
    },
    {
        "loss": 1.9726,
        "grad_norm": 1.7480484247207642,
        "learning_rate": 0.00019880091442221894,
        "epoch": 0.051176546226051174,
        "step": 398
    },
    {
        "loss": 1.6103,
        "grad_norm": 2.1226906776428223,
        "learning_rate": 0.00019879465733004597,
        "epoch": 0.05130513051305131,
        "step": 399
    },
    {
        "loss": 2.1662,
        "grad_norm": 1.553503155708313,
        "learning_rate": 0.00019878838405387107,
        "epoch": 0.05143371480005143,
        "step": 400
    },
    {
        "eval_loss": 2.1450507640838623,
        "eval_runtime": 28.1962,
        "eval_samples_per_second": 2.802,
        "eval_steps_per_second": 2.802,
        "epoch": 0.05143371480005143,
        "step": 400
    },
    {
        "loss": 1.9612,
        "grad_norm": 1.6528050899505615,
        "learning_rate": 0.00019878209459472186,
        "epoch": 0.05156229908705156,
        "step": 401
    },
    {
        "loss": 1.7234,
        "grad_norm": 1.6167727708816528,
        "learning_rate": 0.00019877578895362869,
        "epoch": 0.05169088337405169,
        "step": 402
    },
    {
        "loss": 2.6828,
        "grad_norm": 1.2994165420532227,
        "learning_rate": 0.00019876946713162449,
        "epoch": 0.05181946766105182,
        "step": 403
    },
    {
        "loss": 2.0351,
        "grad_norm": 2.2197959423065186,
        "learning_rate": 0.00019876312912974484,
        "epoch": 0.05194805194805195,
        "step": 404
    },
    {
        "loss": 2.2167,
        "grad_norm": 1.4710018634796143,
        "learning_rate": 0.00019875677494902804,
        "epoch": 0.05207663623505208,
        "step": 405
    },
    {
        "loss": 2.1503,
        "grad_norm": 1.4755011796951294,
        "learning_rate": 0.000198750404590515,
        "epoch": 0.052205220522052204,
        "step": 406
    },
    {
        "loss": 1.8121,
        "grad_norm": 1.8587830066680908,
        "learning_rate": 0.00019874401805524924,
        "epoch": 0.05233380480905234,
        "step": 407
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.2388858795166016,
        "learning_rate": 0.000198737615344277,
        "epoch": 0.05246238909605246,
        "step": 408
    },
    {
        "loss": 2.6095,
        "grad_norm": 1.0326414108276367,
        "learning_rate": 0.0001987311964586471,
        "epoch": 0.05259097338305259,
        "step": 409
    },
    {
        "loss": 1.428,
        "grad_norm": 2.299470901489258,
        "learning_rate": 0.0001987247613994111,
        "epoch": 0.05271955767005272,
        "step": 410
    },
    {
        "loss": 2.4191,
        "grad_norm": 1.0460785627365112,
        "learning_rate": 0.00019871831016762313,
        "epoch": 0.05284814195705285,
        "step": 411
    },
    {
        "loss": 2.1824,
        "grad_norm": 1.882466435432434,
        "learning_rate": 0.00019871184276434,
        "epoch": 0.052976726244052974,
        "step": 412
    },
    {
        "loss": 2.1446,
        "grad_norm": 1.2636505365371704,
        "learning_rate": 0.00019870535919062112,
        "epoch": 0.05310531053105311,
        "step": 413
    },
    {
        "loss": 2.5408,
        "grad_norm": 1.9439713954925537,
        "learning_rate": 0.00019869885944752866,
        "epoch": 0.05323389481805323,
        "step": 414
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.0782619714736938,
        "learning_rate": 0.00019869234353612735,
        "epoch": 0.05336247910505336,
        "step": 415
    },
    {
        "loss": 2.3407,
        "grad_norm": 1.648386836051941,
        "learning_rate": 0.0001986858114574846,
        "epoch": 0.05349106339205349,
        "step": 416
    },
    {
        "loss": 2.2371,
        "grad_norm": 1.5529416799545288,
        "learning_rate": 0.0001986792632126704,
        "epoch": 0.05361964767905362,
        "step": 417
    },
    {
        "loss": 1.9619,
        "grad_norm": 1.2116283178329468,
        "learning_rate": 0.00019867269880275755,
        "epoch": 0.05374823196605375,
        "step": 418
    },
    {
        "loss": 2.1424,
        "grad_norm": 1.6840386390686035,
        "learning_rate": 0.00019866611822882128,
        "epoch": 0.05387681625305388,
        "step": 419
    },
    {
        "loss": 1.8037,
        "grad_norm": 1.9543362855911255,
        "learning_rate": 0.00019865952149193967,
        "epoch": 0.054005400540054004,
        "step": 420
    },
    {
        "loss": 1.8534,
        "grad_norm": 1.4739152193069458,
        "learning_rate": 0.0001986529085931933,
        "epoch": 0.05413398482705414,
        "step": 421
    },
    {
        "loss": 2.1664,
        "grad_norm": 1.4848872423171997,
        "learning_rate": 0.00019864627953366558,
        "epoch": 0.05426256911405426,
        "step": 422
    },
    {
        "loss": 1.6099,
        "grad_norm": 2.245100736618042,
        "learning_rate": 0.00019863963431444227,
        "epoch": 0.05439115340105439,
        "step": 423
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.6352488994598389,
        "learning_rate": 0.0001986329729366121,
        "epoch": 0.05451973768805452,
        "step": 424
    },
    {
        "loss": 2.1663,
        "grad_norm": 1.389104962348938,
        "learning_rate": 0.00019862629540126622,
        "epoch": 0.05464832197505465,
        "step": 425
    },
    {
        "loss": 1.4937,
        "grad_norm": 1.6554114818572998,
        "learning_rate": 0.00019861960170949853,
        "epoch": 0.054776906262054774,
        "step": 426
    },
    {
        "loss": 1.5144,
        "grad_norm": 2.4451136589050293,
        "learning_rate": 0.00019861289186240557,
        "epoch": 0.05490549054905491,
        "step": 427
    },
    {
        "loss": 2.2676,
        "grad_norm": 1.402259111404419,
        "learning_rate": 0.0001986061658610865,
        "epoch": 0.05503407483605503,
        "step": 428
    },
    {
        "loss": 1.6403,
        "grad_norm": 1.5974589586257935,
        "learning_rate": 0.0001985994237066431,
        "epoch": 0.055162659123055166,
        "step": 429
    },
    {
        "loss": 2.4971,
        "grad_norm": 1.5685228109359741,
        "learning_rate": 0.0001985926654001799,
        "epoch": 0.05529124341005529,
        "step": 430
    },
    {
        "loss": 2.3468,
        "grad_norm": 1.1909154653549194,
        "learning_rate": 0.00019858589094280395,
        "epoch": 0.05541982769705542,
        "step": 431
    },
    {
        "loss": 2.2642,
        "grad_norm": 1.7362483739852905,
        "learning_rate": 0.00019857910033562503,
        "epoch": 0.05554841198405555,
        "step": 432
    },
    {
        "loss": 1.9488,
        "grad_norm": 1.4394272565841675,
        "learning_rate": 0.00019857229357975555,
        "epoch": 0.05567699627105568,
        "step": 433
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.7864484786987305,
        "learning_rate": 0.00019856547067631058,
        "epoch": 0.0558055805580558,
        "step": 434
    },
    {
        "loss": 1.9962,
        "grad_norm": 1.8405402898788452,
        "learning_rate": 0.00019855863162640774,
        "epoch": 0.055934164845055936,
        "step": 435
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.505563497543335,
        "learning_rate": 0.00019855177643116737,
        "epoch": 0.05606274913205606,
        "step": 436
    },
    {
        "loss": 2.2412,
        "grad_norm": 1.2957061529159546,
        "learning_rate": 0.00019854490509171253,
        "epoch": 0.05619133341905619,
        "step": 437
    },
    {
        "loss": 1.8301,
        "grad_norm": 1.7295900583267212,
        "learning_rate": 0.00019853801760916876,
        "epoch": 0.05631991770605632,
        "step": 438
    },
    {
        "loss": 1.7045,
        "grad_norm": 1.5093399286270142,
        "learning_rate": 0.0001985311139846644,
        "epoch": 0.05644850199305645,
        "step": 439
    },
    {
        "loss": 2.441,
        "grad_norm": 1.4050406217575073,
        "learning_rate": 0.00019852419421933034,
        "epoch": 0.056577086280056574,
        "step": 440
    },
    {
        "loss": 1.9402,
        "grad_norm": 1.6458498239517212,
        "learning_rate": 0.0001985172583143001,
        "epoch": 0.05670567056705671,
        "step": 441
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.5939948558807373,
        "learning_rate": 0.00019851030627070993,
        "epoch": 0.05683425485405683,
        "step": 442
    },
    {
        "loss": 1.8789,
        "grad_norm": 1.614416480064392,
        "learning_rate": 0.00019850333808969865,
        "epoch": 0.056962839141056966,
        "step": 443
    },
    {
        "loss": 2.7611,
        "grad_norm": 1.3290743827819824,
        "learning_rate": 0.00019849635377240776,
        "epoch": 0.05709142342805709,
        "step": 444
    },
    {
        "loss": 2.3899,
        "grad_norm": 1.9343982934951782,
        "learning_rate": 0.00019848935331998136,
        "epoch": 0.05722000771505722,
        "step": 445
    },
    {
        "loss": 1.7385,
        "grad_norm": 2.3834640979766846,
        "learning_rate": 0.00019848233673356634,
        "epoch": 0.05734859200205735,
        "step": 446
    },
    {
        "loss": 2.0469,
        "grad_norm": 1.4006444215774536,
        "learning_rate": 0.00019847530401431195,
        "epoch": 0.05747717628905748,
        "step": 447
    },
    {
        "loss": 2.4465,
        "grad_norm": 1.841535210609436,
        "learning_rate": 0.00019846825516337037,
        "epoch": 0.0576057605760576,
        "step": 448
    },
    {
        "loss": 2.537,
        "grad_norm": 1.376747727394104,
        "learning_rate": 0.00019846119018189628,
        "epoch": 0.057734344863057736,
        "step": 449
    },
    {
        "loss": 2.5387,
        "grad_norm": 1.3354524374008179,
        "learning_rate": 0.000198454109071047,
        "epoch": 0.05786292915005786,
        "step": 450
    },
    {
        "loss": 1.2822,
        "grad_norm": 1.275469183921814,
        "learning_rate": 0.00019844701183198256,
        "epoch": 0.05799151343705799,
        "step": 451
    },
    {
        "loss": 2.0905,
        "grad_norm": 1.8198531866073608,
        "learning_rate": 0.00019843989846586552,
        "epoch": 0.05812009772405812,
        "step": 452
    },
    {
        "loss": 2.3291,
        "grad_norm": 1.605730414390564,
        "learning_rate": 0.00019843276897386124,
        "epoch": 0.05824868201105825,
        "step": 453
    },
    {
        "loss": 2.1882,
        "grad_norm": 1.6471190452575684,
        "learning_rate": 0.0001984256233571376,
        "epoch": 0.05837726629805838,
        "step": 454
    },
    {
        "loss": 2.1157,
        "grad_norm": 1.3079293966293335,
        "learning_rate": 0.00019841846161686513,
        "epoch": 0.05850585058505851,
        "step": 455
    },
    {
        "loss": 2.2309,
        "grad_norm": 1.1800163984298706,
        "learning_rate": 0.00019841128375421704,
        "epoch": 0.05863443487205863,
        "step": 456
    },
    {
        "loss": 1.7638,
        "grad_norm": 1.6508976221084595,
        "learning_rate": 0.0001984040897703692,
        "epoch": 0.058763019159058766,
        "step": 457
    },
    {
        "loss": 1.9629,
        "grad_norm": 1.6241960525512695,
        "learning_rate": 0.00019839687966650006,
        "epoch": 0.05889160344605889,
        "step": 458
    },
    {
        "loss": 2.2921,
        "grad_norm": 1.401190996170044,
        "learning_rate": 0.00019838965344379074,
        "epoch": 0.05902018773305902,
        "step": 459
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.6242470741271973,
        "learning_rate": 0.00019838241110342502,
        "epoch": 0.05914877202005915,
        "step": 460
    },
    {
        "loss": 2.1642,
        "grad_norm": 1.5454775094985962,
        "learning_rate": 0.0001983751526465893,
        "epoch": 0.05927735630705928,
        "step": 461
    },
    {
        "loss": 1.5283,
        "grad_norm": 2.04107928276062,
        "learning_rate": 0.00019836787807447258,
        "epoch": 0.0594059405940594,
        "step": 462
    },
    {
        "loss": 2.2105,
        "grad_norm": 1.6263049840927124,
        "learning_rate": 0.00019836058738826657,
        "epoch": 0.059534524881059536,
        "step": 463
    },
    {
        "loss": 1.6175,
        "grad_norm": 2.6815664768218994,
        "learning_rate": 0.0001983532805891656,
        "epoch": 0.05966310916805966,
        "step": 464
    },
    {
        "loss": 2.0477,
        "grad_norm": 2.199725389480591,
        "learning_rate": 0.00019834595767836662,
        "epoch": 0.05979169345505979,
        "step": 465
    },
    {
        "loss": 1.2233,
        "grad_norm": 1.824270486831665,
        "learning_rate": 0.00019833861865706923,
        "epoch": 0.05992027774205992,
        "step": 466
    },
    {
        "loss": 2.3834,
        "grad_norm": 1.6215728521347046,
        "learning_rate": 0.00019833126352647562,
        "epoch": 0.06004886202906005,
        "step": 467
    },
    {
        "loss": 2.4582,
        "grad_norm": 1.3603960275650024,
        "learning_rate": 0.00019832389228779075,
        "epoch": 0.06017744631606018,
        "step": 468
    },
    {
        "loss": 1.5367,
        "grad_norm": 1.6700561046600342,
        "learning_rate": 0.00019831650494222213,
        "epoch": 0.06030603060306031,
        "step": 469
    },
    {
        "loss": 2.1429,
        "grad_norm": 2.069716691970825,
        "learning_rate": 0.00019830910149097982,
        "epoch": 0.06043461489006043,
        "step": 470
    },
    {
        "loss": 1.9516,
        "grad_norm": 1.7474828958511353,
        "learning_rate": 0.0001983016819352767,
        "epoch": 0.060563199177060566,
        "step": 471
    },
    {
        "loss": 2.2837,
        "grad_norm": 1.5133627653121948,
        "learning_rate": 0.00019829424627632822,
        "epoch": 0.06069178346406069,
        "step": 472
    },
    {
        "loss": 2.2855,
        "grad_norm": 1.4236199855804443,
        "learning_rate": 0.00019828679451535237,
        "epoch": 0.06082036775106082,
        "step": 473
    },
    {
        "loss": 1.9009,
        "grad_norm": 2.0918383598327637,
        "learning_rate": 0.0001982793266535699,
        "epoch": 0.06094895203806095,
        "step": 474
    },
    {
        "loss": 1.8914,
        "grad_norm": 1.7145106792449951,
        "learning_rate": 0.00019827184269220416,
        "epoch": 0.06107753632506108,
        "step": 475
    },
    {
        "loss": 2.2923,
        "grad_norm": 1.6383758783340454,
        "learning_rate": 0.00019826434263248111,
        "epoch": 0.0612061206120612,
        "step": 476
    },
    {
        "loss": 1.9741,
        "grad_norm": 1.2721716165542603,
        "learning_rate": 0.0001982568264756294,
        "epoch": 0.061334704899061336,
        "step": 477
    },
    {
        "loss": 1.9779,
        "grad_norm": 1.532714605331421,
        "learning_rate": 0.00019824929422288022,
        "epoch": 0.06146328918606146,
        "step": 478
    },
    {
        "loss": 1.7845,
        "grad_norm": 1.8191872835159302,
        "learning_rate": 0.00019824174587546755,
        "epoch": 0.061591873473061595,
        "step": 479
    },
    {
        "loss": 2.1074,
        "grad_norm": 1.4870660305023193,
        "learning_rate": 0.0001982341814346279,
        "epoch": 0.06172045776006172,
        "step": 480
    },
    {
        "loss": 1.9255,
        "grad_norm": 1.666054368019104,
        "learning_rate": 0.00019822660090160037,
        "epoch": 0.06184904204706185,
        "step": 481
    },
    {
        "loss": 2.5243,
        "grad_norm": 1.8133295774459839,
        "learning_rate": 0.0001982190042776268,
        "epoch": 0.06197762633406198,
        "step": 482
    },
    {
        "loss": 1.8472,
        "grad_norm": 1.861417293548584,
        "learning_rate": 0.00019821139156395162,
        "epoch": 0.062106210621062106,
        "step": 483
    },
    {
        "loss": 2.4918,
        "grad_norm": 1.5083247423171997,
        "learning_rate": 0.00019820376276182198,
        "epoch": 0.06223479490806223,
        "step": 484
    },
    {
        "loss": 1.6595,
        "grad_norm": 1.7649985551834106,
        "learning_rate": 0.00019819611787248747,
        "epoch": 0.062363379195062366,
        "step": 485
    },
    {
        "loss": 2.3841,
        "grad_norm": 1.6420713663101196,
        "learning_rate": 0.0001981884568972005,
        "epoch": 0.06249196348206249,
        "step": 486
    },
    {
        "loss": 1.5798,
        "grad_norm": 2.005497932434082,
        "learning_rate": 0.00019818077983721605,
        "epoch": 0.06262054776906262,
        "step": 487
    },
    {
        "loss": 1.9446,
        "grad_norm": 1.9098825454711914,
        "learning_rate": 0.0001981730866937917,
        "epoch": 0.06274913205606275,
        "step": 488
    },
    {
        "loss": 1.895,
        "grad_norm": 2.002930164337158,
        "learning_rate": 0.0001981653774681877,
        "epoch": 0.06287771634306288,
        "step": 489
    },
    {
        "loss": 2.0276,
        "grad_norm": 1.978641390800476,
        "learning_rate": 0.000198157652161667,
        "epoch": 0.063006300630063,
        "step": 490
    },
    {
        "loss": 2.2669,
        "grad_norm": 1.5858452320098877,
        "learning_rate": 0.00019814991077549506,
        "epoch": 0.06313488491706314,
        "step": 491
    },
    {
        "loss": 2.2264,
        "grad_norm": 1.4829270839691162,
        "learning_rate": 0.00019814215331094003,
        "epoch": 0.06326346920406327,
        "step": 492
    },
    {
        "loss": 2.612,
        "grad_norm": 1.1421689987182617,
        "learning_rate": 0.0001981343797692727,
        "epoch": 0.06339205349106339,
        "step": 493
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.4590245485305786,
        "learning_rate": 0.0001981265901517665,
        "epoch": 0.06352063777806352,
        "step": 494
    },
    {
        "loss": 1.8454,
        "grad_norm": 1.8692920207977295,
        "learning_rate": 0.0001981187844596975,
        "epoch": 0.06364922206506365,
        "step": 495
    },
    {
        "loss": 1.6218,
        "grad_norm": 1.7760546207427979,
        "learning_rate": 0.00019811096269434437,
        "epoch": 0.06377780635206377,
        "step": 496
    },
    {
        "loss": 1.6946,
        "grad_norm": 1.793515682220459,
        "learning_rate": 0.00019810312485698842,
        "epoch": 0.0639063906390639,
        "step": 497
    },
    {
        "loss": 2.3719,
        "grad_norm": 1.3979096412658691,
        "learning_rate": 0.00019809527094891358,
        "epoch": 0.06403497492606404,
        "step": 498
    },
    {
        "loss": 1.3185,
        "grad_norm": 1.7370810508728027,
        "learning_rate": 0.00019808740097140647,
        "epoch": 0.06416355921306416,
        "step": 499
    },
    {
        "loss": 2.3984,
        "grad_norm": 1.5101374387741089,
        "learning_rate": 0.00019807951492575634,
        "epoch": 0.06429214350006429,
        "step": 500
    },
    {
        "eval_loss": 2.134582281112671,
        "eval_runtime": 28.2242,
        "eval_samples_per_second": 2.799,
        "eval_steps_per_second": 2.799,
        "epoch": 0.06429214350006429,
        "step": 500
    },
    {
        "loss": 2.07,
        "grad_norm": 1.2722331285476685,
        "learning_rate": 0.00019807161281325494,
        "epoch": 0.06442072778706442,
        "step": 501
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.7790573835372925,
        "learning_rate": 0.00019806369463519686,
        "epoch": 0.06454931207406454,
        "step": 502
    },
    {
        "loss": 1.9941,
        "grad_norm": 1.4375230073928833,
        "learning_rate": 0.00019805576039287913,
        "epoch": 0.06467789636106468,
        "step": 503
    },
    {
        "loss": 1.9034,
        "grad_norm": 2.0702078342437744,
        "learning_rate": 0.0001980478100876015,
        "epoch": 0.06480648064806481,
        "step": 504
    },
    {
        "loss": 1.8333,
        "grad_norm": 1.71517014503479,
        "learning_rate": 0.0001980398437206664,
        "epoch": 0.06493506493506493,
        "step": 505
    },
    {
        "loss": 2.007,
        "grad_norm": 1.6444957256317139,
        "learning_rate": 0.00019803186129337878,
        "epoch": 0.06506364922206506,
        "step": 506
    },
    {
        "loss": 2.4123,
        "grad_norm": 1.3750653266906738,
        "learning_rate": 0.00019802386280704635,
        "epoch": 0.0651922335090652,
        "step": 507
    },
    {
        "loss": 2.0944,
        "grad_norm": 2.117797613143921,
        "learning_rate": 0.0001980158482629793,
        "epoch": 0.06532081779606531,
        "step": 508
    },
    {
        "loss": 1.9728,
        "grad_norm": 1.5276803970336914,
        "learning_rate": 0.00019800781766249058,
        "epoch": 0.06544940208306545,
        "step": 509
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.5456959009170532,
        "learning_rate": 0.0001979997710068957,
        "epoch": 0.06557798637006558,
        "step": 510
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.0735912322998047,
        "learning_rate": 0.0001979917082975128,
        "epoch": 0.06570657065706571,
        "step": 511
    },
    {
        "loss": 2.5853,
        "grad_norm": 1.4415075778961182,
        "learning_rate": 0.0001979836295356627,
        "epoch": 0.06583515494406583,
        "step": 512
    },
    {
        "loss": 2.1156,
        "grad_norm": 1.7315055131912231,
        "learning_rate": 0.00019797553472266884,
        "epoch": 0.06596373923106597,
        "step": 513
    },
    {
        "loss": 1.8114,
        "grad_norm": 1.5564988851547241,
        "learning_rate": 0.0001979674238598572,
        "epoch": 0.0660923235180661,
        "step": 514
    },
    {
        "loss": 1.9584,
        "grad_norm": 1.365657091140747,
        "learning_rate": 0.0001979592969485565,
        "epoch": 0.06622090780506622,
        "step": 515
    },
    {
        "loss": 2.0589,
        "grad_norm": 2.125723361968994,
        "learning_rate": 0.00019795115399009805,
        "epoch": 0.06634949209206635,
        "step": 516
    },
    {
        "loss": 1.4572,
        "grad_norm": 1.924452304840088,
        "learning_rate": 0.00019794299498581577,
        "epoch": 0.06647807637906648,
        "step": 517
    },
    {
        "loss": 1.8823,
        "grad_norm": 1.6175580024719238,
        "learning_rate": 0.00019793481993704624,
        "epoch": 0.0666066606660666,
        "step": 518
    },
    {
        "loss": 2.015,
        "grad_norm": 2.5131959915161133,
        "learning_rate": 0.0001979266288451286,
        "epoch": 0.06673524495306674,
        "step": 519
    },
    {
        "loss": 2.0254,
        "grad_norm": 1.4942104816436768,
        "learning_rate": 0.00019791842171140473,
        "epoch": 0.06686382924006687,
        "step": 520
    },
    {
        "loss": 1.872,
        "grad_norm": 1.7076842784881592,
        "learning_rate": 0.0001979101985372191,
        "epoch": 0.06699241352706699,
        "step": 521
    },
    {
        "loss": 2.4214,
        "grad_norm": 1.7059457302093506,
        "learning_rate": 0.00019790195932391867,
        "epoch": 0.06712099781406712,
        "step": 522
    },
    {
        "loss": 1.9321,
        "grad_norm": 2.376758575439453,
        "learning_rate": 0.00019789370407285328,
        "epoch": 0.06724958210106725,
        "step": 523
    },
    {
        "loss": 1.9131,
        "grad_norm": 1.5933492183685303,
        "learning_rate": 0.00019788543278537516,
        "epoch": 0.06737816638806737,
        "step": 524
    },
    {
        "loss": 1.1127,
        "grad_norm": 2.12726092338562,
        "learning_rate": 0.00019787714546283934,
        "epoch": 0.0675067506750675,
        "step": 525
    },
    {
        "loss": 2.1413,
        "grad_norm": 1.4698123931884766,
        "learning_rate": 0.00019786884210660333,
        "epoch": 0.06763533496206764,
        "step": 526
    },
    {
        "loss": 2.391,
        "grad_norm": 1.1740635633468628,
        "learning_rate": 0.0001978605227180274,
        "epoch": 0.06776391924906776,
        "step": 527
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.8384699821472168,
        "learning_rate": 0.00019785218729847438,
        "epoch": 0.06789250353606789,
        "step": 528
    },
    {
        "loss": 1.9994,
        "grad_norm": 1.806167721748352,
        "learning_rate": 0.00019784383584930967,
        "epoch": 0.06802108782306802,
        "step": 529
    },
    {
        "loss": 2.2328,
        "grad_norm": 1.9909261465072632,
        "learning_rate": 0.00019783546837190145,
        "epoch": 0.06814967211006814,
        "step": 530
    },
    {
        "loss": 2.5996,
        "grad_norm": 1.7063604593276978,
        "learning_rate": 0.0001978270848676204,
        "epoch": 0.06827825639706828,
        "step": 531
    },
    {
        "loss": 2.4869,
        "grad_norm": 1.330743432044983,
        "learning_rate": 0.00019781868533783983,
        "epoch": 0.06840684068406841,
        "step": 532
    },
    {
        "loss": 2.1097,
        "grad_norm": 1.4455902576446533,
        "learning_rate": 0.0001978102697839357,
        "epoch": 0.06853542497106853,
        "step": 533
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.4465906620025635,
        "learning_rate": 0.00019780183820728667,
        "epoch": 0.06866400925806866,
        "step": 534
    },
    {
        "loss": 2.3748,
        "grad_norm": 1.4457145929336548,
        "learning_rate": 0.00019779339060927388,
        "epoch": 0.0687925935450688,
        "step": 535
    },
    {
        "loss": 1.9538,
        "grad_norm": 1.8479626178741455,
        "learning_rate": 0.00019778492699128124,
        "epoch": 0.06892117783206893,
        "step": 536
    },
    {
        "loss": 1.7886,
        "grad_norm": 1.8417367935180664,
        "learning_rate": 0.00019777644735469516,
        "epoch": 0.06904976211906905,
        "step": 537
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.3197920322418213,
        "learning_rate": 0.00019776795170090475,
        "epoch": 0.06917834640606918,
        "step": 538
    },
    {
        "loss": 2.1336,
        "grad_norm": 1.2817636728286743,
        "learning_rate": 0.0001977594400313017,
        "epoch": 0.06930693069306931,
        "step": 539
    },
    {
        "loss": 2.0224,
        "grad_norm": 1.9430969953536987,
        "learning_rate": 0.00019775091234728036,
        "epoch": 0.06943551498006943,
        "step": 540
    },
    {
        "loss": 2.1678,
        "grad_norm": 1.4460086822509766,
        "learning_rate": 0.0001977423686502377,
        "epoch": 0.06956409926706957,
        "step": 541
    },
    {
        "loss": 1.6933,
        "grad_norm": 1.8358616828918457,
        "learning_rate": 0.00019773380894157327,
        "epoch": 0.0696926835540697,
        "step": 542
    },
    {
        "loss": 1.9712,
        "grad_norm": 1.7990214824676514,
        "learning_rate": 0.0001977252332226893,
        "epoch": 0.06982126784106982,
        "step": 543
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.6874855756759644,
        "learning_rate": 0.00019771664149499063,
        "epoch": 0.06994985212806995,
        "step": 544
    },
    {
        "loss": 2.1773,
        "grad_norm": 1.893550157546997,
        "learning_rate": 0.0001977080337598847,
        "epoch": 0.07007843641507008,
        "step": 545
    },
    {
        "loss": 2.2726,
        "grad_norm": 1.923322081565857,
        "learning_rate": 0.00019769941001878153,
        "epoch": 0.0702070207020702,
        "step": 546
    },
    {
        "loss": 1.7717,
        "grad_norm": 1.7935277223587036,
        "learning_rate": 0.0001976907702730939,
        "epoch": 0.07033560498907034,
        "step": 547
    },
    {
        "loss": 2.3365,
        "grad_norm": 1.3790127038955688,
        "learning_rate": 0.0001976821145242371,
        "epoch": 0.07046418927607047,
        "step": 548
    },
    {
        "loss": 2.0935,
        "grad_norm": 1.6001166105270386,
        "learning_rate": 0.000197673442773629,
        "epoch": 0.07059277356307059,
        "step": 549
    },
    {
        "loss": 2.5521,
        "grad_norm": 1.8433897495269775,
        "learning_rate": 0.00019766475502269027,
        "epoch": 0.07072135785007072,
        "step": 550
    },
    {
        "loss": 2.1604,
        "grad_norm": 1.4495571851730347,
        "learning_rate": 0.00019765605127284398,
        "epoch": 0.07084994213707085,
        "step": 551
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.9213179349899292,
        "learning_rate": 0.000197647331525516,
        "epoch": 0.07097852642407097,
        "step": 552
    },
    {
        "loss": 1.8029,
        "grad_norm": 1.320090651512146,
        "learning_rate": 0.00019763859578213476,
        "epoch": 0.0711071107110711,
        "step": 553
    },
    {
        "loss": 1.9184,
        "grad_norm": 1.695670247077942,
        "learning_rate": 0.00019762984404413125,
        "epoch": 0.07123569499807124,
        "step": 554
    },
    {
        "loss": 2.1024,
        "grad_norm": 1.774505853652954,
        "learning_rate": 0.00019762107631293916,
        "epoch": 0.07136427928507136,
        "step": 555
    },
    {
        "loss": 2.4864,
        "grad_norm": 0.8633667230606079,
        "learning_rate": 0.00019761229258999477,
        "epoch": 0.07149286357207149,
        "step": 556
    },
    {
        "loss": 2.0869,
        "grad_norm": 1.6146199703216553,
        "learning_rate": 0.00019760349287673698,
        "epoch": 0.07162144785907162,
        "step": 557
    },
    {
        "loss": 1.9171,
        "grad_norm": 1.4522868394851685,
        "learning_rate": 0.00019759467717460734,
        "epoch": 0.07175003214607174,
        "step": 558
    },
    {
        "loss": 1.8585,
        "grad_norm": 1.6817820072174072,
        "learning_rate": 0.00019758584548504993,
        "epoch": 0.07187861643307188,
        "step": 559
    },
    {
        "loss": 1.6164,
        "grad_norm": 1.5498054027557373,
        "learning_rate": 0.00019757699780951154,
        "epoch": 0.07200720072007201,
        "step": 560
    },
    {
        "loss": 1.6925,
        "grad_norm": 1.4528080224990845,
        "learning_rate": 0.00019756813414944157,
        "epoch": 0.07213578500707214,
        "step": 561
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.1528916358947754,
        "learning_rate": 0.000197559254506292,
        "epoch": 0.07226436929407226,
        "step": 562
    },
    {
        "loss": 1.928,
        "grad_norm": 1.6551614999771118,
        "learning_rate": 0.00019755035888151742,
        "epoch": 0.0723929535810724,
        "step": 563
    },
    {
        "loss": 2.2873,
        "grad_norm": 1.6318247318267822,
        "learning_rate": 0.0001975414472765751,
        "epoch": 0.07252153786807253,
        "step": 564
    },
    {
        "loss": 1.13,
        "grad_norm": 2.6713807582855225,
        "learning_rate": 0.00019753251969292486,
        "epoch": 0.07265012215507265,
        "step": 565
    },
    {
        "loss": 2.1261,
        "grad_norm": 1.8813203573226929,
        "learning_rate": 0.0001975235761320292,
        "epoch": 0.07277870644207278,
        "step": 566
    },
    {
        "loss": 2.4003,
        "grad_norm": 1.3383713960647583,
        "learning_rate": 0.00019751461659535315,
        "epoch": 0.07290729072907291,
        "step": 567
    },
    {
        "loss": 2.0257,
        "grad_norm": 1.7113325595855713,
        "learning_rate": 0.0001975056410843645,
        "epoch": 0.07303587501607303,
        "step": 568
    },
    {
        "loss": 1.4462,
        "grad_norm": 1.6958099603652954,
        "learning_rate": 0.0001974966496005335,
        "epoch": 0.07316445930307316,
        "step": 569
    },
    {
        "loss": 2.3348,
        "grad_norm": 1.2693264484405518,
        "learning_rate": 0.0001974876421453331,
        "epoch": 0.0732930435900733,
        "step": 570
    },
    {
        "loss": 1.8987,
        "grad_norm": 1.5072227716445923,
        "learning_rate": 0.00019747861872023884,
        "epoch": 0.07342162787707342,
        "step": 571
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.9485085010528564,
        "learning_rate": 0.00019746957932672893,
        "epoch": 0.07355021216407355,
        "step": 572
    },
    {
        "loss": 2.4496,
        "grad_norm": 1.6203581094741821,
        "learning_rate": 0.00019746052396628415,
        "epoch": 0.07367879645107368,
        "step": 573
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.919379472732544,
        "learning_rate": 0.00019745145264038784,
        "epoch": 0.0738073807380738,
        "step": 574
    },
    {
        "loss": 2.2073,
        "grad_norm": 1.3306015729904175,
        "learning_rate": 0.00019744236535052608,
        "epoch": 0.07393596502507394,
        "step": 575
    },
    {
        "loss": 1.534,
        "grad_norm": 2.1929373741149902,
        "learning_rate": 0.00019743326209818745,
        "epoch": 0.07406454931207407,
        "step": 576
    },
    {
        "loss": 2.2114,
        "grad_norm": 1.3028818368911743,
        "learning_rate": 0.00019742414288486323,
        "epoch": 0.07419313359907419,
        "step": 577
    },
    {
        "loss": 2.0005,
        "grad_norm": 1.7584316730499268,
        "learning_rate": 0.00019741500771204728,
        "epoch": 0.07432171788607432,
        "step": 578
    },
    {
        "loss": 2.3305,
        "grad_norm": 1.170226812362671,
        "learning_rate": 0.00019740585658123606,
        "epoch": 0.07445030217307445,
        "step": 579
    },
    {
        "loss": 2.3089,
        "grad_norm": 1.9723337888717651,
        "learning_rate": 0.00019739668949392866,
        "epoch": 0.07457888646007457,
        "step": 580
    },
    {
        "loss": 2.2193,
        "grad_norm": 1.3946152925491333,
        "learning_rate": 0.0001973875064516268,
        "epoch": 0.0747074707470747,
        "step": 581
    },
    {
        "loss": 2.1471,
        "grad_norm": 1.310088872909546,
        "learning_rate": 0.00019737830745583478,
        "epoch": 0.07483605503407484,
        "step": 582
    },
    {
        "loss": 1.5047,
        "grad_norm": 1.7313483953475952,
        "learning_rate": 0.0001973690925080595,
        "epoch": 0.07496463932107496,
        "step": 583
    },
    {
        "loss": 2.1937,
        "grad_norm": 1.5014026165008545,
        "learning_rate": 0.00019735986160981055,
        "epoch": 0.07509322360807509,
        "step": 584
    },
    {
        "loss": 2.5791,
        "grad_norm": 1.613698124885559,
        "learning_rate": 0.0001973506147626001,
        "epoch": 0.07522180789507522,
        "step": 585
    },
    {
        "loss": 2.7336,
        "grad_norm": 1.2037906646728516,
        "learning_rate": 0.00019734135196794282,
        "epoch": 0.07535039218207536,
        "step": 586
    },
    {
        "loss": 2.2212,
        "grad_norm": 1.9848419427871704,
        "learning_rate": 0.0001973320732273562,
        "epoch": 0.07547897646907548,
        "step": 587
    },
    {
        "loss": 2.484,
        "grad_norm": 1.5170578956604004,
        "learning_rate": 0.00019732277854236018,
        "epoch": 0.07560756075607561,
        "step": 588
    },
    {
        "loss": 2.3844,
        "grad_norm": 1.4444854259490967,
        "learning_rate": 0.0001973134679144774,
        "epoch": 0.07573614504307574,
        "step": 589
    },
    {
        "loss": 1.809,
        "grad_norm": 1.636713981628418,
        "learning_rate": 0.000197304141345233,
        "epoch": 0.07586472933007586,
        "step": 590
    },
    {
        "loss": 2.164,
        "grad_norm": 1.462611436843872,
        "learning_rate": 0.00019729479883615487,
        "epoch": 0.075993313617076,
        "step": 591
    },
    {
        "loss": 2.3821,
        "grad_norm": 1.1486120223999023,
        "learning_rate": 0.00019728544038877347,
        "epoch": 0.07612189790407613,
        "step": 592
    },
    {
        "loss": 1.9779,
        "grad_norm": 1.5700440406799316,
        "learning_rate": 0.00019727606600462177,
        "epoch": 0.07625048219107625,
        "step": 593
    },
    {
        "loss": 2.0984,
        "grad_norm": 1.8220142126083374,
        "learning_rate": 0.00019726667568523552,
        "epoch": 0.07637906647807638,
        "step": 594
    },
    {
        "loss": 1.6788,
        "grad_norm": 2.278970718383789,
        "learning_rate": 0.00019725726943215294,
        "epoch": 0.07650765076507651,
        "step": 595
    },
    {
        "loss": 2.377,
        "grad_norm": 1.6764600276947021,
        "learning_rate": 0.0001972478472469149,
        "epoch": 0.07663623505207663,
        "step": 596
    },
    {
        "loss": 1.3214,
        "grad_norm": 1.9393091201782227,
        "learning_rate": 0.00019723840913106493,
        "epoch": 0.07676481933907676,
        "step": 597
    },
    {
        "loss": 2.47,
        "grad_norm": 1.0404906272888184,
        "learning_rate": 0.00019722895508614906,
        "epoch": 0.0768934036260769,
        "step": 598
    },
    {
        "loss": 2.1988,
        "grad_norm": 1.6700533628463745,
        "learning_rate": 0.0001972194851137161,
        "epoch": 0.07702198791307702,
        "step": 599
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.2656548023223877,
        "learning_rate": 0.00019720999921531731,
        "epoch": 0.07715057220007715,
        "step": 600
    },
    {
        "eval_loss": 2.110736608505249,
        "eval_runtime": 28.2809,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.07715057220007715,
        "step": 600
    },
    {
        "loss": 2.334,
        "grad_norm": 1.580450177192688,
        "learning_rate": 0.00019720049739250665,
        "epoch": 0.07727915648707728,
        "step": 601
    },
    {
        "loss": 1.5651,
        "grad_norm": 1.6677662134170532,
        "learning_rate": 0.00019719097964684062,
        "epoch": 0.0774077407740774,
        "step": 602
    },
    {
        "loss": 2.085,
        "grad_norm": 1.867325782775879,
        "learning_rate": 0.00019718144597987835,
        "epoch": 0.07753632506107754,
        "step": 603
    },
    {
        "loss": 1.3097,
        "grad_norm": 1.816277265548706,
        "learning_rate": 0.00019717189639318166,
        "epoch": 0.07766490934807767,
        "step": 604
    },
    {
        "loss": 1.9022,
        "grad_norm": 1.7622824907302856,
        "learning_rate": 0.00019716233088831487,
        "epoch": 0.07779349363507779,
        "step": 605
    },
    {
        "loss": 2.1539,
        "grad_norm": 1.88396155834198,
        "learning_rate": 0.00019715274946684492,
        "epoch": 0.07792207792207792,
        "step": 606
    },
    {
        "loss": 1.6502,
        "grad_norm": 1.3846817016601562,
        "learning_rate": 0.00019714315213034144,
        "epoch": 0.07805066220907805,
        "step": 607
    },
    {
        "loss": 2.4825,
        "grad_norm": 1.1310009956359863,
        "learning_rate": 0.0001971335388803766,
        "epoch": 0.07817924649607817,
        "step": 608
    },
    {
        "loss": 2.1832,
        "grad_norm": 2.3514316082000732,
        "learning_rate": 0.00019712390971852518,
        "epoch": 0.0783078307830783,
        "step": 609
    },
    {
        "loss": 2.4923,
        "grad_norm": 1.5025993585586548,
        "learning_rate": 0.00019711426464636458,
        "epoch": 0.07843641507007844,
        "step": 610
    },
    {
        "loss": 2.22,
        "grad_norm": 1.376359224319458,
        "learning_rate": 0.00019710460366547481,
        "epoch": 0.07856499935707857,
        "step": 611
    },
    {
        "loss": 2.2943,
        "grad_norm": 1.0881714820861816,
        "learning_rate": 0.00019709492677743846,
        "epoch": 0.07869358364407869,
        "step": 612
    },
    {
        "loss": 1.8914,
        "grad_norm": 1.8929225206375122,
        "learning_rate": 0.00019708523398384077,
        "epoch": 0.07882216793107882,
        "step": 613
    },
    {
        "loss": 1.7238,
        "grad_norm": 1.8896355628967285,
        "learning_rate": 0.00019707552528626954,
        "epoch": 0.07895075221807896,
        "step": 614
    },
    {
        "loss": 2.2855,
        "grad_norm": 1.9337927103042603,
        "learning_rate": 0.0001970658006863152,
        "epoch": 0.07907933650507908,
        "step": 615
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.3523815870285034,
        "learning_rate": 0.00019705606018557078,
        "epoch": 0.07920792079207921,
        "step": 616
    },
    {
        "loss": 1.8118,
        "grad_norm": 1.4823745489120483,
        "learning_rate": 0.00019704630378563194,
        "epoch": 0.07933650507907934,
        "step": 617
    },
    {
        "loss": 1.6763,
        "grad_norm": 2.4671783447265625,
        "learning_rate": 0.00019703653148809688,
        "epoch": 0.07946508936607946,
        "step": 618
    },
    {
        "loss": 1.25,
        "grad_norm": 1.9687285423278809,
        "learning_rate": 0.00019702674329456646,
        "epoch": 0.0795936736530796,
        "step": 619
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.08680260181427,
        "learning_rate": 0.00019701693920664415,
        "epoch": 0.07972225794007973,
        "step": 620
    },
    {
        "loss": 2.1694,
        "grad_norm": 1.4348740577697754,
        "learning_rate": 0.000197007119225936,
        "epoch": 0.07985084222707985,
        "step": 621
    },
    {
        "loss": 1.8305,
        "grad_norm": 1.773024559020996,
        "learning_rate": 0.00019699728335405064,
        "epoch": 0.07997942651407998,
        "step": 622
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.8321515321731567,
        "learning_rate": 0.00019698743159259934,
        "epoch": 0.08010801080108011,
        "step": 623
    },
    {
        "loss": 2.0105,
        "grad_norm": 1.389325499534607,
        "learning_rate": 0.00019697756394319594,
        "epoch": 0.08023659508808023,
        "step": 624
    },
    {
        "loss": 2.677,
        "grad_norm": 1.313593864440918,
        "learning_rate": 0.00019696768040745695,
        "epoch": 0.08036517937508036,
        "step": 625
    },
    {
        "loss": 1.9361,
        "grad_norm": 2.0300281047821045,
        "learning_rate": 0.0001969577809870014,
        "epoch": 0.0804937636620805,
        "step": 626
    },
    {
        "loss": 2.1748,
        "grad_norm": 1.7110753059387207,
        "learning_rate": 0.00019694786568345097,
        "epoch": 0.08062234794908062,
        "step": 627
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.6161972284317017,
        "learning_rate": 0.00019693793449842995,
        "epoch": 0.08075093223608075,
        "step": 628
    },
    {
        "loss": 2.4811,
        "grad_norm": 0.8432524800300598,
        "learning_rate": 0.0001969279874335652,
        "epoch": 0.08087951652308088,
        "step": 629
    },
    {
        "loss": 2.028,
        "grad_norm": 1.4826279878616333,
        "learning_rate": 0.00019691802449048616,
        "epoch": 0.081008100810081,
        "step": 630
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.452484130859375,
        "learning_rate": 0.00019690804567082496,
        "epoch": 0.08113668509708113,
        "step": 631
    },
    {
        "loss": 2.4139,
        "grad_norm": 1.5240989923477173,
        "learning_rate": 0.00019689805097621626,
        "epoch": 0.08126526938408127,
        "step": 632
    },
    {
        "loss": 2.0873,
        "grad_norm": 1.3556773662567139,
        "learning_rate": 0.0001968880404082973,
        "epoch": 0.08139385367108139,
        "step": 633
    },
    {
        "loss": 2.3729,
        "grad_norm": 1.4339576959609985,
        "learning_rate": 0.000196878013968708,
        "epoch": 0.08152243795808152,
        "step": 634
    },
    {
        "loss": 2.4317,
        "grad_norm": 1.5221633911132812,
        "learning_rate": 0.00019686797165909084,
        "epoch": 0.08165102224508165,
        "step": 635
    },
    {
        "loss": 2.4406,
        "grad_norm": 1.1869555711746216,
        "learning_rate": 0.00019685791348109083,
        "epoch": 0.08177960653208179,
        "step": 636
    },
    {
        "loss": 2.2617,
        "grad_norm": 1.4132202863693237,
        "learning_rate": 0.0001968478394363557,
        "epoch": 0.0819081908190819,
        "step": 637
    },
    {
        "loss": 2.2402,
        "grad_norm": 1.5557734966278076,
        "learning_rate": 0.00019683774952653575,
        "epoch": 0.08203677510608204,
        "step": 638
    },
    {
        "loss": 2.3746,
        "grad_norm": 1.252238154411316,
        "learning_rate": 0.00019682764375328378,
        "epoch": 0.08216535939308217,
        "step": 639
    },
    {
        "loss": 2.3552,
        "grad_norm": 1.6302132606506348,
        "learning_rate": 0.00019681752211825538,
        "epoch": 0.08229394368008229,
        "step": 640
    },
    {
        "loss": 1.5405,
        "grad_norm": 1.6639264822006226,
        "learning_rate": 0.00019680738462310847,
        "epoch": 0.08242252796708242,
        "step": 641
    },
    {
        "loss": 1.4191,
        "grad_norm": 1.940623164176941,
        "learning_rate": 0.0001967972312695038,
        "epoch": 0.08255111225408256,
        "step": 642
    },
    {
        "loss": 2.4564,
        "grad_norm": 1.0009729862213135,
        "learning_rate": 0.00019678706205910465,
        "epoch": 0.08267969654108268,
        "step": 643
    },
    {
        "loss": 2.1009,
        "grad_norm": 1.6270958185195923,
        "learning_rate": 0.0001967768769935769,
        "epoch": 0.08280828082808281,
        "step": 644
    },
    {
        "loss": 1.5298,
        "grad_norm": 1.5934008359909058,
        "learning_rate": 0.00019676667607458894,
        "epoch": 0.08293686511508294,
        "step": 645
    },
    {
        "loss": 1.6981,
        "grad_norm": 2.3936679363250732,
        "learning_rate": 0.0001967564593038119,
        "epoch": 0.08306544940208306,
        "step": 646
    },
    {
        "loss": 2.1528,
        "grad_norm": 1.3326294422149658,
        "learning_rate": 0.00019674622668291935,
        "epoch": 0.0831940336890832,
        "step": 647
    },
    {
        "loss": 2.0997,
        "grad_norm": 1.8798631429672241,
        "learning_rate": 0.00019673597821358763,
        "epoch": 0.08332261797608333,
        "step": 648
    },
    {
        "loss": 2.5265,
        "grad_norm": 1.4716453552246094,
        "learning_rate": 0.00019672571389749553,
        "epoch": 0.08345120226308345,
        "step": 649
    },
    {
        "loss": 2.5373,
        "grad_norm": 1.3984789848327637,
        "learning_rate": 0.00019671543373632456,
        "epoch": 0.08357978655008358,
        "step": 650
    },
    {
        "loss": 1.2976,
        "grad_norm": 1.3204175233840942,
        "learning_rate": 0.00019670513773175868,
        "epoch": 0.08370837083708371,
        "step": 651
    },
    {
        "loss": 2.4978,
        "grad_norm": 1.6359504461288452,
        "learning_rate": 0.00019669482588548456,
        "epoch": 0.08383695512408383,
        "step": 652
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.4216365814208984,
        "learning_rate": 0.00019668449819919146,
        "epoch": 0.08396553941108396,
        "step": 653
    },
    {
        "loss": 2.1651,
        "grad_norm": 2.7723143100738525,
        "learning_rate": 0.00019667415467457115,
        "epoch": 0.0840941236980841,
        "step": 654
    },
    {
        "loss": 2.3878,
        "grad_norm": 1.3378022909164429,
        "learning_rate": 0.00019666379531331808,
        "epoch": 0.08422270798508422,
        "step": 655
    },
    {
        "loss": 1.8841,
        "grad_norm": 1.896523356437683,
        "learning_rate": 0.0001966534201171293,
        "epoch": 0.08435129227208435,
        "step": 656
    },
    {
        "loss": 1.8063,
        "grad_norm": 1.9358365535736084,
        "learning_rate": 0.00019664302908770437,
        "epoch": 0.08447987655908448,
        "step": 657
    },
    {
        "loss": 1.7466,
        "grad_norm": 1.8477667570114136,
        "learning_rate": 0.00019663262222674547,
        "epoch": 0.0846084608460846,
        "step": 658
    },
    {
        "loss": 1.755,
        "grad_norm": 1.7900687456130981,
        "learning_rate": 0.00019662219953595745,
        "epoch": 0.08473704513308473,
        "step": 659
    },
    {
        "loss": 2.3104,
        "grad_norm": 1.1624459028244019,
        "learning_rate": 0.00019661176101704766,
        "epoch": 0.08486562942008487,
        "step": 660
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.2722049951553345,
        "learning_rate": 0.00019660130667172612,
        "epoch": 0.084994213707085,
        "step": 661
    },
    {
        "loss": 1.9249,
        "grad_norm": 1.5984257459640503,
        "learning_rate": 0.00019659083650170535,
        "epoch": 0.08512279799408512,
        "step": 662
    },
    {
        "loss": 2.3697,
        "grad_norm": 1.8831690549850464,
        "learning_rate": 0.0001965803505087006,
        "epoch": 0.08525138228108525,
        "step": 663
    },
    {
        "loss": 2.2931,
        "grad_norm": 1.7925200462341309,
        "learning_rate": 0.00019656984869442952,
        "epoch": 0.08537996656808539,
        "step": 664
    },
    {
        "loss": 2.0739,
        "grad_norm": 1.5142865180969238,
        "learning_rate": 0.00019655933106061253,
        "epoch": 0.0855085508550855,
        "step": 665
    },
    {
        "loss": 1.7936,
        "grad_norm": 1.5616016387939453,
        "learning_rate": 0.00019654879760897255,
        "epoch": 0.08563713514208564,
        "step": 666
    },
    {
        "loss": 2.6023,
        "grad_norm": 1.419094204902649,
        "learning_rate": 0.00019653824834123514,
        "epoch": 0.08576571942908577,
        "step": 667
    },
    {
        "loss": 1.3811,
        "grad_norm": 1.969531774520874,
        "learning_rate": 0.0001965276832591284,
        "epoch": 0.08589430371608589,
        "step": 668
    },
    {
        "loss": 2.2542,
        "grad_norm": 1.5946942567825317,
        "learning_rate": 0.00019651710236438303,
        "epoch": 0.08602288800308602,
        "step": 669
    },
    {
        "loss": 2.3866,
        "grad_norm": 1.960966944694519,
        "learning_rate": 0.00019650650565873235,
        "epoch": 0.08615147229008616,
        "step": 670
    },
    {
        "loss": 1.8292,
        "grad_norm": 1.7027101516723633,
        "learning_rate": 0.00019649589314391227,
        "epoch": 0.08628005657708628,
        "step": 671
    },
    {
        "loss": 1.6863,
        "grad_norm": 2.065201759338379,
        "learning_rate": 0.00019648526482166125,
        "epoch": 0.08640864086408641,
        "step": 672
    },
    {
        "loss": 2.4503,
        "grad_norm": 1.7098802328109741,
        "learning_rate": 0.0001964746206937204,
        "epoch": 0.08653722515108654,
        "step": 673
    },
    {
        "loss": 2.2792,
        "grad_norm": 1.1554733514785767,
        "learning_rate": 0.00019646396076183334,
        "epoch": 0.08666580943808666,
        "step": 674
    },
    {
        "loss": 2.111,
        "grad_norm": 2.0098817348480225,
        "learning_rate": 0.00019645328502774633,
        "epoch": 0.0867943937250868,
        "step": 675
    },
    {
        "loss": 2.4768,
        "grad_norm": 1.5223357677459717,
        "learning_rate": 0.00019644259349320823,
        "epoch": 0.08692297801208693,
        "step": 676
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.9796708822250366,
        "learning_rate": 0.00019643188615997047,
        "epoch": 0.08705156229908705,
        "step": 677
    },
    {
        "loss": 2.1767,
        "grad_norm": 1.8858070373535156,
        "learning_rate": 0.00019642116302978703,
        "epoch": 0.08718014658608718,
        "step": 678
    },
    {
        "loss": 2.5324,
        "grad_norm": 1.4538822174072266,
        "learning_rate": 0.00019641042410441454,
        "epoch": 0.08730873087308731,
        "step": 679
    },
    {
        "loss": 2.382,
        "grad_norm": 1.6109888553619385,
        "learning_rate": 0.0001963996693856122,
        "epoch": 0.08743731516008743,
        "step": 680
    },
    {
        "loss": 1.9651,
        "grad_norm": 1.7865062952041626,
        "learning_rate": 0.00019638889887514175,
        "epoch": 0.08756589944708756,
        "step": 681
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.4462504386901855,
        "learning_rate": 0.00019637811257476763,
        "epoch": 0.0876944837340877,
        "step": 682
    },
    {
        "loss": 2.2084,
        "grad_norm": 2.108253240585327,
        "learning_rate": 0.00019636731048625677,
        "epoch": 0.08782306802108782,
        "step": 683
    },
    {
        "loss": 1.7496,
        "grad_norm": 1.770706295967102,
        "learning_rate": 0.00019635649261137865,
        "epoch": 0.08795165230808795,
        "step": 684
    },
    {
        "loss": 2.3815,
        "grad_norm": 1.8376617431640625,
        "learning_rate": 0.00019634565895190544,
        "epoch": 0.08808023659508808,
        "step": 685
    },
    {
        "loss": 1.6485,
        "grad_norm": 1.970996618270874,
        "learning_rate": 0.00019633480950961184,
        "epoch": 0.08820882088208822,
        "step": 686
    },
    {
        "loss": 2.2668,
        "grad_norm": 1.5101407766342163,
        "learning_rate": 0.00019632394428627516,
        "epoch": 0.08833740516908833,
        "step": 687
    },
    {
        "loss": 2.2115,
        "grad_norm": 1.824116587638855,
        "learning_rate": 0.00019631306328367527,
        "epoch": 0.08846598945608847,
        "step": 688
    },
    {
        "loss": 2.3973,
        "grad_norm": 1.7816897630691528,
        "learning_rate": 0.00019630216650359466,
        "epoch": 0.0885945737430886,
        "step": 689
    },
    {
        "loss": 2.5079,
        "grad_norm": 1.1180089712142944,
        "learning_rate": 0.00019629125394781834,
        "epoch": 0.08872315803008872,
        "step": 690
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.468335747718811,
        "learning_rate": 0.00019628032561813398,
        "epoch": 0.08885174231708885,
        "step": 691
    },
    {
        "loss": 2.1407,
        "grad_norm": 2.0308525562286377,
        "learning_rate": 0.00019626938151633177,
        "epoch": 0.08898032660408899,
        "step": 692
    },
    {
        "loss": 2.5551,
        "grad_norm": 0.8944740295410156,
        "learning_rate": 0.00019625842164420454,
        "epoch": 0.0891089108910891,
        "step": 693
    },
    {
        "loss": 2.0647,
        "grad_norm": 2.018795967102051,
        "learning_rate": 0.00019624744600354769,
        "epoch": 0.08923749517808924,
        "step": 694
    },
    {
        "loss": 2.3579,
        "grad_norm": 1.0586774349212646,
        "learning_rate": 0.0001962364545961591,
        "epoch": 0.08936607946508937,
        "step": 695
    },
    {
        "loss": 1.7306,
        "grad_norm": 1.8039456605911255,
        "learning_rate": 0.00019622544742383945,
        "epoch": 0.08949466375208949,
        "step": 696
    },
    {
        "loss": 2.2254,
        "grad_norm": 1.5719479322433472,
        "learning_rate": 0.0001962144244883918,
        "epoch": 0.08962324803908962,
        "step": 697
    },
    {
        "loss": 1.3969,
        "grad_norm": 1.4484184980392456,
        "learning_rate": 0.00019620338579162184,
        "epoch": 0.08975183232608976,
        "step": 698
    },
    {
        "loss": 2.3065,
        "grad_norm": 1.1528496742248535,
        "learning_rate": 0.00019619233133533797,
        "epoch": 0.08988041661308988,
        "step": 699
    },
    {
        "loss": 2.0502,
        "grad_norm": 1.4021512269973755,
        "learning_rate": 0.00019618126112135097,
        "epoch": 0.09000900090009001,
        "step": 700
    },
    {
        "eval_loss": 2.0918099880218506,
        "eval_runtime": 28.2505,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.09000900090009001,
        "step": 700
    },
    {
        "loss": 2.02,
        "grad_norm": 1.1895337104797363,
        "learning_rate": 0.00019617017515147437,
        "epoch": 0.09013758518709014,
        "step": 701
    },
    {
        "loss": 2.2183,
        "grad_norm": 1.8998342752456665,
        "learning_rate": 0.00019615907342752416,
        "epoch": 0.09026616947409026,
        "step": 702
    },
    {
        "loss": 2.3181,
        "grad_norm": 1.4089118242263794,
        "learning_rate": 0.00019614795595131898,
        "epoch": 0.0903947537610904,
        "step": 703
    },
    {
        "loss": 2.151,
        "grad_norm": 1.4527113437652588,
        "learning_rate": 0.00019613682272468007,
        "epoch": 0.09052333804809053,
        "step": 704
    },
    {
        "loss": 2.503,
        "grad_norm": 1.1111533641815186,
        "learning_rate": 0.00019612567374943117,
        "epoch": 0.09065192233509065,
        "step": 705
    },
    {
        "loss": 2.5813,
        "grad_norm": 1.4994956254959106,
        "learning_rate": 0.00019611450902739868,
        "epoch": 0.09078050662209078,
        "step": 706
    },
    {
        "loss": 1.5539,
        "grad_norm": 1.224432110786438,
        "learning_rate": 0.0001961033285604115,
        "epoch": 0.09090909090909091,
        "step": 707
    },
    {
        "loss": 2.4991,
        "grad_norm": 1.4526875019073486,
        "learning_rate": 0.0001960921323503012,
        "epoch": 0.09103767519609103,
        "step": 708
    },
    {
        "loss": 1.9003,
        "grad_norm": 2.1453611850738525,
        "learning_rate": 0.00019608092039890185,
        "epoch": 0.09116625948309116,
        "step": 709
    },
    {
        "loss": 2.31,
        "grad_norm": 1.4896560907363892,
        "learning_rate": 0.00019606969270805015,
        "epoch": 0.0912948437700913,
        "step": 710
    },
    {
        "loss": 2.7419,
        "grad_norm": 1.167628288269043,
        "learning_rate": 0.00019605844927958534,
        "epoch": 0.09142342805709143,
        "step": 711
    },
    {
        "loss": 2.1039,
        "grad_norm": 1.7552733421325684,
        "learning_rate": 0.0001960471901153493,
        "epoch": 0.09155201234409155,
        "step": 712
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.0532917976379395,
        "learning_rate": 0.00019603591521718638,
        "epoch": 0.09168059663109168,
        "step": 713
    },
    {
        "loss": 1.8643,
        "grad_norm": 2.297710657119751,
        "learning_rate": 0.00019602462458694363,
        "epoch": 0.09180918091809182,
        "step": 714
    },
    {
        "loss": 2.2714,
        "grad_norm": 1.2951079607009888,
        "learning_rate": 0.00019601331822647055,
        "epoch": 0.09193776520509193,
        "step": 715
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.385328769683838,
        "learning_rate": 0.00019600199613761937,
        "epoch": 0.09206634949209207,
        "step": 716
    },
    {
        "loss": 1.779,
        "grad_norm": 1.8460031747817993,
        "learning_rate": 0.00019599065832224478,
        "epoch": 0.0921949337790922,
        "step": 717
    },
    {
        "loss": 2.3497,
        "grad_norm": 1.3761167526245117,
        "learning_rate": 0.00019597930478220405,
        "epoch": 0.09232351806609232,
        "step": 718
    },
    {
        "loss": 1.9535,
        "grad_norm": 1.6049116849899292,
        "learning_rate": 0.0001959679355193571,
        "epoch": 0.09245210235309245,
        "step": 719
    },
    {
        "loss": 2.6051,
        "grad_norm": 1.1562904119491577,
        "learning_rate": 0.00019595655053556633,
        "epoch": 0.09258068664009259,
        "step": 720
    },
    {
        "loss": 2.2967,
        "grad_norm": 1.8866244554519653,
        "learning_rate": 0.00019594514983269678,
        "epoch": 0.0927092709270927,
        "step": 721
    },
    {
        "loss": 1.3289,
        "grad_norm": 1.8004140853881836,
        "learning_rate": 0.00019593373341261612,
        "epoch": 0.09283785521409284,
        "step": 722
    },
    {
        "loss": 2.2216,
        "grad_norm": 1.577350378036499,
        "learning_rate": 0.00019592230127719445,
        "epoch": 0.09296643950109297,
        "step": 723
    },
    {
        "loss": 1.8705,
        "grad_norm": 1.508935809135437,
        "learning_rate": 0.00019591085342830452,
        "epoch": 0.09309502378809309,
        "step": 724
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.6799408197402954,
        "learning_rate": 0.00019589938986782168,
        "epoch": 0.09322360807509322,
        "step": 725
    },
    {
        "loss": 2.0599,
        "grad_norm": 1.9953289031982422,
        "learning_rate": 0.00019588791059762386,
        "epoch": 0.09335219236209336,
        "step": 726
    },
    {
        "loss": 2.5648,
        "grad_norm": 1.0467419624328613,
        "learning_rate": 0.00019587641561959144,
        "epoch": 0.09348077664909347,
        "step": 727
    },
    {
        "loss": 2.5644,
        "grad_norm": 1.1273940801620483,
        "learning_rate": 0.00019586490493560756,
        "epoch": 0.09360936093609361,
        "step": 728
    },
    {
        "loss": 1.9905,
        "grad_norm": 1.6258251667022705,
        "learning_rate": 0.00019585337854755778,
        "epoch": 0.09373794522309374,
        "step": 729
    },
    {
        "loss": 2.2689,
        "grad_norm": 1.3912928104400635,
        "learning_rate": 0.00019584183645733033,
        "epoch": 0.09386652951009386,
        "step": 730
    },
    {
        "loss": 2.1162,
        "grad_norm": 2.3322930335998535,
        "learning_rate": 0.00019583027866681594,
        "epoch": 0.093995113797094,
        "step": 731
    },
    {
        "loss": 1.8386,
        "grad_norm": 1.7446269989013672,
        "learning_rate": 0.00019581870517790793,
        "epoch": 0.09412369808409413,
        "step": 732
    },
    {
        "loss": 2.0511,
        "grad_norm": 1.7889955043792725,
        "learning_rate": 0.00019580711599250224,
        "epoch": 0.09425228237109426,
        "step": 733
    },
    {
        "loss": 2.2662,
        "grad_norm": 1.0657384395599365,
        "learning_rate": 0.00019579551111249734,
        "epoch": 0.09438086665809438,
        "step": 734
    },
    {
        "loss": 2.538,
        "grad_norm": 1.3584073781967163,
        "learning_rate": 0.00019578389053979427,
        "epoch": 0.09450945094509451,
        "step": 735
    },
    {
        "loss": 1.7575,
        "grad_norm": 1.5914905071258545,
        "learning_rate": 0.00019577225427629667,
        "epoch": 0.09463803523209464,
        "step": 736
    },
    {
        "loss": 1.8584,
        "grad_norm": 1.8500415086746216,
        "learning_rate": 0.0001957606023239107,
        "epoch": 0.09476661951909476,
        "step": 737
    },
    {
        "loss": 1.5025,
        "grad_norm": 2.5672101974487305,
        "learning_rate": 0.00019574893468454513,
        "epoch": 0.0948952038060949,
        "step": 738
    },
    {
        "loss": 1.6896,
        "grad_norm": 2.041378974914551,
        "learning_rate": 0.00019573725136011127,
        "epoch": 0.09502378809309503,
        "step": 739
    },
    {
        "loss": 1.8506,
        "grad_norm": 1.789099097251892,
        "learning_rate": 0.00019572555235252307,
        "epoch": 0.09515237238009515,
        "step": 740
    },
    {
        "loss": 2.4755,
        "grad_norm": 1.5910884141921997,
        "learning_rate": 0.00019571383766369698,
        "epoch": 0.09528095666709528,
        "step": 741
    },
    {
        "loss": 2.1721,
        "grad_norm": 1.6814135313034058,
        "learning_rate": 0.00019570210729555202,
        "epoch": 0.09540954095409541,
        "step": 742
    },
    {
        "loss": 2.1552,
        "grad_norm": 1.677464485168457,
        "learning_rate": 0.00019569036125000974,
        "epoch": 0.09553812524109553,
        "step": 743
    },
    {
        "loss": 2.301,
        "grad_norm": 2.2800214290618896,
        "learning_rate": 0.00019567859952899442,
        "epoch": 0.09566670952809567,
        "step": 744
    },
    {
        "loss": 2.2392,
        "grad_norm": 1.5241756439208984,
        "learning_rate": 0.00019566682213443274,
        "epoch": 0.0957952938150958,
        "step": 745
    },
    {
        "loss": 2.094,
        "grad_norm": 1.4904701709747314,
        "learning_rate": 0.00019565502906825403,
        "epoch": 0.09592387810209592,
        "step": 746
    },
    {
        "loss": 2.3097,
        "grad_norm": 1.8776640892028809,
        "learning_rate": 0.00019564322033239017,
        "epoch": 0.09605246238909605,
        "step": 747
    },
    {
        "loss": 2.2517,
        "grad_norm": 1.3272868394851685,
        "learning_rate": 0.00019563139592877556,
        "epoch": 0.09618104667609619,
        "step": 748
    },
    {
        "loss": 2.27,
        "grad_norm": 0.9533711671829224,
        "learning_rate": 0.00019561955585934725,
        "epoch": 0.0963096309630963,
        "step": 749
    },
    {
        "loss": 1.4912,
        "grad_norm": 1.6956316232681274,
        "learning_rate": 0.00019560770012604483,
        "epoch": 0.09643821525009644,
        "step": 750
    },
    {
        "loss": 1.3003,
        "grad_norm": 2.6129207611083984,
        "learning_rate": 0.00019559582873081036,
        "epoch": 0.09656679953709657,
        "step": 751
    },
    {
        "loss": 2.3891,
        "grad_norm": 1.0588116645812988,
        "learning_rate": 0.00019558394167558865,
        "epoch": 0.09669538382409669,
        "step": 752
    },
    {
        "loss": 2.3649,
        "grad_norm": 1.5906859636306763,
        "learning_rate": 0.0001955720389623269,
        "epoch": 0.09682396811109682,
        "step": 753
    },
    {
        "loss": 2.3288,
        "grad_norm": 0.9982314109802246,
        "learning_rate": 0.00019556012059297498,
        "epoch": 0.09695255239809696,
        "step": 754
    },
    {
        "loss": 1.874,
        "grad_norm": 1.9752691984176636,
        "learning_rate": 0.00019554818656948532,
        "epoch": 0.09708113668509707,
        "step": 755
    },
    {
        "loss": 1.2981,
        "grad_norm": 2.0082173347473145,
        "learning_rate": 0.0001955362368938128,
        "epoch": 0.09720972097209721,
        "step": 756
    },
    {
        "loss": 2.4262,
        "grad_norm": 1.415135383605957,
        "learning_rate": 0.00019552427156791505,
        "epoch": 0.09733830525909734,
        "step": 757
    },
    {
        "loss": 1.831,
        "grad_norm": 1.360152244567871,
        "learning_rate": 0.00019551229059375206,
        "epoch": 0.09746688954609747,
        "step": 758
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.7497919797897339,
        "learning_rate": 0.00019550029397328659,
        "epoch": 0.09759547383309759,
        "step": 759
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.1471059322357178,
        "learning_rate": 0.0001954882817084838,
        "epoch": 0.09772405812009773,
        "step": 760
    },
    {
        "loss": 2.0391,
        "grad_norm": 1.290719747543335,
        "learning_rate": 0.0001954762538013115,
        "epoch": 0.09785264240709786,
        "step": 761
    },
    {
        "loss": 1.7785,
        "grad_norm": 1.332524299621582,
        "learning_rate": 0.00019546421025374002,
        "epoch": 0.09798122669409798,
        "step": 762
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.555086612701416,
        "learning_rate": 0.00019545215106774226,
        "epoch": 0.09810981098109811,
        "step": 763
    },
    {
        "loss": 2.0732,
        "grad_norm": 1.5593560934066772,
        "learning_rate": 0.00019544007624529373,
        "epoch": 0.09823839526809824,
        "step": 764
    },
    {
        "loss": 1.9344,
        "grad_norm": 1.8371384143829346,
        "learning_rate": 0.00019542798578837242,
        "epoch": 0.09836697955509836,
        "step": 765
    },
    {
        "loss": 1.4964,
        "grad_norm": 2.56438946723938,
        "learning_rate": 0.00019541587969895894,
        "epoch": 0.0984955638420985,
        "step": 766
    },
    {
        "loss": 1.9902,
        "grad_norm": 1.770790457725525,
        "learning_rate": 0.00019540375797903642,
        "epoch": 0.09862414812909863,
        "step": 767
    },
    {
        "loss": 1.6933,
        "grad_norm": 1.6253958940505981,
        "learning_rate": 0.00019539162063059065,
        "epoch": 0.09875273241609875,
        "step": 768
    },
    {
        "loss": 2.4566,
        "grad_norm": 1.194827675819397,
        "learning_rate": 0.00019537946765560982,
        "epoch": 0.09888131670309888,
        "step": 769
    },
    {
        "loss": 1.59,
        "grad_norm": 2.2235803604125977,
        "learning_rate": 0.00019536729905608477,
        "epoch": 0.09900990099009901,
        "step": 770
    },
    {
        "loss": 1.6786,
        "grad_norm": 2.290649175643921,
        "learning_rate": 0.00019535511483400898,
        "epoch": 0.09913848527709913,
        "step": 771
    },
    {
        "loss": 2.2796,
        "grad_norm": 1.4801028966903687,
        "learning_rate": 0.00019534291499137828,
        "epoch": 0.09926706956409927,
        "step": 772
    },
    {
        "loss": 2.1053,
        "grad_norm": 1.4147932529449463,
        "learning_rate": 0.00019533069953019127,
        "epoch": 0.0993956538510994,
        "step": 773
    },
    {
        "loss": 1.7399,
        "grad_norm": 1.5732415914535522,
        "learning_rate": 0.000195318468452449,
        "epoch": 0.09952423813809952,
        "step": 774
    },
    {
        "loss": 1.8905,
        "grad_norm": 1.4738374948501587,
        "learning_rate": 0.00019530622176015507,
        "epoch": 0.09965282242509965,
        "step": 775
    },
    {
        "loss": 2.3095,
        "grad_norm": 1.9551881551742554,
        "learning_rate": 0.0001952939594553157,
        "epoch": 0.09978140671209978,
        "step": 776
    },
    {
        "loss": 1.9118,
        "grad_norm": 1.628462314605713,
        "learning_rate": 0.0001952816815399396,
        "epoch": 0.0999099909990999,
        "step": 777
    },
    {
        "loss": 1.6333,
        "grad_norm": 1.6025793552398682,
        "learning_rate": 0.00019526938801603814,
        "epoch": 0.10003857528610004,
        "step": 778
    },
    {
        "loss": 2.2925,
        "grad_norm": 1.4714299440383911,
        "learning_rate": 0.0001952570788856251,
        "epoch": 0.10016715957310017,
        "step": 779
    },
    {
        "loss": 2.3113,
        "grad_norm": 1.4448423385620117,
        "learning_rate": 0.00019524475415071693,
        "epoch": 0.10029574386010029,
        "step": 780
    },
    {
        "loss": 2.637,
        "grad_norm": 1.6294294595718384,
        "learning_rate": 0.0001952324138133326,
        "epoch": 0.10042432814710042,
        "step": 781
    },
    {
        "loss": 2.5635,
        "grad_norm": 1.3350379467010498,
        "learning_rate": 0.00019522005787549361,
        "epoch": 0.10055291243410056,
        "step": 782
    },
    {
        "loss": 1.773,
        "grad_norm": 1.6332868337631226,
        "learning_rate": 0.0001952076863392241,
        "epoch": 0.10068149672110069,
        "step": 783
    },
    {
        "loss": 2.344,
        "grad_norm": 2.118520736694336,
        "learning_rate": 0.00019519529920655065,
        "epoch": 0.10081008100810081,
        "step": 784
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.7098886966705322,
        "learning_rate": 0.00019518289647950246,
        "epoch": 0.10093866529510094,
        "step": 785
    },
    {
        "loss": 2.6383,
        "grad_norm": 1.223454236984253,
        "learning_rate": 0.00019517047816011135,
        "epoch": 0.10106724958210107,
        "step": 786
    },
    {
        "loss": 2.0409,
        "grad_norm": 1.896295428276062,
        "learning_rate": 0.0001951580442504115,
        "epoch": 0.10119583386910119,
        "step": 787
    },
    {
        "loss": 1.9441,
        "grad_norm": 1.6893681287765503,
        "learning_rate": 0.00019514559475243987,
        "epoch": 0.10132441815610133,
        "step": 788
    },
    {
        "loss": 1.5166,
        "grad_norm": 1.594115972518921,
        "learning_rate": 0.00019513312966823581,
        "epoch": 0.10145300244310146,
        "step": 789
    },
    {
        "loss": 1.2857,
        "grad_norm": 2.2650091648101807,
        "learning_rate": 0.0001951206489998413,
        "epoch": 0.10158158673010158,
        "step": 790
    },
    {
        "loss": 2.5589,
        "grad_norm": 1.1928455829620361,
        "learning_rate": 0.00019510815274930088,
        "epoch": 0.10171017101710171,
        "step": 791
    },
    {
        "loss": 2.13,
        "grad_norm": 1.7357300519943237,
        "learning_rate": 0.00019509564091866154,
        "epoch": 0.10183875530410184,
        "step": 792
    },
    {
        "loss": 2.0319,
        "grad_norm": 1.7017608880996704,
        "learning_rate": 0.00019508311350997302,
        "epoch": 0.10196733959110196,
        "step": 793
    },
    {
        "loss": 1.9639,
        "grad_norm": 1.6444820165634155,
        "learning_rate": 0.00019507057052528737,
        "epoch": 0.1020959238781021,
        "step": 794
    },
    {
        "loss": 2.04,
        "grad_norm": 1.5136679410934448,
        "learning_rate": 0.0001950580119666594,
        "epoch": 0.10222450816510223,
        "step": 795
    },
    {
        "loss": 2.3799,
        "grad_norm": 1.2133430242538452,
        "learning_rate": 0.00019504543783614633,
        "epoch": 0.10235309245210235,
        "step": 796
    },
    {
        "loss": 2.1421,
        "grad_norm": 1.7990342378616333,
        "learning_rate": 0.00019503284813580802,
        "epoch": 0.10248167673910248,
        "step": 797
    },
    {
        "loss": 2.2508,
        "grad_norm": 2.1218864917755127,
        "learning_rate": 0.00019502024286770683,
        "epoch": 0.10261026102610261,
        "step": 798
    },
    {
        "loss": 1.8049,
        "grad_norm": 2.1703109741210938,
        "learning_rate": 0.0001950076220339077,
        "epoch": 0.10273884531310273,
        "step": 799
    },
    {
        "loss": 1.3114,
        "grad_norm": 2.646686553955078,
        "learning_rate": 0.00019499498563647807,
        "epoch": 0.10286742960010287,
        "step": 800
    },
    {
        "eval_loss": 2.0750620365142822,
        "eval_runtime": 28.3124,
        "eval_samples_per_second": 2.79,
        "eval_steps_per_second": 2.79,
        "epoch": 0.10286742960010287,
        "step": 800
    },
    {
        "loss": 2.1542,
        "grad_norm": 1.751947283744812,
        "learning_rate": 0.00019498233367748802,
        "epoch": 0.102996013887103,
        "step": 801
    },
    {
        "loss": 1.6034,
        "grad_norm": 1.9035193920135498,
        "learning_rate": 0.00019496966615901008,
        "epoch": 0.10312459817410312,
        "step": 802
    },
    {
        "loss": 2.3146,
        "grad_norm": 1.4776458740234375,
        "learning_rate": 0.0001949569830831194,
        "epoch": 0.10325318246110325,
        "step": 803
    },
    {
        "loss": 2.3917,
        "grad_norm": 1.389169692993164,
        "learning_rate": 0.00019494428445189366,
        "epoch": 0.10338176674810338,
        "step": 804
    },
    {
        "loss": 1.5453,
        "grad_norm": 1.5606720447540283,
        "learning_rate": 0.00019493157026741304,
        "epoch": 0.1035103510351035,
        "step": 805
    },
    {
        "loss": 2.1236,
        "grad_norm": 1.412001609802246,
        "learning_rate": 0.00019491884053176033,
        "epoch": 0.10363893532210364,
        "step": 806
    },
    {
        "loss": 2.5128,
        "grad_norm": 2.8321375846862793,
        "learning_rate": 0.00019490609524702084,
        "epoch": 0.10376751960910377,
        "step": 807
    },
    {
        "loss": 2.1185,
        "grad_norm": 2.3292784690856934,
        "learning_rate": 0.00019489333441528246,
        "epoch": 0.1038961038961039,
        "step": 808
    },
    {
        "loss": 2.226,
        "grad_norm": 1.2258484363555908,
        "learning_rate": 0.00019488055803863557,
        "epoch": 0.10402468818310402,
        "step": 809
    },
    {
        "loss": 1.6948,
        "grad_norm": 1.6448819637298584,
        "learning_rate": 0.00019486776611917314,
        "epoch": 0.10415327247010416,
        "step": 810
    },
    {
        "loss": 0.7852,
        "grad_norm": 1.9451593160629272,
        "learning_rate": 0.00019485495865899068,
        "epoch": 0.10428185675710429,
        "step": 811
    },
    {
        "loss": 1.8884,
        "grad_norm": 1.7871747016906738,
        "learning_rate": 0.0001948421356601862,
        "epoch": 0.10441044104410441,
        "step": 812
    },
    {
        "loss": 2.4242,
        "grad_norm": 1.3374383449554443,
        "learning_rate": 0.00019482929712486035,
        "epoch": 0.10453902533110454,
        "step": 813
    },
    {
        "loss": 2.2707,
        "grad_norm": 1.934821367263794,
        "learning_rate": 0.00019481644305511622,
        "epoch": 0.10466760961810467,
        "step": 814
    },
    {
        "loss": 1.8621,
        "grad_norm": 1.4500617980957031,
        "learning_rate": 0.00019480357345305955,
        "epoch": 0.10479619390510479,
        "step": 815
    },
    {
        "loss": 2.3744,
        "grad_norm": 1.7450566291809082,
        "learning_rate": 0.00019479068832079847,
        "epoch": 0.10492477819210493,
        "step": 816
    },
    {
        "loss": 2.4611,
        "grad_norm": 1.3915581703186035,
        "learning_rate": 0.00019477778766044388,
        "epoch": 0.10505336247910506,
        "step": 817
    },
    {
        "loss": 2.5506,
        "grad_norm": 1.8275823593139648,
        "learning_rate": 0.000194764871474109,
        "epoch": 0.10518194676610518,
        "step": 818
    },
    {
        "loss": 2.3595,
        "grad_norm": 1.8813308477401733,
        "learning_rate": 0.0001947519397639097,
        "epoch": 0.10531053105310531,
        "step": 819
    },
    {
        "loss": 1.878,
        "grad_norm": 1.8067787885665894,
        "learning_rate": 0.00019473899253196443,
        "epoch": 0.10543911534010544,
        "step": 820
    },
    {
        "loss": 2.0553,
        "grad_norm": 1.6384005546569824,
        "learning_rate": 0.0001947260297803941,
        "epoch": 0.10556769962710556,
        "step": 821
    },
    {
        "loss": 1.8488,
        "grad_norm": 1.559471607208252,
        "learning_rate": 0.0001947130515113222,
        "epoch": 0.1056962839141057,
        "step": 822
    },
    {
        "loss": 2.3462,
        "grad_norm": 1.105869174003601,
        "learning_rate": 0.00019470005772687476,
        "epoch": 0.10582486820110583,
        "step": 823
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.3472718000411987,
        "learning_rate": 0.00019468704842918035,
        "epoch": 0.10595345248810595,
        "step": 824
    },
    {
        "loss": 1.5874,
        "grad_norm": 2.1081089973449707,
        "learning_rate": 0.00019467402362037008,
        "epoch": 0.10608203677510608,
        "step": 825
    },
    {
        "loss": 1.382,
        "grad_norm": 1.7426068782806396,
        "learning_rate": 0.00019466098330257764,
        "epoch": 0.10621062106210621,
        "step": 826
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.892930269241333,
        "learning_rate": 0.00019464792747793917,
        "epoch": 0.10633920534910633,
        "step": 827
    },
    {
        "loss": 2.3466,
        "grad_norm": 1.7200791835784912,
        "learning_rate": 0.00019463485614859343,
        "epoch": 0.10646778963610647,
        "step": 828
    },
    {
        "loss": 1.438,
        "grad_norm": 1.9923962354660034,
        "learning_rate": 0.00019462176931668167,
        "epoch": 0.1065963739231066,
        "step": 829
    },
    {
        "loss": 2.4682,
        "grad_norm": 1.3692041635513306,
        "learning_rate": 0.00019460866698434775,
        "epoch": 0.10672495821010672,
        "step": 830
    },
    {
        "loss": 1.9831,
        "grad_norm": 1.7401304244995117,
        "learning_rate": 0.000194595549153738,
        "epoch": 0.10685354249710685,
        "step": 831
    },
    {
        "loss": 1.8269,
        "grad_norm": 2.0071256160736084,
        "learning_rate": 0.00019458241582700128,
        "epoch": 0.10698212678410698,
        "step": 832
    },
    {
        "loss": 1.8582,
        "grad_norm": 1.8187175989151,
        "learning_rate": 0.00019456926700628908,
        "epoch": 0.10711071107110712,
        "step": 833
    },
    {
        "loss": 1.7599,
        "grad_norm": 1.5223098993301392,
        "learning_rate": 0.00019455610269375533,
        "epoch": 0.10723929535810724,
        "step": 834
    },
    {
        "loss": 1.7459,
        "grad_norm": 2.1391873359680176,
        "learning_rate": 0.00019454292289155652,
        "epoch": 0.10736787964510737,
        "step": 835
    },
    {
        "loss": 1.9113,
        "grad_norm": 1.701276183128357,
        "learning_rate": 0.00019452972760185174,
        "epoch": 0.1074964639321075,
        "step": 836
    },
    {
        "loss": 2.2373,
        "grad_norm": 1.4841541051864624,
        "learning_rate": 0.0001945165168268025,
        "epoch": 0.10762504821910762,
        "step": 837
    },
    {
        "loss": 2.0394,
        "grad_norm": 1.482230544090271,
        "learning_rate": 0.00019450329056857303,
        "epoch": 0.10775363250610775,
        "step": 838
    },
    {
        "loss": 1.2951,
        "grad_norm": 2.0800418853759766,
        "learning_rate": 0.0001944900488293299,
        "epoch": 0.10788221679310789,
        "step": 839
    },
    {
        "loss": 2.5569,
        "grad_norm": 1.7852685451507568,
        "learning_rate": 0.00019447679161124228,
        "epoch": 0.10801080108010801,
        "step": 840
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.5237839221954346,
        "learning_rate": 0.00019446351891648194,
        "epoch": 0.10813938536710814,
        "step": 841
    },
    {
        "loss": 1.909,
        "grad_norm": 1.6085103750228882,
        "learning_rate": 0.00019445023074722315,
        "epoch": 0.10826796965410827,
        "step": 842
    },
    {
        "loss": 1.6206,
        "grad_norm": 1.4487475156784058,
        "learning_rate": 0.00019443692710564267,
        "epoch": 0.10839655394110839,
        "step": 843
    },
    {
        "loss": 2.1975,
        "grad_norm": 1.596447229385376,
        "learning_rate": 0.00019442360799391985,
        "epoch": 0.10852513822810853,
        "step": 844
    },
    {
        "loss": 2.4729,
        "grad_norm": 1.3468321561813354,
        "learning_rate": 0.00019441027341423657,
        "epoch": 0.10865372251510866,
        "step": 845
    },
    {
        "loss": 1.6288,
        "grad_norm": 1.6662278175354004,
        "learning_rate": 0.0001943969233687772,
        "epoch": 0.10878230680210878,
        "step": 846
    },
    {
        "loss": 2.29,
        "grad_norm": 1.0900535583496094,
        "learning_rate": 0.00019438355785972867,
        "epoch": 0.10891089108910891,
        "step": 847
    },
    {
        "loss": 2.1191,
        "grad_norm": 1.5741184949874878,
        "learning_rate": 0.00019437017688928046,
        "epoch": 0.10903947537610904,
        "step": 848
    },
    {
        "loss": 2.1725,
        "grad_norm": 2.025125026702881,
        "learning_rate": 0.00019435678045962457,
        "epoch": 0.10916805966310916,
        "step": 849
    },
    {
        "loss": 2.1403,
        "grad_norm": 1.4692569971084595,
        "learning_rate": 0.00019434336857295553,
        "epoch": 0.1092966439501093,
        "step": 850
    },
    {
        "loss": 2.4193,
        "grad_norm": 1.064043402671814,
        "learning_rate": 0.0001943299412314704,
        "epoch": 0.10942522823710943,
        "step": 851
    },
    {
        "loss": 2.4711,
        "grad_norm": 1.3625518083572388,
        "learning_rate": 0.00019431649843736873,
        "epoch": 0.10955381252410955,
        "step": 852
    },
    {
        "loss": 2.0768,
        "grad_norm": 1.429775595664978,
        "learning_rate": 0.0001943030401928527,
        "epoch": 0.10968239681110968,
        "step": 853
    },
    {
        "loss": 2.0336,
        "grad_norm": 1.552736759185791,
        "learning_rate": 0.00019428956650012697,
        "epoch": 0.10981098109810981,
        "step": 854
    },
    {
        "loss": 1.5688,
        "grad_norm": 1.9624019861221313,
        "learning_rate": 0.0001942760773613987,
        "epoch": 0.10993956538510993,
        "step": 855
    },
    {
        "loss": 2.0193,
        "grad_norm": 1.7591137886047363,
        "learning_rate": 0.0001942625727788776,
        "epoch": 0.11006814967211007,
        "step": 856
    },
    {
        "loss": 2.0704,
        "grad_norm": 2.01069712638855,
        "learning_rate": 0.00019424905275477596,
        "epoch": 0.1101967339591102,
        "step": 857
    },
    {
        "loss": 2.1862,
        "grad_norm": 1.2823128700256348,
        "learning_rate": 0.0001942355172913085,
        "epoch": 0.11032531824611033,
        "step": 858
    },
    {
        "loss": 1.9928,
        "grad_norm": 1.688124179840088,
        "learning_rate": 0.00019422196639069258,
        "epoch": 0.11045390253311045,
        "step": 859
    },
    {
        "loss": 2.281,
        "grad_norm": 1.4353318214416504,
        "learning_rate": 0.00019420840005514797,
        "epoch": 0.11058248682011058,
        "step": 860
    },
    {
        "loss": 1.9521,
        "grad_norm": 1.3293555974960327,
        "learning_rate": 0.0001941948182868971,
        "epoch": 0.11071107110711072,
        "step": 861
    },
    {
        "loss": 1.6486,
        "grad_norm": 2.848900556564331,
        "learning_rate": 0.00019418122108816484,
        "epoch": 0.11083965539411084,
        "step": 862
    },
    {
        "loss": 2.3534,
        "grad_norm": 1.5199840068817139,
        "learning_rate": 0.00019416760846117858,
        "epoch": 0.11096823968111097,
        "step": 863
    },
    {
        "loss": 1.602,
        "grad_norm": 1.7922443151474,
        "learning_rate": 0.0001941539804081683,
        "epoch": 0.1110968239681111,
        "step": 864
    },
    {
        "loss": 1.8694,
        "grad_norm": 1.6786161661148071,
        "learning_rate": 0.00019414033693136647,
        "epoch": 0.11122540825511122,
        "step": 865
    },
    {
        "loss": 1.9084,
        "grad_norm": 1.9313676357269287,
        "learning_rate": 0.00019412667803300805,
        "epoch": 0.11135399254211135,
        "step": 866
    },
    {
        "loss": 1.7547,
        "grad_norm": 1.9779754877090454,
        "learning_rate": 0.00019411300371533063,
        "epoch": 0.11148257682911149,
        "step": 867
    },
    {
        "loss": 1.7626,
        "grad_norm": 1.68197500705719,
        "learning_rate": 0.00019409931398057425,
        "epoch": 0.1116111611161116,
        "step": 868
    },
    {
        "loss": 2.2096,
        "grad_norm": 1.607354760169983,
        "learning_rate": 0.0001940856088309814,
        "epoch": 0.11173974540311174,
        "step": 869
    },
    {
        "loss": 2.2019,
        "grad_norm": 2.103631019592285,
        "learning_rate": 0.0001940718882687973,
        "epoch": 0.11186832969011187,
        "step": 870
    },
    {
        "loss": 1.377,
        "grad_norm": 1.9582743644714355,
        "learning_rate": 0.00019405815229626953,
        "epoch": 0.11199691397711199,
        "step": 871
    },
    {
        "loss": 2.2445,
        "grad_norm": 1.3021535873413086,
        "learning_rate": 0.00019404440091564821,
        "epoch": 0.11212549826411213,
        "step": 872
    },
    {
        "loss": 2.321,
        "grad_norm": 1.4656853675842285,
        "learning_rate": 0.00019403063412918608,
        "epoch": 0.11225408255111226,
        "step": 873
    },
    {
        "loss": 1.8197,
        "grad_norm": 1.3923758268356323,
        "learning_rate": 0.00019401685193913827,
        "epoch": 0.11238266683811238,
        "step": 874
    },
    {
        "loss": 1.9465,
        "grad_norm": 1.9933668375015259,
        "learning_rate": 0.00019400305434776256,
        "epoch": 0.11251125112511251,
        "step": 875
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.9721170663833618,
        "learning_rate": 0.00019398924135731918,
        "epoch": 0.11263983541211264,
        "step": 876
    },
    {
        "loss": 1.8473,
        "grad_norm": 2.1828198432922363,
        "learning_rate": 0.00019397541297007086,
        "epoch": 0.11276841969911276,
        "step": 877
    },
    {
        "loss": 1.9966,
        "grad_norm": 1.201857089996338,
        "learning_rate": 0.00019396156918828297,
        "epoch": 0.1128970039861129,
        "step": 878
    },
    {
        "loss": 2.1005,
        "grad_norm": 1.0653529167175293,
        "learning_rate": 0.00019394771001422328,
        "epoch": 0.11302558827311303,
        "step": 879
    },
    {
        "loss": 2.2262,
        "grad_norm": 1.7671018838882446,
        "learning_rate": 0.00019393383545016207,
        "epoch": 0.11315417256011315,
        "step": 880
    },
    {
        "loss": 1.8304,
        "grad_norm": 1.7202262878417969,
        "learning_rate": 0.0001939199454983723,
        "epoch": 0.11328275684711328,
        "step": 881
    },
    {
        "loss": 1.6095,
        "grad_norm": 1.7585768699645996,
        "learning_rate": 0.0001939060401611293,
        "epoch": 0.11341134113411341,
        "step": 882
    },
    {
        "loss": 1.7963,
        "grad_norm": 1.4955376386642456,
        "learning_rate": 0.00019389211944071094,
        "epoch": 0.11353992542111355,
        "step": 883
    },
    {
        "loss": 2.1644,
        "grad_norm": 1.538794994354248,
        "learning_rate": 0.00019387818333939762,
        "epoch": 0.11366850970811367,
        "step": 884
    },
    {
        "loss": 1.7023,
        "grad_norm": 2.1065833568573,
        "learning_rate": 0.00019386423185947238,
        "epoch": 0.1137970939951138,
        "step": 885
    },
    {
        "loss": 1.6499,
        "grad_norm": 2.270535707473755,
        "learning_rate": 0.00019385026500322056,
        "epoch": 0.11392567828211393,
        "step": 886
    },
    {
        "loss": 1.6717,
        "grad_norm": 2.0311145782470703,
        "learning_rate": 0.0001938362827729302,
        "epoch": 0.11405426256911405,
        "step": 887
    },
    {
        "loss": 1.9782,
        "grad_norm": 2.72711443901062,
        "learning_rate": 0.00019382228517089177,
        "epoch": 0.11418284685611418,
        "step": 888
    },
    {
        "loss": 2.251,
        "grad_norm": 1.452974557876587,
        "learning_rate": 0.0001938082721993983,
        "epoch": 0.11431143114311432,
        "step": 889
    },
    {
        "loss": 1.9562,
        "grad_norm": 2.006319761276245,
        "learning_rate": 0.0001937942438607453,
        "epoch": 0.11444001543011444,
        "step": 890
    },
    {
        "loss": 2.3045,
        "grad_norm": 1.8613569736480713,
        "learning_rate": 0.00019378020015723083,
        "epoch": 0.11456859971711457,
        "step": 891
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.8896416425704956,
        "learning_rate": 0.00019376614109115542,
        "epoch": 0.1146971840041147,
        "step": 892
    },
    {
        "loss": 2.6056,
        "grad_norm": 1.3157553672790527,
        "learning_rate": 0.00019375206666482218,
        "epoch": 0.11482576829111482,
        "step": 893
    },
    {
        "loss": 1.8191,
        "grad_norm": 1.5886728763580322,
        "learning_rate": 0.0001937379768805367,
        "epoch": 0.11495435257811495,
        "step": 894
    },
    {
        "loss": 1.6921,
        "grad_norm": 1.9536408185958862,
        "learning_rate": 0.00019372387174060708,
        "epoch": 0.11508293686511509,
        "step": 895
    },
    {
        "loss": 1.4581,
        "grad_norm": 1.6937692165374756,
        "learning_rate": 0.00019370975124734398,
        "epoch": 0.1152115211521152,
        "step": 896
    },
    {
        "loss": 1.9816,
        "grad_norm": 1.994895577430725,
        "learning_rate": 0.0001936956154030605,
        "epoch": 0.11534010543911534,
        "step": 897
    },
    {
        "loss": 1.7856,
        "grad_norm": 2.006700277328491,
        "learning_rate": 0.00019368146421007234,
        "epoch": 0.11546868972611547,
        "step": 898
    },
    {
        "loss": 2.2727,
        "grad_norm": 1.596376895904541,
        "learning_rate": 0.00019366729767069765,
        "epoch": 0.11559727401311559,
        "step": 899
    },
    {
        "loss": 2.1924,
        "grad_norm": 1.6101984977722168,
        "learning_rate": 0.0001936531157872571,
        "epoch": 0.11572585830011572,
        "step": 900
    },
    {
        "eval_loss": 2.0774989128112793,
        "eval_runtime": 28.2834,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.11572585830011572,
        "step": 900
    },
    {
        "loss": 2.0908,
        "grad_norm": 2.008538246154785,
        "learning_rate": 0.0001936389185620739,
        "epoch": 0.11585444258711586,
        "step": 901
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.536203384399414,
        "learning_rate": 0.00019362470599747377,
        "epoch": 0.11598302687411598,
        "step": 902
    },
    {
        "loss": 1.9968,
        "grad_norm": 2.0761523246765137,
        "learning_rate": 0.00019361047809578492,
        "epoch": 0.11611161116111611,
        "step": 903
    },
    {
        "loss": 2.2317,
        "grad_norm": 1.4689067602157593,
        "learning_rate": 0.0001935962348593381,
        "epoch": 0.11624019544811624,
        "step": 904
    },
    {
        "loss": 2.1973,
        "grad_norm": 1.9409757852554321,
        "learning_rate": 0.00019358197629046654,
        "epoch": 0.11636877973511636,
        "step": 905
    },
    {
        "loss": 1.4263,
        "grad_norm": 2.241410970687866,
        "learning_rate": 0.00019356770239150603,
        "epoch": 0.1164973640221165,
        "step": 906
    },
    {
        "loss": 2.2741,
        "grad_norm": 1.6832318305969238,
        "learning_rate": 0.00019355341316479484,
        "epoch": 0.11662594830911663,
        "step": 907
    },
    {
        "loss": 2.019,
        "grad_norm": 1.3795020580291748,
        "learning_rate": 0.00019353910861267374,
        "epoch": 0.11675453259611676,
        "step": 908
    },
    {
        "loss": 2.2489,
        "grad_norm": 1.5652611255645752,
        "learning_rate": 0.00019352478873748604,
        "epoch": 0.11688311688311688,
        "step": 909
    },
    {
        "loss": 1.6298,
        "grad_norm": 2.0122592449188232,
        "learning_rate": 0.00019351045354157749,
        "epoch": 0.11701170117011701,
        "step": 910
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.5400646924972534,
        "learning_rate": 0.00019349610302729647,
        "epoch": 0.11714028545711715,
        "step": 911
    },
    {
        "loss": 2.6115,
        "grad_norm": 1.4121859073638916,
        "learning_rate": 0.00019348173719699376,
        "epoch": 0.11726886974411727,
        "step": 912
    },
    {
        "loss": 1.8241,
        "grad_norm": 2.175493001937866,
        "learning_rate": 0.00019346735605302274,
        "epoch": 0.1173974540311174,
        "step": 913
    },
    {
        "loss": 2.5198,
        "grad_norm": 1.3999946117401123,
        "learning_rate": 0.00019345295959773918,
        "epoch": 0.11752603831811753,
        "step": 914
    },
    {
        "loss": 2.0474,
        "grad_norm": 2.2781267166137695,
        "learning_rate": 0.00019343854783350148,
        "epoch": 0.11765462260511765,
        "step": 915
    },
    {
        "loss": 2.463,
        "grad_norm": 1.407091736793518,
        "learning_rate": 0.0001934241207626705,
        "epoch": 0.11778320689211778,
        "step": 916
    },
    {
        "loss": 1.8768,
        "grad_norm": 1.8577231168746948,
        "learning_rate": 0.00019340967838760956,
        "epoch": 0.11791179117911792,
        "step": 917
    },
    {
        "loss": 1.677,
        "grad_norm": 1.696018099784851,
        "learning_rate": 0.00019339522071068458,
        "epoch": 0.11804037546611804,
        "step": 918
    },
    {
        "loss": 1.2647,
        "grad_norm": 1.94475519657135,
        "learning_rate": 0.0001933807477342639,
        "epoch": 0.11816895975311817,
        "step": 919
    },
    {
        "loss": 2.2013,
        "grad_norm": 2.090358018875122,
        "learning_rate": 0.00019336625946071845,
        "epoch": 0.1182975440401183,
        "step": 920
    },
    {
        "loss": 1.9432,
        "grad_norm": 1.8581336736679077,
        "learning_rate": 0.00019335175589242157,
        "epoch": 0.11842612832711842,
        "step": 921
    },
    {
        "loss": 1.5305,
        "grad_norm": 2.1360206604003906,
        "learning_rate": 0.00019333723703174918,
        "epoch": 0.11855471261411855,
        "step": 922
    },
    {
        "loss": 2.2041,
        "grad_norm": 2.2399468421936035,
        "learning_rate": 0.00019332270288107965,
        "epoch": 0.11868329690111869,
        "step": 923
    },
    {
        "loss": 2.1121,
        "grad_norm": 2.08874773979187,
        "learning_rate": 0.00019330815344279395,
        "epoch": 0.1188118811881188,
        "step": 924
    },
    {
        "loss": 2.201,
        "grad_norm": 1.6477843523025513,
        "learning_rate": 0.00019329358871927546,
        "epoch": 0.11894046547511894,
        "step": 925
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.8235557079315186,
        "learning_rate": 0.00019327900871291004,
        "epoch": 0.11906904976211907,
        "step": 926
    },
    {
        "loss": 1.5627,
        "grad_norm": 1.7809230089187622,
        "learning_rate": 0.00019326441342608618,
        "epoch": 0.11919763404911919,
        "step": 927
    },
    {
        "loss": 2.3632,
        "grad_norm": 1.3520444631576538,
        "learning_rate": 0.00019324980286119478,
        "epoch": 0.11932621833611932,
        "step": 928
    },
    {
        "loss": 1.8285,
        "grad_norm": 1.4347319602966309,
        "learning_rate": 0.00019323517702062926,
        "epoch": 0.11945480262311946,
        "step": 929
    },
    {
        "loss": 2.0983,
        "grad_norm": 1.5197741985321045,
        "learning_rate": 0.00019322053590678552,
        "epoch": 0.11958338691011958,
        "step": 930
    },
    {
        "loss": 2.4031,
        "grad_norm": 2.0048675537109375,
        "learning_rate": 0.000193205879522062,
        "epoch": 0.11971197119711971,
        "step": 931
    },
    {
        "loss": 1.8379,
        "grad_norm": 1.334073781967163,
        "learning_rate": 0.00019319120786885966,
        "epoch": 0.11984055548411984,
        "step": 932
    },
    {
        "loss": 1.9235,
        "grad_norm": 1.8779079914093018,
        "learning_rate": 0.00019317652094958193,
        "epoch": 0.11996913977111998,
        "step": 933
    },
    {
        "loss": 1.1088,
        "grad_norm": 2.561284065246582,
        "learning_rate": 0.0001931618187666347,
        "epoch": 0.1200977240581201,
        "step": 934
    },
    {
        "loss": 2.0797,
        "grad_norm": 1.7511615753173828,
        "learning_rate": 0.00019314710132242643,
        "epoch": 0.12022630834512023,
        "step": 935
    },
    {
        "loss": 1.8364,
        "grad_norm": 1.6520723104476929,
        "learning_rate": 0.00019313236861936801,
        "epoch": 0.12035489263212036,
        "step": 936
    },
    {
        "loss": 1.5744,
        "grad_norm": 1.794941782951355,
        "learning_rate": 0.00019311762065987292,
        "epoch": 0.12048347691912048,
        "step": 937
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.0099701881408691,
        "learning_rate": 0.00019310285744635706,
        "epoch": 0.12061206120612061,
        "step": 938
    },
    {
        "loss": 1.5921,
        "grad_norm": 1.8673914670944214,
        "learning_rate": 0.00019308807898123887,
        "epoch": 0.12074064549312075,
        "step": 939
    },
    {
        "loss": 2.0271,
        "grad_norm": 1.8658465147018433,
        "learning_rate": 0.00019307328526693932,
        "epoch": 0.12086922978012087,
        "step": 940
    },
    {
        "loss": 1.5088,
        "grad_norm": 2.2801709175109863,
        "learning_rate": 0.00019305847630588177,
        "epoch": 0.120997814067121,
        "step": 941
    },
    {
        "loss": 1.9736,
        "grad_norm": 1.7885533571243286,
        "learning_rate": 0.00019304365210049215,
        "epoch": 0.12112639835412113,
        "step": 942
    },
    {
        "loss": 2.0747,
        "grad_norm": 1.6332300901412964,
        "learning_rate": 0.00019302881265319895,
        "epoch": 0.12125498264112125,
        "step": 943
    },
    {
        "loss": 1.5524,
        "grad_norm": 1.2752877473831177,
        "learning_rate": 0.00019301395796643298,
        "epoch": 0.12138356692812138,
        "step": 944
    },
    {
        "loss": 2.4086,
        "grad_norm": 0.8323424458503723,
        "learning_rate": 0.00019299908804262775,
        "epoch": 0.12151215121512152,
        "step": 945
    },
    {
        "loss": 2.5086,
        "grad_norm": 1.3648271560668945,
        "learning_rate": 0.0001929842028842191,
        "epoch": 0.12164073550212164,
        "step": 946
    },
    {
        "loss": 2.5916,
        "grad_norm": 1.3762445449829102,
        "learning_rate": 0.00019296930249364548,
        "epoch": 0.12176931978912177,
        "step": 947
    },
    {
        "loss": 2.0825,
        "grad_norm": 1.3816100358963013,
        "learning_rate": 0.0001929543868733478,
        "epoch": 0.1218979040761219,
        "step": 948
    },
    {
        "loss": 2.3204,
        "grad_norm": 1.565229892730713,
        "learning_rate": 0.00019293945602576938,
        "epoch": 0.12202648836312202,
        "step": 949
    },
    {
        "loss": 2.3317,
        "grad_norm": 0.9363146424293518,
        "learning_rate": 0.0001929245099533562,
        "epoch": 0.12215507265012215,
        "step": 950
    },
    {
        "loss": 2.314,
        "grad_norm": 1.311540961265564,
        "learning_rate": 0.00019290954865855658,
        "epoch": 0.12228365693712229,
        "step": 951
    },
    {
        "loss": 2.2178,
        "grad_norm": 1.2120765447616577,
        "learning_rate": 0.00019289457214382148,
        "epoch": 0.1224122412241224,
        "step": 952
    },
    {
        "loss": 2.179,
        "grad_norm": 1.5371536016464233,
        "learning_rate": 0.00019287958041160416,
        "epoch": 0.12254082551112254,
        "step": 953
    },
    {
        "loss": 1.5592,
        "grad_norm": 1.5884708166122437,
        "learning_rate": 0.00019286457346436055,
        "epoch": 0.12266940979812267,
        "step": 954
    },
    {
        "loss": 1.4409,
        "grad_norm": 2.234790325164795,
        "learning_rate": 0.000192849551304549,
        "epoch": 0.12279799408512279,
        "step": 955
    },
    {
        "loss": 2.1411,
        "grad_norm": 1.5828661918640137,
        "learning_rate": 0.00019283451393463036,
        "epoch": 0.12292657837212292,
        "step": 956
    },
    {
        "loss": 1.7039,
        "grad_norm": 1.8761781454086304,
        "learning_rate": 0.00019281946135706797,
        "epoch": 0.12305516265912306,
        "step": 957
    },
    {
        "loss": 2.0135,
        "grad_norm": 1.4141364097595215,
        "learning_rate": 0.00019280439357432762,
        "epoch": 0.12318374694612319,
        "step": 958
    },
    {
        "loss": 2.1275,
        "grad_norm": 1.6269631385803223,
        "learning_rate": 0.0001927893105888777,
        "epoch": 0.12331233123312331,
        "step": 959
    },
    {
        "loss": 2.3081,
        "grad_norm": 1.2816485166549683,
        "learning_rate": 0.00019277421240318897,
        "epoch": 0.12344091552012344,
        "step": 960
    },
    {
        "loss": 1.8526,
        "grad_norm": 1.9729160070419312,
        "learning_rate": 0.00019275909901973474,
        "epoch": 0.12356949980712358,
        "step": 961
    },
    {
        "loss": 1.4804,
        "grad_norm": 1.3937087059020996,
        "learning_rate": 0.00019274397044099085,
        "epoch": 0.1236980840941237,
        "step": 962
    },
    {
        "loss": 2.2452,
        "grad_norm": 0.7868169546127319,
        "learning_rate": 0.0001927288266694355,
        "epoch": 0.12382666838112383,
        "step": 963
    },
    {
        "loss": 2.401,
        "grad_norm": 1.6442233324050903,
        "learning_rate": 0.00019271366770754953,
        "epoch": 0.12395525266812396,
        "step": 964
    },
    {
        "loss": 1.8026,
        "grad_norm": 1.6426236629486084,
        "learning_rate": 0.00019269849355781616,
        "epoch": 0.12408383695512408,
        "step": 965
    },
    {
        "loss": 1.94,
        "grad_norm": 1.7165119647979736,
        "learning_rate": 0.00019268330422272118,
        "epoch": 0.12421242124212421,
        "step": 966
    },
    {
        "loss": 1.3703,
        "grad_norm": 2.1023130416870117,
        "learning_rate": 0.00019266809970475276,
        "epoch": 0.12434100552912435,
        "step": 967
    },
    {
        "loss": 2.2827,
        "grad_norm": 2.0391452312469482,
        "learning_rate": 0.00019265288000640168,
        "epoch": 0.12446958981612447,
        "step": 968
    },
    {
        "loss": 1.7842,
        "grad_norm": 1.4636574983596802,
        "learning_rate": 0.0001926376451301611,
        "epoch": 0.1245981741031246,
        "step": 969
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.1924879550933838,
        "learning_rate": 0.00019262239507852676,
        "epoch": 0.12472675839012473,
        "step": 970
    },
    {
        "loss": 2.7808,
        "grad_norm": 1.1389895677566528,
        "learning_rate": 0.0001926071298539968,
        "epoch": 0.12485534267712485,
        "step": 971
    },
    {
        "loss": 2.4859,
        "grad_norm": 1.2326308488845825,
        "learning_rate": 0.0001925918494590719,
        "epoch": 0.12498392696412498,
        "step": 972
    },
    {
        "loss": 1.9465,
        "grad_norm": 2.064460515975952,
        "learning_rate": 0.00019257655389625522,
        "epoch": 0.1251125112511251,
        "step": 973
    },
    {
        "loss": 2.5709,
        "grad_norm": 1.0020486116409302,
        "learning_rate": 0.0001925612431680524,
        "epoch": 0.12524109553812524,
        "step": 974
    },
    {
        "loss": 1.6657,
        "grad_norm": 1.818186640739441,
        "learning_rate": 0.00019254591727697152,
        "epoch": 0.12536967982512537,
        "step": 975
    },
    {
        "loss": 1.7915,
        "grad_norm": 1.6560888290405273,
        "learning_rate": 0.00019253057622552327,
        "epoch": 0.1254982641121255,
        "step": 976
    },
    {
        "loss": 1.5793,
        "grad_norm": 2.1081395149230957,
        "learning_rate": 0.0001925152200162206,
        "epoch": 0.12562684839912563,
        "step": 977
    },
    {
        "loss": 1.6426,
        "grad_norm": 1.5474456548690796,
        "learning_rate": 0.00019249984865157921,
        "epoch": 0.12575543268612577,
        "step": 978
    },
    {
        "loss": 1.4719,
        "grad_norm": 2.263428211212158,
        "learning_rate": 0.00019248446213411712,
        "epoch": 0.12588401697312587,
        "step": 979
    },
    {
        "loss": 2.3336,
        "grad_norm": 1.7194323539733887,
        "learning_rate": 0.0001924690604663548,
        "epoch": 0.126012601260126,
        "step": 980
    },
    {
        "loss": 2.2022,
        "grad_norm": 1.4908117055892944,
        "learning_rate": 0.00019245364365081538,
        "epoch": 0.12614118554712614,
        "step": 981
    },
    {
        "loss": 2.3241,
        "grad_norm": 1.7888476848602295,
        "learning_rate": 0.00019243821169002425,
        "epoch": 0.12626976983412627,
        "step": 982
    },
    {
        "loss": 1.9706,
        "grad_norm": 2.059514284133911,
        "learning_rate": 0.00019242276458650946,
        "epoch": 0.1263983541211264,
        "step": 983
    },
    {
        "loss": 2.5554,
        "grad_norm": 1.4888120889663696,
        "learning_rate": 0.00019240730234280142,
        "epoch": 0.12652693840812654,
        "step": 984
    },
    {
        "loss": 2.0805,
        "grad_norm": 1.2356618642807007,
        "learning_rate": 0.0001923918249614331,
        "epoch": 0.12665552269512664,
        "step": 985
    },
    {
        "loss": 2.3034,
        "grad_norm": 1.3189371824264526,
        "learning_rate": 0.00019237633244493995,
        "epoch": 0.12678410698212678,
        "step": 986
    },
    {
        "loss": 1.7322,
        "grad_norm": 1.551531195640564,
        "learning_rate": 0.0001923608247958598,
        "epoch": 0.1269126912691269,
        "step": 987
    },
    {
        "loss": 2.4974,
        "grad_norm": 1.3066322803497314,
        "learning_rate": 0.0001923453020167331,
        "epoch": 0.12704127555612704,
        "step": 988
    },
    {
        "loss": 2.5267,
        "grad_norm": 1.557929515838623,
        "learning_rate": 0.00019232976411010265,
        "epoch": 0.12716985984312718,
        "step": 989
    },
    {
        "loss": 1.7701,
        "grad_norm": 1.483643651008606,
        "learning_rate": 0.00019231421107851376,
        "epoch": 0.1272984441301273,
        "step": 990
    },
    {
        "loss": 1.9146,
        "grad_norm": 1.7399301528930664,
        "learning_rate": 0.00019229864292451433,
        "epoch": 0.1274270284171274,
        "step": 991
    },
    {
        "loss": 2.2851,
        "grad_norm": 1.1973447799682617,
        "learning_rate": 0.00019228305965065463,
        "epoch": 0.12755561270412755,
        "step": 992
    },
    {
        "loss": 1.8781,
        "grad_norm": 1.4764947891235352,
        "learning_rate": 0.00019226746125948737,
        "epoch": 0.12768419699112768,
        "step": 993
    },
    {
        "loss": 2.1066,
        "grad_norm": 2.0640313625335693,
        "learning_rate": 0.00019225184775356787,
        "epoch": 0.1278127812781278,
        "step": 994
    },
    {
        "loss": 1.7691,
        "grad_norm": 1.8648875951766968,
        "learning_rate": 0.00019223621913545377,
        "epoch": 0.12794136556512795,
        "step": 995
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.7800474166870117,
        "learning_rate": 0.00019222057540770532,
        "epoch": 0.12806994985212808,
        "step": 996
    },
    {
        "loss": 2.2186,
        "grad_norm": 2.093733549118042,
        "learning_rate": 0.0001922049165728852,
        "epoch": 0.1281985341391282,
        "step": 997
    },
    {
        "loss": 2.094,
        "grad_norm": 1.8256652355194092,
        "learning_rate": 0.00019218924263355848,
        "epoch": 0.12832711842612832,
        "step": 998
    },
    {
        "loss": 1.8461,
        "grad_norm": 1.5062295198440552,
        "learning_rate": 0.00019217355359229287,
        "epoch": 0.12845570271312845,
        "step": 999
    },
    {
        "loss": 1.6806,
        "grad_norm": 1.4207472801208496,
        "learning_rate": 0.0001921578494516584,
        "epoch": 0.12858428700012858,
        "step": 1000
    },
    {
        "eval_loss": 2.0724003314971924,
        "eval_runtime": 28.2742,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.12858428700012858,
        "step": 1000
    },
    {
        "loss": 2.065,
        "grad_norm": 1.4839016199111938,
        "learning_rate": 0.00019214213021422766,
        "epoch": 0.12871287128712872,
        "step": 1001
    },
    {
        "loss": 2.0261,
        "grad_norm": 1.620182752609253,
        "learning_rate": 0.00019212639588257567,
        "epoch": 0.12884145557412885,
        "step": 1002
    },
    {
        "loss": 2.4259,
        "grad_norm": 1.155250072479248,
        "learning_rate": 0.00019211064645928,
        "epoch": 0.12897003986112898,
        "step": 1003
    },
    {
        "loss": 2.5937,
        "grad_norm": 1.6640560626983643,
        "learning_rate": 0.00019209488194692055,
        "epoch": 0.1290986241481291,
        "step": 1004
    },
    {
        "loss": 2.3337,
        "grad_norm": 1.5989381074905396,
        "learning_rate": 0.00019207910234807986,
        "epoch": 0.12922720843512922,
        "step": 1005
    },
    {
        "loss": 1.4551,
        "grad_norm": 1.616711139678955,
        "learning_rate": 0.0001920633076653428,
        "epoch": 0.12935579272212935,
        "step": 1006
    },
    {
        "loss": 2.2294,
        "grad_norm": 1.1961171627044678,
        "learning_rate": 0.00019204749790129678,
        "epoch": 0.1294843770091295,
        "step": 1007
    },
    {
        "loss": 1.8085,
        "grad_norm": 1.7827835083007812,
        "learning_rate": 0.00019203167305853168,
        "epoch": 0.12961296129612962,
        "step": 1008
    },
    {
        "loss": 1.6805,
        "grad_norm": 1.520566701889038,
        "learning_rate": 0.00019201583313963988,
        "epoch": 0.12974154558312975,
        "step": 1009
    },
    {
        "loss": 1.8497,
        "grad_norm": 2.0537421703338623,
        "learning_rate": 0.0001919999781472161,
        "epoch": 0.12987012987012986,
        "step": 1010
    },
    {
        "loss": 2.2574,
        "grad_norm": 1.5207566022872925,
        "learning_rate": 0.00019198410808385767,
        "epoch": 0.12999871415713,
        "step": 1011
    },
    {
        "loss": 2.1029,
        "grad_norm": 1.8610197305679321,
        "learning_rate": 0.00019196822295216434,
        "epoch": 0.13012729844413012,
        "step": 1012
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.169842481613159,
        "learning_rate": 0.0001919523227547383,
        "epoch": 0.13025588273113026,
        "step": 1013
    },
    {
        "loss": 2.2879,
        "grad_norm": 1.7294611930847168,
        "learning_rate": 0.00019193640749418428,
        "epoch": 0.1303844670181304,
        "step": 1014
    },
    {
        "loss": 1.8173,
        "grad_norm": 2.289665699005127,
        "learning_rate": 0.0001919204771731094,
        "epoch": 0.13051305130513052,
        "step": 1015
    },
    {
        "loss": 2.3343,
        "grad_norm": 1.3655016422271729,
        "learning_rate": 0.00019190453179412326,
        "epoch": 0.13064163559213063,
        "step": 1016
    },
    {
        "loss": 2.118,
        "grad_norm": 2.2536168098449707,
        "learning_rate": 0.00019188857135983794,
        "epoch": 0.13077021987913076,
        "step": 1017
    },
    {
        "loss": 1.8255,
        "grad_norm": 2.2374515533447266,
        "learning_rate": 0.00019187259587286808,
        "epoch": 0.1308988041661309,
        "step": 1018
    },
    {
        "loss": 1.6242,
        "grad_norm": 1.4685107469558716,
        "learning_rate": 0.0001918566053358306,
        "epoch": 0.13102738845313103,
        "step": 1019
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.5421247482299805,
        "learning_rate": 0.000191840599751345,
        "epoch": 0.13115597274013116,
        "step": 1020
    },
    {
        "loss": 1.781,
        "grad_norm": 1.6460211277008057,
        "learning_rate": 0.00019182457912203326,
        "epoch": 0.1312845570271313,
        "step": 1021
    },
    {
        "loss": 2.0239,
        "grad_norm": 1.6464333534240723,
        "learning_rate": 0.0001918085434505198,
        "epoch": 0.13141314131413143,
        "step": 1022
    },
    {
        "loss": 1.693,
        "grad_norm": 1.6252639293670654,
        "learning_rate": 0.00019179249273943146,
        "epoch": 0.13154172560113153,
        "step": 1023
    },
    {
        "loss": 2.0089,
        "grad_norm": 1.7430429458618164,
        "learning_rate": 0.00019177642699139756,
        "epoch": 0.13167030988813166,
        "step": 1024
    },
    {
        "loss": 1.5458,
        "grad_norm": 1.7229772806167603,
        "learning_rate": 0.00019176034620904997,
        "epoch": 0.1317988941751318,
        "step": 1025
    },
    {
        "loss": 1.7949,
        "grad_norm": 1.781191110610962,
        "learning_rate": 0.0001917442503950229,
        "epoch": 0.13192747846213193,
        "step": 1026
    },
    {
        "loss": 2.1103,
        "grad_norm": 2.212958812713623,
        "learning_rate": 0.00019172813955195311,
        "epoch": 0.13205606274913206,
        "step": 1027
    },
    {
        "loss": 2.2164,
        "grad_norm": 1.5465792417526245,
        "learning_rate": 0.0001917120136824798,
        "epoch": 0.1321846470361322,
        "step": 1028
    },
    {
        "loss": 2.4041,
        "grad_norm": 1.3180087804794312,
        "learning_rate": 0.00019169587278924458,
        "epoch": 0.1323132313231323,
        "step": 1029
    },
    {
        "loss": 1.8202,
        "grad_norm": 1.325292944908142,
        "learning_rate": 0.00019167971687489158,
        "epoch": 0.13244181561013244,
        "step": 1030
    },
    {
        "loss": 1.92,
        "grad_norm": 1.3549035787582397,
        "learning_rate": 0.0001916635459420674,
        "epoch": 0.13257039989713257,
        "step": 1031
    },
    {
        "loss": 2.4052,
        "grad_norm": 1.8002440929412842,
        "learning_rate": 0.00019164735999342104,
        "epoch": 0.1326989841841327,
        "step": 1032
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.0783259868621826,
        "learning_rate": 0.000191631159031604,
        "epoch": 0.13282756847113283,
        "step": 1033
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.797167420387268,
        "learning_rate": 0.00019161494305927024,
        "epoch": 0.13295615275813297,
        "step": 1034
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.6306244134902954,
        "learning_rate": 0.00019159871207907617,
        "epoch": 0.13308473704513307,
        "step": 1035
    },
    {
        "loss": 2.394,
        "grad_norm": 1.0292103290557861,
        "learning_rate": 0.00019158246609368067,
        "epoch": 0.1332133213321332,
        "step": 1036
    },
    {
        "loss": 2.2254,
        "grad_norm": 1.1019446849822998,
        "learning_rate": 0.00019156620510574505,
        "epoch": 0.13334190561913334,
        "step": 1037
    },
    {
        "loss": 1.6428,
        "grad_norm": 2.225356340408325,
        "learning_rate": 0.00019154992911793312,
        "epoch": 0.13347048990613347,
        "step": 1038
    },
    {
        "loss": 2.3894,
        "grad_norm": 1.707896113395691,
        "learning_rate": 0.00019153363813291112,
        "epoch": 0.1335990741931336,
        "step": 1039
    },
    {
        "loss": 2.4853,
        "grad_norm": 1.3393123149871826,
        "learning_rate": 0.00019151733215334773,
        "epoch": 0.13372765848013374,
        "step": 1040
    },
    {
        "loss": 2.5405,
        "grad_norm": 1.583770513534546,
        "learning_rate": 0.00019150101118191413,
        "epoch": 0.13385624276713384,
        "step": 1041
    },
    {
        "loss": 2.0803,
        "grad_norm": 1.5762434005737305,
        "learning_rate": 0.0001914846752212839,
        "epoch": 0.13398482705413398,
        "step": 1042
    },
    {
        "loss": 1.2588,
        "grad_norm": 2.5809571743011475,
        "learning_rate": 0.00019146832427413316,
        "epoch": 0.1341134113411341,
        "step": 1043
    },
    {
        "loss": 1.1009,
        "grad_norm": 1.9366652965545654,
        "learning_rate": 0.0001914519583431404,
        "epoch": 0.13424199562813424,
        "step": 1044
    },
    {
        "loss": 1.6496,
        "grad_norm": 3.301983594894409,
        "learning_rate": 0.00019143557743098657,
        "epoch": 0.13437057991513437,
        "step": 1045
    },
    {
        "loss": 1.5437,
        "grad_norm": 1.7446908950805664,
        "learning_rate": 0.00019141918154035514,
        "epoch": 0.1344991642021345,
        "step": 1046
    },
    {
        "loss": 1.9573,
        "grad_norm": 1.7099689245224,
        "learning_rate": 0.000191402770673932,
        "epoch": 0.13462774848913464,
        "step": 1047
    },
    {
        "loss": 2.5181,
        "grad_norm": 1.898792028427124,
        "learning_rate": 0.00019138634483440546,
        "epoch": 0.13475633277613475,
        "step": 1048
    },
    {
        "loss": 1.6905,
        "grad_norm": 1.8069671392440796,
        "learning_rate": 0.00019136990402446633,
        "epoch": 0.13488491706313488,
        "step": 1049
    },
    {
        "loss": 2.0169,
        "grad_norm": 1.1523641347885132,
        "learning_rate": 0.00019135344824680784,
        "epoch": 0.135013501350135,
        "step": 1050
    },
    {
        "loss": 1.8584,
        "grad_norm": 1.7570927143096924,
        "learning_rate": 0.00019133697750412572,
        "epoch": 0.13514208563713515,
        "step": 1051
    },
    {
        "loss": 1.7678,
        "grad_norm": 1.6090002059936523,
        "learning_rate": 0.0001913204917991181,
        "epoch": 0.13527066992413528,
        "step": 1052
    },
    {
        "loss": 2.3867,
        "grad_norm": 1.2864763736724854,
        "learning_rate": 0.00019130399113448555,
        "epoch": 0.1353992542111354,
        "step": 1053
    },
    {
        "loss": 1.4062,
        "grad_norm": 1.6414564847946167,
        "learning_rate": 0.00019128747551293117,
        "epoch": 0.13552783849813552,
        "step": 1054
    },
    {
        "loss": 1.2355,
        "grad_norm": 2.6200032234191895,
        "learning_rate": 0.00019127094493716037,
        "epoch": 0.13565642278513565,
        "step": 1055
    },
    {
        "loss": 1.4334,
        "grad_norm": 2.1307873725891113,
        "learning_rate": 0.00019125439940988122,
        "epoch": 0.13578500707213578,
        "step": 1056
    },
    {
        "loss": 1.8896,
        "grad_norm": 1.516886591911316,
        "learning_rate": 0.00019123783893380402,
        "epoch": 0.13591359135913592,
        "step": 1057
    },
    {
        "loss": 1.5881,
        "grad_norm": 2.006989002227783,
        "learning_rate": 0.00019122126351164167,
        "epoch": 0.13604217564613605,
        "step": 1058
    },
    {
        "loss": 1.199,
        "grad_norm": 2.0606749057769775,
        "learning_rate": 0.00019120467314610943,
        "epoch": 0.13617075993313618,
        "step": 1059
    },
    {
        "loss": 2.1866,
        "grad_norm": 1.6812294721603394,
        "learning_rate": 0.0001911880678399251,
        "epoch": 0.1362993442201363,
        "step": 1060
    },
    {
        "loss": 1.8079,
        "grad_norm": 2.1526107788085938,
        "learning_rate": 0.0001911714475958088,
        "epoch": 0.13642792850713642,
        "step": 1061
    },
    {
        "loss": 1.8608,
        "grad_norm": 2.6002042293548584,
        "learning_rate": 0.0001911548124164832,
        "epoch": 0.13655651279413655,
        "step": 1062
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.3288993835449219,
        "learning_rate": 0.00019113816230467338,
        "epoch": 0.13668509708113669,
        "step": 1063
    },
    {
        "loss": 1.8903,
        "grad_norm": 1.665137529373169,
        "learning_rate": 0.0001911214972631069,
        "epoch": 0.13681368136813682,
        "step": 1064
    },
    {
        "loss": 1.5859,
        "grad_norm": 1.941571831703186,
        "learning_rate": 0.00019110481729451372,
        "epoch": 0.13694226565513695,
        "step": 1065
    },
    {
        "loss": 2.0868,
        "grad_norm": 1.7242999076843262,
        "learning_rate": 0.00019108812240162623,
        "epoch": 0.13707084994213706,
        "step": 1066
    },
    {
        "loss": 2.4744,
        "grad_norm": 1.5132273435592651,
        "learning_rate": 0.0001910714125871793,
        "epoch": 0.1371994342291372,
        "step": 1067
    },
    {
        "loss": 1.5258,
        "grad_norm": 1.722334861755371,
        "learning_rate": 0.0001910546878539103,
        "epoch": 0.13732801851613732,
        "step": 1068
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.3830476999282837,
        "learning_rate": 0.00019103794820455892,
        "epoch": 0.13745660280313746,
        "step": 1069
    },
    {
        "loss": 2.0888,
        "grad_norm": 2.00534987449646,
        "learning_rate": 0.0001910211936418674,
        "epoch": 0.1375851870901376,
        "step": 1070
    },
    {
        "loss": 1.6477,
        "grad_norm": 1.5546749830245972,
        "learning_rate": 0.00019100442416858035,
        "epoch": 0.13771377137713772,
        "step": 1071
    },
    {
        "loss": 2.373,
        "grad_norm": 1.5241189002990723,
        "learning_rate": 0.00019098763978744488,
        "epoch": 0.13784235566413786,
        "step": 1072
    },
    {
        "loss": 2.2086,
        "grad_norm": 1.2405916452407837,
        "learning_rate": 0.00019097084050121048,
        "epoch": 0.13797093995113796,
        "step": 1073
    },
    {
        "loss": 2.4068,
        "grad_norm": 1.641310453414917,
        "learning_rate": 0.00019095402631262915,
        "epoch": 0.1380995242381381,
        "step": 1074
    },
    {
        "loss": 2.1492,
        "grad_norm": 1.3424164056777954,
        "learning_rate": 0.0001909371972244553,
        "epoch": 0.13822810852513823,
        "step": 1075
    },
    {
        "loss": 1.3569,
        "grad_norm": 2.6306116580963135,
        "learning_rate": 0.00019092035323944575,
        "epoch": 0.13835669281213836,
        "step": 1076
    },
    {
        "loss": 1.961,
        "grad_norm": 1.8273812532424927,
        "learning_rate": 0.0001909034943603598,
        "epoch": 0.1384852770991385,
        "step": 1077
    },
    {
        "loss": 1.3881,
        "grad_norm": 2.0188751220703125,
        "learning_rate": 0.00019088662058995922,
        "epoch": 0.13861386138613863,
        "step": 1078
    },
    {
        "loss": 1.2371,
        "grad_norm": 2.044774293899536,
        "learning_rate": 0.00019086973193100816,
        "epoch": 0.13874244567313873,
        "step": 1079
    },
    {
        "loss": 2.3585,
        "grad_norm": 1.9953346252441406,
        "learning_rate": 0.00019085282838627316,
        "epoch": 0.13887102996013886,
        "step": 1080
    },
    {
        "loss": 2.3814,
        "grad_norm": 1.0028891563415527,
        "learning_rate": 0.00019083590995852337,
        "epoch": 0.138999614247139,
        "step": 1081
    },
    {
        "loss": 2.2117,
        "grad_norm": 1.8209301233291626,
        "learning_rate": 0.0001908189766505302,
        "epoch": 0.13912819853413913,
        "step": 1082
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.2561273574829102,
        "learning_rate": 0.0001908020284650676,
        "epoch": 0.13925678282113926,
        "step": 1083
    },
    {
        "loss": 1.9797,
        "grad_norm": 1.874348521232605,
        "learning_rate": 0.0001907850654049119,
        "epoch": 0.1393853671081394,
        "step": 1084
    },
    {
        "loss": 2.2716,
        "grad_norm": 1.81117844581604,
        "learning_rate": 0.000190768087472842,
        "epoch": 0.1395139513951395,
        "step": 1085
    },
    {
        "loss": 2.5858,
        "grad_norm": 1.355486512184143,
        "learning_rate": 0.000190751094671639,
        "epoch": 0.13964253568213963,
        "step": 1086
    },
    {
        "loss": 1.7337,
        "grad_norm": 1.7561038732528687,
        "learning_rate": 0.00019073408700408665,
        "epoch": 0.13977111996913977,
        "step": 1087
    },
    {
        "loss": 2.3351,
        "grad_norm": 0.9668310284614563,
        "learning_rate": 0.00019071706447297099,
        "epoch": 0.1398997042561399,
        "step": 1088
    },
    {
        "loss": 1.9918,
        "grad_norm": 1.6477705240249634,
        "learning_rate": 0.00019070002708108062,
        "epoch": 0.14002828854314003,
        "step": 1089
    },
    {
        "loss": 2.1234,
        "grad_norm": 1.437512755393982,
        "learning_rate": 0.0001906829748312065,
        "epoch": 0.14015687283014017,
        "step": 1090
    },
    {
        "loss": 1.7569,
        "grad_norm": 1.4649981260299683,
        "learning_rate": 0.00019066590772614202,
        "epoch": 0.14028545711714027,
        "step": 1091
    },
    {
        "loss": 2.0192,
        "grad_norm": 1.8829078674316406,
        "learning_rate": 0.00019064882576868303,
        "epoch": 0.1404140414041404,
        "step": 1092
    },
    {
        "loss": 1.8144,
        "grad_norm": 1.7980585098266602,
        "learning_rate": 0.00019063172896162777,
        "epoch": 0.14054262569114054,
        "step": 1093
    },
    {
        "loss": 1.5046,
        "grad_norm": 2.359003782272339,
        "learning_rate": 0.00019061461730777697,
        "epoch": 0.14067120997814067,
        "step": 1094
    },
    {
        "loss": 2.4395,
        "grad_norm": 1.4291374683380127,
        "learning_rate": 0.0001905974908099338,
        "epoch": 0.1407997942651408,
        "step": 1095
    },
    {
        "loss": 1.771,
        "grad_norm": 1.648115634918213,
        "learning_rate": 0.0001905803494709038,
        "epoch": 0.14092837855214094,
        "step": 1096
    },
    {
        "loss": 1.7822,
        "grad_norm": 2.2282440662384033,
        "learning_rate": 0.00019056319329349494,
        "epoch": 0.14105696283914107,
        "step": 1097
    },
    {
        "loss": 2.2676,
        "grad_norm": 2.0880515575408936,
        "learning_rate": 0.0001905460222805177,
        "epoch": 0.14118554712614118,
        "step": 1098
    },
    {
        "loss": 2.0231,
        "grad_norm": 1.5978033542633057,
        "learning_rate": 0.00019052883643478494,
        "epoch": 0.1413141314131413,
        "step": 1099
    },
    {
        "loss": 2.0727,
        "grad_norm": 1.4754054546356201,
        "learning_rate": 0.0001905116357591119,
        "epoch": 0.14144271570014144,
        "step": 1100
    },
    {
        "eval_loss": 2.0593698024749756,
        "eval_runtime": 28.2701,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.14144271570014144,
        "step": 1100
    },
    {
        "loss": 1.7008,
        "grad_norm": 1.8183441162109375,
        "learning_rate": 0.00019049442025631636,
        "epoch": 0.14157129998714157,
        "step": 1101
    },
    {
        "loss": 2.1133,
        "grad_norm": 2.0058701038360596,
        "learning_rate": 0.00019047718992921847,
        "epoch": 0.1416998842741417,
        "step": 1102
    },
    {
        "loss": 1.1803,
        "grad_norm": 1.7435821294784546,
        "learning_rate": 0.00019045994478064077,
        "epoch": 0.14182846856114184,
        "step": 1103
    },
    {
        "loss": 2.5736,
        "grad_norm": 1.7543339729309082,
        "learning_rate": 0.00019044268481340827,
        "epoch": 0.14195705284814195,
        "step": 1104
    },
    {
        "loss": 1.8348,
        "grad_norm": 1.7850605249404907,
        "learning_rate": 0.00019042541003034839,
        "epoch": 0.14208563713514208,
        "step": 1105
    },
    {
        "loss": 2.2815,
        "grad_norm": 1.3299708366394043,
        "learning_rate": 0.00019040812043429105,
        "epoch": 0.1422142214221422,
        "step": 1106
    },
    {
        "loss": 2.176,
        "grad_norm": 1.7375086545944214,
        "learning_rate": 0.00019039081602806847,
        "epoch": 0.14234280570914234,
        "step": 1107
    },
    {
        "loss": 1.6012,
        "grad_norm": 1.912243366241455,
        "learning_rate": 0.00019037349681451545,
        "epoch": 0.14247138999614248,
        "step": 1108
    },
    {
        "loss": 2.3708,
        "grad_norm": 1.7406227588653564,
        "learning_rate": 0.00019035616279646905,
        "epoch": 0.1425999742831426,
        "step": 1109
    },
    {
        "loss": 2.1692,
        "grad_norm": 1.735101342201233,
        "learning_rate": 0.00019033881397676885,
        "epoch": 0.14272855857014272,
        "step": 1110
    },
    {
        "loss": 2.4584,
        "grad_norm": 1.3721061944961548,
        "learning_rate": 0.00019032145035825684,
        "epoch": 0.14285714285714285,
        "step": 1111
    },
    {
        "loss": 1.6183,
        "grad_norm": 1.91997230052948,
        "learning_rate": 0.00019030407194377746,
        "epoch": 0.14298572714414298,
        "step": 1112
    },
    {
        "loss": 1.398,
        "grad_norm": 2.2206616401672363,
        "learning_rate": 0.00019028667873617755,
        "epoch": 0.14311431143114312,
        "step": 1113
    },
    {
        "loss": 2.4113,
        "grad_norm": 1.3765772581100464,
        "learning_rate": 0.0001902692707383063,
        "epoch": 0.14324289571814325,
        "step": 1114
    },
    {
        "loss": 2.064,
        "grad_norm": 1.6039438247680664,
        "learning_rate": 0.00019025184795301548,
        "epoch": 0.14337148000514338,
        "step": 1115
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.7652314901351929,
        "learning_rate": 0.00019023441038315916,
        "epoch": 0.1435000642921435,
        "step": 1116
    },
    {
        "loss": 2.352,
        "grad_norm": 1.2529963254928589,
        "learning_rate": 0.00019021695803159383,
        "epoch": 0.14362864857914362,
        "step": 1117
    },
    {
        "loss": 1.9832,
        "grad_norm": 2.038745880126953,
        "learning_rate": 0.0001901994909011785,
        "epoch": 0.14375723286614375,
        "step": 1118
    },
    {
        "loss": 1.9689,
        "grad_norm": 2.2678821086883545,
        "learning_rate": 0.00019018200899477452,
        "epoch": 0.14388581715314389,
        "step": 1119
    },
    {
        "loss": 1.5588,
        "grad_norm": 2.0624020099639893,
        "learning_rate": 0.00019016451231524566,
        "epoch": 0.14401440144014402,
        "step": 1120
    },
    {
        "loss": 1.3169,
        "grad_norm": 2.478787660598755,
        "learning_rate": 0.00019014700086545817,
        "epoch": 0.14414298572714415,
        "step": 1121
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.538212537765503,
        "learning_rate": 0.00019012947464828065,
        "epoch": 0.14427157001414428,
        "step": 1122
    },
    {
        "loss": 2.3297,
        "grad_norm": 2.555905818939209,
        "learning_rate": 0.00019011193366658412,
        "epoch": 0.1444001543011444,
        "step": 1123
    },
    {
        "loss": 2.1731,
        "grad_norm": 1.2156902551651,
        "learning_rate": 0.00019009437792324213,
        "epoch": 0.14452873858814452,
        "step": 1124
    },
    {
        "loss": 2.1351,
        "grad_norm": 1.1445337533950806,
        "learning_rate": 0.00019007680742113049,
        "epoch": 0.14465732287514466,
        "step": 1125
    },
    {
        "loss": 2.1531,
        "grad_norm": 1.4579393863677979,
        "learning_rate": 0.00019005922216312754,
        "epoch": 0.1447859071621448,
        "step": 1126
    },
    {
        "loss": 2.8727,
        "grad_norm": 1.4251596927642822,
        "learning_rate": 0.00019004162215211399,
        "epoch": 0.14491449144914492,
        "step": 1127
    },
    {
        "loss": 2.0402,
        "grad_norm": 1.2378308773040771,
        "learning_rate": 0.00019002400739097296,
        "epoch": 0.14504307573614506,
        "step": 1128
    },
    {
        "loss": 2.2693,
        "grad_norm": 1.2898164987564087,
        "learning_rate": 0.00019000637788259007,
        "epoch": 0.14517166002314516,
        "step": 1129
    },
    {
        "loss": 2.0036,
        "grad_norm": 1.5619884729385376,
        "learning_rate": 0.00018998873362985322,
        "epoch": 0.1453002443101453,
        "step": 1130
    },
    {
        "loss": 2.2601,
        "grad_norm": 1.4765440225601196,
        "learning_rate": 0.00018997107463565284,
        "epoch": 0.14542882859714543,
        "step": 1131
    },
    {
        "loss": 2.5706,
        "grad_norm": 1.0260652303695679,
        "learning_rate": 0.00018995340090288175,
        "epoch": 0.14555741288414556,
        "step": 1132
    },
    {
        "loss": 1.4756,
        "grad_norm": 1.7346410751342773,
        "learning_rate": 0.00018993571243443506,
        "epoch": 0.1456859971711457,
        "step": 1133
    },
    {
        "loss": 1.4481,
        "grad_norm": 1.4089652299880981,
        "learning_rate": 0.00018991800923321052,
        "epoch": 0.14581458145814583,
        "step": 1134
    },
    {
        "loss": 2.6076,
        "grad_norm": 1.3848387002944946,
        "learning_rate": 0.00018990029130210804,
        "epoch": 0.14594316574514593,
        "step": 1135
    },
    {
        "loss": 1.7535,
        "grad_norm": 1.8232035636901855,
        "learning_rate": 0.00018988255864403023,
        "epoch": 0.14607175003214606,
        "step": 1136
    },
    {
        "loss": 1.7831,
        "grad_norm": 1.8829293251037598,
        "learning_rate": 0.00018986481126188182,
        "epoch": 0.1462003343191462,
        "step": 1137
    },
    {
        "loss": 1.8507,
        "grad_norm": 1.8139163255691528,
        "learning_rate": 0.00018984704915857016,
        "epoch": 0.14632891860614633,
        "step": 1138
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.4977210760116577,
        "learning_rate": 0.00018982927233700497,
        "epoch": 0.14645750289314646,
        "step": 1139
    },
    {
        "loss": 1.8698,
        "grad_norm": 1.5266379117965698,
        "learning_rate": 0.00018981148080009826,
        "epoch": 0.1465860871801466,
        "step": 1140
    },
    {
        "loss": 1.9933,
        "grad_norm": 1.8336031436920166,
        "learning_rate": 0.00018979367455076464,
        "epoch": 0.1467146714671467,
        "step": 1141
    },
    {
        "loss": 1.8826,
        "grad_norm": 2.1237738132476807,
        "learning_rate": 0.00018977585359192096,
        "epoch": 0.14684325575414683,
        "step": 1142
    },
    {
        "loss": 2.163,
        "grad_norm": 1.3281153440475464,
        "learning_rate": 0.00018975801792648657,
        "epoch": 0.14697184004114697,
        "step": 1143
    },
    {
        "loss": 1.0689,
        "grad_norm": 2.5588791370391846,
        "learning_rate": 0.00018974016755738323,
        "epoch": 0.1471004243281471,
        "step": 1144
    },
    {
        "loss": 2.7354,
        "grad_norm": 1.2226972579956055,
        "learning_rate": 0.0001897223024875351,
        "epoch": 0.14722900861514723,
        "step": 1145
    },
    {
        "loss": 1.758,
        "grad_norm": 1.5054042339324951,
        "learning_rate": 0.00018970442271986867,
        "epoch": 0.14735759290214737,
        "step": 1146
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.3310446739196777,
        "learning_rate": 0.000189686528257313,
        "epoch": 0.1474861771891475,
        "step": 1147
    },
    {
        "loss": 1.5495,
        "grad_norm": 1.5808402299880981,
        "learning_rate": 0.0001896686191027994,
        "epoch": 0.1476147614761476,
        "step": 1148
    },
    {
        "loss": 2.0914,
        "grad_norm": 1.3873010873794556,
        "learning_rate": 0.00018965069525926168,
        "epoch": 0.14774334576314774,
        "step": 1149
    },
    {
        "loss": 2.4625,
        "grad_norm": 0.9707677960395813,
        "learning_rate": 0.000189632756729636,
        "epoch": 0.14787193005014787,
        "step": 1150
    },
    {
        "loss": 1.9517,
        "grad_norm": 1.6141020059585571,
        "learning_rate": 0.00018961480351686096,
        "epoch": 0.148000514337148,
        "step": 1151
    },
    {
        "loss": 2.2007,
        "grad_norm": 2.1916306018829346,
        "learning_rate": 0.00018959683562387757,
        "epoch": 0.14812909862414814,
        "step": 1152
    },
    {
        "loss": 1.5138,
        "grad_norm": 1.8151493072509766,
        "learning_rate": 0.00018957885305362923,
        "epoch": 0.14825768291114827,
        "step": 1153
    },
    {
        "loss": 2.1653,
        "grad_norm": 1.273253083229065,
        "learning_rate": 0.00018956085580906177,
        "epoch": 0.14838626719814837,
        "step": 1154
    },
    {
        "loss": 1.8067,
        "grad_norm": 1.5845305919647217,
        "learning_rate": 0.0001895428438931233,
        "epoch": 0.1485148514851485,
        "step": 1155
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.8202792406082153,
        "learning_rate": 0.00018952481730876458,
        "epoch": 0.14864343577214864,
        "step": 1156
    },
    {
        "loss": 2.3205,
        "grad_norm": 1.484214186668396,
        "learning_rate": 0.0001895067760589385,
        "epoch": 0.14877202005914877,
        "step": 1157
    },
    {
        "loss": 1.6017,
        "grad_norm": 2.132319688796997,
        "learning_rate": 0.00018948872014660058,
        "epoch": 0.1489006043461489,
        "step": 1158
    },
    {
        "loss": 2.4787,
        "grad_norm": 1.1619956493377686,
        "learning_rate": 0.00018947064957470857,
        "epoch": 0.14902918863314904,
        "step": 1159
    },
    {
        "loss": 2.4148,
        "grad_norm": 1.1775490045547485,
        "learning_rate": 0.00018945256434622272,
        "epoch": 0.14915777292014915,
        "step": 1160
    },
    {
        "loss": 1.5844,
        "grad_norm": 1.8939839601516724,
        "learning_rate": 0.00018943446446410565,
        "epoch": 0.14928635720714928,
        "step": 1161
    },
    {
        "loss": 2.3011,
        "grad_norm": 2.3848745822906494,
        "learning_rate": 0.00018941634993132237,
        "epoch": 0.1494149414941494,
        "step": 1162
    },
    {
        "loss": 2.7069,
        "grad_norm": 1.3612195253372192,
        "learning_rate": 0.00018939822075084031,
        "epoch": 0.14954352578114954,
        "step": 1163
    },
    {
        "loss": 2.4202,
        "grad_norm": 1.7056573629379272,
        "learning_rate": 0.00018938007692562934,
        "epoch": 0.14967211006814968,
        "step": 1164
    },
    {
        "loss": 2.1444,
        "grad_norm": 1.9966073036193848,
        "learning_rate": 0.00018936191845866162,
        "epoch": 0.1498006943551498,
        "step": 1165
    },
    {
        "loss": 1.8389,
        "grad_norm": 2.20046067237854,
        "learning_rate": 0.0001893437453529118,
        "epoch": 0.14992927864214992,
        "step": 1166
    },
    {
        "loss": 1.9673,
        "grad_norm": 1.8371219635009766,
        "learning_rate": 0.0001893255576113569,
        "epoch": 0.15005786292915005,
        "step": 1167
    },
    {
        "loss": 2.5877,
        "grad_norm": 1.2157727479934692,
        "learning_rate": 0.0001893073552369763,
        "epoch": 0.15018644721615018,
        "step": 1168
    },
    {
        "loss": 2.0762,
        "grad_norm": 1.7717632055282593,
        "learning_rate": 0.00018928913823275186,
        "epoch": 0.15031503150315031,
        "step": 1169
    },
    {
        "loss": 1.7415,
        "grad_norm": 1.9321104288101196,
        "learning_rate": 0.00018927090660166777,
        "epoch": 0.15044361579015045,
        "step": 1170
    },
    {
        "loss": 1.4672,
        "grad_norm": 2.559514045715332,
        "learning_rate": 0.00018925266034671066,
        "epoch": 0.15057220007715058,
        "step": 1171
    },
    {
        "loss": 2.1292,
        "grad_norm": 1.7095381021499634,
        "learning_rate": 0.0001892343994708695,
        "epoch": 0.15070078436415071,
        "step": 1172
    },
    {
        "loss": 2.4678,
        "grad_norm": 1.7235822677612305,
        "learning_rate": 0.00018921612397713569,
        "epoch": 0.15082936865115082,
        "step": 1173
    },
    {
        "loss": 2.3615,
        "grad_norm": 1.297212839126587,
        "learning_rate": 0.00018919783386850303,
        "epoch": 0.15095795293815095,
        "step": 1174
    },
    {
        "loss": 2.1767,
        "grad_norm": 1.9702374935150146,
        "learning_rate": 0.00018917952914796772,
        "epoch": 0.15108653722515109,
        "step": 1175
    },
    {
        "loss": 2.1512,
        "grad_norm": 2.2814552783966064,
        "learning_rate": 0.00018916120981852833,
        "epoch": 0.15121512151215122,
        "step": 1176
    },
    {
        "loss": 2.2121,
        "grad_norm": 1.6611264944076538,
        "learning_rate": 0.00018914287588318583,
        "epoch": 0.15134370579915135,
        "step": 1177
    },
    {
        "loss": 2.0623,
        "grad_norm": 1.6980457305908203,
        "learning_rate": 0.00018912452734494358,
        "epoch": 0.15147229008615148,
        "step": 1178
    },
    {
        "loss": 2.4336,
        "grad_norm": 1.6369154453277588,
        "learning_rate": 0.00018910616420680733,
        "epoch": 0.1516008743731516,
        "step": 1179
    },
    {
        "loss": 1.6422,
        "grad_norm": 1.3554624319076538,
        "learning_rate": 0.00018908778647178527,
        "epoch": 0.15172945866015172,
        "step": 1180
    },
    {
        "loss": 2.168,
        "grad_norm": 1.5722742080688477,
        "learning_rate": 0.00018906939414288788,
        "epoch": 0.15185804294715186,
        "step": 1181
    },
    {
        "loss": 1.6258,
        "grad_norm": 1.6668270826339722,
        "learning_rate": 0.00018905098722312813,
        "epoch": 0.151986627234152,
        "step": 1182
    },
    {
        "loss": 1.8769,
        "grad_norm": 1.8882299661636353,
        "learning_rate": 0.00018903256571552136,
        "epoch": 0.15211521152115212,
        "step": 1183
    },
    {
        "loss": 2.2872,
        "grad_norm": 1.4642080068588257,
        "learning_rate": 0.00018901412962308523,
        "epoch": 0.15224379580815225,
        "step": 1184
    },
    {
        "loss": 1.9414,
        "grad_norm": 1.801761507987976,
        "learning_rate": 0.00018899567894883985,
        "epoch": 0.15237238009515236,
        "step": 1185
    },
    {
        "loss": 2.0331,
        "grad_norm": 1.7375366687774658,
        "learning_rate": 0.00018897721369580777,
        "epoch": 0.1525009643821525,
        "step": 1186
    },
    {
        "loss": 2.0726,
        "grad_norm": 1.9303444623947144,
        "learning_rate": 0.00018895873386701376,
        "epoch": 0.15262954866915263,
        "step": 1187
    },
    {
        "loss": 2.2583,
        "grad_norm": 1.604043960571289,
        "learning_rate": 0.0001889402394654852,
        "epoch": 0.15275813295615276,
        "step": 1188
    },
    {
        "loss": 1.4349,
        "grad_norm": 1.300972580909729,
        "learning_rate": 0.00018892173049425167,
        "epoch": 0.1528867172431529,
        "step": 1189
    },
    {
        "loss": 1.9935,
        "grad_norm": 1.4841718673706055,
        "learning_rate": 0.0001889032069563452,
        "epoch": 0.15301530153015303,
        "step": 1190
    },
    {
        "loss": 2.1833,
        "grad_norm": 1.554971694946289,
        "learning_rate": 0.00018888466885480027,
        "epoch": 0.15314388581715313,
        "step": 1191
    },
    {
        "loss": 2.2588,
        "grad_norm": 1.4983899593353271,
        "learning_rate": 0.00018886611619265366,
        "epoch": 0.15327247010415326,
        "step": 1192
    },
    {
        "loss": 1.9474,
        "grad_norm": 1.6727919578552246,
        "learning_rate": 0.00018884754897294457,
        "epoch": 0.1534010543911534,
        "step": 1193
    },
    {
        "loss": 2.5408,
        "grad_norm": 1.740860939025879,
        "learning_rate": 0.00018882896719871455,
        "epoch": 0.15352963867815353,
        "step": 1194
    },
    {
        "loss": 1.7333,
        "grad_norm": 1.6428602933883667,
        "learning_rate": 0.0001888103708730076,
        "epoch": 0.15365822296515366,
        "step": 1195
    },
    {
        "loss": 2.2496,
        "grad_norm": 1.7722429037094116,
        "learning_rate": 0.0001887917599988701,
        "epoch": 0.1537868072521538,
        "step": 1196
    },
    {
        "loss": 2.4404,
        "grad_norm": 1.9399725198745728,
        "learning_rate": 0.00018877313457935072,
        "epoch": 0.15391539153915393,
        "step": 1197
    },
    {
        "loss": 2.6734,
        "grad_norm": 1.0417120456695557,
        "learning_rate": 0.0001887544946175006,
        "epoch": 0.15404397582615403,
        "step": 1198
    },
    {
        "loss": 1.9201,
        "grad_norm": 1.837015986442566,
        "learning_rate": 0.00018873584011637324,
        "epoch": 0.15417256011315417,
        "step": 1199
    },
    {
        "loss": 2.3493,
        "grad_norm": 1.129695177078247,
        "learning_rate": 0.00018871717107902448,
        "epoch": 0.1543011444001543,
        "step": 1200
    },
    {
        "eval_loss": 2.0497868061065674,
        "eval_runtime": 28.2148,
        "eval_samples_per_second": 2.8,
        "eval_steps_per_second": 2.8,
        "epoch": 0.1543011444001543,
        "step": 1200
    },
    {
        "loss": 2.0701,
        "grad_norm": 1.350271463394165,
        "learning_rate": 0.00018869848750851265,
        "epoch": 0.15442972868715443,
        "step": 1201
    },
    {
        "loss": 2.092,
        "grad_norm": 1.8302066326141357,
        "learning_rate": 0.00018867978940789834,
        "epoch": 0.15455831297415457,
        "step": 1202
    },
    {
        "loss": 1.283,
        "grad_norm": 2.057206630706787,
        "learning_rate": 0.00018866107678024459,
        "epoch": 0.1546868972611547,
        "step": 1203
    },
    {
        "loss": 2.339,
        "grad_norm": 2.0293447971343994,
        "learning_rate": 0.0001886423496286168,
        "epoch": 0.1548154815481548,
        "step": 1204
    },
    {
        "loss": 2.2343,
        "grad_norm": 1.2499446868896484,
        "learning_rate": 0.00018862360795608273,
        "epoch": 0.15494406583515494,
        "step": 1205
    },
    {
        "loss": 1.909,
        "grad_norm": 2.162750482559204,
        "learning_rate": 0.00018860485176571258,
        "epoch": 0.15507265012215507,
        "step": 1206
    },
    {
        "loss": 0.9142,
        "grad_norm": 2.0232458114624023,
        "learning_rate": 0.00018858608106057885,
        "epoch": 0.1552012344091552,
        "step": 1207
    },
    {
        "loss": 1.8602,
        "grad_norm": 1.518540382385254,
        "learning_rate": 0.00018856729584375645,
        "epoch": 0.15532981869615534,
        "step": 1208
    },
    {
        "loss": 1.8856,
        "grad_norm": 2.130763053894043,
        "learning_rate": 0.0001885484961183227,
        "epoch": 0.15545840298315547,
        "step": 1209
    },
    {
        "loss": 1.7538,
        "grad_norm": 2.1225969791412354,
        "learning_rate": 0.00018852968188735726,
        "epoch": 0.15558698727015557,
        "step": 1210
    },
    {
        "loss": 1.3896,
        "grad_norm": 1.3058592081069946,
        "learning_rate": 0.00018851085315394216,
        "epoch": 0.1557155715571557,
        "step": 1211
    },
    {
        "loss": 1.9607,
        "grad_norm": 1.668412685394287,
        "learning_rate": 0.00018849200992116183,
        "epoch": 0.15584415584415584,
        "step": 1212
    },
    {
        "loss": 2.1206,
        "grad_norm": 1.2301009893417358,
        "learning_rate": 0.0001884731521921031,
        "epoch": 0.15597274013115597,
        "step": 1213
    },
    {
        "loss": 1.5936,
        "grad_norm": 1.8086315393447876,
        "learning_rate": 0.00018845427996985506,
        "epoch": 0.1561013244181561,
        "step": 1214
    },
    {
        "loss": 1.897,
        "grad_norm": 1.7148383855819702,
        "learning_rate": 0.00018843539325750933,
        "epoch": 0.15622990870515624,
        "step": 1215
    },
    {
        "loss": 1.7017,
        "grad_norm": 1.8092976808547974,
        "learning_rate": 0.0001884164920581598,
        "epoch": 0.15635849299215634,
        "step": 1216
    },
    {
        "loss": 2.0728,
        "grad_norm": 1.5674383640289307,
        "learning_rate": 0.00018839757637490275,
        "epoch": 0.15648707727915648,
        "step": 1217
    },
    {
        "loss": 1.6734,
        "grad_norm": 2.564980983734131,
        "learning_rate": 0.00018837864621083685,
        "epoch": 0.1566156615661566,
        "step": 1218
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.8255691528320312,
        "learning_rate": 0.00018835970156906316,
        "epoch": 0.15674424585315674,
        "step": 1219
    },
    {
        "loss": 1.8734,
        "grad_norm": 2.5767223834991455,
        "learning_rate": 0.00018834074245268506,
        "epoch": 0.15687283014015688,
        "step": 1220
    },
    {
        "loss": 2.2625,
        "grad_norm": 1.5303738117218018,
        "learning_rate": 0.00018832176886480833,
        "epoch": 0.157001414427157,
        "step": 1221
    },
    {
        "loss": 1.6887,
        "grad_norm": 1.6035066843032837,
        "learning_rate": 0.00018830278080854114,
        "epoch": 0.15712999871415714,
        "step": 1222
    },
    {
        "loss": 1.5836,
        "grad_norm": 1.4849834442138672,
        "learning_rate": 0.000188283778286994,
        "epoch": 0.15725858300115725,
        "step": 1223
    },
    {
        "loss": 2.2656,
        "grad_norm": 1.9452731609344482,
        "learning_rate": 0.00018826476130327977,
        "epoch": 0.15738716728815738,
        "step": 1224
    },
    {
        "loss": 1.5404,
        "grad_norm": 1.8096706867218018,
        "learning_rate": 0.00018824572986051377,
        "epoch": 0.15751575157515751,
        "step": 1225
    },
    {
        "loss": 1.4394,
        "grad_norm": 2.0188698768615723,
        "learning_rate": 0.00018822668396181356,
        "epoch": 0.15764433586215765,
        "step": 1226
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.6499310731887817,
        "learning_rate": 0.0001882076236102992,
        "epoch": 0.15777292014915778,
        "step": 1227
    },
    {
        "loss": 2.0643,
        "grad_norm": 1.426521897315979,
        "learning_rate": 0.000188188548809093,
        "epoch": 0.1579015044361579,
        "step": 1228
    },
    {
        "loss": 1.8636,
        "grad_norm": 2.1971397399902344,
        "learning_rate": 0.00018816945956131974,
        "epoch": 0.15803008872315802,
        "step": 1229
    },
    {
        "loss": 2.4287,
        "grad_norm": 1.6467063426971436,
        "learning_rate": 0.00018815035587010646,
        "epoch": 0.15815867301015815,
        "step": 1230
    },
    {
        "loss": 2.1014,
        "grad_norm": 1.3518407344818115,
        "learning_rate": 0.00018813123773858266,
        "epoch": 0.15828725729715828,
        "step": 1231
    },
    {
        "loss": 1.5666,
        "grad_norm": 2.1635894775390625,
        "learning_rate": 0.00018811210516988015,
        "epoch": 0.15841584158415842,
        "step": 1232
    },
    {
        "loss": 2.7123,
        "grad_norm": 2.0062661170959473,
        "learning_rate": 0.00018809295816713312,
        "epoch": 0.15854442587115855,
        "step": 1233
    },
    {
        "loss": 2.4704,
        "grad_norm": 1.3279861211776733,
        "learning_rate": 0.00018807379673347816,
        "epoch": 0.15867301015815868,
        "step": 1234
    },
    {
        "loss": 2.3104,
        "grad_norm": 1.5953974723815918,
        "learning_rate": 0.00018805462087205415,
        "epoch": 0.1588015944451588,
        "step": 1235
    },
    {
        "loss": 0.9137,
        "grad_norm": 2.066983938217163,
        "learning_rate": 0.00018803543058600243,
        "epoch": 0.15893017873215892,
        "step": 1236
    },
    {
        "loss": 2.2807,
        "grad_norm": 1.70842444896698,
        "learning_rate": 0.00018801622587846656,
        "epoch": 0.15905876301915906,
        "step": 1237
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.776197075843811,
        "learning_rate": 0.00018799700675259268,
        "epoch": 0.1591873473061592,
        "step": 1238
    },
    {
        "loss": 2.0283,
        "grad_norm": 1.5785809755325317,
        "learning_rate": 0.00018797777321152902,
        "epoch": 0.15931593159315932,
        "step": 1239
    },
    {
        "loss": 1.995,
        "grad_norm": 1.654134750366211,
        "learning_rate": 0.00018795852525842639,
        "epoch": 0.15944451588015945,
        "step": 1240
    },
    {
        "loss": 2.3234,
        "grad_norm": 1.5978609323501587,
        "learning_rate": 0.0001879392628964379,
        "epoch": 0.15957310016715956,
        "step": 1241
    },
    {
        "loss": 1.7761,
        "grad_norm": 1.8921595811843872,
        "learning_rate": 0.00018791998612871895,
        "epoch": 0.1597016844541597,
        "step": 1242
    },
    {
        "loss": 2.2981,
        "grad_norm": 1.775107979774475,
        "learning_rate": 0.0001879006949584274,
        "epoch": 0.15983026874115983,
        "step": 1243
    },
    {
        "loss": 2.2214,
        "grad_norm": 1.6413041353225708,
        "learning_rate": 0.00018788138938872343,
        "epoch": 0.15995885302815996,
        "step": 1244
    },
    {
        "loss": 1.9061,
        "grad_norm": 1.6939595937728882,
        "learning_rate": 0.00018786206942276952,
        "epoch": 0.1600874373151601,
        "step": 1245
    },
    {
        "loss": 1.9705,
        "grad_norm": 1.5720152854919434,
        "learning_rate": 0.00018784273506373064,
        "epoch": 0.16021602160216022,
        "step": 1246
    },
    {
        "loss": 1.8842,
        "grad_norm": 2.4863264560699463,
        "learning_rate": 0.00018782338631477395,
        "epoch": 0.16034460588916036,
        "step": 1247
    },
    {
        "loss": 1.9812,
        "grad_norm": 1.4661632776260376,
        "learning_rate": 0.0001878040231790691,
        "epoch": 0.16047319017616046,
        "step": 1248
    },
    {
        "loss": 1.8747,
        "grad_norm": 2.1375303268432617,
        "learning_rate": 0.00018778464565978803,
        "epoch": 0.1606017744631606,
        "step": 1249
    },
    {
        "loss": 2.5168,
        "grad_norm": 1.2626794576644897,
        "learning_rate": 0.0001877652537601051,
        "epoch": 0.16073035875016073,
        "step": 1250
    },
    {
        "loss": 2.1367,
        "grad_norm": 1.7086366415023804,
        "learning_rate": 0.00018774584748319702,
        "epoch": 0.16085894303716086,
        "step": 1251
    },
    {
        "loss": 1.621,
        "grad_norm": 1.693581223487854,
        "learning_rate": 0.0001877264268322427,
        "epoch": 0.160987527324161,
        "step": 1252
    },
    {
        "loss": 1.4876,
        "grad_norm": 2.0002167224884033,
        "learning_rate": 0.00018770699181042363,
        "epoch": 0.16111611161116113,
        "step": 1253
    },
    {
        "loss": 1.9028,
        "grad_norm": 2.058941125869751,
        "learning_rate": 0.0001876875424209235,
        "epoch": 0.16124469589816123,
        "step": 1254
    },
    {
        "loss": 1.6075,
        "grad_norm": 1.4888793230056763,
        "learning_rate": 0.0001876680786669284,
        "epoch": 0.16137328018516137,
        "step": 1255
    },
    {
        "loss": 2.3678,
        "grad_norm": 1.0948905944824219,
        "learning_rate": 0.00018764860055162682,
        "epoch": 0.1615018644721615,
        "step": 1256
    },
    {
        "loss": 2.1837,
        "grad_norm": 1.6082321405410767,
        "learning_rate": 0.0001876291080782095,
        "epoch": 0.16163044875916163,
        "step": 1257
    },
    {
        "loss": 2.4127,
        "grad_norm": 0.8622362017631531,
        "learning_rate": 0.00018760960124986965,
        "epoch": 0.16175903304616177,
        "step": 1258
    },
    {
        "loss": 1.4531,
        "grad_norm": 1.632460355758667,
        "learning_rate": 0.00018759008006980271,
        "epoch": 0.1618876173331619,
        "step": 1259
    },
    {
        "loss": 1.562,
        "grad_norm": 1.6703369617462158,
        "learning_rate": 0.00018757054454120658,
        "epoch": 0.162016201620162,
        "step": 1260
    },
    {
        "loss": 2.4347,
        "grad_norm": 1.4780302047729492,
        "learning_rate": 0.0001875509946672815,
        "epoch": 0.16214478590716214,
        "step": 1261
    },
    {
        "loss": 1.9266,
        "grad_norm": 2.5021228790283203,
        "learning_rate": 0.0001875314304512299,
        "epoch": 0.16227337019416227,
        "step": 1262
    },
    {
        "loss": 2.2053,
        "grad_norm": 1.2836045026779175,
        "learning_rate": 0.00018751185189625679,
        "epoch": 0.1624019544811624,
        "step": 1263
    },
    {
        "loss": 2.5219,
        "grad_norm": 1.544089436531067,
        "learning_rate": 0.0001874922590055694,
        "epoch": 0.16253053876816254,
        "step": 1264
    },
    {
        "loss": 1.8982,
        "grad_norm": 2.2891974449157715,
        "learning_rate": 0.00018747265178237728,
        "epoch": 0.16265912305516267,
        "step": 1265
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.734814167022705,
        "learning_rate": 0.00018745303022989245,
        "epoch": 0.16278770734216277,
        "step": 1266
    },
    {
        "loss": 1.9252,
        "grad_norm": 1.2974058389663696,
        "learning_rate": 0.00018743339435132916,
        "epoch": 0.1629162916291629,
        "step": 1267
    },
    {
        "loss": 1.9856,
        "grad_norm": 2.1957905292510986,
        "learning_rate": 0.00018741374414990407,
        "epoch": 0.16304487591616304,
        "step": 1268
    },
    {
        "loss": 1.5671,
        "grad_norm": 1.755379557609558,
        "learning_rate": 0.00018739407962883617,
        "epoch": 0.16317346020316317,
        "step": 1269
    },
    {
        "loss": 1.2828,
        "grad_norm": 1.6615277528762817,
        "learning_rate": 0.00018737440079134675,
        "epoch": 0.1633020444901633,
        "step": 1270
    },
    {
        "loss": 2.0412,
        "grad_norm": 1.286576747894287,
        "learning_rate": 0.00018735470764065957,
        "epoch": 0.16343062877716344,
        "step": 1271
    },
    {
        "loss": 2.0501,
        "grad_norm": 1.6332308053970337,
        "learning_rate": 0.00018733500018000064,
        "epoch": 0.16355921306416357,
        "step": 1272
    },
    {
        "loss": 1.3845,
        "grad_norm": 2.0306077003479004,
        "learning_rate": 0.00018731527841259825,
        "epoch": 0.16368779735116368,
        "step": 1273
    },
    {
        "loss": 1.839,
        "grad_norm": 2.1380412578582764,
        "learning_rate": 0.00018729554234168315,
        "epoch": 0.1638163816381638,
        "step": 1274
    },
    {
        "loss": 1.6289,
        "grad_norm": 2.237058401107788,
        "learning_rate": 0.00018727579197048847,
        "epoch": 0.16394496592516394,
        "step": 1275
    },
    {
        "loss": 2.3672,
        "grad_norm": 1.9421896934509277,
        "learning_rate": 0.0001872560273022495,
        "epoch": 0.16407355021216408,
        "step": 1276
    },
    {
        "loss": 1.7086,
        "grad_norm": 1.700548529624939,
        "learning_rate": 0.00018723624834020405,
        "epoch": 0.1642021344991642,
        "step": 1277
    },
    {
        "loss": 2.0716,
        "grad_norm": 1.945879578590393,
        "learning_rate": 0.0001872164550875922,
        "epoch": 0.16433071878616434,
        "step": 1278
    },
    {
        "loss": 1.9333,
        "grad_norm": 1.683439016342163,
        "learning_rate": 0.0001871966475476563,
        "epoch": 0.16445930307316445,
        "step": 1279
    },
    {
        "loss": 1.5703,
        "grad_norm": 1.5823057889938354,
        "learning_rate": 0.0001871768257236412,
        "epoch": 0.16458788736016458,
        "step": 1280
    },
    {
        "loss": 2.4191,
        "grad_norm": 1.5399755239486694,
        "learning_rate": 0.00018715698961879398,
        "epoch": 0.16471647164716471,
        "step": 1281
    },
    {
        "loss": 1.8875,
        "grad_norm": 2.355846881866455,
        "learning_rate": 0.00018713713923636404,
        "epoch": 0.16484505593416485,
        "step": 1282
    },
    {
        "loss": 1.6068,
        "grad_norm": 2.090531587600708,
        "learning_rate": 0.0001871172745796032,
        "epoch": 0.16497364022116498,
        "step": 1283
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.7457225322723389,
        "learning_rate": 0.0001870973956517656,
        "epoch": 0.1651022245081651,
        "step": 1284
    },
    {
        "loss": 1.2065,
        "grad_norm": 2.016011953353882,
        "learning_rate": 0.00018707750245610766,
        "epoch": 0.16523080879516522,
        "step": 1285
    },
    {
        "loss": 1.8173,
        "grad_norm": 1.6623823642730713,
        "learning_rate": 0.00018705759499588817,
        "epoch": 0.16535939308216535,
        "step": 1286
    },
    {
        "loss": 1.135,
        "grad_norm": 1.6871719360351562,
        "learning_rate": 0.00018703767327436828,
        "epoch": 0.16548797736916548,
        "step": 1287
    },
    {
        "loss": 2.6384,
        "grad_norm": 1.0788848400115967,
        "learning_rate": 0.00018701773729481146,
        "epoch": 0.16561656165616562,
        "step": 1288
    },
    {
        "loss": 1.6503,
        "grad_norm": 1.80449378490448,
        "learning_rate": 0.0001869977870604835,
        "epoch": 0.16574514594316575,
        "step": 1289
    },
    {
        "loss": 2.1962,
        "grad_norm": 1.3575531244277954,
        "learning_rate": 0.00018697782257465255,
        "epoch": 0.16587373023016588,
        "step": 1290
    },
    {
        "loss": 2.298,
        "grad_norm": 1.5015593767166138,
        "learning_rate": 0.00018695784384058908,
        "epoch": 0.166002314517166,
        "step": 1291
    },
    {
        "loss": 1.9838,
        "grad_norm": 2.026935338973999,
        "learning_rate": 0.0001869378508615659,
        "epoch": 0.16613089880416612,
        "step": 1292
    },
    {
        "loss": 1.8799,
        "grad_norm": 1.8294082880020142,
        "learning_rate": 0.00018691784364085817,
        "epoch": 0.16625948309116625,
        "step": 1293
    },
    {
        "loss": 2.0245,
        "grad_norm": 1.9336317777633667,
        "learning_rate": 0.00018689782218174328,
        "epoch": 0.1663880673781664,
        "step": 1294
    },
    {
        "loss": 2.2341,
        "grad_norm": 1.432844638824463,
        "learning_rate": 0.00018687778648750114,
        "epoch": 0.16651665166516652,
        "step": 1295
    },
    {
        "loss": 1.9756,
        "grad_norm": 1.6896711587905884,
        "learning_rate": 0.00018685773656141382,
        "epoch": 0.16664523595216665,
        "step": 1296
    },
    {
        "loss": 1.9448,
        "grad_norm": 1.5200904607772827,
        "learning_rate": 0.00018683767240676585,
        "epoch": 0.1667738202391668,
        "step": 1297
    },
    {
        "loss": 1.3893,
        "grad_norm": 1.2218643426895142,
        "learning_rate": 0.00018681759402684397,
        "epoch": 0.1669024045261669,
        "step": 1298
    },
    {
        "loss": 1.3683,
        "grad_norm": 2.0666022300720215,
        "learning_rate": 0.00018679750142493737,
        "epoch": 0.16703098881316703,
        "step": 1299
    },
    {
        "loss": 2.5803,
        "grad_norm": 1.5733277797698975,
        "learning_rate": 0.00018677739460433746,
        "epoch": 0.16715957310016716,
        "step": 1300
    },
    {
        "eval_loss": 2.053619146347046,
        "eval_runtime": 28.2284,
        "eval_samples_per_second": 2.799,
        "eval_steps_per_second": 2.799,
        "epoch": 0.16715957310016716,
        "step": 1300
    },
    {
        "loss": 1.4634,
        "grad_norm": 1.6240490674972534,
        "learning_rate": 0.000186757273568338,
        "epoch": 0.1672881573871673,
        "step": 1301
    },
    {
        "loss": 1.3613,
        "grad_norm": 2.4917900562286377,
        "learning_rate": 0.0001867371383202352,
        "epoch": 0.16741674167416742,
        "step": 1302
    },
    {
        "loss": 1.4129,
        "grad_norm": 1.8820897340774536,
        "learning_rate": 0.00018671698886332748,
        "epoch": 0.16754532596116756,
        "step": 1303
    },
    {
        "loss": 1.6374,
        "grad_norm": 1.7342538833618164,
        "learning_rate": 0.00018669682520091555,
        "epoch": 0.16767391024816766,
        "step": 1304
    },
    {
        "loss": 1.9082,
        "grad_norm": 1.5704268217086792,
        "learning_rate": 0.0001866766473363026,
        "epoch": 0.1678024945351678,
        "step": 1305
    },
    {
        "loss": 1.9954,
        "grad_norm": 1.6256389617919922,
        "learning_rate": 0.000186656455272794,
        "epoch": 0.16793107882216793,
        "step": 1306
    },
    {
        "loss": 2.0504,
        "grad_norm": 2.517007827758789,
        "learning_rate": 0.00018663624901369747,
        "epoch": 0.16805966310916806,
        "step": 1307
    },
    {
        "loss": 2.4792,
        "grad_norm": 1.5338929891586304,
        "learning_rate": 0.00018661602856232318,
        "epoch": 0.1681882473961682,
        "step": 1308
    },
    {
        "loss": 2.383,
        "grad_norm": 1.9677339792251587,
        "learning_rate": 0.0001865957939219835,
        "epoch": 0.16831683168316833,
        "step": 1309
    },
    {
        "loss": 2.2301,
        "grad_norm": 1.4308953285217285,
        "learning_rate": 0.00018657554509599313,
        "epoch": 0.16844541597016843,
        "step": 1310
    },
    {
        "loss": 1.8625,
        "grad_norm": 1.4789413213729858,
        "learning_rate": 0.00018655528208766917,
        "epoch": 0.16857400025716857,
        "step": 1311
    },
    {
        "loss": 2.563,
        "grad_norm": 1.3977124691009521,
        "learning_rate": 0.00018653500490033094,
        "epoch": 0.1687025845441687,
        "step": 1312
    },
    {
        "loss": 1.7442,
        "grad_norm": 1.645396113395691,
        "learning_rate": 0.00018651471353730017,
        "epoch": 0.16883116883116883,
        "step": 1313
    },
    {
        "loss": 1.8402,
        "grad_norm": 1.616437315940857,
        "learning_rate": 0.00018649440800190087,
        "epoch": 0.16895975311816896,
        "step": 1314
    },
    {
        "loss": 2.0894,
        "grad_norm": 1.3379989862442017,
        "learning_rate": 0.0001864740882974594,
        "epoch": 0.1690883374051691,
        "step": 1315
    },
    {
        "loss": 1.8347,
        "grad_norm": 2.1592955589294434,
        "learning_rate": 0.0001864537544273044,
        "epoch": 0.1692169216921692,
        "step": 1316
    },
    {
        "loss": 2.2767,
        "grad_norm": 1.9426112174987793,
        "learning_rate": 0.0001864334063947669,
        "epoch": 0.16934550597916934,
        "step": 1317
    },
    {
        "loss": 1.5482,
        "grad_norm": 1.7612146139144897,
        "learning_rate": 0.00018641304420318014,
        "epoch": 0.16947409026616947,
        "step": 1318
    },
    {
        "loss": 1.813,
        "grad_norm": 2.0101656913757324,
        "learning_rate": 0.00018639266785587974,
        "epoch": 0.1696026745531696,
        "step": 1319
    },
    {
        "loss": 1.962,
        "grad_norm": 1.3269635438919067,
        "learning_rate": 0.00018637227735620373,
        "epoch": 0.16973125884016974,
        "step": 1320
    },
    {
        "loss": 1.7681,
        "grad_norm": 1.915069341659546,
        "learning_rate": 0.0001863518727074923,
        "epoch": 0.16985984312716987,
        "step": 1321
    },
    {
        "loss": 1.9065,
        "grad_norm": 1.5109846591949463,
        "learning_rate": 0.00018633145391308803,
        "epoch": 0.16998842741417,
        "step": 1322
    },
    {
        "loss": 1.885,
        "grad_norm": 1.842969536781311,
        "learning_rate": 0.0001863110209763358,
        "epoch": 0.1701170117011701,
        "step": 1323
    },
    {
        "loss": 2.1438,
        "grad_norm": 1.7392725944519043,
        "learning_rate": 0.0001862905739005829,
        "epoch": 0.17024559598817024,
        "step": 1324
    },
    {
        "loss": 1.1376,
        "grad_norm": 2.1731603145599365,
        "learning_rate": 0.00018627011268917883,
        "epoch": 0.17037418027517037,
        "step": 1325
    },
    {
        "loss": 1.3838,
        "grad_norm": 1.711361050605774,
        "learning_rate": 0.00018624963734547537,
        "epoch": 0.1705027645621705,
        "step": 1326
    },
    {
        "loss": 2.6181,
        "grad_norm": 1.7987089157104492,
        "learning_rate": 0.00018622914787282672,
        "epoch": 0.17063134884917064,
        "step": 1327
    },
    {
        "loss": 1.5837,
        "grad_norm": 2.065107583999634,
        "learning_rate": 0.00018620864427458934,
        "epoch": 0.17075993313617077,
        "step": 1328
    },
    {
        "loss": 1.353,
        "grad_norm": 2.4445266723632812,
        "learning_rate": 0.00018618812655412205,
        "epoch": 0.17088851742317088,
        "step": 1329
    },
    {
        "loss": 2.1429,
        "grad_norm": 1.8235657215118408,
        "learning_rate": 0.00018616759471478598,
        "epoch": 0.171017101710171,
        "step": 1330
    },
    {
        "loss": 1.7367,
        "grad_norm": 1.6808544397354126,
        "learning_rate": 0.00018614704875994447,
        "epoch": 0.17114568599717114,
        "step": 1331
    },
    {
        "loss": 1.561,
        "grad_norm": 2.6698029041290283,
        "learning_rate": 0.00018612648869296324,
        "epoch": 0.17127427028417128,
        "step": 1332
    },
    {
        "loss": 0.713,
        "grad_norm": 1.6973971128463745,
        "learning_rate": 0.00018610591451721038,
        "epoch": 0.1714028545711714,
        "step": 1333
    },
    {
        "loss": 2.2701,
        "grad_norm": 1.940242886543274,
        "learning_rate": 0.00018608532623605624,
        "epoch": 0.17153143885817154,
        "step": 1334
    },
    {
        "loss": 2.3707,
        "grad_norm": 1.5094969272613525,
        "learning_rate": 0.00018606472385287345,
        "epoch": 0.17166002314517165,
        "step": 1335
    },
    {
        "loss": 1.4522,
        "grad_norm": 2.264704942703247,
        "learning_rate": 0.000186044107371037,
        "epoch": 0.17178860743217178,
        "step": 1336
    },
    {
        "loss": 1.6544,
        "grad_norm": 1.9252829551696777,
        "learning_rate": 0.00018602347679392414,
        "epoch": 0.1719171917191719,
        "step": 1337
    },
    {
        "loss": 2.0447,
        "grad_norm": 1.9594680070877075,
        "learning_rate": 0.00018600283212491452,
        "epoch": 0.17204577600617205,
        "step": 1338
    },
    {
        "loss": 1.9577,
        "grad_norm": 1.8792840242385864,
        "learning_rate": 0.00018598217336738998,
        "epoch": 0.17217436029317218,
        "step": 1339
    },
    {
        "loss": 2.1196,
        "grad_norm": 1.8069099187850952,
        "learning_rate": 0.00018596150052473473,
        "epoch": 0.1723029445801723,
        "step": 1340
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.6224963665008545,
        "learning_rate": 0.0001859408136003353,
        "epoch": 0.17243152886717242,
        "step": 1341
    },
    {
        "loss": 1.4697,
        "grad_norm": 1.5471491813659668,
        "learning_rate": 0.00018592011259758052,
        "epoch": 0.17256011315417255,
        "step": 1342
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.211707353591919,
        "learning_rate": 0.0001858993975198615,
        "epoch": 0.17268869744117268,
        "step": 1343
    },
    {
        "loss": 2.3583,
        "grad_norm": 1.4288487434387207,
        "learning_rate": 0.00018587866837057161,
        "epoch": 0.17281728172817282,
        "step": 1344
    },
    {
        "loss": 1.7547,
        "grad_norm": 2.007345676422119,
        "learning_rate": 0.0001858579251531067,
        "epoch": 0.17294586601517295,
        "step": 1345
    },
    {
        "loss": 2.2171,
        "grad_norm": 1.2242926359176636,
        "learning_rate": 0.00018583716787086473,
        "epoch": 0.17307445030217308,
        "step": 1346
    },
    {
        "loss": 1.6944,
        "grad_norm": 1.8623342514038086,
        "learning_rate": 0.0001858163965272461,
        "epoch": 0.17320303458917322,
        "step": 1347
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.5422518253326416,
        "learning_rate": 0.0001857956111256534,
        "epoch": 0.17333161887617332,
        "step": 1348
    },
    {
        "loss": 2.4306,
        "grad_norm": 1.3850609064102173,
        "learning_rate": 0.00018577481166949164,
        "epoch": 0.17346020316317345,
        "step": 1349
    },
    {
        "loss": 2.0073,
        "grad_norm": 1.8864811658859253,
        "learning_rate": 0.00018575399816216805,
        "epoch": 0.1735887874501736,
        "step": 1350
    },
    {
        "loss": 2.4489,
        "grad_norm": 1.6038613319396973,
        "learning_rate": 0.00018573317060709217,
        "epoch": 0.17371737173717372,
        "step": 1351
    },
    {
        "loss": 1.9138,
        "grad_norm": 1.3348249197006226,
        "learning_rate": 0.00018571232900767585,
        "epoch": 0.17384595602417385,
        "step": 1352
    },
    {
        "loss": 2.635,
        "grad_norm": 0.9480463862419128,
        "learning_rate": 0.00018569147336733327,
        "epoch": 0.173974540311174,
        "step": 1353
    },
    {
        "loss": 2.4825,
        "grad_norm": 1.2324697971343994,
        "learning_rate": 0.0001856706036894809,
        "epoch": 0.1741031245981741,
        "step": 1354
    },
    {
        "loss": 1.3701,
        "grad_norm": 1.7870597839355469,
        "learning_rate": 0.00018564971997753747,
        "epoch": 0.17423170888517422,
        "step": 1355
    },
    {
        "loss": 1.5882,
        "grad_norm": 2.4379079341888428,
        "learning_rate": 0.00018562882223492402,
        "epoch": 0.17436029317217436,
        "step": 1356
    },
    {
        "loss": 2.2814,
        "grad_norm": 1.1588808298110962,
        "learning_rate": 0.00018560791046506395,
        "epoch": 0.1744888774591745,
        "step": 1357
    },
    {
        "loss": 1.8944,
        "grad_norm": 1.9725536108016968,
        "learning_rate": 0.00018558698467138288,
        "epoch": 0.17461746174617462,
        "step": 1358
    },
    {
        "loss": 1.5065,
        "grad_norm": 1.8555986881256104,
        "learning_rate": 0.0001855660448573088,
        "epoch": 0.17474604603317476,
        "step": 1359
    },
    {
        "loss": 1.923,
        "grad_norm": 1.7625805139541626,
        "learning_rate": 0.0001855450910262719,
        "epoch": 0.17487463032017486,
        "step": 1360
    },
    {
        "loss": 2.1172,
        "grad_norm": 1.9362848997116089,
        "learning_rate": 0.00018552412318170473,
        "epoch": 0.175003214607175,
        "step": 1361
    },
    {
        "loss": 1.9082,
        "grad_norm": 2.240565299987793,
        "learning_rate": 0.00018550314132704217,
        "epoch": 0.17513179889417513,
        "step": 1362
    },
    {
        "loss": 1.9803,
        "grad_norm": 1.5457651615142822,
        "learning_rate": 0.0001854821454657213,
        "epoch": 0.17526038318117526,
        "step": 1363
    },
    {
        "loss": 1.8825,
        "grad_norm": 1.646024465560913,
        "learning_rate": 0.00018546113560118162,
        "epoch": 0.1753889674681754,
        "step": 1364
    },
    {
        "loss": 1.8671,
        "grad_norm": 1.7957890033721924,
        "learning_rate": 0.00018544011173686478,
        "epoch": 0.17551755175517553,
        "step": 1365
    },
    {
        "loss": 1.7961,
        "grad_norm": 1.7280209064483643,
        "learning_rate": 0.00018541907387621483,
        "epoch": 0.17564613604217563,
        "step": 1366
    },
    {
        "loss": 1.4178,
        "grad_norm": 2.0296552181243896,
        "learning_rate": 0.00018539802202267806,
        "epoch": 0.17577472032917577,
        "step": 1367
    },
    {
        "loss": 2.1103,
        "grad_norm": 1.4593732357025146,
        "learning_rate": 0.0001853769561797031,
        "epoch": 0.1759033046161759,
        "step": 1368
    },
    {
        "loss": 1.8606,
        "grad_norm": 2.2353668212890625,
        "learning_rate": 0.0001853558763507408,
        "epoch": 0.17603188890317603,
        "step": 1369
    },
    {
        "loss": 2.4191,
        "grad_norm": 1.5641189813613892,
        "learning_rate": 0.00018533478253924437,
        "epoch": 0.17616047319017616,
        "step": 1370
    },
    {
        "loss": 2.4409,
        "grad_norm": 1.536572813987732,
        "learning_rate": 0.00018531367474866928,
        "epoch": 0.1762890574771763,
        "step": 1371
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.2470353841781616,
        "learning_rate": 0.0001852925529824733,
        "epoch": 0.17641764176417643,
        "step": 1372
    },
    {
        "loss": 2.2871,
        "grad_norm": 1.3579822778701782,
        "learning_rate": 0.00018527141724411644,
        "epoch": 0.17654622605117654,
        "step": 1373
    },
    {
        "loss": 1.7066,
        "grad_norm": 1.8307278156280518,
        "learning_rate": 0.00018525026753706108,
        "epoch": 0.17667481033817667,
        "step": 1374
    },
    {
        "loss": 2.5536,
        "grad_norm": 1.5218162536621094,
        "learning_rate": 0.00018522910386477187,
        "epoch": 0.1768033946251768,
        "step": 1375
    },
    {
        "loss": 1.6245,
        "grad_norm": 1.2348493337631226,
        "learning_rate": 0.0001852079262307157,
        "epoch": 0.17693197891217693,
        "step": 1376
    },
    {
        "loss": 2.0046,
        "grad_norm": 1.8609198331832886,
        "learning_rate": 0.00018518673463836178,
        "epoch": 0.17706056319917707,
        "step": 1377
    },
    {
        "loss": 1.9297,
        "grad_norm": 1.8406368494033813,
        "learning_rate": 0.00018516552909118156,
        "epoch": 0.1771891474861772,
        "step": 1378
    },
    {
        "loss": 2.5016,
        "grad_norm": 1.221375584602356,
        "learning_rate": 0.00018514430959264887,
        "epoch": 0.1773177317731773,
        "step": 1379
    },
    {
        "loss": 2.0662,
        "grad_norm": 2.3230185508728027,
        "learning_rate": 0.0001851230761462398,
        "epoch": 0.17744631606017744,
        "step": 1380
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.2429862022399902,
        "learning_rate": 0.0001851018287554326,
        "epoch": 0.17757490034717757,
        "step": 1381
    },
    {
        "loss": 1.8526,
        "grad_norm": 1.8077698945999146,
        "learning_rate": 0.00018508056742370797,
        "epoch": 0.1777034846341777,
        "step": 1382
    },
    {
        "loss": 1.8552,
        "grad_norm": 1.671947956085205,
        "learning_rate": 0.0001850592921545488,
        "epoch": 0.17783206892117784,
        "step": 1383
    },
    {
        "loss": 1.8536,
        "grad_norm": 2.199522018432617,
        "learning_rate": 0.0001850380029514403,
        "epoch": 0.17796065320817797,
        "step": 1384
    },
    {
        "loss": 2.4124,
        "grad_norm": 2.072530508041382,
        "learning_rate": 0.00018501669981786997,
        "epoch": 0.17808923749517808,
        "step": 1385
    },
    {
        "loss": 1.2009,
        "grad_norm": 1.733599305152893,
        "learning_rate": 0.00018499538275732754,
        "epoch": 0.1782178217821782,
        "step": 1386
    },
    {
        "loss": 2.5924,
        "grad_norm": 1.2383854389190674,
        "learning_rate": 0.0001849740517733051,
        "epoch": 0.17834640606917834,
        "step": 1387
    },
    {
        "loss": 1.8717,
        "grad_norm": 2.8268556594848633,
        "learning_rate": 0.00018495270686929687,
        "epoch": 0.17847499035617848,
        "step": 1388
    },
    {
        "loss": 1.5002,
        "grad_norm": 1.821484923362732,
        "learning_rate": 0.0001849313480487996,
        "epoch": 0.1786035746431786,
        "step": 1389
    },
    {
        "loss": 1.5518,
        "grad_norm": 2.4511194229125977,
        "learning_rate": 0.0001849099753153121,
        "epoch": 0.17873215893017874,
        "step": 1390
    },
    {
        "loss": 1.7161,
        "grad_norm": 2.172781467437744,
        "learning_rate": 0.00018488858867233548,
        "epoch": 0.17886074321717885,
        "step": 1391
    },
    {
        "loss": 1.8384,
        "grad_norm": 1.2423776388168335,
        "learning_rate": 0.00018486718812337331,
        "epoch": 0.17898932750417898,
        "step": 1392
    },
    {
        "loss": 2.054,
        "grad_norm": 1.5976407527923584,
        "learning_rate": 0.0001848457736719312,
        "epoch": 0.1791179117911791,
        "step": 1393
    },
    {
        "loss": 1.9843,
        "grad_norm": 1.3363864421844482,
        "learning_rate": 0.00018482434532151722,
        "epoch": 0.17924649607817925,
        "step": 1394
    },
    {
        "loss": 1.6578,
        "grad_norm": 2.120053291320801,
        "learning_rate": 0.0001848029030756416,
        "epoch": 0.17937508036517938,
        "step": 1395
    },
    {
        "loss": 2.3096,
        "grad_norm": 1.6723016500473022,
        "learning_rate": 0.00018478144693781693,
        "epoch": 0.1795036646521795,
        "step": 1396
    },
    {
        "loss": 2.2857,
        "grad_norm": 2.3845016956329346,
        "learning_rate": 0.000184759976911558,
        "epoch": 0.17963224893917965,
        "step": 1397
    },
    {
        "loss": 2.181,
        "grad_norm": 1.500597596168518,
        "learning_rate": 0.00018473849300038194,
        "epoch": 0.17976083322617975,
        "step": 1398
    },
    {
        "loss": 2.051,
        "grad_norm": 2.237790822982788,
        "learning_rate": 0.0001847169952078081,
        "epoch": 0.17988941751317988,
        "step": 1399
    },
    {
        "loss": 2.0638,
        "grad_norm": 1.8499653339385986,
        "learning_rate": 0.00018469548353735818,
        "epoch": 0.18001800180018002,
        "step": 1400
    },
    {
        "eval_loss": 2.044238328933716,
        "eval_runtime": 28.2791,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.18001800180018002,
        "step": 1400
    },
    {
        "loss": 1.9534,
        "grad_norm": 2.092062473297119,
        "learning_rate": 0.00018467395799255605,
        "epoch": 0.18014658608718015,
        "step": 1401
    },
    {
        "loss": 2.5808,
        "grad_norm": 1.3540499210357666,
        "learning_rate": 0.0001846524185769279,
        "epoch": 0.18027517037418028,
        "step": 1402
    },
    {
        "loss": 1.7435,
        "grad_norm": 1.8335058689117432,
        "learning_rate": 0.0001846308652940023,
        "epoch": 0.18040375466118042,
        "step": 1403
    },
    {
        "loss": 1.932,
        "grad_norm": 1.7798361778259277,
        "learning_rate": 0.00018460929814730987,
        "epoch": 0.18053233894818052,
        "step": 1404
    },
    {
        "loss": 2.1748,
        "grad_norm": 1.5877554416656494,
        "learning_rate": 0.00018458771714038367,
        "epoch": 0.18066092323518065,
        "step": 1405
    },
    {
        "loss": 2.604,
        "grad_norm": 1.5664008855819702,
        "learning_rate": 0.00018456612227675902,
        "epoch": 0.1807895075221808,
        "step": 1406
    },
    {
        "loss": 2.7152,
        "grad_norm": 1.7247967720031738,
        "learning_rate": 0.0001845445135599734,
        "epoch": 0.18091809180918092,
        "step": 1407
    },
    {
        "loss": 2.3969,
        "grad_norm": 1.552354097366333,
        "learning_rate": 0.00018452289099356672,
        "epoch": 0.18104667609618105,
        "step": 1408
    },
    {
        "loss": 2.2745,
        "grad_norm": 1.7294036149978638,
        "learning_rate": 0.000184501254581081,
        "epoch": 0.18117526038318119,
        "step": 1409
    },
    {
        "loss": 2.3618,
        "grad_norm": 1.9430909156799316,
        "learning_rate": 0.0001844796043260606,
        "epoch": 0.1813038446701813,
        "step": 1410
    },
    {
        "loss": 1.9542,
        "grad_norm": 1.850852370262146,
        "learning_rate": 0.0001844579402320522,
        "epoch": 0.18143242895718142,
        "step": 1411
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.8198840618133545,
        "learning_rate": 0.0001844362623026046,
        "epoch": 0.18156101324418156,
        "step": 1412
    },
    {
        "loss": 2.2554,
        "grad_norm": 1.7598353624343872,
        "learning_rate": 0.0001844145705412691,
        "epoch": 0.1816895975311817,
        "step": 1413
    },
    {
        "loss": 2.432,
        "grad_norm": 1.3816554546356201,
        "learning_rate": 0.00018439286495159902,
        "epoch": 0.18181818181818182,
        "step": 1414
    },
    {
        "loss": 1.5467,
        "grad_norm": 1.699872374534607,
        "learning_rate": 0.00018437114553715008,
        "epoch": 0.18194676610518196,
        "step": 1415
    },
    {
        "loss": 2.0946,
        "grad_norm": 2.1403543949127197,
        "learning_rate": 0.00018434941230148022,
        "epoch": 0.18207535039218206,
        "step": 1416
    },
    {
        "loss": 2.0635,
        "grad_norm": 1.654358148574829,
        "learning_rate": 0.00018432766524814967,
        "epoch": 0.1822039346791822,
        "step": 1417
    },
    {
        "loss": 1.7221,
        "grad_norm": 1.892749309539795,
        "learning_rate": 0.00018430590438072092,
        "epoch": 0.18233251896618233,
        "step": 1418
    },
    {
        "loss": 2.2043,
        "grad_norm": 2.0982415676116943,
        "learning_rate": 0.00018428412970275873,
        "epoch": 0.18246110325318246,
        "step": 1419
    },
    {
        "loss": 1.9307,
        "grad_norm": 2.192948341369629,
        "learning_rate": 0.0001842623412178301,
        "epoch": 0.1825896875401826,
        "step": 1420
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.5997363328933716,
        "learning_rate": 0.0001842405389295043,
        "epoch": 0.18271827182718273,
        "step": 1421
    },
    {
        "loss": 2.1704,
        "grad_norm": 1.6361241340637207,
        "learning_rate": 0.00018421872284135284,
        "epoch": 0.18284685611418286,
        "step": 1422
    },
    {
        "loss": 2.0134,
        "grad_norm": 1.3100947141647339,
        "learning_rate": 0.00018419689295694954,
        "epoch": 0.18297544040118296,
        "step": 1423
    },
    {
        "loss": 2.2355,
        "grad_norm": 1.6170860528945923,
        "learning_rate": 0.00018417504927987043,
        "epoch": 0.1831040246881831,
        "step": 1424
    },
    {
        "loss": 2.1346,
        "grad_norm": 1.8340015411376953,
        "learning_rate": 0.00018415319181369383,
        "epoch": 0.18323260897518323,
        "step": 1425
    },
    {
        "loss": 1.7628,
        "grad_norm": 1.4985185861587524,
        "learning_rate": 0.00018413132056200034,
        "epoch": 0.18336119326218336,
        "step": 1426
    },
    {
        "loss": 1.5727,
        "grad_norm": 1.7264299392700195,
        "learning_rate": 0.00018410943552837273,
        "epoch": 0.1834897775491835,
        "step": 1427
    },
    {
        "loss": 1.6304,
        "grad_norm": 1.2665132284164429,
        "learning_rate": 0.00018408753671639617,
        "epoch": 0.18361836183618363,
        "step": 1428
    },
    {
        "loss": 2.5757,
        "grad_norm": 1.3420021533966064,
        "learning_rate": 0.00018406562412965794,
        "epoch": 0.18374694612318374,
        "step": 1429
    },
    {
        "loss": 1.795,
        "grad_norm": 2.3901772499084473,
        "learning_rate": 0.0001840436977717476,
        "epoch": 0.18387553041018387,
        "step": 1430
    },
    {
        "loss": 1.9469,
        "grad_norm": 1.7330304384231567,
        "learning_rate": 0.00018402175764625713,
        "epoch": 0.184004114697184,
        "step": 1431
    },
    {
        "loss": 2.0509,
        "grad_norm": 1.8842124938964844,
        "learning_rate": 0.00018399980375678054,
        "epoch": 0.18413269898418413,
        "step": 1432
    },
    {
        "loss": 2.0204,
        "grad_norm": 1.45087468624115,
        "learning_rate": 0.00018397783610691424,
        "epoch": 0.18426128327118427,
        "step": 1433
    },
    {
        "loss": 1.8544,
        "grad_norm": 1.7533873319625854,
        "learning_rate": 0.00018395585470025685,
        "epoch": 0.1843898675581844,
        "step": 1434
    },
    {
        "loss": 1.7666,
        "grad_norm": 1.872687816619873,
        "learning_rate": 0.0001839338595404092,
        "epoch": 0.1845184518451845,
        "step": 1435
    },
    {
        "loss": 2.4238,
        "grad_norm": 1.2835243940353394,
        "learning_rate": 0.0001839118506309745,
        "epoch": 0.18464703613218464,
        "step": 1436
    },
    {
        "loss": 2.1244,
        "grad_norm": 1.5410213470458984,
        "learning_rate": 0.00018388982797555806,
        "epoch": 0.18477562041918477,
        "step": 1437
    },
    {
        "loss": 1.2688,
        "grad_norm": 2.013787269592285,
        "learning_rate": 0.00018386779157776752,
        "epoch": 0.1849042047061849,
        "step": 1438
    },
    {
        "loss": 1.5882,
        "grad_norm": 1.878755807876587,
        "learning_rate": 0.0001838457414412128,
        "epoch": 0.18503278899318504,
        "step": 1439
    },
    {
        "loss": 1.7915,
        "grad_norm": 2.1360952854156494,
        "learning_rate": 0.000183823677569506,
        "epoch": 0.18516137328018517,
        "step": 1440
    },
    {
        "loss": 1.9983,
        "grad_norm": 2.6176645755767822,
        "learning_rate": 0.0001838015999662615,
        "epoch": 0.18528995756718528,
        "step": 1441
    },
    {
        "loss": 1.4199,
        "grad_norm": 2.2844810485839844,
        "learning_rate": 0.00018377950863509595,
        "epoch": 0.1854185418541854,
        "step": 1442
    },
    {
        "loss": 2.675,
        "grad_norm": 1.308779239654541,
        "learning_rate": 0.0001837574035796282,
        "epoch": 0.18554712614118554,
        "step": 1443
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.7290419340133667,
        "learning_rate": 0.00018373528480347947,
        "epoch": 0.18567571042818568,
        "step": 1444
    },
    {
        "loss": 2.069,
        "grad_norm": 1.4652034044265747,
        "learning_rate": 0.000183713152310273,
        "epoch": 0.1858042947151858,
        "step": 1445
    },
    {
        "loss": 1.7424,
        "grad_norm": 2.1222429275512695,
        "learning_rate": 0.0001836910061036345,
        "epoch": 0.18593287900218594,
        "step": 1446
    },
    {
        "loss": 1.8085,
        "grad_norm": 2.0657873153686523,
        "learning_rate": 0.00018366884618719188,
        "epoch": 0.18606146328918607,
        "step": 1447
    },
    {
        "loss": 2.4146,
        "grad_norm": 1.8642526865005493,
        "learning_rate": 0.00018364667256457517,
        "epoch": 0.18619004757618618,
        "step": 1448
    },
    {
        "loss": 1.9467,
        "grad_norm": 1.9702136516571045,
        "learning_rate": 0.00018362448523941675,
        "epoch": 0.1863186318631863,
        "step": 1449
    },
    {
        "loss": 2.467,
        "grad_norm": 1.2679367065429688,
        "learning_rate": 0.00018360228421535125,
        "epoch": 0.18644721615018645,
        "step": 1450
    },
    {
        "loss": 2.3664,
        "grad_norm": 1.403592586517334,
        "learning_rate": 0.00018358006949601555,
        "epoch": 0.18657580043718658,
        "step": 1451
    },
    {
        "loss": 1.6686,
        "grad_norm": 2.203199863433838,
        "learning_rate": 0.00018355784108504869,
        "epoch": 0.1867043847241867,
        "step": 1452
    },
    {
        "loss": 1.8415,
        "grad_norm": 1.634106993675232,
        "learning_rate": 0.000183535598986092,
        "epoch": 0.18683296901118684,
        "step": 1453
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.4739892482757568,
        "learning_rate": 0.0001835133432027891,
        "epoch": 0.18696155329818695,
        "step": 1454
    },
    {
        "loss": 1.3987,
        "grad_norm": 2.0448012351989746,
        "learning_rate": 0.0001834910737387858,
        "epoch": 0.18709013758518708,
        "step": 1455
    },
    {
        "loss": 2.2792,
        "grad_norm": 1.0187851190567017,
        "learning_rate": 0.00018346879059773016,
        "epoch": 0.18721872187218722,
        "step": 1456
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.4185954332351685,
        "learning_rate": 0.00018344649378327247,
        "epoch": 0.18734730615918735,
        "step": 1457
    },
    {
        "loss": 2.0996,
        "grad_norm": 1.375663161277771,
        "learning_rate": 0.00018342418329906528,
        "epoch": 0.18747589044618748,
        "step": 1458
    },
    {
        "loss": 2.169,
        "grad_norm": 1.440076231956482,
        "learning_rate": 0.00018340185914876336,
        "epoch": 0.18760447473318762,
        "step": 1459
    },
    {
        "loss": 1.5495,
        "grad_norm": 2.1979949474334717,
        "learning_rate": 0.00018337952133602378,
        "epoch": 0.18773305902018772,
        "step": 1460
    },
    {
        "loss": 2.0152,
        "grad_norm": 1.5895644426345825,
        "learning_rate": 0.00018335716986450575,
        "epoch": 0.18786164330718785,
        "step": 1461
    },
    {
        "loss": 1.7508,
        "grad_norm": 1.8972781896591187,
        "learning_rate": 0.00018333480473787077,
        "epoch": 0.187990227594188,
        "step": 1462
    },
    {
        "loss": 1.9325,
        "grad_norm": 1.977212905883789,
        "learning_rate": 0.0001833124259597826,
        "epoch": 0.18811881188118812,
        "step": 1463
    },
    {
        "loss": 2.2704,
        "grad_norm": 2.2975735664367676,
        "learning_rate": 0.00018329003353390717,
        "epoch": 0.18824739616818825,
        "step": 1464
    },
    {
        "loss": 1.6653,
        "grad_norm": 1.598185658454895,
        "learning_rate": 0.00018326762746391273,
        "epoch": 0.18837598045518839,
        "step": 1465
    },
    {
        "loss": 1.3643,
        "grad_norm": 2.110149621963501,
        "learning_rate": 0.00018324520775346969,
        "epoch": 0.18850456474218852,
        "step": 1466
    },
    {
        "loss": 2.1457,
        "grad_norm": 2.055023670196533,
        "learning_rate": 0.00018322277440625072,
        "epoch": 0.18863314902918862,
        "step": 1467
    },
    {
        "loss": 1.5057,
        "grad_norm": 2.071737051010132,
        "learning_rate": 0.00018320032742593072,
        "epoch": 0.18876173331618876,
        "step": 1468
    },
    {
        "loss": 1.2274,
        "grad_norm": 2.2173235416412354,
        "learning_rate": 0.0001831778668161869,
        "epoch": 0.1888903176031889,
        "step": 1469
    },
    {
        "loss": 2.3845,
        "grad_norm": 1.5041764974594116,
        "learning_rate": 0.00018315539258069856,
        "epoch": 0.18901890189018902,
        "step": 1470
    },
    {
        "loss": 2.4719,
        "grad_norm": 2.0748984813690186,
        "learning_rate": 0.00018313290472314736,
        "epoch": 0.18914748617718916,
        "step": 1471
    },
    {
        "loss": 2.6136,
        "grad_norm": 1.0060361623764038,
        "learning_rate": 0.00018311040324721706,
        "epoch": 0.1892760704641893,
        "step": 1472
    },
    {
        "loss": 1.6923,
        "grad_norm": 2.0414812564849854,
        "learning_rate": 0.00018308788815659383,
        "epoch": 0.1894046547511894,
        "step": 1473
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.4204288721084595,
        "learning_rate": 0.0001830653594549659,
        "epoch": 0.18953323903818953,
        "step": 1474
    },
    {
        "loss": 2.6675,
        "grad_norm": 1.1419042348861694,
        "learning_rate": 0.00018304281714602382,
        "epoch": 0.18966182332518966,
        "step": 1475
    },
    {
        "loss": 2.1283,
        "grad_norm": 1.6754150390625,
        "learning_rate": 0.00018302026123346034,
        "epoch": 0.1897904076121898,
        "step": 1476
    },
    {
        "loss": 2.131,
        "grad_norm": 1.7393627166748047,
        "learning_rate": 0.00018299769172097044,
        "epoch": 0.18991899189918993,
        "step": 1477
    },
    {
        "loss": 2.0973,
        "grad_norm": 1.7041041851043701,
        "learning_rate": 0.0001829751086122514,
        "epoch": 0.19004757618619006,
        "step": 1478
    },
    {
        "loss": 1.4973,
        "grad_norm": 2.415469169616699,
        "learning_rate": 0.00018295251191100258,
        "epoch": 0.19017616047319016,
        "step": 1479
    },
    {
        "loss": 1.7634,
        "grad_norm": 1.798903465270996,
        "learning_rate": 0.0001829299016209257,
        "epoch": 0.1903047447601903,
        "step": 1480
    },
    {
        "loss": 2.1363,
        "grad_norm": 1.8591749668121338,
        "learning_rate": 0.00018290727774572462,
        "epoch": 0.19043332904719043,
        "step": 1481
    },
    {
        "loss": 1.546,
        "grad_norm": 2.597876787185669,
        "learning_rate": 0.00018288464028910545,
        "epoch": 0.19056191333419056,
        "step": 1482
    },
    {
        "loss": 1.9533,
        "grad_norm": 1.7823976278305054,
        "learning_rate": 0.00018286198925477658,
        "epoch": 0.1906904976211907,
        "step": 1483
    },
    {
        "loss": 2.4095,
        "grad_norm": 1.3135459423065186,
        "learning_rate": 0.00018283932464644855,
        "epoch": 0.19081908190819083,
        "step": 1484
    },
    {
        "loss": 2.0642,
        "grad_norm": 2.0317466259002686,
        "learning_rate": 0.00018281664646783416,
        "epoch": 0.19094766619519093,
        "step": 1485
    },
    {
        "loss": 1.9418,
        "grad_norm": 1.650878667831421,
        "learning_rate": 0.0001827939547226484,
        "epoch": 0.19107625048219107,
        "step": 1486
    },
    {
        "loss": 1.8585,
        "grad_norm": 1.4795228242874146,
        "learning_rate": 0.00018277124941460857,
        "epoch": 0.1912048347691912,
        "step": 1487
    },
    {
        "loss": 1.6157,
        "grad_norm": 2.0058674812316895,
        "learning_rate": 0.00018274853054743408,
        "epoch": 0.19133341905619133,
        "step": 1488
    },
    {
        "loss": 2.0009,
        "grad_norm": 1.938029408454895,
        "learning_rate": 0.0001827257981248466,
        "epoch": 0.19146200334319147,
        "step": 1489
    },
    {
        "loss": 1.826,
        "grad_norm": 1.4890530109405518,
        "learning_rate": 0.0001827030521505701,
        "epoch": 0.1915905876301916,
        "step": 1490
    },
    {
        "loss": 1.677,
        "grad_norm": 1.8633294105529785,
        "learning_rate": 0.00018268029262833062,
        "epoch": 0.19171917191719173,
        "step": 1491
    },
    {
        "loss": 2.3954,
        "grad_norm": 1.243143081665039,
        "learning_rate": 0.00018265751956185656,
        "epoch": 0.19184775620419184,
        "step": 1492
    },
    {
        "loss": 2.3574,
        "grad_norm": 1.8037269115447998,
        "learning_rate": 0.00018263473295487842,
        "epoch": 0.19197634049119197,
        "step": 1493
    },
    {
        "loss": 2.1487,
        "grad_norm": 1.9198249578475952,
        "learning_rate": 0.00018261193281112904,
        "epoch": 0.1921049247781921,
        "step": 1494
    },
    {
        "loss": 2.3102,
        "grad_norm": 1.2924026250839233,
        "learning_rate": 0.0001825891191343434,
        "epoch": 0.19223350906519224,
        "step": 1495
    },
    {
        "loss": 1.9805,
        "grad_norm": 2.1831541061401367,
        "learning_rate": 0.0001825662919282587,
        "epoch": 0.19236209335219237,
        "step": 1496
    },
    {
        "loss": 2.2538,
        "grad_norm": 1.5366058349609375,
        "learning_rate": 0.00018254345119661432,
        "epoch": 0.1924906776391925,
        "step": 1497
    },
    {
        "loss": 1.266,
        "grad_norm": 2.138134241104126,
        "learning_rate": 0.000182520596943152,
        "epoch": 0.1926192619261926,
        "step": 1498
    },
    {
        "loss": 1.9767,
        "grad_norm": 1.8078573942184448,
        "learning_rate": 0.00018249772917161553,
        "epoch": 0.19274784621319274,
        "step": 1499
    },
    {
        "loss": 2.2846,
        "grad_norm": 1.118224024772644,
        "learning_rate": 0.00018247484788575105,
        "epoch": 0.19287643050019287,
        "step": 1500
    },
    {
        "eval_loss": 2.0316555500030518,
        "eval_runtime": 28.3087,
        "eval_samples_per_second": 2.791,
        "eval_steps_per_second": 2.791,
        "epoch": 0.19287643050019287,
        "step": 1500
    },
    {
        "loss": 2.232,
        "grad_norm": 1.075398325920105,
        "learning_rate": 0.00018245195308930678,
        "epoch": 0.193005014787193,
        "step": 1501
    },
    {
        "loss": 1.9627,
        "grad_norm": 1.4684628248214722,
        "learning_rate": 0.0001824290447860332,
        "epoch": 0.19313359907419314,
        "step": 1502
    },
    {
        "loss": 1.8206,
        "grad_norm": 1.6184169054031372,
        "learning_rate": 0.00018240612297968313,
        "epoch": 0.19326218336119327,
        "step": 1503
    },
    {
        "loss": 2.4668,
        "grad_norm": 1.188726544380188,
        "learning_rate": 0.00018238318767401141,
        "epoch": 0.19339076764819338,
        "step": 1504
    },
    {
        "loss": 1.4783,
        "grad_norm": 1.7798835039138794,
        "learning_rate": 0.0001823602388727752,
        "epoch": 0.1935193519351935,
        "step": 1505
    },
    {
        "loss": 1.7285,
        "grad_norm": 1.7004175186157227,
        "learning_rate": 0.00018233727657973386,
        "epoch": 0.19364793622219365,
        "step": 1506
    },
    {
        "loss": 2.015,
        "grad_norm": 1.8265572786331177,
        "learning_rate": 0.00018231430079864892,
        "epoch": 0.19377652050919378,
        "step": 1507
    },
    {
        "loss": 1.9085,
        "grad_norm": 1.462386131286621,
        "learning_rate": 0.00018229131153328415,
        "epoch": 0.1939051047961939,
        "step": 1508
    },
    {
        "loss": 1.7108,
        "grad_norm": 1.7301204204559326,
        "learning_rate": 0.00018226830878740558,
        "epoch": 0.19403368908319404,
        "step": 1509
    },
    {
        "loss": 2.3259,
        "grad_norm": 1.618040680885315,
        "learning_rate": 0.0001822452925647813,
        "epoch": 0.19416227337019415,
        "step": 1510
    },
    {
        "loss": 1.9665,
        "grad_norm": 1.4037219285964966,
        "learning_rate": 0.00018222226286918178,
        "epoch": 0.19429085765719428,
        "step": 1511
    },
    {
        "loss": 2.1468,
        "grad_norm": 2.2049944400787354,
        "learning_rate": 0.00018219921970437958,
        "epoch": 0.19441944194419442,
        "step": 1512
    },
    {
        "loss": 2.0664,
        "grad_norm": 1.5436503887176514,
        "learning_rate": 0.00018217616307414955,
        "epoch": 0.19454802623119455,
        "step": 1513
    },
    {
        "loss": 2.0391,
        "grad_norm": 1.6283795833587646,
        "learning_rate": 0.00018215309298226867,
        "epoch": 0.19467661051819468,
        "step": 1514
    },
    {
        "loss": 1.782,
        "grad_norm": 1.8193608522415161,
        "learning_rate": 0.00018213000943251612,
        "epoch": 0.19480519480519481,
        "step": 1515
    },
    {
        "loss": 2.2769,
        "grad_norm": 1.5221836566925049,
        "learning_rate": 0.00018210691242867338,
        "epoch": 0.19493377909219495,
        "step": 1516
    },
    {
        "loss": 1.7628,
        "grad_norm": 1.8553385734558105,
        "learning_rate": 0.00018208380197452407,
        "epoch": 0.19506236337919505,
        "step": 1517
    },
    {
        "loss": 2.4401,
        "grad_norm": 1.2536566257476807,
        "learning_rate": 0.00018206067807385398,
        "epoch": 0.19519094766619519,
        "step": 1518
    },
    {
        "loss": 2.7352,
        "grad_norm": 1.1054306030273438,
        "learning_rate": 0.00018203754073045116,
        "epoch": 0.19531953195319532,
        "step": 1519
    },
    {
        "loss": 2.5931,
        "grad_norm": 1.4825242757797241,
        "learning_rate": 0.00018201438994810585,
        "epoch": 0.19544811624019545,
        "step": 1520
    },
    {
        "loss": 2.1837,
        "grad_norm": 1.778409719467163,
        "learning_rate": 0.0001819912257306105,
        "epoch": 0.19557670052719558,
        "step": 1521
    },
    {
        "loss": 2.3709,
        "grad_norm": 1.4457008838653564,
        "learning_rate": 0.0001819680480817597,
        "epoch": 0.19570528481419572,
        "step": 1522
    },
    {
        "loss": 2.6161,
        "grad_norm": 1.2551785707473755,
        "learning_rate": 0.00018194485700535033,
        "epoch": 0.19583386910119582,
        "step": 1523
    },
    {
        "loss": 2.1047,
        "grad_norm": 1.6047604084014893,
        "learning_rate": 0.0001819216525051814,
        "epoch": 0.19596245338819596,
        "step": 1524
    },
    {
        "loss": 2.5864,
        "grad_norm": 1.630104422569275,
        "learning_rate": 0.00018189843458505417,
        "epoch": 0.1960910376751961,
        "step": 1525
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.3293933868408203,
        "learning_rate": 0.00018187520324877205,
        "epoch": 0.19621962196219622,
        "step": 1526
    },
    {
        "loss": 1.6811,
        "grad_norm": 2.3821473121643066,
        "learning_rate": 0.00018185195850014068,
        "epoch": 0.19634820624919636,
        "step": 1527
    },
    {
        "loss": 2.5905,
        "grad_norm": 1.3942270278930664,
        "learning_rate": 0.0001818287003429679,
        "epoch": 0.1964767905361965,
        "step": 1528
    },
    {
        "loss": 2.1779,
        "grad_norm": 1.5286833047866821,
        "learning_rate": 0.00018180542878106369,
        "epoch": 0.1966053748231966,
        "step": 1529
    },
    {
        "loss": 2.155,
        "grad_norm": 1.8921356201171875,
        "learning_rate": 0.00018178214381824032,
        "epoch": 0.19673395911019673,
        "step": 1530
    },
    {
        "loss": 1.9707,
        "grad_norm": 1.9714583158493042,
        "learning_rate": 0.00018175884545831218,
        "epoch": 0.19686254339719686,
        "step": 1531
    },
    {
        "loss": 2.0708,
        "grad_norm": 2.028151035308838,
        "learning_rate": 0.00018173553370509589,
        "epoch": 0.196991127684197,
        "step": 1532
    },
    {
        "loss": 2.751,
        "grad_norm": 1.3061364889144897,
        "learning_rate": 0.00018171220856241026,
        "epoch": 0.19711971197119713,
        "step": 1533
    },
    {
        "loss": 1.4333,
        "grad_norm": 1.6674376726150513,
        "learning_rate": 0.00018168887003407625,
        "epoch": 0.19724829625819726,
        "step": 1534
    },
    {
        "loss": 2.0718,
        "grad_norm": 1.8567934036254883,
        "learning_rate": 0.00018166551812391713,
        "epoch": 0.19737688054519736,
        "step": 1535
    },
    {
        "loss": 2.3736,
        "grad_norm": 1.8962136507034302,
        "learning_rate": 0.00018164215283575816,
        "epoch": 0.1975054648321975,
        "step": 1536
    },
    {
        "loss": 2.1845,
        "grad_norm": 1.7260596752166748,
        "learning_rate": 0.00018161877417342703,
        "epoch": 0.19763404911919763,
        "step": 1537
    },
    {
        "loss": 1.495,
        "grad_norm": 1.5601704120635986,
        "learning_rate": 0.00018159538214075345,
        "epoch": 0.19776263340619776,
        "step": 1538
    },
    {
        "loss": 2.0453,
        "grad_norm": 1.8407800197601318,
        "learning_rate": 0.00018157197674156938,
        "epoch": 0.1978912176931979,
        "step": 1539
    },
    {
        "loss": 1.4811,
        "grad_norm": 1.7721197605133057,
        "learning_rate": 0.00018154855797970898,
        "epoch": 0.19801980198019803,
        "step": 1540
    },
    {
        "loss": 1.5268,
        "grad_norm": 2.1156702041625977,
        "learning_rate": 0.00018152512585900858,
        "epoch": 0.19814838626719816,
        "step": 1541
    },
    {
        "loss": 1.2332,
        "grad_norm": 1.7784512042999268,
        "learning_rate": 0.00018150168038330667,
        "epoch": 0.19827697055419827,
        "step": 1542
    },
    {
        "loss": 2.1305,
        "grad_norm": 2.405855178833008,
        "learning_rate": 0.00018147822155644403,
        "epoch": 0.1984055548411984,
        "step": 1543
    },
    {
        "loss": 2.0889,
        "grad_norm": 2.3884992599487305,
        "learning_rate": 0.0001814547493822635,
        "epoch": 0.19853413912819853,
        "step": 1544
    },
    {
        "loss": 1.8136,
        "grad_norm": 1.8917089700698853,
        "learning_rate": 0.00018143126386461017,
        "epoch": 0.19866272341519867,
        "step": 1545
    },
    {
        "loss": 2.1169,
        "grad_norm": 1.3868961334228516,
        "learning_rate": 0.00018140776500733132,
        "epoch": 0.1987913077021988,
        "step": 1546
    },
    {
        "loss": 1.3945,
        "grad_norm": 2.6752846240997314,
        "learning_rate": 0.0001813842528142764,
        "epoch": 0.19891989198919893,
        "step": 1547
    },
    {
        "loss": 2.2495,
        "grad_norm": 1.8521366119384766,
        "learning_rate": 0.00018136072728929702,
        "epoch": 0.19904847627619904,
        "step": 1548
    },
    {
        "loss": 1.4277,
        "grad_norm": 1.324973702430725,
        "learning_rate": 0.00018133718843624705,
        "epoch": 0.19917706056319917,
        "step": 1549
    },
    {
        "loss": 2.1872,
        "grad_norm": 2.1720938682556152,
        "learning_rate": 0.0001813136362589825,
        "epoch": 0.1993056448501993,
        "step": 1550
    },
    {
        "loss": 2.1961,
        "grad_norm": 1.4154760837554932,
        "learning_rate": 0.00018129007076136153,
        "epoch": 0.19943422913719944,
        "step": 1551
    },
    {
        "loss": 1.9558,
        "grad_norm": 1.8649790287017822,
        "learning_rate": 0.00018126649194724452,
        "epoch": 0.19956281342419957,
        "step": 1552
    },
    {
        "loss": 1.7867,
        "grad_norm": 1.7093671560287476,
        "learning_rate": 0.00018124289982049402,
        "epoch": 0.1996913977111997,
        "step": 1553
    },
    {
        "loss": 2.3627,
        "grad_norm": 2.0321481227874756,
        "learning_rate": 0.00018121929438497477,
        "epoch": 0.1998199819981998,
        "step": 1554
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.522932529449463,
        "learning_rate": 0.0001811956756445537,
        "epoch": 0.19994856628519994,
        "step": 1555
    },
    {
        "loss": 2.2112,
        "grad_norm": 1.4782545566558838,
        "learning_rate": 0.00018117204360309987,
        "epoch": 0.20007715057220007,
        "step": 1556
    },
    {
        "loss": 2.0339,
        "grad_norm": 1.681634545326233,
        "learning_rate": 0.00018114839826448455,
        "epoch": 0.2002057348592002,
        "step": 1557
    },
    {
        "loss": 1.6441,
        "grad_norm": 1.9892019033432007,
        "learning_rate": 0.0001811247396325812,
        "epoch": 0.20033431914620034,
        "step": 1558
    },
    {
        "loss": 1.8861,
        "grad_norm": 2.1480965614318848,
        "learning_rate": 0.0001811010677112655,
        "epoch": 0.20046290343320047,
        "step": 1559
    },
    {
        "loss": 1.9704,
        "grad_norm": 2.048724412918091,
        "learning_rate": 0.00018107738250441516,
        "epoch": 0.20059148772020058,
        "step": 1560
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.7992955446243286,
        "learning_rate": 0.00018105368401591024,
        "epoch": 0.2007200720072007,
        "step": 1561
    },
    {
        "loss": 2.146,
        "grad_norm": 1.7933573722839355,
        "learning_rate": 0.00018102997224963288,
        "epoch": 0.20084865629420084,
        "step": 1562
    },
    {
        "loss": 2.0167,
        "grad_norm": 1.604103922843933,
        "learning_rate": 0.00018100624720946736,
        "epoch": 0.20097724058120098,
        "step": 1563
    },
    {
        "loss": 1.7794,
        "grad_norm": 2.072201728820801,
        "learning_rate": 0.00018098250889930025,
        "epoch": 0.2011058248682011,
        "step": 1564
    },
    {
        "loss": 1.4851,
        "grad_norm": 1.7239692211151123,
        "learning_rate": 0.0001809587573230202,
        "epoch": 0.20123440915520124,
        "step": 1565
    },
    {
        "loss": 1.7083,
        "grad_norm": 1.9322272539138794,
        "learning_rate": 0.00018093499248451807,
        "epoch": 0.20136299344220138,
        "step": 1566
    },
    {
        "loss": 2.0855,
        "grad_norm": 2.8616626262664795,
        "learning_rate": 0.0001809112143876869,
        "epoch": 0.20149157772920148,
        "step": 1567
    },
    {
        "loss": 2.2228,
        "grad_norm": 1.8052772283554077,
        "learning_rate": 0.00018088742303642188,
        "epoch": 0.20162016201620162,
        "step": 1568
    },
    {
        "loss": 2.3616,
        "grad_norm": 2.029336929321289,
        "learning_rate": 0.00018086361843462037,
        "epoch": 0.20174874630320175,
        "step": 1569
    },
    {
        "loss": 1.3893,
        "grad_norm": 2.0088040828704834,
        "learning_rate": 0.0001808398005861819,
        "epoch": 0.20187733059020188,
        "step": 1570
    },
    {
        "loss": 2.3252,
        "grad_norm": 2.2226083278656006,
        "learning_rate": 0.0001808159694950082,
        "epoch": 0.20200591487720201,
        "step": 1571
    },
    {
        "loss": 1.8497,
        "grad_norm": 2.074700117111206,
        "learning_rate": 0.00018079212516500316,
        "epoch": 0.20213449916420215,
        "step": 1572
    },
    {
        "loss": 1.2966,
        "grad_norm": 2.038104772567749,
        "learning_rate": 0.00018076826760007279,
        "epoch": 0.20226308345120225,
        "step": 1573
    },
    {
        "loss": 2.5848,
        "grad_norm": 1.3395098447799683,
        "learning_rate": 0.00018074439680412535,
        "epoch": 0.20239166773820239,
        "step": 1574
    },
    {
        "loss": 1.5764,
        "grad_norm": 1.487459421157837,
        "learning_rate": 0.00018072051278107118,
        "epoch": 0.20252025202520252,
        "step": 1575
    },
    {
        "loss": 2.2316,
        "grad_norm": 2.278146505355835,
        "learning_rate": 0.00018069661553482287,
        "epoch": 0.20264883631220265,
        "step": 1576
    },
    {
        "loss": 2.3182,
        "grad_norm": 1.5565283298492432,
        "learning_rate": 0.00018067270506929512,
        "epoch": 0.20277742059920278,
        "step": 1577
    },
    {
        "loss": 2.1166,
        "grad_norm": 2.382474422454834,
        "learning_rate": 0.0001806487813884048,
        "epoch": 0.20290600488620292,
        "step": 1578
    },
    {
        "loss": 1.1075,
        "grad_norm": 1.9387000799179077,
        "learning_rate": 0.00018062484449607097,
        "epoch": 0.20303458917320302,
        "step": 1579
    },
    {
        "loss": 2.37,
        "grad_norm": 1.6070232391357422,
        "learning_rate": 0.00018060089439621483,
        "epoch": 0.20316317346020316,
        "step": 1580
    },
    {
        "loss": 1.1371,
        "grad_norm": 2.0024845600128174,
        "learning_rate": 0.0001805769310927598,
        "epoch": 0.2032917577472033,
        "step": 1581
    },
    {
        "loss": 1.8859,
        "grad_norm": 1.4116249084472656,
        "learning_rate": 0.00018055295458963136,
        "epoch": 0.20342034203420342,
        "step": 1582
    },
    {
        "loss": 1.2692,
        "grad_norm": 2.013697624206543,
        "learning_rate": 0.00018052896489075722,
        "epoch": 0.20354892632120355,
        "step": 1583
    },
    {
        "loss": 1.9939,
        "grad_norm": 2.4809131622314453,
        "learning_rate": 0.00018050496200006726,
        "epoch": 0.2036775106082037,
        "step": 1584
    },
    {
        "loss": 2.0457,
        "grad_norm": 1.5429251194000244,
        "learning_rate": 0.0001804809459214935,
        "epoch": 0.2038060948952038,
        "step": 1585
    },
    {
        "loss": 1.7098,
        "grad_norm": 2.5319952964782715,
        "learning_rate": 0.00018045691665897012,
        "epoch": 0.20393467918220393,
        "step": 1586
    },
    {
        "loss": 2.3627,
        "grad_norm": 1.0637965202331543,
        "learning_rate": 0.0001804328742164335,
        "epoch": 0.20406326346920406,
        "step": 1587
    },
    {
        "loss": 1.2243,
        "grad_norm": 2.084689140319824,
        "learning_rate": 0.00018040881859782203,
        "epoch": 0.2041918477562042,
        "step": 1588
    },
    {
        "loss": 1.2988,
        "grad_norm": 1.8339216709136963,
        "learning_rate": 0.0001803847498070765,
        "epoch": 0.20432043204320433,
        "step": 1589
    },
    {
        "loss": 1.2747,
        "grad_norm": 1.415131688117981,
        "learning_rate": 0.00018036066784813963,
        "epoch": 0.20444901633020446,
        "step": 1590
    },
    {
        "loss": 2.4995,
        "grad_norm": 1.4809269905090332,
        "learning_rate": 0.00018033657272495645,
        "epoch": 0.2045776006172046,
        "step": 1591
    },
    {
        "loss": 1.7114,
        "grad_norm": 1.994909644126892,
        "learning_rate": 0.00018031246444147412,
        "epoch": 0.2047061849042047,
        "step": 1592
    },
    {
        "loss": 0.867,
        "grad_norm": 1.769544005393982,
        "learning_rate": 0.00018028834300164184,
        "epoch": 0.20483476919120483,
        "step": 1593
    },
    {
        "loss": 2.0536,
        "grad_norm": 2.2806129455566406,
        "learning_rate": 0.0001802642084094111,
        "epoch": 0.20496335347820496,
        "step": 1594
    },
    {
        "loss": 1.9607,
        "grad_norm": 2.4809157848358154,
        "learning_rate": 0.00018024006066873552,
        "epoch": 0.2050919377652051,
        "step": 1595
    },
    {
        "loss": 1.8571,
        "grad_norm": 2.521273612976074,
        "learning_rate": 0.00018021589978357082,
        "epoch": 0.20522052205220523,
        "step": 1596
    },
    {
        "loss": 2.1733,
        "grad_norm": 1.9879800081253052,
        "learning_rate": 0.0001801917257578749,
        "epoch": 0.20534910633920536,
        "step": 1597
    },
    {
        "loss": 2.2256,
        "grad_norm": 4.773677825927734,
        "learning_rate": 0.00018016753859560782,
        "epoch": 0.20547769062620547,
        "step": 1598
    },
    {
        "loss": 2.0825,
        "grad_norm": 2.3343005180358887,
        "learning_rate": 0.00018014333830073182,
        "epoch": 0.2056062749132056,
        "step": 1599
    },
    {
        "loss": 2.1798,
        "grad_norm": 1.855865478515625,
        "learning_rate": 0.00018011912487721126,
        "epoch": 0.20573485920020573,
        "step": 1600
    },
    {
        "eval_loss": 2.022305727005005,
        "eval_runtime": 28.1153,
        "eval_samples_per_second": 2.81,
        "eval_steps_per_second": 2.81,
        "epoch": 0.20573485920020573,
        "step": 1600
    },
    {
        "loss": 2.2657,
        "grad_norm": 1.4767332077026367,
        "learning_rate": 0.00018009489832901258,
        "epoch": 0.20586344348720587,
        "step": 1601
    },
    {
        "loss": 1.8332,
        "grad_norm": 1.9259737730026245,
        "learning_rate": 0.00018007065866010451,
        "epoch": 0.205992027774206,
        "step": 1602
    },
    {
        "loss": 1.9321,
        "grad_norm": 1.9918522834777832,
        "learning_rate": 0.00018004640587445785,
        "epoch": 0.20612061206120613,
        "step": 1603
    },
    {
        "loss": 1.9754,
        "grad_norm": 1.4014508724212646,
        "learning_rate": 0.00018002213997604554,
        "epoch": 0.20624919634820624,
        "step": 1604
    },
    {
        "loss": 2.0741,
        "grad_norm": 1.9759819507598877,
        "learning_rate": 0.00017999786096884268,
        "epoch": 0.20637778063520637,
        "step": 1605
    },
    {
        "loss": 2.5054,
        "grad_norm": 1.5532011985778809,
        "learning_rate": 0.00017997356885682657,
        "epoch": 0.2065063649222065,
        "step": 1606
    },
    {
        "loss": 2.3388,
        "grad_norm": 1.6302721500396729,
        "learning_rate": 0.00017994926364397657,
        "epoch": 0.20663494920920664,
        "step": 1607
    },
    {
        "loss": 1.7227,
        "grad_norm": 2.1995954513549805,
        "learning_rate": 0.00017992494533427425,
        "epoch": 0.20676353349620677,
        "step": 1608
    },
    {
        "loss": 1.4042,
        "grad_norm": 1.4364378452301025,
        "learning_rate": 0.00017990061393170328,
        "epoch": 0.2068921177832069,
        "step": 1609
    },
    {
        "loss": 1.4904,
        "grad_norm": 2.773914098739624,
        "learning_rate": 0.0001798762694402495,
        "epoch": 0.207020702070207,
        "step": 1610
    },
    {
        "loss": 2.0431,
        "grad_norm": 1.9659852981567383,
        "learning_rate": 0.00017985191186390093,
        "epoch": 0.20714928635720714,
        "step": 1611
    },
    {
        "loss": 1.1643,
        "grad_norm": 2.1577768325805664,
        "learning_rate": 0.00017982754120664764,
        "epoch": 0.20727787064420727,
        "step": 1612
    },
    {
        "loss": 2.382,
        "grad_norm": 1.5464133024215698,
        "learning_rate": 0.00017980315747248197,
        "epoch": 0.2074064549312074,
        "step": 1613
    },
    {
        "loss": 2.1449,
        "grad_norm": 2.1742334365844727,
        "learning_rate": 0.00017977876066539822,
        "epoch": 0.20753503921820754,
        "step": 1614
    },
    {
        "loss": 1.9867,
        "grad_norm": 1.0668410062789917,
        "learning_rate": 0.00017975435078939307,
        "epoch": 0.20766362350520767,
        "step": 1615
    },
    {
        "loss": 1.9164,
        "grad_norm": 1.4853874444961548,
        "learning_rate": 0.0001797299278484651,
        "epoch": 0.2077922077922078,
        "step": 1616
    },
    {
        "loss": 2.2618,
        "grad_norm": 1.7783695459365845,
        "learning_rate": 0.00017970549184661522,
        "epoch": 0.2079207920792079,
        "step": 1617
    },
    {
        "loss": 1.8963,
        "grad_norm": 2.280182361602783,
        "learning_rate": 0.0001796810427878464,
        "epoch": 0.20804937636620804,
        "step": 1618
    },
    {
        "loss": 1.5973,
        "grad_norm": 2.8454487323760986,
        "learning_rate": 0.0001796565806761637,
        "epoch": 0.20817796065320818,
        "step": 1619
    },
    {
        "loss": 1.7534,
        "grad_norm": 1.3073842525482178,
        "learning_rate": 0.00017963210551557438,
        "epoch": 0.2083065449402083,
        "step": 1620
    },
    {
        "loss": 1.7991,
        "grad_norm": 1.9262208938598633,
        "learning_rate": 0.00017960761731008787,
        "epoch": 0.20843512922720844,
        "step": 1621
    },
    {
        "loss": 1.7356,
        "grad_norm": 1.9072940349578857,
        "learning_rate": 0.00017958311606371566,
        "epoch": 0.20856371351420858,
        "step": 1622
    },
    {
        "loss": 2.4008,
        "grad_norm": 1.2233995199203491,
        "learning_rate": 0.00017955860178047145,
        "epoch": 0.20869229780120868,
        "step": 1623
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.585164189338684,
        "learning_rate": 0.000179534074464371,
        "epoch": 0.20882088208820881,
        "step": 1624
    },
    {
        "loss": 1.9405,
        "grad_norm": 1.8803298473358154,
        "learning_rate": 0.0001795095341194322,
        "epoch": 0.20894946637520895,
        "step": 1625
    },
    {
        "loss": 2.1572,
        "grad_norm": 1.2107720375061035,
        "learning_rate": 0.0001794849807496752,
        "epoch": 0.20907805066220908,
        "step": 1626
    },
    {
        "loss": 1.9376,
        "grad_norm": 1.333456039428711,
        "learning_rate": 0.00017946041435912212,
        "epoch": 0.2092066349492092,
        "step": 1627
    },
    {
        "loss": 2.3619,
        "grad_norm": 1.3734370470046997,
        "learning_rate": 0.00017943583495179738,
        "epoch": 0.20933521923620935,
        "step": 1628
    },
    {
        "loss": 1.3862,
        "grad_norm": 1.5833591222763062,
        "learning_rate": 0.00017941124253172737,
        "epoch": 0.20946380352320945,
        "step": 1629
    },
    {
        "loss": 2.5079,
        "grad_norm": 1.9024379253387451,
        "learning_rate": 0.00017938663710294074,
        "epoch": 0.20959238781020958,
        "step": 1630
    },
    {
        "loss": 2.3455,
        "grad_norm": 1.4396334886550903,
        "learning_rate": 0.00017936201866946817,
        "epoch": 0.20972097209720972,
        "step": 1631
    },
    {
        "loss": 2.4413,
        "grad_norm": 1.3902626037597656,
        "learning_rate": 0.00017933738723534257,
        "epoch": 0.20984955638420985,
        "step": 1632
    },
    {
        "loss": 2.4576,
        "grad_norm": 1.4939168691635132,
        "learning_rate": 0.00017931274280459886,
        "epoch": 0.20997814067120998,
        "step": 1633
    },
    {
        "loss": 1.4333,
        "grad_norm": 1.8670071363449097,
        "learning_rate": 0.00017928808538127423,
        "epoch": 0.21010672495821012,
        "step": 1634
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.4236820936203003,
        "learning_rate": 0.00017926341496940783,
        "epoch": 0.21023530924521022,
        "step": 1635
    },
    {
        "loss": 1.4738,
        "grad_norm": 2.059337854385376,
        "learning_rate": 0.00017923873157304115,
        "epoch": 0.21036389353221036,
        "step": 1636
    },
    {
        "loss": 2.4938,
        "grad_norm": 1.2903416156768799,
        "learning_rate": 0.00017921403519621757,
        "epoch": 0.2104924778192105,
        "step": 1637
    },
    {
        "loss": 2.2656,
        "grad_norm": 1.6499348878860474,
        "learning_rate": 0.0001791893258429828,
        "epoch": 0.21062106210621062,
        "step": 1638
    },
    {
        "loss": 1.8921,
        "grad_norm": 1.3994675874710083,
        "learning_rate": 0.00017916460351738458,
        "epoch": 0.21074964639321075,
        "step": 1639
    },
    {
        "loss": 1.9796,
        "grad_norm": 1.8809558153152466,
        "learning_rate": 0.00017913986822347278,
        "epoch": 0.2108782306802109,
        "step": 1640
    },
    {
        "loss": 1.7363,
        "grad_norm": 1.8822758197784424,
        "learning_rate": 0.00017911511996529937,
        "epoch": 0.21100681496721102,
        "step": 1641
    },
    {
        "loss": 2.4568,
        "grad_norm": 1.362099051475525,
        "learning_rate": 0.0001790903587469185,
        "epoch": 0.21113539925421113,
        "step": 1642
    },
    {
        "loss": 2.2864,
        "grad_norm": 1.4389008283615112,
        "learning_rate": 0.00017906558457238644,
        "epoch": 0.21126398354121126,
        "step": 1643
    },
    {
        "loss": 1.8349,
        "grad_norm": 2.2215778827667236,
        "learning_rate": 0.0001790407974457615,
        "epoch": 0.2113925678282114,
        "step": 1644
    },
    {
        "loss": 1.2536,
        "grad_norm": 1.9254440069198608,
        "learning_rate": 0.00017901599737110422,
        "epoch": 0.21152115211521152,
        "step": 1645
    },
    {
        "loss": 2.1503,
        "grad_norm": 1.752668857574463,
        "learning_rate": 0.00017899118435247722,
        "epoch": 0.21164973640221166,
        "step": 1646
    },
    {
        "loss": 1.774,
        "grad_norm": 1.9675217866897583,
        "learning_rate": 0.0001789663583939452,
        "epoch": 0.2117783206892118,
        "step": 1647
    },
    {
        "loss": 1.0146,
        "grad_norm": 2.0993072986602783,
        "learning_rate": 0.00017894151949957504,
        "epoch": 0.2119069049762119,
        "step": 1648
    },
    {
        "loss": 2.2745,
        "grad_norm": 1.5066975355148315,
        "learning_rate": 0.00017891666767343566,
        "epoch": 0.21203548926321203,
        "step": 1649
    },
    {
        "loss": 2.3941,
        "grad_norm": 2.2781355381011963,
        "learning_rate": 0.00017889180291959822,
        "epoch": 0.21216407355021216,
        "step": 1650
    },
    {
        "loss": 2.3632,
        "grad_norm": 1.1374820470809937,
        "learning_rate": 0.00017886692524213588,
        "epoch": 0.2122926578372123,
        "step": 1651
    },
    {
        "loss": 2.1488,
        "grad_norm": 1.1424150466918945,
        "learning_rate": 0.000178842034645124,
        "epoch": 0.21242124212421243,
        "step": 1652
    },
    {
        "loss": 1.3038,
        "grad_norm": 1.7311162948608398,
        "learning_rate": 0.00017881713113264,
        "epoch": 0.21254982641121256,
        "step": 1653
    },
    {
        "loss": 1.5404,
        "grad_norm": 1.7531152963638306,
        "learning_rate": 0.00017879221470876346,
        "epoch": 0.21267841069821267,
        "step": 1654
    },
    {
        "loss": 1.8741,
        "grad_norm": 1.8792102336883545,
        "learning_rate": 0.000178767285377576,
        "epoch": 0.2128069949852128,
        "step": 1655
    },
    {
        "loss": 2.1214,
        "grad_norm": 2.0860700607299805,
        "learning_rate": 0.0001787423431431615,
        "epoch": 0.21293557927221293,
        "step": 1656
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.126845598220825,
        "learning_rate": 0.00017871738800960576,
        "epoch": 0.21306416355921307,
        "step": 1657
    },
    {
        "loss": 2.2984,
        "grad_norm": 2.096830129623413,
        "learning_rate": 0.00017869241998099685,
        "epoch": 0.2131927478462132,
        "step": 1658
    },
    {
        "loss": 2.2005,
        "grad_norm": 1.480905294418335,
        "learning_rate": 0.00017866743906142486,
        "epoch": 0.21332133213321333,
        "step": 1659
    },
    {
        "loss": 2.2179,
        "grad_norm": 1.3469362258911133,
        "learning_rate": 0.0001786424452549821,
        "epoch": 0.21344991642021344,
        "step": 1660
    },
    {
        "loss": 2.3904,
        "grad_norm": 1.938306212425232,
        "learning_rate": 0.00017861743856576286,
        "epoch": 0.21357850070721357,
        "step": 1661
    },
    {
        "loss": 1.8339,
        "grad_norm": 1.878732442855835,
        "learning_rate": 0.00017859241899786357,
        "epoch": 0.2137070849942137,
        "step": 1662
    },
    {
        "loss": 1.2793,
        "grad_norm": 1.9118857383728027,
        "learning_rate": 0.00017856738655538286,
        "epoch": 0.21383566928121384,
        "step": 1663
    },
    {
        "loss": 2.2207,
        "grad_norm": 1.442064642906189,
        "learning_rate": 0.0001785423412424214,
        "epoch": 0.21396425356821397,
        "step": 1664
    },
    {
        "loss": 1.4319,
        "grad_norm": 1.9098602533340454,
        "learning_rate": 0.00017851728306308195,
        "epoch": 0.2140928378552141,
        "step": 1665
    },
    {
        "loss": 2.2812,
        "grad_norm": 1.3759551048278809,
        "learning_rate": 0.00017849221202146943,
        "epoch": 0.21422142214221424,
        "step": 1666
    },
    {
        "loss": 1.4329,
        "grad_norm": 1.8118964433670044,
        "learning_rate": 0.00017846712812169085,
        "epoch": 0.21435000642921434,
        "step": 1667
    },
    {
        "loss": 2.002,
        "grad_norm": 1.553110957145691,
        "learning_rate": 0.00017844203136785524,
        "epoch": 0.21447859071621447,
        "step": 1668
    },
    {
        "loss": 2.2042,
        "grad_norm": 1.1762921810150146,
        "learning_rate": 0.00017841692176407393,
        "epoch": 0.2146071750032146,
        "step": 1669
    },
    {
        "loss": 2.0969,
        "grad_norm": 0.9432075619697571,
        "learning_rate": 0.00017839179931446015,
        "epoch": 0.21473575929021474,
        "step": 1670
    },
    {
        "loss": 2.2677,
        "grad_norm": 1.1683636903762817,
        "learning_rate": 0.00017836666402312935,
        "epoch": 0.21486434357721487,
        "step": 1671
    },
    {
        "loss": 1.9746,
        "grad_norm": 1.7500712871551514,
        "learning_rate": 0.00017834151589419907,
        "epoch": 0.214992927864215,
        "step": 1672
    },
    {
        "loss": 2.043,
        "grad_norm": 1.778479814529419,
        "learning_rate": 0.00017831635493178893,
        "epoch": 0.2151215121512151,
        "step": 1673
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.6430853605270386,
        "learning_rate": 0.00017829118114002064,
        "epoch": 0.21525009643821524,
        "step": 1674
    },
    {
        "loss": 1.4554,
        "grad_norm": 2.693526029586792,
        "learning_rate": 0.00017826599452301807,
        "epoch": 0.21537868072521538,
        "step": 1675
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.160800576210022,
        "learning_rate": 0.00017824079508490712,
        "epoch": 0.2155072650122155,
        "step": 1676
    },
    {
        "loss": 1.6111,
        "grad_norm": 1.7013096809387207,
        "learning_rate": 0.00017821558282981586,
        "epoch": 0.21563584929921564,
        "step": 1677
    },
    {
        "loss": 1.9708,
        "grad_norm": 1.6520625352859497,
        "learning_rate": 0.0001781903577618744,
        "epoch": 0.21576443358621578,
        "step": 1678
    },
    {
        "loss": 2.1146,
        "grad_norm": 1.9034748077392578,
        "learning_rate": 0.000178165119885215,
        "epoch": 0.21589301787321588,
        "step": 1679
    },
    {
        "loss": 2.014,
        "grad_norm": 1.3106287717819214,
        "learning_rate": 0.0001781398692039719,
        "epoch": 0.21602160216021601,
        "step": 1680
    },
    {
        "loss": 2.2737,
        "grad_norm": 1.9396910667419434,
        "learning_rate": 0.00017811460572228168,
        "epoch": 0.21615018644721615,
        "step": 1681
    },
    {
        "loss": 1.8575,
        "grad_norm": 2.6280264854431152,
        "learning_rate": 0.00017808932944428274,
        "epoch": 0.21627877073421628,
        "step": 1682
    },
    {
        "loss": 2.0781,
        "grad_norm": 1.665012240409851,
        "learning_rate": 0.00017806404037411577,
        "epoch": 0.2164073550212164,
        "step": 1683
    },
    {
        "loss": 1.5423,
        "grad_norm": 2.0554678440093994,
        "learning_rate": 0.00017803873851592346,
        "epoch": 0.21653593930821655,
        "step": 1684
    },
    {
        "loss": 1.7692,
        "grad_norm": 1.472273588180542,
        "learning_rate": 0.0001780134238738506,
        "epoch": 0.21666452359521665,
        "step": 1685
    },
    {
        "loss": 1.9148,
        "grad_norm": 2.1324737071990967,
        "learning_rate": 0.00017798809645204415,
        "epoch": 0.21679310788221678,
        "step": 1686
    },
    {
        "loss": 1.6976,
        "grad_norm": 2.161846160888672,
        "learning_rate": 0.00017796275625465307,
        "epoch": 0.21692169216921692,
        "step": 1687
    },
    {
        "loss": 1.8687,
        "grad_norm": 1.3001701831817627,
        "learning_rate": 0.00017793740328582848,
        "epoch": 0.21705027645621705,
        "step": 1688
    },
    {
        "loss": 0.9074,
        "grad_norm": 2.2440314292907715,
        "learning_rate": 0.00017791203754972354,
        "epoch": 0.21717886074321718,
        "step": 1689
    },
    {
        "loss": 1.8001,
        "grad_norm": 1.5694990158081055,
        "learning_rate": 0.00017788665905049352,
        "epoch": 0.21730744503021732,
        "step": 1690
    },
    {
        "loss": 1.0504,
        "grad_norm": 2.3002700805664062,
        "learning_rate": 0.00017786126779229585,
        "epoch": 0.21743602931721745,
        "step": 1691
    },
    {
        "loss": 2.7483,
        "grad_norm": 1.9433423280715942,
        "learning_rate": 0.00017783586377928992,
        "epoch": 0.21756461360421755,
        "step": 1692
    },
    {
        "loss": 1.6451,
        "grad_norm": 1.7258176803588867,
        "learning_rate": 0.0001778104470156373,
        "epoch": 0.2176931978912177,
        "step": 1693
    },
    {
        "loss": 2.1398,
        "grad_norm": 1.9420379400253296,
        "learning_rate": 0.0001777850175055016,
        "epoch": 0.21782178217821782,
        "step": 1694
    },
    {
        "loss": 1.8658,
        "grad_norm": 1.9134459495544434,
        "learning_rate": 0.00017775957525304858,
        "epoch": 0.21795036646521795,
        "step": 1695
    },
    {
        "loss": 1.9366,
        "grad_norm": 2.1732547283172607,
        "learning_rate": 0.00017773412026244608,
        "epoch": 0.2180789507522181,
        "step": 1696
    },
    {
        "loss": 1.7657,
        "grad_norm": 2.2635104656219482,
        "learning_rate": 0.0001777086525378639,
        "epoch": 0.21820753503921822,
        "step": 1697
    },
    {
        "loss": 1.3596,
        "grad_norm": 2.166821241378784,
        "learning_rate": 0.0001776831720834741,
        "epoch": 0.21833611932621833,
        "step": 1698
    },
    {
        "loss": 2.1114,
        "grad_norm": 1.9869896173477173,
        "learning_rate": 0.00017765767890345075,
        "epoch": 0.21846470361321846,
        "step": 1699
    },
    {
        "loss": 1.8253,
        "grad_norm": 2.0513148307800293,
        "learning_rate": 0.00017763217300196998,
        "epoch": 0.2185932879002186,
        "step": 1700
    },
    {
        "eval_loss": 2.011213779449463,
        "eval_runtime": 28.1831,
        "eval_samples_per_second": 2.803,
        "eval_steps_per_second": 2.803,
        "epoch": 0.2185932879002186,
        "step": 1700
    },
    {
        "loss": 2.6161,
        "grad_norm": 1.327316164970398,
        "learning_rate": 0.00017760665438321002,
        "epoch": 0.21872187218721872,
        "step": 1701
    },
    {
        "loss": 1.4574,
        "grad_norm": 1.6178717613220215,
        "learning_rate": 0.0001775811230513512,
        "epoch": 0.21885045647421886,
        "step": 1702
    },
    {
        "loss": 1.8121,
        "grad_norm": 1.5230604410171509,
        "learning_rate": 0.00017755557901057593,
        "epoch": 0.218979040761219,
        "step": 1703
    },
    {
        "loss": 1.7628,
        "grad_norm": 2.0311429500579834,
        "learning_rate": 0.00017753002226506868,
        "epoch": 0.2191076250482191,
        "step": 1704
    },
    {
        "loss": 1.6279,
        "grad_norm": 1.621240496635437,
        "learning_rate": 0.00017750445281901608,
        "epoch": 0.21923620933521923,
        "step": 1705
    },
    {
        "loss": 2.1841,
        "grad_norm": 1.737987995147705,
        "learning_rate": 0.00017747887067660667,
        "epoch": 0.21936479362221936,
        "step": 1706
    },
    {
        "loss": 2.0124,
        "grad_norm": 1.6175532341003418,
        "learning_rate": 0.00017745327584203126,
        "epoch": 0.2194933779092195,
        "step": 1707
    },
    {
        "loss": 2.3321,
        "grad_norm": 1.3546594381332397,
        "learning_rate": 0.00017742766831948262,
        "epoch": 0.21962196219621963,
        "step": 1708
    },
    {
        "loss": 1.8049,
        "grad_norm": 1.353764533996582,
        "learning_rate": 0.00017740204811315566,
        "epoch": 0.21975054648321976,
        "step": 1709
    },
    {
        "loss": 2.0253,
        "grad_norm": 1.6086980104446411,
        "learning_rate": 0.00017737641522724728,
        "epoch": 0.21987913077021987,
        "step": 1710
    },
    {
        "loss": 2.2832,
        "grad_norm": 1.66640305519104,
        "learning_rate": 0.0001773507696659566,
        "epoch": 0.22000771505722,
        "step": 1711
    },
    {
        "loss": 2.0901,
        "grad_norm": 2.0063962936401367,
        "learning_rate": 0.00017732511143348471,
        "epoch": 0.22013629934422013,
        "step": 1712
    },
    {
        "loss": 1.926,
        "grad_norm": 1.9056397676467896,
        "learning_rate": 0.00017729944053403477,
        "epoch": 0.22026488363122027,
        "step": 1713
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.9118578433990479,
        "learning_rate": 0.00017727375697181206,
        "epoch": 0.2203934679182204,
        "step": 1714
    },
    {
        "loss": 1.6388,
        "grad_norm": 1.6872040033340454,
        "learning_rate": 0.00017724806075102397,
        "epoch": 0.22052205220522053,
        "step": 1715
    },
    {
        "loss": 1.4301,
        "grad_norm": 2.539902448654175,
        "learning_rate": 0.00017722235187587985,
        "epoch": 0.22065063649222066,
        "step": 1716
    },
    {
        "loss": 1.8344,
        "grad_norm": 1.8991681337356567,
        "learning_rate": 0.0001771966303505912,
        "epoch": 0.22077922077922077,
        "step": 1717
    },
    {
        "loss": 1.221,
        "grad_norm": 1.5603585243225098,
        "learning_rate": 0.0001771708961793716,
        "epoch": 0.2209078050662209,
        "step": 1718
    },
    {
        "loss": 2.3749,
        "grad_norm": 1.7244691848754883,
        "learning_rate": 0.00017714514936643673,
        "epoch": 0.22103638935322104,
        "step": 1719
    },
    {
        "loss": 1.4772,
        "grad_norm": 2.1086010932922363,
        "learning_rate": 0.00017711938991600418,
        "epoch": 0.22116497364022117,
        "step": 1720
    },
    {
        "loss": 2.05,
        "grad_norm": 1.1472418308258057,
        "learning_rate": 0.0001770936178322938,
        "epoch": 0.2212935579272213,
        "step": 1721
    },
    {
        "loss": 1.6674,
        "grad_norm": 1.5436615943908691,
        "learning_rate": 0.0001770678331195274,
        "epoch": 0.22142214221422143,
        "step": 1722
    },
    {
        "loss": 1.8731,
        "grad_norm": 3.066676616668701,
        "learning_rate": 0.0001770420357819289,
        "epoch": 0.22155072650122154,
        "step": 1723
    },
    {
        "loss": 1.7769,
        "grad_norm": 2.630629539489746,
        "learning_rate": 0.0001770162258237243,
        "epoch": 0.22167931078822167,
        "step": 1724
    },
    {
        "loss": 1.4618,
        "grad_norm": 2.3262779712677,
        "learning_rate": 0.00017699040324914162,
        "epoch": 0.2218078950752218,
        "step": 1725
    },
    {
        "loss": 2.2654,
        "grad_norm": 1.6930454969406128,
        "learning_rate": 0.00017696456806241096,
        "epoch": 0.22193647936222194,
        "step": 1726
    },
    {
        "loss": 1.7364,
        "grad_norm": 1.983850121498108,
        "learning_rate": 0.00017693872026776456,
        "epoch": 0.22206506364922207,
        "step": 1727
    },
    {
        "loss": 1.7672,
        "grad_norm": 2.074432849884033,
        "learning_rate": 0.00017691285986943663,
        "epoch": 0.2221936479362222,
        "step": 1728
    },
    {
        "loss": 2.3794,
        "grad_norm": 1.2673733234405518,
        "learning_rate": 0.00017688698687166345,
        "epoch": 0.2223222322232223,
        "step": 1729
    },
    {
        "loss": 0.7901,
        "grad_norm": 1.7764437198638916,
        "learning_rate": 0.00017686110127868344,
        "epoch": 0.22245081651022244,
        "step": 1730
    },
    {
        "loss": 2.2684,
        "grad_norm": 2.76369047164917,
        "learning_rate": 0.000176835203094737,
        "epoch": 0.22257940079722258,
        "step": 1731
    },
    {
        "loss": 1.8633,
        "grad_norm": 1.960735559463501,
        "learning_rate": 0.00017680929232406664,
        "epoch": 0.2227079850842227,
        "step": 1732
    },
    {
        "loss": 1.1665,
        "grad_norm": 1.2498408555984497,
        "learning_rate": 0.00017678336897091693,
        "epoch": 0.22283656937122284,
        "step": 1733
    },
    {
        "loss": 2.1376,
        "grad_norm": 2.638389825820923,
        "learning_rate": 0.0001767574330395345,
        "epoch": 0.22296515365822298,
        "step": 1734
    },
    {
        "loss": 2.3263,
        "grad_norm": 2.0196404457092285,
        "learning_rate": 0.000176731484534168,
        "epoch": 0.22309373794522308,
        "step": 1735
    },
    {
        "loss": 1.9966,
        "grad_norm": 1.9002803564071655,
        "learning_rate": 0.0001767055234590682,
        "epoch": 0.2232223222322232,
        "step": 1736
    },
    {
        "loss": 1.5716,
        "grad_norm": 2.1873013973236084,
        "learning_rate": 0.00017667954981848792,
        "epoch": 0.22335090651922335,
        "step": 1737
    },
    {
        "loss": 2.3656,
        "grad_norm": 1.5416918992996216,
        "learning_rate": 0.00017665356361668198,
        "epoch": 0.22347949080622348,
        "step": 1738
    },
    {
        "loss": 2.0034,
        "grad_norm": 2.571526288986206,
        "learning_rate": 0.0001766275648579073,
        "epoch": 0.2236080750932236,
        "step": 1739
    },
    {
        "loss": 2.0179,
        "grad_norm": 1.8289024829864502,
        "learning_rate": 0.0001766015535464229,
        "epoch": 0.22373665938022375,
        "step": 1740
    },
    {
        "loss": 1.6607,
        "grad_norm": 1.5493017435073853,
        "learning_rate": 0.00017657552968648976,
        "epoch": 0.22386524366722388,
        "step": 1741
    },
    {
        "loss": 2.3217,
        "grad_norm": 2.2680718898773193,
        "learning_rate": 0.00017654949328237097,
        "epoch": 0.22399382795422398,
        "step": 1742
    },
    {
        "loss": 1.8934,
        "grad_norm": 1.804112434387207,
        "learning_rate": 0.00017652344433833172,
        "epoch": 0.22412241224122412,
        "step": 1743
    },
    {
        "loss": 2.347,
        "grad_norm": 1.4945124387741089,
        "learning_rate": 0.00017649738285863915,
        "epoch": 0.22425099652822425,
        "step": 1744
    },
    {
        "loss": 1.7188,
        "grad_norm": 2.2205662727355957,
        "learning_rate": 0.00017647130884756255,
        "epoch": 0.22437958081522438,
        "step": 1745
    },
    {
        "loss": 1.9865,
        "grad_norm": 1.4551280736923218,
        "learning_rate": 0.0001764452223093732,
        "epoch": 0.22450816510222452,
        "step": 1746
    },
    {
        "loss": 1.6691,
        "grad_norm": 1.3836040496826172,
        "learning_rate": 0.00017641912324834448,
        "epoch": 0.22463674938922465,
        "step": 1747
    },
    {
        "loss": 2.4878,
        "grad_norm": 0.9944306015968323,
        "learning_rate": 0.00017639301166875172,
        "epoch": 0.22476533367622475,
        "step": 1748
    },
    {
        "loss": 2.3463,
        "grad_norm": 1.3761436939239502,
        "learning_rate": 0.0001763668875748725,
        "epoch": 0.2248939179632249,
        "step": 1749
    },
    {
        "loss": 1.1411,
        "grad_norm": 1.4880423545837402,
        "learning_rate": 0.00017634075097098622,
        "epoch": 0.22502250225022502,
        "step": 1750
    },
    {
        "loss": 2.093,
        "grad_norm": 1.3834060430526733,
        "learning_rate": 0.0001763146018613745,
        "epoch": 0.22515108653722515,
        "step": 1751
    },
    {
        "loss": 1.84,
        "grad_norm": 1.7981516122817993,
        "learning_rate": 0.0001762884402503209,
        "epoch": 0.2252796708242253,
        "step": 1752
    },
    {
        "loss": 2.1437,
        "grad_norm": 1.5791784524917603,
        "learning_rate": 0.0001762622661421111,
        "epoch": 0.22540825511122542,
        "step": 1753
    },
    {
        "loss": 2.5308,
        "grad_norm": 1.5300757884979248,
        "learning_rate": 0.0001762360795410328,
        "epoch": 0.22553683939822552,
        "step": 1754
    },
    {
        "loss": 1.7076,
        "grad_norm": 1.4616289138793945,
        "learning_rate": 0.0001762098804513757,
        "epoch": 0.22566542368522566,
        "step": 1755
    },
    {
        "loss": 2.2128,
        "grad_norm": 1.1253308057785034,
        "learning_rate": 0.00017618366887743166,
        "epoch": 0.2257940079722258,
        "step": 1756
    },
    {
        "loss": 2.6058,
        "grad_norm": 1.7933967113494873,
        "learning_rate": 0.0001761574448234945,
        "epoch": 0.22592259225922592,
        "step": 1757
    },
    {
        "loss": 2.142,
        "grad_norm": 1.4896565675735474,
        "learning_rate": 0.00017613120829386008,
        "epoch": 0.22605117654622606,
        "step": 1758
    },
    {
        "loss": 2.482,
        "grad_norm": 1.8835842609405518,
        "learning_rate": 0.00017610495929282633,
        "epoch": 0.2261797608332262,
        "step": 1759
    },
    {
        "loss": 1.2117,
        "grad_norm": 2.1983864307403564,
        "learning_rate": 0.00017607869782469322,
        "epoch": 0.2263083451202263,
        "step": 1760
    },
    {
        "loss": 1.9563,
        "grad_norm": 1.8099180459976196,
        "learning_rate": 0.00017605242389376276,
        "epoch": 0.22643692940722643,
        "step": 1761
    },
    {
        "loss": 2.0679,
        "grad_norm": 1.4727357625961304,
        "learning_rate": 0.000176026137504339,
        "epoch": 0.22656551369422656,
        "step": 1762
    },
    {
        "loss": 1.9756,
        "grad_norm": 2.365709066390991,
        "learning_rate": 0.00017599983866072806,
        "epoch": 0.2266940979812267,
        "step": 1763
    },
    {
        "loss": 1.7878,
        "grad_norm": 1.6569911241531372,
        "learning_rate": 0.00017597352736723802,
        "epoch": 0.22682268226822683,
        "step": 1764
    },
    {
        "loss": 2.4949,
        "grad_norm": 1.8577157258987427,
        "learning_rate": 0.00017594720362817913,
        "epoch": 0.22695126655522696,
        "step": 1765
    },
    {
        "loss": 1.9214,
        "grad_norm": 2.0685031414031982,
        "learning_rate": 0.0001759208674478635,
        "epoch": 0.2270798508422271,
        "step": 1766
    },
    {
        "loss": 1.8101,
        "grad_norm": 1.448903203010559,
        "learning_rate": 0.00017589451883060547,
        "epoch": 0.2272084351292272,
        "step": 1767
    },
    {
        "loss": 1.4406,
        "grad_norm": 2.0176889896392822,
        "learning_rate": 0.00017586815778072125,
        "epoch": 0.22733701941622733,
        "step": 1768
    },
    {
        "loss": 1.6601,
        "grad_norm": 1.4164061546325684,
        "learning_rate": 0.00017584178430252924,
        "epoch": 0.22746560370322746,
        "step": 1769
    },
    {
        "loss": 1.3353,
        "grad_norm": 2.254438877105713,
        "learning_rate": 0.00017581539840034975,
        "epoch": 0.2275941879902276,
        "step": 1770
    },
    {
        "loss": 2.2505,
        "grad_norm": 1.5082695484161377,
        "learning_rate": 0.00017578900007850522,
        "epoch": 0.22772277227722773,
        "step": 1771
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.8078845739364624,
        "learning_rate": 0.00017576258934132,
        "epoch": 0.22785135656422786,
        "step": 1772
    },
    {
        "loss": 2.2212,
        "grad_norm": 1.8724251985549927,
        "learning_rate": 0.00017573616619312064,
        "epoch": 0.22797994085122797,
        "step": 1773
    },
    {
        "loss": 1.6769,
        "grad_norm": 2.2410082817077637,
        "learning_rate": 0.00017570973063823556,
        "epoch": 0.2281085251382281,
        "step": 1774
    },
    {
        "loss": 2.4278,
        "grad_norm": 1.5753272771835327,
        "learning_rate": 0.00017568328268099536,
        "epoch": 0.22823710942522824,
        "step": 1775
    },
    {
        "loss": 2.2445,
        "grad_norm": 1.246642827987671,
        "learning_rate": 0.00017565682232573255,
        "epoch": 0.22836569371222837,
        "step": 1776
    },
    {
        "loss": 2.0183,
        "grad_norm": 2.0687952041625977,
        "learning_rate": 0.00017563034957678172,
        "epoch": 0.2284942779992285,
        "step": 1777
    },
    {
        "loss": 2.2244,
        "grad_norm": 1.3020635843276978,
        "learning_rate": 0.00017560386443847954,
        "epoch": 0.22862286228622863,
        "step": 1778
    },
    {
        "loss": 1.9209,
        "grad_norm": 1.5744448900222778,
        "learning_rate": 0.0001755773669151646,
        "epoch": 0.22875144657322874,
        "step": 1779
    },
    {
        "loss": 1.6591,
        "grad_norm": 1.7314072847366333,
        "learning_rate": 0.00017555085701117764,
        "epoch": 0.22888003086022887,
        "step": 1780
    },
    {
        "loss": 2.195,
        "grad_norm": 1.058159351348877,
        "learning_rate": 0.00017552433473086128,
        "epoch": 0.229008615147229,
        "step": 1781
    },
    {
        "loss": 2.0074,
        "grad_norm": 1.6536921262741089,
        "learning_rate": 0.00017549780007856036,
        "epoch": 0.22913719943422914,
        "step": 1782
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.5344130992889404,
        "learning_rate": 0.00017547125305862157,
        "epoch": 0.22926578372122927,
        "step": 1783
    },
    {
        "loss": 2.3471,
        "grad_norm": 1.3527073860168457,
        "learning_rate": 0.00017544469367539373,
        "epoch": 0.2293943680082294,
        "step": 1784
    },
    {
        "loss": 1.8269,
        "grad_norm": 1.6011995077133179,
        "learning_rate": 0.00017541812193322764,
        "epoch": 0.2295229522952295,
        "step": 1785
    },
    {
        "loss": 2.1942,
        "grad_norm": 1.9115175008773804,
        "learning_rate": 0.00017539153783647615,
        "epoch": 0.22965153658222964,
        "step": 1786
    },
    {
        "loss": 1.6128,
        "grad_norm": 2.1116833686828613,
        "learning_rate": 0.00017536494138949412,
        "epoch": 0.22978012086922978,
        "step": 1787
    },
    {
        "loss": 1.0467,
        "grad_norm": 3.6002984046936035,
        "learning_rate": 0.00017533833259663842,
        "epoch": 0.2299087051562299,
        "step": 1788
    },
    {
        "loss": 1.594,
        "grad_norm": 1.9431971311569214,
        "learning_rate": 0.00017531171146226794,
        "epoch": 0.23003728944323004,
        "step": 1789
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.580387830734253,
        "learning_rate": 0.0001752850779907437,
        "epoch": 0.23016587373023017,
        "step": 1790
    },
    {
        "loss": 1.7326,
        "grad_norm": 1.869972825050354,
        "learning_rate": 0.0001752584321864285,
        "epoch": 0.2302944580172303,
        "step": 1791
    },
    {
        "loss": 2.0254,
        "grad_norm": 1.9183063507080078,
        "learning_rate": 0.00017523177405368747,
        "epoch": 0.2304230423042304,
        "step": 1792
    },
    {
        "loss": 1.9625,
        "grad_norm": 2.45832896232605,
        "learning_rate": 0.0001752051035968875,
        "epoch": 0.23055162659123055,
        "step": 1793
    },
    {
        "loss": 2.0901,
        "grad_norm": 2.0860965251922607,
        "learning_rate": 0.0001751784208203976,
        "epoch": 0.23068021087823068,
        "step": 1794
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.7484586238861084,
        "learning_rate": 0.00017515172572858884,
        "epoch": 0.2308087951652308,
        "step": 1795
    },
    {
        "loss": 1.1398,
        "grad_norm": 2.2928316593170166,
        "learning_rate": 0.00017512501832583425,
        "epoch": 0.23093737945223095,
        "step": 1796
    },
    {
        "loss": 1.495,
        "grad_norm": 1.4109652042388916,
        "learning_rate": 0.0001750982986165089,
        "epoch": 0.23106596373923108,
        "step": 1797
    },
    {
        "loss": 1.4736,
        "grad_norm": 2.722970962524414,
        "learning_rate": 0.00017507156660498985,
        "epoch": 0.23119454802623118,
        "step": 1798
    },
    {
        "loss": 2.1998,
        "grad_norm": 1.3214092254638672,
        "learning_rate": 0.00017504482229565617,
        "epoch": 0.23132313231323132,
        "step": 1799
    },
    {
        "loss": 2.0293,
        "grad_norm": 1.8350520133972168,
        "learning_rate": 0.00017501806569288905,
        "epoch": 0.23145171660023145,
        "step": 1800
    },
    {
        "eval_loss": 1.9958785772323608,
        "eval_runtime": 28.1501,
        "eval_samples_per_second": 2.806,
        "eval_steps_per_second": 2.806,
        "epoch": 0.23145171660023145,
        "step": 1800
    },
    {
        "loss": 1.7942,
        "grad_norm": 2.5746967792510986,
        "learning_rate": 0.0001749912968010715,
        "epoch": 0.23158030088723158,
        "step": 1801
    },
    {
        "loss": 1.8757,
        "grad_norm": 2.1300199031829834,
        "learning_rate": 0.0001749645156245887,
        "epoch": 0.23170888517423172,
        "step": 1802
    },
    {
        "loss": 2.0797,
        "grad_norm": 2.135651111602783,
        "learning_rate": 0.00017493772216782783,
        "epoch": 0.23183746946123185,
        "step": 1803
    },
    {
        "loss": 1.8654,
        "grad_norm": 2.2032763957977295,
        "learning_rate": 0.000174910916435178,
        "epoch": 0.23196605374823195,
        "step": 1804
    },
    {
        "loss": 2.4039,
        "grad_norm": 1.8427410125732422,
        "learning_rate": 0.00017488409843103043,
        "epoch": 0.2320946380352321,
        "step": 1805
    },
    {
        "loss": 1.5942,
        "grad_norm": 1.9165040254592896,
        "learning_rate": 0.00017485726815977826,
        "epoch": 0.23222322232223222,
        "step": 1806
    },
    {
        "loss": 1.7853,
        "grad_norm": 1.81095552444458,
        "learning_rate": 0.0001748304256258167,
        "epoch": 0.23235180660923235,
        "step": 1807
    },
    {
        "loss": 2.336,
        "grad_norm": 1.401740312576294,
        "learning_rate": 0.00017480357083354288,
        "epoch": 0.23248039089623249,
        "step": 1808
    },
    {
        "loss": 1.3213,
        "grad_norm": 1.9581761360168457,
        "learning_rate": 0.00017477670378735613,
        "epoch": 0.23260897518323262,
        "step": 1809
    },
    {
        "loss": 2.6196,
        "grad_norm": 1.6030296087265015,
        "learning_rate": 0.00017474982449165757,
        "epoch": 0.23273755947023272,
        "step": 1810
    },
    {
        "loss": 1.7993,
        "grad_norm": 1.712654948234558,
        "learning_rate": 0.00017472293295085043,
        "epoch": 0.23286614375723286,
        "step": 1811
    },
    {
        "loss": 2.381,
        "grad_norm": 1.1317464113235474,
        "learning_rate": 0.00017469602916933997,
        "epoch": 0.232994728044233,
        "step": 1812
    },
    {
        "loss": 1.8835,
        "grad_norm": 1.5316029787063599,
        "learning_rate": 0.00017466911315153338,
        "epoch": 0.23312331233123312,
        "step": 1813
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.6505475044250488,
        "learning_rate": 0.00017464218490183993,
        "epoch": 0.23325189661823326,
        "step": 1814
    },
    {
        "loss": 1.8251,
        "grad_norm": 2.2981884479522705,
        "learning_rate": 0.00017461524442467086,
        "epoch": 0.2333804809052334,
        "step": 1815
    },
    {
        "loss": 2.2146,
        "grad_norm": 1.879371166229248,
        "learning_rate": 0.00017458829172443938,
        "epoch": 0.23350906519223352,
        "step": 1816
    },
    {
        "loss": 1.8717,
        "grad_norm": 2.3631649017333984,
        "learning_rate": 0.00017456132680556075,
        "epoch": 0.23363764947923363,
        "step": 1817
    },
    {
        "loss": 1.7103,
        "grad_norm": 2.2175521850585938,
        "learning_rate": 0.00017453434967245221,
        "epoch": 0.23376623376623376,
        "step": 1818
    },
    {
        "loss": 2.2761,
        "grad_norm": 1.8104180097579956,
        "learning_rate": 0.00017450736032953304,
        "epoch": 0.2338948180532339,
        "step": 1819
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.5620193481445312,
        "learning_rate": 0.00017448035878122445,
        "epoch": 0.23402340234023403,
        "step": 1820
    },
    {
        "loss": 1.8917,
        "grad_norm": 1.6029491424560547,
        "learning_rate": 0.00017445334503194966,
        "epoch": 0.23415198662723416,
        "step": 1821
    },
    {
        "loss": 2.0011,
        "grad_norm": 2.3441693782806396,
        "learning_rate": 0.000174426319086134,
        "epoch": 0.2342805709142343,
        "step": 1822
    },
    {
        "loss": 2.2256,
        "grad_norm": 1.4568408727645874,
        "learning_rate": 0.00017439928094820464,
        "epoch": 0.2344091552012344,
        "step": 1823
    },
    {
        "loss": 2.7194,
        "grad_norm": 1.3610918521881104,
        "learning_rate": 0.00017437223062259088,
        "epoch": 0.23453773948823453,
        "step": 1824
    },
    {
        "loss": 2.4642,
        "grad_norm": 2.021226167678833,
        "learning_rate": 0.00017434516811372388,
        "epoch": 0.23466632377523466,
        "step": 1825
    },
    {
        "loss": 1.6328,
        "grad_norm": 1.332956075668335,
        "learning_rate": 0.00017431809342603692,
        "epoch": 0.2347949080622348,
        "step": 1826
    },
    {
        "loss": 2.4557,
        "grad_norm": 1.2343344688415527,
        "learning_rate": 0.00017429100656396525,
        "epoch": 0.23492349234923493,
        "step": 1827
    },
    {
        "loss": 1.6696,
        "grad_norm": 1.7656903266906738,
        "learning_rate": 0.00017426390753194604,
        "epoch": 0.23505207663623506,
        "step": 1828
    },
    {
        "loss": 2.1902,
        "grad_norm": 1.7571748495101929,
        "learning_rate": 0.00017423679633441855,
        "epoch": 0.23518066092323517,
        "step": 1829
    },
    {
        "loss": 2.225,
        "grad_norm": 1.7381271123886108,
        "learning_rate": 0.00017420967297582395,
        "epoch": 0.2353092452102353,
        "step": 1830
    },
    {
        "loss": 1.2377,
        "grad_norm": 1.3464518785476685,
        "learning_rate": 0.00017418253746060547,
        "epoch": 0.23543782949723543,
        "step": 1831
    },
    {
        "loss": 2.2198,
        "grad_norm": 1.8735666275024414,
        "learning_rate": 0.00017415538979320832,
        "epoch": 0.23556641378423557,
        "step": 1832
    },
    {
        "loss": 1.8428,
        "grad_norm": 2.466219902038574,
        "learning_rate": 0.00017412822997807961,
        "epoch": 0.2356949980712357,
        "step": 1833
    },
    {
        "loss": 2.043,
        "grad_norm": 2.3273425102233887,
        "learning_rate": 0.00017410105801966857,
        "epoch": 0.23582358235823583,
        "step": 1834
    },
    {
        "loss": 1.4551,
        "grad_norm": 2.1763179302215576,
        "learning_rate": 0.00017407387392242635,
        "epoch": 0.23595216664523594,
        "step": 1835
    },
    {
        "loss": 2.5422,
        "grad_norm": 1.8816739320755005,
        "learning_rate": 0.0001740466776908061,
        "epoch": 0.23608075093223607,
        "step": 1836
    },
    {
        "loss": 1.4782,
        "grad_norm": 1.3718485832214355,
        "learning_rate": 0.00017401946932926296,
        "epoch": 0.2362093352192362,
        "step": 1837
    },
    {
        "loss": 1.8305,
        "grad_norm": 1.9434235095977783,
        "learning_rate": 0.00017399224884225408,
        "epoch": 0.23633791950623634,
        "step": 1838
    },
    {
        "loss": 2.4109,
        "grad_norm": 1.2583940029144287,
        "learning_rate": 0.0001739650162342385,
        "epoch": 0.23646650379323647,
        "step": 1839
    },
    {
        "loss": 1.8864,
        "grad_norm": 2.0013039112091064,
        "learning_rate": 0.00017393777150967742,
        "epoch": 0.2365950880802366,
        "step": 1840
    },
    {
        "loss": 1.6876,
        "grad_norm": 2.2679619789123535,
        "learning_rate": 0.0001739105146730338,
        "epoch": 0.23672367236723674,
        "step": 1841
    },
    {
        "loss": 1.8521,
        "grad_norm": 1.791687250137329,
        "learning_rate": 0.00017388324572877278,
        "epoch": 0.23685225665423684,
        "step": 1842
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.5217829942703247,
        "learning_rate": 0.00017385596468136143,
        "epoch": 0.23698084094123698,
        "step": 1843
    },
    {
        "loss": 2.0987,
        "grad_norm": 1.0805262327194214,
        "learning_rate": 0.00017382867153526875,
        "epoch": 0.2371094252282371,
        "step": 1844
    },
    {
        "loss": 2.0679,
        "grad_norm": 1.7412149906158447,
        "learning_rate": 0.00017380136629496575,
        "epoch": 0.23723800951523724,
        "step": 1845
    },
    {
        "loss": 2.5185,
        "grad_norm": 1.326636791229248,
        "learning_rate": 0.00017377404896492545,
        "epoch": 0.23736659380223737,
        "step": 1846
    },
    {
        "loss": 1.5496,
        "grad_norm": 1.7318496704101562,
        "learning_rate": 0.00017374671954962278,
        "epoch": 0.2374951780892375,
        "step": 1847
    },
    {
        "loss": 1.9791,
        "grad_norm": 1.786853313446045,
        "learning_rate": 0.00017371937805353476,
        "epoch": 0.2376237623762376,
        "step": 1848
    },
    {
        "loss": 0.6681,
        "grad_norm": 1.8377028703689575,
        "learning_rate": 0.00017369202448114026,
        "epoch": 0.23775234666323775,
        "step": 1849
    },
    {
        "loss": 1.5943,
        "grad_norm": 2.3778798580169678,
        "learning_rate": 0.00017366465883692024,
        "epoch": 0.23788093095023788,
        "step": 1850
    },
    {
        "loss": 2.1941,
        "grad_norm": 1.444019079208374,
        "learning_rate": 0.00017363728112535756,
        "epoch": 0.238009515237238,
        "step": 1851
    },
    {
        "loss": 1.5712,
        "grad_norm": 2.2872724533081055,
        "learning_rate": 0.0001736098913509371,
        "epoch": 0.23813809952423814,
        "step": 1852
    },
    {
        "loss": 2.374,
        "grad_norm": 2.1574575901031494,
        "learning_rate": 0.0001735824895181457,
        "epoch": 0.23826668381123828,
        "step": 1853
    },
    {
        "loss": 2.2626,
        "grad_norm": 1.998515248298645,
        "learning_rate": 0.0001735550756314722,
        "epoch": 0.23839526809823838,
        "step": 1854
    },
    {
        "loss": 2.4746,
        "grad_norm": 1.0416326522827148,
        "learning_rate": 0.00017352764969540737,
        "epoch": 0.23852385238523852,
        "step": 1855
    },
    {
        "loss": 1.7048,
        "grad_norm": 2.540823459625244,
        "learning_rate": 0.000173500211714444,
        "epoch": 0.23865243667223865,
        "step": 1856
    },
    {
        "loss": 2.3359,
        "grad_norm": 1.565299153327942,
        "learning_rate": 0.00017347276169307678,
        "epoch": 0.23878102095923878,
        "step": 1857
    },
    {
        "loss": 2.5051,
        "grad_norm": 1.887662410736084,
        "learning_rate": 0.00017344529963580246,
        "epoch": 0.23890960524623892,
        "step": 1858
    },
    {
        "loss": 1.3255,
        "grad_norm": 2.218564033508301,
        "learning_rate": 0.00017341782554711977,
        "epoch": 0.23903818953323905,
        "step": 1859
    },
    {
        "loss": 1.5933,
        "grad_norm": 1.5899899005889893,
        "learning_rate": 0.00017339033943152926,
        "epoch": 0.23916677382023915,
        "step": 1860
    },
    {
        "loss": 2.4899,
        "grad_norm": 1.4449009895324707,
        "learning_rate": 0.00017336284129353366,
        "epoch": 0.2392953581072393,
        "step": 1861
    },
    {
        "loss": 2.3874,
        "grad_norm": 1.2118775844573975,
        "learning_rate": 0.00017333533113763749,
        "epoch": 0.23942394239423942,
        "step": 1862
    },
    {
        "loss": 1.6557,
        "grad_norm": 2.9061365127563477,
        "learning_rate": 0.00017330780896834735,
        "epoch": 0.23955252668123955,
        "step": 1863
    },
    {
        "loss": 1.6982,
        "grad_norm": 1.9624834060668945,
        "learning_rate": 0.00017328027479017176,
        "epoch": 0.23968111096823969,
        "step": 1864
    },
    {
        "loss": 1.7013,
        "grad_norm": 1.570092797279358,
        "learning_rate": 0.00017325272860762122,
        "epoch": 0.23980969525523982,
        "step": 1865
    },
    {
        "loss": 2.5115,
        "grad_norm": 1.92216956615448,
        "learning_rate": 0.00017322517042520818,
        "epoch": 0.23993827954223995,
        "step": 1866
    },
    {
        "loss": 2.1781,
        "grad_norm": 1.2065863609313965,
        "learning_rate": 0.00017319760024744709,
        "epoch": 0.24006686382924006,
        "step": 1867
    },
    {
        "loss": 1.0621,
        "grad_norm": 2.020338296890259,
        "learning_rate": 0.00017317001807885438,
        "epoch": 0.2401954481162402,
        "step": 1868
    },
    {
        "loss": 2.3422,
        "grad_norm": 1.1834970712661743,
        "learning_rate": 0.00017314242392394834,
        "epoch": 0.24032403240324032,
        "step": 1869
    },
    {
        "loss": 2.5225,
        "grad_norm": 1.9266295433044434,
        "learning_rate": 0.00017311481778724933,
        "epoch": 0.24045261669024046,
        "step": 1870
    },
    {
        "loss": 1.3299,
        "grad_norm": 1.825767159461975,
        "learning_rate": 0.00017308719967327967,
        "epoch": 0.2405812009772406,
        "step": 1871
    },
    {
        "loss": 2.1425,
        "grad_norm": 1.672558069229126,
        "learning_rate": 0.00017305956958656355,
        "epoch": 0.24070978526424072,
        "step": 1872
    },
    {
        "loss": 1.4648,
        "grad_norm": 1.8037325143814087,
        "learning_rate": 0.0001730319275316272,
        "epoch": 0.24083836955124083,
        "step": 1873
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.554918646812439,
        "learning_rate": 0.00017300427351299883,
        "epoch": 0.24096695383824096,
        "step": 1874
    },
    {
        "loss": 2.4933,
        "grad_norm": 1.6136385202407837,
        "learning_rate": 0.00017297660753520848,
        "epoch": 0.2410955381252411,
        "step": 1875
    },
    {
        "loss": 1.7616,
        "grad_norm": 2.208418607711792,
        "learning_rate": 0.00017294892960278833,
        "epoch": 0.24122412241224123,
        "step": 1876
    },
    {
        "loss": 1.8077,
        "grad_norm": 1.878965139389038,
        "learning_rate": 0.00017292123972027237,
        "epoch": 0.24135270669924136,
        "step": 1877
    },
    {
        "loss": 1.967,
        "grad_norm": 1.7133129835128784,
        "learning_rate": 0.00017289353789219666,
        "epoch": 0.2414812909862415,
        "step": 1878
    },
    {
        "loss": 2.194,
        "grad_norm": 1.7280125617980957,
        "learning_rate": 0.0001728658241230991,
        "epoch": 0.2416098752732416,
        "step": 1879
    },
    {
        "loss": 1.1632,
        "grad_norm": 2.0076944828033447,
        "learning_rate": 0.00017283809841751966,
        "epoch": 0.24173845956024173,
        "step": 1880
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.0125906467437744,
        "learning_rate": 0.0001728103607800002,
        "epoch": 0.24186704384724186,
        "step": 1881
    },
    {
        "loss": 2.4636,
        "grad_norm": 1.7007784843444824,
        "learning_rate": 0.00017278261121508453,
        "epoch": 0.241995628134242,
        "step": 1882
    },
    {
        "loss": 2.4325,
        "grad_norm": 1.8150781393051147,
        "learning_rate": 0.00017275484972731844,
        "epoch": 0.24212421242124213,
        "step": 1883
    },
    {
        "loss": 1.5067,
        "grad_norm": 1.364994764328003,
        "learning_rate": 0.00017272707632124968,
        "epoch": 0.24225279670824226,
        "step": 1884
    },
    {
        "loss": 1.967,
        "grad_norm": 1.9566833972930908,
        "learning_rate": 0.0001726992910014279,
        "epoch": 0.24238138099524237,
        "step": 1885
    },
    {
        "loss": 2.4129,
        "grad_norm": 1.624991536140442,
        "learning_rate": 0.0001726714937724048,
        "epoch": 0.2425099652822425,
        "step": 1886
    },
    {
        "loss": 2.2623,
        "grad_norm": 1.6761302947998047,
        "learning_rate": 0.0001726436846387339,
        "epoch": 0.24263854956924263,
        "step": 1887
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.1634047031402588,
        "learning_rate": 0.0001726158636049708,
        "epoch": 0.24276713385624277,
        "step": 1888
    },
    {
        "loss": 1.8445,
        "grad_norm": 1.4079867601394653,
        "learning_rate": 0.000172588030675673,
        "epoch": 0.2428957181432429,
        "step": 1889
    },
    {
        "loss": 2.4774,
        "grad_norm": 1.6427820920944214,
        "learning_rate": 0.00017256018585539988,
        "epoch": 0.24302430243024303,
        "step": 1890
    },
    {
        "loss": 1.1515,
        "grad_norm": 1.9990394115447998,
        "learning_rate": 0.00017253232914871285,
        "epoch": 0.24315288671724317,
        "step": 1891
    },
    {
        "loss": 1.6167,
        "grad_norm": 1.3038519620895386,
        "learning_rate": 0.00017250446056017523,
        "epoch": 0.24328147100424327,
        "step": 1892
    },
    {
        "loss": 1.7927,
        "grad_norm": 2.07608962059021,
        "learning_rate": 0.00017247658009435235,
        "epoch": 0.2434100552912434,
        "step": 1893
    },
    {
        "loss": 2.1718,
        "grad_norm": 1.4994102716445923,
        "learning_rate": 0.0001724486877558114,
        "epoch": 0.24353863957824354,
        "step": 1894
    },
    {
        "loss": 2.1308,
        "grad_norm": 1.7490766048431396,
        "learning_rate": 0.00017242078354912153,
        "epoch": 0.24366722386524367,
        "step": 1895
    },
    {
        "loss": 1.9129,
        "grad_norm": 1.9380347728729248,
        "learning_rate": 0.0001723928674788539,
        "epoch": 0.2437958081522438,
        "step": 1896
    },
    {
        "loss": 1.1359,
        "grad_norm": 2.0278289318084717,
        "learning_rate": 0.00017236493954958152,
        "epoch": 0.24392439243924394,
        "step": 1897
    },
    {
        "loss": 2.4725,
        "grad_norm": 1.0631517171859741,
        "learning_rate": 0.00017233699976587947,
        "epoch": 0.24405297672624404,
        "step": 1898
    },
    {
        "loss": 2.3276,
        "grad_norm": 2.3604886531829834,
        "learning_rate": 0.0001723090481323246,
        "epoch": 0.24418156101324417,
        "step": 1899
    },
    {
        "loss": 1.5586,
        "grad_norm": 2.447240114212036,
        "learning_rate": 0.00017228108465349587,
        "epoch": 0.2443101453002443,
        "step": 1900
    },
    {
        "eval_loss": 1.9868394136428833,
        "eval_runtime": 28.2268,
        "eval_samples_per_second": 2.799,
        "eval_steps_per_second": 2.799,
        "epoch": 0.2443101453002443,
        "step": 1900
    },
    {
        "loss": 2.0681,
        "grad_norm": 1.3407431840896606,
        "learning_rate": 0.000172253109333974,
        "epoch": 0.24443872958724444,
        "step": 1901
    },
    {
        "loss": 1.4601,
        "grad_norm": 1.4637935161590576,
        "learning_rate": 0.00017222512217834188,
        "epoch": 0.24456731387424457,
        "step": 1902
    },
    {
        "loss": 2.2011,
        "grad_norm": 1.4200530052185059,
        "learning_rate": 0.00017219712319118416,
        "epoch": 0.2446958981612447,
        "step": 1903
    },
    {
        "loss": 2.1577,
        "grad_norm": 2.3279452323913574,
        "learning_rate": 0.0001721691123770875,
        "epoch": 0.2448244824482448,
        "step": 1904
    },
    {
        "loss": 2.5948,
        "grad_norm": 1.515648603439331,
        "learning_rate": 0.0001721410897406404,
        "epoch": 0.24495306673524495,
        "step": 1905
    },
    {
        "loss": 1.012,
        "grad_norm": 2.1479573249816895,
        "learning_rate": 0.00017211305528643344,
        "epoch": 0.24508165102224508,
        "step": 1906
    },
    {
        "loss": 1.7699,
        "grad_norm": 1.70318603515625,
        "learning_rate": 0.00017208500901905912,
        "epoch": 0.2452102353092452,
        "step": 1907
    },
    {
        "loss": 1.799,
        "grad_norm": 1.5813599824905396,
        "learning_rate": 0.0001720569509431117,
        "epoch": 0.24533881959624534,
        "step": 1908
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.4786796569824219,
        "learning_rate": 0.00017202888106318762,
        "epoch": 0.24546740388324548,
        "step": 1909
    },
    {
        "loss": 1.5345,
        "grad_norm": 1.9484204053878784,
        "learning_rate": 0.00017200079938388507,
        "epoch": 0.24559598817024558,
        "step": 1910
    },
    {
        "loss": 1.8577,
        "grad_norm": 1.4813328981399536,
        "learning_rate": 0.00017197270590980427,
        "epoch": 0.24572457245724572,
        "step": 1911
    },
    {
        "loss": 1.9102,
        "grad_norm": 1.8650753498077393,
        "learning_rate": 0.00017194460064554732,
        "epoch": 0.24585315674424585,
        "step": 1912
    },
    {
        "loss": 1.4403,
        "grad_norm": 2.6482203006744385,
        "learning_rate": 0.00017191648359571825,
        "epoch": 0.24598174103124598,
        "step": 1913
    },
    {
        "loss": 1.8384,
        "grad_norm": 1.9678102731704712,
        "learning_rate": 0.0001718883547649231,
        "epoch": 0.24611032531824611,
        "step": 1914
    },
    {
        "loss": 1.9835,
        "grad_norm": 1.8915469646453857,
        "learning_rate": 0.00017186021415776975,
        "epoch": 0.24623890960524625,
        "step": 1915
    },
    {
        "loss": 1.5417,
        "grad_norm": 1.810865879058838,
        "learning_rate": 0.000171832061778868,
        "epoch": 0.24636749389224638,
        "step": 1916
    },
    {
        "loss": 1.4532,
        "grad_norm": 2.1994261741638184,
        "learning_rate": 0.0001718038976328297,
        "epoch": 0.24649607817924649,
        "step": 1917
    },
    {
        "loss": 1.4524,
        "grad_norm": 2.206139087677002,
        "learning_rate": 0.00017177572172426847,
        "epoch": 0.24662466246624662,
        "step": 1918
    },
    {
        "loss": 2.4077,
        "grad_norm": 1.8220335245132446,
        "learning_rate": 0.0001717475340578,
        "epoch": 0.24675324675324675,
        "step": 1919
    },
    {
        "loss": 1.9861,
        "grad_norm": 2.2505710124969482,
        "learning_rate": 0.00017171933463804178,
        "epoch": 0.24688183104024689,
        "step": 1920
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.8385567665100098,
        "learning_rate": 0.00017169112346961332,
        "epoch": 0.24701041532724702,
        "step": 1921
    },
    {
        "loss": 1.7468,
        "grad_norm": 2.8055615425109863,
        "learning_rate": 0.00017166290055713602,
        "epoch": 0.24713899961424715,
        "step": 1922
    },
    {
        "loss": 2.38,
        "grad_norm": 1.8356170654296875,
        "learning_rate": 0.00017163466590523323,
        "epoch": 0.24726758390124726,
        "step": 1923
    },
    {
        "loss": 1.89,
        "grad_norm": 2.3532848358154297,
        "learning_rate": 0.00017160641951853011,
        "epoch": 0.2473961681882474,
        "step": 1924
    },
    {
        "loss": 2.0285,
        "grad_norm": 1.355055809020996,
        "learning_rate": 0.00017157816140165393,
        "epoch": 0.24752475247524752,
        "step": 1925
    },
    {
        "loss": 2.0833,
        "grad_norm": 1.4407620429992676,
        "learning_rate": 0.0001715498915592337,
        "epoch": 0.24765333676224766,
        "step": 1926
    },
    {
        "loss": 1.0496,
        "grad_norm": 1.903119444847107,
        "learning_rate": 0.00017152160999590047,
        "epoch": 0.2477819210492478,
        "step": 1927
    },
    {
        "loss": 1.9609,
        "grad_norm": 2.2449586391448975,
        "learning_rate": 0.0001714933167162872,
        "epoch": 0.24791050533624792,
        "step": 1928
    },
    {
        "loss": 1.988,
        "grad_norm": 1.6672552824020386,
        "learning_rate": 0.00017146501172502868,
        "epoch": 0.24803908962324803,
        "step": 1929
    },
    {
        "loss": 1.6899,
        "grad_norm": 1.650740146636963,
        "learning_rate": 0.0001714366950267617,
        "epoch": 0.24816767391024816,
        "step": 1930
    },
    {
        "loss": 1.4358,
        "grad_norm": 1.972619652748108,
        "learning_rate": 0.00017140836662612495,
        "epoch": 0.2482962581972483,
        "step": 1931
    },
    {
        "loss": 1.5221,
        "grad_norm": 2.116795539855957,
        "learning_rate": 0.00017138002652775905,
        "epoch": 0.24842484248424843,
        "step": 1932
    },
    {
        "loss": 1.8826,
        "grad_norm": 2.0433883666992188,
        "learning_rate": 0.00017135167473630652,
        "epoch": 0.24855342677124856,
        "step": 1933
    },
    {
        "loss": 1.6127,
        "grad_norm": 1.8270611763000488,
        "learning_rate": 0.00017132331125641176,
        "epoch": 0.2486820110582487,
        "step": 1934
    },
    {
        "loss": 1.8568,
        "grad_norm": 2.1593875885009766,
        "learning_rate": 0.00017129493609272114,
        "epoch": 0.2488105953452488,
        "step": 1935
    },
    {
        "loss": 2.2774,
        "grad_norm": 1.6990875005722046,
        "learning_rate": 0.00017126654924988296,
        "epoch": 0.24893917963224893,
        "step": 1936
    },
    {
        "loss": 2.1225,
        "grad_norm": 1.6716703176498413,
        "learning_rate": 0.00017123815073254734,
        "epoch": 0.24906776391924906,
        "step": 1937
    },
    {
        "loss": 2.4346,
        "grad_norm": 1.438375473022461,
        "learning_rate": 0.0001712097405453664,
        "epoch": 0.2491963482062492,
        "step": 1938
    },
    {
        "loss": 2.5891,
        "grad_norm": 0.9866007566452026,
        "learning_rate": 0.00017118131869299415,
        "epoch": 0.24932493249324933,
        "step": 1939
    },
    {
        "loss": 1.2611,
        "grad_norm": 1.137174367904663,
        "learning_rate": 0.0001711528851800865,
        "epoch": 0.24945351678024946,
        "step": 1940
    },
    {
        "loss": 1.9005,
        "grad_norm": 1.8660821914672852,
        "learning_rate": 0.00017112444001130122,
        "epoch": 0.2495821010672496,
        "step": 1941
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.58720064163208,
        "learning_rate": 0.00017109598319129817,
        "epoch": 0.2497106853542497,
        "step": 1942
    },
    {
        "loss": 1.4813,
        "grad_norm": 2.123624086380005,
        "learning_rate": 0.00017106751472473883,
        "epoch": 0.24983926964124983,
        "step": 1943
    },
    {
        "loss": 2.0246,
        "grad_norm": 2.3482353687286377,
        "learning_rate": 0.00017103903461628686,
        "epoch": 0.24996785392824997,
        "step": 1944
    },
    {
        "loss": 1.5296,
        "grad_norm": 1.830321192741394,
        "learning_rate": 0.00017101054287060768,
        "epoch": 0.25009643821525007,
        "step": 1945
    },
    {
        "loss": 1.7993,
        "grad_norm": 1.3197506666183472,
        "learning_rate": 0.00017098203949236866,
        "epoch": 0.2502250225022502,
        "step": 1946
    },
    {
        "loss": 1.1806,
        "grad_norm": 2.217726469039917,
        "learning_rate": 0.00017095352448623908,
        "epoch": 0.25035360678925034,
        "step": 1947
    },
    {
        "loss": 1.4488,
        "grad_norm": 2.5290238857269287,
        "learning_rate": 0.00017092499785689006,
        "epoch": 0.25048219107625047,
        "step": 1948
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.386263847351074,
        "learning_rate": 0.00017089645960899471,
        "epoch": 0.2506107753632506,
        "step": 1949
    },
    {
        "loss": 2.166,
        "grad_norm": 1.5524812936782837,
        "learning_rate": 0.00017086790974722807,
        "epoch": 0.25073935965025074,
        "step": 1950
    },
    {
        "loss": 2.0657,
        "grad_norm": 2.2423548698425293,
        "learning_rate": 0.0001708393482762669,
        "epoch": 0.25086794393725087,
        "step": 1951
    },
    {
        "loss": 1.6254,
        "grad_norm": 2.0365793704986572,
        "learning_rate": 0.00017081077520079006,
        "epoch": 0.250996528224251,
        "step": 1952
    },
    {
        "loss": 2.3224,
        "grad_norm": 1.931328535079956,
        "learning_rate": 0.00017078219052547825,
        "epoch": 0.25112511251125114,
        "step": 1953
    },
    {
        "loss": 2.5003,
        "grad_norm": 1.5813795328140259,
        "learning_rate": 0.00017075359425501404,
        "epoch": 0.25125369679825127,
        "step": 1954
    },
    {
        "loss": 1.4257,
        "grad_norm": 1.700094223022461,
        "learning_rate": 0.0001707249863940819,
        "epoch": 0.2513822810852514,
        "step": 1955
    },
    {
        "loss": 2.1067,
        "grad_norm": 1.623376488685608,
        "learning_rate": 0.00017069636694736818,
        "epoch": 0.25151086537225154,
        "step": 1956
    },
    {
        "loss": 1.9129,
        "grad_norm": 1.4401880502700806,
        "learning_rate": 0.00017066773591956124,
        "epoch": 0.2516394496592516,
        "step": 1957
    },
    {
        "loss": 1.6791,
        "grad_norm": 2.2242255210876465,
        "learning_rate": 0.0001706390933153512,
        "epoch": 0.25176803394625175,
        "step": 1958
    },
    {
        "loss": 2.2674,
        "grad_norm": 1.7626162767410278,
        "learning_rate": 0.0001706104391394302,
        "epoch": 0.2518966182332519,
        "step": 1959
    },
    {
        "loss": 1.8396,
        "grad_norm": 2.3446578979492188,
        "learning_rate": 0.00017058177339649214,
        "epoch": 0.252025202520252,
        "step": 1960
    },
    {
        "loss": 2.1541,
        "grad_norm": 1.590714693069458,
        "learning_rate": 0.00017055309609123292,
        "epoch": 0.25215378680725214,
        "step": 1961
    },
    {
        "loss": 2.1017,
        "grad_norm": 1.6846739053726196,
        "learning_rate": 0.0001705244072283503,
        "epoch": 0.2522823710942523,
        "step": 1962
    },
    {
        "loss": 2.3931,
        "grad_norm": 0.877794086933136,
        "learning_rate": 0.00017049570681254388,
        "epoch": 0.2524109553812524,
        "step": 1963
    },
    {
        "loss": 2.3486,
        "grad_norm": 1.3434215784072876,
        "learning_rate": 0.00017046699484851526,
        "epoch": 0.25253953966825254,
        "step": 1964
    },
    {
        "loss": 2.0947,
        "grad_norm": 2.1488258838653564,
        "learning_rate": 0.0001704382713409679,
        "epoch": 0.2526681239552527,
        "step": 1965
    },
    {
        "loss": 1.6269,
        "grad_norm": 2.6070613861083984,
        "learning_rate": 0.00017040953629460709,
        "epoch": 0.2527967082422528,
        "step": 1966
    },
    {
        "loss": 1.5821,
        "grad_norm": 1.8767709732055664,
        "learning_rate": 0.00017038078971414003,
        "epoch": 0.25292529252925294,
        "step": 1967
    },
    {
        "loss": 0.8599,
        "grad_norm": 2.014115333557129,
        "learning_rate": 0.0001703520316042759,
        "epoch": 0.2530538768162531,
        "step": 1968
    },
    {
        "loss": 1.5313,
        "grad_norm": 1.7809544801712036,
        "learning_rate": 0.0001703232619697256,
        "epoch": 0.2531824611032532,
        "step": 1969
    },
    {
        "loss": 1.5845,
        "grad_norm": 1.3586657047271729,
        "learning_rate": 0.00017029448081520209,
        "epoch": 0.2533110453902533,
        "step": 1970
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.695496916770935,
        "learning_rate": 0.0001702656881454201,
        "epoch": 0.2534396296772534,
        "step": 1971
    },
    {
        "loss": 1.5206,
        "grad_norm": 1.983943223953247,
        "learning_rate": 0.00017023688396509629,
        "epoch": 0.25356821396425355,
        "step": 1972
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.4700359106063843,
        "learning_rate": 0.00017020806827894926,
        "epoch": 0.2536967982512537,
        "step": 1973
    },
    {
        "loss": 1.9178,
        "grad_norm": 2.313992500305176,
        "learning_rate": 0.00017017924109169938,
        "epoch": 0.2538253825382538,
        "step": 1974
    },
    {
        "loss": 2.1674,
        "grad_norm": 2.4468934535980225,
        "learning_rate": 0.000170150402408069,
        "epoch": 0.25395396682525395,
        "step": 1975
    },
    {
        "loss": 2.4889,
        "grad_norm": 1.1341667175292969,
        "learning_rate": 0.00017012155223278223,
        "epoch": 0.2540825511122541,
        "step": 1976
    },
    {
        "loss": 1.9156,
        "grad_norm": 1.4385331869125366,
        "learning_rate": 0.00017009269057056527,
        "epoch": 0.2542111353992542,
        "step": 1977
    },
    {
        "loss": 1.4895,
        "grad_norm": 1.556868553161621,
        "learning_rate": 0.000170063817426146,
        "epoch": 0.25433971968625435,
        "step": 1978
    },
    {
        "loss": 2.0764,
        "grad_norm": 1.558882236480713,
        "learning_rate": 0.0001700349328042543,
        "epoch": 0.2544683039732545,
        "step": 1979
    },
    {
        "loss": 2.2433,
        "grad_norm": 0.9693575501441956,
        "learning_rate": 0.00017000603670962189,
        "epoch": 0.2545968882602546,
        "step": 1980
    },
    {
        "loss": 2.0201,
        "grad_norm": 1.2440354824066162,
        "learning_rate": 0.0001699771291469823,
        "epoch": 0.25472547254725475,
        "step": 1981
    },
    {
        "loss": 2.1127,
        "grad_norm": 1.8740500211715698,
        "learning_rate": 0.00016994821012107113,
        "epoch": 0.2548540568342548,
        "step": 1982
    },
    {
        "loss": 1.4645,
        "grad_norm": 1.6571801900863647,
        "learning_rate": 0.00016991927963662566,
        "epoch": 0.25498264112125496,
        "step": 1983
    },
    {
        "loss": 1.453,
        "grad_norm": 2.418414354324341,
        "learning_rate": 0.0001698903376983851,
        "epoch": 0.2551112254082551,
        "step": 1984
    },
    {
        "loss": 0.9282,
        "grad_norm": 1.7176038026809692,
        "learning_rate": 0.00016986138431109065,
        "epoch": 0.2552398096952552,
        "step": 1985
    },
    {
        "loss": 2.5205,
        "grad_norm": 1.940873622894287,
        "learning_rate": 0.0001698324194794852,
        "epoch": 0.25536839398225536,
        "step": 1986
    },
    {
        "loss": 2.2156,
        "grad_norm": 2.0229365825653076,
        "learning_rate": 0.00016980344320831366,
        "epoch": 0.2554969782692555,
        "step": 1987
    },
    {
        "loss": 2.4169,
        "grad_norm": 1.4537986516952515,
        "learning_rate": 0.00016977445550232277,
        "epoch": 0.2556255625562556,
        "step": 1988
    },
    {
        "loss": 2.0509,
        "grad_norm": 1.6372594833374023,
        "learning_rate": 0.00016974545636626114,
        "epoch": 0.25575414684325576,
        "step": 1989
    },
    {
        "loss": 2.4314,
        "grad_norm": 1.7232520580291748,
        "learning_rate": 0.0001697164458048792,
        "epoch": 0.2558827311302559,
        "step": 1990
    },
    {
        "loss": 1.7891,
        "grad_norm": 1.7427685260772705,
        "learning_rate": 0.00016968742382292935,
        "epoch": 0.256011315417256,
        "step": 1991
    },
    {
        "loss": 1.6825,
        "grad_norm": 2.0497565269470215,
        "learning_rate": 0.00016965839042516582,
        "epoch": 0.25613989970425616,
        "step": 1992
    },
    {
        "loss": 1.4903,
        "grad_norm": 1.696716547012329,
        "learning_rate": 0.0001696293456163447,
        "epoch": 0.2562684839912563,
        "step": 1993
    },
    {
        "loss": 1.9992,
        "grad_norm": 1.676866054534912,
        "learning_rate": 0.0001696002894012239,
        "epoch": 0.2563970682782564,
        "step": 1994
    },
    {
        "loss": 1.4747,
        "grad_norm": 2.0716750621795654,
        "learning_rate": 0.0001695712217845633,
        "epoch": 0.2565256525652565,
        "step": 1995
    },
    {
        "loss": 1.9472,
        "grad_norm": 1.965165376663208,
        "learning_rate": 0.00016954214277112458,
        "epoch": 0.25665423685225663,
        "step": 1996
    },
    {
        "loss": 2.1027,
        "grad_norm": 2.0312581062316895,
        "learning_rate": 0.0001695130523656713,
        "epoch": 0.25678282113925677,
        "step": 1997
    },
    {
        "loss": 1.4596,
        "grad_norm": 2.534665584564209,
        "learning_rate": 0.00016948395057296892,
        "epoch": 0.2569114054262569,
        "step": 1998
    },
    {
        "loss": 1.7482,
        "grad_norm": 1.8568472862243652,
        "learning_rate": 0.00016945483739778472,
        "epoch": 0.25703998971325703,
        "step": 1999
    },
    {
        "loss": 2.2394,
        "grad_norm": 1.8622794151306152,
        "learning_rate": 0.00016942571284488787,
        "epoch": 0.25716857400025717,
        "step": 2000
    },
    {
        "eval_loss": 1.9795945882797241,
        "eval_runtime": 28.2747,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.25716857400025717,
        "step": 2000
    },
    {
        "loss": 1.809,
        "grad_norm": 1.777159571647644,
        "learning_rate": 0.0001693965769190494,
        "epoch": 0.2572971582872573,
        "step": 2001
    },
    {
        "loss": 1.6742,
        "grad_norm": 2.282255172729492,
        "learning_rate": 0.00016936742962504213,
        "epoch": 0.25742574257425743,
        "step": 2002
    },
    {
        "loss": 1.8017,
        "grad_norm": 1.8484470844268799,
        "learning_rate": 0.0001693382709676409,
        "epoch": 0.25755432686125757,
        "step": 2003
    },
    {
        "loss": 1.7774,
        "grad_norm": 2.2959072589874268,
        "learning_rate": 0.00016930910095162228,
        "epoch": 0.2576829111482577,
        "step": 2004
    },
    {
        "loss": 2.2487,
        "grad_norm": 1.1923203468322754,
        "learning_rate": 0.00016927991958176478,
        "epoch": 0.25781149543525783,
        "step": 2005
    },
    {
        "loss": 2.1329,
        "grad_norm": 1.4461227655410767,
        "learning_rate": 0.00016925072686284866,
        "epoch": 0.25794007972225796,
        "step": 2006
    },
    {
        "loss": 1.7394,
        "grad_norm": 1.8805004358291626,
        "learning_rate": 0.00016922152279965618,
        "epoch": 0.25806866400925804,
        "step": 2007
    },
    {
        "loss": 1.7131,
        "grad_norm": 1.7053650617599487,
        "learning_rate": 0.00016919230739697138,
        "epoch": 0.2581972482962582,
        "step": 2008
    },
    {
        "loss": 1.8118,
        "grad_norm": 1.6463704109191895,
        "learning_rate": 0.00016916308065958012,
        "epoch": 0.2583258325832583,
        "step": 2009
    },
    {
        "loss": 2.4783,
        "grad_norm": 1.1569095849990845,
        "learning_rate": 0.00016913384259227026,
        "epoch": 0.25845441687025844,
        "step": 2010
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.023128032684326,
        "learning_rate": 0.0001691045931998313,
        "epoch": 0.2585830011572586,
        "step": 2011
    },
    {
        "loss": 1.7053,
        "grad_norm": 2.533829689025879,
        "learning_rate": 0.0001690753324870548,
        "epoch": 0.2587115854442587,
        "step": 2012
    },
    {
        "loss": 1.7881,
        "grad_norm": 1.750773310661316,
        "learning_rate": 0.00016904606045873406,
        "epoch": 0.25884016973125884,
        "step": 2013
    },
    {
        "loss": 0.9938,
        "grad_norm": 2.050259590148926,
        "learning_rate": 0.00016901677711966427,
        "epoch": 0.258968754018259,
        "step": 2014
    },
    {
        "loss": 1.5136,
        "grad_norm": 2.2698752880096436,
        "learning_rate": 0.00016898748247464246,
        "epoch": 0.2590973383052591,
        "step": 2015
    },
    {
        "loss": 1.6956,
        "grad_norm": 1.6214561462402344,
        "learning_rate": 0.00016895817652846752,
        "epoch": 0.25922592259225924,
        "step": 2016
    },
    {
        "loss": 1.8101,
        "grad_norm": 1.765944242477417,
        "learning_rate": 0.00016892885928594022,
        "epoch": 0.25935450687925937,
        "step": 2017
    },
    {
        "loss": 2.1339,
        "grad_norm": 1.8708022832870483,
        "learning_rate": 0.0001688995307518631,
        "epoch": 0.2594830911662595,
        "step": 2018
    },
    {
        "loss": 2.0468,
        "grad_norm": 2.401749610900879,
        "learning_rate": 0.00016887019093104067,
        "epoch": 0.25961167545325964,
        "step": 2019
    },
    {
        "loss": 1.7891,
        "grad_norm": 2.3546652793884277,
        "learning_rate": 0.00016884083982827915,
        "epoch": 0.2597402597402597,
        "step": 2020
    },
    {
        "loss": 2.325,
        "grad_norm": 1.520329236984253,
        "learning_rate": 0.0001688114774483867,
        "epoch": 0.25986884402725985,
        "step": 2021
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.5691039562225342,
        "learning_rate": 0.00016878210379617331,
        "epoch": 0.25999742831426,
        "step": 2022
    },
    {
        "loss": 2.0153,
        "grad_norm": 1.5764522552490234,
        "learning_rate": 0.00016875271887645084,
        "epoch": 0.2601260126012601,
        "step": 2023
    },
    {
        "loss": 1.7346,
        "grad_norm": 2.3073036670684814,
        "learning_rate": 0.00016872332269403292,
        "epoch": 0.26025459688826025,
        "step": 2024
    },
    {
        "loss": 2.441,
        "grad_norm": 1.5089263916015625,
        "learning_rate": 0.00016869391525373506,
        "epoch": 0.2603831811752604,
        "step": 2025
    },
    {
        "loss": 2.8371,
        "grad_norm": 1.2787867784500122,
        "learning_rate": 0.0001686644965603747,
        "epoch": 0.2605117654622605,
        "step": 2026
    },
    {
        "loss": 1.8947,
        "grad_norm": 2.1275558471679688,
        "learning_rate": 0.00016863506661877098,
        "epoch": 0.26064034974926065,
        "step": 2027
    },
    {
        "loss": 1.7826,
        "grad_norm": 1.3925261497497559,
        "learning_rate": 0.00016860562543374497,
        "epoch": 0.2607689340362608,
        "step": 2028
    },
    {
        "loss": 2.4025,
        "grad_norm": 1.3820005655288696,
        "learning_rate": 0.0001685761730101196,
        "epoch": 0.2608975183232609,
        "step": 2029
    },
    {
        "loss": 0.8439,
        "grad_norm": 2.1759228706359863,
        "learning_rate": 0.0001685467093527196,
        "epoch": 0.26102610261026105,
        "step": 2030
    },
    {
        "loss": 1.9952,
        "grad_norm": 1.781671166419983,
        "learning_rate": 0.00016851723446637147,
        "epoch": 0.2611546868972612,
        "step": 2031
    },
    {
        "loss": 2.1228,
        "grad_norm": 1.5887523889541626,
        "learning_rate": 0.0001684877483559037,
        "epoch": 0.26128327118426126,
        "step": 2032
    },
    {
        "loss": 1.4628,
        "grad_norm": 2.396944046020508,
        "learning_rate": 0.00016845825102614653,
        "epoch": 0.2614118554712614,
        "step": 2033
    },
    {
        "loss": 2.713,
        "grad_norm": 1.8906174898147583,
        "learning_rate": 0.00016842874248193206,
        "epoch": 0.2615404397582615,
        "step": 2034
    },
    {
        "loss": 3.0591,
        "grad_norm": 1.8042724132537842,
        "learning_rate": 0.0001683992227280942,
        "epoch": 0.26166902404526166,
        "step": 2035
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.5539582967758179,
        "learning_rate": 0.0001683696917694687,
        "epoch": 0.2617976083322618,
        "step": 2036
    },
    {
        "loss": 1.1865,
        "grad_norm": 2.4573352336883545,
        "learning_rate": 0.00016834014961089318,
        "epoch": 0.2619261926192619,
        "step": 2037
    },
    {
        "loss": 1.8381,
        "grad_norm": 1.4447081089019775,
        "learning_rate": 0.0001683105962572071,
        "epoch": 0.26205477690626205,
        "step": 2038
    },
    {
        "loss": 1.7222,
        "grad_norm": 1.9164892435073853,
        "learning_rate": 0.00016828103171325168,
        "epoch": 0.2621833611932622,
        "step": 2039
    },
    {
        "loss": 2.0473,
        "grad_norm": 2.022763967514038,
        "learning_rate": 0.00016825145598387006,
        "epoch": 0.2623119454802623,
        "step": 2040
    },
    {
        "loss": 0.9923,
        "grad_norm": 2.2154011726379395,
        "learning_rate": 0.00016822186907390716,
        "epoch": 0.26244052976726245,
        "step": 2041
    },
    {
        "loss": 2.3473,
        "grad_norm": 1.435015320777893,
        "learning_rate": 0.00016819227098820974,
        "epoch": 0.2625691140542626,
        "step": 2042
    },
    {
        "loss": 2.2771,
        "grad_norm": 1.3868491649627686,
        "learning_rate": 0.0001681626617316264,
        "epoch": 0.2626976983412627,
        "step": 2043
    },
    {
        "loss": 2.4422,
        "grad_norm": 1.590369462966919,
        "learning_rate": 0.0001681330413090076,
        "epoch": 0.26282628262826285,
        "step": 2044
    },
    {
        "loss": 1.4561,
        "grad_norm": 1.516194462776184,
        "learning_rate": 0.00016810340972520552,
        "epoch": 0.26295486691526293,
        "step": 2045
    },
    {
        "loss": 1.3847,
        "grad_norm": 1.9545561075210571,
        "learning_rate": 0.00016807376698507433,
        "epoch": 0.26308345120226306,
        "step": 2046
    },
    {
        "loss": 2.0637,
        "grad_norm": 1.6677579879760742,
        "learning_rate": 0.0001680441130934699,
        "epoch": 0.2632120354892632,
        "step": 2047
    },
    {
        "loss": 1.8482,
        "grad_norm": 1.8990299701690674,
        "learning_rate": 0.00016801444805524996,
        "epoch": 0.26334061977626333,
        "step": 2048
    },
    {
        "loss": 2.1304,
        "grad_norm": 1.249759554862976,
        "learning_rate": 0.0001679847718752741,
        "epoch": 0.26346920406326346,
        "step": 2049
    },
    {
        "loss": 2.2968,
        "grad_norm": 1.9748480319976807,
        "learning_rate": 0.0001679550845584037,
        "epoch": 0.2635977883502636,
        "step": 2050
    },
    {
        "loss": 2.0429,
        "grad_norm": 1.7616429328918457,
        "learning_rate": 0.00016792538610950196,
        "epoch": 0.26372637263726373,
        "step": 2051
    },
    {
        "loss": 2.4805,
        "grad_norm": 1.1087547540664673,
        "learning_rate": 0.00016789567653343395,
        "epoch": 0.26385495692426386,
        "step": 2052
    },
    {
        "loss": 2.0623,
        "grad_norm": 3.298671007156372,
        "learning_rate": 0.00016786595583506648,
        "epoch": 0.263983541211264,
        "step": 2053
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.800061583518982,
        "learning_rate": 0.00016783622401926826,
        "epoch": 0.2641121254982641,
        "step": 2054
    },
    {
        "loss": 2.4322,
        "grad_norm": 1.5188416242599487,
        "learning_rate": 0.00016780648109090984,
        "epoch": 0.26424070978526426,
        "step": 2055
    },
    {
        "loss": 1.9343,
        "grad_norm": 2.1777987480163574,
        "learning_rate": 0.00016777672705486349,
        "epoch": 0.2643692940722644,
        "step": 2056
    },
    {
        "loss": 1.7145,
        "grad_norm": 1.4989442825317383,
        "learning_rate": 0.00016774696191600334,
        "epoch": 0.26449787835926447,
        "step": 2057
    },
    {
        "loss": 1.6373,
        "grad_norm": 1.7405420541763306,
        "learning_rate": 0.0001677171856792054,
        "epoch": 0.2646264626462646,
        "step": 2058
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.579723834991455,
        "learning_rate": 0.0001676873983493474,
        "epoch": 0.26475504693326474,
        "step": 2059
    },
    {
        "loss": 2.3261,
        "grad_norm": 1.232393741607666,
        "learning_rate": 0.00016765759993130901,
        "epoch": 0.26488363122026487,
        "step": 2060
    },
    {
        "loss": 2.0358,
        "grad_norm": 1.6615852117538452,
        "learning_rate": 0.00016762779042997158,
        "epoch": 0.265012215507265,
        "step": 2061
    },
    {
        "loss": 2.2372,
        "grad_norm": 2.190844774246216,
        "learning_rate": 0.00016759796985021835,
        "epoch": 0.26514079979426514,
        "step": 2062
    },
    {
        "loss": 2.1408,
        "grad_norm": 1.789940595626831,
        "learning_rate": 0.00016756813819693438,
        "epoch": 0.26526938408126527,
        "step": 2063
    },
    {
        "loss": 2.0356,
        "grad_norm": 1.2178423404693604,
        "learning_rate": 0.00016753829547500653,
        "epoch": 0.2653979683682654,
        "step": 2064
    },
    {
        "loss": 1.6402,
        "grad_norm": 2.8051252365112305,
        "learning_rate": 0.00016750844168932346,
        "epoch": 0.26552655265526554,
        "step": 2065
    },
    {
        "loss": 1.0932,
        "grad_norm": 2.708454132080078,
        "learning_rate": 0.00016747857684477565,
        "epoch": 0.26565513694226567,
        "step": 2066
    },
    {
        "loss": 1.366,
        "grad_norm": 2.1079201698303223,
        "learning_rate": 0.00016744870094625544,
        "epoch": 0.2657837212292658,
        "step": 2067
    },
    {
        "loss": 1.6243,
        "grad_norm": 2.362654685974121,
        "learning_rate": 0.0001674188139986569,
        "epoch": 0.26591230551626593,
        "step": 2068
    },
    {
        "loss": 1.4155,
        "grad_norm": 1.7116789817810059,
        "learning_rate": 0.0001673889160068759,
        "epoch": 0.26604088980326607,
        "step": 2069
    },
    {
        "loss": 2.2595,
        "grad_norm": 2.1863908767700195,
        "learning_rate": 0.0001673590069758103,
        "epoch": 0.26616947409026614,
        "step": 2070
    },
    {
        "loss": 1.6514,
        "grad_norm": 1.6260766983032227,
        "learning_rate": 0.0001673290869103595,
        "epoch": 0.2662980583772663,
        "step": 2071
    },
    {
        "loss": 2.0543,
        "grad_norm": 1.9321271181106567,
        "learning_rate": 0.0001672991558154249,
        "epoch": 0.2664266426642664,
        "step": 2072
    },
    {
        "loss": 1.6518,
        "grad_norm": 2.1323840618133545,
        "learning_rate": 0.00016726921369590963,
        "epoch": 0.26655522695126654,
        "step": 2073
    },
    {
        "loss": 1.9256,
        "grad_norm": 2.1628775596618652,
        "learning_rate": 0.00016723926055671866,
        "epoch": 0.2666838112382667,
        "step": 2074
    },
    {
        "loss": 1.3895,
        "grad_norm": 2.169076681137085,
        "learning_rate": 0.00016720929640275876,
        "epoch": 0.2668123955252668,
        "step": 2075
    },
    {
        "loss": 1.0099,
        "grad_norm": 1.7201752662658691,
        "learning_rate": 0.00016717932123893846,
        "epoch": 0.26694097981226694,
        "step": 2076
    },
    {
        "loss": 2.1727,
        "grad_norm": 2.0916850566864014,
        "learning_rate": 0.00016714933507016818,
        "epoch": 0.2670695640992671,
        "step": 2077
    },
    {
        "loss": 2.0744,
        "grad_norm": 1.2977466583251953,
        "learning_rate": 0.00016711933790136002,
        "epoch": 0.2671981483862672,
        "step": 2078
    },
    {
        "loss": 1.9095,
        "grad_norm": 1.8796979188919067,
        "learning_rate": 0.000167089329737428,
        "epoch": 0.26732673267326734,
        "step": 2079
    },
    {
        "loss": 1.804,
        "grad_norm": 1.3820372819900513,
        "learning_rate": 0.00016705931058328788,
        "epoch": 0.2674553169602675,
        "step": 2080
    },
    {
        "loss": 1.5904,
        "grad_norm": 1.9107882976531982,
        "learning_rate": 0.00016702928044385725,
        "epoch": 0.2675839012472676,
        "step": 2081
    },
    {
        "loss": 1.5299,
        "grad_norm": 1.8327372074127197,
        "learning_rate": 0.0001669992393240555,
        "epoch": 0.2677124855342677,
        "step": 2082
    },
    {
        "loss": 1.4255,
        "grad_norm": 1.6301512718200684,
        "learning_rate": 0.00016696918722880372,
        "epoch": 0.2678410698212678,
        "step": 2083
    },
    {
        "loss": 1.2821,
        "grad_norm": 2.16937518119812,
        "learning_rate": 0.00016693912416302498,
        "epoch": 0.26796965410826795,
        "step": 2084
    },
    {
        "loss": 2.0038,
        "grad_norm": 2.085751533508301,
        "learning_rate": 0.000166909050131644,
        "epoch": 0.2680982383952681,
        "step": 2085
    },
    {
        "loss": 1.2037,
        "grad_norm": 2.0118634700775146,
        "learning_rate": 0.00016687896513958733,
        "epoch": 0.2682268226822682,
        "step": 2086
    },
    {
        "loss": 1.5666,
        "grad_norm": 2.9891324043273926,
        "learning_rate": 0.00016684886919178334,
        "epoch": 0.26835540696926835,
        "step": 2087
    },
    {
        "loss": 1.0598,
        "grad_norm": 2.1820056438446045,
        "learning_rate": 0.0001668187622931622,
        "epoch": 0.2684839912562685,
        "step": 2088
    },
    {
        "loss": 2.4189,
        "grad_norm": 1.6120901107788086,
        "learning_rate": 0.00016678864444865586,
        "epoch": 0.2686125755432686,
        "step": 2089
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.7172192335128784,
        "learning_rate": 0.00016675851566319805,
        "epoch": 0.26874115983026875,
        "step": 2090
    },
    {
        "loss": 1.8668,
        "grad_norm": 1.5073739290237427,
        "learning_rate": 0.00016672837594172428,
        "epoch": 0.2688697441172689,
        "step": 2091
    },
    {
        "loss": 2.7445,
        "grad_norm": 1.760773777961731,
        "learning_rate": 0.00016669822528917193,
        "epoch": 0.268998328404269,
        "step": 2092
    },
    {
        "loss": 1.4055,
        "grad_norm": 2.6930675506591797,
        "learning_rate": 0.00016666806371048006,
        "epoch": 0.26912691269126915,
        "step": 2093
    },
    {
        "loss": 2.3674,
        "grad_norm": 1.9631774425506592,
        "learning_rate": 0.00016663789121058956,
        "epoch": 0.2692554969782693,
        "step": 2094
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.6112815141677856,
        "learning_rate": 0.00016660770779444324,
        "epoch": 0.26938408126526936,
        "step": 2095
    },
    {
        "loss": 2.1912,
        "grad_norm": 1.600209355354309,
        "learning_rate": 0.00016657751346698546,
        "epoch": 0.2695126655522695,
        "step": 2096
    },
    {
        "loss": 2.0157,
        "grad_norm": 1.6628856658935547,
        "learning_rate": 0.00016654730823316253,
        "epoch": 0.2696412498392696,
        "step": 2097
    },
    {
        "loss": 1.9938,
        "grad_norm": 1.6755154132843018,
        "learning_rate": 0.00016651709209792253,
        "epoch": 0.26976983412626976,
        "step": 2098
    },
    {
        "loss": 1.8683,
        "grad_norm": 1.755096435546875,
        "learning_rate": 0.00016648686506621528,
        "epoch": 0.2698984184132699,
        "step": 2099
    },
    {
        "loss": 2.5478,
        "grad_norm": 1.4746836423873901,
        "learning_rate": 0.0001664566271429924,
        "epoch": 0.27002700270027,
        "step": 2100
    },
    {
        "eval_loss": 1.9717440605163574,
        "eval_runtime": 28.1921,
        "eval_samples_per_second": 2.802,
        "eval_steps_per_second": 2.802,
        "epoch": 0.27002700270027,
        "step": 2100
    },
    {
        "loss": 2.0023,
        "grad_norm": 1.395267367362976,
        "learning_rate": 0.0001664263783332073,
        "epoch": 0.27015558698727016,
        "step": 2101
    },
    {
        "loss": 1.4647,
        "grad_norm": 1.7013154029846191,
        "learning_rate": 0.00016639611864181523,
        "epoch": 0.2702841712742703,
        "step": 2102
    },
    {
        "loss": 2.3326,
        "grad_norm": 1.8763222694396973,
        "learning_rate": 0.00016636584807377312,
        "epoch": 0.2704127555612704,
        "step": 2103
    },
    {
        "loss": 2.1016,
        "grad_norm": 1.4170937538146973,
        "learning_rate": 0.0001663355666340397,
        "epoch": 0.27054133984827056,
        "step": 2104
    },
    {
        "loss": 2.0699,
        "grad_norm": 1.4721115827560425,
        "learning_rate": 0.00016630527432757558,
        "epoch": 0.2706699241352707,
        "step": 2105
    },
    {
        "loss": 2.0559,
        "grad_norm": 1.9079140424728394,
        "learning_rate": 0.00016627497115934302,
        "epoch": 0.2707985084222708,
        "step": 2106
    },
    {
        "loss": 1.8462,
        "grad_norm": 1.4930627346038818,
        "learning_rate": 0.00016624465713430616,
        "epoch": 0.2709270927092709,
        "step": 2107
    },
    {
        "loss": 1.5086,
        "grad_norm": 2.3243165016174316,
        "learning_rate": 0.00016621433225743086,
        "epoch": 0.27105567699627103,
        "step": 2108
    },
    {
        "loss": 2.0358,
        "grad_norm": 1.664859652519226,
        "learning_rate": 0.00016618399653368479,
        "epoch": 0.27118426128327117,
        "step": 2109
    },
    {
        "loss": 1.9358,
        "grad_norm": 1.8142802715301514,
        "learning_rate": 0.00016615364996803734,
        "epoch": 0.2713128455702713,
        "step": 2110
    },
    {
        "loss": 1.9148,
        "grad_norm": 1.779955267906189,
        "learning_rate": 0.0001661232925654598,
        "epoch": 0.27144142985727143,
        "step": 2111
    },
    {
        "loss": 2.0925,
        "grad_norm": 1.9171130657196045,
        "learning_rate": 0.0001660929243309251,
        "epoch": 0.27157001414427157,
        "step": 2112
    },
    {
        "loss": 2.0414,
        "grad_norm": 2.2907495498657227,
        "learning_rate": 0.00016606254526940796,
        "epoch": 0.2716985984312717,
        "step": 2113
    },
    {
        "loss": 1.291,
        "grad_norm": 2.3484623432159424,
        "learning_rate": 0.00016603215538588497,
        "epoch": 0.27182718271827183,
        "step": 2114
    },
    {
        "loss": 1.5363,
        "grad_norm": 1.6293342113494873,
        "learning_rate": 0.00016600175468533442,
        "epoch": 0.27195576700527196,
        "step": 2115
    },
    {
        "loss": 1.1073,
        "grad_norm": 1.7776530981063843,
        "learning_rate": 0.0001659713431727364,
        "epoch": 0.2720843512922721,
        "step": 2116
    },
    {
        "loss": 2.0621,
        "grad_norm": 2.0452301502227783,
        "learning_rate": 0.0001659409208530728,
        "epoch": 0.27221293557927223,
        "step": 2117
    },
    {
        "loss": 1.7801,
        "grad_norm": 1.878989338874817,
        "learning_rate": 0.0001659104877313271,
        "epoch": 0.27234151986627236,
        "step": 2118
    },
    {
        "loss": 2.1195,
        "grad_norm": 1.867065191268921,
        "learning_rate": 0.00016588004381248484,
        "epoch": 0.2724701041532725,
        "step": 2119
    },
    {
        "loss": 2.2649,
        "grad_norm": 1.2638342380523682,
        "learning_rate": 0.00016584958910153308,
        "epoch": 0.2725986884402726,
        "step": 2120
    },
    {
        "loss": 1.4005,
        "grad_norm": 2.487516403198242,
        "learning_rate": 0.00016581912360346074,
        "epoch": 0.2727272727272727,
        "step": 2121
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.76090669631958,
        "learning_rate": 0.00016578864732325857,
        "epoch": 0.27285585701427284,
        "step": 2122
    },
    {
        "loss": 2.2741,
        "grad_norm": 0.9043759107589722,
        "learning_rate": 0.00016575816026591903,
        "epoch": 0.272984441301273,
        "step": 2123
    },
    {
        "loss": 1.4775,
        "grad_norm": 2.1555933952331543,
        "learning_rate": 0.0001657276624364363,
        "epoch": 0.2731130255882731,
        "step": 2124
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.122305393218994,
        "learning_rate": 0.0001656971538398064,
        "epoch": 0.27324160987527324,
        "step": 2125
    },
    {
        "loss": 1.9301,
        "grad_norm": 2.246932029724121,
        "learning_rate": 0.000165666634481027,
        "epoch": 0.27337019416227337,
        "step": 2126
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.050870895385742,
        "learning_rate": 0.00016563610436509777,
        "epoch": 0.2734987784492735,
        "step": 2127
    },
    {
        "loss": 1.4243,
        "grad_norm": 1.6536906957626343,
        "learning_rate": 0.0001656055634970199,
        "epoch": 0.27362736273627364,
        "step": 2128
    },
    {
        "loss": 1.7375,
        "grad_norm": 2.008775472640991,
        "learning_rate": 0.00016557501188179638,
        "epoch": 0.27375594702327377,
        "step": 2129
    },
    {
        "loss": 2.2725,
        "grad_norm": 2.3376526832580566,
        "learning_rate": 0.00016554444952443205,
        "epoch": 0.2738845313102739,
        "step": 2130
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.7321102619171143,
        "learning_rate": 0.0001655138764299335,
        "epoch": 0.27401311559727404,
        "step": 2131
    },
    {
        "loss": 2.1919,
        "grad_norm": 2.06619930267334,
        "learning_rate": 0.000165483292603309,
        "epoch": 0.2741416998842741,
        "step": 2132
    },
    {
        "loss": 2.0268,
        "grad_norm": 2.3325021266937256,
        "learning_rate": 0.00016545269804956868,
        "epoch": 0.27427028417127425,
        "step": 2133
    },
    {
        "loss": 1.7384,
        "grad_norm": 2.785005569458008,
        "learning_rate": 0.00016542209277372433,
        "epoch": 0.2743988684582744,
        "step": 2134
    },
    {
        "loss": 0.9013,
        "grad_norm": 2.3356595039367676,
        "learning_rate": 0.00016539147678078955,
        "epoch": 0.2745274527452745,
        "step": 2135
    },
    {
        "loss": 2.4099,
        "grad_norm": 1.0897002220153809,
        "learning_rate": 0.00016536085007577968,
        "epoch": 0.27465603703227465,
        "step": 2136
    },
    {
        "loss": 0.7242,
        "grad_norm": 2.842485189437866,
        "learning_rate": 0.00016533021266371185,
        "epoch": 0.2747846213192748,
        "step": 2137
    },
    {
        "loss": 1.8382,
        "grad_norm": 1.8216941356658936,
        "learning_rate": 0.0001652995645496049,
        "epoch": 0.2749132056062749,
        "step": 2138
    },
    {
        "loss": 2.3374,
        "grad_norm": 1.8569726943969727,
        "learning_rate": 0.00016526890573847938,
        "epoch": 0.27504178989327505,
        "step": 2139
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.1856212615966797,
        "learning_rate": 0.00016523823623535772,
        "epoch": 0.2751703741802752,
        "step": 2140
    },
    {
        "loss": 2.0544,
        "grad_norm": 1.8090426921844482,
        "learning_rate": 0.00016520755604526398,
        "epoch": 0.2752989584672753,
        "step": 2141
    },
    {
        "loss": 2.1934,
        "grad_norm": 2.158043146133423,
        "learning_rate": 0.00016517686517322408,
        "epoch": 0.27542754275427545,
        "step": 2142
    },
    {
        "loss": 1.55,
        "grad_norm": 1.2820945978164673,
        "learning_rate": 0.00016514616362426558,
        "epoch": 0.2755561270412756,
        "step": 2143
    },
    {
        "loss": 1.1856,
        "grad_norm": 2.38253116607666,
        "learning_rate": 0.0001651154514034179,
        "epoch": 0.2756847113282757,
        "step": 2144
    },
    {
        "loss": 2.1293,
        "grad_norm": 1.4218553304672241,
        "learning_rate": 0.00016508472851571208,
        "epoch": 0.2758132956152758,
        "step": 2145
    },
    {
        "loss": 2.1779,
        "grad_norm": 1.009016990661621,
        "learning_rate": 0.000165053994966181,
        "epoch": 0.2759418799022759,
        "step": 2146
    },
    {
        "loss": 1.5414,
        "grad_norm": 1.5801137685775757,
        "learning_rate": 0.00016502325075985926,
        "epoch": 0.27607046418927605,
        "step": 2147
    },
    {
        "loss": 1.6521,
        "grad_norm": 1.571215033531189,
        "learning_rate": 0.00016499249590178321,
        "epoch": 0.2761990484762762,
        "step": 2148
    },
    {
        "loss": 2.0473,
        "grad_norm": 1.7028887271881104,
        "learning_rate": 0.000164961730396991,
        "epoch": 0.2763276327632763,
        "step": 2149
    },
    {
        "loss": 1.3115,
        "grad_norm": 1.9739389419555664,
        "learning_rate": 0.00016493095425052236,
        "epoch": 0.27645621705027645,
        "step": 2150
    },
    {
        "loss": 1.8194,
        "grad_norm": 2.3189122676849365,
        "learning_rate": 0.00016490016746741892,
        "epoch": 0.2765848013372766,
        "step": 2151
    },
    {
        "loss": 1.4194,
        "grad_norm": 1.6899151802062988,
        "learning_rate": 0.00016486937005272406,
        "epoch": 0.2767133856242767,
        "step": 2152
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.0383176803588867,
        "learning_rate": 0.00016483856201148273,
        "epoch": 0.27684196991127685,
        "step": 2153
    },
    {
        "loss": 2.2325,
        "grad_norm": 2.055253505706787,
        "learning_rate": 0.00016480774334874188,
        "epoch": 0.276970554198277,
        "step": 2154
    },
    {
        "loss": 1.4976,
        "grad_norm": 2.0998003482818604,
        "learning_rate": 0.0001647769140695499,
        "epoch": 0.2770991384852771,
        "step": 2155
    },
    {
        "loss": 2.5464,
        "grad_norm": 1.4863698482513428,
        "learning_rate": 0.00016474607417895718,
        "epoch": 0.27722772277227725,
        "step": 2156
    },
    {
        "loss": 1.7471,
        "grad_norm": 3.0947585105895996,
        "learning_rate": 0.00016471522368201568,
        "epoch": 0.27735630705927733,
        "step": 2157
    },
    {
        "loss": 1.7669,
        "grad_norm": 1.138524055480957,
        "learning_rate": 0.00016468436258377923,
        "epoch": 0.27748489134627746,
        "step": 2158
    },
    {
        "loss": 1.9782,
        "grad_norm": 3.4065868854522705,
        "learning_rate": 0.00016465349088930328,
        "epoch": 0.2776134756332776,
        "step": 2159
    },
    {
        "loss": 1.8481,
        "grad_norm": 1.2618992328643799,
        "learning_rate": 0.00016462260860364506,
        "epoch": 0.27774205992027773,
        "step": 2160
    },
    {
        "loss": 1.3879,
        "grad_norm": 2.4034805297851562,
        "learning_rate": 0.00016459171573186354,
        "epoch": 0.27787064420727786,
        "step": 2161
    },
    {
        "loss": 2.3909,
        "grad_norm": 2.412604808807373,
        "learning_rate": 0.00016456081227901942,
        "epoch": 0.277999228494278,
        "step": 2162
    },
    {
        "loss": 2.2674,
        "grad_norm": 1.358713150024414,
        "learning_rate": 0.0001645298982501752,
        "epoch": 0.2781278127812781,
        "step": 2163
    },
    {
        "loss": 1.7674,
        "grad_norm": 2.308994770050049,
        "learning_rate": 0.00016449897365039494,
        "epoch": 0.27825639706827826,
        "step": 2164
    },
    {
        "loss": 2.5607,
        "grad_norm": 1.057585597038269,
        "learning_rate": 0.0001644680384847446,
        "epoch": 0.2783849813552784,
        "step": 2165
    },
    {
        "loss": 2.1816,
        "grad_norm": 2.0129289627075195,
        "learning_rate": 0.00016443709275829182,
        "epoch": 0.2785135656422785,
        "step": 2166
    },
    {
        "loss": 2.3106,
        "grad_norm": 0.9517658352851868,
        "learning_rate": 0.00016440613647610594,
        "epoch": 0.27864214992927866,
        "step": 2167
    },
    {
        "loss": 1.5442,
        "grad_norm": 1.4987050294876099,
        "learning_rate": 0.00016437516964325804,
        "epoch": 0.2787707342162788,
        "step": 2168
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.7104129791259766,
        "learning_rate": 0.00016434419226482092,
        "epoch": 0.2788993185032789,
        "step": 2169
    },
    {
        "loss": 2.4741,
        "grad_norm": 1.7017970085144043,
        "learning_rate": 0.00016431320434586922,
        "epoch": 0.279027902790279,
        "step": 2170
    },
    {
        "loss": 1.5283,
        "grad_norm": 1.791089415550232,
        "learning_rate": 0.0001642822058914791,
        "epoch": 0.27915648707727914,
        "step": 2171
    },
    {
        "loss": 2.4105,
        "grad_norm": 1.1030817031860352,
        "learning_rate": 0.00016425119690672858,
        "epoch": 0.27928507136427927,
        "step": 2172
    },
    {
        "loss": 1.8924,
        "grad_norm": 1.2261247634887695,
        "learning_rate": 0.00016422017739669745,
        "epoch": 0.2794136556512794,
        "step": 2173
    },
    {
        "loss": 1.6075,
        "grad_norm": 2.032611846923828,
        "learning_rate": 0.00016418914736646712,
        "epoch": 0.27954223993827954,
        "step": 2174
    },
    {
        "loss": 1.1693,
        "grad_norm": 2.319378614425659,
        "learning_rate": 0.00016415810682112074,
        "epoch": 0.27967082422527967,
        "step": 2175
    },
    {
        "loss": 2.0867,
        "grad_norm": 1.0833202600479126,
        "learning_rate": 0.0001641270557657432,
        "epoch": 0.2797994085122798,
        "step": 2176
    },
    {
        "loss": 1.0105,
        "grad_norm": 2.1249561309814453,
        "learning_rate": 0.00016409599420542118,
        "epoch": 0.27992799279927993,
        "step": 2177
    },
    {
        "loss": 1.8513,
        "grad_norm": 1.6155203580856323,
        "learning_rate": 0.0001640649221452429,
        "epoch": 0.28005657708628007,
        "step": 2178
    },
    {
        "loss": 1.7728,
        "grad_norm": 2.516944646835327,
        "learning_rate": 0.0001640338395902985,
        "epoch": 0.2801851613732802,
        "step": 2179
    },
    {
        "loss": 1.6173,
        "grad_norm": 2.0135459899902344,
        "learning_rate": 0.00016400274654567975,
        "epoch": 0.28031374566028033,
        "step": 2180
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.101250410079956,
        "learning_rate": 0.00016397164301648013,
        "epoch": 0.28044232994728047,
        "step": 2181
    },
    {
        "loss": 1.6411,
        "grad_norm": 1.7817387580871582,
        "learning_rate": 0.00016394052900779485,
        "epoch": 0.28057091423428054,
        "step": 2182
    },
    {
        "loss": 2.2155,
        "grad_norm": 1.230481505393982,
        "learning_rate": 0.00016390940452472083,
        "epoch": 0.2806994985212807,
        "step": 2183
    },
    {
        "loss": 0.8,
        "grad_norm": 2.488344192504883,
        "learning_rate": 0.0001638782695723567,
        "epoch": 0.2808280828082808,
        "step": 2184
    },
    {
        "loss": 2.0026,
        "grad_norm": 1.9570841789245605,
        "learning_rate": 0.0001638471241558029,
        "epoch": 0.28095666709528094,
        "step": 2185
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.5961717367172241,
        "learning_rate": 0.0001638159682801614,
        "epoch": 0.2810852513822811,
        "step": 2186
    },
    {
        "loss": 2.0443,
        "grad_norm": 1.4300724267959595,
        "learning_rate": 0.00016378480195053602,
        "epoch": 0.2812138356692812,
        "step": 2187
    },
    {
        "loss": 2.0194,
        "grad_norm": 1.6009005308151245,
        "learning_rate": 0.00016375362517203227,
        "epoch": 0.28134241995628134,
        "step": 2188
    },
    {
        "loss": 1.2366,
        "grad_norm": 1.4866960048675537,
        "learning_rate": 0.00016372243794975737,
        "epoch": 0.2814710042432815,
        "step": 2189
    },
    {
        "loss": 1.9336,
        "grad_norm": 1.7871179580688477,
        "learning_rate": 0.00016369124028882022,
        "epoch": 0.2815995885302816,
        "step": 2190
    },
    {
        "loss": 2.4172,
        "grad_norm": 1.1991921663284302,
        "learning_rate": 0.00016366003219433144,
        "epoch": 0.28172817281728174,
        "step": 2191
    },
    {
        "loss": 1.3467,
        "grad_norm": 1.671417474746704,
        "learning_rate": 0.0001636288136714034,
        "epoch": 0.2818567571042819,
        "step": 2192
    },
    {
        "loss": 1.4097,
        "grad_norm": 1.8292131423950195,
        "learning_rate": 0.00016359758472515013,
        "epoch": 0.281985341391282,
        "step": 2193
    },
    {
        "loss": 1.9001,
        "grad_norm": 1.5319968461990356,
        "learning_rate": 0.00016356634536068737,
        "epoch": 0.28211392567828214,
        "step": 2194
    },
    {
        "loss": 1.7777,
        "grad_norm": 3.745703935623169,
        "learning_rate": 0.0001635350955831326,
        "epoch": 0.2822425099652822,
        "step": 2195
    },
    {
        "loss": 1.6617,
        "grad_norm": 2.496567487716675,
        "learning_rate": 0.00016350383539760502,
        "epoch": 0.28237109425228235,
        "step": 2196
    },
    {
        "loss": 1.705,
        "grad_norm": 1.5175100564956665,
        "learning_rate": 0.00016347256480922547,
        "epoch": 0.2824996785392825,
        "step": 2197
    },
    {
        "loss": 2.1884,
        "grad_norm": 1.4539928436279297,
        "learning_rate": 0.00016344128382311652,
        "epoch": 0.2826282628262826,
        "step": 2198
    },
    {
        "loss": 1.8987,
        "grad_norm": 1.6918069124221802,
        "learning_rate": 0.00016340999244440245,
        "epoch": 0.28275684711328275,
        "step": 2199
    },
    {
        "loss": 1.5518,
        "grad_norm": 2.0605201721191406,
        "learning_rate": 0.00016337869067820925,
        "epoch": 0.2828854314002829,
        "step": 2200
    },
    {
        "eval_loss": 1.9664117097854614,
        "eval_runtime": 28.2671,
        "eval_samples_per_second": 2.795,
        "eval_steps_per_second": 2.795,
        "epoch": 0.2828854314002829,
        "step": 2200
    },
    {
        "loss": 2.3114,
        "grad_norm": 1.2276214361190796,
        "learning_rate": 0.00016334737852966464,
        "epoch": 0.283014015687283,
        "step": 2201
    },
    {
        "loss": 1.439,
        "grad_norm": 1.8499540090560913,
        "learning_rate": 0.00016331605600389794,
        "epoch": 0.28314259997428315,
        "step": 2202
    },
    {
        "loss": 2.0113,
        "grad_norm": 1.569848656654358,
        "learning_rate": 0.00016328472310604027,
        "epoch": 0.2832711842612833,
        "step": 2203
    },
    {
        "loss": 1.8193,
        "grad_norm": 1.8227343559265137,
        "learning_rate": 0.00016325337984122445,
        "epoch": 0.2833997685482834,
        "step": 2204
    },
    {
        "loss": 1.4262,
        "grad_norm": 1.73139488697052,
        "learning_rate": 0.0001632220262145849,
        "epoch": 0.28352835283528355,
        "step": 2205
    },
    {
        "loss": 1.7834,
        "grad_norm": 2.2943336963653564,
        "learning_rate": 0.00016319066223125783,
        "epoch": 0.2836569371222837,
        "step": 2206
    },
    {
        "loss": 1.5221,
        "grad_norm": 2.688861846923828,
        "learning_rate": 0.00016315928789638112,
        "epoch": 0.28378552140928376,
        "step": 2207
    },
    {
        "loss": 2.2471,
        "grad_norm": 1.5587401390075684,
        "learning_rate": 0.0001631279032150943,
        "epoch": 0.2839141056962839,
        "step": 2208
    },
    {
        "loss": 1.531,
        "grad_norm": 1.6588287353515625,
        "learning_rate": 0.00016309650819253872,
        "epoch": 0.284042689983284,
        "step": 2209
    },
    {
        "loss": 2.3422,
        "grad_norm": 0.8481729030609131,
        "learning_rate": 0.00016306510283385726,
        "epoch": 0.28417127427028416,
        "step": 2210
    },
    {
        "loss": 1.8528,
        "grad_norm": 1.3987189531326294,
        "learning_rate": 0.00016303368714419464,
        "epoch": 0.2842998585572843,
        "step": 2211
    },
    {
        "loss": 2.6887,
        "grad_norm": 1.263450264930725,
        "learning_rate": 0.00016300226112869713,
        "epoch": 0.2844284428442844,
        "step": 2212
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.349737524986267,
        "learning_rate": 0.00016297082479251285,
        "epoch": 0.28455702713128456,
        "step": 2213
    },
    {
        "loss": 1.9818,
        "grad_norm": 1.9412810802459717,
        "learning_rate": 0.00016293937814079149,
        "epoch": 0.2846856114182847,
        "step": 2214
    },
    {
        "loss": 2.4141,
        "grad_norm": 1.4811406135559082,
        "learning_rate": 0.00016290792117868444,
        "epoch": 0.2848141957052848,
        "step": 2215
    },
    {
        "loss": 2.146,
        "grad_norm": 1.935534954071045,
        "learning_rate": 0.00016287645391134488,
        "epoch": 0.28494277999228496,
        "step": 2216
    },
    {
        "loss": 1.425,
        "grad_norm": 1.9136059284210205,
        "learning_rate": 0.00016284497634392754,
        "epoch": 0.2850713642792851,
        "step": 2217
    },
    {
        "loss": 2.5729,
        "grad_norm": 1.3414885997772217,
        "learning_rate": 0.00016281348848158893,
        "epoch": 0.2851999485662852,
        "step": 2218
    },
    {
        "loss": 2.0022,
        "grad_norm": 2.237119197845459,
        "learning_rate": 0.00016278199032948725,
        "epoch": 0.28532853285328535,
        "step": 2219
    },
    {
        "loss": 1.4111,
        "grad_norm": 2.133539915084839,
        "learning_rate": 0.00016275048189278227,
        "epoch": 0.28545711714028543,
        "step": 2220
    },
    {
        "loss": 1.4865,
        "grad_norm": 2.442889451980591,
        "learning_rate": 0.00016271896317663563,
        "epoch": 0.28558570142728557,
        "step": 2221
    },
    {
        "loss": 1.4859,
        "grad_norm": 1.148738145828247,
        "learning_rate": 0.0001626874341862105,
        "epoch": 0.2857142857142857,
        "step": 2222
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.2716251611709595,
        "learning_rate": 0.0001626558949266718,
        "epoch": 0.28584287000128583,
        "step": 2223
    },
    {
        "loss": 1.2689,
        "grad_norm": 1.8052502870559692,
        "learning_rate": 0.0001626243454031861,
        "epoch": 0.28597145428828596,
        "step": 2224
    },
    {
        "loss": 1.78,
        "grad_norm": 1.7716081142425537,
        "learning_rate": 0.00016259278562092167,
        "epoch": 0.2861000385752861,
        "step": 2225
    },
    {
        "loss": 1.8105,
        "grad_norm": 1.7059669494628906,
        "learning_rate": 0.00016256121558504854,
        "epoch": 0.28622862286228623,
        "step": 2226
    },
    {
        "loss": 1.8538,
        "grad_norm": 1.3174265623092651,
        "learning_rate": 0.00016252963530073825,
        "epoch": 0.28635720714928636,
        "step": 2227
    },
    {
        "loss": 2.2315,
        "grad_norm": 1.3215913772583008,
        "learning_rate": 0.00016249804477316411,
        "epoch": 0.2864857914362865,
        "step": 2228
    },
    {
        "loss": 2.3126,
        "grad_norm": 1.5319008827209473,
        "learning_rate": 0.00016246644400750118,
        "epoch": 0.28661437572328663,
        "step": 2229
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.493391513824463,
        "learning_rate": 0.00016243483300892604,
        "epoch": 0.28674296001028676,
        "step": 2230
    },
    {
        "loss": 2.244,
        "grad_norm": 1.7950605154037476,
        "learning_rate": 0.0001624032117826171,
        "epoch": 0.2868715442972869,
        "step": 2231
    },
    {
        "loss": 1.403,
        "grad_norm": 2.0326461791992188,
        "learning_rate": 0.00016237158033375435,
        "epoch": 0.287000128584287,
        "step": 2232
    },
    {
        "loss": 2.0691,
        "grad_norm": 1.667466402053833,
        "learning_rate": 0.00016233993866751948,
        "epoch": 0.2871287128712871,
        "step": 2233
    },
    {
        "loss": 1.2659,
        "grad_norm": 1.9399235248565674,
        "learning_rate": 0.00016230828678909584,
        "epoch": 0.28725729715828724,
        "step": 2234
    },
    {
        "loss": 2.4478,
        "grad_norm": 1.7849271297454834,
        "learning_rate": 0.00016227662470366848,
        "epoch": 0.28738588144528737,
        "step": 2235
    },
    {
        "loss": 1.1434,
        "grad_norm": 2.915174961090088,
        "learning_rate": 0.00016224495241642416,
        "epoch": 0.2875144657322875,
        "step": 2236
    },
    {
        "loss": 2.3405,
        "grad_norm": 1.5492098331451416,
        "learning_rate": 0.0001622132699325512,
        "epoch": 0.28764305001928764,
        "step": 2237
    },
    {
        "loss": 1.9216,
        "grad_norm": 2.0453193187713623,
        "learning_rate": 0.00016218157725723966,
        "epoch": 0.28777163430628777,
        "step": 2238
    },
    {
        "loss": 2.0618,
        "grad_norm": 1.3485400676727295,
        "learning_rate": 0.00016214987439568126,
        "epoch": 0.2879002185932879,
        "step": 2239
    },
    {
        "loss": 2.2905,
        "grad_norm": 1.5224499702453613,
        "learning_rate": 0.00016211816135306938,
        "epoch": 0.28802880288028804,
        "step": 2240
    },
    {
        "loss": 2.4514,
        "grad_norm": 1.2052408456802368,
        "learning_rate": 0.00016208643813459914,
        "epoch": 0.28815738716728817,
        "step": 2241
    },
    {
        "loss": 1.0997,
        "grad_norm": 1.9391947984695435,
        "learning_rate": 0.0001620547047454672,
        "epoch": 0.2882859714542883,
        "step": 2242
    },
    {
        "loss": 2.2102,
        "grad_norm": 2.4430859088897705,
        "learning_rate": 0.00016202296119087201,
        "epoch": 0.28841455574128844,
        "step": 2243
    },
    {
        "loss": 2.5142,
        "grad_norm": 2.3473379611968994,
        "learning_rate": 0.00016199120747601356,
        "epoch": 0.28854314002828857,
        "step": 2244
    },
    {
        "loss": 2.0225,
        "grad_norm": 2.5097107887268066,
        "learning_rate": 0.00016195944360609358,
        "epoch": 0.28867172431528865,
        "step": 2245
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.7321321964263916,
        "learning_rate": 0.00016192766958631548,
        "epoch": 0.2888003086022888,
        "step": 2246
    },
    {
        "loss": 1.4513,
        "grad_norm": 1.9088128805160522,
        "learning_rate": 0.0001618958854218843,
        "epoch": 0.2889288928892889,
        "step": 2247
    },
    {
        "loss": 2.533,
        "grad_norm": 1.2109529972076416,
        "learning_rate": 0.0001618640911180068,
        "epoch": 0.28905747717628905,
        "step": 2248
    },
    {
        "loss": 1.7375,
        "grad_norm": 2.188202142715454,
        "learning_rate": 0.00016183228667989124,
        "epoch": 0.2891860614632892,
        "step": 2249
    },
    {
        "loss": 2.1065,
        "grad_norm": 1.8754814863204956,
        "learning_rate": 0.00016180047211274774,
        "epoch": 0.2893146457502893,
        "step": 2250
    },
    {
        "loss": 1.3786,
        "grad_norm": 1.4645826816558838,
        "learning_rate": 0.00016176864742178793,
        "epoch": 0.28944323003728945,
        "step": 2251
    },
    {
        "loss": 1.9522,
        "grad_norm": 1.7266087532043457,
        "learning_rate": 0.00016173681261222517,
        "epoch": 0.2895718143242896,
        "step": 2252
    },
    {
        "loss": 1.7539,
        "grad_norm": 1.6914623975753784,
        "learning_rate": 0.00016170496768927453,
        "epoch": 0.2897003986112897,
        "step": 2253
    },
    {
        "loss": 1.9548,
        "grad_norm": 1.580007553100586,
        "learning_rate": 0.00016167311265815256,
        "epoch": 0.28982898289828984,
        "step": 2254
    },
    {
        "loss": 2.1174,
        "grad_norm": 2.0854244232177734,
        "learning_rate": 0.00016164124752407767,
        "epoch": 0.28995756718529,
        "step": 2255
    },
    {
        "loss": 1.6981,
        "grad_norm": 1.7522658109664917,
        "learning_rate": 0.0001616093722922698,
        "epoch": 0.2900861514722901,
        "step": 2256
    },
    {
        "loss": 2.0318,
        "grad_norm": 1.9855834245681763,
        "learning_rate": 0.00016157748696795057,
        "epoch": 0.2902147357592902,
        "step": 2257
    },
    {
        "loss": 1.8648,
        "grad_norm": 1.5679386854171753,
        "learning_rate": 0.00016154559155634327,
        "epoch": 0.2903433200462903,
        "step": 2258
    },
    {
        "loss": 2.1047,
        "grad_norm": 2.4646506309509277,
        "learning_rate": 0.00016151368606267283,
        "epoch": 0.29047190433329045,
        "step": 2259
    },
    {
        "loss": 1.4247,
        "grad_norm": 2.2689931392669678,
        "learning_rate": 0.00016148177049216583,
        "epoch": 0.2906004886202906,
        "step": 2260
    },
    {
        "loss": 2.0977,
        "grad_norm": 1.7265918254852295,
        "learning_rate": 0.0001614498448500505,
        "epoch": 0.2907290729072907,
        "step": 2261
    },
    {
        "loss": 0.9616,
        "grad_norm": 1.9038774967193604,
        "learning_rate": 0.00016141790914155673,
        "epoch": 0.29085765719429085,
        "step": 2262
    },
    {
        "loss": 2.029,
        "grad_norm": 1.7124950885772705,
        "learning_rate": 0.00016138596337191607,
        "epoch": 0.290986241481291,
        "step": 2263
    },
    {
        "loss": 1.5397,
        "grad_norm": 2.2801642417907715,
        "learning_rate": 0.00016135400754636168,
        "epoch": 0.2911148257682911,
        "step": 2264
    },
    {
        "loss": 2.201,
        "grad_norm": 1.2142324447631836,
        "learning_rate": 0.00016132204167012838,
        "epoch": 0.29124341005529125,
        "step": 2265
    },
    {
        "loss": 1.7202,
        "grad_norm": 2.0476491451263428,
        "learning_rate": 0.00016129006574845266,
        "epoch": 0.2913719943422914,
        "step": 2266
    },
    {
        "loss": 2.0535,
        "grad_norm": 1.6511098146438599,
        "learning_rate": 0.00016125807978657267,
        "epoch": 0.2915005786292915,
        "step": 2267
    },
    {
        "loss": 1.7118,
        "grad_norm": 1.94663667678833,
        "learning_rate": 0.00016122608378972813,
        "epoch": 0.29162916291629165,
        "step": 2268
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.076929807662964,
        "learning_rate": 0.00016119407776316052,
        "epoch": 0.2917577472032918,
        "step": 2269
    },
    {
        "loss": 2.0102,
        "grad_norm": 1.7055659294128418,
        "learning_rate": 0.00016116206171211278,
        "epoch": 0.29188633149029186,
        "step": 2270
    },
    {
        "loss": 2.139,
        "grad_norm": 1.57645583152771,
        "learning_rate": 0.00016113003564182968,
        "epoch": 0.292014915777292,
        "step": 2271
    },
    {
        "loss": 2.2826,
        "grad_norm": 1.7173099517822266,
        "learning_rate": 0.00016109799955755754,
        "epoch": 0.2921435000642921,
        "step": 2272
    },
    {
        "loss": 2.6151,
        "grad_norm": 2.080030679702759,
        "learning_rate": 0.00016106595346454435,
        "epoch": 0.29227208435129226,
        "step": 2273
    },
    {
        "loss": 1.9747,
        "grad_norm": 1.5286177396774292,
        "learning_rate": 0.0001610338973680397,
        "epoch": 0.2924006686382924,
        "step": 2274
    },
    {
        "loss": 1.3241,
        "grad_norm": 1.9400252103805542,
        "learning_rate": 0.00016100183127329485,
        "epoch": 0.2925292529252925,
        "step": 2275
    },
    {
        "loss": 1.9418,
        "grad_norm": 1.712860107421875,
        "learning_rate": 0.00016096975518556275,
        "epoch": 0.29265783721229266,
        "step": 2276
    },
    {
        "loss": 1.9601,
        "grad_norm": 1.500316858291626,
        "learning_rate": 0.00016093766911009783,
        "epoch": 0.2927864214992928,
        "step": 2277
    },
    {
        "loss": 1.7209,
        "grad_norm": 1.592856764793396,
        "learning_rate": 0.00016090557305215635,
        "epoch": 0.2929150057862929,
        "step": 2278
    },
    {
        "loss": 2.1915,
        "grad_norm": 1.3748021125793457,
        "learning_rate": 0.00016087346701699606,
        "epoch": 0.29304359007329306,
        "step": 2279
    },
    {
        "loss": 1.5698,
        "grad_norm": 2.295797348022461,
        "learning_rate": 0.00016084135100987638,
        "epoch": 0.2931721743602932,
        "step": 2280
    },
    {
        "loss": 2.3534,
        "grad_norm": 1.8871346712112427,
        "learning_rate": 0.00016080922503605843,
        "epoch": 0.2933007586472933,
        "step": 2281
    },
    {
        "loss": 2.174,
        "grad_norm": 1.3165781497955322,
        "learning_rate": 0.00016077708910080485,
        "epoch": 0.2934293429342934,
        "step": 2282
    },
    {
        "loss": 1.873,
        "grad_norm": 2.195223093032837,
        "learning_rate": 0.00016074494320938003,
        "epoch": 0.29355792722129354,
        "step": 2283
    },
    {
        "loss": 1.8004,
        "grad_norm": 2.1091372966766357,
        "learning_rate": 0.00016071278736704993,
        "epoch": 0.29368651150829367,
        "step": 2284
    },
    {
        "loss": 1.7936,
        "grad_norm": 2.182521104812622,
        "learning_rate": 0.00016068062157908214,
        "epoch": 0.2938150957952938,
        "step": 2285
    },
    {
        "loss": 1.9212,
        "grad_norm": 1.802196979522705,
        "learning_rate": 0.00016064844585074583,
        "epoch": 0.29394368008229393,
        "step": 2286
    },
    {
        "loss": 2.0983,
        "grad_norm": 1.885918378829956,
        "learning_rate": 0.00016061626018731192,
        "epoch": 0.29407226436929407,
        "step": 2287
    },
    {
        "loss": 1.6397,
        "grad_norm": 2.9809703826904297,
        "learning_rate": 0.00016058406459405284,
        "epoch": 0.2942008486562942,
        "step": 2288
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.7794075012207031,
        "learning_rate": 0.00016055185907624275,
        "epoch": 0.29432943294329433,
        "step": 2289
    },
    {
        "loss": 2.5366,
        "grad_norm": 1.11891508102417,
        "learning_rate": 0.00016051964363915735,
        "epoch": 0.29445801723029447,
        "step": 2290
    },
    {
        "loss": 1.4402,
        "grad_norm": 2.4817252159118652,
        "learning_rate": 0.00016048741828807397,
        "epoch": 0.2945866015172946,
        "step": 2291
    },
    {
        "loss": 2.0434,
        "grad_norm": 2.0610718727111816,
        "learning_rate": 0.00016045518302827168,
        "epoch": 0.29471518580429473,
        "step": 2292
    },
    {
        "loss": 2.2444,
        "grad_norm": 1.37404203414917,
        "learning_rate": 0.000160422937865031,
        "epoch": 0.29484377009129487,
        "step": 2293
    },
    {
        "loss": 1.5607,
        "grad_norm": 1.9521008729934692,
        "learning_rate": 0.00016039068280363416,
        "epoch": 0.294972354378295,
        "step": 2294
    },
    {
        "loss": 1.1178,
        "grad_norm": 1.8890602588653564,
        "learning_rate": 0.00016035841784936504,
        "epoch": 0.2951009386652951,
        "step": 2295
    },
    {
        "loss": 2.2673,
        "grad_norm": 1.370155930519104,
        "learning_rate": 0.0001603261430075091,
        "epoch": 0.2952295229522952,
        "step": 2296
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.9793423414230347,
        "learning_rate": 0.00016029385828335342,
        "epoch": 0.29535810723929534,
        "step": 2297
    },
    {
        "loss": 2.0434,
        "grad_norm": 2.356431245803833,
        "learning_rate": 0.00016026156368218673,
        "epoch": 0.2954866915262955,
        "step": 2298
    },
    {
        "loss": 2.0207,
        "grad_norm": 2.027590036392212,
        "learning_rate": 0.00016022925920929936,
        "epoch": 0.2956152758132956,
        "step": 2299
    },
    {
        "loss": 2.44,
        "grad_norm": 1.7879191637039185,
        "learning_rate": 0.00016019694486998324,
        "epoch": 0.29574386010029574,
        "step": 2300
    },
    {
        "eval_loss": 1.965776801109314,
        "eval_runtime": 28.151,
        "eval_samples_per_second": 2.806,
        "eval_steps_per_second": 2.806,
        "epoch": 0.29574386010029574,
        "step": 2300
    },
    {
        "loss": 1.3897,
        "grad_norm": 2.143695116043091,
        "learning_rate": 0.00016016462066953188,
        "epoch": 0.2958724443872959,
        "step": 2301
    },
    {
        "loss": 1.6148,
        "grad_norm": 1.6888105869293213,
        "learning_rate": 0.00016013228661324052,
        "epoch": 0.296001028674296,
        "step": 2302
    },
    {
        "loss": 1.1751,
        "grad_norm": 1.9695218801498413,
        "learning_rate": 0.00016009994270640594,
        "epoch": 0.29612961296129614,
        "step": 2303
    },
    {
        "loss": 1.7315,
        "grad_norm": 1.77499258518219,
        "learning_rate": 0.0001600675889543265,
        "epoch": 0.2962581972482963,
        "step": 2304
    },
    {
        "loss": 2.0153,
        "grad_norm": 1.6925221681594849,
        "learning_rate": 0.00016003522536230227,
        "epoch": 0.2963867815352964,
        "step": 2305
    },
    {
        "loss": 1.0277,
        "grad_norm": 1.6482455730438232,
        "learning_rate": 0.00016000285193563485,
        "epoch": 0.29651536582229654,
        "step": 2306
    },
    {
        "loss": 2.1779,
        "grad_norm": 1.5573132038116455,
        "learning_rate": 0.00015997046867962747,
        "epoch": 0.2966439501092966,
        "step": 2307
    },
    {
        "loss": 0.8689,
        "grad_norm": 1.908359169960022,
        "learning_rate": 0.00015993807559958495,
        "epoch": 0.29677253439629675,
        "step": 2308
    },
    {
        "loss": 1.831,
        "grad_norm": 2.3930490016937256,
        "learning_rate": 0.00015990567270081382,
        "epoch": 0.2969011186832969,
        "step": 2309
    },
    {
        "loss": 2.2547,
        "grad_norm": 2.0545427799224854,
        "learning_rate": 0.00015987325998862215,
        "epoch": 0.297029702970297,
        "step": 2310
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.4235477447509766,
        "learning_rate": 0.00015984083746831953,
        "epoch": 0.29715828725729715,
        "step": 2311
    },
    {
        "loss": 1.5424,
        "grad_norm": 2.033903121948242,
        "learning_rate": 0.0001598084051452173,
        "epoch": 0.2972868715442973,
        "step": 2312
    },
    {
        "loss": 1.6159,
        "grad_norm": 2.156837224960327,
        "learning_rate": 0.00015977596302462833,
        "epoch": 0.2974154558312974,
        "step": 2313
    },
    {
        "loss": 1.7744,
        "grad_norm": 1.6281903982162476,
        "learning_rate": 0.00015974351111186708,
        "epoch": 0.29754404011829755,
        "step": 2314
    },
    {
        "loss": 1.3207,
        "grad_norm": 2.6596486568450928,
        "learning_rate": 0.00015971104941224973,
        "epoch": 0.2976726244052977,
        "step": 2315
    },
    {
        "loss": 1.6862,
        "grad_norm": 1.2563532590866089,
        "learning_rate": 0.0001596785779310939,
        "epoch": 0.2978012086922978,
        "step": 2316
    },
    {
        "loss": 2.3298,
        "grad_norm": 1.4133315086364746,
        "learning_rate": 0.0001596460966737189,
        "epoch": 0.29792979297929795,
        "step": 2317
    },
    {
        "loss": 1.8668,
        "grad_norm": 2.301574468612671,
        "learning_rate": 0.00015961360564544568,
        "epoch": 0.2980583772662981,
        "step": 2318
    },
    {
        "loss": 1.3265,
        "grad_norm": 2.724513292312622,
        "learning_rate": 0.0001595811048515967,
        "epoch": 0.2981869615532982,
        "step": 2319
    },
    {
        "loss": 1.2078,
        "grad_norm": 1.3125511407852173,
        "learning_rate": 0.0001595485942974961,
        "epoch": 0.2983155458402983,
        "step": 2320
    },
    {
        "loss": 2.4521,
        "grad_norm": 1.0110365152359009,
        "learning_rate": 0.00015951607398846952,
        "epoch": 0.2984441301272984,
        "step": 2321
    },
    {
        "loss": 1.8975,
        "grad_norm": 1.7707457542419434,
        "learning_rate": 0.0001594835439298443,
        "epoch": 0.29857271441429856,
        "step": 2322
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.3221527338027954,
        "learning_rate": 0.00015945100412694937,
        "epoch": 0.2987012987012987,
        "step": 2323
    },
    {
        "loss": 1.5578,
        "grad_norm": 1.49241304397583,
        "learning_rate": 0.00015941845458511517,
        "epoch": 0.2988298829882988,
        "step": 2324
    },
    {
        "loss": 1.3994,
        "grad_norm": 2.1344075202941895,
        "learning_rate": 0.00015938589530967382,
        "epoch": 0.29895846727529896,
        "step": 2325
    },
    {
        "loss": 1.642,
        "grad_norm": 2.243767738342285,
        "learning_rate": 0.00015935332630595897,
        "epoch": 0.2990870515622991,
        "step": 2326
    },
    {
        "loss": 1.4865,
        "grad_norm": 1.22306489944458,
        "learning_rate": 0.0001593207475793059,
        "epoch": 0.2992156358492992,
        "step": 2327
    },
    {
        "loss": 1.5453,
        "grad_norm": 2.0514841079711914,
        "learning_rate": 0.00015928815913505151,
        "epoch": 0.29934422013629935,
        "step": 2328
    },
    {
        "loss": 0.9029,
        "grad_norm": 2.4700090885162354,
        "learning_rate": 0.00015925556097853426,
        "epoch": 0.2994728044232995,
        "step": 2329
    },
    {
        "loss": 1.7621,
        "grad_norm": 1.344319462776184,
        "learning_rate": 0.00015922295311509416,
        "epoch": 0.2996013887102996,
        "step": 2330
    },
    {
        "loss": 2.0595,
        "grad_norm": 1.3323893547058105,
        "learning_rate": 0.00015919033555007292,
        "epoch": 0.29972997299729975,
        "step": 2331
    },
    {
        "loss": 1.4907,
        "grad_norm": 4.753857612609863,
        "learning_rate": 0.0001591577082888137,
        "epoch": 0.29985855728429983,
        "step": 2332
    },
    {
        "loss": 1.5433,
        "grad_norm": 1.9580940008163452,
        "learning_rate": 0.00015912507133666136,
        "epoch": 0.29998714157129996,
        "step": 2333
    },
    {
        "loss": 2.1437,
        "grad_norm": 1.7764288187026978,
        "learning_rate": 0.00015909242469896228,
        "epoch": 0.3001157258583001,
        "step": 2334
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.2871757745742798,
        "learning_rate": 0.0001590597683810645,
        "epoch": 0.30024431014530023,
        "step": 2335
    },
    {
        "loss": 1.8381,
        "grad_norm": 2.106499671936035,
        "learning_rate": 0.00015902710238831759,
        "epoch": 0.30037289443230036,
        "step": 2336
    },
    {
        "loss": 2.1158,
        "grad_norm": 2.212214946746826,
        "learning_rate": 0.00015899442672607267,
        "epoch": 0.3005014787193005,
        "step": 2337
    },
    {
        "loss": 2.0499,
        "grad_norm": 2.0922696590423584,
        "learning_rate": 0.0001589617413996825,
        "epoch": 0.30063006300630063,
        "step": 2338
    },
    {
        "loss": 2.233,
        "grad_norm": 1.3307572603225708,
        "learning_rate": 0.00015892904641450146,
        "epoch": 0.30075864729330076,
        "step": 2339
    },
    {
        "loss": 1.6151,
        "grad_norm": 1.9343384504318237,
        "learning_rate": 0.0001588963417758854,
        "epoch": 0.3008872315803009,
        "step": 2340
    },
    {
        "loss": 1.8732,
        "grad_norm": 1.7686344385147095,
        "learning_rate": 0.00015886362748919187,
        "epoch": 0.30101581586730103,
        "step": 2341
    },
    {
        "loss": 1.8125,
        "grad_norm": 2.1579277515411377,
        "learning_rate": 0.00015883090355977997,
        "epoch": 0.30114440015430116,
        "step": 2342
    },
    {
        "loss": 2.1501,
        "grad_norm": 1.7067148685455322,
        "learning_rate": 0.00015879816999301023,
        "epoch": 0.3012729844413013,
        "step": 2343
    },
    {
        "loss": 2.0411,
        "grad_norm": 2.0647382736206055,
        "learning_rate": 0.00015876542679424502,
        "epoch": 0.30140156872830143,
        "step": 2344
    },
    {
        "loss": 2.6082,
        "grad_norm": 1.4623360633850098,
        "learning_rate": 0.00015873267396884807,
        "epoch": 0.3015301530153015,
        "step": 2345
    },
    {
        "loss": 1.7133,
        "grad_norm": 1.2701584100723267,
        "learning_rate": 0.00015869991152218482,
        "epoch": 0.30165873730230164,
        "step": 2346
    },
    {
        "loss": 1.8408,
        "grad_norm": 1.5942316055297852,
        "learning_rate": 0.00015866713945962219,
        "epoch": 0.30178732158930177,
        "step": 2347
    },
    {
        "loss": 2.3884,
        "grad_norm": 1.8833887577056885,
        "learning_rate": 0.00015863435778652876,
        "epoch": 0.3019159058763019,
        "step": 2348
    },
    {
        "loss": 1.7623,
        "grad_norm": 1.6244350671768188,
        "learning_rate": 0.0001586015665082746,
        "epoch": 0.30204449016330204,
        "step": 2349
    },
    {
        "loss": 1.7585,
        "grad_norm": 2.0821070671081543,
        "learning_rate": 0.00015856876563023143,
        "epoch": 0.30217307445030217,
        "step": 2350
    },
    {
        "loss": 1.7751,
        "grad_norm": 1.7940099239349365,
        "learning_rate": 0.00015853595515777252,
        "epoch": 0.3023016587373023,
        "step": 2351
    },
    {
        "loss": 1.5141,
        "grad_norm": 1.6635559797286987,
        "learning_rate": 0.0001585031350962727,
        "epoch": 0.30243024302430244,
        "step": 2352
    },
    {
        "loss": 1.2938,
        "grad_norm": 1.8231662511825562,
        "learning_rate": 0.00015847030545110835,
        "epoch": 0.30255882731130257,
        "step": 2353
    },
    {
        "loss": 1.4471,
        "grad_norm": 1.8800122737884521,
        "learning_rate": 0.00015843746622765746,
        "epoch": 0.3026874115983027,
        "step": 2354
    },
    {
        "loss": 2.0856,
        "grad_norm": 1.1551324129104614,
        "learning_rate": 0.00015840461743129954,
        "epoch": 0.30281599588530284,
        "step": 2355
    },
    {
        "loss": 1.5841,
        "grad_norm": 1.4995414018630981,
        "learning_rate": 0.00015837175906741575,
        "epoch": 0.30294458017230297,
        "step": 2356
    },
    {
        "loss": 1.1673,
        "grad_norm": 2.457878351211548,
        "learning_rate": 0.00015833889114138876,
        "epoch": 0.30307316445930305,
        "step": 2357
    },
    {
        "loss": 1.6501,
        "grad_norm": 1.5026174783706665,
        "learning_rate": 0.0001583060136586028,
        "epoch": 0.3032017487463032,
        "step": 2358
    },
    {
        "loss": 1.4739,
        "grad_norm": 2.1786322593688965,
        "learning_rate": 0.00015827312662444366,
        "epoch": 0.3033303330333033,
        "step": 2359
    },
    {
        "loss": 1.9208,
        "grad_norm": 1.8387371301651,
        "learning_rate": 0.00015824023004429874,
        "epoch": 0.30345891732030345,
        "step": 2360
    },
    {
        "loss": 1.2906,
        "grad_norm": 2.7914514541625977,
        "learning_rate": 0.00015820732392355698,
        "epoch": 0.3035875016073036,
        "step": 2361
    },
    {
        "loss": 1.8425,
        "grad_norm": 2.7130236625671387,
        "learning_rate": 0.00015817440826760887,
        "epoch": 0.3037160858943037,
        "step": 2362
    },
    {
        "loss": 1.2548,
        "grad_norm": 2.6745166778564453,
        "learning_rate": 0.0001581414830818465,
        "epoch": 0.30384467018130384,
        "step": 2363
    },
    {
        "loss": 1.8651,
        "grad_norm": 2.488881826400757,
        "learning_rate": 0.0001581085483716634,
        "epoch": 0.303973254468304,
        "step": 2364
    },
    {
        "loss": 2.3515,
        "grad_norm": 1.8680049180984497,
        "learning_rate": 0.00015807560414245493,
        "epoch": 0.3041018387553041,
        "step": 2365
    },
    {
        "loss": 1.5695,
        "grad_norm": 3.190533399581909,
        "learning_rate": 0.00015804265039961768,
        "epoch": 0.30423042304230424,
        "step": 2366
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.9405230283737183,
        "learning_rate": 0.00015800968714855,
        "epoch": 0.3043590073293044,
        "step": 2367
    },
    {
        "loss": 2.0386,
        "grad_norm": 1.9108326435089111,
        "learning_rate": 0.00015797671439465177,
        "epoch": 0.3044875916163045,
        "step": 2368
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.2927570343017578,
        "learning_rate": 0.0001579437321433244,
        "epoch": 0.30461617590330464,
        "step": 2369
    },
    {
        "loss": 2.2879,
        "grad_norm": 2.0437941551208496,
        "learning_rate": 0.00015791074039997084,
        "epoch": 0.3047447601903047,
        "step": 2370
    },
    {
        "loss": 1.7343,
        "grad_norm": 2.0898385047912598,
        "learning_rate": 0.00015787773916999562,
        "epoch": 0.30487334447730485,
        "step": 2371
    },
    {
        "loss": 2.1591,
        "grad_norm": 2.014307737350464,
        "learning_rate": 0.0001578447284588048,
        "epoch": 0.305001928764305,
        "step": 2372
    },
    {
        "loss": 1.1274,
        "grad_norm": 1.5546035766601562,
        "learning_rate": 0.0001578117082718061,
        "epoch": 0.3051305130513051,
        "step": 2373
    },
    {
        "loss": 1.3219,
        "grad_norm": 1.2222471237182617,
        "learning_rate": 0.00015777867861440864,
        "epoch": 0.30525909733830525,
        "step": 2374
    },
    {
        "loss": 1.7132,
        "grad_norm": 1.7288461923599243,
        "learning_rate": 0.00015774563949202318,
        "epoch": 0.3053876816253054,
        "step": 2375
    },
    {
        "loss": 1.6359,
        "grad_norm": 3.1258487701416016,
        "learning_rate": 0.000157712590910062,
        "epoch": 0.3055162659123055,
        "step": 2376
    },
    {
        "loss": 0.9741,
        "grad_norm": 2.023691177368164,
        "learning_rate": 0.00015767953287393894,
        "epoch": 0.30564485019930565,
        "step": 2377
    },
    {
        "loss": 2.3872,
        "grad_norm": 1.5153841972351074,
        "learning_rate": 0.00015764646538906938,
        "epoch": 0.3057734344863058,
        "step": 2378
    },
    {
        "loss": 2.3745,
        "grad_norm": 1.849401593208313,
        "learning_rate": 0.00015761338846087024,
        "epoch": 0.3059020187733059,
        "step": 2379
    },
    {
        "loss": 2.1248,
        "grad_norm": 1.0913175344467163,
        "learning_rate": 0.00015758030209476003,
        "epoch": 0.30603060306030605,
        "step": 2380
    },
    {
        "loss": 1.8991,
        "grad_norm": 1.418927788734436,
        "learning_rate": 0.0001575472062961588,
        "epoch": 0.3061591873473062,
        "step": 2381
    },
    {
        "loss": 1.844,
        "grad_norm": 1.7334908246994019,
        "learning_rate": 0.00015751410107048806,
        "epoch": 0.30628777163430626,
        "step": 2382
    },
    {
        "loss": 1.9133,
        "grad_norm": 1.7932015657424927,
        "learning_rate": 0.00015748098642317103,
        "epoch": 0.3064163559213064,
        "step": 2383
    },
    {
        "loss": 1.9058,
        "grad_norm": 1.994122862815857,
        "learning_rate": 0.00015744786235963223,
        "epoch": 0.3065449402083065,
        "step": 2384
    },
    {
        "loss": 2.4104,
        "grad_norm": 1.2781951427459717,
        "learning_rate": 0.00015741472888529796,
        "epoch": 0.30667352449530666,
        "step": 2385
    },
    {
        "loss": 1.8731,
        "grad_norm": 1.835879921913147,
        "learning_rate": 0.00015738158600559594,
        "epoch": 0.3068021087823068,
        "step": 2386
    },
    {
        "loss": 2.1652,
        "grad_norm": 1.7174148559570312,
        "learning_rate": 0.00015734843372595546,
        "epoch": 0.3069306930693069,
        "step": 2387
    },
    {
        "loss": 1.9569,
        "grad_norm": 1.5996259450912476,
        "learning_rate": 0.00015731527205180737,
        "epoch": 0.30705927735630706,
        "step": 2388
    },
    {
        "loss": 2.1299,
        "grad_norm": 1.7962448596954346,
        "learning_rate": 0.00015728210098858397,
        "epoch": 0.3071878616433072,
        "step": 2389
    },
    {
        "loss": 1.6283,
        "grad_norm": 2.08249831199646,
        "learning_rate": 0.0001572489205417192,
        "epoch": 0.3073164459303073,
        "step": 2390
    },
    {
        "loss": 2.491,
        "grad_norm": 1.8854222297668457,
        "learning_rate": 0.0001572157307166485,
        "epoch": 0.30744503021730746,
        "step": 2391
    },
    {
        "loss": 2.2907,
        "grad_norm": 1.281688928604126,
        "learning_rate": 0.00015718253151880885,
        "epoch": 0.3075736145043076,
        "step": 2392
    },
    {
        "loss": 2.4819,
        "grad_norm": 1.0032801628112793,
        "learning_rate": 0.00015714932295363878,
        "epoch": 0.3077021987913077,
        "step": 2393
    },
    {
        "loss": 1.1865,
        "grad_norm": 1.9980568885803223,
        "learning_rate": 0.00015711610502657831,
        "epoch": 0.30783078307830786,
        "step": 2394
    },
    {
        "loss": 2.0509,
        "grad_norm": 2.1199333667755127,
        "learning_rate": 0.00015708287774306902,
        "epoch": 0.30795936736530793,
        "step": 2395
    },
    {
        "loss": 1.7062,
        "grad_norm": 1.8058555126190186,
        "learning_rate": 0.00015704964110855403,
        "epoch": 0.30808795165230807,
        "step": 2396
    },
    {
        "loss": 2.2297,
        "grad_norm": 1.6022707223892212,
        "learning_rate": 0.00015701639512847795,
        "epoch": 0.3082165359393082,
        "step": 2397
    },
    {
        "loss": 1.5471,
        "grad_norm": 1.517785906791687,
        "learning_rate": 0.00015698313980828702,
        "epoch": 0.30834512022630833,
        "step": 2398
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.485195517539978,
        "learning_rate": 0.0001569498751534289,
        "epoch": 0.30847370451330847,
        "step": 2399
    },
    {
        "loss": 1.98,
        "grad_norm": 2.4194345474243164,
        "learning_rate": 0.0001569166011693528,
        "epoch": 0.3086022888003086,
        "step": 2400
    },
    {
        "eval_loss": 1.956067442893982,
        "eval_runtime": 28.2154,
        "eval_samples_per_second": 2.8,
        "eval_steps_per_second": 2.8,
        "epoch": 0.3086022888003086,
        "step": 2400
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.1368556022644043,
        "learning_rate": 0.00015688331786150954,
        "epoch": 0.30873087308730873,
        "step": 2401
    },
    {
        "loss": 1.7492,
        "grad_norm": 1.7945270538330078,
        "learning_rate": 0.0001568500252353514,
        "epoch": 0.30885945737430887,
        "step": 2402
    },
    {
        "loss": 2.0776,
        "grad_norm": 1.9734899997711182,
        "learning_rate": 0.0001568167232963322,
        "epoch": 0.308988041661309,
        "step": 2403
    },
    {
        "loss": 1.9387,
        "grad_norm": 2.6144204139709473,
        "learning_rate": 0.00015678341204990726,
        "epoch": 0.30911662594830913,
        "step": 2404
    },
    {
        "loss": 2.1225,
        "grad_norm": 1.3187693357467651,
        "learning_rate": 0.00015675009150153342,
        "epoch": 0.30924521023530926,
        "step": 2405
    },
    {
        "loss": 2.5164,
        "grad_norm": 1.6357223987579346,
        "learning_rate": 0.0001567167616566691,
        "epoch": 0.3093737945223094,
        "step": 2406
    },
    {
        "loss": 1.6571,
        "grad_norm": 1.8021714687347412,
        "learning_rate": 0.00015668342252077425,
        "epoch": 0.3095023788093095,
        "step": 2407
    },
    {
        "loss": 1.9642,
        "grad_norm": 1.7980215549468994,
        "learning_rate": 0.00015665007409931025,
        "epoch": 0.3096309630963096,
        "step": 2408
    },
    {
        "loss": 1.9365,
        "grad_norm": 2.3869099617004395,
        "learning_rate": 0.0001566167163977401,
        "epoch": 0.30975954738330974,
        "step": 2409
    },
    {
        "loss": 1.1592,
        "grad_norm": 1.6748192310333252,
        "learning_rate": 0.0001565833494215282,
        "epoch": 0.3098881316703099,
        "step": 2410
    },
    {
        "loss": 2.0182,
        "grad_norm": 2.7399375438690186,
        "learning_rate": 0.00015654997317614066,
        "epoch": 0.31001671595731,
        "step": 2411
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.842114806175232,
        "learning_rate": 0.0001565165876670449,
        "epoch": 0.31014530024431014,
        "step": 2412
    },
    {
        "loss": 1.8534,
        "grad_norm": 1.5530145168304443,
        "learning_rate": 0.00015648319289971003,
        "epoch": 0.3102738845313103,
        "step": 2413
    },
    {
        "loss": 1.5094,
        "grad_norm": 1.9490535259246826,
        "learning_rate": 0.00015644978887960654,
        "epoch": 0.3104024688183104,
        "step": 2414
    },
    {
        "loss": 1.2104,
        "grad_norm": 1.7755751609802246,
        "learning_rate": 0.0001564163756122065,
        "epoch": 0.31053105310531054,
        "step": 2415
    },
    {
        "loss": 2.2037,
        "grad_norm": 1.8266774415969849,
        "learning_rate": 0.0001563829531029835,
        "epoch": 0.3106596373923107,
        "step": 2416
    },
    {
        "loss": 1.1995,
        "grad_norm": 2.43649959564209,
        "learning_rate": 0.00015634952135741264,
        "epoch": 0.3107882216793108,
        "step": 2417
    },
    {
        "loss": 2.4188,
        "grad_norm": 1.531314730644226,
        "learning_rate": 0.0001563160803809705,
        "epoch": 0.31091680596631094,
        "step": 2418
    },
    {
        "loss": 1.8469,
        "grad_norm": 2.326936960220337,
        "learning_rate": 0.0001562826301791352,
        "epoch": 0.31104539025331107,
        "step": 2419
    },
    {
        "loss": 1.8811,
        "grad_norm": 1.5279245376586914,
        "learning_rate": 0.00015624917075738646,
        "epoch": 0.31117397454031115,
        "step": 2420
    },
    {
        "loss": 2.0497,
        "grad_norm": 1.6282585859298706,
        "learning_rate": 0.0001562157021212053,
        "epoch": 0.3113025588273113,
        "step": 2421
    },
    {
        "loss": 1.2042,
        "grad_norm": 2.1677842140197754,
        "learning_rate": 0.00015618222427607443,
        "epoch": 0.3114311431143114,
        "step": 2422
    },
    {
        "loss": 1.42,
        "grad_norm": 2.4131557941436768,
        "learning_rate": 0.00015614873722747798,
        "epoch": 0.31155972740131155,
        "step": 2423
    },
    {
        "loss": 2.1483,
        "grad_norm": 1.496946930885315,
        "learning_rate": 0.00015611524098090165,
        "epoch": 0.3116883116883117,
        "step": 2424
    },
    {
        "loss": 1.9329,
        "grad_norm": 1.27469003200531,
        "learning_rate": 0.00015608173554183257,
        "epoch": 0.3118168959753118,
        "step": 2425
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.2416231632232666,
        "learning_rate": 0.00015604822091575945,
        "epoch": 0.31194548026231195,
        "step": 2426
    },
    {
        "loss": 1.5575,
        "grad_norm": 2.3952746391296387,
        "learning_rate": 0.00015601469710817246,
        "epoch": 0.3120740645493121,
        "step": 2427
    },
    {
        "loss": 1.9052,
        "grad_norm": 1.9543874263763428,
        "learning_rate": 0.00015598116412456333,
        "epoch": 0.3122026488363122,
        "step": 2428
    },
    {
        "loss": 2.1844,
        "grad_norm": 1.6345754861831665,
        "learning_rate": 0.00015594762197042517,
        "epoch": 0.31233123312331235,
        "step": 2429
    },
    {
        "loss": 2.2211,
        "grad_norm": 1.4243464469909668,
        "learning_rate": 0.00015591407065125272,
        "epoch": 0.3124598174103125,
        "step": 2430
    },
    {
        "loss": 2.4359,
        "grad_norm": 1.8313485383987427,
        "learning_rate": 0.00015588051017254218,
        "epoch": 0.3125884016973126,
        "step": 2431
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.9566971063613892,
        "learning_rate": 0.00015584694053979124,
        "epoch": 0.3127169859843127,
        "step": 2432
    },
    {
        "loss": 1.7613,
        "grad_norm": 1.6611756086349487,
        "learning_rate": 0.00015581336175849906,
        "epoch": 0.3128455702713128,
        "step": 2433
    },
    {
        "loss": 1.349,
        "grad_norm": 1.0429319143295288,
        "learning_rate": 0.00015577977383416638,
        "epoch": 0.31297415455831296,
        "step": 2434
    },
    {
        "loss": 2.2617,
        "grad_norm": 1.7380770444869995,
        "learning_rate": 0.00015574617677229538,
        "epoch": 0.3131027388453131,
        "step": 2435
    },
    {
        "loss": 1.6089,
        "grad_norm": 1.788908839225769,
        "learning_rate": 0.00015571257057838973,
        "epoch": 0.3132313231323132,
        "step": 2436
    },
    {
        "loss": 2.3142,
        "grad_norm": 1.942581057548523,
        "learning_rate": 0.0001556789552579546,
        "epoch": 0.31335990741931335,
        "step": 2437
    },
    {
        "loss": 1.2882,
        "grad_norm": 1.7173166275024414,
        "learning_rate": 0.00015564533081649672,
        "epoch": 0.3134884917063135,
        "step": 2438
    },
    {
        "loss": 1.8004,
        "grad_norm": 1.946249008178711,
        "learning_rate": 0.0001556116972595242,
        "epoch": 0.3136170759933136,
        "step": 2439
    },
    {
        "loss": 1.7982,
        "grad_norm": 1.8514775037765503,
        "learning_rate": 0.00015557805459254679,
        "epoch": 0.31374566028031375,
        "step": 2440
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.7328680753707886,
        "learning_rate": 0.00015554440282107557,
        "epoch": 0.3138742445673139,
        "step": 2441
    },
    {
        "loss": 1.8405,
        "grad_norm": 1.247573733329773,
        "learning_rate": 0.0001555107419506232,
        "epoch": 0.314002828854314,
        "step": 2442
    },
    {
        "loss": 1.7678,
        "grad_norm": 2.129300355911255,
        "learning_rate": 0.00015547707198670385,
        "epoch": 0.31413141314131415,
        "step": 2443
    },
    {
        "loss": 2.178,
        "grad_norm": 1.688199520111084,
        "learning_rate": 0.00015544339293483312,
        "epoch": 0.3142599974283143,
        "step": 2444
    },
    {
        "loss": 2.0802,
        "grad_norm": 1.670066475868225,
        "learning_rate": 0.00015540970480052818,
        "epoch": 0.31438858171531436,
        "step": 2445
    },
    {
        "loss": 1.8851,
        "grad_norm": 2.295799732208252,
        "learning_rate": 0.00015537600758930758,
        "epoch": 0.3145171660023145,
        "step": 2446
    },
    {
        "loss": 1.7071,
        "grad_norm": 2.7053520679473877,
        "learning_rate": 0.00015534230130669145,
        "epoch": 0.31464575028931463,
        "step": 2447
    },
    {
        "loss": 2.1086,
        "grad_norm": 2.53973650932312,
        "learning_rate": 0.00015530858595820134,
        "epoch": 0.31477433457631476,
        "step": 2448
    },
    {
        "loss": 1.0463,
        "grad_norm": 2.926633596420288,
        "learning_rate": 0.00015527486154936034,
        "epoch": 0.3149029188633149,
        "step": 2449
    },
    {
        "loss": 2.314,
        "grad_norm": 2.5554051399230957,
        "learning_rate": 0.00015524112808569298,
        "epoch": 0.31503150315031503,
        "step": 2450
    },
    {
        "loss": 1.9135,
        "grad_norm": 1.8995773792266846,
        "learning_rate": 0.0001552073855727253,
        "epoch": 0.31516008743731516,
        "step": 2451
    },
    {
        "loss": 1.827,
        "grad_norm": 2.3976900577545166,
        "learning_rate": 0.00015517363401598483,
        "epoch": 0.3152886717243153,
        "step": 2452
    },
    {
        "loss": 1.5128,
        "grad_norm": 1.8255773782730103,
        "learning_rate": 0.0001551398734210005,
        "epoch": 0.31541725601131543,
        "step": 2453
    },
    {
        "loss": 2.0666,
        "grad_norm": 1.801780104637146,
        "learning_rate": 0.0001551061037933029,
        "epoch": 0.31554584029831556,
        "step": 2454
    },
    {
        "loss": 1.6613,
        "grad_norm": 2.196798801422119,
        "learning_rate": 0.00015507232513842388,
        "epoch": 0.3156744245853157,
        "step": 2455
    },
    {
        "loss": 1.6365,
        "grad_norm": 2.5909368991851807,
        "learning_rate": 0.00015503853746189696,
        "epoch": 0.3158030088723158,
        "step": 2456
    },
    {
        "loss": 1.8104,
        "grad_norm": 1.5526223182678223,
        "learning_rate": 0.000155004740769257,
        "epoch": 0.3159315931593159,
        "step": 2457
    },
    {
        "loss": 1.8354,
        "grad_norm": 2.41180682182312,
        "learning_rate": 0.0001549709350660404,
        "epoch": 0.31606017744631604,
        "step": 2458
    },
    {
        "loss": 1.5552,
        "grad_norm": 2.140585422515869,
        "learning_rate": 0.000154937120357785,
        "epoch": 0.31618876173331617,
        "step": 2459
    },
    {
        "loss": 2.3234,
        "grad_norm": 1.8593913316726685,
        "learning_rate": 0.00015490329665003016,
        "epoch": 0.3163173460203163,
        "step": 2460
    },
    {
        "loss": 1.703,
        "grad_norm": 1.6325452327728271,
        "learning_rate": 0.00015486946394831675,
        "epoch": 0.31644593030731644,
        "step": 2461
    },
    {
        "loss": 1.9593,
        "grad_norm": 1.7148094177246094,
        "learning_rate": 0.00015483562225818698,
        "epoch": 0.31657451459431657,
        "step": 2462
    },
    {
        "loss": 1.7079,
        "grad_norm": 2.0385708808898926,
        "learning_rate": 0.00015480177158518464,
        "epoch": 0.3167030988813167,
        "step": 2463
    },
    {
        "loss": 2.116,
        "grad_norm": 1.8280506134033203,
        "learning_rate": 0.00015476791193485499,
        "epoch": 0.31683168316831684,
        "step": 2464
    },
    {
        "loss": 1.4409,
        "grad_norm": 1.8625891208648682,
        "learning_rate": 0.0001547340433127447,
        "epoch": 0.31696026745531697,
        "step": 2465
    },
    {
        "loss": 2.0442,
        "grad_norm": 1.8057148456573486,
        "learning_rate": 0.0001547001657244019,
        "epoch": 0.3170888517423171,
        "step": 2466
    },
    {
        "loss": 2.1816,
        "grad_norm": 2.182203531265259,
        "learning_rate": 0.00015466627917537637,
        "epoch": 0.31721743602931723,
        "step": 2467
    },
    {
        "loss": 1.9446,
        "grad_norm": 2.1438653469085693,
        "learning_rate": 0.0001546323836712191,
        "epoch": 0.31734602031631737,
        "step": 2468
    },
    {
        "loss": 1.4842,
        "grad_norm": 2.683387517929077,
        "learning_rate": 0.00015459847921748269,
        "epoch": 0.3174746046033175,
        "step": 2469
    },
    {
        "loss": 1.8658,
        "grad_norm": 2.458621025085449,
        "learning_rate": 0.00015456456581972117,
        "epoch": 0.3176031888903176,
        "step": 2470
    },
    {
        "loss": 1.8861,
        "grad_norm": 1.6986966133117676,
        "learning_rate": 0.00015453064348349012,
        "epoch": 0.3177317731773177,
        "step": 2471
    },
    {
        "loss": 1.8264,
        "grad_norm": 2.6868669986724854,
        "learning_rate": 0.00015449671221434645,
        "epoch": 0.31786035746431784,
        "step": 2472
    },
    {
        "loss": 1.5661,
        "grad_norm": 1.8951674699783325,
        "learning_rate": 0.0001544627720178486,
        "epoch": 0.317988941751318,
        "step": 2473
    },
    {
        "loss": 1.8534,
        "grad_norm": 2.3769261837005615,
        "learning_rate": 0.00015442882289955646,
        "epoch": 0.3181175260383181,
        "step": 2474
    },
    {
        "loss": 2.1191,
        "grad_norm": 1.7191238403320312,
        "learning_rate": 0.0001543948648650314,
        "epoch": 0.31824611032531824,
        "step": 2475
    },
    {
        "loss": 2.1727,
        "grad_norm": 1.1939566135406494,
        "learning_rate": 0.00015436089791983628,
        "epoch": 0.3183746946123184,
        "step": 2476
    },
    {
        "loss": 2.1895,
        "grad_norm": 1.2826274633407593,
        "learning_rate": 0.00015432692206953532,
        "epoch": 0.3185032788993185,
        "step": 2477
    },
    {
        "loss": 2.4168,
        "grad_norm": 1.7020368576049805,
        "learning_rate": 0.0001542929373196943,
        "epoch": 0.31863186318631864,
        "step": 2478
    },
    {
        "loss": 1.7947,
        "grad_norm": 1.8347079753875732,
        "learning_rate": 0.00015425894367588038,
        "epoch": 0.3187604474733188,
        "step": 2479
    },
    {
        "loss": 2.0957,
        "grad_norm": 1.7482999563217163,
        "learning_rate": 0.00015422494114366225,
        "epoch": 0.3188890317603189,
        "step": 2480
    },
    {
        "loss": 1.5458,
        "grad_norm": 2.8085362911224365,
        "learning_rate": 0.00015419092972860998,
        "epoch": 0.31901761604731904,
        "step": 2481
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.5287586450576782,
        "learning_rate": 0.0001541569094362952,
        "epoch": 0.3191462003343191,
        "step": 2482
    },
    {
        "loss": 2.3619,
        "grad_norm": 1.457835078239441,
        "learning_rate": 0.00015412288027229085,
        "epoch": 0.31927478462131925,
        "step": 2483
    },
    {
        "loss": 2.305,
        "grad_norm": 1.1168386936187744,
        "learning_rate": 0.00015408884224217144,
        "epoch": 0.3194033689083194,
        "step": 2484
    },
    {
        "loss": 1.8236,
        "grad_norm": 1.7171499729156494,
        "learning_rate": 0.0001540547953515129,
        "epoch": 0.3195319531953195,
        "step": 2485
    },
    {
        "loss": 2.362,
        "grad_norm": 1.4120566844940186,
        "learning_rate": 0.00015402073960589258,
        "epoch": 0.31966053748231965,
        "step": 2486
    },
    {
        "loss": 1.649,
        "grad_norm": 2.1402761936187744,
        "learning_rate": 0.00015398667501088936,
        "epoch": 0.3197891217693198,
        "step": 2487
    },
    {
        "loss": 2.3281,
        "grad_norm": 2.1037137508392334,
        "learning_rate": 0.0001539526015720835,
        "epoch": 0.3199177060563199,
        "step": 2488
    },
    {
        "loss": 1.6643,
        "grad_norm": 1.843553066253662,
        "learning_rate": 0.00015391851929505665,
        "epoch": 0.32004629034332005,
        "step": 2489
    },
    {
        "loss": 2.0693,
        "grad_norm": 1.6527485847473145,
        "learning_rate": 0.00015388442818539206,
        "epoch": 0.3201748746303202,
        "step": 2490
    },
    {
        "loss": 2.2026,
        "grad_norm": 2.156182289123535,
        "learning_rate": 0.0001538503282486743,
        "epoch": 0.3203034589173203,
        "step": 2491
    },
    {
        "loss": 1.9655,
        "grad_norm": 1.3738679885864258,
        "learning_rate": 0.0001538162194904895,
        "epoch": 0.32043204320432045,
        "step": 2492
    },
    {
        "loss": 1.8794,
        "grad_norm": 2.1598780155181885,
        "learning_rate": 0.00015378210191642515,
        "epoch": 0.3205606274913206,
        "step": 2493
    },
    {
        "loss": 1.4386,
        "grad_norm": 2.4332504272460938,
        "learning_rate": 0.00015374797553207014,
        "epoch": 0.3206892117783207,
        "step": 2494
    },
    {
        "loss": 1.9507,
        "grad_norm": 1.7383043766021729,
        "learning_rate": 0.00015371384034301494,
        "epoch": 0.3208177960653208,
        "step": 2495
    },
    {
        "loss": 1.6476,
        "grad_norm": 1.6381746530532837,
        "learning_rate": 0.00015367969635485138,
        "epoch": 0.3209463803523209,
        "step": 2496
    },
    {
        "loss": 2.2305,
        "grad_norm": 1.3259814977645874,
        "learning_rate": 0.00015364554357317274,
        "epoch": 0.32107496463932106,
        "step": 2497
    },
    {
        "loss": 2.0052,
        "grad_norm": 1.4160475730895996,
        "learning_rate": 0.0001536113820035737,
        "epoch": 0.3212035489263212,
        "step": 2498
    },
    {
        "loss": 1.8762,
        "grad_norm": 1.790079951286316,
        "learning_rate": 0.00015357721165165045,
        "epoch": 0.3213321332133213,
        "step": 2499
    },
    {
        "loss": 1.9678,
        "grad_norm": 1.5071061849594116,
        "learning_rate": 0.00015354303252300064,
        "epoch": 0.32146071750032146,
        "step": 2500
    },
    {
        "eval_loss": 1.9566413164138794,
        "eval_runtime": 28.2042,
        "eval_samples_per_second": 2.801,
        "eval_steps_per_second": 2.801,
        "epoch": 0.32146071750032146,
        "step": 2500
    },
    {
        "loss": 2.5348,
        "grad_norm": 1.448632836341858,
        "learning_rate": 0.0001535088446232232,
        "epoch": 0.3215893017873216,
        "step": 2501
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.7193222045898438,
        "learning_rate": 0.00015347464795791873,
        "epoch": 0.3217178860743217,
        "step": 2502
    },
    {
        "loss": 1.7157,
        "grad_norm": 1.4905786514282227,
        "learning_rate": 0.00015344044253268906,
        "epoch": 0.32184647036132186,
        "step": 2503
    },
    {
        "loss": 2.2888,
        "grad_norm": 2.3296971321105957,
        "learning_rate": 0.00015340622835313753,
        "epoch": 0.321975054648322,
        "step": 2504
    },
    {
        "loss": 2.0479,
        "grad_norm": 1.3376731872558594,
        "learning_rate": 0.00015337200542486898,
        "epoch": 0.3221036389353221,
        "step": 2505
    },
    {
        "loss": 1.9754,
        "grad_norm": 1.7712702751159668,
        "learning_rate": 0.00015333777375348956,
        "epoch": 0.32223222322232226,
        "step": 2506
    },
    {
        "loss": 2.0938,
        "grad_norm": 1.560427188873291,
        "learning_rate": 0.00015330353334460698,
        "epoch": 0.32236080750932233,
        "step": 2507
    },
    {
        "loss": 1.9436,
        "grad_norm": 2.453186273574829,
        "learning_rate": 0.0001532692842038303,
        "epoch": 0.32248939179632247,
        "step": 2508
    },
    {
        "loss": 2.544,
        "grad_norm": 1.561987042427063,
        "learning_rate": 0.00015323502633676997,
        "epoch": 0.3226179760833226,
        "step": 2509
    },
    {
        "loss": 1.5205,
        "grad_norm": 1.7581393718719482,
        "learning_rate": 0.00015320075974903797,
        "epoch": 0.32274656037032273,
        "step": 2510
    },
    {
        "loss": 2.2353,
        "grad_norm": 1.4021377563476562,
        "learning_rate": 0.00015316648444624765,
        "epoch": 0.32287514465732287,
        "step": 2511
    },
    {
        "loss": 2.3314,
        "grad_norm": 2.2092247009277344,
        "learning_rate": 0.00015313220043401384,
        "epoch": 0.323003728944323,
        "step": 2512
    },
    {
        "loss": 1.8247,
        "grad_norm": 1.0397177934646606,
        "learning_rate": 0.0001530979077179527,
        "epoch": 0.32313231323132313,
        "step": 2513
    },
    {
        "loss": 2.2877,
        "grad_norm": 1.621420979499817,
        "learning_rate": 0.00015306360630368198,
        "epoch": 0.32326089751832326,
        "step": 2514
    },
    {
        "loss": 2.1518,
        "grad_norm": 1.5356215238571167,
        "learning_rate": 0.00015302929619682063,
        "epoch": 0.3233894818053234,
        "step": 2515
    },
    {
        "loss": 2.0758,
        "grad_norm": 1.6991463899612427,
        "learning_rate": 0.0001529949774029892,
        "epoch": 0.32351806609232353,
        "step": 2516
    },
    {
        "loss": 2.0238,
        "grad_norm": 1.8785556554794312,
        "learning_rate": 0.0001529606499278096,
        "epoch": 0.32364665037932366,
        "step": 2517
    },
    {
        "loss": 2.0949,
        "grad_norm": 2.1439266204833984,
        "learning_rate": 0.0001529263137769052,
        "epoch": 0.3237752346663238,
        "step": 2518
    },
    {
        "loss": 1.9316,
        "grad_norm": 1.4494026899337769,
        "learning_rate": 0.00015289196895590076,
        "epoch": 0.32390381895332393,
        "step": 2519
    },
    {
        "loss": 1.5109,
        "grad_norm": 2.3573877811431885,
        "learning_rate": 0.00015285761547042238,
        "epoch": 0.324032403240324,
        "step": 2520
    },
    {
        "loss": 1.2983,
        "grad_norm": 2.063243865966797,
        "learning_rate": 0.00015282325332609774,
        "epoch": 0.32416098752732414,
        "step": 2521
    },
    {
        "loss": 0.9986,
        "grad_norm": 3.1133344173431396,
        "learning_rate": 0.0001527888825285558,
        "epoch": 0.3242895718143243,
        "step": 2522
    },
    {
        "loss": 1.4872,
        "grad_norm": 1.5155094861984253,
        "learning_rate": 0.00015275450308342706,
        "epoch": 0.3244181561013244,
        "step": 2523
    },
    {
        "loss": 1.4632,
        "grad_norm": 1.9641801118850708,
        "learning_rate": 0.0001527201149963434,
        "epoch": 0.32454674038832454,
        "step": 2524
    },
    {
        "loss": 1.9392,
        "grad_norm": 1.562595248222351,
        "learning_rate": 0.00015268571827293796,
        "epoch": 0.3246753246753247,
        "step": 2525
    },
    {
        "loss": 2.0795,
        "grad_norm": 2.0463051795959473,
        "learning_rate": 0.00015265131291884552,
        "epoch": 0.3248039089623248,
        "step": 2526
    },
    {
        "loss": 1.7375,
        "grad_norm": 2.2131614685058594,
        "learning_rate": 0.00015261689893970217,
        "epoch": 0.32493249324932494,
        "step": 2527
    },
    {
        "loss": 2.4481,
        "grad_norm": 1.4992527961730957,
        "learning_rate": 0.00015258247634114539,
        "epoch": 0.32506107753632507,
        "step": 2528
    },
    {
        "loss": 1.6367,
        "grad_norm": 1.6057507991790771,
        "learning_rate": 0.00015254804512881414,
        "epoch": 0.3251896618233252,
        "step": 2529
    },
    {
        "loss": 1.601,
        "grad_norm": 1.462860107421875,
        "learning_rate": 0.0001525136053083487,
        "epoch": 0.32531824611032534,
        "step": 2530
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.3398337364196777,
        "learning_rate": 0.00015247915688539084,
        "epoch": 0.32544683039732547,
        "step": 2531
    },
    {
        "loss": 1.9614,
        "grad_norm": 1.366707444190979,
        "learning_rate": 0.0001524446998655838,
        "epoch": 0.32557541468432555,
        "step": 2532
    },
    {
        "loss": 1.536,
        "grad_norm": 1.808639645576477,
        "learning_rate": 0.00015241023425457194,
        "epoch": 0.3257039989713257,
        "step": 2533
    },
    {
        "loss": 2.5423,
        "grad_norm": 1.4722001552581787,
        "learning_rate": 0.00015237576005800146,
        "epoch": 0.3258325832583258,
        "step": 2534
    },
    {
        "loss": 1.5061,
        "grad_norm": 1.7686868906021118,
        "learning_rate": 0.00015234127728151953,
        "epoch": 0.32596116754532595,
        "step": 2535
    },
    {
        "loss": 1.5394,
        "grad_norm": 2.209892511367798,
        "learning_rate": 0.00015230678593077508,
        "epoch": 0.3260897518323261,
        "step": 2536
    },
    {
        "loss": 2.0465,
        "grad_norm": 1.554743766784668,
        "learning_rate": 0.00015227228601141823,
        "epoch": 0.3262183361193262,
        "step": 2537
    },
    {
        "loss": 1.9449,
        "grad_norm": 2.154799699783325,
        "learning_rate": 0.00015223777752910057,
        "epoch": 0.32634692040632635,
        "step": 2538
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.4654419422149658,
        "learning_rate": 0.00015220326048947508,
        "epoch": 0.3264755046933265,
        "step": 2539
    },
    {
        "loss": 1.6755,
        "grad_norm": 3.6979641914367676,
        "learning_rate": 0.0001521687348981962,
        "epoch": 0.3266040889803266,
        "step": 2540
    },
    {
        "loss": 2.5052,
        "grad_norm": 5.190366744995117,
        "learning_rate": 0.00015213420076091967,
        "epoch": 0.32673267326732675,
        "step": 2541
    },
    {
        "loss": 1.1003,
        "grad_norm": 1.7380787134170532,
        "learning_rate": 0.00015209965808330274,
        "epoch": 0.3268612575543269,
        "step": 2542
    },
    {
        "loss": 2.1242,
        "grad_norm": 1.6478791236877441,
        "learning_rate": 0.00015206510687100394,
        "epoch": 0.326989841841327,
        "step": 2543
    },
    {
        "loss": 1.9091,
        "grad_norm": 2.5821332931518555,
        "learning_rate": 0.00015203054712968327,
        "epoch": 0.32711842612832714,
        "step": 2544
    },
    {
        "loss": 1.4301,
        "grad_norm": 3.827096462249756,
        "learning_rate": 0.00015199597886500222,
        "epoch": 0.3272470104153272,
        "step": 2545
    },
    {
        "loss": 1.6156,
        "grad_norm": 2.0485680103302,
        "learning_rate": 0.00015196140208262343,
        "epoch": 0.32737559470232735,
        "step": 2546
    },
    {
        "loss": 1.922,
        "grad_norm": 1.675727367401123,
        "learning_rate": 0.0001519268167882112,
        "epoch": 0.3275041789893275,
        "step": 2547
    },
    {
        "loss": 2.0263,
        "grad_norm": 2.0356674194335938,
        "learning_rate": 0.00015189222298743103,
        "epoch": 0.3276327632763276,
        "step": 2548
    },
    {
        "loss": 1.1105,
        "grad_norm": 1.5129685401916504,
        "learning_rate": 0.00015185762068594988,
        "epoch": 0.32776134756332775,
        "step": 2549
    },
    {
        "loss": 2.3456,
        "grad_norm": 1.2187416553497314,
        "learning_rate": 0.00015182300988943615,
        "epoch": 0.3278899318503279,
        "step": 2550
    },
    {
        "loss": 1.6959,
        "grad_norm": 1.5976786613464355,
        "learning_rate": 0.0001517883906035596,
        "epoch": 0.328018516137328,
        "step": 2551
    },
    {
        "loss": 2.2769,
        "grad_norm": 1.5328099727630615,
        "learning_rate": 0.00015175376283399134,
        "epoch": 0.32814710042432815,
        "step": 2552
    },
    {
        "loss": 2.0736,
        "grad_norm": 1.5840286016464233,
        "learning_rate": 0.00015171912658640393,
        "epoch": 0.3282756847113283,
        "step": 2553
    },
    {
        "loss": 1.6004,
        "grad_norm": 2.096781015396118,
        "learning_rate": 0.00015168448186647125,
        "epoch": 0.3284042689983284,
        "step": 2554
    },
    {
        "loss": 2.0661,
        "grad_norm": 2.3050551414489746,
        "learning_rate": 0.00015164982867986866,
        "epoch": 0.32853285328532855,
        "step": 2555
    },
    {
        "loss": 1.6343,
        "grad_norm": 1.6741275787353516,
        "learning_rate": 0.00015161516703227282,
        "epoch": 0.3286614375723287,
        "step": 2556
    },
    {
        "loss": 2.2619,
        "grad_norm": 1.7076371908187866,
        "learning_rate": 0.00015158049692936182,
        "epoch": 0.32879002185932876,
        "step": 2557
    },
    {
        "loss": 2.1651,
        "grad_norm": 1.4853202104568481,
        "learning_rate": 0.00015154581837681515,
        "epoch": 0.3289186061463289,
        "step": 2558
    },
    {
        "loss": 1.6649,
        "grad_norm": 1.9931780099868774,
        "learning_rate": 0.00015151113138031362,
        "epoch": 0.32904719043332903,
        "step": 2559
    },
    {
        "loss": 1.7369,
        "grad_norm": 1.2216986417770386,
        "learning_rate": 0.00015147643594553949,
        "epoch": 0.32917577472032916,
        "step": 2560
    },
    {
        "loss": 1.7162,
        "grad_norm": 1.5742501020431519,
        "learning_rate": 0.0001514417320781764,
        "epoch": 0.3293043590073293,
        "step": 2561
    },
    {
        "loss": 2.2577,
        "grad_norm": 1.7357503175735474,
        "learning_rate": 0.0001514070197839093,
        "epoch": 0.32943294329432943,
        "step": 2562
    },
    {
        "loss": 2.2689,
        "grad_norm": 1.630184292793274,
        "learning_rate": 0.00015137229906842458,
        "epoch": 0.32956152758132956,
        "step": 2563
    },
    {
        "loss": 1.4891,
        "grad_norm": 1.4088793992996216,
        "learning_rate": 0.00015133756993741002,
        "epoch": 0.3296901118683297,
        "step": 2564
    },
    {
        "loss": 2.2066,
        "grad_norm": 1.3117765188217163,
        "learning_rate": 0.00015130283239655476,
        "epoch": 0.3298186961553298,
        "step": 2565
    },
    {
        "loss": 2.3464,
        "grad_norm": 1.8556493520736694,
        "learning_rate": 0.00015126808645154932,
        "epoch": 0.32994728044232996,
        "step": 2566
    },
    {
        "loss": 2.2775,
        "grad_norm": 1.4445663690567017,
        "learning_rate": 0.0001512333321080855,
        "epoch": 0.3300758647293301,
        "step": 2567
    },
    {
        "loss": 1.4729,
        "grad_norm": 2.32966685295105,
        "learning_rate": 0.00015119856937185667,
        "epoch": 0.3302044490163302,
        "step": 2568
    },
    {
        "loss": 1.5835,
        "grad_norm": 2.3541669845581055,
        "learning_rate": 0.00015116379824855745,
        "epoch": 0.33033303330333036,
        "step": 2569
    },
    {
        "loss": 2.2271,
        "grad_norm": 1.4082478284835815,
        "learning_rate": 0.00015112901874388384,
        "epoch": 0.33046161759033044,
        "step": 2570
    },
    {
        "loss": 2.0984,
        "grad_norm": 2.1127371788024902,
        "learning_rate": 0.00015109423086353326,
        "epoch": 0.33059020187733057,
        "step": 2571
    },
    {
        "loss": 2.4164,
        "grad_norm": 1.173863410949707,
        "learning_rate": 0.0001510594346132044,
        "epoch": 0.3307187861643307,
        "step": 2572
    },
    {
        "loss": 1.5737,
        "grad_norm": 1.9625136852264404,
        "learning_rate": 0.0001510246299985975,
        "epoch": 0.33084737045133084,
        "step": 2573
    },
    {
        "loss": 1.9927,
        "grad_norm": 1.6251022815704346,
        "learning_rate": 0.00015098981702541396,
        "epoch": 0.33097595473833097,
        "step": 2574
    },
    {
        "loss": 2.1512,
        "grad_norm": 1.4880638122558594,
        "learning_rate": 0.0001509549956993567,
        "epoch": 0.3311045390253311,
        "step": 2575
    },
    {
        "loss": 1.7609,
        "grad_norm": 2.247515916824341,
        "learning_rate": 0.00015092016602612996,
        "epoch": 0.33123312331233123,
        "step": 2576
    },
    {
        "loss": 2.0673,
        "grad_norm": 2.302603244781494,
        "learning_rate": 0.00015088532801143932,
        "epoch": 0.33136170759933137,
        "step": 2577
    },
    {
        "loss": 2.1182,
        "grad_norm": 2.230187177658081,
        "learning_rate": 0.00015085048166099172,
        "epoch": 0.3314902918863315,
        "step": 2578
    },
    {
        "loss": 1.7556,
        "grad_norm": 2.5493013858795166,
        "learning_rate": 0.00015081562698049565,
        "epoch": 0.33161887617333163,
        "step": 2579
    },
    {
        "loss": 2.5357,
        "grad_norm": 1.7039742469787598,
        "learning_rate": 0.00015078076397566066,
        "epoch": 0.33174746046033177,
        "step": 2580
    },
    {
        "loss": 1.9702,
        "grad_norm": 2.5686137676239014,
        "learning_rate": 0.0001507458926521979,
        "epoch": 0.3318760447473319,
        "step": 2581
    },
    {
        "loss": 2.0967,
        "grad_norm": 2.0578696727752686,
        "learning_rate": 0.00015071101301581975,
        "epoch": 0.332004629034332,
        "step": 2582
    },
    {
        "loss": 1.8321,
        "grad_norm": 2.5923843383789062,
        "learning_rate": 0.00015067612507224003,
        "epoch": 0.3321332133213321,
        "step": 2583
    },
    {
        "loss": 1.9941,
        "grad_norm": 2.224816083908081,
        "learning_rate": 0.0001506412288271739,
        "epoch": 0.33226179760833224,
        "step": 2584
    },
    {
        "loss": 1.8823,
        "grad_norm": 1.4161375761032104,
        "learning_rate": 0.00015060632428633781,
        "epoch": 0.3323903818953324,
        "step": 2585
    },
    {
        "loss": 2.2714,
        "grad_norm": 1.3837064504623413,
        "learning_rate": 0.00015057141145544974,
        "epoch": 0.3325189661823325,
        "step": 2586
    },
    {
        "loss": 1.6007,
        "grad_norm": 1.886989951133728,
        "learning_rate": 0.00015053649034022884,
        "epoch": 0.33264755046933264,
        "step": 2587
    },
    {
        "loss": 1.8123,
        "grad_norm": 2.065838575363159,
        "learning_rate": 0.00015050156094639569,
        "epoch": 0.3327761347563328,
        "step": 2588
    },
    {
        "loss": 1.917,
        "grad_norm": 1.7592476606369019,
        "learning_rate": 0.0001504666232796723,
        "epoch": 0.3329047190433329,
        "step": 2589
    },
    {
        "loss": 1.5573,
        "grad_norm": 2.8226706981658936,
        "learning_rate": 0.0001504316773457819,
        "epoch": 0.33303330333033304,
        "step": 2590
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.0365008115768433,
        "learning_rate": 0.00015039672315044915,
        "epoch": 0.3331618876173332,
        "step": 2591
    },
    {
        "loss": 1.714,
        "grad_norm": 2.2637383937835693,
        "learning_rate": 0.00015036176069940012,
        "epoch": 0.3332904719043333,
        "step": 2592
    },
    {
        "loss": 1.9373,
        "grad_norm": 1.8038116693496704,
        "learning_rate": 0.0001503267899983621,
        "epoch": 0.33341905619133344,
        "step": 2593
    },
    {
        "loss": 1.2567,
        "grad_norm": 2.0415337085723877,
        "learning_rate": 0.00015029181105306384,
        "epoch": 0.3335476404783336,
        "step": 2594
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.747622013092041,
        "learning_rate": 0.00015025682386923537,
        "epoch": 0.33367622476533365,
        "step": 2595
    },
    {
        "loss": 1.4929,
        "grad_norm": 1.7828428745269775,
        "learning_rate": 0.0001502218284526081,
        "epoch": 0.3338048090523338,
        "step": 2596
    },
    {
        "loss": 1.2288,
        "grad_norm": 2.81744384765625,
        "learning_rate": 0.00015018682480891484,
        "epoch": 0.3339333933393339,
        "step": 2597
    },
    {
        "loss": 2.2544,
        "grad_norm": 1.536112666130066,
        "learning_rate": 0.00015015181294388962,
        "epoch": 0.33406197762633405,
        "step": 2598
    },
    {
        "loss": 1.4601,
        "grad_norm": 1.8199447393417358,
        "learning_rate": 0.00015011679286326793,
        "epoch": 0.3341905619133342,
        "step": 2599
    },
    {
        "loss": 1.5899,
        "grad_norm": 1.6502833366394043,
        "learning_rate": 0.0001500817645727866,
        "epoch": 0.3343191462003343,
        "step": 2600
    },
    {
        "eval_loss": 1.9580848217010498,
        "eval_runtime": 28.1986,
        "eval_samples_per_second": 2.802,
        "eval_steps_per_second": 2.802,
        "epoch": 0.3343191462003343,
        "step": 2600
    },
    {
        "loss": 1.5542,
        "grad_norm": 1.9666788578033447,
        "learning_rate": 0.0001500467280781837,
        "epoch": 0.33444773048733445,
        "step": 2601
    },
    {
        "loss": 2.402,
        "grad_norm": 1.9509117603302002,
        "learning_rate": 0.00015001168338519877,
        "epoch": 0.3345763147743346,
        "step": 2602
    },
    {
        "loss": 2.5674,
        "grad_norm": 1.7368284463882446,
        "learning_rate": 0.00014997663049957268,
        "epoch": 0.3347048990613347,
        "step": 2603
    },
    {
        "loss": 1.3646,
        "grad_norm": 1.5779210329055786,
        "learning_rate": 0.00014994156942704749,
        "epoch": 0.33483348334833485,
        "step": 2604
    },
    {
        "loss": 1.8165,
        "grad_norm": 3.061368465423584,
        "learning_rate": 0.0001499065001733668,
        "epoch": 0.334962067635335,
        "step": 2605
    },
    {
        "loss": 2.1201,
        "grad_norm": 1.8395968675613403,
        "learning_rate": 0.00014987142274427542,
        "epoch": 0.3350906519223351,
        "step": 2606
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.800044298171997,
        "learning_rate": 0.0001498363371455196,
        "epoch": 0.3352192362093352,
        "step": 2607
    },
    {
        "loss": 1.3702,
        "grad_norm": 2.7674384117126465,
        "learning_rate": 0.00014980124338284683,
        "epoch": 0.3353478204963353,
        "step": 2608
    },
    {
        "loss": 2.4007,
        "grad_norm": 1.276404857635498,
        "learning_rate": 0.00014976614146200594,
        "epoch": 0.33547640478333546,
        "step": 2609
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.7457884550094604,
        "learning_rate": 0.0001497310313887472,
        "epoch": 0.3356049890703356,
        "step": 2610
    },
    {
        "loss": 1.4844,
        "grad_norm": 2.2953226566314697,
        "learning_rate": 0.00014969591316882214,
        "epoch": 0.3357335733573357,
        "step": 2611
    },
    {
        "loss": 1.3541,
        "grad_norm": 1.3845630884170532,
        "learning_rate": 0.00014966078680798362,
        "epoch": 0.33586215764433586,
        "step": 2612
    },
    {
        "loss": 2.0545,
        "grad_norm": 1.3593815565109253,
        "learning_rate": 0.00014962565231198586,
        "epoch": 0.335990741931336,
        "step": 2613
    },
    {
        "loss": 2.1389,
        "grad_norm": 1.8267772197723389,
        "learning_rate": 0.0001495905096865844,
        "epoch": 0.3361193262183361,
        "step": 2614
    },
    {
        "loss": 2.6346,
        "grad_norm": 1.5727146863937378,
        "learning_rate": 0.00014955535893753608,
        "epoch": 0.33624791050533626,
        "step": 2615
    },
    {
        "loss": 1.5209,
        "grad_norm": 1.6498407125473022,
        "learning_rate": 0.00014952020007059913,
        "epoch": 0.3363764947923364,
        "step": 2616
    },
    {
        "loss": 2.485,
        "grad_norm": 1.6104508638381958,
        "learning_rate": 0.0001494850330915331,
        "epoch": 0.3365050790793365,
        "step": 2617
    },
    {
        "loss": 2.3501,
        "grad_norm": 1.6248301267623901,
        "learning_rate": 0.00014944985800609888,
        "epoch": 0.33663366336633666,
        "step": 2618
    },
    {
        "loss": 1.914,
        "grad_norm": 1.9646821022033691,
        "learning_rate": 0.00014941467482005856,
        "epoch": 0.3367622476533368,
        "step": 2619
    },
    {
        "loss": 2.5479,
        "grad_norm": 2.0281832218170166,
        "learning_rate": 0.0001493794835391757,
        "epoch": 0.33689083194033687,
        "step": 2620
    },
    {
        "loss": 1.2609,
        "grad_norm": 2.1110613346099854,
        "learning_rate": 0.00014934428416921522,
        "epoch": 0.337019416227337,
        "step": 2621
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.3758981227874756,
        "learning_rate": 0.0001493090767159432,
        "epoch": 0.33714800051433713,
        "step": 2622
    },
    {
        "loss": 2.333,
        "grad_norm": 1.2262929677963257,
        "learning_rate": 0.00014927386118512715,
        "epoch": 0.33727658480133726,
        "step": 2623
    },
    {
        "loss": 1.8254,
        "grad_norm": 1.8569724559783936,
        "learning_rate": 0.00014923863758253594,
        "epoch": 0.3374051690883374,
        "step": 2624
    },
    {
        "loss": 2.1473,
        "grad_norm": 2.1242125034332275,
        "learning_rate": 0.00014920340591393966,
        "epoch": 0.33753375337533753,
        "step": 2625
    },
    {
        "loss": 1.6351,
        "grad_norm": 2.1608495712280273,
        "learning_rate": 0.00014916816618510973,
        "epoch": 0.33766233766233766,
        "step": 2626
    },
    {
        "loss": 1.7583,
        "grad_norm": 1.7119685411453247,
        "learning_rate": 0.000149132918401819,
        "epoch": 0.3377909219493378,
        "step": 2627
    },
    {
        "loss": 1.8693,
        "grad_norm": 1.5678375959396362,
        "learning_rate": 0.00014909766256984157,
        "epoch": 0.33791950623633793,
        "step": 2628
    },
    {
        "loss": 1.3782,
        "grad_norm": 2.274298667907715,
        "learning_rate": 0.00014906239869495283,
        "epoch": 0.33804809052333806,
        "step": 2629
    },
    {
        "loss": 1.6966,
        "grad_norm": 1.3337677717208862,
        "learning_rate": 0.00014902712678292946,
        "epoch": 0.3381766748103382,
        "step": 2630
    },
    {
        "loss": 1.6947,
        "grad_norm": 2.802685022354126,
        "learning_rate": 0.00014899184683954967,
        "epoch": 0.33830525909733833,
        "step": 2631
    },
    {
        "loss": 1.7671,
        "grad_norm": 2.4863533973693848,
        "learning_rate": 0.00014895655887059266,
        "epoch": 0.3384338433843384,
        "step": 2632
    },
    {
        "loss": 1.6889,
        "grad_norm": 3.14070463180542,
        "learning_rate": 0.0001489212628818392,
        "epoch": 0.33856242767133854,
        "step": 2633
    },
    {
        "loss": 1.6882,
        "grad_norm": 1.8755568265914917,
        "learning_rate": 0.00014888595887907127,
        "epoch": 0.3386910119583387,
        "step": 2634
    },
    {
        "loss": 2.2129,
        "grad_norm": 2.1046926975250244,
        "learning_rate": 0.00014885064686807217,
        "epoch": 0.3388195962453388,
        "step": 2635
    },
    {
        "loss": 2.3024,
        "grad_norm": 2.031116485595703,
        "learning_rate": 0.00014881532685462657,
        "epoch": 0.33894818053233894,
        "step": 2636
    },
    {
        "loss": 1.4729,
        "grad_norm": 2.1989121437072754,
        "learning_rate": 0.00014877999884452028,
        "epoch": 0.33907676481933907,
        "step": 2637
    },
    {
        "loss": 2.2501,
        "grad_norm": 1.4019185304641724,
        "learning_rate": 0.00014874466284354071,
        "epoch": 0.3392053491063392,
        "step": 2638
    },
    {
        "loss": 2.175,
        "grad_norm": 1.2735178470611572,
        "learning_rate": 0.0001487093188574763,
        "epoch": 0.33933393339333934,
        "step": 2639
    },
    {
        "loss": 1.8517,
        "grad_norm": 1.742377519607544,
        "learning_rate": 0.00014867396689211693,
        "epoch": 0.33946251768033947,
        "step": 2640
    },
    {
        "loss": 2.1837,
        "grad_norm": 2.1070168018341064,
        "learning_rate": 0.00014863860695325372,
        "epoch": 0.3395911019673396,
        "step": 2641
    },
    {
        "loss": 1.3929,
        "grad_norm": 1.5688433647155762,
        "learning_rate": 0.00014860323904667926,
        "epoch": 0.33971968625433974,
        "step": 2642
    },
    {
        "loss": 1.5901,
        "grad_norm": 1.3604735136032104,
        "learning_rate": 0.00014856786317818724,
        "epoch": 0.33984827054133987,
        "step": 2643
    },
    {
        "loss": 2.5197,
        "grad_norm": 1.5689281225204468,
        "learning_rate": 0.00014853247935357278,
        "epoch": 0.33997685482834,
        "step": 2644
    },
    {
        "loss": 1.8988,
        "grad_norm": 2.112931966781616,
        "learning_rate": 0.00014849708757863218,
        "epoch": 0.3401054391153401,
        "step": 2645
    },
    {
        "loss": 1.1209,
        "grad_norm": 2.7701895236968994,
        "learning_rate": 0.00014846168785916322,
        "epoch": 0.3402340234023402,
        "step": 2646
    },
    {
        "loss": 1.386,
        "grad_norm": 2.464235544204712,
        "learning_rate": 0.0001484262802009649,
        "epoch": 0.34036260768934035,
        "step": 2647
    },
    {
        "loss": 2.4883,
        "grad_norm": 1.9858551025390625,
        "learning_rate": 0.00014839086460983745,
        "epoch": 0.3404911919763405,
        "step": 2648
    },
    {
        "loss": 2.1644,
        "grad_norm": 1.4901318550109863,
        "learning_rate": 0.00014835544109158247,
        "epoch": 0.3406197762633406,
        "step": 2649
    },
    {
        "loss": 2.4709,
        "grad_norm": 1.9904725551605225,
        "learning_rate": 0.0001483200096520029,
        "epoch": 0.34074836055034075,
        "step": 2650
    },
    {
        "loss": 1.9586,
        "grad_norm": 1.5414232015609741,
        "learning_rate": 0.00014828457029690285,
        "epoch": 0.3408769448373409,
        "step": 2651
    },
    {
        "loss": 1.5914,
        "grad_norm": 1.5299147367477417,
        "learning_rate": 0.00014824912303208784,
        "epoch": 0.341005529124341,
        "step": 2652
    },
    {
        "loss": 0.8397,
        "grad_norm": 2.414975166320801,
        "learning_rate": 0.00014821366786336464,
        "epoch": 0.34113411341134114,
        "step": 2653
    },
    {
        "loss": 2.5215,
        "grad_norm": 1.7568912506103516,
        "learning_rate": 0.00014817820479654136,
        "epoch": 0.3412626976983413,
        "step": 2654
    },
    {
        "loss": 1.7903,
        "grad_norm": 2.096050500869751,
        "learning_rate": 0.0001481427338374273,
        "epoch": 0.3413912819853414,
        "step": 2655
    },
    {
        "loss": 1.9969,
        "grad_norm": 1.8880789279937744,
        "learning_rate": 0.00014810725499183315,
        "epoch": 0.34151986627234154,
        "step": 2656
    },
    {
        "loss": 2.211,
        "grad_norm": 1.8216803073883057,
        "learning_rate": 0.00014807176826557084,
        "epoch": 0.3416484505593416,
        "step": 2657
    },
    {
        "loss": 2.5597,
        "grad_norm": 2.0065395832061768,
        "learning_rate": 0.00014803627366445366,
        "epoch": 0.34177703484634175,
        "step": 2658
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.8626353740692139,
        "learning_rate": 0.00014800077119429613,
        "epoch": 0.3419056191333419,
        "step": 2659
    },
    {
        "loss": 2.1765,
        "grad_norm": 1.4112619161605835,
        "learning_rate": 0.00014796526086091407,
        "epoch": 0.342034203420342,
        "step": 2660
    },
    {
        "loss": 1.1869,
        "grad_norm": 1.7940713167190552,
        "learning_rate": 0.00014792974267012455,
        "epoch": 0.34216278770734215,
        "step": 2661
    },
    {
        "loss": 1.4569,
        "grad_norm": 1.366112470626831,
        "learning_rate": 0.00014789421662774596,
        "epoch": 0.3422913719943423,
        "step": 2662
    },
    {
        "loss": 1.524,
        "grad_norm": 1.7761305570602417,
        "learning_rate": 0.00014785868273959807,
        "epoch": 0.3424199562813424,
        "step": 2663
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.2773560285568237,
        "learning_rate": 0.00014782314101150176,
        "epoch": 0.34254854056834255,
        "step": 2664
    },
    {
        "loss": 1.697,
        "grad_norm": 1.6875160932540894,
        "learning_rate": 0.00014778759144927936,
        "epoch": 0.3426771248553427,
        "step": 2665
    },
    {
        "loss": 2.1315,
        "grad_norm": 2.312798261642456,
        "learning_rate": 0.00014775203405875432,
        "epoch": 0.3428057091423428,
        "step": 2666
    },
    {
        "loss": 2.2969,
        "grad_norm": 1.6617399454116821,
        "learning_rate": 0.00014771646884575147,
        "epoch": 0.34293429342934295,
        "step": 2667
    },
    {
        "loss": 1.595,
        "grad_norm": 1.86105215549469,
        "learning_rate": 0.000147680895816097,
        "epoch": 0.3430628777163431,
        "step": 2668
    },
    {
        "loss": 2.4947,
        "grad_norm": 1.3326013088226318,
        "learning_rate": 0.00014764531497561817,
        "epoch": 0.3431914620033432,
        "step": 2669
    },
    {
        "loss": 2.16,
        "grad_norm": 1.1543421745300293,
        "learning_rate": 0.00014760972633014375,
        "epoch": 0.3433200462903433,
        "step": 2670
    },
    {
        "loss": 2.0711,
        "grad_norm": 2.387706756591797,
        "learning_rate": 0.00014757412988550362,
        "epoch": 0.34344863057734343,
        "step": 2671
    },
    {
        "loss": 1.802,
        "grad_norm": 1.725719928741455,
        "learning_rate": 0.00014753852564752896,
        "epoch": 0.34357721486434356,
        "step": 2672
    },
    {
        "loss": 1.6985,
        "grad_norm": 1.976125717163086,
        "learning_rate": 0.00014750291362205232,
        "epoch": 0.3437057991513437,
        "step": 2673
    },
    {
        "loss": 1.4776,
        "grad_norm": 1.7588118314743042,
        "learning_rate": 0.00014746729381490744,
        "epoch": 0.3438343834383438,
        "step": 2674
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.5512365102767944,
        "learning_rate": 0.0001474316662319294,
        "epoch": 0.34396296772534396,
        "step": 2675
    },
    {
        "loss": 1.0798,
        "grad_norm": 2.202538251876831,
        "learning_rate": 0.00014739603087895447,
        "epoch": 0.3440915520123441,
        "step": 2676
    },
    {
        "loss": 2.1473,
        "grad_norm": 1.8149226903915405,
        "learning_rate": 0.00014736038776182026,
        "epoch": 0.3442201362993442,
        "step": 2677
    },
    {
        "loss": 2.2406,
        "grad_norm": 1.2361423969268799,
        "learning_rate": 0.00014732473688636562,
        "epoch": 0.34434872058634436,
        "step": 2678
    },
    {
        "loss": 2.0384,
        "grad_norm": 1.7222613096237183,
        "learning_rate": 0.0001472890782584307,
        "epoch": 0.3444773048733445,
        "step": 2679
    },
    {
        "loss": 1.2301,
        "grad_norm": 2.908521890640259,
        "learning_rate": 0.0001472534118838569,
        "epoch": 0.3446058891603446,
        "step": 2680
    },
    {
        "loss": 1.3691,
        "grad_norm": 2.6561131477355957,
        "learning_rate": 0.0001472177377684869,
        "epoch": 0.34473447344734476,
        "step": 2681
    },
    {
        "loss": 1.9188,
        "grad_norm": 2.091644287109375,
        "learning_rate": 0.0001471820559181646,
        "epoch": 0.34486305773434484,
        "step": 2682
    },
    {
        "loss": 2.3516,
        "grad_norm": 1.27853262424469,
        "learning_rate": 0.00014714636633873522,
        "epoch": 0.34499164202134497,
        "step": 2683
    },
    {
        "loss": 1.7347,
        "grad_norm": 1.9968265295028687,
        "learning_rate": 0.00014711066903604523,
        "epoch": 0.3451202263083451,
        "step": 2684
    },
    {
        "loss": 2.0561,
        "grad_norm": 1.4101663827896118,
        "learning_rate": 0.0001470749640159424,
        "epoch": 0.34524881059534523,
        "step": 2685
    },
    {
        "loss": 2.0342,
        "grad_norm": 1.7021863460540771,
        "learning_rate": 0.0001470392512842757,
        "epoch": 0.34537739488234537,
        "step": 2686
    },
    {
        "loss": 2.022,
        "grad_norm": 1.4835405349731445,
        "learning_rate": 0.00014700353084689543,
        "epoch": 0.3455059791693455,
        "step": 2687
    },
    {
        "loss": 1.6445,
        "grad_norm": 2.270336151123047,
        "learning_rate": 0.00014696780270965305,
        "epoch": 0.34563456345634563,
        "step": 2688
    },
    {
        "loss": 2.1613,
        "grad_norm": 1.8265389204025269,
        "learning_rate": 0.00014693206687840143,
        "epoch": 0.34576314774334577,
        "step": 2689
    },
    {
        "loss": 0.749,
        "grad_norm": 1.7892502546310425,
        "learning_rate": 0.0001468963233589945,
        "epoch": 0.3458917320303459,
        "step": 2690
    },
    {
        "loss": 1.5663,
        "grad_norm": 2.3079352378845215,
        "learning_rate": 0.00014686057215728773,
        "epoch": 0.34602031631734603,
        "step": 2691
    },
    {
        "loss": 2.1712,
        "grad_norm": 1.69001305103302,
        "learning_rate": 0.00014682481327913763,
        "epoch": 0.34614890060434617,
        "step": 2692
    },
    {
        "loss": 2.208,
        "grad_norm": 1.394866704940796,
        "learning_rate": 0.00014678904673040192,
        "epoch": 0.3462774848913463,
        "step": 2693
    },
    {
        "loss": 1.4372,
        "grad_norm": 1.515962839126587,
        "learning_rate": 0.00014675327251693982,
        "epoch": 0.34640606917834643,
        "step": 2694
    },
    {
        "loss": 1.9805,
        "grad_norm": 2.2997548580169678,
        "learning_rate": 0.00014671749064461157,
        "epoch": 0.3465346534653465,
        "step": 2695
    },
    {
        "loss": 1.6409,
        "grad_norm": 2.0288801193237305,
        "learning_rate": 0.0001466817011192788,
        "epoch": 0.34666323775234664,
        "step": 2696
    },
    {
        "loss": 1.8946,
        "grad_norm": 2.491117000579834,
        "learning_rate": 0.00014664590394680436,
        "epoch": 0.3467918220393468,
        "step": 2697
    },
    {
        "loss": 1.4712,
        "grad_norm": 1.9896416664123535,
        "learning_rate": 0.00014661009913305236,
        "epoch": 0.3469204063263469,
        "step": 2698
    },
    {
        "loss": 1.8578,
        "grad_norm": 2.181654691696167,
        "learning_rate": 0.00014657428668388812,
        "epoch": 0.34704899061334704,
        "step": 2699
    },
    {
        "loss": 1.7157,
        "grad_norm": 1.871035099029541,
        "learning_rate": 0.00014653846660517823,
        "epoch": 0.3471775749003472,
        "step": 2700
    },
    {
        "eval_loss": 1.946370244026184,
        "eval_runtime": 28.2808,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.3471775749003472,
        "step": 2700
    },
    {
        "loss": 2.2569,
        "grad_norm": 2.082141160964966,
        "learning_rate": 0.00014650263890279058,
        "epoch": 0.3473061591873473,
        "step": 2701
    },
    {
        "loss": 1.413,
        "grad_norm": 2.0093507766723633,
        "learning_rate": 0.00014646680358259423,
        "epoch": 0.34743474347434744,
        "step": 2702
    },
    {
        "loss": 1.9837,
        "grad_norm": 2.111701726913452,
        "learning_rate": 0.00014643096065045956,
        "epoch": 0.3475633277613476,
        "step": 2703
    },
    {
        "loss": 1.7801,
        "grad_norm": 1.5886976718902588,
        "learning_rate": 0.00014639511011225816,
        "epoch": 0.3476919120483477,
        "step": 2704
    },
    {
        "loss": 2.6101,
        "grad_norm": 1.6575969457626343,
        "learning_rate": 0.00014635925197386282,
        "epoch": 0.34782049633534784,
        "step": 2705
    },
    {
        "loss": 1.859,
        "grad_norm": 2.2533228397369385,
        "learning_rate": 0.0001463233862411477,
        "epoch": 0.347949080622348,
        "step": 2706
    },
    {
        "loss": 2.1099,
        "grad_norm": 1.5603797435760498,
        "learning_rate": 0.00014628751291998808,
        "epoch": 0.34807766490934805,
        "step": 2707
    },
    {
        "loss": 2.17,
        "grad_norm": 2.248544931411743,
        "learning_rate": 0.00014625163201626053,
        "epoch": 0.3482062491963482,
        "step": 2708
    },
    {
        "loss": 2.1477,
        "grad_norm": 1.6611069440841675,
        "learning_rate": 0.0001462157435358429,
        "epoch": 0.3483348334833483,
        "step": 2709
    },
    {
        "loss": 2.4006,
        "grad_norm": 1.148287057876587,
        "learning_rate": 0.0001461798474846142,
        "epoch": 0.34846341777034845,
        "step": 2710
    },
    {
        "loss": 2.3731,
        "grad_norm": 1.3818708658218384,
        "learning_rate": 0.00014614394386845475,
        "epoch": 0.3485920020573486,
        "step": 2711
    },
    {
        "loss": 1.3062,
        "grad_norm": 0.9297789931297302,
        "learning_rate": 0.0001461080326932461,
        "epoch": 0.3487205863443487,
        "step": 2712
    },
    {
        "loss": 1.1188,
        "grad_norm": 2.1627190113067627,
        "learning_rate": 0.00014607211396487098,
        "epoch": 0.34884917063134885,
        "step": 2713
    },
    {
        "loss": 1.6643,
        "grad_norm": 1.4958288669586182,
        "learning_rate": 0.00014603618768921345,
        "epoch": 0.348977754918349,
        "step": 2714
    },
    {
        "loss": 1.4047,
        "grad_norm": 2.4659245014190674,
        "learning_rate": 0.00014600025387215875,
        "epoch": 0.3491063392053491,
        "step": 2715
    },
    {
        "loss": 1.9705,
        "grad_norm": 1.7369500398635864,
        "learning_rate": 0.0001459643125195933,
        "epoch": 0.34923492349234925,
        "step": 2716
    },
    {
        "loss": 1.7797,
        "grad_norm": 1.974253535270691,
        "learning_rate": 0.00014592836363740489,
        "epoch": 0.3493635077793494,
        "step": 2717
    },
    {
        "loss": 1.4361,
        "grad_norm": 1.9283803701400757,
        "learning_rate": 0.00014589240723148245,
        "epoch": 0.3494920920663495,
        "step": 2718
    },
    {
        "loss": 2.0692,
        "grad_norm": 2.3279929161071777,
        "learning_rate": 0.00014585644330771612,
        "epoch": 0.34962067635334965,
        "step": 2719
    },
    {
        "loss": 2.0476,
        "grad_norm": 1.4556187391281128,
        "learning_rate": 0.00014582047187199738,
        "epoch": 0.3497492606403497,
        "step": 2720
    },
    {
        "loss": 1.9338,
        "grad_norm": 1.783213496208191,
        "learning_rate": 0.0001457844929302188,
        "epoch": 0.34987784492734986,
        "step": 2721
    },
    {
        "loss": 2.1618,
        "grad_norm": 1.3279757499694824,
        "learning_rate": 0.00014574850648827432,
        "epoch": 0.35000642921435,
        "step": 2722
    },
    {
        "loss": 1.7338,
        "grad_norm": 1.793332576751709,
        "learning_rate": 0.00014571251255205903,
        "epoch": 0.3501350135013501,
        "step": 2723
    },
    {
        "loss": 1.4464,
        "grad_norm": 2.255948781967163,
        "learning_rate": 0.0001456765111274692,
        "epoch": 0.35026359778835026,
        "step": 2724
    },
    {
        "loss": 1.9087,
        "grad_norm": 1.5899235010147095,
        "learning_rate": 0.0001456405022204025,
        "epoch": 0.3503921820753504,
        "step": 2725
    },
    {
        "loss": 1.9445,
        "grad_norm": 1.9087836742401123,
        "learning_rate": 0.0001456044858367576,
        "epoch": 0.3505207663623505,
        "step": 2726
    },
    {
        "loss": 1.8295,
        "grad_norm": 2.0235228538513184,
        "learning_rate": 0.00014556846198243457,
        "epoch": 0.35064935064935066,
        "step": 2727
    },
    {
        "loss": 1.5831,
        "grad_norm": 1.8556941747665405,
        "learning_rate": 0.00014553243066333464,
        "epoch": 0.3507779349363508,
        "step": 2728
    },
    {
        "loss": 1.3349,
        "grad_norm": 2.972362756729126,
        "learning_rate": 0.0001454963918853602,
        "epoch": 0.3509065192233509,
        "step": 2729
    },
    {
        "loss": 2.2457,
        "grad_norm": 1.462316632270813,
        "learning_rate": 0.000145460345654415,
        "epoch": 0.35103510351035105,
        "step": 2730
    },
    {
        "loss": 1.6758,
        "grad_norm": 1.852211356163025,
        "learning_rate": 0.00014542429197640395,
        "epoch": 0.3511636877973512,
        "step": 2731
    },
    {
        "loss": 1.4678,
        "grad_norm": 1.5924142599105835,
        "learning_rate": 0.00014538823085723314,
        "epoch": 0.35129227208435126,
        "step": 2732
    },
    {
        "loss": 2.4119,
        "grad_norm": 1.8715291023254395,
        "learning_rate": 0.00014535216230280985,
        "epoch": 0.3514208563713514,
        "step": 2733
    },
    {
        "loss": 1.398,
        "grad_norm": 2.315995454788208,
        "learning_rate": 0.0001453160863190427,
        "epoch": 0.35154944065835153,
        "step": 2734
    },
    {
        "loss": 1.4247,
        "grad_norm": 1.9365190267562866,
        "learning_rate": 0.00014528000291184144,
        "epoch": 0.35167802494535166,
        "step": 2735
    },
    {
        "loss": 1.6808,
        "grad_norm": 2.406144618988037,
        "learning_rate": 0.00014524391208711705,
        "epoch": 0.3518066092323518,
        "step": 2736
    },
    {
        "loss": 2.4054,
        "grad_norm": 1.5180206298828125,
        "learning_rate": 0.00014520781385078175,
        "epoch": 0.35193519351935193,
        "step": 2737
    },
    {
        "loss": 1.9597,
        "grad_norm": 1.9328798055648804,
        "learning_rate": 0.00014517170820874893,
        "epoch": 0.35206377780635206,
        "step": 2738
    },
    {
        "loss": 1.587,
        "grad_norm": 2.0326316356658936,
        "learning_rate": 0.00014513559516693327,
        "epoch": 0.3521923620933522,
        "step": 2739
    },
    {
        "loss": 2.1295,
        "grad_norm": 1.1558715105056763,
        "learning_rate": 0.00014509947473125053,
        "epoch": 0.35232094638035233,
        "step": 2740
    },
    {
        "loss": 2.1954,
        "grad_norm": 1.7195405960083008,
        "learning_rate": 0.00014506334690761786,
        "epoch": 0.35244953066735246,
        "step": 2741
    },
    {
        "loss": 1.6622,
        "grad_norm": 1.9743496179580688,
        "learning_rate": 0.00014502721170195343,
        "epoch": 0.3525781149543526,
        "step": 2742
    },
    {
        "loss": 1.0273,
        "grad_norm": 1.3500792980194092,
        "learning_rate": 0.00014499106912017678,
        "epoch": 0.35270669924135273,
        "step": 2743
    },
    {
        "loss": 2.0796,
        "grad_norm": 1.798166275024414,
        "learning_rate": 0.00014495491916820857,
        "epoch": 0.35283528352835286,
        "step": 2744
    },
    {
        "loss": 2.5498,
        "grad_norm": 1.865612268447876,
        "learning_rate": 0.00014491876185197065,
        "epoch": 0.35296386781535294,
        "step": 2745
    },
    {
        "loss": 2.0104,
        "grad_norm": 1.6227245330810547,
        "learning_rate": 0.00014488259717738622,
        "epoch": 0.35309245210235307,
        "step": 2746
    },
    {
        "loss": 2.1514,
        "grad_norm": 2.006411075592041,
        "learning_rate": 0.00014484642515037944,
        "epoch": 0.3532210363893532,
        "step": 2747
    },
    {
        "loss": 2.1895,
        "grad_norm": 1.9358830451965332,
        "learning_rate": 0.0001448102457768759,
        "epoch": 0.35334962067635334,
        "step": 2748
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.6072087287902832,
        "learning_rate": 0.0001447740590628023,
        "epoch": 0.35347820496335347,
        "step": 2749
    },
    {
        "loss": 2.1226,
        "grad_norm": 0.866890013217926,
        "learning_rate": 0.00014473786501408652,
        "epoch": 0.3536067892503536,
        "step": 2750
    },
    {
        "loss": 1.5559,
        "grad_norm": 2.110182523727417,
        "learning_rate": 0.00014470166363665774,
        "epoch": 0.35373537353735374,
        "step": 2751
    },
    {
        "loss": 1.9721,
        "grad_norm": 2.1825735569000244,
        "learning_rate": 0.00014466545493644617,
        "epoch": 0.35386395782435387,
        "step": 2752
    },
    {
        "loss": 1.9144,
        "grad_norm": 2.4520699977874756,
        "learning_rate": 0.0001446292389193834,
        "epoch": 0.353992542111354,
        "step": 2753
    },
    {
        "loss": 1.5547,
        "grad_norm": 2.078270673751831,
        "learning_rate": 0.00014459301559140213,
        "epoch": 0.35412112639835414,
        "step": 2754
    },
    {
        "loss": 1.9359,
        "grad_norm": 1.6082274913787842,
        "learning_rate": 0.00014455678495843623,
        "epoch": 0.35424971068535427,
        "step": 2755
    },
    {
        "loss": 1.697,
        "grad_norm": 2.1057560443878174,
        "learning_rate": 0.00014452054702642083,
        "epoch": 0.3543782949723544,
        "step": 2756
    },
    {
        "loss": 1.6816,
        "grad_norm": 2.164029121398926,
        "learning_rate": 0.00014448430180129224,
        "epoch": 0.3545068792593545,
        "step": 2757
    },
    {
        "loss": 1.4076,
        "grad_norm": 2.1682512760162354,
        "learning_rate": 0.00014444804928898797,
        "epoch": 0.3546354635463546,
        "step": 2758
    },
    {
        "loss": 1.6011,
        "grad_norm": 1.8505873680114746,
        "learning_rate": 0.00014441178949544664,
        "epoch": 0.35476404783335475,
        "step": 2759
    },
    {
        "loss": 2.1448,
        "grad_norm": 1.4604995250701904,
        "learning_rate": 0.00014437552242660819,
        "epoch": 0.3548926321203549,
        "step": 2760
    },
    {
        "loss": 2.2932,
        "grad_norm": 1.5643008947372437,
        "learning_rate": 0.00014433924808841366,
        "epoch": 0.355021216407355,
        "step": 2761
    },
    {
        "loss": 1.6893,
        "grad_norm": 2.0317788124084473,
        "learning_rate": 0.00014430296648680537,
        "epoch": 0.35514980069435514,
        "step": 2762
    },
    {
        "loss": 2.1912,
        "grad_norm": 2.018325090408325,
        "learning_rate": 0.0001442666776277267,
        "epoch": 0.3552783849813553,
        "step": 2763
    },
    {
        "loss": 1.7222,
        "grad_norm": 2.879772424697876,
        "learning_rate": 0.00014423038151712236,
        "epoch": 0.3554069692683554,
        "step": 2764
    },
    {
        "loss": 2.1139,
        "grad_norm": 1.8175023794174194,
        "learning_rate": 0.00014419407816093814,
        "epoch": 0.35553555355535554,
        "step": 2765
    },
    {
        "loss": 1.9257,
        "grad_norm": 1.8065515756607056,
        "learning_rate": 0.00014415776756512104,
        "epoch": 0.3556641378423557,
        "step": 2766
    },
    {
        "loss": 1.3405,
        "grad_norm": 2.1664388179779053,
        "learning_rate": 0.00014412144973561934,
        "epoch": 0.3557927221293558,
        "step": 2767
    },
    {
        "loss": 1.9493,
        "grad_norm": 1.7015633583068848,
        "learning_rate": 0.00014408512467838235,
        "epoch": 0.35592130641635594,
        "step": 2768
    },
    {
        "loss": 1.5151,
        "grad_norm": 2.0077285766601562,
        "learning_rate": 0.0001440487923993607,
        "epoch": 0.3560498907033561,
        "step": 2769
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.057689666748047,
        "learning_rate": 0.0001440124529045061,
        "epoch": 0.35617847499035615,
        "step": 2770
    },
    {
        "loss": 2.6831,
        "grad_norm": 1.3702541589736938,
        "learning_rate": 0.0001439761061997715,
        "epoch": 0.3563070592773563,
        "step": 2771
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.7318156957626343,
        "learning_rate": 0.00014393975229111104,
        "epoch": 0.3564356435643564,
        "step": 2772
    },
    {
        "loss": 1.7513,
        "grad_norm": 1.454463005065918,
        "learning_rate": 0.00014390339118447999,
        "epoch": 0.35656422785135655,
        "step": 2773
    },
    {
        "loss": 1.5716,
        "grad_norm": 2.445953369140625,
        "learning_rate": 0.00014386702288583488,
        "epoch": 0.3566928121383567,
        "step": 2774
    },
    {
        "loss": 2.3191,
        "grad_norm": 1.6422361135482788,
        "learning_rate": 0.0001438306474011333,
        "epoch": 0.3568213964253568,
        "step": 2775
    },
    {
        "loss": 2.0965,
        "grad_norm": 1.798203468322754,
        "learning_rate": 0.00014379426473633413,
        "epoch": 0.35694998071235695,
        "step": 2776
    },
    {
        "loss": 2.1549,
        "grad_norm": 1.9688512086868286,
        "learning_rate": 0.00014375787489739734,
        "epoch": 0.3570785649993571,
        "step": 2777
    },
    {
        "loss": 1.8035,
        "grad_norm": 1.6021571159362793,
        "learning_rate": 0.00014372147789028415,
        "epoch": 0.3572071492863572,
        "step": 2778
    },
    {
        "loss": 1.4495,
        "grad_norm": 2.202087640762329,
        "learning_rate": 0.00014368507372095691,
        "epoch": 0.35733573357335735,
        "step": 2779
    },
    {
        "loss": 1.3674,
        "grad_norm": 2.3041157722473145,
        "learning_rate": 0.00014364866239537915,
        "epoch": 0.3574643178603575,
        "step": 2780
    },
    {
        "loss": 1.4059,
        "grad_norm": 2.0592432022094727,
        "learning_rate": 0.00014361224391951555,
        "epoch": 0.3575929021473576,
        "step": 2781
    },
    {
        "loss": 2.6179,
        "grad_norm": 2.024075746536255,
        "learning_rate": 0.00014357581829933206,
        "epoch": 0.3577214864343577,
        "step": 2782
    },
    {
        "loss": 1.903,
        "grad_norm": 2.2872555255889893,
        "learning_rate": 0.00014353938554079561,
        "epoch": 0.3578500707213578,
        "step": 2783
    },
    {
        "loss": 2.0876,
        "grad_norm": 1.5279432535171509,
        "learning_rate": 0.00014350294564987456,
        "epoch": 0.35797865500835796,
        "step": 2784
    },
    {
        "loss": 2.2903,
        "grad_norm": 1.6570125818252563,
        "learning_rate": 0.00014346649863253817,
        "epoch": 0.3581072392953581,
        "step": 2785
    },
    {
        "loss": 1.9625,
        "grad_norm": 2.0375072956085205,
        "learning_rate": 0.00014343004449475703,
        "epoch": 0.3582358235823582,
        "step": 2786
    },
    {
        "loss": 1.1695,
        "grad_norm": 2.306246042251587,
        "learning_rate": 0.0001433935832425029,
        "epoch": 0.35836440786935836,
        "step": 2787
    },
    {
        "loss": 1.8903,
        "grad_norm": 2.095660924911499,
        "learning_rate": 0.0001433571148817486,
        "epoch": 0.3584929921563585,
        "step": 2788
    },
    {
        "loss": 1.6125,
        "grad_norm": 1.981736421585083,
        "learning_rate": 0.0001433206394184682,
        "epoch": 0.3586215764433586,
        "step": 2789
    },
    {
        "loss": 2.2535,
        "grad_norm": 1.828640103340149,
        "learning_rate": 0.00014328415685863695,
        "epoch": 0.35875016073035876,
        "step": 2790
    },
    {
        "loss": 1.7032,
        "grad_norm": 1.6216554641723633,
        "learning_rate": 0.00014324766720823123,
        "epoch": 0.3588787450173589,
        "step": 2791
    },
    {
        "loss": 1.3476,
        "grad_norm": 2.2240724563598633,
        "learning_rate": 0.00014321117047322846,
        "epoch": 0.359007329304359,
        "step": 2792
    },
    {
        "loss": 2.2576,
        "grad_norm": 1.1364519596099854,
        "learning_rate": 0.00014317466665960746,
        "epoch": 0.35913591359135916,
        "step": 2793
    },
    {
        "loss": 1.551,
        "grad_norm": 2.5700531005859375,
        "learning_rate": 0.000143138155773348,
        "epoch": 0.3592644978783593,
        "step": 2794
    },
    {
        "loss": 1.6243,
        "grad_norm": 1.6741005182266235,
        "learning_rate": 0.0001431016378204312,
        "epoch": 0.35939308216535937,
        "step": 2795
    },
    {
        "loss": 1.5069,
        "grad_norm": 2.0278477668762207,
        "learning_rate": 0.00014306511280683915,
        "epoch": 0.3595216664523595,
        "step": 2796
    },
    {
        "loss": 2.6606,
        "grad_norm": 1.7487602233886719,
        "learning_rate": 0.00014302858073855513,
        "epoch": 0.35965025073935963,
        "step": 2797
    },
    {
        "loss": 1.9394,
        "grad_norm": 1.965453028678894,
        "learning_rate": 0.00014299204162156374,
        "epoch": 0.35977883502635977,
        "step": 2798
    },
    {
        "loss": 1.803,
        "grad_norm": 1.6148267984390259,
        "learning_rate": 0.00014295549546185059,
        "epoch": 0.3599074193133599,
        "step": 2799
    },
    {
        "loss": 1.5496,
        "grad_norm": 1.8997617959976196,
        "learning_rate": 0.00014291894226540244,
        "epoch": 0.36003600360036003,
        "step": 2800
    },
    {
        "eval_loss": 1.949604868888855,
        "eval_runtime": 28.2805,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.36003600360036003,
        "step": 2800
    },
    {
        "loss": 2.3452,
        "grad_norm": 1.851650595664978,
        "learning_rate": 0.00014288238203820724,
        "epoch": 0.36016458788736017,
        "step": 2801
    },
    {
        "loss": 2.5071,
        "grad_norm": 2.0138888359069824,
        "learning_rate": 0.00014284581478625404,
        "epoch": 0.3602931721743603,
        "step": 2802
    },
    {
        "loss": 1.3951,
        "grad_norm": 2.201103925704956,
        "learning_rate": 0.0001428092405155332,
        "epoch": 0.36042175646136043,
        "step": 2803
    },
    {
        "loss": 1.6629,
        "grad_norm": 1.7292912006378174,
        "learning_rate": 0.00014277265923203603,
        "epoch": 0.36055034074836056,
        "step": 2804
    },
    {
        "loss": 1.7484,
        "grad_norm": 1.3722259998321533,
        "learning_rate": 0.00014273607094175515,
        "epoch": 0.3606789250353607,
        "step": 2805
    },
    {
        "loss": 2.3936,
        "grad_norm": 1.3817145824432373,
        "learning_rate": 0.0001426994756506842,
        "epoch": 0.36080750932236083,
        "step": 2806
    },
    {
        "loss": 2.1433,
        "grad_norm": 1.573155164718628,
        "learning_rate": 0.000142662873364818,
        "epoch": 0.3609360936093609,
        "step": 2807
    },
    {
        "loss": 1.8007,
        "grad_norm": 1.8527522087097168,
        "learning_rate": 0.00014262626409015255,
        "epoch": 0.36106467789636104,
        "step": 2808
    },
    {
        "loss": 1.3313,
        "grad_norm": 1.7502243518829346,
        "learning_rate": 0.00014258964783268503,
        "epoch": 0.3611932621833612,
        "step": 2809
    },
    {
        "loss": 2.3047,
        "grad_norm": 1.6878212690353394,
        "learning_rate": 0.00014255302459841368,
        "epoch": 0.3613218464703613,
        "step": 2810
    },
    {
        "loss": 1.6449,
        "grad_norm": 1.6338049173355103,
        "learning_rate": 0.00014251639439333792,
        "epoch": 0.36145043075736144,
        "step": 2811
    },
    {
        "loss": 2.2946,
        "grad_norm": 1.3103822469711304,
        "learning_rate": 0.00014247975722345832,
        "epoch": 0.3615790150443616,
        "step": 2812
    },
    {
        "loss": 1.6926,
        "grad_norm": 1.2494714260101318,
        "learning_rate": 0.00014244311309477656,
        "epoch": 0.3617075993313617,
        "step": 2813
    },
    {
        "loss": 2.0345,
        "grad_norm": 1.516641616821289,
        "learning_rate": 0.00014240646201329552,
        "epoch": 0.36183618361836184,
        "step": 2814
    },
    {
        "loss": 2.1391,
        "grad_norm": 1.9387195110321045,
        "learning_rate": 0.00014236980398501912,
        "epoch": 0.361964767905362,
        "step": 2815
    },
    {
        "loss": 1.897,
        "grad_norm": 2.0326411724090576,
        "learning_rate": 0.00014233313901595255,
        "epoch": 0.3620933521923621,
        "step": 2816
    },
    {
        "loss": 2.392,
        "grad_norm": 1.809516191482544,
        "learning_rate": 0.00014229646711210202,
        "epoch": 0.36222193647936224,
        "step": 2817
    },
    {
        "loss": 1.848,
        "grad_norm": 1.6521316766738892,
        "learning_rate": 0.00014225978827947492,
        "epoch": 0.36235052076636237,
        "step": 2818
    },
    {
        "loss": 2.4948,
        "grad_norm": 1.7348283529281616,
        "learning_rate": 0.0001422231025240798,
        "epoch": 0.3624791050533625,
        "step": 2819
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.3089100122451782,
        "learning_rate": 0.0001421864098519263,
        "epoch": 0.3626076893403626,
        "step": 2820
    },
    {
        "loss": 1.597,
        "grad_norm": 2.5552961826324463,
        "learning_rate": 0.00014214971026902523,
        "epoch": 0.3627362736273627,
        "step": 2821
    },
    {
        "loss": 2.0167,
        "grad_norm": 1.2482519149780273,
        "learning_rate": 0.00014211300378138853,
        "epoch": 0.36286485791436285,
        "step": 2822
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.899137258529663,
        "learning_rate": 0.0001420762903950292,
        "epoch": 0.362993442201363,
        "step": 2823
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.4031801223754883,
        "learning_rate": 0.00014203957011596144,
        "epoch": 0.3631220264883631,
        "step": 2824
    },
    {
        "loss": 2.2169,
        "grad_norm": 1.741650104522705,
        "learning_rate": 0.00014200284295020062,
        "epoch": 0.36325061077536325,
        "step": 2825
    },
    {
        "loss": 1.9,
        "grad_norm": 1.4607239961624146,
        "learning_rate": 0.0001419661089037631,
        "epoch": 0.3633791950623634,
        "step": 2826
    },
    {
        "loss": 1.271,
        "grad_norm": 1.44520103931427,
        "learning_rate": 0.00014192936798266656,
        "epoch": 0.3635077793493635,
        "step": 2827
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.1025930643081665,
        "learning_rate": 0.00014189262019292964,
        "epoch": 0.36363636363636365,
        "step": 2828
    },
    {
        "loss": 1.9542,
        "grad_norm": 1.8640934228897095,
        "learning_rate": 0.0001418558655405721,
        "epoch": 0.3637649479233638,
        "step": 2829
    },
    {
        "loss": 1.5647,
        "grad_norm": 2.926832675933838,
        "learning_rate": 0.000141819104031615,
        "epoch": 0.3638935322103639,
        "step": 2830
    },
    {
        "loss": 1.1727,
        "grad_norm": 3.139331102371216,
        "learning_rate": 0.00014178233567208035,
        "epoch": 0.36402211649736405,
        "step": 2831
    },
    {
        "loss": 1.8821,
        "grad_norm": 2.1476387977600098,
        "learning_rate": 0.00014174556046799133,
        "epoch": 0.3641507007843641,
        "step": 2832
    },
    {
        "loss": 1.2692,
        "grad_norm": 2.2145655155181885,
        "learning_rate": 0.0001417087784253723,
        "epoch": 0.36427928507136426,
        "step": 2833
    },
    {
        "loss": 1.624,
        "grad_norm": 2.311981439590454,
        "learning_rate": 0.00014167198955024866,
        "epoch": 0.3644078693583644,
        "step": 2834
    },
    {
        "loss": 1.1392,
        "grad_norm": 2.3230795860290527,
        "learning_rate": 0.00014163519384864697,
        "epoch": 0.3645364536453645,
        "step": 2835
    },
    {
        "loss": 2.1113,
        "grad_norm": 2.008268117904663,
        "learning_rate": 0.0001415983913265949,
        "epoch": 0.36466503793236466,
        "step": 2836
    },
    {
        "loss": 1.5781,
        "grad_norm": 1.737512230873108,
        "learning_rate": 0.0001415615819901212,
        "epoch": 0.3647936222193648,
        "step": 2837
    },
    {
        "loss": 1.6639,
        "grad_norm": 1.842943787574768,
        "learning_rate": 0.00014152476584525586,
        "epoch": 0.3649222065063649,
        "step": 2838
    },
    {
        "loss": 1.8672,
        "grad_norm": 2.2584798336029053,
        "learning_rate": 0.00014148794289802982,
        "epoch": 0.36505079079336505,
        "step": 2839
    },
    {
        "loss": 1.9347,
        "grad_norm": 2.9497082233428955,
        "learning_rate": 0.00014145111315447528,
        "epoch": 0.3651793750803652,
        "step": 2840
    },
    {
        "loss": 1.6152,
        "grad_norm": 2.2897934913635254,
        "learning_rate": 0.00014141427662062544,
        "epoch": 0.3653079593673653,
        "step": 2841
    },
    {
        "loss": 2.1814,
        "grad_norm": 1.5057412385940552,
        "learning_rate": 0.00014137743330251468,
        "epoch": 0.36543654365436545,
        "step": 2842
    },
    {
        "loss": 2.3618,
        "grad_norm": 1.3844422101974487,
        "learning_rate": 0.00014134058320617847,
        "epoch": 0.3655651279413656,
        "step": 2843
    },
    {
        "loss": 1.503,
        "grad_norm": 1.629267930984497,
        "learning_rate": 0.00014130372633765337,
        "epoch": 0.3656937122283657,
        "step": 2844
    },
    {
        "loss": 2.4207,
        "grad_norm": 1.1484408378601074,
        "learning_rate": 0.0001412668627029771,
        "epoch": 0.3658222965153658,
        "step": 2845
    },
    {
        "loss": 1.5954,
        "grad_norm": 1.590480089187622,
        "learning_rate": 0.00014122999230818846,
        "epoch": 0.36595088080236593,
        "step": 2846
    },
    {
        "loss": 1.2409,
        "grad_norm": 2.067335605621338,
        "learning_rate": 0.00014119311515932735,
        "epoch": 0.36607946508936606,
        "step": 2847
    },
    {
        "loss": 2.1794,
        "grad_norm": 1.3052788972854614,
        "learning_rate": 0.00014115623126243477,
        "epoch": 0.3662080493763662,
        "step": 2848
    },
    {
        "loss": 2.089,
        "grad_norm": 1.4331940412521362,
        "learning_rate": 0.00014111934062355284,
        "epoch": 0.36633663366336633,
        "step": 2849
    },
    {
        "loss": 1.7598,
        "grad_norm": 2.0095014572143555,
        "learning_rate": 0.00014108244324872478,
        "epoch": 0.36646521795036646,
        "step": 2850
    },
    {
        "loss": 1.7082,
        "grad_norm": 1.7915318012237549,
        "learning_rate": 0.00014104553914399495,
        "epoch": 0.3665938022373666,
        "step": 2851
    },
    {
        "loss": 1.4024,
        "grad_norm": 1.982215404510498,
        "learning_rate": 0.00014100862831540876,
        "epoch": 0.36672238652436673,
        "step": 2852
    },
    {
        "loss": 1.8562,
        "grad_norm": 1.8999888896942139,
        "learning_rate": 0.00014097171076901275,
        "epoch": 0.36685097081136686,
        "step": 2853
    },
    {
        "loss": 2.1114,
        "grad_norm": 1.4416521787643433,
        "learning_rate": 0.00014093478651085452,
        "epoch": 0.366979555098367,
        "step": 2854
    },
    {
        "loss": 1.9774,
        "grad_norm": 1.8398240804672241,
        "learning_rate": 0.00014089785554698283,
        "epoch": 0.3671081393853671,
        "step": 2855
    },
    {
        "loss": 1.5285,
        "grad_norm": 1.1346056461334229,
        "learning_rate": 0.0001408609178834475,
        "epoch": 0.36723672367236726,
        "step": 2856
    },
    {
        "loss": 1.8646,
        "grad_norm": 2.581148624420166,
        "learning_rate": 0.00014082397352629947,
        "epoch": 0.36736530795936734,
        "step": 2857
    },
    {
        "loss": 1.3413,
        "grad_norm": 1.2083771228790283,
        "learning_rate": 0.00014078702248159072,
        "epoch": 0.36749389224636747,
        "step": 2858
    },
    {
        "loss": 2.234,
        "grad_norm": 1.7324705123901367,
        "learning_rate": 0.00014075006475537444,
        "epoch": 0.3676224765333676,
        "step": 2859
    },
    {
        "loss": 1.6314,
        "grad_norm": 2.422821283340454,
        "learning_rate": 0.00014071310035370472,
        "epoch": 0.36775106082036774,
        "step": 2860
    },
    {
        "loss": 1.4037,
        "grad_norm": 2.440934896469116,
        "learning_rate": 0.00014067612928263704,
        "epoch": 0.36787964510736787,
        "step": 2861
    },
    {
        "loss": 2.3609,
        "grad_norm": 3.1354997158050537,
        "learning_rate": 0.00014063915154822765,
        "epoch": 0.368008229394368,
        "step": 2862
    },
    {
        "loss": 1.7399,
        "grad_norm": 1.8628395795822144,
        "learning_rate": 0.00014060216715653412,
        "epoch": 0.36813681368136814,
        "step": 2863
    },
    {
        "loss": 1.3329,
        "grad_norm": 2.151622772216797,
        "learning_rate": 0.000140565176113615,
        "epoch": 0.36826539796836827,
        "step": 2864
    },
    {
        "loss": 1.4105,
        "grad_norm": 2.716411590576172,
        "learning_rate": 0.00014052817842552997,
        "epoch": 0.3683939822553684,
        "step": 2865
    },
    {
        "loss": 1.2293,
        "grad_norm": 1.5792726278305054,
        "learning_rate": 0.0001404911740983398,
        "epoch": 0.36852256654236853,
        "step": 2866
    },
    {
        "loss": 1.4146,
        "grad_norm": 1.9132680892944336,
        "learning_rate": 0.0001404541631381063,
        "epoch": 0.36865115082936867,
        "step": 2867
    },
    {
        "loss": 1.1927,
        "grad_norm": 1.7264959812164307,
        "learning_rate": 0.00014041714555089246,
        "epoch": 0.3687797351163688,
        "step": 2868
    },
    {
        "loss": 2.1156,
        "grad_norm": 2.06638765335083,
        "learning_rate": 0.0001403801213427623,
        "epoch": 0.36890831940336893,
        "step": 2869
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.8481616973876953,
        "learning_rate": 0.0001403430905197808,
        "epoch": 0.369036903690369,
        "step": 2870
    },
    {
        "loss": 1.8755,
        "grad_norm": 2.4206111431121826,
        "learning_rate": 0.0001403060530880143,
        "epoch": 0.36916548797736914,
        "step": 2871
    },
    {
        "loss": 1.7127,
        "grad_norm": 2.3891286849975586,
        "learning_rate": 0.00014026900905353003,
        "epoch": 0.3692940722643693,
        "step": 2872
    },
    {
        "loss": 1.8947,
        "grad_norm": 2.376858949661255,
        "learning_rate": 0.0001402319584223963,
        "epoch": 0.3694226565513694,
        "step": 2873
    },
    {
        "loss": 2.0587,
        "grad_norm": 1.8671777248382568,
        "learning_rate": 0.00014019490120068255,
        "epoch": 0.36955124083836954,
        "step": 2874
    },
    {
        "loss": 2.4589,
        "grad_norm": 1.5539538860321045,
        "learning_rate": 0.00014015783739445932,
        "epoch": 0.3696798251253697,
        "step": 2875
    },
    {
        "loss": 1.6096,
        "grad_norm": 2.2810423374176025,
        "learning_rate": 0.00014012076700979818,
        "epoch": 0.3698084094123698,
        "step": 2876
    },
    {
        "loss": 2.3942,
        "grad_norm": 1.2811847925186157,
        "learning_rate": 0.0001400836900527718,
        "epoch": 0.36993699369936994,
        "step": 2877
    },
    {
        "loss": 2.3766,
        "grad_norm": 1.5392353534698486,
        "learning_rate": 0.00014004660652945395,
        "epoch": 0.3700655779863701,
        "step": 2878
    },
    {
        "loss": 2.2341,
        "grad_norm": 1.382462739944458,
        "learning_rate": 0.00014000951644591942,
        "epoch": 0.3701941622733702,
        "step": 2879
    },
    {
        "loss": 1.8273,
        "grad_norm": 2.1579408645629883,
        "learning_rate": 0.0001399724198082441,
        "epoch": 0.37032274656037034,
        "step": 2880
    },
    {
        "loss": 2.283,
        "grad_norm": 1.6879839897155762,
        "learning_rate": 0.000139935316622505,
        "epoch": 0.3704513308473705,
        "step": 2881
    },
    {
        "loss": 2.1783,
        "grad_norm": 1.6444965600967407,
        "learning_rate": 0.00013989820689478015,
        "epoch": 0.37057991513437055,
        "step": 2882
    },
    {
        "loss": 2.2222,
        "grad_norm": 1.6553009748458862,
        "learning_rate": 0.00013986109063114863,
        "epoch": 0.3707084994213707,
        "step": 2883
    },
    {
        "loss": 2.3498,
        "grad_norm": 1.781212568283081,
        "learning_rate": 0.00013982396783769063,
        "epoch": 0.3708370837083708,
        "step": 2884
    },
    {
        "loss": 1.8623,
        "grad_norm": 1.7995257377624512,
        "learning_rate": 0.00013978683852048743,
        "epoch": 0.37096566799537095,
        "step": 2885
    },
    {
        "loss": 2.2917,
        "grad_norm": 1.2049411535263062,
        "learning_rate": 0.00013974970268562133,
        "epoch": 0.3710942522823711,
        "step": 2886
    },
    {
        "loss": 2.3305,
        "grad_norm": 1.3409241437911987,
        "learning_rate": 0.00013971256033917575,
        "epoch": 0.3712228365693712,
        "step": 2887
    },
    {
        "loss": 1.2237,
        "grad_norm": 2.008659839630127,
        "learning_rate": 0.00013967541148723512,
        "epoch": 0.37135142085637135,
        "step": 2888
    },
    {
        "loss": 2.2439,
        "grad_norm": 1.2850635051727295,
        "learning_rate": 0.00013963825613588494,
        "epoch": 0.3714800051433715,
        "step": 2889
    },
    {
        "loss": 1.8135,
        "grad_norm": 1.8044685125350952,
        "learning_rate": 0.00013960109429121188,
        "epoch": 0.3716085894303716,
        "step": 2890
    },
    {
        "loss": 1.726,
        "grad_norm": 2.0761008262634277,
        "learning_rate": 0.00013956392595930347,
        "epoch": 0.37173717371737175,
        "step": 2891
    },
    {
        "loss": 2.0107,
        "grad_norm": 2.8605568408966064,
        "learning_rate": 0.00013952675114624853,
        "epoch": 0.3718657580043719,
        "step": 2892
    },
    {
        "loss": 1.7579,
        "grad_norm": 1.6571478843688965,
        "learning_rate": 0.0001394895698581368,
        "epoch": 0.371994342291372,
        "step": 2893
    },
    {
        "loss": 1.3539,
        "grad_norm": 2.0508084297180176,
        "learning_rate": 0.0001394523821010591,
        "epoch": 0.37212292657837215,
        "step": 2894
    },
    {
        "loss": 2.3043,
        "grad_norm": 2.1618244647979736,
        "learning_rate": 0.0001394151878811073,
        "epoch": 0.3722515108653722,
        "step": 2895
    },
    {
        "loss": 1.4933,
        "grad_norm": 1.942440152168274,
        "learning_rate": 0.00013937798720437444,
        "epoch": 0.37238009515237236,
        "step": 2896
    },
    {
        "loss": 2.2057,
        "grad_norm": 1.430894136428833,
        "learning_rate": 0.00013934078007695448,
        "epoch": 0.3725086794393725,
        "step": 2897
    },
    {
        "loss": 1.9898,
        "grad_norm": 1.9940624237060547,
        "learning_rate": 0.00013930356650494248,
        "epoch": 0.3726372637263726,
        "step": 2898
    },
    {
        "loss": 1.8244,
        "grad_norm": 1.9070895910263062,
        "learning_rate": 0.00013926634649443458,
        "epoch": 0.37276584801337276,
        "step": 2899
    },
    {
        "loss": 1.7586,
        "grad_norm": 2.227257013320923,
        "learning_rate": 0.00013922912005152793,
        "epoch": 0.3728944323003729,
        "step": 2900
    },
    {
        "eval_loss": 1.9349820613861084,
        "eval_runtime": 28.2593,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.3728944323003729,
        "step": 2900
    },
    {
        "loss": 2.0218,
        "grad_norm": 2.8774871826171875,
        "learning_rate": 0.00013919188718232082,
        "epoch": 0.373023016587373,
        "step": 2901
    },
    {
        "loss": 1.9181,
        "grad_norm": 1.6602458953857422,
        "learning_rate": 0.00013915464789291247,
        "epoch": 0.37315160087437316,
        "step": 2902
    },
    {
        "loss": 1.5117,
        "grad_norm": 2.2486765384674072,
        "learning_rate": 0.00013911740218940327,
        "epoch": 0.3732801851613733,
        "step": 2903
    },
    {
        "loss": 1.9106,
        "grad_norm": 2.0201189517974854,
        "learning_rate": 0.00013908015007789461,
        "epoch": 0.3734087694483734,
        "step": 2904
    },
    {
        "loss": 1.4955,
        "grad_norm": 1.5955034494400024,
        "learning_rate": 0.00013904289156448888,
        "epoch": 0.37353735373537356,
        "step": 2905
    },
    {
        "loss": 1.8783,
        "grad_norm": 1.9158251285552979,
        "learning_rate": 0.00013900562665528958,
        "epoch": 0.3736659380223737,
        "step": 2906
    },
    {
        "loss": 1.6771,
        "grad_norm": 1.9367311000823975,
        "learning_rate": 0.00013896835535640126,
        "epoch": 0.37379452230937377,
        "step": 2907
    },
    {
        "loss": 1.7958,
        "grad_norm": 2.5857672691345215,
        "learning_rate": 0.00013893107767392953,
        "epoch": 0.3739231065963739,
        "step": 2908
    },
    {
        "loss": 2.1881,
        "grad_norm": 1.6717560291290283,
        "learning_rate": 0.00013889379361398097,
        "epoch": 0.37405169088337403,
        "step": 2909
    },
    {
        "loss": 2.6091,
        "grad_norm": 1.4651587009429932,
        "learning_rate": 0.00013885650318266323,
        "epoch": 0.37418027517037417,
        "step": 2910
    },
    {
        "loss": 2.1074,
        "grad_norm": 2.1369338035583496,
        "learning_rate": 0.0001388192063860851,
        "epoch": 0.3743088594573743,
        "step": 2911
    },
    {
        "loss": 2.0881,
        "grad_norm": 1.9708774089813232,
        "learning_rate": 0.00013878190323035627,
        "epoch": 0.37443744374437443,
        "step": 2912
    },
    {
        "loss": 1.6372,
        "grad_norm": 2.4268407821655273,
        "learning_rate": 0.00013874459372158757,
        "epoch": 0.37456602803137456,
        "step": 2913
    },
    {
        "loss": 1.8167,
        "grad_norm": 1.1778972148895264,
        "learning_rate": 0.00013870727786589087,
        "epoch": 0.3746946123183747,
        "step": 2914
    },
    {
        "loss": 2.146,
        "grad_norm": 1.3341864347457886,
        "learning_rate": 0.000138669955669379,
        "epoch": 0.37482319660537483,
        "step": 2915
    },
    {
        "loss": 2.4421,
        "grad_norm": 1.2841020822525024,
        "learning_rate": 0.00013863262713816589,
        "epoch": 0.37495178089237496,
        "step": 2916
    },
    {
        "loss": 2.1075,
        "grad_norm": 1.8527381420135498,
        "learning_rate": 0.0001385952922783665,
        "epoch": 0.3750803651793751,
        "step": 2917
    },
    {
        "loss": 1.9421,
        "grad_norm": 2.1377573013305664,
        "learning_rate": 0.00013855795109609683,
        "epoch": 0.37520894946637523,
        "step": 2918
    },
    {
        "loss": 2.3381,
        "grad_norm": 1.8137482404708862,
        "learning_rate": 0.00013852060359747395,
        "epoch": 0.37533753375337536,
        "step": 2919
    },
    {
        "loss": 1.2019,
        "grad_norm": 1.8111087083816528,
        "learning_rate": 0.00013848324978861583,
        "epoch": 0.37546611804037544,
        "step": 2920
    },
    {
        "loss": 2.109,
        "grad_norm": 2.171308755874634,
        "learning_rate": 0.0001384458896756416,
        "epoch": 0.3755947023273756,
        "step": 2921
    },
    {
        "loss": 1.8502,
        "grad_norm": 1.5722169876098633,
        "learning_rate": 0.00013840852326467147,
        "epoch": 0.3757232866143757,
        "step": 2922
    },
    {
        "loss": 2.1738,
        "grad_norm": 2.032107353210449,
        "learning_rate": 0.00013837115056182647,
        "epoch": 0.37585187090137584,
        "step": 2923
    },
    {
        "loss": 1.6175,
        "grad_norm": 2.0591633319854736,
        "learning_rate": 0.0001383337715732289,
        "epoch": 0.375980455188376,
        "step": 2924
    },
    {
        "loss": 1.7162,
        "grad_norm": 1.8219776153564453,
        "learning_rate": 0.00013829638630500192,
        "epoch": 0.3761090394753761,
        "step": 2925
    },
    {
        "loss": 1.3451,
        "grad_norm": 1.7363808155059814,
        "learning_rate": 0.0001382589947632698,
        "epoch": 0.37623762376237624,
        "step": 2926
    },
    {
        "loss": 2.2262,
        "grad_norm": 1.4369412660598755,
        "learning_rate": 0.00013822159695415785,
        "epoch": 0.37636620804937637,
        "step": 2927
    },
    {
        "loss": 1.5226,
        "grad_norm": 1.8073747158050537,
        "learning_rate": 0.00013818419288379232,
        "epoch": 0.3764947923363765,
        "step": 2928
    },
    {
        "loss": 2.3132,
        "grad_norm": 1.0082166194915771,
        "learning_rate": 0.0001381467825583006,
        "epoch": 0.37662337662337664,
        "step": 2929
    },
    {
        "loss": 2.0065,
        "grad_norm": 1.517435908317566,
        "learning_rate": 0.000138109365983811,
        "epoch": 0.37675196091037677,
        "step": 2930
    },
    {
        "loss": 2.2155,
        "grad_norm": 1.8342583179473877,
        "learning_rate": 0.00013807194316645287,
        "epoch": 0.3768805451973769,
        "step": 2931
    },
    {
        "loss": 1.017,
        "grad_norm": 1.796554446220398,
        "learning_rate": 0.00013803451411235668,
        "epoch": 0.37700912948437704,
        "step": 2932
    },
    {
        "loss": 1.9889,
        "grad_norm": 1.9168027639389038,
        "learning_rate": 0.00013799707882765382,
        "epoch": 0.3771377137713771,
        "step": 2933
    },
    {
        "loss": 1.6635,
        "grad_norm": 1.7539342641830444,
        "learning_rate": 0.00013795963731847675,
        "epoch": 0.37726629805837725,
        "step": 2934
    },
    {
        "loss": 2.0342,
        "grad_norm": 2.0900356769561768,
        "learning_rate": 0.00013792218959095894,
        "epoch": 0.3773948823453774,
        "step": 2935
    },
    {
        "loss": 2.3904,
        "grad_norm": 1.002886176109314,
        "learning_rate": 0.00013788473565123482,
        "epoch": 0.3775234666323775,
        "step": 2936
    },
    {
        "loss": 1.8773,
        "grad_norm": 1.8647794723510742,
        "learning_rate": 0.00013784727550544,
        "epoch": 0.37765205091937765,
        "step": 2937
    },
    {
        "loss": 2.2611,
        "grad_norm": 1.7106512784957886,
        "learning_rate": 0.00013780980915971086,
        "epoch": 0.3777806352063778,
        "step": 2938
    },
    {
        "loss": 2.1039,
        "grad_norm": 1.9625552892684937,
        "learning_rate": 0.000137772336620185,
        "epoch": 0.3779092194933779,
        "step": 2939
    },
    {
        "loss": 2.007,
        "grad_norm": 2.4527809619903564,
        "learning_rate": 0.00013773485789300095,
        "epoch": 0.37803780378037805,
        "step": 2940
    },
    {
        "loss": 2.4622,
        "grad_norm": 1.0098443031311035,
        "learning_rate": 0.00013769737298429833,
        "epoch": 0.3781663880673782,
        "step": 2941
    },
    {
        "loss": 2.0303,
        "grad_norm": 1.9622024297714233,
        "learning_rate": 0.00013765988190021764,
        "epoch": 0.3782949723543783,
        "step": 2942
    },
    {
        "loss": 2.1266,
        "grad_norm": 1.6216180324554443,
        "learning_rate": 0.00013762238464690052,
        "epoch": 0.37842355664137844,
        "step": 2943
    },
    {
        "loss": 2.0128,
        "grad_norm": 1.7190333604812622,
        "learning_rate": 0.0001375848812304895,
        "epoch": 0.3785521409283786,
        "step": 2944
    },
    {
        "loss": 2.2124,
        "grad_norm": 1.48401939868927,
        "learning_rate": 0.00013754737165712824,
        "epoch": 0.37868072521537866,
        "step": 2945
    },
    {
        "loss": 2.2361,
        "grad_norm": 1.4645392894744873,
        "learning_rate": 0.00013750985593296135,
        "epoch": 0.3788093095023788,
        "step": 2946
    },
    {
        "loss": 1.6464,
        "grad_norm": 2.0686519145965576,
        "learning_rate": 0.0001374723340641344,
        "epoch": 0.3789378937893789,
        "step": 2947
    },
    {
        "loss": 1.4201,
        "grad_norm": 2.9329171180725098,
        "learning_rate": 0.00013743480605679412,
        "epoch": 0.37906647807637905,
        "step": 2948
    },
    {
        "loss": 2.502,
        "grad_norm": 1.7680909633636475,
        "learning_rate": 0.000137397271917088,
        "epoch": 0.3791950623633792,
        "step": 2949
    },
    {
        "loss": 2.4688,
        "grad_norm": 1.2334871292114258,
        "learning_rate": 0.00013735973165116482,
        "epoch": 0.3793236466503793,
        "step": 2950
    },
    {
        "loss": 1.569,
        "grad_norm": 1.796196699142456,
        "learning_rate": 0.00013732218526517416,
        "epoch": 0.37945223093737945,
        "step": 2951
    },
    {
        "loss": 1.4134,
        "grad_norm": 1.8675587177276611,
        "learning_rate": 0.0001372846327652666,
        "epoch": 0.3795808152243796,
        "step": 2952
    },
    {
        "loss": 2.1492,
        "grad_norm": 1.7091361284255981,
        "learning_rate": 0.00013724707415759392,
        "epoch": 0.3797093995113797,
        "step": 2953
    },
    {
        "loss": 1.7643,
        "grad_norm": 1.5931135416030884,
        "learning_rate": 0.00013720950944830866,
        "epoch": 0.37983798379837985,
        "step": 2954
    },
    {
        "loss": 1.9404,
        "grad_norm": 2.0935590267181396,
        "learning_rate": 0.0001371719386435645,
        "epoch": 0.37996656808538,
        "step": 2955
    },
    {
        "loss": 1.4052,
        "grad_norm": 1.9968647956848145,
        "learning_rate": 0.00013713436174951613,
        "epoch": 0.3800951523723801,
        "step": 2956
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.5623040199279785,
        "learning_rate": 0.00013709677877231908,
        "epoch": 0.38022373665938025,
        "step": 2957
    },
    {
        "loss": 1.4139,
        "grad_norm": 1.4086458683013916,
        "learning_rate": 0.0001370591897181301,
        "epoch": 0.38035232094638033,
        "step": 2958
    },
    {
        "loss": 1.4083,
        "grad_norm": 2.096254587173462,
        "learning_rate": 0.00013702159459310677,
        "epoch": 0.38048090523338046,
        "step": 2959
    },
    {
        "loss": 1.382,
        "grad_norm": 2.1094634532928467,
        "learning_rate": 0.0001369839934034077,
        "epoch": 0.3806094895203806,
        "step": 2960
    },
    {
        "loss": 2.4343,
        "grad_norm": 1.7418910264968872,
        "learning_rate": 0.0001369463861551926,
        "epoch": 0.38073807380738073,
        "step": 2961
    },
    {
        "loss": 1.0346,
        "grad_norm": 1.582448959350586,
        "learning_rate": 0.00013690877285462197,
        "epoch": 0.38086665809438086,
        "step": 2962
    },
    {
        "loss": 2.0429,
        "grad_norm": 2.545380115509033,
        "learning_rate": 0.0001368711535078575,
        "epoch": 0.380995242381381,
        "step": 2963
    },
    {
        "loss": 2.1383,
        "grad_norm": 1.4665093421936035,
        "learning_rate": 0.00013683352812106174,
        "epoch": 0.3811238266683811,
        "step": 2964
    },
    {
        "loss": 1.3472,
        "grad_norm": 2.6073899269104004,
        "learning_rate": 0.0001367958967003983,
        "epoch": 0.38125241095538126,
        "step": 2965
    },
    {
        "loss": 1.6199,
        "grad_norm": 1.4849026203155518,
        "learning_rate": 0.00013675825925203174,
        "epoch": 0.3813809952423814,
        "step": 2966
    },
    {
        "loss": 1.5351,
        "grad_norm": 2.056455373764038,
        "learning_rate": 0.00013672061578212766,
        "epoch": 0.3815095795293815,
        "step": 2967
    },
    {
        "loss": 1.9951,
        "grad_norm": 1.7628576755523682,
        "learning_rate": 0.00013668296629685251,
        "epoch": 0.38163816381638166,
        "step": 2968
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.370195150375366,
        "learning_rate": 0.00013664531080237393,
        "epoch": 0.3817667481033818,
        "step": 2969
    },
    {
        "loss": 1.7402,
        "grad_norm": 2.4911673069000244,
        "learning_rate": 0.00013660764930486038,
        "epoch": 0.38189533239038187,
        "step": 2970
    },
    {
        "loss": 1.6406,
        "grad_norm": 1.7492684125900269,
        "learning_rate": 0.0001365699818104814,
        "epoch": 0.382023916677382,
        "step": 2971
    },
    {
        "loss": 1.964,
        "grad_norm": 1.9098926782608032,
        "learning_rate": 0.00013653230832540744,
        "epoch": 0.38215250096438214,
        "step": 2972
    },
    {
        "loss": 2.4731,
        "grad_norm": 1.3883169889450073,
        "learning_rate": 0.00013649462885580998,
        "epoch": 0.38228108525138227,
        "step": 2973
    },
    {
        "loss": 1.3179,
        "grad_norm": 2.1475207805633545,
        "learning_rate": 0.00013645694340786146,
        "epoch": 0.3824096695383824,
        "step": 2974
    },
    {
        "loss": 2.392,
        "grad_norm": 1.6603193283081055,
        "learning_rate": 0.0001364192519877353,
        "epoch": 0.38253825382538253,
        "step": 2975
    },
    {
        "loss": 1.8627,
        "grad_norm": 2.109560966491699,
        "learning_rate": 0.0001363815546016059,
        "epoch": 0.38266683811238267,
        "step": 2976
    },
    {
        "loss": 2.0889,
        "grad_norm": 2.1350085735321045,
        "learning_rate": 0.00013634385125564869,
        "epoch": 0.3827954223993828,
        "step": 2977
    },
    {
        "loss": 1.0573,
        "grad_norm": 1.9160146713256836,
        "learning_rate": 0.00013630614195603993,
        "epoch": 0.38292400668638293,
        "step": 2978
    },
    {
        "loss": 1.9698,
        "grad_norm": 2.0133116245269775,
        "learning_rate": 0.00013626842670895706,
        "epoch": 0.38305259097338307,
        "step": 2979
    },
    {
        "loss": 1.6684,
        "grad_norm": 1.8725626468658447,
        "learning_rate": 0.00013623070552057828,
        "epoch": 0.3831811752603832,
        "step": 2980
    },
    {
        "loss": 1.5374,
        "grad_norm": 1.6549049615859985,
        "learning_rate": 0.00013619297839708293,
        "epoch": 0.38330975954738333,
        "step": 2981
    },
    {
        "loss": 1.7058,
        "grad_norm": 1.9428375959396362,
        "learning_rate": 0.0001361552453446513,
        "epoch": 0.38343834383438347,
        "step": 2982
    },
    {
        "loss": 1.9589,
        "grad_norm": 2.0436480045318604,
        "learning_rate": 0.00013611750636946453,
        "epoch": 0.38356692812138354,
        "step": 2983
    },
    {
        "loss": 1.782,
        "grad_norm": 2.4845290184020996,
        "learning_rate": 0.00013607976147770486,
        "epoch": 0.3836955124083837,
        "step": 2984
    },
    {
        "loss": 1.5402,
        "grad_norm": 2.000986337661743,
        "learning_rate": 0.00013604201067555545,
        "epoch": 0.3838240966953838,
        "step": 2985
    },
    {
        "loss": 2.3092,
        "grad_norm": 1.524132251739502,
        "learning_rate": 0.00013600425396920041,
        "epoch": 0.38395268098238394,
        "step": 2986
    },
    {
        "loss": 2.0696,
        "grad_norm": 2.912745237350464,
        "learning_rate": 0.00013596649136482486,
        "epoch": 0.3840812652693841,
        "step": 2987
    },
    {
        "loss": 2.2559,
        "grad_norm": 1.2982549667358398,
        "learning_rate": 0.00013592872286861485,
        "epoch": 0.3842098495563842,
        "step": 2988
    },
    {
        "loss": 2.3567,
        "grad_norm": 1.8844317197799683,
        "learning_rate": 0.0001358909484867574,
        "epoch": 0.38433843384338434,
        "step": 2989
    },
    {
        "loss": 1.1683,
        "grad_norm": 1.6986913681030273,
        "learning_rate": 0.00013585316822544054,
        "epoch": 0.3844670181303845,
        "step": 2990
    },
    {
        "loss": 1.0257,
        "grad_norm": 2.307321548461914,
        "learning_rate": 0.0001358153820908532,
        "epoch": 0.3845956024173846,
        "step": 2991
    },
    {
        "loss": 1.2675,
        "grad_norm": 1.642977237701416,
        "learning_rate": 0.0001357775900891853,
        "epoch": 0.38472418670438474,
        "step": 2992
    },
    {
        "loss": 1.6978,
        "grad_norm": 2.328636884689331,
        "learning_rate": 0.0001357397922266277,
        "epoch": 0.3848527709913849,
        "step": 2993
    },
    {
        "loss": 0.7527,
        "grad_norm": 1.8223103284835815,
        "learning_rate": 0.0001357019885093723,
        "epoch": 0.384981355278385,
        "step": 2994
    },
    {
        "loss": 1.1695,
        "grad_norm": 2.828007459640503,
        "learning_rate": 0.00013566417894361182,
        "epoch": 0.3851099395653851,
        "step": 2995
    },
    {
        "loss": 1.7903,
        "grad_norm": 1.7786428928375244,
        "learning_rate": 0.00013562636353554007,
        "epoch": 0.3852385238523852,
        "step": 2996
    },
    {
        "loss": 2.1308,
        "grad_norm": 1.5250693559646606,
        "learning_rate": 0.00013558854229135173,
        "epoch": 0.38536710813938535,
        "step": 2997
    },
    {
        "loss": 2.0628,
        "grad_norm": 2.343979835510254,
        "learning_rate": 0.00013555071521724254,
        "epoch": 0.3854956924263855,
        "step": 2998
    },
    {
        "loss": 1.1985,
        "grad_norm": 2.88541316986084,
        "learning_rate": 0.00013551288231940904,
        "epoch": 0.3856242767133856,
        "step": 2999
    },
    {
        "loss": 2.2173,
        "grad_norm": 2.0921263694763184,
        "learning_rate": 0.00013547504360404887,
        "epoch": 0.38575286100038575,
        "step": 3000
    },
    {
        "eval_loss": 1.9300475120544434,
        "eval_runtime": 28.1477,
        "eval_samples_per_second": 2.807,
        "eval_steps_per_second": 2.807,
        "epoch": 0.38575286100038575,
        "step": 3000
    },
    {
        "loss": 1.082,
        "grad_norm": 1.5740560293197632,
        "learning_rate": 0.0001354371990773605,
        "epoch": 0.3858814452873859,
        "step": 3001
    },
    {
        "loss": 2.169,
        "grad_norm": 1.6293426752090454,
        "learning_rate": 0.00013539934874554345,
        "epoch": 0.386010029574386,
        "step": 3002
    },
    {
        "loss": 1.6888,
        "grad_norm": 1.3910549879074097,
        "learning_rate": 0.0001353614926147982,
        "epoch": 0.38613861386138615,
        "step": 3003
    },
    {
        "loss": 2.022,
        "grad_norm": 2.0928802490234375,
        "learning_rate": 0.00013532363069132606,
        "epoch": 0.3862671981483863,
        "step": 3004
    },
    {
        "loss": 2.0094,
        "grad_norm": 1.6981961727142334,
        "learning_rate": 0.0001352857629813294,
        "epoch": 0.3863957824353864,
        "step": 3005
    },
    {
        "loss": 2.2728,
        "grad_norm": 2.0592541694641113,
        "learning_rate": 0.0001352478894910115,
        "epoch": 0.38652436672238655,
        "step": 3006
    },
    {
        "loss": 2.1369,
        "grad_norm": 1.2260644435882568,
        "learning_rate": 0.00013521001022657656,
        "epoch": 0.3866529510093867,
        "step": 3007
    },
    {
        "loss": 2.3766,
        "grad_norm": 1.7119501829147339,
        "learning_rate": 0.00013517212519422982,
        "epoch": 0.38678153529638676,
        "step": 3008
    },
    {
        "loss": 1.6289,
        "grad_norm": 2.4198007583618164,
        "learning_rate": 0.00013513423440017733,
        "epoch": 0.3869101195833869,
        "step": 3009
    },
    {
        "loss": 2.0883,
        "grad_norm": 1.8771690130233765,
        "learning_rate": 0.00013509633785062616,
        "epoch": 0.387038703870387,
        "step": 3010
    },
    {
        "loss": 1.4858,
        "grad_norm": 2.5375490188598633,
        "learning_rate": 0.00013505843555178436,
        "epoch": 0.38716728815738716,
        "step": 3011
    },
    {
        "loss": 0.7434,
        "grad_norm": 1.994073748588562,
        "learning_rate": 0.00013502052750986087,
        "epoch": 0.3872958724443873,
        "step": 3012
    },
    {
        "loss": 1.911,
        "grad_norm": 2.053034782409668,
        "learning_rate": 0.00013498261373106555,
        "epoch": 0.3874244567313874,
        "step": 3013
    },
    {
        "loss": 0.897,
        "grad_norm": 1.9563502073287964,
        "learning_rate": 0.00013494469422160928,
        "epoch": 0.38755304101838756,
        "step": 3014
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.6309114694595337,
        "learning_rate": 0.0001349067689877037,
        "epoch": 0.3876816253053877,
        "step": 3015
    },
    {
        "loss": 2.7215,
        "grad_norm": 1.227882981300354,
        "learning_rate": 0.0001348688380355617,
        "epoch": 0.3878102095923878,
        "step": 3016
    },
    {
        "loss": 1.7662,
        "grad_norm": 2.3798065185546875,
        "learning_rate": 0.00013483090137139675,
        "epoch": 0.38793879387938796,
        "step": 3017
    },
    {
        "loss": 1.3709,
        "grad_norm": 3.28776478767395,
        "learning_rate": 0.00013479295900142355,
        "epoch": 0.3880673781663881,
        "step": 3018
    },
    {
        "loss": 1.7615,
        "grad_norm": 1.5119671821594238,
        "learning_rate": 0.00013475501093185757,
        "epoch": 0.3881959624533882,
        "step": 3019
    },
    {
        "loss": 2.0686,
        "grad_norm": 1.826865553855896,
        "learning_rate": 0.00013471705716891524,
        "epoch": 0.3883245467403883,
        "step": 3020
    },
    {
        "loss": 2.0417,
        "grad_norm": 1.8433785438537598,
        "learning_rate": 0.00013467909771881393,
        "epoch": 0.38845313102738843,
        "step": 3021
    },
    {
        "loss": 2.4313,
        "grad_norm": 1.6897480487823486,
        "learning_rate": 0.00013464113258777197,
        "epoch": 0.38858171531438856,
        "step": 3022
    },
    {
        "loss": 1.9283,
        "grad_norm": 1.1989542245864868,
        "learning_rate": 0.00013460316178200865,
        "epoch": 0.3887102996013887,
        "step": 3023
    },
    {
        "loss": 1.5113,
        "grad_norm": 1.6673049926757812,
        "learning_rate": 0.00013456518530774407,
        "epoch": 0.38883888388838883,
        "step": 3024
    },
    {
        "loss": 1.6141,
        "grad_norm": 1.6093013286590576,
        "learning_rate": 0.0001345272031711993,
        "epoch": 0.38896746817538896,
        "step": 3025
    },
    {
        "loss": 2.1161,
        "grad_norm": 1.5744578838348389,
        "learning_rate": 0.0001344892153785965,
        "epoch": 0.3890960524623891,
        "step": 3026
    },
    {
        "loss": 2.3679,
        "grad_norm": 1.4404202699661255,
        "learning_rate": 0.00013445122193615847,
        "epoch": 0.38922463674938923,
        "step": 3027
    },
    {
        "loss": 2.3892,
        "grad_norm": 1.3462531566619873,
        "learning_rate": 0.00013441322285010916,
        "epoch": 0.38935322103638936,
        "step": 3028
    },
    {
        "loss": 1.7417,
        "grad_norm": 1.8995610475540161,
        "learning_rate": 0.0001343752181266734,
        "epoch": 0.3894818053233895,
        "step": 3029
    },
    {
        "loss": 1.991,
        "grad_norm": 2.3724305629730225,
        "learning_rate": 0.0001343372077720769,
        "epoch": 0.38961038961038963,
        "step": 3030
    },
    {
        "loss": 2.0337,
        "grad_norm": 1.4785345792770386,
        "learning_rate": 0.00013429919179254625,
        "epoch": 0.38973897389738976,
        "step": 3031
    },
    {
        "loss": 1.8718,
        "grad_norm": 1.813362717628479,
        "learning_rate": 0.0001342611701943091,
        "epoch": 0.3898675581843899,
        "step": 3032
    },
    {
        "loss": 0.9566,
        "grad_norm": 2.4447596073150635,
        "learning_rate": 0.0001342231429835939,
        "epoch": 0.38999614247139,
        "step": 3033
    },
    {
        "loss": 2.2313,
        "grad_norm": 2.005774974822998,
        "learning_rate": 0.00013418511016663012,
        "epoch": 0.3901247267583901,
        "step": 3034
    },
    {
        "loss": 1.8661,
        "grad_norm": 2.034456729888916,
        "learning_rate": 0.00013414707174964796,
        "epoch": 0.39025331104539024,
        "step": 3035
    },
    {
        "loss": 1.2223,
        "grad_norm": 2.2057557106018066,
        "learning_rate": 0.00013410902773887875,
        "epoch": 0.39038189533239037,
        "step": 3036
    },
    {
        "loss": 2.5917,
        "grad_norm": 1.2980037927627563,
        "learning_rate": 0.00013407097814055469,
        "epoch": 0.3905104796193905,
        "step": 3037
    },
    {
        "loss": 2.0682,
        "grad_norm": 1.7776566743850708,
        "learning_rate": 0.00013403292296090876,
        "epoch": 0.39063906390639064,
        "step": 3038
    },
    {
        "loss": 1.1832,
        "grad_norm": 1.8082215785980225,
        "learning_rate": 0.00013399486220617505,
        "epoch": 0.39076764819339077,
        "step": 3039
    },
    {
        "loss": 1.9547,
        "grad_norm": 1.8375235795974731,
        "learning_rate": 0.0001339567958825884,
        "epoch": 0.3908962324803909,
        "step": 3040
    },
    {
        "loss": 1.3641,
        "grad_norm": 1.1636545658111572,
        "learning_rate": 0.00013391872399638464,
        "epoch": 0.39102481676739104,
        "step": 3041
    },
    {
        "loss": 2.1219,
        "grad_norm": 1.5249807834625244,
        "learning_rate": 0.00013388064655380056,
        "epoch": 0.39115340105439117,
        "step": 3042
    },
    {
        "loss": 2.0861,
        "grad_norm": 0.9123774766921997,
        "learning_rate": 0.00013384256356107366,
        "epoch": 0.3912819853413913,
        "step": 3043
    },
    {
        "loss": 2.2663,
        "grad_norm": 2.0808279514312744,
        "learning_rate": 0.00013380447502444261,
        "epoch": 0.39141056962839144,
        "step": 3044
    },
    {
        "loss": 1.2647,
        "grad_norm": 2.257235527038574,
        "learning_rate": 0.00013376638095014687,
        "epoch": 0.3915391539153915,
        "step": 3045
    },
    {
        "loss": 2.0916,
        "grad_norm": 1.5658522844314575,
        "learning_rate": 0.0001337282813444267,
        "epoch": 0.39166773820239165,
        "step": 3046
    },
    {
        "loss": 2.2254,
        "grad_norm": 1.4350956678390503,
        "learning_rate": 0.00013369017621352346,
        "epoch": 0.3917963224893918,
        "step": 3047
    },
    {
        "loss": 1.9643,
        "grad_norm": 1.9985522031784058,
        "learning_rate": 0.0001336520655636793,
        "epoch": 0.3919249067763919,
        "step": 3048
    },
    {
        "loss": 2.0473,
        "grad_norm": 1.759520411491394,
        "learning_rate": 0.0001336139494011373,
        "epoch": 0.39205349106339205,
        "step": 3049
    },
    {
        "loss": 1.7169,
        "grad_norm": 1.569798231124878,
        "learning_rate": 0.00013357582773214146,
        "epoch": 0.3921820753503922,
        "step": 3050
    },
    {
        "loss": 1.928,
        "grad_norm": 2.1608855724334717,
        "learning_rate": 0.0001335377005629366,
        "epoch": 0.3923106596373923,
        "step": 3051
    },
    {
        "loss": 1.6552,
        "grad_norm": 2.4082894325256348,
        "learning_rate": 0.00013349956789976853,
        "epoch": 0.39243924392439244,
        "step": 3052
    },
    {
        "loss": 1.8655,
        "grad_norm": 1.6274083852767944,
        "learning_rate": 0.000133461429748884,
        "epoch": 0.3925678282113926,
        "step": 3053
    },
    {
        "loss": 1.6585,
        "grad_norm": 2.3504905700683594,
        "learning_rate": 0.00013342328611653053,
        "epoch": 0.3926964124983927,
        "step": 3054
    },
    {
        "loss": 1.8058,
        "grad_norm": 2.102912664413452,
        "learning_rate": 0.00013338513700895664,
        "epoch": 0.39282499678539284,
        "step": 3055
    },
    {
        "loss": 2.2854,
        "grad_norm": 2.2412168979644775,
        "learning_rate": 0.00013334698243241166,
        "epoch": 0.392953581072393,
        "step": 3056
    },
    {
        "loss": 1.8972,
        "grad_norm": 2.134223222732544,
        "learning_rate": 0.00013330882239314588,
        "epoch": 0.3930821653593931,
        "step": 3057
    },
    {
        "loss": 1.3671,
        "grad_norm": 2.105551242828369,
        "learning_rate": 0.00013327065689741052,
        "epoch": 0.3932107496463932,
        "step": 3058
    },
    {
        "loss": 1.1756,
        "grad_norm": 1.6257938146591187,
        "learning_rate": 0.00013323248595145754,
        "epoch": 0.3933393339333933,
        "step": 3059
    },
    {
        "loss": 1.9276,
        "grad_norm": 1.35238516330719,
        "learning_rate": 0.00013319430956154003,
        "epoch": 0.39346791822039345,
        "step": 3060
    },
    {
        "loss": 2.3276,
        "grad_norm": 1.5096102952957153,
        "learning_rate": 0.00013315612773391172,
        "epoch": 0.3935965025073936,
        "step": 3061
    },
    {
        "loss": 2.0912,
        "grad_norm": 2.2241547107696533,
        "learning_rate": 0.00013311794047482742,
        "epoch": 0.3937250867943937,
        "step": 3062
    },
    {
        "loss": 1.4651,
        "grad_norm": 2.636942148208618,
        "learning_rate": 0.00013307974779054274,
        "epoch": 0.39385367108139385,
        "step": 3063
    },
    {
        "loss": 2.252,
        "grad_norm": 2.1496469974517822,
        "learning_rate": 0.00013304154968731417,
        "epoch": 0.393982255368394,
        "step": 3064
    },
    {
        "loss": 1.0242,
        "grad_norm": 2.181288480758667,
        "learning_rate": 0.00013300334617139916,
        "epoch": 0.3941108396553941,
        "step": 3065
    },
    {
        "loss": 0.9761,
        "grad_norm": 1.856872320175171,
        "learning_rate": 0.000132965137249056,
        "epoch": 0.39423942394239425,
        "step": 3066
    },
    {
        "loss": 1.4176,
        "grad_norm": 1.9186506271362305,
        "learning_rate": 0.0001329269229265438,
        "epoch": 0.3943680082293944,
        "step": 3067
    },
    {
        "loss": 1.789,
        "grad_norm": 2.0182857513427734,
        "learning_rate": 0.00013288870321012268,
        "epoch": 0.3944965925163945,
        "step": 3068
    },
    {
        "loss": 1.9565,
        "grad_norm": 2.0745787620544434,
        "learning_rate": 0.00013285047810605356,
        "epoch": 0.39462517680339465,
        "step": 3069
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.6864324808120728,
        "learning_rate": 0.0001328122476205983,
        "epoch": 0.39475376109039473,
        "step": 3070
    },
    {
        "loss": 2.1439,
        "grad_norm": 1.4501768350601196,
        "learning_rate": 0.00013277401176001963,
        "epoch": 0.39488234537739486,
        "step": 3071
    },
    {
        "loss": 2.4031,
        "grad_norm": 1.9431989192962646,
        "learning_rate": 0.00013273577053058103,
        "epoch": 0.395010929664395,
        "step": 3072
    },
    {
        "loss": 2.1558,
        "grad_norm": 1.9404374361038208,
        "learning_rate": 0.0001326975239385471,
        "epoch": 0.3951395139513951,
        "step": 3073
    },
    {
        "loss": 1.8161,
        "grad_norm": 1.0859360694885254,
        "learning_rate": 0.0001326592719901831,
        "epoch": 0.39526809823839526,
        "step": 3074
    },
    {
        "loss": 2.2661,
        "grad_norm": 1.218000054359436,
        "learning_rate": 0.00013262101469175525,
        "epoch": 0.3953966825253954,
        "step": 3075
    },
    {
        "loss": 1.9385,
        "grad_norm": 2.0640087127685547,
        "learning_rate": 0.00013258275204953076,
        "epoch": 0.3955252668123955,
        "step": 3076
    },
    {
        "loss": 2.3028,
        "grad_norm": 1.9204944372177124,
        "learning_rate": 0.0001325444840697775,
        "epoch": 0.39565385109939566,
        "step": 3077
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.6102999448776245,
        "learning_rate": 0.00013250621075876432,
        "epoch": 0.3957824353863958,
        "step": 3078
    },
    {
        "loss": 2.1211,
        "grad_norm": 1.9797724485397339,
        "learning_rate": 0.00013246793212276105,
        "epoch": 0.3959110196733959,
        "step": 3079
    },
    {
        "loss": 1.49,
        "grad_norm": 1.5185027122497559,
        "learning_rate": 0.00013242964816803817,
        "epoch": 0.39603960396039606,
        "step": 3080
    },
    {
        "loss": 1.2599,
        "grad_norm": 3.136152982711792,
        "learning_rate": 0.00013239135890086724,
        "epoch": 0.3961681882473962,
        "step": 3081
    },
    {
        "loss": 1.5032,
        "grad_norm": 1.6745611429214478,
        "learning_rate": 0.0001323530643275205,
        "epoch": 0.3962967725343963,
        "step": 3082
    },
    {
        "loss": 1.4237,
        "grad_norm": 2.127105236053467,
        "learning_rate": 0.00013231476445427125,
        "epoch": 0.3964253568213964,
        "step": 3083
    },
    {
        "loss": 2.1298,
        "grad_norm": 1.9344439506530762,
        "learning_rate": 0.0001322764592873935,
        "epoch": 0.39655394110839653,
        "step": 3084
    },
    {
        "loss": 2.6909,
        "grad_norm": 1.2852215766906738,
        "learning_rate": 0.00013223814883316226,
        "epoch": 0.39668252539539667,
        "step": 3085
    },
    {
        "loss": 2.4642,
        "grad_norm": 1.9620344638824463,
        "learning_rate": 0.00013219983309785332,
        "epoch": 0.3968111096823968,
        "step": 3086
    },
    {
        "loss": 2.1168,
        "grad_norm": 1.506398320198059,
        "learning_rate": 0.0001321615120877433,
        "epoch": 0.39693969396939693,
        "step": 3087
    },
    {
        "loss": 1.9954,
        "grad_norm": 1.5400785207748413,
        "learning_rate": 0.0001321231858091098,
        "epoch": 0.39706827825639707,
        "step": 3088
    },
    {
        "loss": 1.0588,
        "grad_norm": 1.7232744693756104,
        "learning_rate": 0.0001320848542682312,
        "epoch": 0.3971968625433972,
        "step": 3089
    },
    {
        "loss": 0.7385,
        "grad_norm": 1.661055564880371,
        "learning_rate": 0.0001320465174713867,
        "epoch": 0.39732544683039733,
        "step": 3090
    },
    {
        "loss": 1.6994,
        "grad_norm": 2.2527782917022705,
        "learning_rate": 0.00013200817542485653,
        "epoch": 0.39745403111739747,
        "step": 3091
    },
    {
        "loss": 2.1172,
        "grad_norm": 1.1697207689285278,
        "learning_rate": 0.00013196982813492168,
        "epoch": 0.3975826154043976,
        "step": 3092
    },
    {
        "loss": 2.3106,
        "grad_norm": 1.8229498863220215,
        "learning_rate": 0.0001319314756078639,
        "epoch": 0.39771119969139773,
        "step": 3093
    },
    {
        "loss": 2.2148,
        "grad_norm": 3.037635326385498,
        "learning_rate": 0.00013189311784996596,
        "epoch": 0.39783978397839787,
        "step": 3094
    },
    {
        "loss": 1.461,
        "grad_norm": 2.0269668102264404,
        "learning_rate": 0.00013185475486751134,
        "epoch": 0.39796836826539794,
        "step": 3095
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.3853408098220825,
        "learning_rate": 0.00013181638666678457,
        "epoch": 0.3980969525523981,
        "step": 3096
    },
    {
        "loss": 2.0604,
        "grad_norm": 1.3071030378341675,
        "learning_rate": 0.00013177801325407087,
        "epoch": 0.3982255368393982,
        "step": 3097
    },
    {
        "loss": 1.4971,
        "grad_norm": 1.5211774110794067,
        "learning_rate": 0.00013173963463565633,
        "epoch": 0.39835412112639834,
        "step": 3098
    },
    {
        "loss": 1.8807,
        "grad_norm": 1.410843849182129,
        "learning_rate": 0.00013170125081782792,
        "epoch": 0.3984827054133985,
        "step": 3099
    },
    {
        "loss": 1.7221,
        "grad_norm": 1.9619210958480835,
        "learning_rate": 0.00013166286180687356,
        "epoch": 0.3986112897003986,
        "step": 3100
    },
    {
        "eval_loss": 1.9112772941589355,
        "eval_runtime": 28.2991,
        "eval_samples_per_second": 2.792,
        "eval_steps_per_second": 2.792,
        "epoch": 0.3986112897003986,
        "step": 3100
    },
    {
        "loss": 1.7852,
        "grad_norm": 1.8077846765518188,
        "learning_rate": 0.00013162446760908183,
        "epoch": 0.39873987398739874,
        "step": 3101
    },
    {
        "loss": 1.7231,
        "grad_norm": 1.535569429397583,
        "learning_rate": 0.00013158606823074234,
        "epoch": 0.3988684582743989,
        "step": 3102
    },
    {
        "loss": 1.8068,
        "grad_norm": 1.515188455581665,
        "learning_rate": 0.00013154766367814538,
        "epoch": 0.398997042561399,
        "step": 3103
    },
    {
        "loss": 1.803,
        "grad_norm": 1.6939526796340942,
        "learning_rate": 0.0001315092539575822,
        "epoch": 0.39912562684839914,
        "step": 3104
    },
    {
        "loss": 1.5583,
        "grad_norm": 2.021791696548462,
        "learning_rate": 0.00013147083907534492,
        "epoch": 0.3992542111353993,
        "step": 3105
    },
    {
        "loss": 1.9158,
        "grad_norm": 2.372812271118164,
        "learning_rate": 0.00013143241903772638,
        "epoch": 0.3993827954223994,
        "step": 3106
    },
    {
        "loss": 1.6939,
        "grad_norm": 2.0039029121398926,
        "learning_rate": 0.00013139399385102044,
        "epoch": 0.39951137970939954,
        "step": 3107
    },
    {
        "loss": 2.0427,
        "grad_norm": 1.5720292329788208,
        "learning_rate": 0.00013135556352152162,
        "epoch": 0.3996399639963996,
        "step": 3108
    },
    {
        "loss": 1.5401,
        "grad_norm": 2.4637038707733154,
        "learning_rate": 0.0001313171280555254,
        "epoch": 0.39976854828339975,
        "step": 3109
    },
    {
        "loss": 2.3048,
        "grad_norm": 1.4231436252593994,
        "learning_rate": 0.00013127868745932807,
        "epoch": 0.3998971325703999,
        "step": 3110
    },
    {
        "loss": 2.0726,
        "grad_norm": 2.1230857372283936,
        "learning_rate": 0.00013124024173922675,
        "epoch": 0.4000257168574,
        "step": 3111
    },
    {
        "loss": 2.533,
        "grad_norm": 1.321550726890564,
        "learning_rate": 0.00013120179090151944,
        "epoch": 0.40015430114440015,
        "step": 3112
    },
    {
        "loss": 1.7722,
        "grad_norm": 2.0441815853118896,
        "learning_rate": 0.0001311633349525049,
        "epoch": 0.4002828854314003,
        "step": 3113
    },
    {
        "loss": 2.0446,
        "grad_norm": 2.122774600982666,
        "learning_rate": 0.00013112487389848273,
        "epoch": 0.4004114697184004,
        "step": 3114
    },
    {
        "loss": 2.1378,
        "grad_norm": 2.130481243133545,
        "learning_rate": 0.00013108640774575354,
        "epoch": 0.40054005400540055,
        "step": 3115
    },
    {
        "loss": 2.2667,
        "grad_norm": 2.001260280609131,
        "learning_rate": 0.00013104793650061856,
        "epoch": 0.4006686382924007,
        "step": 3116
    },
    {
        "loss": 1.4979,
        "grad_norm": 2.2354865074157715,
        "learning_rate": 0.00013100946016937992,
        "epoch": 0.4007972225794008,
        "step": 3117
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.5194042921066284,
        "learning_rate": 0.00013097097875834073,
        "epoch": 0.40092580686640095,
        "step": 3118
    },
    {
        "loss": 2.4925,
        "grad_norm": 1.520555019378662,
        "learning_rate": 0.00013093249227380462,
        "epoch": 0.4010543911534011,
        "step": 3119
    },
    {
        "loss": 1.3496,
        "grad_norm": 1.9150456190109253,
        "learning_rate": 0.0001308940007220764,
        "epoch": 0.40118297544040116,
        "step": 3120
    },
    {
        "loss": 2.2598,
        "grad_norm": 1.4590431451797485,
        "learning_rate": 0.0001308555041094614,
        "epoch": 0.4013115597274013,
        "step": 3121
    },
    {
        "loss": 1.5106,
        "grad_norm": 2.5852465629577637,
        "learning_rate": 0.00013081700244226604,
        "epoch": 0.4014401440144014,
        "step": 3122
    },
    {
        "loss": 1.3444,
        "grad_norm": 2.2689778804779053,
        "learning_rate": 0.0001307784957267975,
        "epoch": 0.40156872830140156,
        "step": 3123
    },
    {
        "loss": 1.252,
        "grad_norm": 2.045201301574707,
        "learning_rate": 0.00013073998396936353,
        "epoch": 0.4016973125884017,
        "step": 3124
    },
    {
        "loss": 1.9446,
        "grad_norm": 1.4904013872146606,
        "learning_rate": 0.00013070146717627308,
        "epoch": 0.4018258968754018,
        "step": 3125
    },
    {
        "loss": 1.1836,
        "grad_norm": 2.0636532306671143,
        "learning_rate": 0.00013066294535383574,
        "epoch": 0.40195448116240196,
        "step": 3126
    },
    {
        "loss": 1.8712,
        "grad_norm": 1.9736930131912231,
        "learning_rate": 0.00013062441850836192,
        "epoch": 0.4020830654494021,
        "step": 3127
    },
    {
        "loss": 1.6898,
        "grad_norm": 1.8908116817474365,
        "learning_rate": 0.00013058588664616286,
        "epoch": 0.4022116497364022,
        "step": 3128
    },
    {
        "loss": 2.1337,
        "grad_norm": 2.0086557865142822,
        "learning_rate": 0.0001305473497735507,
        "epoch": 0.40234023402340235,
        "step": 3129
    },
    {
        "loss": 2.6516,
        "grad_norm": 1.0887197256088257,
        "learning_rate": 0.0001305088078968383,
        "epoch": 0.4024688183104025,
        "step": 3130
    },
    {
        "loss": 1.8839,
        "grad_norm": 1.967852234840393,
        "learning_rate": 0.00013047026102233937,
        "epoch": 0.4025974025974026,
        "step": 3131
    },
    {
        "loss": 2.209,
        "grad_norm": 1.9250692129135132,
        "learning_rate": 0.00013043170915636845,
        "epoch": 0.40272598688440275,
        "step": 3132
    },
    {
        "loss": 2.5186,
        "grad_norm": 2.079488754272461,
        "learning_rate": 0.00013039315230524095,
        "epoch": 0.40285457117140283,
        "step": 3133
    },
    {
        "loss": 1.6988,
        "grad_norm": 1.7590194940567017,
        "learning_rate": 0.00013035459047527296,
        "epoch": 0.40298315545840296,
        "step": 3134
    },
    {
        "loss": 2.1947,
        "grad_norm": 1.4675283432006836,
        "learning_rate": 0.00013031602367278154,
        "epoch": 0.4031117397454031,
        "step": 3135
    },
    {
        "loss": 2.2451,
        "grad_norm": 1.3677122592926025,
        "learning_rate": 0.00013027745190408445,
        "epoch": 0.40324032403240323,
        "step": 3136
    },
    {
        "loss": 1.9317,
        "grad_norm": 1.8530992269515991,
        "learning_rate": 0.00013023887517550032,
        "epoch": 0.40336890831940336,
        "step": 3137
    },
    {
        "loss": 1.1169,
        "grad_norm": 0.8900604844093323,
        "learning_rate": 0.00013020029349334854,
        "epoch": 0.4034974926064035,
        "step": 3138
    },
    {
        "loss": 1.8266,
        "grad_norm": 2.272566080093384,
        "learning_rate": 0.00013016170686394948,
        "epoch": 0.40362607689340363,
        "step": 3139
    },
    {
        "loss": 1.6668,
        "grad_norm": 1.8874136209487915,
        "learning_rate": 0.00013012311529362404,
        "epoch": 0.40375466118040376,
        "step": 3140
    },
    {
        "loss": 2.4705,
        "grad_norm": 1.7921637296676636,
        "learning_rate": 0.00013008451878869417,
        "epoch": 0.4038832454674039,
        "step": 3141
    },
    {
        "loss": 1.9164,
        "grad_norm": 1.6179336309432983,
        "learning_rate": 0.00013004591735548248,
        "epoch": 0.40401182975440403,
        "step": 3142
    },
    {
        "loss": 1.6568,
        "grad_norm": 1.8645919561386108,
        "learning_rate": 0.0001300073110003125,
        "epoch": 0.40414041404140416,
        "step": 3143
    },
    {
        "loss": 0.9524,
        "grad_norm": 2.4726650714874268,
        "learning_rate": 0.00012996869972950852,
        "epoch": 0.4042689983284043,
        "step": 3144
    },
    {
        "loss": 1.9629,
        "grad_norm": 1.749226450920105,
        "learning_rate": 0.00012993008354939556,
        "epoch": 0.40439758261540437,
        "step": 3145
    },
    {
        "loss": 1.2802,
        "grad_norm": 3.5494964122772217,
        "learning_rate": 0.00012989146246629956,
        "epoch": 0.4045261669024045,
        "step": 3146
    },
    {
        "loss": 1.7372,
        "grad_norm": 1.9429783821105957,
        "learning_rate": 0.0001298528364865472,
        "epoch": 0.40465475118940464,
        "step": 3147
    },
    {
        "loss": 2.0822,
        "grad_norm": 1.736238956451416,
        "learning_rate": 0.00012981420561646598,
        "epoch": 0.40478333547640477,
        "step": 3148
    },
    {
        "loss": 2.2064,
        "grad_norm": 1.4248141050338745,
        "learning_rate": 0.00012977556986238422,
        "epoch": 0.4049119197634049,
        "step": 3149
    },
    {
        "loss": 1.4643,
        "grad_norm": 1.9017688035964966,
        "learning_rate": 0.00012973692923063098,
        "epoch": 0.40504050405040504,
        "step": 3150
    },
    {
        "loss": 2.5853,
        "grad_norm": 1.9123731851577759,
        "learning_rate": 0.00012969828372753618,
        "epoch": 0.40516908833740517,
        "step": 3151
    },
    {
        "loss": 1.9512,
        "grad_norm": 3.343712091445923,
        "learning_rate": 0.0001296596333594305,
        "epoch": 0.4052976726244053,
        "step": 3152
    },
    {
        "loss": 1.6601,
        "grad_norm": 1.4590660333633423,
        "learning_rate": 0.00012962097813264549,
        "epoch": 0.40542625691140544,
        "step": 3153
    },
    {
        "loss": 1.9972,
        "grad_norm": 1.62697434425354,
        "learning_rate": 0.00012958231805351337,
        "epoch": 0.40555484119840557,
        "step": 3154
    },
    {
        "loss": 2.2957,
        "grad_norm": 1.7227133512496948,
        "learning_rate": 0.00012954365312836728,
        "epoch": 0.4056834254854057,
        "step": 3155
    },
    {
        "loss": 2.2119,
        "grad_norm": 1.8488292694091797,
        "learning_rate": 0.000129504983363541,
        "epoch": 0.40581200977240584,
        "step": 3156
    },
    {
        "loss": 2.1874,
        "grad_norm": 1.7695649862289429,
        "learning_rate": 0.00012946630876536932,
        "epoch": 0.40594059405940597,
        "step": 3157
    },
    {
        "loss": 1.4476,
        "grad_norm": 1.8388944864273071,
        "learning_rate": 0.0001294276293401876,
        "epoch": 0.40606917834640605,
        "step": 3158
    },
    {
        "loss": 1.8725,
        "grad_norm": 1.667132019996643,
        "learning_rate": 0.00012938894509433217,
        "epoch": 0.4061977626334062,
        "step": 3159
    },
    {
        "loss": 1.7673,
        "grad_norm": 1.6234138011932373,
        "learning_rate": 0.00012935025603414004,
        "epoch": 0.4063263469204063,
        "step": 3160
    },
    {
        "loss": 1.3311,
        "grad_norm": 1.820914387702942,
        "learning_rate": 0.00012931156216594903,
        "epoch": 0.40645493120740644,
        "step": 3161
    },
    {
        "loss": 1.697,
        "grad_norm": 3.3495755195617676,
        "learning_rate": 0.00012927286349609775,
        "epoch": 0.4065835154944066,
        "step": 3162
    },
    {
        "loss": 1.6356,
        "grad_norm": 1.0009088516235352,
        "learning_rate": 0.00012923416003092565,
        "epoch": 0.4067120997814067,
        "step": 3163
    },
    {
        "loss": 1.6126,
        "grad_norm": 2.1425862312316895,
        "learning_rate": 0.0001291954517767729,
        "epoch": 0.40684068406840684,
        "step": 3164
    },
    {
        "loss": 2.3927,
        "grad_norm": 1.1440893411636353,
        "learning_rate": 0.0001291567387399805,
        "epoch": 0.406969268355407,
        "step": 3165
    },
    {
        "loss": 1.6052,
        "grad_norm": 2.4504764080047607,
        "learning_rate": 0.0001291180209268901,
        "epoch": 0.4070978526424071,
        "step": 3166
    },
    {
        "loss": 1.0523,
        "grad_norm": 1.715390920639038,
        "learning_rate": 0.00012907929834384436,
        "epoch": 0.40722643692940724,
        "step": 3167
    },
    {
        "loss": 2.1655,
        "grad_norm": 1.9409133195877075,
        "learning_rate": 0.00012904057099718657,
        "epoch": 0.4073550212164074,
        "step": 3168
    },
    {
        "loss": 1.9781,
        "grad_norm": 2.2486391067504883,
        "learning_rate": 0.0001290018388932608,
        "epoch": 0.4074836055034075,
        "step": 3169
    },
    {
        "loss": 1.6901,
        "grad_norm": 1.6768310070037842,
        "learning_rate": 0.000128963102038412,
        "epoch": 0.4076121897904076,
        "step": 3170
    },
    {
        "loss": 1.2645,
        "grad_norm": 1.684309482574463,
        "learning_rate": 0.00012892436043898577,
        "epoch": 0.4077407740774077,
        "step": 3171
    },
    {
        "loss": 2.0879,
        "grad_norm": 1.9638744592666626,
        "learning_rate": 0.00012888561410132857,
        "epoch": 0.40786935836440785,
        "step": 3172
    },
    {
        "loss": 1.3668,
        "grad_norm": 2.5463528633117676,
        "learning_rate": 0.0001288468630317876,
        "epoch": 0.407997942651408,
        "step": 3173
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.6340917348861694,
        "learning_rate": 0.00012880810723671083,
        "epoch": 0.4081265269384081,
        "step": 3174
    },
    {
        "loss": 2.0094,
        "grad_norm": 1.6660635471343994,
        "learning_rate": 0.00012876934672244713,
        "epoch": 0.40825511122540825,
        "step": 3175
    },
    {
        "loss": 2.2191,
        "grad_norm": 2.1336216926574707,
        "learning_rate": 0.0001287305814953459,
        "epoch": 0.4083836955124084,
        "step": 3176
    },
    {
        "loss": 1.5696,
        "grad_norm": 3.4889090061187744,
        "learning_rate": 0.0001286918115617575,
        "epoch": 0.4085122797994085,
        "step": 3177
    },
    {
        "loss": 1.7039,
        "grad_norm": 2.000849723815918,
        "learning_rate": 0.00012865303692803304,
        "epoch": 0.40864086408640865,
        "step": 3178
    },
    {
        "loss": 1.6702,
        "grad_norm": 1.5277187824249268,
        "learning_rate": 0.0001286142576005243,
        "epoch": 0.4087694483734088,
        "step": 3179
    },
    {
        "loss": 1.8471,
        "grad_norm": 2.464406967163086,
        "learning_rate": 0.00012857547358558397,
        "epoch": 0.4088980326604089,
        "step": 3180
    },
    {
        "loss": 2.2959,
        "grad_norm": 1.333421230316162,
        "learning_rate": 0.00012853668488956542,
        "epoch": 0.40902661694740905,
        "step": 3181
    },
    {
        "loss": 1.3045,
        "grad_norm": 2.0546979904174805,
        "learning_rate": 0.00012849789151882275,
        "epoch": 0.4091552012344092,
        "step": 3182
    },
    {
        "loss": 2.0238,
        "grad_norm": 1.5894075632095337,
        "learning_rate": 0.00012845909347971096,
        "epoch": 0.40928378552140926,
        "step": 3183
    },
    {
        "loss": 2.3146,
        "grad_norm": 1.9790757894515991,
        "learning_rate": 0.00012842029077858567,
        "epoch": 0.4094123698084094,
        "step": 3184
    },
    {
        "loss": 2.3335,
        "grad_norm": 1.0451946258544922,
        "learning_rate": 0.00012838148342180335,
        "epoch": 0.4095409540954095,
        "step": 3185
    },
    {
        "loss": 1.214,
        "grad_norm": 1.9939409494400024,
        "learning_rate": 0.00012834267141572124,
        "epoch": 0.40966953838240966,
        "step": 3186
    },
    {
        "loss": 1.5318,
        "grad_norm": 2.0004851818084717,
        "learning_rate": 0.00012830385476669722,
        "epoch": 0.4097981226694098,
        "step": 3187
    },
    {
        "loss": 1.6845,
        "grad_norm": 1.4156323671340942,
        "learning_rate": 0.0001282650334810901,
        "epoch": 0.4099267069564099,
        "step": 3188
    },
    {
        "loss": 1.5702,
        "grad_norm": 1.0730465650558472,
        "learning_rate": 0.00012822620756525936,
        "epoch": 0.41005529124341006,
        "step": 3189
    },
    {
        "loss": 2.0981,
        "grad_norm": 1.6702793836593628,
        "learning_rate": 0.0001281873770255652,
        "epoch": 0.4101838755304102,
        "step": 3190
    },
    {
        "loss": 2.4442,
        "grad_norm": 1.4707379341125488,
        "learning_rate": 0.00012814854186836877,
        "epoch": 0.4103124598174103,
        "step": 3191
    },
    {
        "loss": 2.3335,
        "grad_norm": 1.7648520469665527,
        "learning_rate": 0.00012810970210003165,
        "epoch": 0.41044104410441046,
        "step": 3192
    },
    {
        "loss": 2.2073,
        "grad_norm": 1.400744080543518,
        "learning_rate": 0.00012807085772691647,
        "epoch": 0.4105696283914106,
        "step": 3193
    },
    {
        "loss": 2.0806,
        "grad_norm": 1.965640902519226,
        "learning_rate": 0.00012803200875538648,
        "epoch": 0.4106982126784107,
        "step": 3194
    },
    {
        "loss": 1.6023,
        "grad_norm": 1.832106590270996,
        "learning_rate": 0.00012799315519180566,
        "epoch": 0.4108267969654108,
        "step": 3195
    },
    {
        "loss": 1.2569,
        "grad_norm": 2.7801098823547363,
        "learning_rate": 0.00012795429704253887,
        "epoch": 0.41095538125241093,
        "step": 3196
    },
    {
        "loss": 1.1881,
        "grad_norm": 1.6572924852371216,
        "learning_rate": 0.00012791543431395162,
        "epoch": 0.41108396553941107,
        "step": 3197
    },
    {
        "loss": 1.841,
        "grad_norm": 1.5201901197433472,
        "learning_rate": 0.0001278765670124101,
        "epoch": 0.4112125498264112,
        "step": 3198
    },
    {
        "loss": 1.5875,
        "grad_norm": 2.3630731105804443,
        "learning_rate": 0.00012783769514428147,
        "epoch": 0.41134113411341133,
        "step": 3199
    },
    {
        "loss": 2.0502,
        "grad_norm": 2.0130715370178223,
        "learning_rate": 0.0001277988187159334,
        "epoch": 0.41146971840041147,
        "step": 3200
    },
    {
        "eval_loss": 1.9091976881027222,
        "eval_runtime": 28.2179,
        "eval_samples_per_second": 2.8,
        "eval_steps_per_second": 2.8,
        "epoch": 0.41146971840041147,
        "step": 3200
    },
    {
        "loss": 2.1287,
        "grad_norm": 1.3811191320419312,
        "learning_rate": 0.0001277599377337345,
        "epoch": 0.4115983026874116,
        "step": 3201
    },
    {
        "loss": 1.4192,
        "grad_norm": 2.2246336936950684,
        "learning_rate": 0.000127721052204054,
        "epoch": 0.41172688697441173,
        "step": 3202
    },
    {
        "loss": 1.1872,
        "grad_norm": 2.4040257930755615,
        "learning_rate": 0.0001276821621332619,
        "epoch": 0.41185547126141187,
        "step": 3203
    },
    {
        "loss": 2.2824,
        "grad_norm": 1.414034128189087,
        "learning_rate": 0.00012764326752772898,
        "epoch": 0.411984055548412,
        "step": 3204
    },
    {
        "loss": 1.8718,
        "grad_norm": 1.8477811813354492,
        "learning_rate": 0.00012760436839382675,
        "epoch": 0.41211263983541213,
        "step": 3205
    },
    {
        "loss": 1.483,
        "grad_norm": 1.7776105403900146,
        "learning_rate": 0.00012756546473792742,
        "epoch": 0.41224122412241226,
        "step": 3206
    },
    {
        "loss": 2.3957,
        "grad_norm": 1.3282248973846436,
        "learning_rate": 0.00012752655656640403,
        "epoch": 0.4123698084094124,
        "step": 3207
    },
    {
        "loss": 1.4063,
        "grad_norm": 2.4109883308410645,
        "learning_rate": 0.00012748764388563025,
        "epoch": 0.4124983926964125,
        "step": 3208
    },
    {
        "loss": 1.7495,
        "grad_norm": 2.6150262355804443,
        "learning_rate": 0.00012744872670198055,
        "epoch": 0.4126269769834126,
        "step": 3209
    },
    {
        "loss": 1.4728,
        "grad_norm": 2.273221254348755,
        "learning_rate": 0.0001274098050218302,
        "epoch": 0.41275556127041274,
        "step": 3210
    },
    {
        "loss": 1.9957,
        "grad_norm": 1.625091552734375,
        "learning_rate": 0.00012737087885155503,
        "epoch": 0.4128841455574129,
        "step": 3211
    },
    {
        "loss": 1.4451,
        "grad_norm": 1.7453006505966187,
        "learning_rate": 0.0001273319481975318,
        "epoch": 0.413012729844413,
        "step": 3212
    },
    {
        "loss": 2.1269,
        "grad_norm": 2.495612144470215,
        "learning_rate": 0.00012729301306613786,
        "epoch": 0.41314131413141314,
        "step": 3213
    },
    {
        "loss": 1.6815,
        "grad_norm": 1.5912466049194336,
        "learning_rate": 0.00012725407346375137,
        "epoch": 0.4132698984184133,
        "step": 3214
    },
    {
        "loss": 1.8291,
        "grad_norm": 1.9562119245529175,
        "learning_rate": 0.00012721512939675123,
        "epoch": 0.4133984827054134,
        "step": 3215
    },
    {
        "loss": 1.9668,
        "grad_norm": 1.762542486190796,
        "learning_rate": 0.00012717618087151699,
        "epoch": 0.41352706699241354,
        "step": 3216
    },
    {
        "loss": 2.4893,
        "grad_norm": 1.519817590713501,
        "learning_rate": 0.00012713722789442906,
        "epoch": 0.41365565127941367,
        "step": 3217
    },
    {
        "loss": 1.932,
        "grad_norm": 1.519802451133728,
        "learning_rate": 0.00012709827047186845,
        "epoch": 0.4137842355664138,
        "step": 3218
    },
    {
        "loss": 2.5966,
        "grad_norm": 0.9846546649932861,
        "learning_rate": 0.00012705930861021695,
        "epoch": 0.41391281985341394,
        "step": 3219
    },
    {
        "loss": 0.9272,
        "grad_norm": 1.7983647584915161,
        "learning_rate": 0.0001270203423158571,
        "epoch": 0.414041404140414,
        "step": 3220
    },
    {
        "loss": 1.1063,
        "grad_norm": 2.6036527156829834,
        "learning_rate": 0.00012698137159517214,
        "epoch": 0.41416998842741415,
        "step": 3221
    },
    {
        "loss": 1.7622,
        "grad_norm": 2.461258888244629,
        "learning_rate": 0.00012694239645454604,
        "epoch": 0.4142985727144143,
        "step": 3222
    },
    {
        "loss": 1.7576,
        "grad_norm": 1.6791743040084839,
        "learning_rate": 0.0001269034169003635,
        "epoch": 0.4144271570014144,
        "step": 3223
    },
    {
        "loss": 1.7207,
        "grad_norm": 2.1745715141296387,
        "learning_rate": 0.00012686443293900994,
        "epoch": 0.41455574128841455,
        "step": 3224
    },
    {
        "loss": 1.3859,
        "grad_norm": 2.244997262954712,
        "learning_rate": 0.0001268254445768715,
        "epoch": 0.4146843255754147,
        "step": 3225
    },
    {
        "loss": 1.582,
        "grad_norm": 1.4332531690597534,
        "learning_rate": 0.00012678645182033499,
        "epoch": 0.4148129098624148,
        "step": 3226
    },
    {
        "loss": 1.7511,
        "grad_norm": 2.094451665878296,
        "learning_rate": 0.0001267474546757881,
        "epoch": 0.41494149414941495,
        "step": 3227
    },
    {
        "loss": 1.2487,
        "grad_norm": 1.8592416048049927,
        "learning_rate": 0.00012670845314961902,
        "epoch": 0.4150700784364151,
        "step": 3228
    },
    {
        "loss": 1.8667,
        "grad_norm": 2.225860595703125,
        "learning_rate": 0.00012666944724821678,
        "epoch": 0.4151986627234152,
        "step": 3229
    },
    {
        "loss": 1.9711,
        "grad_norm": 1.9656322002410889,
        "learning_rate": 0.00012663043697797122,
        "epoch": 0.41532724701041535,
        "step": 3230
    },
    {
        "loss": 1.7543,
        "grad_norm": 1.3642808198928833,
        "learning_rate": 0.00012659142234527267,
        "epoch": 0.4154558312974155,
        "step": 3231
    },
    {
        "loss": 1.4004,
        "grad_norm": 2.23577880859375,
        "learning_rate": 0.00012655240335651235,
        "epoch": 0.4155844155844156,
        "step": 3232
    },
    {
        "loss": 1.889,
        "grad_norm": 1.6122995615005493,
        "learning_rate": 0.00012651338001808213,
        "epoch": 0.4157129998714157,
        "step": 3233
    },
    {
        "loss": 1.3246,
        "grad_norm": 2.3828697204589844,
        "learning_rate": 0.0001264743523363746,
        "epoch": 0.4158415841584158,
        "step": 3234
    },
    {
        "loss": 1.484,
        "grad_norm": 1.781416416168213,
        "learning_rate": 0.00012643532031778306,
        "epoch": 0.41597016844541596,
        "step": 3235
    },
    {
        "loss": 1.6639,
        "grad_norm": 1.994065523147583,
        "learning_rate": 0.0001263962839687015,
        "epoch": 0.4160987527324161,
        "step": 3236
    },
    {
        "loss": 2.0047,
        "grad_norm": 2.2510645389556885,
        "learning_rate": 0.0001263572432955247,
        "epoch": 0.4162273370194162,
        "step": 3237
    },
    {
        "loss": 1.9601,
        "grad_norm": 2.697000503540039,
        "learning_rate": 0.000126318198304648,
        "epoch": 0.41635592130641635,
        "step": 3238
    },
    {
        "loss": 1.7474,
        "grad_norm": 1.6986368894577026,
        "learning_rate": 0.00012627914900246767,
        "epoch": 0.4164845055934165,
        "step": 3239
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.0310935974121094,
        "learning_rate": 0.0001262400953953804,
        "epoch": 0.4166130898804166,
        "step": 3240
    },
    {
        "loss": 1.2557,
        "grad_norm": 2.165398597717285,
        "learning_rate": 0.0001262010374897839,
        "epoch": 0.41674167416741675,
        "step": 3241
    },
    {
        "loss": 2.3647,
        "grad_norm": 1.5044496059417725,
        "learning_rate": 0.00012616197529207628,
        "epoch": 0.4168702584544169,
        "step": 3242
    },
    {
        "loss": 1.6646,
        "grad_norm": 1.8810877799987793,
        "learning_rate": 0.00012612290880865655,
        "epoch": 0.416998842741417,
        "step": 3243
    },
    {
        "loss": 1.7936,
        "grad_norm": 1.3569321632385254,
        "learning_rate": 0.0001260838380459244,
        "epoch": 0.41712742702841715,
        "step": 3244
    },
    {
        "loss": 1.6369,
        "grad_norm": 1.9918893575668335,
        "learning_rate": 0.00012604476301028014,
        "epoch": 0.41725601131541723,
        "step": 3245
    },
    {
        "loss": 2.0265,
        "grad_norm": 2.2270872592926025,
        "learning_rate": 0.00012600568370812486,
        "epoch": 0.41738459560241736,
        "step": 3246
    },
    {
        "loss": 2.1334,
        "grad_norm": 1.893320083618164,
        "learning_rate": 0.00012596660014586031,
        "epoch": 0.4175131798894175,
        "step": 3247
    },
    {
        "loss": 2.2812,
        "grad_norm": 1.8570929765701294,
        "learning_rate": 0.00012592751232988895,
        "epoch": 0.41764176417641763,
        "step": 3248
    },
    {
        "loss": 1.7626,
        "grad_norm": 1.4263300895690918,
        "learning_rate": 0.0001258884202666139,
        "epoch": 0.41777034846341776,
        "step": 3249
    },
    {
        "loss": 2.3067,
        "grad_norm": 2.0709352493286133,
        "learning_rate": 0.00012584932396243908,
        "epoch": 0.4178989327504179,
        "step": 3250
    },
    {
        "loss": 1.8776,
        "grad_norm": 1.7302916049957275,
        "learning_rate": 0.00012581022342376895,
        "epoch": 0.41802751703741803,
        "step": 3251
    },
    {
        "loss": 2.3341,
        "grad_norm": 1.750005841255188,
        "learning_rate": 0.0001257711186570088,
        "epoch": 0.41815610132441816,
        "step": 3252
    },
    {
        "loss": 1.9319,
        "grad_norm": 1.574475884437561,
        "learning_rate": 0.00012573200966856457,
        "epoch": 0.4182846856114183,
        "step": 3253
    },
    {
        "loss": 1.8062,
        "grad_norm": 2.0320680141448975,
        "learning_rate": 0.00012569289646484284,
        "epoch": 0.4184132698984184,
        "step": 3254
    },
    {
        "loss": 2.2349,
        "grad_norm": 1.476697564125061,
        "learning_rate": 0.00012565377905225094,
        "epoch": 0.41854185418541856,
        "step": 3255
    },
    {
        "loss": 1.2834,
        "grad_norm": 2.230656623840332,
        "learning_rate": 0.00012561465743719685,
        "epoch": 0.4186704384724187,
        "step": 3256
    },
    {
        "loss": 1.2522,
        "grad_norm": 1.9895765781402588,
        "learning_rate": 0.0001255755316260893,
        "epoch": 0.4187990227594188,
        "step": 3257
    },
    {
        "loss": 1.625,
        "grad_norm": 1.9258630275726318,
        "learning_rate": 0.00012553640162533763,
        "epoch": 0.4189276070464189,
        "step": 3258
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.7045354843139648,
        "learning_rate": 0.00012549726744135196,
        "epoch": 0.41905619133341904,
        "step": 3259
    },
    {
        "loss": 1.8312,
        "grad_norm": 1.3510793447494507,
        "learning_rate": 0.00012545812908054294,
        "epoch": 0.41918477562041917,
        "step": 3260
    },
    {
        "loss": 2.0476,
        "grad_norm": 1.4452226161956787,
        "learning_rate": 0.0001254189865493221,
        "epoch": 0.4193133599074193,
        "step": 3261
    },
    {
        "loss": 1.9986,
        "grad_norm": 1.982885718345642,
        "learning_rate": 0.0001253798398541015,
        "epoch": 0.41944194419441944,
        "step": 3262
    },
    {
        "loss": 1.6491,
        "grad_norm": 2.0942704677581787,
        "learning_rate": 0.000125340689001294,
        "epoch": 0.41957052848141957,
        "step": 3263
    },
    {
        "loss": 2.4734,
        "grad_norm": 2.079974412918091,
        "learning_rate": 0.000125301533997313,
        "epoch": 0.4196991127684197,
        "step": 3264
    },
    {
        "loss": 1.7124,
        "grad_norm": 1.4941967725753784,
        "learning_rate": 0.0001252623748485727,
        "epoch": 0.41982769705541984,
        "step": 3265
    },
    {
        "loss": 2.4109,
        "grad_norm": 1.368603229522705,
        "learning_rate": 0.0001252232115614879,
        "epoch": 0.41995628134241997,
        "step": 3266
    },
    {
        "loss": 2.2141,
        "grad_norm": 2.3969783782958984,
        "learning_rate": 0.0001251840441424742,
        "epoch": 0.4200848656294201,
        "step": 3267
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.959101915359497,
        "learning_rate": 0.0001251448725979477,
        "epoch": 0.42021344991642023,
        "step": 3268
    },
    {
        "loss": 2.2515,
        "grad_norm": 1.7941880226135254,
        "learning_rate": 0.00012510569693432536,
        "epoch": 0.42034203420342037,
        "step": 3269
    },
    {
        "loss": 1.6637,
        "grad_norm": 2.9286317825317383,
        "learning_rate": 0.00012506651715802464,
        "epoch": 0.42047061849042044,
        "step": 3270
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.9375876188278198,
        "learning_rate": 0.00012502733327546375,
        "epoch": 0.4205992027774206,
        "step": 3271
    },
    {
        "loss": 2.0735,
        "grad_norm": 1.9706393480300903,
        "learning_rate": 0.0001249881452930617,
        "epoch": 0.4207277870644207,
        "step": 3272
    },
    {
        "loss": 1.8061,
        "grad_norm": 1.64584481716156,
        "learning_rate": 0.00012494895321723791,
        "epoch": 0.42085637135142084,
        "step": 3273
    },
    {
        "loss": 1.9982,
        "grad_norm": 1.4835249185562134,
        "learning_rate": 0.00012490975705441274,
        "epoch": 0.420984955638421,
        "step": 3274
    },
    {
        "loss": 0.7743,
        "grad_norm": 2.1637330055236816,
        "learning_rate": 0.00012487055681100699,
        "epoch": 0.4211135399254211,
        "step": 3275
    },
    {
        "loss": 2.0792,
        "grad_norm": 1.3756108283996582,
        "learning_rate": 0.00012483135249344226,
        "epoch": 0.42124212421242124,
        "step": 3276
    },
    {
        "loss": 2.2855,
        "grad_norm": 2.0841987133026123,
        "learning_rate": 0.0001247921441081408,
        "epoch": 0.4213707084994214,
        "step": 3277
    },
    {
        "loss": 2.2779,
        "grad_norm": 1.205622911453247,
        "learning_rate": 0.0001247529316615255,
        "epoch": 0.4214992927864215,
        "step": 3278
    },
    {
        "loss": 1.646,
        "grad_norm": 1.4388232231140137,
        "learning_rate": 0.00012471371516001992,
        "epoch": 0.42162787707342164,
        "step": 3279
    },
    {
        "loss": 1.7849,
        "grad_norm": 2.385481595993042,
        "learning_rate": 0.00012467449461004833,
        "epoch": 0.4217564613604218,
        "step": 3280
    },
    {
        "loss": 2.57,
        "grad_norm": 1.7311748266220093,
        "learning_rate": 0.00012463527001803563,
        "epoch": 0.4218850456474219,
        "step": 3281
    },
    {
        "loss": 1.9168,
        "grad_norm": 2.6055665016174316,
        "learning_rate": 0.0001245960413904073,
        "epoch": 0.42201362993442204,
        "step": 3282
    },
    {
        "loss": 2.1079,
        "grad_norm": 2.610046148300171,
        "learning_rate": 0.00012455680873358968,
        "epoch": 0.4221422142214221,
        "step": 3283
    },
    {
        "loss": 2.3175,
        "grad_norm": 2.1264305114746094,
        "learning_rate": 0.00012451757205400956,
        "epoch": 0.42227079850842225,
        "step": 3284
    },
    {
        "loss": 1.9508,
        "grad_norm": 1.4425950050354004,
        "learning_rate": 0.00012447833135809447,
        "epoch": 0.4223993827954224,
        "step": 3285
    },
    {
        "loss": 2.2441,
        "grad_norm": 1.9991194009780884,
        "learning_rate": 0.00012443908665227268,
        "epoch": 0.4225279670824225,
        "step": 3286
    },
    {
        "loss": 1.6809,
        "grad_norm": 1.5683813095092773,
        "learning_rate": 0.00012439983794297298,
        "epoch": 0.42265655136942265,
        "step": 3287
    },
    {
        "loss": 1.5936,
        "grad_norm": 2.3765931129455566,
        "learning_rate": 0.00012436058523662493,
        "epoch": 0.4227851356564228,
        "step": 3288
    },
    {
        "loss": 2.1561,
        "grad_norm": 1.6750924587249756,
        "learning_rate": 0.00012432132853965866,
        "epoch": 0.4229137199434229,
        "step": 3289
    },
    {
        "loss": 1.3654,
        "grad_norm": 2.0148439407348633,
        "learning_rate": 0.000124282067858505,
        "epoch": 0.42304230423042305,
        "step": 3290
    },
    {
        "loss": 1.9297,
        "grad_norm": 2.2044191360473633,
        "learning_rate": 0.0001242428031995954,
        "epoch": 0.4231708885174232,
        "step": 3291
    },
    {
        "loss": 2.0913,
        "grad_norm": 1.7407357692718506,
        "learning_rate": 0.000124203534569362,
        "epoch": 0.4232994728044233,
        "step": 3292
    },
    {
        "loss": 2.0101,
        "grad_norm": 2.958373785018921,
        "learning_rate": 0.0001241642619742376,
        "epoch": 0.42342805709142345,
        "step": 3293
    },
    {
        "loss": 1.6664,
        "grad_norm": 1.7029284238815308,
        "learning_rate": 0.00012412498542065556,
        "epoch": 0.4235566413784236,
        "step": 3294
    },
    {
        "loss": 1.4602,
        "grad_norm": 1.840320348739624,
        "learning_rate": 0.00012408570491505,
        "epoch": 0.42368522566542366,
        "step": 3295
    },
    {
        "loss": 1.3853,
        "grad_norm": 1.965883493423462,
        "learning_rate": 0.00012404642046385565,
        "epoch": 0.4238138099524238,
        "step": 3296
    },
    {
        "loss": 1.4926,
        "grad_norm": 1.7502100467681885,
        "learning_rate": 0.0001240071320735078,
        "epoch": 0.4239423942394239,
        "step": 3297
    },
    {
        "loss": 2.1041,
        "grad_norm": 1.4942179918289185,
        "learning_rate": 0.00012396783975044252,
        "epoch": 0.42407097852642406,
        "step": 3298
    },
    {
        "loss": 2.2963,
        "grad_norm": 1.6168780326843262,
        "learning_rate": 0.00012392854350109646,
        "epoch": 0.4241995628134242,
        "step": 3299
    },
    {
        "loss": 1.8368,
        "grad_norm": 2.004580020904541,
        "learning_rate": 0.00012388924333190692,
        "epoch": 0.4243281471004243,
        "step": 3300
    },
    {
        "eval_loss": 1.9115911722183228,
        "eval_runtime": 28.3408,
        "eval_samples_per_second": 2.788,
        "eval_steps_per_second": 2.788,
        "epoch": 0.4243281471004243,
        "step": 3300
    },
    {
        "loss": 1.9385,
        "grad_norm": 1.6182972192764282,
        "learning_rate": 0.00012384993924931183,
        "epoch": 0.42445673138742446,
        "step": 3301
    },
    {
        "loss": 2.0144,
        "grad_norm": 1.8971165418624878,
        "learning_rate": 0.00012381063125974974,
        "epoch": 0.4245853156744246,
        "step": 3302
    },
    {
        "loss": 1.9097,
        "grad_norm": 1.9766212701797485,
        "learning_rate": 0.0001237713193696599,
        "epoch": 0.4247138999614247,
        "step": 3303
    },
    {
        "loss": 1.7967,
        "grad_norm": 1.9457098245620728,
        "learning_rate": 0.0001237320035854822,
        "epoch": 0.42484248424842486,
        "step": 3304
    },
    {
        "loss": 2.2926,
        "grad_norm": 1.726324200630188,
        "learning_rate": 0.0001236926839136571,
        "epoch": 0.424971068535425,
        "step": 3305
    },
    {
        "loss": 1.8796,
        "grad_norm": 1.8366689682006836,
        "learning_rate": 0.00012365336036062572,
        "epoch": 0.4250996528224251,
        "step": 3306
    },
    {
        "loss": 2.1117,
        "grad_norm": 1.8319200277328491,
        "learning_rate": 0.00012361403293282988,
        "epoch": 0.42522823710942526,
        "step": 3307
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.3683325052261353,
        "learning_rate": 0.00012357470163671189,
        "epoch": 0.42535682139642533,
        "step": 3308
    },
    {
        "loss": 1.4326,
        "grad_norm": 1.9301880598068237,
        "learning_rate": 0.00012353536647871494,
        "epoch": 0.42548540568342547,
        "step": 3309
    },
    {
        "loss": 1.6105,
        "grad_norm": 2.3616604804992676,
        "learning_rate": 0.00012349602746528258,
        "epoch": 0.4256139899704256,
        "step": 3310
    },
    {
        "loss": 1.8092,
        "grad_norm": 1.7515987157821655,
        "learning_rate": 0.0001234566846028591,
        "epoch": 0.42574257425742573,
        "step": 3311
    },
    {
        "loss": 1.7903,
        "grad_norm": 2.194986343383789,
        "learning_rate": 0.00012341733789788953,
        "epoch": 0.42587115854442587,
        "step": 3312
    },
    {
        "loss": 1.3225,
        "grad_norm": 1.6568320989608765,
        "learning_rate": 0.00012337798735681939,
        "epoch": 0.425999742831426,
        "step": 3313
    },
    {
        "loss": 1.9508,
        "grad_norm": 1.7458585500717163,
        "learning_rate": 0.00012333863298609485,
        "epoch": 0.42612832711842613,
        "step": 3314
    },
    {
        "loss": 1.9212,
        "grad_norm": 2.413872480392456,
        "learning_rate": 0.0001232992747921627,
        "epoch": 0.42625691140542626,
        "step": 3315
    },
    {
        "loss": 1.7101,
        "grad_norm": 1.7742594480514526,
        "learning_rate": 0.0001232599127814705,
        "epoch": 0.4263854956924264,
        "step": 3316
    },
    {
        "loss": 1.941,
        "grad_norm": 1.573132038116455,
        "learning_rate": 0.0001232205469604662,
        "epoch": 0.42651407997942653,
        "step": 3317
    },
    {
        "loss": 1.9348,
        "grad_norm": 2.4619498252868652,
        "learning_rate": 0.00012318117733559856,
        "epoch": 0.42664266426642666,
        "step": 3318
    },
    {
        "loss": 1.6482,
        "grad_norm": 2.0409042835235596,
        "learning_rate": 0.0001231418039133169,
        "epoch": 0.4267712485534268,
        "step": 3319
    },
    {
        "loss": 1.5395,
        "grad_norm": 1.7688325643539429,
        "learning_rate": 0.00012310242670007107,
        "epoch": 0.4268998328404269,
        "step": 3320
    },
    {
        "loss": 1.6593,
        "grad_norm": 1.5922019481658936,
        "learning_rate": 0.00012306304570231175,
        "epoch": 0.427028417127427,
        "step": 3321
    },
    {
        "loss": 2.0108,
        "grad_norm": 1.3755429983139038,
        "learning_rate": 0.00012302366092649007,
        "epoch": 0.42715700141442714,
        "step": 3322
    },
    {
        "loss": 1.8621,
        "grad_norm": 1.8872406482696533,
        "learning_rate": 0.00012298427237905779,
        "epoch": 0.4272855857014273,
        "step": 3323
    },
    {
        "loss": 2.0448,
        "grad_norm": 1.7764511108398438,
        "learning_rate": 0.0001229448800664674,
        "epoch": 0.4274141699884274,
        "step": 3324
    },
    {
        "loss": 2.173,
        "grad_norm": 1.9324121475219727,
        "learning_rate": 0.00012290548399517188,
        "epoch": 0.42754275427542754,
        "step": 3325
    },
    {
        "loss": 1.8661,
        "grad_norm": 2.2231173515319824,
        "learning_rate": 0.0001228660841716249,
        "epoch": 0.42767133856242767,
        "step": 3326
    },
    {
        "loss": 2.0484,
        "grad_norm": 2.171025276184082,
        "learning_rate": 0.00012282668060228073,
        "epoch": 0.4277999228494278,
        "step": 3327
    },
    {
        "loss": 1.7376,
        "grad_norm": 1.537413239479065,
        "learning_rate": 0.00012278727329359423,
        "epoch": 0.42792850713642794,
        "step": 3328
    },
    {
        "loss": 2.2397,
        "grad_norm": 1.7058055400848389,
        "learning_rate": 0.00012274786225202088,
        "epoch": 0.42805709142342807,
        "step": 3329
    },
    {
        "loss": 0.8414,
        "grad_norm": 2.5488812923431396,
        "learning_rate": 0.00012270844748401685,
        "epoch": 0.4281856757104282,
        "step": 3330
    },
    {
        "loss": 1.6509,
        "grad_norm": 2.0070393085479736,
        "learning_rate": 0.00012266902899603878,
        "epoch": 0.42831425999742834,
        "step": 3331
    },
    {
        "loss": 2.0371,
        "grad_norm": 1.448518991470337,
        "learning_rate": 0.000122629606794544,
        "epoch": 0.42844284428442847,
        "step": 3332
    },
    {
        "loss": 1.5504,
        "grad_norm": 2.4037859439849854,
        "learning_rate": 0.0001225901808859905,
        "epoch": 0.42857142857142855,
        "step": 3333
    },
    {
        "loss": 1.8593,
        "grad_norm": 1.7099708318710327,
        "learning_rate": 0.00012255075127683672,
        "epoch": 0.4287000128584287,
        "step": 3334
    },
    {
        "loss": 0.8916,
        "grad_norm": 1.2053910493850708,
        "learning_rate": 0.0001225113179735419,
        "epoch": 0.4288285971454288,
        "step": 3335
    },
    {
        "loss": 1.6977,
        "grad_norm": 2.0265941619873047,
        "learning_rate": 0.00012247188098256573,
        "epoch": 0.42895718143242895,
        "step": 3336
    },
    {
        "loss": 1.2649,
        "grad_norm": 1.135962963104248,
        "learning_rate": 0.00012243244031036855,
        "epoch": 0.4290857657194291,
        "step": 3337
    },
    {
        "loss": 1.9696,
        "grad_norm": 1.3059639930725098,
        "learning_rate": 0.00012239299596341138,
        "epoch": 0.4292143500064292,
        "step": 3338
    },
    {
        "loss": 1.4185,
        "grad_norm": 1.9058380126953125,
        "learning_rate": 0.00012235354794815572,
        "epoch": 0.42934293429342935,
        "step": 3339
    },
    {
        "loss": 1.7081,
        "grad_norm": 1.7910587787628174,
        "learning_rate": 0.00012231409627106376,
        "epoch": 0.4294715185804295,
        "step": 3340
    },
    {
        "loss": 2.7168,
        "grad_norm": 2.527827501296997,
        "learning_rate": 0.00012227464093859824,
        "epoch": 0.4296001028674296,
        "step": 3341
    },
    {
        "loss": 1.561,
        "grad_norm": 1.9669620990753174,
        "learning_rate": 0.00012223518195722253,
        "epoch": 0.42972868715442974,
        "step": 3342
    },
    {
        "loss": 2.2452,
        "grad_norm": 2.1175546646118164,
        "learning_rate": 0.00012219571933340062,
        "epoch": 0.4298572714414299,
        "step": 3343
    },
    {
        "loss": 1.764,
        "grad_norm": 2.316417932510376,
        "learning_rate": 0.00012215625307359696,
        "epoch": 0.42998585572843,
        "step": 3344
    },
    {
        "loss": 2.475,
        "grad_norm": 1.7930084466934204,
        "learning_rate": 0.0001221167831842768,
        "epoch": 0.4301144400154301,
        "step": 3345
    },
    {
        "loss": 1.3131,
        "grad_norm": 2.4861042499542236,
        "learning_rate": 0.00012207730967190585,
        "epoch": 0.4302430243024302,
        "step": 3346
    },
    {
        "loss": 2.3095,
        "grad_norm": 1.702839970588684,
        "learning_rate": 0.00012203783254295043,
        "epoch": 0.43037160858943035,
        "step": 3347
    },
    {
        "loss": 2.1256,
        "grad_norm": 2.57838773727417,
        "learning_rate": 0.00012199835180387746,
        "epoch": 0.4305001928764305,
        "step": 3348
    },
    {
        "loss": 1.7945,
        "grad_norm": 1.9720814228057861,
        "learning_rate": 0.00012195886746115448,
        "epoch": 0.4306287771634306,
        "step": 3349
    },
    {
        "loss": 1.7157,
        "grad_norm": 2.0706145763397217,
        "learning_rate": 0.00012191937952124959,
        "epoch": 0.43075736145043075,
        "step": 3350
    },
    {
        "loss": 1.8888,
        "grad_norm": 1.3000199794769287,
        "learning_rate": 0.0001218798879906315,
        "epoch": 0.4308859457374309,
        "step": 3351
    },
    {
        "loss": 1.8739,
        "grad_norm": 3.538156747817993,
        "learning_rate": 0.00012184039287576951,
        "epoch": 0.431014530024431,
        "step": 3352
    },
    {
        "loss": 1.6735,
        "grad_norm": 2.4056828022003174,
        "learning_rate": 0.00012180089418313345,
        "epoch": 0.43114311431143115,
        "step": 3353
    },
    {
        "loss": 1.836,
        "grad_norm": 2.428040027618408,
        "learning_rate": 0.00012176139191919382,
        "epoch": 0.4312716985984313,
        "step": 3354
    },
    {
        "loss": 1.4212,
        "grad_norm": 2.0512280464172363,
        "learning_rate": 0.00012172188609042165,
        "epoch": 0.4314002828854314,
        "step": 3355
    },
    {
        "loss": 1.3956,
        "grad_norm": 1.4661177396774292,
        "learning_rate": 0.00012168237670328858,
        "epoch": 0.43152886717243155,
        "step": 3356
    },
    {
        "loss": 1.3608,
        "grad_norm": 1.2156647443771362,
        "learning_rate": 0.00012164286376426682,
        "epoch": 0.4316574514594317,
        "step": 3357
    },
    {
        "loss": 2.0267,
        "grad_norm": 1.7822425365447998,
        "learning_rate": 0.00012160334727982911,
        "epoch": 0.43178603574643176,
        "step": 3358
    },
    {
        "loss": 2.1251,
        "grad_norm": 1.825525164604187,
        "learning_rate": 0.00012156382725644893,
        "epoch": 0.4319146200334319,
        "step": 3359
    },
    {
        "loss": 1.3229,
        "grad_norm": 2.007664680480957,
        "learning_rate": 0.00012152430370060013,
        "epoch": 0.43204320432043203,
        "step": 3360
    },
    {
        "loss": 1.3649,
        "grad_norm": 1.861687421798706,
        "learning_rate": 0.00012148477661875732,
        "epoch": 0.43217178860743216,
        "step": 3361
    },
    {
        "loss": 1.2851,
        "grad_norm": 1.871619462966919,
        "learning_rate": 0.00012144524601739556,
        "epoch": 0.4323003728944323,
        "step": 3362
    },
    {
        "loss": 1.586,
        "grad_norm": 3.1756978034973145,
        "learning_rate": 0.0001214057119029906,
        "epoch": 0.4324289571814324,
        "step": 3363
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.7455352544784546,
        "learning_rate": 0.00012136617428201862,
        "epoch": 0.43255754146843256,
        "step": 3364
    },
    {
        "loss": 2.116,
        "grad_norm": 1.4982922077178955,
        "learning_rate": 0.00012132663316095651,
        "epoch": 0.4326861257554327,
        "step": 3365
    },
    {
        "loss": 1.463,
        "grad_norm": 2.5627760887145996,
        "learning_rate": 0.00012128708854628168,
        "epoch": 0.4328147100424328,
        "step": 3366
    },
    {
        "loss": 2.2957,
        "grad_norm": 2.0050816535949707,
        "learning_rate": 0.00012124754044447208,
        "epoch": 0.43294329432943296,
        "step": 3367
    },
    {
        "loss": 2.0074,
        "grad_norm": 3.0916216373443604,
        "learning_rate": 0.00012120798886200631,
        "epoch": 0.4330718786164331,
        "step": 3368
    },
    {
        "loss": 2.1451,
        "grad_norm": 2.1618573665618896,
        "learning_rate": 0.00012116843380536349,
        "epoch": 0.4332004629034332,
        "step": 3369
    },
    {
        "loss": 1.9581,
        "grad_norm": 2.6303744316101074,
        "learning_rate": 0.00012112887528102326,
        "epoch": 0.4333290471904333,
        "step": 3370
    },
    {
        "loss": 1.6503,
        "grad_norm": 2.0610246658325195,
        "learning_rate": 0.00012108931329546591,
        "epoch": 0.43345763147743344,
        "step": 3371
    },
    {
        "loss": 1.8092,
        "grad_norm": 2.284000873565674,
        "learning_rate": 0.00012104974785517229,
        "epoch": 0.43358621576443357,
        "step": 3372
    },
    {
        "loss": 2.0901,
        "grad_norm": 1.641330361366272,
        "learning_rate": 0.00012101017896662381,
        "epoch": 0.4337148000514337,
        "step": 3373
    },
    {
        "loss": 1.4679,
        "grad_norm": 2.119860887527466,
        "learning_rate": 0.00012097060663630235,
        "epoch": 0.43384338433843384,
        "step": 3374
    },
    {
        "loss": 1.6582,
        "grad_norm": 1.4424887895584106,
        "learning_rate": 0.00012093103087069054,
        "epoch": 0.43397196862543397,
        "step": 3375
    },
    {
        "loss": 1.5946,
        "grad_norm": 1.7394397258758545,
        "learning_rate": 0.00012089145167627135,
        "epoch": 0.4341005529124341,
        "step": 3376
    },
    {
        "loss": 1.9349,
        "grad_norm": 1.4040993452072144,
        "learning_rate": 0.00012085186905952855,
        "epoch": 0.43422913719943423,
        "step": 3377
    },
    {
        "loss": 2.297,
        "grad_norm": 1.2354257106781006,
        "learning_rate": 0.00012081228302694625,
        "epoch": 0.43435772148643437,
        "step": 3378
    },
    {
        "loss": 2.0009,
        "grad_norm": 2.0512540340423584,
        "learning_rate": 0.00012077269358500924,
        "epoch": 0.4344863057734345,
        "step": 3379
    },
    {
        "loss": 1.9394,
        "grad_norm": 2.3280398845672607,
        "learning_rate": 0.00012073310074020289,
        "epoch": 0.43461489006043463,
        "step": 3380
    },
    {
        "loss": 1.2782,
        "grad_norm": 1.993212103843689,
        "learning_rate": 0.00012069350449901305,
        "epoch": 0.43474347434743477,
        "step": 3381
    },
    {
        "loss": 1.0772,
        "grad_norm": 1.8785356283187866,
        "learning_rate": 0.00012065390486792614,
        "epoch": 0.4348720586344349,
        "step": 3382
    },
    {
        "loss": 1.6336,
        "grad_norm": 1.838112473487854,
        "learning_rate": 0.00012061430185342921,
        "epoch": 0.435000642921435,
        "step": 3383
    },
    {
        "loss": 1.6767,
        "grad_norm": 2.220900058746338,
        "learning_rate": 0.00012057469546200977,
        "epoch": 0.4351292272084351,
        "step": 3384
    },
    {
        "loss": 1.1515,
        "grad_norm": 3.2605676651000977,
        "learning_rate": 0.00012053508570015593,
        "epoch": 0.43525781149543524,
        "step": 3385
    },
    {
        "loss": 2.1607,
        "grad_norm": 1.4553704261779785,
        "learning_rate": 0.00012049547257435633,
        "epoch": 0.4353863957824354,
        "step": 3386
    },
    {
        "loss": 1.6032,
        "grad_norm": 1.6774063110351562,
        "learning_rate": 0.00012045585609110022,
        "epoch": 0.4355149800694355,
        "step": 3387
    },
    {
        "loss": 1.4513,
        "grad_norm": 1.2936350107192993,
        "learning_rate": 0.00012041623625687732,
        "epoch": 0.43564356435643564,
        "step": 3388
    },
    {
        "loss": 1.9617,
        "grad_norm": 3.271214723587036,
        "learning_rate": 0.00012037661307817796,
        "epoch": 0.4357721486434358,
        "step": 3389
    },
    {
        "loss": 1.177,
        "grad_norm": 2.442080020904541,
        "learning_rate": 0.00012033698656149298,
        "epoch": 0.4359007329304359,
        "step": 3390
    },
    {
        "loss": 1.7714,
        "grad_norm": 2.342634916305542,
        "learning_rate": 0.00012029735671331374,
        "epoch": 0.43602931721743604,
        "step": 3391
    },
    {
        "loss": 1.5435,
        "grad_norm": 1.5780144929885864,
        "learning_rate": 0.00012025772354013225,
        "epoch": 0.4361579015044362,
        "step": 3392
    },
    {
        "loss": 1.8216,
        "grad_norm": 2.689401149749756,
        "learning_rate": 0.00012021808704844097,
        "epoch": 0.4362864857914363,
        "step": 3393
    },
    {
        "loss": 1.7581,
        "grad_norm": 1.840786337852478,
        "learning_rate": 0.00012017844724473295,
        "epoch": 0.43641507007843644,
        "step": 3394
    },
    {
        "loss": 2.0324,
        "grad_norm": 2.199420213699341,
        "learning_rate": 0.00012013880413550176,
        "epoch": 0.4365436543654365,
        "step": 3395
    },
    {
        "loss": 2.3959,
        "grad_norm": 1.5039888620376587,
        "learning_rate": 0.0001200991577272415,
        "epoch": 0.43667223865243665,
        "step": 3396
    },
    {
        "loss": 1.5833,
        "grad_norm": 2.029834747314453,
        "learning_rate": 0.00012005950802644684,
        "epoch": 0.4368008229394368,
        "step": 3397
    },
    {
        "loss": 2.1222,
        "grad_norm": 1.3457030057907104,
        "learning_rate": 0.00012001985503961297,
        "epoch": 0.4369294072264369,
        "step": 3398
    },
    {
        "loss": 1.4842,
        "grad_norm": 2.102795362472534,
        "learning_rate": 0.00011998019877323565,
        "epoch": 0.43705799151343705,
        "step": 3399
    },
    {
        "loss": 1.4633,
        "grad_norm": 1.3714878559112549,
        "learning_rate": 0.00011994053923381112,
        "epoch": 0.4371865758004372,
        "step": 3400
    },
    {
        "eval_loss": 1.8982347249984741,
        "eval_runtime": 28.3186,
        "eval_samples_per_second": 2.79,
        "eval_steps_per_second": 2.79,
        "epoch": 0.4371865758004372,
        "step": 3400
    },
    {
        "loss": 1.8727,
        "grad_norm": 2.2279274463653564,
        "learning_rate": 0.00011990087642783626,
        "epoch": 0.4373151600874373,
        "step": 3401
    },
    {
        "loss": 1.6536,
        "grad_norm": 1.2725685834884644,
        "learning_rate": 0.00011986121036180829,
        "epoch": 0.43744374437443745,
        "step": 3402
    },
    {
        "loss": 0.6751,
        "grad_norm": 2.0207180976867676,
        "learning_rate": 0.0001198215410422252,
        "epoch": 0.4375723286614376,
        "step": 3403
    },
    {
        "loss": 1.8885,
        "grad_norm": 2.826164484024048,
        "learning_rate": 0.00011978186847558536,
        "epoch": 0.4377009129484377,
        "step": 3404
    },
    {
        "loss": 2.0852,
        "grad_norm": 1.9259920120239258,
        "learning_rate": 0.0001197421926683877,
        "epoch": 0.43782949723543785,
        "step": 3405
    },
    {
        "loss": 1.3806,
        "grad_norm": 2.041435718536377,
        "learning_rate": 0.00011970251362713174,
        "epoch": 0.437958081522438,
        "step": 3406
    },
    {
        "loss": 1.9853,
        "grad_norm": 1.8294214010238647,
        "learning_rate": 0.00011966283135831741,
        "epoch": 0.4380866658094381,
        "step": 3407
    },
    {
        "loss": 1.7943,
        "grad_norm": 1.4445945024490356,
        "learning_rate": 0.00011962314586844534,
        "epoch": 0.4382152500964382,
        "step": 3408
    },
    {
        "loss": 1.3789,
        "grad_norm": 1.4104896783828735,
        "learning_rate": 0.00011958345716401653,
        "epoch": 0.4383438343834383,
        "step": 3409
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.7150763273239136,
        "learning_rate": 0.00011954376525153254,
        "epoch": 0.43847241867043846,
        "step": 3410
    },
    {
        "loss": 1.2886,
        "grad_norm": 1.5084798336029053,
        "learning_rate": 0.00011950407013749554,
        "epoch": 0.4386010029574386,
        "step": 3411
    },
    {
        "loss": 1.2208,
        "grad_norm": 2.5137691497802734,
        "learning_rate": 0.00011946437182840813,
        "epoch": 0.4387295872444387,
        "step": 3412
    },
    {
        "loss": 2.2303,
        "grad_norm": 2.3816394805908203,
        "learning_rate": 0.00011942467033077349,
        "epoch": 0.43885817153143886,
        "step": 3413
    },
    {
        "loss": 1.5115,
        "grad_norm": 1.9658950567245483,
        "learning_rate": 0.0001193849656510953,
        "epoch": 0.438986755818439,
        "step": 3414
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.7163012027740479,
        "learning_rate": 0.00011934525779587777,
        "epoch": 0.4391153401054391,
        "step": 3415
    },
    {
        "loss": 1.9078,
        "grad_norm": 2.0731241703033447,
        "learning_rate": 0.00011930554677162563,
        "epoch": 0.43924392439243926,
        "step": 3416
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.9302445650100708,
        "learning_rate": 0.00011926583258484408,
        "epoch": 0.4393725086794394,
        "step": 3417
    },
    {
        "loss": 2.0499,
        "grad_norm": 2.1946463584899902,
        "learning_rate": 0.00011922611524203891,
        "epoch": 0.4395010929664395,
        "step": 3418
    },
    {
        "loss": 2.0453,
        "grad_norm": 2.13313889503479,
        "learning_rate": 0.0001191863947497164,
        "epoch": 0.43962967725343965,
        "step": 3419
    },
    {
        "loss": 2.2738,
        "grad_norm": 2.745936870574951,
        "learning_rate": 0.00011914667111438337,
        "epoch": 0.43975826154043973,
        "step": 3420
    },
    {
        "loss": 2.1611,
        "grad_norm": 2.23964786529541,
        "learning_rate": 0.00011910694434254708,
        "epoch": 0.43988684582743987,
        "step": 3421
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.6943480968475342,
        "learning_rate": 0.00011906721444071538,
        "epoch": 0.44001543011444,
        "step": 3422
    },
    {
        "loss": 1.8228,
        "grad_norm": 2.161726474761963,
        "learning_rate": 0.00011902748141539659,
        "epoch": 0.44014401440144013,
        "step": 3423
    },
    {
        "loss": 1.8833,
        "grad_norm": 1.876338243484497,
        "learning_rate": 0.0001189877452730996,
        "epoch": 0.44027259868844026,
        "step": 3424
    },
    {
        "loss": 1.8718,
        "grad_norm": 1.9270050525665283,
        "learning_rate": 0.00011894800602033374,
        "epoch": 0.4404011829754404,
        "step": 3425
    },
    {
        "loss": 2.0039,
        "grad_norm": 1.8832470178604126,
        "learning_rate": 0.00011890826366360883,
        "epoch": 0.44052976726244053,
        "step": 3426
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.9164750576019287,
        "learning_rate": 0.00011886851820943534,
        "epoch": 0.44065835154944066,
        "step": 3427
    },
    {
        "loss": 1.6413,
        "grad_norm": 2.059370994567871,
        "learning_rate": 0.00011882876966432408,
        "epoch": 0.4407869358364408,
        "step": 3428
    },
    {
        "loss": 1.658,
        "grad_norm": 2.2226200103759766,
        "learning_rate": 0.00011878901803478648,
        "epoch": 0.44091552012344093,
        "step": 3429
    },
    {
        "loss": 2.2277,
        "grad_norm": 2.2587811946868896,
        "learning_rate": 0.00011874926332733441,
        "epoch": 0.44104410441044106,
        "step": 3430
    },
    {
        "loss": 2.0024,
        "grad_norm": 2.0377581119537354,
        "learning_rate": 0.00011870950554848025,
        "epoch": 0.4411726886974412,
        "step": 3431
    },
    {
        "loss": 1.8218,
        "grad_norm": 1.7878996133804321,
        "learning_rate": 0.00011866974470473699,
        "epoch": 0.44130127298444133,
        "step": 3432
    },
    {
        "loss": 1.4313,
        "grad_norm": 1.8292732238769531,
        "learning_rate": 0.00011862998080261793,
        "epoch": 0.4414298572714414,
        "step": 3433
    },
    {
        "loss": 2.0514,
        "grad_norm": 1.2266579866409302,
        "learning_rate": 0.00011859021384863704,
        "epoch": 0.44155844155844154,
        "step": 3434
    },
    {
        "loss": 1.9445,
        "grad_norm": 1.7383192777633667,
        "learning_rate": 0.00011855044384930873,
        "epoch": 0.44168702584544167,
        "step": 3435
    },
    {
        "loss": 1.137,
        "grad_norm": 1.8515905141830444,
        "learning_rate": 0.00011851067081114784,
        "epoch": 0.4418156101324418,
        "step": 3436
    },
    {
        "loss": 1.5867,
        "grad_norm": 1.7621233463287354,
        "learning_rate": 0.00011847089474066986,
        "epoch": 0.44194419441944194,
        "step": 3437
    },
    {
        "loss": 1.8253,
        "grad_norm": 1.6707247495651245,
        "learning_rate": 0.0001184311156443906,
        "epoch": 0.44207277870644207,
        "step": 3438
    },
    {
        "loss": 1.7388,
        "grad_norm": 2.5331437587738037,
        "learning_rate": 0.0001183913335288265,
        "epoch": 0.4422013629934422,
        "step": 3439
    },
    {
        "loss": 2.2336,
        "grad_norm": 2.232020854949951,
        "learning_rate": 0.00011835154840049447,
        "epoch": 0.44232994728044234,
        "step": 3440
    },
    {
        "loss": 1.2037,
        "grad_norm": 2.744605779647827,
        "learning_rate": 0.00011831176026591187,
        "epoch": 0.44245853156744247,
        "step": 3441
    },
    {
        "loss": 1.9322,
        "grad_norm": 1.8253237009048462,
        "learning_rate": 0.00011827196913159655,
        "epoch": 0.4425871158544426,
        "step": 3442
    },
    {
        "loss": 1.4223,
        "grad_norm": 1.3876525163650513,
        "learning_rate": 0.00011823217500406693,
        "epoch": 0.44271570014144274,
        "step": 3443
    },
    {
        "loss": 1.4546,
        "grad_norm": 1.6614036560058594,
        "learning_rate": 0.00011819237788984181,
        "epoch": 0.44284428442844287,
        "step": 3444
    },
    {
        "loss": 2.367,
        "grad_norm": 1.5464462041854858,
        "learning_rate": 0.0001181525777954406,
        "epoch": 0.44297286871544295,
        "step": 3445
    },
    {
        "loss": 2.2063,
        "grad_norm": 1.617569088935852,
        "learning_rate": 0.00011811277472738309,
        "epoch": 0.4431014530024431,
        "step": 3446
    },
    {
        "loss": 1.2754,
        "grad_norm": 2.694798231124878,
        "learning_rate": 0.00011807296869218958,
        "epoch": 0.4432300372894432,
        "step": 3447
    },
    {
        "loss": 1.7113,
        "grad_norm": 2.3222954273223877,
        "learning_rate": 0.00011803315969638093,
        "epoch": 0.44335862157644335,
        "step": 3448
    },
    {
        "loss": 1.615,
        "grad_norm": 3.338928461074829,
        "learning_rate": 0.00011799334774647842,
        "epoch": 0.4434872058634435,
        "step": 3449
    },
    {
        "loss": 1.8557,
        "grad_norm": 1.6564102172851562,
        "learning_rate": 0.00011795353284900379,
        "epoch": 0.4436157901504436,
        "step": 3450
    },
    {
        "loss": 1.5424,
        "grad_norm": 1.400194525718689,
        "learning_rate": 0.00011791371501047937,
        "epoch": 0.44374437443744374,
        "step": 3451
    },
    {
        "loss": 1.8657,
        "grad_norm": 2.2533605098724365,
        "learning_rate": 0.0001178738942374278,
        "epoch": 0.4438729587244439,
        "step": 3452
    },
    {
        "loss": 1.848,
        "grad_norm": 1.647347092628479,
        "learning_rate": 0.00011783407053637241,
        "epoch": 0.444001543011444,
        "step": 3453
    },
    {
        "loss": 1.8964,
        "grad_norm": 1.6522002220153809,
        "learning_rate": 0.00011779424391383682,
        "epoch": 0.44413012729844414,
        "step": 3454
    },
    {
        "loss": 2.27,
        "grad_norm": 1.752740502357483,
        "learning_rate": 0.00011775441437634525,
        "epoch": 0.4442587115854443,
        "step": 3455
    },
    {
        "loss": 1.5165,
        "grad_norm": 2.30356764793396,
        "learning_rate": 0.00011771458193042238,
        "epoch": 0.4443872958724444,
        "step": 3456
    },
    {
        "loss": 2.2867,
        "grad_norm": 2.107801914215088,
        "learning_rate": 0.00011767474658259326,
        "epoch": 0.44451588015944454,
        "step": 3457
    },
    {
        "loss": 2.4463,
        "grad_norm": 2.119048833847046,
        "learning_rate": 0.00011763490833938357,
        "epoch": 0.4446444644464446,
        "step": 3458
    },
    {
        "loss": 1.9277,
        "grad_norm": 1.6639572381973267,
        "learning_rate": 0.00011759506720731938,
        "epoch": 0.44477304873344475,
        "step": 3459
    },
    {
        "loss": 2.2491,
        "grad_norm": 1.5800583362579346,
        "learning_rate": 0.00011755522319292724,
        "epoch": 0.4449016330204449,
        "step": 3460
    },
    {
        "loss": 1.0272,
        "grad_norm": 1.8555046319961548,
        "learning_rate": 0.00011751537630273419,
        "epoch": 0.445030217307445,
        "step": 3461
    },
    {
        "loss": 1.5933,
        "grad_norm": 2.348980188369751,
        "learning_rate": 0.00011747552654326772,
        "epoch": 0.44515880159444515,
        "step": 3462
    },
    {
        "loss": 1.6067,
        "grad_norm": 2.061701774597168,
        "learning_rate": 0.0001174356739210558,
        "epoch": 0.4452873858814453,
        "step": 3463
    },
    {
        "loss": 2.3224,
        "grad_norm": 1.1739853620529175,
        "learning_rate": 0.00011739581844262689,
        "epoch": 0.4454159701684454,
        "step": 3464
    },
    {
        "loss": 1.9074,
        "grad_norm": 1.7256666421890259,
        "learning_rate": 0.00011735596011450985,
        "epoch": 0.44554455445544555,
        "step": 3465
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.186314344406128,
        "learning_rate": 0.0001173160989432341,
        "epoch": 0.4456731387424457,
        "step": 3466
    },
    {
        "loss": 2.2226,
        "grad_norm": 1.3235117197036743,
        "learning_rate": 0.00011727623493532947,
        "epoch": 0.4458017230294458,
        "step": 3467
    },
    {
        "loss": 1.6075,
        "grad_norm": 1.5827276706695557,
        "learning_rate": 0.00011723636809732621,
        "epoch": 0.44593030731644595,
        "step": 3468
    },
    {
        "loss": 1.8503,
        "grad_norm": 1.462666630744934,
        "learning_rate": 0.0001171964984357552,
        "epoch": 0.4460588916034461,
        "step": 3469
    },
    {
        "loss": 1.1838,
        "grad_norm": 2.2270328998565674,
        "learning_rate": 0.00011715662595714754,
        "epoch": 0.44618747589044616,
        "step": 3470
    },
    {
        "loss": 1.0533,
        "grad_norm": 2.2120532989501953,
        "learning_rate": 0.00011711675066803502,
        "epoch": 0.4463160601774463,
        "step": 3471
    },
    {
        "loss": 2.4168,
        "grad_norm": 1.367616891860962,
        "learning_rate": 0.0001170768725749498,
        "epoch": 0.4464446444644464,
        "step": 3472
    },
    {
        "loss": 1.2446,
        "grad_norm": 2.5668694972991943,
        "learning_rate": 0.0001170369916844244,
        "epoch": 0.44657322875144656,
        "step": 3473
    },
    {
        "loss": 1.8693,
        "grad_norm": 1.4719756841659546,
        "learning_rate": 0.00011699710800299194,
        "epoch": 0.4467018130384467,
        "step": 3474
    },
    {
        "loss": 1.7324,
        "grad_norm": 2.000743865966797,
        "learning_rate": 0.00011695722153718596,
        "epoch": 0.4468303973254468,
        "step": 3475
    },
    {
        "loss": 0.6663,
        "grad_norm": 2.2070202827453613,
        "learning_rate": 0.00011691733229354045,
        "epoch": 0.44695898161244696,
        "step": 3476
    },
    {
        "loss": 2.2874,
        "grad_norm": 1.4021639823913574,
        "learning_rate": 0.00011687744027858982,
        "epoch": 0.4470875658994471,
        "step": 3477
    },
    {
        "loss": 1.8441,
        "grad_norm": 1.842403769493103,
        "learning_rate": 0.00011683754549886896,
        "epoch": 0.4472161501864472,
        "step": 3478
    },
    {
        "loss": 1.68,
        "grad_norm": 2.327223539352417,
        "learning_rate": 0.00011679764796091324,
        "epoch": 0.44734473447344736,
        "step": 3479
    },
    {
        "loss": 1.9371,
        "grad_norm": 1.3708778619766235,
        "learning_rate": 0.00011675774767125842,
        "epoch": 0.4474733187604475,
        "step": 3480
    },
    {
        "loss": 2.2172,
        "grad_norm": 1.7770898342132568,
        "learning_rate": 0.00011671784463644077,
        "epoch": 0.4476019030474476,
        "step": 3481
    },
    {
        "loss": 2.2049,
        "grad_norm": 2.0664777755737305,
        "learning_rate": 0.00011667793886299705,
        "epoch": 0.44773048733444776,
        "step": 3482
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.2749433517456055,
        "learning_rate": 0.0001166380303574643,
        "epoch": 0.44785907162144784,
        "step": 3483
    },
    {
        "loss": 1.5994,
        "grad_norm": 2.3467776775360107,
        "learning_rate": 0.00011659811912638017,
        "epoch": 0.44798765590844797,
        "step": 3484
    },
    {
        "loss": 2.266,
        "grad_norm": 1.4243073463439941,
        "learning_rate": 0.00011655820517628269,
        "epoch": 0.4481162401954481,
        "step": 3485
    },
    {
        "loss": 2.3837,
        "grad_norm": 1.8259907960891724,
        "learning_rate": 0.00011651828851371032,
        "epoch": 0.44824482448244823,
        "step": 3486
    },
    {
        "loss": 1.9049,
        "grad_norm": 1.888901710510254,
        "learning_rate": 0.00011647836914520206,
        "epoch": 0.44837340876944837,
        "step": 3487
    },
    {
        "loss": 1.8965,
        "grad_norm": 2.3850958347320557,
        "learning_rate": 0.0001164384470772972,
        "epoch": 0.4485019930564485,
        "step": 3488
    },
    {
        "loss": 1.9897,
        "grad_norm": 1.809410810470581,
        "learning_rate": 0.00011639852231653561,
        "epoch": 0.44863057734344863,
        "step": 3489
    },
    {
        "loss": 1.8866,
        "grad_norm": 1.3620330095291138,
        "learning_rate": 0.00011635859486945755,
        "epoch": 0.44875916163044877,
        "step": 3490
    },
    {
        "loss": 1.6001,
        "grad_norm": 1.7439196109771729,
        "learning_rate": 0.00011631866474260366,
        "epoch": 0.4488877459174489,
        "step": 3491
    },
    {
        "loss": 1.0457,
        "grad_norm": 3.010280132293701,
        "learning_rate": 0.00011627873194251516,
        "epoch": 0.44901633020444903,
        "step": 3492
    },
    {
        "loss": 1.8539,
        "grad_norm": 1.6531243324279785,
        "learning_rate": 0.00011623879647573355,
        "epoch": 0.44914491449144917,
        "step": 3493
    },
    {
        "loss": 1.4618,
        "grad_norm": 1.1078202724456787,
        "learning_rate": 0.00011619885834880086,
        "epoch": 0.4492734987784493,
        "step": 3494
    },
    {
        "loss": 1.8611,
        "grad_norm": 1.3387938737869263,
        "learning_rate": 0.00011615891756825957,
        "epoch": 0.4494020830654494,
        "step": 3495
    },
    {
        "loss": 1.9512,
        "grad_norm": 2.031907558441162,
        "learning_rate": 0.00011611897414065249,
        "epoch": 0.4495306673524495,
        "step": 3496
    },
    {
        "loss": 2.2235,
        "grad_norm": 1.947145938873291,
        "learning_rate": 0.00011607902807252302,
        "epoch": 0.44965925163944964,
        "step": 3497
    },
    {
        "loss": 1.9569,
        "grad_norm": 1.8657219409942627,
        "learning_rate": 0.00011603907937041485,
        "epoch": 0.4497878359264498,
        "step": 3498
    },
    {
        "loss": 1.0201,
        "grad_norm": 2.0519464015960693,
        "learning_rate": 0.00011599912804087216,
        "epoch": 0.4499164202134499,
        "step": 3499
    },
    {
        "loss": 1.5543,
        "grad_norm": 2.813184976577759,
        "learning_rate": 0.00011595917409043959,
        "epoch": 0.45004500450045004,
        "step": 3500
    },
    {
        "eval_loss": 1.892218828201294,
        "eval_runtime": 28.3194,
        "eval_samples_per_second": 2.79,
        "eval_steps_per_second": 2.79,
        "epoch": 0.45004500450045004,
        "step": 3500
    },
    {
        "loss": 1.4843,
        "grad_norm": 1.799281120300293,
        "learning_rate": 0.00011591921752566217,
        "epoch": 0.4501735887874502,
        "step": 3501
    },
    {
        "loss": 2.5206,
        "grad_norm": 1.3370336294174194,
        "learning_rate": 0.00011587925835308535,
        "epoch": 0.4503021730744503,
        "step": 3502
    },
    {
        "loss": 1.1677,
        "grad_norm": 2.0476467609405518,
        "learning_rate": 0.00011583929657925502,
        "epoch": 0.45043075736145044,
        "step": 3503
    },
    {
        "loss": 1.8697,
        "grad_norm": 2.31766676902771,
        "learning_rate": 0.00011579933221071752,
        "epoch": 0.4505593416484506,
        "step": 3504
    },
    {
        "loss": 1.4245,
        "grad_norm": 2.0258710384368896,
        "learning_rate": 0.0001157593652540196,
        "epoch": 0.4506879259354507,
        "step": 3505
    },
    {
        "loss": 1.1784,
        "grad_norm": 4.84114933013916,
        "learning_rate": 0.00011571939571570838,
        "epoch": 0.45081651022245084,
        "step": 3506
    },
    {
        "loss": 2.6248,
        "grad_norm": 1.4814050197601318,
        "learning_rate": 0.00011567942360233148,
        "epoch": 0.45094509450945097,
        "step": 3507
    },
    {
        "loss": 1.5796,
        "grad_norm": 2.424128770828247,
        "learning_rate": 0.00011563944892043699,
        "epoch": 0.45107367879645105,
        "step": 3508
    },
    {
        "loss": 1.4619,
        "grad_norm": 1.4191826581954956,
        "learning_rate": 0.00011559947167657324,
        "epoch": 0.4512022630834512,
        "step": 3509
    },
    {
        "loss": 2.0514,
        "grad_norm": 1.7846055030822754,
        "learning_rate": 0.00011555949187728907,
        "epoch": 0.4513308473704513,
        "step": 3510
    },
    {
        "loss": 1.7536,
        "grad_norm": 1.3528602123260498,
        "learning_rate": 0.00011551950952913387,
        "epoch": 0.45145943165745145,
        "step": 3511
    },
    {
        "loss": 2.2742,
        "grad_norm": 2.3679020404815674,
        "learning_rate": 0.00011547952463865721,
        "epoch": 0.4515880159444516,
        "step": 3512
    },
    {
        "loss": 1.2958,
        "grad_norm": 1.4776222705841064,
        "learning_rate": 0.00011543953721240927,
        "epoch": 0.4517166002314517,
        "step": 3513
    },
    {
        "loss": 1.865,
        "grad_norm": 1.7100931406021118,
        "learning_rate": 0.00011539954725694058,
        "epoch": 0.45184518451845185,
        "step": 3514
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.108693838119507,
        "learning_rate": 0.000115359554778802,
        "epoch": 0.451973768805452,
        "step": 3515
    },
    {
        "loss": 1.401,
        "grad_norm": 1.7728931903839111,
        "learning_rate": 0.00011531955978454496,
        "epoch": 0.4521023530924521,
        "step": 3516
    },
    {
        "loss": 2.2138,
        "grad_norm": 1.802794098854065,
        "learning_rate": 0.00011527956228072116,
        "epoch": 0.45223093737945225,
        "step": 3517
    },
    {
        "loss": 1.8746,
        "grad_norm": 1.853454828262329,
        "learning_rate": 0.00011523956227388283,
        "epoch": 0.4523595216664524,
        "step": 3518
    },
    {
        "loss": 1.3069,
        "grad_norm": 2.119098424911499,
        "learning_rate": 0.00011519955977058253,
        "epoch": 0.4524881059534525,
        "step": 3519
    },
    {
        "loss": 1.663,
        "grad_norm": 1.6070092916488647,
        "learning_rate": 0.00011515955477737321,
        "epoch": 0.4526166902404526,
        "step": 3520
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.2187490463256836,
        "learning_rate": 0.00011511954730080832,
        "epoch": 0.4527452745274527,
        "step": 3521
    },
    {
        "loss": 1.6274,
        "grad_norm": 1.5542545318603516,
        "learning_rate": 0.00011507953734744164,
        "epoch": 0.45287385881445286,
        "step": 3522
    },
    {
        "loss": 1.6151,
        "grad_norm": 2.3800182342529297,
        "learning_rate": 0.00011503952492382744,
        "epoch": 0.453002443101453,
        "step": 3523
    },
    {
        "loss": 1.7937,
        "grad_norm": 1.3624024391174316,
        "learning_rate": 0.00011499951003652025,
        "epoch": 0.4531310273884531,
        "step": 3524
    },
    {
        "loss": 2.0047,
        "grad_norm": 1.7561042308807373,
        "learning_rate": 0.00011495949269207513,
        "epoch": 0.45325961167545326,
        "step": 3525
    },
    {
        "loss": 1.9964,
        "grad_norm": 2.8620073795318604,
        "learning_rate": 0.00011491947289704755,
        "epoch": 0.4533881959624534,
        "step": 3526
    },
    {
        "loss": 1.9786,
        "grad_norm": 2.1932945251464844,
        "learning_rate": 0.00011487945065799323,
        "epoch": 0.4535167802494535,
        "step": 3527
    },
    {
        "loss": 1.3789,
        "grad_norm": 2.6032938957214355,
        "learning_rate": 0.00011483942598146849,
        "epoch": 0.45364536453645365,
        "step": 3528
    },
    {
        "loss": 1.8445,
        "grad_norm": 2.1352345943450928,
        "learning_rate": 0.00011479939887402993,
        "epoch": 0.4537739488234538,
        "step": 3529
    },
    {
        "loss": 1.6953,
        "grad_norm": 1.6058951616287231,
        "learning_rate": 0.00011475936934223455,
        "epoch": 0.4539025331104539,
        "step": 3530
    },
    {
        "loss": 1.8093,
        "grad_norm": 3.084319829940796,
        "learning_rate": 0.00011471933739263978,
        "epoch": 0.45403111739745405,
        "step": 3531
    },
    {
        "loss": 2.1794,
        "grad_norm": 1.9006370306015015,
        "learning_rate": 0.0001146793030318034,
        "epoch": 0.4541597016844542,
        "step": 3532
    },
    {
        "loss": 1.8983,
        "grad_norm": 2.5976645946502686,
        "learning_rate": 0.00011463926626628369,
        "epoch": 0.45428828597145426,
        "step": 3533
    },
    {
        "loss": 1.2115,
        "grad_norm": 2.295430898666382,
        "learning_rate": 0.00011459922710263926,
        "epoch": 0.4544168702584544,
        "step": 3534
    },
    {
        "loss": 1.7637,
        "grad_norm": 1.7324095964431763,
        "learning_rate": 0.00011455918554742901,
        "epoch": 0.45454545454545453,
        "step": 3535
    },
    {
        "loss": 1.281,
        "grad_norm": 1.6229442358016968,
        "learning_rate": 0.00011451914160721243,
        "epoch": 0.45467403883245466,
        "step": 3536
    },
    {
        "loss": 1.895,
        "grad_norm": 2.7827768325805664,
        "learning_rate": 0.00011447909528854923,
        "epoch": 0.4548026231194548,
        "step": 3537
    },
    {
        "loss": 2.1548,
        "grad_norm": 1.7211602926254272,
        "learning_rate": 0.00011443904659799962,
        "epoch": 0.45493120740645493,
        "step": 3538
    },
    {
        "loss": 1.6081,
        "grad_norm": 1.9468435049057007,
        "learning_rate": 0.00011439899554212416,
        "epoch": 0.45505979169345506,
        "step": 3539
    },
    {
        "loss": 1.9075,
        "grad_norm": 1.6586116552352905,
        "learning_rate": 0.00011435894212748377,
        "epoch": 0.4551883759804552,
        "step": 3540
    },
    {
        "loss": 1.9641,
        "grad_norm": 1.387578010559082,
        "learning_rate": 0.00011431888636063979,
        "epoch": 0.45531696026745533,
        "step": 3541
    },
    {
        "loss": 2.0181,
        "grad_norm": 1.7396557331085205,
        "learning_rate": 0.00011427882824815395,
        "epoch": 0.45544554455445546,
        "step": 3542
    },
    {
        "loss": 1.8733,
        "grad_norm": 1.9899858236312866,
        "learning_rate": 0.00011423876779658832,
        "epoch": 0.4555741288414556,
        "step": 3543
    },
    {
        "loss": 1.8739,
        "grad_norm": 1.9615949392318726,
        "learning_rate": 0.00011419870501250543,
        "epoch": 0.4557027131284557,
        "step": 3544
    },
    {
        "loss": 2.5588,
        "grad_norm": 1.4593149423599243,
        "learning_rate": 0.00011415863990246813,
        "epoch": 0.4558312974154558,
        "step": 3545
    },
    {
        "loss": 1.3226,
        "grad_norm": 2.443457841873169,
        "learning_rate": 0.00011411857247303963,
        "epoch": 0.45595988170245594,
        "step": 3546
    },
    {
        "loss": 1.6913,
        "grad_norm": 2.3670172691345215,
        "learning_rate": 0.00011407850273078363,
        "epoch": 0.45608846598945607,
        "step": 3547
    },
    {
        "loss": 1.8981,
        "grad_norm": 2.375499725341797,
        "learning_rate": 0.00011403843068226406,
        "epoch": 0.4562170502764562,
        "step": 3548
    },
    {
        "loss": 2.0165,
        "grad_norm": 1.58997642993927,
        "learning_rate": 0.00011399835633404535,
        "epoch": 0.45634563456345634,
        "step": 3549
    },
    {
        "loss": 1.1937,
        "grad_norm": 2.4003825187683105,
        "learning_rate": 0.00011395827969269229,
        "epoch": 0.45647421885045647,
        "step": 3550
    },
    {
        "loss": 1.3688,
        "grad_norm": 1.6715266704559326,
        "learning_rate": 0.00011391820076476991,
        "epoch": 0.4566028031374566,
        "step": 3551
    },
    {
        "loss": 1.4182,
        "grad_norm": 2.261662244796753,
        "learning_rate": 0.00011387811955684385,
        "epoch": 0.45673138742445674,
        "step": 3552
    },
    {
        "loss": 2.4648,
        "grad_norm": 1.6280186176300049,
        "learning_rate": 0.00011383803607547989,
        "epoch": 0.45685997171145687,
        "step": 3553
    },
    {
        "loss": 1.9131,
        "grad_norm": 2.0571069717407227,
        "learning_rate": 0.00011379795032724434,
        "epoch": 0.456988555998457,
        "step": 3554
    },
    {
        "loss": 1.5335,
        "grad_norm": 1.3244009017944336,
        "learning_rate": 0.00011375786231870387,
        "epoch": 0.45711714028545714,
        "step": 3555
    },
    {
        "loss": 2.2129,
        "grad_norm": 1.862102746963501,
        "learning_rate": 0.00011371777205642538,
        "epoch": 0.45724572457245727,
        "step": 3556
    },
    {
        "loss": 1.3008,
        "grad_norm": 2.255495548248291,
        "learning_rate": 0.00011367767954697628,
        "epoch": 0.4573743088594574,
        "step": 3557
    },
    {
        "loss": 1.9409,
        "grad_norm": 1.312741756439209,
        "learning_rate": 0.00011363758479692434,
        "epoch": 0.4575028931464575,
        "step": 3558
    },
    {
        "loss": 1.3608,
        "grad_norm": 1.5409564971923828,
        "learning_rate": 0.00011359748781283763,
        "epoch": 0.4576314774334576,
        "step": 3559
    },
    {
        "loss": 1.9545,
        "grad_norm": 1.8533254861831665,
        "learning_rate": 0.00011355738860128467,
        "epoch": 0.45776006172045774,
        "step": 3560
    },
    {
        "loss": 1.9645,
        "grad_norm": 2.134681224822998,
        "learning_rate": 0.00011351728716883418,
        "epoch": 0.4578886460074579,
        "step": 3561
    },
    {
        "loss": 1.8893,
        "grad_norm": 1.833154559135437,
        "learning_rate": 0.00011347718352205547,
        "epoch": 0.458017230294458,
        "step": 3562
    },
    {
        "loss": 0.921,
        "grad_norm": 2.157520055770874,
        "learning_rate": 0.00011343707766751805,
        "epoch": 0.45814581458145814,
        "step": 3563
    },
    {
        "loss": 1.9673,
        "grad_norm": 1.6596591472625732,
        "learning_rate": 0.00011339696961179185,
        "epoch": 0.4582743988684583,
        "step": 3564
    },
    {
        "loss": 1.5313,
        "grad_norm": 2.4838809967041016,
        "learning_rate": 0.00011335685936144716,
        "epoch": 0.4584029831554584,
        "step": 3565
    },
    {
        "loss": 0.4734,
        "grad_norm": 1.8728783130645752,
        "learning_rate": 0.00011331674692305463,
        "epoch": 0.45853156744245854,
        "step": 3566
    },
    {
        "loss": 2.4094,
        "grad_norm": 1.8926206827163696,
        "learning_rate": 0.00011327663230318521,
        "epoch": 0.4586601517294587,
        "step": 3567
    },
    {
        "loss": 2.1365,
        "grad_norm": 2.359419107437134,
        "learning_rate": 0.00011323651550841032,
        "epoch": 0.4587887360164588,
        "step": 3568
    },
    {
        "loss": 2.0766,
        "grad_norm": 2.252199411392212,
        "learning_rate": 0.00011319639654530163,
        "epoch": 0.45891732030345894,
        "step": 3569
    },
    {
        "loss": 2.0277,
        "grad_norm": 1.8154265880584717,
        "learning_rate": 0.00011315627542043125,
        "epoch": 0.459045904590459,
        "step": 3570
    },
    {
        "loss": 1.7936,
        "grad_norm": 1.9616570472717285,
        "learning_rate": 0.00011311615214037161,
        "epoch": 0.45917448887745915,
        "step": 3571
    },
    {
        "loss": 1.1256,
        "grad_norm": 1.790920615196228,
        "learning_rate": 0.00011307602671169541,
        "epoch": 0.4593030731644593,
        "step": 3572
    },
    {
        "loss": 1.4459,
        "grad_norm": 2.0794224739074707,
        "learning_rate": 0.00011303589914097587,
        "epoch": 0.4594316574514594,
        "step": 3573
    },
    {
        "loss": 1.7993,
        "grad_norm": 2.142150640487671,
        "learning_rate": 0.0001129957694347864,
        "epoch": 0.45956024173845955,
        "step": 3574
    },
    {
        "loss": 2.1605,
        "grad_norm": 2.038115978240967,
        "learning_rate": 0.00011295563759970087,
        "epoch": 0.4596888260254597,
        "step": 3575
    },
    {
        "loss": 1.9562,
        "grad_norm": 1.660255789756775,
        "learning_rate": 0.00011291550364229349,
        "epoch": 0.4598174103124598,
        "step": 3576
    },
    {
        "loss": 2.1424,
        "grad_norm": 2.4513261318206787,
        "learning_rate": 0.00011287536756913868,
        "epoch": 0.45994599459945995,
        "step": 3577
    },
    {
        "loss": 2.3405,
        "grad_norm": 2.225015878677368,
        "learning_rate": 0.0001128352293868114,
        "epoch": 0.4600745788864601,
        "step": 3578
    },
    {
        "loss": 2.135,
        "grad_norm": 1.9098730087280273,
        "learning_rate": 0.00011279508910188686,
        "epoch": 0.4602031631734602,
        "step": 3579
    },
    {
        "loss": 2.3203,
        "grad_norm": 1.4839940071105957,
        "learning_rate": 0.00011275494672094059,
        "epoch": 0.46033174746046035,
        "step": 3580
    },
    {
        "loss": 1.3753,
        "grad_norm": 3.7819137573242188,
        "learning_rate": 0.00011271480225054854,
        "epoch": 0.4604603317474605,
        "step": 3581
    },
    {
        "loss": 1.402,
        "grad_norm": 2.0440938472747803,
        "learning_rate": 0.00011267465569728688,
        "epoch": 0.4605889160344606,
        "step": 3582
    },
    {
        "loss": 1.7283,
        "grad_norm": 2.1440770626068115,
        "learning_rate": 0.00011263450706773229,
        "epoch": 0.4607175003214607,
        "step": 3583
    },
    {
        "loss": 2.5233,
        "grad_norm": 1.343035101890564,
        "learning_rate": 0.00011259435636846165,
        "epoch": 0.4608460846084608,
        "step": 3584
    },
    {
        "loss": 2.4992,
        "grad_norm": 1.6794553995132446,
        "learning_rate": 0.00011255420360605222,
        "epoch": 0.46097466889546096,
        "step": 3585
    },
    {
        "loss": 1.7487,
        "grad_norm": 1.726224660873413,
        "learning_rate": 0.00011251404878708165,
        "epoch": 0.4611032531824611,
        "step": 3586
    },
    {
        "loss": 1.491,
        "grad_norm": 2.2912092208862305,
        "learning_rate": 0.00011247389191812787,
        "epoch": 0.4612318374694612,
        "step": 3587
    },
    {
        "loss": 2.0954,
        "grad_norm": 1.7455406188964844,
        "learning_rate": 0.00011243373300576912,
        "epoch": 0.46136042175646136,
        "step": 3588
    },
    {
        "loss": 2.0938,
        "grad_norm": 1.8100069761276245,
        "learning_rate": 0.00011239357205658405,
        "epoch": 0.4614890060434615,
        "step": 3589
    },
    {
        "loss": 2.1643,
        "grad_norm": 1.7644556760787964,
        "learning_rate": 0.00011235340907715159,
        "epoch": 0.4616175903304616,
        "step": 3590
    },
    {
        "loss": 2.1273,
        "grad_norm": 1.4374799728393555,
        "learning_rate": 0.00011231324407405106,
        "epoch": 0.46174617461746176,
        "step": 3591
    },
    {
        "loss": 1.8649,
        "grad_norm": 1.9692994356155396,
        "learning_rate": 0.00011227307705386203,
        "epoch": 0.4618747589044619,
        "step": 3592
    },
    {
        "loss": 0.5102,
        "grad_norm": 1.5980278253555298,
        "learning_rate": 0.00011223290802316444,
        "epoch": 0.462003343191462,
        "step": 3593
    },
    {
        "loss": 1.6904,
        "grad_norm": 2.3360114097595215,
        "learning_rate": 0.00011219273698853858,
        "epoch": 0.46213192747846216,
        "step": 3594
    },
    {
        "loss": 1.6912,
        "grad_norm": 1.9009156227111816,
        "learning_rate": 0.00011215256395656506,
        "epoch": 0.46226051176546223,
        "step": 3595
    },
    {
        "loss": 1.4611,
        "grad_norm": 2.1794593334198,
        "learning_rate": 0.00011211238893382476,
        "epoch": 0.46238909605246237,
        "step": 3596
    },
    {
        "loss": 1.9666,
        "grad_norm": 2.2912516593933105,
        "learning_rate": 0.00011207221192689901,
        "epoch": 0.4625176803394625,
        "step": 3597
    },
    {
        "loss": 1.5358,
        "grad_norm": 2.0548689365386963,
        "learning_rate": 0.00011203203294236932,
        "epoch": 0.46264626462646263,
        "step": 3598
    },
    {
        "loss": 1.8077,
        "grad_norm": 1.5516828298568726,
        "learning_rate": 0.00011199185198681764,
        "epoch": 0.46277484891346277,
        "step": 3599
    },
    {
        "loss": 1.2998,
        "grad_norm": 1.5618081092834473,
        "learning_rate": 0.00011195166906682613,
        "epoch": 0.4629034332004629,
        "step": 3600
    },
    {
        "eval_loss": 1.892377495765686,
        "eval_runtime": 28.1886,
        "eval_samples_per_second": 2.803,
        "eval_steps_per_second": 2.803,
        "epoch": 0.4629034332004629,
        "step": 3600
    },
    {
        "loss": 0.9898,
        "grad_norm": 3.1082534790039062,
        "learning_rate": 0.00011191148418897737,
        "epoch": 0.46303201748746303,
        "step": 3601
    },
    {
        "loss": 1.6726,
        "grad_norm": 2.057042360305786,
        "learning_rate": 0.00011187129735985428,
        "epoch": 0.46316060177446317,
        "step": 3602
    },
    {
        "loss": 1.2917,
        "grad_norm": 1.4166688919067383,
        "learning_rate": 0.00011183110858603997,
        "epoch": 0.4632891860614633,
        "step": 3603
    },
    {
        "loss": 1.1994,
        "grad_norm": 1.7851011753082275,
        "learning_rate": 0.00011179091787411798,
        "epoch": 0.46341777034846343,
        "step": 3604
    },
    {
        "loss": 1.7632,
        "grad_norm": 2.0456042289733887,
        "learning_rate": 0.00011175072523067217,
        "epoch": 0.46354635463546356,
        "step": 3605
    },
    {
        "loss": 1.0056,
        "grad_norm": 2.2266030311584473,
        "learning_rate": 0.00011171053066228659,
        "epoch": 0.4636749389224637,
        "step": 3606
    },
    {
        "loss": 1.4942,
        "grad_norm": 2.2859504222869873,
        "learning_rate": 0.00011167033417554575,
        "epoch": 0.46380352320946383,
        "step": 3607
    },
    {
        "loss": 1.4634,
        "grad_norm": 2.3391780853271484,
        "learning_rate": 0.00011163013577703445,
        "epoch": 0.4639321074964639,
        "step": 3608
    },
    {
        "loss": 1.8335,
        "grad_norm": 1.2220176458358765,
        "learning_rate": 0.00011158993547333771,
        "epoch": 0.46406069178346404,
        "step": 3609
    },
    {
        "loss": 2.0331,
        "grad_norm": 1.946053147315979,
        "learning_rate": 0.00011154973327104097,
        "epoch": 0.4641892760704642,
        "step": 3610
    },
    {
        "loss": 2.0898,
        "grad_norm": 2.1554055213928223,
        "learning_rate": 0.0001115095291767299,
        "epoch": 0.4643178603574643,
        "step": 3611
    },
    {
        "loss": 0.729,
        "grad_norm": 2.252070665359497,
        "learning_rate": 0.00011146932319699055,
        "epoch": 0.46444644464446444,
        "step": 3612
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.5724700689315796,
        "learning_rate": 0.00011142911533840923,
        "epoch": 0.4645750289314646,
        "step": 3613
    },
    {
        "loss": 2.0371,
        "grad_norm": 2.513749361038208,
        "learning_rate": 0.00011138890560757253,
        "epoch": 0.4647036132184647,
        "step": 3614
    },
    {
        "loss": 1.2824,
        "grad_norm": 2.522306203842163,
        "learning_rate": 0.00011134869401106746,
        "epoch": 0.46483219750546484,
        "step": 3615
    },
    {
        "loss": 2.0153,
        "grad_norm": 1.3017406463623047,
        "learning_rate": 0.00011130848055548122,
        "epoch": 0.46496078179246497,
        "step": 3616
    },
    {
        "loss": 1.8447,
        "grad_norm": 2.4195799827575684,
        "learning_rate": 0.00011126826524740137,
        "epoch": 0.4650893660794651,
        "step": 3617
    },
    {
        "loss": 0.6949,
        "grad_norm": 2.2796471118927,
        "learning_rate": 0.0001112280480934158,
        "epoch": 0.46521795036646524,
        "step": 3618
    },
    {
        "loss": 2.2476,
        "grad_norm": 1.812770962715149,
        "learning_rate": 0.00011118782910011258,
        "epoch": 0.46534653465346537,
        "step": 3619
    },
    {
        "loss": 1.8201,
        "grad_norm": 1.7297320365905762,
        "learning_rate": 0.00011114760827408026,
        "epoch": 0.46547511894046545,
        "step": 3620
    },
    {
        "loss": 0.8013,
        "grad_norm": 2.0497262477874756,
        "learning_rate": 0.00011110738562190753,
        "epoch": 0.4656037032274656,
        "step": 3621
    },
    {
        "loss": 2.0198,
        "grad_norm": 1.6061152219772339,
        "learning_rate": 0.00011106716115018343,
        "epoch": 0.4657322875144657,
        "step": 3622
    },
    {
        "loss": 2.1794,
        "grad_norm": 1.7128808498382568,
        "learning_rate": 0.00011102693486549742,
        "epoch": 0.46586087180146585,
        "step": 3623
    },
    {
        "loss": 1.5914,
        "grad_norm": 1.6840137243270874,
        "learning_rate": 0.00011098670677443903,
        "epoch": 0.465989456088466,
        "step": 3624
    },
    {
        "loss": 1.7846,
        "grad_norm": 2.2156715393066406,
        "learning_rate": 0.00011094647688359828,
        "epoch": 0.4661180403754661,
        "step": 3625
    },
    {
        "loss": 0.7021,
        "grad_norm": 3.6260716915130615,
        "learning_rate": 0.00011090624519956536,
        "epoch": 0.46624662466246625,
        "step": 3626
    },
    {
        "loss": 1.8598,
        "grad_norm": 1.3113782405853271,
        "learning_rate": 0.00011086601172893085,
        "epoch": 0.4663752089494664,
        "step": 3627
    },
    {
        "loss": 1.7177,
        "grad_norm": 2.009594678878784,
        "learning_rate": 0.00011082577647828558,
        "epoch": 0.4665037932364665,
        "step": 3628
    },
    {
        "loss": 2.521,
        "grad_norm": 2.0370779037475586,
        "learning_rate": 0.00011078553945422065,
        "epoch": 0.46663237752346665,
        "step": 3629
    },
    {
        "loss": 1.2922,
        "grad_norm": 1.5206586122512817,
        "learning_rate": 0.00011074530066332744,
        "epoch": 0.4667609618104668,
        "step": 3630
    },
    {
        "loss": 2.1302,
        "grad_norm": 1.5646295547485352,
        "learning_rate": 0.0001107050601121977,
        "epoch": 0.4668895460974669,
        "step": 3631
    },
    {
        "loss": 1.3904,
        "grad_norm": 2.500396728515625,
        "learning_rate": 0.0001106648178074234,
        "epoch": 0.46701813038446705,
        "step": 3632
    },
    {
        "loss": 1.8492,
        "grad_norm": 1.942156195640564,
        "learning_rate": 0.0001106245737555968,
        "epoch": 0.4671467146714671,
        "step": 3633
    },
    {
        "loss": 0.7036,
        "grad_norm": 1.6712442636489868,
        "learning_rate": 0.00011058432796331049,
        "epoch": 0.46727529895846726,
        "step": 3634
    },
    {
        "loss": 1.3355,
        "grad_norm": 1.6964982748031616,
        "learning_rate": 0.00011054408043715728,
        "epoch": 0.4674038832454674,
        "step": 3635
    },
    {
        "loss": 1.7946,
        "grad_norm": 1.7916693687438965,
        "learning_rate": 0.00011050383118373033,
        "epoch": 0.4675324675324675,
        "step": 3636
    },
    {
        "loss": 1.518,
        "grad_norm": 1.8615450859069824,
        "learning_rate": 0.00011046358020962304,
        "epoch": 0.46766105181946765,
        "step": 3637
    },
    {
        "loss": 1.9291,
        "grad_norm": 1.8134965896606445,
        "learning_rate": 0.00011042332752142912,
        "epoch": 0.4677896361064678,
        "step": 3638
    },
    {
        "loss": 1.8908,
        "grad_norm": 2.4992172718048096,
        "learning_rate": 0.00011038307312574253,
        "epoch": 0.4679182203934679,
        "step": 3639
    },
    {
        "loss": 1.5071,
        "grad_norm": 3.034109354019165,
        "learning_rate": 0.00011034281702915751,
        "epoch": 0.46804680468046805,
        "step": 3640
    },
    {
        "loss": 1.6802,
        "grad_norm": 2.1645798683166504,
        "learning_rate": 0.00011030255923826864,
        "epoch": 0.4681753889674682,
        "step": 3641
    },
    {
        "loss": 1.6741,
        "grad_norm": 1.8870508670806885,
        "learning_rate": 0.00011026229975967068,
        "epoch": 0.4683039732544683,
        "step": 3642
    },
    {
        "loss": 1.7047,
        "grad_norm": 2.143872022628784,
        "learning_rate": 0.00011022203859995875,
        "epoch": 0.46843255754146845,
        "step": 3643
    },
    {
        "loss": 1.6339,
        "grad_norm": 1.6866846084594727,
        "learning_rate": 0.00011018177576572823,
        "epoch": 0.4685611418284686,
        "step": 3644
    },
    {
        "loss": 1.4558,
        "grad_norm": 2.60372257232666,
        "learning_rate": 0.0001101415112635747,
        "epoch": 0.46868972611546866,
        "step": 3645
    },
    {
        "loss": 1.6901,
        "grad_norm": 2.336379289627075,
        "learning_rate": 0.00011010124510009412,
        "epoch": 0.4688183104024688,
        "step": 3646
    },
    {
        "loss": 1.6097,
        "grad_norm": 1.5523804426193237,
        "learning_rate": 0.00011006097728188267,
        "epoch": 0.46894689468946893,
        "step": 3647
    },
    {
        "loss": 1.5568,
        "grad_norm": 2.133103370666504,
        "learning_rate": 0.00011002070781553677,
        "epoch": 0.46907547897646906,
        "step": 3648
    },
    {
        "loss": 1.9684,
        "grad_norm": 1.96161687374115,
        "learning_rate": 0.00010998043670765324,
        "epoch": 0.4692040632634692,
        "step": 3649
    },
    {
        "loss": 1.6736,
        "grad_norm": 2.0365824699401855,
        "learning_rate": 0.00010994016396482892,
        "epoch": 0.46933264755046933,
        "step": 3650
    },
    {
        "loss": 1.8575,
        "grad_norm": 1.8128352165222168,
        "learning_rate": 0.00010989988959366117,
        "epoch": 0.46946123183746946,
        "step": 3651
    },
    {
        "loss": 2.0363,
        "grad_norm": 1.6831356287002563,
        "learning_rate": 0.00010985961360074753,
        "epoch": 0.4695898161244696,
        "step": 3652
    },
    {
        "loss": 0.8325,
        "grad_norm": 2.030301332473755,
        "learning_rate": 0.00010981933599268573,
        "epoch": 0.4697184004114697,
        "step": 3653
    },
    {
        "loss": 2.6972,
        "grad_norm": 1.1395632028579712,
        "learning_rate": 0.00010977905677607388,
        "epoch": 0.46984698469846986,
        "step": 3654
    },
    {
        "loss": 2.0766,
        "grad_norm": 3.1811611652374268,
        "learning_rate": 0.00010973877595751028,
        "epoch": 0.46997556898547,
        "step": 3655
    },
    {
        "loss": 1.1425,
        "grad_norm": 1.5556435585021973,
        "learning_rate": 0.00010969849354359352,
        "epoch": 0.4701041532724701,
        "step": 3656
    },
    {
        "loss": 1.3932,
        "grad_norm": 2.5174989700317383,
        "learning_rate": 0.00010965820954092243,
        "epoch": 0.47023273755947026,
        "step": 3657
    },
    {
        "loss": 1.9616,
        "grad_norm": 1.6839699745178223,
        "learning_rate": 0.00010961792395609614,
        "epoch": 0.47036132184647034,
        "step": 3658
    },
    {
        "loss": 2.3098,
        "grad_norm": 1.6549400091171265,
        "learning_rate": 0.000109577636795714,
        "epoch": 0.47048990613347047,
        "step": 3659
    },
    {
        "loss": 1.7262,
        "grad_norm": 2.1024608612060547,
        "learning_rate": 0.00010953734806637565,
        "epoch": 0.4706184904204706,
        "step": 3660
    },
    {
        "loss": 2.3063,
        "grad_norm": 3.7921063899993896,
        "learning_rate": 0.00010949705777468094,
        "epoch": 0.47074707470747074,
        "step": 3661
    },
    {
        "loss": 1.9496,
        "grad_norm": 2.5956907272338867,
        "learning_rate": 0.00010945676592723002,
        "epoch": 0.47087565899447087,
        "step": 3662
    },
    {
        "loss": 1.8362,
        "grad_norm": 1.6719359159469604,
        "learning_rate": 0.00010941647253062328,
        "epoch": 0.471004243281471,
        "step": 3663
    },
    {
        "loss": 1.5442,
        "grad_norm": 1.682316780090332,
        "learning_rate": 0.00010937617759146137,
        "epoch": 0.47113282756847114,
        "step": 3664
    },
    {
        "loss": 1.1552,
        "grad_norm": 1.6961907148361206,
        "learning_rate": 0.0001093358811163452,
        "epoch": 0.47126141185547127,
        "step": 3665
    },
    {
        "loss": 1.2958,
        "grad_norm": 2.6141984462738037,
        "learning_rate": 0.00010929558311187589,
        "epoch": 0.4713899961424714,
        "step": 3666
    },
    {
        "loss": 2.2562,
        "grad_norm": 1.155213475227356,
        "learning_rate": 0.00010925528358465486,
        "epoch": 0.47151858042947153,
        "step": 3667
    },
    {
        "loss": 1.3798,
        "grad_norm": 2.190169095993042,
        "learning_rate": 0.00010921498254128375,
        "epoch": 0.47164716471647167,
        "step": 3668
    },
    {
        "loss": 2.1456,
        "grad_norm": 1.542962908744812,
        "learning_rate": 0.00010917467998836444,
        "epoch": 0.4717757490034718,
        "step": 3669
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.8812875747680664,
        "learning_rate": 0.00010913437593249914,
        "epoch": 0.4719043332904719,
        "step": 3670
    },
    {
        "loss": 1.7604,
        "grad_norm": 2.259687900543213,
        "learning_rate": 0.00010909407038029013,
        "epoch": 0.472032917577472,
        "step": 3671
    },
    {
        "loss": 0.8851,
        "grad_norm": 2.377896785736084,
        "learning_rate": 0.00010905376333834014,
        "epoch": 0.47216150186447214,
        "step": 3672
    },
    {
        "loss": 2.3501,
        "grad_norm": 1.5518581867218018,
        "learning_rate": 0.00010901345481325202,
        "epoch": 0.4722900861514723,
        "step": 3673
    },
    {
        "loss": 1.8081,
        "grad_norm": 1.6293433904647827,
        "learning_rate": 0.00010897314481162888,
        "epoch": 0.4724186704384724,
        "step": 3674
    },
    {
        "loss": 1.4803,
        "grad_norm": 1.9727206230163574,
        "learning_rate": 0.00010893283334007412,
        "epoch": 0.47254725472547254,
        "step": 3675
    },
    {
        "loss": 1.3322,
        "grad_norm": 2.0448429584503174,
        "learning_rate": 0.00010889252040519131,
        "epoch": 0.4726758390124727,
        "step": 3676
    },
    {
        "loss": 1.7483,
        "grad_norm": 1.9679763317108154,
        "learning_rate": 0.0001088522060135843,
        "epoch": 0.4728044232994728,
        "step": 3677
    },
    {
        "loss": 2.073,
        "grad_norm": 1.560913324356079,
        "learning_rate": 0.0001088118901718572,
        "epoch": 0.47293300758647294,
        "step": 3678
    },
    {
        "loss": 1.7667,
        "grad_norm": 2.2062461376190186,
        "learning_rate": 0.00010877157288661428,
        "epoch": 0.4730615918734731,
        "step": 3679
    },
    {
        "loss": 2.072,
        "grad_norm": 2.1814534664154053,
        "learning_rate": 0.00010873125416446015,
        "epoch": 0.4731901761604732,
        "step": 3680
    },
    {
        "loss": 2.0656,
        "grad_norm": 1.8426462411880493,
        "learning_rate": 0.0001086909340119996,
        "epoch": 0.47331876044747334,
        "step": 3681
    },
    {
        "loss": 2.1743,
        "grad_norm": 2.192960500717163,
        "learning_rate": 0.00010865061243583761,
        "epoch": 0.4734473447344735,
        "step": 3682
    },
    {
        "loss": 1.1666,
        "grad_norm": 2.2807929515838623,
        "learning_rate": 0.00010861028944257947,
        "epoch": 0.47357592902147355,
        "step": 3683
    },
    {
        "loss": 2.2492,
        "grad_norm": 2.316711664199829,
        "learning_rate": 0.00010856996503883068,
        "epoch": 0.4737045133084737,
        "step": 3684
    },
    {
        "loss": 2.2079,
        "grad_norm": 1.7581582069396973,
        "learning_rate": 0.00010852963923119695,
        "epoch": 0.4738330975954738,
        "step": 3685
    },
    {
        "loss": 1.5707,
        "grad_norm": 1.2916196584701538,
        "learning_rate": 0.00010848931202628427,
        "epoch": 0.47396168188247395,
        "step": 3686
    },
    {
        "loss": 2.4971,
        "grad_norm": 1.8469003438949585,
        "learning_rate": 0.00010844898343069878,
        "epoch": 0.4740902661694741,
        "step": 3687
    },
    {
        "loss": 1.9734,
        "grad_norm": 3.7270350456237793,
        "learning_rate": 0.00010840865345104689,
        "epoch": 0.4742188504564742,
        "step": 3688
    },
    {
        "loss": 1.3287,
        "grad_norm": 1.5186865329742432,
        "learning_rate": 0.00010836832209393525,
        "epoch": 0.47434743474347435,
        "step": 3689
    },
    {
        "loss": 1.2281,
        "grad_norm": 2.2321746349334717,
        "learning_rate": 0.00010832798936597075,
        "epoch": 0.4744760190304745,
        "step": 3690
    },
    {
        "loss": 2.337,
        "grad_norm": 1.668424129486084,
        "learning_rate": 0.00010828765527376045,
        "epoch": 0.4746046033174746,
        "step": 3691
    },
    {
        "loss": 1.87,
        "grad_norm": 2.113380193710327,
        "learning_rate": 0.00010824731982391165,
        "epoch": 0.47473318760447475,
        "step": 3692
    },
    {
        "loss": 1.7166,
        "grad_norm": 2.485853672027588,
        "learning_rate": 0.0001082069830230319,
        "epoch": 0.4748617718914749,
        "step": 3693
    },
    {
        "loss": 2.1999,
        "grad_norm": 1.4938561916351318,
        "learning_rate": 0.00010816664487772896,
        "epoch": 0.474990356178475,
        "step": 3694
    },
    {
        "loss": 0.8568,
        "grad_norm": 1.761398196220398,
        "learning_rate": 0.00010812630539461078,
        "epoch": 0.4751189404654751,
        "step": 3695
    },
    {
        "loss": 1.1503,
        "grad_norm": 1.8713458776474,
        "learning_rate": 0.00010808596458028559,
        "epoch": 0.4752475247524752,
        "step": 3696
    },
    {
        "loss": 2.1421,
        "grad_norm": 1.232697606086731,
        "learning_rate": 0.00010804562244136179,
        "epoch": 0.47537610903947536,
        "step": 3697
    },
    {
        "loss": 1.4085,
        "grad_norm": 2.292159080505371,
        "learning_rate": 0.00010800527898444796,
        "epoch": 0.4755046933264755,
        "step": 3698
    },
    {
        "loss": 2.3856,
        "grad_norm": 1.0957891941070557,
        "learning_rate": 0.00010796493421615302,
        "epoch": 0.4756332776134756,
        "step": 3699
    },
    {
        "loss": 1.6744,
        "grad_norm": 2.7138915061950684,
        "learning_rate": 0.000107924588143086,
        "epoch": 0.47576186190047576,
        "step": 3700
    },
    {
        "eval_loss": 1.889365315437317,
        "eval_runtime": 28.2685,
        "eval_samples_per_second": 2.795,
        "eval_steps_per_second": 2.795,
        "epoch": 0.47576186190047576,
        "step": 3700
    },
    {
        "loss": 2.0642,
        "grad_norm": 1.5064243078231812,
        "learning_rate": 0.00010788424077185615,
        "epoch": 0.4758904461874759,
        "step": 3701
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.5833394527435303,
        "learning_rate": 0.00010784389210907299,
        "epoch": 0.476019030474476,
        "step": 3702
    },
    {
        "loss": 1.5944,
        "grad_norm": 1.5239133834838867,
        "learning_rate": 0.00010780354216134621,
        "epoch": 0.47614761476147616,
        "step": 3703
    },
    {
        "loss": 2.0483,
        "grad_norm": 2.3465893268585205,
        "learning_rate": 0.00010776319093528571,
        "epoch": 0.4762761990484763,
        "step": 3704
    },
    {
        "loss": 1.4254,
        "grad_norm": 2.3588783740997314,
        "learning_rate": 0.00010772283843750159,
        "epoch": 0.4764047833354764,
        "step": 3705
    },
    {
        "loss": 2.0383,
        "grad_norm": 1.78659188747406,
        "learning_rate": 0.00010768248467460422,
        "epoch": 0.47653336762247656,
        "step": 3706
    },
    {
        "loss": 1.6088,
        "grad_norm": 2.2559125423431396,
        "learning_rate": 0.00010764212965320413,
        "epoch": 0.4766619519094767,
        "step": 3707
    },
    {
        "loss": 1.3354,
        "grad_norm": 2.331780195236206,
        "learning_rate": 0.00010760177337991199,
        "epoch": 0.47679053619647677,
        "step": 3708
    },
    {
        "loss": 1.8914,
        "grad_norm": 1.9523640871047974,
        "learning_rate": 0.00010756141586133884,
        "epoch": 0.4769191204834769,
        "step": 3709
    },
    {
        "loss": 1.9765,
        "grad_norm": 1.727785348892212,
        "learning_rate": 0.00010752105710409575,
        "epoch": 0.47704770477047703,
        "step": 3710
    },
    {
        "loss": 0.6187,
        "grad_norm": 1.598504900932312,
        "learning_rate": 0.0001074806971147941,
        "epoch": 0.47717628905747717,
        "step": 3711
    },
    {
        "loss": 1.1653,
        "grad_norm": 3.414921522140503,
        "learning_rate": 0.00010744033590004549,
        "epoch": 0.4773048733444773,
        "step": 3712
    },
    {
        "loss": 1.4407,
        "grad_norm": 2.0009186267852783,
        "learning_rate": 0.0001073999734664616,
        "epoch": 0.47743345763147743,
        "step": 3713
    },
    {
        "loss": 1.015,
        "grad_norm": 2.085416793823242,
        "learning_rate": 0.00010735960982065442,
        "epoch": 0.47756204191847756,
        "step": 3714
    },
    {
        "loss": 1.7272,
        "grad_norm": 2.091679334640503,
        "learning_rate": 0.0001073192449692361,
        "epoch": 0.4776906262054777,
        "step": 3715
    },
    {
        "loss": 2.0066,
        "grad_norm": 1.6881598234176636,
        "learning_rate": 0.00010727887891881898,
        "epoch": 0.47781921049247783,
        "step": 3716
    },
    {
        "loss": 2.0491,
        "grad_norm": 2.394611358642578,
        "learning_rate": 0.00010723851167601566,
        "epoch": 0.47794779477947796,
        "step": 3717
    },
    {
        "loss": 1.1308,
        "grad_norm": 1.762522578239441,
        "learning_rate": 0.0001071981432474388,
        "epoch": 0.4780763790664781,
        "step": 3718
    },
    {
        "loss": 1.9713,
        "grad_norm": 1.2830673456192017,
        "learning_rate": 0.00010715777363970134,
        "epoch": 0.47820496335347823,
        "step": 3719
    },
    {
        "loss": 1.841,
        "grad_norm": 2.1035375595092773,
        "learning_rate": 0.00010711740285941649,
        "epoch": 0.4783335476404783,
        "step": 3720
    },
    {
        "loss": 2.3385,
        "grad_norm": 1.4471244812011719,
        "learning_rate": 0.00010707703091319749,
        "epoch": 0.47846213192747844,
        "step": 3721
    },
    {
        "loss": 1.6268,
        "grad_norm": 1.9651674032211304,
        "learning_rate": 0.00010703665780765791,
        "epoch": 0.4785907162144786,
        "step": 3722
    },
    {
        "loss": 2.122,
        "grad_norm": 1.699491262435913,
        "learning_rate": 0.00010699628354941143,
        "epoch": 0.4787193005014787,
        "step": 3723
    },
    {
        "loss": 1.7735,
        "grad_norm": 1.8236417770385742,
        "learning_rate": 0.0001069559081450719,
        "epoch": 0.47884788478847884,
        "step": 3724
    },
    {
        "loss": 1.0978,
        "grad_norm": 2.0301239490509033,
        "learning_rate": 0.00010691553160125347,
        "epoch": 0.47897646907547897,
        "step": 3725
    },
    {
        "loss": 1.6194,
        "grad_norm": 2.1749465465545654,
        "learning_rate": 0.00010687515392457035,
        "epoch": 0.4791050533624791,
        "step": 3726
    },
    {
        "loss": 0.9893,
        "grad_norm": 1.2623932361602783,
        "learning_rate": 0.00010683477512163702,
        "epoch": 0.47923363764947924,
        "step": 3727
    },
    {
        "loss": 2.0106,
        "grad_norm": 2.034339427947998,
        "learning_rate": 0.00010679439519906811,
        "epoch": 0.47936222193647937,
        "step": 3728
    },
    {
        "loss": 1.9358,
        "grad_norm": 2.111456871032715,
        "learning_rate": 0.00010675401416347842,
        "epoch": 0.4794908062234795,
        "step": 3729
    },
    {
        "loss": 1.5219,
        "grad_norm": 2.0290279388427734,
        "learning_rate": 0.00010671363202148297,
        "epoch": 0.47961939051047964,
        "step": 3730
    },
    {
        "loss": 1.903,
        "grad_norm": 3.3348538875579834,
        "learning_rate": 0.00010667324877969693,
        "epoch": 0.47974797479747977,
        "step": 3731
    },
    {
        "loss": 1.8339,
        "grad_norm": 2.7069458961486816,
        "learning_rate": 0.0001066328644447357,
        "epoch": 0.4798765590844799,
        "step": 3732
    },
    {
        "loss": 1.2982,
        "grad_norm": 2.3613266944885254,
        "learning_rate": 0.00010659247902321478,
        "epoch": 0.48000514337148,
        "step": 3733
    },
    {
        "loss": 1.4537,
        "grad_norm": 2.0872366428375244,
        "learning_rate": 0.0001065520925217499,
        "epoch": 0.4801337276584801,
        "step": 3734
    },
    {
        "loss": 2.1988,
        "grad_norm": 1.3686636686325073,
        "learning_rate": 0.00010651170494695693,
        "epoch": 0.48026231194548025,
        "step": 3735
    },
    {
        "loss": 1.5289,
        "grad_norm": 2.2121739387512207,
        "learning_rate": 0.000106471316305452,
        "epoch": 0.4803908962324804,
        "step": 3736
    },
    {
        "loss": 1.9404,
        "grad_norm": 2.4173550605773926,
        "learning_rate": 0.0001064309266038513,
        "epoch": 0.4805194805194805,
        "step": 3737
    },
    {
        "loss": 1.7127,
        "grad_norm": 1.8912250995635986,
        "learning_rate": 0.00010639053584877132,
        "epoch": 0.48064806480648065,
        "step": 3738
    },
    {
        "loss": 1.9161,
        "grad_norm": 1.8877573013305664,
        "learning_rate": 0.00010635014404682859,
        "epoch": 0.4807766490934808,
        "step": 3739
    },
    {
        "loss": 2.3076,
        "grad_norm": 1.1094273328781128,
        "learning_rate": 0.00010630975120463989,
        "epoch": 0.4809052333804809,
        "step": 3740
    },
    {
        "loss": 1.2712,
        "grad_norm": 2.621788740158081,
        "learning_rate": 0.00010626935732882217,
        "epoch": 0.48103381766748105,
        "step": 3741
    },
    {
        "loss": 1.8894,
        "grad_norm": 2.278729200363159,
        "learning_rate": 0.0001062289624259925,
        "epoch": 0.4811624019544812,
        "step": 3742
    },
    {
        "loss": 1.2485,
        "grad_norm": 2.1933212280273438,
        "learning_rate": 0.0001061885665027682,
        "epoch": 0.4812909862414813,
        "step": 3743
    },
    {
        "loss": 2.1379,
        "grad_norm": 1.6954171657562256,
        "learning_rate": 0.0001061481695657667,
        "epoch": 0.48141957052848144,
        "step": 3744
    },
    {
        "loss": 2.04,
        "grad_norm": 1.9320168495178223,
        "learning_rate": 0.00010610777162160557,
        "epoch": 0.4815481548154815,
        "step": 3745
    },
    {
        "loss": 2.1833,
        "grad_norm": 1.8078581094741821,
        "learning_rate": 0.00010606737267690263,
        "epoch": 0.48167673910248165,
        "step": 3746
    },
    {
        "loss": 1.6304,
        "grad_norm": 3.244558334350586,
        "learning_rate": 0.00010602697273827576,
        "epoch": 0.4818053233894818,
        "step": 3747
    },
    {
        "loss": 2.4615,
        "grad_norm": 1.62725031375885,
        "learning_rate": 0.0001059865718123431,
        "epoch": 0.4819339076764819,
        "step": 3748
    },
    {
        "loss": 1.3513,
        "grad_norm": 1.8450913429260254,
        "learning_rate": 0.00010594616990572292,
        "epoch": 0.48206249196348205,
        "step": 3749
    },
    {
        "loss": 1.6019,
        "grad_norm": 1.7564722299575806,
        "learning_rate": 0.00010590576702503359,
        "epoch": 0.4821910762504822,
        "step": 3750
    },
    {
        "loss": 2.0303,
        "grad_norm": 1.7593146562576294,
        "learning_rate": 0.0001058653631768937,
        "epoch": 0.4823196605374823,
        "step": 3751
    },
    {
        "loss": 1.781,
        "grad_norm": 1.362044095993042,
        "learning_rate": 0.00010582495836792203,
        "epoch": 0.48244824482448245,
        "step": 3752
    },
    {
        "loss": 1.8327,
        "grad_norm": 2.0582642555236816,
        "learning_rate": 0.00010578455260473746,
        "epoch": 0.4825768291114826,
        "step": 3753
    },
    {
        "loss": 1.3031,
        "grad_norm": 1.2330842018127441,
        "learning_rate": 0.000105744145893959,
        "epoch": 0.4827054133984827,
        "step": 3754
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.6066944599151611,
        "learning_rate": 0.00010570373824220589,
        "epoch": 0.48283399768548285,
        "step": 3755
    },
    {
        "loss": 2.0028,
        "grad_norm": 1.4197136163711548,
        "learning_rate": 0.00010566332965609751,
        "epoch": 0.482962581972483,
        "step": 3756
    },
    {
        "loss": 1.5074,
        "grad_norm": 1.6181575059890747,
        "learning_rate": 0.00010562292014225335,
        "epoch": 0.4830911662594831,
        "step": 3757
    },
    {
        "loss": 2.1709,
        "grad_norm": 1.819890022277832,
        "learning_rate": 0.00010558250970729306,
        "epoch": 0.4832197505464832,
        "step": 3758
    },
    {
        "loss": 1.6656,
        "grad_norm": 1.8552370071411133,
        "learning_rate": 0.00010554209835783651,
        "epoch": 0.48334833483348333,
        "step": 3759
    },
    {
        "loss": 1.1267,
        "grad_norm": 2.582716464996338,
        "learning_rate": 0.00010550168610050359,
        "epoch": 0.48347691912048346,
        "step": 3760
    },
    {
        "loss": 2.1523,
        "grad_norm": 2.0185599327087402,
        "learning_rate": 0.00010546127294191448,
        "epoch": 0.4836055034074836,
        "step": 3761
    },
    {
        "loss": 1.7678,
        "grad_norm": 2.302563428878784,
        "learning_rate": 0.00010542085888868945,
        "epoch": 0.4837340876944837,
        "step": 3762
    },
    {
        "loss": 1.7304,
        "grad_norm": 2.3298895359039307,
        "learning_rate": 0.00010538044394744884,
        "epoch": 0.48386267198148386,
        "step": 3763
    },
    {
        "loss": 1.22,
        "grad_norm": 2.525991678237915,
        "learning_rate": 0.00010534002812481327,
        "epoch": 0.483991256268484,
        "step": 3764
    },
    {
        "loss": 1.784,
        "grad_norm": 1.6608158349990845,
        "learning_rate": 0.00010529961142740341,
        "epoch": 0.4841198405554841,
        "step": 3765
    },
    {
        "loss": 2.5197,
        "grad_norm": 2.3100695610046387,
        "learning_rate": 0.00010525919386184009,
        "epoch": 0.48424842484248426,
        "step": 3766
    },
    {
        "loss": 0.9916,
        "grad_norm": 1.8498618602752686,
        "learning_rate": 0.00010521877543474434,
        "epoch": 0.4843770091294844,
        "step": 3767
    },
    {
        "loss": 1.883,
        "grad_norm": 2.000502824783325,
        "learning_rate": 0.00010517835615273725,
        "epoch": 0.4845055934164845,
        "step": 3768
    },
    {
        "loss": 1.849,
        "grad_norm": 1.846696376800537,
        "learning_rate": 0.00010513793602244013,
        "epoch": 0.48463417770348466,
        "step": 3769
    },
    {
        "loss": 1.8392,
        "grad_norm": 1.7309859991073608,
        "learning_rate": 0.00010509751505047432,
        "epoch": 0.48476276199048474,
        "step": 3770
    },
    {
        "loss": 1.8414,
        "grad_norm": 2.6946637630462646,
        "learning_rate": 0.00010505709324346141,
        "epoch": 0.48489134627748487,
        "step": 3771
    },
    {
        "loss": 2.0211,
        "grad_norm": 1.7653261423110962,
        "learning_rate": 0.00010501667060802305,
        "epoch": 0.485019930564485,
        "step": 3772
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.4890761375427246,
        "learning_rate": 0.00010497624715078108,
        "epoch": 0.48514851485148514,
        "step": 3773
    },
    {
        "loss": 1.9372,
        "grad_norm": 2.3845090866088867,
        "learning_rate": 0.00010493582287835744,
        "epoch": 0.48527709913848527,
        "step": 3774
    },
    {
        "loss": 2.0128,
        "grad_norm": 1.9750694036483765,
        "learning_rate": 0.00010489539779737423,
        "epoch": 0.4854056834254854,
        "step": 3775
    },
    {
        "loss": 0.6649,
        "grad_norm": 2.5904810428619385,
        "learning_rate": 0.00010485497191445363,
        "epoch": 0.48553426771248553,
        "step": 3776
    },
    {
        "loss": 2.0501,
        "grad_norm": 2.164017677307129,
        "learning_rate": 0.00010481454523621803,
        "epoch": 0.48566285199948567,
        "step": 3777
    },
    {
        "loss": 1.2338,
        "grad_norm": 1.3501147031784058,
        "learning_rate": 0.00010477411776928987,
        "epoch": 0.4857914362864858,
        "step": 3778
    },
    {
        "loss": 1.4724,
        "grad_norm": 2.6861612796783447,
        "learning_rate": 0.00010473368952029179,
        "epoch": 0.48592002057348593,
        "step": 3779
    },
    {
        "loss": 1.7324,
        "grad_norm": 1.9402990341186523,
        "learning_rate": 0.00010469326049584651,
        "epoch": 0.48604860486048607,
        "step": 3780
    },
    {
        "loss": 1.6074,
        "grad_norm": 1.6642311811447144,
        "learning_rate": 0.00010465283070257688,
        "epoch": 0.4861771891474862,
        "step": 3781
    },
    {
        "loss": 1.4866,
        "grad_norm": 1.475447416305542,
        "learning_rate": 0.00010461240014710589,
        "epoch": 0.48630577343448633,
        "step": 3782
    },
    {
        "loss": 2.5103,
        "grad_norm": 1.093395709991455,
        "learning_rate": 0.0001045719688360567,
        "epoch": 0.4864343577214864,
        "step": 3783
    },
    {
        "loss": 1.9594,
        "grad_norm": 1.5800141096115112,
        "learning_rate": 0.0001045315367760525,
        "epoch": 0.48656294200848654,
        "step": 3784
    },
    {
        "loss": 1.2254,
        "grad_norm": 2.5743114948272705,
        "learning_rate": 0.00010449110397371664,
        "epoch": 0.4866915262954867,
        "step": 3785
    },
    {
        "loss": 1.2357,
        "grad_norm": 2.395413875579834,
        "learning_rate": 0.00010445067043567263,
        "epoch": 0.4868201105824868,
        "step": 3786
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.189995050430298,
        "learning_rate": 0.00010441023616854406,
        "epoch": 0.48694869486948694,
        "step": 3787
    },
    {
        "loss": 1.468,
        "grad_norm": 2.7191693782806396,
        "learning_rate": 0.00010436980117895466,
        "epoch": 0.4870772791564871,
        "step": 3788
    },
    {
        "loss": 2.1876,
        "grad_norm": 1.1737773418426514,
        "learning_rate": 0.00010432936547352826,
        "epoch": 0.4872058634434872,
        "step": 3789
    },
    {
        "loss": 2.2923,
        "grad_norm": 1.4772287607192993,
        "learning_rate": 0.00010428892905888882,
        "epoch": 0.48733444773048734,
        "step": 3790
    },
    {
        "loss": 1.5825,
        "grad_norm": 2.4823689460754395,
        "learning_rate": 0.00010424849194166042,
        "epoch": 0.4874630320174875,
        "step": 3791
    },
    {
        "loss": 1.9746,
        "grad_norm": 2.4563865661621094,
        "learning_rate": 0.00010420805412846724,
        "epoch": 0.4875916163044876,
        "step": 3792
    },
    {
        "loss": 1.3211,
        "grad_norm": 1.619354009628296,
        "learning_rate": 0.00010416761562593358,
        "epoch": 0.48772020059148774,
        "step": 3793
    },
    {
        "loss": 2.3286,
        "grad_norm": 0.9752840995788574,
        "learning_rate": 0.00010412717644068386,
        "epoch": 0.4878487848784879,
        "step": 3794
    },
    {
        "loss": 1.3227,
        "grad_norm": 1.9830909967422485,
        "learning_rate": 0.0001040867365793426,
        "epoch": 0.48797736916548795,
        "step": 3795
    },
    {
        "loss": 1.5275,
        "grad_norm": 2.059241533279419,
        "learning_rate": 0.00010404629604853447,
        "epoch": 0.4881059534524881,
        "step": 3796
    },
    {
        "loss": 2.3056,
        "grad_norm": 1.714284896850586,
        "learning_rate": 0.00010400585485488415,
        "epoch": 0.4882345377394882,
        "step": 3797
    },
    {
        "loss": 1.6408,
        "grad_norm": 2.671137571334839,
        "learning_rate": 0.00010396541300501656,
        "epoch": 0.48836312202648835,
        "step": 3798
    },
    {
        "loss": 2.3116,
        "grad_norm": 2.166536569595337,
        "learning_rate": 0.0001039249705055566,
        "epoch": 0.4884917063134885,
        "step": 3799
    },
    {
        "loss": 2.1042,
        "grad_norm": 1.6825783252716064,
        "learning_rate": 0.00010388452736312944,
        "epoch": 0.4886202906004886,
        "step": 3800
    },
    {
        "eval_loss": 1.8813878297805786,
        "eval_runtime": 28.3532,
        "eval_samples_per_second": 2.786,
        "eval_steps_per_second": 2.786,
        "epoch": 0.4886202906004886,
        "step": 3800
    },
    {
        "loss": 1.9753,
        "grad_norm": 2.0699002742767334,
        "learning_rate": 0.00010384408358436018,
        "epoch": 0.48874887488748875,
        "step": 3801
    },
    {
        "loss": 1.9406,
        "grad_norm": 2.131080150604248,
        "learning_rate": 0.00010380363917587406,
        "epoch": 0.4888774591744889,
        "step": 3802
    },
    {
        "loss": 1.9986,
        "grad_norm": 3.016956329345703,
        "learning_rate": 0.00010376319414429653,
        "epoch": 0.489006043461489,
        "step": 3803
    },
    {
        "loss": 1.6407,
        "grad_norm": 1.925377368927002,
        "learning_rate": 0.0001037227484962531,
        "epoch": 0.48913462774848915,
        "step": 3804
    },
    {
        "loss": 2.137,
        "grad_norm": 1.860978364944458,
        "learning_rate": 0.0001036823022383693,
        "epoch": 0.4892632120354893,
        "step": 3805
    },
    {
        "loss": 1.809,
        "grad_norm": 1.416720986366272,
        "learning_rate": 0.00010364185537727085,
        "epoch": 0.4893917963224894,
        "step": 3806
    },
    {
        "loss": 2.527,
        "grad_norm": 1.8587416410446167,
        "learning_rate": 0.00010360140791958346,
        "epoch": 0.48952038060948955,
        "step": 3807
    },
    {
        "loss": 1.4975,
        "grad_norm": 2.6607460975646973,
        "learning_rate": 0.00010356095987193311,
        "epoch": 0.4896489648964896,
        "step": 3808
    },
    {
        "loss": 1.4986,
        "grad_norm": 1.3008359670639038,
        "learning_rate": 0.00010352051124094571,
        "epoch": 0.48977754918348976,
        "step": 3809
    },
    {
        "loss": 1.9212,
        "grad_norm": 1.961346983909607,
        "learning_rate": 0.00010348006203324736,
        "epoch": 0.4899061334704899,
        "step": 3810
    },
    {
        "loss": 1.3702,
        "grad_norm": 2.3900771141052246,
        "learning_rate": 0.00010343961225546426,
        "epoch": 0.49003471775749,
        "step": 3811
    },
    {
        "loss": 2.0451,
        "grad_norm": 1.7474623918533325,
        "learning_rate": 0.00010339916191422261,
        "epoch": 0.49016330204449016,
        "step": 3812
    },
    {
        "loss": 1.8707,
        "grad_norm": 2.093672275543213,
        "learning_rate": 0.0001033587110161488,
        "epoch": 0.4902918863314903,
        "step": 3813
    },
    {
        "loss": 1.9819,
        "grad_norm": 1.9114406108856201,
        "learning_rate": 0.00010331825956786929,
        "epoch": 0.4904204706184904,
        "step": 3814
    },
    {
        "loss": 1.2336,
        "grad_norm": 2.09936785697937,
        "learning_rate": 0.00010327780757601056,
        "epoch": 0.49054905490549056,
        "step": 3815
    },
    {
        "loss": 1.5838,
        "grad_norm": 2.269918918609619,
        "learning_rate": 0.0001032373550471993,
        "epoch": 0.4906776391924907,
        "step": 3816
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.6500861644744873,
        "learning_rate": 0.00010319690198806218,
        "epoch": 0.4908062234794908,
        "step": 3817
    },
    {
        "loss": 1.7463,
        "grad_norm": 2.3002753257751465,
        "learning_rate": 0.00010315644840522601,
        "epoch": 0.49093480776649095,
        "step": 3818
    },
    {
        "loss": 1.0412,
        "grad_norm": 2.146871328353882,
        "learning_rate": 0.00010311599430531769,
        "epoch": 0.4910633920534911,
        "step": 3819
    },
    {
        "loss": 1.9564,
        "grad_norm": 1.6376349925994873,
        "learning_rate": 0.00010307553969496415,
        "epoch": 0.49119197634049117,
        "step": 3820
    },
    {
        "loss": 1.3738,
        "grad_norm": 1.9333326816558838,
        "learning_rate": 0.00010303508458079249,
        "epoch": 0.4913205606274913,
        "step": 3821
    },
    {
        "loss": 2.3652,
        "grad_norm": 2.2838845252990723,
        "learning_rate": 0.00010299462896942984,
        "epoch": 0.49144914491449143,
        "step": 3822
    },
    {
        "loss": 1.1421,
        "grad_norm": 2.25862979888916,
        "learning_rate": 0.00010295417286750337,
        "epoch": 0.49157772920149156,
        "step": 3823
    },
    {
        "loss": 2.5995,
        "grad_norm": 1.3424633741378784,
        "learning_rate": 0.00010291371628164044,
        "epoch": 0.4917063134884917,
        "step": 3824
    },
    {
        "loss": 1.419,
        "grad_norm": 2.012073040008545,
        "learning_rate": 0.00010287325921846838,
        "epoch": 0.49183489777549183,
        "step": 3825
    },
    {
        "loss": 2.2529,
        "grad_norm": 1.3913880586624146,
        "learning_rate": 0.00010283280168461468,
        "epoch": 0.49196348206249196,
        "step": 3826
    },
    {
        "loss": 1.397,
        "grad_norm": 2.286022663116455,
        "learning_rate": 0.00010279234368670688,
        "epoch": 0.4920920663494921,
        "step": 3827
    },
    {
        "loss": 2.5404,
        "grad_norm": 1.2135332822799683,
        "learning_rate": 0.00010275188523137253,
        "epoch": 0.49222065063649223,
        "step": 3828
    },
    {
        "loss": 1.9131,
        "grad_norm": 1.8229272365570068,
        "learning_rate": 0.00010271142632523935,
        "epoch": 0.49234923492349236,
        "step": 3829
    },
    {
        "loss": 1.7166,
        "grad_norm": 2.5081279277801514,
        "learning_rate": 0.00010267096697493512,
        "epoch": 0.4924778192104925,
        "step": 3830
    },
    {
        "loss": 1.498,
        "grad_norm": 1.4397543668746948,
        "learning_rate": 0.00010263050718708764,
        "epoch": 0.49260640349749263,
        "step": 3831
    },
    {
        "loss": 1.7609,
        "grad_norm": 2.3727002143859863,
        "learning_rate": 0.00010259004696832481,
        "epoch": 0.49273498778449276,
        "step": 3832
    },
    {
        "loss": 1.6666,
        "grad_norm": 2.185351610183716,
        "learning_rate": 0.00010254958632527462,
        "epoch": 0.49286357207149284,
        "step": 3833
    },
    {
        "loss": 1.9448,
        "grad_norm": 1.776691198348999,
        "learning_rate": 0.00010250912526456508,
        "epoch": 0.49299215635849297,
        "step": 3834
    },
    {
        "loss": 1.5613,
        "grad_norm": 2.067594289779663,
        "learning_rate": 0.00010246866379282439,
        "epoch": 0.4931207406454931,
        "step": 3835
    },
    {
        "loss": 2.0875,
        "grad_norm": 1.6204266548156738,
        "learning_rate": 0.00010242820191668059,
        "epoch": 0.49324932493249324,
        "step": 3836
    },
    {
        "loss": 2.0881,
        "grad_norm": 2.350569486618042,
        "learning_rate": 0.00010238773964276203,
        "epoch": 0.49337790921949337,
        "step": 3837
    },
    {
        "loss": 1.9801,
        "grad_norm": 2.685748815536499,
        "learning_rate": 0.00010234727697769699,
        "epoch": 0.4935064935064935,
        "step": 3838
    },
    {
        "loss": 2.5473,
        "grad_norm": 1.6758379936218262,
        "learning_rate": 0.00010230681392811381,
        "epoch": 0.49363507779349364,
        "step": 3839
    },
    {
        "loss": 2.1854,
        "grad_norm": 1.8043251037597656,
        "learning_rate": 0.00010226635050064098,
        "epoch": 0.49376366208049377,
        "step": 3840
    },
    {
        "loss": 2.0848,
        "grad_norm": 1.53339684009552,
        "learning_rate": 0.00010222588670190693,
        "epoch": 0.4938922463674939,
        "step": 3841
    },
    {
        "loss": 1.0041,
        "grad_norm": 2.670769214630127,
        "learning_rate": 0.00010218542253854031,
        "epoch": 0.49402083065449404,
        "step": 3842
    },
    {
        "loss": 2.1199,
        "grad_norm": 1.8455853462219238,
        "learning_rate": 0.00010214495801716968,
        "epoch": 0.49414941494149417,
        "step": 3843
    },
    {
        "loss": 1.835,
        "grad_norm": 1.9974567890167236,
        "learning_rate": 0.00010210449314442369,
        "epoch": 0.4942779992284943,
        "step": 3844
    },
    {
        "loss": 2.2396,
        "grad_norm": 2.060044050216675,
        "learning_rate": 0.00010206402792693114,
        "epoch": 0.4944065835154944,
        "step": 3845
    },
    {
        "loss": 1.1994,
        "grad_norm": 1.9309520721435547,
        "learning_rate": 0.0001020235623713208,
        "epoch": 0.4945351678024945,
        "step": 3846
    },
    {
        "loss": 2.1276,
        "grad_norm": 2.172152042388916,
        "learning_rate": 0.00010198309648422151,
        "epoch": 0.49466375208949465,
        "step": 3847
    },
    {
        "loss": 2.1976,
        "grad_norm": 2.2478647232055664,
        "learning_rate": 0.00010194263027226216,
        "epoch": 0.4947923363764948,
        "step": 3848
    },
    {
        "loss": 2.0522,
        "grad_norm": 1.1768944263458252,
        "learning_rate": 0.00010190216374207169,
        "epoch": 0.4949209206634949,
        "step": 3849
    },
    {
        "loss": 1.9152,
        "grad_norm": 2.0875766277313232,
        "learning_rate": 0.00010186169690027914,
        "epoch": 0.49504950495049505,
        "step": 3850
    },
    {
        "loss": 0.977,
        "grad_norm": 1.6888662576675415,
        "learning_rate": 0.00010182122975351358,
        "epoch": 0.4951780892374952,
        "step": 3851
    },
    {
        "loss": 1.8258,
        "grad_norm": 3.1009366512298584,
        "learning_rate": 0.0001017807623084041,
        "epoch": 0.4953066735244953,
        "step": 3852
    },
    {
        "loss": 2.0893,
        "grad_norm": 1.0488840341567993,
        "learning_rate": 0.00010174029457157983,
        "epoch": 0.49543525781149544,
        "step": 3853
    },
    {
        "loss": 1.6389,
        "grad_norm": 2.0615737438201904,
        "learning_rate": 0.00010169982654966998,
        "epoch": 0.4955638420984956,
        "step": 3854
    },
    {
        "loss": 1.6736,
        "grad_norm": 1.5579712390899658,
        "learning_rate": 0.00010165935824930384,
        "epoch": 0.4956924263854957,
        "step": 3855
    },
    {
        "loss": 1.3584,
        "grad_norm": 1.7823505401611328,
        "learning_rate": 0.00010161888967711068,
        "epoch": 0.49582101067249584,
        "step": 3856
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.6832367181777954,
        "learning_rate": 0.00010157842083971985,
        "epoch": 0.495949594959496,
        "step": 3857
    },
    {
        "loss": 2.2605,
        "grad_norm": 2.0175070762634277,
        "learning_rate": 0.0001015379517437607,
        "epoch": 0.49607817924649605,
        "step": 3858
    },
    {
        "loss": 2.032,
        "grad_norm": 1.7203519344329834,
        "learning_rate": 0.0001014974823958627,
        "epoch": 0.4962067635334962,
        "step": 3859
    },
    {
        "loss": 1.8799,
        "grad_norm": 2.688906669616699,
        "learning_rate": 0.0001014570128026553,
        "epoch": 0.4963353478204963,
        "step": 3860
    },
    {
        "loss": 2.0045,
        "grad_norm": 1.922344446182251,
        "learning_rate": 0.00010141654297076802,
        "epoch": 0.49646393210749645,
        "step": 3861
    },
    {
        "loss": 1.9818,
        "grad_norm": 1.5529077053070068,
        "learning_rate": 0.00010137607290683035,
        "epoch": 0.4965925163944966,
        "step": 3862
    },
    {
        "loss": 2.2274,
        "grad_norm": 1.7395343780517578,
        "learning_rate": 0.00010133560261747197,
        "epoch": 0.4967211006814967,
        "step": 3863
    },
    {
        "loss": 1.4157,
        "grad_norm": 2.0628504753112793,
        "learning_rate": 0.00010129513210932246,
        "epoch": 0.49684968496849685,
        "step": 3864
    },
    {
        "loss": 2.2271,
        "grad_norm": 1.557981014251709,
        "learning_rate": 0.00010125466138901143,
        "epoch": 0.496978269255497,
        "step": 3865
    },
    {
        "loss": 2.216,
        "grad_norm": 2.07739520072937,
        "learning_rate": 0.00010121419046316864,
        "epoch": 0.4971068535424971,
        "step": 3866
    },
    {
        "loss": 1.8787,
        "grad_norm": 1.9316246509552002,
        "learning_rate": 0.00010117371933842379,
        "epoch": 0.49723543782949725,
        "step": 3867
    },
    {
        "loss": 1.6139,
        "grad_norm": 1.6688852310180664,
        "learning_rate": 0.00010113324802140662,
        "epoch": 0.4973640221164974,
        "step": 3868
    },
    {
        "loss": 1.3062,
        "grad_norm": 2.015810012817383,
        "learning_rate": 0.00010109277651874698,
        "epoch": 0.4974926064034975,
        "step": 3869
    },
    {
        "loss": 1.5035,
        "grad_norm": 2.5989015102386475,
        "learning_rate": 0.00010105230483707462,
        "epoch": 0.4976211906904976,
        "step": 3870
    },
    {
        "loss": 1.9688,
        "grad_norm": 1.9906429052352905,
        "learning_rate": 0.00010101183298301939,
        "epoch": 0.4977497749774977,
        "step": 3871
    },
    {
        "loss": 1.6274,
        "grad_norm": 2.273832321166992,
        "learning_rate": 0.00010097136096321127,
        "epoch": 0.49787835926449786,
        "step": 3872
    },
    {
        "loss": 1.78,
        "grad_norm": 1.7476258277893066,
        "learning_rate": 0.00010093088878428006,
        "epoch": 0.498006943551498,
        "step": 3873
    },
    {
        "loss": 2.2611,
        "grad_norm": 1.7192326784133911,
        "learning_rate": 0.00010089041645285573,
        "epoch": 0.4981355278384981,
        "step": 3874
    },
    {
        "loss": 1.0278,
        "grad_norm": 2.30678129196167,
        "learning_rate": 0.00010084994397556821,
        "epoch": 0.49826411212549826,
        "step": 3875
    },
    {
        "loss": 2.1767,
        "grad_norm": 1.7696815729141235,
        "learning_rate": 0.00010080947135904753,
        "epoch": 0.4983926964124984,
        "step": 3876
    },
    {
        "loss": 1.3852,
        "grad_norm": 2.5475447177886963,
        "learning_rate": 0.00010076899860992366,
        "epoch": 0.4985212806994985,
        "step": 3877
    },
    {
        "loss": 2.0883,
        "grad_norm": 1.695732831954956,
        "learning_rate": 0.00010072852573482665,
        "epoch": 0.49864986498649866,
        "step": 3878
    },
    {
        "loss": 1.3263,
        "grad_norm": 2.699216365814209,
        "learning_rate": 0.00010068805274038649,
        "epoch": 0.4987784492734988,
        "step": 3879
    },
    {
        "loss": 1.9805,
        "grad_norm": 1.9811362028121948,
        "learning_rate": 0.0001006475796332333,
        "epoch": 0.4989070335604989,
        "step": 3880
    },
    {
        "loss": 1.5548,
        "grad_norm": 1.6073309183120728,
        "learning_rate": 0.00010060710641999716,
        "epoch": 0.49903561784749906,
        "step": 3881
    },
    {
        "loss": 2.0293,
        "grad_norm": 1.8144594430923462,
        "learning_rate": 0.00010056663310730814,
        "epoch": 0.4991642021344992,
        "step": 3882
    },
    {
        "loss": 2.0671,
        "grad_norm": 1.4495059251785278,
        "learning_rate": 0.00010052615970179639,
        "epoch": 0.49929278642149927,
        "step": 3883
    },
    {
        "loss": 1.6482,
        "grad_norm": 2.0664427280426025,
        "learning_rate": 0.000100485686210092,
        "epoch": 0.4994213707084994,
        "step": 3884
    },
    {
        "loss": 1.6861,
        "grad_norm": 1.55879807472229,
        "learning_rate": 0.00010044521263882519,
        "epoch": 0.49954995499549953,
        "step": 3885
    },
    {
        "loss": 1.3821,
        "grad_norm": 1.8673161268234253,
        "learning_rate": 0.00010040473899462601,
        "epoch": 0.49967853928249967,
        "step": 3886
    },
    {
        "loss": 1.8533,
        "grad_norm": 1.5844273567199707,
        "learning_rate": 0.00010036426528412475,
        "epoch": 0.4998071235694998,
        "step": 3887
    },
    {
        "loss": 1.9637,
        "grad_norm": 1.654369831085205,
        "learning_rate": 0.0001003237915139515,
        "epoch": 0.49993570785649993,
        "step": 3888
    },
    {
        "loss": 1.6502,
        "grad_norm": 1.8068807125091553,
        "learning_rate": 0.00010028331769073654,
        "epoch": 0.5000642921435,
        "step": 3889
    },
    {
        "loss": 2.2093,
        "grad_norm": 1.6746608018875122,
        "learning_rate": 0.00010024284382110997,
        "epoch": 0.5001928764305001,
        "step": 3890
    },
    {
        "loss": 1.6519,
        "grad_norm": 2.668295383453369,
        "learning_rate": 0.00010020236991170206,
        "epoch": 0.5003214607175003,
        "step": 3891
    },
    {
        "loss": 1.4713,
        "grad_norm": 2.0433413982391357,
        "learning_rate": 0.00010016189596914305,
        "epoch": 0.5004500450045004,
        "step": 3892
    },
    {
        "loss": 2.2169,
        "grad_norm": 2.555978536605835,
        "learning_rate": 0.00010012142200006307,
        "epoch": 0.5005786292915005,
        "step": 3893
    },
    {
        "loss": 1.2467,
        "grad_norm": 1.862240195274353,
        "learning_rate": 0.00010008094801109244,
        "epoch": 0.5007072135785007,
        "step": 3894
    },
    {
        "loss": 1.8477,
        "grad_norm": 2.6698901653289795,
        "learning_rate": 0.00010004047400886136,
        "epoch": 0.5008357978655008,
        "step": 3895
    },
    {
        "loss": 1.8975,
        "grad_norm": 2.3928704261779785,
        "learning_rate": 0.0001,
        "epoch": 0.5009643821525009,
        "step": 3896
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.9979759454727173,
        "learning_rate": 9.995952599113869e-05,
        "epoch": 0.5010929664395011,
        "step": 3897
    },
    {
        "loss": 2.1585,
        "grad_norm": 1.9206528663635254,
        "learning_rate": 9.991905198890758e-05,
        "epoch": 0.5012215507265012,
        "step": 3898
    },
    {
        "loss": 1.8633,
        "grad_norm": 1.9513050317764282,
        "learning_rate": 9.987857799993694e-05,
        "epoch": 0.5013501350135013,
        "step": 3899
    },
    {
        "loss": 2.1923,
        "grad_norm": 1.07079017162323,
        "learning_rate": 9.983810403085698e-05,
        "epoch": 0.5014787193005015,
        "step": 3900
    },
    {
        "eval_loss": 1.857143759727478,
        "eval_runtime": 28.2889,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.5014787193005015,
        "step": 3900
    },
    {
        "loss": 1.628,
        "grad_norm": 1.8361743688583374,
        "learning_rate": 9.979763008829795e-05,
        "epoch": 0.5016073035875016,
        "step": 3901
    },
    {
        "loss": 1.0029,
        "grad_norm": 2.779827117919922,
        "learning_rate": 9.975715617889007e-05,
        "epoch": 0.5017358878745017,
        "step": 3902
    },
    {
        "loss": 1.8935,
        "grad_norm": 1.9789975881576538,
        "learning_rate": 9.97166823092635e-05,
        "epoch": 0.5018644721615019,
        "step": 3903
    },
    {
        "loss": 2.1389,
        "grad_norm": 1.0788850784301758,
        "learning_rate": 9.967620848604852e-05,
        "epoch": 0.501993056448502,
        "step": 3904
    },
    {
        "loss": 2.4824,
        "grad_norm": 1.2080585956573486,
        "learning_rate": 9.963573471587527e-05,
        "epoch": 0.5021216407355021,
        "step": 3905
    },
    {
        "loss": 1.6069,
        "grad_norm": 2.341977834701538,
        "learning_rate": 9.9595261005374e-05,
        "epoch": 0.5022502250225023,
        "step": 3906
    },
    {
        "loss": 1.536,
        "grad_norm": 1.8857073783874512,
        "learning_rate": 9.955478736117486e-05,
        "epoch": 0.5023788093095024,
        "step": 3907
    },
    {
        "loss": 2.138,
        "grad_norm": 2.5629122257232666,
        "learning_rate": 9.951431378990802e-05,
        "epoch": 0.5025073935965025,
        "step": 3908
    },
    {
        "loss": 1.6927,
        "grad_norm": 1.7338662147521973,
        "learning_rate": 9.947384029820366e-05,
        "epoch": 0.5026359778835027,
        "step": 3909
    },
    {
        "loss": 1.4825,
        "grad_norm": 1.8353862762451172,
        "learning_rate": 9.943336689269185e-05,
        "epoch": 0.5027645621705028,
        "step": 3910
    },
    {
        "loss": 2.0202,
        "grad_norm": 3.3170080184936523,
        "learning_rate": 9.939289358000288e-05,
        "epoch": 0.5028931464575029,
        "step": 3911
    },
    {
        "loss": 1.7089,
        "grad_norm": 2.2130796909332275,
        "learning_rate": 9.935242036676673e-05,
        "epoch": 0.5030217307445031,
        "step": 3912
    },
    {
        "loss": 1.753,
        "grad_norm": 1.9698477983474731,
        "learning_rate": 9.931194725961353e-05,
        "epoch": 0.5031503150315032,
        "step": 3913
    },
    {
        "loss": 1.9857,
        "grad_norm": 1.7150942087173462,
        "learning_rate": 9.92714742651734e-05,
        "epoch": 0.5032788993185032,
        "step": 3914
    },
    {
        "loss": 2.1303,
        "grad_norm": 1.608214020729065,
        "learning_rate": 9.923100139007634e-05,
        "epoch": 0.5034074836055034,
        "step": 3915
    },
    {
        "loss": 2.4097,
        "grad_norm": 1.3200161457061768,
        "learning_rate": 9.91905286409525e-05,
        "epoch": 0.5035360678925035,
        "step": 3916
    },
    {
        "loss": 1.4301,
        "grad_norm": 1.5454462766647339,
        "learning_rate": 9.915005602443182e-05,
        "epoch": 0.5036646521795036,
        "step": 3917
    },
    {
        "loss": 1.1333,
        "grad_norm": 2.0878520011901855,
        "learning_rate": 9.91095835471443e-05,
        "epoch": 0.5037932364665038,
        "step": 3918
    },
    {
        "loss": 1.7778,
        "grad_norm": 2.2722208499908447,
        "learning_rate": 9.906911121571999e-05,
        "epoch": 0.5039218207535039,
        "step": 3919
    },
    {
        "loss": 1.7302,
        "grad_norm": 1.719774603843689,
        "learning_rate": 9.902863903678874e-05,
        "epoch": 0.504050405040504,
        "step": 3920
    },
    {
        "loss": 1.6053,
        "grad_norm": 2.0541248321533203,
        "learning_rate": 9.898816701698062e-05,
        "epoch": 0.5041789893275042,
        "step": 3921
    },
    {
        "loss": 1.585,
        "grad_norm": 1.7706958055496216,
        "learning_rate": 9.894769516292539e-05,
        "epoch": 0.5043075736145043,
        "step": 3922
    },
    {
        "loss": 1.1417,
        "grad_norm": 3.8215830326080322,
        "learning_rate": 9.890722348125304e-05,
        "epoch": 0.5044361579015044,
        "step": 3923
    },
    {
        "loss": 1.7625,
        "grad_norm": 1.5440740585327148,
        "learning_rate": 9.88667519785934e-05,
        "epoch": 0.5045647421885046,
        "step": 3924
    },
    {
        "loss": 1.1944,
        "grad_norm": 2.0105555057525635,
        "learning_rate": 9.882628066157622e-05,
        "epoch": 0.5046933264755047,
        "step": 3925
    },
    {
        "loss": 1.469,
        "grad_norm": 1.3299939632415771,
        "learning_rate": 9.878580953683138e-05,
        "epoch": 0.5048219107625048,
        "step": 3926
    },
    {
        "loss": 2.195,
        "grad_norm": 1.6472474336624146,
        "learning_rate": 9.874533861098856e-05,
        "epoch": 0.504950495049505,
        "step": 3927
    },
    {
        "loss": 2.5754,
        "grad_norm": 1.2787678241729736,
        "learning_rate": 9.870486789067756e-05,
        "epoch": 0.5050790793365051,
        "step": 3928
    },
    {
        "loss": 1.6859,
        "grad_norm": 2.432079315185547,
        "learning_rate": 9.866439738252805e-05,
        "epoch": 0.5052076636235052,
        "step": 3929
    },
    {
        "loss": 1.7058,
        "grad_norm": 3.058290719985962,
        "learning_rate": 9.862392709316964e-05,
        "epoch": 0.5053362479105054,
        "step": 3930
    },
    {
        "loss": 1.4997,
        "grad_norm": 1.7465206384658813,
        "learning_rate": 9.858345702923202e-05,
        "epoch": 0.5054648321975055,
        "step": 3931
    },
    {
        "loss": 1.8632,
        "grad_norm": 1.2516826391220093,
        "learning_rate": 9.854298719734471e-05,
        "epoch": 0.5055934164845056,
        "step": 3932
    },
    {
        "loss": 1.2369,
        "grad_norm": 2.3754618167877197,
        "learning_rate": 9.850251760413731e-05,
        "epoch": 0.5057220007715058,
        "step": 3933
    },
    {
        "loss": 1.5743,
        "grad_norm": 2.2583677768707275,
        "learning_rate": 9.846204825623933e-05,
        "epoch": 0.5058505850585059,
        "step": 3934
    },
    {
        "loss": 1.457,
        "grad_norm": 1.8168867826461792,
        "learning_rate": 9.842157916028017e-05,
        "epoch": 0.505979169345506,
        "step": 3935
    },
    {
        "loss": 1.898,
        "grad_norm": 2.0622026920318604,
        "learning_rate": 9.838111032288935e-05,
        "epoch": 0.5061077536325062,
        "step": 3936
    },
    {
        "loss": 2.2785,
        "grad_norm": 2.2584645748138428,
        "learning_rate": 9.834064175069617e-05,
        "epoch": 0.5062363379195063,
        "step": 3937
    },
    {
        "loss": 2.1096,
        "grad_norm": 1.8375705480575562,
        "learning_rate": 9.830017345033003e-05,
        "epoch": 0.5063649222065064,
        "step": 3938
    },
    {
        "loss": 2.1581,
        "grad_norm": 2.6933562755584717,
        "learning_rate": 9.825970542842021e-05,
        "epoch": 0.5064935064935064,
        "step": 3939
    },
    {
        "loss": 1.4505,
        "grad_norm": 1.6929302215576172,
        "learning_rate": 9.821923769159593e-05,
        "epoch": 0.5066220907805066,
        "step": 3940
    },
    {
        "loss": 2.259,
        "grad_norm": 1.9121195077896118,
        "learning_rate": 9.817877024648645e-05,
        "epoch": 0.5067506750675067,
        "step": 3941
    },
    {
        "loss": 1.891,
        "grad_norm": 2.254690647125244,
        "learning_rate": 9.813830309972085e-05,
        "epoch": 0.5068792593545068,
        "step": 3942
    },
    {
        "loss": 2.0395,
        "grad_norm": 2.981796979904175,
        "learning_rate": 9.809783625792832e-05,
        "epoch": 0.507007843641507,
        "step": 3943
    },
    {
        "loss": 1.959,
        "grad_norm": 1.9273213148117065,
        "learning_rate": 9.805736972773789e-05,
        "epoch": 0.5071364279285071,
        "step": 3944
    },
    {
        "loss": 1.7313,
        "grad_norm": 2.45093035697937,
        "learning_rate": 9.801690351577853e-05,
        "epoch": 0.5072650122155072,
        "step": 3945
    },
    {
        "loss": 1.5724,
        "grad_norm": 2.171623706817627,
        "learning_rate": 9.797643762867922e-05,
        "epoch": 0.5073935965025074,
        "step": 3946
    },
    {
        "loss": 1.6525,
        "grad_norm": 2.1961910724639893,
        "learning_rate": 9.793597207306887e-05,
        "epoch": 0.5075221807895075,
        "step": 3947
    },
    {
        "loss": 2.1213,
        "grad_norm": 2.1314985752105713,
        "learning_rate": 9.789550685557632e-05,
        "epoch": 0.5076507650765076,
        "step": 3948
    },
    {
        "loss": 1.6808,
        "grad_norm": 2.3313992023468018,
        "learning_rate": 9.785504198283037e-05,
        "epoch": 0.5077793493635078,
        "step": 3949
    },
    {
        "loss": 1.4116,
        "grad_norm": 1.9562313556671143,
        "learning_rate": 9.781457746145971e-05,
        "epoch": 0.5079079336505079,
        "step": 3950
    },
    {
        "loss": 1.9286,
        "grad_norm": 1.8596104383468628,
        "learning_rate": 9.777411329809308e-05,
        "epoch": 0.508036517937508,
        "step": 3951
    },
    {
        "loss": 1.9618,
        "grad_norm": 1.8179494142532349,
        "learning_rate": 9.773364949935905e-05,
        "epoch": 0.5081651022245082,
        "step": 3952
    },
    {
        "loss": 1.6786,
        "grad_norm": 1.9735153913497925,
        "learning_rate": 9.769318607188621e-05,
        "epoch": 0.5082936865115083,
        "step": 3953
    },
    {
        "loss": 1.8551,
        "grad_norm": 1.7711889743804932,
        "learning_rate": 9.765272302230306e-05,
        "epoch": 0.5084222707985084,
        "step": 3954
    },
    {
        "loss": 1.0504,
        "grad_norm": 2.651977062225342,
        "learning_rate": 9.761226035723799e-05,
        "epoch": 0.5085508550855086,
        "step": 3955
    },
    {
        "loss": 2.0604,
        "grad_norm": 1.7484450340270996,
        "learning_rate": 9.757179808331943e-05,
        "epoch": 0.5086794393725087,
        "step": 3956
    },
    {
        "loss": 1.6218,
        "grad_norm": 2.3231546878814697,
        "learning_rate": 9.753133620717562e-05,
        "epoch": 0.5088080236595088,
        "step": 3957
    },
    {
        "loss": 1.7315,
        "grad_norm": 2.2190933227539062,
        "learning_rate": 9.749087473543493e-05,
        "epoch": 0.508936607946509,
        "step": 3958
    },
    {
        "loss": 1.6673,
        "grad_norm": 1.6008572578430176,
        "learning_rate": 9.745041367472541e-05,
        "epoch": 0.5090651922335091,
        "step": 3959
    },
    {
        "loss": 0.8802,
        "grad_norm": 2.3736214637756348,
        "learning_rate": 9.740995303167521e-05,
        "epoch": 0.5091937765205092,
        "step": 3960
    },
    {
        "loss": 1.4789,
        "grad_norm": 2.061735153198242,
        "learning_rate": 9.73694928129124e-05,
        "epoch": 0.5093223608075094,
        "step": 3961
    },
    {
        "loss": 2.0046,
        "grad_norm": 2.598153591156006,
        "learning_rate": 9.732903302506489e-05,
        "epoch": 0.5094509450945095,
        "step": 3962
    },
    {
        "loss": 1.4579,
        "grad_norm": 1.4498118162155151,
        "learning_rate": 9.728857367476067e-05,
        "epoch": 0.5095795293815096,
        "step": 3963
    },
    {
        "loss": 2.2295,
        "grad_norm": 1.5292211771011353,
        "learning_rate": 9.724811476862752e-05,
        "epoch": 0.5097081136685097,
        "step": 3964
    },
    {
        "loss": 2.0053,
        "grad_norm": 1.9234448671340942,
        "learning_rate": 9.720765631329313e-05,
        "epoch": 0.5098366979555098,
        "step": 3965
    },
    {
        "loss": 1.9892,
        "grad_norm": 1.347344160079956,
        "learning_rate": 9.716719831538535e-05,
        "epoch": 0.5099652822425099,
        "step": 3966
    },
    {
        "loss": 1.4901,
        "grad_norm": 2.2822351455688477,
        "learning_rate": 9.712674078153161e-05,
        "epoch": 0.51009386652951,
        "step": 3967
    },
    {
        "loss": 2.0505,
        "grad_norm": 1.4996693134307861,
        "learning_rate": 9.708628371835957e-05,
        "epoch": 0.5102224508165102,
        "step": 3968
    },
    {
        "loss": 2.0845,
        "grad_norm": 2.0565755367279053,
        "learning_rate": 9.704582713249667e-05,
        "epoch": 0.5103510351035103,
        "step": 3969
    },
    {
        "loss": 1.8832,
        "grad_norm": 1.9503071308135986,
        "learning_rate": 9.70053710305702e-05,
        "epoch": 0.5104796193905105,
        "step": 3970
    },
    {
        "loss": 1.9166,
        "grad_norm": 2.4776954650878906,
        "learning_rate": 9.696491541920756e-05,
        "epoch": 0.5106082036775106,
        "step": 3971
    },
    {
        "loss": 1.8991,
        "grad_norm": 1.5016883611679077,
        "learning_rate": 9.692446030503586e-05,
        "epoch": 0.5107367879645107,
        "step": 3972
    },
    {
        "loss": 2.363,
        "grad_norm": 1.8077969551086426,
        "learning_rate": 9.688400569468235e-05,
        "epoch": 0.5108653722515109,
        "step": 3973
    },
    {
        "loss": 1.6146,
        "grad_norm": 2.236154079437256,
        "learning_rate": 9.6843551594774e-05,
        "epoch": 0.510993956538511,
        "step": 3974
    },
    {
        "loss": 2.1022,
        "grad_norm": 2.598515272140503,
        "learning_rate": 9.680309801193784e-05,
        "epoch": 0.5111225408255111,
        "step": 3975
    },
    {
        "loss": 1.5959,
        "grad_norm": 2.498796224594116,
        "learning_rate": 9.676264495280073e-05,
        "epoch": 0.5112511251125113,
        "step": 3976
    },
    {
        "loss": 2.0137,
        "grad_norm": 3.41084885597229,
        "learning_rate": 9.672219242398945e-05,
        "epoch": 0.5113797093995114,
        "step": 3977
    },
    {
        "loss": 1.7657,
        "grad_norm": 1.622507095336914,
        "learning_rate": 9.668174043213075e-05,
        "epoch": 0.5115082936865115,
        "step": 3978
    },
    {
        "loss": 1.9277,
        "grad_norm": 1.9705655574798584,
        "learning_rate": 9.66412889838512e-05,
        "epoch": 0.5116368779735117,
        "step": 3979
    },
    {
        "loss": 2.2287,
        "grad_norm": 1.5985958576202393,
        "learning_rate": 9.66008380857774e-05,
        "epoch": 0.5117654622605118,
        "step": 3980
    },
    {
        "loss": 2.3001,
        "grad_norm": 2.2038519382476807,
        "learning_rate": 9.656038774453578e-05,
        "epoch": 0.5118940465475119,
        "step": 3981
    },
    {
        "loss": 2.1082,
        "grad_norm": 2.0637459754943848,
        "learning_rate": 9.651993796675264e-05,
        "epoch": 0.512022630834512,
        "step": 3982
    },
    {
        "loss": 0.6784,
        "grad_norm": 2.15997314453125,
        "learning_rate": 9.64794887590543e-05,
        "epoch": 0.5121512151215122,
        "step": 3983
    },
    {
        "loss": 1.7444,
        "grad_norm": 2.161693572998047,
        "learning_rate": 9.64390401280669e-05,
        "epoch": 0.5122797994085123,
        "step": 3984
    },
    {
        "loss": 1.6773,
        "grad_norm": 2.4625236988067627,
        "learning_rate": 9.639859208041655e-05,
        "epoch": 0.5124083836955124,
        "step": 3985
    },
    {
        "loss": 2.2522,
        "grad_norm": 2.0513665676116943,
        "learning_rate": 9.63581446227292e-05,
        "epoch": 0.5125369679825126,
        "step": 3986
    },
    {
        "loss": 2.0494,
        "grad_norm": 1.7161856889724731,
        "learning_rate": 9.631769776163071e-05,
        "epoch": 0.5126655522695127,
        "step": 3987
    },
    {
        "loss": 1.8826,
        "grad_norm": 1.752630352973938,
        "learning_rate": 9.62772515037469e-05,
        "epoch": 0.5127941365565128,
        "step": 3988
    },
    {
        "loss": 1.3364,
        "grad_norm": 1.7962088584899902,
        "learning_rate": 9.623680585570345e-05,
        "epoch": 0.5129227208435129,
        "step": 3989
    },
    {
        "loss": 1.7074,
        "grad_norm": 2.07015323638916,
        "learning_rate": 9.619636082412595e-05,
        "epoch": 0.513051305130513,
        "step": 3990
    },
    {
        "loss": 2.2226,
        "grad_norm": 1.452493667602539,
        "learning_rate": 9.615591641563988e-05,
        "epoch": 0.5131798894175131,
        "step": 3991
    },
    {
        "loss": 2.131,
        "grad_norm": 1.4297871589660645,
        "learning_rate": 9.611547263687058e-05,
        "epoch": 0.5133084737045133,
        "step": 3992
    },
    {
        "loss": 1.8287,
        "grad_norm": 1.7776918411254883,
        "learning_rate": 9.60750294944434e-05,
        "epoch": 0.5134370579915134,
        "step": 3993
    },
    {
        "loss": 1.7306,
        "grad_norm": 1.7688971757888794,
        "learning_rate": 9.603458699498347e-05,
        "epoch": 0.5135656422785135,
        "step": 3994
    },
    {
        "loss": 2.0062,
        "grad_norm": 2.350287437438965,
        "learning_rate": 9.599414514511587e-05,
        "epoch": 0.5136942265655137,
        "step": 3995
    },
    {
        "loss": 1.1241,
        "grad_norm": 3.11788272857666,
        "learning_rate": 9.595370395146558e-05,
        "epoch": 0.5138228108525138,
        "step": 3996
    },
    {
        "loss": 1.5728,
        "grad_norm": 2.5269720554351807,
        "learning_rate": 9.591326342065741e-05,
        "epoch": 0.5139513951395139,
        "step": 3997
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.227447271347046,
        "learning_rate": 9.587282355931617e-05,
        "epoch": 0.5140799794265141,
        "step": 3998
    },
    {
        "loss": 2.4062,
        "grad_norm": 1.9265174865722656,
        "learning_rate": 9.583238437406643e-05,
        "epoch": 0.5142085637135142,
        "step": 3999
    },
    {
        "loss": 1.7266,
        "grad_norm": 1.7497611045837402,
        "learning_rate": 9.579194587153278e-05,
        "epoch": 0.5143371480005143,
        "step": 4000
    },
    {
        "eval_loss": 1.8528872728347778,
        "eval_runtime": 28.2429,
        "eval_samples_per_second": 2.797,
        "eval_steps_per_second": 2.797,
        "epoch": 0.5143371480005143,
        "step": 4000
    },
    {
        "loss": 1.6917,
        "grad_norm": 2.610403299331665,
        "learning_rate": 9.57515080583396e-05,
        "epoch": 0.5144657322875145,
        "step": 4001
    },
    {
        "loss": 1.8223,
        "grad_norm": 1.985853672027588,
        "learning_rate": 9.571107094111119e-05,
        "epoch": 0.5145943165745146,
        "step": 4002
    },
    {
        "loss": 1.0376,
        "grad_norm": 2.212247610092163,
        "learning_rate": 9.567063452647177e-05,
        "epoch": 0.5147229008615147,
        "step": 4003
    },
    {
        "loss": 2.2342,
        "grad_norm": 1.9368594884872437,
        "learning_rate": 9.563019882104534e-05,
        "epoch": 0.5148514851485149,
        "step": 4004
    },
    {
        "loss": 1.599,
        "grad_norm": 2.2889959812164307,
        "learning_rate": 9.558976383145597e-05,
        "epoch": 0.514980069435515,
        "step": 4005
    },
    {
        "loss": 1.6271,
        "grad_norm": 2.050450563430786,
        "learning_rate": 9.554932956432742e-05,
        "epoch": 0.5151086537225151,
        "step": 4006
    },
    {
        "loss": 1.7403,
        "grad_norm": 1.9188345670700073,
        "learning_rate": 9.550889602628337e-05,
        "epoch": 0.5152372380095153,
        "step": 4007
    },
    {
        "loss": 1.9616,
        "grad_norm": 2.5412402153015137,
        "learning_rate": 9.546846322394756e-05,
        "epoch": 0.5153658222965154,
        "step": 4008
    },
    {
        "loss": 2.0835,
        "grad_norm": 2.8657960891723633,
        "learning_rate": 9.542803116394331e-05,
        "epoch": 0.5154944065835155,
        "step": 4009
    },
    {
        "loss": 2.1057,
        "grad_norm": 1.694143533706665,
        "learning_rate": 9.538759985289412e-05,
        "epoch": 0.5156229908705157,
        "step": 4010
    },
    {
        "loss": 1.7153,
        "grad_norm": 2.2442564964294434,
        "learning_rate": 9.534716929742316e-05,
        "epoch": 0.5157515751575158,
        "step": 4011
    },
    {
        "loss": 2.0296,
        "grad_norm": 1.8779608011245728,
        "learning_rate": 9.530673950415351e-05,
        "epoch": 0.5158801594445159,
        "step": 4012
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.082576036453247,
        "learning_rate": 9.526631047970826e-05,
        "epoch": 0.5160087437315161,
        "step": 4013
    },
    {
        "loss": 1.8554,
        "grad_norm": 1.6345874071121216,
        "learning_rate": 9.522588223071013e-05,
        "epoch": 0.5161373280185161,
        "step": 4014
    },
    {
        "loss": 2.1933,
        "grad_norm": 1.40439772605896,
        "learning_rate": 9.518545476378199e-05,
        "epoch": 0.5162659123055162,
        "step": 4015
    },
    {
        "loss": 2.4979,
        "grad_norm": 1.8112430572509766,
        "learning_rate": 9.514502808554642e-05,
        "epoch": 0.5163944965925163,
        "step": 4016
    },
    {
        "loss": 1.3068,
        "grad_norm": 2.0089120864868164,
        "learning_rate": 9.51046022026258e-05,
        "epoch": 0.5165230808795165,
        "step": 4017
    },
    {
        "loss": 1.6113,
        "grad_norm": 1.8151593208312988,
        "learning_rate": 9.506417712164257e-05,
        "epoch": 0.5166516651665166,
        "step": 4018
    },
    {
        "loss": 2.2962,
        "grad_norm": 1.037123441696167,
        "learning_rate": 9.502375284921893e-05,
        "epoch": 0.5167802494535167,
        "step": 4019
    },
    {
        "loss": 1.982,
        "grad_norm": 1.5575846433639526,
        "learning_rate": 9.498332939197696e-05,
        "epoch": 0.5169088337405169,
        "step": 4020
    },
    {
        "loss": 2.4314,
        "grad_norm": 2.1032838821411133,
        "learning_rate": 9.49429067565386e-05,
        "epoch": 0.517037418027517,
        "step": 4021
    },
    {
        "loss": 1.6224,
        "grad_norm": 1.8198060989379883,
        "learning_rate": 9.49024849495257e-05,
        "epoch": 0.5171660023145171,
        "step": 4022
    },
    {
        "loss": 1.2411,
        "grad_norm": 1.5553581714630127,
        "learning_rate": 9.48620639775599e-05,
        "epoch": 0.5172945866015173,
        "step": 4023
    },
    {
        "loss": 2.0979,
        "grad_norm": 2.012805700302124,
        "learning_rate": 9.482164384726274e-05,
        "epoch": 0.5174231708885174,
        "step": 4024
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.7803298234939575,
        "learning_rate": 9.478122456525567e-05,
        "epoch": 0.5175517551755175,
        "step": 4025
    },
    {
        "loss": 2.0116,
        "grad_norm": 1.5153697729110718,
        "learning_rate": 9.474080613815989e-05,
        "epoch": 0.5176803394625177,
        "step": 4026
    },
    {
        "loss": 1.9697,
        "grad_norm": 1.3518126010894775,
        "learning_rate": 9.470038857259661e-05,
        "epoch": 0.5178089237495178,
        "step": 4027
    },
    {
        "loss": 1.6332,
        "grad_norm": 1.8795664310455322,
        "learning_rate": 9.465997187518676e-05,
        "epoch": 0.517937508036518,
        "step": 4028
    },
    {
        "loss": 1.807,
        "grad_norm": 1.5680710077285767,
        "learning_rate": 9.461955605255117e-05,
        "epoch": 0.5180660923235181,
        "step": 4029
    },
    {
        "loss": 1.7638,
        "grad_norm": 1.8917864561080933,
        "learning_rate": 9.457914111131059e-05,
        "epoch": 0.5181946766105182,
        "step": 4030
    },
    {
        "loss": 1.5412,
        "grad_norm": 1.730141043663025,
        "learning_rate": 9.453872705808553e-05,
        "epoch": 0.5183232608975183,
        "step": 4031
    },
    {
        "loss": 1.7405,
        "grad_norm": 2.739250421524048,
        "learning_rate": 9.449831389949642e-05,
        "epoch": 0.5184518451845185,
        "step": 4032
    },
    {
        "loss": 2.2568,
        "grad_norm": 2.1389198303222656,
        "learning_rate": 9.445790164216353e-05,
        "epoch": 0.5185804294715186,
        "step": 4033
    },
    {
        "loss": 1.491,
        "grad_norm": 1.5492618083953857,
        "learning_rate": 9.441749029270695e-05,
        "epoch": 0.5187090137585187,
        "step": 4034
    },
    {
        "loss": 1.4808,
        "grad_norm": 1.6838222742080688,
        "learning_rate": 9.437707985774668e-05,
        "epoch": 0.5188375980455189,
        "step": 4035
    },
    {
        "loss": 1.371,
        "grad_norm": 2.6798367500305176,
        "learning_rate": 9.43366703439025e-05,
        "epoch": 0.518966182332519,
        "step": 4036
    },
    {
        "loss": 1.7707,
        "grad_norm": 1.7672343254089355,
        "learning_rate": 9.429626175779413e-05,
        "epoch": 0.5190947666195191,
        "step": 4037
    },
    {
        "loss": 2.2536,
        "grad_norm": 2.0604350566864014,
        "learning_rate": 9.425585410604103e-05,
        "epoch": 0.5192233509065193,
        "step": 4038
    },
    {
        "loss": 0.6316,
        "grad_norm": 2.056194305419922,
        "learning_rate": 9.421544739526257e-05,
        "epoch": 0.5193519351935193,
        "step": 4039
    },
    {
        "loss": 2.3793,
        "grad_norm": 1.5874451398849487,
        "learning_rate": 9.417504163207799e-05,
        "epoch": 0.5194805194805194,
        "step": 4040
    },
    {
        "loss": 1.2019,
        "grad_norm": 1.6220982074737549,
        "learning_rate": 9.413463682310631e-05,
        "epoch": 0.5196091037675196,
        "step": 4041
    },
    {
        "loss": 2.1512,
        "grad_norm": 2.9377620220184326,
        "learning_rate": 9.409423297496645e-05,
        "epoch": 0.5197376880545197,
        "step": 4042
    },
    {
        "loss": 1.685,
        "grad_norm": 2.525094985961914,
        "learning_rate": 9.405383009427713e-05,
        "epoch": 0.5198662723415198,
        "step": 4043
    },
    {
        "loss": 1.5997,
        "grad_norm": 2.5073864459991455,
        "learning_rate": 9.401342818765692e-05,
        "epoch": 0.51999485662852,
        "step": 4044
    },
    {
        "loss": 2.1285,
        "grad_norm": 1.1466535329818726,
        "learning_rate": 9.397302726172425e-05,
        "epoch": 0.5201234409155201,
        "step": 4045
    },
    {
        "loss": 2.04,
        "grad_norm": 1.6991729736328125,
        "learning_rate": 9.393262732309737e-05,
        "epoch": 0.5202520252025202,
        "step": 4046
    },
    {
        "loss": 1.9689,
        "grad_norm": 1.327345371246338,
        "learning_rate": 9.389222837839444e-05,
        "epoch": 0.5203806094895204,
        "step": 4047
    },
    {
        "loss": 1.188,
        "grad_norm": 2.4056801795959473,
        "learning_rate": 9.385183043423333e-05,
        "epoch": 0.5205091937765205,
        "step": 4048
    },
    {
        "loss": 2.2018,
        "grad_norm": 1.1607753038406372,
        "learning_rate": 9.38114334972318e-05,
        "epoch": 0.5206377780635206,
        "step": 4049
    },
    {
        "loss": 2.1332,
        "grad_norm": 2.4633266925811768,
        "learning_rate": 9.377103757400751e-05,
        "epoch": 0.5207663623505208,
        "step": 4050
    },
    {
        "loss": 1.939,
        "grad_norm": 1.6937353610992432,
        "learning_rate": 9.373064267117784e-05,
        "epoch": 0.5208949466375209,
        "step": 4051
    },
    {
        "loss": 2.1487,
        "grad_norm": 2.482072353363037,
        "learning_rate": 9.369024879536015e-05,
        "epoch": 0.521023530924521,
        "step": 4052
    },
    {
        "loss": 1.4497,
        "grad_norm": 1.4351789951324463,
        "learning_rate": 9.364985595317146e-05,
        "epoch": 0.5211521152115212,
        "step": 4053
    },
    {
        "loss": 1.9849,
        "grad_norm": 1.2345432043075562,
        "learning_rate": 9.360946415122869e-05,
        "epoch": 0.5212806994985213,
        "step": 4054
    },
    {
        "loss": 1.5356,
        "grad_norm": 2.2236697673797607,
        "learning_rate": 9.356907339614872e-05,
        "epoch": 0.5214092837855214,
        "step": 4055
    },
    {
        "loss": 1.4359,
        "grad_norm": 2.925511121749878,
        "learning_rate": 9.3528683694548e-05,
        "epoch": 0.5215378680725216,
        "step": 4056
    },
    {
        "loss": 1.6533,
        "grad_norm": 2.0205047130584717,
        "learning_rate": 9.348829505304311e-05,
        "epoch": 0.5216664523595217,
        "step": 4057
    },
    {
        "loss": 2.1568,
        "grad_norm": 2.990403652191162,
        "learning_rate": 9.344790747825017e-05,
        "epoch": 0.5217950366465218,
        "step": 4058
    },
    {
        "loss": 1.9871,
        "grad_norm": 1.2014285326004028,
        "learning_rate": 9.340752097678524e-05,
        "epoch": 0.521923620933522,
        "step": 4059
    },
    {
        "loss": 2.1939,
        "grad_norm": 1.8082690238952637,
        "learning_rate": 9.336713555526435e-05,
        "epoch": 0.5220522052205221,
        "step": 4060
    },
    {
        "loss": 2.1922,
        "grad_norm": 1.7425577640533447,
        "learning_rate": 9.332675122030307e-05,
        "epoch": 0.5221807895075222,
        "step": 4061
    },
    {
        "loss": 2.4227,
        "grad_norm": 1.5565969944000244,
        "learning_rate": 9.328636797851704e-05,
        "epoch": 0.5223093737945224,
        "step": 4062
    },
    {
        "loss": 1.4959,
        "grad_norm": 2.1471915245056152,
        "learning_rate": 9.324598583652163e-05,
        "epoch": 0.5224379580815225,
        "step": 4063
    },
    {
        "loss": 1.3837,
        "grad_norm": 1.4156889915466309,
        "learning_rate": 9.320560480093191e-05,
        "epoch": 0.5225665423685225,
        "step": 4064
    },
    {
        "loss": 1.8742,
        "grad_norm": 3.412567138671875,
        "learning_rate": 9.3165224878363e-05,
        "epoch": 0.5226951266555226,
        "step": 4065
    },
    {
        "loss": 2.3816,
        "grad_norm": 1.347825288772583,
        "learning_rate": 9.312484607542966e-05,
        "epoch": 0.5228237109425228,
        "step": 4066
    },
    {
        "loss": 2.01,
        "grad_norm": 2.268373966217041,
        "learning_rate": 9.308446839874655e-05,
        "epoch": 0.5229522952295229,
        "step": 4067
    },
    {
        "loss": 1.6039,
        "grad_norm": 3.1918468475341797,
        "learning_rate": 9.304409185492814e-05,
        "epoch": 0.523080879516523,
        "step": 4068
    },
    {
        "loss": 2.1407,
        "grad_norm": 1.9063823223114014,
        "learning_rate": 9.30037164505886e-05,
        "epoch": 0.5232094638035232,
        "step": 4069
    },
    {
        "loss": 1.4576,
        "grad_norm": 1.6439566612243652,
        "learning_rate": 9.29633421923421e-05,
        "epoch": 0.5233380480905233,
        "step": 4070
    },
    {
        "loss": 2.2246,
        "grad_norm": 2.1962547302246094,
        "learning_rate": 9.29229690868025e-05,
        "epoch": 0.5234666323775234,
        "step": 4071
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.2337677478790283,
        "learning_rate": 9.288259714058353e-05,
        "epoch": 0.5235952166645236,
        "step": 4072
    },
    {
        "loss": 2.1048,
        "grad_norm": 2.1911604404449463,
        "learning_rate": 9.284222636029865e-05,
        "epoch": 0.5237238009515237,
        "step": 4073
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.5646672248840332,
        "learning_rate": 9.280185675256124e-05,
        "epoch": 0.5238523852385238,
        "step": 4074
    },
    {
        "loss": 2.6084,
        "grad_norm": 1.3689144849777222,
        "learning_rate": 9.276148832398439e-05,
        "epoch": 0.523980969525524,
        "step": 4075
    },
    {
        "loss": 1.7468,
        "grad_norm": 1.886026382446289,
        "learning_rate": 9.2721121081181e-05,
        "epoch": 0.5241095538125241,
        "step": 4076
    },
    {
        "loss": 1.1304,
        "grad_norm": 1.6196460723876953,
        "learning_rate": 9.268075503076392e-05,
        "epoch": 0.5242381380995242,
        "step": 4077
    },
    {
        "loss": 1.2231,
        "grad_norm": 2.3073699474334717,
        "learning_rate": 9.264039017934559e-05,
        "epoch": 0.5243667223865244,
        "step": 4078
    },
    {
        "loss": 1.9816,
        "grad_norm": 1.5201780796051025,
        "learning_rate": 9.260002653353842e-05,
        "epoch": 0.5244953066735245,
        "step": 4079
    },
    {
        "loss": 2.4503,
        "grad_norm": 1.1891084909439087,
        "learning_rate": 9.255966409995455e-05,
        "epoch": 0.5246238909605246,
        "step": 4080
    },
    {
        "loss": 2.0364,
        "grad_norm": 1.3267332315444946,
        "learning_rate": 9.251930288520589e-05,
        "epoch": 0.5247524752475248,
        "step": 4081
    },
    {
        "loss": 1.3187,
        "grad_norm": 2.1353654861450195,
        "learning_rate": 9.247894289590428e-05,
        "epoch": 0.5248810595345249,
        "step": 4082
    },
    {
        "loss": 1.4973,
        "grad_norm": 1.9495511054992676,
        "learning_rate": 9.243858413866119e-05,
        "epoch": 0.525009643821525,
        "step": 4083
    },
    {
        "loss": 2.1861,
        "grad_norm": 1.7736179828643799,
        "learning_rate": 9.239822662008803e-05,
        "epoch": 0.5251382281085252,
        "step": 4084
    },
    {
        "loss": 1.8201,
        "grad_norm": 2.300905704498291,
        "learning_rate": 9.235787034679593e-05,
        "epoch": 0.5252668123955253,
        "step": 4085
    },
    {
        "loss": 2.3293,
        "grad_norm": 1.4125125408172607,
        "learning_rate": 9.23175153253958e-05,
        "epoch": 0.5253953966825254,
        "step": 4086
    },
    {
        "loss": 1.8906,
        "grad_norm": 2.056455135345459,
        "learning_rate": 9.227716156249843e-05,
        "epoch": 0.5255239809695256,
        "step": 4087
    },
    {
        "loss": 2.708,
        "grad_norm": 1.2628759145736694,
        "learning_rate": 9.223680906471432e-05,
        "epoch": 0.5256525652565257,
        "step": 4088
    },
    {
        "loss": 1.3819,
        "grad_norm": 2.7166037559509277,
        "learning_rate": 9.219645783865381e-05,
        "epoch": 0.5257811495435257,
        "step": 4089
    },
    {
        "loss": 1.3371,
        "grad_norm": 2.4466676712036133,
        "learning_rate": 9.215610789092704e-05,
        "epoch": 0.5259097338305259,
        "step": 4090
    },
    {
        "loss": 2.0036,
        "grad_norm": 1.3284131288528442,
        "learning_rate": 9.211575922814388e-05,
        "epoch": 0.526038318117526,
        "step": 4091
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.3326430320739746,
        "learning_rate": 9.207541185691404e-05,
        "epoch": 0.5261669024045261,
        "step": 4092
    },
    {
        "loss": 1.5035,
        "grad_norm": 1.1243445873260498,
        "learning_rate": 9.203506578384698e-05,
        "epoch": 0.5262954866915263,
        "step": 4093
    },
    {
        "loss": 1.8882,
        "grad_norm": 1.700205683708191,
        "learning_rate": 9.199472101555206e-05,
        "epoch": 0.5264240709785264,
        "step": 4094
    },
    {
        "loss": 1.9829,
        "grad_norm": 1.5103425979614258,
        "learning_rate": 9.195437755863826e-05,
        "epoch": 0.5265526552655265,
        "step": 4095
    },
    {
        "loss": 1.3594,
        "grad_norm": 1.9559698104858398,
        "learning_rate": 9.191403541971444e-05,
        "epoch": 0.5266812395525267,
        "step": 4096
    },
    {
        "loss": 2.131,
        "grad_norm": 1.366365909576416,
        "learning_rate": 9.187369460538924e-05,
        "epoch": 0.5268098238395268,
        "step": 4097
    },
    {
        "loss": 2.4364,
        "grad_norm": 1.2604076862335205,
        "learning_rate": 9.183335512227105e-05,
        "epoch": 0.5269384081265269,
        "step": 4098
    },
    {
        "loss": 1.6645,
        "grad_norm": 1.8238532543182373,
        "learning_rate": 9.179301697696812e-05,
        "epoch": 0.5270669924135271,
        "step": 4099
    },
    {
        "loss": 1.1945,
        "grad_norm": 1.5812711715698242,
        "learning_rate": 9.175268017608838e-05,
        "epoch": 0.5271955767005272,
        "step": 4100
    },
    {
        "eval_loss": 1.8522825241088867,
        "eval_runtime": 28.3107,
        "eval_samples_per_second": 2.79,
        "eval_steps_per_second": 2.79,
        "epoch": 0.5271955767005272,
        "step": 4100
    },
    {
        "loss": 2.3893,
        "grad_norm": 2.4787886142730713,
        "learning_rate": 9.171234472623956e-05,
        "epoch": 0.5273241609875273,
        "step": 4101
    },
    {
        "loss": 1.64,
        "grad_norm": 1.808340311050415,
        "learning_rate": 9.167201063402929e-05,
        "epoch": 0.5274527452745275,
        "step": 4102
    },
    {
        "loss": 2.0665,
        "grad_norm": 1.992586612701416,
        "learning_rate": 9.163167790606474e-05,
        "epoch": 0.5275813295615276,
        "step": 4103
    },
    {
        "loss": 1.7084,
        "grad_norm": 1.7886226177215576,
        "learning_rate": 9.159134654895312e-05,
        "epoch": 0.5277099138485277,
        "step": 4104
    },
    {
        "loss": 1.9403,
        "grad_norm": 1.6402617692947388,
        "learning_rate": 9.155101656930127e-05,
        "epoch": 0.5278384981355279,
        "step": 4105
    },
    {
        "loss": 1.3189,
        "grad_norm": 2.2181522846221924,
        "learning_rate": 9.151068797371575e-05,
        "epoch": 0.527967082422528,
        "step": 4106
    },
    {
        "loss": 2.2888,
        "grad_norm": 1.930619239807129,
        "learning_rate": 9.147036076880307e-05,
        "epoch": 0.5280956667095281,
        "step": 4107
    },
    {
        "loss": 1.2833,
        "grad_norm": 2.0038352012634277,
        "learning_rate": 9.143003496116932e-05,
        "epoch": 0.5282242509965283,
        "step": 4108
    },
    {
        "loss": 1.7816,
        "grad_norm": 2.0093889236450195,
        "learning_rate": 9.138971055742055e-05,
        "epoch": 0.5283528352835284,
        "step": 4109
    },
    {
        "loss": 1.6269,
        "grad_norm": 2.1357321739196777,
        "learning_rate": 9.134938756416245e-05,
        "epoch": 0.5284814195705285,
        "step": 4110
    },
    {
        "loss": 1.6043,
        "grad_norm": 1.7784228324890137,
        "learning_rate": 9.130906598800043e-05,
        "epoch": 0.5286100038575287,
        "step": 4111
    },
    {
        "loss": 2.0082,
        "grad_norm": 1.9760031700134277,
        "learning_rate": 9.126874583553987e-05,
        "epoch": 0.5287385881445288,
        "step": 4112
    },
    {
        "loss": 1.6307,
        "grad_norm": 1.7352910041809082,
        "learning_rate": 9.122842711338573e-05,
        "epoch": 0.5288671724315289,
        "step": 4113
    },
    {
        "loss": 2.1038,
        "grad_norm": 2.1485793590545654,
        "learning_rate": 9.118810982814283e-05,
        "epoch": 0.5289957567185289,
        "step": 4114
    },
    {
        "loss": 1.7393,
        "grad_norm": 2.1357362270355225,
        "learning_rate": 9.114779398641572e-05,
        "epoch": 0.5291243410055291,
        "step": 4115
    },
    {
        "loss": 1.8769,
        "grad_norm": 2.3350675106048584,
        "learning_rate": 9.11074795948087e-05,
        "epoch": 0.5292529252925292,
        "step": 4116
    },
    {
        "loss": 1.7742,
        "grad_norm": 2.4537715911865234,
        "learning_rate": 9.106716665992589e-05,
        "epoch": 0.5293815095795293,
        "step": 4117
    },
    {
        "loss": 2.1572,
        "grad_norm": 1.8523187637329102,
        "learning_rate": 9.102685518837112e-05,
        "epoch": 0.5295100938665295,
        "step": 4118
    },
    {
        "loss": 1.881,
        "grad_norm": 5.785464763641357,
        "learning_rate": 9.0986545186748e-05,
        "epoch": 0.5296386781535296,
        "step": 4119
    },
    {
        "loss": 1.6872,
        "grad_norm": 1.7961548566818237,
        "learning_rate": 9.094623666165985e-05,
        "epoch": 0.5297672624405297,
        "step": 4120
    },
    {
        "loss": 1.7935,
        "grad_norm": 2.0881853103637695,
        "learning_rate": 9.090592961970988e-05,
        "epoch": 0.5298958467275299,
        "step": 4121
    },
    {
        "loss": 2.1811,
        "grad_norm": 1.8958121538162231,
        "learning_rate": 9.086562406750091e-05,
        "epoch": 0.53002443101453,
        "step": 4122
    },
    {
        "loss": 2.002,
        "grad_norm": 2.2423322200775146,
        "learning_rate": 9.082532001163556e-05,
        "epoch": 0.5301530153015301,
        "step": 4123
    },
    {
        "loss": 1.5316,
        "grad_norm": 1.744946002960205,
        "learning_rate": 9.078501745871628e-05,
        "epoch": 0.5302815995885303,
        "step": 4124
    },
    {
        "loss": 1.4461,
        "grad_norm": 1.8555898666381836,
        "learning_rate": 9.074471641534515e-05,
        "epoch": 0.5304101838755304,
        "step": 4125
    },
    {
        "loss": 2.1087,
        "grad_norm": 1.595977544784546,
        "learning_rate": 9.070441688812412e-05,
        "epoch": 0.5305387681625305,
        "step": 4126
    },
    {
        "loss": 0.8198,
        "grad_norm": 2.42834734916687,
        "learning_rate": 9.066411888365483e-05,
        "epoch": 0.5306673524495307,
        "step": 4127
    },
    {
        "loss": 1.1928,
        "grad_norm": 1.3077791929244995,
        "learning_rate": 9.062382240853864e-05,
        "epoch": 0.5307959367365308,
        "step": 4128
    },
    {
        "loss": 1.2112,
        "grad_norm": 2.1774253845214844,
        "learning_rate": 9.058352746937674e-05,
        "epoch": 0.5309245210235309,
        "step": 4129
    },
    {
        "loss": 1.6823,
        "grad_norm": 2.0326919555664062,
        "learning_rate": 9.054323407277e-05,
        "epoch": 0.5310531053105311,
        "step": 4130
    },
    {
        "loss": 1.7946,
        "grad_norm": 3.007525682449341,
        "learning_rate": 9.050294222531908e-05,
        "epoch": 0.5311816895975312,
        "step": 4131
    },
    {
        "loss": 2.0317,
        "grad_norm": 1.6357383728027344,
        "learning_rate": 9.04626519336244e-05,
        "epoch": 0.5313102738845313,
        "step": 4132
    },
    {
        "loss": 1.5694,
        "grad_norm": 1.4822635650634766,
        "learning_rate": 9.042236320428602e-05,
        "epoch": 0.5314388581715315,
        "step": 4133
    },
    {
        "loss": 1.9002,
        "grad_norm": 1.8062623739242554,
        "learning_rate": 9.03820760439039e-05,
        "epoch": 0.5315674424585316,
        "step": 4134
    },
    {
        "loss": 1.733,
        "grad_norm": 1.688225507736206,
        "learning_rate": 9.034179045907758e-05,
        "epoch": 0.5316960267455317,
        "step": 4135
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.7666109800338745,
        "learning_rate": 9.030150645640652e-05,
        "epoch": 0.5318246110325319,
        "step": 4136
    },
    {
        "loss": 2.3801,
        "grad_norm": 1.8662493228912354,
        "learning_rate": 9.026122404248976e-05,
        "epoch": 0.531953195319532,
        "step": 4137
    },
    {
        "loss": 2.408,
        "grad_norm": 1.2848496437072754,
        "learning_rate": 9.022094322392614e-05,
        "epoch": 0.5320817796065321,
        "step": 4138
    },
    {
        "loss": 1.8379,
        "grad_norm": 2.479546308517456,
        "learning_rate": 9.018066400731431e-05,
        "epoch": 0.5322103638935322,
        "step": 4139
    },
    {
        "loss": 1.9472,
        "grad_norm": 1.6059409379959106,
        "learning_rate": 9.014038639925248e-05,
        "epoch": 0.5323389481805323,
        "step": 4140
    },
    {
        "loss": 2.1952,
        "grad_norm": 1.9028719663619995,
        "learning_rate": 9.010011040633885e-05,
        "epoch": 0.5324675324675324,
        "step": 4141
    },
    {
        "loss": 1.5168,
        "grad_norm": 2.4451658725738525,
        "learning_rate": 9.005983603517112e-05,
        "epoch": 0.5325961167545326,
        "step": 4142
    },
    {
        "loss": 1.7173,
        "grad_norm": 1.893432378768921,
        "learning_rate": 9.00195632923468e-05,
        "epoch": 0.5327247010415327,
        "step": 4143
    },
    {
        "loss": 1.8252,
        "grad_norm": 1.1138979196548462,
        "learning_rate": 8.997929218446324e-05,
        "epoch": 0.5328532853285328,
        "step": 4144
    },
    {
        "loss": 1.4955,
        "grad_norm": 1.774587631225586,
        "learning_rate": 8.993902271811733e-05,
        "epoch": 0.532981869615533,
        "step": 4145
    },
    {
        "loss": 1.7452,
        "grad_norm": 1.220411777496338,
        "learning_rate": 8.98987548999059e-05,
        "epoch": 0.5331104539025331,
        "step": 4146
    },
    {
        "loss": 2.3316,
        "grad_norm": 1.5906275510787964,
        "learning_rate": 8.985848873642533e-05,
        "epoch": 0.5332390381895332,
        "step": 4147
    },
    {
        "loss": 2.1812,
        "grad_norm": 1.9937121868133545,
        "learning_rate": 8.981822423427179e-05,
        "epoch": 0.5333676224765334,
        "step": 4148
    },
    {
        "loss": 1.816,
        "grad_norm": 1.7737665176391602,
        "learning_rate": 8.977796140004129e-05,
        "epoch": 0.5334962067635335,
        "step": 4149
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.9570317268371582,
        "learning_rate": 8.973770024032933e-05,
        "epoch": 0.5336247910505336,
        "step": 4150
    },
    {
        "loss": 2.2474,
        "grad_norm": 1.4077491760253906,
        "learning_rate": 8.969744076173139e-05,
        "epoch": 0.5337533753375338,
        "step": 4151
    },
    {
        "loss": 1.5897,
        "grad_norm": 1.378813624382019,
        "learning_rate": 8.965718297084254e-05,
        "epoch": 0.5338819596245339,
        "step": 4152
    },
    {
        "loss": 1.4469,
        "grad_norm": 1.833490014076233,
        "learning_rate": 8.961692687425748e-05,
        "epoch": 0.534010543911534,
        "step": 4153
    },
    {
        "loss": 0.9129,
        "grad_norm": 2.6688313484191895,
        "learning_rate": 8.957667247857091e-05,
        "epoch": 0.5341391281985342,
        "step": 4154
    },
    {
        "loss": 1.477,
        "grad_norm": 1.6887418031692505,
        "learning_rate": 8.953641979037697e-05,
        "epoch": 0.5342677124855343,
        "step": 4155
    },
    {
        "loss": 2.0819,
        "grad_norm": 2.2084567546844482,
        "learning_rate": 8.949616881626969e-05,
        "epoch": 0.5343962967725344,
        "step": 4156
    },
    {
        "loss": 1.9027,
        "grad_norm": 2.0323357582092285,
        "learning_rate": 8.945591956284276e-05,
        "epoch": 0.5345248810595346,
        "step": 4157
    },
    {
        "loss": 1.7767,
        "grad_norm": 1.9321280717849731,
        "learning_rate": 8.941567203668952e-05,
        "epoch": 0.5346534653465347,
        "step": 4158
    },
    {
        "loss": 1.1556,
        "grad_norm": 2.2170612812042236,
        "learning_rate": 8.937542624440322e-05,
        "epoch": 0.5347820496335348,
        "step": 4159
    },
    {
        "loss": 1.6812,
        "grad_norm": 1.695815086364746,
        "learning_rate": 8.933518219257662e-05,
        "epoch": 0.534910633920535,
        "step": 4160
    },
    {
        "loss": 2.1376,
        "grad_norm": 1.9330352544784546,
        "learning_rate": 8.929493988780231e-05,
        "epoch": 0.5350392182075351,
        "step": 4161
    },
    {
        "loss": 1.8575,
        "grad_norm": 2.4105424880981445,
        "learning_rate": 8.925469933667259e-05,
        "epoch": 0.5351678024945352,
        "step": 4162
    },
    {
        "loss": 2.149,
        "grad_norm": 1.658987283706665,
        "learning_rate": 8.921446054577937e-05,
        "epoch": 0.5352963867815353,
        "step": 4163
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.578558087348938,
        "learning_rate": 8.917422352171444e-05,
        "epoch": 0.5354249710685354,
        "step": 4164
    },
    {
        "loss": 2.2145,
        "grad_norm": 1.8616158962249756,
        "learning_rate": 8.913398827106915e-05,
        "epoch": 0.5355535553555355,
        "step": 4165
    },
    {
        "loss": 2.1437,
        "grad_norm": 2.042875051498413,
        "learning_rate": 8.909375480043465e-05,
        "epoch": 0.5356821396425356,
        "step": 4166
    },
    {
        "loss": 2.0586,
        "grad_norm": 1.6544357538223267,
        "learning_rate": 8.905352311640176e-05,
        "epoch": 0.5358107239295358,
        "step": 4167
    },
    {
        "loss": 1.7413,
        "grad_norm": 1.9238426685333252,
        "learning_rate": 8.901329322556099e-05,
        "epoch": 0.5359393082165359,
        "step": 4168
    },
    {
        "loss": 1.8668,
        "grad_norm": 2.2877066135406494,
        "learning_rate": 8.897306513450263e-05,
        "epoch": 0.536067892503536,
        "step": 4169
    },
    {
        "loss": 2.0099,
        "grad_norm": 1.8949098587036133,
        "learning_rate": 8.893283884981656e-05,
        "epoch": 0.5361964767905362,
        "step": 4170
    },
    {
        "loss": 1.7424,
        "grad_norm": 2.1068108081817627,
        "learning_rate": 8.889261437809251e-05,
        "epoch": 0.5363250610775363,
        "step": 4171
    },
    {
        "loss": 1.9272,
        "grad_norm": 2.149409770965576,
        "learning_rate": 8.885239172591976e-05,
        "epoch": 0.5364536453645364,
        "step": 4172
    },
    {
        "loss": 1.6759,
        "grad_norm": 2.04565167427063,
        "learning_rate": 8.881217089988742e-05,
        "epoch": 0.5365822296515366,
        "step": 4173
    },
    {
        "loss": 1.211,
        "grad_norm": 2.8647563457489014,
        "learning_rate": 8.877195190658424e-05,
        "epoch": 0.5367108139385367,
        "step": 4174
    },
    {
        "loss": 1.7088,
        "grad_norm": 1.8833998441696167,
        "learning_rate": 8.873173475259862e-05,
        "epoch": 0.5368393982255368,
        "step": 4175
    },
    {
        "loss": 1.8638,
        "grad_norm": 1.8231438398361206,
        "learning_rate": 8.869151944451879e-05,
        "epoch": 0.536967982512537,
        "step": 4176
    },
    {
        "loss": 1.1365,
        "grad_norm": 2.1992528438568115,
        "learning_rate": 8.865130598893256e-05,
        "epoch": 0.5370965667995371,
        "step": 4177
    },
    {
        "loss": 1.0763,
        "grad_norm": 2.060167074203491,
        "learning_rate": 8.861109439242749e-05,
        "epoch": 0.5372251510865372,
        "step": 4178
    },
    {
        "loss": 1.5241,
        "grad_norm": 1.879450798034668,
        "learning_rate": 8.857088466159082e-05,
        "epoch": 0.5373537353735374,
        "step": 4179
    },
    {
        "loss": 1.4155,
        "grad_norm": 2.82831072807312,
        "learning_rate": 8.853067680300948e-05,
        "epoch": 0.5374823196605375,
        "step": 4180
    },
    {
        "loss": 0.9499,
        "grad_norm": 2.022655963897705,
        "learning_rate": 8.849047082327012e-05,
        "epoch": 0.5376109039475376,
        "step": 4181
    },
    {
        "loss": 1.5556,
        "grad_norm": 2.555724620819092,
        "learning_rate": 8.845026672895902e-05,
        "epoch": 0.5377394882345378,
        "step": 4182
    },
    {
        "loss": 1.7007,
        "grad_norm": 2.8411359786987305,
        "learning_rate": 8.841006452666231e-05,
        "epoch": 0.5378680725215379,
        "step": 4183
    },
    {
        "loss": 2.5991,
        "grad_norm": 2.2212986946105957,
        "learning_rate": 8.836986422296559e-05,
        "epoch": 0.537996656808538,
        "step": 4184
    },
    {
        "loss": 1.3624,
        "grad_norm": 1.784382700920105,
        "learning_rate": 8.832966582445426e-05,
        "epoch": 0.5381252410955382,
        "step": 4185
    },
    {
        "loss": 1.0269,
        "grad_norm": 3.068843364715576,
        "learning_rate": 8.828946933771344e-05,
        "epoch": 0.5382538253825383,
        "step": 4186
    },
    {
        "loss": 1.8143,
        "grad_norm": 1.9324181079864502,
        "learning_rate": 8.824927476932784e-05,
        "epoch": 0.5383824096695384,
        "step": 4187
    },
    {
        "loss": 2.1179,
        "grad_norm": 2.102553129196167,
        "learning_rate": 8.820908212588203e-05,
        "epoch": 0.5385109939565386,
        "step": 4188
    },
    {
        "loss": 1.8923,
        "grad_norm": 2.7078464031219482,
        "learning_rate": 8.816889141396006e-05,
        "epoch": 0.5386395782435386,
        "step": 4189
    },
    {
        "loss": 1.4362,
        "grad_norm": 3.8040523529052734,
        "learning_rate": 8.812870264014573e-05,
        "epoch": 0.5387681625305387,
        "step": 4190
    },
    {
        "loss": 1.4289,
        "grad_norm": Infinity,
        "learning_rate": 8.812870264014573e-05,
        "epoch": 0.5388967468175389,
        "step": 4191
    },
    {
        "loss": 1.6493,
        "grad_norm": 2.498121976852417,
        "learning_rate": 8.808851581102265e-05,
        "epoch": 0.539025331104539,
        "step": 4192
    },
    {
        "loss": 1.9053,
        "grad_norm": 1.4376006126403809,
        "learning_rate": 8.804833093317388e-05,
        "epoch": 0.5391539153915391,
        "step": 4193
    },
    {
        "loss": 1.3925,
        "grad_norm": 1.6130622625350952,
        "learning_rate": 8.80081480131824e-05,
        "epoch": 0.5392824996785393,
        "step": 4194
    },
    {
        "loss": 2.2991,
        "grad_norm": 0.9698556661605835,
        "learning_rate": 8.796796705763073e-05,
        "epoch": 0.5394110839655394,
        "step": 4195
    },
    {
        "loss": 1.5722,
        "grad_norm": 2.1560935974121094,
        "learning_rate": 8.7927788073101e-05,
        "epoch": 0.5395396682525395,
        "step": 4196
    },
    {
        "loss": 1.7835,
        "grad_norm": 1.792575716972351,
        "learning_rate": 8.788761106617526e-05,
        "epoch": 0.5396682525395397,
        "step": 4197
    },
    {
        "loss": 2.0933,
        "grad_norm": 1.5021369457244873,
        "learning_rate": 8.784743604343495e-05,
        "epoch": 0.5397968368265398,
        "step": 4198
    },
    {
        "loss": 1.2968,
        "grad_norm": 1.8561952114105225,
        "learning_rate": 8.780726301146143e-05,
        "epoch": 0.5399254211135399,
        "step": 4199
    },
    {
        "loss": 1.2405,
        "grad_norm": 1.8316518068313599,
        "learning_rate": 8.77670919768356e-05,
        "epoch": 0.54005400540054,
        "step": 4200
    },
    {
        "eval_loss": 1.855446696281433,
        "eval_runtime": 28.3309,
        "eval_samples_per_second": 2.788,
        "eval_steps_per_second": 2.788,
        "epoch": 0.54005400540054,
        "step": 4200
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.0873546600341797,
        "learning_rate": 8.772692294613799e-05,
        "epoch": 0.5401825896875402,
        "step": 4201
    },
    {
        "loss": 2.4348,
        "grad_norm": 1.4844990968704224,
        "learning_rate": 8.768675592594897e-05,
        "epoch": 0.5403111739745403,
        "step": 4202
    },
    {
        "loss": 2.13,
        "grad_norm": 1.8419668674468994,
        "learning_rate": 8.764659092284841e-05,
        "epoch": 0.5404397582615404,
        "step": 4203
    },
    {
        "loss": 2.0614,
        "grad_norm": 1.9508960247039795,
        "learning_rate": 8.760642794341596e-05,
        "epoch": 0.5405683425485406,
        "step": 4204
    },
    {
        "loss": 1.5611,
        "grad_norm": 2.1912224292755127,
        "learning_rate": 8.756626699423093e-05,
        "epoch": 0.5406969268355407,
        "step": 4205
    },
    {
        "loss": 2.2791,
        "grad_norm": 1.9241187572479248,
        "learning_rate": 8.752610808187216e-05,
        "epoch": 0.5408255111225408,
        "step": 4206
    },
    {
        "loss": 1.8539,
        "grad_norm": 2.1181583404541016,
        "learning_rate": 8.748595121291838e-05,
        "epoch": 0.540954095409541,
        "step": 4207
    },
    {
        "loss": 2.1959,
        "grad_norm": 1.6831120252609253,
        "learning_rate": 8.744579639394778e-05,
        "epoch": 0.5410826796965411,
        "step": 4208
    },
    {
        "loss": 1.7808,
        "grad_norm": 1.7254884243011475,
        "learning_rate": 8.740564363153839e-05,
        "epoch": 0.5412112639835412,
        "step": 4209
    },
    {
        "loss": 2.172,
        "grad_norm": 1.748178482055664,
        "learning_rate": 8.736549293226775e-05,
        "epoch": 0.5413398482705414,
        "step": 4210
    },
    {
        "loss": 1.701,
        "grad_norm": 2.2155802249908447,
        "learning_rate": 8.732534430271313e-05,
        "epoch": 0.5414684325575415,
        "step": 4211
    },
    {
        "loss": 2.1009,
        "grad_norm": 2.4631507396698,
        "learning_rate": 8.728519774945151e-05,
        "epoch": 0.5415970168445416,
        "step": 4212
    },
    {
        "loss": 1.7493,
        "grad_norm": 2.580202102661133,
        "learning_rate": 8.724505327905942e-05,
        "epoch": 0.5417256011315418,
        "step": 4213
    },
    {
        "loss": 2.2678,
        "grad_norm": 3.0520315170288086,
        "learning_rate": 8.720491089811318e-05,
        "epoch": 0.5418541854185418,
        "step": 4214
    },
    {
        "loss": 2.3978,
        "grad_norm": 1.9352023601531982,
        "learning_rate": 8.716477061318863e-05,
        "epoch": 0.5419827697055419,
        "step": 4215
    },
    {
        "loss": 1.9907,
        "grad_norm": 2.27874493598938,
        "learning_rate": 8.712463243086134e-05,
        "epoch": 0.5421113539925421,
        "step": 4216
    },
    {
        "loss": 1.543,
        "grad_norm": 2.330320358276367,
        "learning_rate": 8.708449635770657e-05,
        "epoch": 0.5422399382795422,
        "step": 4217
    },
    {
        "loss": 2.5085,
        "grad_norm": 1.8632563352584839,
        "learning_rate": 8.704436240029914e-05,
        "epoch": 0.5423685225665423,
        "step": 4218
    },
    {
        "loss": 1.7407,
        "grad_norm": 2.0167362689971924,
        "learning_rate": 8.700423056521362e-05,
        "epoch": 0.5424971068535425,
        "step": 4219
    },
    {
        "loss": 1.4557,
        "grad_norm": 2.088196277618408,
        "learning_rate": 8.696410085902418e-05,
        "epoch": 0.5426256911405426,
        "step": 4220
    },
    {
        "loss": 2.1392,
        "grad_norm": 1.8383805751800537,
        "learning_rate": 8.69239732883046e-05,
        "epoch": 0.5427542754275427,
        "step": 4221
    },
    {
        "loss": 1.5693,
        "grad_norm": 1.8658660650253296,
        "learning_rate": 8.688384785962844e-05,
        "epoch": 0.5428828597145429,
        "step": 4222
    },
    {
        "loss": 2.477,
        "grad_norm": 1.1804351806640625,
        "learning_rate": 8.684372457956876e-05,
        "epoch": 0.543011444001543,
        "step": 4223
    },
    {
        "loss": 2.0308,
        "grad_norm": 2.2200982570648193,
        "learning_rate": 8.680360345469838e-05,
        "epoch": 0.5431400282885431,
        "step": 4224
    },
    {
        "loss": 1.0515,
        "grad_norm": 2.0148003101348877,
        "learning_rate": 8.676348449158969e-05,
        "epoch": 0.5432686125755433,
        "step": 4225
    },
    {
        "loss": 1.9816,
        "grad_norm": 2.1414356231689453,
        "learning_rate": 8.672336769681481e-05,
        "epoch": 0.5433971968625434,
        "step": 4226
    },
    {
        "loss": 2.043,
        "grad_norm": 1.9668266773223877,
        "learning_rate": 8.668325307694542e-05,
        "epoch": 0.5435257811495435,
        "step": 4227
    },
    {
        "loss": 1.49,
        "grad_norm": 3.527371406555176,
        "learning_rate": 8.664314063855286e-05,
        "epoch": 0.5436543654365437,
        "step": 4228
    },
    {
        "loss": 1.9322,
        "grad_norm": 1.8102526664733887,
        "learning_rate": 8.660303038820818e-05,
        "epoch": 0.5437829497235438,
        "step": 4229
    },
    {
        "loss": 1.815,
        "grad_norm": 2.070383310317993,
        "learning_rate": 8.656292233248196e-05,
        "epoch": 0.5439115340105439,
        "step": 4230
    },
    {
        "loss": 1.772,
        "grad_norm": 2.5663950443267822,
        "learning_rate": 8.652281647794456e-05,
        "epoch": 0.5440401182975441,
        "step": 4231
    },
    {
        "loss": 0.7244,
        "grad_norm": 2.4646761417388916,
        "learning_rate": 8.648271283116584e-05,
        "epoch": 0.5441687025845442,
        "step": 4232
    },
    {
        "loss": 1.8211,
        "grad_norm": 2.1104233264923096,
        "learning_rate": 8.644261139871536e-05,
        "epoch": 0.5442972868715443,
        "step": 4233
    },
    {
        "loss": 1.3253,
        "grad_norm": 2.349169969558716,
        "learning_rate": 8.640251218716238e-05,
        "epoch": 0.5444258711585445,
        "step": 4234
    },
    {
        "loss": 1.9828,
        "grad_norm": 2.41648268699646,
        "learning_rate": 8.636241520307564e-05,
        "epoch": 0.5445544554455446,
        "step": 4235
    },
    {
        "loss": 1.4066,
        "grad_norm": 3.0394175052642822,
        "learning_rate": 8.632232045302373e-05,
        "epoch": 0.5446830397325447,
        "step": 4236
    },
    {
        "loss": 1.7233,
        "grad_norm": 2.690432071685791,
        "learning_rate": 8.628222794357466e-05,
        "epoch": 0.5448116240195449,
        "step": 4237
    },
    {
        "loss": 2.4098,
        "grad_norm": 1.0266224145889282,
        "learning_rate": 8.624213768129615e-05,
        "epoch": 0.544940208306545,
        "step": 4238
    },
    {
        "loss": 2.2186,
        "grad_norm": 1.6353155374526978,
        "learning_rate": 8.620204967275568e-05,
        "epoch": 0.545068792593545,
        "step": 4239
    },
    {
        "loss": 2.0336,
        "grad_norm": 1.0749331712722778,
        "learning_rate": 8.616196392452011e-05,
        "epoch": 0.5451973768805451,
        "step": 4240
    },
    {
        "loss": 2.2571,
        "grad_norm": 1.3874900341033936,
        "learning_rate": 8.612188044315618e-05,
        "epoch": 0.5453259611675453,
        "step": 4241
    },
    {
        "loss": 1.9978,
        "grad_norm": 2.471937894821167,
        "learning_rate": 8.608179923523013e-05,
        "epoch": 0.5454545454545454,
        "step": 4242
    },
    {
        "loss": 1.6987,
        "grad_norm": 2.055323600769043,
        "learning_rate": 8.604172030730775e-05,
        "epoch": 0.5455831297415455,
        "step": 4243
    },
    {
        "loss": 1.5387,
        "grad_norm": 1.8171970844268799,
        "learning_rate": 8.600164366595469e-05,
        "epoch": 0.5457117140285457,
        "step": 4244
    },
    {
        "loss": 1.8116,
        "grad_norm": 1.6671017408370972,
        "learning_rate": 8.596156931773595e-05,
        "epoch": 0.5458402983155458,
        "step": 4245
    },
    {
        "loss": 1.1812,
        "grad_norm": 2.4115986824035645,
        "learning_rate": 8.59214972692164e-05,
        "epoch": 0.545968882602546,
        "step": 4246
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.9313594102859497,
        "learning_rate": 8.588142752696041e-05,
        "epoch": 0.5460974668895461,
        "step": 4247
    },
    {
        "loss": 2.0196,
        "grad_norm": 2.0006768703460693,
        "learning_rate": 8.584136009753189e-05,
        "epoch": 0.5462260511765462,
        "step": 4248
    },
    {
        "loss": 2.0557,
        "grad_norm": 2.3172054290771484,
        "learning_rate": 8.580129498749458e-05,
        "epoch": 0.5463546354635463,
        "step": 4249
    },
    {
        "loss": 1.91,
        "grad_norm": 1.0669453144073486,
        "learning_rate": 8.576123220341168e-05,
        "epoch": 0.5464832197505465,
        "step": 4250
    },
    {
        "loss": 1.7797,
        "grad_norm": 1.8875668048858643,
        "learning_rate": 8.572117175184608e-05,
        "epoch": 0.5466118040375466,
        "step": 4251
    },
    {
        "loss": 1.8006,
        "grad_norm": 1.7166391611099243,
        "learning_rate": 8.568111363936025e-05,
        "epoch": 0.5467403883245467,
        "step": 4252
    },
    {
        "loss": 1.3782,
        "grad_norm": 2.4605910778045654,
        "learning_rate": 8.564105787251626e-05,
        "epoch": 0.5468689726115469,
        "step": 4253
    },
    {
        "loss": 2.3068,
        "grad_norm": 1.9626752138137817,
        "learning_rate": 8.560100445787586e-05,
        "epoch": 0.546997556898547,
        "step": 4254
    },
    {
        "loss": 1.2398,
        "grad_norm": 1.8609737157821655,
        "learning_rate": 8.556095340200039e-05,
        "epoch": 0.5471261411855471,
        "step": 4255
    },
    {
        "loss": 1.1405,
        "grad_norm": 1.376222848892212,
        "learning_rate": 8.552090471145078e-05,
        "epoch": 0.5472547254725473,
        "step": 4256
    },
    {
        "loss": 2.091,
        "grad_norm": 2.6999526023864746,
        "learning_rate": 8.54808583927876e-05,
        "epoch": 0.5473833097595474,
        "step": 4257
    },
    {
        "loss": 1.8652,
        "grad_norm": 1.840236783027649,
        "learning_rate": 8.5440814452571e-05,
        "epoch": 0.5475118940465475,
        "step": 4258
    },
    {
        "loss": 1.4041,
        "grad_norm": 2.040288209915161,
        "learning_rate": 8.540077289736078e-05,
        "epoch": 0.5476404783335477,
        "step": 4259
    },
    {
        "loss": 1.6577,
        "grad_norm": 2.0136682987213135,
        "learning_rate": 8.53607337337163e-05,
        "epoch": 0.5477690626205478,
        "step": 4260
    },
    {
        "loss": 2.1167,
        "grad_norm": 1.9329910278320312,
        "learning_rate": 8.53206969681966e-05,
        "epoch": 0.5478976469075479,
        "step": 4261
    },
    {
        "loss": 1.4886,
        "grad_norm": 2.261568069458008,
        "learning_rate": 8.528066260736028e-05,
        "epoch": 0.5480262311945481,
        "step": 4262
    },
    {
        "loss": 1.6947,
        "grad_norm": 2.8549575805664062,
        "learning_rate": 8.524063065776549e-05,
        "epoch": 0.5481548154815482,
        "step": 4263
    },
    {
        "loss": 2.2458,
        "grad_norm": 2.1163246631622314,
        "learning_rate": 8.52006011259701e-05,
        "epoch": 0.5482833997685482,
        "step": 4264
    },
    {
        "loss": 2.5149,
        "grad_norm": 1.1102280616760254,
        "learning_rate": 8.516057401853152e-05,
        "epoch": 0.5484119840555484,
        "step": 4265
    },
    {
        "loss": 1.2234,
        "grad_norm": 2.3412208557128906,
        "learning_rate": 8.512054934200678e-05,
        "epoch": 0.5485405683425485,
        "step": 4266
    },
    {
        "loss": 1.8001,
        "grad_norm": 1.7273504734039307,
        "learning_rate": 8.508052710295252e-05,
        "epoch": 0.5486691526295486,
        "step": 4267
    },
    {
        "loss": 1.8705,
        "grad_norm": 2.2727372646331787,
        "learning_rate": 8.504050730792487e-05,
        "epoch": 0.5487977369165488,
        "step": 4268
    },
    {
        "loss": 2.2427,
        "grad_norm": 2.331413745880127,
        "learning_rate": 8.500048996347979e-05,
        "epoch": 0.5489263212035489,
        "step": 4269
    },
    {
        "loss": 1.1873,
        "grad_norm": 2.1632297039031982,
        "learning_rate": 8.49604750761726e-05,
        "epoch": 0.549054905490549,
        "step": 4270
    },
    {
        "loss": 2.0932,
        "grad_norm": 2.759134531021118,
        "learning_rate": 8.492046265255837e-05,
        "epoch": 0.5491834897775492,
        "step": 4271
    },
    {
        "loss": 1.612,
        "grad_norm": 3.262948989868164,
        "learning_rate": 8.488045269919169e-05,
        "epoch": 0.5493120740645493,
        "step": 4272
    },
    {
        "loss": 1.4207,
        "grad_norm": 2.0577592849731445,
        "learning_rate": 8.484044522262681e-05,
        "epoch": 0.5494406583515494,
        "step": 4273
    },
    {
        "loss": 2.3217,
        "grad_norm": 1.758614420890808,
        "learning_rate": 8.480044022941752e-05,
        "epoch": 0.5495692426385496,
        "step": 4274
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.581430435180664,
        "learning_rate": 8.476043772611719e-05,
        "epoch": 0.5496978269255497,
        "step": 4275
    },
    {
        "loss": 1.6742,
        "grad_norm": 1.6189639568328857,
        "learning_rate": 8.472043771927886e-05,
        "epoch": 0.5498264112125498,
        "step": 4276
    },
    {
        "loss": 1.7679,
        "grad_norm": 1.5882282257080078,
        "learning_rate": 8.468044021545504e-05,
        "epoch": 0.54995499549955,
        "step": 4277
    },
    {
        "loss": 1.8769,
        "grad_norm": 2.210195779800415,
        "learning_rate": 8.464044522119802e-05,
        "epoch": 0.5500835797865501,
        "step": 4278
    },
    {
        "loss": 1.8116,
        "grad_norm": 1.9300308227539062,
        "learning_rate": 8.460045274305947e-05,
        "epoch": 0.5502121640735502,
        "step": 4279
    },
    {
        "loss": 1.7642,
        "grad_norm": 1.973201870918274,
        "learning_rate": 8.456046278759071e-05,
        "epoch": 0.5503407483605504,
        "step": 4280
    },
    {
        "loss": 1.7469,
        "grad_norm": 1.583729863166809,
        "learning_rate": 8.452047536134282e-05,
        "epoch": 0.5504693326475505,
        "step": 4281
    },
    {
        "loss": 1.1987,
        "grad_norm": 2.4988481998443604,
        "learning_rate": 8.448049047086614e-05,
        "epoch": 0.5505979169345506,
        "step": 4282
    },
    {
        "loss": 1.3271,
        "grad_norm": 1.8557143211364746,
        "learning_rate": 8.444050812271094e-05,
        "epoch": 0.5507265012215508,
        "step": 4283
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.0542993545532227,
        "learning_rate": 8.440052832342681e-05,
        "epoch": 0.5508550855085509,
        "step": 4284
    },
    {
        "loss": 1.8974,
        "grad_norm": 2.6640336513519287,
        "learning_rate": 8.436055107956302e-05,
        "epoch": 0.550983669795551,
        "step": 4285
    },
    {
        "loss": 1.8708,
        "grad_norm": 1.7980051040649414,
        "learning_rate": 8.432057639766853e-05,
        "epoch": 0.5511122540825512,
        "step": 4286
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.101449728012085,
        "learning_rate": 8.428060428429162e-05,
        "epoch": 0.5512408383695513,
        "step": 4287
    },
    {
        "loss": 1.3277,
        "grad_norm": 1.8689838647842407,
        "learning_rate": 8.424063474598043e-05,
        "epoch": 0.5513694226565514,
        "step": 4288
    },
    {
        "loss": 2.603,
        "grad_norm": 2.9829001426696777,
        "learning_rate": 8.420066778928253e-05,
        "epoch": 0.5514980069435514,
        "step": 4289
    },
    {
        "loss": 1.0151,
        "grad_norm": 1.504346489906311,
        "learning_rate": 8.416070342074499e-05,
        "epoch": 0.5516265912305516,
        "step": 4290
    },
    {
        "loss": 1.8194,
        "grad_norm": 1.876691222190857,
        "learning_rate": 8.412074164691469e-05,
        "epoch": 0.5517551755175517,
        "step": 4291
    },
    {
        "loss": 1.9363,
        "grad_norm": 1.5208585262298584,
        "learning_rate": 8.408078247433784e-05,
        "epoch": 0.5518837598045518,
        "step": 4292
    },
    {
        "loss": 2.0518,
        "grad_norm": 2.8123278617858887,
        "learning_rate": 8.404082590956042e-05,
        "epoch": 0.552012344091552,
        "step": 4293
    },
    {
        "loss": 1.4784,
        "grad_norm": 2.1511685848236084,
        "learning_rate": 8.400087195912786e-05,
        "epoch": 0.5521409283785521,
        "step": 4294
    },
    {
        "loss": 2.0234,
        "grad_norm": 2.622046709060669,
        "learning_rate": 8.396092062958516e-05,
        "epoch": 0.5522695126655522,
        "step": 4295
    },
    {
        "loss": 1.6146,
        "grad_norm": 1.5031800270080566,
        "learning_rate": 8.392097192747701e-05,
        "epoch": 0.5523980969525524,
        "step": 4296
    },
    {
        "loss": 2.1713,
        "grad_norm": 1.8695788383483887,
        "learning_rate": 8.38810258593475e-05,
        "epoch": 0.5525266812395525,
        "step": 4297
    },
    {
        "loss": 0.5911,
        "grad_norm": 1.8202885389328003,
        "learning_rate": 8.384108243174046e-05,
        "epoch": 0.5526552655265526,
        "step": 4298
    },
    {
        "loss": 2.1773,
        "grad_norm": 1.5548765659332275,
        "learning_rate": 8.380114165119917e-05,
        "epoch": 0.5527838498135528,
        "step": 4299
    },
    {
        "loss": 1.4802,
        "grad_norm": 1.9603517055511475,
        "learning_rate": 8.376120352426646e-05,
        "epoch": 0.5529124341005529,
        "step": 4300
    },
    {
        "eval_loss": 1.8473753929138184,
        "eval_runtime": 28.2848,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.5529124341005529,
        "step": 4300
    },
    {
        "loss": 2.0365,
        "grad_norm": 1.6353089809417725,
        "learning_rate": 8.372126805748487e-05,
        "epoch": 0.553041018387553,
        "step": 4301
    },
    {
        "loss": 1.9237,
        "grad_norm": 2.5241434574127197,
        "learning_rate": 8.368133525739634e-05,
        "epoch": 0.5531696026745532,
        "step": 4302
    },
    {
        "loss": 2.0102,
        "grad_norm": 1.8402549028396606,
        "learning_rate": 8.364140513054247e-05,
        "epoch": 0.5532981869615533,
        "step": 4303
    },
    {
        "loss": 2.014,
        "grad_norm": 1.9532667398452759,
        "learning_rate": 8.36014776834644e-05,
        "epoch": 0.5534267712485534,
        "step": 4304
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.1110496520996094,
        "learning_rate": 8.35615529227028e-05,
        "epoch": 0.5535553555355536,
        "step": 4305
    },
    {
        "loss": 1.5439,
        "grad_norm": 2.6824257373809814,
        "learning_rate": 8.352163085479797e-05,
        "epoch": 0.5536839398225537,
        "step": 4306
    },
    {
        "loss": 1.7099,
        "grad_norm": 3.684779167175293,
        "learning_rate": 8.348171148628968e-05,
        "epoch": 0.5538125241095538,
        "step": 4307
    },
    {
        "loss": 2.0629,
        "grad_norm": 1.5468782186508179,
        "learning_rate": 8.344179482371734e-05,
        "epoch": 0.553941108396554,
        "step": 4308
    },
    {
        "loss": 1.8619,
        "grad_norm": 2.4620635509490967,
        "learning_rate": 8.340188087361988e-05,
        "epoch": 0.5540696926835541,
        "step": 4309
    },
    {
        "loss": 1.4183,
        "grad_norm": 2.774937152862549,
        "learning_rate": 8.336196964253573e-05,
        "epoch": 0.5541982769705542,
        "step": 4310
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.6308414936065674,
        "learning_rate": 8.3322061137003e-05,
        "epoch": 0.5543268612575544,
        "step": 4311
    },
    {
        "loss": 1.9064,
        "grad_norm": 1.9976385831832886,
        "learning_rate": 8.328215536355922e-05,
        "epoch": 0.5544554455445545,
        "step": 4312
    },
    {
        "loss": 2.525,
        "grad_norm": 2.613858461380005,
        "learning_rate": 8.324225232874161e-05,
        "epoch": 0.5545840298315546,
        "step": 4313
    },
    {
        "loss": 2.1777,
        "grad_norm": 2.013014316558838,
        "learning_rate": 8.320235203908681e-05,
        "epoch": 0.5547126141185547,
        "step": 4314
    },
    {
        "loss": 1.8451,
        "grad_norm": 2.1293301582336426,
        "learning_rate": 8.316245450113107e-05,
        "epoch": 0.5548411984055548,
        "step": 4315
    },
    {
        "loss": 1.4129,
        "grad_norm": 2.4804251194000244,
        "learning_rate": 8.312255972141023e-05,
        "epoch": 0.5549697826925549,
        "step": 4316
    },
    {
        "loss": 1.7272,
        "grad_norm": 1.9868494272232056,
        "learning_rate": 8.308266770645958e-05,
        "epoch": 0.5550983669795551,
        "step": 4317
    },
    {
        "loss": 2.1374,
        "grad_norm": 1.6525386571884155,
        "learning_rate": 8.304277846281406e-05,
        "epoch": 0.5552269512665552,
        "step": 4318
    },
    {
        "loss": 1.7214,
        "grad_norm": 2.521580934524536,
        "learning_rate": 8.300289199700809e-05,
        "epoch": 0.5553555355535553,
        "step": 4319
    },
    {
        "loss": 1.9441,
        "grad_norm": 1.3820878267288208,
        "learning_rate": 8.296300831557564e-05,
        "epoch": 0.5554841198405555,
        "step": 4320
    },
    {
        "loss": 1.7355,
        "grad_norm": 2.468276262283325,
        "learning_rate": 8.292312742505027e-05,
        "epoch": 0.5556127041275556,
        "step": 4321
    },
    {
        "loss": 1.8756,
        "grad_norm": 1.7980644702911377,
        "learning_rate": 8.288324933196499e-05,
        "epoch": 0.5557412884145557,
        "step": 4322
    },
    {
        "loss": 1.9565,
        "grad_norm": 1.5571023225784302,
        "learning_rate": 8.284337404285247e-05,
        "epoch": 0.5558698727015559,
        "step": 4323
    },
    {
        "loss": 1.3833,
        "grad_norm": 1.5243300199508667,
        "learning_rate": 8.280350156424481e-05,
        "epoch": 0.555998456988556,
        "step": 4324
    },
    {
        "loss": 2.4865,
        "grad_norm": 2.1734249591827393,
        "learning_rate": 8.27636319026738e-05,
        "epoch": 0.5561270412755561,
        "step": 4325
    },
    {
        "loss": 2.4144,
        "grad_norm": 1.909640908241272,
        "learning_rate": 8.272376506467059e-05,
        "epoch": 0.5562556255625563,
        "step": 4326
    },
    {
        "loss": 1.9072,
        "grad_norm": 2.054034948348999,
        "learning_rate": 8.268390105676591e-05,
        "epoch": 0.5563842098495564,
        "step": 4327
    },
    {
        "loss": 1.6867,
        "grad_norm": 2.6772282123565674,
        "learning_rate": 8.264403988549019e-05,
        "epoch": 0.5565127941365565,
        "step": 4328
    },
    {
        "loss": 2.1638,
        "grad_norm": 1.4949133396148682,
        "learning_rate": 8.260418155737312e-05,
        "epoch": 0.5566413784235567,
        "step": 4329
    },
    {
        "loss": 1.4646,
        "grad_norm": 1.9455441236495972,
        "learning_rate": 8.25643260789442e-05,
        "epoch": 0.5567699627105568,
        "step": 4330
    },
    {
        "loss": 1.519,
        "grad_norm": 2.7426745891571045,
        "learning_rate": 8.252447345673233e-05,
        "epoch": 0.5568985469975569,
        "step": 4331
    },
    {
        "loss": 1.1703,
        "grad_norm": 2.40993070602417,
        "learning_rate": 8.248462369726582e-05,
        "epoch": 0.557027131284557,
        "step": 4332
    },
    {
        "loss": 1.8055,
        "grad_norm": 2.2926387786865234,
        "learning_rate": 8.24447768070728e-05,
        "epoch": 0.5571557155715572,
        "step": 4333
    },
    {
        "loss": 1.5518,
        "grad_norm": 1.9952878952026367,
        "learning_rate": 8.240493279268063e-05,
        "epoch": 0.5572842998585573,
        "step": 4334
    },
    {
        "loss": 1.5548,
        "grad_norm": 2.5458076000213623,
        "learning_rate": 8.236509166061645e-05,
        "epoch": 0.5574128841455575,
        "step": 4335
    },
    {
        "loss": 1.8188,
        "grad_norm": 1.662261724472046,
        "learning_rate": 8.232525341740679e-05,
        "epoch": 0.5575414684325576,
        "step": 4336
    },
    {
        "loss": 1.7826,
        "grad_norm": 1.462350606918335,
        "learning_rate": 8.228541806957766e-05,
        "epoch": 0.5576700527195577,
        "step": 4337
    },
    {
        "loss": 2.177,
        "grad_norm": 2.6253695487976074,
        "learning_rate": 8.224558562365476e-05,
        "epoch": 0.5577986370065579,
        "step": 4338
    },
    {
        "loss": 1.9705,
        "grad_norm": 2.3484387397766113,
        "learning_rate": 8.220575608616319e-05,
        "epoch": 0.5579272212935579,
        "step": 4339
    },
    {
        "loss": 1.3568,
        "grad_norm": 1.7305216789245605,
        "learning_rate": 8.216592946362763e-05,
        "epoch": 0.558055805580558,
        "step": 4340
    },
    {
        "loss": 1.3838,
        "grad_norm": 2.2376952171325684,
        "learning_rate": 8.212610576257221e-05,
        "epoch": 0.5581843898675581,
        "step": 4341
    },
    {
        "loss": 1.9737,
        "grad_norm": 1.5745275020599365,
        "learning_rate": 8.208628498952067e-05,
        "epoch": 0.5583129741545583,
        "step": 4342
    },
    {
        "loss": 1.657,
        "grad_norm": 2.5612008571624756,
        "learning_rate": 8.204646715099622e-05,
        "epoch": 0.5584415584415584,
        "step": 4343
    },
    {
        "loss": 1.8428,
        "grad_norm": 1.911685824394226,
        "learning_rate": 8.20066522535216e-05,
        "epoch": 0.5585701427285585,
        "step": 4344
    },
    {
        "loss": 2.1441,
        "grad_norm": 1.318566918373108,
        "learning_rate": 8.196684030361908e-05,
        "epoch": 0.5586987270155587,
        "step": 4345
    },
    {
        "loss": 1.575,
        "grad_norm": 1.7814074754714966,
        "learning_rate": 8.192703130781045e-05,
        "epoch": 0.5588273113025588,
        "step": 4346
    },
    {
        "loss": 1.9125,
        "grad_norm": 2.026191473007202,
        "learning_rate": 8.188722527261694e-05,
        "epoch": 0.5589558955895589,
        "step": 4347
    },
    {
        "loss": 1.4479,
        "grad_norm": 1.0671550035476685,
        "learning_rate": 8.184742220455943e-05,
        "epoch": 0.5590844798765591,
        "step": 4348
    },
    {
        "loss": 1.9943,
        "grad_norm": 1.7881335020065308,
        "learning_rate": 8.180762211015819e-05,
        "epoch": 0.5592130641635592,
        "step": 4349
    },
    {
        "loss": 1.6103,
        "grad_norm": 2.2890546321868896,
        "learning_rate": 8.176782499593309e-05,
        "epoch": 0.5593416484505593,
        "step": 4350
    },
    {
        "loss": 2.1023,
        "grad_norm": 1.8658124208450317,
        "learning_rate": 8.172803086840347e-05,
        "epoch": 0.5594702327375595,
        "step": 4351
    },
    {
        "loss": 1.598,
        "grad_norm": 1.7621102333068848,
        "learning_rate": 8.168823973408815e-05,
        "epoch": 0.5595988170245596,
        "step": 4352
    },
    {
        "loss": 1.122,
        "grad_norm": 1.7279001474380493,
        "learning_rate": 8.164845159950555e-05,
        "epoch": 0.5597274013115597,
        "step": 4353
    },
    {
        "loss": 1.2759,
        "grad_norm": 1.5092791318893433,
        "learning_rate": 8.16086664711735e-05,
        "epoch": 0.5598559855985599,
        "step": 4354
    },
    {
        "loss": 1.7971,
        "grad_norm": 1.492800235748291,
        "learning_rate": 8.156888435560943e-05,
        "epoch": 0.55998456988556,
        "step": 4355
    },
    {
        "loss": 2.206,
        "grad_norm": 1.4105584621429443,
        "learning_rate": 8.15291052593302e-05,
        "epoch": 0.5601131541725601,
        "step": 4356
    },
    {
        "loss": 1.441,
        "grad_norm": 2.007643461227417,
        "learning_rate": 8.148932918885217e-05,
        "epoch": 0.5602417384595603,
        "step": 4357
    },
    {
        "loss": 2.5046,
        "grad_norm": 1.8905071020126343,
        "learning_rate": 8.144955615069131e-05,
        "epoch": 0.5603703227465604,
        "step": 4358
    },
    {
        "loss": 1.9293,
        "grad_norm": 1.6842600107192993,
        "learning_rate": 8.140978615136297e-05,
        "epoch": 0.5604989070335605,
        "step": 4359
    },
    {
        "loss": 1.8683,
        "grad_norm": 2.3198130130767822,
        "learning_rate": 8.137001919738209e-05,
        "epoch": 0.5606274913205607,
        "step": 4360
    },
    {
        "loss": 2.4304,
        "grad_norm": 2.2479097843170166,
        "learning_rate": 8.133025529526306e-05,
        "epoch": 0.5607560756075608,
        "step": 4361
    },
    {
        "loss": 2.2174,
        "grad_norm": 2.8600614070892334,
        "learning_rate": 8.129049445151976e-05,
        "epoch": 0.5608846598945609,
        "step": 4362
    },
    {
        "loss": 1.911,
        "grad_norm": 3.3530044555664062,
        "learning_rate": 8.125073667266563e-05,
        "epoch": 0.5610132441815611,
        "step": 4363
    },
    {
        "loss": 2.1254,
        "grad_norm": 1.3888975381851196,
        "learning_rate": 8.121098196521355e-05,
        "epoch": 0.5611418284685611,
        "step": 4364
    },
    {
        "loss": 2.2164,
        "grad_norm": 1.7019379138946533,
        "learning_rate": 8.117123033567595e-05,
        "epoch": 0.5612704127555612,
        "step": 4365
    },
    {
        "loss": 1.213,
        "grad_norm": 2.279186964035034,
        "learning_rate": 8.11314817905647e-05,
        "epoch": 0.5613989970425614,
        "step": 4366
    },
    {
        "loss": 1.9298,
        "grad_norm": 1.5035086870193481,
        "learning_rate": 8.109173633639118e-05,
        "epoch": 0.5615275813295615,
        "step": 4367
    },
    {
        "loss": 1.0549,
        "grad_norm": 2.611517906188965,
        "learning_rate": 8.105199397966631e-05,
        "epoch": 0.5616561656165616,
        "step": 4368
    },
    {
        "loss": 1.8087,
        "grad_norm": 1.8642914295196533,
        "learning_rate": 8.10122547269004e-05,
        "epoch": 0.5617847499035618,
        "step": 4369
    },
    {
        "loss": 1.9034,
        "grad_norm": 2.264258623123169,
        "learning_rate": 8.097251858460342e-05,
        "epoch": 0.5619133341905619,
        "step": 4370
    },
    {
        "loss": 1.3083,
        "grad_norm": 1.5298638343811035,
        "learning_rate": 8.09327855592846e-05,
        "epoch": 0.562041918477562,
        "step": 4371
    },
    {
        "loss": 2.5819,
        "grad_norm": 2.049593687057495,
        "learning_rate": 8.089305565745296e-05,
        "epoch": 0.5621705027645622,
        "step": 4372
    },
    {
        "loss": 1.17,
        "grad_norm": 1.553805947303772,
        "learning_rate": 8.085332888561668e-05,
        "epoch": 0.5622990870515623,
        "step": 4373
    },
    {
        "loss": 1.25,
        "grad_norm": 0.8323317170143127,
        "learning_rate": 8.081360525028358e-05,
        "epoch": 0.5624276713385624,
        "step": 4374
    },
    {
        "loss": 2.2813,
        "grad_norm": 2.3202195167541504,
        "learning_rate": 8.077388475796113e-05,
        "epoch": 0.5625562556255626,
        "step": 4375
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.7665361166000366,
        "learning_rate": 8.073416741515592e-05,
        "epoch": 0.5626848399125627,
        "step": 4376
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.3211709260940552,
        "learning_rate": 8.06944532283744e-05,
        "epoch": 0.5628134241995628,
        "step": 4377
    },
    {
        "loss": 2.2813,
        "grad_norm": 1.3786686658859253,
        "learning_rate": 8.065474220412226e-05,
        "epoch": 0.562942008486563,
        "step": 4378
    },
    {
        "loss": 2.3323,
        "grad_norm": 1.6301175355911255,
        "learning_rate": 8.061503434890469e-05,
        "epoch": 0.5630705927735631,
        "step": 4379
    },
    {
        "loss": 1.7545,
        "grad_norm": 2.3604736328125,
        "learning_rate": 8.057532966922654e-05,
        "epoch": 0.5631991770605632,
        "step": 4380
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.4390711784362793,
        "learning_rate": 8.053562817159187e-05,
        "epoch": 0.5633277613475633,
        "step": 4381
    },
    {
        "loss": 1.1814,
        "grad_norm": 2.5665314197540283,
        "learning_rate": 8.049592986250448e-05,
        "epoch": 0.5634563456345635,
        "step": 4382
    },
    {
        "loss": 1.2857,
        "grad_norm": 2.253460645675659,
        "learning_rate": 8.045623474846751e-05,
        "epoch": 0.5635849299215636,
        "step": 4383
    },
    {
        "loss": 1.2277,
        "grad_norm": 2.224593162536621,
        "learning_rate": 8.04165428359835e-05,
        "epoch": 0.5637135142085637,
        "step": 4384
    },
    {
        "loss": 1.0222,
        "grad_norm": 2.7794642448425293,
        "learning_rate": 8.037685413155469e-05,
        "epoch": 0.5638420984955639,
        "step": 4385
    },
    {
        "loss": 2.1728,
        "grad_norm": 2.442941904067993,
        "learning_rate": 8.033716864168257e-05,
        "epoch": 0.563970682782564,
        "step": 4386
    },
    {
        "loss": 1.5019,
        "grad_norm": 2.050766706466675,
        "learning_rate": 8.029748637286828e-05,
        "epoch": 0.5640992670695641,
        "step": 4387
    },
    {
        "loss": 1.9848,
        "grad_norm": 1.8884680271148682,
        "learning_rate": 8.025780733161232e-05,
        "epoch": 0.5642278513565643,
        "step": 4388
    },
    {
        "loss": 1.2949,
        "grad_norm": 1.846140742301941,
        "learning_rate": 8.021813152441465e-05,
        "epoch": 0.5643564356435643,
        "step": 4389
    },
    {
        "loss": 1.9084,
        "grad_norm": 2.0724987983703613,
        "learning_rate": 8.017845895777481e-05,
        "epoch": 0.5644850199305644,
        "step": 4390
    },
    {
        "loss": 2.4174,
        "grad_norm": 1.5024096965789795,
        "learning_rate": 8.01387896381917e-05,
        "epoch": 0.5646136042175646,
        "step": 4391
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.3229212760925293,
        "learning_rate": 8.009912357216378e-05,
        "epoch": 0.5647421885045647,
        "step": 4392
    },
    {
        "loss": 1.5021,
        "grad_norm": 1.3402847051620483,
        "learning_rate": 8.005946076618889e-05,
        "epoch": 0.5648707727915648,
        "step": 4393
    },
    {
        "loss": 2.3776,
        "grad_norm": 1.9563682079315186,
        "learning_rate": 8.001980122676436e-05,
        "epoch": 0.564999357078565,
        "step": 4394
    },
    {
        "loss": 2.4393,
        "grad_norm": 1.1781773567199707,
        "learning_rate": 7.998014496038705e-05,
        "epoch": 0.5651279413655651,
        "step": 4395
    },
    {
        "loss": 0.988,
        "grad_norm": 1.8212864398956299,
        "learning_rate": 7.994049197355317e-05,
        "epoch": 0.5652565256525652,
        "step": 4396
    },
    {
        "loss": 0.827,
        "grad_norm": 2.0485146045684814,
        "learning_rate": 7.990084227275853e-05,
        "epoch": 0.5653851099395654,
        "step": 4397
    },
    {
        "loss": 1.8291,
        "grad_norm": 2.1307590007781982,
        "learning_rate": 7.986119586449827e-05,
        "epoch": 0.5655136942265655,
        "step": 4398
    },
    {
        "loss": 2.1495,
        "grad_norm": 1.831559658050537,
        "learning_rate": 7.982155275526706e-05,
        "epoch": 0.5656422785135656,
        "step": 4399
    },
    {
        "loss": 1.1616,
        "grad_norm": 2.319707155227661,
        "learning_rate": 7.978191295155905e-05,
        "epoch": 0.5657708628005658,
        "step": 4400
    },
    {
        "eval_loss": 1.8380126953125,
        "eval_runtime": 28.2795,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.5657708628005658,
        "step": 4400
    },
    {
        "loss": 1.4313,
        "grad_norm": 1.4750005006790161,
        "learning_rate": 7.974227645986776e-05,
        "epoch": 0.5658994470875659,
        "step": 4401
    },
    {
        "loss": 1.4738,
        "grad_norm": 1.5433984994888306,
        "learning_rate": 7.970264328668628e-05,
        "epoch": 0.566028031374566,
        "step": 4402
    },
    {
        "loss": 2.0528,
        "grad_norm": 1.6368821859359741,
        "learning_rate": 7.966301343850707e-05,
        "epoch": 0.5661566156615662,
        "step": 4403
    },
    {
        "loss": 1.7981,
        "grad_norm": 1.2436046600341797,
        "learning_rate": 7.962338692182208e-05,
        "epoch": 0.5662851999485663,
        "step": 4404
    },
    {
        "loss": 2.1255,
        "grad_norm": 1.81831693649292,
        "learning_rate": 7.958376374312271e-05,
        "epoch": 0.5664137842355664,
        "step": 4405
    },
    {
        "loss": 1.3766,
        "grad_norm": 2.000694990158081,
        "learning_rate": 7.95441439088998e-05,
        "epoch": 0.5665423685225666,
        "step": 4406
    },
    {
        "loss": 1.6761,
        "grad_norm": 1.5719757080078125,
        "learning_rate": 7.950452742564369e-05,
        "epoch": 0.5666709528095667,
        "step": 4407
    },
    {
        "loss": 2.3479,
        "grad_norm": 1.659131646156311,
        "learning_rate": 7.946491429984412e-05,
        "epoch": 0.5667995370965668,
        "step": 4408
    },
    {
        "loss": 1.6881,
        "grad_norm": 1.703035831451416,
        "learning_rate": 7.942530453799026e-05,
        "epoch": 0.566928121383567,
        "step": 4409
    },
    {
        "loss": 2.4819,
        "grad_norm": 1.2398734092712402,
        "learning_rate": 7.938569814657083e-05,
        "epoch": 0.5670567056705671,
        "step": 4410
    },
    {
        "loss": 1.9512,
        "grad_norm": 1.8805986642837524,
        "learning_rate": 7.934609513207387e-05,
        "epoch": 0.5671852899575672,
        "step": 4411
    },
    {
        "loss": 1.5508,
        "grad_norm": 1.4246819019317627,
        "learning_rate": 7.930649550098699e-05,
        "epoch": 0.5673138742445674,
        "step": 4412
    },
    {
        "loss": 1.696,
        "grad_norm": 2.4490716457366943,
        "learning_rate": 7.926689925979716e-05,
        "epoch": 0.5674424585315675,
        "step": 4413
    },
    {
        "loss": 2.0518,
        "grad_norm": 2.268798589706421,
        "learning_rate": 7.922730641499077e-05,
        "epoch": 0.5675710428185675,
        "step": 4414
    },
    {
        "loss": 1.5573,
        "grad_norm": 2.2922863960266113,
        "learning_rate": 7.918771697305379e-05,
        "epoch": 0.5676996271055677,
        "step": 4415
    },
    {
        "loss": 2.1428,
        "grad_norm": 1.415526032447815,
        "learning_rate": 7.914813094047146e-05,
        "epoch": 0.5678282113925678,
        "step": 4416
    },
    {
        "loss": 2.0874,
        "grad_norm": 2.3260154724121094,
        "learning_rate": 7.910854832372865e-05,
        "epoch": 0.5679567956795679,
        "step": 4417
    },
    {
        "loss": 2.1134,
        "grad_norm": 1.732640027999878,
        "learning_rate": 7.906896912930951e-05,
        "epoch": 0.568085379966568,
        "step": 4418
    },
    {
        "loss": 1.949,
        "grad_norm": 2.3094282150268555,
        "learning_rate": 7.902939336369766e-05,
        "epoch": 0.5682139642535682,
        "step": 4419
    },
    {
        "loss": 1.616,
        "grad_norm": 3.6539371013641357,
        "learning_rate": 7.898982103337623e-05,
        "epoch": 0.5683425485405683,
        "step": 4420
    },
    {
        "loss": 2.4327,
        "grad_norm": 1.9405806064605713,
        "learning_rate": 7.89502521448277e-05,
        "epoch": 0.5684711328275684,
        "step": 4421
    },
    {
        "loss": 1.399,
        "grad_norm": 2.7092437744140625,
        "learning_rate": 7.891068670453413e-05,
        "epoch": 0.5685997171145686,
        "step": 4422
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.6287977695465088,
        "learning_rate": 7.887112471897675e-05,
        "epoch": 0.5687283014015687,
        "step": 4423
    },
    {
        "loss": 1.491,
        "grad_norm": 2.5516624450683594,
        "learning_rate": 7.883156619463654e-05,
        "epoch": 0.5688568856885688,
        "step": 4424
    },
    {
        "loss": 1.8436,
        "grad_norm": 1.8038091659545898,
        "learning_rate": 7.879201113799373e-05,
        "epoch": 0.568985469975569,
        "step": 4425
    },
    {
        "loss": 1.8706,
        "grad_norm": 2.230107069015503,
        "learning_rate": 7.875245955552793e-05,
        "epoch": 0.5691140542625691,
        "step": 4426
    },
    {
        "loss": 1.3843,
        "grad_norm": 2.37394380569458,
        "learning_rate": 7.871291145371834e-05,
        "epoch": 0.5692426385495692,
        "step": 4427
    },
    {
        "loss": 2.1075,
        "grad_norm": 2.648432970046997,
        "learning_rate": 7.86733668390435e-05,
        "epoch": 0.5693712228365694,
        "step": 4428
    },
    {
        "loss": 1.197,
        "grad_norm": 2.9617955684661865,
        "learning_rate": 7.86338257179814e-05,
        "epoch": 0.5694998071235695,
        "step": 4429
    },
    {
        "loss": 1.8707,
        "grad_norm": 1.3904542922973633,
        "learning_rate": 7.859428809700946e-05,
        "epoch": 0.5696283914105696,
        "step": 4430
    },
    {
        "loss": 1.8662,
        "grad_norm": 1.326860785484314,
        "learning_rate": 7.855475398260444e-05,
        "epoch": 0.5697569756975698,
        "step": 4431
    },
    {
        "loss": 0.8264,
        "grad_norm": 2.338642120361328,
        "learning_rate": 7.851522338124269e-05,
        "epoch": 0.5698855599845699,
        "step": 4432
    },
    {
        "loss": 1.9761,
        "grad_norm": 2.053494930267334,
        "learning_rate": 7.847569629939988e-05,
        "epoch": 0.57001414427157,
        "step": 4433
    },
    {
        "loss": 1.6239,
        "grad_norm": 2.029792070388794,
        "learning_rate": 7.843617274355109e-05,
        "epoch": 0.5701427285585702,
        "step": 4434
    },
    {
        "loss": 1.1165,
        "grad_norm": 3.1201846599578857,
        "learning_rate": 7.839665272017091e-05,
        "epoch": 0.5702713128455703,
        "step": 4435
    },
    {
        "loss": 1.1671,
        "grad_norm": 1.9962000846862793,
        "learning_rate": 7.835713623573321e-05,
        "epoch": 0.5703998971325704,
        "step": 4436
    },
    {
        "loss": 2.4971,
        "grad_norm": 1.5338610410690308,
        "learning_rate": 7.831762329671143e-05,
        "epoch": 0.5705284814195706,
        "step": 4437
    },
    {
        "loss": 2.0168,
        "grad_norm": 1.6467101573944092,
        "learning_rate": 7.827811390957836e-05,
        "epoch": 0.5706570657065707,
        "step": 4438
    },
    {
        "loss": 1.7139,
        "grad_norm": 1.7381362915039062,
        "learning_rate": 7.823860808080619e-05,
        "epoch": 0.5707856499935707,
        "step": 4439
    },
    {
        "loss": 1.9828,
        "grad_norm": 1.5117021799087524,
        "learning_rate": 7.819910581686657e-05,
        "epoch": 0.5709142342805709,
        "step": 4440
    },
    {
        "loss": 1.5793,
        "grad_norm": 1.9715787172317505,
        "learning_rate": 7.815960712423051e-05,
        "epoch": 0.571042818567571,
        "step": 4441
    },
    {
        "loss": 2.0506,
        "grad_norm": 1.966284155845642,
        "learning_rate": 7.812011200936851e-05,
        "epoch": 0.5711714028545711,
        "step": 4442
    },
    {
        "loss": 2.0248,
        "grad_norm": 1.9643170833587646,
        "learning_rate": 7.808062047875041e-05,
        "epoch": 0.5712999871415713,
        "step": 4443
    },
    {
        "loss": 1.2001,
        "grad_norm": 1.4256057739257812,
        "learning_rate": 7.804113253884555e-05,
        "epoch": 0.5714285714285714,
        "step": 4444
    },
    {
        "loss": 2.2135,
        "grad_norm": 1.817277431488037,
        "learning_rate": 7.800164819612258e-05,
        "epoch": 0.5715571557155715,
        "step": 4445
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.9366652965545654,
        "learning_rate": 7.796216745704961e-05,
        "epoch": 0.5716857400025717,
        "step": 4446
    },
    {
        "loss": 1.6295,
        "grad_norm": 1.4231913089752197,
        "learning_rate": 7.792269032809418e-05,
        "epoch": 0.5718143242895718,
        "step": 4447
    },
    {
        "loss": 2.5948,
        "grad_norm": 2.4798333644866943,
        "learning_rate": 7.78832168157232e-05,
        "epoch": 0.5719429085765719,
        "step": 4448
    },
    {
        "loss": 1.4067,
        "grad_norm": 1.5648727416992188,
        "learning_rate": 7.784374692640305e-05,
        "epoch": 0.5720714928635721,
        "step": 4449
    },
    {
        "loss": 1.5039,
        "grad_norm": 2.356510639190674,
        "learning_rate": 7.780428066659943e-05,
        "epoch": 0.5722000771505722,
        "step": 4450
    },
    {
        "loss": 1.3784,
        "grad_norm": 1.3977947235107422,
        "learning_rate": 7.776481804277748e-05,
        "epoch": 0.5723286614375723,
        "step": 4451
    },
    {
        "loss": 1.2419,
        "grad_norm": 2.8967559337615967,
        "learning_rate": 7.772535906140178e-05,
        "epoch": 0.5724572457245725,
        "step": 4452
    },
    {
        "loss": 1.4405,
        "grad_norm": 2.4183948040008545,
        "learning_rate": 7.768590372893626e-05,
        "epoch": 0.5725858300115726,
        "step": 4453
    },
    {
        "loss": 1.6377,
        "grad_norm": 2.788681745529175,
        "learning_rate": 7.76464520518443e-05,
        "epoch": 0.5727144142985727,
        "step": 4454
    },
    {
        "loss": 2.2766,
        "grad_norm": 1.3014963865280151,
        "learning_rate": 7.760700403658867e-05,
        "epoch": 0.5728429985855729,
        "step": 4455
    },
    {
        "loss": 2.2218,
        "grad_norm": 1.793042540550232,
        "learning_rate": 7.756755968963147e-05,
        "epoch": 0.572971582872573,
        "step": 4456
    },
    {
        "loss": 1.7382,
        "grad_norm": 2.634561538696289,
        "learning_rate": 7.752811901743433e-05,
        "epoch": 0.5731001671595731,
        "step": 4457
    },
    {
        "loss": 2.0317,
        "grad_norm": 1.6296247243881226,
        "learning_rate": 7.748868202645814e-05,
        "epoch": 0.5732287514465733,
        "step": 4458
    },
    {
        "loss": 1.8468,
        "grad_norm": 1.457898736000061,
        "learning_rate": 7.74492487231633e-05,
        "epoch": 0.5733573357335734,
        "step": 4459
    },
    {
        "loss": 1.6463,
        "grad_norm": 2.204789161682129,
        "learning_rate": 7.740981911400956e-05,
        "epoch": 0.5734859200205735,
        "step": 4460
    },
    {
        "loss": 1.9528,
        "grad_norm": 1.6696913242340088,
        "learning_rate": 7.737039320545603e-05,
        "epoch": 0.5736145043075737,
        "step": 4461
    },
    {
        "loss": 2.2875,
        "grad_norm": 2.0910532474517822,
        "learning_rate": 7.733097100396126e-05,
        "epoch": 0.5737430885945738,
        "step": 4462
    },
    {
        "loss": 2.1906,
        "grad_norm": 2.3406295776367188,
        "learning_rate": 7.729155251598316e-05,
        "epoch": 0.5738716728815739,
        "step": 4463
    },
    {
        "loss": 2.1587,
        "grad_norm": 1.614045262336731,
        "learning_rate": 7.725213774797913e-05,
        "epoch": 0.574000257168574,
        "step": 4464
    },
    {
        "loss": 2.1005,
        "grad_norm": 1.7292639017105103,
        "learning_rate": 7.721272670640582e-05,
        "epoch": 0.5741288414555741,
        "step": 4465
    },
    {
        "loss": 2.1997,
        "grad_norm": 1.4986904859542847,
        "learning_rate": 7.717331939771929e-05,
        "epoch": 0.5742574257425742,
        "step": 4466
    },
    {
        "loss": 1.0465,
        "grad_norm": 2.481516122817993,
        "learning_rate": 7.713391582837515e-05,
        "epoch": 0.5743860100295743,
        "step": 4467
    },
    {
        "loss": 1.0836,
        "grad_norm": 1.6663718223571777,
        "learning_rate": 7.709451600482813e-05,
        "epoch": 0.5745145943165745,
        "step": 4468
    },
    {
        "loss": 1.497,
        "grad_norm": 1.8959530591964722,
        "learning_rate": 7.705511993353266e-05,
        "epoch": 0.5746431786035746,
        "step": 4469
    },
    {
        "loss": 1.7239,
        "grad_norm": 1.867147445678711,
        "learning_rate": 7.70157276209422e-05,
        "epoch": 0.5747717628905747,
        "step": 4470
    },
    {
        "loss": 1.9768,
        "grad_norm": 2.0799360275268555,
        "learning_rate": 7.697633907350997e-05,
        "epoch": 0.5749003471775749,
        "step": 4471
    },
    {
        "loss": 2.3765,
        "grad_norm": 1.9933972358703613,
        "learning_rate": 7.693695429768829e-05,
        "epoch": 0.575028931464575,
        "step": 4472
    },
    {
        "loss": 1.8659,
        "grad_norm": 2.289794445037842,
        "learning_rate": 7.689757329992893e-05,
        "epoch": 0.5751575157515751,
        "step": 4473
    },
    {
        "loss": 1.9182,
        "grad_norm": 2.3057217597961426,
        "learning_rate": 7.685819608668315e-05,
        "epoch": 0.5752861000385753,
        "step": 4474
    },
    {
        "loss": 1.4438,
        "grad_norm": 1.8220255374908447,
        "learning_rate": 7.681882266440144e-05,
        "epoch": 0.5754146843255754,
        "step": 4475
    },
    {
        "loss": 1.7422,
        "grad_norm": 2.0776524543762207,
        "learning_rate": 7.677945303953382e-05,
        "epoch": 0.5755432686125755,
        "step": 4476
    },
    {
        "loss": 1.7616,
        "grad_norm": 1.6434431076049805,
        "learning_rate": 7.674008721852953e-05,
        "epoch": 0.5756718528995757,
        "step": 4477
    },
    {
        "loss": 2.0544,
        "grad_norm": 2.5176644325256348,
        "learning_rate": 7.670072520783728e-05,
        "epoch": 0.5758004371865758,
        "step": 4478
    },
    {
        "loss": 1.9011,
        "grad_norm": 1.9282557964324951,
        "learning_rate": 7.666136701390518e-05,
        "epoch": 0.5759290214735759,
        "step": 4479
    },
    {
        "loss": 2.4529,
        "grad_norm": 1.624431848526001,
        "learning_rate": 7.662201264318062e-05,
        "epoch": 0.5760576057605761,
        "step": 4480
    },
    {
        "loss": 2.0505,
        "grad_norm": 2.114518880844116,
        "learning_rate": 7.658266210211048e-05,
        "epoch": 0.5761861900475762,
        "step": 4481
    },
    {
        "loss": 1.6311,
        "grad_norm": 2.4090828895568848,
        "learning_rate": 7.65433153971409e-05,
        "epoch": 0.5763147743345763,
        "step": 4482
    },
    {
        "loss": 1.6624,
        "grad_norm": 2.235389232635498,
        "learning_rate": 7.650397253471745e-05,
        "epoch": 0.5764433586215765,
        "step": 4483
    },
    {
        "loss": 1.9434,
        "grad_norm": 1.8488550186157227,
        "learning_rate": 7.64646335212851e-05,
        "epoch": 0.5765719429085766,
        "step": 4484
    },
    {
        "loss": 1.9492,
        "grad_norm": 1.6103203296661377,
        "learning_rate": 7.642529836328808e-05,
        "epoch": 0.5767005271955767,
        "step": 4485
    },
    {
        "loss": 2.7011,
        "grad_norm": 1.7869676351547241,
        "learning_rate": 7.638596706717017e-05,
        "epoch": 0.5768291114825769,
        "step": 4486
    },
    {
        "loss": 2.2495,
        "grad_norm": 2.75986385345459,
        "learning_rate": 7.634663963937431e-05,
        "epoch": 0.576957695769577,
        "step": 4487
    },
    {
        "loss": 1.6576,
        "grad_norm": 2.2586348056793213,
        "learning_rate": 7.630731608634293e-05,
        "epoch": 0.5770862800565771,
        "step": 4488
    },
    {
        "loss": 1.594,
        "grad_norm": 1.6676063537597656,
        "learning_rate": 7.626799641451782e-05,
        "epoch": 0.5772148643435772,
        "step": 4489
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.7047837972640991,
        "learning_rate": 7.62286806303401e-05,
        "epoch": 0.5773434486305773,
        "step": 4490
    },
    {
        "loss": 2.1075,
        "grad_norm": 2.324458599090576,
        "learning_rate": 7.618936874025029e-05,
        "epoch": 0.5774720329175774,
        "step": 4491
    },
    {
        "loss": 2.0333,
        "grad_norm": 1.7114700078964233,
        "learning_rate": 7.615006075068822e-05,
        "epoch": 0.5776006172045776,
        "step": 4492
    },
    {
        "loss": 1.1885,
        "grad_norm": 2.093656539916992,
        "learning_rate": 7.61107566680931e-05,
        "epoch": 0.5777292014915777,
        "step": 4493
    },
    {
        "loss": 2.0617,
        "grad_norm": 2.3890771865844727,
        "learning_rate": 7.607145649890356e-05,
        "epoch": 0.5778577857785778,
        "step": 4494
    },
    {
        "loss": 1.9135,
        "grad_norm": 2.015692949295044,
        "learning_rate": 7.603216024955749e-05,
        "epoch": 0.577986370065578,
        "step": 4495
    },
    {
        "loss": 1.4303,
        "grad_norm": 2.140540361404419,
        "learning_rate": 7.599286792649223e-05,
        "epoch": 0.5781149543525781,
        "step": 4496
    },
    {
        "loss": 1.9862,
        "grad_norm": 1.8665274381637573,
        "learning_rate": 7.59535795361444e-05,
        "epoch": 0.5782435386395782,
        "step": 4497
    },
    {
        "loss": 1.7972,
        "grad_norm": 2.131884813308716,
        "learning_rate": 7.591429508495001e-05,
        "epoch": 0.5783721229265784,
        "step": 4498
    },
    {
        "loss": 2.1425,
        "grad_norm": 2.1219050884246826,
        "learning_rate": 7.587501457934446e-05,
        "epoch": 0.5785007072135785,
        "step": 4499
    },
    {
        "loss": 2.1032,
        "grad_norm": 3.3754544258117676,
        "learning_rate": 7.583573802576243e-05,
        "epoch": 0.5786292915005786,
        "step": 4500
    },
    {
        "eval_loss": 1.8299494981765747,
        "eval_runtime": 28.3167,
        "eval_samples_per_second": 2.79,
        "eval_steps_per_second": 2.79,
        "epoch": 0.5786292915005786,
        "step": 4500
    },
    {
        "loss": 1.9393,
        "grad_norm": 2.591161012649536,
        "learning_rate": 7.579646543063802e-05,
        "epoch": 0.5787578757875788,
        "step": 4501
    },
    {
        "loss": 1.868,
        "grad_norm": 1.9001483917236328,
        "learning_rate": 7.575719680040463e-05,
        "epoch": 0.5788864600745789,
        "step": 4502
    },
    {
        "loss": 1.5388,
        "grad_norm": 1.8858247995376587,
        "learning_rate": 7.571793214149503e-05,
        "epoch": 0.579015044361579,
        "step": 4503
    },
    {
        "loss": 1.6499,
        "grad_norm": 2.0748283863067627,
        "learning_rate": 7.567867146034138e-05,
        "epoch": 0.5791436286485792,
        "step": 4504
    },
    {
        "loss": 1.3101,
        "grad_norm": 2.2589433193206787,
        "learning_rate": 7.563941476337507e-05,
        "epoch": 0.5792722129355793,
        "step": 4505
    },
    {
        "loss": 1.3669,
        "grad_norm": 1.991411566734314,
        "learning_rate": 7.560016205702703e-05,
        "epoch": 0.5794007972225794,
        "step": 4506
    },
    {
        "loss": 1.8918,
        "grad_norm": 1.9721100330352783,
        "learning_rate": 7.556091334772735e-05,
        "epoch": 0.5795293815095796,
        "step": 4507
    },
    {
        "loss": 1.6842,
        "grad_norm": 1.8665494918823242,
        "learning_rate": 7.552166864190554e-05,
        "epoch": 0.5796579657965797,
        "step": 4508
    },
    {
        "loss": 1.7255,
        "grad_norm": 1.9644917249679565,
        "learning_rate": 7.548242794599049e-05,
        "epoch": 0.5797865500835798,
        "step": 4509
    },
    {
        "loss": 1.979,
        "grad_norm": 2.5572540760040283,
        "learning_rate": 7.544319126641033e-05,
        "epoch": 0.57991513437058,
        "step": 4510
    },
    {
        "loss": 2.5605,
        "grad_norm": 1.2276500463485718,
        "learning_rate": 7.54039586095927e-05,
        "epoch": 0.5800437186575801,
        "step": 4511
    },
    {
        "loss": 1.4798,
        "grad_norm": 1.6190085411071777,
        "learning_rate": 7.536472998196442e-05,
        "epoch": 0.5801723029445802,
        "step": 4512
    },
    {
        "loss": 1.6806,
        "grad_norm": 1.7009708881378174,
        "learning_rate": 7.532550538995166e-05,
        "epoch": 0.5803008872315804,
        "step": 4513
    },
    {
        "loss": 2.0322,
        "grad_norm": 1.8870850801467896,
        "learning_rate": 7.52862848399801e-05,
        "epoch": 0.5804294715185804,
        "step": 4514
    },
    {
        "loss": 1.3471,
        "grad_norm": 1.8881007432937622,
        "learning_rate": 7.524706833847451e-05,
        "epoch": 0.5805580558055805,
        "step": 4515
    },
    {
        "loss": 1.6222,
        "grad_norm": 3.0979678630828857,
        "learning_rate": 7.520785589185923e-05,
        "epoch": 0.5806866400925806,
        "step": 4516
    },
    {
        "loss": 1.5197,
        "grad_norm": 1.7210623025894165,
        "learning_rate": 7.51686475065578e-05,
        "epoch": 0.5808152243795808,
        "step": 4517
    },
    {
        "loss": 2.0071,
        "grad_norm": 2.027000904083252,
        "learning_rate": 7.512944318899304e-05,
        "epoch": 0.5809438086665809,
        "step": 4518
    },
    {
        "loss": 2.0165,
        "grad_norm": 1.66985023021698,
        "learning_rate": 7.509024294558731e-05,
        "epoch": 0.581072392953581,
        "step": 4519
    },
    {
        "loss": 1.8911,
        "grad_norm": 2.0401971340179443,
        "learning_rate": 7.505104678276207e-05,
        "epoch": 0.5812009772405812,
        "step": 4520
    },
    {
        "loss": 1.71,
        "grad_norm": 1.4245308637619019,
        "learning_rate": 7.501185470693833e-05,
        "epoch": 0.5813295615275813,
        "step": 4521
    },
    {
        "loss": 2.0526,
        "grad_norm": 1.538983702659607,
        "learning_rate": 7.497266672453622e-05,
        "epoch": 0.5814581458145814,
        "step": 4522
    },
    {
        "loss": 1.7447,
        "grad_norm": 2.4093761444091797,
        "learning_rate": 7.493348284197538e-05,
        "epoch": 0.5815867301015816,
        "step": 4523
    },
    {
        "loss": 1.5566,
        "grad_norm": 1.7678651809692383,
        "learning_rate": 7.489430306567468e-05,
        "epoch": 0.5817153143885817,
        "step": 4524
    },
    {
        "loss": 1.1827,
        "grad_norm": 2.8663535118103027,
        "learning_rate": 7.485512740205229e-05,
        "epoch": 0.5818438986755818,
        "step": 4525
    },
    {
        "loss": 2.4312,
        "grad_norm": 2.1511006355285645,
        "learning_rate": 7.481595585752582e-05,
        "epoch": 0.581972482962582,
        "step": 4526
    },
    {
        "loss": 1.4528,
        "grad_norm": 2.111060857772827,
        "learning_rate": 7.477678843851207e-05,
        "epoch": 0.5821010672495821,
        "step": 4527
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.7698695659637451,
        "learning_rate": 7.473762515142733e-05,
        "epoch": 0.5822296515365822,
        "step": 4528
    },
    {
        "loss": 1.772,
        "grad_norm": 2.0081732273101807,
        "learning_rate": 7.469846600268702e-05,
        "epoch": 0.5823582358235824,
        "step": 4529
    },
    {
        "loss": 1.8231,
        "grad_norm": 2.4193975925445557,
        "learning_rate": 7.465931099870603e-05,
        "epoch": 0.5824868201105825,
        "step": 4530
    },
    {
        "loss": 1.3021,
        "grad_norm": 3.311638832092285,
        "learning_rate": 7.46201601458985e-05,
        "epoch": 0.5826154043975826,
        "step": 4531
    },
    {
        "loss": 2.2312,
        "grad_norm": 1.9818973541259766,
        "learning_rate": 7.458101345067789e-05,
        "epoch": 0.5827439886845828,
        "step": 4532
    },
    {
        "loss": 0.8631,
        "grad_norm": 2.370077133178711,
        "learning_rate": 7.454187091945705e-05,
        "epoch": 0.5828725729715829,
        "step": 4533
    },
    {
        "loss": 1.1244,
        "grad_norm": 2.278660774230957,
        "learning_rate": 7.450273255864809e-05,
        "epoch": 0.583001157258583,
        "step": 4534
    },
    {
        "loss": 1.2898,
        "grad_norm": 1.9132403135299683,
        "learning_rate": 7.446359837466237e-05,
        "epoch": 0.5831297415455832,
        "step": 4535
    },
    {
        "loss": 2.3151,
        "grad_norm": 1.2905054092407227,
        "learning_rate": 7.442446837391073e-05,
        "epoch": 0.5832583258325833,
        "step": 4536
    },
    {
        "loss": 2.1605,
        "grad_norm": 1.566344141960144,
        "learning_rate": 7.438534256280316e-05,
        "epoch": 0.5833869101195834,
        "step": 4537
    },
    {
        "loss": 1.7124,
        "grad_norm": 2.5399246215820312,
        "learning_rate": 7.43462209477491e-05,
        "epoch": 0.5835154944065836,
        "step": 4538
    },
    {
        "loss": 1.9144,
        "grad_norm": 2.2601373195648193,
        "learning_rate": 7.430710353515719e-05,
        "epoch": 0.5836440786935836,
        "step": 4539
    },
    {
        "loss": 1.6987,
        "grad_norm": 2.574971914291382,
        "learning_rate": 7.426799033143545e-05,
        "epoch": 0.5837726629805837,
        "step": 4540
    },
    {
        "loss": 1.1455,
        "grad_norm": 2.2912285327911377,
        "learning_rate": 7.422888134299122e-05,
        "epoch": 0.5839012472675839,
        "step": 4541
    },
    {
        "loss": 1.2944,
        "grad_norm": 1.7113921642303467,
        "learning_rate": 7.418977657623105e-05,
        "epoch": 0.584029831554584,
        "step": 4542
    },
    {
        "loss": 2.2723,
        "grad_norm": 1.2327296733856201,
        "learning_rate": 7.415067603756095e-05,
        "epoch": 0.5841584158415841,
        "step": 4543
    },
    {
        "loss": 1.7193,
        "grad_norm": 2.839137315750122,
        "learning_rate": 7.411157973338612e-05,
        "epoch": 0.5842870001285843,
        "step": 4544
    },
    {
        "loss": 1.9819,
        "grad_norm": 1.8034018278121948,
        "learning_rate": 7.407248767011109e-05,
        "epoch": 0.5844155844155844,
        "step": 4545
    },
    {
        "loss": 1.7574,
        "grad_norm": 2.097355842590332,
        "learning_rate": 7.403339985413972e-05,
        "epoch": 0.5845441687025845,
        "step": 4546
    },
    {
        "loss": 1.8324,
        "grad_norm": 1.4603137969970703,
        "learning_rate": 7.399431629187516e-05,
        "epoch": 0.5846727529895847,
        "step": 4547
    },
    {
        "loss": 0.9929,
        "grad_norm": 1.8398141860961914,
        "learning_rate": 7.395523698971989e-05,
        "epoch": 0.5848013372765848,
        "step": 4548
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.9859436750411987,
        "learning_rate": 7.391616195407564e-05,
        "epoch": 0.5849299215635849,
        "step": 4549
    },
    {
        "loss": 1.4225,
        "grad_norm": 1.5786575078964233,
        "learning_rate": 7.387709119134346e-05,
        "epoch": 0.585058505850585,
        "step": 4550
    },
    {
        "loss": 1.7415,
        "grad_norm": 2.047025203704834,
        "learning_rate": 7.383802470792376e-05,
        "epoch": 0.5851870901375852,
        "step": 4551
    },
    {
        "loss": 0.9662,
        "grad_norm": 1.613903522491455,
        "learning_rate": 7.379896251021612e-05,
        "epoch": 0.5853156744245853,
        "step": 4552
    },
    {
        "loss": 1.987,
        "grad_norm": 2.305711030960083,
        "learning_rate": 7.375990460461959e-05,
        "epoch": 0.5854442587115855,
        "step": 4553
    },
    {
        "loss": 0.9614,
        "grad_norm": 3.9251742362976074,
        "learning_rate": 7.372085099753238e-05,
        "epoch": 0.5855728429985856,
        "step": 4554
    },
    {
        "loss": 1.9996,
        "grad_norm": 1.9673831462860107,
        "learning_rate": 7.368180169535199e-05,
        "epoch": 0.5857014272855857,
        "step": 4555
    },
    {
        "loss": 1.9411,
        "grad_norm": 1.6432245969772339,
        "learning_rate": 7.364275670447534e-05,
        "epoch": 0.5858300115725859,
        "step": 4556
    },
    {
        "loss": 2.1944,
        "grad_norm": 1.2679978609085083,
        "learning_rate": 7.36037160312985e-05,
        "epoch": 0.585958595859586,
        "step": 4557
    },
    {
        "loss": 1.3692,
        "grad_norm": 2.377140998840332,
        "learning_rate": 7.356467968221699e-05,
        "epoch": 0.5860871801465861,
        "step": 4558
    },
    {
        "loss": 1.8323,
        "grad_norm": 1.8008534908294678,
        "learning_rate": 7.352564766362545e-05,
        "epoch": 0.5862157644335863,
        "step": 4559
    },
    {
        "loss": 2.004,
        "grad_norm": 1.2303656339645386,
        "learning_rate": 7.348661998191788e-05,
        "epoch": 0.5863443487205864,
        "step": 4560
    },
    {
        "loss": 2.0751,
        "grad_norm": 1.4108246564865112,
        "learning_rate": 7.34475966434877e-05,
        "epoch": 0.5864729330075865,
        "step": 4561
    },
    {
        "loss": 1.4001,
        "grad_norm": 3.252575397491455,
        "learning_rate": 7.340857765472734e-05,
        "epoch": 0.5866015172945866,
        "step": 4562
    },
    {
        "loss": 1.8581,
        "grad_norm": 1.8357837200164795,
        "learning_rate": 7.336956302202881e-05,
        "epoch": 0.5867301015815868,
        "step": 4563
    },
    {
        "loss": 1.8133,
        "grad_norm": 2.285036563873291,
        "learning_rate": 7.333055275178325e-05,
        "epoch": 0.5868586858685868,
        "step": 4564
    },
    {
        "loss": 2.4283,
        "grad_norm": 1.0745097398757935,
        "learning_rate": 7.329154685038102e-05,
        "epoch": 0.5869872701555869,
        "step": 4565
    },
    {
        "loss": 1.8357,
        "grad_norm": 2.2547779083251953,
        "learning_rate": 7.325254532421197e-05,
        "epoch": 0.5871158544425871,
        "step": 4566
    },
    {
        "loss": 2.1074,
        "grad_norm": 1.4448678493499756,
        "learning_rate": 7.321354817966501e-05,
        "epoch": 0.5872444387295872,
        "step": 4567
    },
    {
        "loss": 2.1309,
        "grad_norm": 1.4736212491989136,
        "learning_rate": 7.317455542312854e-05,
        "epoch": 0.5873730230165873,
        "step": 4568
    },
    {
        "loss": 1.6419,
        "grad_norm": 1.4498238563537598,
        "learning_rate": 7.313556706099008e-05,
        "epoch": 0.5875016073035875,
        "step": 4569
    },
    {
        "loss": 1.4754,
        "grad_norm": 1.7152396440505981,
        "learning_rate": 7.309658309963652e-05,
        "epoch": 0.5876301915905876,
        "step": 4570
    },
    {
        "loss": 1.8985,
        "grad_norm": 2.091001272201538,
        "learning_rate": 7.305760354545397e-05,
        "epoch": 0.5877587758775877,
        "step": 4571
    },
    {
        "loss": 1.7397,
        "grad_norm": 2.596543312072754,
        "learning_rate": 7.301862840482787e-05,
        "epoch": 0.5878873601645879,
        "step": 4572
    },
    {
        "loss": 1.6903,
        "grad_norm": 1.6186405420303345,
        "learning_rate": 7.297965768414293e-05,
        "epoch": 0.588015944451588,
        "step": 4573
    },
    {
        "loss": 1.4444,
        "grad_norm": 2.7178332805633545,
        "learning_rate": 7.294069138978306e-05,
        "epoch": 0.5881445287385881,
        "step": 4574
    },
    {
        "loss": 2.0539,
        "grad_norm": 2.100412607192993,
        "learning_rate": 7.290172952813157e-05,
        "epoch": 0.5882731130255883,
        "step": 4575
    },
    {
        "loss": 1.8365,
        "grad_norm": 1.9762901067733765,
        "learning_rate": 7.286277210557096e-05,
        "epoch": 0.5884016973125884,
        "step": 4576
    },
    {
        "loss": 1.7579,
        "grad_norm": 1.8841032981872559,
        "learning_rate": 7.2823819128483e-05,
        "epoch": 0.5885302815995885,
        "step": 4577
    },
    {
        "loss": 2.3781,
        "grad_norm": 1.557590126991272,
        "learning_rate": 7.27848706032488e-05,
        "epoch": 0.5886588658865887,
        "step": 4578
    },
    {
        "loss": 1.8375,
        "grad_norm": 1.7467384338378906,
        "learning_rate": 7.274592653624863e-05,
        "epoch": 0.5887874501735888,
        "step": 4579
    },
    {
        "loss": 1.7247,
        "grad_norm": 1.9869102239608765,
        "learning_rate": 7.270698693386217e-05,
        "epoch": 0.5889160344605889,
        "step": 4580
    },
    {
        "loss": 1.4218,
        "grad_norm": 1.180187702178955,
        "learning_rate": 7.266805180246826e-05,
        "epoch": 0.5890446187475891,
        "step": 4581
    },
    {
        "loss": 1.4878,
        "grad_norm": 1.9832674264907837,
        "learning_rate": 7.2629121148445e-05,
        "epoch": 0.5891732030345892,
        "step": 4582
    },
    {
        "loss": 2.0617,
        "grad_norm": 1.4621134996414185,
        "learning_rate": 7.259019497816985e-05,
        "epoch": 0.5893017873215893,
        "step": 4583
    },
    {
        "loss": 1.1559,
        "grad_norm": 2.196018695831299,
        "learning_rate": 7.255127329801945e-05,
        "epoch": 0.5894303716085895,
        "step": 4584
    },
    {
        "loss": 1.5108,
        "grad_norm": 2.38472056388855,
        "learning_rate": 7.251235611436978e-05,
        "epoch": 0.5895589558955896,
        "step": 4585
    },
    {
        "loss": 0.9439,
        "grad_norm": 1.9597704410552979,
        "learning_rate": 7.247344343359602e-05,
        "epoch": 0.5896875401825897,
        "step": 4586
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.8051565885543823,
        "learning_rate": 7.243453526207258e-05,
        "epoch": 0.5898161244695899,
        "step": 4587
    },
    {
        "loss": 2.3811,
        "grad_norm": 1.7465530633926392,
        "learning_rate": 7.239563160617329e-05,
        "epoch": 0.58994470875659,
        "step": 4588
    },
    {
        "loss": 1.3575,
        "grad_norm": 1.7045835256576538,
        "learning_rate": 7.235673247227104e-05,
        "epoch": 0.59007329304359,
        "step": 4589
    },
    {
        "loss": 1.4195,
        "grad_norm": 2.048274517059326,
        "learning_rate": 7.231783786673814e-05,
        "epoch": 0.5902018773305902,
        "step": 4590
    },
    {
        "loss": 1.8879,
        "grad_norm": 1.7964316606521606,
        "learning_rate": 7.227894779594604e-05,
        "epoch": 0.5903304616175903,
        "step": 4591
    },
    {
        "loss": 1.8328,
        "grad_norm": 2.748602867126465,
        "learning_rate": 7.224006226626552e-05,
        "epoch": 0.5904590459045904,
        "step": 4592
    },
    {
        "loss": 1.5924,
        "grad_norm": 2.134439468383789,
        "learning_rate": 7.220118128406661e-05,
        "epoch": 0.5905876301915906,
        "step": 4593
    },
    {
        "loss": 1.9167,
        "grad_norm": 2.046523332595825,
        "learning_rate": 7.216230485571855e-05,
        "epoch": 0.5907162144785907,
        "step": 4594
    },
    {
        "loss": 1.4106,
        "grad_norm": 2.642487049102783,
        "learning_rate": 7.212343298758993e-05,
        "epoch": 0.5908447987655908,
        "step": 4595
    },
    {
        "loss": 1.2706,
        "grad_norm": 2.3926055431365967,
        "learning_rate": 7.208456568604844e-05,
        "epoch": 0.590973383052591,
        "step": 4596
    },
    {
        "loss": 1.6804,
        "grad_norm": 1.7544859647750854,
        "learning_rate": 7.204570295746115e-05,
        "epoch": 0.5911019673395911,
        "step": 4597
    },
    {
        "loss": 2.2827,
        "grad_norm": 1.6176600456237793,
        "learning_rate": 7.200684480819435e-05,
        "epoch": 0.5912305516265912,
        "step": 4598
    },
    {
        "loss": 1.7391,
        "grad_norm": 2.163177251815796,
        "learning_rate": 7.196799124461353e-05,
        "epoch": 0.5913591359135913,
        "step": 4599
    },
    {
        "loss": 1.7549,
        "grad_norm": 2.383803367614746,
        "learning_rate": 7.192914227308355e-05,
        "epoch": 0.5914877202005915,
        "step": 4600
    },
    {
        "eval_loss": 1.824770212173462,
        "eval_runtime": 28.2959,
        "eval_samples_per_second": 2.792,
        "eval_steps_per_second": 2.792,
        "epoch": 0.5914877202005915,
        "step": 4600
    },
    {
        "loss": 1.3665,
        "grad_norm": 1.5115625858306885,
        "learning_rate": 7.189029789996837e-05,
        "epoch": 0.5916163044875916,
        "step": 4601
    },
    {
        "loss": 2.5258,
        "grad_norm": 1.5967634916305542,
        "learning_rate": 7.185145813163125e-05,
        "epoch": 0.5917448887745917,
        "step": 4602
    },
    {
        "loss": 1.8631,
        "grad_norm": 1.5748568773269653,
        "learning_rate": 7.181262297443478e-05,
        "epoch": 0.5918734730615919,
        "step": 4603
    },
    {
        "loss": 0.9342,
        "grad_norm": 1.7426965236663818,
        "learning_rate": 7.177379243474062e-05,
        "epoch": 0.592002057348592,
        "step": 4604
    },
    {
        "loss": 1.7394,
        "grad_norm": 3.0126395225524902,
        "learning_rate": 7.173496651890992e-05,
        "epoch": 0.5921306416355921,
        "step": 4605
    },
    {
        "loss": 2.0903,
        "grad_norm": 2.2357842922210693,
        "learning_rate": 7.169614523330281e-05,
        "epoch": 0.5922592259225923,
        "step": 4606
    },
    {
        "loss": 1.52,
        "grad_norm": 2.256040096282959,
        "learning_rate": 7.16573285842788e-05,
        "epoch": 0.5923878102095924,
        "step": 4607
    },
    {
        "loss": 1.5928,
        "grad_norm": 2.8393986225128174,
        "learning_rate": 7.161851657819669e-05,
        "epoch": 0.5925163944965925,
        "step": 4608
    },
    {
        "loss": 2.1733,
        "grad_norm": 1.6938502788543701,
        "learning_rate": 7.157970922141433e-05,
        "epoch": 0.5926449787835927,
        "step": 4609
    },
    {
        "loss": 1.8892,
        "grad_norm": 2.0836570262908936,
        "learning_rate": 7.154090652028905e-05,
        "epoch": 0.5927735630705928,
        "step": 4610
    },
    {
        "loss": 1.6786,
        "grad_norm": 1.8060489892959595,
        "learning_rate": 7.150210848117728e-05,
        "epoch": 0.592902147357593,
        "step": 4611
    },
    {
        "loss": 1.6545,
        "grad_norm": 2.192990779876709,
        "learning_rate": 7.14633151104346e-05,
        "epoch": 0.5930307316445931,
        "step": 4612
    },
    {
        "loss": 1.9456,
        "grad_norm": 1.7404968738555908,
        "learning_rate": 7.142452641441603e-05,
        "epoch": 0.5931593159315932,
        "step": 4613
    },
    {
        "loss": 1.2039,
        "grad_norm": 2.2914586067199707,
        "learning_rate": 7.138574239947571e-05,
        "epoch": 0.5932879002185932,
        "step": 4614
    },
    {
        "loss": 2.2542,
        "grad_norm": 1.8249858617782593,
        "learning_rate": 7.1346963071967e-05,
        "epoch": 0.5934164845055934,
        "step": 4615
    },
    {
        "loss": 2.0765,
        "grad_norm": 1.1333037614822388,
        "learning_rate": 7.130818843824256e-05,
        "epoch": 0.5935450687925935,
        "step": 4616
    },
    {
        "loss": 1.9447,
        "grad_norm": 2.367079973220825,
        "learning_rate": 7.126941850465413e-05,
        "epoch": 0.5936736530795936,
        "step": 4617
    },
    {
        "loss": 2.1475,
        "grad_norm": 2.135221004486084,
        "learning_rate": 7.123065327755291e-05,
        "epoch": 0.5938022373665938,
        "step": 4618
    },
    {
        "loss": 1.7593,
        "grad_norm": 1.7540168762207031,
        "learning_rate": 7.119189276328915e-05,
        "epoch": 0.5939308216535939,
        "step": 4619
    },
    {
        "loss": 1.5644,
        "grad_norm": 1.9348583221435547,
        "learning_rate": 7.115313696821242e-05,
        "epoch": 0.594059405940594,
        "step": 4620
    },
    {
        "loss": 1.0364,
        "grad_norm": 1.8207826614379883,
        "learning_rate": 7.111438589867144e-05,
        "epoch": 0.5941879902275942,
        "step": 4621
    },
    {
        "loss": 1.2454,
        "grad_norm": 1.522713303565979,
        "learning_rate": 7.107563956101424e-05,
        "epoch": 0.5943165745145943,
        "step": 4622
    },
    {
        "loss": 1.5253,
        "grad_norm": 2.207209587097168,
        "learning_rate": 7.103689796158801e-05,
        "epoch": 0.5944451588015944,
        "step": 4623
    },
    {
        "loss": 1.1591,
        "grad_norm": 2.461792230606079,
        "learning_rate": 7.099816110673918e-05,
        "epoch": 0.5945737430885946,
        "step": 4624
    },
    {
        "loss": 2.1111,
        "grad_norm": 1.0745811462402344,
        "learning_rate": 7.095942900281345e-05,
        "epoch": 0.5947023273755947,
        "step": 4625
    },
    {
        "loss": 2.238,
        "grad_norm": 1.5471651554107666,
        "learning_rate": 7.092070165615564e-05,
        "epoch": 0.5948309116625948,
        "step": 4626
    },
    {
        "loss": 2.3436,
        "grad_norm": 1.8987023830413818,
        "learning_rate": 7.088197907310991e-05,
        "epoch": 0.594959495949595,
        "step": 4627
    },
    {
        "loss": 1.5276,
        "grad_norm": 2.39821457862854,
        "learning_rate": 7.084326126001957e-05,
        "epoch": 0.5950880802365951,
        "step": 4628
    },
    {
        "loss": 1.1438,
        "grad_norm": 1.7850111722946167,
        "learning_rate": 7.080454822322712e-05,
        "epoch": 0.5952166645235952,
        "step": 4629
    },
    {
        "loss": 2.1208,
        "grad_norm": 1.7184622287750244,
        "learning_rate": 7.076583996907438e-05,
        "epoch": 0.5953452488105954,
        "step": 4630
    },
    {
        "loss": 1.3651,
        "grad_norm": 2.278228759765625,
        "learning_rate": 7.072713650390226e-05,
        "epoch": 0.5954738330975955,
        "step": 4631
    },
    {
        "loss": 1.6169,
        "grad_norm": 1.804868459701538,
        "learning_rate": 7.068843783405101e-05,
        "epoch": 0.5956024173845956,
        "step": 4632
    },
    {
        "loss": 2.2607,
        "grad_norm": 2.071096420288086,
        "learning_rate": 7.064974396586002e-05,
        "epoch": 0.5957310016715958,
        "step": 4633
    },
    {
        "loss": 2.4358,
        "grad_norm": 1.2484327554702759,
        "learning_rate": 7.061105490566787e-05,
        "epoch": 0.5958595859585959,
        "step": 4634
    },
    {
        "loss": 2.0324,
        "grad_norm": 1.4640990495681763,
        "learning_rate": 7.057237065981243e-05,
        "epoch": 0.595988170245596,
        "step": 4635
    },
    {
        "loss": 2.3887,
        "grad_norm": 1.9114142656326294,
        "learning_rate": 7.053369123463072e-05,
        "epoch": 0.5961167545325962,
        "step": 4636
    },
    {
        "loss": 1.8759,
        "grad_norm": 4.793780326843262,
        "learning_rate": 7.049501663645902e-05,
        "epoch": 0.5962453388195963,
        "step": 4637
    },
    {
        "loss": 0.8601,
        "grad_norm": 2.0219340324401855,
        "learning_rate": 7.045634687163278e-05,
        "epoch": 0.5963739231065964,
        "step": 4638
    },
    {
        "loss": 2.2596,
        "grad_norm": 2.8491551876068115,
        "learning_rate": 7.041768194648665e-05,
        "epoch": 0.5965025073935964,
        "step": 4639
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.176847219467163,
        "learning_rate": 7.037902186735454e-05,
        "epoch": 0.5966310916805966,
        "step": 4640
    },
    {
        "loss": 1.1297,
        "grad_norm": 2.010660171508789,
        "learning_rate": 7.034036664056946e-05,
        "epoch": 0.5967596759675967,
        "step": 4641
    },
    {
        "loss": 2.0039,
        "grad_norm": 2.1244680881500244,
        "learning_rate": 7.030171627246383e-05,
        "epoch": 0.5968882602545968,
        "step": 4642
    },
    {
        "loss": 0.9174,
        "grad_norm": 2.6437060832977295,
        "learning_rate": 7.026307076936904e-05,
        "epoch": 0.597016844541597,
        "step": 4643
    },
    {
        "loss": 1.255,
        "grad_norm": 1.516478180885315,
        "learning_rate": 7.022443013761582e-05,
        "epoch": 0.5971454288285971,
        "step": 4644
    },
    {
        "loss": 1.7334,
        "grad_norm": 2.400770425796509,
        "learning_rate": 7.018579438353406e-05,
        "epoch": 0.5972740131155972,
        "step": 4645
    },
    {
        "loss": 1.1413,
        "grad_norm": 1.7696541547775269,
        "learning_rate": 7.014716351345281e-05,
        "epoch": 0.5974025974025974,
        "step": 4646
    },
    {
        "loss": 1.5484,
        "grad_norm": 1.9119528532028198,
        "learning_rate": 7.010853753370048e-05,
        "epoch": 0.5975311816895975,
        "step": 4647
    },
    {
        "loss": 2.1857,
        "grad_norm": 1.8034213781356812,
        "learning_rate": 7.006991645060447e-05,
        "epoch": 0.5976597659765976,
        "step": 4648
    },
    {
        "loss": 1.9322,
        "grad_norm": 2.0330753326416016,
        "learning_rate": 7.003130027049149e-05,
        "epoch": 0.5977883502635978,
        "step": 4649
    },
    {
        "loss": 1.3101,
        "grad_norm": 3.776862621307373,
        "learning_rate": 6.999268899968752e-05,
        "epoch": 0.5979169345505979,
        "step": 4650
    },
    {
        "loss": 2.0174,
        "grad_norm": 1.5313880443572998,
        "learning_rate": 6.99540826445175e-05,
        "epoch": 0.598045518837598,
        "step": 4651
    },
    {
        "loss": 2.1012,
        "grad_norm": 1.8459031581878662,
        "learning_rate": 6.991548121130584e-05,
        "epoch": 0.5981741031245982,
        "step": 4652
    },
    {
        "loss": 1.8854,
        "grad_norm": 1.9041451215744019,
        "learning_rate": 6.9876884706376e-05,
        "epoch": 0.5983026874115983,
        "step": 4653
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.9438605308532715,
        "learning_rate": 6.983829313605055e-05,
        "epoch": 0.5984312716985984,
        "step": 4654
    },
    {
        "loss": 1.6468,
        "grad_norm": 1.9541761875152588,
        "learning_rate": 6.979970650665147e-05,
        "epoch": 0.5985598559855986,
        "step": 4655
    },
    {
        "loss": 2.1513,
        "grad_norm": 1.7201411724090576,
        "learning_rate": 6.97611248244997e-05,
        "epoch": 0.5986884402725987,
        "step": 4656
    },
    {
        "loss": 1.3332,
        "grad_norm": 1.7537720203399658,
        "learning_rate": 6.972254809591559e-05,
        "epoch": 0.5988170245595988,
        "step": 4657
    },
    {
        "loss": 2.2361,
        "grad_norm": 1.2484149932861328,
        "learning_rate": 6.968397632721852e-05,
        "epoch": 0.598945608846599,
        "step": 4658
    },
    {
        "loss": 2.0438,
        "grad_norm": 1.4235047101974487,
        "learning_rate": 6.964540952472705e-05,
        "epoch": 0.5990741931335991,
        "step": 4659
    },
    {
        "loss": 1.6326,
        "grad_norm": 2.9530954360961914,
        "learning_rate": 6.96068476947591e-05,
        "epoch": 0.5992027774205992,
        "step": 4660
    },
    {
        "loss": 1.4437,
        "grad_norm": 1.9838030338287354,
        "learning_rate": 6.956829084363155e-05,
        "epoch": 0.5993313617075994,
        "step": 4661
    },
    {
        "loss": 2.2972,
        "grad_norm": 1.2273569107055664,
        "learning_rate": 6.952973897766066e-05,
        "epoch": 0.5994599459945995,
        "step": 4662
    },
    {
        "loss": 1.4954,
        "grad_norm": 2.3630926609039307,
        "learning_rate": 6.949119210316174e-05,
        "epoch": 0.5995885302815996,
        "step": 4663
    },
    {
        "loss": 1.5837,
        "grad_norm": 1.4704370498657227,
        "learning_rate": 6.945265022644932e-05,
        "epoch": 0.5997171145685997,
        "step": 4664
    },
    {
        "loss": 1.187,
        "grad_norm": 3.3542962074279785,
        "learning_rate": 6.941411335383715e-05,
        "epoch": 0.5998456988555998,
        "step": 4665
    },
    {
        "loss": 2.0012,
        "grad_norm": 2.55430269241333,
        "learning_rate": 6.93755814916381e-05,
        "epoch": 0.5999742831425999,
        "step": 4666
    },
    {
        "loss": 1.4749,
        "grad_norm": 1.9008421897888184,
        "learning_rate": 6.933705464616429e-05,
        "epoch": 0.6001028674296001,
        "step": 4667
    },
    {
        "loss": 1.415,
        "grad_norm": 1.956099033355713,
        "learning_rate": 6.929853282372694e-05,
        "epoch": 0.6002314517166002,
        "step": 4668
    },
    {
        "loss": 1.766,
        "grad_norm": 2.867316961288452,
        "learning_rate": 6.926001603063649e-05,
        "epoch": 0.6003600360036003,
        "step": 4669
    },
    {
        "loss": 1.79,
        "grad_norm": 1.6921428442001343,
        "learning_rate": 6.922150427320257e-05,
        "epoch": 0.6004886202906005,
        "step": 4670
    },
    {
        "loss": 1.7961,
        "grad_norm": 1.7794229984283447,
        "learning_rate": 6.918299755773394e-05,
        "epoch": 0.6006172045776006,
        "step": 4671
    },
    {
        "loss": 1.1758,
        "grad_norm": 2.0248360633850098,
        "learning_rate": 6.91444958905386e-05,
        "epoch": 0.6007457888646007,
        "step": 4672
    },
    {
        "loss": 1.4364,
        "grad_norm": 2.1134138107299805,
        "learning_rate": 6.910599927792363e-05,
        "epoch": 0.6008743731516009,
        "step": 4673
    },
    {
        "loss": 1.5923,
        "grad_norm": 1.7965257167816162,
        "learning_rate": 6.906750772619539e-05,
        "epoch": 0.601002957438601,
        "step": 4674
    },
    {
        "loss": 2.6967,
        "grad_norm": 2.085127830505371,
        "learning_rate": 6.902902124165933e-05,
        "epoch": 0.6011315417256011,
        "step": 4675
    },
    {
        "loss": 1.7006,
        "grad_norm": 2.3323495388031006,
        "learning_rate": 6.899053983062007e-05,
        "epoch": 0.6012601260126013,
        "step": 4676
    },
    {
        "loss": 1.1993,
        "grad_norm": 2.345612049102783,
        "learning_rate": 6.895206349938148e-05,
        "epoch": 0.6013887102996014,
        "step": 4677
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.652188777923584,
        "learning_rate": 6.891359225424647e-05,
        "epoch": 0.6015172945866015,
        "step": 4678
    },
    {
        "loss": 0.6361,
        "grad_norm": 1.9616347551345825,
        "learning_rate": 6.887512610151727e-05,
        "epoch": 0.6016458788736017,
        "step": 4679
    },
    {
        "loss": 1.8391,
        "grad_norm": 2.289020538330078,
        "learning_rate": 6.883666504749517e-05,
        "epoch": 0.6017744631606018,
        "step": 4680
    },
    {
        "loss": 1.3733,
        "grad_norm": 2.6880171298980713,
        "learning_rate": 6.879820909848061e-05,
        "epoch": 0.6019030474476019,
        "step": 4681
    },
    {
        "loss": 2.4307,
        "grad_norm": 1.6858344078063965,
        "learning_rate": 6.875975826077327e-05,
        "epoch": 0.6020316317346021,
        "step": 4682
    },
    {
        "loss": 1.7239,
        "grad_norm": 2.1309921741485596,
        "learning_rate": 6.872131254067194e-05,
        "epoch": 0.6021602160216022,
        "step": 4683
    },
    {
        "loss": 2.2469,
        "grad_norm": 2.0327606201171875,
        "learning_rate": 6.868287194447461e-05,
        "epoch": 0.6022888003086023,
        "step": 4684
    },
    {
        "loss": 2.0571,
        "grad_norm": 1.4779044389724731,
        "learning_rate": 6.86444364784784e-05,
        "epoch": 0.6024173845956025,
        "step": 4685
    },
    {
        "loss": 2.0616,
        "grad_norm": 2.343611478805542,
        "learning_rate": 6.860600614897959e-05,
        "epoch": 0.6025459688826026,
        "step": 4686
    },
    {
        "loss": 2.1379,
        "grad_norm": 2.369499921798706,
        "learning_rate": 6.856758096227362e-05,
        "epoch": 0.6026745531696027,
        "step": 4687
    },
    {
        "loss": 1.4934,
        "grad_norm": 2.148017644882202,
        "learning_rate": 6.852916092465509e-05,
        "epoch": 0.6028031374566029,
        "step": 4688
    },
    {
        "loss": 2.2262,
        "grad_norm": 1.3159562349319458,
        "learning_rate": 6.849074604241783e-05,
        "epoch": 0.6029317217436029,
        "step": 4689
    },
    {
        "loss": 1.7161,
        "grad_norm": 1.7417712211608887,
        "learning_rate": 6.845233632185469e-05,
        "epoch": 0.603060306030603,
        "step": 4690
    },
    {
        "loss": 1.2105,
        "grad_norm": 2.3839974403381348,
        "learning_rate": 6.84139317692577e-05,
        "epoch": 0.6031888903176031,
        "step": 4691
    },
    {
        "loss": 1.8908,
        "grad_norm": 3.165128469467163,
        "learning_rate": 6.837553239091818e-05,
        "epoch": 0.6033174746046033,
        "step": 4692
    },
    {
        "loss": 1.802,
        "grad_norm": 2.098629951477051,
        "learning_rate": 6.833713819312644e-05,
        "epoch": 0.6034460588916034,
        "step": 4693
    },
    {
        "loss": 1.5966,
        "grad_norm": 2.0246763229370117,
        "learning_rate": 6.829874918217207e-05,
        "epoch": 0.6035746431786035,
        "step": 4694
    },
    {
        "loss": 1.4941,
        "grad_norm": 2.731139898300171,
        "learning_rate": 6.826036536434372e-05,
        "epoch": 0.6037032274656037,
        "step": 4695
    },
    {
        "loss": 1.9424,
        "grad_norm": 2.5708768367767334,
        "learning_rate": 6.822198674592914e-05,
        "epoch": 0.6038318117526038,
        "step": 4696
    },
    {
        "loss": 2.2151,
        "grad_norm": 2.8284125328063965,
        "learning_rate": 6.818361333321545e-05,
        "epoch": 0.6039603960396039,
        "step": 4697
    },
    {
        "loss": 2.7017,
        "grad_norm": 1.1499333381652832,
        "learning_rate": 6.814524513248863e-05,
        "epoch": 0.6040889803266041,
        "step": 4698
    },
    {
        "loss": 1.8167,
        "grad_norm": 3.0598578453063965,
        "learning_rate": 6.810688215003407e-05,
        "epoch": 0.6042175646136042,
        "step": 4699
    },
    {
        "loss": 1.3569,
        "grad_norm": 2.338468551635742,
        "learning_rate": 6.806852439213615e-05,
        "epoch": 0.6043461489006043,
        "step": 4700
    },
    {
        "eval_loss": 1.812483787536621,
        "eval_runtime": 28.2498,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.6043461489006043,
        "step": 4700
    },
    {
        "loss": 2.1731,
        "grad_norm": 2.195314884185791,
        "learning_rate": 6.803017186507835e-05,
        "epoch": 0.6044747331876045,
        "step": 4701
    },
    {
        "loss": 1.1671,
        "grad_norm": 2.6569926738739014,
        "learning_rate": 6.799182457514348e-05,
        "epoch": 0.6046033174746046,
        "step": 4702
    },
    {
        "loss": 2.1285,
        "grad_norm": 2.6561648845672607,
        "learning_rate": 6.795348252861329e-05,
        "epoch": 0.6047319017616047,
        "step": 4703
    },
    {
        "loss": 1.8258,
        "grad_norm": 2.485124349594116,
        "learning_rate": 6.791514573176885e-05,
        "epoch": 0.6048604860486049,
        "step": 4704
    },
    {
        "loss": 2.4848,
        "grad_norm": 1.1185451745986938,
        "learning_rate": 6.787681419089027e-05,
        "epoch": 0.604989070335605,
        "step": 4705
    },
    {
        "loss": 2.0133,
        "grad_norm": 1.2328612804412842,
        "learning_rate": 6.783848791225672e-05,
        "epoch": 0.6051176546226051,
        "step": 4706
    },
    {
        "loss": 1.9372,
        "grad_norm": 1.8166451454162598,
        "learning_rate": 6.780016690214672e-05,
        "epoch": 0.6052462389096053,
        "step": 4707
    },
    {
        "loss": 1.0678,
        "grad_norm": 2.1874091625213623,
        "learning_rate": 6.776185116683773e-05,
        "epoch": 0.6053748231966054,
        "step": 4708
    },
    {
        "loss": 0.9068,
        "grad_norm": 2.695138692855835,
        "learning_rate": 6.772354071260649e-05,
        "epoch": 0.6055034074836055,
        "step": 4709
    },
    {
        "loss": 1.5414,
        "grad_norm": 1.6578562259674072,
        "learning_rate": 6.768523554572878e-05,
        "epoch": 0.6056319917706057,
        "step": 4710
    },
    {
        "loss": 1.2058,
        "grad_norm": 1.6754189729690552,
        "learning_rate": 6.76469356724795e-05,
        "epoch": 0.6057605760576058,
        "step": 4711
    },
    {
        "loss": 1.9521,
        "grad_norm": 2.4706811904907227,
        "learning_rate": 6.760864109913281e-05,
        "epoch": 0.6058891603446059,
        "step": 4712
    },
    {
        "loss": 1.6046,
        "grad_norm": 1.5005695819854736,
        "learning_rate": 6.757035183196183e-05,
        "epoch": 0.6060177446316061,
        "step": 4713
    },
    {
        "loss": 2.2038,
        "grad_norm": 1.483886957168579,
        "learning_rate": 6.753206787723898e-05,
        "epoch": 0.6061463289186061,
        "step": 4714
    },
    {
        "loss": 2.0063,
        "grad_norm": 1.5387345552444458,
        "learning_rate": 6.749378924123568e-05,
        "epoch": 0.6062749132056062,
        "step": 4715
    },
    {
        "loss": 1.5468,
        "grad_norm": 1.916961908340454,
        "learning_rate": 6.745551593022252e-05,
        "epoch": 0.6064034974926064,
        "step": 4716
    },
    {
        "loss": 1.5601,
        "grad_norm": 1.8508970737457275,
        "learning_rate": 6.741724795046929e-05,
        "epoch": 0.6065320817796065,
        "step": 4717
    },
    {
        "loss": 1.9045,
        "grad_norm": 1.9222933053970337,
        "learning_rate": 6.737898530824473e-05,
        "epoch": 0.6066606660666066,
        "step": 4718
    },
    {
        "loss": 1.9888,
        "grad_norm": 1.6156830787658691,
        "learning_rate": 6.734072800981694e-05,
        "epoch": 0.6067892503536068,
        "step": 4719
    },
    {
        "loss": 1.9879,
        "grad_norm": 1.890174388885498,
        "learning_rate": 6.730247606145293e-05,
        "epoch": 0.6069178346406069,
        "step": 4720
    },
    {
        "loss": 0.9078,
        "grad_norm": 1.9835293292999268,
        "learning_rate": 6.726422946941898e-05,
        "epoch": 0.607046418927607,
        "step": 4721
    },
    {
        "loss": 1.5432,
        "grad_norm": 2.016515016555786,
        "learning_rate": 6.722598823998043e-05,
        "epoch": 0.6071750032146072,
        "step": 4722
    },
    {
        "loss": 1.6474,
        "grad_norm": 2.233524799346924,
        "learning_rate": 6.718775237940172e-05,
        "epoch": 0.6073035875016073,
        "step": 4723
    },
    {
        "loss": 2.6197,
        "grad_norm": 1.9712539911270142,
        "learning_rate": 6.714952189394645e-05,
        "epoch": 0.6074321717886074,
        "step": 4724
    },
    {
        "loss": 1.3764,
        "grad_norm": 3.033560276031494,
        "learning_rate": 6.711129678987734e-05,
        "epoch": 0.6075607560756076,
        "step": 4725
    },
    {
        "loss": 1.3863,
        "grad_norm": 2.8229684829711914,
        "learning_rate": 6.707307707345623e-05,
        "epoch": 0.6076893403626077,
        "step": 4726
    },
    {
        "loss": 0.8344,
        "grad_norm": 1.5541696548461914,
        "learning_rate": 6.703486275094406e-05,
        "epoch": 0.6078179246496078,
        "step": 4727
    },
    {
        "loss": 1.4283,
        "grad_norm": 2.4113759994506836,
        "learning_rate": 6.699665382860085e-05,
        "epoch": 0.607946508936608,
        "step": 4728
    },
    {
        "loss": 1.5801,
        "grad_norm": 2.06583833694458,
        "learning_rate": 6.695845031268584e-05,
        "epoch": 0.6080750932236081,
        "step": 4729
    },
    {
        "loss": 1.8949,
        "grad_norm": 2.839200735092163,
        "learning_rate": 6.692025220945726e-05,
        "epoch": 0.6082036775106082,
        "step": 4730
    },
    {
        "loss": 1.4665,
        "grad_norm": 1.6893601417541504,
        "learning_rate": 6.688205952517259e-05,
        "epoch": 0.6083322617976084,
        "step": 4731
    },
    {
        "loss": 1.1871,
        "grad_norm": 2.5676093101501465,
        "learning_rate": 6.684387226608831e-05,
        "epoch": 0.6084608460846085,
        "step": 4732
    },
    {
        "loss": 1.3299,
        "grad_norm": 3.7282097339630127,
        "learning_rate": 6.680569043846e-05,
        "epoch": 0.6085894303716086,
        "step": 4733
    },
    {
        "loss": 1.8819,
        "grad_norm": 1.8795442581176758,
        "learning_rate": 6.676751404854247e-05,
        "epoch": 0.6087180146586088,
        "step": 4734
    },
    {
        "loss": 1.6157,
        "grad_norm": 2.0444130897521973,
        "learning_rate": 6.672934310258949e-05,
        "epoch": 0.6088465989456089,
        "step": 4735
    },
    {
        "loss": 2.3922,
        "grad_norm": 1.2992768287658691,
        "learning_rate": 6.669117760685414e-05,
        "epoch": 0.608975183232609,
        "step": 4736
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.763183355331421,
        "learning_rate": 6.665301756758839e-05,
        "epoch": 0.6091037675196092,
        "step": 4737
    },
    {
        "loss": 1.3867,
        "grad_norm": 2.134899377822876,
        "learning_rate": 6.661486299104338e-05,
        "epoch": 0.6092323518066093,
        "step": 4738
    },
    {
        "loss": 2.0795,
        "grad_norm": 1.9522395133972168,
        "learning_rate": 6.657671388346949e-05,
        "epoch": 0.6093609360936093,
        "step": 4739
    },
    {
        "loss": 0.7159,
        "grad_norm": 2.591714382171631,
        "learning_rate": 6.653857025111599e-05,
        "epoch": 0.6094895203806094,
        "step": 4740
    },
    {
        "loss": 1.9871,
        "grad_norm": 2.2551136016845703,
        "learning_rate": 6.650043210023147e-05,
        "epoch": 0.6096181046676096,
        "step": 4741
    },
    {
        "loss": 1.3381,
        "grad_norm": 2.748426914215088,
        "learning_rate": 6.646229943706344e-05,
        "epoch": 0.6097466889546097,
        "step": 4742
    },
    {
        "loss": 2.4104,
        "grad_norm": 2.1376163959503174,
        "learning_rate": 6.642417226785857e-05,
        "epoch": 0.6098752732416098,
        "step": 4743
    },
    {
        "loss": 1.6401,
        "grad_norm": 2.606630563735962,
        "learning_rate": 6.638605059886273e-05,
        "epoch": 0.61000385752861,
        "step": 4744
    },
    {
        "loss": 2.2299,
        "grad_norm": 1.9322335720062256,
        "learning_rate": 6.634793443632069e-05,
        "epoch": 0.6101324418156101,
        "step": 4745
    },
    {
        "loss": 1.7466,
        "grad_norm": 2.095172882080078,
        "learning_rate": 6.630982378647654e-05,
        "epoch": 0.6102610261026102,
        "step": 4746
    },
    {
        "loss": 2.0836,
        "grad_norm": 2.0735056400299072,
        "learning_rate": 6.627171865557334e-05,
        "epoch": 0.6103896103896104,
        "step": 4747
    },
    {
        "loss": 1.3996,
        "grad_norm": 1.7298369407653809,
        "learning_rate": 6.623361904985316e-05,
        "epoch": 0.6105181946766105,
        "step": 4748
    },
    {
        "loss": 2.6766,
        "grad_norm": 1.0823475122451782,
        "learning_rate": 6.619552497555738e-05,
        "epoch": 0.6106467789636106,
        "step": 4749
    },
    {
        "loss": 1.4604,
        "grad_norm": 2.8605141639709473,
        "learning_rate": 6.615743643892634e-05,
        "epoch": 0.6107753632506108,
        "step": 4750
    },
    {
        "loss": 1.3956,
        "grad_norm": 2.003674030303955,
        "learning_rate": 6.611935344619948e-05,
        "epoch": 0.6109039475376109,
        "step": 4751
    },
    {
        "loss": 1.415,
        "grad_norm": 2.1267242431640625,
        "learning_rate": 6.60812760036154e-05,
        "epoch": 0.611032531824611,
        "step": 4752
    },
    {
        "loss": 1.7158,
        "grad_norm": 3.7364659309387207,
        "learning_rate": 6.60432041174116e-05,
        "epoch": 0.6111611161116112,
        "step": 4753
    },
    {
        "loss": 2.0091,
        "grad_norm": 2.245932102203369,
        "learning_rate": 6.600513779382498e-05,
        "epoch": 0.6112897003986113,
        "step": 4754
    },
    {
        "loss": 2.1443,
        "grad_norm": 1.5473344326019287,
        "learning_rate": 6.596707703909123e-05,
        "epoch": 0.6114182846856114,
        "step": 4755
    },
    {
        "loss": 1.4415,
        "grad_norm": 2.3683159351348877,
        "learning_rate": 6.592902185944534e-05,
        "epoch": 0.6115468689726116,
        "step": 4756
    },
    {
        "loss": 1.8357,
        "grad_norm": 1.5188151597976685,
        "learning_rate": 6.589097226112127e-05,
        "epoch": 0.6116754532596117,
        "step": 4757
    },
    {
        "loss": 2.204,
        "grad_norm": 2.274855136871338,
        "learning_rate": 6.585292825035205e-05,
        "epoch": 0.6118040375466118,
        "step": 4758
    },
    {
        "loss": 2.2367,
        "grad_norm": 1.0635241270065308,
        "learning_rate": 6.581488983336994e-05,
        "epoch": 0.611932621833612,
        "step": 4759
    },
    {
        "loss": 1.267,
        "grad_norm": 1.613296389579773,
        "learning_rate": 6.57768570164061e-05,
        "epoch": 0.6120612061206121,
        "step": 4760
    },
    {
        "loss": 1.9395,
        "grad_norm": 1.6592744588851929,
        "learning_rate": 6.57388298056909e-05,
        "epoch": 0.6121897904076122,
        "step": 4761
    },
    {
        "loss": 1.8376,
        "grad_norm": 2.0534040927886963,
        "learning_rate": 6.570080820745376e-05,
        "epoch": 0.6123183746946124,
        "step": 4762
    },
    {
        "loss": 0.9762,
        "grad_norm": 2.5929553508758545,
        "learning_rate": 6.566279222792312e-05,
        "epoch": 0.6124469589816125,
        "step": 4763
    },
    {
        "loss": 1.5085,
        "grad_norm": 1.7479256391525269,
        "learning_rate": 6.562478187332662e-05,
        "epoch": 0.6125755432686125,
        "step": 4764
    },
    {
        "loss": 1.5433,
        "grad_norm": 2.225400924682617,
        "learning_rate": 6.558677714989083e-05,
        "epoch": 0.6127041275556127,
        "step": 4765
    },
    {
        "loss": 2.0488,
        "grad_norm": 1.9064711332321167,
        "learning_rate": 6.554877806384156e-05,
        "epoch": 0.6128327118426128,
        "step": 4766
    },
    {
        "loss": 1.4215,
        "grad_norm": 1.461276888847351,
        "learning_rate": 6.551078462140356e-05,
        "epoch": 0.6129612961296129,
        "step": 4767
    },
    {
        "loss": 2.4431,
        "grad_norm": 1.9725534915924072,
        "learning_rate": 6.54727968288007e-05,
        "epoch": 0.613089880416613,
        "step": 4768
    },
    {
        "loss": 1.248,
        "grad_norm": 2.0544872283935547,
        "learning_rate": 6.543481469225598e-05,
        "epoch": 0.6132184647036132,
        "step": 4769
    },
    {
        "loss": 2.3291,
        "grad_norm": 1.929936408996582,
        "learning_rate": 6.539683821799139e-05,
        "epoch": 0.6133470489906133,
        "step": 4770
    },
    {
        "loss": 1.9928,
        "grad_norm": 1.192606806755066,
        "learning_rate": 6.535886741222803e-05,
        "epoch": 0.6134756332776135,
        "step": 4771
    },
    {
        "loss": 1.4133,
        "grad_norm": 2.060243606567383,
        "learning_rate": 6.532090228118608e-05,
        "epoch": 0.6136042175646136,
        "step": 4772
    },
    {
        "loss": 1.4218,
        "grad_norm": 2.3471484184265137,
        "learning_rate": 6.528294283108479e-05,
        "epoch": 0.6137328018516137,
        "step": 4773
    },
    {
        "loss": 2.0397,
        "grad_norm": 1.767136812210083,
        "learning_rate": 6.524498906814247e-05,
        "epoch": 0.6138613861386139,
        "step": 4774
    },
    {
        "loss": 1.4593,
        "grad_norm": 1.973618745803833,
        "learning_rate": 6.520704099857647e-05,
        "epoch": 0.613989970425614,
        "step": 4775
    },
    {
        "loss": 1.2899,
        "grad_norm": 2.1849172115325928,
        "learning_rate": 6.516909862860325e-05,
        "epoch": 0.6141185547126141,
        "step": 4776
    },
    {
        "loss": 1.8284,
        "grad_norm": 2.3503143787384033,
        "learning_rate": 6.513116196443832e-05,
        "epoch": 0.6142471389996143,
        "step": 4777
    },
    {
        "loss": 1.7549,
        "grad_norm": 2.3401758670806885,
        "learning_rate": 6.50932310122963e-05,
        "epoch": 0.6143757232866144,
        "step": 4778
    },
    {
        "loss": 1.8672,
        "grad_norm": 1.912718415260315,
        "learning_rate": 6.505530577839079e-05,
        "epoch": 0.6145043075736145,
        "step": 4779
    },
    {
        "loss": 2.2162,
        "grad_norm": 1.5328700542449951,
        "learning_rate": 6.501738626893447e-05,
        "epoch": 0.6146328918606146,
        "step": 4780
    },
    {
        "loss": 1.6752,
        "grad_norm": 2.745542049407959,
        "learning_rate": 6.497947249013917e-05,
        "epoch": 0.6147614761476148,
        "step": 4781
    },
    {
        "loss": 2.0675,
        "grad_norm": 1.9920121431350708,
        "learning_rate": 6.494156444821563e-05,
        "epoch": 0.6148900604346149,
        "step": 4782
    },
    {
        "loss": 2.0298,
        "grad_norm": 1.7678818702697754,
        "learning_rate": 6.490366214937385e-05,
        "epoch": 0.615018644721615,
        "step": 4783
    },
    {
        "loss": 1.7854,
        "grad_norm": 2.2606587409973145,
        "learning_rate": 6.486576559982271e-05,
        "epoch": 0.6151472290086152,
        "step": 4784
    },
    {
        "loss": 1.6113,
        "grad_norm": 2.4254543781280518,
        "learning_rate": 6.482787480577019e-05,
        "epoch": 0.6152758132956153,
        "step": 4785
    },
    {
        "loss": 1.4575,
        "grad_norm": 2.1760637760162354,
        "learning_rate": 6.478998977342346e-05,
        "epoch": 0.6154043975826154,
        "step": 4786
    },
    {
        "loss": 1.1653,
        "grad_norm": 3.343919515609741,
        "learning_rate": 6.475211050898851e-05,
        "epoch": 0.6155329818696156,
        "step": 4787
    },
    {
        "loss": 0.7618,
        "grad_norm": 1.1884464025497437,
        "learning_rate": 6.47142370186706e-05,
        "epoch": 0.6156615661566157,
        "step": 4788
    },
    {
        "loss": 1.6999,
        "grad_norm": 2.195190191268921,
        "learning_rate": 6.467636930867398e-05,
        "epoch": 0.6157901504436157,
        "step": 4789
    },
    {
        "loss": 2.2808,
        "grad_norm": 1.621069073677063,
        "learning_rate": 6.46385073852018e-05,
        "epoch": 0.6159187347306159,
        "step": 4790
    },
    {
        "loss": 1.9515,
        "grad_norm": 3.5060815811157227,
        "learning_rate": 6.460065125445657e-05,
        "epoch": 0.616047319017616,
        "step": 4791
    },
    {
        "loss": 2.2641,
        "grad_norm": 1.706650733947754,
        "learning_rate": 6.456280092263951e-05,
        "epoch": 0.6161759033046161,
        "step": 4792
    },
    {
        "loss": 1.6656,
        "grad_norm": 2.611004590988159,
        "learning_rate": 6.452495639595116e-05,
        "epoch": 0.6163044875916163,
        "step": 4793
    },
    {
        "loss": 2.1622,
        "grad_norm": 2.0702476501464844,
        "learning_rate": 6.4487117680591e-05,
        "epoch": 0.6164330718786164,
        "step": 4794
    },
    {
        "loss": 1.4644,
        "grad_norm": 2.5459561347961426,
        "learning_rate": 6.444928478275748e-05,
        "epoch": 0.6165616561656165,
        "step": 4795
    },
    {
        "loss": 2.4086,
        "grad_norm": 1.9117588996887207,
        "learning_rate": 6.441145770864828e-05,
        "epoch": 0.6166902404526167,
        "step": 4796
    },
    {
        "loss": 1.2336,
        "grad_norm": 2.009996175765991,
        "learning_rate": 6.437363646445994e-05,
        "epoch": 0.6168188247396168,
        "step": 4797
    },
    {
        "loss": 2.1154,
        "grad_norm": 1.7664185762405396,
        "learning_rate": 6.43358210563882e-05,
        "epoch": 0.6169474090266169,
        "step": 4798
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.128368616104126,
        "learning_rate": 6.429801149062776e-05,
        "epoch": 0.6170759933136171,
        "step": 4799
    },
    {
        "loss": 1.6284,
        "grad_norm": 1.7353851795196533,
        "learning_rate": 6.426020777337232e-05,
        "epoch": 0.6172045776006172,
        "step": 4800
    },
    {
        "eval_loss": 1.804382085800171,
        "eval_runtime": 28.2167,
        "eval_samples_per_second": 2.8,
        "eval_steps_per_second": 2.8,
        "epoch": 0.6172045776006172,
        "step": 4800
    },
    {
        "loss": 1.1905,
        "grad_norm": 2.0001397132873535,
        "learning_rate": 6.422240991081475e-05,
        "epoch": 0.6173331618876173,
        "step": 4801
    },
    {
        "loss": 1.6706,
        "grad_norm": 2.289855718612671,
        "learning_rate": 6.418461790914682e-05,
        "epoch": 0.6174617461746175,
        "step": 4802
    },
    {
        "loss": 1.8744,
        "grad_norm": 1.8835456371307373,
        "learning_rate": 6.414683177455949e-05,
        "epoch": 0.6175903304616176,
        "step": 4803
    },
    {
        "loss": 1.5288,
        "grad_norm": 1.957229495048523,
        "learning_rate": 6.410905151324262e-05,
        "epoch": 0.6177189147486177,
        "step": 4804
    },
    {
        "loss": 1.7468,
        "grad_norm": 1.8602665662765503,
        "learning_rate": 6.407127713138516e-05,
        "epoch": 0.6178474990356179,
        "step": 4805
    },
    {
        "loss": 1.9461,
        "grad_norm": 1.7074490785598755,
        "learning_rate": 6.403350863517517e-05,
        "epoch": 0.617976083322618,
        "step": 4806
    },
    {
        "loss": 2.4718,
        "grad_norm": 1.5130600929260254,
        "learning_rate": 6.39957460307996e-05,
        "epoch": 0.6181046676096181,
        "step": 4807
    },
    {
        "loss": 2.3763,
        "grad_norm": 1.4140077829360962,
        "learning_rate": 6.395798932444456e-05,
        "epoch": 0.6182332518966183,
        "step": 4808
    },
    {
        "loss": 2.186,
        "grad_norm": 1.7041106224060059,
        "learning_rate": 6.392023852229516e-05,
        "epoch": 0.6183618361836184,
        "step": 4809
    },
    {
        "loss": 1.9181,
        "grad_norm": 2.362612724304199,
        "learning_rate": 6.388249363053548e-05,
        "epoch": 0.6184904204706185,
        "step": 4810
    },
    {
        "loss": 1.7819,
        "grad_norm": 3.0631027221679688,
        "learning_rate": 6.384475465534873e-05,
        "epoch": 0.6186190047576187,
        "step": 4811
    },
    {
        "loss": 2.159,
        "grad_norm": 1.5021014213562012,
        "learning_rate": 6.380702160291705e-05,
        "epoch": 0.6187475890446188,
        "step": 4812
    },
    {
        "loss": 1.7443,
        "grad_norm": 2.1987674236297607,
        "learning_rate": 6.376929447942174e-05,
        "epoch": 0.6188761733316189,
        "step": 4813
    },
    {
        "loss": 1.0174,
        "grad_norm": 1.7326772212982178,
        "learning_rate": 6.373157329104299e-05,
        "epoch": 0.619004757618619,
        "step": 4814
    },
    {
        "loss": 1.483,
        "grad_norm": 1.9473048448562622,
        "learning_rate": 6.369385804396009e-05,
        "epoch": 0.6191333419056191,
        "step": 4815
    },
    {
        "loss": 1.294,
        "grad_norm": 2.0377278327941895,
        "learning_rate": 6.365614874435135e-05,
        "epoch": 0.6192619261926192,
        "step": 4816
    },
    {
        "loss": 1.2028,
        "grad_norm": 2.5535483360290527,
        "learning_rate": 6.36184453983941e-05,
        "epoch": 0.6193905104796193,
        "step": 4817
    },
    {
        "loss": 2.2144,
        "grad_norm": 1.592598795890808,
        "learning_rate": 6.358074801226473e-05,
        "epoch": 0.6195190947666195,
        "step": 4818
    },
    {
        "loss": 1.8205,
        "grad_norm": 2.3425819873809814,
        "learning_rate": 6.354305659213855e-05,
        "epoch": 0.6196476790536196,
        "step": 4819
    },
    {
        "loss": 1.6053,
        "grad_norm": 2.1462745666503906,
        "learning_rate": 6.350537114419004e-05,
        "epoch": 0.6197762633406197,
        "step": 4820
    },
    {
        "loss": 1.9859,
        "grad_norm": 1.5787323713302612,
        "learning_rate": 6.34676916745926e-05,
        "epoch": 0.6199048476276199,
        "step": 4821
    },
    {
        "loss": 2.1554,
        "grad_norm": 2.459656000137329,
        "learning_rate": 6.343001818951861e-05,
        "epoch": 0.62003343191462,
        "step": 4822
    },
    {
        "loss": 1.3125,
        "grad_norm": 2.091996431350708,
        "learning_rate": 6.339235069513964e-05,
        "epoch": 0.6201620162016201,
        "step": 4823
    },
    {
        "loss": 1.3178,
        "grad_norm": 2.9547488689422607,
        "learning_rate": 6.335468919762606e-05,
        "epoch": 0.6202906004886203,
        "step": 4824
    },
    {
        "loss": 1.4163,
        "grad_norm": 2.074676275253296,
        "learning_rate": 6.33170337031475e-05,
        "epoch": 0.6204191847756204,
        "step": 4825
    },
    {
        "loss": 2.3284,
        "grad_norm": 1.0909157991409302,
        "learning_rate": 6.32793842178724e-05,
        "epoch": 0.6205477690626205,
        "step": 4826
    },
    {
        "loss": 1.9755,
        "grad_norm": 2.1301283836364746,
        "learning_rate": 6.324174074796825e-05,
        "epoch": 0.6206763533496207,
        "step": 4827
    },
    {
        "loss": 1.9805,
        "grad_norm": 1.7496161460876465,
        "learning_rate": 6.320410329960174e-05,
        "epoch": 0.6208049376366208,
        "step": 4828
    },
    {
        "loss": 1.528,
        "grad_norm": 1.5410988330841064,
        "learning_rate": 6.316647187893827e-05,
        "epoch": 0.620933521923621,
        "step": 4829
    },
    {
        "loss": 0.9507,
        "grad_norm": 1.29727041721344,
        "learning_rate": 6.312884649214253e-05,
        "epoch": 0.6210621062106211,
        "step": 4830
    },
    {
        "loss": 2.2374,
        "grad_norm": 1.5122917890548706,
        "learning_rate": 6.309122714537806e-05,
        "epoch": 0.6211906904976212,
        "step": 4831
    },
    {
        "loss": 2.2622,
        "grad_norm": 1.8781523704528809,
        "learning_rate": 6.305361384480743e-05,
        "epoch": 0.6213192747846213,
        "step": 4832
    },
    {
        "loss": 2.0657,
        "grad_norm": 2.1316497325897217,
        "learning_rate": 6.301600659659233e-05,
        "epoch": 0.6214478590716215,
        "step": 4833
    },
    {
        "loss": 2.0048,
        "grad_norm": 1.7266499996185303,
        "learning_rate": 6.297840540689324e-05,
        "epoch": 0.6215764433586216,
        "step": 4834
    },
    {
        "loss": 2.0896,
        "grad_norm": 1.7741774320602417,
        "learning_rate": 6.294081028186991e-05,
        "epoch": 0.6217050276456217,
        "step": 4835
    },
    {
        "loss": 1.7303,
        "grad_norm": 2.520561933517456,
        "learning_rate": 6.290322122768095e-05,
        "epoch": 0.6218336119326219,
        "step": 4836
    },
    {
        "loss": 0.9818,
        "grad_norm": 1.8790785074234009,
        "learning_rate": 6.28656382504839e-05,
        "epoch": 0.621962196219622,
        "step": 4837
    },
    {
        "loss": 1.4012,
        "grad_norm": 1.5401310920715332,
        "learning_rate": 6.28280613564355e-05,
        "epoch": 0.6220907805066221,
        "step": 4838
    },
    {
        "loss": 1.9514,
        "grad_norm": 2.34203839302063,
        "learning_rate": 6.279049055169135e-05,
        "epoch": 0.6222193647936222,
        "step": 4839
    },
    {
        "loss": 1.8338,
        "grad_norm": 2.1868784427642822,
        "learning_rate": 6.27529258424061e-05,
        "epoch": 0.6223479490806223,
        "step": 4840
    },
    {
        "loss": 1.3684,
        "grad_norm": 1.9196040630340576,
        "learning_rate": 6.271536723473343e-05,
        "epoch": 0.6224765333676224,
        "step": 4841
    },
    {
        "loss": 2.2318,
        "grad_norm": 1.1910589933395386,
        "learning_rate": 6.267781473482588e-05,
        "epoch": 0.6226051176546226,
        "step": 4842
    },
    {
        "loss": 1.6165,
        "grad_norm": 1.531112551689148,
        "learning_rate": 6.26402683488352e-05,
        "epoch": 0.6227337019416227,
        "step": 4843
    },
    {
        "loss": 1.3928,
        "grad_norm": 1.173082709312439,
        "learning_rate": 6.260272808291199e-05,
        "epoch": 0.6228622862286228,
        "step": 4844
    },
    {
        "loss": 1.9208,
        "grad_norm": 2.205185651779175,
        "learning_rate": 6.256519394320592e-05,
        "epoch": 0.622990870515623,
        "step": 4845
    },
    {
        "loss": 1.6563,
        "grad_norm": 2.3351683616638184,
        "learning_rate": 6.252766593586561e-05,
        "epoch": 0.6231194548026231,
        "step": 4846
    },
    {
        "loss": 1.4881,
        "grad_norm": 1.9357638359069824,
        "learning_rate": 6.249014406703869e-05,
        "epoch": 0.6232480390896232,
        "step": 4847
    },
    {
        "loss": 0.9937,
        "grad_norm": 1.937416434288025,
        "learning_rate": 6.245262834287178e-05,
        "epoch": 0.6233766233766234,
        "step": 4848
    },
    {
        "loss": 1.4553,
        "grad_norm": 2.400510311126709,
        "learning_rate": 6.241511876951052e-05,
        "epoch": 0.6235052076636235,
        "step": 4849
    },
    {
        "loss": 2.0864,
        "grad_norm": 1.938916563987732,
        "learning_rate": 6.237761535309952e-05,
        "epoch": 0.6236337919506236,
        "step": 4850
    },
    {
        "loss": 2.1855,
        "grad_norm": 1.8721264600753784,
        "learning_rate": 6.234011809978238e-05,
        "epoch": 0.6237623762376238,
        "step": 4851
    },
    {
        "loss": 2.2957,
        "grad_norm": 2.1935794353485107,
        "learning_rate": 6.230262701570169e-05,
        "epoch": 0.6238909605246239,
        "step": 4852
    },
    {
        "loss": 1.3655,
        "grad_norm": 1.7065633535385132,
        "learning_rate": 6.226514210699905e-05,
        "epoch": 0.624019544811624,
        "step": 4853
    },
    {
        "loss": 2.2021,
        "grad_norm": 2.1399881839752197,
        "learning_rate": 6.222766337981503e-05,
        "epoch": 0.6241481290986242,
        "step": 4854
    },
    {
        "loss": 1.5678,
        "grad_norm": 2.8144779205322266,
        "learning_rate": 6.219019084028918e-05,
        "epoch": 0.6242767133856243,
        "step": 4855
    },
    {
        "loss": 1.0155,
        "grad_norm": 3.1812219619750977,
        "learning_rate": 6.215272449456007e-05,
        "epoch": 0.6244052976726244,
        "step": 4856
    },
    {
        "loss": 2.3145,
        "grad_norm": 2.3106253147125244,
        "learning_rate": 6.211526434876518e-05,
        "epoch": 0.6245338819596246,
        "step": 4857
    },
    {
        "loss": 1.8723,
        "grad_norm": 2.540951728820801,
        "learning_rate": 6.20778104090411e-05,
        "epoch": 0.6246624662466247,
        "step": 4858
    },
    {
        "loss": 1.8534,
        "grad_norm": 1.6643502712249756,
        "learning_rate": 6.204036268152325e-05,
        "epoch": 0.6247910505336248,
        "step": 4859
    },
    {
        "loss": 2.0011,
        "grad_norm": 1.6458370685577393,
        "learning_rate": 6.20029211723462e-05,
        "epoch": 0.624919634820625,
        "step": 4860
    },
    {
        "loss": 1.1778,
        "grad_norm": 2.3564038276672363,
        "learning_rate": 6.196548588764335e-05,
        "epoch": 0.6250482191076251,
        "step": 4861
    },
    {
        "loss": 2.0179,
        "grad_norm": 2.104036808013916,
        "learning_rate": 6.192805683354715e-05,
        "epoch": 0.6251768033946252,
        "step": 4862
    },
    {
        "loss": 2.1547,
        "grad_norm": 1.688197374343872,
        "learning_rate": 6.189063401618907e-05,
        "epoch": 0.6253053876816254,
        "step": 4863
    },
    {
        "loss": 1.7757,
        "grad_norm": 2.088697671890259,
        "learning_rate": 6.185321744169944e-05,
        "epoch": 0.6254339719686254,
        "step": 4864
    },
    {
        "loss": 2.2117,
        "grad_norm": 1.6668559312820435,
        "learning_rate": 6.181580711620771e-05,
        "epoch": 0.6255625562556255,
        "step": 4865
    },
    {
        "loss": 1.9255,
        "grad_norm": 1.9884504079818726,
        "learning_rate": 6.177840304584219e-05,
        "epoch": 0.6256911405426256,
        "step": 4866
    },
    {
        "loss": 2.1986,
        "grad_norm": 1.6634875535964966,
        "learning_rate": 6.174100523673022e-05,
        "epoch": 0.6258197248296258,
        "step": 4867
    },
    {
        "loss": 2.0238,
        "grad_norm": 1.7053558826446533,
        "learning_rate": 6.170361369499811e-05,
        "epoch": 0.6259483091166259,
        "step": 4868
    },
    {
        "loss": 1.932,
        "grad_norm": 2.2647807598114014,
        "learning_rate": 6.166622842677113e-05,
        "epoch": 0.626076893403626,
        "step": 4869
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.8611611127853394,
        "learning_rate": 6.162884943817355e-05,
        "epoch": 0.6262054776906262,
        "step": 4870
    },
    {
        "loss": 1.6304,
        "grad_norm": 1.8172898292541504,
        "learning_rate": 6.159147673532854e-05,
        "epoch": 0.6263340619776263,
        "step": 4871
    },
    {
        "loss": 1.4498,
        "grad_norm": 2.2525813579559326,
        "learning_rate": 6.15541103243584e-05,
        "epoch": 0.6264626462646264,
        "step": 4872
    },
    {
        "loss": 1.7772,
        "grad_norm": 2.019360303878784,
        "learning_rate": 6.151675021138421e-05,
        "epoch": 0.6265912305516266,
        "step": 4873
    },
    {
        "loss": 2.0456,
        "grad_norm": 1.8302192687988281,
        "learning_rate": 6.147939640252606e-05,
        "epoch": 0.6267198148386267,
        "step": 4874
    },
    {
        "loss": 1.9759,
        "grad_norm": 2.601019859313965,
        "learning_rate": 6.144204890390317e-05,
        "epoch": 0.6268483991256268,
        "step": 4875
    },
    {
        "loss": 1.8079,
        "grad_norm": 1.9776536226272583,
        "learning_rate": 6.140470772163348e-05,
        "epoch": 0.626976983412627,
        "step": 4876
    },
    {
        "loss": 1.526,
        "grad_norm": 2.0985658168792725,
        "learning_rate": 6.136737286183412e-05,
        "epoch": 0.6271055676996271,
        "step": 4877
    },
    {
        "loss": 1.0964,
        "grad_norm": 2.9387142658233643,
        "learning_rate": 6.133004433062104e-05,
        "epoch": 0.6272341519866272,
        "step": 4878
    },
    {
        "loss": 1.2377,
        "grad_norm": 1.7368899583816528,
        "learning_rate": 6.129272213410912e-05,
        "epoch": 0.6273627362736274,
        "step": 4879
    },
    {
        "loss": 2.1623,
        "grad_norm": 1.6732839345932007,
        "learning_rate": 6.125540627841244e-05,
        "epoch": 0.6274913205606275,
        "step": 4880
    },
    {
        "loss": 2.0983,
        "grad_norm": 2.863517999649048,
        "learning_rate": 6.121809676964371e-05,
        "epoch": 0.6276199048476276,
        "step": 4881
    },
    {
        "loss": 1.8753,
        "grad_norm": 1.764605164527893,
        "learning_rate": 6.118079361391491e-05,
        "epoch": 0.6277484891346278,
        "step": 4882
    },
    {
        "loss": 2.5717,
        "grad_norm": 1.1010112762451172,
        "learning_rate": 6.11434968173368e-05,
        "epoch": 0.6278770734216279,
        "step": 4883
    },
    {
        "loss": 2.1181,
        "grad_norm": 1.6051952838897705,
        "learning_rate": 6.110620638601905e-05,
        "epoch": 0.628005657708628,
        "step": 4884
    },
    {
        "loss": 1.2973,
        "grad_norm": 2.4076712131500244,
        "learning_rate": 6.10689223260705e-05,
        "epoch": 0.6281342419956282,
        "step": 4885
    },
    {
        "loss": 1.5288,
        "grad_norm": 3.011932849884033,
        "learning_rate": 6.103164464359874e-05,
        "epoch": 0.6282628262826283,
        "step": 4886
    },
    {
        "loss": 1.4583,
        "grad_norm": 2.4628796577453613,
        "learning_rate": 6.099437334471043e-05,
        "epoch": 0.6283914105696284,
        "step": 4887
    },
    {
        "loss": 2.4744,
        "grad_norm": 1.5845680236816406,
        "learning_rate": 6.095710843551118e-05,
        "epoch": 0.6285199948566286,
        "step": 4888
    },
    {
        "loss": 2.0433,
        "grad_norm": 1.8684107065200806,
        "learning_rate": 6.091984992210542e-05,
        "epoch": 0.6286485791436286,
        "step": 4889
    },
    {
        "loss": 2.2305,
        "grad_norm": 1.3079713582992554,
        "learning_rate": 6.0882597810596745e-05,
        "epoch": 0.6287771634306287,
        "step": 4890
    },
    {
        "loss": 2.2005,
        "grad_norm": 2.2869293689727783,
        "learning_rate": 6.084535210708753e-05,
        "epoch": 0.6289057477176289,
        "step": 4891
    },
    {
        "loss": 1.887,
        "grad_norm": 2.431791305541992,
        "learning_rate": 6.0808112817679194e-05,
        "epoch": 0.629034332004629,
        "step": 4892
    },
    {
        "loss": 1.9837,
        "grad_norm": 2.140371799468994,
        "learning_rate": 6.077087994847208e-05,
        "epoch": 0.6291629162916291,
        "step": 4893
    },
    {
        "loss": 0.7079,
        "grad_norm": 1.82097589969635,
        "learning_rate": 6.0733653505565434e-05,
        "epoch": 0.6292915005786293,
        "step": 4894
    },
    {
        "loss": 2.1665,
        "grad_norm": 1.1525688171386719,
        "learning_rate": 6.0696433495057534e-05,
        "epoch": 0.6294200848656294,
        "step": 4895
    },
    {
        "loss": 2.0349,
        "grad_norm": 1.528504729270935,
        "learning_rate": 6.065921992304553e-05,
        "epoch": 0.6295486691526295,
        "step": 4896
    },
    {
        "loss": 1.5264,
        "grad_norm": 1.6531238555908203,
        "learning_rate": 6.062201279562556e-05,
        "epoch": 0.6296772534396297,
        "step": 4897
    },
    {
        "loss": 1.6462,
        "grad_norm": 2.52268123626709,
        "learning_rate": 6.05848121188927e-05,
        "epoch": 0.6298058377266298,
        "step": 4898
    },
    {
        "loss": 1.8653,
        "grad_norm": 2.2574613094329834,
        "learning_rate": 6.0547617898940925e-05,
        "epoch": 0.6299344220136299,
        "step": 4899
    },
    {
        "loss": 1.3112,
        "grad_norm": 2.7653748989105225,
        "learning_rate": 6.051043014186323e-05,
        "epoch": 0.6300630063006301,
        "step": 4900
    },
    {
        "eval_loss": 1.7996538877487183,
        "eval_runtime": 28.2962,
        "eval_samples_per_second": 2.792,
        "eval_steps_per_second": 2.792,
        "epoch": 0.6300630063006301,
        "step": 4900
    },
    {
        "loss": 1.424,
        "grad_norm": 2.5673437118530273,
        "learning_rate": 6.047324885375148e-05,
        "epoch": 0.6301915905876302,
        "step": 4901
    },
    {
        "loss": 1.7134,
        "grad_norm": 1.9281946420669556,
        "learning_rate": 6.0436074040696536e-05,
        "epoch": 0.6303201748746303,
        "step": 4902
    },
    {
        "loss": 2.2376,
        "grad_norm": 1.3606609106063843,
        "learning_rate": 6.039890570878817e-05,
        "epoch": 0.6304487591616305,
        "step": 4903
    },
    {
        "loss": 1.4123,
        "grad_norm": 1.7447201013565063,
        "learning_rate": 6.036174386411506e-05,
        "epoch": 0.6305773434486306,
        "step": 4904
    },
    {
        "loss": 2.3222,
        "grad_norm": 1.6693085432052612,
        "learning_rate": 6.032458851276492e-05,
        "epoch": 0.6307059277356307,
        "step": 4905
    },
    {
        "loss": 1.6304,
        "grad_norm": 1.2989939451217651,
        "learning_rate": 6.028743966082426e-05,
        "epoch": 0.6308345120226309,
        "step": 4906
    },
    {
        "loss": 2.0199,
        "grad_norm": 2.1982784271240234,
        "learning_rate": 6.025029731437869e-05,
        "epoch": 0.630963096309631,
        "step": 4907
    },
    {
        "loss": 1.5542,
        "grad_norm": 2.6019482612609863,
        "learning_rate": 6.0213161479512603e-05,
        "epoch": 0.6310916805966311,
        "step": 4908
    },
    {
        "loss": 2.422,
        "grad_norm": 1.4871900081634521,
        "learning_rate": 6.017603216230939e-05,
        "epoch": 0.6312202648836313,
        "step": 4909
    },
    {
        "loss": 1.849,
        "grad_norm": 1.9955871105194092,
        "learning_rate": 6.013890936885141e-05,
        "epoch": 0.6313488491706314,
        "step": 4910
    },
    {
        "loss": 1.7641,
        "grad_norm": 1.5834214687347412,
        "learning_rate": 6.0101793105219885e-05,
        "epoch": 0.6314774334576315,
        "step": 4911
    },
    {
        "loss": 1.8232,
        "grad_norm": 2.4691755771636963,
        "learning_rate": 6.006468337749503e-05,
        "epoch": 0.6316060177446317,
        "step": 4912
    },
    {
        "loss": 1.3719,
        "grad_norm": 2.1367437839508057,
        "learning_rate": 6.002758019175592e-05,
        "epoch": 0.6317346020316318,
        "step": 4913
    },
    {
        "loss": 2.2337,
        "grad_norm": 2.0139360427856445,
        "learning_rate": 5.9990483554080613e-05,
        "epoch": 0.6318631863186318,
        "step": 4914
    },
    {
        "loss": 1.5487,
        "grad_norm": 1.4428879022598267,
        "learning_rate": 5.9953393470546096e-05,
        "epoch": 0.6319917706056319,
        "step": 4915
    },
    {
        "loss": 2.1313,
        "grad_norm": 1.4149116277694702,
        "learning_rate": 5.991630994722821e-05,
        "epoch": 0.6321203548926321,
        "step": 4916
    },
    {
        "loss": 1.711,
        "grad_norm": 2.179274797439575,
        "learning_rate": 5.987923299020186e-05,
        "epoch": 0.6322489391796322,
        "step": 4917
    },
    {
        "loss": 1.413,
        "grad_norm": 1.825897216796875,
        "learning_rate": 5.9842162605540687e-05,
        "epoch": 0.6323775234666323,
        "step": 4918
    },
    {
        "loss": 1.5245,
        "grad_norm": 1.7431375980377197,
        "learning_rate": 5.980509879931748e-05,
        "epoch": 0.6325061077536325,
        "step": 4919
    },
    {
        "loss": 1.6712,
        "grad_norm": 1.8246445655822754,
        "learning_rate": 5.976804157760376e-05,
        "epoch": 0.6326346920406326,
        "step": 4920
    },
    {
        "loss": 1.8036,
        "grad_norm": 1.8915398120880127,
        "learning_rate": 5.973099094646999e-05,
        "epoch": 0.6327632763276327,
        "step": 4921
    },
    {
        "loss": 1.3123,
        "grad_norm": 2.0050666332244873,
        "learning_rate": 5.969394691198572e-05,
        "epoch": 0.6328918606146329,
        "step": 4922
    },
    {
        "loss": 0.9117,
        "grad_norm": 1.899502158164978,
        "learning_rate": 5.965690948021918e-05,
        "epoch": 0.633020444901633,
        "step": 4923
    },
    {
        "loss": 1.3521,
        "grad_norm": 1.945379614830017,
        "learning_rate": 5.961987865723775e-05,
        "epoch": 0.6331490291886331,
        "step": 4924
    },
    {
        "loss": 1.1747,
        "grad_norm": 2.0909483432769775,
        "learning_rate": 5.958285444910757e-05,
        "epoch": 0.6332776134756333,
        "step": 4925
    },
    {
        "loss": 1.9736,
        "grad_norm": 1.6283272504806519,
        "learning_rate": 5.9545836861893686e-05,
        "epoch": 0.6334061977626334,
        "step": 4926
    },
    {
        "loss": 1.7676,
        "grad_norm": 2.282313585281372,
        "learning_rate": 5.9508825901660233e-05,
        "epoch": 0.6335347820496335,
        "step": 4927
    },
    {
        "loss": 1.3009,
        "grad_norm": 2.312659502029419,
        "learning_rate": 5.947182157447002e-05,
        "epoch": 0.6336633663366337,
        "step": 4928
    },
    {
        "loss": 0.8923,
        "grad_norm": 2.195913553237915,
        "learning_rate": 5.943482388638502e-05,
        "epoch": 0.6337919506236338,
        "step": 4929
    },
    {
        "loss": 2.0212,
        "grad_norm": 1.8623013496398926,
        "learning_rate": 5.939783284346592e-05,
        "epoch": 0.6339205349106339,
        "step": 4930
    },
    {
        "loss": 1.9633,
        "grad_norm": 2.0575971603393555,
        "learning_rate": 5.9360848451772355e-05,
        "epoch": 0.6340491191976341,
        "step": 4931
    },
    {
        "loss": 1.7208,
        "grad_norm": 2.5542988777160645,
        "learning_rate": 5.9323870717362985e-05,
        "epoch": 0.6341777034846342,
        "step": 4932
    },
    {
        "loss": 1.6988,
        "grad_norm": 1.8461962938308716,
        "learning_rate": 5.928689964629526e-05,
        "epoch": 0.6343062877716343,
        "step": 4933
    },
    {
        "loss": 1.4375,
        "grad_norm": 1.9191309213638306,
        "learning_rate": 5.92499352446256e-05,
        "epoch": 0.6344348720586345,
        "step": 4934
    },
    {
        "loss": 1.7661,
        "grad_norm": 2.1576738357543945,
        "learning_rate": 5.92129775184093e-05,
        "epoch": 0.6345634563456346,
        "step": 4935
    },
    {
        "loss": 2.4383,
        "grad_norm": 1.9245195388793945,
        "learning_rate": 5.9176026473700555e-05,
        "epoch": 0.6346920406326347,
        "step": 4936
    },
    {
        "loss": 1.5478,
        "grad_norm": 1.9631571769714355,
        "learning_rate": 5.9139082116552515e-05,
        "epoch": 0.6348206249196349,
        "step": 4937
    },
    {
        "loss": 1.9951,
        "grad_norm": 2.274263858795166,
        "learning_rate": 5.910214445301717e-05,
        "epoch": 0.634949209206635,
        "step": 4938
    },
    {
        "loss": 1.9703,
        "grad_norm": 2.0397849082946777,
        "learning_rate": 5.906521348914549e-05,
        "epoch": 0.635077793493635,
        "step": 4939
    },
    {
        "loss": 1.9103,
        "grad_norm": 2.1074469089508057,
        "learning_rate": 5.902828923098727e-05,
        "epoch": 0.6352063777806352,
        "step": 4940
    },
    {
        "loss": 1.8818,
        "grad_norm": 3.947798490524292,
        "learning_rate": 5.899137168459123e-05,
        "epoch": 0.6353349620676353,
        "step": 4941
    },
    {
        "loss": 1.3355,
        "grad_norm": 1.9902229309082031,
        "learning_rate": 5.8954460856005056e-05,
        "epoch": 0.6354635463546354,
        "step": 4942
    },
    {
        "loss": 0.5084,
        "grad_norm": 1.3598783016204834,
        "learning_rate": 5.8917556751275215e-05,
        "epoch": 0.6355921306416356,
        "step": 4943
    },
    {
        "loss": 1.553,
        "grad_norm": 2.884688138961792,
        "learning_rate": 5.8880659376447186e-05,
        "epoch": 0.6357207149286357,
        "step": 4944
    },
    {
        "loss": 1.7594,
        "grad_norm": 1.8436002731323242,
        "learning_rate": 5.8843768737565275e-05,
        "epoch": 0.6358492992156358,
        "step": 4945
    },
    {
        "loss": 1.0825,
        "grad_norm": 2.296804428100586,
        "learning_rate": 5.880688484067268e-05,
        "epoch": 0.635977883502636,
        "step": 4946
    },
    {
        "loss": 1.8047,
        "grad_norm": 1.5460684299468994,
        "learning_rate": 5.877000769181158e-05,
        "epoch": 0.6361064677896361,
        "step": 4947
    },
    {
        "loss": 2.2177,
        "grad_norm": 1.9852807521820068,
        "learning_rate": 5.873313729702292e-05,
        "epoch": 0.6362350520766362,
        "step": 4948
    },
    {
        "loss": 1.6024,
        "grad_norm": 2.3498752117156982,
        "learning_rate": 5.869627366234666e-05,
        "epoch": 0.6363636363636364,
        "step": 4949
    },
    {
        "loss": 2.0138,
        "grad_norm": 2.6229710578918457,
        "learning_rate": 5.8659416793821585e-05,
        "epoch": 0.6364922206506365,
        "step": 4950
    },
    {
        "loss": 1.1121,
        "grad_norm": 2.8268983364105225,
        "learning_rate": 5.862256669748536e-05,
        "epoch": 0.6366208049376366,
        "step": 4951
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.4368417263031006,
        "learning_rate": 5.85857233793746e-05,
        "epoch": 0.6367493892246368,
        "step": 4952
    },
    {
        "loss": 1.7387,
        "grad_norm": 1.6856184005737305,
        "learning_rate": 5.8548886845524754e-05,
        "epoch": 0.6368779735116369,
        "step": 4953
    },
    {
        "loss": 2.244,
        "grad_norm": 1.614119529724121,
        "learning_rate": 5.851205710197021e-05,
        "epoch": 0.637006557798637,
        "step": 4954
    },
    {
        "loss": 1.4137,
        "grad_norm": 2.4619054794311523,
        "learning_rate": 5.847523415474418e-05,
        "epoch": 0.6371351420856372,
        "step": 4955
    },
    {
        "loss": 1.6009,
        "grad_norm": 2.736215114593506,
        "learning_rate": 5.843841800987881e-05,
        "epoch": 0.6372637263726373,
        "step": 4956
    },
    {
        "loss": 1.4065,
        "grad_norm": 1.9039477109909058,
        "learning_rate": 5.840160867340516e-05,
        "epoch": 0.6373923106596374,
        "step": 4957
    },
    {
        "loss": 1.3994,
        "grad_norm": 2.100501537322998,
        "learning_rate": 5.836480615135305e-05,
        "epoch": 0.6375208949466376,
        "step": 4958
    },
    {
        "loss": 2.247,
        "grad_norm": 2.1509366035461426,
        "learning_rate": 5.832801044975138e-05,
        "epoch": 0.6376494792336377,
        "step": 4959
    },
    {
        "loss": 1.8078,
        "grad_norm": 1.7970736026763916,
        "learning_rate": 5.829122157462772e-05,
        "epoch": 0.6377780635206378,
        "step": 4960
    },
    {
        "loss": 2.0692,
        "grad_norm": 2.8241562843322754,
        "learning_rate": 5.8254439532008664e-05,
        "epoch": 0.637906647807638,
        "step": 4961
    },
    {
        "loss": 1.3649,
        "grad_norm": 2.2665672302246094,
        "learning_rate": 5.821766432791968e-05,
        "epoch": 0.6380352320946381,
        "step": 4962
    },
    {
        "loss": 1.487,
        "grad_norm": 2.076547145843506,
        "learning_rate": 5.818089596838501e-05,
        "epoch": 0.6381638163816382,
        "step": 4963
    },
    {
        "loss": 1.8457,
        "grad_norm": 2.3200809955596924,
        "learning_rate": 5.814413445942789e-05,
        "epoch": 0.6382924006686382,
        "step": 4964
    },
    {
        "loss": 2.3886,
        "grad_norm": 2.1892991065979004,
        "learning_rate": 5.8107379807070415e-05,
        "epoch": 0.6384209849556384,
        "step": 4965
    },
    {
        "loss": 1.7738,
        "grad_norm": 2.675434112548828,
        "learning_rate": 5.807063201733346e-05,
        "epoch": 0.6385495692426385,
        "step": 4966
    },
    {
        "loss": 1.5351,
        "grad_norm": 2.00811505317688,
        "learning_rate": 5.803389109623689e-05,
        "epoch": 0.6386781535296386,
        "step": 4967
    },
    {
        "loss": 2.534,
        "grad_norm": 1.5870394706726074,
        "learning_rate": 5.79971570497994e-05,
        "epoch": 0.6388067378166388,
        "step": 4968
    },
    {
        "loss": 1.8124,
        "grad_norm": 2.105374574661255,
        "learning_rate": 5.796042988403859e-05,
        "epoch": 0.6389353221036389,
        "step": 4969
    },
    {
        "loss": 1.3076,
        "grad_norm": 1.8980189561843872,
        "learning_rate": 5.792370960497085e-05,
        "epoch": 0.639063906390639,
        "step": 4970
    },
    {
        "loss": 2.085,
        "grad_norm": 1.996408224105835,
        "learning_rate": 5.788699621861151e-05,
        "epoch": 0.6391924906776392,
        "step": 4971
    },
    {
        "loss": 2.4333,
        "grad_norm": 2.453425884246826,
        "learning_rate": 5.7850289730974804e-05,
        "epoch": 0.6393210749646393,
        "step": 4972
    },
    {
        "loss": 2.0997,
        "grad_norm": 2.1303391456604004,
        "learning_rate": 5.781359014807372e-05,
        "epoch": 0.6394496592516394,
        "step": 4973
    },
    {
        "loss": 1.6121,
        "grad_norm": 2.065566062927246,
        "learning_rate": 5.777689747592021e-05,
        "epoch": 0.6395782435386396,
        "step": 4974
    },
    {
        "loss": 1.1071,
        "grad_norm": 2.221982002258301,
        "learning_rate": 5.774021172052507e-05,
        "epoch": 0.6397068278256397,
        "step": 4975
    },
    {
        "loss": 2.337,
        "grad_norm": 2.057204484939575,
        "learning_rate": 5.7703532887898006e-05,
        "epoch": 0.6398354121126398,
        "step": 4976
    },
    {
        "loss": 1.1258,
        "grad_norm": 2.337430238723755,
        "learning_rate": 5.7666860984047464e-05,
        "epoch": 0.63996399639964,
        "step": 4977
    },
    {
        "loss": 1.2974,
        "grad_norm": 1.9296003580093384,
        "learning_rate": 5.7630196014980874e-05,
        "epoch": 0.6400925806866401,
        "step": 4978
    },
    {
        "loss": 1.3718,
        "grad_norm": 2.3020033836364746,
        "learning_rate": 5.7593537986704524e-05,
        "epoch": 0.6402211649736402,
        "step": 4979
    },
    {
        "loss": 1.565,
        "grad_norm": 2.397705078125,
        "learning_rate": 5.755688690522345e-05,
        "epoch": 0.6403497492606404,
        "step": 4980
    },
    {
        "loss": 1.6433,
        "grad_norm": 1.8824293613433838,
        "learning_rate": 5.7520242776541686e-05,
        "epoch": 0.6404783335476405,
        "step": 4981
    },
    {
        "loss": 1.4099,
        "grad_norm": 1.8104612827301025,
        "learning_rate": 5.7483605606662114e-05,
        "epoch": 0.6406069178346406,
        "step": 4982
    },
    {
        "loss": 1.94,
        "grad_norm": 2.31514573097229,
        "learning_rate": 5.744697540158634e-05,
        "epoch": 0.6407355021216408,
        "step": 4983
    },
    {
        "loss": 0.8166,
        "grad_norm": 2.7960450649261475,
        "learning_rate": 5.7410352167314984e-05,
        "epoch": 0.6408640864086409,
        "step": 4984
    },
    {
        "loss": 1.802,
        "grad_norm": 2.0933971405029297,
        "learning_rate": 5.7373735909847445e-05,
        "epoch": 0.640992670695641,
        "step": 4985
    },
    {
        "loss": 1.4959,
        "grad_norm": 2.350649118423462,
        "learning_rate": 5.733712663518206e-05,
        "epoch": 0.6411212549826412,
        "step": 4986
    },
    {
        "loss": 1.701,
        "grad_norm": 2.042941093444824,
        "learning_rate": 5.7300524349315856e-05,
        "epoch": 0.6412498392696413,
        "step": 4987
    },
    {
        "loss": 2.1997,
        "grad_norm": 3.10688853263855,
        "learning_rate": 5.726392905824487e-05,
        "epoch": 0.6413784235566414,
        "step": 4988
    },
    {
        "loss": 1.1332,
        "grad_norm": 2.3306186199188232,
        "learning_rate": 5.722734076796399e-05,
        "epoch": 0.6415070078436415,
        "step": 4989
    },
    {
        "loss": 1.7019,
        "grad_norm": 2.7983078956604004,
        "learning_rate": 5.719075948446679e-05,
        "epoch": 0.6416355921306416,
        "step": 4990
    },
    {
        "loss": 1.9431,
        "grad_norm": 1.979198932647705,
        "learning_rate": 5.715418521374596e-05,
        "epoch": 0.6417641764176417,
        "step": 4991
    },
    {
        "loss": 1.9381,
        "grad_norm": 1.761284589767456,
        "learning_rate": 5.711761796179284e-05,
        "epoch": 0.6418927607046419,
        "step": 4992
    },
    {
        "loss": 2.0813,
        "grad_norm": 2.1262810230255127,
        "learning_rate": 5.708105773459758e-05,
        "epoch": 0.642021344991642,
        "step": 4993
    },
    {
        "loss": 2.4498,
        "grad_norm": 1.8833305835723877,
        "learning_rate": 5.7044504538149444e-05,
        "epoch": 0.6421499292786421,
        "step": 4994
    },
    {
        "loss": 1.4423,
        "grad_norm": 3.069373369216919,
        "learning_rate": 5.7007958378436254e-05,
        "epoch": 0.6422785135656423,
        "step": 4995
    },
    {
        "loss": 1.2476,
        "grad_norm": 2.2700047492980957,
        "learning_rate": 5.6971419261444844e-05,
        "epoch": 0.6424070978526424,
        "step": 4996
    },
    {
        "loss": 0.7912,
        "grad_norm": 2.0268964767456055,
        "learning_rate": 5.693488719316091e-05,
        "epoch": 0.6425356821396425,
        "step": 4997
    },
    {
        "loss": 1.6778,
        "grad_norm": 1.9058040380477905,
        "learning_rate": 5.689836217956883e-05,
        "epoch": 0.6426642664266426,
        "step": 4998
    },
    {
        "loss": 2.5257,
        "grad_norm": 1.6573563814163208,
        "learning_rate": 5.6861844226651975e-05,
        "epoch": 0.6427928507136428,
        "step": 4999
    },
    {
        "loss": 1.6851,
        "grad_norm": 2.3691890239715576,
        "learning_rate": 5.682533334039254e-05,
        "epoch": 0.6429214350006429,
        "step": 5000
    },
    {
        "eval_loss": 1.8007404804229736,
        "eval_runtime": 28.2596,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.6429214350006429,
        "step": 5000
    },
    {
        "loss": 2.1746,
        "grad_norm": 1.716254711151123,
        "learning_rate": 5.678882952677156e-05,
        "epoch": 0.643050019287643,
        "step": 5001
    },
    {
        "loss": 1.9651,
        "grad_norm": 2.1899781227111816,
        "learning_rate": 5.675233279176885e-05,
        "epoch": 0.6431786035746432,
        "step": 5002
    },
    {
        "loss": 1.37,
        "grad_norm": 2.01344895362854,
        "learning_rate": 5.671584314136304e-05,
        "epoch": 0.6433071878616433,
        "step": 5003
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.236593008041382,
        "learning_rate": 5.667936058153181e-05,
        "epoch": 0.6434357721486434,
        "step": 5004
    },
    {
        "loss": 0.8905,
        "grad_norm": 1.8452435731887817,
        "learning_rate": 5.664288511825141e-05,
        "epoch": 0.6435643564356436,
        "step": 5005
    },
    {
        "loss": 2.0122,
        "grad_norm": 3.038516044616699,
        "learning_rate": 5.660641675749712e-05,
        "epoch": 0.6436929407226437,
        "step": 5006
    },
    {
        "loss": 1.8932,
        "grad_norm": 2.403411865234375,
        "learning_rate": 5.6569955505243e-05,
        "epoch": 0.6438215250096438,
        "step": 5007
    },
    {
        "loss": 0.7014,
        "grad_norm": 2.4737207889556885,
        "learning_rate": 5.653350136746186e-05,
        "epoch": 0.643950109296644,
        "step": 5008
    },
    {
        "loss": 0.9996,
        "grad_norm": 2.492490768432617,
        "learning_rate": 5.6497054350125466e-05,
        "epoch": 0.6440786935836441,
        "step": 5009
    },
    {
        "loss": 2.1811,
        "grad_norm": 1.0859137773513794,
        "learning_rate": 5.646061445920436e-05,
        "epoch": 0.6442072778706442,
        "step": 5010
    },
    {
        "loss": 1.1238,
        "grad_norm": 2.1407647132873535,
        "learning_rate": 5.642418170066799e-05,
        "epoch": 0.6443358621576444,
        "step": 5011
    },
    {
        "loss": 1.8221,
        "grad_norm": 3.0163156986236572,
        "learning_rate": 5.6387756080484456e-05,
        "epoch": 0.6444644464446445,
        "step": 5012
    },
    {
        "loss": 1.7454,
        "grad_norm": 4.727006912231445,
        "learning_rate": 5.635133760462087e-05,
        "epoch": 0.6445930307316446,
        "step": 5013
    },
    {
        "loss": 1.8492,
        "grad_norm": 2.2082812786102295,
        "learning_rate": 5.6314926279043135e-05,
        "epoch": 0.6447216150186447,
        "step": 5014
    },
    {
        "loss": 1.5803,
        "grad_norm": 2.346299648284912,
        "learning_rate": 5.627852210971587e-05,
        "epoch": 0.6448501993056448,
        "step": 5015
    },
    {
        "loss": 2.0007,
        "grad_norm": 1.9761234521865845,
        "learning_rate": 5.624212510260267e-05,
        "epoch": 0.6449787835926449,
        "step": 5016
    },
    {
        "loss": 1.5579,
        "grad_norm": 1.6110799312591553,
        "learning_rate": 5.6205735263665885e-05,
        "epoch": 0.6451073678796451,
        "step": 5017
    },
    {
        "loss": 1.9699,
        "grad_norm": 2.0727171897888184,
        "learning_rate": 5.6169352598866734e-05,
        "epoch": 0.6452359521666452,
        "step": 5018
    },
    {
        "loss": 2.248,
        "grad_norm": 1.5556656122207642,
        "learning_rate": 5.6132977114165144e-05,
        "epoch": 0.6453645364536453,
        "step": 5019
    },
    {
        "loss": 1.7409,
        "grad_norm": 1.8830995559692383,
        "learning_rate": 5.6096608815520004e-05,
        "epoch": 0.6454931207406455,
        "step": 5020
    },
    {
        "loss": 1.7705,
        "grad_norm": 2.3090319633483887,
        "learning_rate": 5.606024770888899e-05,
        "epoch": 0.6456217050276456,
        "step": 5021
    },
    {
        "loss": 2.3408,
        "grad_norm": 1.3005950450897217,
        "learning_rate": 5.602389380022852e-05,
        "epoch": 0.6457502893146457,
        "step": 5022
    },
    {
        "loss": 2.167,
        "grad_norm": 1.5904427766799927,
        "learning_rate": 5.598754709549392e-05,
        "epoch": 0.6458788736016459,
        "step": 5023
    },
    {
        "loss": 1.8928,
        "grad_norm": 2.2461013793945312,
        "learning_rate": 5.595120760063934e-05,
        "epoch": 0.646007457888646,
        "step": 5024
    },
    {
        "loss": 1.9637,
        "grad_norm": 2.9697113037109375,
        "learning_rate": 5.591487532161768e-05,
        "epoch": 0.6461360421756461,
        "step": 5025
    },
    {
        "loss": 2.0037,
        "grad_norm": 2.060601234436035,
        "learning_rate": 5.587855026438068e-05,
        "epoch": 0.6462646264626463,
        "step": 5026
    },
    {
        "loss": 2.0507,
        "grad_norm": 1.7181113958358765,
        "learning_rate": 5.584223243487894e-05,
        "epoch": 0.6463932107496464,
        "step": 5027
    },
    {
        "loss": 1.5646,
        "grad_norm": 2.332216739654541,
        "learning_rate": 5.5805921839061904e-05,
        "epoch": 0.6465217950366465,
        "step": 5028
    },
    {
        "loss": 1.7116,
        "grad_norm": 2.4178409576416016,
        "learning_rate": 5.576961848287767e-05,
        "epoch": 0.6466503793236467,
        "step": 5029
    },
    {
        "loss": 1.1232,
        "grad_norm": 1.9870485067367554,
        "learning_rate": 5.573332237227331e-05,
        "epoch": 0.6467789636106468,
        "step": 5030
    },
    {
        "loss": 0.8169,
        "grad_norm": 2.006542444229126,
        "learning_rate": 5.569703351319469e-05,
        "epoch": 0.6469075478976469,
        "step": 5031
    },
    {
        "loss": 1.7733,
        "grad_norm": 1.7533636093139648,
        "learning_rate": 5.566075191158632e-05,
        "epoch": 0.6470361321846471,
        "step": 5032
    },
    {
        "loss": 1.3086,
        "grad_norm": 1.2319785356521606,
        "learning_rate": 5.5624477573391845e-05,
        "epoch": 0.6471647164716472,
        "step": 5033
    },
    {
        "loss": 2.2589,
        "grad_norm": 1.992146611213684,
        "learning_rate": 5.558821050455341e-05,
        "epoch": 0.6472933007586473,
        "step": 5034
    },
    {
        "loss": 1.7435,
        "grad_norm": 1.8624210357666016,
        "learning_rate": 5.555195071101205e-05,
        "epoch": 0.6474218850456475,
        "step": 5035
    },
    {
        "loss": 1.9731,
        "grad_norm": 2.1773898601531982,
        "learning_rate": 5.551569819870778e-05,
        "epoch": 0.6475504693326476,
        "step": 5036
    },
    {
        "loss": 2.1003,
        "grad_norm": 2.364651918411255,
        "learning_rate": 5.547945297357917e-05,
        "epoch": 0.6476790536196477,
        "step": 5037
    },
    {
        "loss": 2.3557,
        "grad_norm": 1.6838501691818237,
        "learning_rate": 5.544321504156381e-05,
        "epoch": 0.6478076379066479,
        "step": 5038
    },
    {
        "loss": 2.1915,
        "grad_norm": 1.3154064416885376,
        "learning_rate": 5.540698440859791e-05,
        "epoch": 0.6479362221936479,
        "step": 5039
    },
    {
        "loss": 1.7485,
        "grad_norm": 1.738168478012085,
        "learning_rate": 5.5370761080616607e-05,
        "epoch": 0.648064806480648,
        "step": 5040
    },
    {
        "loss": 1.2643,
        "grad_norm": 2.0089175701141357,
        "learning_rate": 5.5334545063553865e-05,
        "epoch": 0.6481933907676481,
        "step": 5041
    },
    {
        "loss": 1.6674,
        "grad_norm": 3.094677448272705,
        "learning_rate": 5.5298336363342274e-05,
        "epoch": 0.6483219750546483,
        "step": 5042
    },
    {
        "loss": 0.721,
        "grad_norm": 1.7702314853668213,
        "learning_rate": 5.5262134985913486e-05,
        "epoch": 0.6484505593416484,
        "step": 5043
    },
    {
        "loss": 0.8804,
        "grad_norm": 1.7944612503051758,
        "learning_rate": 5.522594093719775e-05,
        "epoch": 0.6485791436286485,
        "step": 5044
    },
    {
        "loss": 1.3804,
        "grad_norm": 1.6734346151351929,
        "learning_rate": 5.518975422312409e-05,
        "epoch": 0.6487077279156487,
        "step": 5045
    },
    {
        "loss": 2.071,
        "grad_norm": 2.0036020278930664,
        "learning_rate": 5.515357484962058e-05,
        "epoch": 0.6488363122026488,
        "step": 5046
    },
    {
        "loss": 1.7943,
        "grad_norm": 1.8186557292938232,
        "learning_rate": 5.5117402822613815e-05,
        "epoch": 0.648964896489649,
        "step": 5047
    },
    {
        "loss": 1.7609,
        "grad_norm": 3.1885414123535156,
        "learning_rate": 5.508123814802932e-05,
        "epoch": 0.6490934807766491,
        "step": 5048
    },
    {
        "loss": 1.5873,
        "grad_norm": 2.545912027359009,
        "learning_rate": 5.504508083179146e-05,
        "epoch": 0.6492220650636492,
        "step": 5049
    },
    {
        "loss": 1.1428,
        "grad_norm": 1.282609462738037,
        "learning_rate": 5.500893087982324e-05,
        "epoch": 0.6493506493506493,
        "step": 5050
    },
    {
        "loss": 2.3361,
        "grad_norm": 1.863250494003296,
        "learning_rate": 5.497278829804657e-05,
        "epoch": 0.6494792336376495,
        "step": 5051
    },
    {
        "loss": 2.3309,
        "grad_norm": 1.9215725660324097,
        "learning_rate": 5.493665309238214e-05,
        "epoch": 0.6496078179246496,
        "step": 5052
    },
    {
        "loss": 1.7003,
        "grad_norm": 4.305570125579834,
        "learning_rate": 5.490052526874948e-05,
        "epoch": 0.6497364022116497,
        "step": 5053
    },
    {
        "loss": 1.9116,
        "grad_norm": 2.3634414672851562,
        "learning_rate": 5.4864404833066763e-05,
        "epoch": 0.6498649864986499,
        "step": 5054
    },
    {
        "loss": 2.1461,
        "grad_norm": 1.9827467203140259,
        "learning_rate": 5.4828291791251064e-05,
        "epoch": 0.64999357078565,
        "step": 5055
    },
    {
        "loss": 1.6616,
        "grad_norm": 2.208090305328369,
        "learning_rate": 5.4792186149218286e-05,
        "epoch": 0.6501221550726501,
        "step": 5056
    },
    {
        "loss": 2.2196,
        "grad_norm": 1.3929723501205444,
        "learning_rate": 5.475608791288297e-05,
        "epoch": 0.6502507393596503,
        "step": 5057
    },
    {
        "loss": 1.7392,
        "grad_norm": 2.012824296951294,
        "learning_rate": 5.471999708815858e-05,
        "epoch": 0.6503793236466504,
        "step": 5058
    },
    {
        "loss": 0.9807,
        "grad_norm": 2.529359817504883,
        "learning_rate": 5.468391368095734e-05,
        "epoch": 0.6505079079336505,
        "step": 5059
    },
    {
        "loss": 1.6626,
        "grad_norm": 4.556376934051514,
        "learning_rate": 5.464783769719017e-05,
        "epoch": 0.6506364922206507,
        "step": 5060
    },
    {
        "loss": 1.8742,
        "grad_norm": 2.7307326793670654,
        "learning_rate": 5.461176914276689e-05,
        "epoch": 0.6507650765076508,
        "step": 5061
    },
    {
        "loss": 1.9526,
        "grad_norm": 1.562036156654358,
        "learning_rate": 5.457570802359604e-05,
        "epoch": 0.6508936607946509,
        "step": 5062
    },
    {
        "loss": 1.5947,
        "grad_norm": 2.0170998573303223,
        "learning_rate": 5.453965434558499e-05,
        "epoch": 0.6510222450816511,
        "step": 5063
    },
    {
        "loss": 1.9392,
        "grad_norm": 2.3902995586395264,
        "learning_rate": 5.450360811463979e-05,
        "epoch": 0.6511508293686511,
        "step": 5064
    },
    {
        "loss": 1.8707,
        "grad_norm": 1.7654294967651367,
        "learning_rate": 5.446756933666539e-05,
        "epoch": 0.6512794136556512,
        "step": 5065
    },
    {
        "loss": 1.4097,
        "grad_norm": 1.6129815578460693,
        "learning_rate": 5.4431538017565464e-05,
        "epoch": 0.6514079979426514,
        "step": 5066
    },
    {
        "loss": 1.2136,
        "grad_norm": 3.7232229709625244,
        "learning_rate": 5.439551416324242e-05,
        "epoch": 0.6515365822296515,
        "step": 5067
    },
    {
        "loss": 1.2141,
        "grad_norm": 2.9256811141967773,
        "learning_rate": 5.4359497779597536e-05,
        "epoch": 0.6516651665166516,
        "step": 5068
    },
    {
        "loss": 1.7893,
        "grad_norm": 2.2187092304229736,
        "learning_rate": 5.432348887253079e-05,
        "epoch": 0.6517937508036518,
        "step": 5069
    },
    {
        "loss": 1.0158,
        "grad_norm": 1.8977296352386475,
        "learning_rate": 5.4287487447941024e-05,
        "epoch": 0.6519223350906519,
        "step": 5070
    },
    {
        "loss": 1.1078,
        "grad_norm": 2.217543601989746,
        "learning_rate": 5.425149351172571e-05,
        "epoch": 0.652050919377652,
        "step": 5071
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.0326356887817383,
        "learning_rate": 5.42155070697812e-05,
        "epoch": 0.6521795036646522,
        "step": 5072
    },
    {
        "loss": 1.5297,
        "grad_norm": 1.727452039718628,
        "learning_rate": 5.417952812800268e-05,
        "epoch": 0.6523080879516523,
        "step": 5073
    },
    {
        "loss": 2.1012,
        "grad_norm": 1.7994379997253418,
        "learning_rate": 5.414355669228387e-05,
        "epoch": 0.6524366722386524,
        "step": 5074
    },
    {
        "loss": 0.707,
        "grad_norm": 2.423583507537842,
        "learning_rate": 5.4107592768517575e-05,
        "epoch": 0.6525652565256526,
        "step": 5075
    },
    {
        "loss": 1.7644,
        "grad_norm": 1.6085437536239624,
        "learning_rate": 5.407163636259516e-05,
        "epoch": 0.6526938408126527,
        "step": 5076
    },
    {
        "loss": 1.5482,
        "grad_norm": 2.0731587409973145,
        "learning_rate": 5.403568748040673e-05,
        "epoch": 0.6528224250996528,
        "step": 5077
    },
    {
        "loss": 2.0479,
        "grad_norm": 1.7510261535644531,
        "learning_rate": 5.3999746127841277e-05,
        "epoch": 0.652951009386653,
        "step": 5078
    },
    {
        "loss": 2.3027,
        "grad_norm": 1.9599733352661133,
        "learning_rate": 5.3963812310786545e-05,
        "epoch": 0.6530795936736531,
        "step": 5079
    },
    {
        "loss": 1.0554,
        "grad_norm": 1.4034711122512817,
        "learning_rate": 5.3927886035129036e-05,
        "epoch": 0.6532081779606532,
        "step": 5080
    },
    {
        "loss": 2.0363,
        "grad_norm": 1.439583420753479,
        "learning_rate": 5.389196730675392e-05,
        "epoch": 0.6533367622476534,
        "step": 5081
    },
    {
        "loss": 1.6804,
        "grad_norm": 3.0494890213012695,
        "learning_rate": 5.3856056131545254e-05,
        "epoch": 0.6534653465346535,
        "step": 5082
    },
    {
        "loss": 1.4766,
        "grad_norm": 3.2554943561553955,
        "learning_rate": 5.3820152515385835e-05,
        "epoch": 0.6535939308216536,
        "step": 5083
    },
    {
        "loss": 1.0519,
        "grad_norm": 2.094862699508667,
        "learning_rate": 5.37842564641571e-05,
        "epoch": 0.6537225151086538,
        "step": 5084
    },
    {
        "loss": 1.6706,
        "grad_norm": 2.306580066680908,
        "learning_rate": 5.374836798373949e-05,
        "epoch": 0.6538510993956539,
        "step": 5085
    },
    {
        "loss": 1.2677,
        "grad_norm": 2.0211617946624756,
        "learning_rate": 5.3712487080011975e-05,
        "epoch": 0.653979683682654,
        "step": 5086
    },
    {
        "loss": 2.2316,
        "grad_norm": 2.36683988571167,
        "learning_rate": 5.3676613758852314e-05,
        "epoch": 0.6541082679696542,
        "step": 5087
    },
    {
        "loss": 2.662,
        "grad_norm": 2.2636640071868896,
        "learning_rate": 5.36407480261372e-05,
        "epoch": 0.6542368522566543,
        "step": 5088
    },
    {
        "loss": 1.0391,
        "grad_norm": 1.9982151985168457,
        "learning_rate": 5.3604889887741875e-05,
        "epoch": 0.6543654365436543,
        "step": 5089
    },
    {
        "loss": 0.8415,
        "grad_norm": 2.1586766242980957,
        "learning_rate": 5.356903934954045e-05,
        "epoch": 0.6544940208306544,
        "step": 5090
    },
    {
        "loss": 2.0249,
        "grad_norm": 4.8794779777526855,
        "learning_rate": 5.3533196417405795e-05,
        "epoch": 0.6546226051176546,
        "step": 5091
    },
    {
        "loss": 2.011,
        "grad_norm": 1.6678071022033691,
        "learning_rate": 5.3497361097209445e-05,
        "epoch": 0.6547511894046547,
        "step": 5092
    },
    {
        "loss": 2.052,
        "grad_norm": 1.1160260438919067,
        "learning_rate": 5.346153339482178e-05,
        "epoch": 0.6548797736916548,
        "step": 5093
    },
    {
        "loss": 1.8167,
        "grad_norm": 1.993658423423767,
        "learning_rate": 5.342571331611188e-05,
        "epoch": 0.655008357978655,
        "step": 5094
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.9611592292785645,
        "learning_rate": 5.338990086694766e-05,
        "epoch": 0.6551369422656551,
        "step": 5095
    },
    {
        "loss": 2.2195,
        "grad_norm": 1.7580983638763428,
        "learning_rate": 5.3354096053195635e-05,
        "epoch": 0.6552655265526552,
        "step": 5096
    },
    {
        "loss": 1.7068,
        "grad_norm": 1.8520511388778687,
        "learning_rate": 5.33182988807212e-05,
        "epoch": 0.6553941108396554,
        "step": 5097
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.7027887105941772,
        "learning_rate": 5.3282509355388456e-05,
        "epoch": 0.6555226951266555,
        "step": 5098
    },
    {
        "loss": 1.2759,
        "grad_norm": 1.9143645763397217,
        "learning_rate": 5.324672748306021e-05,
        "epoch": 0.6556512794136556,
        "step": 5099
    },
    {
        "loss": 1.3686,
        "grad_norm": 1.8472681045532227,
        "learning_rate": 5.321095326959807e-05,
        "epoch": 0.6557798637006558,
        "step": 5100
    },
    {
        "eval_loss": 1.7939035892486572,
        "eval_runtime": 28.2675,
        "eval_samples_per_second": 2.795,
        "eval_steps_per_second": 2.795,
        "epoch": 0.6557798637006558,
        "step": 5100
    },
    {
        "loss": 1.9924,
        "grad_norm": 1.9862546920776367,
        "learning_rate": 5.317518672086242e-05,
        "epoch": 0.6559084479876559,
        "step": 5101
    },
    {
        "loss": 1.7978,
        "grad_norm": 1.9619654417037964,
        "learning_rate": 5.313942784271226e-05,
        "epoch": 0.656037032274656,
        "step": 5102
    },
    {
        "loss": 2.2494,
        "grad_norm": 2.702509641647339,
        "learning_rate": 5.310367664100546e-05,
        "epoch": 0.6561656165616562,
        "step": 5103
    },
    {
        "loss": 2.3877,
        "grad_norm": 1.2952467203140259,
        "learning_rate": 5.306793312159859e-05,
        "epoch": 0.6562942008486563,
        "step": 5104
    },
    {
        "loss": 2.1079,
        "grad_norm": 2.13326358795166,
        "learning_rate": 5.3032197290346986e-05,
        "epoch": 0.6564227851356564,
        "step": 5105
    },
    {
        "loss": 1.1585,
        "grad_norm": 1.3800848722457886,
        "learning_rate": 5.299646915310461e-05,
        "epoch": 0.6565513694226566,
        "step": 5106
    },
    {
        "loss": 2.5863,
        "grad_norm": 1.5471012592315674,
        "learning_rate": 5.296074871572431e-05,
        "epoch": 0.6566799537096567,
        "step": 5107
    },
    {
        "loss": 2.0993,
        "grad_norm": 2.3896052837371826,
        "learning_rate": 5.2925035984057645e-05,
        "epoch": 0.6568085379966568,
        "step": 5108
    },
    {
        "loss": 2.0007,
        "grad_norm": 1.9391541481018066,
        "learning_rate": 5.2889330963954784e-05,
        "epoch": 0.656937122283657,
        "step": 5109
    },
    {
        "loss": 1.6822,
        "grad_norm": 2.484699249267578,
        "learning_rate": 5.2853633661264814e-05,
        "epoch": 0.6570657065706571,
        "step": 5110
    },
    {
        "loss": 1.4391,
        "grad_norm": 2.3604819774627686,
        "learning_rate": 5.2817944081835466e-05,
        "epoch": 0.6571942908576572,
        "step": 5111
    },
    {
        "loss": 1.3576,
        "grad_norm": 1.2294121980667114,
        "learning_rate": 5.2782262231513146e-05,
        "epoch": 0.6573228751446574,
        "step": 5112
    },
    {
        "loss": 1.9393,
        "grad_norm": 1.802107810974121,
        "learning_rate": 5.274658811614311e-05,
        "epoch": 0.6574514594316575,
        "step": 5113
    },
    {
        "loss": 0.7432,
        "grad_norm": 1.7357993125915527,
        "learning_rate": 5.271092174156928e-05,
        "epoch": 0.6575800437186575,
        "step": 5114
    },
    {
        "loss": 1.5223,
        "grad_norm": 1.6314212083816528,
        "learning_rate": 5.26752631136344e-05,
        "epoch": 0.6577086280056577,
        "step": 5115
    },
    {
        "loss": 2.208,
        "grad_norm": 1.8830158710479736,
        "learning_rate": 5.263961223817976e-05,
        "epoch": 0.6578372122926578,
        "step": 5116
    },
    {
        "loss": 2.4428,
        "grad_norm": 1.6670629978179932,
        "learning_rate": 5.2603969121045536e-05,
        "epoch": 0.6579657965796579,
        "step": 5117
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.3011648654937744,
        "learning_rate": 5.2568333768070645e-05,
        "epoch": 0.6580943808666581,
        "step": 5118
    },
    {
        "loss": 1.6323,
        "grad_norm": 2.369489908218384,
        "learning_rate": 5.2532706185092565e-05,
        "epoch": 0.6582229651536582,
        "step": 5119
    },
    {
        "loss": 1.6833,
        "grad_norm": 2.4017481803894043,
        "learning_rate": 5.249708637794769e-05,
        "epoch": 0.6583515494406583,
        "step": 5120
    },
    {
        "loss": 1.8891,
        "grad_norm": 2.4042975902557373,
        "learning_rate": 5.246147435247104e-05,
        "epoch": 0.6584801337276585,
        "step": 5121
    },
    {
        "loss": 1.7107,
        "grad_norm": 1.4935117959976196,
        "learning_rate": 5.242587011449643e-05,
        "epoch": 0.6586087180146586,
        "step": 5122
    },
    {
        "loss": 2.1344,
        "grad_norm": 1.8698536157608032,
        "learning_rate": 5.239027366985627e-05,
        "epoch": 0.6587373023016587,
        "step": 5123
    },
    {
        "loss": 2.2189,
        "grad_norm": 1.9601918458938599,
        "learning_rate": 5.235468502438181e-05,
        "epoch": 0.6588658865886589,
        "step": 5124
    },
    {
        "loss": 1.1247,
        "grad_norm": 2.4535253047943115,
        "learning_rate": 5.2319104183903046e-05,
        "epoch": 0.658994470875659,
        "step": 5125
    },
    {
        "loss": 2.0033,
        "grad_norm": 2.0388758182525635,
        "learning_rate": 5.22835311542485e-05,
        "epoch": 0.6591230551626591,
        "step": 5126
    },
    {
        "loss": 2.425,
        "grad_norm": 1.6490098237991333,
        "learning_rate": 5.2247965941245714e-05,
        "epoch": 0.6592516394496593,
        "step": 5127
    },
    {
        "loss": 1.0679,
        "grad_norm": 1.7714800834655762,
        "learning_rate": 5.2212408550720714e-05,
        "epoch": 0.6593802237366594,
        "step": 5128
    },
    {
        "loss": 1.5894,
        "grad_norm": 2.060432195663452,
        "learning_rate": 5.217685898849822e-05,
        "epoch": 0.6595088080236595,
        "step": 5129
    },
    {
        "loss": 1.4806,
        "grad_norm": 2.361783504486084,
        "learning_rate": 5.214131726040196e-05,
        "epoch": 0.6596373923106597,
        "step": 5130
    },
    {
        "loss": 2.4248,
        "grad_norm": 1.6924655437469482,
        "learning_rate": 5.210578337225404e-05,
        "epoch": 0.6597659765976598,
        "step": 5131
    },
    {
        "loss": 1.9641,
        "grad_norm": 2.238892078399658,
        "learning_rate": 5.2070257329875474e-05,
        "epoch": 0.6598945608846599,
        "step": 5132
    },
    {
        "loss": 1.5687,
        "grad_norm": 1.8620954751968384,
        "learning_rate": 5.2034739139085986e-05,
        "epoch": 0.66002314517166,
        "step": 5133
    },
    {
        "loss": 1.9468,
        "grad_norm": 2.1374833583831787,
        "learning_rate": 5.199922880570389e-05,
        "epoch": 0.6601517294586602,
        "step": 5134
    },
    {
        "loss": 1.7219,
        "grad_norm": 2.030470848083496,
        "learning_rate": 5.196372633554634e-05,
        "epoch": 0.6602803137456603,
        "step": 5135
    },
    {
        "loss": 2.0181,
        "grad_norm": 2.305823564529419,
        "learning_rate": 5.1928231734429146e-05,
        "epoch": 0.6604088980326605,
        "step": 5136
    },
    {
        "loss": 1.9,
        "grad_norm": 2.165541172027588,
        "learning_rate": 5.189274500816689e-05,
        "epoch": 0.6605374823196606,
        "step": 5137
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.8157165050506592,
        "learning_rate": 5.185726616257277e-05,
        "epoch": 0.6606660666066607,
        "step": 5138
    },
    {
        "loss": 1.9463,
        "grad_norm": 1.8486865758895874,
        "learning_rate": 5.182179520345867e-05,
        "epoch": 0.6607946508936607,
        "step": 5139
    },
    {
        "loss": 2.0968,
        "grad_norm": 1.833718180656433,
        "learning_rate": 5.178633213663538e-05,
        "epoch": 0.6609232351806609,
        "step": 5140
    },
    {
        "loss": 1.916,
        "grad_norm": 1.9148911237716675,
        "learning_rate": 5.1750876967912186e-05,
        "epoch": 0.661051819467661,
        "step": 5141
    },
    {
        "loss": 0.8665,
        "grad_norm": 2.2270309925079346,
        "learning_rate": 5.171542970309716e-05,
        "epoch": 0.6611804037546611,
        "step": 5142
    },
    {
        "loss": 2.3864,
        "grad_norm": 1.6873185634613037,
        "learning_rate": 5.1679990347997157e-05,
        "epoch": 0.6613089880416613,
        "step": 5143
    },
    {
        "loss": 1.678,
        "grad_norm": 2.634941816329956,
        "learning_rate": 5.164455890841753e-05,
        "epoch": 0.6614375723286614,
        "step": 5144
    },
    {
        "loss": 1.6807,
        "grad_norm": 2.536163091659546,
        "learning_rate": 5.1609135390162555e-05,
        "epoch": 0.6615661566156615,
        "step": 5145
    },
    {
        "loss": 1.9806,
        "grad_norm": 1.911331057548523,
        "learning_rate": 5.157371979903509e-05,
        "epoch": 0.6616947409026617,
        "step": 5146
    },
    {
        "loss": 2.1843,
        "grad_norm": 1.920919418334961,
        "learning_rate": 5.1538312140836785e-05,
        "epoch": 0.6618233251896618,
        "step": 5147
    },
    {
        "loss": 2.3878,
        "grad_norm": 1.5186874866485596,
        "learning_rate": 5.150291242136782e-05,
        "epoch": 0.6619519094766619,
        "step": 5148
    },
    {
        "loss": 1.5101,
        "grad_norm": 2.221980571746826,
        "learning_rate": 5.146752064642726e-05,
        "epoch": 0.6620804937636621,
        "step": 5149
    },
    {
        "loss": 1.3865,
        "grad_norm": 2.5610475540161133,
        "learning_rate": 5.14321368218128e-05,
        "epoch": 0.6622090780506622,
        "step": 5150
    },
    {
        "loss": 1.9456,
        "grad_norm": 2.301351308822632,
        "learning_rate": 5.139676095332077e-05,
        "epoch": 0.6623376623376623,
        "step": 5151
    },
    {
        "loss": 1.657,
        "grad_norm": 2.4060349464416504,
        "learning_rate": 5.136139304674628e-05,
        "epoch": 0.6624662466246625,
        "step": 5152
    },
    {
        "loss": 2.2539,
        "grad_norm": 1.3123583793640137,
        "learning_rate": 5.132603310788313e-05,
        "epoch": 0.6625948309116626,
        "step": 5153
    },
    {
        "loss": 1.4423,
        "grad_norm": 2.584033966064453,
        "learning_rate": 5.129068114252373e-05,
        "epoch": 0.6627234151986627,
        "step": 5154
    },
    {
        "loss": 1.8116,
        "grad_norm": 2.2242374420166016,
        "learning_rate": 5.1255337156459316e-05,
        "epoch": 0.6628519994856629,
        "step": 5155
    },
    {
        "loss": 1.9277,
        "grad_norm": 2.0044827461242676,
        "learning_rate": 5.122000115547969e-05,
        "epoch": 0.662980583772663,
        "step": 5156
    },
    {
        "loss": 0.9983,
        "grad_norm": 2.7460291385650635,
        "learning_rate": 5.118467314537348e-05,
        "epoch": 0.6631091680596631,
        "step": 5157
    },
    {
        "loss": 1.8242,
        "grad_norm": 3.3892507553100586,
        "learning_rate": 5.1149353131927835e-05,
        "epoch": 0.6632377523466633,
        "step": 5158
    },
    {
        "loss": 2.1593,
        "grad_norm": 1.7033272981643677,
        "learning_rate": 5.1114041120928743e-05,
        "epoch": 0.6633663366336634,
        "step": 5159
    },
    {
        "loss": 1.2353,
        "grad_norm": 3.1225407123565674,
        "learning_rate": 5.107873711816085e-05,
        "epoch": 0.6634949209206635,
        "step": 5160
    },
    {
        "loss": 2.258,
        "grad_norm": 2.340627908706665,
        "learning_rate": 5.1043441129407375e-05,
        "epoch": 0.6636235052076637,
        "step": 5161
    },
    {
        "loss": 2.3223,
        "grad_norm": 2.24759578704834,
        "learning_rate": 5.100815316045037e-05,
        "epoch": 0.6637520894946638,
        "step": 5162
    },
    {
        "loss": 1.9326,
        "grad_norm": 2.2958154678344727,
        "learning_rate": 5.097287321707056e-05,
        "epoch": 0.6638806737816639,
        "step": 5163
    },
    {
        "loss": 1.4969,
        "grad_norm": 2.5274765491485596,
        "learning_rate": 5.093760130504722e-05,
        "epoch": 0.664009258068664,
        "step": 5164
    },
    {
        "loss": 2.0311,
        "grad_norm": 1.715991497039795,
        "learning_rate": 5.0902337430158455e-05,
        "epoch": 0.6641378423556641,
        "step": 5165
    },
    {
        "loss": 1.2122,
        "grad_norm": 2.532836675643921,
        "learning_rate": 5.0867081598181e-05,
        "epoch": 0.6642664266426642,
        "step": 5166
    },
    {
        "loss": 1.9228,
        "grad_norm": 1.1301063299179077,
        "learning_rate": 5.08318338148903e-05,
        "epoch": 0.6643950109296644,
        "step": 5167
    },
    {
        "loss": 1.5223,
        "grad_norm": 2.5754752159118652,
        "learning_rate": 5.079659408606035e-05,
        "epoch": 0.6645235952166645,
        "step": 5168
    },
    {
        "loss": 2.1948,
        "grad_norm": 1.6573407649993896,
        "learning_rate": 5.076136241746408e-05,
        "epoch": 0.6646521795036646,
        "step": 5169
    },
    {
        "loss": 1.5453,
        "grad_norm": 2.6714818477630615,
        "learning_rate": 5.072613881487287e-05,
        "epoch": 0.6647807637906648,
        "step": 5170
    },
    {
        "loss": 1.7558,
        "grad_norm": 1.9468644857406616,
        "learning_rate": 5.069092328405679e-05,
        "epoch": 0.6649093480776649,
        "step": 5171
    },
    {
        "loss": 1.0006,
        "grad_norm": 2.9649016857147217,
        "learning_rate": 5.06557158307848e-05,
        "epoch": 0.665037932364665,
        "step": 5172
    },
    {
        "loss": 2.0868,
        "grad_norm": 1.7566704750061035,
        "learning_rate": 5.062051646082429e-05,
        "epoch": 0.6651665166516652,
        "step": 5173
    },
    {
        "loss": 1.3406,
        "grad_norm": 1.9409440755844116,
        "learning_rate": 5.058532517994146e-05,
        "epoch": 0.6652951009386653,
        "step": 5174
    },
    {
        "loss": 1.9477,
        "grad_norm": 2.010915994644165,
        "learning_rate": 5.055014199390118e-05,
        "epoch": 0.6654236852256654,
        "step": 5175
    },
    {
        "loss": 2.0541,
        "grad_norm": 1.2323352098464966,
        "learning_rate": 5.051496690846691e-05,
        "epoch": 0.6655522695126656,
        "step": 5176
    },
    {
        "loss": 1.4856,
        "grad_norm": 1.9417858123779297,
        "learning_rate": 5.0479799929400906e-05,
        "epoch": 0.6656808537996657,
        "step": 5177
    },
    {
        "loss": 2.0991,
        "grad_norm": 1.2826348543167114,
        "learning_rate": 5.0444641062463916e-05,
        "epoch": 0.6658094380866658,
        "step": 5178
    },
    {
        "loss": 1.4301,
        "grad_norm": 3.307028293609619,
        "learning_rate": 5.040949031341564e-05,
        "epoch": 0.665938022373666,
        "step": 5179
    },
    {
        "loss": 1.9476,
        "grad_norm": 1.8289026021957397,
        "learning_rate": 5.037434768801419e-05,
        "epoch": 0.6660666066606661,
        "step": 5180
    },
    {
        "loss": 2.3646,
        "grad_norm": 1.3049867153167725,
        "learning_rate": 5.0339213192016375e-05,
        "epoch": 0.6661951909476662,
        "step": 5181
    },
    {
        "loss": 1.8218,
        "grad_norm": 1.9048171043395996,
        "learning_rate": 5.0304086831177886e-05,
        "epoch": 0.6663237752346663,
        "step": 5182
    },
    {
        "loss": 1.7446,
        "grad_norm": 1.587228775024414,
        "learning_rate": 5.026896861125281e-05,
        "epoch": 0.6664523595216665,
        "step": 5183
    },
    {
        "loss": 1.098,
        "grad_norm": 2.1178226470947266,
        "learning_rate": 5.023385853799406e-05,
        "epoch": 0.6665809438086666,
        "step": 5184
    },
    {
        "loss": 1.5564,
        "grad_norm": 1.83589768409729,
        "learning_rate": 5.019875661715323e-05,
        "epoch": 0.6667095280956667,
        "step": 5185
    },
    {
        "loss": 1.7352,
        "grad_norm": 1.987630009651184,
        "learning_rate": 5.016366285448042e-05,
        "epoch": 0.6668381123826669,
        "step": 5186
    },
    {
        "loss": 1.7304,
        "grad_norm": 1.8376470804214478,
        "learning_rate": 5.0128577255724577e-05,
        "epoch": 0.666966696669667,
        "step": 5187
    },
    {
        "loss": 1.6038,
        "grad_norm": 2.8544163703918457,
        "learning_rate": 5.0093499826633204e-05,
        "epoch": 0.6670952809566671,
        "step": 5188
    },
    {
        "loss": 1.5093,
        "grad_norm": 2.505223035812378,
        "learning_rate": 5.0058430572952544e-05,
        "epoch": 0.6672238652436672,
        "step": 5189
    },
    {
        "loss": 2.0188,
        "grad_norm": 2.4130539894104004,
        "learning_rate": 5.0023369500427365e-05,
        "epoch": 0.6673524495306673,
        "step": 5190
    },
    {
        "loss": 1.9564,
        "grad_norm": 1.2837512493133545,
        "learning_rate": 4.9988316614801213e-05,
        "epoch": 0.6674810338176674,
        "step": 5191
    },
    {
        "loss": 2.0748,
        "grad_norm": 1.484566569328308,
        "learning_rate": 4.995327192181632e-05,
        "epoch": 0.6676096181046676,
        "step": 5192
    },
    {
        "loss": 1.1971,
        "grad_norm": 1.4212836027145386,
        "learning_rate": 4.9918235427213435e-05,
        "epoch": 0.6677382023916677,
        "step": 5193
    },
    {
        "loss": 2.0827,
        "grad_norm": 2.1183905601501465,
        "learning_rate": 4.9883207136732066e-05,
        "epoch": 0.6678667866786678,
        "step": 5194
    },
    {
        "loss": 1.0331,
        "grad_norm": 1.497621774673462,
        "learning_rate": 4.9848187056110416e-05,
        "epoch": 0.667995370965668,
        "step": 5195
    },
    {
        "loss": 1.8205,
        "grad_norm": 2.2223496437072754,
        "learning_rate": 4.9813175191085196e-05,
        "epoch": 0.6681239552526681,
        "step": 5196
    },
    {
        "loss": 1.7238,
        "grad_norm": 2.2910938262939453,
        "learning_rate": 4.97781715473919e-05,
        "epoch": 0.6682525395396682,
        "step": 5197
    },
    {
        "loss": 1.6297,
        "grad_norm": 2.5669798851013184,
        "learning_rate": 4.974317613076464e-05,
        "epoch": 0.6683811238266684,
        "step": 5198
    },
    {
        "loss": 2.2543,
        "grad_norm": 2.1001670360565186,
        "learning_rate": 4.9708188946936194e-05,
        "epoch": 0.6685097081136685,
        "step": 5199
    },
    {
        "loss": 1.5158,
        "grad_norm": 1.6179487705230713,
        "learning_rate": 4.96732100016379e-05,
        "epoch": 0.6686382924006686,
        "step": 5200
    },
    {
        "eval_loss": 1.7843462228775024,
        "eval_runtime": 28.3043,
        "eval_samples_per_second": 2.791,
        "eval_steps_per_second": 2.791,
        "epoch": 0.6686382924006686,
        "step": 5200
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.721487283706665,
        "learning_rate": 4.9638239300599886e-05,
        "epoch": 0.6687668766876688,
        "step": 5201
    },
    {
        "loss": 1.1946,
        "grad_norm": 1.3660924434661865,
        "learning_rate": 4.960327684955086e-05,
        "epoch": 0.6688954609746689,
        "step": 5202
    },
    {
        "loss": 1.4912,
        "grad_norm": 2.6488001346588135,
        "learning_rate": 4.9568322654218126e-05,
        "epoch": 0.669024045261669,
        "step": 5203
    },
    {
        "loss": 2.2738,
        "grad_norm": 1.7137237787246704,
        "learning_rate": 4.953337672032773e-05,
        "epoch": 0.6691526295486692,
        "step": 5204
    },
    {
        "loss": 2.2016,
        "grad_norm": 1.3236794471740723,
        "learning_rate": 4.949843905360434e-05,
        "epoch": 0.6692812138356693,
        "step": 5205
    },
    {
        "loss": 1.9927,
        "grad_norm": 2.179140090942383,
        "learning_rate": 4.9463509659771204e-05,
        "epoch": 0.6694097981226694,
        "step": 5206
    },
    {
        "loss": 2.6284,
        "grad_norm": 1.5372731685638428,
        "learning_rate": 4.942858854455028e-05,
        "epoch": 0.6695383824096696,
        "step": 5207
    },
    {
        "loss": 1.3685,
        "grad_norm": 1.9467872381210327,
        "learning_rate": 4.939367571366218e-05,
        "epoch": 0.6696669666966697,
        "step": 5208
    },
    {
        "loss": 1.621,
        "grad_norm": 1.9710603952407837,
        "learning_rate": 4.935877117282614e-05,
        "epoch": 0.6697955509836698,
        "step": 5209
    },
    {
        "loss": 1.9374,
        "grad_norm": 2.2291712760925293,
        "learning_rate": 4.932387492776e-05,
        "epoch": 0.66992413527067,
        "step": 5210
    },
    {
        "loss": 2.1642,
        "grad_norm": 1.3144782781600952,
        "learning_rate": 4.9288986984180263e-05,
        "epoch": 0.6700527195576701,
        "step": 5211
    },
    {
        "loss": 2.0196,
        "grad_norm": 2.062544584274292,
        "learning_rate": 4.925410734780215e-05,
        "epoch": 0.6701813038446702,
        "step": 5212
    },
    {
        "loss": 2.2815,
        "grad_norm": 1.4065519571304321,
        "learning_rate": 4.921923602433935e-05,
        "epoch": 0.6703098881316704,
        "step": 5213
    },
    {
        "loss": 1.7386,
        "grad_norm": 2.448268413543701,
        "learning_rate": 4.918437301950437e-05,
        "epoch": 0.6704384724186704,
        "step": 5214
    },
    {
        "loss": 1.9573,
        "grad_norm": 2.637205123901367,
        "learning_rate": 4.9149518339008285e-05,
        "epoch": 0.6705670567056705,
        "step": 5215
    },
    {
        "loss": 1.6476,
        "grad_norm": 1.6177595853805542,
        "learning_rate": 4.9114671988560735e-05,
        "epoch": 0.6706956409926706,
        "step": 5216
    },
    {
        "loss": 1.141,
        "grad_norm": 1.961556315422058,
        "learning_rate": 4.907983397387008e-05,
        "epoch": 0.6708242252796708,
        "step": 5217
    },
    {
        "loss": 1.9096,
        "grad_norm": 2.705134391784668,
        "learning_rate": 4.904500430064331e-05,
        "epoch": 0.6709528095666709,
        "step": 5218
    },
    {
        "loss": 1.5043,
        "grad_norm": 1.734803557395935,
        "learning_rate": 4.9010182974586085e-05,
        "epoch": 0.671081393853671,
        "step": 5219
    },
    {
        "loss": 1.9313,
        "grad_norm": 2.506838083267212,
        "learning_rate": 4.89753700014025e-05,
        "epoch": 0.6712099781406712,
        "step": 5220
    },
    {
        "loss": 2.001,
        "grad_norm": 2.008896589279175,
        "learning_rate": 4.894056538679559e-05,
        "epoch": 0.6713385624276713,
        "step": 5221
    },
    {
        "loss": 1.7635,
        "grad_norm": 1.4300466775894165,
        "learning_rate": 4.890576913646678e-05,
        "epoch": 0.6714671467146714,
        "step": 5222
    },
    {
        "loss": 1.0608,
        "grad_norm": 1.842399001121521,
        "learning_rate": 4.8870981256116134e-05,
        "epoch": 0.6715957310016716,
        "step": 5223
    },
    {
        "loss": 0.8469,
        "grad_norm": 2.0230958461761475,
        "learning_rate": 4.8836201751442556e-05,
        "epoch": 0.6717243152886717,
        "step": 5224
    },
    {
        "loss": 1.8729,
        "grad_norm": 3.4204092025756836,
        "learning_rate": 4.880143062814333e-05,
        "epoch": 0.6718528995756718,
        "step": 5225
    },
    {
        "loss": 2.0001,
        "grad_norm": 2.0052757263183594,
        "learning_rate": 4.87666678919145e-05,
        "epoch": 0.671981483862672,
        "step": 5226
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.958248257637024,
        "learning_rate": 4.8731913548450746e-05,
        "epoch": 0.6721100681496721,
        "step": 5227
    },
    {
        "loss": 1.4118,
        "grad_norm": 2.393021583557129,
        "learning_rate": 4.869716760344526e-05,
        "epoch": 0.6722386524366722,
        "step": 5228
    },
    {
        "loss": 2.1366,
        "grad_norm": 2.4576427936553955,
        "learning_rate": 4.866243006258998e-05,
        "epoch": 0.6723672367236724,
        "step": 5229
    },
    {
        "loss": 1.8769,
        "grad_norm": 2.2685670852661133,
        "learning_rate": 4.8627700931575416e-05,
        "epoch": 0.6724958210106725,
        "step": 5230
    },
    {
        "loss": 1.0562,
        "grad_norm": 2.71768856048584,
        "learning_rate": 4.8592980216090735e-05,
        "epoch": 0.6726244052976726,
        "step": 5231
    },
    {
        "loss": 1.867,
        "grad_norm": 1.8173285722732544,
        "learning_rate": 4.855826792182362e-05,
        "epoch": 0.6727529895846728,
        "step": 5232
    },
    {
        "loss": 2.1622,
        "grad_norm": 1.3079897165298462,
        "learning_rate": 4.8523564054460504e-05,
        "epoch": 0.6728815738716729,
        "step": 5233
    },
    {
        "loss": 2.1769,
        "grad_norm": 1.4029868841171265,
        "learning_rate": 4.84888686196864e-05,
        "epoch": 0.673010158158673,
        "step": 5234
    },
    {
        "loss": 1.7528,
        "grad_norm": 2.074145555496216,
        "learning_rate": 4.845418162318487e-05,
        "epoch": 0.6731387424456732,
        "step": 5235
    },
    {
        "loss": 2.1046,
        "grad_norm": 1.7970370054244995,
        "learning_rate": 4.8419503070638185e-05,
        "epoch": 0.6732673267326733,
        "step": 5236
    },
    {
        "loss": 1.4452,
        "grad_norm": 2.927781581878662,
        "learning_rate": 4.838483296772721e-05,
        "epoch": 0.6733959110196734,
        "step": 5237
    },
    {
        "loss": 2.1563,
        "grad_norm": 1.9803911447525024,
        "learning_rate": 4.835017132013136e-05,
        "epoch": 0.6735244953066736,
        "step": 5238
    },
    {
        "loss": 1.7036,
        "grad_norm": 2.3479344844818115,
        "learning_rate": 4.831551813352876e-05,
        "epoch": 0.6736530795936736,
        "step": 5239
    },
    {
        "loss": 1.4628,
        "grad_norm": 1.8968710899353027,
        "learning_rate": 4.8280873413596085e-05,
        "epoch": 0.6737816638806737,
        "step": 5240
    },
    {
        "loss": 0.9588,
        "grad_norm": 1.5048843622207642,
        "learning_rate": 4.824623716600868e-05,
        "epoch": 0.6739102481676739,
        "step": 5241
    },
    {
        "loss": 1.4111,
        "grad_norm": 1.9177230596542358,
        "learning_rate": 4.821160939644042e-05,
        "epoch": 0.674038832454674,
        "step": 5242
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.531278133392334,
        "learning_rate": 4.817699011056385e-05,
        "epoch": 0.6741674167416741,
        "step": 5243
    },
    {
        "loss": 1.742,
        "grad_norm": 1.7697806358337402,
        "learning_rate": 4.814237931405015e-05,
        "epoch": 0.6742960010286743,
        "step": 5244
    },
    {
        "loss": 2.3875,
        "grad_norm": 1.1434855461120605,
        "learning_rate": 4.810777701256901e-05,
        "epoch": 0.6744245853156744,
        "step": 5245
    },
    {
        "loss": 1.163,
        "grad_norm": 2.3289453983306885,
        "learning_rate": 4.807318321178882e-05,
        "epoch": 0.6745531696026745,
        "step": 5246
    },
    {
        "loss": 1.7517,
        "grad_norm": 2.444709300994873,
        "learning_rate": 4.803859791737659e-05,
        "epoch": 0.6746817538896747,
        "step": 5247
    },
    {
        "loss": 1.4472,
        "grad_norm": 1.2944234609603882,
        "learning_rate": 4.8004021134997815e-05,
        "epoch": 0.6748103381766748,
        "step": 5248
    },
    {
        "loss": 1.7331,
        "grad_norm": 1.5635364055633545,
        "learning_rate": 4.796945287031671e-05,
        "epoch": 0.6749389224636749,
        "step": 5249
    },
    {
        "loss": 1.8827,
        "grad_norm": 2.0863077640533447,
        "learning_rate": 4.793489312899607e-05,
        "epoch": 0.6750675067506751,
        "step": 5250
    },
    {
        "loss": 1.0864,
        "grad_norm": 2.044498920440674,
        "learning_rate": 4.790034191669731e-05,
        "epoch": 0.6751960910376752,
        "step": 5251
    },
    {
        "loss": 1.8742,
        "grad_norm": 1.6152070760726929,
        "learning_rate": 4.7865799239080344e-05,
        "epoch": 0.6753246753246753,
        "step": 5252
    },
    {
        "loss": 0.9647,
        "grad_norm": 1.9102102518081665,
        "learning_rate": 4.783126510180382e-05,
        "epoch": 0.6754532596116755,
        "step": 5253
    },
    {
        "loss": 1.6434,
        "grad_norm": 1.5687772035598755,
        "learning_rate": 4.7796739510524954e-05,
        "epoch": 0.6755818438986756,
        "step": 5254
    },
    {
        "loss": 1.3005,
        "grad_norm": 1.5429930686950684,
        "learning_rate": 4.7762222470899465e-05,
        "epoch": 0.6757104281856757,
        "step": 5255
    },
    {
        "loss": 1.1111,
        "grad_norm": 1.4751838445663452,
        "learning_rate": 4.772771398858179e-05,
        "epoch": 0.6758390124726759,
        "step": 5256
    },
    {
        "loss": 2.2863,
        "grad_norm": 3.215747117996216,
        "learning_rate": 4.7693214069224965e-05,
        "epoch": 0.675967596759676,
        "step": 5257
    },
    {
        "loss": 2.1236,
        "grad_norm": 2.1851906776428223,
        "learning_rate": 4.765872271848048e-05,
        "epoch": 0.6760961810466761,
        "step": 5258
    },
    {
        "loss": 1.42,
        "grad_norm": 1.9973477125167847,
        "learning_rate": 4.7624239941998585e-05,
        "epoch": 0.6762247653336763,
        "step": 5259
    },
    {
        "loss": 1.8617,
        "grad_norm": 1.8224437236785889,
        "learning_rate": 4.758976574542804e-05,
        "epoch": 0.6763533496206764,
        "step": 5260
    },
    {
        "loss": 1.6272,
        "grad_norm": 1.324336051940918,
        "learning_rate": 4.755530013441627e-05,
        "epoch": 0.6764819339076765,
        "step": 5261
    },
    {
        "loss": 2.0394,
        "grad_norm": 2.0164523124694824,
        "learning_rate": 4.752084311460916e-05,
        "epoch": 0.6766105181946767,
        "step": 5262
    },
    {
        "loss": 1.3473,
        "grad_norm": 2.382573366165161,
        "learning_rate": 4.7486394691651316e-05,
        "epoch": 0.6767391024816768,
        "step": 5263
    },
    {
        "loss": 1.7033,
        "grad_norm": 1.7457541227340698,
        "learning_rate": 4.745195487118591e-05,
        "epoch": 0.6768676867686768,
        "step": 5264
    },
    {
        "loss": 1.7782,
        "grad_norm": 2.022883415222168,
        "learning_rate": 4.7417523658854604e-05,
        "epoch": 0.676996271055677,
        "step": 5265
    },
    {
        "loss": 1.8293,
        "grad_norm": 2.04779052734375,
        "learning_rate": 4.738310106029785e-05,
        "epoch": 0.6771248553426771,
        "step": 5266
    },
    {
        "loss": 1.7476,
        "grad_norm": 2.773958683013916,
        "learning_rate": 4.7348687081154485e-05,
        "epoch": 0.6772534396296772,
        "step": 5267
    },
    {
        "loss": 2.0362,
        "grad_norm": 1.8286330699920654,
        "learning_rate": 4.7314281727062036e-05,
        "epoch": 0.6773820239166773,
        "step": 5268
    },
    {
        "loss": 1.4619,
        "grad_norm": 3.5096240043640137,
        "learning_rate": 4.727988500365664e-05,
        "epoch": 0.6775106082036775,
        "step": 5269
    },
    {
        "loss": 1.384,
        "grad_norm": 2.6613593101501465,
        "learning_rate": 4.724549691657293e-05,
        "epoch": 0.6776391924906776,
        "step": 5270
    },
    {
        "loss": 2.1219,
        "grad_norm": 2.1056067943573,
        "learning_rate": 4.721111747144418e-05,
        "epoch": 0.6777677767776777,
        "step": 5271
    },
    {
        "loss": 1.5615,
        "grad_norm": 2.984088897705078,
        "learning_rate": 4.717674667390226e-05,
        "epoch": 0.6778963610646779,
        "step": 5272
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.6975421905517578,
        "learning_rate": 4.7142384529577644e-05,
        "epoch": 0.678024945351678,
        "step": 5273
    },
    {
        "loss": 1.007,
        "grad_norm": 3.1791741847991943,
        "learning_rate": 4.710803104409931e-05,
        "epoch": 0.6781535296386781,
        "step": 5274
    },
    {
        "loss": 2.1958,
        "grad_norm": 1.577136516571045,
        "learning_rate": 4.7073686223094795e-05,
        "epoch": 0.6782821139256783,
        "step": 5275
    },
    {
        "loss": 1.7922,
        "grad_norm": 2.201929807662964,
        "learning_rate": 4.70393500721904e-05,
        "epoch": 0.6784106982126784,
        "step": 5276
    },
    {
        "loss": 1.997,
        "grad_norm": 2.2796757221221924,
        "learning_rate": 4.7005022597010805e-05,
        "epoch": 0.6785392824996785,
        "step": 5277
    },
    {
        "loss": 1.6979,
        "grad_norm": 1.7208263874053955,
        "learning_rate": 4.697070380317937e-05,
        "epoch": 0.6786678667866787,
        "step": 5278
    },
    {
        "loss": 2.1527,
        "grad_norm": 3.144312858581543,
        "learning_rate": 4.693639369631807e-05,
        "epoch": 0.6787964510736788,
        "step": 5279
    },
    {
        "loss": 0.6553,
        "grad_norm": 1.7262732982635498,
        "learning_rate": 4.690209228204729e-05,
        "epoch": 0.6789250353606789,
        "step": 5280
    },
    {
        "loss": 1.0716,
        "grad_norm": 2.1341793537139893,
        "learning_rate": 4.686779956598617e-05,
        "epoch": 0.6790536196476791,
        "step": 5281
    },
    {
        "loss": 1.1715,
        "grad_norm": 1.9761412143707275,
        "learning_rate": 4.683351555375235e-05,
        "epoch": 0.6791822039346792,
        "step": 5282
    },
    {
        "loss": 1.7091,
        "grad_norm": 2.2902305126190186,
        "learning_rate": 4.679924025096207e-05,
        "epoch": 0.6793107882216793,
        "step": 5283
    },
    {
        "loss": 1.5891,
        "grad_norm": 2.101905345916748,
        "learning_rate": 4.676497366323006e-05,
        "epoch": 0.6794393725086795,
        "step": 5284
    },
    {
        "loss": 1.4617,
        "grad_norm": 2.352933883666992,
        "learning_rate": 4.6730715796169734e-05,
        "epoch": 0.6795679567956796,
        "step": 5285
    },
    {
        "loss": 0.6166,
        "grad_norm": 1.864645004272461,
        "learning_rate": 4.669646665539305e-05,
        "epoch": 0.6796965410826797,
        "step": 5286
    },
    {
        "loss": 1.6648,
        "grad_norm": 2.589171886444092,
        "learning_rate": 4.666222624651044e-05,
        "epoch": 0.6798251253696799,
        "step": 5287
    },
    {
        "loss": 1.6391,
        "grad_norm": 1.6903517246246338,
        "learning_rate": 4.6627994575131026e-05,
        "epoch": 0.67995370965668,
        "step": 5288
    },
    {
        "loss": 0.7534,
        "grad_norm": 2.309406042098999,
        "learning_rate": 4.6593771646862496e-05,
        "epoch": 0.68008229394368,
        "step": 5289
    },
    {
        "loss": 1.8257,
        "grad_norm": 1.982025384902954,
        "learning_rate": 4.655955746731098e-05,
        "epoch": 0.6802108782306802,
        "step": 5290
    },
    {
        "loss": 1.587,
        "grad_norm": 1.596235990524292,
        "learning_rate": 4.65253520420813e-05,
        "epoch": 0.6803394625176803,
        "step": 5291
    },
    {
        "loss": 2.0016,
        "grad_norm": 1.6554841995239258,
        "learning_rate": 4.6491155376776786e-05,
        "epoch": 0.6804680468046804,
        "step": 5292
    },
    {
        "loss": 1.905,
        "grad_norm": 2.014087200164795,
        "learning_rate": 4.6456967476999403e-05,
        "epoch": 0.6805966310916806,
        "step": 5293
    },
    {
        "loss": 1.488,
        "grad_norm": 2.407566547393799,
        "learning_rate": 4.6422788348349556e-05,
        "epoch": 0.6807252153786807,
        "step": 5294
    },
    {
        "loss": 2.0222,
        "grad_norm": 1.3331176042556763,
        "learning_rate": 4.638861799642632e-05,
        "epoch": 0.6808537996656808,
        "step": 5295
    },
    {
        "loss": 1.4702,
        "grad_norm": 1.9397015571594238,
        "learning_rate": 4.635445642682732e-05,
        "epoch": 0.680982383952681,
        "step": 5296
    },
    {
        "loss": 1.1839,
        "grad_norm": 3.0861849784851074,
        "learning_rate": 4.632030364514864e-05,
        "epoch": 0.6811109682396811,
        "step": 5297
    },
    {
        "loss": 1.6699,
        "grad_norm": 1.8651424646377563,
        "learning_rate": 4.6286159656985065e-05,
        "epoch": 0.6812395525266812,
        "step": 5298
    },
    {
        "loss": 1.7321,
        "grad_norm": 2.689791202545166,
        "learning_rate": 4.625202446792989e-05,
        "epoch": 0.6813681368136814,
        "step": 5299
    },
    {
        "loss": 1.9171,
        "grad_norm": 1.4794045686721802,
        "learning_rate": 4.6217898083574896e-05,
        "epoch": 0.6814967211006815,
        "step": 5300
    },
    {
        "eval_loss": 1.7723830938339233,
        "eval_runtime": 28.3062,
        "eval_samples_per_second": 2.791,
        "eval_steps_per_second": 2.791,
        "epoch": 0.6814967211006815,
        "step": 5300
    },
    {
        "loss": 1.4195,
        "grad_norm": 2.710221767425537,
        "learning_rate": 4.618378050951051e-05,
        "epoch": 0.6816253053876816,
        "step": 5301
    },
    {
        "loss": 1.4319,
        "grad_norm": 1.5493371486663818,
        "learning_rate": 4.614967175132568e-05,
        "epoch": 0.6817538896746818,
        "step": 5302
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.6361595392227173,
        "learning_rate": 4.611557181460798e-05,
        "epoch": 0.6818824739616819,
        "step": 5303
    },
    {
        "loss": 1.8536,
        "grad_norm": 1.9539202451705933,
        "learning_rate": 4.608148070494338e-05,
        "epoch": 0.682011058248682,
        "step": 5304
    },
    {
        "loss": 2.5802,
        "grad_norm": 1.4866210222244263,
        "learning_rate": 4.604739842791654e-05,
        "epoch": 0.6821396425356822,
        "step": 5305
    },
    {
        "loss": 0.8698,
        "grad_norm": 3.993116855621338,
        "learning_rate": 4.601332498911067e-05,
        "epoch": 0.6822682268226823,
        "step": 5306
    },
    {
        "loss": 1.5604,
        "grad_norm": 2.490880250930786,
        "learning_rate": 4.597926039410738e-05,
        "epoch": 0.6823968111096824,
        "step": 5307
    },
    {
        "loss": 1.1838,
        "grad_norm": 2.220017671585083,
        "learning_rate": 4.59452046484871e-05,
        "epoch": 0.6825253953966826,
        "step": 5308
    },
    {
        "loss": 1.623,
        "grad_norm": 1.8202710151672363,
        "learning_rate": 4.591115775782859e-05,
        "epoch": 0.6826539796836827,
        "step": 5309
    },
    {
        "loss": 1.9653,
        "grad_norm": 1.7990245819091797,
        "learning_rate": 4.587711972770915e-05,
        "epoch": 0.6827825639706828,
        "step": 5310
    },
    {
        "loss": 1.9561,
        "grad_norm": 1.9956010580062866,
        "learning_rate": 4.584309056370484e-05,
        "epoch": 0.682911148257683,
        "step": 5311
    },
    {
        "loss": 1.3761,
        "grad_norm": 2.7804176807403564,
        "learning_rate": 4.580907027139002e-05,
        "epoch": 0.6830397325446831,
        "step": 5312
    },
    {
        "loss": 1.3381,
        "grad_norm": 1.7299995422363281,
        "learning_rate": 4.5775058856337794e-05,
        "epoch": 0.6831683168316832,
        "step": 5313
    },
    {
        "loss": 1.8413,
        "grad_norm": 1.740828037261963,
        "learning_rate": 4.5741056324119645e-05,
        "epoch": 0.6832969011186832,
        "step": 5314
    },
    {
        "loss": 1.5998,
        "grad_norm": 2.223919153213501,
        "learning_rate": 4.5707062680305724e-05,
        "epoch": 0.6834254854056834,
        "step": 5315
    },
    {
        "loss": 1.0851,
        "grad_norm": 2.0635290145874023,
        "learning_rate": 4.567307793046472e-05,
        "epoch": 0.6835540696926835,
        "step": 5316
    },
    {
        "loss": 1.2251,
        "grad_norm": 2.1992177963256836,
        "learning_rate": 4.563910208016372e-05,
        "epoch": 0.6836826539796836,
        "step": 5317
    },
    {
        "loss": 1.927,
        "grad_norm": 1.7717640399932861,
        "learning_rate": 4.560513513496861e-05,
        "epoch": 0.6838112382666838,
        "step": 5318
    },
    {
        "loss": 1.6033,
        "grad_norm": 3.168586015701294,
        "learning_rate": 4.5571177100443564e-05,
        "epoch": 0.6839398225536839,
        "step": 5319
    },
    {
        "loss": 1.6745,
        "grad_norm": 1.8226007223129272,
        "learning_rate": 4.553722798215142e-05,
        "epoch": 0.684068406840684,
        "step": 5320
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.0808202028274536,
        "learning_rate": 4.550328778565359e-05,
        "epoch": 0.6841969911276842,
        "step": 5321
    },
    {
        "loss": 2.0324,
        "grad_norm": 1.4566057920455933,
        "learning_rate": 4.546935651650991e-05,
        "epoch": 0.6843255754146843,
        "step": 5322
    },
    {
        "loss": 2.0353,
        "grad_norm": 2.502992868423462,
        "learning_rate": 4.543543418027882e-05,
        "epoch": 0.6844541597016844,
        "step": 5323
    },
    {
        "loss": 2.0392,
        "grad_norm": 2.3524861335754395,
        "learning_rate": 4.540152078251732e-05,
        "epoch": 0.6845827439886846,
        "step": 5324
    },
    {
        "loss": 1.9323,
        "grad_norm": 1.8769469261169434,
        "learning_rate": 4.536761632878094e-05,
        "epoch": 0.6847113282756847,
        "step": 5325
    },
    {
        "loss": 1.6787,
        "grad_norm": 2.227288246154785,
        "learning_rate": 4.533372082462365e-05,
        "epoch": 0.6848399125626848,
        "step": 5326
    },
    {
        "loss": 2.0672,
        "grad_norm": 2.4534614086151123,
        "learning_rate": 4.529983427559806e-05,
        "epoch": 0.684968496849685,
        "step": 5327
    },
    {
        "loss": 2.2636,
        "grad_norm": 1.6936912536621094,
        "learning_rate": 4.5265956687255336e-05,
        "epoch": 0.6850970811366851,
        "step": 5328
    },
    {
        "loss": 1.7988,
        "grad_norm": 1.7669752836227417,
        "learning_rate": 4.523208806514504e-05,
        "epoch": 0.6852256654236852,
        "step": 5329
    },
    {
        "loss": 1.7797,
        "grad_norm": 3.692753791809082,
        "learning_rate": 4.519822841481536e-05,
        "epoch": 0.6853542497106854,
        "step": 5330
    },
    {
        "loss": 1.74,
        "grad_norm": 2.9173521995544434,
        "learning_rate": 4.516437774181306e-05,
        "epoch": 0.6854828339976855,
        "step": 5331
    },
    {
        "loss": 1.5506,
        "grad_norm": 2.6036980152130127,
        "learning_rate": 4.5130536051683283e-05,
        "epoch": 0.6856114182846856,
        "step": 5332
    },
    {
        "loss": 1.9922,
        "grad_norm": 1.4180749654769897,
        "learning_rate": 4.5096703349969836e-05,
        "epoch": 0.6857400025716858,
        "step": 5333
    },
    {
        "loss": 1.5962,
        "grad_norm": 2.658566951751709,
        "learning_rate": 4.5062879642215015e-05,
        "epoch": 0.6858685868586859,
        "step": 5334
    },
    {
        "loss": 1.5641,
        "grad_norm": 2.161848306655884,
        "learning_rate": 4.502906493395965e-05,
        "epoch": 0.685997171145686,
        "step": 5335
    },
    {
        "loss": 1.6229,
        "grad_norm": 3.0880184173583984,
        "learning_rate": 4.4995259230743024e-05,
        "epoch": 0.6861257554326862,
        "step": 5336
    },
    {
        "loss": 1.1411,
        "grad_norm": 1.9545855522155762,
        "learning_rate": 4.496146253810305e-05,
        "epoch": 0.6862543397196863,
        "step": 5337
    },
    {
        "loss": 1.2008,
        "grad_norm": 1.4733790159225464,
        "learning_rate": 4.492767486157613e-05,
        "epoch": 0.6863829240066864,
        "step": 5338
    },
    {
        "loss": 0.862,
        "grad_norm": 2.906273603439331,
        "learning_rate": 4.4893896206697116e-05,
        "epoch": 0.6865115082936865,
        "step": 5339
    },
    {
        "loss": 1.796,
        "grad_norm": 2.6169183254241943,
        "learning_rate": 4.486012657899949e-05,
        "epoch": 0.6866400925806866,
        "step": 5340
    },
    {
        "loss": 1.9128,
        "grad_norm": 1.5822075605392456,
        "learning_rate": 4.4826365984015216e-05,
        "epoch": 0.6867686768676867,
        "step": 5341
    },
    {
        "loss": 1.5303,
        "grad_norm": 1.7522577047348022,
        "learning_rate": 4.4792614427274725e-05,
        "epoch": 0.6868972611546869,
        "step": 5342
    },
    {
        "loss": 1.9836,
        "grad_norm": 2.081515312194824,
        "learning_rate": 4.475887191430703e-05,
        "epoch": 0.687025845441687,
        "step": 5343
    },
    {
        "loss": 1.766,
        "grad_norm": 2.102238416671753,
        "learning_rate": 4.472513845063967e-05,
        "epoch": 0.6871544297286871,
        "step": 5344
    },
    {
        "loss": 1.6246,
        "grad_norm": 1.5727601051330566,
        "learning_rate": 4.4691414041798686e-05,
        "epoch": 0.6872830140156873,
        "step": 5345
    },
    {
        "loss": 1.6074,
        "grad_norm": 2.61846923828125,
        "learning_rate": 4.465769869330857e-05,
        "epoch": 0.6874115983026874,
        "step": 5346
    },
    {
        "loss": 1.3709,
        "grad_norm": 2.797302007675171,
        "learning_rate": 4.4623992410692425e-05,
        "epoch": 0.6875401825896875,
        "step": 5347
    },
    {
        "loss": 2.0026,
        "grad_norm": 2.1895241737365723,
        "learning_rate": 4.459029519947185e-05,
        "epoch": 0.6876687668766877,
        "step": 5348
    },
    {
        "loss": 2.0408,
        "grad_norm": 1.1671760082244873,
        "learning_rate": 4.455660706516686e-05,
        "epoch": 0.6877973511636878,
        "step": 5349
    },
    {
        "loss": 1.4389,
        "grad_norm": 1.8952442407608032,
        "learning_rate": 4.452292801329617e-05,
        "epoch": 0.6879259354506879,
        "step": 5350
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.7891632318496704,
        "learning_rate": 4.448925804937685e-05,
        "epoch": 0.688054519737688,
        "step": 5351
    },
    {
        "loss": 2.15,
        "grad_norm": 1.5353081226348877,
        "learning_rate": 4.445559717892448e-05,
        "epoch": 0.6881831040246882,
        "step": 5352
    },
    {
        "loss": 1.0812,
        "grad_norm": 3.026853322982788,
        "learning_rate": 4.4421945407453245e-05,
        "epoch": 0.6883116883116883,
        "step": 5353
    },
    {
        "loss": 1.5169,
        "grad_norm": 2.1133079528808594,
        "learning_rate": 4.43883027404758e-05,
        "epoch": 0.6884402725986885,
        "step": 5354
    },
    {
        "loss": 2.4791,
        "grad_norm": 2.4529802799224854,
        "learning_rate": 4.435466918350333e-05,
        "epoch": 0.6885688568856886,
        "step": 5355
    },
    {
        "loss": 1.6679,
        "grad_norm": 2.001943826675415,
        "learning_rate": 4.432104474204543e-05,
        "epoch": 0.6886974411726887,
        "step": 5356
    },
    {
        "loss": 2.1211,
        "grad_norm": 1.5962579250335693,
        "learning_rate": 4.42874294216103e-05,
        "epoch": 0.6888260254596889,
        "step": 5357
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.8720836639404297,
        "learning_rate": 4.425382322770467e-05,
        "epoch": 0.688954609746689,
        "step": 5358
    },
    {
        "loss": 1.9989,
        "grad_norm": 2.1806771755218506,
        "learning_rate": 4.422022616583362e-05,
        "epoch": 0.6890831940336891,
        "step": 5359
    },
    {
        "loss": 1.0183,
        "grad_norm": 2.3232784271240234,
        "learning_rate": 4.418663824150096e-05,
        "epoch": 0.6892117783206893,
        "step": 5360
    },
    {
        "loss": 0.9251,
        "grad_norm": 3.9925520420074463,
        "learning_rate": 4.4153059460208827e-05,
        "epoch": 0.6893403626076894,
        "step": 5361
    },
    {
        "loss": 1.1869,
        "grad_norm": 2.1806914806365967,
        "learning_rate": 4.4119489827457826e-05,
        "epoch": 0.6894689468946895,
        "step": 5362
    },
    {
        "loss": 2.3173,
        "grad_norm": 2.5346059799194336,
        "learning_rate": 4.40859293487473e-05,
        "epoch": 0.6895975311816896,
        "step": 5363
    },
    {
        "loss": 2.0638,
        "grad_norm": 1.971335768699646,
        "learning_rate": 4.4052378029574845e-05,
        "epoch": 0.6897261154686897,
        "step": 5364
    },
    {
        "loss": 2.3995,
        "grad_norm": 1.5989718437194824,
        "learning_rate": 4.4018835875436695e-05,
        "epoch": 0.6898546997556898,
        "step": 5365
    },
    {
        "loss": 1.6563,
        "grad_norm": 1.8390769958496094,
        "learning_rate": 4.398530289182752e-05,
        "epoch": 0.6899832840426899,
        "step": 5366
    },
    {
        "loss": 2.1737,
        "grad_norm": 2.273545503616333,
        "learning_rate": 4.395177908424056e-05,
        "epoch": 0.6901118683296901,
        "step": 5367
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.3899343013763428,
        "learning_rate": 4.391826445816744e-05,
        "epoch": 0.6902404526166902,
        "step": 5368
    },
    {
        "loss": 1.9876,
        "grad_norm": 2.3129682540893555,
        "learning_rate": 4.3884759019098355e-05,
        "epoch": 0.6903690369036903,
        "step": 5369
    },
    {
        "loss": 2.3888,
        "grad_norm": 2.483722448348999,
        "learning_rate": 4.385126277252204e-05,
        "epoch": 0.6904976211906905,
        "step": 5370
    },
    {
        "loss": 1.4096,
        "grad_norm": 2.515285015106201,
        "learning_rate": 4.381777572392558e-05,
        "epoch": 0.6906262054776906,
        "step": 5371
    },
    {
        "loss": 1.9994,
        "grad_norm": 1.5807479619979858,
        "learning_rate": 4.37842978787947e-05,
        "epoch": 0.6907547897646907,
        "step": 5372
    },
    {
        "loss": 0.452,
        "grad_norm": 1.911015272140503,
        "learning_rate": 4.375082924261357e-05,
        "epoch": 0.6908833740516909,
        "step": 5373
    },
    {
        "loss": 1.7486,
        "grad_norm": 2.7250330448150635,
        "learning_rate": 4.3717369820864784e-05,
        "epoch": 0.691011958338691,
        "step": 5374
    },
    {
        "loss": 1.5337,
        "grad_norm": 1.829156517982483,
        "learning_rate": 4.36839196190295e-05,
        "epoch": 0.6911405426256911,
        "step": 5375
    },
    {
        "loss": 1.4137,
        "grad_norm": 1.2899929285049438,
        "learning_rate": 4.365047864258737e-05,
        "epoch": 0.6912691269126913,
        "step": 5376
    },
    {
        "loss": 1.4459,
        "grad_norm": 1.924856185913086,
        "learning_rate": 4.361704689701652e-05,
        "epoch": 0.6913977111996914,
        "step": 5377
    },
    {
        "loss": 1.8956,
        "grad_norm": 2.533494234085083,
        "learning_rate": 4.358362438779352e-05,
        "epoch": 0.6915262954866915,
        "step": 5378
    },
    {
        "loss": 2.0372,
        "grad_norm": 2.3624813556671143,
        "learning_rate": 4.355021112039347e-05,
        "epoch": 0.6916548797736917,
        "step": 5379
    },
    {
        "loss": 1.8594,
        "grad_norm": 1.8919156789779663,
        "learning_rate": 4.3516807100290004e-05,
        "epoch": 0.6917834640606918,
        "step": 5380
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.4943829774856567,
        "learning_rate": 4.3483412332955096e-05,
        "epoch": 0.6919120483476919,
        "step": 5381
    },
    {
        "loss": 1.8151,
        "grad_norm": 1.7242923974990845,
        "learning_rate": 4.3450026823859336e-05,
        "epoch": 0.6920406326346921,
        "step": 5382
    },
    {
        "loss": 2.1353,
        "grad_norm": 1.4197393655776978,
        "learning_rate": 4.3416650578471804e-05,
        "epoch": 0.6921692169216922,
        "step": 5383
    },
    {
        "loss": 2.2137,
        "grad_norm": 1.7979475259780884,
        "learning_rate": 4.3383283602259936e-05,
        "epoch": 0.6922978012086923,
        "step": 5384
    },
    {
        "loss": 2.3141,
        "grad_norm": 2.351849317550659,
        "learning_rate": 4.334992590068977e-05,
        "epoch": 0.6924263854956925,
        "step": 5385
    },
    {
        "loss": 1.4212,
        "grad_norm": 2.5656888484954834,
        "learning_rate": 4.3316577479225765e-05,
        "epoch": 0.6925549697826926,
        "step": 5386
    },
    {
        "loss": 1.5781,
        "grad_norm": 2.3102595806121826,
        "learning_rate": 4.3283238343330924e-05,
        "epoch": 0.6926835540696927,
        "step": 5387
    },
    {
        "loss": 1.319,
        "grad_norm": 1.8705803155899048,
        "learning_rate": 4.324990849846661e-05,
        "epoch": 0.6928121383566929,
        "step": 5388
    },
    {
        "loss": 1.465,
        "grad_norm": 1.597796082496643,
        "learning_rate": 4.321658795009278e-05,
        "epoch": 0.6929407226436929,
        "step": 5389
    },
    {
        "loss": 2.289,
        "grad_norm": 1.5738438367843628,
        "learning_rate": 4.3183276703667855e-05,
        "epoch": 0.693069306930693,
        "step": 5390
    },
    {
        "loss": 1.3807,
        "grad_norm": 1.7275362014770508,
        "learning_rate": 4.314997476464861e-05,
        "epoch": 0.6931978912176932,
        "step": 5391
    },
    {
        "loss": 0.7983,
        "grad_norm": 1.97834050655365,
        "learning_rate": 4.311668213849046e-05,
        "epoch": 0.6933264755046933,
        "step": 5392
    },
    {
        "loss": 1.9562,
        "grad_norm": 1.5121324062347412,
        "learning_rate": 4.308339883064724e-05,
        "epoch": 0.6934550597916934,
        "step": 5393
    },
    {
        "loss": 1.9097,
        "grad_norm": 1.9341251850128174,
        "learning_rate": 4.305012484657115e-05,
        "epoch": 0.6935836440786936,
        "step": 5394
    },
    {
        "loss": 2.171,
        "grad_norm": 1.4765849113464355,
        "learning_rate": 4.3016860191713014e-05,
        "epoch": 0.6937122283656937,
        "step": 5395
    },
    {
        "loss": 1.2807,
        "grad_norm": 1.7325316667556763,
        "learning_rate": 4.298360487152206e-05,
        "epoch": 0.6938408126526938,
        "step": 5396
    },
    {
        "loss": 1.6803,
        "grad_norm": 1.8479576110839844,
        "learning_rate": 4.295035889144603e-05,
        "epoch": 0.693969396939694,
        "step": 5397
    },
    {
        "loss": 1.3605,
        "grad_norm": 2.2962818145751953,
        "learning_rate": 4.291712225693102e-05,
        "epoch": 0.6940979812266941,
        "step": 5398
    },
    {
        "loss": 1.7273,
        "grad_norm": 2.2520012855529785,
        "learning_rate": 4.288389497342171e-05,
        "epoch": 0.6942265655136942,
        "step": 5399
    },
    {
        "loss": 1.7636,
        "grad_norm": 1.907187819480896,
        "learning_rate": 4.2850677046361254e-05,
        "epoch": 0.6943551498006943,
        "step": 5400
    },
    {
        "eval_loss": 1.7687801122665405,
        "eval_runtime": 28.3049,
        "eval_samples_per_second": 2.791,
        "eval_steps_per_second": 2.791,
        "epoch": 0.6943551498006943,
        "step": 5400
    },
    {
        "loss": 1.3576,
        "grad_norm": 2.602320909500122,
        "learning_rate": 4.2817468481191124e-05,
        "epoch": 0.6944837340876945,
        "step": 5401
    },
    {
        "loss": 1.1856,
        "grad_norm": 1.5593897104263306,
        "learning_rate": 4.2784269283351506e-05,
        "epoch": 0.6946123183746946,
        "step": 5402
    },
    {
        "loss": 2.2149,
        "grad_norm": 1.792424201965332,
        "learning_rate": 4.2751079458280854e-05,
        "epoch": 0.6947409026616947,
        "step": 5403
    },
    {
        "loss": 1.6901,
        "grad_norm": 3.653988838195801,
        "learning_rate": 4.271789901141604e-05,
        "epoch": 0.6948694869486949,
        "step": 5404
    },
    {
        "loss": 1.1047,
        "grad_norm": 1.5977566242218018,
        "learning_rate": 4.2684727948192685e-05,
        "epoch": 0.694998071235695,
        "step": 5405
    },
    {
        "loss": 1.1734,
        "grad_norm": 1.6476624011993408,
        "learning_rate": 4.265156627404455e-05,
        "epoch": 0.6951266555226951,
        "step": 5406
    },
    {
        "loss": 0.4097,
        "grad_norm": 1.5617341995239258,
        "learning_rate": 4.261841399440406e-05,
        "epoch": 0.6952552398096953,
        "step": 5407
    },
    {
        "loss": 1.5354,
        "grad_norm": 2.4430925846099854,
        "learning_rate": 4.258527111470207e-05,
        "epoch": 0.6953838240966954,
        "step": 5408
    },
    {
        "loss": 1.7548,
        "grad_norm": 2.215986728668213,
        "learning_rate": 4.2552137640367794e-05,
        "epoch": 0.6955124083836955,
        "step": 5409
    },
    {
        "loss": 1.332,
        "grad_norm": 2.374460220336914,
        "learning_rate": 4.251901357682905e-05,
        "epoch": 0.6956409926706957,
        "step": 5410
    },
    {
        "loss": 2.3082,
        "grad_norm": 2.6055006980895996,
        "learning_rate": 4.2485898929511914e-05,
        "epoch": 0.6957695769576958,
        "step": 5411
    },
    {
        "loss": 1.9632,
        "grad_norm": 2.165008783340454,
        "learning_rate": 4.245279370384122e-05,
        "epoch": 0.695898161244696,
        "step": 5412
    },
    {
        "loss": 2.1318,
        "grad_norm": 2.1694400310516357,
        "learning_rate": 4.241969790524001e-05,
        "epoch": 0.6960267455316961,
        "step": 5413
    },
    {
        "loss": 1.4024,
        "grad_norm": 3.0538206100463867,
        "learning_rate": 4.2386611539129764e-05,
        "epoch": 0.6961553298186961,
        "step": 5414
    },
    {
        "loss": 2.2206,
        "grad_norm": 1.9561718702316284,
        "learning_rate": 4.2353534610930665e-05,
        "epoch": 0.6962839141056962,
        "step": 5415
    },
    {
        "loss": 1.5599,
        "grad_norm": 2.8123972415924072,
        "learning_rate": 4.2320467126061093e-05,
        "epoch": 0.6964124983926964,
        "step": 5416
    },
    {
        "loss": 1.2575,
        "grad_norm": 1.6851887702941895,
        "learning_rate": 4.2287409089938015e-05,
        "epoch": 0.6965410826796965,
        "step": 5417
    },
    {
        "loss": 2.1867,
        "grad_norm": 1.5785917043685913,
        "learning_rate": 4.225436050797682e-05,
        "epoch": 0.6966696669666966,
        "step": 5418
    },
    {
        "loss": 1.3747,
        "grad_norm": 1.2585896253585815,
        "learning_rate": 4.222132138559137e-05,
        "epoch": 0.6967982512536968,
        "step": 5419
    },
    {
        "loss": 1.4461,
        "grad_norm": 1.9856297969818115,
        "learning_rate": 4.2188291728193906e-05,
        "epoch": 0.6969268355406969,
        "step": 5420
    },
    {
        "loss": 0.8339,
        "grad_norm": 1.9703960418701172,
        "learning_rate": 4.2155271541195174e-05,
        "epoch": 0.697055419827697,
        "step": 5421
    },
    {
        "loss": 2.1418,
        "grad_norm": 2.151587963104248,
        "learning_rate": 4.2122260830004426e-05,
        "epoch": 0.6971840041146972,
        "step": 5422
    },
    {
        "loss": 1.401,
        "grad_norm": 1.9943597316741943,
        "learning_rate": 4.208925960002919e-05,
        "epoch": 0.6973125884016973,
        "step": 5423
    },
    {
        "loss": 1.2348,
        "grad_norm": 1.5568077564239502,
        "learning_rate": 4.205626785667563e-05,
        "epoch": 0.6974411726886974,
        "step": 5424
    },
    {
        "loss": 1.0097,
        "grad_norm": 2.3276076316833496,
        "learning_rate": 4.2023285605348264e-05,
        "epoch": 0.6975697569756976,
        "step": 5425
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.0955348014831543,
        "learning_rate": 4.199031285145002e-05,
        "epoch": 0.6976983412626977,
        "step": 5426
    },
    {
        "loss": 2.0938,
        "grad_norm": 2.33939790725708,
        "learning_rate": 4.195734960038233e-05,
        "epoch": 0.6978269255496978,
        "step": 5427
    },
    {
        "loss": 1.5349,
        "grad_norm": 1.5626851320266724,
        "learning_rate": 4.192439585754509e-05,
        "epoch": 0.697955509836698,
        "step": 5428
    },
    {
        "loss": 2.102,
        "grad_norm": 1.000888705253601,
        "learning_rate": 4.1891451628336585e-05,
        "epoch": 0.6980840941236981,
        "step": 5429
    },
    {
        "loss": 1.8269,
        "grad_norm": 2.0166752338409424,
        "learning_rate": 4.1858516918153536e-05,
        "epoch": 0.6982126784106982,
        "step": 5430
    },
    {
        "loss": 1.9635,
        "grad_norm": 2.398560047149658,
        "learning_rate": 4.1825591732391134e-05,
        "epoch": 0.6983412626976984,
        "step": 5431
    },
    {
        "loss": 1.1638,
        "grad_norm": 1.4305601119995117,
        "learning_rate": 4.179267607644306e-05,
        "epoch": 0.6984698469846985,
        "step": 5432
    },
    {
        "loss": 1.6909,
        "grad_norm": 2.0214293003082275,
        "learning_rate": 4.1759769955701286e-05,
        "epoch": 0.6985984312716986,
        "step": 5433
    },
    {
        "loss": 2.0466,
        "grad_norm": 2.588177442550659,
        "learning_rate": 4.1726873375556365e-05,
        "epoch": 0.6987270155586988,
        "step": 5434
    },
    {
        "loss": 1.7922,
        "grad_norm": 3.263080596923828,
        "learning_rate": 4.169398634139726e-05,
        "epoch": 0.6988555998456989,
        "step": 5435
    },
    {
        "loss": 1.7886,
        "grad_norm": 2.105675220489502,
        "learning_rate": 4.166110885861127e-05,
        "epoch": 0.698984184132699,
        "step": 5436
    },
    {
        "loss": 1.265,
        "grad_norm": 2.1864726543426514,
        "learning_rate": 4.162824093258426e-05,
        "epoch": 0.6991127684196992,
        "step": 5437
    },
    {
        "loss": 1.013,
        "grad_norm": 2.3303537368774414,
        "learning_rate": 4.159538256870046e-05,
        "epoch": 0.6992413527066993,
        "step": 5438
    },
    {
        "loss": 2.2482,
        "grad_norm": 1.3552018404006958,
        "learning_rate": 4.1562533772342596e-05,
        "epoch": 0.6993699369936993,
        "step": 5439
    },
    {
        "loss": 1.2425,
        "grad_norm": 1.893606424331665,
        "learning_rate": 4.152969454889168e-05,
        "epoch": 0.6994985212806994,
        "step": 5440
    },
    {
        "loss": 2.147,
        "grad_norm": 2.4167966842651367,
        "learning_rate": 4.1496864903727316e-05,
        "epoch": 0.6996271055676996,
        "step": 5441
    },
    {
        "loss": 1.6126,
        "grad_norm": 2.6346864700317383,
        "learning_rate": 4.1464044842227514e-05,
        "epoch": 0.6997556898546997,
        "step": 5442
    },
    {
        "loss": 2.0343,
        "grad_norm": 1.3449918031692505,
        "learning_rate": 4.143123436976856e-05,
        "epoch": 0.6998842741416998,
        "step": 5443
    },
    {
        "loss": 1.5526,
        "grad_norm": 2.3985135555267334,
        "learning_rate": 4.139843349172542e-05,
        "epoch": 0.7000128584287,
        "step": 5444
    },
    {
        "loss": 2.0644,
        "grad_norm": 2.1770100593566895,
        "learning_rate": 4.13656422134713e-05,
        "epoch": 0.7001414427157001,
        "step": 5445
    },
    {
        "loss": 0.7206,
        "grad_norm": 5.107601642608643,
        "learning_rate": 4.1332860540377804e-05,
        "epoch": 0.7002700270027002,
        "step": 5446
    },
    {
        "loss": 1.8365,
        "grad_norm": 2.184227228164673,
        "learning_rate": 4.130008847781521e-05,
        "epoch": 0.7003986112897004,
        "step": 5447
    },
    {
        "loss": 1.5197,
        "grad_norm": 2.629939317703247,
        "learning_rate": 4.126732603115193e-05,
        "epoch": 0.7005271955767005,
        "step": 5448
    },
    {
        "loss": 1.7684,
        "grad_norm": 2.0355496406555176,
        "learning_rate": 4.123457320575502e-05,
        "epoch": 0.7006557798637006,
        "step": 5449
    },
    {
        "loss": 2.2616,
        "grad_norm": 1.5006762742996216,
        "learning_rate": 4.120183000698978e-05,
        "epoch": 0.7007843641507008,
        "step": 5450
    },
    {
        "loss": 2.0987,
        "grad_norm": 2.4533166885375977,
        "learning_rate": 4.1169096440220066e-05,
        "epoch": 0.7009129484377009,
        "step": 5451
    },
    {
        "loss": 2.1312,
        "grad_norm": 1.5279992818832397,
        "learning_rate": 4.113637251080815e-05,
        "epoch": 0.701041532724701,
        "step": 5452
    },
    {
        "loss": 2.2696,
        "grad_norm": 2.0230841636657715,
        "learning_rate": 4.110365822411457e-05,
        "epoch": 0.7011701170117012,
        "step": 5453
    },
    {
        "loss": 2.5414,
        "grad_norm": 1.2467106580734253,
        "learning_rate": 4.107095358549856e-05,
        "epoch": 0.7012987012987013,
        "step": 5454
    },
    {
        "loss": 1.6026,
        "grad_norm": 1.2710869312286377,
        "learning_rate": 4.103825860031755e-05,
        "epoch": 0.7014272855857014,
        "step": 5455
    },
    {
        "loss": 1.2945,
        "grad_norm": 2.6090359687805176,
        "learning_rate": 4.1005573273927344e-05,
        "epoch": 0.7015558698727016,
        "step": 5456
    },
    {
        "loss": 2.0746,
        "grad_norm": 1.80239737033844,
        "learning_rate": 4.097289761168246e-05,
        "epoch": 0.7016844541597017,
        "step": 5457
    },
    {
        "loss": 2.1743,
        "grad_norm": 1.6937662363052368,
        "learning_rate": 4.094023161893551e-05,
        "epoch": 0.7018130384467018,
        "step": 5458
    },
    {
        "loss": 2.1049,
        "grad_norm": 1.832025408744812,
        "learning_rate": 4.090757530103772e-05,
        "epoch": 0.701941622733702,
        "step": 5459
    },
    {
        "loss": 1.9107,
        "grad_norm": 2.1165943145751953,
        "learning_rate": 4.087492866333869e-05,
        "epoch": 0.7020702070207021,
        "step": 5460
    },
    {
        "loss": 1.7617,
        "grad_norm": 2.1268296241760254,
        "learning_rate": 4.0842291711186334e-05,
        "epoch": 0.7021987913077022,
        "step": 5461
    },
    {
        "loss": 0.9669,
        "grad_norm": 2.2081515789031982,
        "learning_rate": 4.080966444992711e-05,
        "epoch": 0.7023273755947024,
        "step": 5462
    },
    {
        "loss": 1.811,
        "grad_norm": 1.8879941701889038,
        "learning_rate": 4.0777046884905835e-05,
        "epoch": 0.7024559598817025,
        "step": 5463
    },
    {
        "loss": 1.5165,
        "grad_norm": 1.8736201524734497,
        "learning_rate": 4.0744439021465776e-05,
        "epoch": 0.7025845441687025,
        "step": 5464
    },
    {
        "loss": 0.9471,
        "grad_norm": 1.793256163597107,
        "learning_rate": 4.07118408649485e-05,
        "epoch": 0.7027131284557027,
        "step": 5465
    },
    {
        "loss": 1.5832,
        "grad_norm": 3.069436550140381,
        "learning_rate": 4.06792524206941e-05,
        "epoch": 0.7028417127427028,
        "step": 5466
    },
    {
        "loss": 1.419,
        "grad_norm": 1.8355714082717896,
        "learning_rate": 4.064667369404108e-05,
        "epoch": 0.7029702970297029,
        "step": 5467
    },
    {
        "loss": 1.8249,
        "grad_norm": 1.9110887050628662,
        "learning_rate": 4.061410469032621e-05,
        "epoch": 0.7030988813167031,
        "step": 5468
    },
    {
        "loss": 1.7565,
        "grad_norm": 2.336655378341675,
        "learning_rate": 4.058154541488485e-05,
        "epoch": 0.7032274656037032,
        "step": 5469
    },
    {
        "loss": 1.5568,
        "grad_norm": 2.328511953353882,
        "learning_rate": 4.0548995873050635e-05,
        "epoch": 0.7033560498907033,
        "step": 5470
    },
    {
        "loss": 1.7249,
        "grad_norm": 2.0804367065429688,
        "learning_rate": 4.0516456070155704e-05,
        "epoch": 0.7034846341777035,
        "step": 5471
    },
    {
        "loss": 1.7751,
        "grad_norm": 2.0000908374786377,
        "learning_rate": 4.0483926011530495e-05,
        "epoch": 0.7036132184647036,
        "step": 5472
    },
    {
        "loss": 1.7423,
        "grad_norm": 1.4545882940292358,
        "learning_rate": 4.045140570250392e-05,
        "epoch": 0.7037418027517037,
        "step": 5473
    },
    {
        "loss": 1.5387,
        "grad_norm": 1.6895803213119507,
        "learning_rate": 4.041889514840332e-05,
        "epoch": 0.7038703870387039,
        "step": 5474
    },
    {
        "loss": 1.1949,
        "grad_norm": 2.278852701187134,
        "learning_rate": 4.038639435455434e-05,
        "epoch": 0.703998971325704,
        "step": 5475
    },
    {
        "loss": 2.2889,
        "grad_norm": 1.6081088781356812,
        "learning_rate": 4.0353903326281104e-05,
        "epoch": 0.7041275556127041,
        "step": 5476
    },
    {
        "loss": 1.4753,
        "grad_norm": 2.5180680751800537,
        "learning_rate": 4.032142206890615e-05,
        "epoch": 0.7042561398997043,
        "step": 5477
    },
    {
        "loss": 1.8604,
        "grad_norm": 1.793941617012024,
        "learning_rate": 4.0288950587750304e-05,
        "epoch": 0.7043847241867044,
        "step": 5478
    },
    {
        "loss": 1.3992,
        "grad_norm": 1.8034791946411133,
        "learning_rate": 4.025648888813293e-05,
        "epoch": 0.7045133084737045,
        "step": 5479
    },
    {
        "loss": 2.3914,
        "grad_norm": 1.2028988599777222,
        "learning_rate": 4.022403697537169e-05,
        "epoch": 0.7046418927607047,
        "step": 5480
    },
    {
        "loss": 0.6633,
        "grad_norm": 2.1600852012634277,
        "learning_rate": 4.019159485478274e-05,
        "epoch": 0.7047704770477048,
        "step": 5481
    },
    {
        "loss": 0.6252,
        "grad_norm": 2.13582444190979,
        "learning_rate": 4.01591625316805e-05,
        "epoch": 0.7048990613347049,
        "step": 5482
    },
    {
        "loss": 1.5091,
        "grad_norm": 2.26102352142334,
        "learning_rate": 4.0126740011377874e-05,
        "epoch": 0.7050276456217051,
        "step": 5483
    },
    {
        "loss": 1.72,
        "grad_norm": 2.248791217803955,
        "learning_rate": 4.0094327299186186e-05,
        "epoch": 0.7051562299087052,
        "step": 5484
    },
    {
        "loss": 1.4478,
        "grad_norm": 2.624803304672241,
        "learning_rate": 4.006192440041502e-05,
        "epoch": 0.7052848141957053,
        "step": 5485
    },
    {
        "loss": 1.5328,
        "grad_norm": 2.870609998703003,
        "learning_rate": 4.002953132037256e-05,
        "epoch": 0.7054133984827055,
        "step": 5486
    },
    {
        "loss": 1.8785,
        "grad_norm": 1.9161376953125,
        "learning_rate": 3.9997148064365195e-05,
        "epoch": 0.7055419827697056,
        "step": 5487
    },
    {
        "loss": 1.6725,
        "grad_norm": 2.9411725997924805,
        "learning_rate": 3.9964774637697756e-05,
        "epoch": 0.7056705670567057,
        "step": 5488
    },
    {
        "loss": 1.407,
        "grad_norm": 3.307689905166626,
        "learning_rate": 3.99324110456735e-05,
        "epoch": 0.7057991513437057,
        "step": 5489
    },
    {
        "loss": 2.1334,
        "grad_norm": 1.7339040040969849,
        "learning_rate": 3.990005729359407e-05,
        "epoch": 0.7059277356307059,
        "step": 5490
    },
    {
        "loss": 1.3364,
        "grad_norm": 1.8939458131790161,
        "learning_rate": 3.98677133867595e-05,
        "epoch": 0.706056319917706,
        "step": 5491
    },
    {
        "loss": 2.172,
        "grad_norm": 1.8723390102386475,
        "learning_rate": 3.9835379330468126e-05,
        "epoch": 0.7061849042047061,
        "step": 5492
    },
    {
        "loss": 1.7398,
        "grad_norm": 1.8359578847885132,
        "learning_rate": 3.980305513001679e-05,
        "epoch": 0.7063134884917063,
        "step": 5493
    },
    {
        "loss": 1.3115,
        "grad_norm": 2.139834403991699,
        "learning_rate": 3.977074079070067e-05,
        "epoch": 0.7064420727787064,
        "step": 5494
    },
    {
        "loss": 2.2614,
        "grad_norm": 1.643782377243042,
        "learning_rate": 3.973843631781323e-05,
        "epoch": 0.7065706570657065,
        "step": 5495
    },
    {
        "loss": 1.7968,
        "grad_norm": 2.168750286102295,
        "learning_rate": 3.970614171664657e-05,
        "epoch": 0.7066992413527067,
        "step": 5496
    },
    {
        "loss": 0.9094,
        "grad_norm": 2.4816012382507324,
        "learning_rate": 3.967385699249093e-05,
        "epoch": 0.7068278256397068,
        "step": 5497
    },
    {
        "loss": 1.8969,
        "grad_norm": 2.0315451622009277,
        "learning_rate": 3.964158215063496e-05,
        "epoch": 0.7069564099267069,
        "step": 5498
    },
    {
        "loss": 2.2275,
        "grad_norm": 1.5539039373397827,
        "learning_rate": 3.9609317196365866e-05,
        "epoch": 0.7070849942137071,
        "step": 5499
    },
    {
        "loss": 2.2543,
        "grad_norm": 1.6933778524398804,
        "learning_rate": 3.9577062134969035e-05,
        "epoch": 0.7072135785007072,
        "step": 5500
    },
    {
        "eval_loss": 1.767235279083252,
        "eval_runtime": 28.3745,
        "eval_samples_per_second": 2.784,
        "eval_steps_per_second": 2.784,
        "epoch": 0.7072135785007072,
        "step": 5500
    },
    {
        "loss": 2.1088,
        "grad_norm": 1.8074837923049927,
        "learning_rate": 3.954481697172834e-05,
        "epoch": 0.7073421627877073,
        "step": 5501
    },
    {
        "loss": 2.1777,
        "grad_norm": 2.0577733516693115,
        "learning_rate": 3.9512581711926035e-05,
        "epoch": 0.7074707470747075,
        "step": 5502
    },
    {
        "loss": 1.5343,
        "grad_norm": 2.8065834045410156,
        "learning_rate": 3.948035636084267e-05,
        "epoch": 0.7075993313617076,
        "step": 5503
    },
    {
        "loss": 1.4896,
        "grad_norm": 2.0413668155670166,
        "learning_rate": 3.944814092375726e-05,
        "epoch": 0.7077279156487077,
        "step": 5504
    },
    {
        "loss": 2.4977,
        "grad_norm": 1.778217077255249,
        "learning_rate": 3.9415935405947146e-05,
        "epoch": 0.7078564999357079,
        "step": 5505
    },
    {
        "loss": 1.0337,
        "grad_norm": 2.2795791625976562,
        "learning_rate": 3.938373981268811e-05,
        "epoch": 0.707985084222708,
        "step": 5506
    },
    {
        "loss": 1.6662,
        "grad_norm": 2.4010415077209473,
        "learning_rate": 3.9351554149254224e-05,
        "epoch": 0.7081136685097081,
        "step": 5507
    },
    {
        "loss": 2.0776,
        "grad_norm": 2.3637871742248535,
        "learning_rate": 3.9319378420917886e-05,
        "epoch": 0.7082422527967083,
        "step": 5508
    },
    {
        "loss": 2.2297,
        "grad_norm": 1.2322076559066772,
        "learning_rate": 3.9287212632950084e-05,
        "epoch": 0.7083708370837084,
        "step": 5509
    },
    {
        "loss": 2.136,
        "grad_norm": 1.739939570426941,
        "learning_rate": 3.925505679061997e-05,
        "epoch": 0.7084994213707085,
        "step": 5510
    },
    {
        "loss": 1.0674,
        "grad_norm": 1.4419835805892944,
        "learning_rate": 3.922291089919514e-05,
        "epoch": 0.7086280056577087,
        "step": 5511
    },
    {
        "loss": 1.8472,
        "grad_norm": 2.0849757194519043,
        "learning_rate": 3.9190774963941613e-05,
        "epoch": 0.7087565899447088,
        "step": 5512
    },
    {
        "loss": 1.5985,
        "grad_norm": 2.047313690185547,
        "learning_rate": 3.915864899012364e-05,
        "epoch": 0.7088851742317089,
        "step": 5513
    },
    {
        "loss": 1.2315,
        "grad_norm": 2.3734562397003174,
        "learning_rate": 3.912653298300396e-05,
        "epoch": 0.709013758518709,
        "step": 5514
    },
    {
        "loss": 2.3846,
        "grad_norm": 1.55176842212677,
        "learning_rate": 3.909442694784365e-05,
        "epoch": 0.7091423428057091,
        "step": 5515
    },
    {
        "loss": 1.6076,
        "grad_norm": 2.1078028678894043,
        "learning_rate": 3.9062330889902175e-05,
        "epoch": 0.7092709270927092,
        "step": 5516
    },
    {
        "loss": 2.1074,
        "grad_norm": 2.0723624229431152,
        "learning_rate": 3.903024481443727e-05,
        "epoch": 0.7093995113797094,
        "step": 5517
    },
    {
        "loss": 1.3025,
        "grad_norm": 1.8941508531570435,
        "learning_rate": 3.899816872670513e-05,
        "epoch": 0.7095280956667095,
        "step": 5518
    },
    {
        "loss": 2.0634,
        "grad_norm": 1.4248391389846802,
        "learning_rate": 3.896610263196032e-05,
        "epoch": 0.7096566799537096,
        "step": 5519
    },
    {
        "loss": 1.1725,
        "grad_norm": 2.1330692768096924,
        "learning_rate": 3.893404653545567e-05,
        "epoch": 0.7097852642407098,
        "step": 5520
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.255855917930603,
        "learning_rate": 3.8902000442442464e-05,
        "epoch": 0.7099138485277099,
        "step": 5521
    },
    {
        "loss": 1.7052,
        "grad_norm": 1.7204841375350952,
        "learning_rate": 3.886996435817032e-05,
        "epoch": 0.71004243281471,
        "step": 5522
    },
    {
        "loss": 1.7999,
        "grad_norm": 1.9742697477340698,
        "learning_rate": 3.8837938287887254e-05,
        "epoch": 0.7101710171017102,
        "step": 5523
    },
    {
        "loss": 2.1412,
        "grad_norm": 1.68087899684906,
        "learning_rate": 3.880592223683952e-05,
        "epoch": 0.7102996013887103,
        "step": 5524
    },
    {
        "loss": 1.699,
        "grad_norm": 2.0947623252868652,
        "learning_rate": 3.877391621027185e-05,
        "epoch": 0.7104281856757104,
        "step": 5525
    },
    {
        "loss": 2.207,
        "grad_norm": 1.3983434438705444,
        "learning_rate": 3.874192021342735e-05,
        "epoch": 0.7105567699627106,
        "step": 5526
    },
    {
        "loss": 2.0039,
        "grad_norm": 1.4432770013809204,
        "learning_rate": 3.870993425154733e-05,
        "epoch": 0.7106853542497107,
        "step": 5527
    },
    {
        "loss": 2.196,
        "grad_norm": 2.070951461791992,
        "learning_rate": 3.8677958329871624e-05,
        "epoch": 0.7108139385367108,
        "step": 5528
    },
    {
        "loss": 1.0,
        "grad_norm": 2.4096477031707764,
        "learning_rate": 3.864599245363837e-05,
        "epoch": 0.710942522823711,
        "step": 5529
    },
    {
        "loss": 1.177,
        "grad_norm": 1.900276780128479,
        "learning_rate": 3.861403662808396e-05,
        "epoch": 0.7110711071107111,
        "step": 5530
    },
    {
        "loss": 2.1919,
        "grad_norm": 2.570124387741089,
        "learning_rate": 3.8582090858443284e-05,
        "epoch": 0.7111996913977112,
        "step": 5531
    },
    {
        "loss": 0.801,
        "grad_norm": 2.007408857345581,
        "learning_rate": 3.855015514994951e-05,
        "epoch": 0.7113282756847114,
        "step": 5532
    },
    {
        "loss": 2.3425,
        "grad_norm": 2.4938037395477295,
        "learning_rate": 3.851822950783421e-05,
        "epoch": 0.7114568599717115,
        "step": 5533
    },
    {
        "loss": 1.6617,
        "grad_norm": 2.9793059825897217,
        "learning_rate": 3.84863139373272e-05,
        "epoch": 0.7115854442587116,
        "step": 5534
    },
    {
        "loss": 1.2627,
        "grad_norm": 1.9264839887619019,
        "learning_rate": 3.845440844365674e-05,
        "epoch": 0.7117140285457118,
        "step": 5535
    },
    {
        "loss": 1.0454,
        "grad_norm": 1.7441558837890625,
        "learning_rate": 3.8422513032049466e-05,
        "epoch": 0.7118426128327119,
        "step": 5536
    },
    {
        "loss": 1.2848,
        "grad_norm": 2.5737175941467285,
        "learning_rate": 3.83906277077302e-05,
        "epoch": 0.711971197119712,
        "step": 5537
    },
    {
        "loss": 1.9966,
        "grad_norm": 2.185750722885132,
        "learning_rate": 3.8358752475922346e-05,
        "epoch": 0.7120997814067122,
        "step": 5538
    },
    {
        "loss": 2.0788,
        "grad_norm": 2.600727081298828,
        "learning_rate": 3.832688734184747e-05,
        "epoch": 0.7122283656937122,
        "step": 5539
    },
    {
        "loss": 0.7035,
        "grad_norm": 2.809351921081543,
        "learning_rate": 3.8295032310725484e-05,
        "epoch": 0.7123569499807123,
        "step": 5540
    },
    {
        "loss": 0.8891,
        "grad_norm": 2.2084853649139404,
        "learning_rate": 3.826318738777485e-05,
        "epoch": 0.7124855342677124,
        "step": 5541
    },
    {
        "loss": 2.054,
        "grad_norm": 2.1857311725616455,
        "learning_rate": 3.823135257821209e-05,
        "epoch": 0.7126141185547126,
        "step": 5542
    },
    {
        "loss": 1.8495,
        "grad_norm": 1.5240949392318726,
        "learning_rate": 3.819952788725229e-05,
        "epoch": 0.7127427028417127,
        "step": 5543
    },
    {
        "loss": 1.7341,
        "grad_norm": 2.2693753242492676,
        "learning_rate": 3.816771332010879e-05,
        "epoch": 0.7128712871287128,
        "step": 5544
    },
    {
        "loss": 2.1143,
        "grad_norm": 1.7604584693908691,
        "learning_rate": 3.813590888199323e-05,
        "epoch": 0.712999871415713,
        "step": 5545
    },
    {
        "loss": 1.5938,
        "grad_norm": 1.8622010946273804,
        "learning_rate": 3.810411457811571e-05,
        "epoch": 0.7131284557027131,
        "step": 5546
    },
    {
        "loss": 1.7178,
        "grad_norm": 2.3447344303131104,
        "learning_rate": 3.80723304136845e-05,
        "epoch": 0.7132570399897132,
        "step": 5547
    },
    {
        "loss": 1.1388,
        "grad_norm": 2.1719002723693848,
        "learning_rate": 3.8040556393906434e-05,
        "epoch": 0.7133856242767134,
        "step": 5548
    },
    {
        "loss": 1.4151,
        "grad_norm": 1.6408764123916626,
        "learning_rate": 3.800879252398649e-05,
        "epoch": 0.7135142085637135,
        "step": 5549
    },
    {
        "loss": 1.1195,
        "grad_norm": 2.6069962978363037,
        "learning_rate": 3.7977038809127996e-05,
        "epoch": 0.7136427928507136,
        "step": 5550
    },
    {
        "loss": 1.3428,
        "grad_norm": 2.4499013423919678,
        "learning_rate": 3.794529525453281e-05,
        "epoch": 0.7137713771377138,
        "step": 5551
    },
    {
        "loss": 1.2492,
        "grad_norm": 1.7851628065109253,
        "learning_rate": 3.791356186540086e-05,
        "epoch": 0.7138999614247139,
        "step": 5552
    },
    {
        "loss": 1.7637,
        "grad_norm": 2.951127290725708,
        "learning_rate": 3.78818386469306e-05,
        "epoch": 0.714028545711714,
        "step": 5553
    },
    {
        "loss": 1.957,
        "grad_norm": 2.2324178218841553,
        "learning_rate": 3.7850125604318774e-05,
        "epoch": 0.7141571299987142,
        "step": 5554
    },
    {
        "loss": 1.6271,
        "grad_norm": 1.4582549333572388,
        "learning_rate": 3.7818422742760376e-05,
        "epoch": 0.7142857142857143,
        "step": 5555
    },
    {
        "loss": 1.6275,
        "grad_norm": 2.34468936920166,
        "learning_rate": 3.778673006744882e-05,
        "epoch": 0.7144142985727144,
        "step": 5556
    },
    {
        "loss": 1.6849,
        "grad_norm": 2.4178802967071533,
        "learning_rate": 3.775504758357584e-05,
        "epoch": 0.7145428828597146,
        "step": 5557
    },
    {
        "loss": 1.7051,
        "grad_norm": 1.838152527809143,
        "learning_rate": 3.772337529633152e-05,
        "epoch": 0.7146714671467147,
        "step": 5558
    },
    {
        "loss": 2.0041,
        "grad_norm": 2.283310651779175,
        "learning_rate": 3.769171321090417e-05,
        "epoch": 0.7148000514337148,
        "step": 5559
    },
    {
        "loss": 1.5594,
        "grad_norm": 2.1863856315612793,
        "learning_rate": 3.766006133248053e-05,
        "epoch": 0.714928635720715,
        "step": 5560
    },
    {
        "loss": 1.4307,
        "grad_norm": 1.8877718448638916,
        "learning_rate": 3.762841966624568e-05,
        "epoch": 0.7150572200077151,
        "step": 5561
    },
    {
        "loss": 1.2069,
        "grad_norm": 2.657743215560913,
        "learning_rate": 3.759678821738293e-05,
        "epoch": 0.7151858042947152,
        "step": 5562
    },
    {
        "loss": 2.2788,
        "grad_norm": 1.4815037250518799,
        "learning_rate": 3.756516699107396e-05,
        "epoch": 0.7153143885817154,
        "step": 5563
    },
    {
        "loss": 1.7136,
        "grad_norm": 2.0817527770996094,
        "learning_rate": 3.7533555992498834e-05,
        "epoch": 0.7154429728687154,
        "step": 5564
    },
    {
        "loss": 1.4588,
        "grad_norm": 1.9076430797576904,
        "learning_rate": 3.7501955226835916e-05,
        "epoch": 0.7155715571557155,
        "step": 5565
    },
    {
        "loss": 1.6387,
        "grad_norm": 2.4629056453704834,
        "learning_rate": 3.747036469926178e-05,
        "epoch": 0.7157001414427157,
        "step": 5566
    },
    {
        "loss": 1.6542,
        "grad_norm": 1.9730005264282227,
        "learning_rate": 3.743878441495148e-05,
        "epoch": 0.7158287257297158,
        "step": 5567
    },
    {
        "loss": 0.9603,
        "grad_norm": 2.5389745235443115,
        "learning_rate": 3.7407214379078335e-05,
        "epoch": 0.7159573100167159,
        "step": 5568
    },
    {
        "loss": 0.9014,
        "grad_norm": 2.035029888153076,
        "learning_rate": 3.737565459681392e-05,
        "epoch": 0.716085894303716,
        "step": 5569
    },
    {
        "loss": 1.6992,
        "grad_norm": 1.9707005023956299,
        "learning_rate": 3.734410507332822e-05,
        "epoch": 0.7162144785907162,
        "step": 5570
    },
    {
        "loss": 1.3056,
        "grad_norm": 1.981760859489441,
        "learning_rate": 3.731256581378954e-05,
        "epoch": 0.7163430628777163,
        "step": 5571
    },
    {
        "loss": 1.0969,
        "grad_norm": 2.2297515869140625,
        "learning_rate": 3.728103682336439e-05,
        "epoch": 0.7164716471647165,
        "step": 5572
    },
    {
        "loss": 1.0527,
        "grad_norm": 2.7219178676605225,
        "learning_rate": 3.724951810721773e-05,
        "epoch": 0.7166002314517166,
        "step": 5573
    },
    {
        "loss": 1.7655,
        "grad_norm": 1.9852174520492554,
        "learning_rate": 3.721800967051278e-05,
        "epoch": 0.7167288157387167,
        "step": 5574
    },
    {
        "loss": 1.6558,
        "grad_norm": 3.6284189224243164,
        "learning_rate": 3.7186511518411097e-05,
        "epoch": 0.7168574000257169,
        "step": 5575
    },
    {
        "loss": 1.9218,
        "grad_norm": 2.122738838195801,
        "learning_rate": 3.715502365607249e-05,
        "epoch": 0.716985984312717,
        "step": 5576
    },
    {
        "loss": 1.8698,
        "grad_norm": 2.4693496227264404,
        "learning_rate": 3.712354608865515e-05,
        "epoch": 0.7171145685997171,
        "step": 5577
    },
    {
        "loss": 1.7222,
        "grad_norm": 2.58465576171875,
        "learning_rate": 3.709207882131559e-05,
        "epoch": 0.7172431528867173,
        "step": 5578
    },
    {
        "loss": 1.3837,
        "grad_norm": 2.342104434967041,
        "learning_rate": 3.7060621859208524e-05,
        "epoch": 0.7173717371737174,
        "step": 5579
    },
    {
        "loss": 1.8061,
        "grad_norm": 2.089198589324951,
        "learning_rate": 3.702917520748718e-05,
        "epoch": 0.7175003214607175,
        "step": 5580
    },
    {
        "loss": 1.9065,
        "grad_norm": 2.5467584133148193,
        "learning_rate": 3.699773887130291e-05,
        "epoch": 0.7176289057477176,
        "step": 5581
    },
    {
        "loss": 1.5363,
        "grad_norm": 2.477761745452881,
        "learning_rate": 3.6966312855805374e-05,
        "epoch": 0.7177574900347178,
        "step": 5582
    },
    {
        "loss": 1.8522,
        "grad_norm": 1.7998936176300049,
        "learning_rate": 3.693489716614277e-05,
        "epoch": 0.7178860743217179,
        "step": 5583
    },
    {
        "loss": 1.7797,
        "grad_norm": 2.3184378147125244,
        "learning_rate": 3.69034918074613e-05,
        "epoch": 0.718014658608718,
        "step": 5584
    },
    {
        "loss": 1.6672,
        "grad_norm": 1.8328776359558105,
        "learning_rate": 3.687209678490573e-05,
        "epoch": 0.7181432428957182,
        "step": 5585
    },
    {
        "loss": 2.1155,
        "grad_norm": 1.8700823783874512,
        "learning_rate": 3.6840712103618923e-05,
        "epoch": 0.7182718271827183,
        "step": 5586
    },
    {
        "loss": 1.9198,
        "grad_norm": 2.2948827743530273,
        "learning_rate": 3.6809337768742195e-05,
        "epoch": 0.7184004114697184,
        "step": 5587
    },
    {
        "loss": 1.8108,
        "grad_norm": 1.5102214813232422,
        "learning_rate": 3.677797378541514e-05,
        "epoch": 0.7185289957567186,
        "step": 5588
    },
    {
        "loss": 1.6364,
        "grad_norm": 2.234447479248047,
        "learning_rate": 3.6746620158775546e-05,
        "epoch": 0.7186575800437186,
        "step": 5589
    },
    {
        "loss": 1.8645,
        "grad_norm": 2.0117342472076416,
        "learning_rate": 3.671527689395973e-05,
        "epoch": 0.7187861643307187,
        "step": 5590
    },
    {
        "loss": 1.4556,
        "grad_norm": 2.151216745376587,
        "learning_rate": 3.66839439961021e-05,
        "epoch": 0.7189147486177189,
        "step": 5591
    },
    {
        "loss": 1.5611,
        "grad_norm": 2.8621811866760254,
        "learning_rate": 3.6652621470335365e-05,
        "epoch": 0.719043332904719,
        "step": 5592
    },
    {
        "loss": 1.3977,
        "grad_norm": 1.441798210144043,
        "learning_rate": 3.6621309321790763e-05,
        "epoch": 0.7191719171917191,
        "step": 5593
    },
    {
        "loss": 1.2101,
        "grad_norm": 2.742570400238037,
        "learning_rate": 3.659000755559756e-05,
        "epoch": 0.7193005014787193,
        "step": 5594
    },
    {
        "loss": 1.6598,
        "grad_norm": 2.6105237007141113,
        "learning_rate": 3.655871617688349e-05,
        "epoch": 0.7194290857657194,
        "step": 5595
    },
    {
        "loss": 1.3366,
        "grad_norm": 2.4280312061309814,
        "learning_rate": 3.652743519077456e-05,
        "epoch": 0.7195576700527195,
        "step": 5596
    },
    {
        "loss": 1.5892,
        "grad_norm": 2.7581465244293213,
        "learning_rate": 3.649616460239499e-05,
        "epoch": 0.7196862543397197,
        "step": 5597
    },
    {
        "loss": 2.3903,
        "grad_norm": 1.2723795175552368,
        "learning_rate": 3.646490441686738e-05,
        "epoch": 0.7198148386267198,
        "step": 5598
    },
    {
        "loss": 2.5129,
        "grad_norm": 1.1761558055877686,
        "learning_rate": 3.643365463931263e-05,
        "epoch": 0.7199434229137199,
        "step": 5599
    },
    {
        "loss": 1.9818,
        "grad_norm": 2.1423897743225098,
        "learning_rate": 3.640241527484991e-05,
        "epoch": 0.7200720072007201,
        "step": 5600
    },
    {
        "eval_loss": 1.763490080833435,
        "eval_runtime": 28.2542,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.7200720072007201,
        "step": 5600
    },
    {
        "loss": 1.0658,
        "grad_norm": 1.8046936988830566,
        "learning_rate": 3.6371186328596617e-05,
        "epoch": 0.7202005914877202,
        "step": 5601
    },
    {
        "loss": 1.4583,
        "grad_norm": 2.547863483428955,
        "learning_rate": 3.6339967805668565e-05,
        "epoch": 0.7203291757747203,
        "step": 5602
    },
    {
        "loss": 1.1399,
        "grad_norm": 2.332606315612793,
        "learning_rate": 3.630875971117982e-05,
        "epoch": 0.7204577600617205,
        "step": 5603
    },
    {
        "loss": 0.6779,
        "grad_norm": 2.164883613586426,
        "learning_rate": 3.627756205024265e-05,
        "epoch": 0.7205863443487206,
        "step": 5604
    },
    {
        "loss": 1.9807,
        "grad_norm": 2.37597918510437,
        "learning_rate": 3.624637482796772e-05,
        "epoch": 0.7207149286357207,
        "step": 5605
    },
    {
        "loss": 1.4887,
        "grad_norm": 1.702382206916809,
        "learning_rate": 3.6215198049464006e-05,
        "epoch": 0.7208435129227209,
        "step": 5606
    },
    {
        "loss": 2.1757,
        "grad_norm": 2.2509970664978027,
        "learning_rate": 3.618403171983863e-05,
        "epoch": 0.720972097209721,
        "step": 5607
    },
    {
        "loss": 0.7674,
        "grad_norm": 2.024470567703247,
        "learning_rate": 3.615287584419712e-05,
        "epoch": 0.7211006814967211,
        "step": 5608
    },
    {
        "loss": 1.4312,
        "grad_norm": 2.1565439701080322,
        "learning_rate": 3.612173042764327e-05,
        "epoch": 0.7212292657837213,
        "step": 5609
    },
    {
        "loss": 1.8985,
        "grad_norm": 1.662965178489685,
        "learning_rate": 3.609059547527919e-05,
        "epoch": 0.7213578500707214,
        "step": 5610
    },
    {
        "loss": 2.0872,
        "grad_norm": 2.4888648986816406,
        "learning_rate": 3.605947099220516e-05,
        "epoch": 0.7214864343577215,
        "step": 5611
    },
    {
        "loss": 1.7722,
        "grad_norm": 2.3314340114593506,
        "learning_rate": 3.6028356983519874e-05,
        "epoch": 0.7216150186447217,
        "step": 5612
    },
    {
        "loss": 1.108,
        "grad_norm": 2.312826156616211,
        "learning_rate": 3.599725345432027e-05,
        "epoch": 0.7217436029317218,
        "step": 5613
    },
    {
        "loss": 1.7206,
        "grad_norm": 1.9035571813583374,
        "learning_rate": 3.596616040970151e-05,
        "epoch": 0.7218721872187218,
        "step": 5614
    },
    {
        "loss": 2.2267,
        "grad_norm": 1.5363004207611084,
        "learning_rate": 3.593507785475711e-05,
        "epoch": 0.722000771505722,
        "step": 5615
    },
    {
        "loss": 1.8537,
        "grad_norm": 2.3829329013824463,
        "learning_rate": 3.5904005794578854e-05,
        "epoch": 0.7221293557927221,
        "step": 5616
    },
    {
        "loss": 1.7823,
        "grad_norm": 1.5754222869873047,
        "learning_rate": 3.587294423425682e-05,
        "epoch": 0.7222579400797222,
        "step": 5617
    },
    {
        "loss": 1.4242,
        "grad_norm": 2.4787447452545166,
        "learning_rate": 3.5841893178879285e-05,
        "epoch": 0.7223865243667223,
        "step": 5618
    },
    {
        "loss": 1.5707,
        "grad_norm": 3.066774845123291,
        "learning_rate": 3.581085263353289e-05,
        "epoch": 0.7225151086537225,
        "step": 5619
    },
    {
        "loss": 1.7862,
        "grad_norm": 2.4369847774505615,
        "learning_rate": 3.577982260330257e-05,
        "epoch": 0.7226436929407226,
        "step": 5620
    },
    {
        "loss": 1.6194,
        "grad_norm": 2.0115768909454346,
        "learning_rate": 3.5748803093271386e-05,
        "epoch": 0.7227722772277227,
        "step": 5621
    },
    {
        "loss": 1.7158,
        "grad_norm": 1.9733673334121704,
        "learning_rate": 3.571779410852092e-05,
        "epoch": 0.7229008615147229,
        "step": 5622
    },
    {
        "loss": 1.9405,
        "grad_norm": 1.921572208404541,
        "learning_rate": 3.5686795654130834e-05,
        "epoch": 0.723029445801723,
        "step": 5623
    },
    {
        "loss": 1.452,
        "grad_norm": 1.9895819425582886,
        "learning_rate": 3.565580773517909e-05,
        "epoch": 0.7231580300887231,
        "step": 5624
    },
    {
        "loss": 1.5765,
        "grad_norm": 1.5378117561340332,
        "learning_rate": 3.562483035674199e-05,
        "epoch": 0.7232866143757233,
        "step": 5625
    },
    {
        "loss": 2.1445,
        "grad_norm": 1.2301921844482422,
        "learning_rate": 3.5593863523894076e-05,
        "epoch": 0.7234151986627234,
        "step": 5626
    },
    {
        "loss": 2.2267,
        "grad_norm": 2.1920254230499268,
        "learning_rate": 3.556290724170822e-05,
        "epoch": 0.7235437829497235,
        "step": 5627
    },
    {
        "loss": 1.8637,
        "grad_norm": 2.362910032272339,
        "learning_rate": 3.553196151525542e-05,
        "epoch": 0.7236723672367237,
        "step": 5628
    },
    {
        "loss": 1.8306,
        "grad_norm": 2.2177603244781494,
        "learning_rate": 3.550102634960507e-05,
        "epoch": 0.7238009515237238,
        "step": 5629
    },
    {
        "loss": 1.7101,
        "grad_norm": 3.395688533782959,
        "learning_rate": 3.547010174982486e-05,
        "epoch": 0.723929535810724,
        "step": 5630
    },
    {
        "loss": 1.6491,
        "grad_norm": 1.9332932233810425,
        "learning_rate": 3.543918772098056e-05,
        "epoch": 0.7240581200977241,
        "step": 5631
    },
    {
        "loss": 1.2834,
        "grad_norm": 3.6084275245666504,
        "learning_rate": 3.540828426813648e-05,
        "epoch": 0.7241867043847242,
        "step": 5632
    },
    {
        "loss": 2.3067,
        "grad_norm": 2.026440382003784,
        "learning_rate": 3.5377391396355e-05,
        "epoch": 0.7243152886717243,
        "step": 5633
    },
    {
        "loss": 1.0373,
        "grad_norm": 1.9873749017715454,
        "learning_rate": 3.534650911069673e-05,
        "epoch": 0.7244438729587245,
        "step": 5634
    },
    {
        "loss": 1.9517,
        "grad_norm": 1.2867611646652222,
        "learning_rate": 3.5315637416220795e-05,
        "epoch": 0.7245724572457246,
        "step": 5635
    },
    {
        "loss": 2.4216,
        "grad_norm": 1.847388505935669,
        "learning_rate": 3.5284776317984326e-05,
        "epoch": 0.7247010415327247,
        "step": 5636
    },
    {
        "loss": 2.2858,
        "grad_norm": 2.11138653755188,
        "learning_rate": 3.525392582104283e-05,
        "epoch": 0.7248296258197249,
        "step": 5637
    },
    {
        "loss": 1.8709,
        "grad_norm": 1.393786907196045,
        "learning_rate": 3.522308593045013e-05,
        "epoch": 0.724958210106725,
        "step": 5638
    },
    {
        "loss": 1.9572,
        "grad_norm": 2.747403383255005,
        "learning_rate": 3.519225665125817e-05,
        "epoch": 0.725086794393725,
        "step": 5639
    },
    {
        "loss": 1.2467,
        "grad_norm": 2.2151904106140137,
        "learning_rate": 3.5161437988517265e-05,
        "epoch": 0.7252153786807252,
        "step": 5640
    },
    {
        "loss": 2.165,
        "grad_norm": 1.9932695627212524,
        "learning_rate": 3.513062994727595e-05,
        "epoch": 0.7253439629677253,
        "step": 5641
    },
    {
        "loss": 0.9727,
        "grad_norm": 2.7784512042999268,
        "learning_rate": 3.5099832532581086e-05,
        "epoch": 0.7254725472547254,
        "step": 5642
    },
    {
        "loss": 1.8208,
        "grad_norm": 2.281930446624756,
        "learning_rate": 3.506904574947766e-05,
        "epoch": 0.7256011315417256,
        "step": 5643
    },
    {
        "loss": 2.086,
        "grad_norm": 1.7439218759536743,
        "learning_rate": 3.503826960300903e-05,
        "epoch": 0.7257297158287257,
        "step": 5644
    },
    {
        "loss": 1.8224,
        "grad_norm": 2.0639679431915283,
        "learning_rate": 3.50075040982168e-05,
        "epoch": 0.7258583001157258,
        "step": 5645
    },
    {
        "loss": 0.8247,
        "grad_norm": 3.375983953475952,
        "learning_rate": 3.497674924014076e-05,
        "epoch": 0.725986884402726,
        "step": 5646
    },
    {
        "loss": 1.483,
        "grad_norm": 1.933760166168213,
        "learning_rate": 3.494600503381902e-05,
        "epoch": 0.7261154686897261,
        "step": 5647
    },
    {
        "loss": 1.4806,
        "grad_norm": 2.9463112354278564,
        "learning_rate": 3.491527148428797e-05,
        "epoch": 0.7262440529767262,
        "step": 5648
    },
    {
        "loss": 1.805,
        "grad_norm": 2.546412467956543,
        "learning_rate": 3.4884548596582136e-05,
        "epoch": 0.7263726372637264,
        "step": 5649
    },
    {
        "loss": 1.4305,
        "grad_norm": 2.4737980365753174,
        "learning_rate": 3.485383637573442e-05,
        "epoch": 0.7265012215507265,
        "step": 5650
    },
    {
        "loss": 1.5831,
        "grad_norm": 2.142411708831787,
        "learning_rate": 3.482313482677592e-05,
        "epoch": 0.7266298058377266,
        "step": 5651
    },
    {
        "loss": 1.4078,
        "grad_norm": 1.9914997816085815,
        "learning_rate": 3.479244395473603e-05,
        "epoch": 0.7267583901247268,
        "step": 5652
    },
    {
        "loss": 1.3826,
        "grad_norm": 1.7430565357208252,
        "learning_rate": 3.4761763764642305e-05,
        "epoch": 0.7268869744117269,
        "step": 5653
    },
    {
        "loss": 2.3553,
        "grad_norm": 2.0454375743865967,
        "learning_rate": 3.473109426152064e-05,
        "epoch": 0.727015558698727,
        "step": 5654
    },
    {
        "loss": 2.069,
        "grad_norm": 1.8991491794586182,
        "learning_rate": 3.470043545039515e-05,
        "epoch": 0.7271441429857272,
        "step": 5655
    },
    {
        "loss": 1.2772,
        "grad_norm": 3.102144241333008,
        "learning_rate": 3.4669787336288176e-05,
        "epoch": 0.7272727272727273,
        "step": 5656
    },
    {
        "loss": 1.1758,
        "grad_norm": 1.7309813499450684,
        "learning_rate": 3.4639149924220314e-05,
        "epoch": 0.7274013115597274,
        "step": 5657
    },
    {
        "loss": 1.5113,
        "grad_norm": 2.3589978218078613,
        "learning_rate": 3.4608523219210475e-05,
        "epoch": 0.7275298958467276,
        "step": 5658
    },
    {
        "loss": 1.6368,
        "grad_norm": 1.3990811109542847,
        "learning_rate": 3.457790722627568e-05,
        "epoch": 0.7276584801337277,
        "step": 5659
    },
    {
        "loss": 1.8612,
        "grad_norm": 1.8156148195266724,
        "learning_rate": 3.454730195043133e-05,
        "epoch": 0.7277870644207278,
        "step": 5660
    },
    {
        "loss": 1.9758,
        "grad_norm": 1.3454722166061401,
        "learning_rate": 3.451670739669098e-05,
        "epoch": 0.727915648707728,
        "step": 5661
    },
    {
        "loss": 2.0511,
        "grad_norm": 1.579777479171753,
        "learning_rate": 3.448612357006653e-05,
        "epoch": 0.7280442329947281,
        "step": 5662
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.669201374053955,
        "learning_rate": 3.445555047556797e-05,
        "epoch": 0.7281728172817282,
        "step": 5663
    },
    {
        "loss": 1.8758,
        "grad_norm": 2.4386322498321533,
        "learning_rate": 3.442498811820365e-05,
        "epoch": 0.7283014015687282,
        "step": 5664
    },
    {
        "loss": 1.6756,
        "grad_norm": 2.1857540607452393,
        "learning_rate": 3.439443650298017e-05,
        "epoch": 0.7284299858557284,
        "step": 5665
    },
    {
        "loss": 1.904,
        "grad_norm": 1.9283504486083984,
        "learning_rate": 3.436389563490226e-05,
        "epoch": 0.7285585701427285,
        "step": 5666
    },
    {
        "loss": 1.5825,
        "grad_norm": 1.9377416372299194,
        "learning_rate": 3.4333365518972984e-05,
        "epoch": 0.7286871544297286,
        "step": 5667
    },
    {
        "loss": 1.8872,
        "grad_norm": 1.8692079782485962,
        "learning_rate": 3.430284616019363e-05,
        "epoch": 0.7288157387167288,
        "step": 5668
    },
    {
        "loss": 2.113,
        "grad_norm": 1.545025110244751,
        "learning_rate": 3.427233756356373e-05,
        "epoch": 0.7289443230037289,
        "step": 5669
    },
    {
        "loss": 1.1377,
        "grad_norm": 3.1040303707122803,
        "learning_rate": 3.4241839734081e-05,
        "epoch": 0.729072907290729,
        "step": 5670
    },
    {
        "loss": 1.8337,
        "grad_norm": 2.1090354919433594,
        "learning_rate": 3.421135267674142e-05,
        "epoch": 0.7292014915777292,
        "step": 5671
    },
    {
        "loss": 1.7079,
        "grad_norm": 1.9266241788864136,
        "learning_rate": 3.4180876396539285e-05,
        "epoch": 0.7293300758647293,
        "step": 5672
    },
    {
        "loss": 1.5838,
        "grad_norm": 2.365748405456543,
        "learning_rate": 3.4150410898466934e-05,
        "epoch": 0.7294586601517294,
        "step": 5673
    },
    {
        "loss": 1.4565,
        "grad_norm": 2.1967170238494873,
        "learning_rate": 3.411995618751519e-05,
        "epoch": 0.7295872444387296,
        "step": 5674
    },
    {
        "loss": 1.8331,
        "grad_norm": 2.583057403564453,
        "learning_rate": 3.408951226867293e-05,
        "epoch": 0.7297158287257297,
        "step": 5675
    },
    {
        "loss": 1.8206,
        "grad_norm": 1.6202346086502075,
        "learning_rate": 3.405907914692722e-05,
        "epoch": 0.7298444130127298,
        "step": 5676
    },
    {
        "loss": 1.7654,
        "grad_norm": 1.4958000183105469,
        "learning_rate": 3.402865682726359e-05,
        "epoch": 0.72997299729973,
        "step": 5677
    },
    {
        "loss": 1.3917,
        "grad_norm": 1.9546064138412476,
        "learning_rate": 3.399824531466557e-05,
        "epoch": 0.7301015815867301,
        "step": 5678
    },
    {
        "loss": 1.6381,
        "grad_norm": 2.4674928188323975,
        "learning_rate": 3.396784461411502e-05,
        "epoch": 0.7302301658737302,
        "step": 5679
    },
    {
        "loss": 1.8586,
        "grad_norm": 2.019843101501465,
        "learning_rate": 3.3937454730592066e-05,
        "epoch": 0.7303587501607304,
        "step": 5680
    },
    {
        "loss": 1.2725,
        "grad_norm": 2.062209367752075,
        "learning_rate": 3.390707566907494e-05,
        "epoch": 0.7304873344477305,
        "step": 5681
    },
    {
        "loss": 1.0164,
        "grad_norm": 1.472678542137146,
        "learning_rate": 3.387670743454025e-05,
        "epoch": 0.7306159187347306,
        "step": 5682
    },
    {
        "loss": 0.5745,
        "grad_norm": 1.4852546453475952,
        "learning_rate": 3.384635003196264e-05,
        "epoch": 0.7307445030217308,
        "step": 5683
    },
    {
        "loss": 1.3355,
        "grad_norm": 1.9930323362350464,
        "learning_rate": 3.381600346631524e-05,
        "epoch": 0.7308730873087309,
        "step": 5684
    },
    {
        "loss": 1.6155,
        "grad_norm": 2.709028720855713,
        "learning_rate": 3.378566774256918e-05,
        "epoch": 0.731001671595731,
        "step": 5685
    },
    {
        "loss": 1.1102,
        "grad_norm": 2.8677408695220947,
        "learning_rate": 3.375534286569384e-05,
        "epoch": 0.7311302558827312,
        "step": 5686
    },
    {
        "loss": 2.0428,
        "grad_norm": 2.140793561935425,
        "learning_rate": 3.3725028840657e-05,
        "epoch": 0.7312588401697313,
        "step": 5687
    },
    {
        "loss": 1.66,
        "grad_norm": 1.9736491441726685,
        "learning_rate": 3.369472567242444e-05,
        "epoch": 0.7313874244567314,
        "step": 5688
    },
    {
        "loss": 2.1011,
        "grad_norm": 2.3203682899475098,
        "learning_rate": 3.36644333659603e-05,
        "epoch": 0.7315160087437315,
        "step": 5689
    },
    {
        "loss": 2.2,
        "grad_norm": 2.080730438232422,
        "learning_rate": 3.363415192622693e-05,
        "epoch": 0.7316445930307316,
        "step": 5690
    },
    {
        "loss": 0.9037,
        "grad_norm": 1.3783341646194458,
        "learning_rate": 3.3603881358184795e-05,
        "epoch": 0.7317731773177317,
        "step": 5691
    },
    {
        "loss": 1.7792,
        "grad_norm": 2.3515000343322754,
        "learning_rate": 3.3573621666792695e-05,
        "epoch": 0.7319017616047319,
        "step": 5692
    },
    {
        "loss": 1.5907,
        "grad_norm": 2.0053348541259766,
        "learning_rate": 3.3543372857007605e-05,
        "epoch": 0.732030345891732,
        "step": 5693
    },
    {
        "loss": 1.7491,
        "grad_norm": 2.4293839931488037,
        "learning_rate": 3.351313493378476e-05,
        "epoch": 0.7321589301787321,
        "step": 5694
    },
    {
        "loss": 1.8133,
        "grad_norm": 1.9847222566604614,
        "learning_rate": 3.3482907902077495e-05,
        "epoch": 0.7322875144657323,
        "step": 5695
    },
    {
        "loss": 1.2882,
        "grad_norm": 4.718419551849365,
        "learning_rate": 3.3452691766837475e-05,
        "epoch": 0.7324160987527324,
        "step": 5696
    },
    {
        "loss": 1.2995,
        "grad_norm": 1.9344708919525146,
        "learning_rate": 3.342248653301459e-05,
        "epoch": 0.7325446830397325,
        "step": 5697
    },
    {
        "loss": 1.1706,
        "grad_norm": 2.5183277130126953,
        "learning_rate": 3.33922922055568e-05,
        "epoch": 0.7326732673267327,
        "step": 5698
    },
    {
        "loss": 1.2646,
        "grad_norm": 2.1783087253570557,
        "learning_rate": 3.3362108789410426e-05,
        "epoch": 0.7328018516137328,
        "step": 5699
    },
    {
        "loss": 1.9842,
        "grad_norm": 3.0185413360595703,
        "learning_rate": 3.333193628951999e-05,
        "epoch": 0.7329304359007329,
        "step": 5700
    },
    {
        "eval_loss": 1.7593178749084473,
        "eval_runtime": 28.3449,
        "eval_samples_per_second": 2.787,
        "eval_steps_per_second": 2.787,
        "epoch": 0.7329304359007329,
        "step": 5700
    },
    {
        "loss": 1.73,
        "grad_norm": 2.121461868286133,
        "learning_rate": 3.3301774710828114e-05,
        "epoch": 0.7330590201877331,
        "step": 5701
    },
    {
        "loss": 2.1318,
        "grad_norm": 1.6536012887954712,
        "learning_rate": 3.327162405827573e-05,
        "epoch": 0.7331876044747332,
        "step": 5702
    },
    {
        "loss": 2.3094,
        "grad_norm": 1.736069679260254,
        "learning_rate": 3.324148433680196e-05,
        "epoch": 0.7333161887617333,
        "step": 5703
    },
    {
        "loss": 1.9481,
        "grad_norm": 1.067180871963501,
        "learning_rate": 3.3211355551344156e-05,
        "epoch": 0.7334447730487335,
        "step": 5704
    },
    {
        "loss": 0.9771,
        "grad_norm": 2.152273416519165,
        "learning_rate": 3.318123770683781e-05,
        "epoch": 0.7335733573357336,
        "step": 5705
    },
    {
        "loss": 0.9096,
        "grad_norm": 1.5161798000335693,
        "learning_rate": 3.315113080821666e-05,
        "epoch": 0.7337019416227337,
        "step": 5706
    },
    {
        "loss": 2.2031,
        "grad_norm": 1.4571709632873535,
        "learning_rate": 3.31210348604127e-05,
        "epoch": 0.7338305259097339,
        "step": 5707
    },
    {
        "loss": 1.0019,
        "grad_norm": 2.3312478065490723,
        "learning_rate": 3.3090949868356025e-05,
        "epoch": 0.733959110196734,
        "step": 5708
    },
    {
        "loss": 1.7177,
        "grad_norm": 2.639068126678467,
        "learning_rate": 3.3060875836975026e-05,
        "epoch": 0.7340876944837341,
        "step": 5709
    },
    {
        "loss": 1.934,
        "grad_norm": 2.114161729812622,
        "learning_rate": 3.3030812771196294e-05,
        "epoch": 0.7342162787707343,
        "step": 5710
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.1771252155303955,
        "learning_rate": 3.300076067594453e-05,
        "epoch": 0.7343448630577344,
        "step": 5711
    },
    {
        "loss": 1.1197,
        "grad_norm": 1.7166097164154053,
        "learning_rate": 3.297071955614275e-05,
        "epoch": 0.7344734473447345,
        "step": 5712
    },
    {
        "loss": 2.1233,
        "grad_norm": 2.08115816116333,
        "learning_rate": 3.2940689416712113e-05,
        "epoch": 0.7346020316317347,
        "step": 5713
    },
    {
        "loss": 1.6391,
        "grad_norm": 2.9062371253967285,
        "learning_rate": 3.291067026257203e-05,
        "epoch": 0.7347306159187347,
        "step": 5714
    },
    {
        "loss": 0.839,
        "grad_norm": 2.480565309524536,
        "learning_rate": 3.288066209863997e-05,
        "epoch": 0.7348592002057348,
        "step": 5715
    },
    {
        "loss": 1.3241,
        "grad_norm": 1.990615725517273,
        "learning_rate": 3.2850664929831856e-05,
        "epoch": 0.7349877844927349,
        "step": 5716
    },
    {
        "loss": 1.789,
        "grad_norm": 2.442546844482422,
        "learning_rate": 3.282067876106157e-05,
        "epoch": 0.7351163687797351,
        "step": 5717
    },
    {
        "loss": 2.5416,
        "grad_norm": 1.7573171854019165,
        "learning_rate": 3.279070359724124e-05,
        "epoch": 0.7352449530667352,
        "step": 5718
    },
    {
        "loss": 1.6881,
        "grad_norm": 2.0268681049346924,
        "learning_rate": 3.2760739443281364e-05,
        "epoch": 0.7353735373537353,
        "step": 5719
    },
    {
        "loss": 0.7559,
        "grad_norm": 2.091116189956665,
        "learning_rate": 3.273078630409039e-05,
        "epoch": 0.7355021216407355,
        "step": 5720
    },
    {
        "loss": 1.6262,
        "grad_norm": 3.9597318172454834,
        "learning_rate": 3.2700844184575144e-05,
        "epoch": 0.7356307059277356,
        "step": 5721
    },
    {
        "loss": 1.7394,
        "grad_norm": 1.2331829071044922,
        "learning_rate": 3.267091308964053e-05,
        "epoch": 0.7357592902147357,
        "step": 5722
    },
    {
        "loss": 1.694,
        "grad_norm": 1.7728815078735352,
        "learning_rate": 3.2640993024189725e-05,
        "epoch": 0.7358878745017359,
        "step": 5723
    },
    {
        "loss": 1.5815,
        "grad_norm": 1.5254806280136108,
        "learning_rate": 3.2611083993124105e-05,
        "epoch": 0.736016458788736,
        "step": 5724
    },
    {
        "loss": 1.4625,
        "grad_norm": 2.241568088531494,
        "learning_rate": 3.2581186001343104e-05,
        "epoch": 0.7361450430757361,
        "step": 5725
    },
    {
        "loss": 1.3827,
        "grad_norm": 3.576202154159546,
        "learning_rate": 3.2551299053744564e-05,
        "epoch": 0.7362736273627363,
        "step": 5726
    },
    {
        "loss": 1.8728,
        "grad_norm": 2.3971798419952393,
        "learning_rate": 3.252142315522437e-05,
        "epoch": 0.7364022116497364,
        "step": 5727
    },
    {
        "loss": 1.3742,
        "grad_norm": 1.96384596824646,
        "learning_rate": 3.249155831067653e-05,
        "epoch": 0.7365307959367365,
        "step": 5728
    },
    {
        "loss": 1.2928,
        "grad_norm": 1.9353322982788086,
        "learning_rate": 3.24617045249935e-05,
        "epoch": 0.7366593802237367,
        "step": 5729
    },
    {
        "loss": 2.2309,
        "grad_norm": 1.268288493156433,
        "learning_rate": 3.243186180306563e-05,
        "epoch": 0.7367879645107368,
        "step": 5730
    },
    {
        "loss": 1.4259,
        "grad_norm": 1.9038006067276,
        "learning_rate": 3.240203014978166e-05,
        "epoch": 0.7369165487977369,
        "step": 5731
    },
    {
        "loss": 1.4245,
        "grad_norm": 1.9089906215667725,
        "learning_rate": 3.237220957002847e-05,
        "epoch": 0.7370451330847371,
        "step": 5732
    },
    {
        "loss": 1.4687,
        "grad_norm": 3.053048849105835,
        "learning_rate": 3.234240006869103e-05,
        "epoch": 0.7371737173717372,
        "step": 5733
    },
    {
        "loss": 1.1885,
        "grad_norm": 1.0504752397537231,
        "learning_rate": 3.2312601650652596e-05,
        "epoch": 0.7373023016587373,
        "step": 5734
    },
    {
        "loss": 2.2429,
        "grad_norm": 2.0472347736358643,
        "learning_rate": 3.2282814320794606e-05,
        "epoch": 0.7374308859457375,
        "step": 5735
    },
    {
        "loss": 1.7217,
        "grad_norm": 1.688528060913086,
        "learning_rate": 3.225303808399669e-05,
        "epoch": 0.7375594702327376,
        "step": 5736
    },
    {
        "loss": 2.2901,
        "grad_norm": 1.5410614013671875,
        "learning_rate": 3.222327294513654e-05,
        "epoch": 0.7376880545197377,
        "step": 5737
    },
    {
        "loss": 1.5619,
        "grad_norm": 2.9862358570098877,
        "learning_rate": 3.219351890909017e-05,
        "epoch": 0.7378166388067379,
        "step": 5738
    },
    {
        "loss": 1.5652,
        "grad_norm": 2.3348169326782227,
        "learning_rate": 3.216377598073175e-05,
        "epoch": 0.7379452230937379,
        "step": 5739
    },
    {
        "loss": 2.1989,
        "grad_norm": 1.5720280408859253,
        "learning_rate": 3.213404416493354e-05,
        "epoch": 0.738073807380738,
        "step": 5740
    },
    {
        "loss": 2.2123,
        "grad_norm": 2.1894447803497314,
        "learning_rate": 3.2104323466566076e-05,
        "epoch": 0.7382023916677382,
        "step": 5741
    },
    {
        "loss": 2.1592,
        "grad_norm": 2.151522159576416,
        "learning_rate": 3.207461389049807e-05,
        "epoch": 0.7383309759547383,
        "step": 5742
    },
    {
        "loss": 1.0758,
        "grad_norm": 2.8917129039764404,
        "learning_rate": 3.204491544159632e-05,
        "epoch": 0.7384595602417384,
        "step": 5743
    },
    {
        "loss": 1.7198,
        "grad_norm": 3.694014549255371,
        "learning_rate": 3.2015228124725915e-05,
        "epoch": 0.7385881445287386,
        "step": 5744
    },
    {
        "loss": 1.6794,
        "grad_norm": 3.134768486022949,
        "learning_rate": 3.1985551944750036e-05,
        "epoch": 0.7387167288157387,
        "step": 5745
    },
    {
        "loss": 1.6106,
        "grad_norm": 2.2331032752990723,
        "learning_rate": 3.195588690653013e-05,
        "epoch": 0.7388453131027388,
        "step": 5746
    },
    {
        "loss": 1.8939,
        "grad_norm": 1.9503995180130005,
        "learning_rate": 3.1926233014925686e-05,
        "epoch": 0.738973897389739,
        "step": 5747
    },
    {
        "loss": 2.2576,
        "grad_norm": 1.664455771446228,
        "learning_rate": 3.189659027479448e-05,
        "epoch": 0.7391024816767391,
        "step": 5748
    },
    {
        "loss": 2.0014,
        "grad_norm": 1.7396268844604492,
        "learning_rate": 3.1866958690992446e-05,
        "epoch": 0.7392310659637392,
        "step": 5749
    },
    {
        "loss": 2.3847,
        "grad_norm": 1.8437221050262451,
        "learning_rate": 3.183733826837362e-05,
        "epoch": 0.7393596502507394,
        "step": 5750
    },
    {
        "loss": 2.6592,
        "grad_norm": 1.6328164339065552,
        "learning_rate": 3.1807729011790275e-05,
        "epoch": 0.7394882345377395,
        "step": 5751
    },
    {
        "loss": 1.7256,
        "grad_norm": 2.016204595565796,
        "learning_rate": 3.1778130926092884e-05,
        "epoch": 0.7396168188247396,
        "step": 5752
    },
    {
        "loss": 1.7716,
        "grad_norm": 1.6781153678894043,
        "learning_rate": 3.174854401612998e-05,
        "epoch": 0.7397454031117398,
        "step": 5753
    },
    {
        "loss": 2.5735,
        "grad_norm": 1.3675512075424194,
        "learning_rate": 3.171896828674834e-05,
        "epoch": 0.7398739873987399,
        "step": 5754
    },
    {
        "loss": 1.9174,
        "grad_norm": 1.538103461265564,
        "learning_rate": 3.168940374279292e-05,
        "epoch": 0.74000257168574,
        "step": 5755
    },
    {
        "loss": 1.4494,
        "grad_norm": 2.3644399642944336,
        "learning_rate": 3.1659850389106846e-05,
        "epoch": 0.7401311559727402,
        "step": 5756
    },
    {
        "loss": 1.6386,
        "grad_norm": 1.9414927959442139,
        "learning_rate": 3.163030823053132e-05,
        "epoch": 0.7402597402597403,
        "step": 5757
    },
    {
        "loss": 1.6527,
        "grad_norm": 2.1897246837615967,
        "learning_rate": 3.1600777271905824e-05,
        "epoch": 0.7403883245467404,
        "step": 5758
    },
    {
        "loss": 1.8965,
        "grad_norm": 1.831351399421692,
        "learning_rate": 3.157125751806797e-05,
        "epoch": 0.7405169088337406,
        "step": 5759
    },
    {
        "loss": 1.8167,
        "grad_norm": 2.1760377883911133,
        "learning_rate": 3.154174897385349e-05,
        "epoch": 0.7406454931207407,
        "step": 5760
    },
    {
        "loss": 1.4624,
        "grad_norm": 2.06561279296875,
        "learning_rate": 3.1512251644096314e-05,
        "epoch": 0.7407740774077408,
        "step": 5761
    },
    {
        "loss": 1.502,
        "grad_norm": 2.0602710247039795,
        "learning_rate": 3.148276553362857e-05,
        "epoch": 0.740902661694741,
        "step": 5762
    },
    {
        "loss": 1.5701,
        "grad_norm": 2.411057233810425,
        "learning_rate": 3.145329064728045e-05,
        "epoch": 0.7410312459817411,
        "step": 5763
    },
    {
        "loss": 2.483,
        "grad_norm": 2.0334253311157227,
        "learning_rate": 3.142382698988041e-05,
        "epoch": 0.7411598302687411,
        "step": 5764
    },
    {
        "loss": 1.0065,
        "grad_norm": 2.433865785598755,
        "learning_rate": 3.1394374566255025e-05,
        "epoch": 0.7412884145557412,
        "step": 5765
    },
    {
        "loss": 1.5563,
        "grad_norm": 2.0534844398498535,
        "learning_rate": 3.1364933381229064e-05,
        "epoch": 0.7414169988427414,
        "step": 5766
    },
    {
        "loss": 2.1751,
        "grad_norm": 1.9278844594955444,
        "learning_rate": 3.133550343962531e-05,
        "epoch": 0.7415455831297415,
        "step": 5767
    },
    {
        "loss": 1.4044,
        "grad_norm": 1.9793413877487183,
        "learning_rate": 3.130608474626495e-05,
        "epoch": 0.7416741674167416,
        "step": 5768
    },
    {
        "loss": 1.5577,
        "grad_norm": 1.9898097515106201,
        "learning_rate": 3.127667730596714e-05,
        "epoch": 0.7418027517037418,
        "step": 5769
    },
    {
        "loss": 1.417,
        "grad_norm": 1.806217074394226,
        "learning_rate": 3.124728112354918e-05,
        "epoch": 0.7419313359907419,
        "step": 5770
    },
    {
        "loss": 1.9829,
        "grad_norm": 3.0400938987731934,
        "learning_rate": 3.12178962038267e-05,
        "epoch": 0.742059920277742,
        "step": 5771
    },
    {
        "loss": 1.7146,
        "grad_norm": 1.7968707084655762,
        "learning_rate": 3.118852255161331e-05,
        "epoch": 0.7421885045647422,
        "step": 5772
    },
    {
        "loss": 1.101,
        "grad_norm": 1.8624149560928345,
        "learning_rate": 3.1159160171720855e-05,
        "epoch": 0.7423170888517423,
        "step": 5773
    },
    {
        "loss": 1.4951,
        "grad_norm": 6.605782985687256,
        "learning_rate": 3.112980906895936e-05,
        "epoch": 0.7424456731387424,
        "step": 5774
    },
    {
        "loss": 2.6629,
        "grad_norm": 1.1843371391296387,
        "learning_rate": 3.11004692481369e-05,
        "epoch": 0.7425742574257426,
        "step": 5775
    },
    {
        "loss": 2.0613,
        "grad_norm": 2.070033073425293,
        "learning_rate": 3.107114071405979e-05,
        "epoch": 0.7427028417127427,
        "step": 5776
    },
    {
        "loss": 2.3726,
        "grad_norm": 1.7247490882873535,
        "learning_rate": 3.104182347153246e-05,
        "epoch": 0.7428314259997428,
        "step": 5777
    },
    {
        "loss": 1.7854,
        "grad_norm": 2.0491983890533447,
        "learning_rate": 3.1012517525357564e-05,
        "epoch": 0.742960010286743,
        "step": 5778
    },
    {
        "loss": 1.9534,
        "grad_norm": 2.161665439605713,
        "learning_rate": 3.098322288033575e-05,
        "epoch": 0.7430885945737431,
        "step": 5779
    },
    {
        "loss": 1.5697,
        "grad_norm": 1.9285848140716553,
        "learning_rate": 3.0953939541265954e-05,
        "epoch": 0.7432171788607432,
        "step": 5780
    },
    {
        "loss": 1.2825,
        "grad_norm": 1.7630376815795898,
        "learning_rate": 3.0924667512945225e-05,
        "epoch": 0.7433457631477434,
        "step": 5781
    },
    {
        "loss": 2.2071,
        "grad_norm": 2.496325969696045,
        "learning_rate": 3.0895406800168714e-05,
        "epoch": 0.7434743474347435,
        "step": 5782
    },
    {
        "loss": 1.2093,
        "grad_norm": 2.194739818572998,
        "learning_rate": 3.086615740772977e-05,
        "epoch": 0.7436029317217436,
        "step": 5783
    },
    {
        "loss": 2.2814,
        "grad_norm": 1.698294758796692,
        "learning_rate": 3.083691934041989e-05,
        "epoch": 0.7437315160087438,
        "step": 5784
    },
    {
        "loss": 0.4168,
        "grad_norm": 1.425400733947754,
        "learning_rate": 3.080769260302864e-05,
        "epoch": 0.7438601002957439,
        "step": 5785
    },
    {
        "loss": 1.3103,
        "grad_norm": 2.2965102195739746,
        "learning_rate": 3.077847720034381e-05,
        "epoch": 0.743988684582744,
        "step": 5786
    },
    {
        "loss": 2.2113,
        "grad_norm": 2.2537944316864014,
        "learning_rate": 3.0749273137151324e-05,
        "epoch": 0.7441172688697442,
        "step": 5787
    },
    {
        "loss": 1.9911,
        "grad_norm": 3.0861079692840576,
        "learning_rate": 3.072008041823525e-05,
        "epoch": 0.7442458531567443,
        "step": 5788
    },
    {
        "loss": 1.6401,
        "grad_norm": 2.024301528930664,
        "learning_rate": 3.0690899048377716e-05,
        "epoch": 0.7443744374437443,
        "step": 5789
    },
    {
        "loss": 2.0295,
        "grad_norm": 1.5326734781265259,
        "learning_rate": 3.0661729032359096e-05,
        "epoch": 0.7445030217307445,
        "step": 5790
    },
    {
        "loss": 1.8237,
        "grad_norm": 2.5687501430511475,
        "learning_rate": 3.063257037495789e-05,
        "epoch": 0.7446316060177446,
        "step": 5791
    },
    {
        "loss": 1.4859,
        "grad_norm": 1.7700953483581543,
        "learning_rate": 3.060342308095064e-05,
        "epoch": 0.7447601903047447,
        "step": 5792
    },
    {
        "loss": 1.3636,
        "grad_norm": 2.318626642227173,
        "learning_rate": 3.0574287155112144e-05,
        "epoch": 0.7448887745917449,
        "step": 5793
    },
    {
        "loss": 1.4134,
        "grad_norm": 1.9709722995758057,
        "learning_rate": 3.05451626022153e-05,
        "epoch": 0.745017358878745,
        "step": 5794
    },
    {
        "loss": 1.5905,
        "grad_norm": 1.5934826135635376,
        "learning_rate": 3.051604942703109e-05,
        "epoch": 0.7451459431657451,
        "step": 5795
    },
    {
        "loss": 1.6761,
        "grad_norm": 2.3583106994628906,
        "learning_rate": 3.048694763432871e-05,
        "epoch": 0.7452745274527453,
        "step": 5796
    },
    {
        "loss": 2.2998,
        "grad_norm": 2.535806655883789,
        "learning_rate": 3.0457857228875432e-05,
        "epoch": 0.7454031117397454,
        "step": 5797
    },
    {
        "loss": 2.4359,
        "grad_norm": 1.6829824447631836,
        "learning_rate": 3.0428778215436737e-05,
        "epoch": 0.7455316960267455,
        "step": 5798
    },
    {
        "loss": 1.4278,
        "grad_norm": 2.116257667541504,
        "learning_rate": 3.0399710598776132e-05,
        "epoch": 0.7456602803137456,
        "step": 5799
    },
    {
        "loss": 2.0941,
        "grad_norm": 3.058765172958374,
        "learning_rate": 3.0370654383655338e-05,
        "epoch": 0.7457888646007458,
        "step": 5800
    },
    {
        "eval_loss": 1.7582958936691284,
        "eval_runtime": 28.2971,
        "eval_samples_per_second": 2.792,
        "eval_steps_per_second": 2.792,
        "epoch": 0.7457888646007458,
        "step": 5800
    },
    {
        "loss": 1.4666,
        "grad_norm": 1.7991652488708496,
        "learning_rate": 3.034160957483422e-05,
        "epoch": 0.7459174488877459,
        "step": 5801
    },
    {
        "loss": 1.3055,
        "grad_norm": 1.6582527160644531,
        "learning_rate": 3.031257617707066e-05,
        "epoch": 0.746046033174746,
        "step": 5802
    },
    {
        "loss": 0.8867,
        "grad_norm": 1.492837905883789,
        "learning_rate": 3.0283554195120812e-05,
        "epoch": 0.7461746174617462,
        "step": 5803
    },
    {
        "loss": 2.2452,
        "grad_norm": 1.156171202659607,
        "learning_rate": 3.025454363373892e-05,
        "epoch": 0.7463032017487463,
        "step": 5804
    },
    {
        "loss": 1.1917,
        "grad_norm": 2.767271041870117,
        "learning_rate": 3.0225544497677262e-05,
        "epoch": 0.7464317860357464,
        "step": 5805
    },
    {
        "loss": 1.788,
        "grad_norm": 2.1914780139923096,
        "learning_rate": 3.019655679168636e-05,
        "epoch": 0.7465603703227466,
        "step": 5806
    },
    {
        "loss": 1.9373,
        "grad_norm": 1.3505786657333374,
        "learning_rate": 3.0167580520514815e-05,
        "epoch": 0.7466889546097467,
        "step": 5807
    },
    {
        "loss": 1.753,
        "grad_norm": 1.9941731691360474,
        "learning_rate": 3.01386156889094e-05,
        "epoch": 0.7468175388967468,
        "step": 5808
    },
    {
        "loss": 1.2772,
        "grad_norm": 1.6027114391326904,
        "learning_rate": 3.0109662301614917e-05,
        "epoch": 0.746946123183747,
        "step": 5809
    },
    {
        "loss": 1.8216,
        "grad_norm": 2.520224094390869,
        "learning_rate": 3.008072036337436e-05,
        "epoch": 0.7470747074707471,
        "step": 5810
    },
    {
        "loss": 1.1505,
        "grad_norm": 2.173318862915039,
        "learning_rate": 3.0051789878928904e-05,
        "epoch": 0.7472032917577472,
        "step": 5811
    },
    {
        "loss": 1.6609,
        "grad_norm": 3.482877492904663,
        "learning_rate": 3.002287085301766e-05,
        "epoch": 0.7473318760447474,
        "step": 5812
    },
    {
        "loss": 1.5919,
        "grad_norm": 2.725770950317383,
        "learning_rate": 2.9993963290378134e-05,
        "epoch": 0.7474604603317475,
        "step": 5813
    },
    {
        "loss": 1.4967,
        "grad_norm": 1.6246358156204224,
        "learning_rate": 2.99650671957457e-05,
        "epoch": 0.7475890446187475,
        "step": 5814
    },
    {
        "loss": 1.3233,
        "grad_norm": 2.375553846359253,
        "learning_rate": 2.9936182573853977e-05,
        "epoch": 0.7477176289057477,
        "step": 5815
    },
    {
        "loss": 2.3577,
        "grad_norm": 1.3789455890655518,
        "learning_rate": 2.990730942943475e-05,
        "epoch": 0.7478462131927478,
        "step": 5816
    },
    {
        "loss": 1.5464,
        "grad_norm": 2.4505653381347656,
        "learning_rate": 2.9878447767217765e-05,
        "epoch": 0.7479747974797479,
        "step": 5817
    },
    {
        "loss": 1.5206,
        "grad_norm": 2.313955068588257,
        "learning_rate": 2.9849597591931023e-05,
        "epoch": 0.7481033817667481,
        "step": 5818
    },
    {
        "loss": 1.3747,
        "grad_norm": 1.4860948324203491,
        "learning_rate": 2.982075890830062e-05,
        "epoch": 0.7482319660537482,
        "step": 5819
    },
    {
        "loss": 1.6924,
        "grad_norm": 2.2984652519226074,
        "learning_rate": 2.979193172105076e-05,
        "epoch": 0.7483605503407483,
        "step": 5820
    },
    {
        "loss": 2.3119,
        "grad_norm": 1.7507113218307495,
        "learning_rate": 2.9763116034903727e-05,
        "epoch": 0.7484891346277485,
        "step": 5821
    },
    {
        "loss": 1.2458,
        "grad_norm": 4.30928897857666,
        "learning_rate": 2.973431185457991e-05,
        "epoch": 0.7486177189147486,
        "step": 5822
    },
    {
        "loss": 0.7604,
        "grad_norm": 1.880797266960144,
        "learning_rate": 2.9705519184797948e-05,
        "epoch": 0.7487463032017487,
        "step": 5823
    },
    {
        "loss": 2.2761,
        "grad_norm": 1.215340495109558,
        "learning_rate": 2.9676738030274422e-05,
        "epoch": 0.7488748874887489,
        "step": 5824
    },
    {
        "loss": 2.0538,
        "grad_norm": 1.9591106176376343,
        "learning_rate": 2.9647968395724134e-05,
        "epoch": 0.749003471775749,
        "step": 5825
    },
    {
        "loss": 2.0677,
        "grad_norm": 2.3924779891967773,
        "learning_rate": 2.961921028586e-05,
        "epoch": 0.7491320560627491,
        "step": 5826
    },
    {
        "loss": 2.2586,
        "grad_norm": 1.735027551651001,
        "learning_rate": 2.9590463705392944e-05,
        "epoch": 0.7492606403497493,
        "step": 5827
    },
    {
        "loss": 1.6953,
        "grad_norm": 3.2671566009521484,
        "learning_rate": 2.9561728659032116e-05,
        "epoch": 0.7493892246367494,
        "step": 5828
    },
    {
        "loss": 1.7928,
        "grad_norm": 2.1930840015411377,
        "learning_rate": 2.9533005151484738e-05,
        "epoch": 0.7495178089237495,
        "step": 5829
    },
    {
        "loss": 1.5627,
        "grad_norm": 2.292555570602417,
        "learning_rate": 2.9504293187456156e-05,
        "epoch": 0.7496463932107497,
        "step": 5830
    },
    {
        "loss": 1.3886,
        "grad_norm": 1.8639267683029175,
        "learning_rate": 2.947559277164975e-05,
        "epoch": 0.7497749774977498,
        "step": 5831
    },
    {
        "loss": 1.9424,
        "grad_norm": 1.650415301322937,
        "learning_rate": 2.94469039087671e-05,
        "epoch": 0.7499035617847499,
        "step": 5832
    },
    {
        "loss": 2.4892,
        "grad_norm": 1.1507744789123535,
        "learning_rate": 2.9418226603507892e-05,
        "epoch": 0.7500321460717501,
        "step": 5833
    },
    {
        "loss": 1.3005,
        "grad_norm": 2.39367938041687,
        "learning_rate": 2.938956086056982e-05,
        "epoch": 0.7501607303587502,
        "step": 5834
    },
    {
        "loss": 1.7914,
        "grad_norm": 1.6530184745788574,
        "learning_rate": 2.936090668464878e-05,
        "epoch": 0.7502893146457503,
        "step": 5835
    },
    {
        "loss": 2.2453,
        "grad_norm": 1.691323161125183,
        "learning_rate": 2.9332264080438777e-05,
        "epoch": 0.7504178989327505,
        "step": 5836
    },
    {
        "loss": 1.5956,
        "grad_norm": 1.9628781080245972,
        "learning_rate": 2.9303633052631817e-05,
        "epoch": 0.7505464832197506,
        "step": 5837
    },
    {
        "loss": 2.1351,
        "grad_norm": 1.759775161743164,
        "learning_rate": 2.9275013605918123e-05,
        "epoch": 0.7506750675067507,
        "step": 5838
    },
    {
        "loss": 1.3741,
        "grad_norm": 3.544419050216675,
        "learning_rate": 2.924640574498597e-05,
        "epoch": 0.7508036517937509,
        "step": 5839
    },
    {
        "loss": 2.1923,
        "grad_norm": 2.2342700958251953,
        "learning_rate": 2.9217809474521763e-05,
        "epoch": 0.7509322360807509,
        "step": 5840
    },
    {
        "loss": 2.1299,
        "grad_norm": 2.6633975505828857,
        "learning_rate": 2.9189224799209935e-05,
        "epoch": 0.751060820367751,
        "step": 5841
    },
    {
        "loss": 2.0798,
        "grad_norm": 2.748206853866577,
        "learning_rate": 2.9160651723733112e-05,
        "epoch": 0.7511894046547511,
        "step": 5842
    },
    {
        "loss": 1.8103,
        "grad_norm": 1.433544397354126,
        "learning_rate": 2.9132090252771993e-05,
        "epoch": 0.7513179889417513,
        "step": 5843
    },
    {
        "loss": 1.4125,
        "grad_norm": 1.3047080039978027,
        "learning_rate": 2.9103540391005302e-05,
        "epoch": 0.7514465732287514,
        "step": 5844
    },
    {
        "loss": 2.2493,
        "grad_norm": 1.7075148820877075,
        "learning_rate": 2.9075002143109964e-05,
        "epoch": 0.7515751575157515,
        "step": 5845
    },
    {
        "loss": 1.0216,
        "grad_norm": 2.0792324542999268,
        "learning_rate": 2.9046475513760972e-05,
        "epoch": 0.7517037418027517,
        "step": 5846
    },
    {
        "loss": 1.3826,
        "grad_norm": 2.574962854385376,
        "learning_rate": 2.901796050763137e-05,
        "epoch": 0.7518323260897518,
        "step": 5847
    },
    {
        "loss": 1.4021,
        "grad_norm": 2.497830629348755,
        "learning_rate": 2.898945712939233e-05,
        "epoch": 0.751960910376752,
        "step": 5848
    },
    {
        "loss": 1.8469,
        "grad_norm": 2.2156453132629395,
        "learning_rate": 2.8960965383713146e-05,
        "epoch": 0.7520894946637521,
        "step": 5849
    },
    {
        "loss": 1.933,
        "grad_norm": 1.8258014917373657,
        "learning_rate": 2.8932485275261188e-05,
        "epoch": 0.7522180789507522,
        "step": 5850
    },
    {
        "loss": 2.4136,
        "grad_norm": 1.198840856552124,
        "learning_rate": 2.8904016808701874e-05,
        "epoch": 0.7523466632377523,
        "step": 5851
    },
    {
        "loss": 1.6632,
        "grad_norm": 2.362490653991699,
        "learning_rate": 2.8875559988698763e-05,
        "epoch": 0.7524752475247525,
        "step": 5852
    },
    {
        "loss": 2.009,
        "grad_norm": 1.6482776403427124,
        "learning_rate": 2.8847114819913536e-05,
        "epoch": 0.7526038318117526,
        "step": 5853
    },
    {
        "loss": 1.8114,
        "grad_norm": 1.5421745777130127,
        "learning_rate": 2.8818681307005835e-05,
        "epoch": 0.7527324160987527,
        "step": 5854
    },
    {
        "loss": 1.1459,
        "grad_norm": 1.5727968215942383,
        "learning_rate": 2.8790259454633607e-05,
        "epoch": 0.7528610003857529,
        "step": 5855
    },
    {
        "loss": 1.8316,
        "grad_norm": 1.7144101858139038,
        "learning_rate": 2.876184926745269e-05,
        "epoch": 0.752989584672753,
        "step": 5856
    },
    {
        "loss": 2.3984,
        "grad_norm": 1.8310304880142212,
        "learning_rate": 2.8733450750117076e-05,
        "epoch": 0.7531181689597531,
        "step": 5857
    },
    {
        "loss": 1.0373,
        "grad_norm": 2.1919467449188232,
        "learning_rate": 2.8705063907278873e-05,
        "epoch": 0.7532467532467533,
        "step": 5858
    },
    {
        "loss": 1.5058,
        "grad_norm": 1.5105810165405273,
        "learning_rate": 2.8676688743588252e-05,
        "epoch": 0.7533753375337534,
        "step": 5859
    },
    {
        "loss": 1.7781,
        "grad_norm": 1.9396158456802368,
        "learning_rate": 2.8648325263693533e-05,
        "epoch": 0.7535039218207535,
        "step": 5860
    },
    {
        "loss": 1.5861,
        "grad_norm": 2.3422417640686035,
        "learning_rate": 2.861997347224098e-05,
        "epoch": 0.7536325061077537,
        "step": 5861
    },
    {
        "loss": 1.8047,
        "grad_norm": 1.6548932790756226,
        "learning_rate": 2.859163337387507e-05,
        "epoch": 0.7537610903947538,
        "step": 5862
    },
    {
        "loss": 1.8409,
        "grad_norm": 1.7614798545837402,
        "learning_rate": 2.8563304973238348e-05,
        "epoch": 0.7538896746817539,
        "step": 5863
    },
    {
        "loss": 2.1119,
        "grad_norm": 2.170645236968994,
        "learning_rate": 2.853498827497133e-05,
        "epoch": 0.7540182589687541,
        "step": 5864
    },
    {
        "loss": 1.6635,
        "grad_norm": 2.3995485305786133,
        "learning_rate": 2.850668328371283e-05,
        "epoch": 0.7541468432557541,
        "step": 5865
    },
    {
        "loss": 2.0451,
        "grad_norm": 1.6557716131210327,
        "learning_rate": 2.8478390004099532e-05,
        "epoch": 0.7542754275427542,
        "step": 5866
    },
    {
        "loss": 2.1455,
        "grad_norm": 1.3948765993118286,
        "learning_rate": 2.84501084407663e-05,
        "epoch": 0.7544040118297544,
        "step": 5867
    },
    {
        "loss": 2.1378,
        "grad_norm": 2.1607134342193604,
        "learning_rate": 2.8421838598346107e-05,
        "epoch": 0.7545325961167545,
        "step": 5868
    },
    {
        "loss": 2.1171,
        "grad_norm": 2.889047145843506,
        "learning_rate": 2.8393580481469896e-05,
        "epoch": 0.7546611804037546,
        "step": 5869
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.4394309520721436,
        "learning_rate": 2.8365334094766782e-05,
        "epoch": 0.7547897646907548,
        "step": 5870
    },
    {
        "loss": 2.0208,
        "grad_norm": 2.1195220947265625,
        "learning_rate": 2.8337099442863957e-05,
        "epoch": 0.7549183489777549,
        "step": 5871
    },
    {
        "loss": 2.1018,
        "grad_norm": 1.6636155843734741,
        "learning_rate": 2.8308876530386685e-05,
        "epoch": 0.755046933264755,
        "step": 5872
    },
    {
        "loss": 1.5563,
        "grad_norm": 1.560228705406189,
        "learning_rate": 2.8280665361958225e-05,
        "epoch": 0.7551755175517552,
        "step": 5873
    },
    {
        "loss": 1.2682,
        "grad_norm": 2.167365312576294,
        "learning_rate": 2.825246594220001e-05,
        "epoch": 0.7553041018387553,
        "step": 5874
    },
    {
        "loss": 2.3172,
        "grad_norm": 2.0858073234558105,
        "learning_rate": 2.8224278275731552e-05,
        "epoch": 0.7554326861257554,
        "step": 5875
    },
    {
        "loss": 2.4936,
        "grad_norm": 1.3894848823547363,
        "learning_rate": 2.819610236717033e-05,
        "epoch": 0.7555612704127556,
        "step": 5876
    },
    {
        "loss": 1.8308,
        "grad_norm": 2.1178274154663086,
        "learning_rate": 2.8167938221132007e-05,
        "epoch": 0.7556898546997557,
        "step": 5877
    },
    {
        "loss": 1.9716,
        "grad_norm": 1.5638706684112549,
        "learning_rate": 2.8139785842230304e-05,
        "epoch": 0.7558184389867558,
        "step": 5878
    },
    {
        "loss": 1.9477,
        "grad_norm": 1.8218320608139038,
        "learning_rate": 2.8111645235076922e-05,
        "epoch": 0.755947023273756,
        "step": 5879
    },
    {
        "loss": 0.824,
        "grad_norm": 2.5752854347229004,
        "learning_rate": 2.808351640428175e-05,
        "epoch": 0.7560756075607561,
        "step": 5880
    },
    {
        "loss": 1.7491,
        "grad_norm": 1.6411678791046143,
        "learning_rate": 2.8055399354452693e-05,
        "epoch": 0.7562041918477562,
        "step": 5881
    },
    {
        "loss": 1.5219,
        "grad_norm": 1.8598445653915405,
        "learning_rate": 2.8027294090195753e-05,
        "epoch": 0.7563327761347564,
        "step": 5882
    },
    {
        "loss": 2.1699,
        "grad_norm": 1.6607342958450317,
        "learning_rate": 2.799920061611494e-05,
        "epoch": 0.7564613604217565,
        "step": 5883
    },
    {
        "loss": 1.5336,
        "grad_norm": 2.0109429359436035,
        "learning_rate": 2.7971118936812378e-05,
        "epoch": 0.7565899447087566,
        "step": 5884
    },
    {
        "loss": 0.7224,
        "grad_norm": 1.913299560546875,
        "learning_rate": 2.7943049056888316e-05,
        "epoch": 0.7567185289957568,
        "step": 5885
    },
    {
        "loss": 1.6264,
        "grad_norm": 1.6528085470199585,
        "learning_rate": 2.7914990980940913e-05,
        "epoch": 0.7568471132827569,
        "step": 5886
    },
    {
        "loss": 1.4585,
        "grad_norm": 1.866753339767456,
        "learning_rate": 2.7886944713566555e-05,
        "epoch": 0.756975697569757,
        "step": 5887
    },
    {
        "loss": 1.8917,
        "grad_norm": 1.5879210233688354,
        "learning_rate": 2.7858910259359638e-05,
        "epoch": 0.7571042818567572,
        "step": 5888
    },
    {
        "loss": 1.2291,
        "grad_norm": 2.105837821960449,
        "learning_rate": 2.783088762291255e-05,
        "epoch": 0.7572328661437573,
        "step": 5889
    },
    {
        "loss": 0.7862,
        "grad_norm": 2.2617106437683105,
        "learning_rate": 2.7802876808815846e-05,
        "epoch": 0.7573614504307573,
        "step": 5890
    },
    {
        "loss": 1.8013,
        "grad_norm": 2.585597038269043,
        "learning_rate": 2.7774877821658108e-05,
        "epoch": 0.7574900347177574,
        "step": 5891
    },
    {
        "loss": 1.8383,
        "grad_norm": 2.4537603855133057,
        "learning_rate": 2.7746890666026004e-05,
        "epoch": 0.7576186190047576,
        "step": 5892
    },
    {
        "loss": 2.2774,
        "grad_norm": 2.094374656677246,
        "learning_rate": 2.771891534650417e-05,
        "epoch": 0.7577472032917577,
        "step": 5893
    },
    {
        "loss": 1.7705,
        "grad_norm": 2.1674087047576904,
        "learning_rate": 2.7690951867675407e-05,
        "epoch": 0.7578757875787578,
        "step": 5894
    },
    {
        "loss": 2.1645,
        "grad_norm": 2.159454822540283,
        "learning_rate": 2.7663000234120573e-05,
        "epoch": 0.758004371865758,
        "step": 5895
    },
    {
        "loss": 1.5842,
        "grad_norm": 2.2200210094451904,
        "learning_rate": 2.763506045041848e-05,
        "epoch": 0.7581329561527581,
        "step": 5896
    },
    {
        "loss": 1.524,
        "grad_norm": 2.265810966491699,
        "learning_rate": 2.7607132521146117e-05,
        "epoch": 0.7582615404397582,
        "step": 5897
    },
    {
        "loss": 1.1294,
        "grad_norm": 2.365727424621582,
        "learning_rate": 2.757921645087851e-05,
        "epoch": 0.7583901247267584,
        "step": 5898
    },
    {
        "loss": 1.3795,
        "grad_norm": 2.051271438598633,
        "learning_rate": 2.7551312244188643e-05,
        "epoch": 0.7585187090137585,
        "step": 5899
    },
    {
        "loss": 1.8039,
        "grad_norm": 1.8141474723815918,
        "learning_rate": 2.7523419905647675e-05,
        "epoch": 0.7586472933007586,
        "step": 5900
    },
    {
        "eval_loss": 1.7536109685897827,
        "eval_runtime": 28.2492,
        "eval_samples_per_second": 2.797,
        "eval_steps_per_second": 2.797,
        "epoch": 0.7586472933007586,
        "step": 5900
    },
    {
        "loss": 1.8449,
        "grad_norm": 2.2205584049224854,
        "learning_rate": 2.749553943982477e-05,
        "epoch": 0.7587758775877588,
        "step": 5901
    },
    {
        "loss": 1.8016,
        "grad_norm": 3.2391464710235596,
        "learning_rate": 2.7467670851287186e-05,
        "epoch": 0.7589044618747589,
        "step": 5902
    },
    {
        "loss": 2.0061,
        "grad_norm": 1.8989908695220947,
        "learning_rate": 2.743981414460015e-05,
        "epoch": 0.759033046161759,
        "step": 5903
    },
    {
        "loss": 1.4865,
        "grad_norm": 2.5274569988250732,
        "learning_rate": 2.7411969324327025e-05,
        "epoch": 0.7591616304487592,
        "step": 5904
    },
    {
        "loss": 1.7201,
        "grad_norm": 2.3615336418151855,
        "learning_rate": 2.7384136395029213e-05,
        "epoch": 0.7592902147357593,
        "step": 5905
    },
    {
        "loss": 1.0242,
        "grad_norm": 2.5817809104919434,
        "learning_rate": 2.7356315361266083e-05,
        "epoch": 0.7594187990227594,
        "step": 5906
    },
    {
        "loss": 1.9906,
        "grad_norm": 2.0920333862304688,
        "learning_rate": 2.732850622759523e-05,
        "epoch": 0.7595473833097596,
        "step": 5907
    },
    {
        "loss": 1.4233,
        "grad_norm": 2.935068368911743,
        "learning_rate": 2.7300708998572145e-05,
        "epoch": 0.7596759675967597,
        "step": 5908
    },
    {
        "loss": 1.4469,
        "grad_norm": 2.6448814868927,
        "learning_rate": 2.727292367875034e-05,
        "epoch": 0.7598045518837598,
        "step": 5909
    },
    {
        "loss": 1.3419,
        "grad_norm": 2.308573007583618,
        "learning_rate": 2.7245150272681598e-05,
        "epoch": 0.75993313617076,
        "step": 5910
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.8853728771209717,
        "learning_rate": 2.72173887849155e-05,
        "epoch": 0.7600617204577601,
        "step": 5911
    },
    {
        "loss": 1.7362,
        "grad_norm": 2.8968498706817627,
        "learning_rate": 2.7189639219999818e-05,
        "epoch": 0.7601903047447602,
        "step": 5912
    },
    {
        "loss": 1.2094,
        "grad_norm": 1.9894074201583862,
        "learning_rate": 2.7161901582480344e-05,
        "epoch": 0.7603188890317604,
        "step": 5913
    },
    {
        "loss": 1.7478,
        "grad_norm": 1.7425613403320312,
        "learning_rate": 2.7134175876900915e-05,
        "epoch": 0.7604474733187605,
        "step": 5914
    },
    {
        "loss": 1.795,
        "grad_norm": 2.0997567176818848,
        "learning_rate": 2.7106462107803355e-05,
        "epoch": 0.7605760576057605,
        "step": 5915
    },
    {
        "loss": 1.4268,
        "grad_norm": 1.6935901641845703,
        "learning_rate": 2.7078760279727623e-05,
        "epoch": 0.7607046418927607,
        "step": 5916
    },
    {
        "loss": 2.1345,
        "grad_norm": 1.7755638360977173,
        "learning_rate": 2.7051070397211697e-05,
        "epoch": 0.7608332261797608,
        "step": 5917
    },
    {
        "loss": 1.6706,
        "grad_norm": 2.9284698963165283,
        "learning_rate": 2.702339246479153e-05,
        "epoch": 0.7609618104667609,
        "step": 5918
    },
    {
        "loss": 1.8484,
        "grad_norm": 2.2479419708251953,
        "learning_rate": 2.6995726487001194e-05,
        "epoch": 0.7610903947537611,
        "step": 5919
    },
    {
        "loss": 1.2947,
        "grad_norm": 2.0797781944274902,
        "learning_rate": 2.6968072468372818e-05,
        "epoch": 0.7612189790407612,
        "step": 5920
    },
    {
        "loss": 2.1999,
        "grad_norm": 1.5231308937072754,
        "learning_rate": 2.694043041343647e-05,
        "epoch": 0.7613475633277613,
        "step": 5921
    },
    {
        "loss": 1.3447,
        "grad_norm": 1.3917145729064941,
        "learning_rate": 2.6912800326720345e-05,
        "epoch": 0.7614761476147615,
        "step": 5922
    },
    {
        "loss": 1.9243,
        "grad_norm": 1.7280117273330688,
        "learning_rate": 2.6885182212750646e-05,
        "epoch": 0.7616047319017616,
        "step": 5923
    },
    {
        "loss": 1.8738,
        "grad_norm": 2.3964505195617676,
        "learning_rate": 2.6857576076051673e-05,
        "epoch": 0.7617333161887617,
        "step": 5924
    },
    {
        "loss": 1.8259,
        "grad_norm": 1.900852084159851,
        "learning_rate": 2.682998192114564e-05,
        "epoch": 0.7618619004757619,
        "step": 5925
    },
    {
        "loss": 1.3638,
        "grad_norm": 1.5240252017974854,
        "learning_rate": 2.6802399752552897e-05,
        "epoch": 0.761990484762762,
        "step": 5926
    },
    {
        "loss": 1.649,
        "grad_norm": 2.1857869625091553,
        "learning_rate": 2.6774829574791838e-05,
        "epoch": 0.7621190690497621,
        "step": 5927
    },
    {
        "loss": 1.6721,
        "grad_norm": 4.097353935241699,
        "learning_rate": 2.6747271392378815e-05,
        "epoch": 0.7622476533367623,
        "step": 5928
    },
    {
        "loss": 1.2139,
        "grad_norm": 2.717771530151367,
        "learning_rate": 2.671972520982826e-05,
        "epoch": 0.7623762376237624,
        "step": 5929
    },
    {
        "loss": 1.8409,
        "grad_norm": 1.8325839042663574,
        "learning_rate": 2.669219103165269e-05,
        "epoch": 0.7625048219107625,
        "step": 5930
    },
    {
        "loss": 2.3174,
        "grad_norm": 1.8043529987335205,
        "learning_rate": 2.666466886236254e-05,
        "epoch": 0.7626334061977627,
        "step": 5931
    },
    {
        "loss": 2.0519,
        "grad_norm": 2.3179562091827393,
        "learning_rate": 2.6637158706466357e-05,
        "epoch": 0.7627619904847628,
        "step": 5932
    },
    {
        "loss": 2.1802,
        "grad_norm": 1.4956196546554565,
        "learning_rate": 2.660966056847073e-05,
        "epoch": 0.7628905747717629,
        "step": 5933
    },
    {
        "loss": 1.6574,
        "grad_norm": 2.1676719188690186,
        "learning_rate": 2.658217445288026e-05,
        "epoch": 0.763019159058763,
        "step": 5934
    },
    {
        "loss": 2.1984,
        "grad_norm": 1.6466476917266846,
        "learning_rate": 2.655470036419754e-05,
        "epoch": 0.7631477433457632,
        "step": 5935
    },
    {
        "loss": 1.4937,
        "grad_norm": 2.8070216178894043,
        "learning_rate": 2.6527238306923218e-05,
        "epoch": 0.7632763276327633,
        "step": 5936
    },
    {
        "loss": 1.4811,
        "grad_norm": 1.8868855237960815,
        "learning_rate": 2.6499788285556037e-05,
        "epoch": 0.7634049119197635,
        "step": 5937
    },
    {
        "loss": 1.654,
        "grad_norm": 1.7884140014648438,
        "learning_rate": 2.6472350304592642e-05,
        "epoch": 0.7635334962067636,
        "step": 5938
    },
    {
        "loss": 1.9102,
        "grad_norm": 2.1396796703338623,
        "learning_rate": 2.6444924368527813e-05,
        "epoch": 0.7636620804937637,
        "step": 5939
    },
    {
        "loss": 1.4528,
        "grad_norm": 2.7555673122406006,
        "learning_rate": 2.6417510481854323e-05,
        "epoch": 0.7637906647807637,
        "step": 5940
    },
    {
        "loss": 1.9993,
        "grad_norm": 2.025324821472168,
        "learning_rate": 2.6390108649062928e-05,
        "epoch": 0.7639192490677639,
        "step": 5941
    },
    {
        "loss": 1.635,
        "grad_norm": 2.1070878505706787,
        "learning_rate": 2.6362718874642467e-05,
        "epoch": 0.764047833354764,
        "step": 5942
    },
    {
        "loss": 1.3672,
        "grad_norm": 1.873566746711731,
        "learning_rate": 2.6335341163079786e-05,
        "epoch": 0.7641764176417641,
        "step": 5943
    },
    {
        "loss": 0.2316,
        "grad_norm": 0.9958951473236084,
        "learning_rate": 2.6307975518859774e-05,
        "epoch": 0.7643050019287643,
        "step": 5944
    },
    {
        "loss": 1.2969,
        "grad_norm": 2.1668548583984375,
        "learning_rate": 2.628062194646528e-05,
        "epoch": 0.7644335862157644,
        "step": 5945
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.8462066650390625,
        "learning_rate": 2.6253280450377227e-05,
        "epoch": 0.7645621705027645,
        "step": 5946
    },
    {
        "loss": 1.7685,
        "grad_norm": 2.306459665298462,
        "learning_rate": 2.6225951035074593e-05,
        "epoch": 0.7646907547897647,
        "step": 5947
    },
    {
        "loss": 1.7351,
        "grad_norm": 1.609222650527954,
        "learning_rate": 2.6198633705034247e-05,
        "epoch": 0.7648193390767648,
        "step": 5948
    },
    {
        "loss": 2.0807,
        "grad_norm": 1.7735484838485718,
        "learning_rate": 2.6171328464731272e-05,
        "epoch": 0.7649479233637649,
        "step": 5949
    },
    {
        "loss": 2.343,
        "grad_norm": 2.2588841915130615,
        "learning_rate": 2.6144035318638604e-05,
        "epoch": 0.7650765076507651,
        "step": 5950
    },
    {
        "loss": 1.7998,
        "grad_norm": 1.8498772382736206,
        "learning_rate": 2.6116754271227205e-05,
        "epoch": 0.7652050919377652,
        "step": 5951
    },
    {
        "loss": 1.8713,
        "grad_norm": 2.3701012134552,
        "learning_rate": 2.6089485326966224e-05,
        "epoch": 0.7653336762247653,
        "step": 5952
    },
    {
        "loss": 1.8671,
        "grad_norm": 2.1391546726226807,
        "learning_rate": 2.6062228490322625e-05,
        "epoch": 0.7654622605117655,
        "step": 5953
    },
    {
        "loss": 2.225,
        "grad_norm": 2.333317279815674,
        "learning_rate": 2.60349837657615e-05,
        "epoch": 0.7655908447987656,
        "step": 5954
    },
    {
        "loss": 1.44,
        "grad_norm": 2.5867462158203125,
        "learning_rate": 2.6007751157745962e-05,
        "epoch": 0.7657194290857657,
        "step": 5955
    },
    {
        "loss": 2.5734,
        "grad_norm": 1.5918992757797241,
        "learning_rate": 2.5980530670737048e-05,
        "epoch": 0.7658480133727659,
        "step": 5956
    },
    {
        "loss": 1.4129,
        "grad_norm": 2.3583109378814697,
        "learning_rate": 2.595332230919393e-05,
        "epoch": 0.765976597659766,
        "step": 5957
    },
    {
        "loss": 2.4879,
        "grad_norm": 1.4344903230667114,
        "learning_rate": 2.5926126077573653e-05,
        "epoch": 0.7661051819467661,
        "step": 5958
    },
    {
        "loss": 1.382,
        "grad_norm": 2.6575777530670166,
        "learning_rate": 2.589894198033145e-05,
        "epoch": 0.7662337662337663,
        "step": 5959
    },
    {
        "loss": 1.5868,
        "grad_norm": 3.7921316623687744,
        "learning_rate": 2.587177002192044e-05,
        "epoch": 0.7663623505207664,
        "step": 5960
    },
    {
        "loss": 1.9517,
        "grad_norm": 2.335629940032959,
        "learning_rate": 2.5844610206791707e-05,
        "epoch": 0.7664909348077665,
        "step": 5961
    },
    {
        "loss": 2.2556,
        "grad_norm": 1.5250948667526245,
        "learning_rate": 2.5817462539394556e-05,
        "epoch": 0.7666195190947667,
        "step": 5962
    },
    {
        "loss": 2.4865,
        "grad_norm": 1.700028896331787,
        "learning_rate": 2.5790327024176065e-05,
        "epoch": 0.7667481033817668,
        "step": 5963
    },
    {
        "loss": 2.0649,
        "grad_norm": 1.9225707054138184,
        "learning_rate": 2.576320366558146e-05,
        "epoch": 0.7668766876687669,
        "step": 5964
    },
    {
        "loss": 1.6781,
        "grad_norm": 1.9977740049362183,
        "learning_rate": 2.573609246805395e-05,
        "epoch": 0.767005271955767,
        "step": 5965
    },
    {
        "loss": 1.8419,
        "grad_norm": 1.862500786781311,
        "learning_rate": 2.570899343603478e-05,
        "epoch": 0.7671338562427671,
        "step": 5966
    },
    {
        "loss": 1.6915,
        "grad_norm": 2.5140609741210938,
        "learning_rate": 2.568190657396309e-05,
        "epoch": 0.7672624405297672,
        "step": 5967
    },
    {
        "loss": 1.3227,
        "grad_norm": 1.0730496644973755,
        "learning_rate": 2.565483188627613e-05,
        "epoch": 0.7673910248167674,
        "step": 5968
    },
    {
        "loss": 1.664,
        "grad_norm": 2.2069954872131348,
        "learning_rate": 2.562776937740917e-05,
        "epoch": 0.7675196091037675,
        "step": 5969
    },
    {
        "loss": 1.5453,
        "grad_norm": 2.5868561267852783,
        "learning_rate": 2.5600719051795376e-05,
        "epoch": 0.7676481933907676,
        "step": 5970
    },
    {
        "loss": 1.5569,
        "grad_norm": 2.051887035369873,
        "learning_rate": 2.5573680913866016e-05,
        "epoch": 0.7677767776777678,
        "step": 5971
    },
    {
        "loss": 1.7443,
        "grad_norm": 1.5233200788497925,
        "learning_rate": 2.5546654968050365e-05,
        "epoch": 0.7679053619647679,
        "step": 5972
    },
    {
        "loss": 1.4833,
        "grad_norm": 2.7325549125671387,
        "learning_rate": 2.551964121877559e-05,
        "epoch": 0.768033946251768,
        "step": 5973
    },
    {
        "loss": 1.6527,
        "grad_norm": 2.5789527893066406,
        "learning_rate": 2.549263967046699e-05,
        "epoch": 0.7681625305387682,
        "step": 5974
    },
    {
        "loss": 1.6008,
        "grad_norm": 2.150327444076538,
        "learning_rate": 2.5465650327547796e-05,
        "epoch": 0.7682911148257683,
        "step": 5975
    },
    {
        "loss": 1.8767,
        "grad_norm": 2.857240915298462,
        "learning_rate": 2.543867319443929e-05,
        "epoch": 0.7684196991127684,
        "step": 5976
    },
    {
        "loss": 1.2192,
        "grad_norm": 1.3467856645584106,
        "learning_rate": 2.5411708275560642e-05,
        "epoch": 0.7685482833997686,
        "step": 5977
    },
    {
        "loss": 1.8301,
        "grad_norm": 2.0041210651397705,
        "learning_rate": 2.5384755575329157e-05,
        "epoch": 0.7686768676867687,
        "step": 5978
    },
    {
        "loss": 1.7979,
        "grad_norm": 2.032870054244995,
        "learning_rate": 2.5357815098160098e-05,
        "epoch": 0.7688054519737688,
        "step": 5979
    },
    {
        "loss": 2.274,
        "grad_norm": 2.0209310054779053,
        "learning_rate": 2.533088684846663e-05,
        "epoch": 0.768934036260769,
        "step": 5980
    },
    {
        "loss": 1.6015,
        "grad_norm": 2.8838050365448,
        "learning_rate": 2.530397083066005e-05,
        "epoch": 0.7690626205477691,
        "step": 5981
    },
    {
        "loss": 1.4264,
        "grad_norm": 2.3840293884277344,
        "learning_rate": 2.5277067049149604e-05,
        "epoch": 0.7691912048347692,
        "step": 5982
    },
    {
        "loss": 1.8045,
        "grad_norm": 2.1972317695617676,
        "learning_rate": 2.5250175508342465e-05,
        "epoch": 0.7693197891217693,
        "step": 5983
    },
    {
        "loss": 0.7743,
        "grad_norm": 2.5786030292510986,
        "learning_rate": 2.52232962126439e-05,
        "epoch": 0.7694483734087695,
        "step": 5984
    },
    {
        "loss": 1.4262,
        "grad_norm": 2.7621545791625977,
        "learning_rate": 2.5196429166457103e-05,
        "epoch": 0.7695769576957696,
        "step": 5985
    },
    {
        "loss": 1.2634,
        "grad_norm": 2.4217777252197266,
        "learning_rate": 2.5169574374183346e-05,
        "epoch": 0.7697055419827697,
        "step": 5986
    },
    {
        "loss": 1.9795,
        "grad_norm": 2.2921690940856934,
        "learning_rate": 2.5142731840221766e-05,
        "epoch": 0.7698341262697699,
        "step": 5987
    },
    {
        "loss": 1.4991,
        "grad_norm": 2.5615172386169434,
        "learning_rate": 2.511590156896958e-05,
        "epoch": 0.76996271055677,
        "step": 5988
    },
    {
        "loss": 2.3997,
        "grad_norm": 2.1177115440368652,
        "learning_rate": 2.5089083564822024e-05,
        "epoch": 0.7700912948437701,
        "step": 5989
    },
    {
        "loss": 1.2501,
        "grad_norm": 2.246577262878418,
        "learning_rate": 2.5062277832172167e-05,
        "epoch": 0.7702198791307702,
        "step": 5990
    },
    {
        "loss": 1.195,
        "grad_norm": 2.5699033737182617,
        "learning_rate": 2.503548437541131e-05,
        "epoch": 0.7703484634177703,
        "step": 5991
    },
    {
        "loss": 1.4096,
        "grad_norm": 2.5295069217681885,
        "learning_rate": 2.5008703198928552e-05,
        "epoch": 0.7704770477047704,
        "step": 5992
    },
    {
        "loss": 1.163,
        "grad_norm": 2.0476598739624023,
        "learning_rate": 2.4981934307110977e-05,
        "epoch": 0.7706056319917706,
        "step": 5993
    },
    {
        "loss": 1.1454,
        "grad_norm": 3.1896250247955322,
        "learning_rate": 2.4955177704343835e-05,
        "epoch": 0.7707342162787707,
        "step": 5994
    },
    {
        "loss": 1.2402,
        "grad_norm": 1.5419634580612183,
        "learning_rate": 2.4928433395010163e-05,
        "epoch": 0.7708628005657708,
        "step": 5995
    },
    {
        "loss": 1.0455,
        "grad_norm": 2.4595842361450195,
        "learning_rate": 2.4901701383491126e-05,
        "epoch": 0.770991384852771,
        "step": 5996
    },
    {
        "loss": 1.1781,
        "grad_norm": 2.1967318058013916,
        "learning_rate": 2.4874981674165753e-05,
        "epoch": 0.7711199691397711,
        "step": 5997
    },
    {
        "loss": 2.0092,
        "grad_norm": 2.225844144821167,
        "learning_rate": 2.4848274271411155e-05,
        "epoch": 0.7712485534267712,
        "step": 5998
    },
    {
        "loss": 1.5456,
        "grad_norm": 1.8326658010482788,
        "learning_rate": 2.4821579179602415e-05,
        "epoch": 0.7713771377137714,
        "step": 5999
    },
    {
        "loss": 1.1848,
        "grad_norm": 4.368070125579834,
        "learning_rate": 2.47948964031125e-05,
        "epoch": 0.7715057220007715,
        "step": 6000
    },
    {
        "eval_loss": 1.753496527671814,
        "eval_runtime": 28.1176,
        "eval_samples_per_second": 2.81,
        "eval_steps_per_second": 2.81,
        "epoch": 0.7715057220007715,
        "step": 6000
    },
    {
        "loss": 1.5952,
        "grad_norm": 2.1721863746643066,
        "learning_rate": 2.476822594631255e-05,
        "epoch": 0.7716343062877716,
        "step": 6001
    },
    {
        "loss": 1.6265,
        "grad_norm": 2.0677647590637207,
        "learning_rate": 2.474156781357151e-05,
        "epoch": 0.7717628905747718,
        "step": 6002
    },
    {
        "loss": 2.2886,
        "grad_norm": 1.3910411596298218,
        "learning_rate": 2.4714922009256325e-05,
        "epoch": 0.7718914748617719,
        "step": 6003
    },
    {
        "loss": 0.8167,
        "grad_norm": 2.0585575103759766,
        "learning_rate": 2.4688288537732062e-05,
        "epoch": 0.772020059148772,
        "step": 6004
    },
    {
        "loss": 1.9114,
        "grad_norm": 1.971632957458496,
        "learning_rate": 2.46616674033616e-05,
        "epoch": 0.7721486434357722,
        "step": 6005
    },
    {
        "loss": 1.7561,
        "grad_norm": 2.1982367038726807,
        "learning_rate": 2.463505861050589e-05,
        "epoch": 0.7722772277227723,
        "step": 6006
    },
    {
        "loss": 1.6163,
        "grad_norm": 1.514051079750061,
        "learning_rate": 2.460846216352387e-05,
        "epoch": 0.7724058120097724,
        "step": 6007
    },
    {
        "loss": 1.6121,
        "grad_norm": 1.363342046737671,
        "learning_rate": 2.458187806677237e-05,
        "epoch": 0.7725343962967726,
        "step": 6008
    },
    {
        "loss": 1.5733,
        "grad_norm": 4.285159111022949,
        "learning_rate": 2.4555306324606276e-05,
        "epoch": 0.7726629805837727,
        "step": 6009
    },
    {
        "loss": 1.6334,
        "grad_norm": 1.3192894458770752,
        "learning_rate": 2.452874694137842e-05,
        "epoch": 0.7727915648707728,
        "step": 6010
    },
    {
        "loss": 1.0284,
        "grad_norm": 2.14215350151062,
        "learning_rate": 2.450219992143966e-05,
        "epoch": 0.772920149157773,
        "step": 6011
    },
    {
        "loss": 2.0377,
        "grad_norm": 2.0662522315979004,
        "learning_rate": 2.4475665269138713e-05,
        "epoch": 0.7730487334447731,
        "step": 6012
    },
    {
        "loss": 1.4422,
        "grad_norm": 1.501882791519165,
        "learning_rate": 2.444914298882238e-05,
        "epoch": 0.7731773177317732,
        "step": 6013
    },
    {
        "loss": 2.3889,
        "grad_norm": 1.3703845739364624,
        "learning_rate": 2.442263308483542e-05,
        "epoch": 0.7733059020187734,
        "step": 6014
    },
    {
        "loss": 1.6404,
        "grad_norm": 1.7994322776794434,
        "learning_rate": 2.4396135561520483e-05,
        "epoch": 0.7734344863057734,
        "step": 6015
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.7856061458587646,
        "learning_rate": 2.4369650423218282e-05,
        "epoch": 0.7735630705927735,
        "step": 6016
    },
    {
        "loss": 2.3651,
        "grad_norm": 1.6060748100280762,
        "learning_rate": 2.434317767426746e-05,
        "epoch": 0.7736916548797736,
        "step": 6017
    },
    {
        "loss": 1.8385,
        "grad_norm": 1.4095603227615356,
        "learning_rate": 2.4316717319004667e-05,
        "epoch": 0.7738202391667738,
        "step": 6018
    },
    {
        "loss": 1.5011,
        "grad_norm": 2.992730140686035,
        "learning_rate": 2.429026936176444e-05,
        "epoch": 0.7739488234537739,
        "step": 6019
    },
    {
        "loss": 1.5101,
        "grad_norm": 2.0759613513946533,
        "learning_rate": 2.4263833806879367e-05,
        "epoch": 0.774077407740774,
        "step": 6020
    },
    {
        "loss": 1.4261,
        "grad_norm": 2.6113884449005127,
        "learning_rate": 2.4237410658680016e-05,
        "epoch": 0.7742059920277742,
        "step": 6021
    },
    {
        "loss": 1.7806,
        "grad_norm": 2.0651817321777344,
        "learning_rate": 2.4210999921494804e-05,
        "epoch": 0.7743345763147743,
        "step": 6022
    },
    {
        "loss": 1.2197,
        "grad_norm": 2.2799651622772217,
        "learning_rate": 2.4184601599650248e-05,
        "epoch": 0.7744631606017744,
        "step": 6023
    },
    {
        "loss": 1.5935,
        "grad_norm": 2.068204164505005,
        "learning_rate": 2.4158215697470777e-05,
        "epoch": 0.7745917448887746,
        "step": 6024
    },
    {
        "loss": 2.0307,
        "grad_norm": 1.6796680688858032,
        "learning_rate": 2.4131842219278754e-05,
        "epoch": 0.7747203291757747,
        "step": 6025
    },
    {
        "loss": 1.4307,
        "grad_norm": 2.5567262172698975,
        "learning_rate": 2.410548116939456e-05,
        "epoch": 0.7748489134627748,
        "step": 6026
    },
    {
        "loss": 1.3792,
        "grad_norm": 1.8510386943817139,
        "learning_rate": 2.4079132552136508e-05,
        "epoch": 0.774977497749775,
        "step": 6027
    },
    {
        "loss": 2.1077,
        "grad_norm": 2.2861835956573486,
        "learning_rate": 2.4052796371820918e-05,
        "epoch": 0.7751060820367751,
        "step": 6028
    },
    {
        "loss": 2.1077,
        "grad_norm": 1.7491413354873657,
        "learning_rate": 2.402647263276199e-05,
        "epoch": 0.7752346663237752,
        "step": 6029
    },
    {
        "loss": 2.1472,
        "grad_norm": 1.9456716775894165,
        "learning_rate": 2.4000161339271953e-05,
        "epoch": 0.7753632506107754,
        "step": 6030
    },
    {
        "loss": 2.1484,
        "grad_norm": 1.4984973669052124,
        "learning_rate": 2.3973862495661025e-05,
        "epoch": 0.7754918348977755,
        "step": 6031
    },
    {
        "loss": 1.635,
        "grad_norm": 2.233748435974121,
        "learning_rate": 2.394757610623727e-05,
        "epoch": 0.7756204191847756,
        "step": 6032
    },
    {
        "loss": 2.4996,
        "grad_norm": 2.9356188774108887,
        "learning_rate": 2.3921302175306804e-05,
        "epoch": 0.7757490034717758,
        "step": 6033
    },
    {
        "loss": 1.5697,
        "grad_norm": 2.6165809631347656,
        "learning_rate": 2.389504070717371e-05,
        "epoch": 0.7758775877587759,
        "step": 6034
    },
    {
        "loss": 2.1155,
        "grad_norm": 1.8043410778045654,
        "learning_rate": 2.386879170613995e-05,
        "epoch": 0.776006172045776,
        "step": 6035
    },
    {
        "loss": 2.2483,
        "grad_norm": 1.163179874420166,
        "learning_rate": 2.384255517650552e-05,
        "epoch": 0.7761347563327762,
        "step": 6036
    },
    {
        "loss": 2.0705,
        "grad_norm": 1.5597678422927856,
        "learning_rate": 2.3816331122568336e-05,
        "epoch": 0.7762633406197763,
        "step": 6037
    },
    {
        "loss": 2.5216,
        "grad_norm": 1.4353420734405518,
        "learning_rate": 2.3790119548624314e-05,
        "epoch": 0.7763919249067764,
        "step": 6038
    },
    {
        "loss": 2.6059,
        "grad_norm": 3.4257662296295166,
        "learning_rate": 2.3763920458967238e-05,
        "epoch": 0.7765205091937766,
        "step": 6039
    },
    {
        "loss": 1.1601,
        "grad_norm": 2.526698589324951,
        "learning_rate": 2.373773385788891e-05,
        "epoch": 0.7766490934807766,
        "step": 6040
    },
    {
        "loss": 1.4029,
        "grad_norm": 2.561312437057495,
        "learning_rate": 2.3711559749679134e-05,
        "epoch": 0.7767776777677767,
        "step": 6041
    },
    {
        "loss": 1.7807,
        "grad_norm": 1.935986876487732,
        "learning_rate": 2.3685398138625503e-05,
        "epoch": 0.7769062620547769,
        "step": 6042
    },
    {
        "loss": 1.3793,
        "grad_norm": 2.401261806488037,
        "learning_rate": 2.3659249029013785e-05,
        "epoch": 0.777034846341777,
        "step": 6043
    },
    {
        "loss": 1.1553,
        "grad_norm": 1.78850257396698,
        "learning_rate": 2.3633112425127536e-05,
        "epoch": 0.7771634306287771,
        "step": 6044
    },
    {
        "loss": 2.1286,
        "grad_norm": 2.112661123275757,
        "learning_rate": 2.360698833124826e-05,
        "epoch": 0.7772920149157773,
        "step": 6045
    },
    {
        "loss": 2.0229,
        "grad_norm": 1.93459153175354,
        "learning_rate": 2.3580876751655558e-05,
        "epoch": 0.7774205992027774,
        "step": 6046
    },
    {
        "loss": 2.0743,
        "grad_norm": 1.881284236907959,
        "learning_rate": 2.3554777690626806e-05,
        "epoch": 0.7775491834897775,
        "step": 6047
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.3018361330032349,
        "learning_rate": 2.3528691152437456e-05,
        "epoch": 0.7776777677767777,
        "step": 6048
    },
    {
        "loss": 2.1373,
        "grad_norm": 1.349976897239685,
        "learning_rate": 2.3502617141360862e-05,
        "epoch": 0.7778063520637778,
        "step": 6049
    },
    {
        "loss": 0.9301,
        "grad_norm": 2.7092020511627197,
        "learning_rate": 2.34765556616683e-05,
        "epoch": 0.7779349363507779,
        "step": 6050
    },
    {
        "loss": 1.5877,
        "grad_norm": 2.1695382595062256,
        "learning_rate": 2.345050671762903e-05,
        "epoch": 0.7780635206377781,
        "step": 6051
    },
    {
        "loss": 2.4642,
        "grad_norm": 2.72062087059021,
        "learning_rate": 2.342447031351025e-05,
        "epoch": 0.7781921049247782,
        "step": 6052
    },
    {
        "loss": 2.0252,
        "grad_norm": 1.6153476238250732,
        "learning_rate": 2.3398446453577126e-05,
        "epoch": 0.7783206892117783,
        "step": 6053
    },
    {
        "loss": 1.5841,
        "grad_norm": 1.727771520614624,
        "learning_rate": 2.337243514209273e-05,
        "epoch": 0.7784492734987785,
        "step": 6054
    },
    {
        "loss": 2.1929,
        "grad_norm": 1.821511149406433,
        "learning_rate": 2.3346436383318027e-05,
        "epoch": 0.7785778577857786,
        "step": 6055
    },
    {
        "loss": 2.0108,
        "grad_norm": 2.0677311420440674,
        "learning_rate": 2.332045018151211e-05,
        "epoch": 0.7787064420727787,
        "step": 6056
    },
    {
        "loss": 1.6954,
        "grad_norm": 1.9058903455734253,
        "learning_rate": 2.3294476540931808e-05,
        "epoch": 0.7788350263597789,
        "step": 6057
    },
    {
        "loss": 1.6006,
        "grad_norm": 2.374089002609253,
        "learning_rate": 2.3268515465832007e-05,
        "epoch": 0.778963610646779,
        "step": 6058
    },
    {
        "loss": 2.0883,
        "grad_norm": 2.466768503189087,
        "learning_rate": 2.324256696046554e-05,
        "epoch": 0.7790921949337791,
        "step": 6059
    },
    {
        "loss": 1.7638,
        "grad_norm": 2.2646775245666504,
        "learning_rate": 2.3216631029083092e-05,
        "epoch": 0.7792207792207793,
        "step": 6060
    },
    {
        "loss": 1.4073,
        "grad_norm": 2.5482773780822754,
        "learning_rate": 2.3190707675933377e-05,
        "epoch": 0.7793493635077794,
        "step": 6061
    },
    {
        "loss": 1.9565,
        "grad_norm": 1.4889390468597412,
        "learning_rate": 2.316479690526302e-05,
        "epoch": 0.7794779477947795,
        "step": 6062
    },
    {
        "loss": 1.8778,
        "grad_norm": 1.905719518661499,
        "learning_rate": 2.3138898721316603e-05,
        "epoch": 0.7796065320817797,
        "step": 6063
    },
    {
        "loss": 2.1737,
        "grad_norm": 2.164893865585327,
        "learning_rate": 2.311301312833657e-05,
        "epoch": 0.7797351163687798,
        "step": 6064
    },
    {
        "loss": 0.8515,
        "grad_norm": 3.3034305572509766,
        "learning_rate": 2.308714013056339e-05,
        "epoch": 0.7798637006557798,
        "step": 6065
    },
    {
        "loss": 1.7502,
        "grad_norm": 1.940379023551941,
        "learning_rate": 2.3061279732235464e-05,
        "epoch": 0.77999228494278,
        "step": 6066
    },
    {
        "loss": 1.5069,
        "grad_norm": Infinity,
        "learning_rate": 2.3061279732235464e-05,
        "epoch": 0.7801208692297801,
        "step": 6067
    },
    {
        "loss": 1.9044,
        "grad_norm": 2.551788568496704,
        "learning_rate": 2.3035431937589038e-05,
        "epoch": 0.7802494535167802,
        "step": 6068
    },
    {
        "loss": 0.9426,
        "grad_norm": 2.3850317001342773,
        "learning_rate": 2.3009596750858387e-05,
        "epoch": 0.7803780378037803,
        "step": 6069
    },
    {
        "loss": 2.3036,
        "grad_norm": 1.3020777702331543,
        "learning_rate": 2.298377417627571e-05,
        "epoch": 0.7805066220907805,
        "step": 6070
    },
    {
        "loss": 1.5353,
        "grad_norm": 2.622621536254883,
        "learning_rate": 2.2957964218071117e-05,
        "epoch": 0.7806352063777806,
        "step": 6071
    },
    {
        "loss": 1.7376,
        "grad_norm": 2.5087730884552,
        "learning_rate": 2.2932166880472616e-05,
        "epoch": 0.7807637906647807,
        "step": 6072
    },
    {
        "loss": 0.4551,
        "grad_norm": 1.458419680595398,
        "learning_rate": 2.2906382167706207e-05,
        "epoch": 0.7808923749517809,
        "step": 6073
    },
    {
        "loss": 2.0611,
        "grad_norm": 1.7842228412628174,
        "learning_rate": 2.2880610083995846e-05,
        "epoch": 0.781020959238781,
        "step": 6074
    },
    {
        "loss": 1.9553,
        "grad_norm": 2.702298164367676,
        "learning_rate": 2.2854850633563308e-05,
        "epoch": 0.7811495435257811,
        "step": 6075
    },
    {
        "loss": 2.1536,
        "grad_norm": 2.2459490299224854,
        "learning_rate": 2.2829103820628382e-05,
        "epoch": 0.7812781278127813,
        "step": 6076
    },
    {
        "loss": 1.6004,
        "grad_norm": 2.1649868488311768,
        "learning_rate": 2.280336964940881e-05,
        "epoch": 0.7814067120997814,
        "step": 6077
    },
    {
        "loss": 1.1284,
        "grad_norm": 1.631521463394165,
        "learning_rate": 2.2777648124120178e-05,
        "epoch": 0.7815352963867815,
        "step": 6078
    },
    {
        "loss": 1.2139,
        "grad_norm": 2.442330837249756,
        "learning_rate": 2.275193924897605e-05,
        "epoch": 0.7816638806737817,
        "step": 6079
    },
    {
        "loss": 2.115,
        "grad_norm": 2.707148313522339,
        "learning_rate": 2.2726243028187932e-05,
        "epoch": 0.7817924649607818,
        "step": 6080
    },
    {
        "loss": 1.4067,
        "grad_norm": 2.817192792892456,
        "learning_rate": 2.2700559465965254e-05,
        "epoch": 0.7819210492477819,
        "step": 6081
    },
    {
        "loss": 1.3173,
        "grad_norm": 1.8781132698059082,
        "learning_rate": 2.267488856651532e-05,
        "epoch": 0.7820496335347821,
        "step": 6082
    },
    {
        "loss": 2.3006,
        "grad_norm": 1.7570406198501587,
        "learning_rate": 2.2649230334043403e-05,
        "epoch": 0.7821782178217822,
        "step": 6083
    },
    {
        "loss": 1.982,
        "grad_norm": 1.5671637058258057,
        "learning_rate": 2.2623584772752736e-05,
        "epoch": 0.7823068021087823,
        "step": 6084
    },
    {
        "loss": 1.7963,
        "grad_norm": 2.531919479370117,
        "learning_rate": 2.259795188684435e-05,
        "epoch": 0.7824353863957825,
        "step": 6085
    },
    {
        "loss": 2.379,
        "grad_norm": 3.5525951385498047,
        "learning_rate": 2.257233168051739e-05,
        "epoch": 0.7825639706827826,
        "step": 6086
    },
    {
        "loss": 1.1768,
        "grad_norm": 1.7336102724075317,
        "learning_rate": 2.254672415796877e-05,
        "epoch": 0.7826925549697827,
        "step": 6087
    },
    {
        "loss": 2.007,
        "grad_norm": 2.0692086219787598,
        "learning_rate": 2.252112932339332e-05,
        "epoch": 0.7828211392567829,
        "step": 6088
    },
    {
        "loss": 1.1776,
        "grad_norm": 1.6212435960769653,
        "learning_rate": 2.2495547180983944e-05,
        "epoch": 0.782949723543783,
        "step": 6089
    },
    {
        "loss": 1.8198,
        "grad_norm": 1.8547589778900146,
        "learning_rate": 2.24699777349313e-05,
        "epoch": 0.783078307830783,
        "step": 6090
    },
    {
        "loss": 2.3056,
        "grad_norm": 1.8922853469848633,
        "learning_rate": 2.2444420989424054e-05,
        "epoch": 0.7832068921177832,
        "step": 6091
    },
    {
        "loss": 1.7351,
        "grad_norm": 2.469217300415039,
        "learning_rate": 2.2418876948648814e-05,
        "epoch": 0.7833354764047833,
        "step": 6092
    },
    {
        "loss": 1.8348,
        "grad_norm": 2.3879456520080566,
        "learning_rate": 2.2393345616789986e-05,
        "epoch": 0.7834640606917834,
        "step": 6093
    },
    {
        "loss": 2.0263,
        "grad_norm": 2.5672240257263184,
        "learning_rate": 2.236782699803005e-05,
        "epoch": 0.7835926449787836,
        "step": 6094
    },
    {
        "loss": 1.3417,
        "grad_norm": 1.6918864250183105,
        "learning_rate": 2.2342321096549235e-05,
        "epoch": 0.7837212292657837,
        "step": 6095
    },
    {
        "loss": 1.4743,
        "grad_norm": 2.690295457839966,
        "learning_rate": 2.2316827916525895e-05,
        "epoch": 0.7838498135527838,
        "step": 6096
    },
    {
        "loss": 1.3887,
        "grad_norm": 2.5918240547180176,
        "learning_rate": 2.2291347462136124e-05,
        "epoch": 0.783978397839784,
        "step": 6097
    },
    {
        "loss": 1.5997,
        "grad_norm": 2.01947021484375,
        "learning_rate": 2.226587973755394e-05,
        "epoch": 0.7841069821267841,
        "step": 6098
    },
    {
        "loss": 1.7235,
        "grad_norm": 2.5707614421844482,
        "learning_rate": 2.2240424746951428e-05,
        "epoch": 0.7842355664137842,
        "step": 6099
    },
    {
        "loss": 1.095,
        "grad_norm": 1.6710655689239502,
        "learning_rate": 2.2214982494498415e-05,
        "epoch": 0.7843641507007844,
        "step": 6100
    },
    {
        "eval_loss": 1.7497817277908325,
        "eval_runtime": 28.3618,
        "eval_samples_per_second": 2.785,
        "eval_steps_per_second": 2.785,
        "epoch": 0.7843641507007844,
        "step": 6100
    },
    {
        "loss": 2.3702,
        "grad_norm": 2.010263442993164,
        "learning_rate": 2.2189552984362728e-05,
        "epoch": 0.7844927349877845,
        "step": 6101
    },
    {
        "loss": 2.001,
        "grad_norm": 2.3013274669647217,
        "learning_rate": 2.216413622071012e-05,
        "epoch": 0.7846213192747846,
        "step": 6102
    },
    {
        "loss": 1.0899,
        "grad_norm": 1.6854629516601562,
        "learning_rate": 2.2138732207704184e-05,
        "epoch": 0.7847499035617848,
        "step": 6103
    },
    {
        "loss": 1.57,
        "grad_norm": 2.2454850673675537,
        "learning_rate": 2.2113340949506478e-05,
        "epoch": 0.7848784878487849,
        "step": 6104
    },
    {
        "loss": 1.292,
        "grad_norm": 1.7292035818099976,
        "learning_rate": 2.2087962450276466e-05,
        "epoch": 0.785007072135785,
        "step": 6105
    },
    {
        "loss": 1.3337,
        "grad_norm": 1.9772244691848755,
        "learning_rate": 2.2062596714171547e-05,
        "epoch": 0.7851356564227852,
        "step": 6106
    },
    {
        "loss": 2.1324,
        "grad_norm": 2.2684361934661865,
        "learning_rate": 2.2037243745346948e-05,
        "epoch": 0.7852642407097853,
        "step": 6107
    },
    {
        "loss": 1.704,
        "grad_norm": 2.1233694553375244,
        "learning_rate": 2.201190354795586e-05,
        "epoch": 0.7853928249967854,
        "step": 6108
    },
    {
        "loss": 1.691,
        "grad_norm": 2.1340930461883545,
        "learning_rate": 2.1986576126149415e-05,
        "epoch": 0.7855214092837856,
        "step": 6109
    },
    {
        "loss": 1.3522,
        "grad_norm": 2.3369784355163574,
        "learning_rate": 2.1961261484076567e-05,
        "epoch": 0.7856499935707857,
        "step": 6110
    },
    {
        "loss": 1.9417,
        "grad_norm": 1.799085021018982,
        "learning_rate": 2.1935959625884238e-05,
        "epoch": 0.7857785778577858,
        "step": 6111
    },
    {
        "loss": 1.3283,
        "grad_norm": 2.027693510055542,
        "learning_rate": 2.1910670555717285e-05,
        "epoch": 0.785907162144786,
        "step": 6112
    },
    {
        "loss": 1.9728,
        "grad_norm": 2.081857442855835,
        "learning_rate": 2.188539427771834e-05,
        "epoch": 0.7860357464317861,
        "step": 6113
    },
    {
        "loss": 1.8086,
        "grad_norm": 2.2455496788024902,
        "learning_rate": 2.186013079602808e-05,
        "epoch": 0.7861643307187862,
        "step": 6114
    },
    {
        "loss": 1.396,
        "grad_norm": 2.2117369174957275,
        "learning_rate": 2.1834880114785018e-05,
        "epoch": 0.7862929150057862,
        "step": 6115
    },
    {
        "loss": 1.6344,
        "grad_norm": 2.73561954498291,
        "learning_rate": 2.1809642238125617e-05,
        "epoch": 0.7864214992927864,
        "step": 6116
    },
    {
        "loss": 1.6346,
        "grad_norm": 2.242079257965088,
        "learning_rate": 2.1784417170184146e-05,
        "epoch": 0.7865500835797865,
        "step": 6117
    },
    {
        "loss": 1.5204,
        "grad_norm": 2.6733713150024414,
        "learning_rate": 2.175920491509288e-05,
        "epoch": 0.7866786678667866,
        "step": 6118
    },
    {
        "loss": 1.8484,
        "grad_norm": 1.8001735210418701,
        "learning_rate": 2.1734005476981955e-05,
        "epoch": 0.7868072521537868,
        "step": 6119
    },
    {
        "loss": 1.108,
        "grad_norm": 2.9012207984924316,
        "learning_rate": 2.1708818859979375e-05,
        "epoch": 0.7869358364407869,
        "step": 6120
    },
    {
        "loss": 0.8988,
        "grad_norm": 2.544589042663574,
        "learning_rate": 2.168364506821109e-05,
        "epoch": 0.787064420727787,
        "step": 6121
    },
    {
        "loss": 1.865,
        "grad_norm": 2.141796827316284,
        "learning_rate": 2.1658484105800936e-05,
        "epoch": 0.7871930050147872,
        "step": 6122
    },
    {
        "loss": 1.0201,
        "grad_norm": 2.297811508178711,
        "learning_rate": 2.163333597687067e-05,
        "epoch": 0.7873215893017873,
        "step": 6123
    },
    {
        "loss": 1.3258,
        "grad_norm": 2.601419687271118,
        "learning_rate": 2.160820068553987e-05,
        "epoch": 0.7874501735887874,
        "step": 6124
    },
    {
        "loss": 2.1938,
        "grad_norm": 1.8380002975463867,
        "learning_rate": 2.1583078235926092e-05,
        "epoch": 0.7875787578757876,
        "step": 6125
    },
    {
        "loss": 2.403,
        "grad_norm": 2.7633442878723145,
        "learning_rate": 2.1557968632144776e-05,
        "epoch": 0.7877073421627877,
        "step": 6126
    },
    {
        "loss": 2.1867,
        "grad_norm": 2.3347833156585693,
        "learning_rate": 2.1532871878309157e-05,
        "epoch": 0.7878359264497878,
        "step": 6127
    },
    {
        "loss": 1.2161,
        "grad_norm": 1.7349447011947632,
        "learning_rate": 2.150778797853058e-05,
        "epoch": 0.787964510736788,
        "step": 6128
    },
    {
        "loss": 2.3839,
        "grad_norm": 2.4101035594940186,
        "learning_rate": 2.1482716936918068e-05,
        "epoch": 0.7880930950237881,
        "step": 6129
    },
    {
        "loss": 1.6928,
        "grad_norm": 2.162074089050293,
        "learning_rate": 2.1457658757578592e-05,
        "epoch": 0.7882216793107882,
        "step": 6130
    },
    {
        "loss": 1.5379,
        "grad_norm": 2.126246929168701,
        "learning_rate": 2.1432613444617146e-05,
        "epoch": 0.7883502635977884,
        "step": 6131
    },
    {
        "loss": 1.7042,
        "grad_norm": 2.047165870666504,
        "learning_rate": 2.1407581002136433e-05,
        "epoch": 0.7884788478847885,
        "step": 6132
    },
    {
        "loss": 1.4444,
        "grad_norm": 2.21966290473938,
        "learning_rate": 2.138256143423719e-05,
        "epoch": 0.7886074321717886,
        "step": 6133
    },
    {
        "loss": 0.7022,
        "grad_norm": 1.9336127042770386,
        "learning_rate": 2.1357554745017927e-05,
        "epoch": 0.7887360164587888,
        "step": 6134
    },
    {
        "loss": 1.7811,
        "grad_norm": 1.4147013425827026,
        "learning_rate": 2.1332560938575142e-05,
        "epoch": 0.7888646007457889,
        "step": 6135
    },
    {
        "loss": 2.0039,
        "grad_norm": 1.6335065364837646,
        "learning_rate": 2.1307580019003192e-05,
        "epoch": 0.788993185032789,
        "step": 6136
    },
    {
        "loss": 2.3331,
        "grad_norm": 2.0075998306274414,
        "learning_rate": 2.128261199039424e-05,
        "epoch": 0.7891217693197892,
        "step": 6137
    },
    {
        "loss": 1.9989,
        "grad_norm": 1.7767736911773682,
        "learning_rate": 2.1257656856838527e-05,
        "epoch": 0.7892503536067893,
        "step": 6138
    },
    {
        "loss": 2.4662,
        "grad_norm": 1.6477861404418945,
        "learning_rate": 2.123271462242402e-05,
        "epoch": 0.7893789378937894,
        "step": 6139
    },
    {
        "loss": 1.4949,
        "grad_norm": 2.281034231185913,
        "learning_rate": 2.1207785291236547e-05,
        "epoch": 0.7895075221807895,
        "step": 6140
    },
    {
        "loss": 1.919,
        "grad_norm": 1.8951387405395508,
        "learning_rate": 2.1182868867360006e-05,
        "epoch": 0.7896361064677896,
        "step": 6141
    },
    {
        "loss": 1.6696,
        "grad_norm": 2.0014185905456543,
        "learning_rate": 2.1157965354876007e-05,
        "epoch": 0.7897646907547897,
        "step": 6142
    },
    {
        "loss": 2.3366,
        "grad_norm": 2.0028302669525146,
        "learning_rate": 2.1133074757864113e-05,
        "epoch": 0.7898932750417899,
        "step": 6143
    },
    {
        "loss": 2.0483,
        "grad_norm": 1.818671464920044,
        "learning_rate": 2.1108197080401815e-05,
        "epoch": 0.79002185932879,
        "step": 6144
    },
    {
        "loss": 2.1006,
        "grad_norm": 2.3144114017486572,
        "learning_rate": 2.1083332326564353e-05,
        "epoch": 0.7901504436157901,
        "step": 6145
    },
    {
        "loss": 1.7737,
        "grad_norm": 2.6643226146698,
        "learning_rate": 2.1058480500424992e-05,
        "epoch": 0.7902790279027903,
        "step": 6146
    },
    {
        "loss": 1.8419,
        "grad_norm": 1.9398208856582642,
        "learning_rate": 2.103364160605481e-05,
        "epoch": 0.7904076121897904,
        "step": 6147
    },
    {
        "loss": 1.9755,
        "grad_norm": 1.7835131883621216,
        "learning_rate": 2.1008815647522816e-05,
        "epoch": 0.7905361964767905,
        "step": 6148
    },
    {
        "loss": 1.2538,
        "grad_norm": 2.721890449523926,
        "learning_rate": 2.0984002628895793e-05,
        "epoch": 0.7906647807637907,
        "step": 6149
    },
    {
        "loss": 1.8151,
        "grad_norm": 1.8553279638290405,
        "learning_rate": 2.095920255423851e-05,
        "epoch": 0.7907933650507908,
        "step": 6150
    },
    {
        "loss": 1.4882,
        "grad_norm": 1.946792483329773,
        "learning_rate": 2.0934415427613597e-05,
        "epoch": 0.7909219493377909,
        "step": 6151
    },
    {
        "loss": 1.2758,
        "grad_norm": 1.7418588399887085,
        "learning_rate": 2.0909641253081514e-05,
        "epoch": 0.791050533624791,
        "step": 6152
    },
    {
        "loss": 1.84,
        "grad_norm": 1.7145459651947021,
        "learning_rate": 2.088488003470064e-05,
        "epoch": 0.7911791179117912,
        "step": 6153
    },
    {
        "loss": 1.5521,
        "grad_norm": 1.6771491765975952,
        "learning_rate": 2.0860131776527258e-05,
        "epoch": 0.7913077021987913,
        "step": 6154
    },
    {
        "loss": 1.5921,
        "grad_norm": 1.8952734470367432,
        "learning_rate": 2.0835396482615432e-05,
        "epoch": 0.7914362864857915,
        "step": 6155
    },
    {
        "loss": 1.6543,
        "grad_norm": 1.5473556518554688,
        "learning_rate": 2.0810674157017197e-05,
        "epoch": 0.7915648707727916,
        "step": 6156
    },
    {
        "loss": 1.2542,
        "grad_norm": 2.6580090522766113,
        "learning_rate": 2.078596480378242e-05,
        "epoch": 0.7916934550597917,
        "step": 6157
    },
    {
        "loss": 2.105,
        "grad_norm": 1.727667212486267,
        "learning_rate": 2.076126842695889e-05,
        "epoch": 0.7918220393467919,
        "step": 6158
    },
    {
        "loss": 2.0344,
        "grad_norm": 1.837677240371704,
        "learning_rate": 2.073658503059217e-05,
        "epoch": 0.791950623633792,
        "step": 6159
    },
    {
        "loss": 2.0174,
        "grad_norm": 1.3622709512710571,
        "learning_rate": 2.0711914618725803e-05,
        "epoch": 0.7920792079207921,
        "step": 6160
    },
    {
        "loss": 1.8236,
        "grad_norm": 1.8248027563095093,
        "learning_rate": 2.0687257195401167e-05,
        "epoch": 0.7922077922077922,
        "step": 6161
    },
    {
        "loss": 1.0557,
        "grad_norm": 3.249189615249634,
        "learning_rate": 2.0662612764657462e-05,
        "epoch": 0.7923363764947924,
        "step": 6162
    },
    {
        "loss": 1.7777,
        "grad_norm": 2.132368803024292,
        "learning_rate": 2.0637981330531832e-05,
        "epoch": 0.7924649607817925,
        "step": 6163
    },
    {
        "loss": 1.5393,
        "grad_norm": 2.0317394733428955,
        "learning_rate": 2.0613362897059263e-05,
        "epoch": 0.7925935450687926,
        "step": 6164
    },
    {
        "loss": 1.8585,
        "grad_norm": 2.115159511566162,
        "learning_rate": 2.0588757468272635e-05,
        "epoch": 0.7927221293557927,
        "step": 6165
    },
    {
        "loss": 1.9752,
        "grad_norm": 1.4967397451400757,
        "learning_rate": 2.0564165048202632e-05,
        "epoch": 0.7928507136427928,
        "step": 6166
    },
    {
        "loss": 1.4599,
        "grad_norm": 1.891045093536377,
        "learning_rate": 2.053958564087787e-05,
        "epoch": 0.7929792979297929,
        "step": 6167
    },
    {
        "loss": 1.936,
        "grad_norm": 2.4639203548431396,
        "learning_rate": 2.051501925032484e-05,
        "epoch": 0.7931078822167931,
        "step": 6168
    },
    {
        "loss": 1.422,
        "grad_norm": 1.9413577318191528,
        "learning_rate": 2.049046588056779e-05,
        "epoch": 0.7932364665037932,
        "step": 6169
    },
    {
        "loss": 1.7819,
        "grad_norm": 2.256300687789917,
        "learning_rate": 2.0465925535629038e-05,
        "epoch": 0.7933650507907933,
        "step": 6170
    },
    {
        "loss": 1.5155,
        "grad_norm": 2.9640634059906006,
        "learning_rate": 2.0441398219528583e-05,
        "epoch": 0.7934936350777935,
        "step": 6171
    },
    {
        "loss": 2.3357,
        "grad_norm": 1.0949904918670654,
        "learning_rate": 2.041688393628435e-05,
        "epoch": 0.7936222193647936,
        "step": 6172
    },
    {
        "loss": 0.8935,
        "grad_norm": 1.8447010517120361,
        "learning_rate": 2.0392382689912126e-05,
        "epoch": 0.7937508036517937,
        "step": 6173
    },
    {
        "loss": 0.9437,
        "grad_norm": 2.019892454147339,
        "learning_rate": 2.036789448442561e-05,
        "epoch": 0.7938793879387939,
        "step": 6174
    },
    {
        "loss": 1.8116,
        "grad_norm": 2.4802985191345215,
        "learning_rate": 2.0343419323836333e-05,
        "epoch": 0.794007972225794,
        "step": 6175
    },
    {
        "loss": 1.6361,
        "grad_norm": 3.4258573055267334,
        "learning_rate": 2.031895721215362e-05,
        "epoch": 0.7941365565127941,
        "step": 6176
    },
    {
        "loss": 1.8246,
        "grad_norm": 1.4185680150985718,
        "learning_rate": 2.0294508153384774e-05,
        "epoch": 0.7942651407997943,
        "step": 6177
    },
    {
        "loss": 2.0276,
        "grad_norm": 4.228812217712402,
        "learning_rate": 2.0270072151534914e-05,
        "epoch": 0.7943937250867944,
        "step": 6178
    },
    {
        "loss": 1.8075,
        "grad_norm": 2.6458656787872314,
        "learning_rate": 2.024564921060693e-05,
        "epoch": 0.7945223093737945,
        "step": 6179
    },
    {
        "loss": 1.4507,
        "grad_norm": 1.4377477169036865,
        "learning_rate": 2.022123933460177e-05,
        "epoch": 0.7946508936607947,
        "step": 6180
    },
    {
        "loss": 2.5902,
        "grad_norm": 2.35385799407959,
        "learning_rate": 2.0196842527518077e-05,
        "epoch": 0.7947794779477948,
        "step": 6181
    },
    {
        "loss": 1.8284,
        "grad_norm": 1.1580463647842407,
        "learning_rate": 2.0172458793352345e-05,
        "epoch": 0.7949080622347949,
        "step": 6182
    },
    {
        "loss": 2.0083,
        "grad_norm": 2.914860486984253,
        "learning_rate": 2.0148088136099087e-05,
        "epoch": 0.7950366465217951,
        "step": 6183
    },
    {
        "loss": 2.1082,
        "grad_norm": 1.766664743423462,
        "learning_rate": 2.01237305597505e-05,
        "epoch": 0.7951652308087952,
        "step": 6184
    },
    {
        "loss": 1.8975,
        "grad_norm": 1.4065672159194946,
        "learning_rate": 2.009938606829672e-05,
        "epoch": 0.7952938150957953,
        "step": 6185
    },
    {
        "loss": 1.3583,
        "grad_norm": 2.864046812057495,
        "learning_rate": 2.007505466572578e-05,
        "epoch": 0.7954223993827955,
        "step": 6186
    },
    {
        "loss": 1.6449,
        "grad_norm": 1.694853663444519,
        "learning_rate": 2.0050736356023448e-05,
        "epoch": 0.7955509836697956,
        "step": 6187
    },
    {
        "loss": 1.472,
        "grad_norm": 2.0840694904327393,
        "learning_rate": 2.002643114317344e-05,
        "epoch": 0.7956795679567957,
        "step": 6188
    },
    {
        "loss": 1.9187,
        "grad_norm": 1.6035417318344116,
        "learning_rate": 2.0002139031157307e-05,
        "epoch": 0.7958081522437959,
        "step": 6189
    },
    {
        "loss": 1.4137,
        "grad_norm": 2.1051535606384277,
        "learning_rate": 1.9977860023954486e-05,
        "epoch": 0.7959367365307959,
        "step": 6190
    },
    {
        "loss": 1.468,
        "grad_norm": 3.175093412399292,
        "learning_rate": 1.9953594125542197e-05,
        "epoch": 0.796065320817796,
        "step": 6191
    },
    {
        "loss": 1.3573,
        "grad_norm": 1.8479524850845337,
        "learning_rate": 1.9929341339895492e-05,
        "epoch": 0.7961939051047962,
        "step": 6192
    },
    {
        "loss": 1.8614,
        "grad_norm": 2.227322816848755,
        "learning_rate": 1.990510167098745e-05,
        "epoch": 0.7963224893917963,
        "step": 6193
    },
    {
        "loss": 2.0006,
        "grad_norm": 1.9009839296340942,
        "learning_rate": 1.988087512278878e-05,
        "epoch": 0.7964510736787964,
        "step": 6194
    },
    {
        "loss": 1.5573,
        "grad_norm": 2.263160467147827,
        "learning_rate": 1.9856661699268188e-05,
        "epoch": 0.7965796579657966,
        "step": 6195
    },
    {
        "loss": 1.6895,
        "grad_norm": 2.8521459102630615,
        "learning_rate": 1.9832461404392188e-05,
        "epoch": 0.7967082422527967,
        "step": 6196
    },
    {
        "loss": 1.3283,
        "grad_norm": 2.1723077297210693,
        "learning_rate": 1.9808274242125124e-05,
        "epoch": 0.7968368265397968,
        "step": 6197
    },
    {
        "loss": 1.3531,
        "grad_norm": 2.1477389335632324,
        "learning_rate": 1.9784100216429203e-05,
        "epoch": 0.796965410826797,
        "step": 6198
    },
    {
        "loss": 1.1548,
        "grad_norm": 2.042851209640503,
        "learning_rate": 1.9759939331264486e-05,
        "epoch": 0.7970939951137971,
        "step": 6199
    },
    {
        "loss": 2.5238,
        "grad_norm": 1.0218524932861328,
        "learning_rate": 1.9735791590588915e-05,
        "epoch": 0.7972225794007972,
        "step": 6200
    },
    {
        "eval_loss": 1.7500513792037964,
        "eval_runtime": 28.1793,
        "eval_samples_per_second": 2.803,
        "eval_steps_per_second": 2.803,
        "epoch": 0.7972225794007972,
        "step": 6200
    },
    {
        "loss": 1.4627,
        "grad_norm": 1.5822193622589111,
        "learning_rate": 1.9711656998358174e-05,
        "epoch": 0.7973511636877973,
        "step": 6201
    },
    {
        "loss": 2.043,
        "grad_norm": 1.9197425842285156,
        "learning_rate": 1.9687535558525905e-05,
        "epoch": 0.7974797479747975,
        "step": 6202
    },
    {
        "loss": 1.8222,
        "grad_norm": 2.432058334350586,
        "learning_rate": 1.9663427275043554e-05,
        "epoch": 0.7976083322617976,
        "step": 6203
    },
    {
        "loss": 1.5067,
        "grad_norm": 2.399132251739502,
        "learning_rate": 1.9639332151860378e-05,
        "epoch": 0.7977369165487977,
        "step": 6204
    },
    {
        "loss": 1.6028,
        "grad_norm": 1.7762430906295776,
        "learning_rate": 1.961525019292352e-05,
        "epoch": 0.7978655008357979,
        "step": 6205
    },
    {
        "loss": 1.5498,
        "grad_norm": 2.047560453414917,
        "learning_rate": 1.9591181402177994e-05,
        "epoch": 0.797994085122798,
        "step": 6206
    },
    {
        "loss": 2.3663,
        "grad_norm": 1.3673467636108398,
        "learning_rate": 1.9567125783566543e-05,
        "epoch": 0.7981226694097981,
        "step": 6207
    },
    {
        "loss": 1.6505,
        "grad_norm": 1.4967535734176636,
        "learning_rate": 1.9543083341029877e-05,
        "epoch": 0.7982512536967983,
        "step": 6208
    },
    {
        "loss": 2.048,
        "grad_norm": 1.953166127204895,
        "learning_rate": 1.9519054078506495e-05,
        "epoch": 0.7983798379837984,
        "step": 6209
    },
    {
        "loss": 1.1401,
        "grad_norm": 2.1664071083068848,
        "learning_rate": 1.9495037999932752e-05,
        "epoch": 0.7985084222707985,
        "step": 6210
    },
    {
        "loss": 2.0744,
        "grad_norm": 2.529578924179077,
        "learning_rate": 1.9471035109242784e-05,
        "epoch": 0.7986370065577987,
        "step": 6211
    },
    {
        "loss": 0.9528,
        "grad_norm": 3.9665822982788086,
        "learning_rate": 1.9447045410368647e-05,
        "epoch": 0.7987655908447988,
        "step": 6212
    },
    {
        "loss": 1.7446,
        "grad_norm": 1.688813328742981,
        "learning_rate": 1.9423068907240228e-05,
        "epoch": 0.798894175131799,
        "step": 6213
    },
    {
        "loss": 2.2577,
        "grad_norm": 1.474391222000122,
        "learning_rate": 1.9399105603785172e-05,
        "epoch": 0.7990227594187991,
        "step": 6214
    },
    {
        "loss": 1.2739,
        "grad_norm": 2.7640106678009033,
        "learning_rate": 1.9375155503929032e-05,
        "epoch": 0.7991513437057991,
        "step": 6215
    },
    {
        "loss": 1.7191,
        "grad_norm": 1.9528027772903442,
        "learning_rate": 1.93512186115952e-05,
        "epoch": 0.7992799279927992,
        "step": 6216
    },
    {
        "loss": 1.8844,
        "grad_norm": 2.1495094299316406,
        "learning_rate": 1.932729493070491e-05,
        "epoch": 0.7994085122797994,
        "step": 6217
    },
    {
        "loss": 1.5609,
        "grad_norm": 2.729379415512085,
        "learning_rate": 1.9303384465177144e-05,
        "epoch": 0.7995370965667995,
        "step": 6218
    },
    {
        "loss": 2.238,
        "grad_norm": 1.3917990922927856,
        "learning_rate": 1.9279487218928827e-05,
        "epoch": 0.7996656808537996,
        "step": 6219
    },
    {
        "loss": 1.7923,
        "grad_norm": 2.094822406768799,
        "learning_rate": 1.9255603195874682e-05,
        "epoch": 0.7997942651407998,
        "step": 6220
    },
    {
        "loss": 1.8407,
        "grad_norm": 3.1293201446533203,
        "learning_rate": 1.9231732399927204e-05,
        "epoch": 0.7999228494277999,
        "step": 6221
    },
    {
        "loss": 1.659,
        "grad_norm": 1.809653401374817,
        "learning_rate": 1.9207874834996865e-05,
        "epoch": 0.8000514337148,
        "step": 6222
    },
    {
        "loss": 2.1853,
        "grad_norm": 1.8963665962219238,
        "learning_rate": 1.9184030504991824e-05,
        "epoch": 0.8001800180018002,
        "step": 6223
    },
    {
        "loss": 0.3145,
        "grad_norm": 1.2644392251968384,
        "learning_rate": 1.9160199413818103e-05,
        "epoch": 0.8003086022888003,
        "step": 6224
    },
    {
        "loss": 2.3008,
        "grad_norm": 1.9496419429779053,
        "learning_rate": 1.913638156537966e-05,
        "epoch": 0.8004371865758004,
        "step": 6225
    },
    {
        "loss": 1.8595,
        "grad_norm": 2.2766826152801514,
        "learning_rate": 1.9112576963578133e-05,
        "epoch": 0.8005657708628006,
        "step": 6226
    },
    {
        "loss": 1.773,
        "grad_norm": 2.13639760017395,
        "learning_rate": 1.90887856123131e-05,
        "epoch": 0.8006943551498007,
        "step": 6227
    },
    {
        "loss": 1.9285,
        "grad_norm": 2.6910085678100586,
        "learning_rate": 1.9065007515481938e-05,
        "epoch": 0.8008229394368008,
        "step": 6228
    },
    {
        "loss": 1.5811,
        "grad_norm": 2.54716420173645,
        "learning_rate": 1.904124267697981e-05,
        "epoch": 0.800951523723801,
        "step": 6229
    },
    {
        "loss": 2.1835,
        "grad_norm": 1.5458345413208008,
        "learning_rate": 1.901749110069978e-05,
        "epoch": 0.8010801080108011,
        "step": 6230
    },
    {
        "loss": 1.7252,
        "grad_norm": 1.5003700256347656,
        "learning_rate": 1.8993752790532637e-05,
        "epoch": 0.8012086922978012,
        "step": 6231
    },
    {
        "loss": 1.6236,
        "grad_norm": 2.8324592113494873,
        "learning_rate": 1.897002775036716e-05,
        "epoch": 0.8013372765848014,
        "step": 6232
    },
    {
        "loss": 1.3011,
        "grad_norm": 1.807604432106018,
        "learning_rate": 1.894631598408979e-05,
        "epoch": 0.8014658608718015,
        "step": 6233
    },
    {
        "loss": 1.8481,
        "grad_norm": 2.36696720123291,
        "learning_rate": 1.892261749558484e-05,
        "epoch": 0.8015944451588016,
        "step": 6234
    },
    {
        "loss": 2.1439,
        "grad_norm": 1.6243910789489746,
        "learning_rate": 1.889893228873454e-05,
        "epoch": 0.8017230294458018,
        "step": 6235
    },
    {
        "loss": 1.3344,
        "grad_norm": 3.0979056358337402,
        "learning_rate": 1.8875260367418802e-05,
        "epoch": 0.8018516137328019,
        "step": 6236
    },
    {
        "loss": 2.2463,
        "grad_norm": 1.621608853340149,
        "learning_rate": 1.8851601735515456e-05,
        "epoch": 0.801980198019802,
        "step": 6237
    },
    {
        "loss": 1.3892,
        "grad_norm": 2.0299928188323975,
        "learning_rate": 1.882795639690017e-05,
        "epoch": 0.8021087823068022,
        "step": 6238
    },
    {
        "loss": 1.6708,
        "grad_norm": 2.338411331176758,
        "learning_rate": 1.8804324355446322e-05,
        "epoch": 0.8022373665938023,
        "step": 6239
    },
    {
        "loss": 1.7819,
        "grad_norm": 2.1517388820648193,
        "learning_rate": 1.878070561502523e-05,
        "epoch": 0.8023659508808023,
        "step": 6240
    },
    {
        "loss": 1.9787,
        "grad_norm": 2.1842761039733887,
        "learning_rate": 1.875710017950597e-05,
        "epoch": 0.8024945351678024,
        "step": 6241
    },
    {
        "loss": 1.5259,
        "grad_norm": 1.3803129196166992,
        "learning_rate": 1.87335080527555e-05,
        "epoch": 0.8026231194548026,
        "step": 6242
    },
    {
        "loss": 1.9792,
        "grad_norm": 2.242521286010742,
        "learning_rate": 1.870992923863848e-05,
        "epoch": 0.8027517037418027,
        "step": 6243
    },
    {
        "loss": 1.624,
        "grad_norm": 2.5642757415771484,
        "learning_rate": 1.8686363741017497e-05,
        "epoch": 0.8028802880288028,
        "step": 6244
    },
    {
        "loss": 1.9121,
        "grad_norm": 1.5606718063354492,
        "learning_rate": 1.8662811563752956e-05,
        "epoch": 0.803008872315803,
        "step": 6245
    },
    {
        "loss": 2.0654,
        "grad_norm": 2.1961379051208496,
        "learning_rate": 1.8639272710702994e-05,
        "epoch": 0.8031374566028031,
        "step": 6246
    },
    {
        "loss": 1.6785,
        "grad_norm": 2.056849479675293,
        "learning_rate": 1.861574718572363e-05,
        "epoch": 0.8032660408898032,
        "step": 6247
    },
    {
        "loss": 1.9285,
        "grad_norm": 2.000595808029175,
        "learning_rate": 1.8592234992668724e-05,
        "epoch": 0.8033946251768034,
        "step": 6248
    },
    {
        "loss": 1.5329,
        "grad_norm": 2.113713026046753,
        "learning_rate": 1.8568736135389864e-05,
        "epoch": 0.8035232094638035,
        "step": 6249
    },
    {
        "loss": 1.1409,
        "grad_norm": 1.5579818487167358,
        "learning_rate": 1.854525061773652e-05,
        "epoch": 0.8036517937508036,
        "step": 6250
    },
    {
        "loss": 1.9366,
        "grad_norm": 2.4443368911743164,
        "learning_rate": 1.852177844355597e-05,
        "epoch": 0.8037803780378038,
        "step": 6251
    },
    {
        "loss": 1.6899,
        "grad_norm": 1.819139003753662,
        "learning_rate": 1.8498319616693326e-05,
        "epoch": 0.8039089623248039,
        "step": 6252
    },
    {
        "loss": 2.3786,
        "grad_norm": 2.2544960975646973,
        "learning_rate": 1.8474874140991426e-05,
        "epoch": 0.804037546611804,
        "step": 6253
    },
    {
        "loss": 1.6695,
        "grad_norm": 1.8889771699905396,
        "learning_rate": 1.8451442020291022e-05,
        "epoch": 0.8041661308988042,
        "step": 6254
    },
    {
        "loss": 1.3666,
        "grad_norm": 2.3583579063415527,
        "learning_rate": 1.8428023258430637e-05,
        "epoch": 0.8042947151858043,
        "step": 6255
    },
    {
        "loss": 0.5516,
        "grad_norm": 1.8964239358901978,
        "learning_rate": 1.8404617859246566e-05,
        "epoch": 0.8044232994728044,
        "step": 6256
    },
    {
        "loss": 1.9219,
        "grad_norm": 1.6416356563568115,
        "learning_rate": 1.838122582657298e-05,
        "epoch": 0.8045518837598046,
        "step": 6257
    },
    {
        "loss": 1.8431,
        "grad_norm": 2.692549228668213,
        "learning_rate": 1.8357847164241858e-05,
        "epoch": 0.8046804680468047,
        "step": 6258
    },
    {
        "loss": 1.3565,
        "grad_norm": 2.6379334926605225,
        "learning_rate": 1.833448187608292e-05,
        "epoch": 0.8048090523338048,
        "step": 6259
    },
    {
        "loss": 1.9739,
        "grad_norm": 1.8461049795150757,
        "learning_rate": 1.8311129965923756e-05,
        "epoch": 0.804937636620805,
        "step": 6260
    },
    {
        "loss": 2.1145,
        "grad_norm": 2.0780539512634277,
        "learning_rate": 1.8287791437589763e-05,
        "epoch": 0.8050662209078051,
        "step": 6261
    },
    {
        "loss": 0.7774,
        "grad_norm": 3.1785967350006104,
        "learning_rate": 1.8264466294904137e-05,
        "epoch": 0.8051948051948052,
        "step": 6262
    },
    {
        "loss": 0.9342,
        "grad_norm": 1.9018633365631104,
        "learning_rate": 1.824115454168781e-05,
        "epoch": 0.8053233894818054,
        "step": 6263
    },
    {
        "loss": 2.2627,
        "grad_norm": 1.4225019216537476,
        "learning_rate": 1.8217856181759694e-05,
        "epoch": 0.8054519737688055,
        "step": 6264
    },
    {
        "loss": 1.9058,
        "grad_norm": 2.2081151008605957,
        "learning_rate": 1.819457121893633e-05,
        "epoch": 0.8055805580558055,
        "step": 6265
    },
    {
        "loss": 1.4144,
        "grad_norm": 2.594067335128784,
        "learning_rate": 1.8171299657032104e-05,
        "epoch": 0.8057091423428057,
        "step": 6266
    },
    {
        "loss": 1.5773,
        "grad_norm": 1.9996227025985718,
        "learning_rate": 1.8148041499859325e-05,
        "epoch": 0.8058377266298058,
        "step": 6267
    },
    {
        "loss": 1.2093,
        "grad_norm": 1.5109498500823975,
        "learning_rate": 1.8124796751227946e-05,
        "epoch": 0.8059663109168059,
        "step": 6268
    },
    {
        "loss": 1.1739,
        "grad_norm": 1.6243689060211182,
        "learning_rate": 1.8101565414945842e-05,
        "epoch": 0.8060948952038061,
        "step": 6269
    },
    {
        "loss": 1.9373,
        "grad_norm": 1.909515619277954,
        "learning_rate": 1.8078347494818604e-05,
        "epoch": 0.8062234794908062,
        "step": 6270
    },
    {
        "loss": 1.494,
        "grad_norm": 1.3017245531082153,
        "learning_rate": 1.8055142994649676e-05,
        "epoch": 0.8063520637778063,
        "step": 6271
    },
    {
        "loss": 0.7706,
        "grad_norm": 1.6749470233917236,
        "learning_rate": 1.803195191824032e-05,
        "epoch": 0.8064806480648065,
        "step": 6272
    },
    {
        "loss": 1.6987,
        "grad_norm": 2.57169508934021,
        "learning_rate": 1.8008774269389506e-05,
        "epoch": 0.8066092323518066,
        "step": 6273
    },
    {
        "loss": 2.2897,
        "grad_norm": 1.3072906732559204,
        "learning_rate": 1.7985610051894165e-05,
        "epoch": 0.8067378166388067,
        "step": 6274
    },
    {
        "loss": 1.8153,
        "grad_norm": 1.880090594291687,
        "learning_rate": 1.796245926954887e-05,
        "epoch": 0.8068664009258069,
        "step": 6275
    },
    {
        "loss": 1.3199,
        "grad_norm": 2.2332491874694824,
        "learning_rate": 1.7939321926146035e-05,
        "epoch": 0.806994985212807,
        "step": 6276
    },
    {
        "loss": 0.7354,
        "grad_norm": 2.2927963733673096,
        "learning_rate": 1.791619802547596e-05,
        "epoch": 0.8071235694998071,
        "step": 6277
    },
    {
        "loss": 1.6919,
        "grad_norm": 2.1920928955078125,
        "learning_rate": 1.7893087571326628e-05,
        "epoch": 0.8072521537868073,
        "step": 6278
    },
    {
        "loss": 1.676,
        "grad_norm": 1.3909927606582642,
        "learning_rate": 1.7869990567483885e-05,
        "epoch": 0.8073807380738074,
        "step": 6279
    },
    {
        "loss": 1.6258,
        "grad_norm": 2.2517051696777344,
        "learning_rate": 1.7846907017731374e-05,
        "epoch": 0.8075093223608075,
        "step": 6280
    },
    {
        "loss": 1.3369,
        "grad_norm": 1.4176676273345947,
        "learning_rate": 1.7823836925850466e-05,
        "epoch": 0.8076379066478077,
        "step": 6281
    },
    {
        "loss": 2.1354,
        "grad_norm": 1.531498908996582,
        "learning_rate": 1.780078029562041e-05,
        "epoch": 0.8077664909348078,
        "step": 6282
    },
    {
        "loss": 2.1819,
        "grad_norm": 1.4192084074020386,
        "learning_rate": 1.7777737130818217e-05,
        "epoch": 0.8078950752218079,
        "step": 6283
    },
    {
        "loss": 1.6298,
        "grad_norm": 1.7520310878753662,
        "learning_rate": 1.775470743521872e-05,
        "epoch": 0.8080236595088081,
        "step": 6284
    },
    {
        "loss": 1.5971,
        "grad_norm": 1.657726526260376,
        "learning_rate": 1.773169121259445e-05,
        "epoch": 0.8081522437958082,
        "step": 6285
    },
    {
        "loss": 2.0864,
        "grad_norm": 2.9394869804382324,
        "learning_rate": 1.770868846671585e-05,
        "epoch": 0.8082808280828083,
        "step": 6286
    },
    {
        "loss": 1.7617,
        "grad_norm": 3.920919895172119,
        "learning_rate": 1.7685699201351112e-05,
        "epoch": 0.8084094123698085,
        "step": 6287
    },
    {
        "loss": 1.7149,
        "grad_norm": 1.6072592735290527,
        "learning_rate": 1.766272342026617e-05,
        "epoch": 0.8085379966568086,
        "step": 6288
    },
    {
        "loss": 1.6859,
        "grad_norm": 1.9834859371185303,
        "learning_rate": 1.7639761127224806e-05,
        "epoch": 0.8086665809438087,
        "step": 6289
    },
    {
        "loss": 1.5954,
        "grad_norm": 1.5606731176376343,
        "learning_rate": 1.761681232598861e-05,
        "epoch": 0.8087951652308087,
        "step": 6290
    },
    {
        "loss": 1.2873,
        "grad_norm": 2.16342830657959,
        "learning_rate": 1.7593877020316883e-05,
        "epoch": 0.8089237495178089,
        "step": 6291
    },
    {
        "loss": 2.6155,
        "grad_norm": 1.8990139961242676,
        "learning_rate": 1.757095521396679e-05,
        "epoch": 0.809052333804809,
        "step": 6292
    },
    {
        "loss": 1.1452,
        "grad_norm": 2.487199544906616,
        "learning_rate": 1.7548046910693238e-05,
        "epoch": 0.8091809180918091,
        "step": 6293
    },
    {
        "loss": 1.5826,
        "grad_norm": 3.0970065593719482,
        "learning_rate": 1.7525152114248978e-05,
        "epoch": 0.8093095023788093,
        "step": 6294
    },
    {
        "loss": 1.7719,
        "grad_norm": 1.8444663286209106,
        "learning_rate": 1.7502270828384458e-05,
        "epoch": 0.8094380866658094,
        "step": 6295
    },
    {
        "loss": 1.7827,
        "grad_norm": 1.914124608039856,
        "learning_rate": 1.7479403056848e-05,
        "epoch": 0.8095666709528095,
        "step": 6296
    },
    {
        "loss": 2.4114,
        "grad_norm": 1.9562727212905884,
        "learning_rate": 1.7456548803385688e-05,
        "epoch": 0.8096952552398097,
        "step": 6297
    },
    {
        "loss": 2.3071,
        "grad_norm": 1.996429681777954,
        "learning_rate": 1.743370807174134e-05,
        "epoch": 0.8098238395268098,
        "step": 6298
    },
    {
        "loss": 1.9116,
        "grad_norm": 2.400972604751587,
        "learning_rate": 1.741088086565662e-05,
        "epoch": 0.8099524238138099,
        "step": 6299
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.1059024333953857,
        "learning_rate": 1.7388067188870983e-05,
        "epoch": 0.8100810081008101,
        "step": 6300
    },
    {
        "eval_loss": 1.7478286027908325,
        "eval_runtime": 28.2077,
        "eval_samples_per_second": 2.801,
        "eval_steps_per_second": 2.801,
        "epoch": 0.8100810081008101,
        "step": 6300
    },
    {
        "loss": 1.8861,
        "grad_norm": 1.7969685792922974,
        "learning_rate": 1.7365267045121592e-05,
        "epoch": 0.8102095923878102,
        "step": 6301
    },
    {
        "loss": 0.8806,
        "grad_norm": 1.4651970863342285,
        "learning_rate": 1.7342480438143473e-05,
        "epoch": 0.8103381766748103,
        "step": 6302
    },
    {
        "loss": 1.5182,
        "grad_norm": 1.9180755615234375,
        "learning_rate": 1.7319707371669392e-05,
        "epoch": 0.8104667609618105,
        "step": 6303
    },
    {
        "loss": 1.8416,
        "grad_norm": 1.9233911037445068,
        "learning_rate": 1.7296947849429934e-05,
        "epoch": 0.8105953452488106,
        "step": 6304
    },
    {
        "loss": 1.3664,
        "grad_norm": 1.9026134014129639,
        "learning_rate": 1.7274201875153405e-05,
        "epoch": 0.8107239295358107,
        "step": 6305
    },
    {
        "loss": 1.3029,
        "grad_norm": 2.5571675300598145,
        "learning_rate": 1.7251469452565937e-05,
        "epoch": 0.8108525138228109,
        "step": 6306
    },
    {
        "loss": 1.1762,
        "grad_norm": 1.4990042448043823,
        "learning_rate": 1.7228750585391463e-05,
        "epoch": 0.810981098109811,
        "step": 6307
    },
    {
        "loss": 2.1902,
        "grad_norm": 1.7785518169403076,
        "learning_rate": 1.7206045277351612e-05,
        "epoch": 0.8111096823968111,
        "step": 6308
    },
    {
        "loss": 1.7863,
        "grad_norm": 2.134579658508301,
        "learning_rate": 1.7183353532165868e-05,
        "epoch": 0.8112382666838113,
        "step": 6309
    },
    {
        "loss": 1.2417,
        "grad_norm": 2.0123589038848877,
        "learning_rate": 1.7160675353551493e-05,
        "epoch": 0.8113668509708114,
        "step": 6310
    },
    {
        "loss": 1.2137,
        "grad_norm": 1.6001428365707397,
        "learning_rate": 1.7138010745223454e-05,
        "epoch": 0.8114954352578115,
        "step": 6311
    },
    {
        "loss": 1.168,
        "grad_norm": 1.803425669670105,
        "learning_rate": 1.7115359710894573e-05,
        "epoch": 0.8116240195448117,
        "step": 6312
    },
    {
        "loss": 1.891,
        "grad_norm": 1.6494771242141724,
        "learning_rate": 1.7092722254275405e-05,
        "epoch": 0.8117526038318118,
        "step": 6313
    },
    {
        "loss": 2.4705,
        "grad_norm": 1.307152509689331,
        "learning_rate": 1.707009837907434e-05,
        "epoch": 0.8118811881188119,
        "step": 6314
    },
    {
        "loss": 1.8354,
        "grad_norm": 1.5604125261306763,
        "learning_rate": 1.704748808899741e-05,
        "epoch": 0.812009772405812,
        "step": 6315
    },
    {
        "loss": 1.4117,
        "grad_norm": 2.407360315322876,
        "learning_rate": 1.702489138774861e-05,
        "epoch": 0.8121383566928121,
        "step": 6316
    },
    {
        "loss": 1.714,
        "grad_norm": 1.5424829721450806,
        "learning_rate": 1.7002308279029556e-05,
        "epoch": 0.8122669409798122,
        "step": 6317
    },
    {
        "loss": 1.7703,
        "grad_norm": 2.7156825065612793,
        "learning_rate": 1.6979738766539654e-05,
        "epoch": 0.8123955252668124,
        "step": 6318
    },
    {
        "loss": 1.7468,
        "grad_norm": 2.1686384677886963,
        "learning_rate": 1.6957182853976193e-05,
        "epoch": 0.8125241095538125,
        "step": 6319
    },
    {
        "loss": 1.7093,
        "grad_norm": 2.025324583053589,
        "learning_rate": 1.693464054503411e-05,
        "epoch": 0.8126526938408126,
        "step": 6320
    },
    {
        "loss": 1.4558,
        "grad_norm": 2.379822254180908,
        "learning_rate": 1.6912111843406176e-05,
        "epoch": 0.8127812781278128,
        "step": 6321
    },
    {
        "loss": 1.644,
        "grad_norm": 1.6297351121902466,
        "learning_rate": 1.6889596752782955e-05,
        "epoch": 0.8129098624148129,
        "step": 6322
    },
    {
        "loss": 1.3119,
        "grad_norm": 2.678236246109009,
        "learning_rate": 1.6867095276852674e-05,
        "epoch": 0.813038446701813,
        "step": 6323
    },
    {
        "loss": 1.5614,
        "grad_norm": 2.2453420162200928,
        "learning_rate": 1.6844607419301452e-05,
        "epoch": 0.8131670309888132,
        "step": 6324
    },
    {
        "loss": 1.5273,
        "grad_norm": 2.6290838718414307,
        "learning_rate": 1.6822133183813105e-05,
        "epoch": 0.8132956152758133,
        "step": 6325
    },
    {
        "loss": 1.7326,
        "grad_norm": 2.0702764987945557,
        "learning_rate": 1.679967257406928e-05,
        "epoch": 0.8134241995628134,
        "step": 6326
    },
    {
        "loss": 1.9469,
        "grad_norm": 2.0672409534454346,
        "learning_rate": 1.6777225593749324e-05,
        "epoch": 0.8135527838498136,
        "step": 6327
    },
    {
        "loss": 1.4123,
        "grad_norm": 2.4279565811157227,
        "learning_rate": 1.6754792246530338e-05,
        "epoch": 0.8136813681368137,
        "step": 6328
    },
    {
        "loss": 1.0943,
        "grad_norm": 2.234539747238159,
        "learning_rate": 1.6732372536087304e-05,
        "epoch": 0.8138099524238138,
        "step": 6329
    },
    {
        "loss": 1.4676,
        "grad_norm": 2.1534383296966553,
        "learning_rate": 1.670996646609284e-05,
        "epoch": 0.813938536710814,
        "step": 6330
    },
    {
        "loss": 1.0271,
        "grad_norm": 1.9492381811141968,
        "learning_rate": 1.6687574040217414e-05,
        "epoch": 0.8140671209978141,
        "step": 6331
    },
    {
        "loss": 2.4094,
        "grad_norm": 1.3881478309631348,
        "learning_rate": 1.666519526212925e-05,
        "epoch": 0.8141957052848142,
        "step": 6332
    },
    {
        "loss": 1.8378,
        "grad_norm": 2.579150915145874,
        "learning_rate": 1.6642830135494268e-05,
        "epoch": 0.8143242895718144,
        "step": 6333
    },
    {
        "loss": 1.4998,
        "grad_norm": 3.0385169982910156,
        "learning_rate": 1.6620478663976237e-05,
        "epoch": 0.8144528738588145,
        "step": 6334
    },
    {
        "loss": 0.8509,
        "grad_norm": 1.3444713354110718,
        "learning_rate": 1.6598140851236632e-05,
        "epoch": 0.8145814581458146,
        "step": 6335
    },
    {
        "loss": 1.4738,
        "grad_norm": 2.736523151397705,
        "learning_rate": 1.6575816700934745e-05,
        "epoch": 0.8147100424328148,
        "step": 6336
    },
    {
        "loss": 1.5162,
        "grad_norm": 2.5689918994903564,
        "learning_rate": 1.6553506216727555e-05,
        "epoch": 0.8148386267198149,
        "step": 6337
    },
    {
        "loss": 2.0035,
        "grad_norm": 1.9114575386047363,
        "learning_rate": 1.6531209402269855e-05,
        "epoch": 0.814967211006815,
        "step": 6338
    },
    {
        "loss": 1.9048,
        "grad_norm": 4.008535861968994,
        "learning_rate": 1.650892626121423e-05,
        "epoch": 0.8150957952938152,
        "step": 6339
    },
    {
        "loss": 1.5155,
        "grad_norm": 2.3353400230407715,
        "learning_rate": 1.648665679721091e-05,
        "epoch": 0.8152243795808152,
        "step": 6340
    },
    {
        "loss": 1.9437,
        "grad_norm": 1.6864198446273804,
        "learning_rate": 1.6464401013908004e-05,
        "epoch": 0.8153529638678153,
        "step": 6341
    },
    {
        "loss": 2.131,
        "grad_norm": 1.3982208967208862,
        "learning_rate": 1.6442158914951354e-05,
        "epoch": 0.8154815481548154,
        "step": 6342
    },
    {
        "loss": 2.3994,
        "grad_norm": 1.6701372861862183,
        "learning_rate": 1.6419930503984472e-05,
        "epoch": 0.8156101324418156,
        "step": 6343
    },
    {
        "loss": 1.7758,
        "grad_norm": 2.549584150314331,
        "learning_rate": 1.639771578464874e-05,
        "epoch": 0.8157387167288157,
        "step": 6344
    },
    {
        "loss": 2.074,
        "grad_norm": 2.3240909576416016,
        "learning_rate": 1.6375514760583244e-05,
        "epoch": 0.8158673010158158,
        "step": 6345
    },
    {
        "loss": 0.6439,
        "grad_norm": 2.0769243240356445,
        "learning_rate": 1.6353327435424858e-05,
        "epoch": 0.815995885302816,
        "step": 6346
    },
    {
        "loss": 1.0684,
        "grad_norm": 1.2922149896621704,
        "learning_rate": 1.6331153812808142e-05,
        "epoch": 0.8161244695898161,
        "step": 6347
    },
    {
        "loss": 1.5593,
        "grad_norm": 1.7159689664840698,
        "learning_rate": 1.6308993896365488e-05,
        "epoch": 0.8162530538768162,
        "step": 6348
    },
    {
        "loss": 2.0964,
        "grad_norm": 1.5884772539138794,
        "learning_rate": 1.6286847689727026e-05,
        "epoch": 0.8163816381638164,
        "step": 6349
    },
    {
        "loss": 1.6439,
        "grad_norm": 1.9236338138580322,
        "learning_rate": 1.6264715196520576e-05,
        "epoch": 0.8165102224508165,
        "step": 6350
    },
    {
        "loss": 1.6858,
        "grad_norm": 2.098942995071411,
        "learning_rate": 1.62425964203718e-05,
        "epoch": 0.8166388067378166,
        "step": 6351
    },
    {
        "loss": 1.4327,
        "grad_norm": 2.0442867279052734,
        "learning_rate": 1.62204913649041e-05,
        "epoch": 0.8167673910248168,
        "step": 6352
    },
    {
        "loss": 0.9773,
        "grad_norm": 1.0839073657989502,
        "learning_rate": 1.6198400033738536e-05,
        "epoch": 0.8168959753118169,
        "step": 6353
    },
    {
        "loss": 1.211,
        "grad_norm": 2.4582173824310303,
        "learning_rate": 1.617632243049404e-05,
        "epoch": 0.817024559598817,
        "step": 6354
    },
    {
        "loss": 1.4566,
        "grad_norm": 1.796822190284729,
        "learning_rate": 1.6154258558787226e-05,
        "epoch": 0.8171531438858172,
        "step": 6355
    },
    {
        "loss": 1.6703,
        "grad_norm": 1.755088448524475,
        "learning_rate": 1.613220842223251e-05,
        "epoch": 0.8172817281728173,
        "step": 6356
    },
    {
        "loss": 1.6394,
        "grad_norm": 2.2229020595550537,
        "learning_rate": 1.6110172024441973e-05,
        "epoch": 0.8174103124598174,
        "step": 6357
    },
    {
        "loss": 2.0382,
        "grad_norm": 1.7668050527572632,
        "learning_rate": 1.608814936902552e-05,
        "epoch": 0.8175388967468176,
        "step": 6358
    },
    {
        "loss": 2.0495,
        "grad_norm": 1.3525493144989014,
        "learning_rate": 1.606614045959082e-05,
        "epoch": 0.8176674810338177,
        "step": 6359
    },
    {
        "loss": 1.2507,
        "grad_norm": 2.450836181640625,
        "learning_rate": 1.604414529974316e-05,
        "epoch": 0.8177960653208178,
        "step": 6360
    },
    {
        "loss": 1.966,
        "grad_norm": 2.7039310932159424,
        "learning_rate": 1.6022163893085772e-05,
        "epoch": 0.817924649607818,
        "step": 6361
    },
    {
        "loss": 2.0906,
        "grad_norm": 1.3965879678726196,
        "learning_rate": 1.600019624321947e-05,
        "epoch": 0.8180532338948181,
        "step": 6362
    },
    {
        "loss": 2.5053,
        "grad_norm": 2.874905824661255,
        "learning_rate": 1.5978242353742877e-05,
        "epoch": 0.8181818181818182,
        "step": 6363
    },
    {
        "loss": 1.4691,
        "grad_norm": 3.5519957542419434,
        "learning_rate": 1.5956302228252395e-05,
        "epoch": 0.8183104024688184,
        "step": 6364
    },
    {
        "loss": 1.3459,
        "grad_norm": 2.2708654403686523,
        "learning_rate": 1.59343758703421e-05,
        "epoch": 0.8184389867558184,
        "step": 6365
    },
    {
        "loss": 2.3946,
        "grad_norm": 1.9265090227127075,
        "learning_rate": 1.5912463283603872e-05,
        "epoch": 0.8185675710428185,
        "step": 6366
    },
    {
        "loss": 1.5853,
        "grad_norm": 2.627047300338745,
        "learning_rate": 1.5890564471627245e-05,
        "epoch": 0.8186961553298187,
        "step": 6367
    },
    {
        "loss": 1.532,
        "grad_norm": 3.1150929927825928,
        "learning_rate": 1.586867943799968e-05,
        "epoch": 0.8188247396168188,
        "step": 6368
    },
    {
        "loss": 1.7541,
        "grad_norm": 2.6399319171905518,
        "learning_rate": 1.5846808186306195e-05,
        "epoch": 0.8189533239038189,
        "step": 6369
    },
    {
        "loss": 1.2964,
        "grad_norm": 2.4172394275665283,
        "learning_rate": 1.582495072012957e-05,
        "epoch": 0.819081908190819,
        "step": 6370
    },
    {
        "loss": 1.7887,
        "grad_norm": 1.2291536331176758,
        "learning_rate": 1.580310704305048e-05,
        "epoch": 0.8192104924778192,
        "step": 6371
    },
    {
        "loss": 0.9078,
        "grad_norm": 2.1686511039733887,
        "learning_rate": 1.5781277158647168e-05,
        "epoch": 0.8193390767648193,
        "step": 6372
    },
    {
        "loss": 1.3128,
        "grad_norm": 3.9961769580841064,
        "learning_rate": 1.5759461070495707e-05,
        "epoch": 0.8194676610518195,
        "step": 6373
    },
    {
        "loss": 2.0162,
        "grad_norm": 1.2917331457138062,
        "learning_rate": 1.5737658782169907e-05,
        "epoch": 0.8195962453388196,
        "step": 6374
    },
    {
        "loss": 1.5314,
        "grad_norm": 2.3409581184387207,
        "learning_rate": 1.5715870297241264e-05,
        "epoch": 0.8197248296258197,
        "step": 6375
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.941372275352478,
        "learning_rate": 1.569409561927907e-05,
        "epoch": 0.8198534139128199,
        "step": 6376
    },
    {
        "loss": 1.8547,
        "grad_norm": 2.035088300704956,
        "learning_rate": 1.5672334751850326e-05,
        "epoch": 0.81998199819982,
        "step": 6377
    },
    {
        "loss": 1.4548,
        "grad_norm": 2.157614231109619,
        "learning_rate": 1.5650587698519804e-05,
        "epoch": 0.8201105824868201,
        "step": 6378
    },
    {
        "loss": 1.744,
        "grad_norm": 1.6020034551620483,
        "learning_rate": 1.562885446284994e-05,
        "epoch": 0.8202391667738202,
        "step": 6379
    },
    {
        "loss": 1.6803,
        "grad_norm": 1.868799090385437,
        "learning_rate": 1.5607135048400988e-05,
        "epoch": 0.8203677510608204,
        "step": 6380
    },
    {
        "loss": 1.1336,
        "grad_norm": 1.801983118057251,
        "learning_rate": 1.5585429458730917e-05,
        "epoch": 0.8204963353478205,
        "step": 6381
    },
    {
        "loss": 1.7882,
        "grad_norm": 1.5290229320526123,
        "learning_rate": 1.5563737697395374e-05,
        "epoch": 0.8206249196348206,
        "step": 6382
    },
    {
        "loss": 0.5511,
        "grad_norm": 2.1603453159332275,
        "learning_rate": 1.5542059767947815e-05,
        "epoch": 0.8207535039218208,
        "step": 6383
    },
    {
        "loss": 1.4552,
        "grad_norm": 2.0377681255340576,
        "learning_rate": 1.5520395673939414e-05,
        "epoch": 0.8208820882088209,
        "step": 6384
    },
    {
        "loss": 2.0902,
        "grad_norm": 1.724747657775879,
        "learning_rate": 1.5498745418919024e-05,
        "epoch": 0.821010672495821,
        "step": 6385
    },
    {
        "loss": 2.0971,
        "grad_norm": 2.2530999183654785,
        "learning_rate": 1.54771090064333e-05,
        "epoch": 0.8211392567828212,
        "step": 6386
    },
    {
        "loss": 1.5119,
        "grad_norm": 2.855901002883911,
        "learning_rate": 1.545548644002658e-05,
        "epoch": 0.8212678410698213,
        "step": 6387
    },
    {
        "loss": 1.7138,
        "grad_norm": 2.1258766651153564,
        "learning_rate": 1.5433877723240998e-05,
        "epoch": 0.8213964253568214,
        "step": 6388
    },
    {
        "loss": 1.8337,
        "grad_norm": 1.8896294832229614,
        "learning_rate": 1.5412282859616322e-05,
        "epoch": 0.8215250096438216,
        "step": 6389
    },
    {
        "loss": 1.4193,
        "grad_norm": 2.085753917694092,
        "learning_rate": 1.5390701852690136e-05,
        "epoch": 0.8216535939308216,
        "step": 6390
    },
    {
        "loss": 1.1665,
        "grad_norm": 2.2632689476013184,
        "learning_rate": 1.5369134705997734e-05,
        "epoch": 0.8217821782178217,
        "step": 6391
    },
    {
        "loss": 1.2937,
        "grad_norm": 2.176002264022827,
        "learning_rate": 1.534758142307209e-05,
        "epoch": 0.8219107625048219,
        "step": 6392
    },
    {
        "loss": 2.2541,
        "grad_norm": 2.292368173599243,
        "learning_rate": 1.5326042007443963e-05,
        "epoch": 0.822039346791822,
        "step": 6393
    },
    {
        "loss": 1.4648,
        "grad_norm": 2.3567724227905273,
        "learning_rate": 1.530451646264186e-05,
        "epoch": 0.8221679310788221,
        "step": 6394
    },
    {
        "loss": 1.3196,
        "grad_norm": 2.106618881225586,
        "learning_rate": 1.5283004792191914e-05,
        "epoch": 0.8222965153658223,
        "step": 6395
    },
    {
        "loss": 1.5891,
        "grad_norm": 1.3138346672058105,
        "learning_rate": 1.5261506999618068e-05,
        "epoch": 0.8224250996528224,
        "step": 6396
    },
    {
        "loss": 1.7648,
        "grad_norm": 2.7067441940307617,
        "learning_rate": 1.5240023088442001e-05,
        "epoch": 0.8225536839398225,
        "step": 6397
    },
    {
        "loss": 2.3348,
        "grad_norm": 1.7818623781204224,
        "learning_rate": 1.5218553062183094e-05,
        "epoch": 0.8226822682268227,
        "step": 6398
    },
    {
        "loss": 2.0763,
        "grad_norm": 2.675778865814209,
        "learning_rate": 1.51970969243584e-05,
        "epoch": 0.8228108525138228,
        "step": 6399
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.3946664333343506,
        "learning_rate": 1.5175654678482786e-05,
        "epoch": 0.8229394368008229,
        "step": 6400
    },
    {
        "eval_loss": 1.746172308921814,
        "eval_runtime": 28.2007,
        "eval_samples_per_second": 2.801,
        "eval_steps_per_second": 2.801,
        "epoch": 0.8229394368008229,
        "step": 6400
    },
    {
        "loss": 1.8606,
        "grad_norm": 1.607473373413086,
        "learning_rate": 1.515422632806881e-05,
        "epoch": 0.8230680210878231,
        "step": 6401
    },
    {
        "loss": 1.3141,
        "grad_norm": 2.4235777854919434,
        "learning_rate": 1.5132811876626685e-05,
        "epoch": 0.8231966053748232,
        "step": 6402
    },
    {
        "loss": 1.3824,
        "grad_norm": 2.4058966636657715,
        "learning_rate": 1.5111411327664505e-05,
        "epoch": 0.8233251896618233,
        "step": 6403
    },
    {
        "loss": 2.3111,
        "grad_norm": 1.831766128540039,
        "learning_rate": 1.5090024684687943e-05,
        "epoch": 0.8234537739488235,
        "step": 6404
    },
    {
        "loss": 2.3374,
        "grad_norm": 1.9678834676742554,
        "learning_rate": 1.5068651951200418e-05,
        "epoch": 0.8235823582358236,
        "step": 6405
    },
    {
        "loss": 1.9282,
        "grad_norm": 2.249692440032959,
        "learning_rate": 1.5047293130703121e-05,
        "epoch": 0.8237109425228237,
        "step": 6406
    },
    {
        "loss": 1.7405,
        "grad_norm": 1.264792799949646,
        "learning_rate": 1.502594822669493e-05,
        "epoch": 0.8238395268098239,
        "step": 6407
    },
    {
        "loss": 1.6927,
        "grad_norm": 2.378872871398926,
        "learning_rate": 1.5004617242672481e-05,
        "epoch": 0.823968111096824,
        "step": 6408
    },
    {
        "loss": 1.9671,
        "grad_norm": 2.3792567253112793,
        "learning_rate": 1.4983300182130045e-05,
        "epoch": 0.8240966953838241,
        "step": 6409
    },
    {
        "loss": 1.7675,
        "grad_norm": 2.2849106788635254,
        "learning_rate": 1.4961997048559706e-05,
        "epoch": 0.8242252796708243,
        "step": 6410
    },
    {
        "loss": 1.5859,
        "grad_norm": 2.7369637489318848,
        "learning_rate": 1.494070784545123e-05,
        "epoch": 0.8243538639578244,
        "step": 6411
    },
    {
        "loss": 1.7338,
        "grad_norm": 3.093644380569458,
        "learning_rate": 1.4919432576292036e-05,
        "epoch": 0.8244824482448245,
        "step": 6412
    },
    {
        "loss": 1.9723,
        "grad_norm": 1.8827873468399048,
        "learning_rate": 1.4898171244567428e-05,
        "epoch": 0.8246110325318247,
        "step": 6413
    },
    {
        "loss": 2.5888,
        "grad_norm": 1.029262661933899,
        "learning_rate": 1.4876923853760238e-05,
        "epoch": 0.8247396168188248,
        "step": 6414
    },
    {
        "loss": 1.6407,
        "grad_norm": 3.2353556156158447,
        "learning_rate": 1.485569040735112e-05,
        "epoch": 0.8248682011058248,
        "step": 6415
    },
    {
        "loss": 1.9603,
        "grad_norm": 2.315417766571045,
        "learning_rate": 1.4834470908818454e-05,
        "epoch": 0.824996785392825,
        "step": 6416
    },
    {
        "loss": 1.8378,
        "grad_norm": 1.3953044414520264,
        "learning_rate": 1.481326536163825e-05,
        "epoch": 0.8251253696798251,
        "step": 6417
    },
    {
        "loss": 1.6414,
        "grad_norm": 1.5580594539642334,
        "learning_rate": 1.479207376928431e-05,
        "epoch": 0.8252539539668252,
        "step": 6418
    },
    {
        "loss": 1.4392,
        "grad_norm": 2.0727381706237793,
        "learning_rate": 1.4770896135228119e-05,
        "epoch": 0.8253825382538253,
        "step": 6419
    },
    {
        "loss": 1.2287,
        "grad_norm": 3.5856239795684814,
        "learning_rate": 1.4749732462938915e-05,
        "epoch": 0.8255111225408255,
        "step": 6420
    },
    {
        "loss": 1.5123,
        "grad_norm": 1.800864577293396,
        "learning_rate": 1.4728582755883568e-05,
        "epoch": 0.8256397068278256,
        "step": 6421
    },
    {
        "loss": 2.2007,
        "grad_norm": 1.8628392219543457,
        "learning_rate": 1.470744701752672e-05,
        "epoch": 0.8257682911148257,
        "step": 6422
    },
    {
        "loss": 1.8684,
        "grad_norm": 1.598156213760376,
        "learning_rate": 1.4686325251330746e-05,
        "epoch": 0.8258968754018259,
        "step": 6423
    },
    {
        "loss": 2.148,
        "grad_norm": 1.5365452766418457,
        "learning_rate": 1.4665217460755643e-05,
        "epoch": 0.826025459688826,
        "step": 6424
    },
    {
        "loss": 1.9894,
        "grad_norm": 1.679563283920288,
        "learning_rate": 1.4644123649259211e-05,
        "epoch": 0.8261540439758261,
        "step": 6425
    },
    {
        "loss": 1.0237,
        "grad_norm": 3.1904845237731934,
        "learning_rate": 1.4623043820296933e-05,
        "epoch": 0.8262826282628263,
        "step": 6426
    },
    {
        "loss": 1.4031,
        "grad_norm": 2.324549436569214,
        "learning_rate": 1.4601977977321957e-05,
        "epoch": 0.8264112125498264,
        "step": 6427
    },
    {
        "loss": 1.8537,
        "grad_norm": 2.7231016159057617,
        "learning_rate": 1.4580926123785187e-05,
        "epoch": 0.8265397968368265,
        "step": 6428
    },
    {
        "loss": 1.6761,
        "grad_norm": 2.1207094192504883,
        "learning_rate": 1.4559888263135223e-05,
        "epoch": 0.8266683811238267,
        "step": 6429
    },
    {
        "loss": 2.0624,
        "grad_norm": 1.3142328262329102,
        "learning_rate": 1.4538864398818408e-05,
        "epoch": 0.8267969654108268,
        "step": 6430
    },
    {
        "loss": 1.2372,
        "grad_norm": 2.3225722312927246,
        "learning_rate": 1.4517854534278696e-05,
        "epoch": 0.826925549697827,
        "step": 6431
    },
    {
        "loss": 1.9397,
        "grad_norm": 2.1243927478790283,
        "learning_rate": 1.4496858672957848e-05,
        "epoch": 0.8270541339848271,
        "step": 6432
    },
    {
        "loss": 1.8568,
        "grad_norm": 2.287520408630371,
        "learning_rate": 1.4475876818295297e-05,
        "epoch": 0.8271827182718272,
        "step": 6433
    },
    {
        "loss": 1.7545,
        "grad_norm": 3.651231050491333,
        "learning_rate": 1.4454908973728132e-05,
        "epoch": 0.8273113025588273,
        "step": 6434
    },
    {
        "loss": 1.1254,
        "grad_norm": 2.4876809120178223,
        "learning_rate": 1.4433955142691224e-05,
        "epoch": 0.8274398868458275,
        "step": 6435
    },
    {
        "loss": 1.7557,
        "grad_norm": 1.8346909284591675,
        "learning_rate": 1.441301532861714e-05,
        "epoch": 0.8275684711328276,
        "step": 6436
    },
    {
        "loss": 1.4592,
        "grad_norm": 3.319192886352539,
        "learning_rate": 1.439208953493607e-05,
        "epoch": 0.8276970554198277,
        "step": 6437
    },
    {
        "loss": 2.0797,
        "grad_norm": 2.1323049068450928,
        "learning_rate": 1.4371177765075983e-05,
        "epoch": 0.8278256397068279,
        "step": 6438
    },
    {
        "loss": 1.6324,
        "grad_norm": 2.1602141857147217,
        "learning_rate": 1.4350280022462547e-05,
        "epoch": 0.827954223993828,
        "step": 6439
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.7300198078155518,
        "learning_rate": 1.4329396310519127e-05,
        "epoch": 0.828082808280828,
        "step": 6440
    },
    {
        "loss": 1.8863,
        "grad_norm": 3.0451877117156982,
        "learning_rate": 1.4308526632666741e-05,
        "epoch": 0.8282113925678282,
        "step": 6441
    },
    {
        "loss": 1.5869,
        "grad_norm": 1.9293320178985596,
        "learning_rate": 1.4287670992324153e-05,
        "epoch": 0.8283399768548283,
        "step": 6442
    },
    {
        "loss": 1.5043,
        "grad_norm": 3.7649002075195312,
        "learning_rate": 1.4266829392907865e-05,
        "epoch": 0.8284685611418284,
        "step": 6443
    },
    {
        "loss": 1.2086,
        "grad_norm": 2.217024803161621,
        "learning_rate": 1.4246001837831969e-05,
        "epoch": 0.8285971454288286,
        "step": 6444
    },
    {
        "loss": 2.0076,
        "grad_norm": 1.993204951286316,
        "learning_rate": 1.4225188330508366e-05,
        "epoch": 0.8287257297158287,
        "step": 6445
    },
    {
        "loss": 2.2949,
        "grad_norm": 1.616555094718933,
        "learning_rate": 1.4204388874346608e-05,
        "epoch": 0.8288543140028288,
        "step": 6446
    },
    {
        "loss": 1.2504,
        "grad_norm": 2.9856038093566895,
        "learning_rate": 1.4183603472753925e-05,
        "epoch": 0.828982898289829,
        "step": 6447
    },
    {
        "loss": 0.968,
        "grad_norm": 2.595118761062622,
        "learning_rate": 1.4162832129135272e-05,
        "epoch": 0.8291114825768291,
        "step": 6448
    },
    {
        "loss": 1.3546,
        "grad_norm": 2.890439987182617,
        "learning_rate": 1.4142074846893316e-05,
        "epoch": 0.8292400668638292,
        "step": 6449
    },
    {
        "loss": 1.4207,
        "grad_norm": 2.3953239917755127,
        "learning_rate": 1.4121331629428414e-05,
        "epoch": 0.8293686511508294,
        "step": 6450
    },
    {
        "loss": 1.3648,
        "grad_norm": 2.660573959350586,
        "learning_rate": 1.4100602480138558e-05,
        "epoch": 0.8294972354378295,
        "step": 6451
    },
    {
        "loss": 1.0708,
        "grad_norm": 2.9881298542022705,
        "learning_rate": 1.4079887402419512e-05,
        "epoch": 0.8296258197248296,
        "step": 6452
    },
    {
        "loss": 1.8763,
        "grad_norm": 1.9896085262298584,
        "learning_rate": 1.4059186399664737e-05,
        "epoch": 0.8297544040118298,
        "step": 6453
    },
    {
        "loss": 2.4036,
        "grad_norm": 2.203519105911255,
        "learning_rate": 1.4038499475265276e-05,
        "epoch": 0.8298829882988299,
        "step": 6454
    },
    {
        "loss": 1.7106,
        "grad_norm": 2.116792917251587,
        "learning_rate": 1.4017826632610054e-05,
        "epoch": 0.83001157258583,
        "step": 6455
    },
    {
        "loss": 1.4667,
        "grad_norm": 2.1493918895721436,
        "learning_rate": 1.3997167875085515e-05,
        "epoch": 0.8301401568728302,
        "step": 6456
    },
    {
        "loss": 1.1673,
        "grad_norm": 1.7284091711044312,
        "learning_rate": 1.3976523206075842e-05,
        "epoch": 0.8302687411598303,
        "step": 6457
    },
    {
        "loss": 1.3189,
        "grad_norm": 2.4619064331054688,
        "learning_rate": 1.3955892628963018e-05,
        "epoch": 0.8303973254468304,
        "step": 6458
    },
    {
        "loss": 2.2163,
        "grad_norm": 1.5859856605529785,
        "learning_rate": 1.3935276147126553e-05,
        "epoch": 0.8305259097338306,
        "step": 6459
    },
    {
        "loss": 2.3892,
        "grad_norm": 1.4730514287948608,
        "learning_rate": 1.3914673763943764e-05,
        "epoch": 0.8306544940208307,
        "step": 6460
    },
    {
        "loss": 1.5074,
        "grad_norm": 1.1450861692428589,
        "learning_rate": 1.389408548278961e-05,
        "epoch": 0.8307830783078308,
        "step": 6461
    },
    {
        "loss": 2.0192,
        "grad_norm": 2.0940873622894287,
        "learning_rate": 1.3873511307036769e-05,
        "epoch": 0.830911662594831,
        "step": 6462
    },
    {
        "loss": 2.061,
        "grad_norm": 2.6038663387298584,
        "learning_rate": 1.3852951240055567e-05,
        "epoch": 0.8310402468818311,
        "step": 6463
    },
    {
        "loss": 1.8529,
        "grad_norm": 2.4313347339630127,
        "learning_rate": 1.3832405285214034e-05,
        "epoch": 0.8311688311688312,
        "step": 6464
    },
    {
        "loss": 1.0591,
        "grad_norm": 1.0296341180801392,
        "learning_rate": 1.3811873445877944e-05,
        "epoch": 0.8312974154558312,
        "step": 6465
    },
    {
        "loss": 1.511,
        "grad_norm": 1.5615651607513428,
        "learning_rate": 1.379135572541066e-05,
        "epoch": 0.8314259997428314,
        "step": 6466
    },
    {
        "loss": 1.2539,
        "grad_norm": 2.7928521633148193,
        "learning_rate": 1.3770852127173296e-05,
        "epoch": 0.8315545840298315,
        "step": 6467
    },
    {
        "loss": 1.6767,
        "grad_norm": 2.326612949371338,
        "learning_rate": 1.3750362654524673e-05,
        "epoch": 0.8316831683168316,
        "step": 6468
    },
    {
        "loss": 1.9964,
        "grad_norm": 2.575364351272583,
        "learning_rate": 1.372988731082121e-05,
        "epoch": 0.8318117526038318,
        "step": 6469
    },
    {
        "loss": 1.925,
        "grad_norm": 2.672487258911133,
        "learning_rate": 1.37094260994171e-05,
        "epoch": 0.8319403368908319,
        "step": 6470
    },
    {
        "loss": 2.2311,
        "grad_norm": 2.0567257404327393,
        "learning_rate": 1.3688979023664172e-05,
        "epoch": 0.832068921177832,
        "step": 6471
    },
    {
        "loss": 1.3915,
        "grad_norm": 3.224283456802368,
        "learning_rate": 1.3668546086911993e-05,
        "epoch": 0.8321975054648322,
        "step": 6472
    },
    {
        "loss": 1.4856,
        "grad_norm": 2.4171156883239746,
        "learning_rate": 1.3648127292507717e-05,
        "epoch": 0.8323260897518323,
        "step": 6473
    },
    {
        "loss": 1.8677,
        "grad_norm": 1.8544737100601196,
        "learning_rate": 1.362772264379628e-05,
        "epoch": 0.8324546740388324,
        "step": 6474
    },
    {
        "loss": 1.4008,
        "grad_norm": 2.6216843128204346,
        "learning_rate": 1.3607332144120266e-05,
        "epoch": 0.8325832583258326,
        "step": 6475
    },
    {
        "loss": 1.6153,
        "grad_norm": 2.680475950241089,
        "learning_rate": 1.358695579681989e-05,
        "epoch": 0.8327118426128327,
        "step": 6476
    },
    {
        "loss": 2.4505,
        "grad_norm": 2.4239773750305176,
        "learning_rate": 1.3566593605233124e-05,
        "epoch": 0.8328404268998328,
        "step": 6477
    },
    {
        "loss": 1.329,
        "grad_norm": 2.5715129375457764,
        "learning_rate": 1.3546245572695615e-05,
        "epoch": 0.832969011186833,
        "step": 6478
    },
    {
        "loss": 1.9436,
        "grad_norm": 1.515478253364563,
        "learning_rate": 1.3525911702540616e-05,
        "epoch": 0.8330975954738331,
        "step": 6479
    },
    {
        "loss": 0.9685,
        "grad_norm": 2.073512077331543,
        "learning_rate": 1.3505591998099143e-05,
        "epoch": 0.8332261797608332,
        "step": 6480
    },
    {
        "loss": 1.6775,
        "grad_norm": 1.7335938215255737,
        "learning_rate": 1.3485286462699843e-05,
        "epoch": 0.8333547640478334,
        "step": 6481
    },
    {
        "loss": 2.2306,
        "grad_norm": 1.7447831630706787,
        "learning_rate": 1.3464995099669087e-05,
        "epoch": 0.8334833483348335,
        "step": 6482
    },
    {
        "loss": 1.5897,
        "grad_norm": 1.9532719850540161,
        "learning_rate": 1.3444717912330861e-05,
        "epoch": 0.8336119326218336,
        "step": 6483
    },
    {
        "loss": 1.3578,
        "grad_norm": 2.4751601219177246,
        "learning_rate": 1.3424454904006878e-05,
        "epoch": 0.8337405169088338,
        "step": 6484
    },
    {
        "loss": 1.8843,
        "grad_norm": 2.326401710510254,
        "learning_rate": 1.3404206078016523e-05,
        "epoch": 0.8338691011958339,
        "step": 6485
    },
    {
        "loss": 1.159,
        "grad_norm": 2.2835652828216553,
        "learning_rate": 1.3383971437676823e-05,
        "epoch": 0.833997685482834,
        "step": 6486
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.041327714920044,
        "learning_rate": 1.3363750986302525e-05,
        "epoch": 0.8341262697698342,
        "step": 6487
    },
    {
        "loss": 1.1628,
        "grad_norm": 1.9486266374588013,
        "learning_rate": 1.3343544727206058e-05,
        "epoch": 0.8342548540568343,
        "step": 6488
    },
    {
        "loss": 2.3581,
        "grad_norm": 1.3095214366912842,
        "learning_rate": 1.3323352663697442e-05,
        "epoch": 0.8343834383438344,
        "step": 6489
    },
    {
        "loss": 1.722,
        "grad_norm": 2.476869583129883,
        "learning_rate": 1.330317479908446e-05,
        "epoch": 0.8345120226308345,
        "step": 6490
    },
    {
        "loss": 2.277,
        "grad_norm": 1.4987059831619263,
        "learning_rate": 1.328301113667254e-05,
        "epoch": 0.8346406069178346,
        "step": 6491
    },
    {
        "loss": 1.6123,
        "grad_norm": 2.6414058208465576,
        "learning_rate": 1.3262861679764816e-05,
        "epoch": 0.8347691912048347,
        "step": 6492
    },
    {
        "loss": 2.2493,
        "grad_norm": 2.078747510910034,
        "learning_rate": 1.3242726431662e-05,
        "epoch": 0.8348977754918349,
        "step": 6493
    },
    {
        "loss": 1.7038,
        "grad_norm": 2.0546481609344482,
        "learning_rate": 1.3222605395662569e-05,
        "epoch": 0.835026359778835,
        "step": 6494
    },
    {
        "loss": 0.8342,
        "grad_norm": 2.8664016723632812,
        "learning_rate": 1.3202498575062671e-05,
        "epoch": 0.8351549440658351,
        "step": 6495
    },
    {
        "loss": 1.7383,
        "grad_norm": 2.108314037322998,
        "learning_rate": 1.318240597315602e-05,
        "epoch": 0.8352835283528353,
        "step": 6496
    },
    {
        "loss": 0.8134,
        "grad_norm": 2.0515475273132324,
        "learning_rate": 1.3162327593234158e-05,
        "epoch": 0.8354121126398354,
        "step": 6497
    },
    {
        "loss": 1.3705,
        "grad_norm": 2.32236385345459,
        "learning_rate": 1.314226343858619e-05,
        "epoch": 0.8355406969268355,
        "step": 6498
    },
    {
        "loss": 1.0852,
        "grad_norm": 1.533475399017334,
        "learning_rate": 1.312221351249886e-05,
        "epoch": 0.8356692812138357,
        "step": 6499
    },
    {
        "loss": 1.6529,
        "grad_norm": 1.9882527589797974,
        "learning_rate": 1.3102177818256733e-05,
        "epoch": 0.8357978655008358,
        "step": 6500
    },
    {
        "eval_loss": 1.7432948350906372,
        "eval_runtime": 28.2014,
        "eval_samples_per_second": 2.801,
        "eval_steps_per_second": 2.801,
        "epoch": 0.8357978655008358,
        "step": 6500
    },
    {
        "loss": 1.7884,
        "grad_norm": 3.1933183670043945,
        "learning_rate": 1.3082156359141873e-05,
        "epoch": 0.8359264497878359,
        "step": 6501
    },
    {
        "loss": 1.7591,
        "grad_norm": 3.2954986095428467,
        "learning_rate": 1.3062149138434132e-05,
        "epoch": 0.8360550340748361,
        "step": 6502
    },
    {
        "loss": 1.3362,
        "grad_norm": 2.1803715229034424,
        "learning_rate": 1.3042156159410935e-05,
        "epoch": 0.8361836183618362,
        "step": 6503
    },
    {
        "loss": 2.2765,
        "grad_norm": 2.0086257457733154,
        "learning_rate": 1.302217742534746e-05,
        "epoch": 0.8363122026488363,
        "step": 6504
    },
    {
        "loss": 1.9821,
        "grad_norm": 2.3628180027008057,
        "learning_rate": 1.3002212939516534e-05,
        "epoch": 0.8364407869358365,
        "step": 6505
    },
    {
        "loss": 2.236,
        "grad_norm": 2.1372873783111572,
        "learning_rate": 1.2982262705188541e-05,
        "epoch": 0.8365693712228366,
        "step": 6506
    },
    {
        "loss": 2.1865,
        "grad_norm": 1.4732918739318848,
        "learning_rate": 1.2962326725631736e-05,
        "epoch": 0.8366979555098367,
        "step": 6507
    },
    {
        "loss": 2.0158,
        "grad_norm": 2.662173271179199,
        "learning_rate": 1.2942405004111868e-05,
        "epoch": 0.8368265397968369,
        "step": 6508
    },
    {
        "loss": 1.4633,
        "grad_norm": 2.3189609050750732,
        "learning_rate": 1.2922497543892353e-05,
        "epoch": 0.836955124083837,
        "step": 6509
    },
    {
        "loss": 1.935,
        "grad_norm": 1.8124513626098633,
        "learning_rate": 1.2902604348234427e-05,
        "epoch": 0.8370837083708371,
        "step": 6510
    },
    {
        "loss": 1.7621,
        "grad_norm": 1.837333083152771,
        "learning_rate": 1.288272542039679e-05,
        "epoch": 0.8372122926578373,
        "step": 6511
    },
    {
        "loss": 1.8492,
        "grad_norm": 1.5229568481445312,
        "learning_rate": 1.2862860763635953e-05,
        "epoch": 0.8373408769448374,
        "step": 6512
    },
    {
        "loss": 1.6408,
        "grad_norm": 1.5259472131729126,
        "learning_rate": 1.2843010381206022e-05,
        "epoch": 0.8374694612318375,
        "step": 6513
    },
    {
        "loss": 1.1469,
        "grad_norm": 1.5085556507110596,
        "learning_rate": 1.2823174276358807e-05,
        "epoch": 0.8375980455188377,
        "step": 6514
    },
    {
        "loss": 2.0922,
        "grad_norm": 1.9021952152252197,
        "learning_rate": 1.2803352452343697e-05,
        "epoch": 0.8377266298058377,
        "step": 6515
    },
    {
        "loss": 1.6082,
        "grad_norm": 2.4475722312927246,
        "learning_rate": 1.2783544912407818e-05,
        "epoch": 0.8378552140928378,
        "step": 6516
    },
    {
        "loss": 1.0251,
        "grad_norm": 2.7177140712738037,
        "learning_rate": 1.2763751659795964e-05,
        "epoch": 0.8379837983798379,
        "step": 6517
    },
    {
        "loss": 1.6959,
        "grad_norm": 2.1102261543273926,
        "learning_rate": 1.2743972697750506e-05,
        "epoch": 0.8381123826668381,
        "step": 6518
    },
    {
        "loss": 1.2816,
        "grad_norm": 2.847243309020996,
        "learning_rate": 1.2724208029511542e-05,
        "epoch": 0.8382409669538382,
        "step": 6519
    },
    {
        "loss": 1.6975,
        "grad_norm": 2.343137264251709,
        "learning_rate": 1.2704457658316848e-05,
        "epoch": 0.8383695512408383,
        "step": 6520
    },
    {
        "loss": 1.3661,
        "grad_norm": 2.2323296070098877,
        "learning_rate": 1.2684721587401772e-05,
        "epoch": 0.8384981355278385,
        "step": 6521
    },
    {
        "loss": 1.0135,
        "grad_norm": 2.4770467281341553,
        "learning_rate": 1.266499981999939e-05,
        "epoch": 0.8386267198148386,
        "step": 6522
    },
    {
        "loss": 1.9114,
        "grad_norm": 1.7673135995864868,
        "learning_rate": 1.2645292359340421e-05,
        "epoch": 0.8387553041018387,
        "step": 6523
    },
    {
        "loss": 2.3256,
        "grad_norm": 1.9727550745010376,
        "learning_rate": 1.262559920865325e-05,
        "epoch": 0.8388838883888389,
        "step": 6524
    },
    {
        "loss": 1.7322,
        "grad_norm": 2.2979795932769775,
        "learning_rate": 1.2605920371163848e-05,
        "epoch": 0.839012472675839,
        "step": 6525
    },
    {
        "loss": 2.4005,
        "grad_norm": 1.3480916023254395,
        "learning_rate": 1.258625585009594e-05,
        "epoch": 0.8391410569628391,
        "step": 6526
    },
    {
        "loss": 2.235,
        "grad_norm": 2.4304487705230713,
        "learning_rate": 1.256660564867086e-05,
        "epoch": 0.8392696412498393,
        "step": 6527
    },
    {
        "loss": 1.5334,
        "grad_norm": 1.802938461303711,
        "learning_rate": 1.2546969770107564e-05,
        "epoch": 0.8393982255368394,
        "step": 6528
    },
    {
        "loss": 1.864,
        "grad_norm": 2.16629958152771,
        "learning_rate": 1.2527348217622724e-05,
        "epoch": 0.8395268098238395,
        "step": 6529
    },
    {
        "loss": 1.8507,
        "grad_norm": 1.716261625289917,
        "learning_rate": 1.2507740994430639e-05,
        "epoch": 0.8396553941108397,
        "step": 6530
    },
    {
        "loss": 1.8752,
        "grad_norm": 2.7136483192443848,
        "learning_rate": 1.2488148103743224e-05,
        "epoch": 0.8397839783978398,
        "step": 6531
    },
    {
        "loss": 1.0545,
        "grad_norm": 1.3516254425048828,
        "learning_rate": 1.2468569548770103e-05,
        "epoch": 0.8399125626848399,
        "step": 6532
    },
    {
        "loss": 2.0444,
        "grad_norm": 2.799644708633423,
        "learning_rate": 1.2449005332718522e-05,
        "epoch": 0.8400411469718401,
        "step": 6533
    },
    {
        "loss": 1.5244,
        "grad_norm": 1.7646639347076416,
        "learning_rate": 1.2429455458793416e-05,
        "epoch": 0.8401697312588402,
        "step": 6534
    },
    {
        "loss": 0.9416,
        "grad_norm": 2.3217830657958984,
        "learning_rate": 1.240991993019729e-05,
        "epoch": 0.8402983155458403,
        "step": 6535
    },
    {
        "loss": 2.1663,
        "grad_norm": 2.806007146835327,
        "learning_rate": 1.239039875013036e-05,
        "epoch": 0.8404268998328405,
        "step": 6536
    },
    {
        "loss": 1.2791,
        "grad_norm": 2.4550881385803223,
        "learning_rate": 1.2370891921790506e-05,
        "epoch": 0.8405554841198406,
        "step": 6537
    },
    {
        "loss": 0.9527,
        "grad_norm": 2.4609262943267822,
        "learning_rate": 1.235139944837318e-05,
        "epoch": 0.8406840684068407,
        "step": 6538
    },
    {
        "loss": 1.589,
        "grad_norm": 1.9945775270462036,
        "learning_rate": 1.2331921333071605e-05,
        "epoch": 0.8408126526938409,
        "step": 6539
    },
    {
        "loss": 1.7687,
        "grad_norm": 1.9523707628250122,
        "learning_rate": 1.2312457579076531e-05,
        "epoch": 0.8409412369808409,
        "step": 6540
    },
    {
        "loss": 1.6571,
        "grad_norm": 1.3839385509490967,
        "learning_rate": 1.2293008189576393e-05,
        "epoch": 0.841069821267841,
        "step": 6541
    },
    {
        "loss": 2.0166,
        "grad_norm": 2.205099105834961,
        "learning_rate": 1.2273573167757313e-05,
        "epoch": 0.8411984055548412,
        "step": 6542
    },
    {
        "loss": 1.6865,
        "grad_norm": 2.4225590229034424,
        "learning_rate": 1.2254152516803008e-05,
        "epoch": 0.8413269898418413,
        "step": 6543
    },
    {
        "loss": 1.6658,
        "grad_norm": 3.187638282775879,
        "learning_rate": 1.22347462398949e-05,
        "epoch": 0.8414555741288414,
        "step": 6544
    },
    {
        "loss": 1.6313,
        "grad_norm": 2.2411694526672363,
        "learning_rate": 1.2215354340211982e-05,
        "epoch": 0.8415841584158416,
        "step": 6545
    },
    {
        "loss": 2.3306,
        "grad_norm": 1.5376635789871216,
        "learning_rate": 1.2195976820930932e-05,
        "epoch": 0.8417127427028417,
        "step": 6546
    },
    {
        "loss": 1.8244,
        "grad_norm": 2.2748477458953857,
        "learning_rate": 1.2176613685226102e-05,
        "epoch": 0.8418413269898418,
        "step": 6547
    },
    {
        "loss": 1.8239,
        "grad_norm": 2.089153528213501,
        "learning_rate": 1.2157264936269386e-05,
        "epoch": 0.841969911276842,
        "step": 6548
    },
    {
        "loss": 1.2399,
        "grad_norm": 3.329965114593506,
        "learning_rate": 1.2137930577230493e-05,
        "epoch": 0.8420984955638421,
        "step": 6549
    },
    {
        "loss": 0.8468,
        "grad_norm": 1.8980759382247925,
        "learning_rate": 1.2118610611276604e-05,
        "epoch": 0.8422270798508422,
        "step": 6550
    },
    {
        "loss": 1.6222,
        "grad_norm": 1.9149606227874756,
        "learning_rate": 1.2099305041572584e-05,
        "epoch": 0.8423556641378424,
        "step": 6551
    },
    {
        "loss": 1.6412,
        "grad_norm": 2.0627028942108154,
        "learning_rate": 1.208001387128106e-05,
        "epoch": 0.8424842484248425,
        "step": 6552
    },
    {
        "loss": 1.9861,
        "grad_norm": 1.969234585762024,
        "learning_rate": 1.2060737103562115e-05,
        "epoch": 0.8426128327118426,
        "step": 6553
    },
    {
        "loss": 2.2711,
        "grad_norm": 1.7926385402679443,
        "learning_rate": 1.20414747415736e-05,
        "epoch": 0.8427414169988428,
        "step": 6554
    },
    {
        "loss": 1.3838,
        "grad_norm": 2.4721689224243164,
        "learning_rate": 1.2022226788471003e-05,
        "epoch": 0.8428700012858429,
        "step": 6555
    },
    {
        "loss": 1.6231,
        "grad_norm": 2.0064938068389893,
        "learning_rate": 1.2002993247407357e-05,
        "epoch": 0.842998585572843,
        "step": 6556
    },
    {
        "loss": 1.8426,
        "grad_norm": 2.2019922733306885,
        "learning_rate": 1.1983774121533432e-05,
        "epoch": 0.8431271698598432,
        "step": 6557
    },
    {
        "loss": 1.5101,
        "grad_norm": 1.774438500404358,
        "learning_rate": 1.1964569413997584e-05,
        "epoch": 0.8432557541468433,
        "step": 6558
    },
    {
        "loss": 1.0202,
        "grad_norm": 1.5484440326690674,
        "learning_rate": 1.1945379127945855e-05,
        "epoch": 0.8433843384338434,
        "step": 6559
    },
    {
        "loss": 1.1371,
        "grad_norm": 2.0212671756744385,
        "learning_rate": 1.1926203266521852e-05,
        "epoch": 0.8435129227208435,
        "step": 6560
    },
    {
        "loss": 1.9661,
        "grad_norm": 2.254652500152588,
        "learning_rate": 1.1907041832866884e-05,
        "epoch": 0.8436415070078437,
        "step": 6561
    },
    {
        "loss": 1.9171,
        "grad_norm": 1.597551941871643,
        "learning_rate": 1.1887894830119883e-05,
        "epoch": 0.8437700912948438,
        "step": 6562
    },
    {
        "loss": 1.8076,
        "grad_norm": 2.079819917678833,
        "learning_rate": 1.1868762261417376e-05,
        "epoch": 0.843898675581844,
        "step": 6563
    },
    {
        "loss": 1.5078,
        "grad_norm": 1.9308099746704102,
        "learning_rate": 1.1849644129893568e-05,
        "epoch": 0.8440272598688441,
        "step": 6564
    },
    {
        "loss": 2.0633,
        "grad_norm": 1.8661354780197144,
        "learning_rate": 1.1830540438680282e-05,
        "epoch": 0.8441558441558441,
        "step": 6565
    },
    {
        "loss": 1.561,
        "grad_norm": 1.9641116857528687,
        "learning_rate": 1.181145119090702e-05,
        "epoch": 0.8442844284428442,
        "step": 6566
    },
    {
        "loss": 1.7161,
        "grad_norm": 1.915320634841919,
        "learning_rate": 1.1792376389700821e-05,
        "epoch": 0.8444130127298444,
        "step": 6567
    },
    {
        "loss": 1.5732,
        "grad_norm": 2.604461669921875,
        "learning_rate": 1.1773316038186444e-05,
        "epoch": 0.8445415970168445,
        "step": 6568
    },
    {
        "loss": 0.7507,
        "grad_norm": 2.2018773555755615,
        "learning_rate": 1.1754270139486267e-05,
        "epoch": 0.8446701813038446,
        "step": 6569
    },
    {
        "loss": 1.6272,
        "grad_norm": 3.8895456790924072,
        "learning_rate": 1.173523869672024e-05,
        "epoch": 0.8447987655908448,
        "step": 6570
    },
    {
        "loss": 0.8919,
        "grad_norm": 2.5936388969421387,
        "learning_rate": 1.1716221713006027e-05,
        "epoch": 0.8449273498778449,
        "step": 6571
    },
    {
        "loss": 1.826,
        "grad_norm": 2.0808122158050537,
        "learning_rate": 1.1697219191458896e-05,
        "epoch": 0.845055934164845,
        "step": 6572
    },
    {
        "loss": 1.6961,
        "grad_norm": 2.621027946472168,
        "learning_rate": 1.1678231135191697e-05,
        "epoch": 0.8451845184518452,
        "step": 6573
    },
    {
        "loss": 1.8392,
        "grad_norm": 2.545060157775879,
        "learning_rate": 1.1659257547314961e-05,
        "epoch": 0.8453131027388453,
        "step": 6574
    },
    {
        "loss": 1.64,
        "grad_norm": 1.5539186000823975,
        "learning_rate": 1.1640298430936857e-05,
        "epoch": 0.8454416870258454,
        "step": 6575
    },
    {
        "loss": 1.9241,
        "grad_norm": 2.3233940601348877,
        "learning_rate": 1.162135378916317e-05,
        "epoch": 0.8455702713128456,
        "step": 6576
    },
    {
        "loss": 2.4572,
        "grad_norm": 1.0565433502197266,
        "learning_rate": 1.160242362509727e-05,
        "epoch": 0.8456988555998457,
        "step": 6577
    },
    {
        "loss": 1.5806,
        "grad_norm": 1.9473843574523926,
        "learning_rate": 1.1583507941840221e-05,
        "epoch": 0.8458274398868458,
        "step": 6578
    },
    {
        "loss": 1.5269,
        "grad_norm": 1.7911421060562134,
        "learning_rate": 1.1564606742490702e-05,
        "epoch": 0.845956024173846,
        "step": 6579
    },
    {
        "loss": 1.6101,
        "grad_norm": 2.5967659950256348,
        "learning_rate": 1.1545720030144957e-05,
        "epoch": 0.8460846084608461,
        "step": 6580
    },
    {
        "loss": 1.1978,
        "grad_norm": 2.396777391433716,
        "learning_rate": 1.1526847807896923e-05,
        "epoch": 0.8462131927478462,
        "step": 6581
    },
    {
        "loss": 2.3239,
        "grad_norm": 1.572804570198059,
        "learning_rate": 1.1507990078838183e-05,
        "epoch": 0.8463417770348464,
        "step": 6582
    },
    {
        "loss": 1.9865,
        "grad_norm": 1.8918538093566895,
        "learning_rate": 1.1489146846057864e-05,
        "epoch": 0.8464703613218465,
        "step": 6583
    },
    {
        "loss": 1.6308,
        "grad_norm": 2.4515573978424072,
        "learning_rate": 1.1470318112642753e-05,
        "epoch": 0.8465989456088466,
        "step": 6584
    },
    {
        "loss": 2.3633,
        "grad_norm": 1.899624228477478,
        "learning_rate": 1.1451503881677305e-05,
        "epoch": 0.8467275298958468,
        "step": 6585
    },
    {
        "loss": 1.6878,
        "grad_norm": 2.514685869216919,
        "learning_rate": 1.1432704156243568e-05,
        "epoch": 0.8468561141828469,
        "step": 6586
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.931013822555542,
        "learning_rate": 1.1413918939421176e-05,
        "epoch": 0.846984698469847,
        "step": 6587
    },
    {
        "loss": 1.8639,
        "grad_norm": 2.1237070560455322,
        "learning_rate": 1.1395148234287435e-05,
        "epoch": 0.8471132827568472,
        "step": 6588
    },
    {
        "loss": 1.8434,
        "grad_norm": 1.5143588781356812,
        "learning_rate": 1.1376392043917283e-05,
        "epoch": 0.8472418670438473,
        "step": 6589
    },
    {
        "loss": 1.8869,
        "grad_norm": 2.5242838859558105,
        "learning_rate": 1.13576503713832e-05,
        "epoch": 0.8473704513308473,
        "step": 6590
    },
    {
        "loss": 1.7767,
        "grad_norm": 2.8263020515441895,
        "learning_rate": 1.1338923219755426e-05,
        "epoch": 0.8474990356178475,
        "step": 6591
    },
    {
        "loss": 1.541,
        "grad_norm": 2.0855846405029297,
        "learning_rate": 1.1320210592101688e-05,
        "epoch": 0.8476276199048476,
        "step": 6592
    },
    {
        "loss": 2.1326,
        "grad_norm": 2.094421625137329,
        "learning_rate": 1.1301512491487354e-05,
        "epoch": 0.8477562041918477,
        "step": 6593
    },
    {
        "loss": 1.6378,
        "grad_norm": 2.334059715270996,
        "learning_rate": 1.128282892097553e-05,
        "epoch": 0.8478847884788479,
        "step": 6594
    },
    {
        "loss": 1.7477,
        "grad_norm": 2.042328357696533,
        "learning_rate": 1.126415988362679e-05,
        "epoch": 0.848013372765848,
        "step": 6595
    },
    {
        "loss": 1.3806,
        "grad_norm": 1.5777273178100586,
        "learning_rate": 1.1245505382499411e-05,
        "epoch": 0.8481419570528481,
        "step": 6596
    },
    {
        "loss": 0.3933,
        "grad_norm": 1.4610615968704224,
        "learning_rate": 1.1226865420649313e-05,
        "epoch": 0.8482705413398482,
        "step": 6597
    },
    {
        "loss": 2.4021,
        "grad_norm": 1.3098204135894775,
        "learning_rate": 1.1208240001129922e-05,
        "epoch": 0.8483991256268484,
        "step": 6598
    },
    {
        "loss": 1.9813,
        "grad_norm": 1.6590065956115723,
        "learning_rate": 1.1189629126992385e-05,
        "epoch": 0.8485277099138485,
        "step": 6599
    },
    {
        "loss": 1.806,
        "grad_norm": 1.870607852935791,
        "learning_rate": 1.1171032801285441e-05,
        "epoch": 0.8486562942008486,
        "step": 6600
    },
    {
        "eval_loss": 1.7424312829971313,
        "eval_runtime": 28.2509,
        "eval_samples_per_second": 2.796,
        "eval_steps_per_second": 2.796,
        "epoch": 0.8486562942008486,
        "step": 6600
    },
    {
        "loss": 1.6725,
        "grad_norm": 1.9312561750411987,
        "learning_rate": 1.1152451027055455e-05,
        "epoch": 0.8487848784878488,
        "step": 6601
    },
    {
        "loss": 1.3059,
        "grad_norm": 3.0552453994750977,
        "learning_rate": 1.113388380734638e-05,
        "epoch": 0.8489134627748489,
        "step": 6602
    },
    {
        "loss": 1.8026,
        "grad_norm": 2.001049280166626,
        "learning_rate": 1.1115331145199736e-05,
        "epoch": 0.849042047061849,
        "step": 6603
    },
    {
        "loss": 1.1309,
        "grad_norm": 2.2342474460601807,
        "learning_rate": 1.1096793043654807e-05,
        "epoch": 0.8491706313488492,
        "step": 6604
    },
    {
        "loss": 1.2917,
        "grad_norm": 2.975151777267456,
        "learning_rate": 1.107826950574835e-05,
        "epoch": 0.8492992156358493,
        "step": 6605
    },
    {
        "loss": 1.1606,
        "grad_norm": 3.642604112625122,
        "learning_rate": 1.1059760534514818e-05,
        "epoch": 0.8494277999228494,
        "step": 6606
    },
    {
        "loss": 1.6471,
        "grad_norm": 3.65173077583313,
        "learning_rate": 1.104126613298625e-05,
        "epoch": 0.8495563842098496,
        "step": 6607
    },
    {
        "loss": 1.8427,
        "grad_norm": 1.7743120193481445,
        "learning_rate": 1.1022786304192268e-05,
        "epoch": 0.8496849684968497,
        "step": 6608
    },
    {
        "loss": 1.629,
        "grad_norm": 3.1423227787017822,
        "learning_rate": 1.100432105116015e-05,
        "epoch": 0.8498135527838498,
        "step": 6609
    },
    {
        "loss": 0.6665,
        "grad_norm": 3.636422872543335,
        "learning_rate": 1.0985870376914787e-05,
        "epoch": 0.84994213707085,
        "step": 6610
    },
    {
        "loss": 1.7321,
        "grad_norm": 2.1750311851501465,
        "learning_rate": 1.0967434284478672e-05,
        "epoch": 0.8500707213578501,
        "step": 6611
    },
    {
        "loss": 1.6008,
        "grad_norm": 2.039064645767212,
        "learning_rate": 1.0949012776871869e-05,
        "epoch": 0.8501993056448502,
        "step": 6612
    },
    {
        "loss": 1.6493,
        "grad_norm": 2.7530367374420166,
        "learning_rate": 1.093060585711212e-05,
        "epoch": 0.8503278899318504,
        "step": 6613
    },
    {
        "loss": 0.9378,
        "grad_norm": 2.1626546382904053,
        "learning_rate": 1.0912213528214765e-05,
        "epoch": 0.8504564742188505,
        "step": 6614
    },
    {
        "loss": 2.3889,
        "grad_norm": 1.833167552947998,
        "learning_rate": 1.0893835793192675e-05,
        "epoch": 0.8505850585058505,
        "step": 6615
    },
    {
        "loss": 2.1049,
        "grad_norm": 1.6876838207244873,
        "learning_rate": 1.087547265505643e-05,
        "epoch": 0.8507136427928507,
        "step": 6616
    },
    {
        "loss": 1.8038,
        "grad_norm": 2.170208215713501,
        "learning_rate": 1.0857124116814177e-05,
        "epoch": 0.8508422270798508,
        "step": 6617
    },
    {
        "loss": 2.5616,
        "grad_norm": 1.9318912029266357,
        "learning_rate": 1.083879018147168e-05,
        "epoch": 0.8509708113668509,
        "step": 6618
    },
    {
        "loss": 1.1055,
        "grad_norm": 1.7737414836883545,
        "learning_rate": 1.0820470852032284e-05,
        "epoch": 0.8510993956538511,
        "step": 6619
    },
    {
        "loss": 1.2809,
        "grad_norm": 2.1393845081329346,
        "learning_rate": 1.080216613149696e-05,
        "epoch": 0.8512279799408512,
        "step": 6620
    },
    {
        "loss": 1.9339,
        "grad_norm": 1.4794895648956299,
        "learning_rate": 1.0783876022864326e-05,
        "epoch": 0.8513565642278513,
        "step": 6621
    },
    {
        "loss": 2.0177,
        "grad_norm": 2.6010007858276367,
        "learning_rate": 1.0765600529130526e-05,
        "epoch": 0.8514851485148515,
        "step": 6622
    },
    {
        "loss": 1.9424,
        "grad_norm": 1.9003045558929443,
        "learning_rate": 1.0747339653289356e-05,
        "epoch": 0.8516137328018516,
        "step": 6623
    },
    {
        "loss": 1.8245,
        "grad_norm": 2.1088578701019287,
        "learning_rate": 1.0729093398332246e-05,
        "epoch": 0.8517423170888517,
        "step": 6624
    },
    {
        "loss": 2.0265,
        "grad_norm": 4.4664716720581055,
        "learning_rate": 1.0710861767248159e-05,
        "epoch": 0.8518709013758519,
        "step": 6625
    },
    {
        "loss": 2.0811,
        "grad_norm": 1.4245953559875488,
        "learning_rate": 1.0692644763023718e-05,
        "epoch": 0.851999485662852,
        "step": 6626
    },
    {
        "loss": 0.4697,
        "grad_norm": 1.4624414443969727,
        "learning_rate": 1.0674442388643135e-05,
        "epoch": 0.8521280699498521,
        "step": 6627
    },
    {
        "loss": 2.5593,
        "grad_norm": 2.093310832977295,
        "learning_rate": 1.0656254647088237e-05,
        "epoch": 0.8522566542368523,
        "step": 6628
    },
    {
        "loss": 1.3786,
        "grad_norm": 2.087599992752075,
        "learning_rate": 1.0638081541338408e-05,
        "epoch": 0.8523852385238524,
        "step": 6629
    },
    {
        "loss": 1.7894,
        "grad_norm": 2.0336427688598633,
        "learning_rate": 1.0619923074370675e-05,
        "epoch": 0.8525138228108525,
        "step": 6630
    },
    {
        "loss": 2.3175,
        "grad_norm": 1.9761741161346436,
        "learning_rate": 1.0601779249159705e-05,
        "epoch": 0.8526424070978527,
        "step": 6631
    },
    {
        "loss": 1.5547,
        "grad_norm": 1.9155734777450562,
        "learning_rate": 1.0583650068677631e-05,
        "epoch": 0.8527709913848528,
        "step": 6632
    },
    {
        "loss": 1.9332,
        "grad_norm": 2.3772969245910645,
        "learning_rate": 1.0565535535894378e-05,
        "epoch": 0.8528995756718529,
        "step": 6633
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.1774330139160156,
        "learning_rate": 1.0547435653777316e-05,
        "epoch": 0.8530281599588531,
        "step": 6634
    },
    {
        "loss": 1.202,
        "grad_norm": 1.3569507598876953,
        "learning_rate": 1.052935042529144e-05,
        "epoch": 0.8531567442458532,
        "step": 6635
    },
    {
        "loss": 2.1612,
        "grad_norm": 2.3124873638153076,
        "learning_rate": 1.0511279853399436e-05,
        "epoch": 0.8532853285328533,
        "step": 6636
    },
    {
        "loss": 1.877,
        "grad_norm": 2.5251624584198,
        "learning_rate": 1.0493223941061491e-05,
        "epoch": 0.8534139128198535,
        "step": 6637
    },
    {
        "loss": 1.841,
        "grad_norm": 1.782570242881775,
        "learning_rate": 1.047518269123543e-05,
        "epoch": 0.8535424971068536,
        "step": 6638
    },
    {
        "loss": 0.9965,
        "grad_norm": 1.81716787815094,
        "learning_rate": 1.0457156106876698e-05,
        "epoch": 0.8536710813938537,
        "step": 6639
    },
    {
        "loss": 1.6913,
        "grad_norm": 1.9793354272842407,
        "learning_rate": 1.0439144190938265e-05,
        "epoch": 0.8537996656808537,
        "step": 6640
    },
    {
        "loss": 1.6357,
        "grad_norm": 2.3820302486419678,
        "learning_rate": 1.0421146946370786e-05,
        "epoch": 0.8539282499678539,
        "step": 6641
    },
    {
        "loss": 0.9746,
        "grad_norm": 1.8834824562072754,
        "learning_rate": 1.0403164376122421e-05,
        "epoch": 0.854056834254854,
        "step": 6642
    },
    {
        "loss": 2.2536,
        "grad_norm": 1.5866210460662842,
        "learning_rate": 1.0385196483139047e-05,
        "epoch": 0.8541854185418541,
        "step": 6643
    },
    {
        "loss": 1.1036,
        "grad_norm": 1.9904754161834717,
        "learning_rate": 1.0367243270364035e-05,
        "epoch": 0.8543140028288543,
        "step": 6644
    },
    {
        "loss": 1.7066,
        "grad_norm": 1.948630452156067,
        "learning_rate": 1.034930474073833e-05,
        "epoch": 0.8544425871158544,
        "step": 6645
    },
    {
        "loss": 1.6059,
        "grad_norm": 1.881569266319275,
        "learning_rate": 1.0331380897200616e-05,
        "epoch": 0.8545711714028545,
        "step": 6646
    },
    {
        "loss": 1.9327,
        "grad_norm": 2.44343638420105,
        "learning_rate": 1.0313471742687009e-05,
        "epoch": 0.8546997556898547,
        "step": 6647
    },
    {
        "loss": 0.8553,
        "grad_norm": 2.1104369163513184,
        "learning_rate": 1.029557728013132e-05,
        "epoch": 0.8548283399768548,
        "step": 6648
    },
    {
        "loss": 1.3753,
        "grad_norm": 2.550840139389038,
        "learning_rate": 1.0277697512464935e-05,
        "epoch": 0.854956924263855,
        "step": 6649
    },
    {
        "loss": 1.5381,
        "grad_norm": 2.9019954204559326,
        "learning_rate": 1.0259832442616779e-05,
        "epoch": 0.8550855085508551,
        "step": 6650
    },
    {
        "loss": 1.1145,
        "grad_norm": 2.7470195293426514,
        "learning_rate": 1.0241982073513434e-05,
        "epoch": 0.8552140928378552,
        "step": 6651
    },
    {
        "loss": 2.4386,
        "grad_norm": 1.7328120470046997,
        "learning_rate": 1.022414640807905e-05,
        "epoch": 0.8553426771248553,
        "step": 6652
    },
    {
        "loss": 1.8461,
        "grad_norm": 1.898062825202942,
        "learning_rate": 1.0206325449235388e-05,
        "epoch": 0.8554712614118555,
        "step": 6653
    },
    {
        "loss": 1.1712,
        "grad_norm": 2.604707956314087,
        "learning_rate": 1.0188519199901737e-05,
        "epoch": 0.8555998456988556,
        "step": 6654
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.7876944541931152,
        "learning_rate": 1.0170727662995049e-05,
        "epoch": 0.8557284299858557,
        "step": 6655
    },
    {
        "loss": 1.1149,
        "grad_norm": 2.029578685760498,
        "learning_rate": 1.0152950841429842e-05,
        "epoch": 0.8558570142728559,
        "step": 6656
    },
    {
        "loss": 1.8702,
        "grad_norm": 1.6614065170288086,
        "learning_rate": 1.0135188738118195e-05,
        "epoch": 0.855985598559856,
        "step": 6657
    },
    {
        "loss": 1.1047,
        "grad_norm": 1.7298082113265991,
        "learning_rate": 1.0117441355969804e-05,
        "epoch": 0.8561141828468561,
        "step": 6658
    },
    {
        "loss": 1.7656,
        "grad_norm": 2.3466665744781494,
        "learning_rate": 1.0099708697891975e-05,
        "epoch": 0.8562427671338563,
        "step": 6659
    },
    {
        "loss": 1.7026,
        "grad_norm": 2.0047340393066406,
        "learning_rate": 1.0081990766789529e-05,
        "epoch": 0.8563713514208564,
        "step": 6660
    },
    {
        "loss": 0.6122,
        "grad_norm": 2.3640756607055664,
        "learning_rate": 1.0064287565564957e-05,
        "epoch": 0.8564999357078565,
        "step": 6661
    },
    {
        "loss": 2.1216,
        "grad_norm": 1.9107635021209717,
        "learning_rate": 1.004659909711828e-05,
        "epoch": 0.8566285199948567,
        "step": 6662
    },
    {
        "loss": 2.2704,
        "grad_norm": 1.94278883934021,
        "learning_rate": 1.0028925364347163e-05,
        "epoch": 0.8567571042818568,
        "step": 6663
    },
    {
        "loss": 1.8511,
        "grad_norm": 1.904815435409546,
        "learning_rate": 1.0011266370146766e-05,
        "epoch": 0.8568856885688569,
        "step": 6664
    },
    {
        "loss": 1.5961,
        "grad_norm": 2.578961133956909,
        "learning_rate": 9.99362211740993e-06,
        "epoch": 0.857014272855857,
        "step": 6665
    },
    {
        "loss": 1.554,
        "grad_norm": 1.935961127281189,
        "learning_rate": 9.97599260902704e-06,
        "epoch": 0.8571428571428571,
        "step": 6666
    },
    {
        "loss": 1.8891,
        "grad_norm": 2.502819538116455,
        "learning_rate": 9.958377847886036e-06,
        "epoch": 0.8572714414298572,
        "step": 6667
    },
    {
        "loss": 1.9597,
        "grad_norm": 2.347262382507324,
        "learning_rate": 9.940777836872484e-06,
        "epoch": 0.8574000257168574,
        "step": 6668
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.7088816165924072,
        "learning_rate": 9.923192578869533e-06,
        "epoch": 0.8575286100038575,
        "step": 6669
    },
    {
        "loss": 2.0952,
        "grad_norm": 1.3289273977279663,
        "learning_rate": 9.905622076757915e-06,
        "epoch": 0.8576571942908576,
        "step": 6670
    },
    {
        "loss": 2.2029,
        "grad_norm": 2.627863645553589,
        "learning_rate": 9.888066333415902e-06,
        "epoch": 0.8577857785778578,
        "step": 6671
    },
    {
        "loss": 1.6271,
        "grad_norm": 1.9842817783355713,
        "learning_rate": 9.870525351719384e-06,
        "epoch": 0.8579143628648579,
        "step": 6672
    },
    {
        "loss": 2.0948,
        "grad_norm": 2.9586732387542725,
        "learning_rate": 9.852999134541874e-06,
        "epoch": 0.858042947151858,
        "step": 6673
    },
    {
        "loss": 1.4002,
        "grad_norm": 3.1106441020965576,
        "learning_rate": 9.83548768475434e-06,
        "epoch": 0.8581715314388582,
        "step": 6674
    },
    {
        "loss": 2.1016,
        "grad_norm": 2.0414884090423584,
        "learning_rate": 9.817991005225502e-06,
        "epoch": 0.8583001157258583,
        "step": 6675
    },
    {
        "loss": 1.9477,
        "grad_norm": 1.300148606300354,
        "learning_rate": 9.800509098821532e-06,
        "epoch": 0.8584287000128584,
        "step": 6676
    },
    {
        "loss": 1.3286,
        "grad_norm": 1.4568724632263184,
        "learning_rate": 9.783041968406192e-06,
        "epoch": 0.8585572842998586,
        "step": 6677
    },
    {
        "loss": 2.11,
        "grad_norm": 1.5997991561889648,
        "learning_rate": 9.765589616840875e-06,
        "epoch": 0.8586858685868587,
        "step": 6678
    },
    {
        "loss": 2.2336,
        "grad_norm": 2.2843050956726074,
        "learning_rate": 9.748152046984538e-06,
        "epoch": 0.8588144528738588,
        "step": 6679
    },
    {
        "loss": 1.4265,
        "grad_norm": 5.037639617919922,
        "learning_rate": 9.730729261693717e-06,
        "epoch": 0.858943037160859,
        "step": 6680
    },
    {
        "loss": 1.0815,
        "grad_norm": 2.017548084259033,
        "learning_rate": 9.713321263822484e-06,
        "epoch": 0.8590716214478591,
        "step": 6681
    },
    {
        "loss": 2.0153,
        "grad_norm": 2.835818290710449,
        "learning_rate": 9.695928056222547e-06,
        "epoch": 0.8592002057348592,
        "step": 6682
    },
    {
        "loss": 2.3961,
        "grad_norm": 1.184613585472107,
        "learning_rate": 9.678549641743174e-06,
        "epoch": 0.8593287900218594,
        "step": 6683
    },
    {
        "loss": 1.9518,
        "grad_norm": 2.0277936458587646,
        "learning_rate": 9.661186023231161e-06,
        "epoch": 0.8594573743088595,
        "step": 6684
    },
    {
        "loss": 2.2174,
        "grad_norm": 2.189793586730957,
        "learning_rate": 9.64383720353097e-06,
        "epoch": 0.8595859585958596,
        "step": 6685
    },
    {
        "loss": 1.893,
        "grad_norm": 2.055621862411499,
        "learning_rate": 9.626503185484581e-06,
        "epoch": 0.8597145428828598,
        "step": 6686
    },
    {
        "loss": 1.3589,
        "grad_norm": 1.5652878284454346,
        "learning_rate": 9.609183971931512e-06,
        "epoch": 0.8598431271698599,
        "step": 6687
    },
    {
        "loss": 1.7657,
        "grad_norm": 2.4398717880249023,
        "learning_rate": 9.591879565708972e-06,
        "epoch": 0.85997171145686,
        "step": 6688
    },
    {
        "loss": 1.5858,
        "grad_norm": 2.0288894176483154,
        "learning_rate": 9.574589969651615e-06,
        "epoch": 0.8601002957438602,
        "step": 6689
    },
    {
        "loss": 1.9985,
        "grad_norm": 1.586982011795044,
        "learning_rate": 9.557315186591753e-06,
        "epoch": 0.8602288800308602,
        "step": 6690
    },
    {
        "loss": 1.2557,
        "grad_norm": 1.3800766468048096,
        "learning_rate": 9.540055219359267e-06,
        "epoch": 0.8603574643178603,
        "step": 6691
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.2628495693206787,
        "learning_rate": 9.522810070781563e-06,
        "epoch": 0.8604860486048604,
        "step": 6692
    },
    {
        "loss": 1.7007,
        "grad_norm": 2.246533155441284,
        "learning_rate": 9.505579743683635e-06,
        "epoch": 0.8606146328918606,
        "step": 6693
    },
    {
        "loss": 2.0104,
        "grad_norm": 2.0004968643188477,
        "learning_rate": 9.488364240888092e-06,
        "epoch": 0.8607432171788607,
        "step": 6694
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.5415722131729126,
        "learning_rate": 9.471163565215091e-06,
        "epoch": 0.8608718014658608,
        "step": 6695
    },
    {
        "loss": 2.0631,
        "grad_norm": 1.780561089515686,
        "learning_rate": 9.45397771948231e-06,
        "epoch": 0.861000385752861,
        "step": 6696
    },
    {
        "loss": 2.0604,
        "grad_norm": 2.032332420349121,
        "learning_rate": 9.436806706505064e-06,
        "epoch": 0.8611289700398611,
        "step": 6697
    },
    {
        "loss": 2.0287,
        "grad_norm": 2.4183874130249023,
        "learning_rate": 9.419650529096236e-06,
        "epoch": 0.8612575543268612,
        "step": 6698
    },
    {
        "loss": 1.7938,
        "grad_norm": 2.3575305938720703,
        "learning_rate": 9.402509190066223e-06,
        "epoch": 0.8613861386138614,
        "step": 6699
    },
    {
        "loss": 2.2731,
        "grad_norm": 2.147299289703369,
        "learning_rate": 9.385382692223033e-06,
        "epoch": 0.8615147229008615,
        "step": 6700
    },
    {
        "eval_loss": 1.7419100999832153,
        "eval_runtime": 28.2732,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.8615147229008615,
        "step": 6700
    },
    {
        "loss": 1.6778,
        "grad_norm": 2.0647330284118652,
        "learning_rate": 9.368271038372267e-06,
        "epoch": 0.8616433071878616,
        "step": 6701
    },
    {
        "loss": 1.4406,
        "grad_norm": 2.0905957221984863,
        "learning_rate": 9.351174231317005e-06,
        "epoch": 0.8617718914748618,
        "step": 6702
    },
    {
        "loss": 1.3744,
        "grad_norm": 2.122830867767334,
        "learning_rate": 9.334092273857997e-06,
        "epoch": 0.8619004757618619,
        "step": 6703
    },
    {
        "loss": 2.3,
        "grad_norm": 2.2752668857574463,
        "learning_rate": 9.317025168793503e-06,
        "epoch": 0.862029060048862,
        "step": 6704
    },
    {
        "loss": 1.7749,
        "grad_norm": 2.2147300243377686,
        "learning_rate": 9.299972918919386e-06,
        "epoch": 0.8621576443358622,
        "step": 6705
    },
    {
        "loss": 2.1449,
        "grad_norm": 1.9364597797393799,
        "learning_rate": 9.282935527029014e-06,
        "epoch": 0.8622862286228623,
        "step": 6706
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.3723206520080566,
        "learning_rate": 9.265912995913362e-06,
        "epoch": 0.8624148129098624,
        "step": 6707
    },
    {
        "loss": 2.1856,
        "grad_norm": 2.2188401222229004,
        "learning_rate": 9.248905328361013e-06,
        "epoch": 0.8625433971968626,
        "step": 6708
    },
    {
        "loss": 1.6434,
        "grad_norm": 1.7968639135360718,
        "learning_rate": 9.231912527158026e-06,
        "epoch": 0.8626719814838627,
        "step": 6709
    },
    {
        "loss": 1.9209,
        "grad_norm": 1.2626073360443115,
        "learning_rate": 9.214934595088076e-06,
        "epoch": 0.8628005657708628,
        "step": 6710
    },
    {
        "loss": 2.0691,
        "grad_norm": 1.5675917863845825,
        "learning_rate": 9.197971534932404e-06,
        "epoch": 0.862929150057863,
        "step": 6711
    },
    {
        "loss": 2.145,
        "grad_norm": 2.30031681060791,
        "learning_rate": 9.18102334946982e-06,
        "epoch": 0.8630577343448631,
        "step": 6712
    },
    {
        "loss": 1.5906,
        "grad_norm": 2.139303684234619,
        "learning_rate": 9.164090041476647e-06,
        "epoch": 0.8631863186318632,
        "step": 6713
    },
    {
        "loss": 2.0709,
        "grad_norm": 1.6351683139801025,
        "learning_rate": 9.147171613726834e-06,
        "epoch": 0.8633149029188634,
        "step": 6714
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.2387802600860596,
        "learning_rate": 9.130268068991876e-06,
        "epoch": 0.8634434872058634,
        "step": 6715
    },
    {
        "loss": 1.1565,
        "grad_norm": 2.447234630584717,
        "learning_rate": 9.11337941004079e-06,
        "epoch": 0.8635720714928635,
        "step": 6716
    },
    {
        "loss": 2.3669,
        "grad_norm": 2.1659700870513916,
        "learning_rate": 9.096505639640185e-06,
        "epoch": 0.8637006557798637,
        "step": 6717
    },
    {
        "loss": 1.4722,
        "grad_norm": 1.6661067008972168,
        "learning_rate": 9.079646760554273e-06,
        "epoch": 0.8638292400668638,
        "step": 6718
    },
    {
        "loss": 1.9051,
        "grad_norm": 2.1403584480285645,
        "learning_rate": 9.062802775544731e-06,
        "epoch": 0.8639578243538639,
        "step": 6719
    },
    {
        "loss": 1.5087,
        "grad_norm": 2.0744457244873047,
        "learning_rate": 9.045973687370868e-06,
        "epoch": 0.8640864086408641,
        "step": 6720
    },
    {
        "loss": 1.3514,
        "grad_norm": 1.5902515649795532,
        "learning_rate": 9.02915949878953e-06,
        "epoch": 0.8642149929278642,
        "step": 6721
    },
    {
        "loss": 1.4518,
        "grad_norm": 2.122840166091919,
        "learning_rate": 9.01236021255516e-06,
        "epoch": 0.8643435772148643,
        "step": 6722
    },
    {
        "loss": 1.1404,
        "grad_norm": 2.532794237136841,
        "learning_rate": 8.995575831419678e-06,
        "epoch": 0.8644721615018645,
        "step": 6723
    },
    {
        "loss": 1.8531,
        "grad_norm": 1.5677015781402588,
        "learning_rate": 8.978806358132619e-06,
        "epoch": 0.8646007457888646,
        "step": 6724
    },
    {
        "loss": 1.3601,
        "grad_norm": 1.1274924278259277,
        "learning_rate": 8.962051795441107e-06,
        "epoch": 0.8647293300758647,
        "step": 6725
    },
    {
        "loss": 1.7395,
        "grad_norm": 2.0331871509552,
        "learning_rate": 8.945312146089702e-06,
        "epoch": 0.8648579143628649,
        "step": 6726
    },
    {
        "loss": 1.4006,
        "grad_norm": 3.139688491821289,
        "learning_rate": 8.9285874128207e-06,
        "epoch": 0.864986498649865,
        "step": 6727
    },
    {
        "loss": 1.3893,
        "grad_norm": 2.0788626670837402,
        "learning_rate": 8.911877598373807e-06,
        "epoch": 0.8651150829368651,
        "step": 6728
    },
    {
        "loss": 0.8182,
        "grad_norm": 2.4396824836730957,
        "learning_rate": 8.895182705486293e-06,
        "epoch": 0.8652436672238653,
        "step": 6729
    },
    {
        "loss": 1.6699,
        "grad_norm": 1.705797553062439,
        "learning_rate": 8.878502736893113e-06,
        "epoch": 0.8653722515108654,
        "step": 6730
    },
    {
        "loss": 1.9026,
        "grad_norm": 2.208871364593506,
        "learning_rate": 8.861837695326614e-06,
        "epoch": 0.8655008357978655,
        "step": 6731
    },
    {
        "loss": 2.4582,
        "grad_norm": 1.5269792079925537,
        "learning_rate": 8.8451875835168e-06,
        "epoch": 0.8656294200848657,
        "step": 6732
    },
    {
        "loss": 1.2056,
        "grad_norm": 2.0649728775024414,
        "learning_rate": 8.828552404191226e-06,
        "epoch": 0.8657580043718658,
        "step": 6733
    },
    {
        "loss": 2.1108,
        "grad_norm": 2.7039551734924316,
        "learning_rate": 8.811932160074931e-06,
        "epoch": 0.8658865886588659,
        "step": 6734
    },
    {
        "loss": 1.1664,
        "grad_norm": 1.5781508684158325,
        "learning_rate": 8.795326853890573e-06,
        "epoch": 0.866015172945866,
        "step": 6735
    },
    {
        "loss": 1.7827,
        "grad_norm": 2.104095220565796,
        "learning_rate": 8.778736488358341e-06,
        "epoch": 0.8661437572328662,
        "step": 6736
    },
    {
        "loss": 1.5067,
        "grad_norm": 1.3903578519821167,
        "learning_rate": 8.762161066195995e-06,
        "epoch": 0.8662723415198663,
        "step": 6737
    },
    {
        "loss": 1.33,
        "grad_norm": 2.1365301609039307,
        "learning_rate": 8.745600590118818e-06,
        "epoch": 0.8664009258068665,
        "step": 6738
    },
    {
        "loss": 1.6299,
        "grad_norm": 2.8096954822540283,
        "learning_rate": 8.729055062839619e-06,
        "epoch": 0.8665295100938666,
        "step": 6739
    },
    {
        "loss": 2.0392,
        "grad_norm": 2.588843584060669,
        "learning_rate": 8.712524487068874e-06,
        "epoch": 0.8666580943808666,
        "step": 6740
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.519413948059082,
        "learning_rate": 8.69600886551447e-06,
        "epoch": 0.8667866786678667,
        "step": 6741
    },
    {
        "loss": 1.9089,
        "grad_norm": 2.1838884353637695,
        "learning_rate": 8.679508200881914e-06,
        "epoch": 0.8669152629548669,
        "step": 6742
    },
    {
        "loss": 1.6926,
        "grad_norm": 2.333575963973999,
        "learning_rate": 8.663022495874295e-06,
        "epoch": 0.867043847241867,
        "step": 6743
    },
    {
        "loss": 1.8012,
        "grad_norm": 1.928986668586731,
        "learning_rate": 8.646551753192156e-06,
        "epoch": 0.8671724315288671,
        "step": 6744
    },
    {
        "loss": 1.7344,
        "grad_norm": 2.7206854820251465,
        "learning_rate": 8.63009597553368e-06,
        "epoch": 0.8673010158158673,
        "step": 6745
    },
    {
        "loss": 2.047,
        "grad_norm": 1.6608364582061768,
        "learning_rate": 8.613655165594547e-06,
        "epoch": 0.8674296001028674,
        "step": 6746
    },
    {
        "loss": 1.4455,
        "grad_norm": 1.8618932962417603,
        "learning_rate": 8.597229326068034e-06,
        "epoch": 0.8675581843898675,
        "step": 6747
    },
    {
        "loss": 1.5435,
        "grad_norm": 2.9896252155303955,
        "learning_rate": 8.580818459644879e-06,
        "epoch": 0.8676867686768677,
        "step": 6748
    },
    {
        "loss": 2.037,
        "grad_norm": 1.808659315109253,
        "learning_rate": 8.564422569013453e-06,
        "epoch": 0.8678153529638678,
        "step": 6749
    },
    {
        "loss": 1.7192,
        "grad_norm": 2.706733465194702,
        "learning_rate": 8.54804165685964e-06,
        "epoch": 0.8679439372508679,
        "step": 6750
    },
    {
        "loss": 1.3484,
        "grad_norm": 2.15739107131958,
        "learning_rate": 8.531675725866861e-06,
        "epoch": 0.8680725215378681,
        "step": 6751
    },
    {
        "loss": 1.697,
        "grad_norm": 2.415347099304199,
        "learning_rate": 8.515324778716105e-06,
        "epoch": 0.8682011058248682,
        "step": 6752
    },
    {
        "loss": 1.1376,
        "grad_norm": 1.747167706489563,
        "learning_rate": 8.498988818085896e-06,
        "epoch": 0.8683296901118683,
        "step": 6753
    },
    {
        "loss": 1.9269,
        "grad_norm": 1.825534701347351,
        "learning_rate": 8.482667846652281e-06,
        "epoch": 0.8684582743988685,
        "step": 6754
    },
    {
        "loss": 2.0347,
        "grad_norm": 1.771241307258606,
        "learning_rate": 8.46636186708889e-06,
        "epoch": 0.8685868586858686,
        "step": 6755
    },
    {
        "loss": 1.4137,
        "grad_norm": 2.188690662384033,
        "learning_rate": 8.450070882066886e-06,
        "epoch": 0.8687154429728687,
        "step": 6756
    },
    {
        "loss": 2.3153,
        "grad_norm": 1.9833253622055054,
        "learning_rate": 8.433794894254965e-06,
        "epoch": 0.8688440272598689,
        "step": 6757
    },
    {
        "loss": 0.6521,
        "grad_norm": 2.1518256664276123,
        "learning_rate": 8.417533906319352e-06,
        "epoch": 0.868972611546869,
        "step": 6758
    },
    {
        "loss": 1.0511,
        "grad_norm": 1.4857499599456787,
        "learning_rate": 8.401287920923851e-06,
        "epoch": 0.8691011958338691,
        "step": 6759
    },
    {
        "loss": 2.1668,
        "grad_norm": 2.3370089530944824,
        "learning_rate": 8.385056940729796e-06,
        "epoch": 0.8692297801208693,
        "step": 6760
    },
    {
        "loss": 1.8997,
        "grad_norm": 1.78384530544281,
        "learning_rate": 8.368840968396041e-06,
        "epoch": 0.8693583644078694,
        "step": 6761
    },
    {
        "loss": 2.0257,
        "grad_norm": 2.502782106399536,
        "learning_rate": 8.352640006579005e-06,
        "epoch": 0.8694869486948695,
        "step": 6762
    },
    {
        "loss": 2.0168,
        "grad_norm": 2.1728007793426514,
        "learning_rate": 8.33645405793263e-06,
        "epoch": 0.8696155329818697,
        "step": 6763
    },
    {
        "loss": 1.6743,
        "grad_norm": 1.8896522521972656,
        "learning_rate": 8.320283125108453e-06,
        "epoch": 0.8697441172688698,
        "step": 6764
    },
    {
        "loss": 1.4304,
        "grad_norm": 2.3287153244018555,
        "learning_rate": 8.304127210755453e-06,
        "epoch": 0.8698727015558698,
        "step": 6765
    },
    {
        "loss": 1.6194,
        "grad_norm": 2.2493081092834473,
        "learning_rate": 8.287986317520225e-06,
        "epoch": 0.87000128584287,
        "step": 6766
    },
    {
        "loss": 1.0895,
        "grad_norm": 1.8571995496749878,
        "learning_rate": 8.271860448046909e-06,
        "epoch": 0.8701298701298701,
        "step": 6767
    },
    {
        "loss": 2.2544,
        "grad_norm": 1.5879496335983276,
        "learning_rate": 8.255749604977093e-06,
        "epoch": 0.8702584544168702,
        "step": 6768
    },
    {
        "loss": 1.3162,
        "grad_norm": 1.5499345064163208,
        "learning_rate": 8.239653790950042e-06,
        "epoch": 0.8703870387038704,
        "step": 6769
    },
    {
        "loss": 1.2663,
        "grad_norm": 2.5021796226501465,
        "learning_rate": 8.223573008602458e-06,
        "epoch": 0.8705156229908705,
        "step": 6770
    },
    {
        "loss": 1.9106,
        "grad_norm": 2.2652690410614014,
        "learning_rate": 8.20750726056856e-06,
        "epoch": 0.8706442072778706,
        "step": 6771
    },
    {
        "loss": 2.4126,
        "grad_norm": 1.2341417074203491,
        "learning_rate": 8.191456549480214e-06,
        "epoch": 0.8707727915648708,
        "step": 6772
    },
    {
        "loss": 1.8686,
        "grad_norm": 1.2928956747055054,
        "learning_rate": 8.175420877966732e-06,
        "epoch": 0.8709013758518709,
        "step": 6773
    },
    {
        "loss": 1.1789,
        "grad_norm": 1.9476158618927002,
        "learning_rate": 8.159400248654992e-06,
        "epoch": 0.871029960138871,
        "step": 6774
    },
    {
        "loss": 1.9215,
        "grad_norm": 2.1471123695373535,
        "learning_rate": 8.143394664169424e-06,
        "epoch": 0.8711585444258712,
        "step": 6775
    },
    {
        "loss": 1.7917,
        "grad_norm": 2.5981698036193848,
        "learning_rate": 8.127404127131933e-06,
        "epoch": 0.8712871287128713,
        "step": 6776
    },
    {
        "loss": 2.0815,
        "grad_norm": 1.5941166877746582,
        "learning_rate": 8.111428640162055e-06,
        "epoch": 0.8714157129998714,
        "step": 6777
    },
    {
        "loss": 1.9398,
        "grad_norm": 1.7408934831619263,
        "learning_rate": 8.095468205876745e-06,
        "epoch": 0.8715442972868715,
        "step": 6778
    },
    {
        "loss": 1.2247,
        "grad_norm": 1.9224330186843872,
        "learning_rate": 8.079522826890629e-06,
        "epoch": 0.8716728815738717,
        "step": 6779
    },
    {
        "loss": 1.4677,
        "grad_norm": 2.6784348487854004,
        "learning_rate": 8.063592505815743e-06,
        "epoch": 0.8718014658608718,
        "step": 6780
    },
    {
        "loss": 1.658,
        "grad_norm": 1.8730360269546509,
        "learning_rate": 8.047677245261686e-06,
        "epoch": 0.871930050147872,
        "step": 6781
    },
    {
        "loss": 1.6633,
        "grad_norm": 1.6882250308990479,
        "learning_rate": 8.031777047835676e-06,
        "epoch": 0.8720586344348721,
        "step": 6782
    },
    {
        "loss": 1.8095,
        "grad_norm": 1.7696136236190796,
        "learning_rate": 8.015891916142338e-06,
        "epoch": 0.8721872187218722,
        "step": 6783
    },
    {
        "loss": 1.1095,
        "grad_norm": 1.5774343013763428,
        "learning_rate": 8.000021852783912e-06,
        "epoch": 0.8723158030088723,
        "step": 6784
    },
    {
        "loss": 2.5191,
        "grad_norm": 1.7794528007507324,
        "learning_rate": 7.984166860360154e-06,
        "epoch": 0.8724443872958725,
        "step": 6785
    },
    {
        "loss": 2.116,
        "grad_norm": 1.9817909002304077,
        "learning_rate": 7.968326941468307e-06,
        "epoch": 0.8725729715828726,
        "step": 6786
    },
    {
        "loss": 1.2815,
        "grad_norm": 2.4108967781066895,
        "learning_rate": 7.952502098703219e-06,
        "epoch": 0.8727015558698727,
        "step": 6787
    },
    {
        "loss": 0.4176,
        "grad_norm": 1.8134762048721313,
        "learning_rate": 7.936692334657203e-06,
        "epoch": 0.8728301401568729,
        "step": 6788
    },
    {
        "loss": 1.7049,
        "grad_norm": 2.1870014667510986,
        "learning_rate": 7.920897651920155e-06,
        "epoch": 0.872958724443873,
        "step": 6789
    },
    {
        "loss": 1.5697,
        "grad_norm": 1.9890648126602173,
        "learning_rate": 7.905118053079453e-06,
        "epoch": 0.873087308730873,
        "step": 6790
    },
    {
        "loss": 1.1791,
        "grad_norm": 2.91556453704834,
        "learning_rate": 7.889353540720012e-06,
        "epoch": 0.8732158930178732,
        "step": 6791
    },
    {
        "loss": 1.7336,
        "grad_norm": 1.8745895624160767,
        "learning_rate": 7.873604117424338e-06,
        "epoch": 0.8733444773048733,
        "step": 6792
    },
    {
        "loss": 2.4246,
        "grad_norm": 1.3100086450576782,
        "learning_rate": 7.857869785772365e-06,
        "epoch": 0.8734730615918734,
        "step": 6793
    },
    {
        "loss": 1.1412,
        "grad_norm": 2.409093141555786,
        "learning_rate": 7.842150548341621e-06,
        "epoch": 0.8736016458788736,
        "step": 6794
    },
    {
        "loss": 1.2762,
        "grad_norm": 2.6771159172058105,
        "learning_rate": 7.826446407707167e-06,
        "epoch": 0.8737302301658737,
        "step": 6795
    },
    {
        "loss": 1.9513,
        "grad_norm": 1.4317450523376465,
        "learning_rate": 7.810757366441534e-06,
        "epoch": 0.8738588144528738,
        "step": 6796
    },
    {
        "loss": 1.859,
        "grad_norm": 2.014439821243286,
        "learning_rate": 7.795083427114825e-06,
        "epoch": 0.873987398739874,
        "step": 6797
    },
    {
        "loss": 2.2554,
        "grad_norm": 1.4009032249450684,
        "learning_rate": 7.77942459229467e-06,
        "epoch": 0.8741159830268741,
        "step": 6798
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.8877015113830566,
        "learning_rate": 7.763780864546233e-06,
        "epoch": 0.8742445673138742,
        "step": 6799
    },
    {
        "loss": 2.0591,
        "grad_norm": 1.7243638038635254,
        "learning_rate": 7.74815224643215e-06,
        "epoch": 0.8743731516008744,
        "step": 6800
    },
    {
        "eval_loss": 1.7398959398269653,
        "eval_runtime": 28.3463,
        "eval_samples_per_second": 2.787,
        "eval_steps_per_second": 2.787,
        "epoch": 0.8743731516008744,
        "step": 6800
    },
    {
        "loss": 2.0259,
        "grad_norm": 2.1151068210601807,
        "learning_rate": 7.732538740512618e-06,
        "epoch": 0.8745017358878745,
        "step": 6801
    },
    {
        "loss": 2.1636,
        "grad_norm": 1.863055944442749,
        "learning_rate": 7.716940349345392e-06,
        "epoch": 0.8746303201748746,
        "step": 6802
    },
    {
        "loss": 2.0062,
        "grad_norm": 1.622449517250061,
        "learning_rate": 7.701357075485672e-06,
        "epoch": 0.8747589044618748,
        "step": 6803
    },
    {
        "loss": 2.2342,
        "grad_norm": 1.5760531425476074,
        "learning_rate": 7.685788921486236e-06,
        "epoch": 0.8748874887488749,
        "step": 6804
    },
    {
        "loss": 1.8395,
        "grad_norm": 1.8811568021774292,
        "learning_rate": 7.670235889897404e-06,
        "epoch": 0.875016073035875,
        "step": 6805
    },
    {
        "loss": 1.2823,
        "grad_norm": 2.645185947418213,
        "learning_rate": 7.654697983266945e-06,
        "epoch": 0.8751446573228752,
        "step": 6806
    },
    {
        "loss": 1.4721,
        "grad_norm": 2.6342763900756836,
        "learning_rate": 7.639175204140213e-06,
        "epoch": 0.8752732416098753,
        "step": 6807
    },
    {
        "loss": 1.9618,
        "grad_norm": 1.9265929460525513,
        "learning_rate": 7.623667555060065e-06,
        "epoch": 0.8754018258968754,
        "step": 6808
    },
    {
        "loss": 1.7024,
        "grad_norm": 2.2743003368377686,
        "learning_rate": 7.6081750385669e-06,
        "epoch": 0.8755304101838756,
        "step": 6809
    },
    {
        "loss": 1.4386,
        "grad_norm": 2.0908520221710205,
        "learning_rate": 7.592697657198566e-06,
        "epoch": 0.8756589944708757,
        "step": 6810
    },
    {
        "loss": 1.9211,
        "grad_norm": 2.1792469024658203,
        "learning_rate": 7.577235413490558e-06,
        "epoch": 0.8757875787578758,
        "step": 6811
    },
    {
        "loss": 1.8115,
        "grad_norm": 1.5112566947937012,
        "learning_rate": 7.56178830997577e-06,
        "epoch": 0.875916163044876,
        "step": 6812
    },
    {
        "loss": 1.6761,
        "grad_norm": 1.990600347518921,
        "learning_rate": 7.546356349184625e-06,
        "epoch": 0.8760447473318761,
        "step": 6813
    },
    {
        "loss": 1.8067,
        "grad_norm": 1.7495126724243164,
        "learning_rate": 7.530939533645187e-06,
        "epoch": 0.8761733316188762,
        "step": 6814
    },
    {
        "loss": 1.1509,
        "grad_norm": 2.8295183181762695,
        "learning_rate": 7.515537865882894e-06,
        "epoch": 0.8763019159058762,
        "step": 6815
    },
    {
        "loss": 1.1851,
        "grad_norm": 1.5222641229629517,
        "learning_rate": 7.500151348420792e-06,
        "epoch": 0.8764305001928764,
        "step": 6816
    },
    {
        "loss": 1.4198,
        "grad_norm": 2.541977643966675,
        "learning_rate": 7.484779983779389e-06,
        "epoch": 0.8765590844798765,
        "step": 6817
    },
    {
        "loss": 1.4401,
        "grad_norm": 1.728261113166809,
        "learning_rate": 7.4694237744767585e-06,
        "epoch": 0.8766876687668766,
        "step": 6818
    },
    {
        "loss": 2.2113,
        "grad_norm": 1.7665051221847534,
        "learning_rate": 7.454082723028488e-06,
        "epoch": 0.8768162530538768,
        "step": 6819
    },
    {
        "loss": 2.287,
        "grad_norm": 1.3762764930725098,
        "learning_rate": 7.438756831947602e-06,
        "epoch": 0.8769448373408769,
        "step": 6820
    },
    {
        "loss": 2.4201,
        "grad_norm": 1.3253123760223389,
        "learning_rate": 7.423446103744791e-06,
        "epoch": 0.877073421627877,
        "step": 6821
    },
    {
        "loss": 1.3148,
        "grad_norm": 2.020313024520874,
        "learning_rate": 7.408150540928116e-06,
        "epoch": 0.8772020059148772,
        "step": 6822
    },
    {
        "loss": 2.1476,
        "grad_norm": 2.344120502471924,
        "learning_rate": 7.392870146003206e-06,
        "epoch": 0.8773305902018773,
        "step": 6823
    },
    {
        "loss": 1.1331,
        "grad_norm": 2.894179344177246,
        "learning_rate": 7.3776049214732575e-06,
        "epoch": 0.8774591744888774,
        "step": 6824
    },
    {
        "loss": 1.6878,
        "grad_norm": 3.4914166927337646,
        "learning_rate": 7.362354869838905e-06,
        "epoch": 0.8775877587758776,
        "step": 6825
    },
    {
        "loss": 1.9324,
        "grad_norm": 1.830051064491272,
        "learning_rate": 7.347119993598328e-06,
        "epoch": 0.8777163430628777,
        "step": 6826
    },
    {
        "loss": 1.3938,
        "grad_norm": 2.353806495666504,
        "learning_rate": 7.331900295247251e-06,
        "epoch": 0.8778449273498778,
        "step": 6827
    },
    {
        "loss": 1.3623,
        "grad_norm": 1.262130856513977,
        "learning_rate": 7.316695777278837e-06,
        "epoch": 0.877973511636878,
        "step": 6828
    },
    {
        "loss": 1.3544,
        "grad_norm": 1.8637487888336182,
        "learning_rate": 7.301506442183825e-06,
        "epoch": 0.8781020959238781,
        "step": 6829
    },
    {
        "loss": 1.12,
        "grad_norm": 1.8825000524520874,
        "learning_rate": 7.28633229245046e-06,
        "epoch": 0.8782306802108782,
        "step": 6830
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.756819486618042,
        "learning_rate": 7.271173330564507e-06,
        "epoch": 0.8783592644978784,
        "step": 6831
    },
    {
        "loss": 1.2636,
        "grad_norm": 1.9888689517974854,
        "learning_rate": 7.25602955900917e-06,
        "epoch": 0.8784878487848785,
        "step": 6832
    },
    {
        "loss": 1.3378,
        "grad_norm": 2.5478527545928955,
        "learning_rate": 7.240900980265253e-06,
        "epoch": 0.8786164330718786,
        "step": 6833
    },
    {
        "loss": 1.8116,
        "grad_norm": 2.0880589485168457,
        "learning_rate": 7.225787596811051e-06,
        "epoch": 0.8787450173588788,
        "step": 6834
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.9712235927581787,
        "learning_rate": 7.2106894111223176e-06,
        "epoch": 0.8788736016458789,
        "step": 6835
    },
    {
        "loss": 2.0382,
        "grad_norm": 2.811460494995117,
        "learning_rate": 7.195606425672385e-06,
        "epoch": 0.879002185932879,
        "step": 6836
    },
    {
        "loss": 1.6726,
        "grad_norm": 2.3936848640441895,
        "learning_rate": 7.180538642932066e-06,
        "epoch": 0.8791307702198792,
        "step": 6837
    },
    {
        "loss": 1.9454,
        "grad_norm": 1.9194884300231934,
        "learning_rate": 7.165486065369653e-06,
        "epoch": 0.8792593545068793,
        "step": 6838
    },
    {
        "loss": 1.4171,
        "grad_norm": 2.0944299697875977,
        "learning_rate": 7.150448695451006e-06,
        "epoch": 0.8793879387938794,
        "step": 6839
    },
    {
        "loss": 1.9339,
        "grad_norm": 1.8107746839523315,
        "learning_rate": 7.135426535639455e-06,
        "epoch": 0.8795165230808795,
        "step": 6840
    },
    {
        "loss": 1.5754,
        "grad_norm": 1.9738867282867432,
        "learning_rate": 7.120419588395855e-06,
        "epoch": 0.8796451073678796,
        "step": 6841
    },
    {
        "loss": 1.8341,
        "grad_norm": 2.1725685596466064,
        "learning_rate": 7.105427856178548e-06,
        "epoch": 0.8797736916548797,
        "step": 6842
    },
    {
        "loss": 2.2753,
        "grad_norm": 1.2926784753799438,
        "learning_rate": 7.0904513414434045e-06,
        "epoch": 0.8799022759418799,
        "step": 6843
    },
    {
        "loss": 1.9122,
        "grad_norm": 2.107408285140991,
        "learning_rate": 7.075490046643818e-06,
        "epoch": 0.88003086022888,
        "step": 6844
    },
    {
        "loss": 1.5371,
        "grad_norm": 2.5840892791748047,
        "learning_rate": 7.060543974230627e-06,
        "epoch": 0.8801594445158801,
        "step": 6845
    },
    {
        "loss": 1.6359,
        "grad_norm": 1.9288562536239624,
        "learning_rate": 7.045613126652228e-06,
        "epoch": 0.8802880288028803,
        "step": 6846
    },
    {
        "loss": 0.932,
        "grad_norm": 4.07468843460083,
        "learning_rate": 7.030697506354534e-06,
        "epoch": 0.8804166130898804,
        "step": 6847
    },
    {
        "loss": 1.6079,
        "grad_norm": 1.8895750045776367,
        "learning_rate": 7.015797115780909e-06,
        "epoch": 0.8805451973768805,
        "step": 6848
    },
    {
        "loss": 1.2616,
        "grad_norm": 4.637692928314209,
        "learning_rate": 7.0009119573722805e-06,
        "epoch": 0.8806737816638807,
        "step": 6849
    },
    {
        "loss": 1.6071,
        "grad_norm": 1.6746394634246826,
        "learning_rate": 6.986042033567031e-06,
        "epoch": 0.8808023659508808,
        "step": 6850
    },
    {
        "loss": 1.9424,
        "grad_norm": 2.188967227935791,
        "learning_rate": 6.971187346801089e-06,
        "epoch": 0.8809309502378809,
        "step": 6851
    },
    {
        "loss": 2.1356,
        "grad_norm": 2.803656578063965,
        "learning_rate": 6.956347899507854e-06,
        "epoch": 0.8810595345248811,
        "step": 6852
    },
    {
        "loss": 2.1246,
        "grad_norm": 2.1415982246398926,
        "learning_rate": 6.941523694118257e-06,
        "epoch": 0.8811881188118812,
        "step": 6853
    },
    {
        "loss": 1.1879,
        "grad_norm": 3.1312620639801025,
        "learning_rate": 6.926714733060713e-06,
        "epoch": 0.8813167030988813,
        "step": 6854
    },
    {
        "loss": 2.1211,
        "grad_norm": 1.7920962572097778,
        "learning_rate": 6.911921018761136e-06,
        "epoch": 0.8814452873858815,
        "step": 6855
    },
    {
        "loss": 1.6701,
        "grad_norm": 1.912108063697815,
        "learning_rate": 6.897142553642955e-06,
        "epoch": 0.8815738716728816,
        "step": 6856
    },
    {
        "loss": 1.2426,
        "grad_norm": 1.9583768844604492,
        "learning_rate": 6.8823793401271206e-06,
        "epoch": 0.8817024559598817,
        "step": 6857
    },
    {
        "loss": 1.7268,
        "grad_norm": 2.537436008453369,
        "learning_rate": 6.867631380632022e-06,
        "epoch": 0.8818310402468819,
        "step": 6858
    },
    {
        "loss": 1.9609,
        "grad_norm": 2.0432968139648438,
        "learning_rate": 6.852898677573616e-06,
        "epoch": 0.881959624533882,
        "step": 6859
    },
    {
        "loss": 1.5959,
        "grad_norm": 2.55183744430542,
        "learning_rate": 6.838181233365326e-06,
        "epoch": 0.8820882088208821,
        "step": 6860
    },
    {
        "loss": 2.1659,
        "grad_norm": 2.421205997467041,
        "learning_rate": 6.823479050418102e-06,
        "epoch": 0.8822167931078823,
        "step": 6861
    },
    {
        "loss": 1.9702,
        "grad_norm": 2.0012755393981934,
        "learning_rate": 6.808792131140318e-06,
        "epoch": 0.8823453773948824,
        "step": 6862
    },
    {
        "loss": 1.3209,
        "grad_norm": 1.9603697061538696,
        "learning_rate": 6.794120477937993e-06,
        "epoch": 0.8824739616818825,
        "step": 6863
    },
    {
        "loss": 2.3149,
        "grad_norm": 1.545936107635498,
        "learning_rate": 6.7794640932145045e-06,
        "epoch": 0.8826025459688827,
        "step": 6864
    },
    {
        "loss": 1.4164,
        "grad_norm": 2.4074788093566895,
        "learning_rate": 6.764822979370755e-06,
        "epoch": 0.8827311302558827,
        "step": 6865
    },
    {
        "loss": 2.1767,
        "grad_norm": 1.2638683319091797,
        "learning_rate": 6.750197138805226e-06,
        "epoch": 0.8828597145428828,
        "step": 6866
    },
    {
        "loss": 1.7508,
        "grad_norm": 1.9579447507858276,
        "learning_rate": 6.735586573913821e-06,
        "epoch": 0.882988298829883,
        "step": 6867
    },
    {
        "loss": 1.9716,
        "grad_norm": 2.431516647338867,
        "learning_rate": 6.720991287089951e-06,
        "epoch": 0.8831168831168831,
        "step": 6868
    },
    {
        "loss": 2.2537,
        "grad_norm": 1.6633435487747192,
        "learning_rate": 6.706411280724567e-06,
        "epoch": 0.8832454674038832,
        "step": 6869
    },
    {
        "loss": 2.1307,
        "grad_norm": 2.106222152709961,
        "learning_rate": 6.6918465572060605e-06,
        "epoch": 0.8833740516908833,
        "step": 6870
    },
    {
        "loss": 1.7079,
        "grad_norm": 1.8859243392944336,
        "learning_rate": 6.677297118920345e-06,
        "epoch": 0.8835026359778835,
        "step": 6871
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.8394510746002197,
        "learning_rate": 6.6627629682508356e-06,
        "epoch": 0.8836312202648836,
        "step": 6872
    },
    {
        "loss": 1.9035,
        "grad_norm": 2.4764740467071533,
        "learning_rate": 6.64824410757845e-06,
        "epoch": 0.8837598045518837,
        "step": 6873
    },
    {
        "loss": 2.0724,
        "grad_norm": 2.1456756591796875,
        "learning_rate": 6.633740539281585e-06,
        "epoch": 0.8838883888388839,
        "step": 6874
    },
    {
        "loss": 1.6597,
        "grad_norm": 2.2372796535491943,
        "learning_rate": 6.619252265736098e-06,
        "epoch": 0.884016973125884,
        "step": 6875
    },
    {
        "loss": 1.6815,
        "grad_norm": 2.5058176517486572,
        "learning_rate": 6.604779289315444e-06,
        "epoch": 0.8841455574128841,
        "step": 6876
    },
    {
        "loss": 1.3831,
        "grad_norm": 2.141345500946045,
        "learning_rate": 6.590321612390449e-06,
        "epoch": 0.8842741416998843,
        "step": 6877
    },
    {
        "loss": 1.7717,
        "grad_norm": 2.1747841835021973,
        "learning_rate": 6.575879237329519e-06,
        "epoch": 0.8844027259868844,
        "step": 6878
    },
    {
        "loss": 1.52,
        "grad_norm": 2.135957956314087,
        "learning_rate": 6.561452166498538e-06,
        "epoch": 0.8845313102738845,
        "step": 6879
    },
    {
        "loss": 2.0015,
        "grad_norm": 1.6669344902038574,
        "learning_rate": 6.54704040226084e-06,
        "epoch": 0.8846598945608847,
        "step": 6880
    },
    {
        "loss": 1.3408,
        "grad_norm": 1.7489140033721924,
        "learning_rate": 6.53264394697729e-06,
        "epoch": 0.8847884788478848,
        "step": 6881
    },
    {
        "loss": 1.1745,
        "grad_norm": 2.3115577697753906,
        "learning_rate": 6.5182628030062365e-06,
        "epoch": 0.8849170631348849,
        "step": 6882
    },
    {
        "loss": 2.328,
        "grad_norm": 1.709203839302063,
        "learning_rate": 6.503896972703549e-06,
        "epoch": 0.8850456474218851,
        "step": 6883
    },
    {
        "loss": 1.8101,
        "grad_norm": 2.64801025390625,
        "learning_rate": 6.489546458422524e-06,
        "epoch": 0.8851742317088852,
        "step": 6884
    },
    {
        "loss": 1.9708,
        "grad_norm": 1.3666080236434937,
        "learning_rate": 6.47521126251398e-06,
        "epoch": 0.8853028159958853,
        "step": 6885
    },
    {
        "loss": 2.1746,
        "grad_norm": 1.4211101531982422,
        "learning_rate": 6.460891387326273e-06,
        "epoch": 0.8854314002828855,
        "step": 6886
    },
    {
        "loss": 1.8368,
        "grad_norm": 2.0821776390075684,
        "learning_rate": 6.44658683520516e-06,
        "epoch": 0.8855599845698856,
        "step": 6887
    },
    {
        "loss": 0.8406,
        "grad_norm": 2.4840376377105713,
        "learning_rate": 6.432297608493964e-06,
        "epoch": 0.8856885688568857,
        "step": 6888
    },
    {
        "loss": 2.1022,
        "grad_norm": 1.7499312162399292,
        "learning_rate": 6.418023709533471e-06,
        "epoch": 0.8858171531438859,
        "step": 6889
    },
    {
        "loss": 1.2919,
        "grad_norm": 1.145209789276123,
        "learning_rate": 6.403765140661921e-06,
        "epoch": 0.8859457374308859,
        "step": 6890
    },
    {
        "loss": 1.5184,
        "grad_norm": 1.8229111433029175,
        "learning_rate": 6.38952190421509e-06,
        "epoch": 0.886074321717886,
        "step": 6891
    },
    {
        "loss": 2.2284,
        "grad_norm": 1.8501598834991455,
        "learning_rate": 6.3752940025262444e-06,
        "epoch": 0.8862029060048862,
        "step": 6892
    },
    {
        "loss": 1.4247,
        "grad_norm": 2.194922924041748,
        "learning_rate": 6.36108143792612e-06,
        "epoch": 0.8863314902918863,
        "step": 6893
    },
    {
        "loss": 1.5198,
        "grad_norm": 1.6836744546890259,
        "learning_rate": 6.346884212742921e-06,
        "epoch": 0.8864600745788864,
        "step": 6894
    },
    {
        "loss": 1.6154,
        "grad_norm": 2.5066680908203125,
        "learning_rate": 6.332702329302364e-06,
        "epoch": 0.8865886588658866,
        "step": 6895
    },
    {
        "loss": 2.494,
        "grad_norm": 1.3167452812194824,
        "learning_rate": 6.318535789927671e-06,
        "epoch": 0.8867172431528867,
        "step": 6896
    },
    {
        "loss": 1.8524,
        "grad_norm": 2.1429545879364014,
        "learning_rate": 6.304384596939494e-06,
        "epoch": 0.8868458274398868,
        "step": 6897
    },
    {
        "loss": 0.6201,
        "grad_norm": 2.721632242202759,
        "learning_rate": 6.290248752656025e-06,
        "epoch": 0.886974411726887,
        "step": 6898
    },
    {
        "loss": 1.0413,
        "grad_norm": 1.2437504529953003,
        "learning_rate": 6.276128259392933e-06,
        "epoch": 0.8871029960138871,
        "step": 6899
    },
    {
        "loss": 1.8012,
        "grad_norm": 2.107908010482788,
        "learning_rate": 6.262023119463312e-06,
        "epoch": 0.8872315803008872,
        "step": 6900
    },
    {
        "eval_loss": 1.7389401197433472,
        "eval_runtime": 28.2704,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.8872315803008872,
        "step": 6900
    },
    {
        "loss": 1.5893,
        "grad_norm": 1.6458462476730347,
        "learning_rate": 6.247933335177836e-06,
        "epoch": 0.8873601645878874,
        "step": 6901
    },
    {
        "loss": 2.0305,
        "grad_norm": 2.4195849895477295,
        "learning_rate": 6.233858908844592e-06,
        "epoch": 0.8874887488748875,
        "step": 6902
    },
    {
        "loss": 1.9227,
        "grad_norm": 1.1610842943191528,
        "learning_rate": 6.219799842769203e-06,
        "epoch": 0.8876173331618876,
        "step": 6903
    },
    {
        "loss": 1.6244,
        "grad_norm": 1.6397267580032349,
        "learning_rate": 6.205756139254726e-06,
        "epoch": 0.8877459174488878,
        "step": 6904
    },
    {
        "loss": 1.3071,
        "grad_norm": 3.2945847511291504,
        "learning_rate": 6.19172780060171e-06,
        "epoch": 0.8878745017358879,
        "step": 6905
    },
    {
        "loss": 1.9367,
        "grad_norm": 2.019617795944214,
        "learning_rate": 6.17771482910825e-06,
        "epoch": 0.888003086022888,
        "step": 6906
    },
    {
        "loss": 0.4705,
        "grad_norm": 1.554510474205017,
        "learning_rate": 6.163717227069799e-06,
        "epoch": 0.8881316703098882,
        "step": 6907
    },
    {
        "loss": 1.863,
        "grad_norm": 2.6437580585479736,
        "learning_rate": 6.149734996779455e-06,
        "epoch": 0.8882602545968883,
        "step": 6908
    },
    {
        "loss": 1.0095,
        "grad_norm": 1.7505760192871094,
        "learning_rate": 6.135768140527642e-06,
        "epoch": 0.8883888388838884,
        "step": 6909
    },
    {
        "loss": 1.5996,
        "grad_norm": 1.8281079530715942,
        "learning_rate": 6.121816660602364e-06,
        "epoch": 0.8885174231708886,
        "step": 6910
    },
    {
        "loss": 1.5778,
        "grad_norm": 2.071629524230957,
        "learning_rate": 6.107880559289092e-06,
        "epoch": 0.8886460074578887,
        "step": 6911
    },
    {
        "loss": 2.0089,
        "grad_norm": 2.632477045059204,
        "learning_rate": 6.093959838870722e-06,
        "epoch": 0.8887745917448888,
        "step": 6912
    },
    {
        "loss": 1.3919,
        "grad_norm": 1.9307842254638672,
        "learning_rate": 6.080054501627719e-06,
        "epoch": 0.888903176031889,
        "step": 6913
    },
    {
        "loss": 1.8683,
        "grad_norm": 3.1376888751983643,
        "learning_rate": 6.066164549837905e-06,
        "epoch": 0.8890317603188891,
        "step": 6914
    },
    {
        "loss": 1.636,
        "grad_norm": 2.8492660522460938,
        "learning_rate": 6.052289985776749e-06,
        "epoch": 0.8891603446058891,
        "step": 6915
    },
    {
        "loss": 1.5584,
        "grad_norm": 1.7211146354675293,
        "learning_rate": 6.0384308117170444e-06,
        "epoch": 0.8892889288928892,
        "step": 6916
    },
    {
        "loss": 0.7449,
        "grad_norm": 1.7309494018554688,
        "learning_rate": 6.024587029929118e-06,
        "epoch": 0.8894175131798894,
        "step": 6917
    },
    {
        "loss": 1.9453,
        "grad_norm": 2.158139944076538,
        "learning_rate": 6.010758642680836e-06,
        "epoch": 0.8895460974668895,
        "step": 6918
    },
    {
        "loss": 2.1421,
        "grad_norm": 2.302612066268921,
        "learning_rate": 5.9969456522374514e-06,
        "epoch": 0.8896746817538896,
        "step": 6919
    },
    {
        "loss": 1.8891,
        "grad_norm": 1.9700253009796143,
        "learning_rate": 5.983148060861732e-06,
        "epoch": 0.8898032660408898,
        "step": 6920
    },
    {
        "loss": 2.4127,
        "grad_norm": 2.2179553508758545,
        "learning_rate": 5.969365870813959e-06,
        "epoch": 0.8899318503278899,
        "step": 6921
    },
    {
        "loss": 1.3942,
        "grad_norm": 1.509329915046692,
        "learning_rate": 5.955599084351804e-06,
        "epoch": 0.89006043461489,
        "step": 6922
    },
    {
        "loss": 1.5304,
        "grad_norm": 2.2863569259643555,
        "learning_rate": 5.941847703730497e-06,
        "epoch": 0.8901890189018902,
        "step": 6923
    },
    {
        "loss": 1.9375,
        "grad_norm": 1.5709408521652222,
        "learning_rate": 5.928111731202712e-06,
        "epoch": 0.8903176031888903,
        "step": 6924
    },
    {
        "loss": 1.4273,
        "grad_norm": 2.8421730995178223,
        "learning_rate": 5.914391169018607e-06,
        "epoch": 0.8904461874758904,
        "step": 6925
    },
    {
        "loss": 1.6624,
        "grad_norm": 3.774383783340454,
        "learning_rate": 5.900686019425794e-06,
        "epoch": 0.8905747717628906,
        "step": 6926
    },
    {
        "loss": 2.054,
        "grad_norm": 2.7752976417541504,
        "learning_rate": 5.886996284669377e-06,
        "epoch": 0.8907033560498907,
        "step": 6927
    },
    {
        "loss": 1.7143,
        "grad_norm": 1.3556665182113647,
        "learning_rate": 5.873321966991951e-06,
        "epoch": 0.8908319403368908,
        "step": 6928
    },
    {
        "loss": 0.9669,
        "grad_norm": 2.178027868270874,
        "learning_rate": 5.859663068633559e-06,
        "epoch": 0.890960524623891,
        "step": 6929
    },
    {
        "loss": 1.6774,
        "grad_norm": 2.0291895866394043,
        "learning_rate": 5.8460195918317085e-06,
        "epoch": 0.8910891089108911,
        "step": 6930
    },
    {
        "loss": 1.8932,
        "grad_norm": 2.7493486404418945,
        "learning_rate": 5.8323915388214355e-06,
        "epoch": 0.8912176931978912,
        "step": 6931
    },
    {
        "loss": 1.5972,
        "grad_norm": 2.5917141437530518,
        "learning_rate": 5.8187789118351875e-06,
        "epoch": 0.8913462774848914,
        "step": 6932
    },
    {
        "loss": 2.3297,
        "grad_norm": 1.662157416343689,
        "learning_rate": 5.805181713102914e-06,
        "epoch": 0.8914748617718915,
        "step": 6933
    },
    {
        "loss": 1.0904,
        "grad_norm": 1.8326717615127563,
        "learning_rate": 5.791599944852033e-06,
        "epoch": 0.8916034460588916,
        "step": 6934
    },
    {
        "loss": 2.0981,
        "grad_norm": 2.6850478649139404,
        "learning_rate": 5.778033609307454e-06,
        "epoch": 0.8917320303458918,
        "step": 6935
    },
    {
        "loss": 2.2385,
        "grad_norm": 1.879631757736206,
        "learning_rate": 5.764482708691521e-06,
        "epoch": 0.8918606146328919,
        "step": 6936
    },
    {
        "loss": 0.655,
        "grad_norm": 1.9462075233459473,
        "learning_rate": 5.7509472452240585e-06,
        "epoch": 0.891989198919892,
        "step": 6937
    },
    {
        "loss": 2.1519,
        "grad_norm": 2.40419602394104,
        "learning_rate": 5.737427221122416e-06,
        "epoch": 0.8921177832068922,
        "step": 6938
    },
    {
        "loss": 1.6008,
        "grad_norm": 2.1573519706726074,
        "learning_rate": 5.72392263860132e-06,
        "epoch": 0.8922463674938923,
        "step": 6939
    },
    {
        "loss": 1.8928,
        "grad_norm": 1.6102999448776245,
        "learning_rate": 5.710433499873047e-06,
        "epoch": 0.8923749517808923,
        "step": 6940
    },
    {
        "loss": 2.2957,
        "grad_norm": 1.5106993913650513,
        "learning_rate": 5.696959807147317e-06,
        "epoch": 0.8925035360678925,
        "step": 6941
    },
    {
        "loss": 1.521,
        "grad_norm": 1.0299873352050781,
        "learning_rate": 5.683501562631288e-06,
        "epoch": 0.8926321203548926,
        "step": 6942
    },
    {
        "loss": 1.4083,
        "grad_norm": 1.5379559993743896,
        "learning_rate": 5.67005876852964e-06,
        "epoch": 0.8927607046418927,
        "step": 6943
    },
    {
        "loss": 1.5501,
        "grad_norm": 2.7559070587158203,
        "learning_rate": 5.6566314270444895e-06,
        "epoch": 0.8928892889288929,
        "step": 6944
    },
    {
        "loss": 1.6549,
        "grad_norm": 2.3009748458862305,
        "learning_rate": 5.643219540375444e-06,
        "epoch": 0.893017873215893,
        "step": 6945
    },
    {
        "loss": 0.6393,
        "grad_norm": 1.5337238311767578,
        "learning_rate": 5.629823110719545e-06,
        "epoch": 0.8931464575028931,
        "step": 6946
    },
    {
        "loss": 1.4316,
        "grad_norm": 2.7283804416656494,
        "learning_rate": 5.616442140271339e-06,
        "epoch": 0.8932750417898933,
        "step": 6947
    },
    {
        "loss": 0.9511,
        "grad_norm": 1.9622629880905151,
        "learning_rate": 5.603076631222826e-06,
        "epoch": 0.8934036260768934,
        "step": 6948
    },
    {
        "loss": 1.1148,
        "grad_norm": 1.6851614713668823,
        "learning_rate": 5.589726585763433e-06,
        "epoch": 0.8935322103638935,
        "step": 6949
    },
    {
        "loss": 1.5995,
        "grad_norm": 1.1356476545333862,
        "learning_rate": 5.576392006080144e-06,
        "epoch": 0.8936607946508937,
        "step": 6950
    },
    {
        "loss": 1.6832,
        "grad_norm": 2.483455181121826,
        "learning_rate": 5.5630728943573466e-06,
        "epoch": 0.8937893789378938,
        "step": 6951
    },
    {
        "loss": 1.8467,
        "grad_norm": 2.0442750453948975,
        "learning_rate": 5.549769252776871e-06,
        "epoch": 0.8939179632248939,
        "step": 6952
    },
    {
        "loss": 1.7201,
        "grad_norm": 1.9586598873138428,
        "learning_rate": 5.5364810835180635e-06,
        "epoch": 0.894046547511894,
        "step": 6953
    },
    {
        "loss": 2.0792,
        "grad_norm": 1.5296595096588135,
        "learning_rate": 5.523208388757728e-06,
        "epoch": 0.8941751317988942,
        "step": 6954
    },
    {
        "loss": 1.4754,
        "grad_norm": 2.1389055252075195,
        "learning_rate": 5.5099511706701465e-06,
        "epoch": 0.8943037160858943,
        "step": 6955
    },
    {
        "loss": 1.8445,
        "grad_norm": 4.255430698394775,
        "learning_rate": 5.496709431426994e-06,
        "epoch": 0.8944323003728945,
        "step": 6956
    },
    {
        "loss": 1.4195,
        "grad_norm": 2.2830684185028076,
        "learning_rate": 5.48348317319749e-06,
        "epoch": 0.8945608846598946,
        "step": 6957
    },
    {
        "loss": 1.4974,
        "grad_norm": 2.8247432708740234,
        "learning_rate": 5.470272398148291e-06,
        "epoch": 0.8946894689468947,
        "step": 6958
    },
    {
        "loss": 1.7798,
        "grad_norm": 2.633310317993164,
        "learning_rate": 5.457077108443487e-06,
        "epoch": 0.8948180532338949,
        "step": 6959
    },
    {
        "loss": 1.4794,
        "grad_norm": 2.371192216873169,
        "learning_rate": 5.443897306244694e-06,
        "epoch": 0.894946637520895,
        "step": 6960
    },
    {
        "loss": 2.0905,
        "grad_norm": 1.7065377235412598,
        "learning_rate": 5.43073299371093e-06,
        "epoch": 0.8950752218078951,
        "step": 6961
    },
    {
        "loss": 1.4352,
        "grad_norm": 2.008904457092285,
        "learning_rate": 5.417584172998713e-06,
        "epoch": 0.8952038060948952,
        "step": 6962
    },
    {
        "loss": 1.0719,
        "grad_norm": 2.0752358436584473,
        "learning_rate": 5.4044508462620215e-06,
        "epoch": 0.8953323903818954,
        "step": 6963
    },
    {
        "loss": 1.9416,
        "grad_norm": 1.966017246246338,
        "learning_rate": 5.3913330156522556e-06,
        "epoch": 0.8954609746688955,
        "step": 6964
    },
    {
        "loss": 1.4475,
        "grad_norm": 2.0779261589050293,
        "learning_rate": 5.37823068331833e-06,
        "epoch": 0.8955895589558955,
        "step": 6965
    },
    {
        "loss": 1.7113,
        "grad_norm": 1.5043002367019653,
        "learning_rate": 5.365143851406585e-06,
        "epoch": 0.8957181432428957,
        "step": 6966
    },
    {
        "loss": 1.4449,
        "grad_norm": 2.6024856567382812,
        "learning_rate": 5.352072522060858e-06,
        "epoch": 0.8958467275298958,
        "step": 6967
    },
    {
        "loss": 2.3757,
        "grad_norm": 1.7643883228302002,
        "learning_rate": 5.339016697422383e-06,
        "epoch": 0.8959753118168959,
        "step": 6968
    },
    {
        "loss": 1.6629,
        "grad_norm": 1.8259096145629883,
        "learning_rate": 5.325976379629916e-06,
        "epoch": 0.8961038961038961,
        "step": 6969
    },
    {
        "loss": 1.5571,
        "grad_norm": 1.995214581489563,
        "learning_rate": 5.312951570819669e-06,
        "epoch": 0.8962324803908962,
        "step": 6970
    },
    {
        "loss": 2.1718,
        "grad_norm": 1.8322434425354004,
        "learning_rate": 5.299942273125258e-06,
        "epoch": 0.8963610646778963,
        "step": 6971
    },
    {
        "loss": 1.3111,
        "grad_norm": 2.2593119144439697,
        "learning_rate": 5.286948488677812e-06,
        "epoch": 0.8964896489648965,
        "step": 6972
    },
    {
        "loss": 0.8765,
        "grad_norm": 1.6318343877792358,
        "learning_rate": 5.273970219605928e-06,
        "epoch": 0.8966182332518966,
        "step": 6973
    },
    {
        "loss": 1.6833,
        "grad_norm": 2.2147316932678223,
        "learning_rate": 5.2610074680355835e-06,
        "epoch": 0.8967468175388967,
        "step": 6974
    },
    {
        "loss": 1.7258,
        "grad_norm": 1.9668691158294678,
        "learning_rate": 5.248060236090302e-06,
        "epoch": 0.8968754018258969,
        "step": 6975
    },
    {
        "loss": 1.1479,
        "grad_norm": 1.5753341913223267,
        "learning_rate": 5.2351285258910195e-06,
        "epoch": 0.897003986112897,
        "step": 6976
    },
    {
        "loss": 1.6042,
        "grad_norm": 2.108995199203491,
        "learning_rate": 5.222212339556143e-06,
        "epoch": 0.8971325703998971,
        "step": 6977
    },
    {
        "loss": 1.1608,
        "grad_norm": 2.796748399734497,
        "learning_rate": 5.209311679201523e-06,
        "epoch": 0.8972611546868973,
        "step": 6978
    },
    {
        "loss": 1.9087,
        "grad_norm": 2.375702142715454,
        "learning_rate": 5.1964265469404694e-06,
        "epoch": 0.8973897389738974,
        "step": 6979
    },
    {
        "loss": 1.5316,
        "grad_norm": 2.7794508934020996,
        "learning_rate": 5.183556944883783e-06,
        "epoch": 0.8975183232608975,
        "step": 6980
    },
    {
        "loss": 1.2709,
        "grad_norm": 2.282165050506592,
        "learning_rate": 5.170702875139665e-06,
        "epoch": 0.8976469075478977,
        "step": 6981
    },
    {
        "loss": 2.2207,
        "grad_norm": 2.247170925140381,
        "learning_rate": 5.1578643398138e-06,
        "epoch": 0.8977754918348978,
        "step": 6982
    },
    {
        "loss": 1.8482,
        "grad_norm": 1.8166426420211792,
        "learning_rate": 5.145041341009349e-06,
        "epoch": 0.8979040761218979,
        "step": 6983
    },
    {
        "loss": 0.8663,
        "grad_norm": 2.445345640182495,
        "learning_rate": 5.132233880826875e-06,
        "epoch": 0.8980326604088981,
        "step": 6984
    },
    {
        "loss": 1.6118,
        "grad_norm": 2.266788959503174,
        "learning_rate": 5.119441961364435e-06,
        "epoch": 0.8981612446958982,
        "step": 6985
    },
    {
        "loss": 1.6248,
        "grad_norm": 1.7316875457763672,
        "learning_rate": 5.1066655847175515e-06,
        "epoch": 0.8982898289828983,
        "step": 6986
    },
    {
        "loss": 1.0291,
        "grad_norm": 2.0856738090515137,
        "learning_rate": 5.0939047529791726e-06,
        "epoch": 0.8984184132698985,
        "step": 6987
    },
    {
        "loss": 1.9935,
        "grad_norm": 1.5948615074157715,
        "learning_rate": 5.081159468239693e-06,
        "epoch": 0.8985469975568986,
        "step": 6988
    },
    {
        "loss": 1.9023,
        "grad_norm": 2.326854944229126,
        "learning_rate": 5.068429732586988e-06,
        "epoch": 0.8986755818438987,
        "step": 6989
    },
    {
        "loss": 1.4937,
        "grad_norm": 2.676894187927246,
        "learning_rate": 5.055715548106377e-06,
        "epoch": 0.8988041661308988,
        "step": 6990
    },
    {
        "loss": 1.7607,
        "grad_norm": 2.31408953666687,
        "learning_rate": 5.043016916880616e-06,
        "epoch": 0.8989327504178989,
        "step": 6991
    },
    {
        "loss": 1.896,
        "grad_norm": 2.8108468055725098,
        "learning_rate": 5.0303338409899205e-06,
        "epoch": 0.899061334704899,
        "step": 6992
    },
    {
        "loss": 1.1049,
        "grad_norm": 1.0968849658966064,
        "learning_rate": 5.017666322512005e-06,
        "epoch": 0.8991899189918992,
        "step": 6993
    },
    {
        "loss": 2.1677,
        "grad_norm": 2.319725751876831,
        "learning_rate": 5.005014363521932e-06,
        "epoch": 0.8993185032788993,
        "step": 6994
    },
    {
        "loss": 1.6482,
        "grad_norm": 2.123332977294922,
        "learning_rate": 4.992377966092321e-06,
        "epoch": 0.8994470875658994,
        "step": 6995
    },
    {
        "loss": 1.416,
        "grad_norm": 1.8928637504577637,
        "learning_rate": 4.979757132293173e-06,
        "epoch": 0.8995756718528995,
        "step": 6996
    },
    {
        "loss": 0.9738,
        "grad_norm": 2.695800542831421,
        "learning_rate": 4.967151864191999e-06,
        "epoch": 0.8997042561398997,
        "step": 6997
    },
    {
        "loss": 1.7205,
        "grad_norm": 1.7710453271865845,
        "learning_rate": 4.954562163853682e-06,
        "epoch": 0.8998328404268998,
        "step": 6998
    },
    {
        "loss": 2.1018,
        "grad_norm": 1.6564124822616577,
        "learning_rate": 4.941988033340617e-06,
        "epoch": 0.8999614247139,
        "step": 6999
    },
    {
        "loss": 2.1701,
        "grad_norm": 2.0655064582824707,
        "learning_rate": 4.929429474712643e-06,
        "epoch": 0.9000900090009001,
        "step": 7000
    },
    {
        "eval_loss": 1.7385673522949219,
        "eval_runtime": 28.2716,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.9000900090009001,
        "step": 7000
    },
    {
        "loss": 1.0745,
        "grad_norm": 3.1815690994262695,
        "learning_rate": 4.916886490026995e-06,
        "epoch": 0.9002185932879002,
        "step": 7001
    },
    {
        "loss": 1.3025,
        "grad_norm": 1.760266661643982,
        "learning_rate": 4.90435908133845e-06,
        "epoch": 0.9003471775749003,
        "step": 7002
    },
    {
        "loss": 1.6202,
        "grad_norm": 2.606496810913086,
        "learning_rate": 4.891847250699155e-06,
        "epoch": 0.9004757618619005,
        "step": 7003
    },
    {
        "loss": 1.945,
        "grad_norm": 2.175541400909424,
        "learning_rate": 4.879351000158694e-06,
        "epoch": 0.9006043461489006,
        "step": 7004
    },
    {
        "loss": 1.4831,
        "grad_norm": 2.0028305053710938,
        "learning_rate": 4.866870331764195e-06,
        "epoch": 0.9007329304359007,
        "step": 7005
    },
    {
        "loss": 1.577,
        "grad_norm": 2.1246092319488525,
        "learning_rate": 4.854405247560145e-06,
        "epoch": 0.9008615147229009,
        "step": 7006
    },
    {
        "loss": 1.4722,
        "grad_norm": 2.076584577560425,
        "learning_rate": 4.8419557495884896e-06,
        "epoch": 0.900990099009901,
        "step": 7007
    },
    {
        "loss": 1.7987,
        "grad_norm": 1.3655877113342285,
        "learning_rate": 4.829521839888673e-06,
        "epoch": 0.9011186832969011,
        "step": 7008
    },
    {
        "loss": 2.1397,
        "grad_norm": 1.3764232397079468,
        "learning_rate": 4.8171035204975345e-06,
        "epoch": 0.9012472675839013,
        "step": 7009
    },
    {
        "loss": 2.188,
        "grad_norm": 2.2503905296325684,
        "learning_rate": 4.8047007934493685e-06,
        "epoch": 0.9013758518709014,
        "step": 7010
    },
    {
        "loss": 1.7424,
        "grad_norm": 2.276624917984009,
        "learning_rate": 4.792313660775916e-06,
        "epoch": 0.9015044361579015,
        "step": 7011
    },
    {
        "loss": 1.763,
        "grad_norm": 2.516815185546875,
        "learning_rate": 4.779942124506398e-06,
        "epoch": 0.9016330204449017,
        "step": 7012
    },
    {
        "loss": 2.0211,
        "grad_norm": 2.8670239448547363,
        "learning_rate": 4.767586186667417e-06,
        "epoch": 0.9017616047319018,
        "step": 7013
    },
    {
        "loss": 2.1551,
        "grad_norm": 1.7027016878128052,
        "learning_rate": 4.755245849283086e-06,
        "epoch": 0.9018901890189019,
        "step": 7014
    },
    {
        "loss": 1.2083,
        "grad_norm": 1.6511154174804688,
        "learning_rate": 4.742921114374921e-06,
        "epoch": 0.902018773305902,
        "step": 7015
    },
    {
        "loss": 2.1598,
        "grad_norm": 2.298398494720459,
        "learning_rate": 4.7306119839618745e-06,
        "epoch": 0.9021473575929021,
        "step": 7016
    },
    {
        "loss": 0.8773,
        "grad_norm": 2.783784866333008,
        "learning_rate": 4.718318460060389e-06,
        "epoch": 0.9022759418799022,
        "step": 7017
    },
    {
        "loss": 1.5436,
        "grad_norm": 2.413896083831787,
        "learning_rate": 4.7060405446842985e-06,
        "epoch": 0.9024045261669024,
        "step": 7018
    },
    {
        "loss": 1.2676,
        "grad_norm": 1.3117002248764038,
        "learning_rate": 4.693778239844937e-06,
        "epoch": 0.9025331104539025,
        "step": 7019
    },
    {
        "loss": 1.8341,
        "grad_norm": 2.619046449661255,
        "learning_rate": 4.681531547551021e-06,
        "epoch": 0.9026616947409026,
        "step": 7020
    },
    {
        "loss": 0.6916,
        "grad_norm": 2.3936712741851807,
        "learning_rate": 4.6693004698087354e-06,
        "epoch": 0.9027902790279028,
        "step": 7021
    },
    {
        "loss": 1.5147,
        "grad_norm": 2.1576626300811768,
        "learning_rate": 4.657085008621731e-06,
        "epoch": 0.9029188633149029,
        "step": 7022
    },
    {
        "loss": 1.9472,
        "grad_norm": 1.5048723220825195,
        "learning_rate": 4.644885165991053e-06,
        "epoch": 0.903047447601903,
        "step": 7023
    },
    {
        "loss": 1.7875,
        "grad_norm": 2.160996437072754,
        "learning_rate": 4.6327009439152245e-06,
        "epoch": 0.9031760318889032,
        "step": 7024
    },
    {
        "loss": 1.1827,
        "grad_norm": 1.8299702405929565,
        "learning_rate": 4.620532344390205e-06,
        "epoch": 0.9033046161759033,
        "step": 7025
    },
    {
        "loss": 2.0718,
        "grad_norm": 2.08713698387146,
        "learning_rate": 4.608379369409377e-06,
        "epoch": 0.9034332004629034,
        "step": 7026
    },
    {
        "loss": 2.1394,
        "grad_norm": 2.0405259132385254,
        "learning_rate": 4.59624202096357e-06,
        "epoch": 0.9035617847499036,
        "step": 7027
    },
    {
        "loss": 1.9729,
        "grad_norm": 1.8477466106414795,
        "learning_rate": 4.584120301041072e-06,
        "epoch": 0.9036903690369037,
        "step": 7028
    },
    {
        "loss": 2.0694,
        "grad_norm": 2.008263349533081,
        "learning_rate": 4.572014211627607e-06,
        "epoch": 0.9038189533239038,
        "step": 7029
    },
    {
        "loss": 2.1381,
        "grad_norm": 2.120088577270508,
        "learning_rate": 4.5599237547062965e-06,
        "epoch": 0.903947537610904,
        "step": 7030
    },
    {
        "loss": 2.4151,
        "grad_norm": 1.7221322059631348,
        "learning_rate": 4.547848932257748e-06,
        "epoch": 0.9040761218979041,
        "step": 7031
    },
    {
        "loss": 1.1558,
        "grad_norm": 2.4187171459198,
        "learning_rate": 4.535789746260011e-06,
        "epoch": 0.9042047061849042,
        "step": 7032
    },
    {
        "loss": 2.3308,
        "grad_norm": 1.7895127534866333,
        "learning_rate": 4.523746198688527e-06,
        "epoch": 0.9043332904719044,
        "step": 7033
    },
    {
        "loss": 2.1396,
        "grad_norm": 1.7107539176940918,
        "learning_rate": 4.511718291516209e-06,
        "epoch": 0.9044618747589045,
        "step": 7034
    },
    {
        "loss": 2.6629,
        "grad_norm": 2.018958330154419,
        "learning_rate": 4.499706026713435e-06,
        "epoch": 0.9045904590459046,
        "step": 7035
    },
    {
        "loss": 1.1229,
        "grad_norm": 3.0089480876922607,
        "learning_rate": 4.487709406247953e-06,
        "epoch": 0.9047190433329048,
        "step": 7036
    },
    {
        "loss": 1.9052,
        "grad_norm": 1.8224735260009766,
        "learning_rate": 4.475728432084991e-06,
        "epoch": 0.9048476276199049,
        "step": 7037
    },
    {
        "loss": 0.8601,
        "grad_norm": 1.7879247665405273,
        "learning_rate": 4.463763106187202e-06,
        "epoch": 0.904976211906905,
        "step": 7038
    },
    {
        "loss": 1.9747,
        "grad_norm": 2.5877723693847656,
        "learning_rate": 4.4518134305147175e-06,
        "epoch": 0.9051047961939052,
        "step": 7039
    },
    {
        "loss": 1.5216,
        "grad_norm": 1.769215703010559,
        "learning_rate": 4.439879407025027e-06,
        "epoch": 0.9052333804809052,
        "step": 7040
    },
    {
        "loss": 1.6175,
        "grad_norm": 1.6015515327453613,
        "learning_rate": 4.4279610376731005e-06,
        "epoch": 0.9053619647679053,
        "step": 7041
    },
    {
        "loss": 2.0825,
        "grad_norm": 2.0479235649108887,
        "learning_rate": 4.416058324411376e-06,
        "epoch": 0.9054905490549054,
        "step": 7042
    },
    {
        "loss": 1.5956,
        "grad_norm": 2.5951626300811768,
        "learning_rate": 4.404171269189628e-06,
        "epoch": 0.9056191333419056,
        "step": 7043
    },
    {
        "loss": 1.5945,
        "grad_norm": 1.8772822618484497,
        "learning_rate": 4.392299873955208e-06,
        "epoch": 0.9057477176289057,
        "step": 7044
    },
    {
        "loss": 1.3452,
        "grad_norm": 1.935042142868042,
        "learning_rate": 4.380444140652762e-06,
        "epoch": 0.9058763019159058,
        "step": 7045
    },
    {
        "loss": 2.235,
        "grad_norm": 1.227473258972168,
        "learning_rate": 4.368604071224447e-06,
        "epoch": 0.906004886202906,
        "step": 7046
    },
    {
        "loss": 1.6589,
        "grad_norm": 1.60921311378479,
        "learning_rate": 4.356779667609856e-06,
        "epoch": 0.9061334704899061,
        "step": 7047
    },
    {
        "loss": 1.456,
        "grad_norm": 1.7470521926879883,
        "learning_rate": 4.344970931745973e-06,
        "epoch": 0.9062620547769062,
        "step": 7048
    },
    {
        "loss": 1.1352,
        "grad_norm": 2.2167210578918457,
        "learning_rate": 4.333177865567273e-06,
        "epoch": 0.9063906390639064,
        "step": 7049
    },
    {
        "loss": 2.028,
        "grad_norm": 1.8915811777114868,
        "learning_rate": 4.321400471005587e-06,
        "epoch": 0.9065192233509065,
        "step": 7050
    },
    {
        "loss": 1.4763,
        "grad_norm": 1.8959429264068604,
        "learning_rate": 4.309638749990264e-06,
        "epoch": 0.9066478076379066,
        "step": 7051
    },
    {
        "loss": 1.4097,
        "grad_norm": 3.2095377445220947,
        "learning_rate": 4.297892704448025e-06,
        "epoch": 0.9067763919249068,
        "step": 7052
    },
    {
        "loss": 0.9347,
        "grad_norm": 1.6283928155899048,
        "learning_rate": 4.286162336303035e-06,
        "epoch": 0.9069049762119069,
        "step": 7053
    },
    {
        "loss": 1.0559,
        "grad_norm": 2.1547656059265137,
        "learning_rate": 4.274447647476931e-06,
        "epoch": 0.907033560498907,
        "step": 7054
    },
    {
        "loss": 2.2913,
        "grad_norm": 1.4326128959655762,
        "learning_rate": 4.262748639888736e-06,
        "epoch": 0.9071621447859072,
        "step": 7055
    },
    {
        "loss": 1.2571,
        "grad_norm": 1.6731067895889282,
        "learning_rate": 4.251065315454883e-06,
        "epoch": 0.9072907290729073,
        "step": 7056
    },
    {
        "loss": 1.5092,
        "grad_norm": 2.8841323852539062,
        "learning_rate": 4.239397676089318e-06,
        "epoch": 0.9074193133599074,
        "step": 7057
    },
    {
        "loss": 1.4349,
        "grad_norm": 2.3629891872406006,
        "learning_rate": 4.227745723703347e-06,
        "epoch": 0.9075478976469076,
        "step": 7058
    },
    {
        "loss": 1.8133,
        "grad_norm": 2.1036698818206787,
        "learning_rate": 4.21610946020573e-06,
        "epoch": 0.9076764819339077,
        "step": 7059
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.698119044303894,
        "learning_rate": 4.204488887502667e-06,
        "epoch": 0.9078050662209078,
        "step": 7060
    },
    {
        "loss": 1.267,
        "grad_norm": 2.3335537910461426,
        "learning_rate": 4.192884007497777e-06,
        "epoch": 0.907933650507908,
        "step": 7061
    },
    {
        "loss": 1.6745,
        "grad_norm": 2.4910974502563477,
        "learning_rate": 4.181294822092085e-06,
        "epoch": 0.9080622347949081,
        "step": 7062
    },
    {
        "loss": 0.8321,
        "grad_norm": 2.792400598526001,
        "learning_rate": 4.169721333184084e-06,
        "epoch": 0.9081908190819082,
        "step": 7063
    },
    {
        "loss": 1.3023,
        "grad_norm": 3.1231319904327393,
        "learning_rate": 4.158163542669691e-06,
        "epoch": 0.9083194033689084,
        "step": 7064
    },
    {
        "loss": 2.2897,
        "grad_norm": 1.755092740058899,
        "learning_rate": 4.146621452442223e-06,
        "epoch": 0.9084479876559084,
        "step": 7065
    },
    {
        "loss": 1.6182,
        "grad_norm": 2.514087200164795,
        "learning_rate": 4.135095064392447e-06,
        "epoch": 0.9085765719429085,
        "step": 7066
    },
    {
        "loss": 1.1831,
        "grad_norm": 4.095390319824219,
        "learning_rate": 4.123584380408562e-06,
        "epoch": 0.9087051562299087,
        "step": 7067
    },
    {
        "loss": 2.1953,
        "grad_norm": 2.658207893371582,
        "learning_rate": 4.1120894023761625e-06,
        "epoch": 0.9088337405169088,
        "step": 7068
    },
    {
        "loss": 1.9864,
        "grad_norm": 2.559512138366699,
        "learning_rate": 4.1006101321783175e-06,
        "epoch": 0.9089623248039089,
        "step": 7069
    },
    {
        "loss": 1.4733,
        "grad_norm": 2.2844207286834717,
        "learning_rate": 4.089146571695479e-06,
        "epoch": 0.9090909090909091,
        "step": 7070
    },
    {
        "loss": 1.9272,
        "grad_norm": 1.7530217170715332,
        "learning_rate": 4.0776987228055785e-06,
        "epoch": 0.9092194933779092,
        "step": 7071
    },
    {
        "loss": 1.7576,
        "grad_norm": 2.1220595836639404,
        "learning_rate": 4.0662665873838935e-06,
        "epoch": 0.9093480776649093,
        "step": 7072
    },
    {
        "loss": 1.5676,
        "grad_norm": 1.8852893114089966,
        "learning_rate": 4.054850167303203e-06,
        "epoch": 0.9094766619519095,
        "step": 7073
    },
    {
        "loss": 1.6814,
        "grad_norm": 1.8029506206512451,
        "learning_rate": 4.043449464433691e-06,
        "epoch": 0.9096052462389096,
        "step": 7074
    },
    {
        "loss": 1.7427,
        "grad_norm": 2.3350985050201416,
        "learning_rate": 4.0320644806429385e-06,
        "epoch": 0.9097338305259097,
        "step": 7075
    },
    {
        "loss": 2.0961,
        "grad_norm": 1.6840301752090454,
        "learning_rate": 4.020695217795966e-06,
        "epoch": 0.9098624148129099,
        "step": 7076
    },
    {
        "loss": 1.4049,
        "grad_norm": 1.4352985620498657,
        "learning_rate": 4.009341677755252e-06,
        "epoch": 0.90999099909991,
        "step": 7077
    },
    {
        "loss": 1.5105,
        "grad_norm": 2.7693991661071777,
        "learning_rate": 3.998003862380639e-06,
        "epoch": 0.9101195833869101,
        "step": 7078
    },
    {
        "loss": 1.9202,
        "grad_norm": 1.9304094314575195,
        "learning_rate": 3.986681773529444e-06,
        "epoch": 0.9102481676739103,
        "step": 7079
    },
    {
        "loss": 2.2134,
        "grad_norm": 2.571701765060425,
        "learning_rate": 3.975375413056393e-06,
        "epoch": 0.9103767519609104,
        "step": 7080
    },
    {
        "loss": 1.302,
        "grad_norm": 2.948845148086548,
        "learning_rate": 3.964084782813638e-06,
        "epoch": 0.9105053362479105,
        "step": 7081
    },
    {
        "loss": 1.9206,
        "grad_norm": 2.371793508529663,
        "learning_rate": 3.952809884650721e-06,
        "epoch": 0.9106339205349107,
        "step": 7082
    },
    {
        "loss": 2.1863,
        "grad_norm": 1.447021245956421,
        "learning_rate": 3.941550720414655e-06,
        "epoch": 0.9107625048219108,
        "step": 7083
    },
    {
        "loss": 1.0515,
        "grad_norm": 2.051124334335327,
        "learning_rate": 3.930307291949864e-06,
        "epoch": 0.9108910891089109,
        "step": 7084
    },
    {
        "loss": 2.5258,
        "grad_norm": 1.4773032665252686,
        "learning_rate": 3.919079601098141e-06,
        "epoch": 0.9110196733959111,
        "step": 7085
    },
    {
        "loss": 1.6965,
        "grad_norm": 2.423011541366577,
        "learning_rate": 3.907867649698804e-06,
        "epoch": 0.9111482576829112,
        "step": 7086
    },
    {
        "loss": 2.0133,
        "grad_norm": 1.657578468322754,
        "learning_rate": 3.896671439588506e-06,
        "epoch": 0.9112768419699113,
        "step": 7087
    },
    {
        "loss": 1.2513,
        "grad_norm": 1.879273772239685,
        "learning_rate": 3.885490972601335e-06,
        "epoch": 0.9114054262569115,
        "step": 7088
    },
    {
        "loss": 2.1726,
        "grad_norm": 1.8316062688827515,
        "learning_rate": 3.874326250568838e-06,
        "epoch": 0.9115340105439116,
        "step": 7089
    },
    {
        "loss": 1.6436,
        "grad_norm": 2.336024761199951,
        "learning_rate": 3.86317727531994e-06,
        "epoch": 0.9116625948309116,
        "step": 7090
    },
    {
        "loss": 1.7942,
        "grad_norm": 2.677088975906372,
        "learning_rate": 3.852044048681025e-06,
        "epoch": 0.9117911791179117,
        "step": 7091
    },
    {
        "loss": 1.2099,
        "grad_norm": 2.3270998001098633,
        "learning_rate": 3.840926572475867e-06,
        "epoch": 0.9119197634049119,
        "step": 7092
    },
    {
        "loss": 2.1598,
        "grad_norm": 1.6922821998596191,
        "learning_rate": 3.829824848525654e-06,
        "epoch": 0.912048347691912,
        "step": 7093
    },
    {
        "loss": 1.9256,
        "grad_norm": 2.2830307483673096,
        "learning_rate": 3.818738878649053e-06,
        "epoch": 0.9121769319789121,
        "step": 7094
    },
    {
        "loss": 2.0239,
        "grad_norm": 1.3723251819610596,
        "learning_rate": 3.807668664662045e-06,
        "epoch": 0.9123055162659123,
        "step": 7095
    },
    {
        "loss": 1.6911,
        "grad_norm": 1.3493762016296387,
        "learning_rate": 3.7966142083781575e-06,
        "epoch": 0.9124341005529124,
        "step": 7096
    },
    {
        "loss": 1.6788,
        "grad_norm": 2.7943406105041504,
        "learning_rate": 3.7855755116082302e-06,
        "epoch": 0.9125626848399125,
        "step": 7097
    },
    {
        "loss": 1.0188,
        "grad_norm": 2.449573278427124,
        "learning_rate": 3.7745525761605616e-06,
        "epoch": 0.9126912691269127,
        "step": 7098
    },
    {
        "loss": 1.0815,
        "grad_norm": 2.4487104415893555,
        "learning_rate": 3.763545403840896e-06,
        "epoch": 0.9128198534139128,
        "step": 7099
    },
    {
        "loss": 0.953,
        "grad_norm": 2.2873423099517822,
        "learning_rate": 3.752553996452346e-06,
        "epoch": 0.9129484377009129,
        "step": 7100
    },
    {
        "eval_loss": 1.7383503913879395,
        "eval_runtime": 28.2455,
        "eval_samples_per_second": 2.797,
        "eval_steps_per_second": 2.797,
        "epoch": 0.9129484377009129,
        "step": 7100
    },
    {
        "loss": 2.2265,
        "grad_norm": 2.146827459335327,
        "learning_rate": 3.741578355795472e-06,
        "epoch": 0.9130770219879131,
        "step": 7101
    },
    {
        "loss": 1.4863,
        "grad_norm": 1.6744215488433838,
        "learning_rate": 3.730618483668247e-06,
        "epoch": 0.9132056062749132,
        "step": 7102
    },
    {
        "loss": 1.0021,
        "grad_norm": 1.8789342641830444,
        "learning_rate": 3.7196743818660453e-06,
        "epoch": 0.9133341905619133,
        "step": 7103
    },
    {
        "loss": 1.68,
        "grad_norm": 2.055748224258423,
        "learning_rate": 3.7087460521816775e-06,
        "epoch": 0.9134627748489135,
        "step": 7104
    },
    {
        "loss": 1.3717,
        "grad_norm": 2.2081453800201416,
        "learning_rate": 3.6978334964053565e-06,
        "epoch": 0.9135913591359136,
        "step": 7105
    },
    {
        "loss": 1.417,
        "grad_norm": 1.9095375537872314,
        "learning_rate": 3.68693671632474e-06,
        "epoch": 0.9137199434229137,
        "step": 7106
    },
    {
        "loss": 1.6606,
        "grad_norm": 2.6379568576812744,
        "learning_rate": 3.6760557137248684e-06,
        "epoch": 0.9138485277099139,
        "step": 7107
    },
    {
        "loss": 1.4744,
        "grad_norm": 2.244145631790161,
        "learning_rate": 3.6651904903881707e-06,
        "epoch": 0.913977111996914,
        "step": 7108
    },
    {
        "loss": 1.144,
        "grad_norm": 2.330821990966797,
        "learning_rate": 3.6543410480945896e-06,
        "epoch": 0.9141056962839141,
        "step": 7109
    },
    {
        "loss": 1.4433,
        "grad_norm": 2.460472822189331,
        "learning_rate": 3.643507388621381e-06,
        "epoch": 0.9142342805709143,
        "step": 7110
    },
    {
        "loss": 2.1221,
        "grad_norm": 2.357928991317749,
        "learning_rate": 3.6326895137432595e-06,
        "epoch": 0.9143628648579144,
        "step": 7111
    },
    {
        "loss": 1.3553,
        "grad_norm": 2.852281332015991,
        "learning_rate": 3.621887425232373e-06,
        "epoch": 0.9144914491449145,
        "step": 7112
    },
    {
        "loss": 1.4913,
        "grad_norm": 1.8306670188903809,
        "learning_rate": 3.6111011248582403e-06,
        "epoch": 0.9146200334319147,
        "step": 7113
    },
    {
        "loss": 2.188,
        "grad_norm": 2.247767448425293,
        "learning_rate": 3.600330614387826e-06,
        "epoch": 0.9147486177189148,
        "step": 7114
    },
    {
        "loss": 1.9757,
        "grad_norm": 1.9897336959838867,
        "learning_rate": 3.5895758955854732e-06,
        "epoch": 0.9148772020059148,
        "step": 7115
    },
    {
        "loss": 1.6175,
        "grad_norm": 1.8563976287841797,
        "learning_rate": 3.5788369702130063e-06,
        "epoch": 0.915005786292915,
        "step": 7116
    },
    {
        "loss": 1.4447,
        "grad_norm": 3.5219411849975586,
        "learning_rate": 3.568113840029563e-06,
        "epoch": 0.9151343705799151,
        "step": 7117
    },
    {
        "loss": 1.7728,
        "grad_norm": 1.6038380861282349,
        "learning_rate": 3.557406506791783e-06,
        "epoch": 0.9152629548669152,
        "step": 7118
    },
    {
        "loss": 1.5899,
        "grad_norm": 1.6264941692352295,
        "learning_rate": 3.546714972253684e-06,
        "epoch": 0.9153915391539154,
        "step": 7119
    },
    {
        "loss": 1.7831,
        "grad_norm": 2.2372405529022217,
        "learning_rate": 3.5360392381666774e-06,
        "epoch": 0.9155201234409155,
        "step": 7120
    },
    {
        "loss": 2.1488,
        "grad_norm": 1.8710289001464844,
        "learning_rate": 3.5253793062796194e-06,
        "epoch": 0.9156487077279156,
        "step": 7121
    },
    {
        "loss": 1.5291,
        "grad_norm": 2.137507200241089,
        "learning_rate": 3.514735178338746e-06,
        "epoch": 0.9157772920149158,
        "step": 7122
    },
    {
        "loss": 1.4922,
        "grad_norm": 2.6149113178253174,
        "learning_rate": 3.504106856087741e-06,
        "epoch": 0.9159058763019159,
        "step": 7123
    },
    {
        "loss": 1.7278,
        "grad_norm": 2.4894003868103027,
        "learning_rate": 3.493494341267645e-06,
        "epoch": 0.916034460588916,
        "step": 7124
    },
    {
        "loss": 1.5105,
        "grad_norm": 1.4761801958084106,
        "learning_rate": 3.4828976356169772e-06,
        "epoch": 0.9161630448759162,
        "step": 7125
    },
    {
        "loss": 2.132,
        "grad_norm": 1.7063963413238525,
        "learning_rate": 3.4723167408716286e-06,
        "epoch": 0.9162916291629163,
        "step": 7126
    },
    {
        "loss": 1.7794,
        "grad_norm": 2.505701780319214,
        "learning_rate": 3.4617516587648778e-06,
        "epoch": 0.9164202134499164,
        "step": 7127
    },
    {
        "loss": 1.7091,
        "grad_norm": 2.198483943939209,
        "learning_rate": 3.4512023910274527e-06,
        "epoch": 0.9165487977369166,
        "step": 7128
    },
    {
        "loss": 1.2697,
        "grad_norm": 1.5367892980575562,
        "learning_rate": 3.4406689393874926e-06,
        "epoch": 0.9166773820239167,
        "step": 7129
    },
    {
        "loss": 1.3627,
        "grad_norm": 2.5680065155029297,
        "learning_rate": 3.430151305570506e-06,
        "epoch": 0.9168059663109168,
        "step": 7130
    },
    {
        "loss": 0.7139,
        "grad_norm": 1.7165530920028687,
        "learning_rate": 3.419649491299437e-06,
        "epoch": 0.916934550597917,
        "step": 7131
    },
    {
        "loss": 1.3028,
        "grad_norm": 1.962698221206665,
        "learning_rate": 3.4091634982946542e-06,
        "epoch": 0.9170631348849171,
        "step": 7132
    },
    {
        "loss": 1.8288,
        "grad_norm": 1.9675796031951904,
        "learning_rate": 3.398693328273905e-06,
        "epoch": 0.9171917191719172,
        "step": 7133
    },
    {
        "loss": 0.9972,
        "grad_norm": 2.5084075927734375,
        "learning_rate": 3.388238982952352e-06,
        "epoch": 0.9173203034589174,
        "step": 7134
    },
    {
        "loss": 1.1105,
        "grad_norm": 2.0927090644836426,
        "learning_rate": 3.3778004640425686e-06,
        "epoch": 0.9174488877459175,
        "step": 7135
    },
    {
        "loss": 1.4824,
        "grad_norm": 2.008678674697876,
        "learning_rate": 3.367377773254554e-06,
        "epoch": 0.9175774720329176,
        "step": 7136
    },
    {
        "loss": 1.612,
        "grad_norm": 3.270494222640991,
        "learning_rate": 3.356970912295654e-06,
        "epoch": 0.9177060563199178,
        "step": 7137
    },
    {
        "loss": 2.065,
        "grad_norm": 2.602029323577881,
        "learning_rate": 3.346579882870715e-06,
        "epoch": 0.9178346406069179,
        "step": 7138
    },
    {
        "loss": 2.1355,
        "grad_norm": 2.1723623275756836,
        "learning_rate": 3.3362046866819206e-06,
        "epoch": 0.917963224893918,
        "step": 7139
    },
    {
        "loss": 1.8233,
        "grad_norm": 2.1738078594207764,
        "learning_rate": 3.3258453254288447e-06,
        "epoch": 0.918091809180918,
        "step": 7140
    },
    {
        "loss": 1.38,
        "grad_norm": 2.4381797313690186,
        "learning_rate": 3.3155018008085624e-06,
        "epoch": 0.9182203934679182,
        "step": 7141
    },
    {
        "loss": 1.5555,
        "grad_norm": 2.7576918601989746,
        "learning_rate": 3.305174114515441e-06,
        "epoch": 0.9183489777549183,
        "step": 7142
    },
    {
        "loss": 2.0285,
        "grad_norm": 1.9917908906936646,
        "learning_rate": 3.2948622682413277e-06,
        "epoch": 0.9184775620419184,
        "step": 7143
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.0691722631454468,
        "learning_rate": 3.2845662636754702e-06,
        "epoch": 0.9186061463289186,
        "step": 7144
    },
    {
        "loss": 1.3636,
        "grad_norm": 2.6265900135040283,
        "learning_rate": 3.2742861025044648e-06,
        "epoch": 0.9187347306159187,
        "step": 7145
    },
    {
        "loss": 1.5309,
        "grad_norm": 2.4706459045410156,
        "learning_rate": 3.2640217864123967e-06,
        "epoch": 0.9188633149029188,
        "step": 7146
    },
    {
        "loss": 2.1933,
        "grad_norm": 1.621537446975708,
        "learning_rate": 3.253773317080655e-06,
        "epoch": 0.918991899189919,
        "step": 7147
    },
    {
        "loss": 1.4527,
        "grad_norm": 2.1268301010131836,
        "learning_rate": 3.24354069618813e-06,
        "epoch": 0.9191204834769191,
        "step": 7148
    },
    {
        "loss": 1.5692,
        "grad_norm": 2.1065053939819336,
        "learning_rate": 3.2333239254110802e-06,
        "epoch": 0.9192490677639192,
        "step": 7149
    },
    {
        "loss": 2.2707,
        "grad_norm": 2.064269542694092,
        "learning_rate": 3.2231230064231124e-06,
        "epoch": 0.9193776520509194,
        "step": 7150
    },
    {
        "loss": 0.7094,
        "grad_norm": 1.8652775287628174,
        "learning_rate": 3.212937940895333e-06,
        "epoch": 0.9195062363379195,
        "step": 7151
    },
    {
        "loss": 1.4111,
        "grad_norm": 2.347580909729004,
        "learning_rate": 3.2027687304961863e-06,
        "epoch": 0.9196348206249196,
        "step": 7152
    },
    {
        "loss": 1.9137,
        "grad_norm": 1.7818025350570679,
        "learning_rate": 3.192615376891528e-06,
        "epoch": 0.9197634049119198,
        "step": 7153
    },
    {
        "loss": 1.4287,
        "grad_norm": 1.5898761749267578,
        "learning_rate": 3.182477881744661e-06,
        "epoch": 0.9198919891989199,
        "step": 7154
    },
    {
        "loss": 1.8059,
        "grad_norm": 2.438337802886963,
        "learning_rate": 3.1723562467162017e-06,
        "epoch": 0.92002057348592,
        "step": 7155
    },
    {
        "loss": 1.5983,
        "grad_norm": 1.5542163848876953,
        "learning_rate": 3.1622504734642566e-06,
        "epoch": 0.9201491577729202,
        "step": 7156
    },
    {
        "loss": 1.9555,
        "grad_norm": 1.9031239748001099,
        "learning_rate": 3.152160563644291e-06,
        "epoch": 0.9202777420599203,
        "step": 7157
    },
    {
        "loss": 1.8608,
        "grad_norm": 1.957343578338623,
        "learning_rate": 3.142086518909182e-06,
        "epoch": 0.9204063263469204,
        "step": 7158
    },
    {
        "loss": 1.2621,
        "grad_norm": 2.58951997756958,
        "learning_rate": 3.132028340909199e-06,
        "epoch": 0.9205349106339206,
        "step": 7159
    },
    {
        "loss": 2.0048,
        "grad_norm": 1.9263089895248413,
        "learning_rate": 3.1219860312920122e-06,
        "epoch": 0.9206634949209207,
        "step": 7160
    },
    {
        "loss": 0.8823,
        "grad_norm": 1.7909377813339233,
        "learning_rate": 3.1119595917027067e-06,
        "epoch": 0.9207920792079208,
        "step": 7161
    },
    {
        "loss": 1.7273,
        "grad_norm": 2.37214994430542,
        "learning_rate": 3.1019490237837566e-06,
        "epoch": 0.920920663494921,
        "step": 7162
    },
    {
        "loss": 1.6912,
        "grad_norm": 2.175446033477783,
        "learning_rate": 3.09195432917504e-06,
        "epoch": 0.9210492477819211,
        "step": 7163
    },
    {
        "loss": 1.9137,
        "grad_norm": 1.7776193618774414,
        "learning_rate": 3.0819755095138346e-06,
        "epoch": 0.9211778320689212,
        "step": 7164
    },
    {
        "loss": 1.9049,
        "grad_norm": 2.088639974594116,
        "learning_rate": 3.072012566434823e-06,
        "epoch": 0.9213064163559213,
        "step": 7165
    },
    {
        "loss": 1.4887,
        "grad_norm": 3.6942315101623535,
        "learning_rate": 3.062065501570055e-06,
        "epoch": 0.9214350006429214,
        "step": 7166
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.4300504922866821,
        "learning_rate": 3.0521343165490268e-06,
        "epoch": 0.9215635849299215,
        "step": 7167
    },
    {
        "loss": 1.4434,
        "grad_norm": 2.330638885498047,
        "learning_rate": 3.0422190129986152e-06,
        "epoch": 0.9216921692169217,
        "step": 7168
    },
    {
        "loss": 2.6113,
        "grad_norm": 1.6901599168777466,
        "learning_rate": 3.0323195925430647e-06,
        "epoch": 0.9218207535039218,
        "step": 7169
    },
    {
        "loss": 2.1389,
        "grad_norm": 1.7943012714385986,
        "learning_rate": 3.0224360568040676e-06,
        "epoch": 0.9219493377909219,
        "step": 7170
    },
    {
        "loss": 0.7768,
        "grad_norm": 2.9857263565063477,
        "learning_rate": 3.0125684074006953e-06,
        "epoch": 0.922077922077922,
        "step": 7171
    },
    {
        "loss": 1.5702,
        "grad_norm": 1.7175147533416748,
        "learning_rate": 3.002716645949388e-06,
        "epoch": 0.9222065063649222,
        "step": 7172
    },
    {
        "loss": 1.9537,
        "grad_norm": 1.684759259223938,
        "learning_rate": 2.992880774064011e-06,
        "epoch": 0.9223350906519223,
        "step": 7173
    },
    {
        "loss": 0.3902,
        "grad_norm": 1.4522912502288818,
        "learning_rate": 2.983060793355841e-06,
        "epoch": 0.9224636749389225,
        "step": 7174
    },
    {
        "loss": 2.0993,
        "grad_norm": 1.6259757280349731,
        "learning_rate": 2.973256705433536e-06,
        "epoch": 0.9225922592259226,
        "step": 7175
    },
    {
        "loss": 0.4524,
        "grad_norm": 1.5415722131729126,
        "learning_rate": 2.9634685119031335e-06,
        "epoch": 0.9227208435129227,
        "step": 7176
    },
    {
        "loss": 1.8093,
        "grad_norm": 1.6823087930679321,
        "learning_rate": 2.9536962143680737e-06,
        "epoch": 0.9228494277999229,
        "step": 7177
    },
    {
        "loss": 1.8773,
        "grad_norm": 2.208430290222168,
        "learning_rate": 2.9439398144292307e-06,
        "epoch": 0.922978012086923,
        "step": 7178
    },
    {
        "loss": 1.9074,
        "grad_norm": 2.003255844116211,
        "learning_rate": 2.9341993136848047e-06,
        "epoch": 0.9231065963739231,
        "step": 7179
    },
    {
        "loss": 1.0442,
        "grad_norm": 2.194849967956543,
        "learning_rate": 2.9244747137304738e-06,
        "epoch": 0.9232351806609232,
        "step": 7180
    },
    {
        "loss": 1.1475,
        "grad_norm": 2.8969054222106934,
        "learning_rate": 2.9147660161592536e-06,
        "epoch": 0.9233637649479234,
        "step": 7181
    },
    {
        "loss": 2.1502,
        "grad_norm": 2.0976712703704834,
        "learning_rate": 2.9050732225615384e-06,
        "epoch": 0.9234923492349235,
        "step": 7182
    },
    {
        "loss": 1.8925,
        "grad_norm": 1.738655686378479,
        "learning_rate": 2.8953963345252022e-06,
        "epoch": 0.9236209335219236,
        "step": 7183
    },
    {
        "loss": 2.0057,
        "grad_norm": 1.7247424125671387,
        "learning_rate": 2.8857353536354217e-06,
        "epoch": 0.9237495178089238,
        "step": 7184
    },
    {
        "loss": 2.2333,
        "grad_norm": 2.7048981189727783,
        "learning_rate": 2.8760902814748302e-06,
        "epoch": 0.9238781020959239,
        "step": 7185
    },
    {
        "loss": 1.1267,
        "grad_norm": 2.676438808441162,
        "learning_rate": 2.866461119623409e-06,
        "epoch": 0.924006686382924,
        "step": 7186
    },
    {
        "loss": 1.6258,
        "grad_norm": 2.796069860458374,
        "learning_rate": 2.8568478696585633e-06,
        "epoch": 0.9241352706699242,
        "step": 7187
    },
    {
        "loss": 1.1279,
        "grad_norm": 2.332942008972168,
        "learning_rate": 2.847250533155088e-06,
        "epoch": 0.9242638549569243,
        "step": 7188
    },
    {
        "loss": 2.0495,
        "grad_norm": 1.8580052852630615,
        "learning_rate": 2.8376691116851483e-06,
        "epoch": 0.9243924392439244,
        "step": 7189
    },
    {
        "loss": 2.0575,
        "grad_norm": 2.5118355751037598,
        "learning_rate": 2.828103606818366e-06,
        "epoch": 0.9245210235309245,
        "step": 7190
    },
    {
        "loss": 1.871,
        "grad_norm": 2.467104196548462,
        "learning_rate": 2.8185540201216666e-06,
        "epoch": 0.9246496078179246,
        "step": 7191
    },
    {
        "loss": 1.6524,
        "grad_norm": 2.092844009399414,
        "learning_rate": 2.8090203531594084e-06,
        "epoch": 0.9247781921049247,
        "step": 7192
    },
    {
        "loss": 1.6709,
        "grad_norm": 2.0562567710876465,
        "learning_rate": 2.7995026074933762e-06,
        "epoch": 0.9249067763919249,
        "step": 7193
    },
    {
        "loss": 1.4977,
        "grad_norm": 2.377953052520752,
        "learning_rate": 2.7900007846826893e-06,
        "epoch": 0.925035360678925,
        "step": 7194
    },
    {
        "loss": 1.8069,
        "grad_norm": 2.4545717239379883,
        "learning_rate": 2.7805148862838915e-06,
        "epoch": 0.9251639449659251,
        "step": 7195
    },
    {
        "loss": 0.6839,
        "grad_norm": 2.2134790420532227,
        "learning_rate": 2.771044913850929e-06,
        "epoch": 0.9252925292529253,
        "step": 7196
    },
    {
        "loss": 2.1895,
        "grad_norm": 2.3176536560058594,
        "learning_rate": 2.7615908689351043e-06,
        "epoch": 0.9254211135399254,
        "step": 7197
    },
    {
        "loss": 1.626,
        "grad_norm": 3.186103343963623,
        "learning_rate": 2.7521527530851134e-06,
        "epoch": 0.9255496978269255,
        "step": 7198
    },
    {
        "loss": 1.1488,
        "grad_norm": 1.8727507591247559,
        "learning_rate": 2.742730567847085e-06,
        "epoch": 0.9256782821139257,
        "step": 7199
    },
    {
        "loss": 1.5966,
        "grad_norm": 2.828131914138794,
        "learning_rate": 2.733324314764496e-06,
        "epoch": 0.9258068664009258,
        "step": 7200
    },
    {
        "eval_loss": 1.7382186651229858,
        "eval_runtime": 28.3256,
        "eval_samples_per_second": 2.789,
        "eval_steps_per_second": 2.789,
        "epoch": 0.9258068664009258,
        "step": 7200
    },
    {
        "loss": 2.6078,
        "grad_norm": 1.4268971681594849,
        "learning_rate": 2.723933995378225e-06,
        "epoch": 0.9259354506879259,
        "step": 7201
    },
    {
        "loss": 0.937,
        "grad_norm": 2.563448190689087,
        "learning_rate": 2.7145596112265525e-06,
        "epoch": 0.9260640349749261,
        "step": 7202
    },
    {
        "loss": 1.4915,
        "grad_norm": 2.450018882751465,
        "learning_rate": 2.705201163845139e-06,
        "epoch": 0.9261926192619262,
        "step": 7203
    },
    {
        "loss": 2.3338,
        "grad_norm": 1.3101856708526611,
        "learning_rate": 2.6958586547670138e-06,
        "epoch": 0.9263212035489263,
        "step": 7204
    },
    {
        "loss": 1.2989,
        "grad_norm": 1.8993421792984009,
        "learning_rate": 2.6865320855226306e-06,
        "epoch": 0.9264497878359265,
        "step": 7205
    },
    {
        "loss": 2.1489,
        "grad_norm": 2.4047276973724365,
        "learning_rate": 2.6772214576398336e-06,
        "epoch": 0.9265783721229266,
        "step": 7206
    },
    {
        "loss": 2.3889,
        "grad_norm": 2.2712786197662354,
        "learning_rate": 2.667926772643814e-06,
        "epoch": 0.9267069564099267,
        "step": 7207
    },
    {
        "loss": 1.9688,
        "grad_norm": 2.9322798252105713,
        "learning_rate": 2.658648032057176e-06,
        "epoch": 0.9268355406969269,
        "step": 7208
    },
    {
        "loss": 1.8805,
        "grad_norm": 2.318251371383667,
        "learning_rate": 2.6493852373999373e-06,
        "epoch": 0.926964124983927,
        "step": 7209
    },
    {
        "loss": 1.4067,
        "grad_norm": 2.654768705368042,
        "learning_rate": 2.6401383901894614e-06,
        "epoch": 0.9270927092709271,
        "step": 7210
    },
    {
        "loss": 2.007,
        "grad_norm": 2.001098155975342,
        "learning_rate": 2.630907491940504e-06,
        "epoch": 0.9272212935579273,
        "step": 7211
    },
    {
        "loss": 1.3021,
        "grad_norm": 6.715257167816162,
        "learning_rate": 2.621692544165244e-06,
        "epoch": 0.9273498778449274,
        "step": 7212
    },
    {
        "loss": 2.063,
        "grad_norm": 1.8585573434829712,
        "learning_rate": 2.6124935483732182e-06,
        "epoch": 0.9274784621319275,
        "step": 7213
    },
    {
        "loss": 2.0212,
        "grad_norm": 2.2708492279052734,
        "learning_rate": 2.603310506071344e-06,
        "epoch": 0.9276070464189277,
        "step": 7214
    },
    {
        "loss": 2.531,
        "grad_norm": 1.7576923370361328,
        "learning_rate": 2.594143418763939e-06,
        "epoch": 0.9277356307059277,
        "step": 7215
    },
    {
        "loss": 1.5471,
        "grad_norm": 2.6123645305633545,
        "learning_rate": 2.5849922879527146e-06,
        "epoch": 0.9278642149929278,
        "step": 7216
    },
    {
        "loss": 1.7915,
        "grad_norm": 2.402148723602295,
        "learning_rate": 2.5758571151367706e-06,
        "epoch": 0.927992799279928,
        "step": 7217
    },
    {
        "loss": 1.464,
        "grad_norm": 2.296039342880249,
        "learning_rate": 2.566737901812555e-06,
        "epoch": 0.9281213835669281,
        "step": 7218
    },
    {
        "loss": 1.547,
        "grad_norm": 2.0276906490325928,
        "learning_rate": 2.5576346494739387e-06,
        "epoch": 0.9282499678539282,
        "step": 7219
    },
    {
        "loss": 1.684,
        "grad_norm": 1.95085608959198,
        "learning_rate": 2.5485473596121746e-06,
        "epoch": 0.9283785521409283,
        "step": 7220
    },
    {
        "loss": 1.1678,
        "grad_norm": 2.1758639812469482,
        "learning_rate": 2.539476033715871e-06,
        "epoch": 0.9285071364279285,
        "step": 7221
    },
    {
        "loss": 1.9047,
        "grad_norm": 2.300771713256836,
        "learning_rate": 2.5304206732710724e-06,
        "epoch": 0.9286357207149286,
        "step": 7222
    },
    {
        "loss": 2.1776,
        "grad_norm": 1.8134993314743042,
        "learning_rate": 2.521381279761159e-06,
        "epoch": 0.9287643050019287,
        "step": 7223
    },
    {
        "loss": 1.3346,
        "grad_norm": 2.8304619789123535,
        "learning_rate": 2.5123578546669246e-06,
        "epoch": 0.9288928892889289,
        "step": 7224
    },
    {
        "loss": 0.5869,
        "grad_norm": 1.9290450811386108,
        "learning_rate": 2.5033503994665197e-06,
        "epoch": 0.929021473575929,
        "step": 7225
    },
    {
        "loss": 1.8074,
        "grad_norm": 2.356308698654175,
        "learning_rate": 2.4943589156355194e-06,
        "epoch": 0.9291500578629291,
        "step": 7226
    },
    {
        "loss": 0.9767,
        "grad_norm": 2.4505038261413574,
        "learning_rate": 2.485383404646846e-06,
        "epoch": 0.9292786421499293,
        "step": 7227
    },
    {
        "loss": 2.0593,
        "grad_norm": 1.5686653852462769,
        "learning_rate": 2.476423867970823e-06,
        "epoch": 0.9294072264369294,
        "step": 7228
    },
    {
        "loss": 1.725,
        "grad_norm": 3.259091377258301,
        "learning_rate": 2.467480307075154e-06,
        "epoch": 0.9295358107239295,
        "step": 7229
    },
    {
        "loss": 1.339,
        "grad_norm": 1.8466078042984009,
        "learning_rate": 2.4585527234249227e-06,
        "epoch": 0.9296643950109297,
        "step": 7230
    },
    {
        "loss": 1.2685,
        "grad_norm": 1.8570008277893066,
        "learning_rate": 2.449641118482593e-06,
        "epoch": 0.9297929792979298,
        "step": 7231
    },
    {
        "loss": 2.5383,
        "grad_norm": 1.857110619544983,
        "learning_rate": 2.4407454937080297e-06,
        "epoch": 0.9299215635849299,
        "step": 7232
    },
    {
        "loss": 2.0403,
        "grad_norm": 1.781203031539917,
        "learning_rate": 2.431865850558446e-06,
        "epoch": 0.9300501478719301,
        "step": 7233
    },
    {
        "loss": 1.5705,
        "grad_norm": 2.1877846717834473,
        "learning_rate": 2.4230021904884547e-06,
        "epoch": 0.9301787321589302,
        "step": 7234
    },
    {
        "loss": 2.0018,
        "grad_norm": 1.7637766599655151,
        "learning_rate": 2.4141545149500843e-06,
        "epoch": 0.9303073164459303,
        "step": 7235
    },
    {
        "loss": 0.4197,
        "grad_norm": 2.4584317207336426,
        "learning_rate": 2.4053228253926864e-06,
        "epoch": 0.9304359007329305,
        "step": 7236
    },
    {
        "loss": 1.565,
        "grad_norm": 2.2045769691467285,
        "learning_rate": 2.3965071232630253e-06,
        "epoch": 0.9305644850199306,
        "step": 7237
    },
    {
        "loss": 1.8432,
        "grad_norm": 2.2212820053100586,
        "learning_rate": 2.3877074100052465e-06,
        "epoch": 0.9306930693069307,
        "step": 7238
    },
    {
        "loss": 1.8501,
        "grad_norm": 1.3834004402160645,
        "learning_rate": 2.3789236870608524e-06,
        "epoch": 0.9308216535939309,
        "step": 7239
    },
    {
        "loss": 1.4697,
        "grad_norm": 2.641838550567627,
        "learning_rate": 2.370155955868758e-06,
        "epoch": 0.9309502378809309,
        "step": 7240
    },
    {
        "loss": 1.6021,
        "grad_norm": 1.3313276767730713,
        "learning_rate": 2.3614042178652597e-06,
        "epoch": 0.931078822167931,
        "step": 7241
    },
    {
        "loss": 2.2932,
        "grad_norm": 1.6796739101409912,
        "learning_rate": 2.352668474483999e-06,
        "epoch": 0.9312074064549312,
        "step": 7242
    },
    {
        "loss": 2.3624,
        "grad_norm": 1.8924044370651245,
        "learning_rate": 2.34394872715602e-06,
        "epoch": 0.9313359907419313,
        "step": 7243
    },
    {
        "loss": 1.798,
        "grad_norm": 1.728808879852295,
        "learning_rate": 2.3352449773097584e-06,
        "epoch": 0.9314645750289314,
        "step": 7244
    },
    {
        "loss": 1.3223,
        "grad_norm": 2.1902754306793213,
        "learning_rate": 2.3265572263710067e-06,
        "epoch": 0.9315931593159316,
        "step": 7245
    },
    {
        "loss": 1.878,
        "grad_norm": 2.6957480907440186,
        "learning_rate": 2.3178854757629266e-06,
        "epoch": 0.9317217436029317,
        "step": 7246
    },
    {
        "loss": 1.536,
        "grad_norm": 1.6974984407424927,
        "learning_rate": 2.3092297269061036e-06,
        "epoch": 0.9318503278899318,
        "step": 7247
    },
    {
        "loss": 1.8045,
        "grad_norm": 2.3486123085021973,
        "learning_rate": 2.3005899812184706e-06,
        "epoch": 0.931978912176932,
        "step": 7248
    },
    {
        "loss": 1.4284,
        "grad_norm": 2.3929495811462402,
        "learning_rate": 2.2919662401153284e-06,
        "epoch": 0.9321074964639321,
        "step": 7249
    },
    {
        "loss": 1.5025,
        "grad_norm": 1.3558573722839355,
        "learning_rate": 2.283358505009381e-06,
        "epoch": 0.9322360807509322,
        "step": 7250
    },
    {
        "loss": 1.6802,
        "grad_norm": 2.819260597229004,
        "learning_rate": 2.2747667773106996e-06,
        "epoch": 0.9323646650379324,
        "step": 7251
    },
    {
        "loss": 1.7473,
        "grad_norm": 1.726304531097412,
        "learning_rate": 2.2661910584267474e-06,
        "epoch": 0.9324932493249325,
        "step": 7252
    },
    {
        "loss": 1.7106,
        "grad_norm": 1.7700212001800537,
        "learning_rate": 2.2576313497623236e-06,
        "epoch": 0.9326218336119326,
        "step": 7253
    },
    {
        "loss": 1.2933,
        "grad_norm": 1.3158730268478394,
        "learning_rate": 2.24908765271965e-06,
        "epoch": 0.9327504178989328,
        "step": 7254
    },
    {
        "loss": 1.8225,
        "grad_norm": 4.734935283660889,
        "learning_rate": 2.24055996869833e-06,
        "epoch": 0.9328790021859329,
        "step": 7255
    },
    {
        "loss": 1.6898,
        "grad_norm": 2.6617674827575684,
        "learning_rate": 2.2320482990952797e-06,
        "epoch": 0.933007586472933,
        "step": 7256
    },
    {
        "loss": 1.1905,
        "grad_norm": 2.28902530670166,
        "learning_rate": 2.2235526453048495e-06,
        "epoch": 0.9331361707599332,
        "step": 7257
    },
    {
        "loss": 1.7678,
        "grad_norm": 2.185363531112671,
        "learning_rate": 2.215073008718771e-06,
        "epoch": 0.9332647550469333,
        "step": 7258
    },
    {
        "loss": 2.0839,
        "grad_norm": 1.7472314834594727,
        "learning_rate": 2.206609390726111e-06,
        "epoch": 0.9333933393339334,
        "step": 7259
    },
    {
        "loss": 2.2761,
        "grad_norm": 1.6979584693908691,
        "learning_rate": 2.198161792713338e-06,
        "epoch": 0.9335219236209336,
        "step": 7260
    },
    {
        "loss": 1.5244,
        "grad_norm": 2.177013397216797,
        "learning_rate": 2.1897302160643007e-06,
        "epoch": 0.9336505079079337,
        "step": 7261
    },
    {
        "loss": 1.8913,
        "grad_norm": 2.425442934036255,
        "learning_rate": 2.1813146621602055e-06,
        "epoch": 0.9337790921949338,
        "step": 7262
    },
    {
        "loss": 1.5458,
        "grad_norm": 2.8873276710510254,
        "learning_rate": 2.1729151323796273e-06,
        "epoch": 0.933907676481934,
        "step": 7263
    },
    {
        "loss": 1.4126,
        "grad_norm": 2.183216094970703,
        "learning_rate": 2.1645316280985537e-06,
        "epoch": 0.9340362607689341,
        "step": 7264
    },
    {
        "loss": 1.3611,
        "grad_norm": 2.2678897380828857,
        "learning_rate": 2.156164150690332e-06,
        "epoch": 0.9341648450559341,
        "step": 7265
    },
    {
        "loss": 2.1446,
        "grad_norm": 1.7385480403900146,
        "learning_rate": 2.147812701525653e-06,
        "epoch": 0.9342934293429342,
        "step": 7266
    },
    {
        "loss": 2.304,
        "grad_norm": 1.819615125656128,
        "learning_rate": 2.139477281972613e-06,
        "epoch": 0.9344220136299344,
        "step": 7267
    },
    {
        "loss": 1.9405,
        "grad_norm": 2.551640748977661,
        "learning_rate": 2.1311578933966736e-06,
        "epoch": 0.9345505979169345,
        "step": 7268
    },
    {
        "loss": 1.5663,
        "grad_norm": 2.8518667221069336,
        "learning_rate": 2.12285453716069e-06,
        "epoch": 0.9346791822039346,
        "step": 7269
    },
    {
        "loss": 1.3775,
        "grad_norm": 2.1254279613494873,
        "learning_rate": 2.114567214624841e-06,
        "epoch": 0.9348077664909348,
        "step": 7270
    },
    {
        "loss": 1.6608,
        "grad_norm": 2.088541030883789,
        "learning_rate": 2.1062959271467398e-06,
        "epoch": 0.9349363507779349,
        "step": 7271
    },
    {
        "loss": 2.1976,
        "grad_norm": 1.2877675294876099,
        "learning_rate": 2.0980406760813256e-06,
        "epoch": 0.935064935064935,
        "step": 7272
    },
    {
        "loss": 0.8407,
        "grad_norm": 2.842564582824707,
        "learning_rate": 2.089801462780927e-06,
        "epoch": 0.9351935193519352,
        "step": 7273
    },
    {
        "loss": 1.0495,
        "grad_norm": 2.1208550930023193,
        "learning_rate": 2.0815782885952652e-06,
        "epoch": 0.9353221036389353,
        "step": 7274
    },
    {
        "loss": 2.3452,
        "grad_norm": 2.2477550506591797,
        "learning_rate": 2.0733711548714174e-06,
        "epoch": 0.9354506879259354,
        "step": 7275
    },
    {
        "loss": 1.9521,
        "grad_norm": 2.8790102005004883,
        "learning_rate": 2.0651800629537867e-06,
        "epoch": 0.9355792722129356,
        "step": 7276
    },
    {
        "loss": 1.0856,
        "grad_norm": 2.087733268737793,
        "learning_rate": 2.0570050141842544e-06,
        "epoch": 0.9357078564999357,
        "step": 7277
    },
    {
        "loss": 1.9854,
        "grad_norm": 2.2087769508361816,
        "learning_rate": 2.0488460099019725e-06,
        "epoch": 0.9358364407869358,
        "step": 7278
    },
    {
        "loss": 1.2909,
        "grad_norm": 2.210298776626587,
        "learning_rate": 2.0407030514435043e-06,
        "epoch": 0.935965025073936,
        "step": 7279
    },
    {
        "loss": 2.0305,
        "grad_norm": 1.9710863828659058,
        "learning_rate": 2.0325761401428166e-06,
        "epoch": 0.9360936093609361,
        "step": 7280
    },
    {
        "loss": 1.9401,
        "grad_norm": 1.8350188732147217,
        "learning_rate": 2.024465277331178e-06,
        "epoch": 0.9362221936479362,
        "step": 7281
    },
    {
        "loss": 2.1939,
        "grad_norm": 2.4969990253448486,
        "learning_rate": 2.016370464337303e-06,
        "epoch": 0.9363507779349364,
        "step": 7282
    },
    {
        "loss": 1.9145,
        "grad_norm": 2.012211561203003,
        "learning_rate": 2.0082917024871973e-06,
        "epoch": 0.9364793622219365,
        "step": 7283
    },
    {
        "loss": 1.267,
        "grad_norm": 1.4145183563232422,
        "learning_rate": 2.0002289931043138e-06,
        "epoch": 0.9366079465089366,
        "step": 7284
    },
    {
        "loss": 1.1792,
        "grad_norm": 2.541600227355957,
        "learning_rate": 1.9921823375094407e-06,
        "epoch": 0.9367365307959368,
        "step": 7285
    },
    {
        "loss": 1.4926,
        "grad_norm": 2.1125316619873047,
        "learning_rate": 1.9841517370207008e-06,
        "epoch": 0.9368651150829369,
        "step": 7286
    },
    {
        "loss": 1.2576,
        "grad_norm": 2.1960971355438232,
        "learning_rate": 1.9761371929536645e-06,
        "epoch": 0.936993699369937,
        "step": 7287
    },
    {
        "loss": 2.1452,
        "grad_norm": 1.8722021579742432,
        "learning_rate": 1.9681387066212142e-06,
        "epoch": 0.9371222836569372,
        "step": 7288
    },
    {
        "loss": 2.0258,
        "grad_norm": 1.5820497274398804,
        "learning_rate": 1.960156279333614e-06,
        "epoch": 0.9372508679439373,
        "step": 7289
    },
    {
        "loss": 1.9605,
        "grad_norm": 1.7394205331802368,
        "learning_rate": 1.9521899123985167e-06,
        "epoch": 0.9373794522309373,
        "step": 7290
    },
    {
        "loss": 2.3555,
        "grad_norm": 2.019198417663574,
        "learning_rate": 1.9442396071209014e-06,
        "epoch": 0.9375080365179375,
        "step": 7291
    },
    {
        "loss": 0.9847,
        "grad_norm": 1.3189804553985596,
        "learning_rate": 1.93630536480317e-06,
        "epoch": 0.9376366208049376,
        "step": 7292
    },
    {
        "loss": 1.2741,
        "grad_norm": 1.7980855703353882,
        "learning_rate": 1.928387186745062e-06,
        "epoch": 0.9377652050919377,
        "step": 7293
    },
    {
        "loss": 2.1578,
        "grad_norm": 1.7964510917663574,
        "learning_rate": 1.9204850742436943e-06,
        "epoch": 0.9378937893789379,
        "step": 7294
    },
    {
        "loss": 1.7858,
        "grad_norm": 2.122687339782715,
        "learning_rate": 1.91259902859352e-06,
        "epoch": 0.938022373665938,
        "step": 7295
    },
    {
        "loss": 1.5473,
        "grad_norm": 2.8432199954986572,
        "learning_rate": 1.9047290510864179e-06,
        "epoch": 0.9381509579529381,
        "step": 7296
    },
    {
        "loss": 1.2853,
        "grad_norm": 3.6983346939086914,
        "learning_rate": 1.8968751430116005e-06,
        "epoch": 0.9382795422399383,
        "step": 7297
    },
    {
        "loss": 1.6401,
        "grad_norm": 2.4525718688964844,
        "learning_rate": 1.8890373056556388e-06,
        "epoch": 0.9384081265269384,
        "step": 7298
    },
    {
        "loss": 1.5606,
        "grad_norm": 2.47804856300354,
        "learning_rate": 1.8812155403024944e-06,
        "epoch": 0.9385367108139385,
        "step": 7299
    },
    {
        "loss": 2.004,
        "grad_norm": 2.015796661376953,
        "learning_rate": 1.8734098482334983e-06,
        "epoch": 0.9386652951009387,
        "step": 7300
    },
    {
        "eval_loss": 1.7376606464385986,
        "eval_runtime": 28.2948,
        "eval_samples_per_second": 2.792,
        "eval_steps_per_second": 2.792,
        "epoch": 0.9386652951009387,
        "step": 7300
    },
    {
        "loss": 2.0597,
        "grad_norm": 1.5865951776504517,
        "learning_rate": 1.8656202307273051e-06,
        "epoch": 0.9387938793879388,
        "step": 7301
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.131910562515259,
        "learning_rate": 1.8578466890599834e-06,
        "epoch": 0.9389224636749389,
        "step": 7302
    },
    {
        "loss": 0.8877,
        "grad_norm": 1.7013384103775024,
        "learning_rate": 1.8500892245049584e-06,
        "epoch": 0.9390510479619391,
        "step": 7303
    },
    {
        "loss": 1.7845,
        "grad_norm": 1.6210737228393555,
        "learning_rate": 1.842347838333014e-06,
        "epoch": 0.9391796322489392,
        "step": 7304
    },
    {
        "loss": 1.4784,
        "grad_norm": 2.873577833175659,
        "learning_rate": 1.8346225318122911e-06,
        "epoch": 0.9393082165359393,
        "step": 7305
    },
    {
        "loss": 1.0738,
        "grad_norm": 2.414141893386841,
        "learning_rate": 1.826913306208322e-06,
        "epoch": 0.9394368008229395,
        "step": 7306
    },
    {
        "loss": 1.3893,
        "grad_norm": 1.7288544178009033,
        "learning_rate": 1.8192201627839855e-06,
        "epoch": 0.9395653851099396,
        "step": 7307
    },
    {
        "loss": 1.8285,
        "grad_norm": 2.428448438644409,
        "learning_rate": 1.8115431027995178e-06,
        "epoch": 0.9396939693969397,
        "step": 7308
    },
    {
        "loss": 1.9516,
        "grad_norm": 2.668436050415039,
        "learning_rate": 1.803882127512546e-06,
        "epoch": 0.9398225536839399,
        "step": 7309
    },
    {
        "loss": 1.6546,
        "grad_norm": 1.9738527536392212,
        "learning_rate": 1.7962372381780447e-06,
        "epoch": 0.93995113797094,
        "step": 7310
    },
    {
        "loss": 1.0758,
        "grad_norm": 2.3344104290008545,
        "learning_rate": 1.7886084360483779e-06,
        "epoch": 0.9400797222579401,
        "step": 7311
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.8670806884765625,
        "learning_rate": 1.7809957223732132e-06,
        "epoch": 0.9402083065449403,
        "step": 7312
    },
    {
        "loss": 1.4713,
        "grad_norm": 1.6765046119689941,
        "learning_rate": 1.773399098399653e-06,
        "epoch": 0.9403368908319404,
        "step": 7313
    },
    {
        "loss": 2.0614,
        "grad_norm": 2.1605477333068848,
        "learning_rate": 1.7658185653721348e-06,
        "epoch": 0.9404654751189405,
        "step": 7314
    },
    {
        "loss": 1.2923,
        "grad_norm": 1.6412245035171509,
        "learning_rate": 1.7582541245324436e-06,
        "epoch": 0.9405940594059405,
        "step": 7315
    },
    {
        "loss": 1.9661,
        "grad_norm": 2.338012933731079,
        "learning_rate": 1.7507057771197655e-06,
        "epoch": 0.9407226436929407,
        "step": 7316
    },
    {
        "loss": 1.8896,
        "grad_norm": 2.0071215629577637,
        "learning_rate": 1.7431735243706226e-06,
        "epoch": 0.9408512279799408,
        "step": 7317
    },
    {
        "loss": 1.6879,
        "grad_norm": 1.7077515125274658,
        "learning_rate": 1.7356573675188837e-06,
        "epoch": 0.9409798122669409,
        "step": 7318
    },
    {
        "loss": 1.5553,
        "grad_norm": 2.0160088539123535,
        "learning_rate": 1.7281573077958413e-06,
        "epoch": 0.9411083965539411,
        "step": 7319
    },
    {
        "loss": 1.1876,
        "grad_norm": 2.0322608947753906,
        "learning_rate": 1.7206733464301017e-06,
        "epoch": 0.9412369808409412,
        "step": 7320
    },
    {
        "loss": 2.5227,
        "grad_norm": 1.9001271724700928,
        "learning_rate": 1.7132054846476396e-06,
        "epoch": 0.9413655651279413,
        "step": 7321
    },
    {
        "loss": 2.1716,
        "grad_norm": 1.511478066444397,
        "learning_rate": 1.7057537236717992e-06,
        "epoch": 0.9414941494149415,
        "step": 7322
    },
    {
        "loss": 1.9218,
        "grad_norm": 2.0554938316345215,
        "learning_rate": 1.698318064723281e-06,
        "epoch": 0.9416227337019416,
        "step": 7323
    },
    {
        "loss": 1.3102,
        "grad_norm": 2.9174768924713135,
        "learning_rate": 1.6908985090201778e-06,
        "epoch": 0.9417513179889417,
        "step": 7324
    },
    {
        "loss": 2.147,
        "grad_norm": 2.1279687881469727,
        "learning_rate": 1.6834950577778952e-06,
        "epoch": 0.9418799022759419,
        "step": 7325
    },
    {
        "loss": 2.3175,
        "grad_norm": 1.9779356718063354,
        "learning_rate": 1.6761077122092516e-06,
        "epoch": 0.942008486562942,
        "step": 7326
    },
    {
        "loss": 1.4621,
        "grad_norm": 1.9547991752624512,
        "learning_rate": 1.6687364735243905e-06,
        "epoch": 0.9421370708499421,
        "step": 7327
    },
    {
        "loss": 0.7028,
        "grad_norm": 1.9675532579421997,
        "learning_rate": 1.661381342930801e-06,
        "epoch": 0.9422656551369423,
        "step": 7328
    },
    {
        "loss": 2.5351,
        "grad_norm": 1.9384220838546753,
        "learning_rate": 1.654042321633409e-06,
        "epoch": 0.9423942394239424,
        "step": 7329
    },
    {
        "loss": 1.735,
        "grad_norm": 2.9218835830688477,
        "learning_rate": 1.6467194108344187e-06,
        "epoch": 0.9425228237109425,
        "step": 7330
    },
    {
        "loss": 1.7132,
        "grad_norm": 1.6768354177474976,
        "learning_rate": 1.6394126117334374e-06,
        "epoch": 0.9426514079979427,
        "step": 7331
    },
    {
        "loss": 1.6667,
        "grad_norm": 2.7042269706726074,
        "learning_rate": 1.632121925527441e-06,
        "epoch": 0.9427799922849428,
        "step": 7332
    },
    {
        "loss": 2.005,
        "grad_norm": 1.951310396194458,
        "learning_rate": 1.6248473534107189e-06,
        "epoch": 0.9429085765719429,
        "step": 7333
    },
    {
        "loss": 1.7455,
        "grad_norm": 2.287527084350586,
        "learning_rate": 1.617588896574973e-06,
        "epoch": 0.9430371608589431,
        "step": 7334
    },
    {
        "loss": 1.7255,
        "grad_norm": 1.8622454404830933,
        "learning_rate": 1.6103465562092412e-06,
        "epoch": 0.9431657451459432,
        "step": 7335
    },
    {
        "loss": 2.259,
        "grad_norm": 1.5942031145095825,
        "learning_rate": 1.6031203334999412e-06,
        "epoch": 0.9432943294329433,
        "step": 7336
    },
    {
        "loss": 1.9097,
        "grad_norm": 1.8400214910507202,
        "learning_rate": 1.5959102296307926e-06,
        "epoch": 0.9434229137199435,
        "step": 7337
    },
    {
        "loss": 1.427,
        "grad_norm": 1.7277268171310425,
        "learning_rate": 1.5887162457829397e-06,
        "epoch": 0.9435514980069436,
        "step": 7338
    },
    {
        "loss": 1.6143,
        "grad_norm": 2.5364830493927,
        "learning_rate": 1.5815383831348729e-06,
        "epoch": 0.9436800822939437,
        "step": 7339
    },
    {
        "loss": 1.6955,
        "grad_norm": 1.8699078559875488,
        "learning_rate": 1.5743766428624185e-06,
        "epoch": 0.9438086665809438,
        "step": 7340
    },
    {
        "loss": 0.9937,
        "grad_norm": 1.6358462572097778,
        "learning_rate": 1.5672310261387602e-06,
        "epoch": 0.9439372508679439,
        "step": 7341
    },
    {
        "loss": 1.3961,
        "grad_norm": 1.9852896928787231,
        "learning_rate": 1.5601015341344728e-06,
        "epoch": 0.944065835154944,
        "step": 7342
    },
    {
        "loss": 0.6507,
        "grad_norm": 2.6017863750457764,
        "learning_rate": 1.5529881680174663e-06,
        "epoch": 0.9441944194419442,
        "step": 7343
    },
    {
        "loss": 1.6608,
        "grad_norm": 1.9957518577575684,
        "learning_rate": 1.5458909289530089e-06,
        "epoch": 0.9443230037289443,
        "step": 7344
    },
    {
        "loss": 1.5478,
        "grad_norm": 2.3062851428985596,
        "learning_rate": 1.538809818103737e-06,
        "epoch": 0.9444515880159444,
        "step": 7345
    },
    {
        "loss": 2.2454,
        "grad_norm": 1.389290452003479,
        "learning_rate": 1.5317448366296338e-06,
        "epoch": 0.9445801723029446,
        "step": 7346
    },
    {
        "loss": 1.6147,
        "grad_norm": 2.0153582096099854,
        "learning_rate": 1.5246959856880516e-06,
        "epoch": 0.9447087565899447,
        "step": 7347
    },
    {
        "loss": 2.0,
        "grad_norm": 1.0273765325546265,
        "learning_rate": 1.5176632664336887e-06,
        "epoch": 0.9448373408769448,
        "step": 7348
    },
    {
        "loss": 1.3925,
        "grad_norm": 2.631744146347046,
        "learning_rate": 1.5106466800186236e-06,
        "epoch": 0.944965925163945,
        "step": 7349
    },
    {
        "loss": 2.4051,
        "grad_norm": 2.0631489753723145,
        "learning_rate": 1.503646227592248e-06,
        "epoch": 0.9450945094509451,
        "step": 7350
    },
    {
        "loss": 1.7565,
        "grad_norm": 1.4436217546463013,
        "learning_rate": 1.4966619103013558e-06,
        "epoch": 0.9452230937379452,
        "step": 7351
    },
    {
        "loss": 1.3792,
        "grad_norm": 1.9505127668380737,
        "learning_rate": 1.4896937292900869e-06,
        "epoch": 0.9453516780249454,
        "step": 7352
    },
    {
        "loss": 2.2128,
        "grad_norm": 2.091031789779663,
        "learning_rate": 1.4827416856999066e-06,
        "epoch": 0.9454802623119455,
        "step": 7353
    },
    {
        "loss": 1.8057,
        "grad_norm": 1.522361159324646,
        "learning_rate": 1.4758057806696813e-06,
        "epoch": 0.9456088465989456,
        "step": 7354
    },
    {
        "loss": 1.9477,
        "grad_norm": 2.5789694786071777,
        "learning_rate": 1.468886015335602e-06,
        "epoch": 0.9457374308859458,
        "step": 7355
    },
    {
        "loss": 0.8823,
        "grad_norm": 2.069824457168579,
        "learning_rate": 1.4619823908312402e-06,
        "epoch": 0.9458660151729459,
        "step": 7356
    },
    {
        "loss": 1.2351,
        "grad_norm": 1.3983179330825806,
        "learning_rate": 1.4550949082874798e-06,
        "epoch": 0.945994599459946,
        "step": 7357
    },
    {
        "loss": 1.8733,
        "grad_norm": 2.3057429790496826,
        "learning_rate": 1.4482235688326295e-06,
        "epoch": 0.9461231837469462,
        "step": 7358
    },
    {
        "loss": 1.8838,
        "grad_norm": 2.478546380996704,
        "learning_rate": 1.4413683735923e-06,
        "epoch": 0.9462517680339463,
        "step": 7359
    },
    {
        "loss": 1.2413,
        "grad_norm": 1.9328066110610962,
        "learning_rate": 1.4345293236894486e-06,
        "epoch": 0.9463803523209464,
        "step": 7360
    },
    {
        "loss": 1.4862,
        "grad_norm": 1.9938796758651733,
        "learning_rate": 1.4277064202444458e-06,
        "epoch": 0.9465089366079465,
        "step": 7361
    },
    {
        "loss": 1.2878,
        "grad_norm": 2.3682351112365723,
        "learning_rate": 1.420899664374964e-06,
        "epoch": 0.9466375208949467,
        "step": 7362
    },
    {
        "loss": 1.5026,
        "grad_norm": 2.2192344665527344,
        "learning_rate": 1.4141090571960558e-06,
        "epoch": 0.9467661051819468,
        "step": 7363
    },
    {
        "loss": 0.772,
        "grad_norm": 1.9936742782592773,
        "learning_rate": 1.4073345998201205e-06,
        "epoch": 0.946894689468947,
        "step": 7364
    },
    {
        "loss": 1.422,
        "grad_norm": 2.5694284439086914,
        "learning_rate": 1.4005762933569033e-06,
        "epoch": 0.947023273755947,
        "step": 7365
    },
    {
        "loss": 2.1638,
        "grad_norm": 2.2894160747528076,
        "learning_rate": 1.3938341389135302e-06,
        "epoch": 0.9471518580429471,
        "step": 7366
    },
    {
        "loss": 1.8884,
        "grad_norm": 1.920701503753662,
        "learning_rate": 1.3871081375944285e-06,
        "epoch": 0.9472804423299472,
        "step": 7367
    },
    {
        "loss": 1.0002,
        "grad_norm": 2.3510758876800537,
        "learning_rate": 1.3803982905014722e-06,
        "epoch": 0.9474090266169474,
        "step": 7368
    },
    {
        "loss": 1.338,
        "grad_norm": 1.928544282913208,
        "learning_rate": 1.3737045987337938e-06,
        "epoch": 0.9475376109039475,
        "step": 7369
    },
    {
        "loss": 1.051,
        "grad_norm": 2.8943543434143066,
        "learning_rate": 1.3670270633879045e-06,
        "epoch": 0.9476661951909476,
        "step": 7370
    },
    {
        "loss": 1.5222,
        "grad_norm": 2.24576997756958,
        "learning_rate": 1.3603656855577185e-06,
        "epoch": 0.9477947794779478,
        "step": 7371
    },
    {
        "loss": 2.2216,
        "grad_norm": 1.4506409168243408,
        "learning_rate": 1.3537204663344516e-06,
        "epoch": 0.9479233637649479,
        "step": 7372
    },
    {
        "loss": 1.1913,
        "grad_norm": 2.2243757247924805,
        "learning_rate": 1.3470914068066775e-06,
        "epoch": 0.948051948051948,
        "step": 7373
    },
    {
        "loss": 1.5635,
        "grad_norm": 2.240403413772583,
        "learning_rate": 1.340478508060339e-06,
        "epoch": 0.9481805323389482,
        "step": 7374
    },
    {
        "loss": 2.1978,
        "grad_norm": 2.1686010360717773,
        "learning_rate": 1.3338817711787355e-06,
        "epoch": 0.9483091166259483,
        "step": 7375
    },
    {
        "loss": 1.9286,
        "grad_norm": 1.973128080368042,
        "learning_rate": 1.327301197242481e-06,
        "epoch": 0.9484377009129484,
        "step": 7376
    },
    {
        "loss": 1.3606,
        "grad_norm": 1.6780083179473877,
        "learning_rate": 1.3207367873296017e-06,
        "epoch": 0.9485662851999486,
        "step": 7377
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.5926247835159302,
        "learning_rate": 1.3141885425154265e-06,
        "epoch": 0.9486948694869487,
        "step": 7378
    },
    {
        "loss": 2.0366,
        "grad_norm": 2.2750632762908936,
        "learning_rate": 1.307656463872664e-06,
        "epoch": 0.9488234537739488,
        "step": 7379
    },
    {
        "loss": 1.6081,
        "grad_norm": 2.3287932872772217,
        "learning_rate": 1.301140552471336e-06,
        "epoch": 0.948952038060949,
        "step": 7380
    },
    {
        "loss": 1.7279,
        "grad_norm": 1.8775453567504883,
        "learning_rate": 1.2946408093788886e-06,
        "epoch": 0.9490806223479491,
        "step": 7381
    },
    {
        "loss": 1.7923,
        "grad_norm": 1.939956784248352,
        "learning_rate": 1.2881572356600259e-06,
        "epoch": 0.9492092066349492,
        "step": 7382
    },
    {
        "loss": 1.2946,
        "grad_norm": 2.2008590698242188,
        "learning_rate": 1.2816898323768866e-06,
        "epoch": 0.9493377909219494,
        "step": 7383
    },
    {
        "loss": 1.8445,
        "grad_norm": 1.6734760999679565,
        "learning_rate": 1.2752386005889127e-06,
        "epoch": 0.9494663752089495,
        "step": 7384
    },
    {
        "loss": 1.214,
        "grad_norm": 1.9805339574813843,
        "learning_rate": 1.2688035413529032e-06,
        "epoch": 0.9495949594959496,
        "step": 7385
    },
    {
        "loss": 0.8346,
        "grad_norm": 3.837524652481079,
        "learning_rate": 1.2623846557230257e-06,
        "epoch": 0.9497235437829498,
        "step": 7386
    },
    {
        "loss": 1.6839,
        "grad_norm": 2.050956964492798,
        "learning_rate": 1.2559819447507726e-06,
        "epoch": 0.9498521280699499,
        "step": 7387
    },
    {
        "loss": 1.7753,
        "grad_norm": 1.412146806716919,
        "learning_rate": 1.249595409485027e-06,
        "epoch": 0.94998071235695,
        "step": 7388
    },
    {
        "loss": 2.0785,
        "grad_norm": 1.1132344007492065,
        "learning_rate": 1.2432250509719746e-06,
        "epoch": 0.9501092966439502,
        "step": 7389
    },
    {
        "loss": 1.72,
        "grad_norm": 2.019481658935547,
        "learning_rate": 1.23687087025518e-06,
        "epoch": 0.9502378809309502,
        "step": 7390
    },
    {
        "loss": 1.3771,
        "grad_norm": 2.557222843170166,
        "learning_rate": 1.2305328683755557e-06,
        "epoch": 0.9503664652179503,
        "step": 7391
    },
    {
        "loss": 1.0964,
        "grad_norm": 1.4909045696258545,
        "learning_rate": 1.2242110463713375e-06,
        "epoch": 0.9504950495049505,
        "step": 7392
    },
    {
        "loss": 1.951,
        "grad_norm": 1.5004971027374268,
        "learning_rate": 1.2179054052781525e-06,
        "epoch": 0.9506236337919506,
        "step": 7393
    },
    {
        "loss": 2.0068,
        "grad_norm": 1.9709522724151611,
        "learning_rate": 1.2116159461289523e-06,
        "epoch": 0.9507522180789507,
        "step": 7394
    },
    {
        "loss": 1.2026,
        "grad_norm": 2.5727076530456543,
        "learning_rate": 1.205342669954046e-06,
        "epoch": 0.9508808023659509,
        "step": 7395
    },
    {
        "loss": 1.8382,
        "grad_norm": 1.855943202972412,
        "learning_rate": 1.1990855777810673e-06,
        "epoch": 0.951009386652951,
        "step": 7396
    },
    {
        "loss": 1.5838,
        "grad_norm": 3.6515755653381348,
        "learning_rate": 1.1928446706350404e-06,
        "epoch": 0.9511379709399511,
        "step": 7397
    },
    {
        "loss": 2.1287,
        "grad_norm": 1.4723058938980103,
        "learning_rate": 1.1866199495383145e-06,
        "epoch": 0.9512665552269512,
        "step": 7398
    },
    {
        "loss": 1.5811,
        "grad_norm": 2.320112943649292,
        "learning_rate": 1.1804114155105738e-06,
        "epoch": 0.9513951395139514,
        "step": 7399
    },
    {
        "loss": 1.2717,
        "grad_norm": 2.5653276443481445,
        "learning_rate": 1.1742190695688825e-06,
        "epoch": 0.9515237238009515,
        "step": 7400
    },
    {
        "eval_loss": 1.737512469291687,
        "eval_runtime": 28.2718,
        "eval_samples_per_second": 2.794,
        "eval_steps_per_second": 2.794,
        "epoch": 0.9515237238009515,
        "step": 7400
    },
    {
        "loss": 1.4935,
        "grad_norm": 2.1671133041381836,
        "learning_rate": 1.1680429127276294e-06,
        "epoch": 0.9516523080879516,
        "step": 7401
    },
    {
        "loss": 1.6572,
        "grad_norm": 2.4433133602142334,
        "learning_rate": 1.1618829459985603e-06,
        "epoch": 0.9517808923749518,
        "step": 7402
    },
    {
        "loss": 2.1537,
        "grad_norm": 1.6053410768508911,
        "learning_rate": 1.1557391703907683e-06,
        "epoch": 0.9519094766619519,
        "step": 7403
    },
    {
        "loss": 2.0081,
        "grad_norm": 1.4465572834014893,
        "learning_rate": 1.1496115869107038e-06,
        "epoch": 0.952038060948952,
        "step": 7404
    },
    {
        "loss": 2.0272,
        "grad_norm": 1.811676025390625,
        "learning_rate": 1.1435001965621307e-06,
        "epoch": 0.9521666452359522,
        "step": 7405
    },
    {
        "loss": 0.9116,
        "grad_norm": 2.1004433631896973,
        "learning_rate": 1.1374050003461921e-06,
        "epoch": 0.9522952295229523,
        "step": 7406
    },
    {
        "loss": 2.4281,
        "grad_norm": 1.1845228672027588,
        "learning_rate": 1.131325999261379e-06,
        "epoch": 0.9524238138099524,
        "step": 7407
    },
    {
        "loss": 0.9614,
        "grad_norm": 2.6365649700164795,
        "learning_rate": 1.1252631943035164e-06,
        "epoch": 0.9525523980969526,
        "step": 7408
    },
    {
        "loss": 1.065,
        "grad_norm": 2.3196258544921875,
        "learning_rate": 1.1192165864657767e-06,
        "epoch": 0.9526809823839527,
        "step": 7409
    },
    {
        "loss": 1.6417,
        "grad_norm": 1.7327165603637695,
        "learning_rate": 1.1131861767386898e-06,
        "epoch": 0.9528095666709528,
        "step": 7410
    },
    {
        "loss": 1.8836,
        "grad_norm": 1.8049968481063843,
        "learning_rate": 1.107171966110121e-06,
        "epoch": 0.952938150957953,
        "step": 7411
    },
    {
        "loss": 1.6007,
        "grad_norm": 2.3698887825012207,
        "learning_rate": 1.10117395556526e-06,
        "epoch": 0.9530667352449531,
        "step": 7412
    },
    {
        "loss": 1.5632,
        "grad_norm": 2.5583181381225586,
        "learning_rate": 1.09519214608671e-06,
        "epoch": 0.9531953195319532,
        "step": 7413
    },
    {
        "loss": 1.4496,
        "grad_norm": 1.7953418493270874,
        "learning_rate": 1.0892265386543643e-06,
        "epoch": 0.9533239038189534,
        "step": 7414
    },
    {
        "loss": 2.2305,
        "grad_norm": 1.8610727787017822,
        "learning_rate": 1.083277134245453e-06,
        "epoch": 0.9534524881059534,
        "step": 7415
    },
    {
        "loss": 0.9101,
        "grad_norm": 2.5376553535461426,
        "learning_rate": 1.077343933834618e-06,
        "epoch": 0.9535810723929535,
        "step": 7416
    },
    {
        "loss": 1.1594,
        "grad_norm": 2.509467601776123,
        "learning_rate": 1.0714269383937603e-06,
        "epoch": 0.9537096566799537,
        "step": 7417
    },
    {
        "loss": 1.1866,
        "grad_norm": 2.9401681423187256,
        "learning_rate": 1.0655261488921931e-06,
        "epoch": 0.9538382409669538,
        "step": 7418
    },
    {
        "loss": 1.2789,
        "grad_norm": 2.5649948120117188,
        "learning_rate": 1.059641566296543e-06,
        "epoch": 0.9539668252539539,
        "step": 7419
    },
    {
        "loss": 1.9377,
        "grad_norm": 1.4589083194732666,
        "learning_rate": 1.053773191570806e-06,
        "epoch": 0.9540954095409541,
        "step": 7420
    },
    {
        "loss": 0.918,
        "grad_norm": 1.4675315618515015,
        "learning_rate": 1.0479210256763017e-06,
        "epoch": 0.9542239938279542,
        "step": 7421
    },
    {
        "loss": 2.0758,
        "grad_norm": 1.827873945236206,
        "learning_rate": 1.0420850695716745e-06,
        "epoch": 0.9543525781149543,
        "step": 7422
    },
    {
        "loss": 0.8964,
        "grad_norm": 2.4800612926483154,
        "learning_rate": 1.0362653242129705e-06,
        "epoch": 0.9544811624019545,
        "step": 7423
    },
    {
        "loss": 1.0119,
        "grad_norm": 2.776057481765747,
        "learning_rate": 1.0304617905535386e-06,
        "epoch": 0.9546097466889546,
        "step": 7424
    },
    {
        "loss": 1.4543,
        "grad_norm": 1.9926538467407227,
        "learning_rate": 1.0246744695440735e-06,
        "epoch": 0.9547383309759547,
        "step": 7425
    },
    {
        "loss": 2.1077,
        "grad_norm": 1.5657767057418823,
        "learning_rate": 1.0189033621326282e-06,
        "epoch": 0.9548669152629549,
        "step": 7426
    },
    {
        "loss": 2.0724,
        "grad_norm": 1.5894590616226196,
        "learning_rate": 1.0131484692646021e-06,
        "epoch": 0.954995499549955,
        "step": 7427
    },
    {
        "loss": 1.8226,
        "grad_norm": 2.0638980865478516,
        "learning_rate": 1.0074097918827186e-06,
        "epoch": 0.9551240838369551,
        "step": 7428
    },
    {
        "loss": 1.6577,
        "grad_norm": 2.129650354385376,
        "learning_rate": 1.0016873309270592e-06,
        "epoch": 0.9552526681239553,
        "step": 7429
    },
    {
        "loss": 2.038,
        "grad_norm": 2.135483741760254,
        "learning_rate": 9.959810873350517e-07,
        "epoch": 0.9553812524109554,
        "step": 7430
    },
    {
        "loss": 1.9192,
        "grad_norm": 2.29634165763855,
        "learning_rate": 9.902910620414596e-07,
        "epoch": 0.9555098366979555,
        "step": 7431
    },
    {
        "loss": 1.5927,
        "grad_norm": 2.558302164077759,
        "learning_rate": 9.846172559783928e-07,
        "epoch": 0.9556384209849557,
        "step": 7432
    },
    {
        "loss": 1.4812,
        "grad_norm": 2.3811142444610596,
        "learning_rate": 9.789596700752967e-07,
        "epoch": 0.9557670052719558,
        "step": 7433
    },
    {
        "loss": 1.7761,
        "grad_norm": 1.4612479209899902,
        "learning_rate": 9.733183052589745e-07,
        "epoch": 0.9558955895589559,
        "step": 7434
    },
    {
        "loss": 1.2067,
        "grad_norm": 2.323228597640991,
        "learning_rate": 9.676931624535534e-07,
        "epoch": 0.9560241738459561,
        "step": 7435
    },
    {
        "loss": 1.4114,
        "grad_norm": 1.3205491304397583,
        "learning_rate": 9.620842425805298e-07,
        "epoch": 0.9561527581329562,
        "step": 7436
    },
    {
        "loss": 1.7403,
        "grad_norm": 1.8496403694152832,
        "learning_rate": 9.564915465587022e-07,
        "epoch": 0.9562813424199563,
        "step": 7437
    },
    {
        "loss": 1.4854,
        "grad_norm": 1.9475243091583252,
        "learning_rate": 9.509150753042594e-07,
        "epoch": 0.9564099267069565,
        "step": 7438
    },
    {
        "loss": 0.8878,
        "grad_norm": 2.303044557571411,
        "learning_rate": 9.453548297306935e-07,
        "epoch": 0.9565385109939566,
        "step": 7439
    },
    {
        "loss": 2.4347,
        "grad_norm": 1.383541464805603,
        "learning_rate": 9.398108107488757e-07,
        "epoch": 0.9566670952809566,
        "step": 7440
    },
    {
        "loss": 1.6224,
        "grad_norm": 1.6581367254257202,
        "learning_rate": 9.342830192669683e-07,
        "epoch": 0.9567956795679567,
        "step": 7441
    },
    {
        "loss": 1.3735,
        "grad_norm": 1.8858063220977783,
        "learning_rate": 9.287714561905136e-07,
        "epoch": 0.9569242638549569,
        "step": 7442
    },
    {
        "loss": 1.1049,
        "grad_norm": 2.4912261962890625,
        "learning_rate": 9.232761224223896e-07,
        "epoch": 0.957052848141957,
        "step": 7443
    },
    {
        "loss": 2.0491,
        "grad_norm": 2.1589622497558594,
        "learning_rate": 9.177970188628205e-07,
        "epoch": 0.9571814324289571,
        "step": 7444
    },
    {
        "loss": 1.4674,
        "grad_norm": 1.729985237121582,
        "learning_rate": 9.123341464093438e-07,
        "epoch": 0.9573100167159573,
        "step": 7445
    },
    {
        "loss": 2.0439,
        "grad_norm": 1.580523133277893,
        "learning_rate": 9.06887505956866e-07,
        "epoch": 0.9574386010029574,
        "step": 7446
    },
    {
        "loss": 2.0741,
        "grad_norm": 1.3776041269302368,
        "learning_rate": 9.014570983976289e-07,
        "epoch": 0.9575671852899575,
        "step": 7447
    },
    {
        "loss": 1.9248,
        "grad_norm": 3.0769059658050537,
        "learning_rate": 8.960429246212099e-07,
        "epoch": 0.9576957695769577,
        "step": 7448
    },
    {
        "loss": 1.3756,
        "grad_norm": 2.0984630584716797,
        "learning_rate": 8.906449855145327e-07,
        "epoch": 0.9578243538639578,
        "step": 7449
    },
    {
        "loss": 2.0459,
        "grad_norm": 2.3705320358276367,
        "learning_rate": 8.852632819618568e-07,
        "epoch": 0.9579529381509579,
        "step": 7450
    },
    {
        "loss": 1.5828,
        "grad_norm": 1.9005564451217651,
        "learning_rate": 8.798978148447767e-07,
        "epoch": 0.9580815224379581,
        "step": 7451
    },
    {
        "loss": 1.4429,
        "grad_norm": 2.295797824859619,
        "learning_rate": 8.745485850422452e-07,
        "epoch": 0.9582101067249582,
        "step": 7452
    },
    {
        "loss": 1.9855,
        "grad_norm": 1.9569743871688843,
        "learning_rate": 8.69215593430539e-07,
        "epoch": 0.9583386910119583,
        "step": 7453
    },
    {
        "loss": 1.9622,
        "grad_norm": 2.5131070613861084,
        "learning_rate": 8.638988408832704e-07,
        "epoch": 0.9584672752989585,
        "step": 7454
    },
    {
        "loss": 2.0017,
        "grad_norm": 1.623250126838684,
        "learning_rate": 8.585983282714094e-07,
        "epoch": 0.9585958595859586,
        "step": 7455
    },
    {
        "loss": 1.2547,
        "grad_norm": 2.282191276550293,
        "learning_rate": 8.533140564632614e-07,
        "epoch": 0.9587244438729587,
        "step": 7456
    },
    {
        "loss": 2.2509,
        "grad_norm": 2.3995425701141357,
        "learning_rate": 8.480460263244672e-07,
        "epoch": 0.9588530281599589,
        "step": 7457
    },
    {
        "loss": 1.922,
        "grad_norm": 1.497376561164856,
        "learning_rate": 8.427942387180032e-07,
        "epoch": 0.958981612446959,
        "step": 7458
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.9477665424346924,
        "learning_rate": 8.375586945041702e-07,
        "epoch": 0.9591101967339591,
        "step": 7459
    },
    {
        "loss": 1.6466,
        "grad_norm": 2.4426653385162354,
        "learning_rate": 8.323393945406599e-07,
        "epoch": 0.9592387810209593,
        "step": 7460
    },
    {
        "loss": 1.6051,
        "grad_norm": 1.9251950979232788,
        "learning_rate": 8.271363396824438e-07,
        "epoch": 0.9593673653079594,
        "step": 7461
    },
    {
        "loss": 0.8584,
        "grad_norm": 2.8142683506011963,
        "learning_rate": 8.219495307818848e-07,
        "epoch": 0.9594959495949595,
        "step": 7462
    },
    {
        "loss": 1.2573,
        "grad_norm": 1.7928969860076904,
        "learning_rate": 8.167789686886252e-07,
        "epoch": 0.9596245338819597,
        "step": 7463
    },
    {
        "loss": 0.4351,
        "grad_norm": 1.613796353340149,
        "learning_rate": 8.116246542496986e-07,
        "epoch": 0.9597531181689598,
        "step": 7464
    },
    {
        "loss": 1.7506,
        "grad_norm": 1.8490499258041382,
        "learning_rate": 8.064865883094519e-07,
        "epoch": 0.9598817024559598,
        "step": 7465
    },
    {
        "loss": 2.2382,
        "grad_norm": 1.32695472240448,
        "learning_rate": 8.013647717095674e-07,
        "epoch": 0.96001028674296,
        "step": 7466
    },
    {
        "loss": 1.4271,
        "grad_norm": 2.0312368869781494,
        "learning_rate": 7.96259205289085e-07,
        "epoch": 0.9601388710299601,
        "step": 7467
    },
    {
        "loss": 1.4402,
        "grad_norm": 2.4791617393493652,
        "learning_rate": 7.911698898843689e-07,
        "epoch": 0.9602674553169602,
        "step": 7468
    },
    {
        "loss": 1.5504,
        "grad_norm": 2.7431225776672363,
        "learning_rate": 7.860968263291191e-07,
        "epoch": 0.9603960396039604,
        "step": 7469
    },
    {
        "loss": 1.4407,
        "grad_norm": 2.1914031505584717,
        "learning_rate": 7.810400154543817e-07,
        "epoch": 0.9605246238909605,
        "step": 7470
    },
    {
        "loss": 2.0197,
        "grad_norm": 1.976965069770813,
        "learning_rate": 7.759994580885277e-07,
        "epoch": 0.9606532081779606,
        "step": 7471
    },
    {
        "loss": 1.2399,
        "grad_norm": 2.2119297981262207,
        "learning_rate": 7.709751550572852e-07,
        "epoch": 0.9607817924649608,
        "step": 7472
    },
    {
        "loss": 1.9072,
        "grad_norm": 2.0526235103607178,
        "learning_rate": 7.65967107183696e-07,
        "epoch": 0.9609103767519609,
        "step": 7473
    },
    {
        "loss": 1.1746,
        "grad_norm": 2.792651653289795,
        "learning_rate": 7.609753152881594e-07,
        "epoch": 0.961038961038961,
        "step": 7474
    },
    {
        "loss": 1.9513,
        "grad_norm": 1.8894667625427246,
        "learning_rate": 7.559997801883989e-07,
        "epoch": 0.9611675453259612,
        "step": 7475
    },
    {
        "loss": 1.4211,
        "grad_norm": 2.7612054347991943,
        "learning_rate": 7.510405026994849e-07,
        "epoch": 0.9612961296129613,
        "step": 7476
    },
    {
        "loss": 1.311,
        "grad_norm": 2.8186304569244385,
        "learning_rate": 7.460974836338119e-07,
        "epoch": 0.9614247138999614,
        "step": 7477
    },
    {
        "loss": 2.1599,
        "grad_norm": 1.570252537727356,
        "learning_rate": 7.411707238011212e-07,
        "epoch": 0.9615532981869616,
        "step": 7478
    },
    {
        "loss": 1.6444,
        "grad_norm": 3.419135570526123,
        "learning_rate": 7.362602240084893e-07,
        "epoch": 0.9616818824739617,
        "step": 7479
    },
    {
        "loss": 1.6333,
        "grad_norm": 2.8322694301605225,
        "learning_rate": 7.313659850603283e-07,
        "epoch": 0.9618104667609618,
        "step": 7480
    },
    {
        "loss": 1.3031,
        "grad_norm": 3.0027167797088623,
        "learning_rate": 7.264880077583857e-07,
        "epoch": 0.961939051047962,
        "step": 7481
    },
    {
        "loss": 1.9458,
        "grad_norm": 3.3039443492889404,
        "learning_rate": 7.216262929017448e-07,
        "epoch": 0.9620676353349621,
        "step": 7482
    },
    {
        "loss": 1.935,
        "grad_norm": 1.9797054529190063,
        "learning_rate": 7.167808412868238e-07,
        "epoch": 0.9621962196219622,
        "step": 7483
    },
    {
        "loss": 1.8636,
        "grad_norm": 1.0845952033996582,
        "learning_rate": 7.119516537073767e-07,
        "epoch": 0.9623248039089624,
        "step": 7484
    },
    {
        "loss": 1.794,
        "grad_norm": 2.2697932720184326,
        "learning_rate": 7.07138730954493e-07,
        "epoch": 0.9624533881959625,
        "step": 7485
    },
    {
        "loss": 1.2344,
        "grad_norm": 2.6046013832092285,
        "learning_rate": 7.023420738166087e-07,
        "epoch": 0.9625819724829626,
        "step": 7486
    },
    {
        "loss": 1.2605,
        "grad_norm": 1.5491079092025757,
        "learning_rate": 6.975616830794729e-07,
        "epoch": 0.9627105567699628,
        "step": 7487
    },
    {
        "loss": 2.2775,
        "grad_norm": 2.331747531890869,
        "learning_rate": 6.927975595262038e-07,
        "epoch": 0.9628391410569629,
        "step": 7488
    },
    {
        "loss": 2.1799,
        "grad_norm": 3.1151862144470215,
        "learning_rate": 6.880497039372102e-07,
        "epoch": 0.962967725343963,
        "step": 7489
    },
    {
        "loss": 2.0897,
        "grad_norm": 1.285340428352356,
        "learning_rate": 6.833181170902814e-07,
        "epoch": 0.963096309630963,
        "step": 7490
    },
    {
        "loss": 1.7848,
        "grad_norm": 2.514246702194214,
        "learning_rate": 6.786027997604971e-07,
        "epoch": 0.9632248939179632,
        "step": 7491
    },
    {
        "loss": 1.3984,
        "grad_norm": 2.2880144119262695,
        "learning_rate": 6.739037527203173e-07,
        "epoch": 0.9633534782049633,
        "step": 7492
    },
    {
        "loss": 1.7564,
        "grad_norm": 2.0252015590667725,
        "learning_rate": 6.692209767394931e-07,
        "epoch": 0.9634820624919634,
        "step": 7493
    },
    {
        "loss": 0.9379,
        "grad_norm": 1.972440481185913,
        "learning_rate": 6.645544725851438e-07,
        "epoch": 0.9636106467789636,
        "step": 7494
    },
    {
        "loss": 1.4168,
        "grad_norm": 2.2019741535186768,
        "learning_rate": 6.599042410217249e-07,
        "epoch": 0.9637392310659637,
        "step": 7495
    },
    {
        "loss": 1.8823,
        "grad_norm": 1.8894970417022705,
        "learning_rate": 6.552702828109825e-07,
        "epoch": 0.9638678153529638,
        "step": 7496
    },
    {
        "loss": 2.0729,
        "grad_norm": 2.764258861541748,
        "learning_rate": 6.506525987120427e-07,
        "epoch": 0.963996399639964,
        "step": 7497
    },
    {
        "loss": 1.3233,
        "grad_norm": 2.7145519256591797,
        "learning_rate": 6.460511894813448e-07,
        "epoch": 0.9641249839269641,
        "step": 7498
    },
    {
        "loss": 2.092,
        "grad_norm": 1.579545259475708,
        "learning_rate": 6.414660558726749e-07,
        "epoch": 0.9642535682139642,
        "step": 7499
    },
    {
        "loss": 0.9747,
        "grad_norm": 3.158811569213867,
        "learning_rate": 6.368971986371319e-07,
        "epoch": 0.9643821525009644,
        "step": 7500
    },
    {
        "eval_loss": 1.7373230457305908,
        "eval_runtime": 28.2893,
        "eval_samples_per_second": 2.793,
        "eval_steps_per_second": 2.793,
        "epoch": 0.9643821525009644,
        "step": 7500
    },
    {
        "loss": 1.6244,
        "grad_norm": 1.812962293624878,
        "learning_rate": 6.323446185231619e-07,
        "epoch": 0.9645107367879645,
        "step": 7501
    },
    {
        "loss": 1.9734,
        "grad_norm": 2.569889545440674,
        "learning_rate": 6.27808316276568e-07,
        "epoch": 0.9646393210749646,
        "step": 7502
    },
    {
        "loss": 1.829,
        "grad_norm": 2.2953782081604004,
        "learning_rate": 6.232882926404227e-07,
        "epoch": 0.9647679053619648,
        "step": 7503
    },
    {
        "loss": 2.203,
        "grad_norm": 1.993264079093933,
        "learning_rate": 6.187845483552002e-07,
        "epoch": 0.9648964896489649,
        "step": 7504
    },
    {
        "loss": 1.0546,
        "grad_norm": 2.169626235961914,
        "learning_rate": 6.14297084158677e-07,
        "epoch": 0.965025073935965,
        "step": 7505
    },
    {
        "loss": 1.3276,
        "grad_norm": 2.1905455589294434,
        "learning_rate": 6.098259007859541e-07,
        "epoch": 0.9651536582229652,
        "step": 7506
    },
    {
        "loss": 1.5788,
        "grad_norm": 1.9622882604599,
        "learning_rate": 6.0537099896949e-07,
        "epoch": 0.9652822425099653,
        "step": 7507
    },
    {
        "loss": 1.6741,
        "grad_norm": 2.647473096847534,
        "learning_rate": 6.009323794390453e-07,
        "epoch": 0.9654108267969654,
        "step": 7508
    },
    {
        "loss": 1.9834,
        "grad_norm": 1.5821566581726074,
        "learning_rate": 5.965100429217385e-07,
        "epoch": 0.9655394110839656,
        "step": 7509
    },
    {
        "loss": 0.787,
        "grad_norm": 2.6467413902282715,
        "learning_rate": 5.921039901420234e-07,
        "epoch": 0.9656679953709657,
        "step": 7510
    },
    {
        "loss": 1.3531,
        "grad_norm": 2.8823678493499756,
        "learning_rate": 5.877142218216669e-07,
        "epoch": 0.9657965796579658,
        "step": 7511
    },
    {
        "loss": 1.239,
        "grad_norm": 2.5418808460235596,
        "learning_rate": 5.833407386797607e-07,
        "epoch": 0.965925163944966,
        "step": 7512
    },
    {
        "loss": 1.5904,
        "grad_norm": 2.7506842613220215,
        "learning_rate": 5.78983541432776e-07,
        "epoch": 0.9660537482319661,
        "step": 7513
    },
    {
        "loss": 1.9577,
        "grad_norm": 2.612377405166626,
        "learning_rate": 5.746426307944641e-07,
        "epoch": 0.9661823325189662,
        "step": 7514
    },
    {
        "loss": 1.8405,
        "grad_norm": 1.4500365257263184,
        "learning_rate": 5.70318007475934e-07,
        "epoch": 0.9663109168059663,
        "step": 7515
    },
    {
        "loss": 1.4126,
        "grad_norm": 1.4207631349563599,
        "learning_rate": 5.660096721856189e-07,
        "epoch": 0.9664395010929664,
        "step": 7516
    },
    {
        "loss": 0.919,
        "grad_norm": 3.0388760566711426,
        "learning_rate": 5.617176256292988e-07,
        "epoch": 0.9665680853799665,
        "step": 7517
    },
    {
        "loss": 2.2457,
        "grad_norm": 1.4365607500076294,
        "learning_rate": 5.574418685100557e-07,
        "epoch": 0.9666966696669667,
        "step": 7518
    },
    {
        "loss": 1.7848,
        "grad_norm": 2.48932147026062,
        "learning_rate": 5.531824015283293e-07,
        "epoch": 0.9668252539539668,
        "step": 7519
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.948020100593567,
        "learning_rate": 5.489392253818948e-07,
        "epoch": 0.9669538382409669,
        "step": 7520
    },
    {
        "loss": 1.8887,
        "grad_norm": 1.8160322904586792,
        "learning_rate": 5.447123407658183e-07,
        "epoch": 0.9670824225279671,
        "step": 7521
    },
    {
        "loss": 2.0883,
        "grad_norm": 1.6512229442596436,
        "learning_rate": 5.405017483725461e-07,
        "epoch": 0.9672110068149672,
        "step": 7522
    },
    {
        "loss": 1.5977,
        "grad_norm": 2.3932135105133057,
        "learning_rate": 5.363074488918374e-07,
        "epoch": 0.9673395911019673,
        "step": 7523
    },
    {
        "loss": 1.1191,
        "grad_norm": 2.0518648624420166,
        "learning_rate": 5.321294430107649e-07,
        "epoch": 0.9674681753889675,
        "step": 7524
    },
    {
        "loss": 1.1202,
        "grad_norm": 1.8750643730163574,
        "learning_rate": 5.279677314137588e-07,
        "epoch": 0.9675967596759676,
        "step": 7525
    },
    {
        "loss": 0.8534,
        "grad_norm": 2.2337069511413574,
        "learning_rate": 5.238223147825516e-07,
        "epoch": 0.9677253439629677,
        "step": 7526
    },
    {
        "loss": 2.0082,
        "grad_norm": 2.173619031906128,
        "learning_rate": 5.196931937962446e-07,
        "epoch": 0.9678539282499679,
        "step": 7527
    },
    {
        "loss": 0.7041,
        "grad_norm": 1.7533897161483765,
        "learning_rate": 5.1558036913123e-07,
        "epoch": 0.967982512536968,
        "step": 7528
    },
    {
        "loss": 1.384,
        "grad_norm": 2.1966474056243896,
        "learning_rate": 5.114838414612578e-07,
        "epoch": 0.9681110968239681,
        "step": 7529
    },
    {
        "loss": 2.4725,
        "grad_norm": 1.824164867401123,
        "learning_rate": 5.074036114574021e-07,
        "epoch": 0.9682396811109683,
        "step": 7530
    },
    {
        "loss": 1.8441,
        "grad_norm": 2.0716769695281982,
        "learning_rate": 5.033396797880507e-07,
        "epoch": 0.9683682653979684,
        "step": 7531
    },
    {
        "loss": 1.2892,
        "grad_norm": 1.9823229312896729,
        "learning_rate": 4.992920471189377e-07,
        "epoch": 0.9684968496849685,
        "step": 7532
    },
    {
        "loss": 1.4423,
        "grad_norm": 2.0932424068450928,
        "learning_rate": 4.952607141131327e-07,
        "epoch": 0.9686254339719687,
        "step": 7533
    },
    {
        "loss": 2.3643,
        "grad_norm": 1.5726197957992554,
        "learning_rate": 4.912456814310295e-07,
        "epoch": 0.9687540182589688,
        "step": 7534
    },
    {
        "loss": 1.6556,
        "grad_norm": 1.616197943687439,
        "learning_rate": 4.872469497303355e-07,
        "epoch": 0.9688826025459689,
        "step": 7535
    },
    {
        "loss": 1.7795,
        "grad_norm": 2.073657751083374,
        "learning_rate": 4.832645196661045e-07,
        "epoch": 0.969011186832969,
        "step": 7536
    },
    {
        "loss": 1.2864,
        "grad_norm": 2.318763494491577,
        "learning_rate": 4.792983918907257e-07,
        "epoch": 0.9691397711199692,
        "step": 7537
    },
    {
        "loss": 1.2423,
        "grad_norm": 2.465362071990967,
        "learning_rate": 4.753485670539015e-07,
        "epoch": 0.9692683554069693,
        "step": 7538
    },
    {
        "loss": 1.474,
        "grad_norm": 2.8510494232177734,
        "learning_rate": 4.7141504580267007e-07,
        "epoch": 0.9693969396939695,
        "step": 7539
    },
    {
        "loss": 1.9445,
        "grad_norm": 1.8079465627670288,
        "learning_rate": 4.674978287814047e-07,
        "epoch": 0.9695255239809695,
        "step": 7540
    },
    {
        "loss": 1.6048,
        "grad_norm": 2.3739209175109863,
        "learning_rate": 4.6359691663179217e-07,
        "epoch": 0.9696541082679696,
        "step": 7541
    },
    {
        "loss": 1.7125,
        "grad_norm": 1.9424680471420288,
        "learning_rate": 4.5971230999286575e-07,
        "epoch": 0.9697826925549697,
        "step": 7542
    },
    {
        "loss": 1.2829,
        "grad_norm": 1.3781474828720093,
        "learning_rate": 4.5584400950098306e-07,
        "epoch": 0.9699112768419699,
        "step": 7543
    },
    {
        "loss": 1.6337,
        "grad_norm": 1.7486077547073364,
        "learning_rate": 4.519920157898261e-07,
        "epoch": 0.97003986112897,
        "step": 7544
    },
    {
        "loss": 1.819,
        "grad_norm": 2.9818332195281982,
        "learning_rate": 4.481563294904012e-07,
        "epoch": 0.9701684454159701,
        "step": 7545
    },
    {
        "loss": 2.5082,
        "grad_norm": 1.5901137590408325,
        "learning_rate": 4.4433695123106135e-07,
        "epoch": 0.9702970297029703,
        "step": 7546
    },
    {
        "loss": 0.6438,
        "grad_norm": 1.6768181324005127,
        "learning_rate": 4.405338816374727e-07,
        "epoch": 0.9704256139899704,
        "step": 7547
    },
    {
        "loss": 1.646,
        "grad_norm": 1.7372190952301025,
        "learning_rate": 4.367471213326146e-07,
        "epoch": 0.9705541982769705,
        "step": 7548
    },
    {
        "loss": 2.586,
        "grad_norm": 1.361830711364746,
        "learning_rate": 4.329766709368355e-07,
        "epoch": 0.9706827825639707,
        "step": 7549
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.3329156637191772,
        "learning_rate": 4.292225310677966e-07,
        "epoch": 0.9708113668509708,
        "step": 7550
    },
    {
        "loss": 2.0769,
        "grad_norm": 1.5335719585418701,
        "learning_rate": 4.254847023404507e-07,
        "epoch": 0.9709399511379709,
        "step": 7551
    },
    {
        "loss": 1.7219,
        "grad_norm": 2.0846214294433594,
        "learning_rate": 4.2176318536713e-07,
        "epoch": 0.9710685354249711,
        "step": 7552
    },
    {
        "loss": 0.63,
        "grad_norm": 2.4674808979034424,
        "learning_rate": 4.1805798075746917e-07,
        "epoch": 0.9711971197119712,
        "step": 7553
    },
    {
        "loss": 1.9919,
        "grad_norm": 1.4623823165893555,
        "learning_rate": 4.143690891184382e-07,
        "epoch": 0.9713257039989713,
        "step": 7554
    },
    {
        "loss": 0.8324,
        "grad_norm": 2.967459201812744,
        "learning_rate": 4.1069651105432037e-07,
        "epoch": 0.9714542882859715,
        "step": 7555
    },
    {
        "loss": 1.7889,
        "grad_norm": 5.071134090423584,
        "learning_rate": 4.070402471667456e-07,
        "epoch": 0.9715828725729716,
        "step": 7556
    },
    {
        "loss": 1.4531,
        "grad_norm": 2.9047837257385254,
        "learning_rate": 4.034002980546681e-07,
        "epoch": 0.9717114568599717,
        "step": 7557
    },
    {
        "loss": 1.0124,
        "grad_norm": 1.4309359788894653,
        "learning_rate": 3.997766643143441e-07,
        "epoch": 0.9718400411469719,
        "step": 7558
    },
    {
        "loss": 2.3933,
        "grad_norm": 1.891053557395935,
        "learning_rate": 3.9616934653941005e-07,
        "epoch": 0.971968625433972,
        "step": 7559
    },
    {
        "loss": 1.9494,
        "grad_norm": 2.0487003326416016,
        "learning_rate": 3.9257834532075985e-07,
        "epoch": 0.9720972097209721,
        "step": 7560
    },
    {
        "loss": 1.3192,
        "grad_norm": 2.3874154090881348,
        "learning_rate": 3.8900366124667854e-07,
        "epoch": 0.9722257940079723,
        "step": 7561
    },
    {
        "loss": 0.8885,
        "grad_norm": 2.3903114795684814,
        "learning_rate": 3.854452949027532e-07,
        "epoch": 0.9723543782949724,
        "step": 7562
    },
    {
        "loss": 1.1107,
        "grad_norm": 2.6951887607574463,
        "learning_rate": 3.8190324687187306e-07,
        "epoch": 0.9724829625819725,
        "step": 7563
    },
    {
        "loss": 2.1489,
        "grad_norm": 1.7964614629745483,
        "learning_rate": 3.783775177342852e-07,
        "epoch": 0.9726115468689727,
        "step": 7564
    },
    {
        "loss": 1.4194,
        "grad_norm": 2.517214298248291,
        "learning_rate": 3.74868108067572e-07,
        "epoch": 0.9727401311559727,
        "step": 7565
    },
    {
        "loss": 1.4514,
        "grad_norm": 1.6124582290649414,
        "learning_rate": 3.713750184466069e-07,
        "epoch": 0.9728687154429728,
        "step": 7566
    },
    {
        "loss": 2.1105,
        "grad_norm": 1.7010061740875244,
        "learning_rate": 3.678982494436212e-07,
        "epoch": 0.972997299729973,
        "step": 7567
    },
    {
        "loss": 1.5721,
        "grad_norm": 2.5348870754241943,
        "learning_rate": 3.64437801628148e-07,
        "epoch": 0.9731258840169731,
        "step": 7568
    },
    {
        "loss": 2.2481,
        "grad_norm": 1.7182722091674805,
        "learning_rate": 3.609936755670673e-07,
        "epoch": 0.9732544683039732,
        "step": 7569
    },
    {
        "loss": 2.0733,
        "grad_norm": 1.1910500526428223,
        "learning_rate": 3.5756587182458334e-07,
        "epoch": 0.9733830525909734,
        "step": 7570
    },
    {
        "loss": 1.7798,
        "grad_norm": 2.1832363605499268,
        "learning_rate": 3.541543909622025e-07,
        "epoch": 0.9735116368779735,
        "step": 7571
    },
    {
        "loss": 2.0464,
        "grad_norm": 1.8668321371078491,
        "learning_rate": 3.507592335387888e-07,
        "epoch": 0.9736402211649736,
        "step": 7572
    },
    {
        "loss": 1.1419,
        "grad_norm": 2.154977798461914,
        "learning_rate": 3.4738040011051964e-07,
        "epoch": 0.9737688054519738,
        "step": 7573
    },
    {
        "loss": 1.8621,
        "grad_norm": 2.15380597114563,
        "learning_rate": 3.4401789123088557e-07,
        "epoch": 0.9738973897389739,
        "step": 7574
    },
    {
        "loss": 1.5911,
        "grad_norm": 1.9765061140060425,
        "learning_rate": 3.4067170745071263e-07,
        "epoch": 0.974025974025974,
        "step": 7575
    },
    {
        "loss": 1.2513,
        "grad_norm": 1.7043015956878662,
        "learning_rate": 3.373418493181735e-07,
        "epoch": 0.9741545583129742,
        "step": 7576
    },
    {
        "loss": 1.834,
        "grad_norm": 2.4948723316192627,
        "learning_rate": 3.3402831737874286e-07,
        "epoch": 0.9742831425999743,
        "step": 7577
    },
    {
        "loss": 1.4308,
        "grad_norm": 2.8171088695526123,
        "learning_rate": 3.307311121752088e-07,
        "epoch": 0.9744117268869744,
        "step": 7578
    },
    {
        "loss": 1.315,
        "grad_norm": 1.1552647352218628,
        "learning_rate": 3.2745023424770593e-07,
        "epoch": 0.9745403111739745,
        "step": 7579
    },
    {
        "loss": 1.521,
        "grad_norm": 4.337444305419922,
        "learning_rate": 3.241856841337043e-07,
        "epoch": 0.9746688954609747,
        "step": 7580
    },
    {
        "loss": 2.1311,
        "grad_norm": 1.9738868474960327,
        "learning_rate": 3.2093746236797617e-07,
        "epoch": 0.9747974797479748,
        "step": 7581
    },
    {
        "loss": 1.4076,
        "grad_norm": 3.1848561763763428,
        "learning_rate": 3.177055694826292e-07,
        "epoch": 0.974926064034975,
        "step": 7582
    },
    {
        "loss": 1.7435,
        "grad_norm": 1.791258692741394,
        "learning_rate": 3.1449000600708435e-07,
        "epoch": 0.9750546483219751,
        "step": 7583
    },
    {
        "loss": 2.2303,
        "grad_norm": 1.996110439300537,
        "learning_rate": 3.1129077246810913e-07,
        "epoch": 0.9751832326089752,
        "step": 7584
    },
    {
        "loss": 1.5,
        "grad_norm": 2.2096076011657715,
        "learning_rate": 3.081078693897843e-07,
        "epoch": 0.9753118168959753,
        "step": 7585
    },
    {
        "loss": 1.9645,
        "grad_norm": 1.3598361015319824,
        "learning_rate": 3.0494129729351504e-07,
        "epoch": 0.9754404011829755,
        "step": 7586
    },
    {
        "loss": 1.1721,
        "grad_norm": 2.5723390579223633,
        "learning_rate": 3.017910566980309e-07,
        "epoch": 0.9755689854699756,
        "step": 7587
    },
    {
        "loss": 2.3144,
        "grad_norm": 0.9971112608909607,
        "learning_rate": 2.986571481193745e-07,
        "epoch": 0.9756975697569757,
        "step": 7588
    },
    {
        "loss": 0.9642,
        "grad_norm": 2.310106039047241,
        "learning_rate": 2.955395720709464e-07,
        "epoch": 0.9758261540439759,
        "step": 7589
    },
    {
        "loss": 1.8111,
        "grad_norm": 1.9232021570205688,
        "learning_rate": 2.92438329063438e-07,
        "epoch": 0.9759547383309759,
        "step": 7590
    },
    {
        "loss": 1.3261,
        "grad_norm": 2.288893222808838,
        "learning_rate": 2.893534196048764e-07,
        "epoch": 0.976083322617976,
        "step": 7591
    },
    {
        "loss": 1.2122,
        "grad_norm": 3.296558141708374,
        "learning_rate": 2.8628484420062387e-07,
        "epoch": 0.9762119069049762,
        "step": 7592
    },
    {
        "loss": 2.0181,
        "grad_norm": 1.922020435333252,
        "learning_rate": 2.832326033533561e-07,
        "epoch": 0.9763404911919763,
        "step": 7593
    },
    {
        "loss": 1.3432,
        "grad_norm": 1.250504970550537,
        "learning_rate": 2.801966975630732e-07,
        "epoch": 0.9764690754789764,
        "step": 7594
    },
    {
        "loss": 1.7981,
        "grad_norm": 3.9360318183898926,
        "learning_rate": 2.7717712732709957e-07,
        "epoch": 0.9765976597659766,
        "step": 7595
    },
    {
        "loss": 2.2944,
        "grad_norm": 1.7221934795379639,
        "learning_rate": 2.741738931400839e-07,
        "epoch": 0.9767262440529767,
        "step": 7596
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.6373282670974731,
        "learning_rate": 2.711869954939883e-07,
        "epoch": 0.9768548283399768,
        "step": 7597
    },
    {
        "loss": 1.6696,
        "grad_norm": 2.4954147338867188,
        "learning_rate": 2.6821643487813243e-07,
        "epoch": 0.976983412626977,
        "step": 7598
    },
    {
        "loss": 1.2281,
        "grad_norm": 2.4915263652801514,
        "learning_rate": 2.65262211779127e-07,
        "epoch": 0.9771119969139771,
        "step": 7599
    },
    {
        "loss": 1.0139,
        "grad_norm": 1.5398666858673096,
        "learning_rate": 2.6232432668091836e-07,
        "epoch": 0.9772405812009772,
        "step": 7600
    },
    {
        "eval_loss": 1.7372772693634033,
        "eval_runtime": 28.2268,
        "eval_samples_per_second": 2.799,
        "eval_steps_per_second": 2.799,
        "epoch": 0.9772405812009772,
        "step": 7600
    },
    {
        "loss": 1.6596,
        "grad_norm": 1.4580960273742676,
        "learning_rate": 2.594027800647658e-07,
        "epoch": 0.9773691654879774,
        "step": 7601
    },
    {
        "loss": 2.0059,
        "grad_norm": 1.243768334388733,
        "learning_rate": 2.564975724092755e-07,
        "epoch": 0.9774977497749775,
        "step": 7602
    },
    {
        "loss": 1.4025,
        "grad_norm": 2.3088219165802,
        "learning_rate": 2.5360870419035564e-07,
        "epoch": 0.9776263340619776,
        "step": 7603
    },
    {
        "loss": 1.1719,
        "grad_norm": 2.189802885055542,
        "learning_rate": 2.507361758812499e-07,
        "epoch": 0.9777549183489778,
        "step": 7604
    },
    {
        "loss": 2.0613,
        "grad_norm": 1.4501607418060303,
        "learning_rate": 2.47879987952504e-07,
        "epoch": 0.9778835026359779,
        "step": 7605
    },
    {
        "loss": 1.6902,
        "grad_norm": 1.947178840637207,
        "learning_rate": 2.4504014087202154e-07,
        "epoch": 0.978012086922978,
        "step": 7606
    },
    {
        "loss": 1.2164,
        "grad_norm": 2.432004451751709,
        "learning_rate": 2.42216635104997e-07,
        "epoch": 0.9781406712099782,
        "step": 7607
    },
    {
        "loss": 2.2065,
        "grad_norm": 1.2056677341461182,
        "learning_rate": 2.394094711139716e-07,
        "epoch": 0.9782692554969783,
        "step": 7608
    },
    {
        "loss": 2.1156,
        "grad_norm": 2.348252773284912,
        "learning_rate": 2.3661864935879962e-07,
        "epoch": 0.9783978397839784,
        "step": 7609
    },
    {
        "loss": 2.1339,
        "grad_norm": 1.307565450668335,
        "learning_rate": 2.3384417029664875e-07,
        "epoch": 0.9785264240709786,
        "step": 7610
    },
    {
        "loss": 2.3696,
        "grad_norm": 2.3210036754608154,
        "learning_rate": 2.3108603438203314e-07,
        "epoch": 0.9786550083579787,
        "step": 7611
    },
    {
        "loss": 1.9439,
        "grad_norm": 2.1746928691864014,
        "learning_rate": 2.2834424206675808e-07,
        "epoch": 0.9787835926449788,
        "step": 7612
    },
    {
        "loss": 1.5475,
        "grad_norm": 1.8858767747879028,
        "learning_rate": 2.256187937999865e-07,
        "epoch": 0.978912176931979,
        "step": 7613
    },
    {
        "loss": 2.0744,
        "grad_norm": 1.826714277267456,
        "learning_rate": 2.229096900281724e-07,
        "epoch": 0.9790407612189791,
        "step": 7614
    },
    {
        "loss": 1.0914,
        "grad_norm": 2.484959602355957,
        "learning_rate": 2.2021693119511632e-07,
        "epoch": 0.9791693455059791,
        "step": 7615
    },
    {
        "loss": 1.5701,
        "grad_norm": 1.7675429582595825,
        "learning_rate": 2.1754051774193206e-07,
        "epoch": 0.9792979297929792,
        "step": 7616
    },
    {
        "loss": 1.555,
        "grad_norm": 3.0489449501037598,
        "learning_rate": 2.1488045010704672e-07,
        "epoch": 0.9794265140799794,
        "step": 7617
    },
    {
        "loss": 1.2694,
        "grad_norm": 4.752913951873779,
        "learning_rate": 2.122367287262228e-07,
        "epoch": 0.9795550983669795,
        "step": 7618
    },
    {
        "loss": 2.0451,
        "grad_norm": 8.49459171295166,
        "learning_rate": 2.0960935403253612e-07,
        "epoch": 0.9796836826539796,
        "step": 7619
    },
    {
        "loss": 0.8672,
        "grad_norm": 1.8194206953048706,
        "learning_rate": 2.0699832645638683e-07,
        "epoch": 0.9798122669409798,
        "step": 7620
    },
    {
        "loss": 2.0114,
        "grad_norm": 2.6796274185180664,
        "learning_rate": 2.0440364642552169e-07,
        "epoch": 0.9799408512279799,
        "step": 7621
    },
    {
        "loss": 1.6715,
        "grad_norm": 1.9379487037658691,
        "learning_rate": 2.018253143649562e-07,
        "epoch": 0.98006943551498,
        "step": 7622
    },
    {
        "loss": 1.8393,
        "grad_norm": 1.861664891242981,
        "learning_rate": 1.9926333069707482e-07,
        "epoch": 0.9801980198019802,
        "step": 7623
    },
    {
        "loss": 1.8031,
        "grad_norm": 2.277414083480835,
        "learning_rate": 1.9671769584156395e-07,
        "epoch": 0.9803266040889803,
        "step": 7624
    },
    {
        "loss": 1.9024,
        "grad_norm": 2.48830509185791,
        "learning_rate": 1.9418841021544566e-07,
        "epoch": 0.9804551883759804,
        "step": 7625
    },
    {
        "loss": 1.0669,
        "grad_norm": 2.5072836875915527,
        "learning_rate": 1.916754742330329e-07,
        "epoch": 0.9805837726629806,
        "step": 7626
    },
    {
        "loss": 1.7961,
        "grad_norm": 2.3304667472839355,
        "learning_rate": 1.891788883060075e-07,
        "epoch": 0.9807123569499807,
        "step": 7627
    },
    {
        "loss": 1.7345,
        "grad_norm": 2.4671502113342285,
        "learning_rate": 1.8669865284332012e-07,
        "epoch": 0.9808409412369808,
        "step": 7628
    },
    {
        "loss": 2.4128,
        "grad_norm": 1.745984673500061,
        "learning_rate": 1.8423476825129015e-07,
        "epoch": 0.980969525523981,
        "step": 7629
    },
    {
        "loss": 1.5763,
        "grad_norm": 1.5916436910629272,
        "learning_rate": 1.81787234933517e-07,
        "epoch": 0.9810981098109811,
        "step": 7630
    },
    {
        "loss": 1.6436,
        "grad_norm": 3.6606087684631348,
        "learning_rate": 1.7935605329096883e-07,
        "epoch": 0.9812266940979812,
        "step": 7631
    },
    {
        "loss": 1.7071,
        "grad_norm": 1.6453330516815186,
        "learning_rate": 1.769412237218826e-07,
        "epoch": 0.9813552783849814,
        "step": 7632
    },
    {
        "loss": 1.481,
        "grad_norm": 1.6464935541152954,
        "learning_rate": 1.745427466218641e-07,
        "epoch": 0.9814838626719815,
        "step": 7633
    },
    {
        "loss": 1.8529,
        "grad_norm": 2.5440189838409424,
        "learning_rate": 1.7216062238379905e-07,
        "epoch": 0.9816124469589816,
        "step": 7634
    },
    {
        "loss": 1.3286,
        "grad_norm": 1.6090359687805176,
        "learning_rate": 1.6979485139793082e-07,
        "epoch": 0.9817410312459818,
        "step": 7635
    },
    {
        "loss": 2.1076,
        "grad_norm": 1.9259464740753174,
        "learning_rate": 1.674454340517939e-07,
        "epoch": 0.9818696155329819,
        "step": 7636
    },
    {
        "loss": 2.2833,
        "grad_norm": 1.664044737815857,
        "learning_rate": 1.6511237073026932e-07,
        "epoch": 0.981998199819982,
        "step": 7637
    },
    {
        "loss": 1.4678,
        "grad_norm": 1.9741873741149902,
        "learning_rate": 1.6279566181554017e-07,
        "epoch": 0.9821267841069822,
        "step": 7638
    },
    {
        "loss": 2.0341,
        "grad_norm": 1.5649346113204956,
        "learning_rate": 1.6049530768711406e-07,
        "epoch": 0.9822553683939823,
        "step": 7639
    },
    {
        "loss": 1.0742,
        "grad_norm": 1.5783976316452026,
        "learning_rate": 1.5821130872183398e-07,
        "epoch": 0.9823839526809823,
        "step": 7640
    },
    {
        "loss": 1.8447,
        "grad_norm": 1.8634960651397705,
        "learning_rate": 1.5594366529383397e-07,
        "epoch": 0.9825125369679825,
        "step": 7641
    },
    {
        "loss": 1.7621,
        "grad_norm": 1.798687219619751,
        "learning_rate": 1.5369237777459465e-07,
        "epoch": 0.9826411212549826,
        "step": 7642
    },
    {
        "loss": 1.7548,
        "grad_norm": 2.020531415939331,
        "learning_rate": 1.5145744653292106e-07,
        "epoch": 0.9827697055419827,
        "step": 7643
    },
    {
        "loss": 1.7716,
        "grad_norm": 2.5124123096466064,
        "learning_rate": 1.4923887193492025e-07,
        "epoch": 0.9828982898289829,
        "step": 7644
    },
    {
        "loss": 1.2119,
        "grad_norm": 2.378122329711914,
        "learning_rate": 1.4703665434402382e-07,
        "epoch": 0.983026874115983,
        "step": 7645
    },
    {
        "loss": 1.558,
        "grad_norm": 2.8022196292877197,
        "learning_rate": 1.4485079412098756e-07,
        "epoch": 0.9831554584029831,
        "step": 7646
    },
    {
        "loss": 2.5048,
        "grad_norm": 1.9181482791900635,
        "learning_rate": 1.4268129162388066e-07,
        "epoch": 0.9832840426899833,
        "step": 7647
    },
    {
        "loss": 1.7263,
        "grad_norm": 2.285205364227295,
        "learning_rate": 1.405281472081077e-07,
        "epoch": 0.9834126269769834,
        "step": 7648
    },
    {
        "loss": 1.6276,
        "grad_norm": 2.09316349029541,
        "learning_rate": 1.383913612263976e-07,
        "epoch": 0.9835412112639835,
        "step": 7649
    },
    {
        "loss": 1.8513,
        "grad_norm": 1.9120540618896484,
        "learning_rate": 1.3627093402875935e-07,
        "epoch": 0.9836697955509837,
        "step": 7650
    },
    {
        "loss": 2.5218,
        "grad_norm": 2.4195384979248047,
        "learning_rate": 1.3416686596257054e-07,
        "epoch": 0.9837983798379838,
        "step": 7651
    },
    {
        "loss": 1.1597,
        "grad_norm": 2.6533849239349365,
        "learning_rate": 1.3207915737249999e-07,
        "epoch": 0.9839269641249839,
        "step": 7652
    },
    {
        "loss": 1.6899,
        "grad_norm": 2.3517909049987793,
        "learning_rate": 1.3000780860055184e-07,
        "epoch": 0.9840555484119841,
        "step": 7653
    },
    {
        "loss": 2.283,
        "grad_norm": 1.918387532234192,
        "learning_rate": 1.2795281998603247e-07,
        "epoch": 0.9841841326989842,
        "step": 7654
    },
    {
        "loss": 0.8829,
        "grad_norm": 1.7618284225463867,
        "learning_rate": 1.2591419186557262e-07,
        "epoch": 0.9843127169859843,
        "step": 7655
    },
    {
        "loss": 0.6925,
        "grad_norm": 2.3263094425201416,
        "learning_rate": 1.2389192457316067e-07,
        "epoch": 0.9844413012729845,
        "step": 7656
    },
    {
        "loss": 2.0831,
        "grad_norm": 2.039684772491455,
        "learning_rate": 1.2188601844004277e-07,
        "epoch": 0.9845698855599846,
        "step": 7657
    },
    {
        "loss": 1.6575,
        "grad_norm": 2.2936863899230957,
        "learning_rate": 1.198964737948227e-07,
        "epoch": 0.9846984698469847,
        "step": 7658
    },
    {
        "loss": 1.5096,
        "grad_norm": 1.9433964490890503,
        "learning_rate": 1.1792329096342869e-07,
        "epoch": 0.9848270541339849,
        "step": 7659
    },
    {
        "loss": 2.1505,
        "grad_norm": 2.3372108936309814,
        "learning_rate": 1.1596647026907991e-07,
        "epoch": 0.984955638420985,
        "step": 7660
    },
    {
        "loss": 1.8365,
        "grad_norm": 1.9457648992538452,
        "learning_rate": 1.140260120323422e-07,
        "epoch": 0.9850842227079851,
        "step": 7661
    },
    {
        "loss": 1.6871,
        "grad_norm": 2.621248245239258,
        "learning_rate": 1.121019165710946e-07,
        "epoch": 0.9852128069949853,
        "step": 7662
    },
    {
        "loss": 1.6239,
        "grad_norm": 1.2933707237243652,
        "learning_rate": 1.101941842005294e-07,
        "epoch": 0.9853413912819854,
        "step": 7663
    },
    {
        "loss": 2.4369,
        "grad_norm": 2.0969338417053223,
        "learning_rate": 1.0830281523315222e-07,
        "epoch": 0.9854699755689855,
        "step": 7664
    },
    {
        "loss": 1.1803,
        "grad_norm": 2.343665599822998,
        "learning_rate": 1.0642780997879299e-07,
        "epoch": 0.9855985598559855,
        "step": 7665
    },
    {
        "loss": 1.7662,
        "grad_norm": 2.270115852355957,
        "learning_rate": 1.0456916874462819e-07,
        "epoch": 0.9857271441429857,
        "step": 7666
    },
    {
        "loss": 1.6029,
        "grad_norm": 2.9074668884277344,
        "learning_rate": 1.027268918351143e-07,
        "epoch": 0.9858557284299858,
        "step": 7667
    },
    {
        "loss": 1.6362,
        "grad_norm": 2.8256821632385254,
        "learning_rate": 1.0090097955204326e-07,
        "epoch": 0.9859843127169859,
        "step": 7668
    },
    {
        "loss": 1.4668,
        "grad_norm": 4.220898151397705,
        "learning_rate": 9.909143219452022e-08,
        "epoch": 0.9861128970039861,
        "step": 7669
    },
    {
        "loss": 1.7292,
        "grad_norm": 2.0933799743652344,
        "learning_rate": 9.729825005899695e-08,
        "epoch": 0.9862414812909862,
        "step": 7670
    },
    {
        "loss": 1.9747,
        "grad_norm": 1.598408818244934,
        "learning_rate": 9.552143343919407e-08,
        "epoch": 0.9863700655779863,
        "step": 7671
    },
    {
        "loss": 2.2247,
        "grad_norm": 0.9937467575073242,
        "learning_rate": 9.376098262621203e-08,
        "epoch": 0.9864986498649865,
        "step": 7672
    },
    {
        "loss": 1.7629,
        "grad_norm": 2.333742380142212,
        "learning_rate": 9.201689790840907e-08,
        "epoch": 0.9866272341519866,
        "step": 7673
    },
    {
        "loss": 1.2799,
        "grad_norm": 3.3207895755767822,
        "learning_rate": 9.028917957151217e-08,
        "epoch": 0.9867558184389867,
        "step": 7674
    },
    {
        "loss": 2.2091,
        "grad_norm": 1.8937019109725952,
        "learning_rate": 8.857782789852831e-08,
        "epoch": 0.9868844027259869,
        "step": 7675
    },
    {
        "loss": 1.4458,
        "grad_norm": 2.0234215259552,
        "learning_rate": 8.688284316981099e-08,
        "epoch": 0.987012987012987,
        "step": 7676
    },
    {
        "loss": 1.8947,
        "grad_norm": 2.165903329849243,
        "learning_rate": 8.520422566303809e-08,
        "epoch": 0.9871415712999871,
        "step": 7677
    },
    {
        "loss": 0.5811,
        "grad_norm": 2.0340089797973633,
        "learning_rate": 8.354197565316746e-08,
        "epoch": 0.9872701555869873,
        "step": 7678
    },
    {
        "loss": 1.8344,
        "grad_norm": 1.8367280960083008,
        "learning_rate": 8.189609341250348e-08,
        "epoch": 0.9873987398739874,
        "step": 7679
    },
    {
        "loss": 1.5582,
        "grad_norm": 1.856147289276123,
        "learning_rate": 8.026657921068603e-08,
        "epoch": 0.9875273241609875,
        "step": 7680
    },
    {
        "loss": 0.8619,
        "grad_norm": 2.7275612354278564,
        "learning_rate": 7.865343331463493e-08,
        "epoch": 0.9876559084479877,
        "step": 7681
    },
    {
        "loss": 1.8471,
        "grad_norm": 2.6216936111450195,
        "learning_rate": 7.705665598860546e-08,
        "epoch": 0.9877844927349878,
        "step": 7682
    },
    {
        "loss": 1.6254,
        "grad_norm": 1.7028776407241821,
        "learning_rate": 7.547624749418835e-08,
        "epoch": 0.9879130770219879,
        "step": 7683
    },
    {
        "loss": 1.7121,
        "grad_norm": 2.3606975078582764,
        "learning_rate": 7.391220809025435e-08,
        "epoch": 0.9880416613089881,
        "step": 7684
    },
    {
        "loss": 0.8931,
        "grad_norm": 2.9125006198883057,
        "learning_rate": 7.236453803304288e-08,
        "epoch": 0.9881702455959882,
        "step": 7685
    },
    {
        "loss": 1.0412,
        "grad_norm": 2.4705607891082764,
        "learning_rate": 7.08332375760734e-08,
        "epoch": 0.9882988298829883,
        "step": 7686
    },
    {
        "loss": 1.7543,
        "grad_norm": 2.1696808338165283,
        "learning_rate": 6.93183069701786e-08,
        "epoch": 0.9884274141699885,
        "step": 7687
    },
    {
        "loss": 1.3987,
        "grad_norm": 2.3775503635406494,
        "learning_rate": 6.781974646354883e-08,
        "epoch": 0.9885559984569886,
        "step": 7688
    },
    {
        "loss": 1.8693,
        "grad_norm": 2.7312116622924805,
        "learning_rate": 6.633755630166549e-08,
        "epoch": 0.9886845827439887,
        "step": 7689
    },
    {
        "loss": 1.2908,
        "grad_norm": 2.228285551071167,
        "learning_rate": 6.487173672732327e-08,
        "epoch": 0.9888131670309888,
        "step": 7690
    },
    {
        "loss": 1.9185,
        "grad_norm": 1.608664631843567,
        "learning_rate": 6.342228798065231e-08,
        "epoch": 0.9889417513179889,
        "step": 7691
    },
    {
        "loss": 1.5338,
        "grad_norm": 2.09787917137146,
        "learning_rate": 6.198921029909598e-08,
        "epoch": 0.989070335604989,
        "step": 7692
    },
    {
        "loss": 1.0585,
        "grad_norm": 2.6037330627441406,
        "learning_rate": 6.057250391739988e-08,
        "epoch": 0.9891989198919892,
        "step": 7693
    },
    {
        "loss": 1.723,
        "grad_norm": 1.926411747932434,
        "learning_rate": 5.91721690676561e-08,
        "epoch": 0.9893275041789893,
        "step": 7694
    },
    {
        "loss": 1.5415,
        "grad_norm": 2.1407132148742676,
        "learning_rate": 5.7788205979258936e-08,
        "epoch": 0.9894560884659894,
        "step": 7695
    },
    {
        "loss": 2.1125,
        "grad_norm": 2.2096354961395264,
        "learning_rate": 5.6420614878915925e-08,
        "epoch": 0.9895846727529896,
        "step": 7696
    },
    {
        "loss": 1.8921,
        "grad_norm": 2.3791518211364746,
        "learning_rate": 5.5069395990647865e-08,
        "epoch": 0.9897132570399897,
        "step": 7697
    },
    {
        "loss": 2.1631,
        "grad_norm": 2.7200112342834473,
        "learning_rate": 5.373454953583324e-08,
        "epoch": 0.9898418413269898,
        "step": 7698
    },
    {
        "loss": 1.4646,
        "grad_norm": 2.3529045581817627,
        "learning_rate": 5.241607573310825e-08,
        "epoch": 0.98997042561399,
        "step": 7699
    },
    {
        "loss": 1.6376,
        "grad_norm": 2.1430916786193848,
        "learning_rate": 5.1113974798477904e-08,
        "epoch": 0.9900990099009901,
        "step": 7700
    },
    {
        "eval_loss": 1.7372504472732544,
        "eval_runtime": 28.2258,
        "eval_samples_per_second": 2.799,
        "eval_steps_per_second": 2.799,
        "epoch": 0.9900990099009901,
        "step": 7700
    },
    {
        "loss": 1.7126,
        "grad_norm": 2.902083158493042,
        "learning_rate": 4.9828246945238245e-08,
        "epoch": 0.9902275941879902,
        "step": 7701
    },
    {
        "loss": 2.0538,
        "grad_norm": 1.752555012702942,
        "learning_rate": 4.855889238400968e-08,
        "epoch": 0.9903561784749904,
        "step": 7702
    },
    {
        "loss": 1.1298,
        "grad_norm": 1.945520043373108,
        "learning_rate": 4.730591132272588e-08,
        "epoch": 0.9904847627619905,
        "step": 7703
    },
    {
        "loss": 1.4934,
        "grad_norm": 2.4281554222106934,
        "learning_rate": 4.6069303966655984e-08,
        "epoch": 0.9906133470489906,
        "step": 7704
    },
    {
        "loss": 1.4903,
        "grad_norm": 2.2616443634033203,
        "learning_rate": 4.484907051837128e-08,
        "epoch": 0.9907419313359908,
        "step": 7705
    },
    {
        "loss": 2.1293,
        "grad_norm": 2.367908477783203,
        "learning_rate": 4.364521117775633e-08,
        "epoch": 0.9908705156229909,
        "step": 7706
    },
    {
        "loss": 2.0281,
        "grad_norm": 2.1067676544189453,
        "learning_rate": 4.245772614203114e-08,
        "epoch": 0.990999099909991,
        "step": 7707
    },
    {
        "loss": 0.5631,
        "grad_norm": 1.7496827840805054,
        "learning_rate": 4.128661560570679e-08,
        "epoch": 0.9911276841969912,
        "step": 7708
    },
    {
        "loss": 2.1076,
        "grad_norm": 1.0835999250411987,
        "learning_rate": 4.013187976064092e-08,
        "epoch": 0.9912562684839913,
        "step": 7709
    },
    {
        "loss": 1.3868,
        "grad_norm": 2.571967601776123,
        "learning_rate": 3.899351879599333e-08,
        "epoch": 0.9913848527709914,
        "step": 7710
    },
    {
        "loss": 1.1568,
        "grad_norm": 1.9722291231155396,
        "learning_rate": 3.7871532898248185e-08,
        "epoch": 0.9915134370579916,
        "step": 7711
    },
    {
        "loss": 1.3784,
        "grad_norm": 1.972865104675293,
        "learning_rate": 3.67659222512029e-08,
        "epoch": 0.9916420213449917,
        "step": 7712
    },
    {
        "loss": 0.6853,
        "grad_norm": 2.4182851314544678,
        "learning_rate": 3.567668703595706e-08,
        "epoch": 0.9917706056319918,
        "step": 7713
    },
    {
        "loss": 1.6699,
        "grad_norm": 2.052572250366211,
        "learning_rate": 3.4603827430967906e-08,
        "epoch": 0.991899189918992,
        "step": 7714
    },
    {
        "loss": 1.915,
        "grad_norm": 1.8379735946655273,
        "learning_rate": 3.354734361196155e-08,
        "epoch": 0.992027774205992,
        "step": 7715
    },
    {
        "loss": 1.9275,
        "grad_norm": 1.530749797821045,
        "learning_rate": 3.2507235752021745e-08,
        "epoch": 0.9921563584929921,
        "step": 7716
    },
    {
        "loss": 1.5804,
        "grad_norm": 2.5868966579437256,
        "learning_rate": 3.148350402152334e-08,
        "epoch": 0.9922849427799922,
        "step": 7717
    },
    {
        "loss": 1.0754,
        "grad_norm": 2.2358908653259277,
        "learning_rate": 3.0476148588176603e-08,
        "epoch": 0.9924135270669924,
        "step": 7718
    },
    {
        "loss": 1.3117,
        "grad_norm": 2.8245186805725098,
        "learning_rate": 2.948516961699399e-08,
        "epoch": 0.9925421113539925,
        "step": 7719
    },
    {
        "loss": 1.4229,
        "grad_norm": 2.0483386516571045,
        "learning_rate": 2.8510567270312315e-08,
        "epoch": 0.9926706956409926,
        "step": 7720
    },
    {
        "loss": 1.9667,
        "grad_norm": 1.457828402519226,
        "learning_rate": 2.7552341707803852e-08,
        "epoch": 0.9927992799279928,
        "step": 7721
    },
    {
        "loss": 0.8711,
        "grad_norm": 1.5459545850753784,
        "learning_rate": 2.6610493086409728e-08,
        "epoch": 0.9929278642149929,
        "step": 7722
    },
    {
        "loss": 1.2139,
        "grad_norm": 1.818019986152649,
        "learning_rate": 2.5685021560439837e-08,
        "epoch": 0.993056448501993,
        "step": 7723
    },
    {
        "loss": 1.6906,
        "grad_norm": 1.6679033041000366,
        "learning_rate": 2.4775927281495137e-08,
        "epoch": 0.9931850327889932,
        "step": 7724
    },
    {
        "loss": 2.0245,
        "grad_norm": 2.9326107501983643,
        "learning_rate": 2.388321039848984e-08,
        "epoch": 0.9933136170759933,
        "step": 7725
    },
    {
        "loss": 2.1611,
        "grad_norm": 2.4215946197509766,
        "learning_rate": 2.300687105768473e-08,
        "epoch": 0.9934422013629934,
        "step": 7726
    },
    {
        "loss": 1.3201,
        "grad_norm": 2.314034938812256,
        "learning_rate": 2.2146909402609438e-08,
        "epoch": 0.9935707856499936,
        "step": 7727
    },
    {
        "loss": 2.0679,
        "grad_norm": 2.7299704551696777,
        "learning_rate": 2.130332557416237e-08,
        "epoch": 0.9936993699369937,
        "step": 7728
    },
    {
        "loss": 1.566,
        "grad_norm": 1.939436674118042,
        "learning_rate": 2.047611971052188e-08,
        "epoch": 0.9938279542239938,
        "step": 7729
    },
    {
        "loss": 1.7311,
        "grad_norm": 2.141721487045288,
        "learning_rate": 1.966529194719069e-08,
        "epoch": 0.993956538510994,
        "step": 7730
    },
    {
        "loss": 1.3584,
        "grad_norm": 1.0831317901611328,
        "learning_rate": 1.8870842417018086e-08,
        "epoch": 0.9940851227979941,
        "step": 7731
    },
    {
        "loss": 1.212,
        "grad_norm": 1.478234887123108,
        "learning_rate": 1.809277125012221e-08,
        "epoch": 0.9942137070849942,
        "step": 7732
    },
    {
        "loss": 0.9688,
        "grad_norm": 2.1349856853485107,
        "learning_rate": 1.7331078573967763e-08,
        "epoch": 0.9943422913719944,
        "step": 7733
    },
    {
        "loss": 1.1069,
        "grad_norm": 1.702179193496704,
        "learning_rate": 1.6585764513332712e-08,
        "epoch": 0.9944708756589945,
        "step": 7734
    },
    {
        "loss": 1.501,
        "grad_norm": 2.0557618141174316,
        "learning_rate": 1.585682919031939e-08,
        "epoch": 0.9945994599459946,
        "step": 7735
    },
    {
        "loss": 1.051,
        "grad_norm": 2.402909278869629,
        "learning_rate": 1.5144272724332274e-08,
        "epoch": 0.9947280442329948,
        "step": 7736
    },
    {
        "loss": 1.5338,
        "grad_norm": 2.892807960510254,
        "learning_rate": 1.444809523208912e-08,
        "epoch": 0.9948566285199949,
        "step": 7737
    },
    {
        "loss": 0.7077,
        "grad_norm": 1.9697190523147583,
        "learning_rate": 1.3768296827654236e-08,
        "epoch": 0.994985212806995,
        "step": 7738
    },
    {
        "loss": 1.9103,
        "grad_norm": 1.8602089881896973,
        "learning_rate": 1.310487762236079e-08,
        "epoch": 0.9951137970939952,
        "step": 7739
    },
    {
        "loss": 1.513,
        "grad_norm": 3.0346951484680176,
        "learning_rate": 1.2457837724910716e-08,
        "epoch": 0.9952423813809952,
        "step": 7740
    },
    {
        "loss": 0.6792,
        "grad_norm": 2.595479726791382,
        "learning_rate": 1.1827177241285902e-08,
        "epoch": 0.9953709656679953,
        "step": 7741
    },
    {
        "loss": 1.29,
        "grad_norm": 1.6796808242797852,
        "learning_rate": 1.1212896274803709e-08,
        "epoch": 0.9954995499549955,
        "step": 7742
    },
    {
        "loss": 2.2161,
        "grad_norm": 2.4644880294799805,
        "learning_rate": 1.0614994926094745e-08,
        "epoch": 0.9956281342419956,
        "step": 7743
    },
    {
        "loss": 1.0323,
        "grad_norm": 2.4037351608276367,
        "learning_rate": 1.0033473293091788e-08,
        "epoch": 0.9957567185289957,
        "step": 7744
    },
    {
        "loss": 1.5841,
        "grad_norm": 3.1756114959716797,
        "learning_rate": 9.468331471063074e-09,
        "epoch": 0.9958853028159959,
        "step": 7745
    },
    {
        "loss": 1.2971,
        "grad_norm": 1.9960355758666992,
        "learning_rate": 8.919569552590102e-09,
        "epoch": 0.996013887102996,
        "step": 7746
    },
    {
        "loss": 1.7674,
        "grad_norm": 2.159959316253662,
        "learning_rate": 8.387187627567628e-09,
        "epoch": 0.9961424713899961,
        "step": 7747
    },
    {
        "loss": 1.8964,
        "grad_norm": 2.103776216506958,
        "learning_rate": 7.871185783203672e-09,
        "epoch": 0.9962710556769963,
        "step": 7748
    },
    {
        "loss": 1.0056,
        "grad_norm": 2.683953046798706,
        "learning_rate": 7.371564104030615e-09,
        "epoch": 0.9963996399639964,
        "step": 7749
    },
    {
        "loss": 1.6708,
        "grad_norm": 1.8875956535339355,
        "learning_rate": 6.888322671894098e-09,
        "epoch": 0.9965282242509965,
        "step": 7750
    },
    {
        "loss": 2.0681,
        "grad_norm": 1.9201300144195557,
        "learning_rate": 6.421461565953024e-09,
        "epoch": 0.9966568085379967,
        "step": 7751
    },
    {
        "loss": 1.4462,
        "grad_norm": 1.17183518409729,
        "learning_rate": 5.970980862690656e-09,
        "epoch": 0.9967853928249968,
        "step": 7752
    },
    {
        "loss": 2.0525,
        "grad_norm": 2.946911334991455,
        "learning_rate": 5.536880635892417e-09,
        "epoch": 0.9969139771119969,
        "step": 7753
    },
    {
        "loss": 1.8402,
        "grad_norm": 1.5386680364608765,
        "learning_rate": 5.119160956679192e-09,
        "epoch": 0.997042561398997,
        "step": 7754
    },
    {
        "loss": 2.1113,
        "grad_norm": 2.3525280952453613,
        "learning_rate": 4.717821893474028e-09,
        "epoch": 0.9971711456859972,
        "step": 7755
    },
    {
        "loss": 2.2312,
        "grad_norm": 2.628606081008911,
        "learning_rate": 4.332863512024332e-09,
        "epoch": 0.9972997299729973,
        "step": 7756
    },
    {
        "loss": 1.7909,
        "grad_norm": 1.9602304697036743,
        "learning_rate": 3.964285875390772e-09,
        "epoch": 0.9974283142599975,
        "step": 7757
    },
    {
        "loss": 1.2105,
        "grad_norm": 1.9847780466079712,
        "learning_rate": 3.6120890439583776e-09,
        "epoch": 0.9975568985469976,
        "step": 7758
    },
    {
        "loss": 1.9886,
        "grad_norm": 2.4624948501586914,
        "learning_rate": 3.2762730754143377e-09,
        "epoch": 0.9976854828339977,
        "step": 7759
    },
    {
        "loss": 2.1399,
        "grad_norm": 1.6790257692337036,
        "learning_rate": 2.9568380247813055e-09,
        "epoch": 0.9978140671209978,
        "step": 7760
    },
    {
        "loss": 1.9093,
        "grad_norm": 2.8341145515441895,
        "learning_rate": 2.6537839443729897e-09,
        "epoch": 0.997942651407998,
        "step": 7761
    },
    {
        "loss": 0.9804,
        "grad_norm": 2.8630130290985107,
        "learning_rate": 2.3671108838385638e-09,
        "epoch": 0.9980712356949981,
        "step": 7762
    },
    {
        "loss": 1.0335,
        "grad_norm": 1.7806082963943481,
        "learning_rate": 2.096818890140462e-09,
        "epoch": 0.9981998199819982,
        "step": 7763
    },
    {
        "loss": 0.9761,
        "grad_norm": 2.0782113075256348,
        "learning_rate": 1.842908007554378e-09,
        "epoch": 0.9983284042689984,
        "step": 7764
    },
    {
        "loss": 1.8954,
        "grad_norm": 3.026895523071289,
        "learning_rate": 1.6053782776803693e-09,
        "epoch": 0.9984569885559984,
        "step": 7765
    },
    {
        "loss": 1.7233,
        "grad_norm": 2.722689151763916,
        "learning_rate": 1.3842297394317527e-09,
        "epoch": 0.9985855728429985,
        "step": 7766
    },
    {
        "loss": 1.8878,
        "grad_norm": 1.6708179712295532,
        "learning_rate": 1.179462429024003e-09,
        "epoch": 0.9987141571299987,
        "step": 7767
    },
    {
        "loss": 1.87,
        "grad_norm": 2.571438789367676,
        "learning_rate": 9.910763800080603e-10,
        "epoch": 0.9988427414169988,
        "step": 7768
    },
    {
        "loss": 1.8889,
        "grad_norm": 2.2542130947113037,
        "learning_rate": 8.190716232481244e-10,
        "epoch": 0.9989713257039989,
        "step": 7769
    },
    {
        "loss": 1.0323,
        "grad_norm": 2.0054385662078857,
        "learning_rate": 6.634481869105535e-10,
        "epoch": 0.9990999099909991,
        "step": 7770
    },
    {
        "loss": 1.3969,
        "grad_norm": 2.068855047225952,
        "learning_rate": 5.242060964971707e-10,
        "epoch": 0.9992284942779992,
        "step": 7771
    },
    {
        "loss": 1.6299,
        "grad_norm": 1.7499984502792358,
        "learning_rate": 4.0134537481195666e-10,
        "epoch": 0.9993570785649993,
        "step": 7772
    },
    {
        "loss": 2.2702,
        "grad_norm": 1.727150797843933,
        "learning_rate": 2.94866041983255e-10,
        "epoch": 0.9994856628519995,
        "step": 7773
    },
    {
        "loss": 1.9054,
        "grad_norm": 1.9524140357971191,
        "learning_rate": 2.0476811546377151e-10,
        "epoch": 0.9996142471389996,
        "step": 7774
    },
    {
        "loss": 0.8462,
        "grad_norm": 2.2370505332946777,
        "learning_rate": 1.3105160999726807e-10,
        "epoch": 0.9997428314259997,
        "step": 7775
    },
    {
        "loss": 2.1632,
        "grad_norm": 2.745305299758911,
        "learning_rate": 7.371653766297115e-11,
        "epoch": 0.9998714157129999,
        "step": 7776
    },
    {
        "loss": 1.6407,
        "grad_norm": 2.5452182292938232,
        "learning_rate": 3.276290786446978e-11,
        "epoch": 1.0,
        "step": 7777
    },
    {
        "train_runtime": 19471.1685,
        "train_samples_per_second": 0.799,
        "train_steps_per_second": 0.399,
        "total_flos": 2.1999737380755456e+17,
        "train_loss": 1.8294640636461719,
        "epoch": 1.0,
        "step": 7777
    }
]