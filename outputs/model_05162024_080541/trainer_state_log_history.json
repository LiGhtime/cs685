[
    {
        "loss": 4.1138,
        "grad_norm": 2.705730676651001,
        "learning_rate": 8.000000000000001e-06,
        "epoch": 0.00027570995312930797,
        "step": 1
    },
    {
        "loss": 4.3345,
        "grad_norm": 3.2811758518218994,
        "learning_rate": 1.6000000000000003e-05,
        "epoch": 0.0005514199062586159,
        "step": 2
    },
    {
        "loss": 4.2245,
        "grad_norm": 3.449366569519043,
        "learning_rate": 2.4e-05,
        "epoch": 0.0008271298593879239,
        "step": 3
    },
    {
        "loss": 3.5452,
        "grad_norm": 2.8737716674804688,
        "learning_rate": 3.2000000000000005e-05,
        "epoch": 0.0011028398125172319,
        "step": 4
    },
    {
        "loss": 4.1957,
        "grad_norm": 4.064652919769287,
        "learning_rate": 4e-05,
        "epoch": 0.0013785497656465398,
        "step": 5
    },
    {
        "loss": 4.0836,
        "grad_norm": 3.9311392307281494,
        "learning_rate": 4.8e-05,
        "epoch": 0.0016542597187758478,
        "step": 6
    },
    {
        "loss": 3.7817,
        "grad_norm": 2.525315046310425,
        "learning_rate": 5.6000000000000006e-05,
        "epoch": 0.0019299696719051558,
        "step": 7
    },
    {
        "loss": 3.1614,
        "grad_norm": 2.523017644882202,
        "learning_rate": 6.400000000000001e-05,
        "epoch": 0.0022056796250344637,
        "step": 8
    },
    {
        "loss": 2.7618,
        "grad_norm": 2.0102505683898926,
        "learning_rate": 7.2e-05,
        "epoch": 0.0024813895781637717,
        "step": 9
    },
    {
        "loss": 3.1915,
        "grad_norm": 1.9407920837402344,
        "learning_rate": 8e-05,
        "epoch": 0.0027570995312930797,
        "step": 10
    },
    {
        "loss": 3.1671,
        "grad_norm": 1.8173410892486572,
        "learning_rate": 8.800000000000001e-05,
        "epoch": 0.0030328094844223876,
        "step": 11
    },
    {
        "loss": 3.2132,
        "grad_norm": 2.9980688095092773,
        "learning_rate": 9.6e-05,
        "epoch": 0.0033085194375516956,
        "step": 12
    },
    {
        "loss": 3.4374,
        "grad_norm": 3.3506829738616943,
        "learning_rate": 0.00010400000000000001,
        "epoch": 0.0035842293906810036,
        "step": 13
    },
    {
        "loss": 2.9911,
        "grad_norm": 1.8077147006988525,
        "learning_rate": 0.00011200000000000001,
        "epoch": 0.0038599393438103115,
        "step": 14
    },
    {
        "loss": 3.2854,
        "grad_norm": 1.7523375749588013,
        "learning_rate": 0.00012,
        "epoch": 0.0041356492969396195,
        "step": 15
    },
    {
        "loss": 2.8067,
        "grad_norm": 3.4071412086486816,
        "learning_rate": 0.00012800000000000002,
        "epoch": 0.0044113592500689275,
        "step": 16
    },
    {
        "loss": 2.9776,
        "grad_norm": 2.2983226776123047,
        "learning_rate": 0.00013600000000000003,
        "epoch": 0.0046870692031982355,
        "step": 17
    },
    {
        "loss": 3.298,
        "grad_norm": 2.588330030441284,
        "learning_rate": 0.000144,
        "epoch": 0.004962779156327543,
        "step": 18
    },
    {
        "loss": 2.7667,
        "grad_norm": 1.704809308052063,
        "learning_rate": 0.000152,
        "epoch": 0.005238489109456851,
        "step": 19
    },
    {
        "loss": 2.9566,
        "grad_norm": 2.1209702491760254,
        "learning_rate": 0.00016,
        "epoch": 0.005514199062586159,
        "step": 20
    },
    {
        "loss": 2.6854,
        "grad_norm": 3.222133159637451,
        "learning_rate": 0.000168,
        "epoch": 0.005789909015715467,
        "step": 21
    },
    {
        "loss": 2.7728,
        "grad_norm": 1.638214111328125,
        "learning_rate": 0.00017600000000000002,
        "epoch": 0.006065618968844775,
        "step": 22
    },
    {
        "loss": 2.7564,
        "grad_norm": 2.098283529281616,
        "learning_rate": 0.00018400000000000003,
        "epoch": 0.006341328921974083,
        "step": 23
    },
    {
        "loss": 2.746,
        "grad_norm": 2.5074803829193115,
        "learning_rate": 0.000192,
        "epoch": 0.006617038875103391,
        "step": 24
    },
    {
        "loss": 2.3585,
        "grad_norm": 2.221285820007324,
        "learning_rate": 0.0002,
        "epoch": 0.006892748828232699,
        "step": 25
    },
    {
        "loss": 2.3024,
        "grad_norm": 2.0789895057678223,
        "learning_rate": 0.00019999965768605272,
        "epoch": 0.007168458781362007,
        "step": 26
    },
    {
        "loss": 2.8526,
        "grad_norm": 1.4786521196365356,
        "learning_rate": 0.00019999863074655455,
        "epoch": 0.007444168734491315,
        "step": 27
    },
    {
        "loss": 2.2774,
        "grad_norm": 2.602529287338257,
        "learning_rate": 0.0001999969191885361,
        "epoch": 0.007719878687620623,
        "step": 28
    },
    {
        "loss": 2.4029,
        "grad_norm": 2.1563961505889893,
        "learning_rate": 0.00019999452302371524,
        "epoch": 0.007995588640749932,
        "step": 29
    },
    {
        "loss": 3.0378,
        "grad_norm": 1.4279791116714478,
        "learning_rate": 0.00019999144226849673,
        "epoch": 0.008271298593879239,
        "step": 30
    },
    {
        "loss": 1.8271,
        "grad_norm": 1.5569912195205688,
        "learning_rate": 0.00019998767694397236,
        "epoch": 0.008547008547008548,
        "step": 31
    },
    {
        "loss": 2.1892,
        "grad_norm": 2.5386533737182617,
        "learning_rate": 0.0001999832270759205,
        "epoch": 0.008822718500137855,
        "step": 32
    },
    {
        "loss": 2.509,
        "grad_norm": 1.8587461709976196,
        "learning_rate": 0.00019997809269480625,
        "epoch": 0.009098428453267164,
        "step": 33
    },
    {
        "loss": 2.363,
        "grad_norm": 2.2009012699127197,
        "learning_rate": 0.000199972273835781,
        "epoch": 0.009374138406396471,
        "step": 34
    },
    {
        "loss": 2.7728,
        "grad_norm": 1.6395940780639648,
        "learning_rate": 0.00019996577053868229,
        "epoch": 0.00964984835952578,
        "step": 35
    },
    {
        "loss": 2.5624,
        "grad_norm": 2.0330095291137695,
        "learning_rate": 0.00019995858284803348,
        "epoch": 0.009925558312655087,
        "step": 36
    },
    {
        "loss": 2.2783,
        "grad_norm": 1.9652208089828491,
        "learning_rate": 0.00019995071081304352,
        "epoch": 0.010201268265784396,
        "step": 37
    },
    {
        "loss": 2.5247,
        "grad_norm": 1.6416884660720825,
        "learning_rate": 0.0001999421544876066,
        "epoch": 0.010476978218913703,
        "step": 38
    },
    {
        "loss": 2.6303,
        "grad_norm": 1.352368950843811,
        "learning_rate": 0.00019993291393030163,
        "epoch": 0.010752688172043012,
        "step": 39
    },
    {
        "loss": 2.8423,
        "grad_norm": 1.1773836612701416,
        "learning_rate": 0.00019992298920439207,
        "epoch": 0.011028398125172319,
        "step": 40
    },
    {
        "loss": 2.1614,
        "grad_norm": 2.419611692428589,
        "learning_rate": 0.0001999123803778254,
        "epoch": 0.011304108078301628,
        "step": 41
    },
    {
        "loss": 2.755,
        "grad_norm": 1.5384854078292847,
        "learning_rate": 0.00019990108752323258,
        "epoch": 0.011579818031430935,
        "step": 42
    },
    {
        "loss": 2.145,
        "grad_norm": 2.230390787124634,
        "learning_rate": 0.00019988911071792763,
        "epoch": 0.011855527984560243,
        "step": 43
    },
    {
        "loss": 2.7228,
        "grad_norm": 1.5660169124603271,
        "learning_rate": 0.00019987645004390712,
        "epoch": 0.01213123793768955,
        "step": 44
    },
    {
        "loss": 1.7526,
        "grad_norm": 2.958688735961914,
        "learning_rate": 0.00019986310558784957,
        "epoch": 0.01240694789081886,
        "step": 45
    },
    {
        "loss": 2.3506,
        "grad_norm": 1.2376165390014648,
        "learning_rate": 0.00019984907744111482,
        "epoch": 0.012682657843948167,
        "step": 46
    },
    {
        "loss": 2.6355,
        "grad_norm": 0.9831042289733887,
        "learning_rate": 0.00019983436569974346,
        "epoch": 0.012958367797077475,
        "step": 47
    },
    {
        "loss": 2.315,
        "grad_norm": 1.746725082397461,
        "learning_rate": 0.00019981897046445618,
        "epoch": 0.013234077750206782,
        "step": 48
    },
    {
        "loss": 1.7518,
        "grad_norm": 2.671208381652832,
        "learning_rate": 0.00019980289184065312,
        "epoch": 0.013509787703336091,
        "step": 49
    },
    {
        "loss": 2.512,
        "grad_norm": 1.747990369796753,
        "learning_rate": 0.00019978612993841295,
        "epoch": 0.013785497656465398,
        "step": 50
    },
    {
        "loss": 2.161,
        "grad_norm": 2.021958351135254,
        "learning_rate": 0.0001997686848724924,
        "epoch": 0.014061207609594707,
        "step": 51
    },
    {
        "loss": 2.814,
        "grad_norm": 1.6899888515472412,
        "learning_rate": 0.00019975055676232516,
        "epoch": 0.014336917562724014,
        "step": 52
    },
    {
        "loss": 2.1382,
        "grad_norm": 1.7601797580718994,
        "learning_rate": 0.0001997317457320214,
        "epoch": 0.014612627515853323,
        "step": 53
    },
    {
        "loss": 2.2991,
        "grad_norm": 1.6865047216415405,
        "learning_rate": 0.00019971225191036667,
        "epoch": 0.01488833746898263,
        "step": 54
    },
    {
        "loss": 2.9444,
        "grad_norm": 1.9311267137527466,
        "learning_rate": 0.00019969207543082108,
        "epoch": 0.015164047422111939,
        "step": 55
    },
    {
        "loss": 2.4519,
        "grad_norm": 1.510703206062317,
        "learning_rate": 0.0001996712164315185,
        "epoch": 0.015439757375241246,
        "step": 56
    },
    {
        "loss": 2.0893,
        "grad_norm": 2.500588893890381,
        "learning_rate": 0.0001996496750552654,
        "epoch": 0.015715467328370553,
        "step": 57
    },
    {
        "loss": 2.4904,
        "grad_norm": 1.4021106958389282,
        "learning_rate": 0.00019962745144954007,
        "epoch": 0.015991177281499864,
        "step": 58
    },
    {
        "loss": 2.453,
        "grad_norm": 1.561508297920227,
        "learning_rate": 0.0001996045457664915,
        "epoch": 0.01626688723462917,
        "step": 59
    },
    {
        "loss": 2.6219,
        "grad_norm": 1.4688889980316162,
        "learning_rate": 0.0001995809581629384,
        "epoch": 0.016542597187758478,
        "step": 60
    },
    {
        "loss": 2.1679,
        "grad_norm": 1.8857589960098267,
        "learning_rate": 0.0001995566888003681,
        "epoch": 0.016818307140887785,
        "step": 61
    },
    {
        "loss": 2.6194,
        "grad_norm": 1.3378539085388184,
        "learning_rate": 0.0001995317378449354,
        "epoch": 0.017094017094017096,
        "step": 62
    },
    {
        "loss": 2.443,
        "grad_norm": 1.668859601020813,
        "learning_rate": 0.0001995061054674615,
        "epoch": 0.017369727047146403,
        "step": 63
    },
    {
        "loss": 2.8124,
        "grad_norm": 1.2072217464447021,
        "learning_rate": 0.00019947979184343283,
        "epoch": 0.01764543700027571,
        "step": 64
    },
    {
        "loss": 2.3076,
        "grad_norm": 1.7727712392807007,
        "learning_rate": 0.00019945279715299975,
        "epoch": 0.017921146953405017,
        "step": 65
    },
    {
        "loss": 2.2934,
        "grad_norm": 1.445913314819336,
        "learning_rate": 0.00019942512158097552,
        "epoch": 0.018196856906534328,
        "step": 66
    },
    {
        "loss": 1.9886,
        "grad_norm": 2.2750589847564697,
        "learning_rate": 0.00019939676531683477,
        "epoch": 0.018472566859663635,
        "step": 67
    },
    {
        "loss": 2.3539,
        "grad_norm": 2.536874532699585,
        "learning_rate": 0.00019936772855471243,
        "epoch": 0.018748276812792942,
        "step": 68
    },
    {
        "loss": 2.5407,
        "grad_norm": 1.5302448272705078,
        "learning_rate": 0.0001993380114934022,
        "epoch": 0.01902398676592225,
        "step": 69
    },
    {
        "loss": 2.4984,
        "grad_norm": 1.8210512399673462,
        "learning_rate": 0.00019930761433635544,
        "epoch": 0.01929969671905156,
        "step": 70
    },
    {
        "loss": 2.5889,
        "grad_norm": 1.0648701190948486,
        "learning_rate": 0.00019927653729167957,
        "epoch": 0.019575406672180867,
        "step": 71
    },
    {
        "loss": 2.3999,
        "grad_norm": 1.2760976552963257,
        "learning_rate": 0.00019924478057213664,
        "epoch": 0.019851116625310174,
        "step": 72
    },
    {
        "loss": 1.5989,
        "grad_norm": 2.9598488807678223,
        "learning_rate": 0.0001992123443951421,
        "epoch": 0.02012682657843948,
        "step": 73
    },
    {
        "loss": 2.0431,
        "grad_norm": 2.657315731048584,
        "learning_rate": 0.00019917922898276297,
        "epoch": 0.02040253653156879,
        "step": 74
    },
    {
        "loss": 2.0286,
        "grad_norm": 2.4505951404571533,
        "learning_rate": 0.0001991454345617167,
        "epoch": 0.0206782464846981,
        "step": 75
    },
    {
        "loss": 2.1891,
        "grad_norm": 2.2831053733825684,
        "learning_rate": 0.00019911096136336924,
        "epoch": 0.020953956437827406,
        "step": 76
    },
    {
        "loss": 2.4039,
        "grad_norm": 1.8457340002059937,
        "learning_rate": 0.00019907580962373378,
        "epoch": 0.021229666390956713,
        "step": 77
    },
    {
        "loss": 1.3255,
        "grad_norm": 2.6805155277252197,
        "learning_rate": 0.0001990399795834689,
        "epoch": 0.021505376344086023,
        "step": 78
    },
    {
        "loss": 2.0417,
        "grad_norm": 2.057202100753784,
        "learning_rate": 0.00019900347148787707,
        "epoch": 0.02178108629721533,
        "step": 79
    },
    {
        "loss": 2.0644,
        "grad_norm": 1.829014778137207,
        "learning_rate": 0.00019896628558690283,
        "epoch": 0.022056796250344637,
        "step": 80
    },
    {
        "loss": 1.7388,
        "grad_norm": 2.2503843307495117,
        "learning_rate": 0.00019892842213513134,
        "epoch": 0.022332506203473945,
        "step": 81
    },
    {
        "loss": 2.0872,
        "grad_norm": 2.152492046356201,
        "learning_rate": 0.0001988898813917863,
        "epoch": 0.022608216156603255,
        "step": 82
    },
    {
        "loss": 2.3753,
        "grad_norm": 1.4078766107559204,
        "learning_rate": 0.0001988506636207284,
        "epoch": 0.022883926109732562,
        "step": 83
    },
    {
        "loss": 1.5378,
        "grad_norm": 2.1673734188079834,
        "learning_rate": 0.0001988107690904534,
        "epoch": 0.02315963606286187,
        "step": 84
    },
    {
        "loss": 2.1993,
        "grad_norm": 1.8975999355316162,
        "learning_rate": 0.00019877019807409044,
        "epoch": 0.023435346015991176,
        "step": 85
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.9999436140060425,
        "learning_rate": 0.00019872895084939996,
        "epoch": 0.023711055969120487,
        "step": 86
    },
    {
        "loss": 1.969,
        "grad_norm": 1.4934109449386597,
        "learning_rate": 0.000198687027698772,
        "epoch": 0.023986765922249794,
        "step": 87
    },
    {
        "loss": 2.5258,
        "grad_norm": 1.7304884195327759,
        "learning_rate": 0.00019864442890922415,
        "epoch": 0.0242624758753791,
        "step": 88
    },
    {
        "loss": 2.2676,
        "grad_norm": 1.9289227724075317,
        "learning_rate": 0.0001986011547723996,
        "epoch": 0.024538185828508408,
        "step": 89
    },
    {
        "loss": 2.1897,
        "grad_norm": 1.6007721424102783,
        "learning_rate": 0.00019855720558456514,
        "epoch": 0.02481389578163772,
        "step": 90
    },
    {
        "loss": 2.3232,
        "grad_norm": 1.2166473865509033,
        "learning_rate": 0.00019851258164660919,
        "epoch": 0.025089605734767026,
        "step": 91
    },
    {
        "loss": 2.6458,
        "grad_norm": 1.174578070640564,
        "learning_rate": 0.00019846728326403966,
        "epoch": 0.025365315687896333,
        "step": 92
    },
    {
        "loss": 2.3059,
        "grad_norm": 1.7169604301452637,
        "learning_rate": 0.00019842131074698194,
        "epoch": 0.02564102564102564,
        "step": 93
    },
    {
        "loss": 1.6134,
        "grad_norm": 1.6211402416229248,
        "learning_rate": 0.0001983746644101767,
        "epoch": 0.02591673559415495,
        "step": 94
    },
    {
        "loss": 2.1525,
        "grad_norm": 1.4755640029907227,
        "learning_rate": 0.00019832734457297775,
        "epoch": 0.026192445547284258,
        "step": 95
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.620052456855774,
        "learning_rate": 0.0001982793515593499,
        "epoch": 0.026468155500413565,
        "step": 96
    },
    {
        "loss": 2.4335,
        "grad_norm": 1.5366065502166748,
        "learning_rate": 0.0001982306856978667,
        "epoch": 0.026743865453542872,
        "step": 97
    },
    {
        "loss": 1.8269,
        "grad_norm": 1.8900015354156494,
        "learning_rate": 0.00019818134732170828,
        "epoch": 0.027019575406672183,
        "step": 98
    },
    {
        "loss": 2.3556,
        "grad_norm": 1.2714598178863525,
        "learning_rate": 0.00019813133676865885,
        "epoch": 0.02729528535980149,
        "step": 99
    },
    {
        "loss": 2.4247,
        "grad_norm": 1.1877846717834473,
        "learning_rate": 0.00019808065438110464,
        "epoch": 0.027570995312930797,
        "step": 100
    },
    {
        "loss": 1.8245,
        "grad_norm": 2.116424560546875,
        "learning_rate": 0.00019802930050603142,
        "epoch": 0.027846705266060104,
        "step": 101
    },
    {
        "loss": 1.9825,
        "grad_norm": 1.9991416931152344,
        "learning_rate": 0.00019797727549502206,
        "epoch": 0.028122415219189414,
        "step": 102
    },
    {
        "loss": 2.2646,
        "grad_norm": 1.6944513320922852,
        "learning_rate": 0.00019792457970425445,
        "epoch": 0.02839812517231872,
        "step": 103
    },
    {
        "loss": 2.4145,
        "grad_norm": 1.1947126388549805,
        "learning_rate": 0.00019787121349449855,
        "epoch": 0.02867383512544803,
        "step": 104
    },
    {
        "loss": 2.3655,
        "grad_norm": 1.1788376569747925,
        "learning_rate": 0.00019781717723111437,
        "epoch": 0.028949545078577336,
        "step": 105
    },
    {
        "loss": 2.6638,
        "grad_norm": 1.1583210229873657,
        "learning_rate": 0.0001977624712840492,
        "epoch": 0.029225255031706646,
        "step": 106
    },
    {
        "loss": 1.9633,
        "grad_norm": 2.112976312637329,
        "learning_rate": 0.00019770709602783527,
        "epoch": 0.029500964984835953,
        "step": 107
    },
    {
        "loss": 2.1359,
        "grad_norm": 1.1548566818237305,
        "learning_rate": 0.000197651051841587,
        "epoch": 0.02977667493796526,
        "step": 108
    },
    {
        "loss": 2.3319,
        "grad_norm": 1.6829923391342163,
        "learning_rate": 0.00019759433910899855,
        "epoch": 0.030052384891094568,
        "step": 109
    },
    {
        "loss": 2.3176,
        "grad_norm": 2.42655086517334,
        "learning_rate": 0.00019753695821834105,
        "epoch": 0.030328094844223878,
        "step": 110
    },
    {
        "loss": 2.0902,
        "grad_norm": 1.538285255432129,
        "learning_rate": 0.0001974789095624601,
        "epoch": 0.030603804797353185,
        "step": 111
    },
    {
        "loss": 2.3341,
        "grad_norm": 1.702961802482605,
        "learning_rate": 0.00019742019353877302,
        "epoch": 0.030879514750482492,
        "step": 112
    },
    {
        "loss": 2.7574,
        "grad_norm": 1.304307460784912,
        "learning_rate": 0.00019736081054926613,
        "epoch": 0.0311552247036118,
        "step": 113
    },
    {
        "loss": 2.2968,
        "grad_norm": 1.9522106647491455,
        "learning_rate": 0.00019730076100049181,
        "epoch": 0.03143093465674111,
        "step": 114
    },
    {
        "loss": 1.9367,
        "grad_norm": 1.8986964225769043,
        "learning_rate": 0.00019724004530356616,
        "epoch": 0.03170664460987042,
        "step": 115
    },
    {
        "loss": 2.1288,
        "grad_norm": 1.289465069770813,
        "learning_rate": 0.00019717866387416568,
        "epoch": 0.03198235456299973,
        "step": 116
    },
    {
        "loss": 2.508,
        "grad_norm": 1.561758041381836,
        "learning_rate": 0.00019711661713252477,
        "epoch": 0.03225806451612903,
        "step": 117
    },
    {
        "loss": 2.4595,
        "grad_norm": 1.81086266040802,
        "learning_rate": 0.0001970539055034328,
        "epoch": 0.03253377446925834,
        "step": 118
    },
    {
        "loss": 2.8204,
        "grad_norm": 1.2866474390029907,
        "learning_rate": 0.00019699052941623098,
        "epoch": 0.032809484422387646,
        "step": 119
    },
    {
        "loss": 1.8565,
        "grad_norm": 1.5693913698196411,
        "learning_rate": 0.00019692648930480977,
        "epoch": 0.033085194375516956,
        "step": 120
    },
    {
        "loss": 2.4439,
        "grad_norm": 1.008136510848999,
        "learning_rate": 0.0001968617856076056,
        "epoch": 0.03336090432864627,
        "step": 121
    },
    {
        "loss": 1.0762,
        "grad_norm": 1.377092957496643,
        "learning_rate": 0.000196796418767598,
        "epoch": 0.03363661428177557,
        "step": 122
    },
    {
        "loss": 2.323,
        "grad_norm": 1.5860487222671509,
        "learning_rate": 0.00019673038923230662,
        "epoch": 0.03391232423490488,
        "step": 123
    },
    {
        "loss": 2.199,
        "grad_norm": 2.2648401260375977,
        "learning_rate": 0.0001966636974537881,
        "epoch": 0.03418803418803419,
        "step": 124
    },
    {
        "loss": 2.2796,
        "grad_norm": 1.8629733324050903,
        "learning_rate": 0.0001965963438886329,
        "epoch": 0.034463744141163495,
        "step": 125
    },
    {
        "loss": 2.2202,
        "grad_norm": 2.111325263977051,
        "learning_rate": 0.00019652832899796235,
        "epoch": 0.034739454094292806,
        "step": 126
    },
    {
        "loss": 2.5895,
        "grad_norm": 1.1945444345474243,
        "learning_rate": 0.0001964596532474254,
        "epoch": 0.03501516404742211,
        "step": 127
    },
    {
        "loss": 2.4654,
        "grad_norm": 1.7933075428009033,
        "learning_rate": 0.0001963903171071953,
        "epoch": 0.03529087400055142,
        "step": 128
    },
    {
        "loss": 2.5206,
        "grad_norm": 1.1801326274871826,
        "learning_rate": 0.00019632032105196671,
        "epoch": 0.03556658395368073,
        "step": 129
    },
    {
        "loss": 2.5187,
        "grad_norm": 1.470766305923462,
        "learning_rate": 0.00019624966556095207,
        "epoch": 0.035842293906810034,
        "step": 130
    },
    {
        "loss": 1.6596,
        "grad_norm": 1.4479029178619385,
        "learning_rate": 0.00019617835111787865,
        "epoch": 0.036118003859939345,
        "step": 131
    },
    {
        "loss": 2.1687,
        "grad_norm": 1.9406580924987793,
        "learning_rate": 0.00019610637821098497,
        "epoch": 0.036393713813068655,
        "step": 132
    },
    {
        "loss": 2.6192,
        "grad_norm": 1.4657093286514282,
        "learning_rate": 0.00019603374733301763,
        "epoch": 0.03666942376619796,
        "step": 133
    },
    {
        "loss": 2.0304,
        "grad_norm": 1.8521171808242798,
        "learning_rate": 0.00019596045898122788,
        "epoch": 0.03694513371932727,
        "step": 134
    },
    {
        "loss": 2.3508,
        "grad_norm": 1.588974952697754,
        "learning_rate": 0.00019588651365736825,
        "epoch": 0.03722084367245657,
        "step": 135
    },
    {
        "loss": 1.7741,
        "grad_norm": 1.9757194519042969,
        "learning_rate": 0.00019581191186768905,
        "epoch": 0.037496553625585884,
        "step": 136
    },
    {
        "loss": 1.9529,
        "grad_norm": 2.00935435295105,
        "learning_rate": 0.0001957366541229349,
        "epoch": 0.037772263578715194,
        "step": 137
    },
    {
        "loss": 1.2136,
        "grad_norm": 2.0920426845550537,
        "learning_rate": 0.00019566074093834134,
        "epoch": 0.0380479735318445,
        "step": 138
    },
    {
        "loss": 2.4096,
        "grad_norm": 1.682861089706421,
        "learning_rate": 0.00019558417283363123,
        "epoch": 0.03832368348497381,
        "step": 139
    },
    {
        "loss": 2.2136,
        "grad_norm": 1.5438604354858398,
        "learning_rate": 0.00019550695033301115,
        "epoch": 0.03859939343810312,
        "step": 140
    },
    {
        "loss": 1.9014,
        "grad_norm": 1.3788999319076538,
        "learning_rate": 0.00019542907396516784,
        "epoch": 0.03887510339123242,
        "step": 141
    },
    {
        "loss": 1.7291,
        "grad_norm": 2.0813229084014893,
        "learning_rate": 0.00019535054426326472,
        "epoch": 0.03915081334436173,
        "step": 142
    },
    {
        "loss": 1.8934,
        "grad_norm": 2.0415444374084473,
        "learning_rate": 0.000195271361764938,
        "epoch": 0.03942652329749104,
        "step": 143
    },
    {
        "loss": 1.8409,
        "grad_norm": 1.9656014442443848,
        "learning_rate": 0.00019519152701229312,
        "epoch": 0.03970223325062035,
        "step": 144
    },
    {
        "loss": 2.848,
        "grad_norm": 1.332241415977478,
        "learning_rate": 0.0001951110405519011,
        "epoch": 0.03997794320374966,
        "step": 145
    },
    {
        "loss": 1.8447,
        "grad_norm": 1.9069569110870361,
        "learning_rate": 0.0001950299029347947,
        "epoch": 0.04025365315687896,
        "step": 146
    },
    {
        "loss": 2.4907,
        "grad_norm": 1.273656964302063,
        "learning_rate": 0.00019494811471646464,
        "epoch": 0.04052936311000827,
        "step": 147
    },
    {
        "loss": 2.5619,
        "grad_norm": 1.1884000301361084,
        "learning_rate": 0.00019486567645685593,
        "epoch": 0.04080507306313758,
        "step": 148
    },
    {
        "loss": 2.1826,
        "grad_norm": 1.2234896421432495,
        "learning_rate": 0.00019478258872036386,
        "epoch": 0.041080783016266886,
        "step": 149
    },
    {
        "loss": 2.4275,
        "grad_norm": 1.8371915817260742,
        "learning_rate": 0.00019469885207583026,
        "epoch": 0.0413564929693962,
        "step": 150
    },
    {
        "loss": 2.6863,
        "grad_norm": 1.3103464841842651,
        "learning_rate": 0.00019461446709653955,
        "epoch": 0.0416322029225255,
        "step": 151
    },
    {
        "loss": 2.0392,
        "grad_norm": 1.5424087047576904,
        "learning_rate": 0.00019452943436021482,
        "epoch": 0.04190791287565481,
        "step": 152
    },
    {
        "loss": 2.8782,
        "grad_norm": 0.9398432970046997,
        "learning_rate": 0.00019444375444901396,
        "epoch": 0.04218362282878412,
        "step": 153
    },
    {
        "loss": 2.4255,
        "grad_norm": 1.924322485923767,
        "learning_rate": 0.0001943574279495255,
        "epoch": 0.042459332781913425,
        "step": 154
    },
    {
        "loss": 1.1725,
        "grad_norm": 2.12216854095459,
        "learning_rate": 0.00019427045545276474,
        "epoch": 0.042735042735042736,
        "step": 155
    },
    {
        "loss": 2.0418,
        "grad_norm": 2.291464328765869,
        "learning_rate": 0.00019418283755416967,
        "epoch": 0.043010752688172046,
        "step": 156
    },
    {
        "loss": 2.527,
        "grad_norm": 1.6756590604782104,
        "learning_rate": 0.00019409457485359681,
        "epoch": 0.04328646264130135,
        "step": 157
    },
    {
        "loss": 2.5021,
        "grad_norm": 1.671195149421692,
        "learning_rate": 0.0001940056679553173,
        "epoch": 0.04356217259443066,
        "step": 158
    },
    {
        "loss": 2.3771,
        "grad_norm": 1.1648333072662354,
        "learning_rate": 0.00019391611746801254,
        "epoch": 0.043837882547559964,
        "step": 159
    },
    {
        "loss": 2.463,
        "grad_norm": 1.0411858558654785,
        "learning_rate": 0.0001938259240047701,
        "epoch": 0.044113592500689275,
        "step": 160
    },
    {
        "loss": 2.0851,
        "grad_norm": 2.088357448577881,
        "learning_rate": 0.00019373508818307965,
        "epoch": 0.044389302453818585,
        "step": 161
    },
    {
        "loss": 2.1375,
        "grad_norm": 1.7604632377624512,
        "learning_rate": 0.00019364361062482858,
        "epoch": 0.04466501240694789,
        "step": 162
    },
    {
        "loss": 2.5552,
        "grad_norm": 1.2678459882736206,
        "learning_rate": 0.0001935514919562977,
        "epoch": 0.0449407223600772,
        "step": 163
    },
    {
        "loss": 2.2297,
        "grad_norm": 1.7659392356872559,
        "learning_rate": 0.00019345873280815712,
        "epoch": 0.04521643231320651,
        "step": 164
    },
    {
        "loss": 1.9377,
        "grad_norm": 1.4737002849578857,
        "learning_rate": 0.0001933653338154619,
        "epoch": 0.045492142266335814,
        "step": 165
    },
    {
        "loss": 1.7268,
        "grad_norm": 1.6904114484786987,
        "learning_rate": 0.00019327129561764753,
        "epoch": 0.045767852219465124,
        "step": 166
    },
    {
        "loss": 2.2154,
        "grad_norm": 1.6506826877593994,
        "learning_rate": 0.00019317661885852582,
        "epoch": 0.04604356217259443,
        "step": 167
    },
    {
        "loss": 2.2884,
        "grad_norm": 1.5700254440307617,
        "learning_rate": 0.0001930813041862802,
        "epoch": 0.04631927212572374,
        "step": 168
    },
    {
        "loss": 2.582,
        "grad_norm": 1.0093413591384888,
        "learning_rate": 0.00019298535225346157,
        "epoch": 0.04659498207885305,
        "step": 169
    },
    {
        "loss": 2.125,
        "grad_norm": 1.4075994491577148,
        "learning_rate": 0.00019288876371698356,
        "epoch": 0.04687069203198235,
        "step": 170
    },
    {
        "loss": 2.3031,
        "grad_norm": 1.680688500404358,
        "learning_rate": 0.0001927915392381183,
        "epoch": 0.04714640198511166,
        "step": 171
    },
    {
        "loss": 2.0612,
        "grad_norm": 1.9254822731018066,
        "learning_rate": 0.0001926936794824916,
        "epoch": 0.047422111938240974,
        "step": 172
    },
    {
        "loss": 2.4167,
        "grad_norm": 1.2092030048370361,
        "learning_rate": 0.0001925951851200787,
        "epoch": 0.04769782189137028,
        "step": 173
    },
    {
        "loss": 1.9591,
        "grad_norm": 1.9648500680923462,
        "learning_rate": 0.00019249605682519953,
        "epoch": 0.04797353184449959,
        "step": 174
    },
    {
        "loss": 2.4873,
        "grad_norm": 1.4693918228149414,
        "learning_rate": 0.000192396295276514,
        "epoch": 0.04824924179762889,
        "step": 175
    },
    {
        "loss": 1.9291,
        "grad_norm": 2.0615041255950928,
        "learning_rate": 0.00019229590115701748,
        "epoch": 0.0485249517507582,
        "step": 176
    },
    {
        "loss": 2.3,
        "grad_norm": 1.9479913711547852,
        "learning_rate": 0.00019219487515403612,
        "epoch": 0.04880066170388751,
        "step": 177
    },
    {
        "loss": 2.0666,
        "grad_norm": 1.588051676750183,
        "learning_rate": 0.00019209321795922218,
        "epoch": 0.049076371657016817,
        "step": 178
    },
    {
        "loss": 2.2956,
        "grad_norm": 1.2703078985214233,
        "learning_rate": 0.0001919909302685491,
        "epoch": 0.04935208161014613,
        "step": 179
    },
    {
        "loss": 1.9982,
        "grad_norm": 2.179994821548462,
        "learning_rate": 0.00019188801278230698,
        "epoch": 0.04962779156327544,
        "step": 180
    },
    {
        "loss": 2.5479,
        "grad_norm": 0.9245792627334595,
        "learning_rate": 0.0001917844662050976,
        "epoch": 0.04990350151640474,
        "step": 181
    },
    {
        "loss": 2.6204,
        "grad_norm": 1.3587220907211304,
        "learning_rate": 0.00019168029124582976,
        "epoch": 0.05017921146953405,
        "step": 182
    },
    {
        "loss": 1.3083,
        "grad_norm": 2.1936428546905518,
        "learning_rate": 0.00019157548861771425,
        "epoch": 0.050454921422663356,
        "step": 183
    },
    {
        "loss": 2.3665,
        "grad_norm": 1.7492389678955078,
        "learning_rate": 0.00019147005903825915,
        "epoch": 0.050730631375792666,
        "step": 184
    },
    {
        "loss": 2.0937,
        "grad_norm": 1.2396854162216187,
        "learning_rate": 0.00019136400322926472,
        "epoch": 0.05100634132892198,
        "step": 185
    },
    {
        "loss": 2.0955,
        "grad_norm": 1.5189579725265503,
        "learning_rate": 0.00019125732191681866,
        "epoch": 0.05128205128205128,
        "step": 186
    },
    {
        "loss": 2.3538,
        "grad_norm": 1.2037168741226196,
        "learning_rate": 0.0001911500158312909,
        "epoch": 0.05155776123518059,
        "step": 187
    },
    {
        "loss": 2.5102,
        "grad_norm": 1.2534291744232178,
        "learning_rate": 0.00019104208570732895,
        "epoch": 0.0518334711883099,
        "step": 188
    },
    {
        "loss": 2.2176,
        "grad_norm": 1.7891546487808228,
        "learning_rate": 0.00019093353228385245,
        "epoch": 0.052109181141439205,
        "step": 189
    },
    {
        "loss": 1.4785,
        "grad_norm": 1.9904396533966064,
        "learning_rate": 0.0001908243563040485,
        "epoch": 0.052384891094568516,
        "step": 190
    },
    {
        "loss": 2.2689,
        "grad_norm": 1.4577851295471191,
        "learning_rate": 0.00019071455851536623,
        "epoch": 0.05266060104769782,
        "step": 191
    },
    {
        "loss": 1.9156,
        "grad_norm": 1.6826039552688599,
        "learning_rate": 0.000190604139669512,
        "epoch": 0.05293631100082713,
        "step": 192
    },
    {
        "loss": 1.1276,
        "grad_norm": 2.305579423904419,
        "learning_rate": 0.000190493100522444,
        "epoch": 0.05321202095395644,
        "step": 193
    },
    {
        "loss": 1.4816,
        "grad_norm": 1.830501914024353,
        "learning_rate": 0.00019038144183436718,
        "epoch": 0.053487730907085744,
        "step": 194
    },
    {
        "loss": 2.4455,
        "grad_norm": 1.3476959466934204,
        "learning_rate": 0.00019026916436972808,
        "epoch": 0.053763440860215055,
        "step": 195
    },
    {
        "loss": 2.3879,
        "grad_norm": 1.429496169090271,
        "learning_rate": 0.0001901562688972096,
        "epoch": 0.054039150813344365,
        "step": 196
    },
    {
        "loss": 2.4321,
        "grad_norm": 1.6449624300003052,
        "learning_rate": 0.00019004275618972555,
        "epoch": 0.05431486076647367,
        "step": 197
    },
    {
        "loss": 1.3719,
        "grad_norm": 1.9039720296859741,
        "learning_rate": 0.00018992862702441564,
        "epoch": 0.05459057071960298,
        "step": 198
    },
    {
        "loss": 2.4558,
        "grad_norm": 1.0366519689559937,
        "learning_rate": 0.00018981388218263995,
        "epoch": 0.05486628067273228,
        "step": 199
    },
    {
        "loss": 2.3592,
        "grad_norm": 1.523840069770813,
        "learning_rate": 0.0001896985224499737,
        "epoch": 0.055141990625861594,
        "step": 200
    },
    {
        "loss": 3.0106,
        "grad_norm": 1.2922604084014893,
        "learning_rate": 0.00018958254861620175,
        "epoch": 0.055417700578990904,
        "step": 201
    },
    {
        "loss": 2.0075,
        "grad_norm": 1.879531979560852,
        "learning_rate": 0.00018946596147531336,
        "epoch": 0.05569341053212021,
        "step": 202
    },
    {
        "loss": 1.9402,
        "grad_norm": 1.9642468690872192,
        "learning_rate": 0.0001893487618254966,
        "epoch": 0.05596912048524952,
        "step": 203
    },
    {
        "loss": 1.937,
        "grad_norm": 1.8499188423156738,
        "learning_rate": 0.000189230950469133,
        "epoch": 0.05624483043837883,
        "step": 204
    },
    {
        "loss": 2.024,
        "grad_norm": 1.3118634223937988,
        "learning_rate": 0.0001891125282127919,
        "epoch": 0.05652054039150813,
        "step": 205
    },
    {
        "loss": 2.2395,
        "grad_norm": 1.8890599012374878,
        "learning_rate": 0.0001889934958672252,
        "epoch": 0.05679625034463744,
        "step": 206
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.8755099773406982,
        "learning_rate": 0.00018887385424736143,
        "epoch": 0.05707196029776675,
        "step": 207
    },
    {
        "loss": 1.7233,
        "grad_norm": 2.0118942260742188,
        "learning_rate": 0.00018875360417230054,
        "epoch": 0.05734767025089606,
        "step": 208
    },
    {
        "loss": 2.0507,
        "grad_norm": 1.4614933729171753,
        "learning_rate": 0.0001886327464653081,
        "epoch": 0.05762338020402537,
        "step": 209
    },
    {
        "loss": 2.3139,
        "grad_norm": 1.6348956823349,
        "learning_rate": 0.0001885112819538097,
        "epoch": 0.05789909015715467,
        "step": 210
    },
    {
        "loss": 2.2086,
        "grad_norm": 1.4634698629379272,
        "learning_rate": 0.00018838921146938523,
        "epoch": 0.05817480011028398,
        "step": 211
    },
    {
        "loss": 1.5621,
        "grad_norm": 2.0722789764404297,
        "learning_rate": 0.00018826653584776327,
        "epoch": 0.05845051006341329,
        "step": 212
    },
    {
        "loss": 1.8087,
        "grad_norm": 1.8139216899871826,
        "learning_rate": 0.00018814325592881538,
        "epoch": 0.058726220016542596,
        "step": 213
    },
    {
        "loss": 2.1583,
        "grad_norm": 1.3549447059631348,
        "learning_rate": 0.0001880193725565503,
        "epoch": 0.05900192996967191,
        "step": 214
    },
    {
        "loss": 1.3443,
        "grad_norm": 2.1236119270324707,
        "learning_rate": 0.00018789488657910806,
        "epoch": 0.05927763992280121,
        "step": 215
    },
    {
        "loss": 2.7653,
        "grad_norm": 2.272022247314453,
        "learning_rate": 0.00018776979884875446,
        "epoch": 0.05955334987593052,
        "step": 216
    },
    {
        "loss": 2.7313,
        "grad_norm": 1.0918000936508179,
        "learning_rate": 0.00018764411022187497,
        "epoch": 0.05982905982905983,
        "step": 217
    },
    {
        "loss": 1.7982,
        "grad_norm": 1.0236722230911255,
        "learning_rate": 0.00018751782155896897,
        "epoch": 0.060104769782189135,
        "step": 218
    },
    {
        "loss": 2.3867,
        "grad_norm": 1.4788095951080322,
        "learning_rate": 0.00018739093372464397,
        "epoch": 0.060380479735318446,
        "step": 219
    },
    {
        "loss": 2.416,
        "grad_norm": 1.3058284521102905,
        "learning_rate": 0.00018726344758760935,
        "epoch": 0.060656189688447756,
        "step": 220
    },
    {
        "loss": 0.9657,
        "grad_norm": 1.9128844738006592,
        "learning_rate": 0.00018713536402067081,
        "epoch": 0.06093189964157706,
        "step": 221
    },
    {
        "loss": 1.2216,
        "grad_norm": 1.668300747871399,
        "learning_rate": 0.0001870066839007242,
        "epoch": 0.06120760959470637,
        "step": 222
    },
    {
        "loss": 2.678,
        "grad_norm": 1.9951337575912476,
        "learning_rate": 0.00018687740810874952,
        "epoch": 0.061483319547835674,
        "step": 223
    },
    {
        "loss": 1.8904,
        "grad_norm": 2.0573482513427734,
        "learning_rate": 0.0001867475375298049,
        "epoch": 0.061759029500964985,
        "step": 224
    },
    {
        "loss": 1.8996,
        "grad_norm": 2.597205638885498,
        "learning_rate": 0.0001866170730530205,
        "epoch": 0.062034739454094295,
        "step": 225
    },
    {
        "loss": 2.5658,
        "grad_norm": 1.0617753267288208,
        "learning_rate": 0.00018648601557159258,
        "epoch": 0.0623104494072236,
        "step": 226
    },
    {
        "loss": 1.9201,
        "grad_norm": 1.6389894485473633,
        "learning_rate": 0.0001863543659827772,
        "epoch": 0.0625861593603529,
        "step": 227
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.5480966567993164,
        "learning_rate": 0.00018622212518788417,
        "epoch": 0.06286186931348221,
        "step": 228
    },
    {
        "loss": 2.5307,
        "grad_norm": 1.289801001548767,
        "learning_rate": 0.0001860892940922708,
        "epoch": 0.06313757926661152,
        "step": 229
    },
    {
        "loss": 2.3322,
        "grad_norm": 1.2286489009857178,
        "learning_rate": 0.00018595587360533596,
        "epoch": 0.06341328921974083,
        "step": 230
    },
    {
        "loss": 2.313,
        "grad_norm": 1.5542986392974854,
        "learning_rate": 0.00018582186464051337,
        "epoch": 0.06368899917287014,
        "step": 231
    },
    {
        "loss": 1.7335,
        "grad_norm": 1.9650331735610962,
        "learning_rate": 0.00018568726811526586,
        "epoch": 0.06396470912599946,
        "step": 232
    },
    {
        "loss": 2.1635,
        "grad_norm": 1.846877098083496,
        "learning_rate": 0.0001855520849510788,
        "epoch": 0.06424041907912875,
        "step": 233
    },
    {
        "loss": 2.4556,
        "grad_norm": 1.2070982456207275,
        "learning_rate": 0.0001854163160734538,
        "epoch": 0.06451612903225806,
        "step": 234
    },
    {
        "loss": 2.1356,
        "grad_norm": 1.570696234703064,
        "learning_rate": 0.00018527996241190248,
        "epoch": 0.06479183898538737,
        "step": 235
    },
    {
        "loss": 2.0204,
        "grad_norm": 1.6592830419540405,
        "learning_rate": 0.0001851430248999401,
        "epoch": 0.06506754893851668,
        "step": 236
    },
    {
        "loss": 2.5616,
        "grad_norm": 0.9576131701469421,
        "learning_rate": 0.00018500550447507895,
        "epoch": 0.065343258891646,
        "step": 237
    },
    {
        "loss": 2.4181,
        "grad_norm": 0.9267147183418274,
        "learning_rate": 0.0001848674020788223,
        "epoch": 0.06561896884477529,
        "step": 238
    },
    {
        "loss": 2.4655,
        "grad_norm": 1.2785706520080566,
        "learning_rate": 0.00018472871865665767,
        "epoch": 0.0658946787979046,
        "step": 239
    },
    {
        "loss": 2.2207,
        "grad_norm": 1.7734373807907104,
        "learning_rate": 0.00018458945515805044,
        "epoch": 0.06617038875103391,
        "step": 240
    },
    {
        "loss": 1.9481,
        "grad_norm": 2.1795876026153564,
        "learning_rate": 0.00018444961253643737,
        "epoch": 0.06644609870416322,
        "step": 241
    },
    {
        "loss": 2.2397,
        "grad_norm": 1.4876956939697266,
        "learning_rate": 0.00018430919174922006,
        "epoch": 0.06672180865729253,
        "step": 242
    },
    {
        "loss": 2.0335,
        "grad_norm": 2.0753729343414307,
        "learning_rate": 0.0001841681937577584,
        "epoch": 0.06699751861042183,
        "step": 243
    },
    {
        "loss": 1.8066,
        "grad_norm": 2.1141090393066406,
        "learning_rate": 0.00018402661952736393,
        "epoch": 0.06727322856355114,
        "step": 244
    },
    {
        "loss": 2.1623,
        "grad_norm": 1.3827167749404907,
        "learning_rate": 0.0001838844700272934,
        "epoch": 0.06754893851668045,
        "step": 245
    },
    {
        "loss": 1.6741,
        "grad_norm": 2.0907628536224365,
        "learning_rate": 0.00018374174623074182,
        "epoch": 0.06782464846980976,
        "step": 246
    },
    {
        "loss": 1.2082,
        "grad_norm": 2.088219404220581,
        "learning_rate": 0.00018359844911483624,
        "epoch": 0.06810035842293907,
        "step": 247
    },
    {
        "loss": 2.284,
        "grad_norm": 1.2626320123672485,
        "learning_rate": 0.00018345457966062857,
        "epoch": 0.06837606837606838,
        "step": 248
    },
    {
        "loss": 2.4407,
        "grad_norm": 1.3916069269180298,
        "learning_rate": 0.00018331013885308935,
        "epoch": 0.06865177832919768,
        "step": 249
    },
    {
        "loss": 1.6428,
        "grad_norm": 2.0657923221588135,
        "learning_rate": 0.00018316512768110055,
        "epoch": 0.06892748828232699,
        "step": 250
    },
    {
        "loss": 1.9467,
        "grad_norm": 1.7169053554534912,
        "learning_rate": 0.00018301954713744912,
        "epoch": 0.0692031982354563,
        "step": 251
    },
    {
        "loss": 2.3901,
        "grad_norm": 1.1342278718948364,
        "learning_rate": 0.00018287339821882007,
        "epoch": 0.06947890818858561,
        "step": 252
    },
    {
        "loss": 1.4412,
        "grad_norm": 1.6639684438705444,
        "learning_rate": 0.0001827266819257897,
        "epoch": 0.06975461814171492,
        "step": 253
    },
    {
        "loss": 2.1883,
        "grad_norm": 1.5733850002288818,
        "learning_rate": 0.00018257939926281865,
        "epoch": 0.07003032809484422,
        "step": 254
    },
    {
        "loss": 2.4217,
        "grad_norm": 1.0460541248321533,
        "learning_rate": 0.00018243155123824512,
        "epoch": 0.07030603804797353,
        "step": 255
    },
    {
        "loss": 2.147,
        "grad_norm": 1.5849438905715942,
        "learning_rate": 0.00018228313886427792,
        "epoch": 0.07058174800110284,
        "step": 256
    },
    {
        "loss": 1.5074,
        "grad_norm": 1.8993059396743774,
        "learning_rate": 0.00018213416315698955,
        "epoch": 0.07085745795423215,
        "step": 257
    },
    {
        "loss": 1.7693,
        "grad_norm": 1.8440438508987427,
        "learning_rate": 0.00018198462513630928,
        "epoch": 0.07113316790736146,
        "step": 258
    },
    {
        "loss": 2.1093,
        "grad_norm": 0.974493682384491,
        "learning_rate": 0.00018183452582601613,
        "epoch": 0.07140887786049076,
        "step": 259
    },
    {
        "loss": 1.6768,
        "grad_norm": 1.8082865476608276,
        "learning_rate": 0.00018168386625373178,
        "epoch": 0.07168458781362007,
        "step": 260
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.4270007610321045,
        "learning_rate": 0.00018153264745091377,
        "epoch": 0.07196029776674938,
        "step": 261
    },
    {
        "loss": 2.2012,
        "grad_norm": 1.5896300077438354,
        "learning_rate": 0.00018138087045284812,
        "epoch": 0.07223600771987869,
        "step": 262
    },
    {
        "loss": 2.1878,
        "grad_norm": 1.6115690469741821,
        "learning_rate": 0.00018122853629864261,
        "epoch": 0.072511717673008,
        "step": 263
    },
    {
        "loss": 2.5547,
        "grad_norm": 1.0972237586975098,
        "learning_rate": 0.0001810756460312192,
        "epoch": 0.07278742762613731,
        "step": 264
    },
    {
        "loss": 2.4594,
        "grad_norm": 0.957391619682312,
        "learning_rate": 0.0001809222006973075,
        "epoch": 0.07306313757926661,
        "step": 265
    },
    {
        "loss": 2.2454,
        "grad_norm": 1.7693620920181274,
        "learning_rate": 0.0001807682013474369,
        "epoch": 0.07333884753239592,
        "step": 266
    },
    {
        "loss": 2.1216,
        "grad_norm": 1.324980616569519,
        "learning_rate": 0.00018061364903592994,
        "epoch": 0.07361455748552523,
        "step": 267
    },
    {
        "loss": 1.6271,
        "grad_norm": 1.9026269912719727,
        "learning_rate": 0.00018045854482089495,
        "epoch": 0.07389026743865454,
        "step": 268
    },
    {
        "loss": 2.8136,
        "grad_norm": 1.2094812393188477,
        "learning_rate": 0.00018030288976421854,
        "epoch": 0.07416597739178385,
        "step": 269
    },
    {
        "loss": 2.5809,
        "grad_norm": 1.4177930355072021,
        "learning_rate": 0.00018014668493155874,
        "epoch": 0.07444168734491315,
        "step": 270
    },
    {
        "loss": 2.6839,
        "grad_norm": 1.0901979207992554,
        "learning_rate": 0.00017998993139233736,
        "epoch": 0.07471739729804246,
        "step": 271
    },
    {
        "loss": 1.9167,
        "grad_norm": 1.8075437545776367,
        "learning_rate": 0.00017983263021973281,
        "epoch": 0.07499310725117177,
        "step": 272
    },
    {
        "loss": 1.9384,
        "grad_norm": 1.938753604888916,
        "learning_rate": 0.00017967478249067287,
        "epoch": 0.07526881720430108,
        "step": 273
    },
    {
        "loss": 2.536,
        "grad_norm": 0.9040928483009338,
        "learning_rate": 0.0001795163892858271,
        "epoch": 0.07554452715743039,
        "step": 274
    },
    {
        "loss": 2.1777,
        "grad_norm": 0.9957439303398132,
        "learning_rate": 0.00017935745168959957,
        "epoch": 0.07582023711055969,
        "step": 275
    },
    {
        "loss": 2.0993,
        "grad_norm": 2.049960136413574,
        "learning_rate": 0.00017919797079012139,
        "epoch": 0.076095947063689,
        "step": 276
    },
    {
        "loss": 2.0705,
        "grad_norm": 1.07697594165802,
        "learning_rate": 0.00017903794767924324,
        "epoch": 0.0763716570168183,
        "step": 277
    },
    {
        "loss": 1.7991,
        "grad_norm": 1.7094316482543945,
        "learning_rate": 0.00017887738345252806,
        "epoch": 0.07664736696994762,
        "step": 278
    },
    {
        "loss": 1.8547,
        "grad_norm": 1.8545390367507935,
        "learning_rate": 0.0001787162792092433,
        "epoch": 0.07692307692307693,
        "step": 279
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.8647346496582031,
        "learning_rate": 0.0001785546360523535,
        "epoch": 0.07719878687620624,
        "step": 280
    },
    {
        "loss": 2.1417,
        "grad_norm": 1.4951014518737793,
        "learning_rate": 0.0001783924550885129,
        "epoch": 0.07747449682933553,
        "step": 281
    },
    {
        "loss": 1.4524,
        "grad_norm": 1.9267343282699585,
        "learning_rate": 0.00017822973742805752,
        "epoch": 0.07775020678246485,
        "step": 282
    },
    {
        "loss": 2.2313,
        "grad_norm": 1.522479772567749,
        "learning_rate": 0.0001780664841849979,
        "epoch": 0.07802591673559416,
        "step": 283
    },
    {
        "loss": 2.2595,
        "grad_norm": 1.762189269065857,
        "learning_rate": 0.00017790269647701128,
        "epoch": 0.07830162668872347,
        "step": 284
    },
    {
        "loss": 2.4324,
        "grad_norm": 0.9611438512802124,
        "learning_rate": 0.000177738375425434,
        "epoch": 0.07857733664185278,
        "step": 285
    },
    {
        "loss": 2.6608,
        "grad_norm": 0.9304969906806946,
        "learning_rate": 0.00017757352215525378,
        "epoch": 0.07885304659498207,
        "step": 286
    },
    {
        "loss": 2.2915,
        "grad_norm": 1.0724570751190186,
        "learning_rate": 0.00017740813779510214,
        "epoch": 0.07912875654811138,
        "step": 287
    },
    {
        "loss": 1.7469,
        "grad_norm": 2.2865869998931885,
        "learning_rate": 0.00017724222347724654,
        "epoch": 0.0794044665012407,
        "step": 288
    },
    {
        "loss": 2.5007,
        "grad_norm": 1.056511402130127,
        "learning_rate": 0.00017707578033758264,
        "epoch": 0.07968017645437,
        "step": 289
    },
    {
        "loss": 2.3585,
        "grad_norm": 1.716109275817871,
        "learning_rate": 0.00017690880951562666,
        "epoch": 0.07995588640749932,
        "step": 290
    },
    {
        "loss": 2.0781,
        "grad_norm": 0.8579834699630737,
        "learning_rate": 0.00017674131215450736,
        "epoch": 0.08023159636062861,
        "step": 291
    },
    {
        "loss": 2.1146,
        "grad_norm": 1.5821149349212646,
        "learning_rate": 0.00017657328940095843,
        "epoch": 0.08050730631375792,
        "step": 292
    },
    {
        "loss": 1.6614,
        "grad_norm": 2.0600154399871826,
        "learning_rate": 0.0001764047424053105,
        "epoch": 0.08078301626688723,
        "step": 293
    },
    {
        "loss": 1.4681,
        "grad_norm": 2.0578484535217285,
        "learning_rate": 0.00017623567232148335,
        "epoch": 0.08105872622001654,
        "step": 294
    },
    {
        "loss": 2.2423,
        "grad_norm": 1.6991605758666992,
        "learning_rate": 0.0001760660803069779,
        "epoch": 0.08133443617314585,
        "step": 295
    },
    {
        "loss": 2.3594,
        "grad_norm": 0.9896408915519714,
        "learning_rate": 0.00017589596752286838,
        "epoch": 0.08161014612627517,
        "step": 296
    },
    {
        "loss": 2.3099,
        "grad_norm": 1.2965588569641113,
        "learning_rate": 0.0001757253351337944,
        "epoch": 0.08188585607940446,
        "step": 297
    },
    {
        "loss": 2.2287,
        "grad_norm": 0.9631878733634949,
        "learning_rate": 0.00017555418430795284,
        "epoch": 0.08216156603253377,
        "step": 298
    },
    {
        "loss": 2.6323,
        "grad_norm": 1.249947428703308,
        "learning_rate": 0.00017538251621709007,
        "epoch": 0.08243727598566308,
        "step": 299
    },
    {
        "loss": 2.7602,
        "grad_norm": 1.8548526763916016,
        "learning_rate": 0.00017521033203649365,
        "epoch": 0.0827129859387924,
        "step": 300
    },
    {
        "loss": 2.2713,
        "grad_norm": 1.4382786750793457,
        "learning_rate": 0.00017503763294498458,
        "epoch": 0.0829886958919217,
        "step": 301
    },
    {
        "loss": 2.4514,
        "grad_norm": 1.3209048509597778,
        "learning_rate": 0.00017486442012490893,
        "epoch": 0.083264405845051,
        "step": 302
    },
    {
        "loss": 1.8947,
        "grad_norm": 2.095248222351074,
        "learning_rate": 0.00017469069476213007,
        "epoch": 0.08354011579818031,
        "step": 303
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.2779604196548462,
        "learning_rate": 0.00017451645804602024,
        "epoch": 0.08381582575130962,
        "step": 304
    },
    {
        "loss": 2.6129,
        "grad_norm": 1.1858960390090942,
        "learning_rate": 0.00017434171116945262,
        "epoch": 0.08409153570443893,
        "step": 305
    },
    {
        "loss": 2.5859,
        "grad_norm": 0.8378031253814697,
        "learning_rate": 0.00017416645532879305,
        "epoch": 0.08436724565756824,
        "step": 306
    },
    {
        "loss": 1.5666,
        "grad_norm": 1.9190924167633057,
        "learning_rate": 0.00017399069172389196,
        "epoch": 0.08464295561069754,
        "step": 307
    },
    {
        "loss": 2.5868,
        "grad_norm": 1.4107561111450195,
        "learning_rate": 0.00017381442155807594,
        "epoch": 0.08491866556382685,
        "step": 308
    },
    {
        "loss": 2.155,
        "grad_norm": 1.594030499458313,
        "learning_rate": 0.00017363764603813974,
        "epoch": 0.08519437551695616,
        "step": 309
    },
    {
        "loss": 2.5227,
        "grad_norm": 1.484945297241211,
        "learning_rate": 0.00017346036637433794,
        "epoch": 0.08547008547008547,
        "step": 310
    },
    {
        "loss": 2.7612,
        "grad_norm": 1.5182323455810547,
        "learning_rate": 0.0001732825837803765,
        "epoch": 0.08574579542321478,
        "step": 311
    },
    {
        "loss": 2.1308,
        "grad_norm": 1.6185730695724487,
        "learning_rate": 0.00017310429947340464,
        "epoch": 0.08602150537634409,
        "step": 312
    },
    {
        "loss": 2.0676,
        "grad_norm": 1.7134385108947754,
        "learning_rate": 0.00017292551467400653,
        "epoch": 0.08629721532947339,
        "step": 313
    },
    {
        "loss": 2.3034,
        "grad_norm": 1.4737229347229004,
        "learning_rate": 0.00017274623060619274,
        "epoch": 0.0865729252826027,
        "step": 314
    },
    {
        "loss": 2.244,
        "grad_norm": 1.7968534231185913,
        "learning_rate": 0.00017256644849739198,
        "epoch": 0.08684863523573201,
        "step": 315
    },
    {
        "loss": 2.3101,
        "grad_norm": 1.6085190773010254,
        "learning_rate": 0.00017238616957844274,
        "epoch": 0.08712434518886132,
        "step": 316
    },
    {
        "loss": 2.7686,
        "grad_norm": 1.480194091796875,
        "learning_rate": 0.0001722053950835848,
        "epoch": 0.08740005514199063,
        "step": 317
    },
    {
        "loss": 2.382,
        "grad_norm": 0.9717708230018616,
        "learning_rate": 0.00017202412625045077,
        "epoch": 0.08767576509511993,
        "step": 318
    },
    {
        "loss": 2.3142,
        "grad_norm": 1.4796454906463623,
        "learning_rate": 0.00017184236432005764,
        "epoch": 0.08795147504824924,
        "step": 319
    },
    {
        "loss": 2.2153,
        "grad_norm": 1.7110748291015625,
        "learning_rate": 0.00017166011053679827,
        "epoch": 0.08822718500137855,
        "step": 320
    },
    {
        "loss": 2.7209,
        "grad_norm": 1.0923866033554077,
        "learning_rate": 0.00017147736614843296,
        "epoch": 0.08850289495450786,
        "step": 321
    },
    {
        "loss": 1.9555,
        "grad_norm": 1.7599074840545654,
        "learning_rate": 0.0001712941324060807,
        "epoch": 0.08877860490763717,
        "step": 322
    },
    {
        "loss": 1.684,
        "grad_norm": 1.8514742851257324,
        "learning_rate": 0.0001711104105642108,
        "epoch": 0.08905431486076647,
        "step": 323
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.627328872680664,
        "learning_rate": 0.00017092620188063432,
        "epoch": 0.08933002481389578,
        "step": 324
    },
    {
        "loss": 2.1736,
        "grad_norm": 1.1978434324264526,
        "learning_rate": 0.00017074150761649522,
        "epoch": 0.08960573476702509,
        "step": 325
    },
    {
        "loss": 2.5977,
        "grad_norm": 1.1374677419662476,
        "learning_rate": 0.00017055632903626195,
        "epoch": 0.0898814447201544,
        "step": 326
    },
    {
        "loss": 2.2626,
        "grad_norm": 1.2952167987823486,
        "learning_rate": 0.00017037066740771878,
        "epoch": 0.09015715467328371,
        "step": 327
    },
    {
        "loss": 1.7869,
        "grad_norm": 1.7074122428894043,
        "learning_rate": 0.00017018452400195693,
        "epoch": 0.09043286462641302,
        "step": 328
    },
    {
        "loss": 2.3337,
        "grad_norm": 1.0416738986968994,
        "learning_rate": 0.00016999790009336615,
        "epoch": 0.09070857457954232,
        "step": 329
    },
    {
        "loss": 2.5087,
        "grad_norm": 1.1965688467025757,
        "learning_rate": 0.0001698107969596258,
        "epoch": 0.09098428453267163,
        "step": 330
    },
    {
        "loss": 2.2116,
        "grad_norm": 1.5511990785598755,
        "learning_rate": 0.000169623215881696,
        "epoch": 0.09125999448580094,
        "step": 331
    },
    {
        "loss": 2.8815,
        "grad_norm": 0.963909387588501,
        "learning_rate": 0.00016943515814380923,
        "epoch": 0.09153570443893025,
        "step": 332
    },
    {
        "loss": 2.4631,
        "grad_norm": 1.5421624183654785,
        "learning_rate": 0.0001692466250334612,
        "epoch": 0.09181141439205956,
        "step": 333
    },
    {
        "loss": 2.0947,
        "grad_norm": 1.6201343536376953,
        "learning_rate": 0.0001690576178414022,
        "epoch": 0.09208712434518886,
        "step": 334
    },
    {
        "loss": 2.2833,
        "grad_norm": 1.561150074005127,
        "learning_rate": 0.00016886813786162814,
        "epoch": 0.09236283429831817,
        "step": 335
    },
    {
        "loss": 1.8264,
        "grad_norm": 1.6104211807250977,
        "learning_rate": 0.00016867818639137183,
        "epoch": 0.09263854425144748,
        "step": 336
    },
    {
        "loss": 1.5591,
        "grad_norm": 2.155148983001709,
        "learning_rate": 0.00016848776473109406,
        "epoch": 0.09291425420457679,
        "step": 337
    },
    {
        "loss": 2.4259,
        "grad_norm": 1.6759594678878784,
        "learning_rate": 0.00016829687418447458,
        "epoch": 0.0931899641577061,
        "step": 338
    },
    {
        "loss": 2.5232,
        "grad_norm": 1.4620277881622314,
        "learning_rate": 0.00016810551605840336,
        "epoch": 0.0934656741108354,
        "step": 339
    },
    {
        "loss": 1.6853,
        "grad_norm": 2.163818120956421,
        "learning_rate": 0.00016791369166297147,
        "epoch": 0.0937413840639647,
        "step": 340
    },
    {
        "loss": 1.8919,
        "grad_norm": 1.7519413232803345,
        "learning_rate": 0.0001677214023114623,
        "epoch": 0.09401709401709402,
        "step": 341
    },
    {
        "loss": 2.423,
        "grad_norm": 0.9988484382629395,
        "learning_rate": 0.00016752864932034233,
        "epoch": 0.09429280397022333,
        "step": 342
    },
    {
        "loss": 2.2681,
        "grad_norm": 1.0920652151107788,
        "learning_rate": 0.0001673354340092523,
        "epoch": 0.09456851392335264,
        "step": 343
    },
    {
        "loss": 2.159,
        "grad_norm": 1.4124109745025635,
        "learning_rate": 0.00016714175770099813,
        "epoch": 0.09484422387648195,
        "step": 344
    },
    {
        "loss": 2.0528,
        "grad_norm": 1.3283851146697998,
        "learning_rate": 0.00016694762172154187,
        "epoch": 0.09511993382961124,
        "step": 345
    },
    {
        "loss": 2.1953,
        "grad_norm": 1.7002493143081665,
        "learning_rate": 0.0001667530273999926,
        "epoch": 0.09539564378274056,
        "step": 346
    },
    {
        "loss": 1.6803,
        "grad_norm": 2.2068111896514893,
        "learning_rate": 0.00016655797606859732,
        "epoch": 0.09567135373586987,
        "step": 347
    },
    {
        "loss": 1.9236,
        "grad_norm": 2.1665189266204834,
        "learning_rate": 0.00016636246906273186,
        "epoch": 0.09594706368899918,
        "step": 348
    },
    {
        "loss": 2.2186,
        "grad_norm": 1.7561405897140503,
        "learning_rate": 0.00016616650772089165,
        "epoch": 0.09622277364212849,
        "step": 349
    },
    {
        "loss": 2.3451,
        "grad_norm": 1.237045407295227,
        "learning_rate": 0.00016597009338468276,
        "epoch": 0.09649848359525778,
        "step": 350
    },
    {
        "loss": 2.7055,
        "grad_norm": 1.4046801328659058,
        "learning_rate": 0.00016577322739881254,
        "epoch": 0.0967741935483871,
        "step": 351
    },
    {
        "loss": 2.1649,
        "grad_norm": 1.3092617988586426,
        "learning_rate": 0.0001655759111110804,
        "epoch": 0.0970499035015164,
        "step": 352
    },
    {
        "loss": 2.1725,
        "grad_norm": 1.228277325630188,
        "learning_rate": 0.00016537814587236872,
        "epoch": 0.09732561345464572,
        "step": 353
    },
    {
        "loss": 2.2363,
        "grad_norm": 1.490598440170288,
        "learning_rate": 0.00016517993303663347,
        "epoch": 0.09760132340777503,
        "step": 354
    },
    {
        "loss": 2.1378,
        "grad_norm": 1.6784133911132812,
        "learning_rate": 0.000164981273960895,
        "epoch": 0.09787703336090432,
        "step": 355
    },
    {
        "loss": 1.8921,
        "grad_norm": 1.341226577758789,
        "learning_rate": 0.00016478217000522882,
        "epoch": 0.09815274331403363,
        "step": 356
    },
    {
        "loss": 1.7655,
        "grad_norm": 1.780712604522705,
        "learning_rate": 0.00016458262253275606,
        "epoch": 0.09842845326716294,
        "step": 357
    },
    {
        "loss": 1.9317,
        "grad_norm": 1.2965525388717651,
        "learning_rate": 0.00016438263290963448,
        "epoch": 0.09870416322029225,
        "step": 358
    },
    {
        "loss": 2.1522,
        "grad_norm": 1.3252414464950562,
        "learning_rate": 0.0001641822025050487,
        "epoch": 0.09897987317342156,
        "step": 359
    },
    {
        "loss": 2.1013,
        "grad_norm": 1.2735202312469482,
        "learning_rate": 0.0001639813326912013,
        "epoch": 0.09925558312655088,
        "step": 360
    },
    {
        "loss": 1.0393,
        "grad_norm": 2.006883382797241,
        "learning_rate": 0.000163780024843303,
        "epoch": 0.09953129307968017,
        "step": 361
    },
    {
        "loss": 2.2039,
        "grad_norm": 1.4319195747375488,
        "learning_rate": 0.0001635782803395635,
        "epoch": 0.09980700303280948,
        "step": 362
    },
    {
        "loss": 2.2741,
        "grad_norm": 1.545574426651001,
        "learning_rate": 0.0001633761005611819,
        "epoch": 0.1000827129859388,
        "step": 363
    },
    {
        "loss": 2.4006,
        "grad_norm": 1.251721978187561,
        "learning_rate": 0.00016317348689233742,
        "epoch": 0.1003584229390681,
        "step": 364
    },
    {
        "loss": 2.1534,
        "grad_norm": 1.1540601253509521,
        "learning_rate": 0.00016297044072017972,
        "epoch": 0.10063413289219741,
        "step": 365
    },
    {
        "loss": 2.4324,
        "grad_norm": 1.3125367164611816,
        "learning_rate": 0.00016276696343481956,
        "epoch": 0.10090984284532671,
        "step": 366
    },
    {
        "loss": 2.1482,
        "grad_norm": 1.2006222009658813,
        "learning_rate": 0.0001625630564293192,
        "epoch": 0.10118555279845602,
        "step": 367
    },
    {
        "loss": 1.4584,
        "grad_norm": 1.8124608993530273,
        "learning_rate": 0.00016235872109968282,
        "epoch": 0.10146126275158533,
        "step": 368
    },
    {
        "loss": 2.1804,
        "grad_norm": 1.0711488723754883,
        "learning_rate": 0.0001621539588448471,
        "epoch": 0.10173697270471464,
        "step": 369
    },
    {
        "loss": 2.8299,
        "grad_norm": 1.3441582918167114,
        "learning_rate": 0.00016194877106667164,
        "epoch": 0.10201268265784395,
        "step": 370
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.3428071737289429,
        "learning_rate": 0.00016174315916992914,
        "epoch": 0.10228839261097325,
        "step": 371
    },
    {
        "loss": 2.1306,
        "grad_norm": 1.4869085550308228,
        "learning_rate": 0.00016153712456229595,
        "epoch": 0.10256410256410256,
        "step": 372
    },
    {
        "loss": 2.2158,
        "grad_norm": 1.219022274017334,
        "learning_rate": 0.00016133066865434254,
        "epoch": 0.10283981251723187,
        "step": 373
    },
    {
        "loss": 1.3698,
        "grad_norm": 2.509876012802124,
        "learning_rate": 0.00016112379285952364,
        "epoch": 0.10311552247036118,
        "step": 374
    },
    {
        "loss": 1.3753,
        "grad_norm": 2.067582607269287,
        "learning_rate": 0.00016091649859416863,
        "epoch": 0.10339123242349049,
        "step": 375
    },
    {
        "loss": 2.1692,
        "grad_norm": 1.4968661069869995,
        "learning_rate": 0.00016070878727747189,
        "epoch": 0.1036669423766198,
        "step": 376
    },
    {
        "loss": 2.0121,
        "grad_norm": 1.7349270582199097,
        "learning_rate": 0.00016050066033148296,
        "epoch": 0.1039426523297491,
        "step": 377
    },
    {
        "loss": 2.0758,
        "grad_norm": 1.8004978895187378,
        "learning_rate": 0.0001602921191810971,
        "epoch": 0.10421836228287841,
        "step": 378
    },
    {
        "loss": 2.4957,
        "grad_norm": 0.7610899209976196,
        "learning_rate": 0.00016008316525404509,
        "epoch": 0.10449407223600772,
        "step": 379
    },
    {
        "loss": 1.9698,
        "grad_norm": 0.9485490918159485,
        "learning_rate": 0.00015987379998088383,
        "epoch": 0.10476978218913703,
        "step": 380
    },
    {
        "loss": 1.4194,
        "grad_norm": 1.717867136001587,
        "learning_rate": 0.0001596640247949864,
        "epoch": 0.10504549214226634,
        "step": 381
    },
    {
        "loss": 2.5154,
        "grad_norm": 1.3880585432052612,
        "learning_rate": 0.00015945384113253224,
        "epoch": 0.10532120209539564,
        "step": 382
    },
    {
        "loss": 2.3281,
        "grad_norm": 1.3960189819335938,
        "learning_rate": 0.0001592432504324973,
        "epoch": 0.10559691204852495,
        "step": 383
    },
    {
        "loss": 1.8778,
        "grad_norm": 1.977263331413269,
        "learning_rate": 0.0001590322541366443,
        "epoch": 0.10587262200165426,
        "step": 384
    },
    {
        "loss": 1.5791,
        "grad_norm": 2.387446403503418,
        "learning_rate": 0.0001588208536895127,
        "epoch": 0.10614833195478357,
        "step": 385
    },
    {
        "loss": 2.2402,
        "grad_norm": 1.2007046937942505,
        "learning_rate": 0.00015860905053840895,
        "epoch": 0.10642404190791288,
        "step": 386
    },
    {
        "loss": 2.3943,
        "grad_norm": 1.1147210597991943,
        "learning_rate": 0.0001583968461333965,
        "epoch": 0.10669975186104218,
        "step": 387
    },
    {
        "loss": 2.0837,
        "grad_norm": 2.244765043258667,
        "learning_rate": 0.0001581842419272859,
        "epoch": 0.10697546181417149,
        "step": 388
    },
    {
        "loss": 1.9732,
        "grad_norm": 1.2101422548294067,
        "learning_rate": 0.00015797123937562484,
        "epoch": 0.1072511717673008,
        "step": 389
    },
    {
        "loss": 2.5084,
        "grad_norm": 0.9430099129676819,
        "learning_rate": 0.00015775783993668822,
        "epoch": 0.10752688172043011,
        "step": 390
    },
    {
        "loss": 1.9254,
        "grad_norm": 1.6685580015182495,
        "learning_rate": 0.0001575440450714681,
        "epoch": 0.10780259167355942,
        "step": 391
    },
    {
        "loss": 1.7055,
        "grad_norm": 1.5076537132263184,
        "learning_rate": 0.0001573298562436638,
        "epoch": 0.10807830162668873,
        "step": 392
    },
    {
        "loss": 2.1099,
        "grad_norm": 1.9648040533065796,
        "learning_rate": 0.00015711527491967176,
        "epoch": 0.10835401157981803,
        "step": 393
    },
    {
        "loss": 2.5517,
        "grad_norm": 1.3080418109893799,
        "learning_rate": 0.00015690030256857557,
        "epoch": 0.10862972153294734,
        "step": 394
    },
    {
        "loss": 2.0574,
        "grad_norm": 1.2464900016784668,
        "learning_rate": 0.00015668494066213597,
        "epoch": 0.10890543148607665,
        "step": 395
    },
    {
        "loss": 2.164,
        "grad_norm": 0.8987696766853333,
        "learning_rate": 0.00015646919067478055,
        "epoch": 0.10918114143920596,
        "step": 396
    },
    {
        "loss": 2.3894,
        "grad_norm": 1.3109813928604126,
        "learning_rate": 0.00015625305408359402,
        "epoch": 0.10945685139233527,
        "step": 397
    },
    {
        "loss": 2.3681,
        "grad_norm": 1.9069582223892212,
        "learning_rate": 0.00015603653236830766,
        "epoch": 0.10973256134546457,
        "step": 398
    },
    {
        "loss": 2.3351,
        "grad_norm": 1.117161750793457,
        "learning_rate": 0.00015581962701128962,
        "epoch": 0.11000827129859388,
        "step": 399
    },
    {
        "loss": 1.1631,
        "grad_norm": 1.8934999704360962,
        "learning_rate": 0.00015560233949753443,
        "epoch": 0.11028398125172319,
        "step": 400
    },
    {
        "loss": 2.253,
        "grad_norm": 1.278192162513733,
        "learning_rate": 0.00015538467131465298,
        "epoch": 0.1105596912048525,
        "step": 401
    },
    {
        "loss": 1.7469,
        "grad_norm": 1.952303409576416,
        "learning_rate": 0.0001551666239528625,
        "epoch": 0.11083540115798181,
        "step": 402
    },
    {
        "loss": 1.7395,
        "grad_norm": 1.8292205333709717,
        "learning_rate": 0.00015494819890497593,
        "epoch": 0.1111111111111111,
        "step": 403
    },
    {
        "loss": 1.1222,
        "grad_norm": 1.5999467372894287,
        "learning_rate": 0.00015472939766639213,
        "epoch": 0.11138682106424042,
        "step": 404
    },
    {
        "loss": 1.7452,
        "grad_norm": 1.807928204536438,
        "learning_rate": 0.00015451022173508538,
        "epoch": 0.11166253101736973,
        "step": 405
    },
    {
        "loss": 1.5036,
        "grad_norm": 2.271697998046875,
        "learning_rate": 0.00015429067261159531,
        "epoch": 0.11193824097049904,
        "step": 406
    },
    {
        "loss": 2.5942,
        "grad_norm": 1.486126184463501,
        "learning_rate": 0.00015407075179901637,
        "epoch": 0.11221395092362835,
        "step": 407
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.6260085105895996,
        "learning_rate": 0.00015385046080298786,
        "epoch": 0.11248966087675766,
        "step": 408
    },
    {
        "loss": 2.569,
        "grad_norm": 1.4007984399795532,
        "learning_rate": 0.00015362980113168336,
        "epoch": 0.11276537082988695,
        "step": 409
    },
    {
        "loss": 2.4094,
        "grad_norm": 2.2887613773345947,
        "learning_rate": 0.00015340877429580052,
        "epoch": 0.11304108078301627,
        "step": 410
    },
    {
        "loss": 1.7521,
        "grad_norm": 1.6643455028533936,
        "learning_rate": 0.00015318738180855073,
        "epoch": 0.11331679073614558,
        "step": 411
    },
    {
        "loss": 2.5403,
        "grad_norm": 1.033072829246521,
        "learning_rate": 0.00015296562518564874,
        "epoch": 0.11359250068927489,
        "step": 412
    },
    {
        "loss": 1.7943,
        "grad_norm": 1.5837395191192627,
        "learning_rate": 0.00015274350594530217,
        "epoch": 0.1138682106424042,
        "step": 413
    },
    {
        "loss": 2.5332,
        "grad_norm": 1.5499402284622192,
        "learning_rate": 0.00015252102560820138,
        "epoch": 0.1141439205955335,
        "step": 414
    },
    {
        "loss": 2.015,
        "grad_norm": 1.5055814981460571,
        "learning_rate": 0.00015229818569750879,
        "epoch": 0.1144196305486628,
        "step": 415
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.5991500616073608,
        "learning_rate": 0.0001520749877388486,
        "epoch": 0.11469534050179211,
        "step": 416
    },
    {
        "loss": 1.8256,
        "grad_norm": 1.6059845685958862,
        "learning_rate": 0.00015185143326029623,
        "epoch": 0.11497105045492143,
        "step": 417
    },
    {
        "loss": 2.4143,
        "grad_norm": 1.090876817703247,
        "learning_rate": 0.00015162752379236808,
        "epoch": 0.11524676040805074,
        "step": 418
    },
    {
        "loss": 2.6421,
        "grad_norm": 1.2709529399871826,
        "learning_rate": 0.0001514032608680108,
        "epoch": 0.11552247036118003,
        "step": 419
    },
    {
        "loss": 2.163,
        "grad_norm": 1.4065892696380615,
        "learning_rate": 0.00015117864602259094,
        "epoch": 0.11579818031430934,
        "step": 420
    },
    {
        "loss": 1.8693,
        "grad_norm": 1.3070539236068726,
        "learning_rate": 0.00015095368079388432,
        "epoch": 0.11607389026743865,
        "step": 421
    },
    {
        "loss": 1.9708,
        "grad_norm": 1.338052749633789,
        "learning_rate": 0.00015072836672206573,
        "epoch": 0.11634960022056796,
        "step": 422
    },
    {
        "loss": 2.6879,
        "grad_norm": 1.072634220123291,
        "learning_rate": 0.0001505027053496981,
        "epoch": 0.11662531017369727,
        "step": 423
    },
    {
        "loss": 2.4067,
        "grad_norm": 1.3932133913040161,
        "learning_rate": 0.00015027669822172222,
        "epoch": 0.11690102012682659,
        "step": 424
    },
    {
        "loss": 1.9692,
        "grad_norm": 1.7806365489959717,
        "learning_rate": 0.00015005034688544578,
        "epoch": 0.11717673007995588,
        "step": 425
    },
    {
        "loss": 2.0522,
        "grad_norm": 0.8793577551841736,
        "learning_rate": 0.00014982365289053332,
        "epoch": 0.11745244003308519,
        "step": 426
    },
    {
        "loss": 2.3214,
        "grad_norm": 1.0103801488876343,
        "learning_rate": 0.0001495966177889951,
        "epoch": 0.1177281499862145,
        "step": 427
    },
    {
        "loss": 2.087,
        "grad_norm": 1.0587024688720703,
        "learning_rate": 0.0001493692431351767,
        "epoch": 0.11800385993934381,
        "step": 428
    },
    {
        "loss": 1.8125,
        "grad_norm": 1.868972659111023,
        "learning_rate": 0.00014914153048574852,
        "epoch": 0.11827956989247312,
        "step": 429
    },
    {
        "loss": 2.0992,
        "grad_norm": 2.3353259563446045,
        "learning_rate": 0.00014891348139969485,
        "epoch": 0.11855527984560242,
        "step": 430
    },
    {
        "loss": 1.422,
        "grad_norm": 2.329892635345459,
        "learning_rate": 0.00014868509743830332,
        "epoch": 0.11883098979873173,
        "step": 431
    },
    {
        "loss": 1.5084,
        "grad_norm": 1.847027063369751,
        "learning_rate": 0.00014845638016515423,
        "epoch": 0.11910669975186104,
        "step": 432
    },
    {
        "loss": 2.3377,
        "grad_norm": 1.4710429906845093,
        "learning_rate": 0.0001482273311461099,
        "epoch": 0.11938240970499035,
        "step": 433
    },
    {
        "loss": 2.0372,
        "grad_norm": 1.3635727167129517,
        "learning_rate": 0.00014799795194930373,
        "epoch": 0.11965811965811966,
        "step": 434
    },
    {
        "loss": 2.3861,
        "grad_norm": 1.1419836282730103,
        "learning_rate": 0.0001477682441451297,
        "epoch": 0.11993382961124896,
        "step": 435
    },
    {
        "loss": 2.6641,
        "grad_norm": 1.4538055658340454,
        "learning_rate": 0.00014753820930623156,
        "epoch": 0.12020953956437827,
        "step": 436
    },
    {
        "loss": 1.3727,
        "grad_norm": 2.486743688583374,
        "learning_rate": 0.00014730784900749195,
        "epoch": 0.12048524951750758,
        "step": 437
    },
    {
        "loss": 2.0557,
        "grad_norm": 1.137927770614624,
        "learning_rate": 0.00014707716482602172,
        "epoch": 0.12076095947063689,
        "step": 438
    },
    {
        "loss": 1.7938,
        "grad_norm": 1.5852980613708496,
        "learning_rate": 0.00014684615834114915,
        "epoch": 0.1210366694237662,
        "step": 439
    },
    {
        "loss": 2.0179,
        "grad_norm": 2.9905693531036377,
        "learning_rate": 0.00014661483113440906,
        "epoch": 0.12131237937689551,
        "step": 440
    },
    {
        "loss": 1.8607,
        "grad_norm": 2.1128311157226562,
        "learning_rate": 0.00014638318478953202,
        "epoch": 0.12158808933002481,
        "step": 441
    },
    {
        "loss": 2.4948,
        "grad_norm": 1.7051129341125488,
        "learning_rate": 0.00014615122089243357,
        "epoch": 0.12186379928315412,
        "step": 442
    },
    {
        "loss": 2.7169,
        "grad_norm": 1.7954457998275757,
        "learning_rate": 0.0001459189410312032,
        "epoch": 0.12213950923628343,
        "step": 443
    },
    {
        "loss": 2.2521,
        "grad_norm": 1.1060466766357422,
        "learning_rate": 0.0001456863467960937,
        "epoch": 0.12241521918941274,
        "step": 444
    },
    {
        "loss": 2.089,
        "grad_norm": 2.180144786834717,
        "learning_rate": 0.00014545343977951001,
        "epoch": 0.12269092914254205,
        "step": 445
    },
    {
        "loss": 2.0259,
        "grad_norm": 2.1382737159729004,
        "learning_rate": 0.0001452202215759986,
        "epoch": 0.12296663909567135,
        "step": 446
    },
    {
        "loss": 2.0603,
        "grad_norm": 1.5879566669464111,
        "learning_rate": 0.00014498669378223627,
        "epoch": 0.12324234904880066,
        "step": 447
    },
    {
        "loss": 1.5732,
        "grad_norm": 2.1799981594085693,
        "learning_rate": 0.00014475285799701951,
        "epoch": 0.12351805900192997,
        "step": 448
    },
    {
        "loss": 1.5792,
        "grad_norm": 1.3723539113998413,
        "learning_rate": 0.0001445187158212533,
        "epoch": 0.12379376895505928,
        "step": 449
    },
    {
        "loss": 2.421,
        "grad_norm": 1.0964560508728027,
        "learning_rate": 0.0001442842688579403,
        "epoch": 0.12406947890818859,
        "step": 450
    },
    {
        "loss": 1.3143,
        "grad_norm": 2.1196353435516357,
        "learning_rate": 0.0001440495187121698,
        "epoch": 0.12434518886131789,
        "step": 451
    },
    {
        "loss": 1.7168,
        "grad_norm": 1.7141674757003784,
        "learning_rate": 0.00014381446699110678,
        "epoch": 0.1246208988144472,
        "step": 452
    },
    {
        "loss": 1.1597,
        "grad_norm": 1.963308334350586,
        "learning_rate": 0.00014357911530398095,
        "epoch": 0.12489660876757651,
        "step": 453
    },
    {
        "loss": 2.6397,
        "grad_norm": 1.4613267183303833,
        "learning_rate": 0.00014334346526207554,
        "epoch": 0.1251723187207058,
        "step": 454
    },
    {
        "loss": 1.9784,
        "grad_norm": 1.7481728792190552,
        "learning_rate": 0.00014310751847871651,
        "epoch": 0.12544802867383512,
        "step": 455
    },
    {
        "loss": 1.9738,
        "grad_norm": 1.6396669149398804,
        "learning_rate": 0.0001428712765692613,
        "epoch": 0.12572373862696443,
        "step": 456
    },
    {
        "loss": 2.3899,
        "grad_norm": 1.3951536417007446,
        "learning_rate": 0.00014263474115108797,
        "epoch": 0.12599944858009374,
        "step": 457
    },
    {
        "loss": 2.4491,
        "grad_norm": 1.5814653635025024,
        "learning_rate": 0.00014239791384358397,
        "epoch": 0.12627515853322305,
        "step": 458
    },
    {
        "loss": 2.4154,
        "grad_norm": 1.1807942390441895,
        "learning_rate": 0.0001421607962681351,
        "epoch": 0.12655086848635236,
        "step": 459
    },
    {
        "loss": 1.0922,
        "grad_norm": 1.728318452835083,
        "learning_rate": 0.00014192339004811443,
        "epoch": 0.12682657843948167,
        "step": 460
    },
    {
        "loss": 1.9985,
        "grad_norm": 1.5989004373550415,
        "learning_rate": 0.00014168569680887114,
        "epoch": 0.12710228839261098,
        "step": 461
    },
    {
        "loss": 2.3207,
        "grad_norm": 1.3369715213775635,
        "learning_rate": 0.0001414477181777195,
        "epoch": 0.1273779983457403,
        "step": 462
    },
    {
        "loss": 2.1841,
        "grad_norm": 1.1765741109848022,
        "learning_rate": 0.00014120945578392755,
        "epoch": 0.1276537082988696,
        "step": 463
    },
    {
        "loss": 2.1036,
        "grad_norm": 1.690159797668457,
        "learning_rate": 0.0001409709112587061,
        "epoch": 0.1279294182519989,
        "step": 464
    },
    {
        "loss": 1.4462,
        "grad_norm": 2.230299711227417,
        "learning_rate": 0.00014073208623519758,
        "epoch": 0.1282051282051282,
        "step": 465
    },
    {
        "loss": 2.5527,
        "grad_norm": 1.503279447555542,
        "learning_rate": 0.00014049298234846465,
        "epoch": 0.1284808381582575,
        "step": 466
    },
    {
        "loss": 2.1596,
        "grad_norm": 1.6582101583480835,
        "learning_rate": 0.00014025360123547925,
        "epoch": 0.12875654811138681,
        "step": 467
    },
    {
        "loss": 2.1609,
        "grad_norm": 1.7115451097488403,
        "learning_rate": 0.00014001394453511123,
        "epoch": 0.12903225806451613,
        "step": 468
    },
    {
        "loss": 2.0037,
        "grad_norm": 1.152652382850647,
        "learning_rate": 0.00013977401388811723,
        "epoch": 0.12930796801764544,
        "step": 469
    },
    {
        "loss": 1.829,
        "grad_norm": 1.8329598903656006,
        "learning_rate": 0.00013953381093712936,
        "epoch": 0.12958367797077475,
        "step": 470
    },
    {
        "loss": 1.441,
        "grad_norm": 2.082808494567871,
        "learning_rate": 0.00013929333732664404,
        "epoch": 0.12985938792390406,
        "step": 471
    },
    {
        "loss": 2.1344,
        "grad_norm": 1.611271858215332,
        "learning_rate": 0.0001390525947030107,
        "epoch": 0.13013509787703337,
        "step": 472
    },
    {
        "loss": 2.0695,
        "grad_norm": 1.906800627708435,
        "learning_rate": 0.00013881158471442054,
        "epoch": 0.13041080783016268,
        "step": 473
    },
    {
        "loss": 2.5322,
        "grad_norm": 0.9591571092605591,
        "learning_rate": 0.00013857030901089505,
        "epoch": 0.130686517783292,
        "step": 474
    },
    {
        "loss": 2.0639,
        "grad_norm": 1.9870139360427856,
        "learning_rate": 0.0001383287692442751,
        "epoch": 0.13096222773642127,
        "step": 475
    },
    {
        "loss": 2.4429,
        "grad_norm": 1.9890336990356445,
        "learning_rate": 0.00013808696706820928,
        "epoch": 0.13123793768955058,
        "step": 476
    },
    {
        "loss": 2.6155,
        "grad_norm": 1.420355200767517,
        "learning_rate": 0.0001378449041381427,
        "epoch": 0.1315136476426799,
        "step": 477
    },
    {
        "loss": 2.5483,
        "grad_norm": 1.0978217124938965,
        "learning_rate": 0.0001376025821113058,
        "epoch": 0.1317893575958092,
        "step": 478
    },
    {
        "loss": 2.4014,
        "grad_norm": 1.1721522808074951,
        "learning_rate": 0.00013736000264670263,
        "epoch": 0.13206506754893851,
        "step": 479
    },
    {
        "loss": 1.8462,
        "grad_norm": 1.9339020252227783,
        "learning_rate": 0.00013711716740509998,
        "epoch": 0.13234077750206782,
        "step": 480
    },
    {
        "loss": 2.3179,
        "grad_norm": 1.0227497816085815,
        "learning_rate": 0.00013687407804901562,
        "epoch": 0.13261648745519714,
        "step": 481
    },
    {
        "loss": 0.9763,
        "grad_norm": 2.011302947998047,
        "learning_rate": 0.00013663073624270707,
        "epoch": 0.13289219740832645,
        "step": 482
    },
    {
        "loss": 2.3692,
        "grad_norm": 1.8129955530166626,
        "learning_rate": 0.00013638714365216028,
        "epoch": 0.13316790736145576,
        "step": 483
    },
    {
        "loss": 1.8252,
        "grad_norm": 2.051600694656372,
        "learning_rate": 0.000136143301945078,
        "epoch": 0.13344361731458507,
        "step": 484
    },
    {
        "loss": 2.4234,
        "grad_norm": 1.186882495880127,
        "learning_rate": 0.0001358992127908686,
        "epoch": 0.13371932726771438,
        "step": 485
    },
    {
        "loss": 2.1967,
        "grad_norm": 1.3501982688903809,
        "learning_rate": 0.0001356548778606345,
        "epoch": 0.13399503722084366,
        "step": 486
    },
    {
        "loss": 2.2147,
        "grad_norm": 1.833902359008789,
        "learning_rate": 0.00013541029882716083,
        "epoch": 0.13427074717397297,
        "step": 487
    },
    {
        "loss": 1.5936,
        "grad_norm": 1.8577191829681396,
        "learning_rate": 0.00013516547736490384,
        "epoch": 0.13454645712710228,
        "step": 488
    },
    {
        "loss": 2.2537,
        "grad_norm": 1.433768391609192,
        "learning_rate": 0.00013492041514997957,
        "epoch": 0.1348221670802316,
        "step": 489
    },
    {
        "loss": 2.2388,
        "grad_norm": 1.1610466241836548,
        "learning_rate": 0.0001346751138601523,
        "epoch": 0.1350978770333609,
        "step": 490
    },
    {
        "loss": 1.6876,
        "grad_norm": 1.852582335472107,
        "learning_rate": 0.0001344295751748231,
        "epoch": 0.1353735869864902,
        "step": 491
    },
    {
        "loss": 1.8827,
        "grad_norm": 2.2998578548431396,
        "learning_rate": 0.00013418380077501827,
        "epoch": 0.13564929693961952,
        "step": 492
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.52867591381073,
        "learning_rate": 0.00013393779234337794,
        "epoch": 0.13592500689274883,
        "step": 493
    },
    {
        "loss": 2.0715,
        "grad_norm": 1.09952974319458,
        "learning_rate": 0.00013369155156414442,
        "epoch": 0.13620071684587814,
        "step": 494
    },
    {
        "loss": 2.114,
        "grad_norm": 1.6614564657211304,
        "learning_rate": 0.0001334450801231508,
        "epoch": 0.13647642679900746,
        "step": 495
    },
    {
        "loss": 2.2885,
        "grad_norm": 1.4426450729370117,
        "learning_rate": 0.00013319837970780934,
        "epoch": 0.13675213675213677,
        "step": 496
    },
    {
        "loss": 1.6914,
        "grad_norm": 1.6665452718734741,
        "learning_rate": 0.00013295145200709988,
        "epoch": 0.13702784670526605,
        "step": 497
    },
    {
        "loss": 2.2084,
        "grad_norm": 1.0464143753051758,
        "learning_rate": 0.00013270429871155826,
        "epoch": 0.13730355665839536,
        "step": 498
    },
    {
        "loss": 2.6706,
        "grad_norm": 1.2649153470993042,
        "learning_rate": 0.00013245692151326498,
        "epoch": 0.13757926661152467,
        "step": 499
    },
    {
        "loss": 1.9606,
        "grad_norm": 1.4969888925552368,
        "learning_rate": 0.00013220932210583335,
        "epoch": 0.13785497656465398,
        "step": 500
    },
    {
        "loss": 1.4256,
        "grad_norm": 1.7098878622055054,
        "learning_rate": 0.0001319615021843979,
        "epoch": 0.1381306865177833,
        "step": 501
    },
    {
        "loss": 1.6317,
        "grad_norm": 1.9048218727111816,
        "learning_rate": 0.00013171346344560304,
        "epoch": 0.1384063964709126,
        "step": 502
    },
    {
        "loss": 1.4849,
        "grad_norm": 1.6909899711608887,
        "learning_rate": 0.00013146520758759107,
        "epoch": 0.1386821064240419,
        "step": 503
    },
    {
        "loss": 1.4803,
        "grad_norm": 1.895270586013794,
        "learning_rate": 0.0001312167363099909,
        "epoch": 0.13895781637717122,
        "step": 504
    },
    {
        "loss": 1.9962,
        "grad_norm": 1.4822933673858643,
        "learning_rate": 0.0001309680513139062,
        "epoch": 0.13923352633030053,
        "step": 505
    },
    {
        "loss": 2.4561,
        "grad_norm": 1.5459229946136475,
        "learning_rate": 0.00013071915430190378,
        "epoch": 0.13950923628342984,
        "step": 506
    },
    {
        "loss": 2.1722,
        "grad_norm": 1.766727328300476,
        "learning_rate": 0.0001304700469780021,
        "epoch": 0.13978494623655913,
        "step": 507
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.2491921186447144,
        "learning_rate": 0.0001302207310476593,
        "epoch": 0.14006065618968844,
        "step": 508
    },
    {
        "loss": 2.2754,
        "grad_norm": 1.3464000225067139,
        "learning_rate": 0.00012997120821776182,
        "epoch": 0.14033636614281775,
        "step": 509
    },
    {
        "loss": 2.1311,
        "grad_norm": 1.863861083984375,
        "learning_rate": 0.00012972148019661255,
        "epoch": 0.14061207609594706,
        "step": 510
    },
    {
        "loss": 2.1702,
        "grad_norm": 1.5028667449951172,
        "learning_rate": 0.0001294715486939192,
        "epoch": 0.14088778604907637,
        "step": 511
    },
    {
        "loss": 1.6506,
        "grad_norm": 1.4035756587982178,
        "learning_rate": 0.00012922141542078255,
        "epoch": 0.14116349600220568,
        "step": 512
    },
    {
        "loss": 2.5913,
        "grad_norm": 1.4428036212921143,
        "learning_rate": 0.00012897108208968474,
        "epoch": 0.141439205955335,
        "step": 513
    },
    {
        "loss": 1.8321,
        "grad_norm": 1.6205137968063354,
        "learning_rate": 0.0001287205504144776,
        "epoch": 0.1417149159084643,
        "step": 514
    },
    {
        "loss": 2.3492,
        "grad_norm": 1.3726075887680054,
        "learning_rate": 0.00012846982211037086,
        "epoch": 0.1419906258615936,
        "step": 515
    },
    {
        "loss": 2.6404,
        "grad_norm": 1.1067783832550049,
        "learning_rate": 0.00012821889889392044,
        "epoch": 0.14226633581472292,
        "step": 516
    },
    {
        "loss": 2.4349,
        "grad_norm": 1.5468330383300781,
        "learning_rate": 0.00012796778248301666,
        "epoch": 0.14254204576785223,
        "step": 517
    },
    {
        "loss": 1.5119,
        "grad_norm": 1.8082550764083862,
        "learning_rate": 0.00012771647459687254,
        "epoch": 0.14281775572098152,
        "step": 518
    },
    {
        "loss": 1.7548,
        "grad_norm": 1.4041868448257446,
        "learning_rate": 0.0001274649769560119,
        "epoch": 0.14309346567411083,
        "step": 519
    },
    {
        "loss": 1.7675,
        "grad_norm": 1.6337413787841797,
        "learning_rate": 0.00012721329128225785,
        "epoch": 0.14336917562724014,
        "step": 520
    },
    {
        "loss": 2.4911,
        "grad_norm": 1.1348958015441895,
        "learning_rate": 0.00012696141929872064,
        "epoch": 0.14364488558036945,
        "step": 521
    },
    {
        "loss": 2.8079,
        "grad_norm": 1.003402829170227,
        "learning_rate": 0.00012670936272978614,
        "epoch": 0.14392059553349876,
        "step": 522
    },
    {
        "loss": 2.5623,
        "grad_norm": 1.091962456703186,
        "learning_rate": 0.00012645712330110396,
        "epoch": 0.14419630548662807,
        "step": 523
    },
    {
        "loss": 1.5286,
        "grad_norm": 2.0930328369140625,
        "learning_rate": 0.00012620470273957558,
        "epoch": 0.14447201543975738,
        "step": 524
    },
    {
        "loss": 2.8214,
        "grad_norm": 1.5879980325698853,
        "learning_rate": 0.00012595210277334255,
        "epoch": 0.1447477253928867,
        "step": 525
    },
    {
        "loss": 2.2987,
        "grad_norm": 1.4409091472625732,
        "learning_rate": 0.00012569932513177472,
        "epoch": 0.145023435346016,
        "step": 526
    },
    {
        "loss": 1.9665,
        "grad_norm": 2.2697031497955322,
        "learning_rate": 0.00012544637154545836,
        "epoch": 0.1452991452991453,
        "step": 527
    },
    {
        "loss": 2.13,
        "grad_norm": 0.8733723163604736,
        "learning_rate": 0.00012519324374618419,
        "epoch": 0.14557485525227462,
        "step": 528
    },
    {
        "loss": 2.2947,
        "grad_norm": 1.4988762140274048,
        "learning_rate": 0.00012493994346693586,
        "epoch": 0.1458505652054039,
        "step": 529
    },
    {
        "loss": 2.5321,
        "grad_norm": 1.5014928579330444,
        "learning_rate": 0.00012468647244187766,
        "epoch": 0.14612627515853321,
        "step": 530
    },
    {
        "loss": 2.0619,
        "grad_norm": 1.2105481624603271,
        "learning_rate": 0.00012443283240634294,
        "epoch": 0.14640198511166252,
        "step": 531
    },
    {
        "loss": 2.5109,
        "grad_norm": 1.7147419452667236,
        "learning_rate": 0.00012417902509682214,
        "epoch": 0.14667769506479184,
        "step": 532
    },
    {
        "loss": 2.2297,
        "grad_norm": 1.0976080894470215,
        "learning_rate": 0.0001239250522509509,
        "epoch": 0.14695340501792115,
        "step": 533
    },
    {
        "loss": 2.35,
        "grad_norm": 1.5083484649658203,
        "learning_rate": 0.00012367091560749817,
        "epoch": 0.14722911497105046,
        "step": 534
    },
    {
        "loss": 2.0265,
        "grad_norm": 1.5638450384140015,
        "learning_rate": 0.00012341661690635434,
        "epoch": 0.14750482492417977,
        "step": 535
    },
    {
        "loss": 1.3408,
        "grad_norm": 1.8562982082366943,
        "learning_rate": 0.0001231621578885192,
        "epoch": 0.14778053487730908,
        "step": 536
    },
    {
        "loss": 2.5063,
        "grad_norm": 1.1955457925796509,
        "learning_rate": 0.00012290754029609016,
        "epoch": 0.1480562448304384,
        "step": 537
    },
    {
        "loss": 1.7932,
        "grad_norm": 2.1592438220977783,
        "learning_rate": 0.0001226527658722503,
        "epoch": 0.1483319547835677,
        "step": 538
    },
    {
        "loss": 2.6632,
        "grad_norm": 1.061814308166504,
        "learning_rate": 0.00012239783636125643,
        "epoch": 0.14860766473669698,
        "step": 539
    },
    {
        "loss": 1.9383,
        "grad_norm": 1.3837987184524536,
        "learning_rate": 0.00012214275350842704,
        "epoch": 0.1488833746898263,
        "step": 540
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.3670272827148438,
        "learning_rate": 0.00012188751906013052,
        "epoch": 0.1491590846429556,
        "step": 541
    },
    {
        "loss": 2.0133,
        "grad_norm": 1.708617091178894,
        "learning_rate": 0.00012163213476377309,
        "epoch": 0.1494347945960849,
        "step": 542
    },
    {
        "loss": 1.5273,
        "grad_norm": 1.746543526649475,
        "learning_rate": 0.0001213766023677869,
        "epoch": 0.14971050454921422,
        "step": 543
    },
    {
        "loss": 2.0723,
        "grad_norm": 1.0822004079818726,
        "learning_rate": 0.00012112092362161798,
        "epoch": 0.14998621450234353,
        "step": 544
    },
    {
        "loss": 2.186,
        "grad_norm": 1.5819780826568604,
        "learning_rate": 0.00012086510027571438,
        "epoch": 0.15026192445547285,
        "step": 545
    },
    {
        "loss": 1.8432,
        "grad_norm": 1.2861610651016235,
        "learning_rate": 0.00012060913408151409,
        "epoch": 0.15053763440860216,
        "step": 546
    },
    {
        "loss": 2.522,
        "grad_norm": 1.1876779794692993,
        "learning_rate": 0.00012035302679143301,
        "epoch": 0.15081334436173147,
        "step": 547
    },
    {
        "loss": 2.2452,
        "grad_norm": 0.8907331824302673,
        "learning_rate": 0.00012009678015885318,
        "epoch": 0.15108905431486078,
        "step": 548
    },
    {
        "loss": 2.0278,
        "grad_norm": 0.8956038355827332,
        "learning_rate": 0.00011984039593811045,
        "epoch": 0.1513647642679901,
        "step": 549
    },
    {
        "loss": 2.0631,
        "grad_norm": 1.0546671152114868,
        "learning_rate": 0.00011958387588448274,
        "epoch": 0.15164047422111937,
        "step": 550
    },
    {
        "loss": 2.2071,
        "grad_norm": 0.9430925846099854,
        "learning_rate": 0.00011932722175417795,
        "epoch": 0.15191618417424868,
        "step": 551
    },
    {
        "loss": 2.0899,
        "grad_norm": 1.972536325454712,
        "learning_rate": 0.00011907043530432173,
        "epoch": 0.152191894127378,
        "step": 552
    },
    {
        "loss": 2.4141,
        "grad_norm": 1.5011812448501587,
        "learning_rate": 0.00011881351829294585,
        "epoch": 0.1524676040805073,
        "step": 553
    },
    {
        "loss": 2.0761,
        "grad_norm": 1.699872374534607,
        "learning_rate": 0.00011855647247897577,
        "epoch": 0.1527433140336366,
        "step": 554
    },
    {
        "loss": 2.401,
        "grad_norm": 1.1934176683425903,
        "learning_rate": 0.00011829929962221887,
        "epoch": 0.15301902398676592,
        "step": 555
    },
    {
        "loss": 2.3734,
        "grad_norm": 1.1242727041244507,
        "learning_rate": 0.00011804200148335227,
        "epoch": 0.15329473393989523,
        "step": 556
    },
    {
        "loss": 2.2502,
        "grad_norm": 1.673578143119812,
        "learning_rate": 0.00011778457982391075,
        "epoch": 0.15357044389302454,
        "step": 557
    },
    {
        "loss": 2.4844,
        "grad_norm": 1.3379998207092285,
        "learning_rate": 0.00011752703640627487,
        "epoch": 0.15384615384615385,
        "step": 558
    },
    {
        "loss": 1.9636,
        "grad_norm": 1.5263770818710327,
        "learning_rate": 0.00011726937299365863,
        "epoch": 0.15412186379928317,
        "step": 559
    },
    {
        "loss": 2.1145,
        "grad_norm": 1.3036611080169678,
        "learning_rate": 0.00011701159135009771,
        "epoch": 0.15439757375241248,
        "step": 560
    },
    {
        "loss": 2.0671,
        "grad_norm": 1.1580296754837036,
        "learning_rate": 0.00011675369324043707,
        "epoch": 0.15467328370554176,
        "step": 561
    },
    {
        "loss": 1.2845,
        "grad_norm": 1.7962589263916016,
        "learning_rate": 0.00011649568043031916,
        "epoch": 0.15494899365867107,
        "step": 562
    },
    {
        "loss": 1.9926,
        "grad_norm": 1.4635552167892456,
        "learning_rate": 0.00011623755468617163,
        "epoch": 0.15522470361180038,
        "step": 563
    },
    {
        "loss": 1.9667,
        "grad_norm": 1.3241373300552368,
        "learning_rate": 0.00011597931777519531,
        "epoch": 0.1555004135649297,
        "step": 564
    },
    {
        "loss": 2.2734,
        "grad_norm": 1.010034203529358,
        "learning_rate": 0.00011572097146535217,
        "epoch": 0.155776123518059,
        "step": 565
    },
    {
        "loss": 1.3446,
        "grad_norm": 1.9280810356140137,
        "learning_rate": 0.00011546251752535308,
        "epoch": 0.1560518334711883,
        "step": 566
    },
    {
        "loss": 1.9063,
        "grad_norm": 1.680540680885315,
        "learning_rate": 0.0001152039577246458,
        "epoch": 0.15632754342431762,
        "step": 567
    },
    {
        "loss": 1.8876,
        "grad_norm": 2.280961513519287,
        "learning_rate": 0.00011494529383340291,
        "epoch": 0.15660325337744693,
        "step": 568
    },
    {
        "loss": 2.0681,
        "grad_norm": 0.8911132216453552,
        "learning_rate": 0.00011468652762250946,
        "epoch": 0.15687896333057624,
        "step": 569
    },
    {
        "loss": 2.1611,
        "grad_norm": 1.1997774839401245,
        "learning_rate": 0.00011442766086355123,
        "epoch": 0.15715467328370555,
        "step": 570
    },
    {
        "loss": 2.3183,
        "grad_norm": 0.9163613319396973,
        "learning_rate": 0.00011416869532880217,
        "epoch": 0.15743038323683484,
        "step": 571
    },
    {
        "loss": 1.9154,
        "grad_norm": 1.463243842124939,
        "learning_rate": 0.00011390963279121259,
        "epoch": 0.15770609318996415,
        "step": 572
    },
    {
        "loss": 2.0934,
        "grad_norm": 1.6360167264938354,
        "learning_rate": 0.00011365047502439689,
        "epoch": 0.15798180314309346,
        "step": 573
    },
    {
        "loss": 2.0447,
        "grad_norm": 1.8466386795043945,
        "learning_rate": 0.00011339122380262145,
        "epoch": 0.15825751309622277,
        "step": 574
    },
    {
        "loss": 1.9966,
        "grad_norm": 1.4382219314575195,
        "learning_rate": 0.00011313188090079245,
        "epoch": 0.15853322304935208,
        "step": 575
    },
    {
        "loss": 2.6818,
        "grad_norm": 0.9763526320457458,
        "learning_rate": 0.00011287244809444372,
        "epoch": 0.1588089330024814,
        "step": 576
    },
    {
        "loss": 1.9326,
        "grad_norm": 1.8999377489089966,
        "learning_rate": 0.00011261292715972462,
        "epoch": 0.1590846429556107,
        "step": 577
    },
    {
        "loss": 2.366,
        "grad_norm": 1.1597400903701782,
        "learning_rate": 0.00011235331987338788,
        "epoch": 0.15936035290874,
        "step": 578
    },
    {
        "loss": 2.0554,
        "grad_norm": 1.7095668315887451,
        "learning_rate": 0.00011209362801277739,
        "epoch": 0.15963606286186932,
        "step": 579
    },
    {
        "loss": 2.355,
        "grad_norm": 1.476503849029541,
        "learning_rate": 0.00011183385335581606,
        "epoch": 0.15991177281499863,
        "step": 580
    },
    {
        "loss": 2.2065,
        "grad_norm": 1.4324206113815308,
        "learning_rate": 0.00011157399768099366,
        "epoch": 0.16018748276812794,
        "step": 581
    },
    {
        "loss": 1.3122,
        "grad_norm": 1.922777533531189,
        "learning_rate": 0.00011131406276735461,
        "epoch": 0.16046319272125723,
        "step": 582
    },
    {
        "loss": 2.3221,
        "grad_norm": 1.7503623962402344,
        "learning_rate": 0.00011105405039448586,
        "epoch": 0.16073890267438654,
        "step": 583
    },
    {
        "loss": 1.6672,
        "grad_norm": 1.7720054388046265,
        "learning_rate": 0.00011079396234250463,
        "epoch": 0.16101461262751585,
        "step": 584
    },
    {
        "loss": 1.7794,
        "grad_norm": 2.2374777793884277,
        "learning_rate": 0.00011053380039204631,
        "epoch": 0.16129032258064516,
        "step": 585
    },
    {
        "loss": 2.0944,
        "grad_norm": 1.640228271484375,
        "learning_rate": 0.0001102735663242521,
        "epoch": 0.16156603253377447,
        "step": 586
    },
    {
        "loss": 2.0397,
        "grad_norm": 1.6546862125396729,
        "learning_rate": 0.00011001326192075709,
        "epoch": 0.16184174248690378,
        "step": 587
    },
    {
        "loss": 2.466,
        "grad_norm": 1.3521677255630493,
        "learning_rate": 0.00010975288896367783,
        "epoch": 0.1621174524400331,
        "step": 588
    },
    {
        "loss": 1.8063,
        "grad_norm": 1.1644338369369507,
        "learning_rate": 0.0001094924492356002,
        "epoch": 0.1623931623931624,
        "step": 589
    },
    {
        "loss": 2.3942,
        "grad_norm": 0.9750558137893677,
        "learning_rate": 0.00010923194451956724,
        "epoch": 0.1626688723462917,
        "step": 590
    },
    {
        "loss": 2.4274,
        "grad_norm": 1.0005598068237305,
        "learning_rate": 0.00010897137659906688,
        "epoch": 0.16294458229942102,
        "step": 591
    },
    {
        "loss": 2.3156,
        "grad_norm": 1.4968045949935913,
        "learning_rate": 0.00010871074725801979,
        "epoch": 0.16322029225255033,
        "step": 592
    },
    {
        "loss": 2.5692,
        "grad_norm": 1.1047213077545166,
        "learning_rate": 0.00010845005828076717,
        "epoch": 0.1634960022056796,
        "step": 593
    },
    {
        "loss": 2.7202,
        "grad_norm": 1.339833378791809,
        "learning_rate": 0.00010818931145205844,
        "epoch": 0.16377171215880892,
        "step": 594
    },
    {
        "loss": 2.0422,
        "grad_norm": 1.5670738220214844,
        "learning_rate": 0.00010792850855703916,
        "epoch": 0.16404742211193823,
        "step": 595
    },
    {
        "loss": 2.4182,
        "grad_norm": 1.2509208917617798,
        "learning_rate": 0.00010766765138123866,
        "epoch": 0.16432313206506755,
        "step": 596
    },
    {
        "loss": 2.532,
        "grad_norm": 1.4881492853164673,
        "learning_rate": 0.00010740674171055798,
        "epoch": 0.16459884201819686,
        "step": 597
    },
    {
        "loss": 2.2792,
        "grad_norm": 1.1085864305496216,
        "learning_rate": 0.00010714578133125743,
        "epoch": 0.16487455197132617,
        "step": 598
    },
    {
        "loss": 2.6203,
        "grad_norm": 1.3822768926620483,
        "learning_rate": 0.00010688477202994463,
        "epoch": 0.16515026192445548,
        "step": 599
    },
    {
        "loss": 1.9047,
        "grad_norm": 1.7896090745925903,
        "learning_rate": 0.00010662371559356204,
        "epoch": 0.1654259718775848,
        "step": 600
    },
    {
        "loss": 2.249,
        "grad_norm": 1.8161505460739136,
        "learning_rate": 0.00010636261380937482,
        "epoch": 0.1657016818307141,
        "step": 601
    },
    {
        "loss": 2.4664,
        "grad_norm": 0.9528531432151794,
        "learning_rate": 0.00010610146846495867,
        "epoch": 0.1659773917838434,
        "step": 602
    },
    {
        "loss": 1.5915,
        "grad_norm": 1.6238787174224854,
        "learning_rate": 0.0001058402813481874,
        "epoch": 0.1662531017369727,
        "step": 603
    },
    {
        "loss": 2.2644,
        "grad_norm": 1.1717373132705688,
        "learning_rate": 0.00010557905424722092,
        "epoch": 0.166528811690102,
        "step": 604
    },
    {
        "loss": 1.2122,
        "grad_norm": 1.636211633682251,
        "learning_rate": 0.00010531778895049282,
        "epoch": 0.1668045216432313,
        "step": 605
    },
    {
        "loss": 2.2744,
        "grad_norm": 1.0329926013946533,
        "learning_rate": 0.0001050564872466982,
        "epoch": 0.16708023159636062,
        "step": 606
    },
    {
        "loss": 1.995,
        "grad_norm": 2.1625988483428955,
        "learning_rate": 0.00010479515092478138,
        "epoch": 0.16735594154948993,
        "step": 607
    },
    {
        "loss": 1.6749,
        "grad_norm": 1.8828123807907104,
        "learning_rate": 0.00010453378177392375,
        "epoch": 0.16763165150261924,
        "step": 608
    },
    {
        "loss": 0.9585,
        "grad_norm": 1.7287918329238892,
        "learning_rate": 0.00010427238158353144,
        "epoch": 0.16790736145574855,
        "step": 609
    },
    {
        "loss": 1.1941,
        "grad_norm": 1.9120761156082153,
        "learning_rate": 0.00010401095214322303,
        "epoch": 0.16818307140887787,
        "step": 610
    },
    {
        "loss": 1.8819,
        "grad_norm": 1.7945833206176758,
        "learning_rate": 0.0001037494952428174,
        "epoch": 0.16845878136200718,
        "step": 611
    },
    {
        "loss": 1.7844,
        "grad_norm": 1.4691007137298584,
        "learning_rate": 0.00010348801267232147,
        "epoch": 0.1687344913151365,
        "step": 612
    },
    {
        "loss": 1.7763,
        "grad_norm": 2.1881728172302246,
        "learning_rate": 0.00010322650622191778,
        "epoch": 0.1690102012682658,
        "step": 613
    },
    {
        "loss": 2.3932,
        "grad_norm": 0.9264997243881226,
        "learning_rate": 0.0001029649776819525,
        "epoch": 0.16928591122139508,
        "step": 614
    },
    {
        "loss": 2.4353,
        "grad_norm": 1.2131105661392212,
        "learning_rate": 0.00010270342884292295,
        "epoch": 0.1695616211745244,
        "step": 615
    },
    {
        "loss": 2.5482,
        "grad_norm": 0.897010862827301,
        "learning_rate": 0.00010244186149546542,
        "epoch": 0.1698373311276537,
        "step": 616
    },
    {
        "loss": 2.6012,
        "grad_norm": 0.7904813289642334,
        "learning_rate": 0.00010218027743034295,
        "epoch": 0.170113041080783,
        "step": 617
    },
    {
        "loss": 2.003,
        "grad_norm": 2.2371702194213867,
        "learning_rate": 0.00010191867843843302,
        "epoch": 0.17038875103391232,
        "step": 618
    },
    {
        "loss": 1.4659,
        "grad_norm": 2.851133346557617,
        "learning_rate": 0.00010165706631071528,
        "epoch": 0.17066446098704163,
        "step": 619
    },
    {
        "loss": 2.3,
        "grad_norm": 1.2137407064437866,
        "learning_rate": 0.00010139544283825935,
        "epoch": 0.17094017094017094,
        "step": 620
    },
    {
        "loss": 2.2458,
        "grad_norm": 1.6645861864089966,
        "learning_rate": 0.0001011338098122125,
        "epoch": 0.17121588089330025,
        "step": 621
    },
    {
        "loss": 2.0312,
        "grad_norm": 1.3477020263671875,
        "learning_rate": 0.00010087216902378742,
        "epoch": 0.17149159084642956,
        "step": 622
    },
    {
        "loss": 1.9911,
        "grad_norm": 1.2763290405273438,
        "learning_rate": 0.00010061052226424987,
        "epoch": 0.17176730079955888,
        "step": 623
    },
    {
        "loss": 1.4595,
        "grad_norm": 1.8823010921478271,
        "learning_rate": 0.00010034887132490662,
        "epoch": 0.17204301075268819,
        "step": 624
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.6756292581558228,
        "learning_rate": 0.000100087217997093,
        "epoch": 0.17231872070581747,
        "step": 625
    },
    {
        "loss": 2.2715,
        "grad_norm": 1.2858197689056396,
        "learning_rate": 9.98255640721606e-05,
        "epoch": 0.17259443065894678,
        "step": 626
    },
    {
        "loss": 1.553,
        "grad_norm": 1.959567904472351,
        "learning_rate": 9.956391134146525e-05,
        "epoch": 0.1728701406120761,
        "step": 627
    },
    {
        "loss": 2.7008,
        "grad_norm": 0.9889084696769714,
        "learning_rate": 9.930226159635447e-05,
        "epoch": 0.1731458505652054,
        "step": 628
    },
    {
        "loss": 1.6366,
        "grad_norm": 1.4913092851638794,
        "learning_rate": 9.904061662815545e-05,
        "epoch": 0.1734215605183347,
        "step": 629
    },
    {
        "loss": 1.8384,
        "grad_norm": 2.228550434112549,
        "learning_rate": 9.877897822816261e-05,
        "epoch": 0.17369727047146402,
        "step": 630
    },
    {
        "loss": 2.7751,
        "grad_norm": 1.3420871496200562,
        "learning_rate": 9.85173481876254e-05,
        "epoch": 0.17397298042459333,
        "step": 631
    },
    {
        "loss": 2.2942,
        "grad_norm": 0.9477081894874573,
        "learning_rate": 9.825572829773611e-05,
        "epoch": 0.17424869037772264,
        "step": 632
    },
    {
        "loss": 2.5223,
        "grad_norm": 1.0790820121765137,
        "learning_rate": 9.799412034961742e-05,
        "epoch": 0.17452440033085195,
        "step": 633
    },
    {
        "loss": 2.3418,
        "grad_norm": 1.3308205604553223,
        "learning_rate": 9.773252613431037e-05,
        "epoch": 0.17480011028398126,
        "step": 634
    },
    {
        "loss": 2.3121,
        "grad_norm": 1.125872015953064,
        "learning_rate": 9.747094744276189e-05,
        "epoch": 0.17507582023711055,
        "step": 635
    },
    {
        "loss": 2.4894,
        "grad_norm": 1.312630295753479,
        "learning_rate": 9.720938606581268e-05,
        "epoch": 0.17535153019023986,
        "step": 636
    },
    {
        "loss": 0.9465,
        "grad_norm": 1.7550430297851562,
        "learning_rate": 9.69478437941849e-05,
        "epoch": 0.17562724014336917,
        "step": 637
    },
    {
        "loss": 2.1182,
        "grad_norm": 1.565777063369751,
        "learning_rate": 9.668632241846988e-05,
        "epoch": 0.17590295009649848,
        "step": 638
    },
    {
        "loss": 2.2072,
        "grad_norm": 1.0871607065200806,
        "learning_rate": 9.642482372911595e-05,
        "epoch": 0.1761786600496278,
        "step": 639
    },
    {
        "loss": 2.6258,
        "grad_norm": 0.9765799045562744,
        "learning_rate": 9.616334951641601e-05,
        "epoch": 0.1764543700027571,
        "step": 640
    },
    {
        "loss": 2.4195,
        "grad_norm": 1.4112910032272339,
        "learning_rate": 9.59019015704955e-05,
        "epoch": 0.1767300799558864,
        "step": 641
    },
    {
        "loss": 1.3914,
        "grad_norm": 1.8647363185882568,
        "learning_rate": 9.564048168130002e-05,
        "epoch": 0.17700578990901572,
        "step": 642
    },
    {
        "loss": 1.8198,
        "grad_norm": 1.8086812496185303,
        "learning_rate": 9.537909163858298e-05,
        "epoch": 0.17728149986214503,
        "step": 643
    },
    {
        "loss": 2.9105,
        "grad_norm": 0.9190798997879028,
        "learning_rate": 9.511773323189358e-05,
        "epoch": 0.17755720981527434,
        "step": 644
    },
    {
        "loss": 2.512,
        "grad_norm": 1.2702107429504395,
        "learning_rate": 9.485640825056434e-05,
        "epoch": 0.17783291976840365,
        "step": 645
    },
    {
        "loss": 1.8524,
        "grad_norm": 1.7743016481399536,
        "learning_rate": 9.4595118483699e-05,
        "epoch": 0.17810862972153294,
        "step": 646
    },
    {
        "loss": 2.3551,
        "grad_norm": 1.0573885440826416,
        "learning_rate": 9.433386572016022e-05,
        "epoch": 0.17838433967466225,
        "step": 647
    },
    {
        "loss": 1.9615,
        "grad_norm": 2.002237558364868,
        "learning_rate": 9.407265174855723e-05,
        "epoch": 0.17866004962779156,
        "step": 648
    },
    {
        "loss": 1.3156,
        "grad_norm": 1.837923526763916,
        "learning_rate": 9.381147835723379e-05,
        "epoch": 0.17893575958092087,
        "step": 649
    },
    {
        "loss": 2.2119,
        "grad_norm": 1.7931753396987915,
        "learning_rate": 9.355034733425576e-05,
        "epoch": 0.17921146953405018,
        "step": 650
    },
    {
        "loss": 2.2826,
        "grad_norm": 0.8880873322486877,
        "learning_rate": 9.328926046739899e-05,
        "epoch": 0.1794871794871795,
        "step": 651
    },
    {
        "loss": 2.3106,
        "grad_norm": 1.369165062904358,
        "learning_rate": 9.3028219544137e-05,
        "epoch": 0.1797628894403088,
        "step": 652
    },
    {
        "loss": 1.8615,
        "grad_norm": 1.4531452655792236,
        "learning_rate": 9.276722635162874e-05,
        "epoch": 0.1800385993934381,
        "step": 653
    },
    {
        "loss": 1.6903,
        "grad_norm": 1.6096676588058472,
        "learning_rate": 9.250628267670642e-05,
        "epoch": 0.18031430934656742,
        "step": 654
    },
    {
        "loss": 1.9511,
        "grad_norm": 1.4895356893539429,
        "learning_rate": 9.224539030586323e-05,
        "epoch": 0.18059001929969673,
        "step": 655
    },
    {
        "loss": 1.9209,
        "grad_norm": 1.5685627460479736,
        "learning_rate": 9.198455102524113e-05,
        "epoch": 0.18086572925282604,
        "step": 656
    },
    {
        "loss": 2.4524,
        "grad_norm": 0.9332872629165649,
        "learning_rate": 9.172376662061859e-05,
        "epoch": 0.18114143920595532,
        "step": 657
    },
    {
        "loss": 1.6705,
        "grad_norm": 2.0842156410217285,
        "learning_rate": 9.146303887739836e-05,
        "epoch": 0.18141714915908463,
        "step": 658
    },
    {
        "loss": 2.5965,
        "grad_norm": 1.119370698928833,
        "learning_rate": 9.120236958059535e-05,
        "epoch": 0.18169285911221394,
        "step": 659
    },
    {
        "loss": 2.2882,
        "grad_norm": 1.319873332977295,
        "learning_rate": 9.094176051482422e-05,
        "epoch": 0.18196856906534326,
        "step": 660
    },
    {
        "loss": 2.3533,
        "grad_norm": 1.1839077472686768,
        "learning_rate": 9.068121346428735e-05,
        "epoch": 0.18224427901847257,
        "step": 661
    },
    {
        "loss": 2.3807,
        "grad_norm": 0.9447193145751953,
        "learning_rate": 9.042073021276256e-05,
        "epoch": 0.18251998897160188,
        "step": 662
    },
    {
        "loss": 2.3379,
        "grad_norm": 1.1095614433288574,
        "learning_rate": 9.016031254359081e-05,
        "epoch": 0.1827956989247312,
        "step": 663
    },
    {
        "loss": 2.4474,
        "grad_norm": 0.8530705571174622,
        "learning_rate": 8.989996223966414e-05,
        "epoch": 0.1830714088778605,
        "step": 664
    },
    {
        "loss": 2.2503,
        "grad_norm": 1.6099234819412231,
        "learning_rate": 8.963968108341331e-05,
        "epoch": 0.1833471188309898,
        "step": 665
    },
    {
        "loss": 2.4009,
        "grad_norm": 1.0592021942138672,
        "learning_rate": 8.937947085679576e-05,
        "epoch": 0.18362282878411912,
        "step": 666
    },
    {
        "loss": 2.597,
        "grad_norm": 1.9560561180114746,
        "learning_rate": 8.911933334128327e-05,
        "epoch": 0.1838985387372484,
        "step": 667
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.7107094526290894,
        "learning_rate": 8.885927031784983e-05,
        "epoch": 0.1841742486903777,
        "step": 668
    },
    {
        "loss": 2.5094,
        "grad_norm": 1.1712902784347534,
        "learning_rate": 8.859928356695947e-05,
        "epoch": 0.18444995864350702,
        "step": 669
    },
    {
        "loss": 1.5611,
        "grad_norm": 1.9516205787658691,
        "learning_rate": 8.833937486855395e-05,
        "epoch": 0.18472566859663633,
        "step": 670
    },
    {
        "loss": 1.8151,
        "grad_norm": 1.885986089706421,
        "learning_rate": 8.807954600204079e-05,
        "epoch": 0.18500137854976564,
        "step": 671
    },
    {
        "loss": 2.586,
        "grad_norm": 1.9029775857925415,
        "learning_rate": 8.781979874628082e-05,
        "epoch": 0.18527708850289495,
        "step": 672
    },
    {
        "loss": 2.2298,
        "grad_norm": 1.4575947523117065,
        "learning_rate": 8.756013487957625e-05,
        "epoch": 0.18555279845602426,
        "step": 673
    },
    {
        "loss": 2.3989,
        "grad_norm": 1.4039840698242188,
        "learning_rate": 8.730055617965835e-05,
        "epoch": 0.18582850840915358,
        "step": 674
    },
    {
        "loss": 1.6669,
        "grad_norm": 1.7426769733428955,
        "learning_rate": 8.704106442367529e-05,
        "epoch": 0.18610421836228289,
        "step": 675
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.335096836090088,
        "learning_rate": 8.678166138818003e-05,
        "epoch": 0.1863799283154122,
        "step": 676
    },
    {
        "loss": 1.5862,
        "grad_norm": 1.9090840816497803,
        "learning_rate": 8.652234884911807e-05,
        "epoch": 0.1866556382685415,
        "step": 677
    },
    {
        "loss": 2.3454,
        "grad_norm": 2.0168120861053467,
        "learning_rate": 8.626312858181543e-05,
        "epoch": 0.1869313482216708,
        "step": 678
    },
    {
        "loss": 2.1393,
        "grad_norm": 1.6392899751663208,
        "learning_rate": 8.600400236096636e-05,
        "epoch": 0.1872070581748001,
        "step": 679
    },
    {
        "loss": 2.1531,
        "grad_norm": 0.9306721687316895,
        "learning_rate": 8.574497196062124e-05,
        "epoch": 0.1874827681279294,
        "step": 680
    },
    {
        "loss": 2.041,
        "grad_norm": 1.615552306175232,
        "learning_rate": 8.548603915417445e-05,
        "epoch": 0.18775847808105872,
        "step": 681
    },
    {
        "loss": 1.8099,
        "grad_norm": 1.661819338798523,
        "learning_rate": 8.522720571435221e-05,
        "epoch": 0.18803418803418803,
        "step": 682
    },
    {
        "loss": 1.6144,
        "grad_norm": 1.8260927200317383,
        "learning_rate": 8.496847341320045e-05,
        "epoch": 0.18830989798731734,
        "step": 683
    },
    {
        "loss": 2.6259,
        "grad_norm": 0.987581193447113,
        "learning_rate": 8.470984402207267e-05,
        "epoch": 0.18858560794044665,
        "step": 684
    },
    {
        "loss": 2.4192,
        "grad_norm": 1.538090705871582,
        "learning_rate": 8.445131931161783e-05,
        "epoch": 0.18886131789357596,
        "step": 685
    },
    {
        "loss": 2.297,
        "grad_norm": 1.1217527389526367,
        "learning_rate": 8.419290105176824e-05,
        "epoch": 0.18913702784670527,
        "step": 686
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.139613389968872,
        "learning_rate": 8.393459101172734e-05,
        "epoch": 0.18941273779983459,
        "step": 687
    },
    {
        "loss": 2.3919,
        "grad_norm": 1.6709314584732056,
        "learning_rate": 8.367639095995773e-05,
        "epoch": 0.1896884477529639,
        "step": 688
    },
    {
        "loss": 2.5111,
        "grad_norm": 1.7351677417755127,
        "learning_rate": 8.341830266416903e-05,
        "epoch": 0.18996415770609318,
        "step": 689
    },
    {
        "loss": 1.4237,
        "grad_norm": 1.4882103204727173,
        "learning_rate": 8.316032789130566e-05,
        "epoch": 0.1902398676592225,
        "step": 690
    },
    {
        "loss": 2.2398,
        "grad_norm": 1.4890302419662476,
        "learning_rate": 8.29024684075349e-05,
        "epoch": 0.1905155776123518,
        "step": 691
    },
    {
        "loss": 2.2945,
        "grad_norm": 1.121212124824524,
        "learning_rate": 8.26447259782347e-05,
        "epoch": 0.1907912875654811,
        "step": 692
    },
    {
        "loss": 2.4202,
        "grad_norm": 1.0211867094039917,
        "learning_rate": 8.23871023679816e-05,
        "epoch": 0.19106699751861042,
        "step": 693
    },
    {
        "loss": 2.6018,
        "grad_norm": 1.0170155763626099,
        "learning_rate": 8.212959934053876e-05,
        "epoch": 0.19134270747173973,
        "step": 694
    },
    {
        "loss": 1.8529,
        "grad_norm": 1.398200511932373,
        "learning_rate": 8.187221865884367e-05,
        "epoch": 0.19161841742486904,
        "step": 695
    },
    {
        "loss": 2.4575,
        "grad_norm": 1.0657705068588257,
        "learning_rate": 8.161496208499633e-05,
        "epoch": 0.19189412737799835,
        "step": 696
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.17122220993042,
        "learning_rate": 8.135783138024694e-05,
        "epoch": 0.19216983733112766,
        "step": 697
    },
    {
        "loss": 2.1128,
        "grad_norm": 1.7643362283706665,
        "learning_rate": 8.110082830498407e-05,
        "epoch": 0.19244554728425697,
        "step": 698
    },
    {
        "loss": 2.0484,
        "grad_norm": 1.6636583805084229,
        "learning_rate": 8.084395461872249e-05,
        "epoch": 0.19272125723738626,
        "step": 699
    },
    {
        "loss": 2.401,
        "grad_norm": 1.0657775402069092,
        "learning_rate": 8.058721208009103e-05,
        "epoch": 0.19299696719051557,
        "step": 700
    },
    {
        "loss": 2.4474,
        "grad_norm": 1.9180172681808472,
        "learning_rate": 8.033060244682082e-05,
        "epoch": 0.19327267714364488,
        "step": 701
    },
    {
        "loss": 2.813,
        "grad_norm": 1.5376036167144775,
        "learning_rate": 8.007412747573291e-05,
        "epoch": 0.1935483870967742,
        "step": 702
    },
    {
        "loss": 2.1709,
        "grad_norm": 1.8639214038848877,
        "learning_rate": 7.981778892272653e-05,
        "epoch": 0.1938240970499035,
        "step": 703
    },
    {
        "loss": 2.1307,
        "grad_norm": 1.3061463832855225,
        "learning_rate": 7.956158854276695e-05,
        "epoch": 0.1940998070030328,
        "step": 704
    },
    {
        "loss": 1.6068,
        "grad_norm": 1.884768009185791,
        "learning_rate": 7.930552808987337e-05,
        "epoch": 0.19437551695616212,
        "step": 705
    },
    {
        "loss": 2.3213,
        "grad_norm": 1.3614041805267334,
        "learning_rate": 7.904960931710714e-05,
        "epoch": 0.19465122690929143,
        "step": 706
    },
    {
        "loss": 2.0636,
        "grad_norm": 1.4729455709457397,
        "learning_rate": 7.879383397655951e-05,
        "epoch": 0.19492693686242074,
        "step": 707
    },
    {
        "loss": 2.0194,
        "grad_norm": 1.1716452836990356,
        "learning_rate": 7.853820381933986e-05,
        "epoch": 0.19520264681555005,
        "step": 708
    },
    {
        "loss": 2.112,
        "grad_norm": 1.8014994859695435,
        "learning_rate": 7.82827205955635e-05,
        "epoch": 0.19547835676867936,
        "step": 709
    },
    {
        "loss": 2.5546,
        "grad_norm": 1.297736406326294,
        "learning_rate": 7.80273860543399e-05,
        "epoch": 0.19575406672180864,
        "step": 710
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.7226649522781372,
        "learning_rate": 7.777220194376051e-05,
        "epoch": 0.19602977667493796,
        "step": 711
    },
    {
        "loss": 2.2047,
        "grad_norm": 1.8017690181732178,
        "learning_rate": 7.751717001088696e-05,
        "epoch": 0.19630548662806727,
        "step": 712
    },
    {
        "loss": 2.4596,
        "grad_norm": 1.4233603477478027,
        "learning_rate": 7.7262292001739e-05,
        "epoch": 0.19658119658119658,
        "step": 713
    },
    {
        "loss": 2.2969,
        "grad_norm": 1.2203305959701538,
        "learning_rate": 7.700756966128256e-05,
        "epoch": 0.1968569065343259,
        "step": 714
    },
    {
        "loss": 1.356,
        "grad_norm": 2.036580801010132,
        "learning_rate": 7.675300473341784e-05,
        "epoch": 0.1971326164874552,
        "step": 715
    },
    {
        "loss": 2.4812,
        "grad_norm": 1.1439361572265625,
        "learning_rate": 7.649859896096738e-05,
        "epoch": 0.1974083264405845,
        "step": 716
    },
    {
        "loss": 1.9301,
        "grad_norm": 1.6274901628494263,
        "learning_rate": 7.624435408566401e-05,
        "epoch": 0.19768403639371382,
        "step": 717
    },
    {
        "loss": 1.8648,
        "grad_norm": 1.8294340372085571,
        "learning_rate": 7.599027184813913e-05,
        "epoch": 0.19795974634684313,
        "step": 718
    },
    {
        "loss": 2.3215,
        "grad_norm": 1.1811190843582153,
        "learning_rate": 7.573635398791055e-05,
        "epoch": 0.19823545629997244,
        "step": 719
    },
    {
        "loss": 2.5233,
        "grad_norm": 1.3299185037612915,
        "learning_rate": 7.548260224337078e-05,
        "epoch": 0.19851116625310175,
        "step": 720
    },
    {
        "loss": 2.3459,
        "grad_norm": 0.9788181781768799,
        "learning_rate": 7.52290183517751e-05,
        "epoch": 0.19878687620623103,
        "step": 721
    },
    {
        "loss": 2.4647,
        "grad_norm": 1.12461256980896,
        "learning_rate": 7.49756040492295e-05,
        "epoch": 0.19906258615936034,
        "step": 722
    },
    {
        "loss": 2.4196,
        "grad_norm": 1.5503045320510864,
        "learning_rate": 7.472236107067905e-05,
        "epoch": 0.19933829611248965,
        "step": 723
    },
    {
        "loss": 2.4101,
        "grad_norm": 1.3668489456176758,
        "learning_rate": 7.446929114989576e-05,
        "epoch": 0.19961400606561897,
        "step": 724
    },
    {
        "loss": 2.376,
        "grad_norm": 1.4854109287261963,
        "learning_rate": 7.421639601946693e-05,
        "epoch": 0.19988971601874828,
        "step": 725
    },
    {
        "loss": 2.057,
        "grad_norm": 1.6542444229125977,
        "learning_rate": 7.396367741078318e-05,
        "epoch": 0.2001654259718776,
        "step": 726
    },
    {
        "loss": 2.6404,
        "grad_norm": 0.9860324263572693,
        "learning_rate": 7.371113705402657e-05,
        "epoch": 0.2004411359250069,
        "step": 727
    },
    {
        "loss": 2.0754,
        "grad_norm": 1.1212855577468872,
        "learning_rate": 7.345877667815885e-05,
        "epoch": 0.2007168458781362,
        "step": 728
    },
    {
        "loss": 2.1811,
        "grad_norm": 1.57492995262146,
        "learning_rate": 7.320659801090953e-05,
        "epoch": 0.20099255583126552,
        "step": 729
    },
    {
        "loss": 2.2408,
        "grad_norm": 1.5212407112121582,
        "learning_rate": 7.295460277876411e-05,
        "epoch": 0.20126826578439483,
        "step": 730
    },
    {
        "loss": 1.8501,
        "grad_norm": 2.0456767082214355,
        "learning_rate": 7.270279270695228e-05,
        "epoch": 0.2015439757375241,
        "step": 731
    },
    {
        "loss": 2.1219,
        "grad_norm": 1.2544392347335815,
        "learning_rate": 7.245116951943598e-05,
        "epoch": 0.20181968569065342,
        "step": 732
    },
    {
        "loss": 2.0517,
        "grad_norm": 1.3384734392166138,
        "learning_rate": 7.219973493889779e-05,
        "epoch": 0.20209539564378273,
        "step": 733
    },
    {
        "loss": 2.5488,
        "grad_norm": 1.0477101802825928,
        "learning_rate": 7.194849068672893e-05,
        "epoch": 0.20237110559691204,
        "step": 734
    },
    {
        "loss": 1.8987,
        "grad_norm": 2.022982358932495,
        "learning_rate": 7.169743848301768e-05,
        "epoch": 0.20264681555004135,
        "step": 735
    },
    {
        "loss": 2.1661,
        "grad_norm": 1.4712659120559692,
        "learning_rate": 7.144658004653745e-05,
        "epoch": 0.20292252550317066,
        "step": 736
    },
    {
        "loss": 2.4934,
        "grad_norm": 1.3988581895828247,
        "learning_rate": 7.119591709473503e-05,
        "epoch": 0.20319823545629997,
        "step": 737
    },
    {
        "loss": 1.8134,
        "grad_norm": 1.8540756702423096,
        "learning_rate": 7.094545134371898e-05,
        "epoch": 0.20347394540942929,
        "step": 738
    },
    {
        "loss": 2.1589,
        "grad_norm": 1.5045043230056763,
        "learning_rate": 7.069518450824764e-05,
        "epoch": 0.2037496553625586,
        "step": 739
    },
    {
        "loss": 1.7359,
        "grad_norm": 1.941535472869873,
        "learning_rate": 7.044511830171758e-05,
        "epoch": 0.2040253653156879,
        "step": 740
    },
    {
        "loss": 2.4488,
        "grad_norm": 1.908367395401001,
        "learning_rate": 7.019525443615185e-05,
        "epoch": 0.20430107526881722,
        "step": 741
    },
    {
        "loss": 2.28,
        "grad_norm": 1.1979796886444092,
        "learning_rate": 6.994559462218811e-05,
        "epoch": 0.2045767852219465,
        "step": 742
    },
    {
        "loss": 2.6315,
        "grad_norm": 1.2597298622131348,
        "learning_rate": 6.969614056906716e-05,
        "epoch": 0.2048524951750758,
        "step": 743
    },
    {
        "loss": 2.3773,
        "grad_norm": 1.4014471769332886,
        "learning_rate": 6.944689398462096e-05,
        "epoch": 0.20512820512820512,
        "step": 744
    },
    {
        "loss": 2.0787,
        "grad_norm": 2.2158679962158203,
        "learning_rate": 6.919785657526118e-05,
        "epoch": 0.20540391508133443,
        "step": 745
    },
    {
        "loss": 2.1996,
        "grad_norm": 1.526684045791626,
        "learning_rate": 6.894903004596742e-05,
        "epoch": 0.20567962503446374,
        "step": 746
    },
    {
        "loss": 2.214,
        "grad_norm": 1.4646666049957275,
        "learning_rate": 6.870041610027547e-05,
        "epoch": 0.20595533498759305,
        "step": 747
    },
    {
        "loss": 2.1644,
        "grad_norm": 0.9652612805366516,
        "learning_rate": 6.84520164402658e-05,
        "epoch": 0.20623104494072236,
        "step": 748
    },
    {
        "loss": 1.727,
        "grad_norm": 2.606895685195923,
        "learning_rate": 6.820383276655169e-05,
        "epoch": 0.20650675489385167,
        "step": 749
    },
    {
        "loss": 2.1386,
        "grad_norm": 1.5770589113235474,
        "learning_rate": 6.795586677826789e-05,
        "epoch": 0.20678246484698098,
        "step": 750
    },
    {
        "loss": 2.157,
        "grad_norm": 1.776670217514038,
        "learning_rate": 6.770812017305865e-05,
        "epoch": 0.2070581748001103,
        "step": 751
    },
    {
        "loss": 2.206,
        "grad_norm": 1.4741969108581543,
        "learning_rate": 6.74605946470664e-05,
        "epoch": 0.2073338847532396,
        "step": 752
    },
    {
        "loss": 2.3833,
        "grad_norm": 1.0471957921981812,
        "learning_rate": 6.721329189491989e-05,
        "epoch": 0.2076095947063689,
        "step": 753
    },
    {
        "loss": 1.8975,
        "grad_norm": 2.0085766315460205,
        "learning_rate": 6.696621360972277e-05,
        "epoch": 0.2078853046594982,
        "step": 754
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.742013931274414,
        "learning_rate": 6.671936148304191e-05,
        "epoch": 0.2081610146126275,
        "step": 755
    },
    {
        "loss": 2.5483,
        "grad_norm": 1.4837924242019653,
        "learning_rate": 6.64727372048958e-05,
        "epoch": 0.20843672456575682,
        "step": 756
    },
    {
        "loss": 2.4527,
        "grad_norm": 1.2823896408081055,
        "learning_rate": 6.622634246374302e-05,
        "epoch": 0.20871243451888613,
        "step": 757
    },
    {
        "loss": 1.9422,
        "grad_norm": 2.5710227489471436,
        "learning_rate": 6.598017894647076e-05,
        "epoch": 0.20898814447201544,
        "step": 758
    },
    {
        "loss": 1.9337,
        "grad_norm": 2.031590223312378,
        "learning_rate": 6.573424833838312e-05,
        "epoch": 0.20926385442514475,
        "step": 759
    },
    {
        "loss": 2.2797,
        "grad_norm": 1.372973084449768,
        "learning_rate": 6.54885523231896e-05,
        "epoch": 0.20953956437827406,
        "step": 760
    },
    {
        "loss": 2.5596,
        "grad_norm": 0.9799384474754333,
        "learning_rate": 6.524309258299368e-05,
        "epoch": 0.20981527433140337,
        "step": 761
    },
    {
        "loss": 2.1687,
        "grad_norm": 2.1511991024017334,
        "learning_rate": 6.499787079828125e-05,
        "epoch": 0.21009098428453268,
        "step": 762
    },
    {
        "loss": 1.6934,
        "grad_norm": 1.729629397392273,
        "learning_rate": 6.475288864790897e-05,
        "epoch": 0.21036669423766197,
        "step": 763
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.7112306356430054,
        "learning_rate": 6.450814780909305e-05,
        "epoch": 0.21064240419079128,
        "step": 764
    },
    {
        "loss": 2.4367,
        "grad_norm": 1.2055494785308838,
        "learning_rate": 6.426364995739749e-05,
        "epoch": 0.2109181141439206,
        "step": 765
    },
    {
        "loss": 1.9228,
        "grad_norm": 1.7703752517700195,
        "learning_rate": 6.40193967667228e-05,
        "epoch": 0.2111938240970499,
        "step": 766
    },
    {
        "loss": 2.2173,
        "grad_norm": 1.3061500787734985,
        "learning_rate": 6.377538990929449e-05,
        "epoch": 0.2114695340501792,
        "step": 767
    },
    {
        "loss": 2.0714,
        "grad_norm": 1.833627700805664,
        "learning_rate": 6.353163105565151e-05,
        "epoch": 0.21174524400330852,
        "step": 768
    },
    {
        "loss": 2.5234,
        "grad_norm": 1.8346834182739258,
        "learning_rate": 6.328812187463504e-05,
        "epoch": 0.21202095395643783,
        "step": 769
    },
    {
        "loss": 2.0159,
        "grad_norm": 2.0173285007476807,
        "learning_rate": 6.304486403337679e-05,
        "epoch": 0.21229666390956714,
        "step": 770
    },
    {
        "loss": 2.2689,
        "grad_norm": 1.593567132949829,
        "learning_rate": 6.280185919728783e-05,
        "epoch": 0.21257237386269645,
        "step": 771
    },
    {
        "loss": 2.6822,
        "grad_norm": 1.3980591297149658,
        "learning_rate": 6.255910903004708e-05,
        "epoch": 0.21284808381582576,
        "step": 772
    },
    {
        "loss": 2.0972,
        "grad_norm": 1.5214072465896606,
        "learning_rate": 6.231661519358984e-05,
        "epoch": 0.21312379376895507,
        "step": 773
    },
    {
        "loss": 1.9513,
        "grad_norm": 1.4875694513320923,
        "learning_rate": 6.20743793480966e-05,
        "epoch": 0.21339950372208435,
        "step": 774
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.5406676530838013,
        "learning_rate": 6.18324031519815e-05,
        "epoch": 0.21367521367521367,
        "step": 775
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.638297200202942,
        "learning_rate": 6.159068826188109e-05,
        "epoch": 0.21395092362834298,
        "step": 776
    },
    {
        "loss": 2.2939,
        "grad_norm": 1.1118110418319702,
        "learning_rate": 6.134923633264296e-05,
        "epoch": 0.2142266335814723,
        "step": 777
    },
    {
        "loss": 1.0723,
        "grad_norm": 1.786136507987976,
        "learning_rate": 6.110804901731431e-05,
        "epoch": 0.2145023435346016,
        "step": 778
    },
    {
        "loss": 2.3128,
        "grad_norm": 1.718432903289795,
        "learning_rate": 6.0867127967130834e-05,
        "epoch": 0.2147780534877309,
        "step": 779
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.702266812324524,
        "learning_rate": 6.062647483150521e-05,
        "epoch": 0.21505376344086022,
        "step": 780
    },
    {
        "loss": 2.3441,
        "grad_norm": 1.6336089372634888,
        "learning_rate": 6.038609125801596e-05,
        "epoch": 0.21532947339398953,
        "step": 781
    },
    {
        "loss": 2.0675,
        "grad_norm": 1.5257418155670166,
        "learning_rate": 6.014597889239606e-05,
        "epoch": 0.21560518334711884,
        "step": 782
    },
    {
        "loss": 1.2734,
        "grad_norm": 1.9121286869049072,
        "learning_rate": 5.9906139378521744e-05,
        "epoch": 0.21588089330024815,
        "step": 783
    },
    {
        "loss": 2.57,
        "grad_norm": 0.8411188721656799,
        "learning_rate": 5.966657435840126e-05,
        "epoch": 0.21615660325337746,
        "step": 784
    },
    {
        "loss": 1.9745,
        "grad_norm": 2.099916934967041,
        "learning_rate": 5.9427285472163505e-05,
        "epoch": 0.21643231320650674,
        "step": 785
    },
    {
        "loss": 2.5606,
        "grad_norm": 1.2712678909301758,
        "learning_rate": 5.9188274358047e-05,
        "epoch": 0.21670802315963605,
        "step": 786
    },
    {
        "loss": 2.2287,
        "grad_norm": 1.4880703687667847,
        "learning_rate": 5.894954265238846e-05,
        "epoch": 0.21698373311276536,
        "step": 787
    },
    {
        "loss": 2.1496,
        "grad_norm": 1.297744870185852,
        "learning_rate": 5.871109198961174e-05,
        "epoch": 0.21725944306589468,
        "step": 788
    },
    {
        "loss": 2.561,
        "grad_norm": 1.1860219240188599,
        "learning_rate": 5.847292400221663e-05,
        "epoch": 0.21753515301902399,
        "step": 789
    },
    {
        "loss": 2.1994,
        "grad_norm": 1.7411134243011475,
        "learning_rate": 5.8235040320767586e-05,
        "epoch": 0.2178108629721533,
        "step": 790
    },
    {
        "loss": 1.8793,
        "grad_norm": 1.5780296325683594,
        "learning_rate": 5.79974425738826e-05,
        "epoch": 0.2180865729252826,
        "step": 791
    },
    {
        "loss": 2.5125,
        "grad_norm": 1.6238036155700684,
        "learning_rate": 5.77601323882222e-05,
        "epoch": 0.21836228287841192,
        "step": 792
    },
    {
        "loss": 2.1974,
        "grad_norm": 1.3124407529830933,
        "learning_rate": 5.752311138847809e-05,
        "epoch": 0.21863799283154123,
        "step": 793
    },
    {
        "loss": 2.0425,
        "grad_norm": 2.4223432540893555,
        "learning_rate": 5.7286381197362094e-05,
        "epoch": 0.21891370278467054,
        "step": 794
    },
    {
        "loss": 1.5903,
        "grad_norm": 1.733455777168274,
        "learning_rate": 5.704994343559522e-05,
        "epoch": 0.21918941273779982,
        "step": 795
    },
    {
        "loss": 2.1487,
        "grad_norm": 0.9738588333129883,
        "learning_rate": 5.681379972189631e-05,
        "epoch": 0.21946512269092913,
        "step": 796
    },
    {
        "loss": 1.6763,
        "grad_norm": 1.963472843170166,
        "learning_rate": 5.657795167297104e-05,
        "epoch": 0.21974083264405844,
        "step": 797
    },
    {
        "loss": 1.3896,
        "grad_norm": 2.177518606185913,
        "learning_rate": 5.6342400903501033e-05,
        "epoch": 0.22001654259718775,
        "step": 798
    },
    {
        "loss": 2.6802,
        "grad_norm": 1.4116370677947998,
        "learning_rate": 5.610714902613252e-05,
        "epoch": 0.22029225255031706,
        "step": 799
    },
    {
        "loss": 2.3406,
        "grad_norm": 1.0562891960144043,
        "learning_rate": 5.5872197651465476e-05,
        "epoch": 0.22056796250344637,
        "step": 800
    },
    {
        "loss": 2.1351,
        "grad_norm": 1.6872968673706055,
        "learning_rate": 5.563754838804251e-05,
        "epoch": 0.22084367245657568,
        "step": 801
    },
    {
        "loss": 1.2092,
        "grad_norm": 2.8534812927246094,
        "learning_rate": 5.540320284233802e-05,
        "epoch": 0.221119382409705,
        "step": 802
    },
    {
        "loss": 1.9251,
        "grad_norm": 1.544638991355896,
        "learning_rate": 5.516916261874692e-05,
        "epoch": 0.2213950923628343,
        "step": 803
    },
    {
        "loss": 1.932,
        "grad_norm": 1.8669153451919556,
        "learning_rate": 5.493542931957385e-05,
        "epoch": 0.22167080231596362,
        "step": 804
    },
    {
        "loss": 2.501,
        "grad_norm": 1.2219563722610474,
        "learning_rate": 5.4702004545022256e-05,
        "epoch": 0.22194651226909293,
        "step": 805
    },
    {
        "loss": 1.666,
        "grad_norm": 2.3281748294830322,
        "learning_rate": 5.44688898931832e-05,
        "epoch": 0.2222222222222222,
        "step": 806
    },
    {
        "loss": 1.578,
        "grad_norm": 2.4398601055145264,
        "learning_rate": 5.423608696002461e-05,
        "epoch": 0.22249793217535152,
        "step": 807
    },
    {
        "loss": 1.1707,
        "grad_norm": 1.9266873598098755,
        "learning_rate": 5.400359733938028e-05,
        "epoch": 0.22277364212848083,
        "step": 808
    },
    {
        "loss": 1.5401,
        "grad_norm": 1.6746546030044556,
        "learning_rate": 5.37714226229391e-05,
        "epoch": 0.22304935208161014,
        "step": 809
    },
    {
        "loss": 2.1982,
        "grad_norm": 1.823819875717163,
        "learning_rate": 5.353956440023387e-05,
        "epoch": 0.22332506203473945,
        "step": 810
    },
    {
        "loss": 2.3929,
        "grad_norm": 1.2655370235443115,
        "learning_rate": 5.330802425863064e-05,
        "epoch": 0.22360077198786876,
        "step": 811
    },
    {
        "loss": 2.2645,
        "grad_norm": 1.5147284269332886,
        "learning_rate": 5.307680378331788e-05,
        "epoch": 0.22387648194099807,
        "step": 812
    },
    {
        "loss": 1.9356,
        "grad_norm": 1.451503038406372,
        "learning_rate": 5.284590455729541e-05,
        "epoch": 0.22415219189412738,
        "step": 813
    },
    {
        "loss": 2.6072,
        "grad_norm": 1.5584394931793213,
        "learning_rate": 5.261532816136372e-05,
        "epoch": 0.2244279018472567,
        "step": 814
    },
    {
        "loss": 2.1759,
        "grad_norm": 1.6270126104354858,
        "learning_rate": 5.2385076174113215e-05,
        "epoch": 0.224703611800386,
        "step": 815
    },
    {
        "loss": 2.5671,
        "grad_norm": 2.2739548683166504,
        "learning_rate": 5.215515017191316e-05,
        "epoch": 0.22497932175351532,
        "step": 816
    },
    {
        "loss": 2.2778,
        "grad_norm": 1.4094585180282593,
        "learning_rate": 5.192555172890112e-05,
        "epoch": 0.2252550317066446,
        "step": 817
    },
    {
        "loss": 2.3575,
        "grad_norm": 1.5434739589691162,
        "learning_rate": 5.1696282416972066e-05,
        "epoch": 0.2255307416597739,
        "step": 818
    },
    {
        "loss": 1.8715,
        "grad_norm": 2.3042612075805664,
        "learning_rate": 5.1467343805767696e-05,
        "epoch": 0.22580645161290322,
        "step": 819
    },
    {
        "loss": 2.0936,
        "grad_norm": 1.4607164859771729,
        "learning_rate": 5.12387374626656e-05,
        "epoch": 0.22608216156603253,
        "step": 820
    },
    {
        "loss": 2.1071,
        "grad_norm": 1.2294151782989502,
        "learning_rate": 5.101046495276851e-05,
        "epoch": 0.22635787151916184,
        "step": 821
    },
    {
        "loss": 2.0206,
        "grad_norm": 1.6341114044189453,
        "learning_rate": 5.078252783889379e-05,
        "epoch": 0.22663358147229115,
        "step": 822
    },
    {
        "loss": 2.3384,
        "grad_norm": 1.117535948753357,
        "learning_rate": 5.055492768156247e-05,
        "epoch": 0.22690929142542046,
        "step": 823
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.943354606628418,
        "learning_rate": 5.0327666038988665e-05,
        "epoch": 0.22718500137854977,
        "step": 824
    },
    {
        "loss": 1.9351,
        "grad_norm": 2.2222471237182617,
        "learning_rate": 5.010074446706905e-05,
        "epoch": 0.22746071133167908,
        "step": 825
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.129907488822937,
        "learning_rate": 4.987416451937198e-05,
        "epoch": 0.2277364212848084,
        "step": 826
    },
    {
        "loss": 2.2283,
        "grad_norm": 1.2050613164901733,
        "learning_rate": 4.964792774712698e-05,
        "epoch": 0.22801213123793768,
        "step": 827
    },
    {
        "loss": 1.9539,
        "grad_norm": 1.6111961603164673,
        "learning_rate": 4.9422035699214054e-05,
        "epoch": 0.228287841191067,
        "step": 828
    },
    {
        "loss": 1.7897,
        "grad_norm": 1.984961986541748,
        "learning_rate": 4.919648992215324e-05,
        "epoch": 0.2285635511441963,
        "step": 829
    },
    {
        "loss": 1.7572,
        "grad_norm": 2.0871548652648926,
        "learning_rate": 4.8971291960093826e-05,
        "epoch": 0.2288392610973256,
        "step": 830
    },
    {
        "loss": 1.9215,
        "grad_norm": 1.4211885929107666,
        "learning_rate": 4.874644335480383e-05,
        "epoch": 0.22911497105045492,
        "step": 831
    },
    {
        "loss": 2.7155,
        "grad_norm": 1.016507625579834,
        "learning_rate": 4.8521945645659595e-05,
        "epoch": 0.22939068100358423,
        "step": 832
    },
    {
        "loss": 1.7815,
        "grad_norm": 1.7210967540740967,
        "learning_rate": 4.829780036963504e-05,
        "epoch": 0.22966639095671354,
        "step": 833
    },
    {
        "loss": 2.3607,
        "grad_norm": 1.605621337890625,
        "learning_rate": 4.8074009061291194e-05,
        "epoch": 0.22994210090984285,
        "step": 834
    },
    {
        "loss": 1.9224,
        "grad_norm": 1.4679328203201294,
        "learning_rate": 4.785057325276585e-05,
        "epoch": 0.23021781086297216,
        "step": 835
    },
    {
        "loss": 2.0954,
        "grad_norm": 1.6621818542480469,
        "learning_rate": 4.7627494473762856e-05,
        "epoch": 0.23049352081610147,
        "step": 836
    },
    {
        "loss": 2.414,
        "grad_norm": 1.1075701713562012,
        "learning_rate": 4.740477425154176e-05,
        "epoch": 0.23076923076923078,
        "step": 837
    },
    {
        "loss": 1.9062,
        "grad_norm": 2.278355360031128,
        "learning_rate": 4.718241411090729e-05,
        "epoch": 0.23104494072236006,
        "step": 838
    },
    {
        "loss": 1.8383,
        "grad_norm": 1.5766170024871826,
        "learning_rate": 4.696041557419907e-05,
        "epoch": 0.23132065067548938,
        "step": 839
    },
    {
        "loss": 2.3695,
        "grad_norm": 1.9370719194412231,
        "learning_rate": 4.673878016128096e-05,
        "epoch": 0.23159636062861869,
        "step": 840
    },
    {
        "loss": 2.3713,
        "grad_norm": 0.8185913562774658,
        "learning_rate": 4.65175093895308e-05,
        "epoch": 0.231872070581748,
        "step": 841
    },
    {
        "loss": 2.1634,
        "grad_norm": 0.7431369423866272,
        "learning_rate": 4.6296604773830074e-05,
        "epoch": 0.2321477805348773,
        "step": 842
    },
    {
        "loss": 2.3642,
        "grad_norm": 1.4383292198181152,
        "learning_rate": 4.607606782655338e-05,
        "epoch": 0.23242349048800662,
        "step": 843
    },
    {
        "loss": 1.6476,
        "grad_norm": 0.9453057050704956,
        "learning_rate": 4.5855900057558175e-05,
        "epoch": 0.23269920044113593,
        "step": 844
    },
    {
        "loss": 2.5196,
        "grad_norm": 1.1841031312942505,
        "learning_rate": 4.563610297417437e-05,
        "epoch": 0.23297491039426524,
        "step": 845
    },
    {
        "loss": 2.4157,
        "grad_norm": 1.7018691301345825,
        "learning_rate": 4.541667808119419e-05,
        "epoch": 0.23325062034739455,
        "step": 846
    },
    {
        "loss": 2.2658,
        "grad_norm": 1.4469884634017944,
        "learning_rate": 4.519762688086162e-05,
        "epoch": 0.23352633030052386,
        "step": 847
    },
    {
        "loss": 2.4288,
        "grad_norm": 1.5487151145935059,
        "learning_rate": 4.4978950872862255e-05,
        "epoch": 0.23380204025365317,
        "step": 848
    },
    {
        "loss": 2.245,
        "grad_norm": 1.480194091796875,
        "learning_rate": 4.476065155431312e-05,
        "epoch": 0.23407775020678245,
        "step": 849
    },
    {
        "loss": 2.6022,
        "grad_norm": 1.5702511072158813,
        "learning_rate": 4.454273041975219e-05,
        "epoch": 0.23435346015991176,
        "step": 850
    },
    {
        "loss": 1.8087,
        "grad_norm": 2.1042373180389404,
        "learning_rate": 4.4325188961128307e-05,
        "epoch": 0.23462917011304107,
        "step": 851
    },
    {
        "loss": 1.9945,
        "grad_norm": 1.5946441888809204,
        "learning_rate": 4.410802866779105e-05,
        "epoch": 0.23490488006617039,
        "step": 852
    },
    {
        "loss": 1.4268,
        "grad_norm": 1.6776831150054932,
        "learning_rate": 4.389125102648034e-05,
        "epoch": 0.2351805900192997,
        "step": 853
    },
    {
        "loss": 2.1774,
        "grad_norm": 1.6598681211471558,
        "learning_rate": 4.367485752131637e-05,
        "epoch": 0.235456299972429,
        "step": 854
    },
    {
        "loss": 2.261,
        "grad_norm": 1.7121025323867798,
        "learning_rate": 4.345884963378941e-05,
        "epoch": 0.23573200992555832,
        "step": 855
    },
    {
        "loss": 1.8422,
        "grad_norm": 2.8776252269744873,
        "learning_rate": 4.3243228842749776e-05,
        "epoch": 0.23600771987868763,
        "step": 856
    },
    {
        "loss": 1.7272,
        "grad_norm": 2.125260591506958,
        "learning_rate": 4.3027996624397504e-05,
        "epoch": 0.23628342983181694,
        "step": 857
    },
    {
        "loss": 2.3721,
        "grad_norm": 1.2095474004745483,
        "learning_rate": 4.281315445227239e-05,
        "epoch": 0.23655913978494625,
        "step": 858
    },
    {
        "loss": 2.4031,
        "grad_norm": 1.2339088916778564,
        "learning_rate": 4.259870379724392e-05,
        "epoch": 0.23683484973807553,
        "step": 859
    },
    {
        "loss": 2.0162,
        "grad_norm": 2.310744524002075,
        "learning_rate": 4.238464612750107e-05,
        "epoch": 0.23711055969120484,
        "step": 860
    },
    {
        "loss": 1.8492,
        "grad_norm": 1.1546955108642578,
        "learning_rate": 4.2170982908542336e-05,
        "epoch": 0.23738626964433415,
        "step": 861
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.528180480003357,
        "learning_rate": 4.195771560316577e-05,
        "epoch": 0.23766197959746346,
        "step": 862
    },
    {
        "loss": 2.2436,
        "grad_norm": 1.3367847204208374,
        "learning_rate": 4.1744845671458796e-05,
        "epoch": 0.23793768955059277,
        "step": 863
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.1801429986953735,
        "learning_rate": 4.1532374570788355e-05,
        "epoch": 0.23821339950372208,
        "step": 864
    },
    {
        "loss": 2.1807,
        "grad_norm": 1.3089070320129395,
        "learning_rate": 4.132030375579084e-05,
        "epoch": 0.2384891094568514,
        "step": 865
    },
    {
        "loss": 1.5361,
        "grad_norm": 1.879054307937622,
        "learning_rate": 4.110863467836227e-05,
        "epoch": 0.2387648194099807,
        "step": 866
    },
    {
        "loss": 2.0301,
        "grad_norm": 1.595908284187317,
        "learning_rate": 4.089736878764815e-05,
        "epoch": 0.23904052936311002,
        "step": 867
    },
    {
        "loss": 2.7049,
        "grad_norm": 1.2166521549224854,
        "learning_rate": 4.068650753003368e-05,
        "epoch": 0.23931623931623933,
        "step": 868
    },
    {
        "loss": 2.0115,
        "grad_norm": 1.110996961593628,
        "learning_rate": 4.047605234913391e-05,
        "epoch": 0.23959194926936864,
        "step": 869
    },
    {
        "loss": 1.8719,
        "grad_norm": 1.689857840538025,
        "learning_rate": 4.0266004685783646e-05,
        "epoch": 0.23986765922249792,
        "step": 870
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.0539519786834717,
        "learning_rate": 4.005636597802784e-05,
        "epoch": 0.24014336917562723,
        "step": 871
    },
    {
        "loss": 0.8747,
        "grad_norm": 1.8390578031539917,
        "learning_rate": 3.984713766111152e-05,
        "epoch": 0.24041907912875654,
        "step": 872
    },
    {
        "loss": 1.6059,
        "grad_norm": 1.6376622915267944,
        "learning_rate": 3.963832116747015e-05,
        "epoch": 0.24069478908188585,
        "step": 873
    },
    {
        "loss": 2.3234,
        "grad_norm": 0.9584583044052124,
        "learning_rate": 3.9429917926719686e-05,
        "epoch": 0.24097049903501516,
        "step": 874
    },
    {
        "loss": 2.2519,
        "grad_norm": 1.7551639080047607,
        "learning_rate": 3.9221929365646814e-05,
        "epoch": 0.24124620898814447,
        "step": 875
    },
    {
        "loss": 2.2102,
        "grad_norm": 1.7607150077819824,
        "learning_rate": 3.901435690819923e-05,
        "epoch": 0.24152191894127378,
        "step": 876
    },
    {
        "loss": 1.9484,
        "grad_norm": 1.6541837453842163,
        "learning_rate": 3.8807201975475945e-05,
        "epoch": 0.2417976288944031,
        "step": 877
    },
    {
        "loss": 1.9804,
        "grad_norm": 2.1973907947540283,
        "learning_rate": 3.860046598571737e-05,
        "epoch": 0.2420733388475324,
        "step": 878
    },
    {
        "loss": 1.7008,
        "grad_norm": 1.8592911958694458,
        "learning_rate": 3.839415035429573e-05,
        "epoch": 0.24234904880066171,
        "step": 879
    },
    {
        "loss": 1.6048,
        "grad_norm": 2.087329387664795,
        "learning_rate": 3.818825649370547e-05,
        "epoch": 0.24262475875379103,
        "step": 880
    },
    {
        "loss": 1.7922,
        "grad_norm": 1.6808817386627197,
        "learning_rate": 3.798278581355334e-05,
        "epoch": 0.2429004687069203,
        "step": 881
    },
    {
        "loss": 2.28,
        "grad_norm": 1.5545909404754639,
        "learning_rate": 3.777773972054889e-05,
        "epoch": 0.24317617866004962,
        "step": 882
    },
    {
        "loss": 1.2933,
        "grad_norm": 2.19852352142334,
        "learning_rate": 3.757311961849497e-05,
        "epoch": 0.24345188861317893,
        "step": 883
    },
    {
        "loss": 2.1851,
        "grad_norm": 1.3109368085861206,
        "learning_rate": 3.73689269082778e-05,
        "epoch": 0.24372759856630824,
        "step": 884
    },
    {
        "loss": 2.4334,
        "grad_norm": 1.3846689462661743,
        "learning_rate": 3.7165162987857674e-05,
        "epoch": 0.24400330851943755,
        "step": 885
    },
    {
        "loss": 2.1273,
        "grad_norm": 0.9399989247322083,
        "learning_rate": 3.696182925225917e-05,
        "epoch": 0.24427901847256686,
        "step": 886
    },
    {
        "loss": 1.5037,
        "grad_norm": 1.9742711782455444,
        "learning_rate": 3.675892709356185e-05,
        "epoch": 0.24455472842569617,
        "step": 887
    },
    {
        "loss": 2.3182,
        "grad_norm": 1.8796899318695068,
        "learning_rate": 3.655645790089043e-05,
        "epoch": 0.24483043837882548,
        "step": 888
    },
    {
        "loss": 2.5433,
        "grad_norm": 1.8647713661193848,
        "learning_rate": 3.6354423060405465e-05,
        "epoch": 0.2451061483319548,
        "step": 889
    },
    {
        "loss": 2.5033,
        "grad_norm": 0.7347517609596252,
        "learning_rate": 3.615282395529389e-05,
        "epoch": 0.2453818582850841,
        "step": 890
    },
    {
        "loss": 2.0005,
        "grad_norm": 1.0591343641281128,
        "learning_rate": 3.595166196575938e-05,
        "epoch": 0.2456575682382134,
        "step": 891
    },
    {
        "loss": 1.6881,
        "grad_norm": 1.9251492023468018,
        "learning_rate": 3.5750938469013006e-05,
        "epoch": 0.2459332781913427,
        "step": 892
    },
    {
        "loss": 1.807,
        "grad_norm": 1.368888020515442,
        "learning_rate": 3.555065483926387e-05,
        "epoch": 0.246208988144472,
        "step": 893
    },
    {
        "loss": 2.5628,
        "grad_norm": 1.1590523719787598,
        "learning_rate": 3.5350812447709544e-05,
        "epoch": 0.24648469809760132,
        "step": 894
    },
    {
        "loss": 1.9835,
        "grad_norm": 1.494539499282837,
        "learning_rate": 3.5151412662526786e-05,
        "epoch": 0.24676040805073063,
        "step": 895
    },
    {
        "loss": 2.5835,
        "grad_norm": 0.9379034638404846,
        "learning_rate": 3.495245684886211e-05,
        "epoch": 0.24703611800385994,
        "step": 896
    },
    {
        "loss": 2.1694,
        "grad_norm": 1.185263991355896,
        "learning_rate": 3.4753946368822586e-05,
        "epoch": 0.24731182795698925,
        "step": 897
    },
    {
        "loss": 2.234,
        "grad_norm": 1.6182953119277954,
        "learning_rate": 3.4555882581466295e-05,
        "epoch": 0.24758753791011856,
        "step": 898
    },
    {
        "loss": 2.256,
        "grad_norm": 1.4179458618164062,
        "learning_rate": 3.435826684279314e-05,
        "epoch": 0.24786324786324787,
        "step": 899
    },
    {
        "loss": 1.6324,
        "grad_norm": 1.9673545360565186,
        "learning_rate": 3.4161100505735665e-05,
        "epoch": 0.24813895781637718,
        "step": 900
    },
    {
        "loss": 2.1275,
        "grad_norm": 1.427893877029419,
        "learning_rate": 3.396438492014957e-05,
        "epoch": 0.2484146677695065,
        "step": 901
    },
    {
        "loss": 1.9066,
        "grad_norm": 1.5347031354904175,
        "learning_rate": 3.3768121432804614e-05,
        "epoch": 0.24869037772263577,
        "step": 902
    },
    {
        "loss": 2.3181,
        "grad_norm": 1.2126123905181885,
        "learning_rate": 3.357231138737542e-05,
        "epoch": 0.24896608767576509,
        "step": 903
    },
    {
        "loss": 2.0315,
        "grad_norm": 1.391985297203064,
        "learning_rate": 3.337695612443216e-05,
        "epoch": 0.2492417976288944,
        "step": 904
    },
    {
        "loss": 1.9395,
        "grad_norm": 1.663934350013733,
        "learning_rate": 3.318205698143144e-05,
        "epoch": 0.2495175075820237,
        "step": 905
    },
    {
        "loss": 1.4155,
        "grad_norm": 1.5294060707092285,
        "learning_rate": 3.298761529270714e-05,
        "epoch": 0.24979321753515302,
        "step": 906
    },
    {
        "loss": 2.3379,
        "grad_norm": 0.9466174840927124,
        "learning_rate": 3.279363238946138e-05,
        "epoch": 0.2500689274882823,
        "step": 907
    },
    {
        "loss": 2.3428,
        "grad_norm": 1.5187381505966187,
        "learning_rate": 3.260010959975517e-05,
        "epoch": 0.2503446374414116,
        "step": 908
    },
    {
        "loss": 2.1121,
        "grad_norm": 1.6632410287857056,
        "learning_rate": 3.2407048248499485e-05,
        "epoch": 0.2506203473945409,
        "step": 909
    },
    {
        "loss": 1.853,
        "grad_norm": 1.324999451637268,
        "learning_rate": 3.2214449657446254e-05,
        "epoch": 0.25089605734767023,
        "step": 910
    },
    {
        "loss": 1.866,
        "grad_norm": 2.330991268157959,
        "learning_rate": 3.2022315145179124e-05,
        "epoch": 0.25117176730079954,
        "step": 911
    },
    {
        "loss": 1.2636,
        "grad_norm": 2.1355502605438232,
        "learning_rate": 3.183064602710457e-05,
        "epoch": 0.25144747725392885,
        "step": 912
    },
    {
        "loss": 1.9819,
        "grad_norm": 1.4700359106063843,
        "learning_rate": 3.16394436154428e-05,
        "epoch": 0.25172318720705816,
        "step": 913
    },
    {
        "loss": 2.247,
        "grad_norm": 1.2351256608963013,
        "learning_rate": 3.144870921921891e-05,
        "epoch": 0.2519988971601875,
        "step": 914
    },
    {
        "loss": 1.0028,
        "grad_norm": 1.9201276302337646,
        "learning_rate": 3.1258444144253776e-05,
        "epoch": 0.2522746071133168,
        "step": 915
    },
    {
        "loss": 1.0521,
        "grad_norm": 1.9901212453842163,
        "learning_rate": 3.106864969315512e-05,
        "epoch": 0.2525503170664461,
        "step": 916
    },
    {
        "loss": 1.7955,
        "grad_norm": 1.7394973039627075,
        "learning_rate": 3.087932716530876e-05,
        "epoch": 0.2528260270195754,
        "step": 917
    },
    {
        "loss": 2.148,
        "grad_norm": 1.486023187637329,
        "learning_rate": 3.0690477856869525e-05,
        "epoch": 0.2531017369727047,
        "step": 918
    },
    {
        "loss": 2.4938,
        "grad_norm": 1.3775495290756226,
        "learning_rate": 3.0502103060752397e-05,
        "epoch": 0.253377446925834,
        "step": 919
    },
    {
        "loss": 2.1486,
        "grad_norm": 1.4762334823608398,
        "learning_rate": 3.0314204066623864e-05,
        "epoch": 0.25365315687896334,
        "step": 920
    },
    {
        "loss": 2.4026,
        "grad_norm": 1.0376712083816528,
        "learning_rate": 3.012678216089281e-05,
        "epoch": 0.25392886683209265,
        "step": 921
    },
    {
        "loss": 2.4299,
        "grad_norm": 1.0536060333251953,
        "learning_rate": 2.9939838626701888e-05,
        "epoch": 0.25420457678522196,
        "step": 922
    },
    {
        "loss": 2.1706,
        "grad_norm": 1.393053412437439,
        "learning_rate": 2.9753374743918637e-05,
        "epoch": 0.25448028673835127,
        "step": 923
    },
    {
        "loss": 1.7791,
        "grad_norm": 1.3241968154907227,
        "learning_rate": 2.9567391789126884e-05,
        "epoch": 0.2547559966914806,
        "step": 924
    },
    {
        "loss": 2.4073,
        "grad_norm": 2.0052144527435303,
        "learning_rate": 2.9381891035617792e-05,
        "epoch": 0.2550317066446099,
        "step": 925
    },
    {
        "loss": 2.1693,
        "grad_norm": 1.2894394397735596,
        "learning_rate": 2.9196873753381215e-05,
        "epoch": 0.2553074165977392,
        "step": 926
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.3494019508361816,
        "learning_rate": 2.901234120909715e-05,
        "epoch": 0.2555831265508685,
        "step": 927
    },
    {
        "loss": 2.5426,
        "grad_norm": 1.3608869314193726,
        "learning_rate": 2.8828294666126832e-05,
        "epoch": 0.2558588365039978,
        "step": 928
    },
    {
        "loss": 2.6344,
        "grad_norm": 1.4601095914840698,
        "learning_rate": 2.864473538450422e-05,
        "epoch": 0.2561345464571271,
        "step": 929
    },
    {
        "loss": 1.5693,
        "grad_norm": 1.9358941316604614,
        "learning_rate": 2.8461664620927396e-05,
        "epoch": 0.2564102564102564,
        "step": 930
    },
    {
        "loss": 2.2846,
        "grad_norm": 0.7752336263656616,
        "learning_rate": 2.8279083628749858e-05,
        "epoch": 0.2566859663633857,
        "step": 931
    },
    {
        "loss": 2.3363,
        "grad_norm": 1.041519284248352,
        "learning_rate": 2.8096993657972015e-05,
        "epoch": 0.256961676316515,
        "step": 932
    },
    {
        "loss": 2.5504,
        "grad_norm": 1.3469845056533813,
        "learning_rate": 2.791539595523255e-05,
        "epoch": 0.2572373862696443,
        "step": 933
    },
    {
        "loss": 1.5155,
        "grad_norm": 1.9120420217514038,
        "learning_rate": 2.773429176380007e-05,
        "epoch": 0.25751309622277363,
        "step": 934
    },
    {
        "loss": 2.075,
        "grad_norm": 2.0880279541015625,
        "learning_rate": 2.7553682323564344e-05,
        "epoch": 0.25778880617590294,
        "step": 935
    },
    {
        "loss": 2.713,
        "grad_norm": 0.9277793765068054,
        "learning_rate": 2.7373568871027955e-05,
        "epoch": 0.25806451612903225,
        "step": 936
    },
    {
        "loss": 2.0214,
        "grad_norm": 1.8892098665237427,
        "learning_rate": 2.7193952639297917e-05,
        "epoch": 0.25834022608216156,
        "step": 937
    },
    {
        "loss": 2.404,
        "grad_norm": 1.1866402626037598,
        "learning_rate": 2.7014834858076997e-05,
        "epoch": 0.25861593603529087,
        "step": 938
    },
    {
        "loss": 1.8652,
        "grad_norm": 1.8365164995193481,
        "learning_rate": 2.6836216753655476e-05,
        "epoch": 0.2588916459884202,
        "step": 939
    },
    {
        "loss": 1.9857,
        "grad_norm": 1.4007127285003662,
        "learning_rate": 2.6658099548902783e-05,
        "epoch": 0.2591673559415495,
        "step": 940
    },
    {
        "loss": 1.7861,
        "grad_norm": 2.1362035274505615,
        "learning_rate": 2.6480484463258935e-05,
        "epoch": 0.2594430658946788,
        "step": 941
    },
    {
        "loss": 2.0655,
        "grad_norm": 1.5026533603668213,
        "learning_rate": 2.6303372712726393e-05,
        "epoch": 0.2597187758478081,
        "step": 942
    },
    {
        "loss": 2.4351,
        "grad_norm": 1.0427724123001099,
        "learning_rate": 2.6126765509861538e-05,
        "epoch": 0.2599944858009374,
        "step": 943
    },
    {
        "loss": 1.8962,
        "grad_norm": 1.643267273902893,
        "learning_rate": 2.5950664063766626e-05,
        "epoch": 0.26027019575406674,
        "step": 944
    },
    {
        "loss": 2.1832,
        "grad_norm": 1.4048560857772827,
        "learning_rate": 2.5775069580081246e-05,
        "epoch": 0.26054590570719605,
        "step": 945
    },
    {
        "loss": 2.2703,
        "grad_norm": 1.0604503154754639,
        "learning_rate": 2.559998326097418e-05,
        "epoch": 0.26082161566032536,
        "step": 946
    },
    {
        "loss": 1.9547,
        "grad_norm": 1.599611759185791,
        "learning_rate": 2.542540630513527e-05,
        "epoch": 0.26109732561345467,
        "step": 947
    },
    {
        "loss": 2.3168,
        "grad_norm": 1.3869808912277222,
        "learning_rate": 2.5251339907767012e-05,
        "epoch": 0.261373035566584,
        "step": 948
    },
    {
        "loss": 1.6651,
        "grad_norm": 2.041736125946045,
        "learning_rate": 2.5077785260576535e-05,
        "epoch": 0.2616487455197133,
        "step": 949
    },
    {
        "loss": 1.7861,
        "grad_norm": 1.9787763357162476,
        "learning_rate": 2.490474355176734e-05,
        "epoch": 0.26192445547284254,
        "step": 950
    },
    {
        "loss": 1.1283,
        "grad_norm": 1.9120798110961914,
        "learning_rate": 2.473221596603127e-05,
        "epoch": 0.26220016542597185,
        "step": 951
    },
    {
        "loss": 2.1405,
        "grad_norm": 1.593298077583313,
        "learning_rate": 2.4560203684540295e-05,
        "epoch": 0.26247587537910116,
        "step": 952
    },
    {
        "loss": 2.1558,
        "grad_norm": 1.3017868995666504,
        "learning_rate": 2.4388707884938432e-05,
        "epoch": 0.2627515853322305,
        "step": 953
    },
    {
        "loss": 2.5228,
        "grad_norm": 1.3245012760162354,
        "learning_rate": 2.421772974133384e-05,
        "epoch": 0.2630272952853598,
        "step": 954
    },
    {
        "loss": 1.9682,
        "grad_norm": 1.4623321294784546,
        "learning_rate": 2.4047270424290523e-05,
        "epoch": 0.2633030052384891,
        "step": 955
    },
    {
        "loss": 2.2427,
        "grad_norm": 1.0908758640289307,
        "learning_rate": 2.387733110082052e-05,
        "epoch": 0.2635787151916184,
        "step": 956
    },
    {
        "loss": 2.476,
        "grad_norm": 0.9164993166923523,
        "learning_rate": 2.3707912934375888e-05,
        "epoch": 0.2638544251447477,
        "step": 957
    },
    {
        "loss": 2.3279,
        "grad_norm": 1.6070109605789185,
        "learning_rate": 2.3539017084840608e-05,
        "epoch": 0.26413013509787703,
        "step": 958
    },
    {
        "loss": 2.1193,
        "grad_norm": 2.079954147338867,
        "learning_rate": 2.3370644708522803e-05,
        "epoch": 0.26440584505100634,
        "step": 959
    },
    {
        "loss": 2.2035,
        "grad_norm": 1.1693586111068726,
        "learning_rate": 2.320279695814669e-05,
        "epoch": 0.26468155500413565,
        "step": 960
    },
    {
        "loss": 2.3235,
        "grad_norm": 1.5742639303207397,
        "learning_rate": 2.303547498284483e-05,
        "epoch": 0.26495726495726496,
        "step": 961
    },
    {
        "loss": 2.7455,
        "grad_norm": 1.2989141941070557,
        "learning_rate": 2.2868679928150138e-05,
        "epoch": 0.26523297491039427,
        "step": 962
    },
    {
        "loss": 2.3174,
        "grad_norm": 1.5485073328018188,
        "learning_rate": 2.2702412935988037e-05,
        "epoch": 0.2655086848635236,
        "step": 963
    },
    {
        "loss": 2.4688,
        "grad_norm": 1.324538230895996,
        "learning_rate": 2.2536675144668805e-05,
        "epoch": 0.2657843948166529,
        "step": 964
    },
    {
        "loss": 2.4647,
        "grad_norm": 1.7323393821716309,
        "learning_rate": 2.2371467688879554e-05,
        "epoch": 0.2660601047697822,
        "step": 965
    },
    {
        "loss": 1.6921,
        "grad_norm": 1.5236384868621826,
        "learning_rate": 2.2206791699676588e-05,
        "epoch": 0.2663358147229115,
        "step": 966
    },
    {
        "loss": 2.158,
        "grad_norm": 1.7317428588867188,
        "learning_rate": 2.204264830447772e-05,
        "epoch": 0.2666115246760408,
        "step": 967
    },
    {
        "loss": 1.8844,
        "grad_norm": 1.823218584060669,
        "learning_rate": 2.1879038627054394e-05,
        "epoch": 0.26688723462917013,
        "step": 968
    },
    {
        "loss": 1.8659,
        "grad_norm": 2.2785000801086426,
        "learning_rate": 2.1715963787524098e-05,
        "epoch": 0.26716294458229944,
        "step": 969
    },
    {
        "loss": 1.9473,
        "grad_norm": 1.5253223180770874,
        "learning_rate": 2.1553424902342635e-05,
        "epoch": 0.26743865453542875,
        "step": 970
    },
    {
        "loss": 1.6119,
        "grad_norm": 1.7351011037826538,
        "learning_rate": 2.1391423084296623e-05,
        "epoch": 0.267714364488558,
        "step": 971
    },
    {
        "loss": 2.0846,
        "grad_norm": 1.1944507360458374,
        "learning_rate": 2.1229959442495663e-05,
        "epoch": 0.2679900744416873,
        "step": 972
    },
    {
        "loss": 2.285,
        "grad_norm": 1.1304068565368652,
        "learning_rate": 2.1069035082364864e-05,
        "epoch": 0.26826578439481663,
        "step": 973
    },
    {
        "loss": 2.4342,
        "grad_norm": 0.8394913077354431,
        "learning_rate": 2.0908651105637332e-05,
        "epoch": 0.26854149434794594,
        "step": 974
    },
    {
        "loss": 2.0061,
        "grad_norm": 1.2239859104156494,
        "learning_rate": 2.0748808610346492e-05,
        "epoch": 0.26881720430107525,
        "step": 975
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.494499921798706,
        "learning_rate": 2.0589508690818617e-05,
        "epoch": 0.26909291425420456,
        "step": 976
    },
    {
        "loss": 1.2787,
        "grad_norm": 1.9622069597244263,
        "learning_rate": 2.0430752437665447e-05,
        "epoch": 0.2693686242073339,
        "step": 977
    },
    {
        "loss": 1.5726,
        "grad_norm": 2.088148832321167,
        "learning_rate": 2.027254093777655e-05,
        "epoch": 0.2696443341604632,
        "step": 978
    },
    {
        "loss": 2.0715,
        "grad_norm": 1.655678391456604,
        "learning_rate": 2.0114875274311994e-05,
        "epoch": 0.2699200441135925,
        "step": 979
    },
    {
        "loss": 1.5229,
        "grad_norm": 2.131235361099243,
        "learning_rate": 1.9957756526694848e-05,
        "epoch": 0.2701957540667218,
        "step": 980
    },
    {
        "loss": 1.5042,
        "grad_norm": 1.8239773511886597,
        "learning_rate": 1.980118577060397e-05,
        "epoch": 0.2704714640198511,
        "step": 981
    },
    {
        "loss": 1.5726,
        "grad_norm": 1.7681310176849365,
        "learning_rate": 1.9645164077966372e-05,
        "epoch": 0.2707471739729804,
        "step": 982
    },
    {
        "loss": 2.5179,
        "grad_norm": 1.3368233442306519,
        "learning_rate": 1.9489692516950065e-05,
        "epoch": 0.27102288392610974,
        "step": 983
    },
    {
        "loss": 2.4419,
        "grad_norm": 0.9964707493782043,
        "learning_rate": 1.9334772151956782e-05,
        "epoch": 0.27129859387923905,
        "step": 984
    },
    {
        "loss": 1.9731,
        "grad_norm": 1.8878318071365356,
        "learning_rate": 1.918040404361453e-05,
        "epoch": 0.27157430383236836,
        "step": 985
    },
    {
        "loss": 1.996,
        "grad_norm": 1.4890384674072266,
        "learning_rate": 1.902658924877043e-05,
        "epoch": 0.27185001378549767,
        "step": 986
    },
    {
        "loss": 2.0485,
        "grad_norm": 2.348191499710083,
        "learning_rate": 1.887332882048347e-05,
        "epoch": 0.272125723738627,
        "step": 987
    },
    {
        "loss": 2.2261,
        "grad_norm": 1.191663384437561,
        "learning_rate": 1.8720623808017323e-05,
        "epoch": 0.2724014336917563,
        "step": 988
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.5155421495437622,
        "learning_rate": 1.8568475256833073e-05,
        "epoch": 0.2726771436448856,
        "step": 989
    },
    {
        "loss": 2.33,
        "grad_norm": 1.1695369482040405,
        "learning_rate": 1.841688420858213e-05,
        "epoch": 0.2729528535980149,
        "step": 990
    },
    {
        "loss": 2.3906,
        "grad_norm": 1.0369139909744263,
        "learning_rate": 1.8265851701099145e-05,
        "epoch": 0.2732285635511442,
        "step": 991
    },
    {
        "loss": 1.9628,
        "grad_norm": 1.9771926403045654,
        "learning_rate": 1.8115378768394774e-05,
        "epoch": 0.27350427350427353,
        "step": 992
    },
    {
        "loss": 2.2788,
        "grad_norm": 1.4873119592666626,
        "learning_rate": 1.7965466440648636e-05,
        "epoch": 0.2737799834574028,
        "step": 993
    },
    {
        "loss": 1.601,
        "grad_norm": 1.7915735244750977,
        "learning_rate": 1.7816115744202434e-05,
        "epoch": 0.2740556934105321,
        "step": 994
    },
    {
        "loss": 2.1176,
        "grad_norm": 0.9721940159797668,
        "learning_rate": 1.766732770155264e-05,
        "epoch": 0.2743314033636614,
        "step": 995
    },
    {
        "loss": 1.9212,
        "grad_norm": 1.8775980472564697,
        "learning_rate": 1.75191033313437e-05,
        "epoch": 0.2746071133167907,
        "step": 996
    },
    {
        "loss": 2.0726,
        "grad_norm": 1.6978890895843506,
        "learning_rate": 1.7371443648360995e-05,
        "epoch": 0.27488282326992003,
        "step": 997
    },
    {
        "loss": 1.7275,
        "grad_norm": 1.8323365449905396,
        "learning_rate": 1.722434966352393e-05,
        "epoch": 0.27515853322304934,
        "step": 998
    },
    {
        "loss": 1.8697,
        "grad_norm": 2.107830286026001,
        "learning_rate": 1.7077822383878948e-05,
        "epoch": 0.27543424317617865,
        "step": 999
    },
    {
        "loss": 2.2413,
        "grad_norm": 1.110397219657898,
        "learning_rate": 1.6931862812592648e-05,
        "epoch": 0.27570995312930796,
        "step": 1000
    },
    {
        "loss": 1.9527,
        "grad_norm": 1.9603265523910522,
        "learning_rate": 1.6786471948945027e-05,
        "epoch": 0.27598566308243727,
        "step": 1001
    },
    {
        "loss": 1.7545,
        "grad_norm": 1.9627480506896973,
        "learning_rate": 1.6641650788322448e-05,
        "epoch": 0.2762613730355666,
        "step": 1002
    },
    {
        "loss": 2.1842,
        "grad_norm": 1.4805244207382202,
        "learning_rate": 1.6497400322210966e-05,
        "epoch": 0.2765370829886959,
        "step": 1003
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.8205400705337524,
        "learning_rate": 1.635372153818956e-05,
        "epoch": 0.2768127929418252,
        "step": 1004
    },
    {
        "loss": 2.4592,
        "grad_norm": 1.450458288192749,
        "learning_rate": 1.6210615419923226e-05,
        "epoch": 0.2770885028949545,
        "step": 1005
    },
    {
        "loss": 2.3245,
        "grad_norm": 1.5348856449127197,
        "learning_rate": 1.6068082947156382e-05,
        "epoch": 0.2773642128480838,
        "step": 1006
    },
    {
        "loss": 2.2209,
        "grad_norm": 1.4289112091064453,
        "learning_rate": 1.5926125095706068e-05,
        "epoch": 0.27763992280121313,
        "step": 1007
    },
    {
        "loss": 2.2847,
        "grad_norm": 1.0508142709732056,
        "learning_rate": 1.578474283745538e-05,
        "epoch": 0.27791563275434245,
        "step": 1008
    },
    {
        "loss": 1.761,
        "grad_norm": 1.453265905380249,
        "learning_rate": 1.5643937140346677e-05,
        "epoch": 0.27819134270747176,
        "step": 1009
    },
    {
        "loss": 1.5941,
        "grad_norm": 1.6001287698745728,
        "learning_rate": 1.5503708968374998e-05,
        "epoch": 0.27846705266060107,
        "step": 1010
    },
    {
        "loss": 2.0071,
        "grad_norm": 1.6126422882080078,
        "learning_rate": 1.5364059281581587e-05,
        "epoch": 0.2787427626137304,
        "step": 1011
    },
    {
        "loss": 2.1211,
        "grad_norm": 1.3339606523513794,
        "learning_rate": 1.5224989036047133e-05,
        "epoch": 0.2790184725668597,
        "step": 1012
    },
    {
        "loss": 2.167,
        "grad_norm": 1.4146981239318848,
        "learning_rate": 1.5086499183885294e-05,
        "epoch": 0.279294182519989,
        "step": 1013
    },
    {
        "loss": 1.73,
        "grad_norm": 1.855363368988037,
        "learning_rate": 1.4948590673236285e-05,
        "epoch": 0.27956989247311825,
        "step": 1014
    },
    {
        "loss": 2.3576,
        "grad_norm": 1.6215319633483887,
        "learning_rate": 1.481126444826021e-05,
        "epoch": 0.27984560242624756,
        "step": 1015
    },
    {
        "loss": 1.7123,
        "grad_norm": 1.7350748777389526,
        "learning_rate": 1.4674521449130718e-05,
        "epoch": 0.2801213123793769,
        "step": 1016
    },
    {
        "loss": 1.4895,
        "grad_norm": 2.2552618980407715,
        "learning_rate": 1.45383626120285e-05,
        "epoch": 0.2803970223325062,
        "step": 1017
    },
    {
        "loss": 2.2731,
        "grad_norm": 1.3905057907104492,
        "learning_rate": 1.4402788869134975e-05,
        "epoch": 0.2806727322856355,
        "step": 1018
    },
    {
        "loss": 2.458,
        "grad_norm": 1.0775752067565918,
        "learning_rate": 1.4267801148625792e-05,
        "epoch": 0.2809484422387648,
        "step": 1019
    },
    {
        "loss": 2.3412,
        "grad_norm": 2.076014995574951,
        "learning_rate": 1.4133400374664507e-05,
        "epoch": 0.2812241521918941,
        "step": 1020
    },
    {
        "loss": 1.34,
        "grad_norm": 2.281008720397949,
        "learning_rate": 1.3999587467396358e-05,
        "epoch": 0.2814998621450234,
        "step": 1021
    },
    {
        "loss": 2.2217,
        "grad_norm": 1.661905288696289,
        "learning_rate": 1.3866363342941813e-05,
        "epoch": 0.28177557209815274,
        "step": 1022
    },
    {
        "loss": 2.372,
        "grad_norm": 1.0738894939422607,
        "learning_rate": 1.3733728913390398e-05,
        "epoch": 0.28205128205128205,
        "step": 1023
    },
    {
        "loss": 2.6108,
        "grad_norm": 1.334444284439087,
        "learning_rate": 1.360168508679438e-05,
        "epoch": 0.28232699200441136,
        "step": 1024
    },
    {
        "loss": 1.7635,
        "grad_norm": 2.133348226547241,
        "learning_rate": 1.3470232767162671e-05,
        "epoch": 0.28260270195754067,
        "step": 1025
    },
    {
        "loss": 2.0444,
        "grad_norm": 1.6748437881469727,
        "learning_rate": 1.3339372854454513e-05,
        "epoch": 0.28287841191067,
        "step": 1026
    },
    {
        "loss": 2.1457,
        "grad_norm": 1.7746803760528564,
        "learning_rate": 1.3209106244573344e-05,
        "epoch": 0.2831541218637993,
        "step": 1027
    },
    {
        "loss": 2.2863,
        "grad_norm": 1.5756146907806396,
        "learning_rate": 1.3079433829360754e-05,
        "epoch": 0.2834298318169286,
        "step": 1028
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.9236811399459839,
        "learning_rate": 1.2950356496590233e-05,
        "epoch": 0.2837055417700579,
        "step": 1029
    },
    {
        "loss": 2.2078,
        "grad_norm": 1.221556544303894,
        "learning_rate": 1.2821875129961214e-05,
        "epoch": 0.2839812517231872,
        "step": 1030
    },
    {
        "loss": 2.0567,
        "grad_norm": 1.4781873226165771,
        "learning_rate": 1.2693990609092986e-05,
        "epoch": 0.28425696167631653,
        "step": 1031
    },
    {
        "loss": 1.7828,
        "grad_norm": 1.7941499948501587,
        "learning_rate": 1.2566703809518654e-05,
        "epoch": 0.28453267162944584,
        "step": 1032
    },
    {
        "loss": 2.2769,
        "grad_norm": 1.973793864250183,
        "learning_rate": 1.2440015602679145e-05,
        "epoch": 0.28480838158257515,
        "step": 1033
    },
    {
        "loss": 1.8539,
        "grad_norm": 1.6209328174591064,
        "learning_rate": 1.2313926855917236e-05,
        "epoch": 0.28508409153570446,
        "step": 1034
    },
    {
        "loss": 2.2273,
        "grad_norm": 0.9791462421417236,
        "learning_rate": 1.2188438432471705e-05,
        "epoch": 0.2853598014888337,
        "step": 1035
    },
    {
        "loss": 2.0296,
        "grad_norm": 2.2126262187957764,
        "learning_rate": 1.2063551191471278e-05,
        "epoch": 0.28563551144196303,
        "step": 1036
    },
    {
        "loss": 2.5274,
        "grad_norm": 1.1964391469955444,
        "learning_rate": 1.1939265987928826e-05,
        "epoch": 0.28591122139509234,
        "step": 1037
    },
    {
        "loss": 1.6211,
        "grad_norm": 2.2887532711029053,
        "learning_rate": 1.1815583672735575e-05,
        "epoch": 0.28618693134822165,
        "step": 1038
    },
    {
        "loss": 2.3462,
        "grad_norm": 1.665006399154663,
        "learning_rate": 1.1692505092655071e-05,
        "epoch": 0.28646264130135096,
        "step": 1039
    },
    {
        "loss": 1.6328,
        "grad_norm": 2.3128719329833984,
        "learning_rate": 1.1570031090317713e-05,
        "epoch": 0.2867383512544803,
        "step": 1040
    },
    {
        "loss": 1.4489,
        "grad_norm": 2.0736560821533203,
        "learning_rate": 1.1448162504214621e-05,
        "epoch": 0.2870140612076096,
        "step": 1041
    },
    {
        "loss": 1.1732,
        "grad_norm": 2.1351828575134277,
        "learning_rate": 1.1326900168692112e-05,
        "epoch": 0.2872897711607389,
        "step": 1042
    },
    {
        "loss": 2.0698,
        "grad_norm": 2.135159492492676,
        "learning_rate": 1.1206244913946017e-05,
        "epoch": 0.2875654811138682,
        "step": 1043
    },
    {
        "loss": 2.2715,
        "grad_norm": 1.25619637966156,
        "learning_rate": 1.1086197566015832e-05,
        "epoch": 0.2878411910669975,
        "step": 1044
    },
    {
        "loss": 2.1502,
        "grad_norm": 1.6618638038635254,
        "learning_rate": 1.0966758946779176e-05,
        "epoch": 0.2881169010201268,
        "step": 1045
    },
    {
        "loss": 2.2149,
        "grad_norm": 1.7582664489746094,
        "learning_rate": 1.0847929873946183e-05,
        "epoch": 0.28839261097325614,
        "step": 1046
    },
    {
        "loss": 1.5321,
        "grad_norm": 1.7906928062438965,
        "learning_rate": 1.0729711161053823e-05,
        "epoch": 0.28866832092638545,
        "step": 1047
    },
    {
        "loss": 2.302,
        "grad_norm": 1.3394906520843506,
        "learning_rate": 1.0612103617460379e-05,
        "epoch": 0.28894403087951476,
        "step": 1048
    },
    {
        "loss": 2.1564,
        "grad_norm": 1.1044589281082153,
        "learning_rate": 1.0495108048339875e-05,
        "epoch": 0.28921974083264407,
        "step": 1049
    },
    {
        "loss": 2.2587,
        "grad_norm": 1.3934285640716553,
        "learning_rate": 1.0378725254676657e-05,
        "epoch": 0.2894954507857734,
        "step": 1050
    },
    {
        "loss": 2.0276,
        "grad_norm": 1.6033809185028076,
        "learning_rate": 1.0262956033259775e-05,
        "epoch": 0.2897711607389027,
        "step": 1051
    },
    {
        "loss": 2.2669,
        "grad_norm": 0.960766077041626,
        "learning_rate": 1.0147801176677585e-05,
        "epoch": 0.290046870692032,
        "step": 1052
    },
    {
        "loss": 2.0844,
        "grad_norm": 1.8899208307266235,
        "learning_rate": 1.0033261473312405e-05,
        "epoch": 0.2903225806451613,
        "step": 1053
    },
    {
        "loss": 2.3162,
        "grad_norm": 1.317327618598938,
        "learning_rate": 9.919337707334974e-06,
        "epoch": 0.2905982905982906,
        "step": 1054
    },
    {
        "loss": 1.7135,
        "grad_norm": 2.0115439891815186,
        "learning_rate": 9.80603065869915e-06,
        "epoch": 0.29087400055141993,
        "step": 1055
    },
    {
        "loss": 1.7319,
        "grad_norm": 1.624996542930603,
        "learning_rate": 9.693341103136632e-06,
        "epoch": 0.29114971050454924,
        "step": 1056
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.7229961156845093,
        "learning_rate": 9.581269812151539e-06,
        "epoch": 0.2914254204576785,
        "step": 1057
    },
    {
        "loss": 2.2021,
        "grad_norm": 1.6489611864089966,
        "learning_rate": 9.46981755301518e-06,
        "epoch": 0.2917011304108078,
        "step": 1058
    },
    {
        "loss": 2.1029,
        "grad_norm": 1.4423463344573975,
        "learning_rate": 9.358985088760796e-06,
        "epoch": 0.2919768403639371,
        "step": 1059
    },
    {
        "loss": 1.5144,
        "grad_norm": 1.715557336807251,
        "learning_rate": 9.248773178178405e-06,
        "epoch": 0.29225255031706643,
        "step": 1060
    },
    {
        "loss": 2.3197,
        "grad_norm": 1.4963706731796265,
        "learning_rate": 9.139182575809446e-06,
        "epoch": 0.29252826027019574,
        "step": 1061
    },
    {
        "loss": 1.8856,
        "grad_norm": 1.5430203676223755,
        "learning_rate": 9.030214031941753e-06,
        "epoch": 0.29280397022332505,
        "step": 1062
    },
    {
        "loss": 2.1284,
        "grad_norm": 1.467685341835022,
        "learning_rate": 8.921868292604407e-06,
        "epoch": 0.29307968017645436,
        "step": 1063
    },
    {
        "loss": 2.2053,
        "grad_norm": 2.2180843353271484,
        "learning_rate": 8.814146099562536e-06,
        "epoch": 0.29335539012958367,
        "step": 1064
    },
    {
        "loss": 2.3236,
        "grad_norm": 1.902105450630188,
        "learning_rate": 8.707048190312317e-06,
        "epoch": 0.293631100082713,
        "step": 1065
    },
    {
        "loss": 2.1474,
        "grad_norm": 1.2903053760528564,
        "learning_rate": 8.60057529807593e-06,
        "epoch": 0.2939068100358423,
        "step": 1066
    },
    {
        "loss": 1.9693,
        "grad_norm": 1.5095659494400024,
        "learning_rate": 8.494728151796493e-06,
        "epoch": 0.2941825199889716,
        "step": 1067
    },
    {
        "loss": 2.1387,
        "grad_norm": 1.4283480644226074,
        "learning_rate": 8.389507476133085e-06,
        "epoch": 0.2944582299421009,
        "step": 1068
    },
    {
        "loss": 2.483,
        "grad_norm": 1.3743053674697876,
        "learning_rate": 8.28491399145579e-06,
        "epoch": 0.2947339398952302,
        "step": 1069
    },
    {
        "loss": 2.1809,
        "grad_norm": 1.6552199125289917,
        "learning_rate": 8.180948413840816e-06,
        "epoch": 0.29500964984835953,
        "step": 1070
    },
    {
        "loss": 2.8125,
        "grad_norm": 1.1188753843307495,
        "learning_rate": 8.077611455065492e-06,
        "epoch": 0.29528535980148884,
        "step": 1071
    },
    {
        "loss": 2.424,
        "grad_norm": 1.2018100023269653,
        "learning_rate": 7.97490382260343e-06,
        "epoch": 0.29556106975461816,
        "step": 1072
    },
    {
        "loss": 2.2465,
        "grad_norm": 1.4085772037506104,
        "learning_rate": 7.872826219619788e-06,
        "epoch": 0.29583677970774747,
        "step": 1073
    },
    {
        "loss": 1.1387,
        "grad_norm": 1.7852072715759277,
        "learning_rate": 7.77137934496628e-06,
        "epoch": 0.2961124896608768,
        "step": 1074
    },
    {
        "loss": 2.5147,
        "grad_norm": 1.1028474569320679,
        "learning_rate": 7.670563893176496e-06,
        "epoch": 0.2963881996140061,
        "step": 1075
    },
    {
        "loss": 2.489,
        "grad_norm": 1.2377347946166992,
        "learning_rate": 7.570380554461165e-06,
        "epoch": 0.2966639095671354,
        "step": 1076
    },
    {
        "loss": 2.3189,
        "grad_norm": 1.519134759902954,
        "learning_rate": 7.470830014703367e-06,
        "epoch": 0.2969396195202647,
        "step": 1077
    },
    {
        "loss": 2.1935,
        "grad_norm": 1.8203017711639404,
        "learning_rate": 7.371912955453863e-06,
        "epoch": 0.29721532947339396,
        "step": 1078
    },
    {
        "loss": 1.9518,
        "grad_norm": 1.839554786682129,
        "learning_rate": 7.273630053926406e-06,
        "epoch": 0.2974910394265233,
        "step": 1079
    },
    {
        "loss": 2.2581,
        "grad_norm": 1.726731777191162,
        "learning_rate": 7.175981982993208e-06,
        "epoch": 0.2977667493796526,
        "step": 1080
    },
    {
        "loss": 2.1957,
        "grad_norm": 1.4408507347106934,
        "learning_rate": 7.078969411180158e-06,
        "epoch": 0.2980424593327819,
        "step": 1081
    },
    {
        "loss": 2.3388,
        "grad_norm": 1.4542776346206665,
        "learning_rate": 6.982593002662385e-06,
        "epoch": 0.2983181692859112,
        "step": 1082
    },
    {
        "loss": 2.2478,
        "grad_norm": 1.6041150093078613,
        "learning_rate": 6.886853417259698e-06,
        "epoch": 0.2985938792390405,
        "step": 1083
    },
    {
        "loss": 1.7321,
        "grad_norm": 1.3366132974624634,
        "learning_rate": 6.791751310431993e-06,
        "epoch": 0.2988695891921698,
        "step": 1084
    },
    {
        "loss": 2.0337,
        "grad_norm": 0.9603475332260132,
        "learning_rate": 6.6972873332747825e-06,
        "epoch": 0.29914529914529914,
        "step": 1085
    },
    {
        "loss": 2.0064,
        "grad_norm": 1.9144160747528076,
        "learning_rate": 6.6034621325148595e-06,
        "epoch": 0.29942100909842845,
        "step": 1086
    },
    {
        "loss": 1.971,
        "grad_norm": 1.708146095275879,
        "learning_rate": 6.510276350505706e-06,
        "epoch": 0.29969671905155776,
        "step": 1087
    },
    {
        "loss": 2.0304,
        "grad_norm": 1.5516304969787598,
        "learning_rate": 6.417730625223162e-06,
        "epoch": 0.29997242900468707,
        "step": 1088
    },
    {
        "loss": 2.2914,
        "grad_norm": 1.0485347509384155,
        "learning_rate": 6.3258255902610695e-06,
        "epoch": 0.3002481389578164,
        "step": 1089
    },
    {
        "loss": 2.4154,
        "grad_norm": 1.7613935470581055,
        "learning_rate": 6.234561874826961e-06,
        "epoch": 0.3005238489109457,
        "step": 1090
    },
    {
        "loss": 2.2072,
        "grad_norm": 1.3806629180908203,
        "learning_rate": 6.143940103737689e-06,
        "epoch": 0.300799558864075,
        "step": 1091
    },
    {
        "loss": 2.0206,
        "grad_norm": 1.9463629722595215,
        "learning_rate": 6.053960897415156e-06,
        "epoch": 0.3010752688172043,
        "step": 1092
    },
    {
        "loss": 1.793,
        "grad_norm": 1.6635184288024902,
        "learning_rate": 5.9646248718821185e-06,
        "epoch": 0.3013509787703336,
        "step": 1093
    },
    {
        "loss": 2.5836,
        "grad_norm": 1.621773600578308,
        "learning_rate": 5.875932638757942e-06,
        "epoch": 0.30162668872346293,
        "step": 1094
    },
    {
        "loss": 1.9904,
        "grad_norm": 2.31146240234375,
        "learning_rate": 5.787884805254384e-06,
        "epoch": 0.30190239867659224,
        "step": 1095
    },
    {
        "loss": 2.3526,
        "grad_norm": 1.757901668548584,
        "learning_rate": 5.700481974171445e-06,
        "epoch": 0.30217810862972155,
        "step": 1096
    },
    {
        "loss": 1.0157,
        "grad_norm": 1.8063249588012695,
        "learning_rate": 5.613724743893334e-06,
        "epoch": 0.30245381858285086,
        "step": 1097
    },
    {
        "loss": 1.3503,
        "grad_norm": 2.0920188426971436,
        "learning_rate": 5.527613708384227e-06,
        "epoch": 0.3027295285359802,
        "step": 1098
    },
    {
        "loss": 2.1425,
        "grad_norm": 1.8567556142807007,
        "learning_rate": 5.4421494571842755e-06,
        "epoch": 0.30300523848910943,
        "step": 1099
    },
    {
        "loss": 2.4157,
        "grad_norm": 1.4439020156860352,
        "learning_rate": 5.357332575405627e-06,
        "epoch": 0.30328094844223874,
        "step": 1100
    },
    {
        "loss": 2.4512,
        "grad_norm": 1.183168649673462,
        "learning_rate": 5.273163643728274e-06,
        "epoch": 0.30355665839536805,
        "step": 1101
    },
    {
        "loss": 1.6905,
        "grad_norm": 2.026597261428833,
        "learning_rate": 5.189643238396214e-06,
        "epoch": 0.30383236834849736,
        "step": 1102
    },
    {
        "loss": 2.2601,
        "grad_norm": 1.684449553489685,
        "learning_rate": 5.10677193121345e-06,
        "epoch": 0.30410807830162667,
        "step": 1103
    },
    {
        "loss": 1.5133,
        "grad_norm": 2.259207248687744,
        "learning_rate": 5.0245502895400645e-06,
        "epoch": 0.304383788254756,
        "step": 1104
    },
    {
        "loss": 2.257,
        "grad_norm": 1.5404421091079712,
        "learning_rate": 4.94297887628834e-06,
        "epoch": 0.3046594982078853,
        "step": 1105
    },
    {
        "loss": 1.7229,
        "grad_norm": 1.5964397192001343,
        "learning_rate": 4.8620582499189326e-06,
        "epoch": 0.3049352081610146,
        "step": 1106
    },
    {
        "loss": 1.9709,
        "grad_norm": 1.7463626861572266,
        "learning_rate": 4.781788964437017e-06,
        "epoch": 0.3052109181141439,
        "step": 1107
    },
    {
        "loss": 1.3462,
        "grad_norm": 2.2008657455444336,
        "learning_rate": 4.702171569388525e-06,
        "epoch": 0.3054866280672732,
        "step": 1108
    },
    {
        "loss": 2.1103,
        "grad_norm": 1.648034691810608,
        "learning_rate": 4.623206609856334e-06,
        "epoch": 0.30576233802040254,
        "step": 1109
    },
    {
        "loss": 2.4504,
        "grad_norm": 1.8177776336669922,
        "learning_rate": 4.5448946264566084e-06,
        "epoch": 0.30603804797353185,
        "step": 1110
    },
    {
        "loss": 2.2626,
        "grad_norm": 1.3833585977554321,
        "learning_rate": 4.4672361553350085e-06,
        "epoch": 0.30631375792666116,
        "step": 1111
    },
    {
        "loss": 1.422,
        "grad_norm": 1.8276500701904297,
        "learning_rate": 4.390231728163097e-06,
        "epoch": 0.30658946787979047,
        "step": 1112
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.4431171417236328,
        "learning_rate": 4.313881872134673e-06,
        "epoch": 0.3068651778329198,
        "step": 1113
    },
    {
        "loss": 1.714,
        "grad_norm": 1.8152730464935303,
        "learning_rate": 4.238187109962144e-06,
        "epoch": 0.3071408877860491,
        "step": 1114
    },
    {
        "loss": 2.2826,
        "grad_norm": 1.0310381650924683,
        "learning_rate": 4.163147959872949e-06,
        "epoch": 0.3074165977391784,
        "step": 1115
    },
    {
        "loss": 1.4728,
        "grad_norm": 2.1528167724609375,
        "learning_rate": 4.088764935606049e-06,
        "epoch": 0.3076923076923077,
        "step": 1116
    },
    {
        "loss": 2.0096,
        "grad_norm": 1.7080332040786743,
        "learning_rate": 4.015038546408401e-06,
        "epoch": 0.307968017645437,
        "step": 1117
    },
    {
        "loss": 1.8067,
        "grad_norm": 1.9841722249984741,
        "learning_rate": 3.941969297031401e-06,
        "epoch": 0.30824372759856633,
        "step": 1118
    },
    {
        "loss": 1.6434,
        "grad_norm": 1.5193372964859009,
        "learning_rate": 3.8695576877275205e-06,
        "epoch": 0.30851943755169564,
        "step": 1119
    },
    {
        "loss": 2.0567,
        "grad_norm": 1.522721290588379,
        "learning_rate": 3.797804214246847e-06,
        "epoch": 0.30879514750482495,
        "step": 1120
    },
    {
        "loss": 2.0667,
        "grad_norm": 1.5111783742904663,
        "learning_rate": 3.7267093678336693e-06,
        "epoch": 0.3090708574579542,
        "step": 1121
    },
    {
        "loss": 2.1051,
        "grad_norm": 1.1761163473129272,
        "learning_rate": 3.6562736352231286e-06,
        "epoch": 0.3093465674110835,
        "step": 1122
    },
    {
        "loss": 2.0667,
        "grad_norm": 1.541649580001831,
        "learning_rate": 3.5864974986379085e-06,
        "epoch": 0.30962227736421283,
        "step": 1123
    },
    {
        "loss": 2.6271,
        "grad_norm": 1.0372754335403442,
        "learning_rate": 3.5173814357849057e-06,
        "epoch": 0.30989798731734214,
        "step": 1124
    },
    {
        "loss": 1.8583,
        "grad_norm": 1.512929916381836,
        "learning_rate": 3.448925919851953e-06,
        "epoch": 0.31017369727047145,
        "step": 1125
    },
    {
        "loss": 1.5318,
        "grad_norm": 1.854924201965332,
        "learning_rate": 3.381131419504613e-06,
        "epoch": 0.31044940722360076,
        "step": 1126
    },
    {
        "loss": 2.1708,
        "grad_norm": 1.5170143842697144,
        "learning_rate": 3.313998398882956e-06,
        "epoch": 0.31072511717673007,
        "step": 1127
    },
    {
        "loss": 1.2901,
        "grad_norm": 1.8717498779296875,
        "learning_rate": 3.2475273175983535e-06,
        "epoch": 0.3110008271298594,
        "step": 1128
    },
    {
        "loss": 2.363,
        "grad_norm": 1.1619242429733276,
        "learning_rate": 3.181718630730379e-06,
        "epoch": 0.3112765370829887,
        "step": 1129
    },
    {
        "loss": 2.0967,
        "grad_norm": 1.1216890811920166,
        "learning_rate": 3.1165727888236573e-06,
        "epoch": 0.311552247036118,
        "step": 1130
    },
    {
        "loss": 1.6761,
        "grad_norm": 1.772851586341858,
        "learning_rate": 3.052090237884797e-06,
        "epoch": 0.3118279569892473,
        "step": 1131
    },
    {
        "loss": 2.0127,
        "grad_norm": 1.5494276285171509,
        "learning_rate": 2.9882714193793293e-06,
        "epoch": 0.3121036669423766,
        "step": 1132
    },
    {
        "loss": 1.9926,
        "grad_norm": 1.4506374597549438,
        "learning_rate": 2.9251167702286754e-06,
        "epoch": 0.31237937689550593,
        "step": 1133
    },
    {
        "loss": 2.5526,
        "grad_norm": 0.9855694770812988,
        "learning_rate": 2.8626267228071933e-06,
        "epoch": 0.31265508684863524,
        "step": 1134
    },
    {
        "loss": 1.5881,
        "grad_norm": 1.5388518571853638,
        "learning_rate": 2.8008017049391823e-06,
        "epoch": 0.31293079680176455,
        "step": 1135
    },
    {
        "loss": 2.3149,
        "grad_norm": 1.0195565223693848,
        "learning_rate": 2.739642139895937e-06,
        "epoch": 0.31320650675489387,
        "step": 1136
    },
    {
        "loss": 2.178,
        "grad_norm": 1.1820956468582153,
        "learning_rate": 2.6791484463929205e-06,
        "epoch": 0.3134822167080232,
        "step": 1137
    },
    {
        "loss": 1.9854,
        "grad_norm": 1.1912212371826172,
        "learning_rate": 2.6193210385868417e-06,
        "epoch": 0.3137579266611525,
        "step": 1138
    },
    {
        "loss": 2.1422,
        "grad_norm": 1.5585263967514038,
        "learning_rate": 2.5601603260727914e-06,
        "epoch": 0.3140336366142818,
        "step": 1139
    },
    {
        "loss": 1.8725,
        "grad_norm": 1.758775234222412,
        "learning_rate": 2.5016667138815343e-06,
        "epoch": 0.3143093465674111,
        "step": 1140
    },
    {
        "loss": 2.5284,
        "grad_norm": 1.2024363279342651,
        "learning_rate": 2.4438406024766546e-06,
        "epoch": 0.3145850565205404,
        "step": 1141
    },
    {
        "loss": 2.3419,
        "grad_norm": 1.398672342300415,
        "learning_rate": 2.3866823877518376e-06,
        "epoch": 0.3148607664736697,
        "step": 1142
    },
    {
        "loss": 2.1848,
        "grad_norm": 1.6229032278060913,
        "learning_rate": 2.3301924610281577e-06,
        "epoch": 0.315136476426799,
        "step": 1143
    },
    {
        "loss": 2.1597,
        "grad_norm": 1.3161640167236328,
        "learning_rate": 2.274371209051429e-06,
        "epoch": 0.3154121863799283,
        "step": 1144
    },
    {
        "loss": 2.2714,
        "grad_norm": 1.402946949005127,
        "learning_rate": 2.219219013989493e-06,
        "epoch": 0.3156878963330576,
        "step": 1145
    },
    {
        "loss": 2.0031,
        "grad_norm": 1.725660800933838,
        "learning_rate": 2.1647362534296667e-06,
        "epoch": 0.3159636062861869,
        "step": 1146
    },
    {
        "loss": 1.8057,
        "grad_norm": 1.8269253969192505,
        "learning_rate": 2.1109233003761333e-06,
        "epoch": 0.3162393162393162,
        "step": 1147
    },
    {
        "loss": 2.4192,
        "grad_norm": 0.9231036305427551,
        "learning_rate": 2.057780523247388e-06,
        "epoch": 0.31651502619244554,
        "step": 1148
    },
    {
        "loss": 2.376,
        "grad_norm": 1.6108057498931885,
        "learning_rate": 2.005308285873675e-06,
        "epoch": 0.31679073614557485,
        "step": 1149
    },
    {
        "loss": 2.4663,
        "grad_norm": 0.8201878666877747,
        "learning_rate": 1.95350694749461e-06,
        "epoch": 0.31706644609870416,
        "step": 1150
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.4109398126602173,
        "learning_rate": 1.902376862756583e-06,
        "epoch": 0.31734215605183347,
        "step": 1151
    },
    {
        "loss": 1.8607,
        "grad_norm": 2.5549395084381104,
        "learning_rate": 1.8519183817104157e-06,
        "epoch": 0.3176178660049628,
        "step": 1152
    },
    {
        "loss": 2.1838,
        "grad_norm": 1.3347634077072144,
        "learning_rate": 1.8021318498089301e-06,
        "epoch": 0.3178935759580921,
        "step": 1153
    },
    {
        "loss": 1.963,
        "grad_norm": 1.1992536783218384,
        "learning_rate": 1.75301760790465e-06,
        "epoch": 0.3181692859112214,
        "step": 1154
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.8595868349075317,
        "learning_rate": 1.7045759922473481e-06,
        "epoch": 0.3184449958643507,
        "step": 1155
    },
    {
        "loss": 1.529,
        "grad_norm": 1.735414981842041,
        "learning_rate": 1.6568073344818469e-06,
        "epoch": 0.31872070581748,
        "step": 1156
    },
    {
        "loss": 1.6038,
        "grad_norm": 1.8727668523788452,
        "learning_rate": 1.609711961645699e-06,
        "epoch": 0.31899641577060933,
        "step": 1157
    },
    {
        "loss": 1.5041,
        "grad_norm": 1.7745171785354614,
        "learning_rate": 1.5632901961669554e-06,
        "epoch": 0.31927212572373864,
        "step": 1158
    },
    {
        "loss": 0.6973,
        "grad_norm": 1.723145842552185,
        "learning_rate": 1.5175423558619895e-06,
        "epoch": 0.31954783567686795,
        "step": 1159
    },
    {
        "loss": 2.3826,
        "grad_norm": 1.7928528785705566,
        "learning_rate": 1.4724687539332649e-06,
        "epoch": 0.31982354562999726,
        "step": 1160
    },
    {
        "loss": 1.5307,
        "grad_norm": 2.020148277282715,
        "learning_rate": 1.4280696989672383e-06,
        "epoch": 0.3200992555831266,
        "step": 1161
    },
    {
        "loss": 1.9713,
        "grad_norm": 1.675320029258728,
        "learning_rate": 1.3843454949322266e-06,
        "epoch": 0.3203749655362559,
        "step": 1162
    },
    {
        "loss": 2.4412,
        "grad_norm": 1.0224696397781372,
        "learning_rate": 1.3412964411763096e-06,
        "epoch": 0.32065067548938514,
        "step": 1163
    },
    {
        "loss": 1.0702,
        "grad_norm": 1.7915852069854736,
        "learning_rate": 1.2989228324253422e-06,
        "epoch": 0.32092638544251445,
        "step": 1164
    },
    {
        "loss": 1.5493,
        "grad_norm": 1.9592838287353516,
        "learning_rate": 1.257224958780867e-06,
        "epoch": 0.32120209539564376,
        "step": 1165
    },
    {
        "loss": 2.4453,
        "grad_norm": 1.253659725189209,
        "learning_rate": 1.216203105718139e-06,
        "epoch": 0.32147780534877307,
        "step": 1166
    },
    {
        "loss": 2.1238,
        "grad_norm": 2.010758876800537,
        "learning_rate": 1.1758575540842254e-06,
        "epoch": 0.3217535153019024,
        "step": 1167
    },
    {
        "loss": 2.2627,
        "grad_norm": 0.9543688893318176,
        "learning_rate": 1.1361885800960204e-06,
        "epoch": 0.3220292252550317,
        "step": 1168
    },
    {
        "loss": 2.1885,
        "grad_norm": 1.639122724533081,
        "learning_rate": 1.0971964553383784e-06,
        "epoch": 0.322304935208161,
        "step": 1169
    },
    {
        "loss": 1.8593,
        "grad_norm": 2.145939826965332,
        "learning_rate": 1.058881446762272e-06,
        "epoch": 0.3225806451612903,
        "step": 1170
    },
    {
        "loss": 2.4144,
        "grad_norm": 0.9973244667053223,
        "learning_rate": 1.0212438166829374e-06,
        "epoch": 0.3228563551144196,
        "step": 1171
    },
    {
        "loss": 2.3103,
        "grad_norm": 1.3766577243804932,
        "learning_rate": 9.842838227780982e-07,
        "epoch": 0.32313206506754893,
        "step": 1172
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.7083044052124023,
        "learning_rate": 9.480017180861556e-07,
        "epoch": 0.32340777502067825,
        "step": 1173
    },
    {
        "loss": 2.5091,
        "grad_norm": 1.6641223430633545,
        "learning_rate": 9.123977510045345e-07,
        "epoch": 0.32368348497380756,
        "step": 1174
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.3435019254684448,
        "learning_rate": 8.774721652879403e-07,
        "epoch": 0.32395919492693687,
        "step": 1175
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.0333815813064575,
        "learning_rate": 8.432252000466378e-07,
        "epoch": 0.3242349048800662,
        "step": 1176
    },
    {
        "loss": 2.3546,
        "grad_norm": 1.2720701694488525,
        "learning_rate": 8.096570897449418e-07,
        "epoch": 0.3245106148331955,
        "step": 1177
    },
    {
        "loss": 2.0033,
        "grad_norm": 1.4992989301681519,
        "learning_rate": 7.767680641994845e-07,
        "epoch": 0.3247863247863248,
        "step": 1178
    },
    {
        "loss": 1.8188,
        "grad_norm": 1.852197289466858,
        "learning_rate": 7.445583485776952e-07,
        "epoch": 0.3250620347394541,
        "step": 1179
    },
    {
        "loss": 2.3697,
        "grad_norm": 1.3258275985717773,
        "learning_rate": 7.130281633962898e-07,
        "epoch": 0.3253377446925834,
        "step": 1180
    },
    {
        "loss": 1.1173,
        "grad_norm": 1.568833827972412,
        "learning_rate": 6.82177724519717e-07,
        "epoch": 0.32561345464571273,
        "step": 1181
    },
    {
        "loss": 2.0528,
        "grad_norm": 1.5669080018997192,
        "learning_rate": 6.520072431586699e-07,
        "epoch": 0.32588916459884204,
        "step": 1182
    },
    {
        "loss": 2.2522,
        "grad_norm": 1.0796053409576416,
        "learning_rate": 6.225169258686769e-07,
        "epoch": 0.32616487455197135,
        "step": 1183
    },
    {
        "loss": 1.9055,
        "grad_norm": 1.9309531450271606,
        "learning_rate": 5.937069745486911e-07,
        "epoch": 0.32644058450510066,
        "step": 1184
    },
    {
        "loss": 2.0695,
        "grad_norm": 2.1182258129119873,
        "learning_rate": 5.655775864396695e-07,
        "epoch": 0.3267162944582299,
        "step": 1185
    },
    {
        "loss": 2.2047,
        "grad_norm": 1.4586493968963623,
        "learning_rate": 5.381289541232515e-07,
        "epoch": 0.3269920044113592,
        "step": 1186
    },
    {
        "loss": 1.2075,
        "grad_norm": 1.8725250959396362,
        "learning_rate": 5.11361265520427e-07,
        "epoch": 0.32726771436448854,
        "step": 1187
    },
    {
        "loss": 2.57,
        "grad_norm": 1.0993562936782837,
        "learning_rate": 4.852747038902594e-07,
        "epoch": 0.32754342431761785,
        "step": 1188
    },
    {
        "loss": 2.3847,
        "grad_norm": 1.0395641326904297,
        "learning_rate": 4.598694478286314e-07,
        "epoch": 0.32781913427074716,
        "step": 1189
    },
    {
        "loss": 1.8065,
        "grad_norm": 1.9912828207015991,
        "learning_rate": 4.351456712670121e-07,
        "epoch": 0.32809484422387647,
        "step": 1190
    },
    {
        "loss": 2.1551,
        "grad_norm": 0.9124318957328796,
        "learning_rate": 4.1110354347125844e-07,
        "epoch": 0.3283705541770058,
        "step": 1191
    },
    {
        "loss": 1.8493,
        "grad_norm": 2.0474956035614014,
        "learning_rate": 3.8774322904050476e-07,
        "epoch": 0.3286462641301351,
        "step": 1192
    },
    {
        "loss": 2.4211,
        "grad_norm": 1.2309054136276245,
        "learning_rate": 3.6506488790596374e-07,
        "epoch": 0.3289219740832644,
        "step": 1193
    },
    {
        "loss": 1.8121,
        "grad_norm": 1.336963415145874,
        "learning_rate": 3.4306867532990506e-07,
        "epoch": 0.3291976840363937,
        "step": 1194
    },
    {
        "loss": 2.3949,
        "grad_norm": 1.1799609661102295,
        "learning_rate": 3.2175474190451194e-07,
        "epoch": 0.329473393989523,
        "step": 1195
    },
    {
        "loss": 2.417,
        "grad_norm": 1.7905566692352295,
        "learning_rate": 3.011232335509151e-07,
        "epoch": 0.32974910394265233,
        "step": 1196
    },
    {
        "loss": 2.0244,
        "grad_norm": 1.775560975074768,
        "learning_rate": 2.811742915182047e-07,
        "epoch": 0.33002481389578164,
        "step": 1197
    },
    {
        "loss": 2.078,
        "grad_norm": 1.4618090391159058,
        "learning_rate": 2.619080523823869e-07,
        "epoch": 0.33030052384891095,
        "step": 1198
    },
    {
        "loss": 1.9599,
        "grad_norm": 1.761512041091919,
        "learning_rate": 2.433246480455065e-07,
        "epoch": 0.33057623380204026,
        "step": 1199
    },
    {
        "loss": 2.124,
        "grad_norm": 1.7444043159484863,
        "learning_rate": 2.2542420573472556e-07,
        "epoch": 0.3308519437551696,
        "step": 1200
    },
    {
        "loss": 1.4492,
        "grad_norm": 1.6306142807006836,
        "learning_rate": 2.0820684800147983e-07,
        "epoch": 0.3311276537082989,
        "step": 1201
    },
    {
        "loss": 2.2295,
        "grad_norm": 1.3211798667907715,
        "learning_rate": 1.916726927206014e-07,
        "epoch": 0.3314033636614282,
        "step": 1202
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.4078524112701416,
        "learning_rate": 1.7582185308953058e-07,
        "epoch": 0.3316790736145575,
        "step": 1203
    },
    {
        "loss": 2.0264,
        "grad_norm": 1.7785959243774414,
        "learning_rate": 1.6065443762752764e-07,
        "epoch": 0.3319547835676868,
        "step": 1204
    },
    {
        "loss": 1.1169,
        "grad_norm": 2.159813642501831,
        "learning_rate": 1.4617055017495108e-07,
        "epoch": 0.33223049352081613,
        "step": 1205
    },
    {
        "loss": 1.4969,
        "grad_norm": 2.3365495204925537,
        "learning_rate": 1.3237028989254718e-07,
        "epoch": 0.3325062034739454,
        "step": 1206
    },
    {
        "loss": 2.42,
        "grad_norm": 1.2301969528198242,
        "learning_rate": 1.1925375126072835e-07,
        "epoch": 0.3327819134270747,
        "step": 1207
    },
    {
        "loss": 2.3123,
        "grad_norm": 1.315603256225586,
        "learning_rate": 1.0682102407899574e-07,
        "epoch": 0.333057623380204,
        "step": 1208
    },
    {
        "loss": 1.9597,
        "grad_norm": 1.249979019165039,
        "learning_rate": 9.507219346526209e-08,
        "epoch": 0.3333333333333333,
        "step": 1209
    },
    {
        "loss": 1.4378,
        "grad_norm": 1.6229546070098877,
        "learning_rate": 8.400733985528542e-08,
        "epoch": 0.3336090432864626,
        "step": 1210
    },
    {
        "loss": 2.3005,
        "grad_norm": 1.027666449546814,
        "learning_rate": 7.362653900215844e-08,
        "epoch": 0.33388475323959194,
        "step": 1211
    },
    {
        "loss": 1.6864,
        "grad_norm": 1.5134004354476929,
        "learning_rate": 6.392986197572005e-08,
        "epoch": 0.33416046319272125,
        "step": 1212
    },
    {
        "loss": 2.0752,
        "grad_norm": 1.8753488063812256,
        "learning_rate": 5.491737516214457e-08,
        "epoch": 0.33443617314585056,
        "step": 1213
    },
    {
        "loss": 1.8126,
        "grad_norm": 2.081758975982666,
        "learning_rate": 4.6589140263431084e-08,
        "epoch": 0.33471188309897987,
        "step": 1214
    },
    {
        "loss": 2.582,
        "grad_norm": 0.9802137017250061,
        "learning_rate": 3.89452142969815e-08,
        "epoch": 0.3349875930521092,
        "step": 1215
    },
    {
        "loss": 1.6038,
        "grad_norm": 1.8968368768692017,
        "learning_rate": 3.198564959526751e-08,
        "epoch": 0.3352633030052385,
        "step": 1216
    },
    {
        "loss": 1.5246,
        "grad_norm": 2.47214412689209,
        "learning_rate": 2.5710493805386483e-08,
        "epoch": 0.3355390129583678,
        "step": 1217
    },
    {
        "loss": 2.1607,
        "grad_norm": 1.6172101497650146,
        "learning_rate": 2.011978988881724e-08,
        "epoch": 0.3358147229114971,
        "step": 1218
    },
    {
        "loss": 1.9586,
        "grad_norm": 1.8756829500198364,
        "learning_rate": 1.521357612108698e-08,
        "epoch": 0.3360904328646264,
        "step": 1219
    },
    {
        "loss": 1.9099,
        "grad_norm": 1.654079794883728,
        "learning_rate": 1.0991886091482606e-08,
        "epoch": 0.33636614281775573,
        "step": 1220
    },
    {
        "loss": 0.9224,
        "grad_norm": 1.6459259986877441,
        "learning_rate": 7.45474870289531e-09,
        "epoch": 0.33664185277088504,
        "step": 1221
    },
    {
        "loss": 2.1646,
        "grad_norm": 1.5079041719436646,
        "learning_rate": 4.602188171543009e-09,
        "epoch": 0.33691756272401435,
        "step": 1222
    },
    {
        "loss": 2.0713,
        "grad_norm": 1.5515720844268799,
        "learning_rate": 2.4342240268371288e-09,
        "epoch": 0.33719327267714366,
        "step": 1223
    },
    {
        "loss": 1.2474,
        "grad_norm": 1.8785326480865479,
        "learning_rate": 9.50871111293772e-10,
        "epoch": 0.337468982630273,
        "step": 1224
    },
    {
        "loss": 2.2674,
        "grad_norm": 1.6110845804214478,
        "learning_rate": 1.5213958033388765e-10,
        "epoch": 0.3377446925834023,
        "step": 1225
    },
    {
        "loss": 1.919,
        "grad_norm": 1.9091812372207642,
        "learning_rate": 0.0001999999619650977,
        "epoch": 0.3380204025365316,
        "step": 1226
    },
    {
        "loss": 1.9654,
        "grad_norm": 1.9427493810653687,
        "learning_rate": 0.00019999939144214158,
        "epoch": 0.33829611248966085,
        "step": 1227
    },
    {
        "loss": 1.3063,
        "grad_norm": 1.6136484146118164,
        "learning_rate": 0.00019999813629545732,
        "epoch": 0.33857182244279016,
        "step": 1228
    },
    {
        "loss": 2.2616,
        "grad_norm": 1.3808478116989136,
        "learning_rate": 0.00019999619653363797,
        "epoch": 0.33884753239591947,
        "step": 1229
    },
    {
        "loss": 2.0448,
        "grad_norm": 1.1394033432006836,
        "learning_rate": 0.00019999357216996368,
        "epoch": 0.3391232423490488,
        "step": 1230
    },
    {
        "loss": 2.4644,
        "grad_norm": 1.4504057168960571,
        "learning_rate": 0.00019999026322240157,
        "epoch": 0.3393989523021781,
        "step": 1231
    },
    {
        "loss": 2.125,
        "grad_norm": 1.5055843591690063,
        "learning_rate": 0.00019998626971360562,
        "epoch": 0.3396746622553074,
        "step": 1232
    },
    {
        "loss": 2.4384,
        "grad_norm": 1.401262640953064,
        "learning_rate": 0.00019998159167091656,
        "epoch": 0.3399503722084367,
        "step": 1233
    },
    {
        "loss": 2.4361,
        "grad_norm": 1.2768614292144775,
        "learning_rate": 0.0001999762291263615,
        "epoch": 0.340226082161566,
        "step": 1234
    },
    {
        "loss": 2.121,
        "grad_norm": 1.8885694742202759,
        "learning_rate": 0.000199970182116654,
        "epoch": 0.34050179211469533,
        "step": 1235
    },
    {
        "loss": 1.5098,
        "grad_norm": 2.190598964691162,
        "learning_rate": 0.00019996345068319345,
        "epoch": 0.34077750206782464,
        "step": 1236
    },
    {
        "loss": 1.9923,
        "grad_norm": 1.4614882469177246,
        "learning_rate": 0.00019995603487206527,
        "epoch": 0.34105321202095396,
        "step": 1237
    },
    {
        "loss": 1.9729,
        "grad_norm": 1.2507485151290894,
        "learning_rate": 0.00019994793473404005,
        "epoch": 0.34132892197408327,
        "step": 1238
    },
    {
        "loss": 2.4918,
        "grad_norm": 1.319942593574524,
        "learning_rate": 0.00019993915032457364,
        "epoch": 0.3416046319272126,
        "step": 1239
    },
    {
        "loss": 2.1749,
        "grad_norm": 1.5414832830429077,
        "learning_rate": 0.00019992968170380655,
        "epoch": 0.3418803418803419,
        "step": 1240
    },
    {
        "loss": 2.2336,
        "grad_norm": 1.913981318473816,
        "learning_rate": 0.00019991952893656364,
        "epoch": 0.3421560518334712,
        "step": 1241
    },
    {
        "loss": 2.3428,
        "grad_norm": 2.122938394546509,
        "learning_rate": 0.00019990869209235353,
        "epoch": 0.3424317617866005,
        "step": 1242
    },
    {
        "loss": 2.439,
        "grad_norm": 1.398268222808838,
        "learning_rate": 0.0001998971712453683,
        "epoch": 0.3427074717397298,
        "step": 1243
    },
    {
        "loss": 2.4573,
        "grad_norm": 1.3139067888259888,
        "learning_rate": 0.00019988496647448288,
        "epoch": 0.34298318169285913,
        "step": 1244
    },
    {
        "loss": 2.1223,
        "grad_norm": 1.737505555152893,
        "learning_rate": 0.00019987207786325458,
        "epoch": 0.34325889164598844,
        "step": 1245
    },
    {
        "loss": 1.8818,
        "grad_norm": 2.0766475200653076,
        "learning_rate": 0.00019985850549992233,
        "epoch": 0.34353460159911775,
        "step": 1246
    },
    {
        "loss": 2.1968,
        "grad_norm": 1.7100533246994019,
        "learning_rate": 0.0001998442494774064,
        "epoch": 0.34381031155224706,
        "step": 1247
    },
    {
        "loss": 2.3765,
        "grad_norm": 1.3535187244415283,
        "learning_rate": 0.00019982930989330747,
        "epoch": 0.34408602150537637,
        "step": 1248
    },
    {
        "loss": 1.837,
        "grad_norm": 1.1137019395828247,
        "learning_rate": 0.0001998136868499061,
        "epoch": 0.3443617314585056,
        "step": 1249
    },
    {
        "loss": 1.7871,
        "grad_norm": 1.7918860912322998,
        "learning_rate": 0.000199797380454162,
        "epoch": 0.34463744141163494,
        "step": 1250
    },
    {
        "loss": 1.6839,
        "grad_norm": 2.1915442943573,
        "learning_rate": 0.0001997803908177133,
        "epoch": 0.34491315136476425,
        "step": 1251
    },
    {
        "loss": 1.9744,
        "grad_norm": 1.2688215970993042,
        "learning_rate": 0.00019976271805687582,
        "epoch": 0.34518886131789356,
        "step": 1252
    },
    {
        "loss": 2.2434,
        "grad_norm": 1.693837285041809,
        "learning_rate": 0.00019974436229264217,
        "epoch": 0.34546457127102287,
        "step": 1253
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.282989025115967,
        "learning_rate": 0.00019972532365068103,
        "epoch": 0.3457402812241522,
        "step": 1254
    },
    {
        "loss": 1.8962,
        "grad_norm": 1.5717259645462036,
        "learning_rate": 0.0001997056022613363,
        "epoch": 0.3460159911772815,
        "step": 1255
    },
    {
        "loss": 2.1617,
        "grad_norm": 1.325856328010559,
        "learning_rate": 0.0001996851982596261,
        "epoch": 0.3462917011304108,
        "step": 1256
    },
    {
        "loss": 1.613,
        "grad_norm": 1.8844186067581177,
        "learning_rate": 0.00019966411178524188,
        "epoch": 0.3465674110835401,
        "step": 1257
    },
    {
        "loss": 2.6231,
        "grad_norm": 1.386649489402771,
        "learning_rate": 0.00019964234298254754,
        "epoch": 0.3468431210366694,
        "step": 1258
    },
    {
        "loss": 2.2542,
        "grad_norm": 1.7417097091674805,
        "learning_rate": 0.00019961989200057838,
        "epoch": 0.34711883098979873,
        "step": 1259
    },
    {
        "loss": 2.4513,
        "grad_norm": 1.6657905578613281,
        "learning_rate": 0.00019959675899304012,
        "epoch": 0.34739454094292804,
        "step": 1260
    },
    {
        "loss": 2.1105,
        "grad_norm": 1.9163390398025513,
        "learning_rate": 0.00019957294411830775,
        "epoch": 0.34767025089605735,
        "step": 1261
    },
    {
        "loss": 1.9982,
        "grad_norm": 1.9715509414672852,
        "learning_rate": 0.0001995484475394245,
        "epoch": 0.34794596084918666,
        "step": 1262
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.846083164215088,
        "learning_rate": 0.0001995232694241009,
        "epoch": 0.348221670802316,
        "step": 1263
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.330139398574829,
        "learning_rate": 0.00019949740994471322,
        "epoch": 0.3484973807554453,
        "step": 1264
    },
    {
        "loss": 2.0684,
        "grad_norm": 1.5601141452789307,
        "learning_rate": 0.00019947086927830275,
        "epoch": 0.3487730907085746,
        "step": 1265
    },
    {
        "loss": 2.2296,
        "grad_norm": 1.1433171033859253,
        "learning_rate": 0.0001994436476065743,
        "epoch": 0.3490488006617039,
        "step": 1266
    },
    {
        "loss": 1.4149,
        "grad_norm": 2.1331887245178223,
        "learning_rate": 0.00019941574511589497,
        "epoch": 0.3493245106148332,
        "step": 1267
    },
    {
        "loss": 2.0453,
        "grad_norm": 1.5427943468093872,
        "learning_rate": 0.00019938716199729303,
        "epoch": 0.3496002205679625,
        "step": 1268
    },
    {
        "loss": 2.2184,
        "grad_norm": 1.6412875652313232,
        "learning_rate": 0.0001993578984464565,
        "epoch": 0.34987593052109184,
        "step": 1269
    },
    {
        "loss": 1.1233,
        "grad_norm": 2.131411552429199,
        "learning_rate": 0.00019932795466373179,
        "epoch": 0.3501516404742211,
        "step": 1270
    },
    {
        "loss": 1.9968,
        "grad_norm": 1.2569032907485962,
        "learning_rate": 0.0001992973308541224,
        "epoch": 0.3504273504273504,
        "step": 1271
    },
    {
        "loss": 1.9521,
        "grad_norm": 1.6074429750442505,
        "learning_rate": 0.00019926602722728743,
        "epoch": 0.3507030603804797,
        "step": 1272
    },
    {
        "loss": 2.4915,
        "grad_norm": 1.191929578781128,
        "learning_rate": 0.0001992340439975403,
        "epoch": 0.350978770333609,
        "step": 1273
    },
    {
        "loss": 1.607,
        "grad_norm": 1.7292577028274536,
        "learning_rate": 0.0001992013813838471,
        "epoch": 0.35125448028673834,
        "step": 1274
    },
    {
        "loss": 1.4428,
        "grad_norm": 1.9600908756256104,
        "learning_rate": 0.0001991680396098252,
        "epoch": 0.35153019023986765,
        "step": 1275
    },
    {
        "loss": 1.3195,
        "grad_norm": 2.079904794692993,
        "learning_rate": 0.00019913401890374164,
        "epoch": 0.35180590019299696,
        "step": 1276
    },
    {
        "loss": 1.5712,
        "grad_norm": 1.8898046016693115,
        "learning_rate": 0.00019909931949851174,
        "epoch": 0.35208161014612627,
        "step": 1277
    },
    {
        "loss": 2.0689,
        "grad_norm": 1.5148557424545288,
        "learning_rate": 0.00019906394163169723,
        "epoch": 0.3523573200992556,
        "step": 1278
    },
    {
        "loss": 2.3338,
        "grad_norm": 1.3539859056472778,
        "learning_rate": 0.00019902788554550493,
        "epoch": 0.3526330300523849,
        "step": 1279
    },
    {
        "loss": 1.2581,
        "grad_norm": 2.259077548980713,
        "learning_rate": 0.00019899115148678482,
        "epoch": 0.3529087400055142,
        "step": 1280
    },
    {
        "loss": 2.5161,
        "grad_norm": 1.5021659135818481,
        "learning_rate": 0.00019895373970702852,
        "epoch": 0.3531844499586435,
        "step": 1281
    },
    {
        "loss": 2.5461,
        "grad_norm": 1.0311143398284912,
        "learning_rate": 0.00019891565046236754,
        "epoch": 0.3534601599117728,
        "step": 1282
    },
    {
        "loss": 1.7032,
        "grad_norm": 1.4929262399673462,
        "learning_rate": 0.0001988768840135714,
        "epoch": 0.35373586986490213,
        "step": 1283
    },
    {
        "loss": 2.71,
        "grad_norm": 1.1654279232025146,
        "learning_rate": 0.0001988374406260461,
        "epoch": 0.35401157981803144,
        "step": 1284
    },
    {
        "loss": 2.5285,
        "grad_norm": 1.354423999786377,
        "learning_rate": 0.000198797320569832,
        "epoch": 0.35428728977116075,
        "step": 1285
    },
    {
        "loss": 2.4986,
        "grad_norm": 0.8791297078132629,
        "learning_rate": 0.00019875652411960228,
        "epoch": 0.35456299972429006,
        "step": 1286
    },
    {
        "loss": 2.4501,
        "grad_norm": 1.4929898977279663,
        "learning_rate": 0.00019871505155466074,
        "epoch": 0.3548387096774194,
        "step": 1287
    },
    {
        "loss": 2.3698,
        "grad_norm": 1.1512117385864258,
        "learning_rate": 0.00019867290315894018,
        "epoch": 0.3551144196305487,
        "step": 1288
    },
    {
        "loss": 2.23,
        "grad_norm": 0.9473703503608704,
        "learning_rate": 0.00019863007922100024,
        "epoch": 0.355390129583678,
        "step": 1289
    },
    {
        "loss": 2.3077,
        "grad_norm": 1.9838799238204956,
        "learning_rate": 0.00019858658003402558,
        "epoch": 0.3556658395368073,
        "step": 1290
    },
    {
        "loss": 2.1272,
        "grad_norm": 1.6578853130340576,
        "learning_rate": 0.00019854240589582374,
        "epoch": 0.35594154948993656,
        "step": 1291
    },
    {
        "loss": 2.4915,
        "grad_norm": 1.2226377725601196,
        "learning_rate": 0.00019849755710882322,
        "epoch": 0.35621725944306587,
        "step": 1292
    },
    {
        "loss": 2.2381,
        "grad_norm": 1.698506474494934,
        "learning_rate": 0.00019845203398007134,
        "epoch": 0.3564929693961952,
        "step": 1293
    },
    {
        "loss": 1.5495,
        "grad_norm": 1.9767316579818726,
        "learning_rate": 0.00019840583682123204,
        "epoch": 0.3567686793493245,
        "step": 1294
    },
    {
        "loss": 2.1094,
        "grad_norm": 1.6166151762008667,
        "learning_rate": 0.00019835896594858405,
        "epoch": 0.3570443893024538,
        "step": 1295
    },
    {
        "loss": 1.9111,
        "grad_norm": 2.2468531131744385,
        "learning_rate": 0.00019831142168301842,
        "epoch": 0.3573200992555831,
        "step": 1296
    },
    {
        "loss": 1.9112,
        "grad_norm": 1.8061250448226929,
        "learning_rate": 0.0001982632043500364,
        "epoch": 0.3575958092087124,
        "step": 1297
    },
    {
        "loss": 1.5667,
        "grad_norm": 1.737283706665039,
        "learning_rate": 0.0001982143142797474,
        "epoch": 0.35787151916184173,
        "step": 1298
    },
    {
        "loss": 2.0897,
        "grad_norm": 2.4799249172210693,
        "learning_rate": 0.00019816475180686643,
        "epoch": 0.35814722911497104,
        "step": 1299
    },
    {
        "loss": 2.0332,
        "grad_norm": 1.9202574491500854,
        "learning_rate": 0.00019811451727071196,
        "epoch": 0.35842293906810035,
        "step": 1300
    },
    {
        "loss": 1.9962,
        "grad_norm": 2.0261807441711426,
        "learning_rate": 0.0001980636110152037,
        "epoch": 0.35869864902122967,
        "step": 1301
    },
    {
        "loss": 2.5592,
        "grad_norm": 1.432080864906311,
        "learning_rate": 0.00019801203338886002,
        "epoch": 0.358974358974359,
        "step": 1302
    },
    {
        "loss": 2.0107,
        "grad_norm": 2.2034194469451904,
        "learning_rate": 0.0001979597847447958,
        "epoch": 0.3592500689274883,
        "step": 1303
    },
    {
        "loss": 1.0957,
        "grad_norm": 2.6015570163726807,
        "learning_rate": 0.0001979068654407198,
        "epoch": 0.3595257788806176,
        "step": 1304
    },
    {
        "loss": 1.8984,
        "grad_norm": 2.127321720123291,
        "learning_rate": 0.00019785327583893235,
        "epoch": 0.3598014888337469,
        "step": 1305
    },
    {
        "loss": 2.2924,
        "grad_norm": 1.6652486324310303,
        "learning_rate": 0.00019779901630632277,
        "epoch": 0.3600771987868762,
        "step": 1306
    },
    {
        "loss": 1.5136,
        "grad_norm": 2.4738287925720215,
        "learning_rate": 0.000197744087214367,
        "epoch": 0.36035290874000553,
        "step": 1307
    },
    {
        "loss": 1.8599,
        "grad_norm": 2.4349186420440674,
        "learning_rate": 0.0001976884889391249,
        "epoch": 0.36062861869313484,
        "step": 1308
    },
    {
        "loss": 2.133,
        "grad_norm": 1.078498363494873,
        "learning_rate": 0.00019763222186123782,
        "epoch": 0.36090432864626415,
        "step": 1309
    },
    {
        "loss": 1.6922,
        "grad_norm": 1.7687734365463257,
        "learning_rate": 0.00019757528636592578,
        "epoch": 0.36118003859939346,
        "step": 1310
    },
    {
        "loss": 2.0824,
        "grad_norm": 1.6412370204925537,
        "learning_rate": 0.00019751768284298517,
        "epoch": 0.36145574855252277,
        "step": 1311
    },
    {
        "loss": 1.8377,
        "grad_norm": 2.290249824523926,
        "learning_rate": 0.0001974594116867857,
        "epoch": 0.3617314585056521,
        "step": 1312
    },
    {
        "loss": 1.7574,
        "grad_norm": 2.0749967098236084,
        "learning_rate": 0.00019740047329626798,
        "epoch": 0.36200716845878134,
        "step": 1313
    },
    {
        "loss": 2.4922,
        "grad_norm": 1.4281535148620605,
        "learning_rate": 0.00019734086807494068,
        "epoch": 0.36228287841191065,
        "step": 1314
    },
    {
        "loss": 2.353,
        "grad_norm": 1.430267333984375,
        "learning_rate": 0.00019728059643087778,
        "epoch": 0.36255858836503996,
        "step": 1315
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.6820086240768433,
        "learning_rate": 0.00019721965877671577,
        "epoch": 0.36283429831816927,
        "step": 1316
    },
    {
        "loss": 2.4729,
        "grad_norm": 1.210565209388733,
        "learning_rate": 0.00019715805552965077,
        "epoch": 0.3631100082712986,
        "step": 1317
    },
    {
        "loss": 2.3046,
        "grad_norm": 1.6801611185073853,
        "learning_rate": 0.00019709578711143584,
        "epoch": 0.3633857182244279,
        "step": 1318
    },
    {
        "loss": 1.6501,
        "grad_norm": 1.723061203956604,
        "learning_rate": 0.00019703285394837797,
        "epoch": 0.3636614281775572,
        "step": 1319
    },
    {
        "loss": 1.561,
        "grad_norm": 1.9372341632843018,
        "learning_rate": 0.00019696925647133507,
        "epoch": 0.3639371381306865,
        "step": 1320
    },
    {
        "loss": 2.1095,
        "grad_norm": 1.6532385349273682,
        "learning_rate": 0.0001969049951157133,
        "epoch": 0.3642128480838158,
        "step": 1321
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.5111123323440552,
        "learning_rate": 0.00019684007032146376,
        "epoch": 0.36448855803694513,
        "step": 1322
    },
    {
        "loss": 2.2856,
        "grad_norm": 1.9166464805603027,
        "learning_rate": 0.00019677448253307972,
        "epoch": 0.36476426799007444,
        "step": 1323
    },
    {
        "loss": 1.3446,
        "grad_norm": 2.0700244903564453,
        "learning_rate": 0.00019670823219959348,
        "epoch": 0.36503997794320375,
        "step": 1324
    },
    {
        "loss": 1.9981,
        "grad_norm": 2.6535942554473877,
        "learning_rate": 0.0001966413197745733,
        "epoch": 0.36531568789633306,
        "step": 1325
    },
    {
        "loss": 2.3335,
        "grad_norm": 1.3045828342437744,
        "learning_rate": 0.00019657374571612033,
        "epoch": 0.3655913978494624,
        "step": 1326
    },
    {
        "loss": 2.1583,
        "grad_norm": 1.6539793014526367,
        "learning_rate": 0.00019650551048686537,
        "epoch": 0.3658671078025917,
        "step": 1327
    },
    {
        "loss": 2.5533,
        "grad_norm": 1.0386384725570679,
        "learning_rate": 0.0001964366145539659,
        "epoch": 0.366142817755721,
        "step": 1328
    },
    {
        "loss": 1.6198,
        "grad_norm": 1.7282389402389526,
        "learning_rate": 0.00019636705838910263,
        "epoch": 0.3664185277088503,
        "step": 1329
    },
    {
        "loss": 1.3837,
        "grad_norm": 2.393484354019165,
        "learning_rate": 0.00019629684246847648,
        "epoch": 0.3666942376619796,
        "step": 1330
    },
    {
        "loss": 2.5906,
        "grad_norm": 1.780095100402832,
        "learning_rate": 0.0001962259672728053,
        "epoch": 0.3669699476151089,
        "step": 1331
    },
    {
        "loss": 2.1535,
        "grad_norm": 2.2303385734558105,
        "learning_rate": 0.00019615443328732036,
        "epoch": 0.36724565756823824,
        "step": 1332
    },
    {
        "loss": 1.2985,
        "grad_norm": 2.1013104915618896,
        "learning_rate": 0.00019608224100176332,
        "epoch": 0.36752136752136755,
        "step": 1333
    },
    {
        "loss": 1.9688,
        "grad_norm": 1.3684643507003784,
        "learning_rate": 0.00019600939091038265,
        "epoch": 0.3677970774744968,
        "step": 1334
    },
    {
        "loss": 2.2284,
        "grad_norm": 1.3728739023208618,
        "learning_rate": 0.0001959358835119305,
        "epoch": 0.3680727874276261,
        "step": 1335
    },
    {
        "loss": 1.762,
        "grad_norm": 1.7821035385131836,
        "learning_rate": 0.00019586171930965898,
        "epoch": 0.3683484973807554,
        "step": 1336
    },
    {
        "loss": 2.4114,
        "grad_norm": 1.7853542566299438,
        "learning_rate": 0.00019578689881131686,
        "epoch": 0.36862420733388473,
        "step": 1337
    },
    {
        "loss": 1.9715,
        "grad_norm": 1.6657737493515015,
        "learning_rate": 0.0001957114225291462,
        "epoch": 0.36889991728701405,
        "step": 1338
    },
    {
        "loss": 2.5253,
        "grad_norm": 1.9020497798919678,
        "learning_rate": 0.00019563529097987864,
        "epoch": 0.36917562724014336,
        "step": 1339
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.5077438354492188,
        "learning_rate": 0.00019555850468473205,
        "epoch": 0.36945133719327267,
        "step": 1340
    },
    {
        "loss": 2.1659,
        "grad_norm": 1.8401150703430176,
        "learning_rate": 0.00019548106416940678,
        "epoch": 0.369727047146402,
        "step": 1341
    },
    {
        "loss": 1.7614,
        "grad_norm": 2.345759630203247,
        "learning_rate": 0.0001954029699640822,
        "epoch": 0.3700027570995313,
        "step": 1342
    },
    {
        "loss": 2.0333,
        "grad_norm": 2.112562656402588,
        "learning_rate": 0.00019532422260341305,
        "epoch": 0.3702784670526606,
        "step": 1343
    },
    {
        "loss": 2.6266,
        "grad_norm": 1.7801636457443237,
        "learning_rate": 0.00019524482262652575,
        "epoch": 0.3705541770057899,
        "step": 1344
    },
    {
        "loss": 2.3649,
        "grad_norm": 1.491726279258728,
        "learning_rate": 0.00019516477057701464,
        "epoch": 0.3708298869589192,
        "step": 1345
    },
    {
        "loss": 2.4733,
        "grad_norm": 1.311031460762024,
        "learning_rate": 0.0001950840670029384,
        "epoch": 0.37110559691204853,
        "step": 1346
    },
    {
        "loss": 2.3683,
        "grad_norm": 1.5442936420440674,
        "learning_rate": 0.0001950027124568162,
        "epoch": 0.37138130686517784,
        "step": 1347
    },
    {
        "loss": 2.4737,
        "grad_norm": 1.0905903577804565,
        "learning_rate": 0.000194920707495624,
        "epoch": 0.37165701681830715,
        "step": 1348
    },
    {
        "loss": 2.3746,
        "grad_norm": 1.022727131843567,
        "learning_rate": 0.00019483805268079055,
        "epoch": 0.37193272677143646,
        "step": 1349
    },
    {
        "loss": 1.9882,
        "grad_norm": 1.7110151052474976,
        "learning_rate": 0.00019475474857819387,
        "epoch": 0.37220843672456577,
        "step": 1350
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.4934780597686768,
        "learning_rate": 0.00019467079575815702,
        "epoch": 0.3724841466776951,
        "step": 1351
    },
    {
        "loss": 2.449,
        "grad_norm": 1.1104185581207275,
        "learning_rate": 0.00019458619479544443,
        "epoch": 0.3727598566308244,
        "step": 1352
    },
    {
        "loss": 1.797,
        "grad_norm": 1.867255449295044,
        "learning_rate": 0.00019450094626925792,
        "epoch": 0.3730355665839537,
        "step": 1353
    },
    {
        "loss": 2.2484,
        "grad_norm": 1.388066053390503,
        "learning_rate": 0.00019441505076323265,
        "epoch": 0.373311276537083,
        "step": 1354
    },
    {
        "loss": 1.9269,
        "grad_norm": 1.4060235023498535,
        "learning_rate": 0.00019432850886543323,
        "epoch": 0.37358698649021227,
        "step": 1355
    },
    {
        "loss": 1.6525,
        "grad_norm": 1.9646238088607788,
        "learning_rate": 0.00019424132116834965,
        "epoch": 0.3738626964433416,
        "step": 1356
    },
    {
        "loss": 2.2233,
        "grad_norm": 1.5757780075073242,
        "learning_rate": 0.00019415348826889317,
        "epoch": 0.3741384063964709,
        "step": 1357
    },
    {
        "loss": 2.5568,
        "grad_norm": 1.2399649620056152,
        "learning_rate": 0.00019406501076839233,
        "epoch": 0.3744141163496002,
        "step": 1358
    },
    {
        "loss": 1.4795,
        "grad_norm": 2.937682867050171,
        "learning_rate": 0.0001939758892725888,
        "epoch": 0.3746898263027295,
        "step": 1359
    },
    {
        "loss": 1.4022,
        "grad_norm": 2.1106038093566895,
        "learning_rate": 0.00019388612439163314,
        "epoch": 0.3749655362558588,
        "step": 1360
    },
    {
        "loss": 2.5344,
        "grad_norm": 1.855394959449768,
        "learning_rate": 0.00019379571674008083,
        "epoch": 0.37524124620898813,
        "step": 1361
    },
    {
        "loss": 2.453,
        "grad_norm": 1.7434170246124268,
        "learning_rate": 0.00019370466693688785,
        "epoch": 0.37551695616211744,
        "step": 1362
    },
    {
        "loss": 2.7208,
        "grad_norm": 1.3085416555404663,
        "learning_rate": 0.00019361297560540654,
        "epoch": 0.37579266611524675,
        "step": 1363
    },
    {
        "loss": 2.0508,
        "grad_norm": 1.5564196109771729,
        "learning_rate": 0.00019352064337338135,
        "epoch": 0.37606837606837606,
        "step": 1364
    },
    {
        "loss": 2.3754,
        "grad_norm": 1.2163193225860596,
        "learning_rate": 0.00019342767087294452,
        "epoch": 0.3763440860215054,
        "step": 1365
    },
    {
        "loss": 2.8107,
        "grad_norm": 1.4254010915756226,
        "learning_rate": 0.00019333405874061163,
        "epoch": 0.3766197959746347,
        "step": 1366
    },
    {
        "loss": 1.6631,
        "grad_norm": 2.068439483642578,
        "learning_rate": 0.00019323980761727752,
        "epoch": 0.376895505927764,
        "step": 1367
    },
    {
        "loss": 2.2958,
        "grad_norm": 1.7010374069213867,
        "learning_rate": 0.00019314491814821172,
        "epoch": 0.3771712158808933,
        "step": 1368
    },
    {
        "loss": 2.0531,
        "grad_norm": 1.7578017711639404,
        "learning_rate": 0.0001930493909830539,
        "epoch": 0.3774469258340226,
        "step": 1369
    },
    {
        "loss": 2.2245,
        "grad_norm": 0.8650469183921814,
        "learning_rate": 0.0001929532267758097,
        "epoch": 0.37772263578715193,
        "step": 1370
    },
    {
        "loss": 2.0009,
        "grad_norm": 2.4461545944213867,
        "learning_rate": 0.0001928564261848462,
        "epoch": 0.37799834574028124,
        "step": 1371
    },
    {
        "loss": 2.0019,
        "grad_norm": 2.321887969970703,
        "learning_rate": 0.00019275898987288712,
        "epoch": 0.37827405569341055,
        "step": 1372
    },
    {
        "loss": 2.1234,
        "grad_norm": 2.0721237659454346,
        "learning_rate": 0.00019266091850700875,
        "epoch": 0.37854976564653986,
        "step": 1373
    },
    {
        "loss": 2.2358,
        "grad_norm": 2.0776240825653076,
        "learning_rate": 0.0001925622127586349,
        "epoch": 0.37882547559966917,
        "step": 1374
    },
    {
        "loss": 1.5189,
        "grad_norm": 1.888770580291748,
        "learning_rate": 0.00019246287330353275,
        "epoch": 0.3791011855527985,
        "step": 1375
    },
    {
        "loss": 2.381,
        "grad_norm": 1.2892853021621704,
        "learning_rate": 0.00019236290082180787,
        "epoch": 0.3793768955059278,
        "step": 1376
    },
    {
        "loss": 2.4539,
        "grad_norm": 1.2226067781448364,
        "learning_rate": 0.00019226229599789978,
        "epoch": 0.37965260545905705,
        "step": 1377
    },
    {
        "loss": 1.4219,
        "grad_norm": 2.301090955734253,
        "learning_rate": 0.00019216105952057715,
        "epoch": 0.37992831541218636,
        "step": 1378
    },
    {
        "loss": 2.2633,
        "grad_norm": 1.4728814363479614,
        "learning_rate": 0.00019205919208293315,
        "epoch": 0.38020402536531567,
        "step": 1379
    },
    {
        "loss": 2.1159,
        "grad_norm": 1.8886992931365967,
        "learning_rate": 0.00019195669438238069,
        "epoch": 0.380479735318445,
        "step": 1380
    },
    {
        "loss": 2.0303,
        "grad_norm": 1.3706958293914795,
        "learning_rate": 0.0001918535671206476,
        "epoch": 0.3807554452715743,
        "step": 1381
    },
    {
        "loss": 2.4847,
        "grad_norm": 1.6995749473571777,
        "learning_rate": 0.0001917498110037719,
        "epoch": 0.3810311552247036,
        "step": 1382
    },
    {
        "loss": 2.0369,
        "grad_norm": 1.8195412158966064,
        "learning_rate": 0.0001916454267420969,
        "epoch": 0.3813068651778329,
        "step": 1383
    },
    {
        "loss": 1.0379,
        "grad_norm": 2.0533342361450195,
        "learning_rate": 0.00019154041505026634,
        "epoch": 0.3815825751309622,
        "step": 1384
    },
    {
        "loss": 2.5382,
        "grad_norm": 1.070371150970459,
        "learning_rate": 0.0001914347766472196,
        "epoch": 0.38185828508409153,
        "step": 1385
    },
    {
        "loss": 1.5037,
        "grad_norm": 1.9416027069091797,
        "learning_rate": 0.00019132851225618666,
        "epoch": 0.38213399503722084,
        "step": 1386
    },
    {
        "loss": 2.0888,
        "grad_norm": 1.2674672603607178,
        "learning_rate": 0.0001912216226046831,
        "epoch": 0.38240970499035015,
        "step": 1387
    },
    {
        "loss": 2.3487,
        "grad_norm": 1.8014357089996338,
        "learning_rate": 0.0001911141084245054,
        "epoch": 0.38268541494347946,
        "step": 1388
    },
    {
        "loss": 2.5488,
        "grad_norm": 1.0868769884109497,
        "learning_rate": 0.00019100597045172558,
        "epoch": 0.3829611248966088,
        "step": 1389
    },
    {
        "loss": 2.0306,
        "grad_norm": 1.5659793615341187,
        "learning_rate": 0.00019089720942668635,
        "epoch": 0.3832368348497381,
        "step": 1390
    },
    {
        "loss": 1.8543,
        "grad_norm": 1.7973685264587402,
        "learning_rate": 0.000190787826093996,
        "epoch": 0.3835125448028674,
        "step": 1391
    },
    {
        "loss": 2.1588,
        "grad_norm": 1.6049017906188965,
        "learning_rate": 0.00019067782120252345,
        "epoch": 0.3837882547559967,
        "step": 1392
    },
    {
        "loss": 2.442,
        "grad_norm": 1.589530348777771,
        "learning_rate": 0.00019056719550539277,
        "epoch": 0.384063964709126,
        "step": 1393
    },
    {
        "loss": 2.0963,
        "grad_norm": 1.7757338285446167,
        "learning_rate": 0.00019045594975997836,
        "epoch": 0.3843396746622553,
        "step": 1394
    },
    {
        "loss": 2.0367,
        "grad_norm": 1.5672415494918823,
        "learning_rate": 0.00019034408472789967,
        "epoch": 0.38461538461538464,
        "step": 1395
    },
    {
        "loss": 2.408,
        "grad_norm": 1.5668630599975586,
        "learning_rate": 0.00019023160117501587,
        "epoch": 0.38489109456851395,
        "step": 1396
    },
    {
        "loss": 0.8356,
        "grad_norm": 1.7714416980743408,
        "learning_rate": 0.00019011849987142073,
        "epoch": 0.38516680452164326,
        "step": 1397
    },
    {
        "loss": 2.2649,
        "grad_norm": 1.6410578489303589,
        "learning_rate": 0.00019000478159143734,
        "epoch": 0.3854425144747725,
        "step": 1398
    },
    {
        "loss": 2.2709,
        "grad_norm": 1.6831305027008057,
        "learning_rate": 0.0001898904471136128,
        "epoch": 0.3857182244279018,
        "step": 1399
    },
    {
        "loss": 1.7912,
        "grad_norm": 1.9257556200027466,
        "learning_rate": 0.00018977549722071278,
        "epoch": 0.38599393438103113,
        "step": 1400
    },
    {
        "loss": 1.7941,
        "grad_norm": 2.227233409881592,
        "learning_rate": 0.00018965993269971632,
        "epoch": 0.38626964433416044,
        "step": 1401
    },
    {
        "loss": 2.4351,
        "grad_norm": 1.2516928911209106,
        "learning_rate": 0.00018954375434181045,
        "epoch": 0.38654535428728976,
        "step": 1402
    },
    {
        "loss": 2.1836,
        "grad_norm": 2.134995937347412,
        "learning_rate": 0.00018942696294238452,
        "epoch": 0.38682106424041907,
        "step": 1403
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.1475924253463745,
        "learning_rate": 0.00018930955930102506,
        "epoch": 0.3870967741935484,
        "step": 1404
    },
    {
        "loss": 1.4525,
        "grad_norm": 2.0266289710998535,
        "learning_rate": 0.00018919154422151015,
        "epoch": 0.3873724841466777,
        "step": 1405
    },
    {
        "loss": 2.2153,
        "grad_norm": 1.426112174987793,
        "learning_rate": 0.00018907291851180392,
        "epoch": 0.387648194099807,
        "step": 1406
    },
    {
        "loss": 1.9503,
        "grad_norm": 1.9301588535308838,
        "learning_rate": 0.00018895368298405113,
        "epoch": 0.3879239040529363,
        "step": 1407
    },
    {
        "loss": 1.57,
        "grad_norm": 1.540409803390503,
        "learning_rate": 0.0001888338384545714,
        "epoch": 0.3881996140060656,
        "step": 1408
    },
    {
        "loss": 2.4069,
        "grad_norm": 1.1675437688827515,
        "learning_rate": 0.00018871338574385385,
        "epoch": 0.38847532395919493,
        "step": 1409
    },
    {
        "loss": 1.2107,
        "grad_norm": 2.510453224182129,
        "learning_rate": 0.0001885923256765513,
        "epoch": 0.38875103391232424,
        "step": 1410
    },
    {
        "loss": 2.2511,
        "grad_norm": 0.9828686118125916,
        "learning_rate": 0.0001884706590814748,
        "epoch": 0.38902674386545355,
        "step": 1411
    },
    {
        "loss": 1.4408,
        "grad_norm": 1.312264323234558,
        "learning_rate": 0.0001883483867915877,
        "epoch": 0.38930245381858286,
        "step": 1412
    },
    {
        "loss": 1.8437,
        "grad_norm": 1.824172854423523,
        "learning_rate": 0.0001882255096440003,
        "epoch": 0.38957816377171217,
        "step": 1413
    },
    {
        "loss": 2.0912,
        "grad_norm": 2.0138871669769287,
        "learning_rate": 0.0001881020284799638,
        "epoch": 0.3898538737248415,
        "step": 1414
    },
    {
        "loss": 2.131,
        "grad_norm": 1.4229466915130615,
        "learning_rate": 0.00018797794414486467,
        "epoch": 0.3901295836779708,
        "step": 1415
    },
    {
        "loss": 1.87,
        "grad_norm": 2.137314796447754,
        "learning_rate": 0.0001878532574882189,
        "epoch": 0.3904052936311001,
        "step": 1416
    },
    {
        "loss": 1.5034,
        "grad_norm": 2.1966845989227295,
        "learning_rate": 0.0001877279693636661,
        "epoch": 0.3906810035842294,
        "step": 1417
    },
    {
        "loss": 2.4214,
        "grad_norm": 1.494225263595581,
        "learning_rate": 0.00018760208062896375,
        "epoch": 0.3909567135373587,
        "step": 1418
    },
    {
        "loss": 2.0759,
        "grad_norm": 1.6215273141860962,
        "learning_rate": 0.00018747559214598122,
        "epoch": 0.391232423490488,
        "step": 1419
    },
    {
        "loss": 1.7069,
        "grad_norm": 2.0006256103515625,
        "learning_rate": 0.000187348504780694,
        "epoch": 0.3915081334436173,
        "step": 1420
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.2395832538604736,
        "learning_rate": 0.00018722081940317754,
        "epoch": 0.3917838433967466,
        "step": 1421
    },
    {
        "loss": 2.3335,
        "grad_norm": 1.156277060508728,
        "learning_rate": 0.00018709253688760165,
        "epoch": 0.3920595533498759,
        "step": 1422
    },
    {
        "loss": 2.44,
        "grad_norm": 1.5499625205993652,
        "learning_rate": 0.00018696365811222417,
        "epoch": 0.3923352633030052,
        "step": 1423
    },
    {
        "loss": 2.3906,
        "grad_norm": 1.087444543838501,
        "learning_rate": 0.00018683418395938517,
        "epoch": 0.39261097325613453,
        "step": 1424
    },
    {
        "loss": 2.3624,
        "grad_norm": 1.587770700454712,
        "learning_rate": 0.0001867041153155008,
        "epoch": 0.39288668320926384,
        "step": 1425
    },
    {
        "loss": 1.9006,
        "grad_norm": 2.023252248764038,
        "learning_rate": 0.00018657345307105728,
        "epoch": 0.39316239316239315,
        "step": 1426
    },
    {
        "loss": 2.353,
        "grad_norm": 1.2818262577056885,
        "learning_rate": 0.00018644219812060476,
        "epoch": 0.39343810311552246,
        "step": 1427
    },
    {
        "loss": 1.9571,
        "grad_norm": 1.9891541004180908,
        "learning_rate": 0.00018631035136275128,
        "epoch": 0.3937138130686518,
        "step": 1428
    },
    {
        "loss": 2.2401,
        "grad_norm": 1.2348949909210205,
        "learning_rate": 0.0001861779137001565,
        "epoch": 0.3939895230217811,
        "step": 1429
    },
    {
        "loss": 2.2781,
        "grad_norm": 1.338029384613037,
        "learning_rate": 0.00018604488603952564,
        "epoch": 0.3942652329749104,
        "step": 1430
    },
    {
        "loss": 2.1635,
        "grad_norm": 1.1903951168060303,
        "learning_rate": 0.0001859112692916031,
        "epoch": 0.3945409429280397,
        "step": 1431
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.645240068435669,
        "learning_rate": 0.0001857770643711665,
        "epoch": 0.394816652881169,
        "step": 1432
    },
    {
        "loss": 2.1782,
        "grad_norm": 1.8584628105163574,
        "learning_rate": 0.00018564227219702005,
        "epoch": 0.3950923628342983,
        "step": 1433
    },
    {
        "loss": 2.3491,
        "grad_norm": 1.1703131198883057,
        "learning_rate": 0.00018550689369198868,
        "epoch": 0.39536807278742764,
        "step": 1434
    },
    {
        "loss": 2.2242,
        "grad_norm": 1.237404704093933,
        "learning_rate": 0.00018537092978291133,
        "epoch": 0.39564378274055695,
        "step": 1435
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.9129875898361206,
        "learning_rate": 0.0001852343814006349,
        "epoch": 0.39591949269368626,
        "step": 1436
    },
    {
        "loss": 1.7079,
        "grad_norm": 1.9123514890670776,
        "learning_rate": 0.00018509724948000765,
        "epoch": 0.39619520264681557,
        "step": 1437
    },
    {
        "loss": 2.0734,
        "grad_norm": 2.1911418437957764,
        "learning_rate": 0.000184959534959873,
        "epoch": 0.3964709125999449,
        "step": 1438
    },
    {
        "loss": 1.1788,
        "grad_norm": 2.255202531814575,
        "learning_rate": 0.00018482123878306295,
        "epoch": 0.3967466225530742,
        "step": 1439
    },
    {
        "loss": 1.7227,
        "grad_norm": 1.4226398468017578,
        "learning_rate": 0.0001846823618963917,
        "epoch": 0.3970223325062035,
        "step": 1440
    },
    {
        "loss": 1.4935,
        "grad_norm": 1.6118264198303223,
        "learning_rate": 0.00018454290525064918,
        "epoch": 0.39729804245933276,
        "step": 1441
    },
    {
        "loss": 2.2252,
        "grad_norm": 1.5674524307250977,
        "learning_rate": 0.00018440286980059447,
        "epoch": 0.39757375241246207,
        "step": 1442
    },
    {
        "loss": 2.3636,
        "grad_norm": 1.3682594299316406,
        "learning_rate": 0.00018426225650494934,
        "epoch": 0.3978494623655914,
        "step": 1443
    },
    {
        "loss": 2.5035,
        "grad_norm": 0.8583685159683228,
        "learning_rate": 0.0001841210663263916,
        "epoch": 0.3981251723187207,
        "step": 1444
    },
    {
        "loss": 1.0621,
        "grad_norm": 2.615896463394165,
        "learning_rate": 0.00018397930023154863,
        "epoch": 0.39840088227185,
        "step": 1445
    },
    {
        "loss": 2.498,
        "grad_norm": 1.3330999612808228,
        "learning_rate": 0.00018383695919099065,
        "epoch": 0.3986765922249793,
        "step": 1446
    },
    {
        "loss": 0.9191,
        "grad_norm": 1.9790232181549072,
        "learning_rate": 0.00018369404417922414,
        "epoch": 0.3989523021781086,
        "step": 1447
    },
    {
        "loss": 2.2909,
        "grad_norm": 1.0206271409988403,
        "learning_rate": 0.00018355055617468507,
        "epoch": 0.39922801213123793,
        "step": 1448
    },
    {
        "loss": 2.0146,
        "grad_norm": 1.4228805303573608,
        "learning_rate": 0.00018340649615973247,
        "epoch": 0.39950372208436724,
        "step": 1449
    },
    {
        "loss": 2.3461,
        "grad_norm": 1.2735519409179688,
        "learning_rate": 0.00018326186512064127,
        "epoch": 0.39977943203749655,
        "step": 1450
    },
    {
        "loss": 1.9581,
        "grad_norm": 1.9123183488845825,
        "learning_rate": 0.00018311666404759595,
        "epoch": 0.40005514199062586,
        "step": 1451
    },
    {
        "loss": 2.8073,
        "grad_norm": 1.487230896949768,
        "learning_rate": 0.00018297089393468362,
        "epoch": 0.4003308519437552,
        "step": 1452
    },
    {
        "loss": 2.3289,
        "grad_norm": 1.6891613006591797,
        "learning_rate": 0.000182824555779887,
        "epoch": 0.4006065618968845,
        "step": 1453
    },
    {
        "loss": 2.0729,
        "grad_norm": 1.329465627670288,
        "learning_rate": 0.00018267765058507804,
        "epoch": 0.4008822718500138,
        "step": 1454
    },
    {
        "loss": 2.5189,
        "grad_norm": 1.6101452112197876,
        "learning_rate": 0.00018253017935601062,
        "epoch": 0.4011579818031431,
        "step": 1455
    },
    {
        "loss": 1.491,
        "grad_norm": 2.2690963745117188,
        "learning_rate": 0.00018238214310231394,
        "epoch": 0.4014336917562724,
        "step": 1456
    },
    {
        "loss": 2.3567,
        "grad_norm": 1.3149919509887695,
        "learning_rate": 0.00018223354283748545,
        "epoch": 0.4017094017094017,
        "step": 1457
    },
    {
        "loss": 1.8815,
        "grad_norm": 2.3842732906341553,
        "learning_rate": 0.00018208437957888406,
        "epoch": 0.40198511166253104,
        "step": 1458
    },
    {
        "loss": 1.7512,
        "grad_norm": 2.074086904525757,
        "learning_rate": 0.00018193465434772303,
        "epoch": 0.40226082161566035,
        "step": 1459
    },
    {
        "loss": 1.7124,
        "grad_norm": 1.1882556676864624,
        "learning_rate": 0.00018178436816906305,
        "epoch": 0.40253653156878966,
        "step": 1460
    },
    {
        "loss": 1.3574,
        "grad_norm": 1.9252077341079712,
        "learning_rate": 0.00018163352207180525,
        "epoch": 0.40281224152191897,
        "step": 1461
    },
    {
        "loss": 1.4031,
        "grad_norm": 1.7594938278198242,
        "learning_rate": 0.00018148211708868403,
        "epoch": 0.4030879514750482,
        "step": 1462
    },
    {
        "loss": 2.1275,
        "grad_norm": 1.630272626876831,
        "learning_rate": 0.00018133015425626016,
        "epoch": 0.40336366142817753,
        "step": 1463
    },
    {
        "loss": 2.3229,
        "grad_norm": 1.7081390619277954,
        "learning_rate": 0.00018117763461491363,
        "epoch": 0.40363937138130684,
        "step": 1464
    },
    {
        "loss": 2.0772,
        "grad_norm": 1.6031073331832886,
        "learning_rate": 0.0001810245592088364,
        "epoch": 0.40391508133443615,
        "step": 1465
    },
    {
        "loss": 2.8026,
        "grad_norm": 1.4783376455307007,
        "learning_rate": 0.00018087092908602542,
        "epoch": 0.40419079128756547,
        "step": 1466
    },
    {
        "loss": 2.6938,
        "grad_norm": 1.732438087463379,
        "learning_rate": 0.00018071674529827533,
        "epoch": 0.4044665012406948,
        "step": 1467
    },
    {
        "loss": 2.2083,
        "grad_norm": 1.801927924156189,
        "learning_rate": 0.00018056200890117138,
        "epoch": 0.4047422111938241,
        "step": 1468
    },
    {
        "loss": 1.9576,
        "grad_norm": 1.4428733587265015,
        "learning_rate": 0.0001804067209540821,
        "epoch": 0.4050179211469534,
        "step": 1469
    },
    {
        "loss": 2.329,
        "grad_norm": 1.4853219985961914,
        "learning_rate": 0.0001802508825201521,
        "epoch": 0.4052936311000827,
        "step": 1470
    },
    {
        "loss": 2.2375,
        "grad_norm": 1.2008733749389648,
        "learning_rate": 0.0001800944946662948,
        "epoch": 0.405569341053212,
        "step": 1471
    },
    {
        "loss": 1.8137,
        "grad_norm": 1.8599872589111328,
        "learning_rate": 0.000179937558463185,
        "epoch": 0.40584505100634133,
        "step": 1472
    },
    {
        "loss": 2.2844,
        "grad_norm": 0.9841075539588928,
        "learning_rate": 0.00017978007498525175,
        "epoch": 0.40612076095947064,
        "step": 1473
    },
    {
        "loss": 1.5878,
        "grad_norm": 1.8594770431518555,
        "learning_rate": 0.0001796220453106709,
        "epoch": 0.40639647091259995,
        "step": 1474
    },
    {
        "loss": 2.4078,
        "grad_norm": 2.173039436340332,
        "learning_rate": 0.00017946347052135767,
        "epoch": 0.40667218086572926,
        "step": 1475
    },
    {
        "loss": 1.8665,
        "grad_norm": 1.8049781322479248,
        "learning_rate": 0.0001793043517029593,
        "epoch": 0.40694789081885857,
        "step": 1476
    },
    {
        "loss": 2.3579,
        "grad_norm": 2.137218475341797,
        "learning_rate": 0.0001791446899448476,
        "epoch": 0.4072236007719879,
        "step": 1477
    },
    {
        "loss": 2.0695,
        "grad_norm": 1.639009952545166,
        "learning_rate": 0.00017898448634011147,
        "epoch": 0.4074993107251172,
        "step": 1478
    },
    {
        "loss": 1.2727,
        "grad_norm": 2.038013219833374,
        "learning_rate": 0.00017882374198554952,
        "epoch": 0.4077750206782465,
        "step": 1479
    },
    {
        "loss": 2.3953,
        "grad_norm": 0.9953293800354004,
        "learning_rate": 0.00017866245798166245,
        "epoch": 0.4080507306313758,
        "step": 1480
    },
    {
        "loss": 1.8216,
        "grad_norm": 2.1401681900024414,
        "learning_rate": 0.0001785006354326455,
        "epoch": 0.4083264405845051,
        "step": 1481
    },
    {
        "loss": 2.2453,
        "grad_norm": 1.3075215816497803,
        "learning_rate": 0.000178338275446381,
        "epoch": 0.40860215053763443,
        "step": 1482
    },
    {
        "loss": 2.2476,
        "grad_norm": 1.173500895500183,
        "learning_rate": 0.00017817537913443073,
        "epoch": 0.4088778604907637,
        "step": 1483
    },
    {
        "loss": 0.8313,
        "grad_norm": 2.020745038986206,
        "learning_rate": 0.0001780119476120282,
        "epoch": 0.409153570443893,
        "step": 1484
    },
    {
        "loss": 1.7581,
        "grad_norm": 1.9013993740081787,
        "learning_rate": 0.0001778479819980713,
        "epoch": 0.4094292803970223,
        "step": 1485
    },
    {
        "loss": 1.5808,
        "grad_norm": 2.2188146114349365,
        "learning_rate": 0.0001776834834151143,
        "epoch": 0.4097049903501516,
        "step": 1486
    },
    {
        "loss": 1.9204,
        "grad_norm": 1.855876088142395,
        "learning_rate": 0.0001775184529893604,
        "epoch": 0.40998070030328093,
        "step": 1487
    },
    {
        "loss": 2.1811,
        "grad_norm": 2.0453035831451416,
        "learning_rate": 0.00017735289185065396,
        "epoch": 0.41025641025641024,
        "step": 1488
    },
    {
        "loss": 1.4558,
        "grad_norm": 1.07882559299469,
        "learning_rate": 0.00017718680113247264,
        "epoch": 0.41053212020953955,
        "step": 1489
    },
    {
        "loss": 2.1268,
        "grad_norm": 1.8300708532333374,
        "learning_rate": 0.0001770201819719199,
        "epoch": 0.41080783016266886,
        "step": 1490
    },
    {
        "loss": 1.5314,
        "grad_norm": 2.515645980834961,
        "learning_rate": 0.00017685303550971698,
        "epoch": 0.4110835401157982,
        "step": 1491
    },
    {
        "loss": 1.39,
        "grad_norm": 2.1757709980010986,
        "learning_rate": 0.00017668536289019518,
        "epoch": 0.4113592500689275,
        "step": 1492
    },
    {
        "loss": 1.8904,
        "grad_norm": 1.9463175535202026,
        "learning_rate": 0.00017651716526128798,
        "epoch": 0.4116349600220568,
        "step": 1493
    },
    {
        "loss": 1.307,
        "grad_norm": 2.278367042541504,
        "learning_rate": 0.00017634844377452334,
        "epoch": 0.4119106699751861,
        "step": 1494
    },
    {
        "loss": 1.465,
        "grad_norm": 1.9377778768539429,
        "learning_rate": 0.00017617919958501556,
        "epoch": 0.4121863799283154,
        "step": 1495
    },
    {
        "loss": 1.7656,
        "grad_norm": 1.7228271961212158,
        "learning_rate": 0.0001760094338514576,
        "epoch": 0.4124620898814447,
        "step": 1496
    },
    {
        "loss": 2.4882,
        "grad_norm": 1.3914647102355957,
        "learning_rate": 0.00017583914773611304,
        "epoch": 0.41273779983457404,
        "step": 1497
    },
    {
        "loss": 2.2892,
        "grad_norm": 1.74522864818573,
        "learning_rate": 0.0001756683424048081,
        "epoch": 0.41301350978770335,
        "step": 1498
    },
    {
        "loss": 2.3307,
        "grad_norm": 1.0490492582321167,
        "learning_rate": 0.00017549701902692372,
        "epoch": 0.41328921974083266,
        "step": 1499
    },
    {
        "loss": 1.86,
        "grad_norm": 2.1972949504852295,
        "learning_rate": 0.00017532517877538758,
        "epoch": 0.41356492969396197,
        "step": 1500
    },
    {
        "loss": 2.3396,
        "grad_norm": 1.450692892074585,
        "learning_rate": 0.0001751528228266659,
        "epoch": 0.4138406396470913,
        "step": 1501
    },
    {
        "loss": 1.2965,
        "grad_norm": 2.4019453525543213,
        "learning_rate": 0.00017497995236075568,
        "epoch": 0.4141163496002206,
        "step": 1502
    },
    {
        "loss": 2.108,
        "grad_norm": 1.9746949672698975,
        "learning_rate": 0.00017480656856117625,
        "epoch": 0.4143920595533499,
        "step": 1503
    },
    {
        "loss": 1.7718,
        "grad_norm": 2.0378737449645996,
        "learning_rate": 0.00017463267261496153,
        "epoch": 0.4146677695064792,
        "step": 1504
    },
    {
        "loss": 1.6768,
        "grad_norm": 1.809615135192871,
        "learning_rate": 0.00017445826571265167,
        "epoch": 0.41494347945960847,
        "step": 1505
    },
    {
        "loss": 2.2958,
        "grad_norm": 1.8248119354248047,
        "learning_rate": 0.00017428334904828497,
        "epoch": 0.4152191894127378,
        "step": 1506
    },
    {
        "loss": 1.4097,
        "grad_norm": 1.9852575063705444,
        "learning_rate": 0.00017410792381938967,
        "epoch": 0.4154948993658671,
        "step": 1507
    },
    {
        "loss": 2.0453,
        "grad_norm": 1.8583729267120361,
        "learning_rate": 0.00017393199122697585,
        "epoch": 0.4157706093189964,
        "step": 1508
    },
    {
        "loss": 2.3055,
        "grad_norm": 1.9549590349197388,
        "learning_rate": 0.00017375555247552712,
        "epoch": 0.4160463192721257,
        "step": 1509
    },
    {
        "loss": 2.1695,
        "grad_norm": 1.911712646484375,
        "learning_rate": 0.00017357860877299238,
        "epoch": 0.416322029225255,
        "step": 1510
    },
    {
        "loss": 2.1629,
        "grad_norm": 1.516137719154358,
        "learning_rate": 0.0001734011613307776,
        "epoch": 0.41659773917838433,
        "step": 1511
    },
    {
        "loss": 2.342,
        "grad_norm": 1.4549367427825928,
        "learning_rate": 0.00017322321136373743,
        "epoch": 0.41687344913151364,
        "step": 1512
    },
    {
        "loss": 2.4994,
        "grad_norm": 1.124861478805542,
        "learning_rate": 0.000173044760090167,
        "epoch": 0.41714915908464295,
        "step": 1513
    },
    {
        "loss": 2.1316,
        "grad_norm": 1.101503610610962,
        "learning_rate": 0.0001728658087317935,
        "epoch": 0.41742486903777226,
        "step": 1514
    },
    {
        "loss": 2.1696,
        "grad_norm": 1.7043025493621826,
        "learning_rate": 0.00017268635851376785,
        "epoch": 0.41770057899090157,
        "step": 1515
    },
    {
        "loss": 2.3438,
        "grad_norm": 1.9858819246292114,
        "learning_rate": 0.00017250641066465628,
        "epoch": 0.4179762889440309,
        "step": 1516
    },
    {
        "loss": 2.0422,
        "grad_norm": 2.3087916374206543,
        "learning_rate": 0.00017232596641643202,
        "epoch": 0.4182519988971602,
        "step": 1517
    },
    {
        "loss": 2.6118,
        "grad_norm": 1.3406858444213867,
        "learning_rate": 0.00017214502700446668,
        "epoch": 0.4185277088502895,
        "step": 1518
    },
    {
        "loss": 2.3507,
        "grad_norm": 1.3980790376663208,
        "learning_rate": 0.00017196359366752195,
        "epoch": 0.4188034188034188,
        "step": 1519
    },
    {
        "loss": 1.9932,
        "grad_norm": 2.040679931640625,
        "learning_rate": 0.00017178166764774108,
        "epoch": 0.4190791287565481,
        "step": 1520
    },
    {
        "loss": 2.1319,
        "grad_norm": 1.1699715852737427,
        "learning_rate": 0.00017159925019064032,
        "epoch": 0.41935483870967744,
        "step": 1521
    },
    {
        "loss": 2.0762,
        "grad_norm": 1.3967734575271606,
        "learning_rate": 0.0001714163425451005,
        "epoch": 0.41963054866280675,
        "step": 1522
    },
    {
        "loss": 1.8916,
        "grad_norm": 2.5432701110839844,
        "learning_rate": 0.00017123294596335838,
        "epoch": 0.41990625861593606,
        "step": 1523
    },
    {
        "loss": 1.5985,
        "grad_norm": 1.8918484449386597,
        "learning_rate": 0.00017104906170099807,
        "epoch": 0.42018196856906537,
        "step": 1524
    },
    {
        "loss": 1.9351,
        "grad_norm": 1.629756212234497,
        "learning_rate": 0.00017086469101694257,
        "epoch": 0.4204576785221947,
        "step": 1525
    },
    {
        "loss": 1.7529,
        "grad_norm": 1.6101446151733398,
        "learning_rate": 0.000170679835173445,
        "epoch": 0.42073338847532393,
        "step": 1526
    },
    {
        "loss": 2.2793,
        "grad_norm": 1.4816362857818604,
        "learning_rate": 0.00017049449543608003,
        "epoch": 0.42100909842845324,
        "step": 1527
    },
    {
        "loss": 2.3979,
        "grad_norm": 1.381973147392273,
        "learning_rate": 0.00017030867307373518,
        "epoch": 0.42128480838158255,
        "step": 1528
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.8025422096252441,
        "learning_rate": 0.00017012236935860225,
        "epoch": 0.42156051833471186,
        "step": 1529
    },
    {
        "loss": 1.5981,
        "grad_norm": 1.906508207321167,
        "learning_rate": 0.00016993558556616835,
        "epoch": 0.4218362282878412,
        "step": 1530
    },
    {
        "loss": 1.804,
        "grad_norm": 1.404626488685608,
        "learning_rate": 0.00016974832297520748,
        "epoch": 0.4221119382409705,
        "step": 1531
    },
    {
        "loss": 1.2481,
        "grad_norm": 1.8835452795028687,
        "learning_rate": 0.00016956058286777154,
        "epoch": 0.4223876481940998,
        "step": 1532
    },
    {
        "loss": 2.1785,
        "grad_norm": 1.9096599817276,
        "learning_rate": 0.00016937236652918172,
        "epoch": 0.4226633581472291,
        "step": 1533
    },
    {
        "loss": 2.1724,
        "grad_norm": 1.7394866943359375,
        "learning_rate": 0.00016918367524801955,
        "epoch": 0.4229390681003584,
        "step": 1534
    },
    {
        "loss": 2.2569,
        "grad_norm": 1.99227774143219,
        "learning_rate": 0.00016899451031611811,
        "epoch": 0.42321477805348773,
        "step": 1535
    },
    {
        "loss": 2.4106,
        "grad_norm": 2.7689712047576904,
        "learning_rate": 0.00016880487302855347,
        "epoch": 0.42349048800661704,
        "step": 1536
    },
    {
        "loss": 2.303,
        "grad_norm": 1.0959258079528809,
        "learning_rate": 0.0001686147646836352,
        "epoch": 0.42376619795974635,
        "step": 1537
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.9762587547302246,
        "learning_rate": 0.00016842418658289816,
        "epoch": 0.42404190791287566,
        "step": 1538
    },
    {
        "loss": 0.9805,
        "grad_norm": 2.261502265930176,
        "learning_rate": 0.0001682331400310932,
        "epoch": 0.42431761786600497,
        "step": 1539
    },
    {
        "loss": 1.8321,
        "grad_norm": 1.2253022193908691,
        "learning_rate": 0.00016804162633617825,
        "epoch": 0.4245933278191343,
        "step": 1540
    },
    {
        "loss": 2.3351,
        "grad_norm": 1.6013370752334595,
        "learning_rate": 0.0001678496468093095,
        "epoch": 0.4248690377722636,
        "step": 1541
    },
    {
        "loss": 1.8535,
        "grad_norm": 1.8538165092468262,
        "learning_rate": 0.0001676572027648324,
        "epoch": 0.4251447477253929,
        "step": 1542
    },
    {
        "loss": 1.8474,
        "grad_norm": 1.6814250946044922,
        "learning_rate": 0.00016746429552027249,
        "epoch": 0.4254204576785222,
        "step": 1543
    },
    {
        "loss": 2.4091,
        "grad_norm": 2.1805663108825684,
        "learning_rate": 0.0001672709263963266,
        "epoch": 0.4256961676316515,
        "step": 1544
    },
    {
        "loss": 1.8697,
        "grad_norm": 2.3502306938171387,
        "learning_rate": 0.0001670770967168537,
        "epoch": 0.42597187758478083,
        "step": 1545
    },
    {
        "loss": 1.9608,
        "grad_norm": 1.5721579790115356,
        "learning_rate": 0.00016688280780886584,
        "epoch": 0.42624758753791014,
        "step": 1546
    },
    {
        "loss": 1.2476,
        "grad_norm": 2.4234259128570557,
        "learning_rate": 0.0001666880610025191,
        "epoch": 0.4265232974910394,
        "step": 1547
    },
    {
        "loss": 2.3903,
        "grad_norm": 1.5153594017028809,
        "learning_rate": 0.00016649285763110442,
        "epoch": 0.4267990074441687,
        "step": 1548
    },
    {
        "loss": 2.1153,
        "grad_norm": 1.5367504358291626,
        "learning_rate": 0.00016629719903103851,
        "epoch": 0.427074717397298,
        "step": 1549
    },
    {
        "loss": 2.0396,
        "grad_norm": 1.8435803651809692,
        "learning_rate": 0.00016610108654185475,
        "epoch": 0.42735042735042733,
        "step": 1550
    },
    {
        "loss": 2.0148,
        "grad_norm": 1.910520315170288,
        "learning_rate": 0.00016590452150619394,
        "epoch": 0.42762613730355664,
        "step": 1551
    },
    {
        "loss": 2.6499,
        "grad_norm": 0.9608818888664246,
        "learning_rate": 0.00016570750526979515,
        "epoch": 0.42790184725668595,
        "step": 1552
    },
    {
        "loss": 2.2204,
        "grad_norm": 1.2935287952423096,
        "learning_rate": 0.00016551003918148648,
        "epoch": 0.42817755720981526,
        "step": 1553
    },
    {
        "loss": 1.8897,
        "grad_norm": 1.3335570096969604,
        "learning_rate": 0.00016531212459317587,
        "epoch": 0.4284532671629446,
        "step": 1554
    },
    {
        "loss": 1.9453,
        "grad_norm": 1.7112252712249756,
        "learning_rate": 0.00016511376285984177,
        "epoch": 0.4287289771160739,
        "step": 1555
    },
    {
        "loss": 1.3878,
        "grad_norm": 1.646127462387085,
        "learning_rate": 0.000164914955339524,
        "epoch": 0.4290046870692032,
        "step": 1556
    },
    {
        "loss": 2.1216,
        "grad_norm": 1.1882402896881104,
        "learning_rate": 0.0001647157033933142,
        "epoch": 0.4292803970223325,
        "step": 1557
    },
    {
        "loss": 2.2249,
        "grad_norm": 1.4290920495986938,
        "learning_rate": 0.00016451600838534688,
        "epoch": 0.4295561069754618,
        "step": 1558
    },
    {
        "loss": 2.2787,
        "grad_norm": 1.7048848867416382,
        "learning_rate": 0.0001643158716827897,
        "epoch": 0.4298318169285911,
        "step": 1559
    },
    {
        "loss": 2.3874,
        "grad_norm": 1.325304627418518,
        "learning_rate": 0.00016411529465583437,
        "epoch": 0.43010752688172044,
        "step": 1560
    },
    {
        "loss": 2.2107,
        "grad_norm": 1.3966822624206543,
        "learning_rate": 0.00016391427867768719,
        "epoch": 0.43038323683484975,
        "step": 1561
    },
    {
        "loss": 1.7906,
        "grad_norm": 1.701771855354309,
        "learning_rate": 0.00016371282512455957,
        "epoch": 0.43065894678797906,
        "step": 1562
    },
    {
        "loss": 1.8508,
        "grad_norm": 1.551527738571167,
        "learning_rate": 0.00016351093537565875,
        "epoch": 0.43093465674110837,
        "step": 1563
    },
    {
        "loss": 2.6422,
        "grad_norm": 1.0247408151626587,
        "learning_rate": 0.00016330861081317833,
        "epoch": 0.4312103666942377,
        "step": 1564
    },
    {
        "loss": 2.1067,
        "grad_norm": 1.1047557592391968,
        "learning_rate": 0.0001631058528222886,
        "epoch": 0.431486076647367,
        "step": 1565
    },
    {
        "loss": 2.6016,
        "grad_norm": 1.0557385683059692,
        "learning_rate": 0.0001629026627911274,
        "epoch": 0.4317617866004963,
        "step": 1566
    },
    {
        "loss": 1.8395,
        "grad_norm": 1.7575281858444214,
        "learning_rate": 0.00016269904211079031,
        "epoch": 0.4320374965536256,
        "step": 1567
    },
    {
        "loss": 2.5395,
        "grad_norm": 1.1625126600265503,
        "learning_rate": 0.00016249499217532135,
        "epoch": 0.4323132065067549,
        "step": 1568
    },
    {
        "loss": 2.2486,
        "grad_norm": 1.7114630937576294,
        "learning_rate": 0.00016229051438170327,
        "epoch": 0.4325889164598842,
        "step": 1569
    },
    {
        "loss": 2.3298,
        "grad_norm": 1.3394498825073242,
        "learning_rate": 0.00016208561012984806,
        "epoch": 0.4328646264130135,
        "step": 1570
    },
    {
        "loss": 2.3309,
        "grad_norm": 1.2562335729599,
        "learning_rate": 0.00016188028082258745,
        "epoch": 0.4331403363661428,
        "step": 1571
    },
    {
        "loss": 2.2552,
        "grad_norm": 2.2501370906829834,
        "learning_rate": 0.0001616745278656631,
        "epoch": 0.4334160463192721,
        "step": 1572
    },
    {
        "loss": 2.2651,
        "grad_norm": 1.3216091394424438,
        "learning_rate": 0.0001614683526677172,
        "epoch": 0.4336917562724014,
        "step": 1573
    },
    {
        "loss": 2.0315,
        "grad_norm": 1.4145276546478271,
        "learning_rate": 0.00016126175664028263,
        "epoch": 0.43396746622553073,
        "step": 1574
    },
    {
        "loss": 2.0196,
        "grad_norm": 1.7694445848464966,
        "learning_rate": 0.00016105474119777337,
        "epoch": 0.43424317617866004,
        "step": 1575
    },
    {
        "loss": 1.8214,
        "grad_norm": 1.502588152885437,
        "learning_rate": 0.00016084730775747498,
        "epoch": 0.43451888613178935,
        "step": 1576
    },
    {
        "loss": 1.9041,
        "grad_norm": 1.8068041801452637,
        "learning_rate": 0.0001606394577395346,
        "epoch": 0.43479459608491866,
        "step": 1577
    },
    {
        "loss": 1.5933,
        "grad_norm": 1.693629503250122,
        "learning_rate": 0.00016043119256695148,
        "epoch": 0.43507030603804797,
        "step": 1578
    },
    {
        "loss": 1.2687,
        "grad_norm": 2.093583106994629,
        "learning_rate": 0.00016022251366556702,
        "epoch": 0.4353460159911773,
        "step": 1579
    },
    {
        "loss": 2.1672,
        "grad_norm": 1.3872087001800537,
        "learning_rate": 0.00016001342246405525,
        "epoch": 0.4356217259443066,
        "step": 1580
    },
    {
        "loss": 2.7013,
        "grad_norm": 1.4524815082550049,
        "learning_rate": 0.0001598039203939128,
        "epoch": 0.4358974358974359,
        "step": 1581
    },
    {
        "loss": 2.359,
        "grad_norm": 1.3977571725845337,
        "learning_rate": 0.00015959400888944933,
        "epoch": 0.4361731458505652,
        "step": 1582
    },
    {
        "loss": 2.5442,
        "grad_norm": 1.7489556074142456,
        "learning_rate": 0.00015938368938777755,
        "epoch": 0.4364488558036945,
        "step": 1583
    },
    {
        "loss": 1.9253,
        "grad_norm": 0.9751399755477905,
        "learning_rate": 0.0001591729633288034,
        "epoch": 0.43672456575682383,
        "step": 1584
    },
    {
        "loss": 2.3364,
        "grad_norm": 1.1674766540527344,
        "learning_rate": 0.0001589618321552163,
        "epoch": 0.43700027570995315,
        "step": 1585
    },
    {
        "loss": 1.6356,
        "grad_norm": 2.567812204360962,
        "learning_rate": 0.00015875029731247914,
        "epoch": 0.43727598566308246,
        "step": 1586
    },
    {
        "loss": 1.8845,
        "grad_norm": 1.7345300912857056,
        "learning_rate": 0.00015853836024881847,
        "epoch": 0.43755169561621177,
        "step": 1587
    },
    {
        "loss": 1.5176,
        "grad_norm": 2.0288658142089844,
        "learning_rate": 0.00015832602241521452,
        "epoch": 0.4378274055693411,
        "step": 1588
    },
    {
        "loss": 2.67,
        "grad_norm": 1.038030743598938,
        "learning_rate": 0.00015811328526539134,
        "epoch": 0.4381031155224704,
        "step": 1589
    },
    {
        "loss": 2.3824,
        "grad_norm": 1.070857286453247,
        "learning_rate": 0.00015790015025580684,
        "epoch": 0.43837882547559964,
        "step": 1590
    },
    {
        "loss": 2.1774,
        "grad_norm": 1.5016365051269531,
        "learning_rate": 0.00015768661884564273,
        "epoch": 0.43865453542872895,
        "step": 1591
    },
    {
        "loss": 1.2278,
        "grad_norm": 2.327399730682373,
        "learning_rate": 0.00015747269249679458,
        "epoch": 0.43893024538185826,
        "step": 1592
    },
    {
        "loss": 2.5203,
        "grad_norm": 1.6600077152252197,
        "learning_rate": 0.00015725837267386184,
        "epoch": 0.4392059553349876,
        "step": 1593
    },
    {
        "loss": 1.738,
        "grad_norm": 2.057985782623291,
        "learning_rate": 0.00015704366084413784,
        "epoch": 0.4394816652881169,
        "step": 1594
    },
    {
        "loss": 1.8926,
        "grad_norm": 2.500734567642212,
        "learning_rate": 0.00015682855847759964,
        "epoch": 0.4397573752412462,
        "step": 1595
    },
    {
        "loss": 2.5,
        "grad_norm": 1.140152931213379,
        "learning_rate": 0.00015661306704689806,
        "epoch": 0.4400330851943755,
        "step": 1596
    },
    {
        "loss": 1.3914,
        "grad_norm": 2.054032325744629,
        "learning_rate": 0.0001563971880273475,
        "epoch": 0.4403087951475048,
        "step": 1597
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.137873649597168,
        "learning_rate": 0.000156180922896916,
        "epoch": 0.4405845051006341,
        "step": 1598
    },
    {
        "loss": 1.8689,
        "grad_norm": 1.603444218635559,
        "learning_rate": 0.00015596427313621495,
        "epoch": 0.44086021505376344,
        "step": 1599
    },
    {
        "loss": 1.9082,
        "grad_norm": 1.3662176132202148,
        "learning_rate": 0.00015574724022848906,
        "epoch": 0.44113592500689275,
        "step": 1600
    },
    {
        "loss": 2.2583,
        "grad_norm": 1.8712607622146606,
        "learning_rate": 0.0001555298256596061,
        "epoch": 0.44141163496002206,
        "step": 1601
    },
    {
        "loss": 1.6707,
        "grad_norm": 1.9943387508392334,
        "learning_rate": 0.0001553120309180469,
        "epoch": 0.44168734491315137,
        "step": 1602
    },
    {
        "loss": 1.7617,
        "grad_norm": 1.8933804035186768,
        "learning_rate": 0.00015509385749489502,
        "epoch": 0.4419630548662807,
        "step": 1603
    },
    {
        "loss": 1.2645,
        "grad_norm": 1.6696819067001343,
        "learning_rate": 0.00015487530688382655,
        "epoch": 0.44223876481941,
        "step": 1604
    },
    {
        "loss": 0.7432,
        "grad_norm": 2.25483775138855,
        "learning_rate": 0.00015465638058109998,
        "epoch": 0.4425144747725393,
        "step": 1605
    },
    {
        "loss": 1.778,
        "grad_norm": 2.078141927719116,
        "learning_rate": 0.00015443708008554578,
        "epoch": 0.4427901847256686,
        "step": 1606
    },
    {
        "loss": 1.9248,
        "grad_norm": 1.9721300601959229,
        "learning_rate": 0.00015421740689855638,
        "epoch": 0.4430658946787979,
        "step": 1607
    },
    {
        "loss": 1.9348,
        "grad_norm": 2.0953586101531982,
        "learning_rate": 0.00015399736252407563,
        "epoch": 0.44334160463192723,
        "step": 1608
    },
    {
        "loss": 2.6503,
        "grad_norm": 0.9955100417137146,
        "learning_rate": 0.00015377694846858874,
        "epoch": 0.44361731458505654,
        "step": 1609
    },
    {
        "loss": 2.0341,
        "grad_norm": 1.7842400074005127,
        "learning_rate": 0.0001535561662411118,
        "epoch": 0.44389302453818585,
        "step": 1610
    },
    {
        "loss": 1.7535,
        "grad_norm": 1.9413073062896729,
        "learning_rate": 0.00015333501735318158,
        "epoch": 0.4441687344913151,
        "step": 1611
    },
    {
        "loss": 1.993,
        "grad_norm": 1.9652636051177979,
        "learning_rate": 0.00015311350331884499,
        "epoch": 0.4444444444444444,
        "step": 1612
    },
    {
        "loss": 2.2502,
        "grad_norm": 1.3323818445205688,
        "learning_rate": 0.0001528916256546489,
        "epoch": 0.44472015439757373,
        "step": 1613
    },
    {
        "loss": 2.1202,
        "grad_norm": 1.3347489833831787,
        "learning_rate": 0.0001526693858796297,
        "epoch": 0.44499586435070304,
        "step": 1614
    },
    {
        "loss": 2.5784,
        "grad_norm": 1.412596583366394,
        "learning_rate": 0.00015244678551530293,
        "epoch": 0.44527157430383235,
        "step": 1615
    },
    {
        "loss": 2.3087,
        "grad_norm": 1.796787142753601,
        "learning_rate": 0.00015222382608565273,
        "epoch": 0.44554728425696166,
        "step": 1616
    },
    {
        "loss": 1.9065,
        "grad_norm": 2.255856990814209,
        "learning_rate": 0.00015200050911712157,
        "epoch": 0.445822994210091,
        "step": 1617
    },
    {
        "loss": 0.7947,
        "grad_norm": 1.9243016242980957,
        "learning_rate": 0.0001517768361385997,
        "epoch": 0.4460987041632203,
        "step": 1618
    },
    {
        "loss": 2.5371,
        "grad_norm": 1.2813977003097534,
        "learning_rate": 0.0001515528086814147,
        "epoch": 0.4463744141163496,
        "step": 1619
    },
    {
        "loss": 1.6907,
        "grad_norm": 1.9958033561706543,
        "learning_rate": 0.00015132842827932108,
        "epoch": 0.4466501240694789,
        "step": 1620
    },
    {
        "loss": 2.1064,
        "grad_norm": 1.6071369647979736,
        "learning_rate": 0.00015110369646848963,
        "epoch": 0.4469258340226082,
        "step": 1621
    },
    {
        "loss": 2.3594,
        "grad_norm": 1.317020297050476,
        "learning_rate": 0.000150878614787497,
        "epoch": 0.4472015439757375,
        "step": 1622
    },
    {
        "loss": 1.5134,
        "grad_norm": 2.2766454219818115,
        "learning_rate": 0.00015065318477731522,
        "epoch": 0.44747725392886684,
        "step": 1623
    },
    {
        "loss": 1.8369,
        "grad_norm": 2.0128374099731445,
        "learning_rate": 0.00015042740798130097,
        "epoch": 0.44775296388199615,
        "step": 1624
    },
    {
        "loss": 1.5279,
        "grad_norm": 1.6724079847335815,
        "learning_rate": 0.0001502012859451852,
        "epoch": 0.44802867383512546,
        "step": 1625
    },
    {
        "loss": 2.2875,
        "grad_norm": 1.4466698169708252,
        "learning_rate": 0.00014997482021706247,
        "epoch": 0.44830438378825477,
        "step": 1626
    },
    {
        "loss": 2.2933,
        "grad_norm": 1.7234123945236206,
        "learning_rate": 0.00014974801234738026,
        "epoch": 0.4485800937413841,
        "step": 1627
    },
    {
        "loss": 1.8374,
        "grad_norm": 1.2233467102050781,
        "learning_rate": 0.00014952086388892854,
        "epoch": 0.4488558036945134,
        "step": 1628
    },
    {
        "loss": 1.4554,
        "grad_norm": 1.764247179031372,
        "learning_rate": 0.00014929337639682906,
        "epoch": 0.4491315136476427,
        "step": 1629
    },
    {
        "loss": 1.8606,
        "grad_norm": 1.3322099447250366,
        "learning_rate": 0.0001490655514285246,
        "epoch": 0.449407223600772,
        "step": 1630
    },
    {
        "loss": 2.0884,
        "grad_norm": 1.607540249824524,
        "learning_rate": 0.00014883739054376847,
        "epoch": 0.4496829335539013,
        "step": 1631
    },
    {
        "loss": 2.4487,
        "grad_norm": 1.674438238143921,
        "learning_rate": 0.0001486088953046137,
        "epoch": 0.44995864350703063,
        "step": 1632
    },
    {
        "loss": 2.0964,
        "grad_norm": 1.4835566282272339,
        "learning_rate": 0.00014838006727540247,
        "epoch": 0.4502343534601599,
        "step": 1633
    },
    {
        "loss": 1.5445,
        "grad_norm": 1.7460685968399048,
        "learning_rate": 0.00014815090802275524,
        "epoch": 0.4505100634132892,
        "step": 1634
    },
    {
        "loss": 2.3046,
        "grad_norm": 2.09059739112854,
        "learning_rate": 0.0001479214191155603,
        "epoch": 0.4507857733664185,
        "step": 1635
    },
    {
        "loss": 1.4163,
        "grad_norm": 2.2354419231414795,
        "learning_rate": 0.00014769160212496255,
        "epoch": 0.4510614833195478,
        "step": 1636
    },
    {
        "loss": 1.552,
        "grad_norm": 2.035243034362793,
        "learning_rate": 0.00014746145862435338,
        "epoch": 0.45133719327267713,
        "step": 1637
    },
    {
        "loss": 2.0784,
        "grad_norm": 1.677374243736267,
        "learning_rate": 0.00014723099018935928,
        "epoch": 0.45161290322580644,
        "step": 1638
    },
    {
        "loss": 1.9998,
        "grad_norm": 1.4885356426239014,
        "learning_rate": 0.0001470001983978315,
        "epoch": 0.45188861317893575,
        "step": 1639
    },
    {
        "loss": 1.4202,
        "grad_norm": 1.9970073699951172,
        "learning_rate": 0.00014676908482983501,
        "epoch": 0.45216432313206506,
        "step": 1640
    },
    {
        "loss": 2.1865,
        "grad_norm": 1.8748935461044312,
        "learning_rate": 0.00014653765106763776,
        "epoch": 0.45244003308519437,
        "step": 1641
    },
    {
        "loss": 1.868,
        "grad_norm": 2.0319902896881104,
        "learning_rate": 0.00014630589869569984,
        "epoch": 0.4527157430383237,
        "step": 1642
    },
    {
        "loss": 2.4541,
        "grad_norm": 1.436102032661438,
        "learning_rate": 0.00014607382930066266,
        "epoch": 0.452991452991453,
        "step": 1643
    },
    {
        "loss": 1.8363,
        "grad_norm": 1.2803709506988525,
        "learning_rate": 0.00014584144447133795,
        "epoch": 0.4532671629445823,
        "step": 1644
    },
    {
        "loss": 2.4629,
        "grad_norm": 1.5889034271240234,
        "learning_rate": 0.00014560874579869722,
        "epoch": 0.4535428728977116,
        "step": 1645
    },
    {
        "loss": 1.7602,
        "grad_norm": 2.2657504081726074,
        "learning_rate": 0.00014537573487586033,
        "epoch": 0.4538185828508409,
        "step": 1646
    },
    {
        "loss": 2.5482,
        "grad_norm": 2.181550979614258,
        "learning_rate": 0.0001451424132980852,
        "epoch": 0.45409429280397023,
        "step": 1647
    },
    {
        "loss": 2.2159,
        "grad_norm": 1.1138464212417603,
        "learning_rate": 0.00014490878266275634,
        "epoch": 0.45437000275709954,
        "step": 1648
    },
    {
        "loss": 2.3007,
        "grad_norm": 1.3800855875015259,
        "learning_rate": 0.00014467484456937428,
        "epoch": 0.45464571271022886,
        "step": 1649
    },
    {
        "loss": 1.4586,
        "grad_norm": 2.1463985443115234,
        "learning_rate": 0.00014444060061954446,
        "epoch": 0.45492142266335817,
        "step": 1650
    },
    {
        "loss": 2.0319,
        "grad_norm": 2.033769130706787,
        "learning_rate": 0.0001442060524169663,
        "epoch": 0.4551971326164875,
        "step": 1651
    },
    {
        "loss": 2.11,
        "grad_norm": 1.2941479682922363,
        "learning_rate": 0.00014397120156742227,
        "epoch": 0.4554728425696168,
        "step": 1652
    },
    {
        "loss": 2.2423,
        "grad_norm": 1.8110491037368774,
        "learning_rate": 0.0001437360496787667,
        "epoch": 0.4557485525227461,
        "step": 1653
    },
    {
        "loss": 2.1601,
        "grad_norm": 1.8899449110031128,
        "learning_rate": 0.00014350059836091506,
        "epoch": 0.45602426247587535,
        "step": 1654
    },
    {
        "loss": 2.0876,
        "grad_norm": 1.6630786657333374,
        "learning_rate": 0.00014326484922583282,
        "epoch": 0.45629997242900466,
        "step": 1655
    },
    {
        "loss": 2.1579,
        "grad_norm": 0.909913182258606,
        "learning_rate": 0.0001430288038875242,
        "epoch": 0.456575682382134,
        "step": 1656
    },
    {
        "loss": 2.2828,
        "grad_norm": 2.068020820617676,
        "learning_rate": 0.0001427924639620215,
        "epoch": 0.4568513923352633,
        "step": 1657
    },
    {
        "loss": 2.1733,
        "grad_norm": 1.414900302886963,
        "learning_rate": 0.00014255583106737377,
        "epoch": 0.4571271022883926,
        "step": 1658
    },
    {
        "loss": 2.3365,
        "grad_norm": 1.334121584892273,
        "learning_rate": 0.00014231890682363577,
        "epoch": 0.4574028122415219,
        "step": 1659
    },
    {
        "loss": 2.1347,
        "grad_norm": 2.2248153686523438,
        "learning_rate": 0.00014208169285285707,
        "epoch": 0.4576785221946512,
        "step": 1660
    },
    {
        "loss": 2.1325,
        "grad_norm": 1.9318852424621582,
        "learning_rate": 0.00014184419077907054,
        "epoch": 0.4579542321477805,
        "step": 1661
    },
    {
        "loss": 2.0784,
        "grad_norm": 1.5776128768920898,
        "learning_rate": 0.00014160640222828175,
        "epoch": 0.45822994210090984,
        "step": 1662
    },
    {
        "loss": 1.466,
        "grad_norm": 2.248107433319092,
        "learning_rate": 0.00014136832882845736,
        "epoch": 0.45850565205403915,
        "step": 1663
    },
    {
        "loss": 2.3014,
        "grad_norm": 1.122541069984436,
        "learning_rate": 0.00014112997220951435,
        "epoch": 0.45878136200716846,
        "step": 1664
    },
    {
        "loss": 2.318,
        "grad_norm": 1.3447844982147217,
        "learning_rate": 0.00014089133400330855,
        "epoch": 0.45905707196029777,
        "step": 1665
    },
    {
        "loss": 1.5642,
        "grad_norm": 2.25532865524292,
        "learning_rate": 0.00014065241584362373,
        "epoch": 0.4593327819134271,
        "step": 1666
    },
    {
        "loss": 1.3584,
        "grad_norm": 1.6614763736724854,
        "learning_rate": 0.00014041321936616026,
        "epoch": 0.4596084918665564,
        "step": 1667
    },
    {
        "loss": 1.425,
        "grad_norm": 1.7977467775344849,
        "learning_rate": 0.0001401737462085239,
        "epoch": 0.4598842018196857,
        "step": 1668
    },
    {
        "loss": 2.5834,
        "grad_norm": 1.165747046470642,
        "learning_rate": 0.00013993399801021477,
        "epoch": 0.460159911772815,
        "step": 1669
    },
    {
        "loss": 2.2379,
        "grad_norm": 1.2776637077331543,
        "learning_rate": 0.0001396939764126158,
        "epoch": 0.4604356217259443,
        "step": 1670
    },
    {
        "loss": 2.306,
        "grad_norm": 1.8439193964004517,
        "learning_rate": 0.00013945368305898193,
        "epoch": 0.46071133167907363,
        "step": 1671
    },
    {
        "loss": 2.2866,
        "grad_norm": 1.374754786491394,
        "learning_rate": 0.0001392131195944284,
        "epoch": 0.46098704163220294,
        "step": 1672
    },
    {
        "loss": 2.1916,
        "grad_norm": 1.224205732345581,
        "learning_rate": 0.0001389722876659198,
        "epoch": 0.46126275158533225,
        "step": 1673
    },
    {
        "loss": 1.5406,
        "grad_norm": 0.8897287249565125,
        "learning_rate": 0.0001387311889222587,
        "epoch": 0.46153846153846156,
        "step": 1674
    },
    {
        "loss": 2.2664,
        "grad_norm": 1.240332841873169,
        "learning_rate": 0.00013848982501407436,
        "epoch": 0.4618141714915908,
        "step": 1675
    },
    {
        "loss": 1.9294,
        "grad_norm": 1.4762046337127686,
        "learning_rate": 0.00013824819759381143,
        "epoch": 0.46208988144472013,
        "step": 1676
    },
    {
        "loss": 2.2784,
        "grad_norm": 1.4335694313049316,
        "learning_rate": 0.00013800630831571864,
        "epoch": 0.46236559139784944,
        "step": 1677
    },
    {
        "loss": 2.3253,
        "grad_norm": 1.364615797996521,
        "learning_rate": 0.0001377641588358374,
        "epoch": 0.46264130135097875,
        "step": 1678
    },
    {
        "loss": 2.4245,
        "grad_norm": 1.4203892946243286,
        "learning_rate": 0.0001375217508119907,
        "epoch": 0.46291701130410806,
        "step": 1679
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.6658921241760254,
        "learning_rate": 0.00013727908590377134,
        "epoch": 0.46319272125723737,
        "step": 1680
    },
    {
        "loss": 1.9009,
        "grad_norm": 2.756603717803955,
        "learning_rate": 0.0001370361657725311,
        "epoch": 0.4634684312103667,
        "step": 1681
    },
    {
        "loss": 1.8478,
        "grad_norm": 1.5340802669525146,
        "learning_rate": 0.00013679299208136893,
        "epoch": 0.463744141163496,
        "step": 1682
    },
    {
        "loss": 1.6951,
        "grad_norm": 2.6869940757751465,
        "learning_rate": 0.00013654956649511974,
        "epoch": 0.4640198511166253,
        "step": 1683
    },
    {
        "loss": 2.4522,
        "grad_norm": 1.1127233505249023,
        "learning_rate": 0.000136305890680343,
        "epoch": 0.4642955610697546,
        "step": 1684
    },
    {
        "loss": 2.4596,
        "grad_norm": 1.2787108421325684,
        "learning_rate": 0.0001360619663053113,
        "epoch": 0.4645712710228839,
        "step": 1685
    },
    {
        "loss": 2.1967,
        "grad_norm": 2.2517054080963135,
        "learning_rate": 0.00013581779503999894,
        "epoch": 0.46484698097601324,
        "step": 1686
    },
    {
        "loss": 2.3551,
        "grad_norm": 0.9749916195869446,
        "learning_rate": 0.00013557337855607055,
        "epoch": 0.46512269092914255,
        "step": 1687
    },
    {
        "loss": 2.0902,
        "grad_norm": 1.895097255706787,
        "learning_rate": 0.00013532871852686956,
        "epoch": 0.46539840088227186,
        "step": 1688
    },
    {
        "loss": 2.0428,
        "grad_norm": 1.852571725845337,
        "learning_rate": 0.00013508381662740674,
        "epoch": 0.46567411083540117,
        "step": 1689
    },
    {
        "loss": 1.7612,
        "grad_norm": 1.9381401538848877,
        "learning_rate": 0.00013483867453434879,
        "epoch": 0.4659498207885305,
        "step": 1690
    },
    {
        "loss": 1.9454,
        "grad_norm": 2.1117491722106934,
        "learning_rate": 0.00013459329392600692,
        "epoch": 0.4662255307416598,
        "step": 1691
    },
    {
        "loss": 2.004,
        "grad_norm": 1.811966896057129,
        "learning_rate": 0.00013434767648232523,
        "epoch": 0.4665012406947891,
        "step": 1692
    },
    {
        "loss": 2.0009,
        "grad_norm": 0.8999805450439453,
        "learning_rate": 0.0001341018238848692,
        "epoch": 0.4667769506479184,
        "step": 1693
    },
    {
        "loss": 2.1316,
        "grad_norm": 1.3071677684783936,
        "learning_rate": 0.00013385573781681434,
        "epoch": 0.4670526606010477,
        "step": 1694
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.2080293893814087,
        "learning_rate": 0.00013360941996293444,
        "epoch": 0.46732837055417703,
        "step": 1695
    },
    {
        "loss": 2.2002,
        "grad_norm": 1.5925341844558716,
        "learning_rate": 0.00013336287200959028,
        "epoch": 0.46760408050730634,
        "step": 1696
    },
    {
        "loss": 1.7058,
        "grad_norm": 1.9130332469940186,
        "learning_rate": 0.000133116095644718,
        "epoch": 0.4678797904604356,
        "step": 1697
    },
    {
        "loss": 2.1553,
        "grad_norm": 1.1165313720703125,
        "learning_rate": 0.00013286909255781726,
        "epoch": 0.4681555004135649,
        "step": 1698
    },
    {
        "loss": 2.0646,
        "grad_norm": 1.8000935316085815,
        "learning_rate": 0.00013262186443994026,
        "epoch": 0.4684312103666942,
        "step": 1699
    },
    {
        "loss": 2.4585,
        "grad_norm": 1.261587142944336,
        "learning_rate": 0.00013237441298367958,
        "epoch": 0.46870692031982353,
        "step": 1700
    },
    {
        "loss": 2.3824,
        "grad_norm": 1.1745131015777588,
        "learning_rate": 0.00013212673988315693,
        "epoch": 0.46898263027295284,
        "step": 1701
    },
    {
        "loss": 2.1335,
        "grad_norm": 1.0225191116333008,
        "learning_rate": 0.00013187884683401143,
        "epoch": 0.46925834022608215,
        "step": 1702
    },
    {
        "loss": 2.0675,
        "grad_norm": 1.3849049806594849,
        "learning_rate": 0.00013163073553338803,
        "epoch": 0.46953405017921146,
        "step": 1703
    },
    {
        "loss": 2.3701,
        "grad_norm": 1.1601248979568481,
        "learning_rate": 0.000131382407679926,
        "epoch": 0.46980976013234077,
        "step": 1704
    },
    {
        "loss": 2.462,
        "grad_norm": 1.34527587890625,
        "learning_rate": 0.00013113386497374694,
        "epoch": 0.4700854700854701,
        "step": 1705
    },
    {
        "loss": 1.5309,
        "grad_norm": 2.973550319671631,
        "learning_rate": 0.00013088510911644365,
        "epoch": 0.4703611800385994,
        "step": 1706
    },
    {
        "loss": 2.0668,
        "grad_norm": 1.773281216621399,
        "learning_rate": 0.0001306361418110681,
        "epoch": 0.4706368899917287,
        "step": 1707
    },
    {
        "loss": 2.016,
        "grad_norm": 1.6281287670135498,
        "learning_rate": 0.00013038696476211988,
        "epoch": 0.470912599944858,
        "step": 1708
    },
    {
        "loss": 2.277,
        "grad_norm": 1.3646084070205688,
        "learning_rate": 0.00013013757967553463,
        "epoch": 0.4711883098979873,
        "step": 1709
    },
    {
        "loss": 2.1077,
        "grad_norm": 1.3143587112426758,
        "learning_rate": 0.0001298879882586722,
        "epoch": 0.47146401985111663,
        "step": 1710
    },
    {
        "loss": 2.217,
        "grad_norm": 0.8573747873306274,
        "learning_rate": 0.00012963819222030503,
        "epoch": 0.47173972980424594,
        "step": 1711
    },
    {
        "loss": 2.0011,
        "grad_norm": 1.6503111124038696,
        "learning_rate": 0.00012938819327060644,
        "epoch": 0.47201543975737525,
        "step": 1712
    },
    {
        "loss": 2.0009,
        "grad_norm": 1.4010331630706787,
        "learning_rate": 0.00012913799312113904,
        "epoch": 0.47229114971050457,
        "step": 1713
    },
    {
        "loss": 1.9156,
        "grad_norm": 1.7569539546966553,
        "learning_rate": 0.00012888759348484281,
        "epoch": 0.4725668596636339,
        "step": 1714
    },
    {
        "loss": 1.6899,
        "grad_norm": 1.9084851741790771,
        "learning_rate": 0.00012863699607602356,
        "epoch": 0.4728425696167632,
        "step": 1715
    },
    {
        "loss": 1.6461,
        "grad_norm": 2.53708553314209,
        "learning_rate": 0.000128386202610341,
        "epoch": 0.4731182795698925,
        "step": 1716
    },
    {
        "loss": 2.4046,
        "grad_norm": 1.465865135192871,
        "learning_rate": 0.00012813521480479722,
        "epoch": 0.4733939895230218,
        "step": 1717
    },
    {
        "loss": 2.228,
        "grad_norm": 1.3902919292449951,
        "learning_rate": 0.00012788403437772466,
        "epoch": 0.47366969947615106,
        "step": 1718
    },
    {
        "loss": 2.1433,
        "grad_norm": 1.9440605640411377,
        "learning_rate": 0.00012763266304877466,
        "epoch": 0.4739454094292804,
        "step": 1719
    },
    {
        "loss": 2.3881,
        "grad_norm": 1.7866641283035278,
        "learning_rate": 0.00012738110253890543,
        "epoch": 0.4742211193824097,
        "step": 1720
    },
    {
        "loss": 2.3339,
        "grad_norm": 1.3861844539642334,
        "learning_rate": 0.00012712935457037037,
        "epoch": 0.474496829335539,
        "step": 1721
    },
    {
        "loss": 1.9771,
        "grad_norm": 1.5431076288223267,
        "learning_rate": 0.00012687742086670632,
        "epoch": 0.4747725392886683,
        "step": 1722
    },
    {
        "loss": 2.47,
        "grad_norm": 1.2882065773010254,
        "learning_rate": 0.00012662530315272168,
        "epoch": 0.4750482492417976,
        "step": 1723
    },
    {
        "loss": 2.3973,
        "grad_norm": 1.038364291191101,
        "learning_rate": 0.0001263730031544847,
        "epoch": 0.4753239591949269,
        "step": 1724
    },
    {
        "loss": 1.9037,
        "grad_norm": 2.519639015197754,
        "learning_rate": 0.00012612052259931145,
        "epoch": 0.47559966914805624,
        "step": 1725
    },
    {
        "loss": 1.865,
        "grad_norm": 1.6148868799209595,
        "learning_rate": 0.0001258678632157543,
        "epoch": 0.47587537910118555,
        "step": 1726
    },
    {
        "loss": 2.0012,
        "grad_norm": 1.8777416944503784,
        "learning_rate": 0.00012561502673358988,
        "epoch": 0.47615108905431486,
        "step": 1727
    },
    {
        "loss": 1.3697,
        "grad_norm": 1.899983286857605,
        "learning_rate": 0.00012536201488380722,
        "epoch": 0.47642679900744417,
        "step": 1728
    },
    {
        "loss": 2.1822,
        "grad_norm": 1.4105325937271118,
        "learning_rate": 0.00012510882939859608,
        "epoch": 0.4767025089605735,
        "step": 1729
    },
    {
        "loss": 1.5411,
        "grad_norm": 1.9943448305130005,
        "learning_rate": 0.00012485547201133488,
        "epoch": 0.4769782189137028,
        "step": 1730
    },
    {
        "loss": 1.7682,
        "grad_norm": 1.9948514699935913,
        "learning_rate": 0.00012460194445657898,
        "epoch": 0.4772539288668321,
        "step": 1731
    },
    {
        "loss": 1.8629,
        "grad_norm": 2.156743288040161,
        "learning_rate": 0.00012434824847004872,
        "epoch": 0.4775296388199614,
        "step": 1732
    },
    {
        "loss": 1.5742,
        "grad_norm": 2.500283718109131,
        "learning_rate": 0.0001240943857886176,
        "epoch": 0.4778053487730907,
        "step": 1733
    },
    {
        "loss": 1.8722,
        "grad_norm": 2.802286148071289,
        "learning_rate": 0.00012384035815030039,
        "epoch": 0.47808105872622003,
        "step": 1734
    },
    {
        "loss": 1.3917,
        "grad_norm": 1.832479476928711,
        "learning_rate": 0.00012358616729424112,
        "epoch": 0.47835676867934934,
        "step": 1735
    },
    {
        "loss": 1.9886,
        "grad_norm": 1.6843883991241455,
        "learning_rate": 0.0001233318149607013,
        "epoch": 0.47863247863247865,
        "step": 1736
    },
    {
        "loss": 1.9649,
        "grad_norm": 1.4613851308822632,
        "learning_rate": 0.00012307730289104794,
        "epoch": 0.47890818858560796,
        "step": 1737
    },
    {
        "loss": 1.1741,
        "grad_norm": 1.7012478113174438,
        "learning_rate": 0.0001228226328277417,
        "epoch": 0.4791838985387373,
        "step": 1738
    },
    {
        "loss": 2.5954,
        "grad_norm": 1.364699125289917,
        "learning_rate": 0.00012256780651432486,
        "epoch": 0.47945960849186653,
        "step": 1739
    },
    {
        "loss": 1.8842,
        "grad_norm": 1.9243532419204712,
        "learning_rate": 0.00012231282569540942,
        "epoch": 0.47973531844499584,
        "step": 1740
    },
    {
        "loss": 1.8557,
        "grad_norm": 1.834519386291504,
        "learning_rate": 0.00012205769211666525,
        "epoch": 0.48001102839812515,
        "step": 1741
    },
    {
        "loss": 1.8387,
        "grad_norm": 2.2758452892303467,
        "learning_rate": 0.00012180240752480791,
        "epoch": 0.48028673835125446,
        "step": 1742
    },
    {
        "loss": 1.9604,
        "grad_norm": 1.2141549587249756,
        "learning_rate": 0.000121546973667587,
        "epoch": 0.48056244830438377,
        "step": 1743
    },
    {
        "loss": 2.1229,
        "grad_norm": 1.6609487533569336,
        "learning_rate": 0.00012129139229377398,
        "epoch": 0.4808381582575131,
        "step": 1744
    },
    {
        "loss": 1.4061,
        "grad_norm": 2.213613986968994,
        "learning_rate": 0.00012103566515315012,
        "epoch": 0.4811138682106424,
        "step": 1745
    },
    {
        "loss": 1.6861,
        "grad_norm": 2.215824842453003,
        "learning_rate": 0.00012077979399649486,
        "epoch": 0.4813895781637717,
        "step": 1746
    },
    {
        "loss": 1.7808,
        "grad_norm": 1.5333483219146729,
        "learning_rate": 0.00012052378057557347,
        "epoch": 0.481665288116901,
        "step": 1747
    },
    {
        "loss": 1.7861,
        "grad_norm": 1.8039857149124146,
        "learning_rate": 0.00012026762664312523,
        "epoch": 0.4819409980700303,
        "step": 1748
    },
    {
        "loss": 2.1183,
        "grad_norm": 0.9501398801803589,
        "learning_rate": 0.00012001133395285147,
        "epoch": 0.48221670802315963,
        "step": 1749
    },
    {
        "loss": 1.6808,
        "grad_norm": 2.2699294090270996,
        "learning_rate": 0.00011975490425940336,
        "epoch": 0.48249241797628895,
        "step": 1750
    },
    {
        "loss": 2.2397,
        "grad_norm": 1.205385684967041,
        "learning_rate": 0.0001194983393183702,
        "epoch": 0.48276812792941826,
        "step": 1751
    },
    {
        "loss": 1.908,
        "grad_norm": 1.363304853439331,
        "learning_rate": 0.00011924164088626706,
        "epoch": 0.48304383788254757,
        "step": 1752
    },
    {
        "loss": 2.2421,
        "grad_norm": 1.7782524824142456,
        "learning_rate": 0.00011898481072052306,
        "epoch": 0.4833195478356769,
        "step": 1753
    },
    {
        "loss": 1.0904,
        "grad_norm": 2.003880739212036,
        "learning_rate": 0.00011872785057946914,
        "epoch": 0.4835952577888062,
        "step": 1754
    },
    {
        "loss": 1.3805,
        "grad_norm": 1.7956571578979492,
        "learning_rate": 0.00011847076222232609,
        "epoch": 0.4838709677419355,
        "step": 1755
    },
    {
        "loss": 2.1257,
        "grad_norm": 1.1556099653244019,
        "learning_rate": 0.00011821354740919259,
        "epoch": 0.4841466776950648,
        "step": 1756
    },
    {
        "loss": 1.5483,
        "grad_norm": 1.9405603408813477,
        "learning_rate": 0.0001179562079010329,
        "epoch": 0.4844223876481941,
        "step": 1757
    },
    {
        "loss": 1.4702,
        "grad_norm": 1.8777509927749634,
        "learning_rate": 0.00011769874545966513,
        "epoch": 0.48469809760132343,
        "step": 1758
    },
    {
        "loss": 2.3325,
        "grad_norm": 1.0051085948944092,
        "learning_rate": 0.00011744116184774896,
        "epoch": 0.48497380755445274,
        "step": 1759
    },
    {
        "loss": 2.3865,
        "grad_norm": 1.5307321548461914,
        "learning_rate": 0.00011718345882877366,
        "epoch": 0.48524951750758205,
        "step": 1760
    },
    {
        "loss": 1.7035,
        "grad_norm": 1.866736888885498,
        "learning_rate": 0.00011692563816704598,
        "epoch": 0.4855252274607113,
        "step": 1761
    },
    {
        "loss": 1.3558,
        "grad_norm": 2.5556347370147705,
        "learning_rate": 0.00011666770162767806,
        "epoch": 0.4858009374138406,
        "step": 1762
    },
    {
        "loss": 2.2923,
        "grad_norm": 1.4854698181152344,
        "learning_rate": 0.00011640965097657544,
        "epoch": 0.4860766473669699,
        "step": 1763
    },
    {
        "loss": 1.5275,
        "grad_norm": 1.8838711977005005,
        "learning_rate": 0.00011615148798042483,
        "epoch": 0.48635235732009924,
        "step": 1764
    },
    {
        "loss": 2.4997,
        "grad_norm": 1.0547881126403809,
        "learning_rate": 0.00011589321440668213,
        "epoch": 0.48662806727322855,
        "step": 1765
    },
    {
        "loss": 2.158,
        "grad_norm": 1.3634283542633057,
        "learning_rate": 0.00011563483202356026,
        "epoch": 0.48690377722635786,
        "step": 1766
    },
    {
        "loss": 2.4682,
        "grad_norm": 1.3572275638580322,
        "learning_rate": 0.00011537634260001711,
        "epoch": 0.48717948717948717,
        "step": 1767
    },
    {
        "loss": 2.2626,
        "grad_norm": 1.2229869365692139,
        "learning_rate": 0.00011511774790574335,
        "epoch": 0.4874551971326165,
        "step": 1768
    },
    {
        "loss": 1.3482,
        "grad_norm": 2.264734983444214,
        "learning_rate": 0.0001148590497111504,
        "epoch": 0.4877309070857458,
        "step": 1769
    },
    {
        "loss": 2.5057,
        "grad_norm": 1.7000982761383057,
        "learning_rate": 0.00011460024978735826,
        "epoch": 0.4880066170388751,
        "step": 1770
    },
    {
        "loss": 2.4269,
        "grad_norm": 1.1031676530838013,
        "learning_rate": 0.00011434134990618344,
        "epoch": 0.4882823269920044,
        "step": 1771
    },
    {
        "loss": 2.7134,
        "grad_norm": 1.0754753351211548,
        "learning_rate": 0.00011408235184012668,
        "epoch": 0.4885580369451337,
        "step": 1772
    },
    {
        "loss": 2.0455,
        "grad_norm": 1.6402219533920288,
        "learning_rate": 0.00011382325736236103,
        "epoch": 0.48883374689826303,
        "step": 1773
    },
    {
        "loss": 2.494,
        "grad_norm": 1.772373080253601,
        "learning_rate": 0.00011356406824671953,
        "epoch": 0.48910945685139234,
        "step": 1774
    },
    {
        "loss": 2.5861,
        "grad_norm": 0.9814426302909851,
        "learning_rate": 0.00011330478626768319,
        "epoch": 0.48938516680452165,
        "step": 1775
    },
    {
        "loss": 1.9181,
        "grad_norm": 1.9005969762802124,
        "learning_rate": 0.00011304541320036876,
        "epoch": 0.48966087675765096,
        "step": 1776
    },
    {
        "loss": 2.286,
        "grad_norm": 0.9370946884155273,
        "learning_rate": 0.0001127859508205166,
        "epoch": 0.4899365867107803,
        "step": 1777
    },
    {
        "loss": 1.2792,
        "grad_norm": 2.5780029296875,
        "learning_rate": 0.00011252640090447855,
        "epoch": 0.4902122966639096,
        "step": 1778
    },
    {
        "loss": 2.4978,
        "grad_norm": 1.1063451766967773,
        "learning_rate": 0.00011226676522920571,
        "epoch": 0.4904880066170389,
        "step": 1779
    },
    {
        "loss": 2.3755,
        "grad_norm": 1.2312018871307373,
        "learning_rate": 0.00011200704557223636,
        "epoch": 0.4907637165701682,
        "step": 1780
    },
    {
        "loss": 2.3005,
        "grad_norm": 1.234494924545288,
        "learning_rate": 0.00011174724371168374,
        "epoch": 0.4910394265232975,
        "step": 1781
    },
    {
        "loss": 2.5559,
        "grad_norm": 1.2961492538452148,
        "learning_rate": 0.0001114873614262238,
        "epoch": 0.4913151364764268,
        "step": 1782
    },
    {
        "loss": 0.9293,
        "grad_norm": 2.195399284362793,
        "learning_rate": 0.00011122740049508322,
        "epoch": 0.4915908464295561,
        "step": 1783
    },
    {
        "loss": 2.1163,
        "grad_norm": 1.4118421077728271,
        "learning_rate": 0.00011096736269802699,
        "epoch": 0.4918665563826854,
        "step": 1784
    },
    {
        "loss": 1.8613,
        "grad_norm": 1.8247981071472168,
        "learning_rate": 0.00011070724981534644,
        "epoch": 0.4921422663358147,
        "step": 1785
    },
    {
        "loss": 1.0101,
        "grad_norm": 1.832827091217041,
        "learning_rate": 0.00011044706362784694,
        "epoch": 0.492417976288944,
        "step": 1786
    },
    {
        "loss": 2.0159,
        "grad_norm": 1.900149941444397,
        "learning_rate": 0.00011018680591683567,
        "epoch": 0.4926936862420733,
        "step": 1787
    },
    {
        "loss": 2.1452,
        "grad_norm": 1.0704938173294067,
        "learning_rate": 0.00010992647846410955,
        "epoch": 0.49296939619520264,
        "step": 1788
    },
    {
        "loss": 2.0393,
        "grad_norm": 1.4649578332901,
        "learning_rate": 0.0001096660830519429,
        "epoch": 0.49324510614833195,
        "step": 1789
    },
    {
        "loss": 1.6723,
        "grad_norm": 2.2892136573791504,
        "learning_rate": 0.00010940562146307536,
        "epoch": 0.49352081610146126,
        "step": 1790
    },
    {
        "loss": 1.9921,
        "grad_norm": 1.439374327659607,
        "learning_rate": 0.00010914509548069967,
        "epoch": 0.49379652605459057,
        "step": 1791
    },
    {
        "loss": 2.2143,
        "grad_norm": 1.4941468238830566,
        "learning_rate": 0.00010888450688844929,
        "epoch": 0.4940722360077199,
        "step": 1792
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.1419522762298584,
        "learning_rate": 0.0001086238574703865,
        "epoch": 0.4943479459608492,
        "step": 1793
    },
    {
        "loss": 2.4835,
        "grad_norm": 1.4435484409332275,
        "learning_rate": 0.00010836314901098985,
        "epoch": 0.4946236559139785,
        "step": 1794
    },
    {
        "loss": 1.7714,
        "grad_norm": 2.6204569339752197,
        "learning_rate": 0.00010810238329514224,
        "epoch": 0.4948993658671078,
        "step": 1795
    },
    {
        "loss": 2.4366,
        "grad_norm": 1.247454047203064,
        "learning_rate": 0.00010784156210811846,
        "epoch": 0.4951750758202371,
        "step": 1796
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.110140085220337,
        "learning_rate": 0.0001075806872355731,
        "epoch": 0.49545078577336643,
        "step": 1797
    },
    {
        "loss": 1.978,
        "grad_norm": 1.740519642829895,
        "learning_rate": 0.00010731976046352836,
        "epoch": 0.49572649572649574,
        "step": 1798
    },
    {
        "loss": 1.2438,
        "grad_norm": 1.9516314268112183,
        "learning_rate": 0.00010705878357836168,
        "epoch": 0.49600220567962505,
        "step": 1799
    },
    {
        "loss": 1.3233,
        "grad_norm": 2.223973512649536,
        "learning_rate": 0.00010679775836679357,
        "epoch": 0.49627791563275436,
        "step": 1800
    },
    {
        "loss": 1.8134,
        "grad_norm": 2.3093295097351074,
        "learning_rate": 0.00010653668661587551,
        "epoch": 0.4965536255858837,
        "step": 1801
    },
    {
        "loss": 1.8984,
        "grad_norm": 1.00754976272583,
        "learning_rate": 0.00010627557011297748,
        "epoch": 0.496829335539013,
        "step": 1802
    },
    {
        "loss": 1.9775,
        "grad_norm": 1.7116589546203613,
        "learning_rate": 0.00010601441064577595,
        "epoch": 0.49710504549214224,
        "step": 1803
    },
    {
        "loss": 2.3768,
        "grad_norm": 1.7164855003356934,
        "learning_rate": 0.00010575321000224142,
        "epoch": 0.49738075544527155,
        "step": 1804
    },
    {
        "loss": 1.9368,
        "grad_norm": 2.1952242851257324,
        "learning_rate": 0.00010549196997062636,
        "epoch": 0.49765646539840086,
        "step": 1805
    },
    {
        "loss": 2.2086,
        "grad_norm": 1.5209779739379883,
        "learning_rate": 0.00010523069233945297,
        "epoch": 0.49793217535153017,
        "step": 1806
    },
    {
        "loss": 2.2044,
        "grad_norm": 1.5446172952651978,
        "learning_rate": 0.00010496937889750071,
        "epoch": 0.4982078853046595,
        "step": 1807
    },
    {
        "loss": 2.107,
        "grad_norm": 1.1146512031555176,
        "learning_rate": 0.00010470803143379437,
        "epoch": 0.4984835952577888,
        "step": 1808
    },
    {
        "loss": 1.9749,
        "grad_norm": 2.079702854156494,
        "learning_rate": 0.00010444665173759152,
        "epoch": 0.4987593052109181,
        "step": 1809
    },
    {
        "loss": 1.8751,
        "grad_norm": 1.79228937625885,
        "learning_rate": 0.00010418524159837053,
        "epoch": 0.4990350151640474,
        "step": 1810
    },
    {
        "loss": 2.0375,
        "grad_norm": 1.4478604793548584,
        "learning_rate": 0.00010392380280581808,
        "epoch": 0.4993107251171767,
        "step": 1811
    },
    {
        "loss": 2.2225,
        "grad_norm": 1.88852059841156,
        "learning_rate": 0.0001036623371498171,
        "epoch": 0.49958643507030603,
        "step": 1812
    },
    {
        "loss": 1.8411,
        "grad_norm": 1.8546082973480225,
        "learning_rate": 0.00010340084642043441,
        "epoch": 0.49986214502343534,
        "step": 1813
    },
    {
        "loss": 2.38,
        "grad_norm": 1.6667014360427856,
        "learning_rate": 0.00010313933240790848,
        "epoch": 0.5001378549765646,
        "step": 1814
    },
    {
        "loss": 1.2653,
        "grad_norm": 2.15205979347229,
        "learning_rate": 0.00010287779690263717,
        "epoch": 0.5004135649296939,
        "step": 1815
    },
    {
        "loss": 1.5187,
        "grad_norm": 2.5400755405426025,
        "learning_rate": 0.0001026162416951655,
        "epoch": 0.5006892748828232,
        "step": 1816
    },
    {
        "loss": 2.52,
        "grad_norm": 1.2268980741500854,
        "learning_rate": 0.00010235466857617342,
        "epoch": 0.5009649848359525,
        "step": 1817
    },
    {
        "loss": 2.3074,
        "grad_norm": 1.016769528388977,
        "learning_rate": 0.00010209307933646346,
        "epoch": 0.5012406947890818,
        "step": 1818
    },
    {
        "loss": 1.9515,
        "grad_norm": 1.6035770177841187,
        "learning_rate": 0.00010183147576694847,
        "epoch": 0.5015164047422112,
        "step": 1819
    },
    {
        "loss": 2.0653,
        "grad_norm": 1.9504594802856445,
        "learning_rate": 0.00010156985965863954,
        "epoch": 0.5017921146953405,
        "step": 1820
    },
    {
        "loss": 2.0491,
        "grad_norm": 1.3249752521514893,
        "learning_rate": 0.00010130823280263344,
        "epoch": 0.5020678246484698,
        "step": 1821
    },
    {
        "loss": 2.0835,
        "grad_norm": 1.8212599754333496,
        "learning_rate": 0.00010104659699010066,
        "epoch": 0.5023435346015991,
        "step": 1822
    },
    {
        "loss": 2.4004,
        "grad_norm": 1.4311705827713013,
        "learning_rate": 0.00010078495401227296,
        "epoch": 0.5026192445547284,
        "step": 1823
    },
    {
        "loss": 2.1014,
        "grad_norm": 1.4072074890136719,
        "learning_rate": 0.00010052330566043113,
        "epoch": 0.5028949545078577,
        "step": 1824
    },
    {
        "loss": 1.846,
        "grad_norm": 2.254923105239868,
        "learning_rate": 0.00010026165372589278,
        "epoch": 0.503170664460987,
        "step": 1825
    },
    {
        "loss": 1.3329,
        "grad_norm": 1.6561299562454224,
        "learning_rate": 0.0001,
        "epoch": 0.5034463744141163,
        "step": 1826
    },
    {
        "loss": 2.11,
        "grad_norm": 1.6643580198287964,
        "learning_rate": 9.973834627410725e-05,
        "epoch": 0.5037220843672456,
        "step": 1827
    },
    {
        "loss": 1.6528,
        "grad_norm": 1.7747573852539062,
        "learning_rate": 9.94766943395689e-05,
        "epoch": 0.503997794320375,
        "step": 1828
    },
    {
        "loss": 1.941,
        "grad_norm": 1.8918136358261108,
        "learning_rate": 9.921504598772705e-05,
        "epoch": 0.5042735042735043,
        "step": 1829
    },
    {
        "loss": 2.1788,
        "grad_norm": 1.583827018737793,
        "learning_rate": 9.895340300989935e-05,
        "epoch": 0.5045492142266336,
        "step": 1830
    },
    {
        "loss": 1.6332,
        "grad_norm": 2.723811388015747,
        "learning_rate": 9.869176719736657e-05,
        "epoch": 0.5048249241797629,
        "step": 1831
    },
    {
        "loss": 1.7787,
        "grad_norm": 1.4871642589569092,
        "learning_rate": 9.843014034136047e-05,
        "epoch": 0.5051006341328922,
        "step": 1832
    },
    {
        "loss": 1.9548,
        "grad_norm": 2.7905027866363525,
        "learning_rate": 9.816852423305154e-05,
        "epoch": 0.5053763440860215,
        "step": 1833
    },
    {
        "loss": 2.4717,
        "grad_norm": 1.7000763416290283,
        "learning_rate": 9.790692066353656e-05,
        "epoch": 0.5056520540391508,
        "step": 1834
    },
    {
        "loss": 1.6368,
        "grad_norm": 1.990541934967041,
        "learning_rate": 9.76453314238266e-05,
        "epoch": 0.5059277639922801,
        "step": 1835
    },
    {
        "loss": 2.3829,
        "grad_norm": 1.5235270261764526,
        "learning_rate": 9.73837583048345e-05,
        "epoch": 0.5062034739454094,
        "step": 1836
    },
    {
        "loss": 2.0117,
        "grad_norm": 1.8702110052108765,
        "learning_rate": 9.712220309736286e-05,
        "epoch": 0.5064791838985387,
        "step": 1837
    },
    {
        "loss": 2.0384,
        "grad_norm": 1.7648675441741943,
        "learning_rate": 9.686066759209156e-05,
        "epoch": 0.506754893851668,
        "step": 1838
    },
    {
        "loss": 1.0136,
        "grad_norm": 1.6940315961837769,
        "learning_rate": 9.659915357956561e-05,
        "epoch": 0.5070306038047974,
        "step": 1839
    },
    {
        "loss": 2.0986,
        "grad_norm": 2.076960802078247,
        "learning_rate": 9.633766285018291e-05,
        "epoch": 0.5073063137579267,
        "step": 1840
    },
    {
        "loss": 2.5998,
        "grad_norm": 1.2717909812927246,
        "learning_rate": 9.607619719418194e-05,
        "epoch": 0.507582023711056,
        "step": 1841
    },
    {
        "loss": 2.1602,
        "grad_norm": 0.9911787509918213,
        "learning_rate": 9.581475840162949e-05,
        "epoch": 0.5078577336641853,
        "step": 1842
    },
    {
        "loss": 2.3602,
        "grad_norm": 1.444977879524231,
        "learning_rate": 9.555334826240849e-05,
        "epoch": 0.5081334436173146,
        "step": 1843
    },
    {
        "loss": 2.0078,
        "grad_norm": 1.7361565828323364,
        "learning_rate": 9.529196856620565e-05,
        "epoch": 0.5084091535704439,
        "step": 1844
    },
    {
        "loss": 1.4571,
        "grad_norm": 2.2920925617218018,
        "learning_rate": 9.503062110249931e-05,
        "epoch": 0.5086848635235732,
        "step": 1845
    },
    {
        "loss": 2.4559,
        "grad_norm": 1.5215760469436646,
        "learning_rate": 9.476930766054705e-05,
        "epoch": 0.5089605734767025,
        "step": 1846
    },
    {
        "loss": 2.594,
        "grad_norm": 1.2937813997268677,
        "learning_rate": 9.450803002937365e-05,
        "epoch": 0.5092362834298318,
        "step": 1847
    },
    {
        "loss": 2.3697,
        "grad_norm": 1.1979347467422485,
        "learning_rate": 9.424678999775861e-05,
        "epoch": 0.5095119933829612,
        "step": 1848
    },
    {
        "loss": 2.3383,
        "grad_norm": 1.2001243829727173,
        "learning_rate": 9.398558935422408e-05,
        "epoch": 0.5097877033360905,
        "step": 1849
    },
    {
        "loss": 2.0178,
        "grad_norm": 1.8021345138549805,
        "learning_rate": 9.372442988702253e-05,
        "epoch": 0.5100634132892198,
        "step": 1850
    },
    {
        "loss": 1.9231,
        "grad_norm": 1.5057053565979004,
        "learning_rate": 9.34633133841245e-05,
        "epoch": 0.5103391232423491,
        "step": 1851
    },
    {
        "loss": 1.4982,
        "grad_norm": 1.9341505765914917,
        "learning_rate": 9.320224163320645e-05,
        "epoch": 0.5106148331954784,
        "step": 1852
    },
    {
        "loss": 2.0793,
        "grad_norm": 1.5824989080429077,
        "learning_rate": 9.294121642163836e-05,
        "epoch": 0.5108905431486077,
        "step": 1853
    },
    {
        "loss": 2.482,
        "grad_norm": 0.9331084489822388,
        "learning_rate": 9.268023953647165e-05,
        "epoch": 0.511166253101737,
        "step": 1854
    },
    {
        "loss": 1.915,
        "grad_norm": 1.7116106748580933,
        "learning_rate": 9.241931276442692e-05,
        "epoch": 0.5114419630548663,
        "step": 1855
    },
    {
        "loss": 2.0113,
        "grad_norm": 1.9927303791046143,
        "learning_rate": 9.215843789188156e-05,
        "epoch": 0.5117176730079956,
        "step": 1856
    },
    {
        "loss": 1.6907,
        "grad_norm": 2.554823637008667,
        "learning_rate": 9.189761670485779e-05,
        "epoch": 0.5119933829611248,
        "step": 1857
    },
    {
        "loss": 2.2278,
        "grad_norm": 1.8047816753387451,
        "learning_rate": 9.163685098901016e-05,
        "epoch": 0.5122690929142542,
        "step": 1858
    },
    {
        "loss": 2.1216,
        "grad_norm": 1.291881799697876,
        "learning_rate": 9.137614252961352e-05,
        "epoch": 0.5125448028673835,
        "step": 1859
    },
    {
        "loss": 1.3827,
        "grad_norm": 2.2833781242370605,
        "learning_rate": 9.111549311155074e-05,
        "epoch": 0.5128205128205128,
        "step": 1860
    },
    {
        "loss": 1.4946,
        "grad_norm": 2.337851047515869,
        "learning_rate": 9.085490451930037e-05,
        "epoch": 0.5130962227736421,
        "step": 1861
    },
    {
        "loss": 2.1591,
        "grad_norm": 1.2672851085662842,
        "learning_rate": 9.059437853692466e-05,
        "epoch": 0.5133719327267714,
        "step": 1862
    },
    {
        "loss": 2.4718,
        "grad_norm": 2.458746910095215,
        "learning_rate": 9.033391694805713e-05,
        "epoch": 0.5136476426799007,
        "step": 1863
    },
    {
        "loss": 1.7601,
        "grad_norm": 1.223858118057251,
        "learning_rate": 9.007352153589047e-05,
        "epoch": 0.51392335263303,
        "step": 1864
    },
    {
        "loss": 2.4533,
        "grad_norm": 1.0915186405181885,
        "learning_rate": 8.981319408316435e-05,
        "epoch": 0.5141990625861593,
        "step": 1865
    },
    {
        "loss": 1.6824,
        "grad_norm": 1.6816518306732178,
        "learning_rate": 8.955293637215307e-05,
        "epoch": 0.5144747725392886,
        "step": 1866
    },
    {
        "loss": 1.2502,
        "grad_norm": 1.7760884761810303,
        "learning_rate": 8.929275018465357e-05,
        "epoch": 0.514750482492418,
        "step": 1867
    },
    {
        "loss": 1.4386,
        "grad_norm": 1.6260099411010742,
        "learning_rate": 8.903263730197303e-05,
        "epoch": 0.5150261924455473,
        "step": 1868
    },
    {
        "loss": 2.0365,
        "grad_norm": 2.168776035308838,
        "learning_rate": 8.87725995049168e-05,
        "epoch": 0.5153019023986766,
        "step": 1869
    },
    {
        "loss": 2.3297,
        "grad_norm": 0.9813822507858276,
        "learning_rate": 8.851263857377622e-05,
        "epoch": 0.5155776123518059,
        "step": 1870
    },
    {
        "loss": 1.6285,
        "grad_norm": 1.9418140649795532,
        "learning_rate": 8.825275628831628e-05,
        "epoch": 0.5158533223049352,
        "step": 1871
    },
    {
        "loss": 2.2271,
        "grad_norm": 1.8197649717330933,
        "learning_rate": 8.799295442776366e-05,
        "epoch": 0.5161290322580645,
        "step": 1872
    },
    {
        "loss": 2.3957,
        "grad_norm": 1.3219319581985474,
        "learning_rate": 8.77332347707943e-05,
        "epoch": 0.5164047422111938,
        "step": 1873
    },
    {
        "loss": 2.3347,
        "grad_norm": 1.72980797290802,
        "learning_rate": 8.747359909552146e-05,
        "epoch": 0.5166804521643231,
        "step": 1874
    },
    {
        "loss": 1.8159,
        "grad_norm": 1.4988501071929932,
        "learning_rate": 8.721404917948342e-05,
        "epoch": 0.5169561621174524,
        "step": 1875
    },
    {
        "loss": 1.9259,
        "grad_norm": 1.5399640798568726,
        "learning_rate": 8.695458679963126e-05,
        "epoch": 0.5172318720705817,
        "step": 1876
    },
    {
        "loss": 2.117,
        "grad_norm": 1.514496088027954,
        "learning_rate": 8.669521373231683e-05,
        "epoch": 0.517507582023711,
        "step": 1877
    },
    {
        "loss": 1.7296,
        "grad_norm": 2.121586322784424,
        "learning_rate": 8.643593175328048e-05,
        "epoch": 0.5177832919768404,
        "step": 1878
    },
    {
        "loss": 2.4191,
        "grad_norm": 1.2126893997192383,
        "learning_rate": 8.6176742637639e-05,
        "epoch": 0.5180590019299697,
        "step": 1879
    },
    {
        "loss": 2.0973,
        "grad_norm": 0.9508761763572693,
        "learning_rate": 8.591764815987336e-05,
        "epoch": 0.518334711883099,
        "step": 1880
    },
    {
        "loss": 2.0481,
        "grad_norm": 1.0343105792999268,
        "learning_rate": 8.565865009381659e-05,
        "epoch": 0.5186104218362283,
        "step": 1881
    },
    {
        "loss": 1.872,
        "grad_norm": 1.9463928937911987,
        "learning_rate": 8.539975021264177e-05,
        "epoch": 0.5188861317893576,
        "step": 1882
    },
    {
        "loss": 2.3992,
        "grad_norm": 1.1073569059371948,
        "learning_rate": 8.514095028884962e-05,
        "epoch": 0.5191618417424869,
        "step": 1883
    },
    {
        "loss": 1.9352,
        "grad_norm": 1.3972493410110474,
        "learning_rate": 8.488225209425669e-05,
        "epoch": 0.5194375516956162,
        "step": 1884
    },
    {
        "loss": 1.6776,
        "grad_norm": 2.3815863132476807,
        "learning_rate": 8.462365739998293e-05,
        "epoch": 0.5197132616487455,
        "step": 1885
    },
    {
        "loss": 1.5581,
        "grad_norm": 2.0195369720458984,
        "learning_rate": 8.436516797643975e-05,
        "epoch": 0.5199889716018748,
        "step": 1886
    },
    {
        "loss": 2.4026,
        "grad_norm": 1.1947792768478394,
        "learning_rate": 8.410678559331791e-05,
        "epoch": 0.5202646815550042,
        "step": 1887
    },
    {
        "loss": 2.0898,
        "grad_norm": 1.3026312589645386,
        "learning_rate": 8.384851201957519e-05,
        "epoch": 0.5205403915081335,
        "step": 1888
    },
    {
        "loss": 2.3244,
        "grad_norm": 1.5049397945404053,
        "learning_rate": 8.35903490234246e-05,
        "epoch": 0.5208161014612628,
        "step": 1889
    },
    {
        "loss": 2.3902,
        "grad_norm": 2.0661425590515137,
        "learning_rate": 8.333229837232196e-05,
        "epoch": 0.5210918114143921,
        "step": 1890
    },
    {
        "loss": 2.4093,
        "grad_norm": 0.8975827097892761,
        "learning_rate": 8.307436183295405e-05,
        "epoch": 0.5213675213675214,
        "step": 1891
    },
    {
        "loss": 1.8533,
        "grad_norm": 2.0438475608825684,
        "learning_rate": 8.281654117122636e-05,
        "epoch": 0.5216432313206507,
        "step": 1892
    },
    {
        "loss": 2.4973,
        "grad_norm": 1.6810282468795776,
        "learning_rate": 8.255883815225105e-05,
        "epoch": 0.52191894127378,
        "step": 1893
    },
    {
        "loss": 2.3783,
        "grad_norm": 1.8454598188400269,
        "learning_rate": 8.23012545403349e-05,
        "epoch": 0.5221946512269093,
        "step": 1894
    },
    {
        "loss": 2.168,
        "grad_norm": 1.423539638519287,
        "learning_rate": 8.204379209896712e-05,
        "epoch": 0.5224703611800386,
        "step": 1895
    },
    {
        "loss": 1.6393,
        "grad_norm": 1.7784315347671509,
        "learning_rate": 8.178645259080743e-05,
        "epoch": 0.522746071133168,
        "step": 1896
    },
    {
        "loss": 2.2395,
        "grad_norm": 1.5374163389205933,
        "learning_rate": 8.152923777767392e-05,
        "epoch": 0.5230217810862973,
        "step": 1897
    },
    {
        "loss": 2.042,
        "grad_norm": 1.4215216636657715,
        "learning_rate": 8.127214942053087e-05,
        "epoch": 0.5232974910394266,
        "step": 1898
    },
    {
        "loss": 1.4985,
        "grad_norm": 1.6664719581604004,
        "learning_rate": 8.101518927947698e-05,
        "epoch": 0.5235732009925558,
        "step": 1899
    },
    {
        "loss": 1.4638,
        "grad_norm": 2.2447094917297363,
        "learning_rate": 8.075835911373297e-05,
        "epoch": 0.5238489109456851,
        "step": 1900
    },
    {
        "loss": 2.4677,
        "grad_norm": 0.937960147857666,
        "learning_rate": 8.050166068162983e-05,
        "epoch": 0.5241246208988144,
        "step": 1901
    },
    {
        "loss": 1.4652,
        "grad_norm": 2.1254329681396484,
        "learning_rate": 8.024509574059666e-05,
        "epoch": 0.5244003308519437,
        "step": 1902
    },
    {
        "loss": 2.2841,
        "grad_norm": 1.9677224159240723,
        "learning_rate": 7.998866604714857e-05,
        "epoch": 0.524676040805073,
        "step": 1903
    },
    {
        "loss": 1.3371,
        "grad_norm": 1.941046118736267,
        "learning_rate": 7.97323733568748e-05,
        "epoch": 0.5249517507582023,
        "step": 1904
    },
    {
        "loss": 1.6947,
        "grad_norm": 1.6950116157531738,
        "learning_rate": 7.947621942442655e-05,
        "epoch": 0.5252274607113316,
        "step": 1905
    },
    {
        "loss": 2.3243,
        "grad_norm": 1.4239304065704346,
        "learning_rate": 7.922020600350515e-05,
        "epoch": 0.525503170664461,
        "step": 1906
    },
    {
        "loss": 2.3486,
        "grad_norm": 1.561249017715454,
        "learning_rate": 7.89643348468499e-05,
        "epoch": 0.5257788806175903,
        "step": 1907
    },
    {
        "loss": 1.8132,
        "grad_norm": 1.735253930091858,
        "learning_rate": 7.870860770622606e-05,
        "epoch": 0.5260545905707196,
        "step": 1908
    },
    {
        "loss": 2.0728,
        "grad_norm": 1.324459195137024,
        "learning_rate": 7.8453026332413e-05,
        "epoch": 0.5263303005238489,
        "step": 1909
    },
    {
        "loss": 2.3183,
        "grad_norm": 1.1387120485305786,
        "learning_rate": 7.81975924751921e-05,
        "epoch": 0.5266060104769782,
        "step": 1910
    },
    {
        "loss": 2.344,
        "grad_norm": 1.3318597078323364,
        "learning_rate": 7.794230788333477e-05,
        "epoch": 0.5268817204301075,
        "step": 1911
    },
    {
        "loss": 1.9052,
        "grad_norm": 1.0600911378860474,
        "learning_rate": 7.768717430459061e-05,
        "epoch": 0.5271574303832368,
        "step": 1912
    },
    {
        "loss": 2.2042,
        "grad_norm": 1.6342253684997559,
        "learning_rate": 7.743219348567517e-05,
        "epoch": 0.5274331403363661,
        "step": 1913
    },
    {
        "loss": 1.5895,
        "grad_norm": 2.321598768234253,
        "learning_rate": 7.717736717225835e-05,
        "epoch": 0.5277088502894954,
        "step": 1914
    },
    {
        "loss": 2.5945,
        "grad_norm": 1.6511974334716797,
        "learning_rate": 7.692269710895209e-05,
        "epoch": 0.5279845602426247,
        "step": 1915
    },
    {
        "loss": 1.8757,
        "grad_norm": 1.9020519256591797,
        "learning_rate": 7.666818503929874e-05,
        "epoch": 0.5282602701957541,
        "step": 1916
    },
    {
        "loss": 1.8785,
        "grad_norm": 1.5437266826629639,
        "learning_rate": 7.641383270575892e-05,
        "epoch": 0.5285359801488834,
        "step": 1917
    },
    {
        "loss": 2.7263,
        "grad_norm": 1.4584583044052124,
        "learning_rate": 7.615964184969964e-05,
        "epoch": 0.5288116901020127,
        "step": 1918
    },
    {
        "loss": 1.7114,
        "grad_norm": 2.600153923034668,
        "learning_rate": 7.590561421138241e-05,
        "epoch": 0.529087400055142,
        "step": 1919
    },
    {
        "loss": 2.1752,
        "grad_norm": 1.7860147953033447,
        "learning_rate": 7.565175152995129e-05,
        "epoch": 0.5293631100082713,
        "step": 1920
    },
    {
        "loss": 2.3054,
        "grad_norm": 1.4854648113250732,
        "learning_rate": 7.539805554342104e-05,
        "epoch": 0.5296388199614006,
        "step": 1921
    },
    {
        "loss": 1.8334,
        "grad_norm": 0.9525311589241028,
        "learning_rate": 7.514452798866514e-05,
        "epoch": 0.5299145299145299,
        "step": 1922
    },
    {
        "loss": 2.1501,
        "grad_norm": 1.2971246242523193,
        "learning_rate": 7.489117060140393e-05,
        "epoch": 0.5301902398676592,
        "step": 1923
    },
    {
        "loss": 1.9802,
        "grad_norm": 1.6274412870407104,
        "learning_rate": 7.46379851161928e-05,
        "epoch": 0.5304659498207885,
        "step": 1924
    },
    {
        "loss": 1.9597,
        "grad_norm": 2.0926849842071533,
        "learning_rate": 7.438497326641014e-05,
        "epoch": 0.5307416597739179,
        "step": 1925
    },
    {
        "loss": 1.3907,
        "grad_norm": 1.6212615966796875,
        "learning_rate": 7.413213678424573e-05,
        "epoch": 0.5310173697270472,
        "step": 1926
    },
    {
        "loss": 1.8213,
        "grad_norm": 1.7638980150222778,
        "learning_rate": 7.387947740068856e-05,
        "epoch": 0.5312930796801765,
        "step": 1927
    },
    {
        "loss": 1.8984,
        "grad_norm": 1.472144365310669,
        "learning_rate": 7.362699684551533e-05,
        "epoch": 0.5315687896333058,
        "step": 1928
    },
    {
        "loss": 1.2738,
        "grad_norm": 2.25711727142334,
        "learning_rate": 7.337469684727833e-05,
        "epoch": 0.5318444995864351,
        "step": 1929
    },
    {
        "loss": 1.5971,
        "grad_norm": 2.3712563514709473,
        "learning_rate": 7.31225791332937e-05,
        "epoch": 0.5321202095395644,
        "step": 1930
    },
    {
        "loss": 2.1764,
        "grad_norm": 1.4505107402801514,
        "learning_rate": 7.287064542962966e-05,
        "epoch": 0.5323959194926937,
        "step": 1931
    },
    {
        "loss": 2.1728,
        "grad_norm": 1.141376256942749,
        "learning_rate": 7.26188974610946e-05,
        "epoch": 0.532671629445823,
        "step": 1932
    },
    {
        "loss": 1.9528,
        "grad_norm": 1.920363426208496,
        "learning_rate": 7.236733695122535e-05,
        "epoch": 0.5329473393989523,
        "step": 1933
    },
    {
        "loss": 2.3729,
        "grad_norm": 1.0831981897354126,
        "learning_rate": 7.211596562227536e-05,
        "epoch": 0.5332230493520816,
        "step": 1934
    },
    {
        "loss": 1.7344,
        "grad_norm": 1.6256922483444214,
        "learning_rate": 7.18647851952028e-05,
        "epoch": 0.533498759305211,
        "step": 1935
    },
    {
        "loss": 1.3291,
        "grad_norm": 1.6944576501846313,
        "learning_rate": 7.161379738965902e-05,
        "epoch": 0.5337744692583403,
        "step": 1936
    },
    {
        "loss": 2.033,
        "grad_norm": 1.8289399147033691,
        "learning_rate": 7.136300392397647e-05,
        "epoch": 0.5340501792114696,
        "step": 1937
    },
    {
        "loss": 2.2407,
        "grad_norm": 1.5787266492843628,
        "learning_rate": 7.111240651515721e-05,
        "epoch": 0.5343258891645989,
        "step": 1938
    },
    {
        "loss": 1.9982,
        "grad_norm": 1.9068177938461304,
        "learning_rate": 7.086200687886103e-05,
        "epoch": 0.5346015991177282,
        "step": 1939
    },
    {
        "loss": 2.2048,
        "grad_norm": 1.4255987405776978,
        "learning_rate": 7.061180672939363e-05,
        "epoch": 0.5348773090708575,
        "step": 1940
    },
    {
        "loss": 1.823,
        "grad_norm": 1.6807072162628174,
        "learning_rate": 7.03618077796951e-05,
        "epoch": 0.5351530190239868,
        "step": 1941
    },
    {
        "loss": 1.5298,
        "grad_norm": 1.3739906549453735,
        "learning_rate": 7.01120117413279e-05,
        "epoch": 0.535428728977116,
        "step": 1942
    },
    {
        "loss": 1.3565,
        "grad_norm": 1.9167903661727905,
        "learning_rate": 6.986242032446545e-05,
        "epoch": 0.5357044389302453,
        "step": 1943
    },
    {
        "loss": 2.5937,
        "grad_norm": 1.1774448156356812,
        "learning_rate": 6.961303523788021e-05,
        "epoch": 0.5359801488833746,
        "step": 1944
    },
    {
        "loss": 2.473,
        "grad_norm": 1.5231983661651611,
        "learning_rate": 6.9363858188932e-05,
        "epoch": 0.536255858836504,
        "step": 1945
    },
    {
        "loss": 1.9322,
        "grad_norm": 2.872187376022339,
        "learning_rate": 6.911489088355645e-05,
        "epoch": 0.5365315687896333,
        "step": 1946
    },
    {
        "loss": 2.2791,
        "grad_norm": 1.6185944080352783,
        "learning_rate": 6.886613502625316e-05,
        "epoch": 0.5368072787427626,
        "step": 1947
    },
    {
        "loss": 1.9709,
        "grad_norm": 2.0274851322174072,
        "learning_rate": 6.861759232007412e-05,
        "epoch": 0.5370829886958919,
        "step": 1948
    },
    {
        "loss": 2.3565,
        "grad_norm": 1.4083242416381836,
        "learning_rate": 6.836926446661204e-05,
        "epoch": 0.5373586986490212,
        "step": 1949
    },
    {
        "loss": 2.3724,
        "grad_norm": 1.5399808883666992,
        "learning_rate": 6.812115316598866e-05,
        "epoch": 0.5376344086021505,
        "step": 1950
    },
    {
        "loss": 2.1973,
        "grad_norm": 1.163218379020691,
        "learning_rate": 6.787326011684318e-05,
        "epoch": 0.5379101185552798,
        "step": 1951
    },
    {
        "loss": 2.2134,
        "grad_norm": 0.935287356376648,
        "learning_rate": 6.76255870163205e-05,
        "epoch": 0.5381858285084091,
        "step": 1952
    },
    {
        "loss": 2.1366,
        "grad_norm": 1.4571812152862549,
        "learning_rate": 6.73781355600598e-05,
        "epoch": 0.5384615384615384,
        "step": 1953
    },
    {
        "loss": 1.7947,
        "grad_norm": 2.206486463546753,
        "learning_rate": 6.71309074421828e-05,
        "epoch": 0.5387372484146677,
        "step": 1954
    },
    {
        "loss": 2.2277,
        "grad_norm": 1.1269489526748657,
        "learning_rate": 6.688390435528211e-05,
        "epoch": 0.5390129583677971,
        "step": 1955
    },
    {
        "loss": 1.6425,
        "grad_norm": 1.888274908065796,
        "learning_rate": 6.66371279904098e-05,
        "epoch": 0.5392886683209264,
        "step": 1956
    },
    {
        "loss": 2.2922,
        "grad_norm": 1.1844497919082642,
        "learning_rate": 6.639058003706565e-05,
        "epoch": 0.5395643782740557,
        "step": 1957
    },
    {
        "loss": 2.5202,
        "grad_norm": 1.1658014059066772,
        "learning_rate": 6.614426218318576e-05,
        "epoch": 0.539840088227185,
        "step": 1958
    },
    {
        "loss": 2.0941,
        "grad_norm": 1.4061238765716553,
        "learning_rate": 6.58981761151309e-05,
        "epoch": 0.5401157981803143,
        "step": 1959
    },
    {
        "loss": 2.6705,
        "grad_norm": 1.1372027397155762,
        "learning_rate": 6.565232351767485e-05,
        "epoch": 0.5403915081334436,
        "step": 1960
    },
    {
        "loss": 2.2729,
        "grad_norm": 1.0318379402160645,
        "learning_rate": 6.540670607399315e-05,
        "epoch": 0.5406672180865729,
        "step": 1961
    },
    {
        "loss": 1.095,
        "grad_norm": 1.9368245601654053,
        "learning_rate": 6.516132546565128e-05,
        "epoch": 0.5409429280397022,
        "step": 1962
    },
    {
        "loss": 2.7754,
        "grad_norm": 1.7043651342391968,
        "learning_rate": 6.491618337259338e-05,
        "epoch": 0.5412186379928315,
        "step": 1963
    },
    {
        "loss": 1.7999,
        "grad_norm": 1.619162678718567,
        "learning_rate": 6.467128147313053e-05,
        "epoch": 0.5414943479459609,
        "step": 1964
    },
    {
        "loss": 2.0611,
        "grad_norm": 0.9819346070289612,
        "learning_rate": 6.442662144392953e-05,
        "epoch": 0.5417700578990902,
        "step": 1965
    },
    {
        "loss": 1.4029,
        "grad_norm": 2.1006357669830322,
        "learning_rate": 6.418220496000115e-05,
        "epoch": 0.5420457678522195,
        "step": 1966
    },
    {
        "loss": 1.3935,
        "grad_norm": 2.3265650272369385,
        "learning_rate": 6.39380336946888e-05,
        "epoch": 0.5423214778053488,
        "step": 1967
    },
    {
        "loss": 2.2125,
        "grad_norm": 1.588043451309204,
        "learning_rate": 6.369410931965712e-05,
        "epoch": 0.5425971877584781,
        "step": 1968
    },
    {
        "loss": 2.1113,
        "grad_norm": 1.5797556638717651,
        "learning_rate": 6.345043350488035e-05,
        "epoch": 0.5428728977116074,
        "step": 1969
    },
    {
        "loss": 2.493,
        "grad_norm": 2.0594985485076904,
        "learning_rate": 6.320700791863114e-05,
        "epoch": 0.5431486076647367,
        "step": 1970
    },
    {
        "loss": 1.815,
        "grad_norm": 1.872948408126831,
        "learning_rate": 6.296383422746899e-05,
        "epoch": 0.543424317617866,
        "step": 1971
    },
    {
        "loss": 1.9832,
        "grad_norm": 2.062959671020508,
        "learning_rate": 6.272091409622873e-05,
        "epoch": 0.5437000275709953,
        "step": 1972
    },
    {
        "loss": 2.4233,
        "grad_norm": 1.2544375658035278,
        "learning_rate": 6.247824918800943e-05,
        "epoch": 0.5439757375241246,
        "step": 1973
    },
    {
        "loss": 1.1523,
        "grad_norm": 2.1654911041259766,
        "learning_rate": 6.223584116416267e-05,
        "epoch": 0.544251447477254,
        "step": 1974
    },
    {
        "loss": 1.3799,
        "grad_norm": 1.8817650079727173,
        "learning_rate": 6.199369168428145e-05,
        "epoch": 0.5445271574303833,
        "step": 1975
    },
    {
        "loss": 2.5034,
        "grad_norm": 1.2415186166763306,
        "learning_rate": 6.175180240618865e-05,
        "epoch": 0.5448028673835126,
        "step": 1976
    },
    {
        "loss": 2.0006,
        "grad_norm": 1.6742537021636963,
        "learning_rate": 6.15101749859257e-05,
        "epoch": 0.5450785773366419,
        "step": 1977
    },
    {
        "loss": 2.4027,
        "grad_norm": 1.3918476104736328,
        "learning_rate": 6.126881107774138e-05,
        "epoch": 0.5453542872897712,
        "step": 1978
    },
    {
        "loss": 1.6001,
        "grad_norm": 1.546004056930542,
        "learning_rate": 6.102771233408029e-05,
        "epoch": 0.5456299972429005,
        "step": 1979
    },
    {
        "loss": 2.2771,
        "grad_norm": 1.4914625883102417,
        "learning_rate": 6.078688040557169e-05,
        "epoch": 0.5459057071960298,
        "step": 1980
    },
    {
        "loss": 1.58,
        "grad_norm": 1.953295350074768,
        "learning_rate": 6.054631694101817e-05,
        "epoch": 0.5461814171491591,
        "step": 1981
    },
    {
        "loss": 2.2224,
        "grad_norm": 1.2248833179473877,
        "learning_rate": 6.0306023587384265e-05,
        "epoch": 0.5464571271022884,
        "step": 1982
    },
    {
        "loss": 2.0882,
        "grad_norm": 1.0382167100906372,
        "learning_rate": 6.0066001989785334e-05,
        "epoch": 0.5467328370554178,
        "step": 1983
    },
    {
        "loss": 2.0158,
        "grad_norm": 1.1777825355529785,
        "learning_rate": 5.982625379147617e-05,
        "epoch": 0.5470085470085471,
        "step": 1984
    },
    {
        "loss": 2.4576,
        "grad_norm": 1.2614109516143799,
        "learning_rate": 5.9586780633839825e-05,
        "epoch": 0.5472842569616763,
        "step": 1985
    },
    {
        "loss": 1.7497,
        "grad_norm": 1.625207543373108,
        "learning_rate": 5.934758415637636e-05,
        "epoch": 0.5475599669148056,
        "step": 1986
    },
    {
        "loss": 2.0948,
        "grad_norm": 1.430214762687683,
        "learning_rate": 5.9108665996691536e-05,
        "epoch": 0.5478356768679349,
        "step": 1987
    },
    {
        "loss": 1.3698,
        "grad_norm": 2.4097797870635986,
        "learning_rate": 5.887002779048576e-05,
        "epoch": 0.5481113868210642,
        "step": 1988
    },
    {
        "loss": 2.3747,
        "grad_norm": 1.4445109367370605,
        "learning_rate": 5.8631671171542715e-05,
        "epoch": 0.5483870967741935,
        "step": 1989
    },
    {
        "loss": 2.097,
        "grad_norm": 1.7336187362670898,
        "learning_rate": 5.8393597771718336e-05,
        "epoch": 0.5486628067273228,
        "step": 1990
    },
    {
        "loss": 2.1601,
        "grad_norm": 2.009324073791504,
        "learning_rate": 5.815580922092954e-05,
        "epoch": 0.5489385166804521,
        "step": 1991
    },
    {
        "loss": 1.6259,
        "grad_norm": 3.870896577835083,
        "learning_rate": 5.7918307147143036e-05,
        "epoch": 0.5492142266335814,
        "step": 1992
    },
    {
        "loss": 2.4917,
        "grad_norm": 1.5715333223342896,
        "learning_rate": 5.7681093176364274e-05,
        "epoch": 0.5494899365867107,
        "step": 1993
    },
    {
        "loss": 1.4377,
        "grad_norm": 2.054586410522461,
        "learning_rate": 5.744416893262632e-05,
        "epoch": 0.5497656465398401,
        "step": 1994
    },
    {
        "loss": 2.359,
        "grad_norm": 1.0197376012802124,
        "learning_rate": 5.7207536037978595e-05,
        "epoch": 0.5500413564929694,
        "step": 1995
    },
    {
        "loss": 2.0847,
        "grad_norm": 2.0736210346221924,
        "learning_rate": 5.697119611247587e-05,
        "epoch": 0.5503170664460987,
        "step": 1996
    },
    {
        "loss": 2.2014,
        "grad_norm": 1.2057758569717407,
        "learning_rate": 5.673515077416728e-05,
        "epoch": 0.550592776399228,
        "step": 1997
    },
    {
        "loss": 1.9979,
        "grad_norm": 1.6396682262420654,
        "learning_rate": 5.6499401639085e-05,
        "epoch": 0.5508684863523573,
        "step": 1998
    },
    {
        "loss": 1.2333,
        "grad_norm": 1.9459840059280396,
        "learning_rate": 5.626395032123341e-05,
        "epoch": 0.5511441963054866,
        "step": 1999
    },
    {
        "loss": 2.1388,
        "grad_norm": 0.9526737332344055,
        "learning_rate": 5.6028798432577825e-05,
        "epoch": 0.5514199062586159,
        "step": 2000
    },
    {
        "loss": 1.1409,
        "grad_norm": 2.132794141769409,
        "learning_rate": 5.579394758303378e-05,
        "epoch": 0.5516956162117452,
        "step": 2001
    },
    {
        "loss": 2.4387,
        "grad_norm": 1.6145578622817993,
        "learning_rate": 5.5559399380455635e-05,
        "epoch": 0.5519713261648745,
        "step": 2002
    },
    {
        "loss": 1.9147,
        "grad_norm": 1.9253665208816528,
        "learning_rate": 5.53251554306258e-05,
        "epoch": 0.5522470361180039,
        "step": 2003
    },
    {
        "loss": 2.4128,
        "grad_norm": 1.1624999046325684,
        "learning_rate": 5.509121733724374e-05,
        "epoch": 0.5525227460711332,
        "step": 2004
    },
    {
        "loss": 1.9069,
        "grad_norm": 1.4345396757125854,
        "learning_rate": 5.4857586701914895e-05,
        "epoch": 0.5527984560242625,
        "step": 2005
    },
    {
        "loss": 2.4967,
        "grad_norm": 1.3608570098876953,
        "learning_rate": 5.4624265124139715e-05,
        "epoch": 0.5530741659773918,
        "step": 2006
    },
    {
        "loss": 1.2498,
        "grad_norm": 1.482427954673767,
        "learning_rate": 5.439125420130288e-05,
        "epoch": 0.5533498759305211,
        "step": 2007
    },
    {
        "loss": 1.0211,
        "grad_norm": 2.616243362426758,
        "learning_rate": 5.415855552866211e-05,
        "epoch": 0.5536255858836504,
        "step": 2008
    },
    {
        "loss": 1.7284,
        "grad_norm": 1.6314716339111328,
        "learning_rate": 5.392617069933746e-05,
        "epoch": 0.5539012958367797,
        "step": 2009
    },
    {
        "loss": 2.0855,
        "grad_norm": 1.2552690505981445,
        "learning_rate": 5.369410130430024e-05,
        "epoch": 0.554177005789909,
        "step": 2010
    },
    {
        "loss": 2.1792,
        "grad_norm": 1.6367286443710327,
        "learning_rate": 5.3462348932362324e-05,
        "epoch": 0.5544527157430383,
        "step": 2011
    },
    {
        "loss": 0.9849,
        "grad_norm": 2.1733736991882324,
        "learning_rate": 5.3230915170165084e-05,
        "epoch": 0.5547284256961676,
        "step": 2012
    },
    {
        "loss": 1.8804,
        "grad_norm": 1.8648608922958374,
        "learning_rate": 5.2999801602168566e-05,
        "epoch": 0.555004135649297,
        "step": 2013
    },
    {
        "loss": 2.3452,
        "grad_norm": 1.2580198049545288,
        "learning_rate": 5.2769009810640794e-05,
        "epoch": 0.5552798456024263,
        "step": 2014
    },
    {
        "loss": 1.9758,
        "grad_norm": 1.525604486465454,
        "learning_rate": 5.253854137564671e-05,
        "epoch": 0.5555555555555556,
        "step": 2015
    },
    {
        "loss": 1.6595,
        "grad_norm": 2.2608730792999268,
        "learning_rate": 5.23083978750375e-05,
        "epoch": 0.5558312655086849,
        "step": 2016
    },
    {
        "loss": 1.7817,
        "grad_norm": 2.5128583908081055,
        "learning_rate": 5.20785808844398e-05,
        "epoch": 0.5561069754618142,
        "step": 2017
    },
    {
        "loss": 2.3259,
        "grad_norm": 1.3526331186294556,
        "learning_rate": 5.1849091977244835e-05,
        "epoch": 0.5563826854149435,
        "step": 2018
    },
    {
        "loss": 2.0624,
        "grad_norm": 2.148597002029419,
        "learning_rate": 5.161993272459764e-05,
        "epoch": 0.5566583953680728,
        "step": 2019
    },
    {
        "loss": 2.621,
        "grad_norm": 0.9719493985176086,
        "learning_rate": 5.139110469538638e-05,
        "epoch": 0.5569341053212021,
        "step": 2020
    },
    {
        "loss": 2.2041,
        "grad_norm": 1.520530104637146,
        "learning_rate": 5.116260945623164e-05,
        "epoch": 0.5572098152743314,
        "step": 2021
    },
    {
        "loss": 1.8268,
        "grad_norm": 1.612718939781189,
        "learning_rate": 5.09344485714755e-05,
        "epoch": 0.5574855252274608,
        "step": 2022
    },
    {
        "loss": 2.1873,
        "grad_norm": 1.30860435962677,
        "learning_rate": 5.070662360317101e-05,
        "epoch": 0.5577612351805901,
        "step": 2023
    },
    {
        "loss": 1.4469,
        "grad_norm": 1.9569584131240845,
        "learning_rate": 5.047913611107153e-05,
        "epoch": 0.5580369451337194,
        "step": 2024
    },
    {
        "loss": 1.638,
        "grad_norm": 1.8249701261520386,
        "learning_rate": 5.0251987652619835e-05,
        "epoch": 0.5583126550868487,
        "step": 2025
    },
    {
        "loss": 0.9915,
        "grad_norm": 2.2933883666992188,
        "learning_rate": 5.0025179782937645e-05,
        "epoch": 0.558588365039978,
        "step": 2026
    },
    {
        "loss": 2.2132,
        "grad_norm": 1.1117578744888306,
        "learning_rate": 4.979871405481486e-05,
        "epoch": 0.5588640749931072,
        "step": 2027
    },
    {
        "loss": 1.3202,
        "grad_norm": 2.207653522491455,
        "learning_rate": 4.9572592018699105e-05,
        "epoch": 0.5591397849462365,
        "step": 2028
    },
    {
        "loss": 1.5457,
        "grad_norm": 2.166743755340576,
        "learning_rate": 4.9346815222684875e-05,
        "epoch": 0.5594154948993658,
        "step": 2029
    },
    {
        "loss": 2.3574,
        "grad_norm": 1.5483999252319336,
        "learning_rate": 4.9121385212503056e-05,
        "epoch": 0.5596912048524951,
        "step": 2030
    },
    {
        "loss": 1.7768,
        "grad_norm": 1.5865669250488281,
        "learning_rate": 4.889630353151046e-05,
        "epoch": 0.5599669148056244,
        "step": 2031
    },
    {
        "loss": 1.8598,
        "grad_norm": 1.257306456565857,
        "learning_rate": 4.867157172067902e-05,
        "epoch": 0.5602426247587537,
        "step": 2032
    },
    {
        "loss": 1.8157,
        "grad_norm": 1.8243690729141235,
        "learning_rate": 4.844719131858536e-05,
        "epoch": 0.5605183347118831,
        "step": 2033
    },
    {
        "loss": 1.4491,
        "grad_norm": 2.498903274536133,
        "learning_rate": 4.822316386140039e-05,
        "epoch": 0.5607940446650124,
        "step": 2034
    },
    {
        "loss": 2.0096,
        "grad_norm": 1.8483219146728516,
        "learning_rate": 4.799949088287852e-05,
        "epoch": 0.5610697546181417,
        "step": 2035
    },
    {
        "loss": 2.4128,
        "grad_norm": 1.1593613624572754,
        "learning_rate": 4.777617391434736e-05,
        "epoch": 0.561345464571271,
        "step": 2036
    },
    {
        "loss": 1.5325,
        "grad_norm": 2.045290946960449,
        "learning_rate": 4.755321448469713e-05,
        "epoch": 0.5616211745244003,
        "step": 2037
    },
    {
        "loss": 2.1886,
        "grad_norm": 1.5144264698028564,
        "learning_rate": 4.7330614120370363e-05,
        "epoch": 0.5618968844775296,
        "step": 2038
    },
    {
        "loss": 1.9319,
        "grad_norm": 1.4141597747802734,
        "learning_rate": 4.71083743453512e-05,
        "epoch": 0.5621725944306589,
        "step": 2039
    },
    {
        "loss": 1.9191,
        "grad_norm": 1.7276099920272827,
        "learning_rate": 4.688649668115509e-05,
        "epoch": 0.5624483043837882,
        "step": 2040
    },
    {
        "loss": 2.4684,
        "grad_norm": 2.1251983642578125,
        "learning_rate": 4.6664982646818515e-05,
        "epoch": 0.5627240143369175,
        "step": 2041
    },
    {
        "loss": 1.9399,
        "grad_norm": 2.294384717941284,
        "learning_rate": 4.644383375888828e-05,
        "epoch": 0.5629997242900469,
        "step": 2042
    },
    {
        "loss": 1.8823,
        "grad_norm": 2.136218786239624,
        "learning_rate": 4.622305153141132e-05,
        "epoch": 0.5632754342431762,
        "step": 2043
    },
    {
        "loss": 1.875,
        "grad_norm": 1.6134716272354126,
        "learning_rate": 4.600263747592445e-05,
        "epoch": 0.5635511441963055,
        "step": 2044
    },
    {
        "loss": 1.9306,
        "grad_norm": 1.6908434629440308,
        "learning_rate": 4.578259310144373e-05,
        "epoch": 0.5638268541494348,
        "step": 2045
    },
    {
        "loss": 1.4934,
        "grad_norm": 3.54632568359375,
        "learning_rate": 4.556291991445432e-05,
        "epoch": 0.5641025641025641,
        "step": 2046
    },
    {
        "loss": 2.2419,
        "grad_norm": 1.7206746339797974,
        "learning_rate": 4.53436194189001e-05,
        "epoch": 0.5643782740556934,
        "step": 2047
    },
    {
        "loss": 2.3678,
        "grad_norm": 1.1001733541488647,
        "learning_rate": 4.512469311617351e-05,
        "epoch": 0.5646539840088227,
        "step": 2048
    },
    {
        "loss": 1.4497,
        "grad_norm": 2.1284852027893066,
        "learning_rate": 4.490614250510507e-05,
        "epoch": 0.564929693961952,
        "step": 2049
    },
    {
        "loss": 2.3561,
        "grad_norm": 1.7678923606872559,
        "learning_rate": 4.468796908195315e-05,
        "epoch": 0.5652054039150813,
        "step": 2050
    },
    {
        "loss": 2.3592,
        "grad_norm": 1.3663362264633179,
        "learning_rate": 4.4470174340393975e-05,
        "epoch": 0.5654811138682106,
        "step": 2051
    },
    {
        "loss": 1.0004,
        "grad_norm": 1.8719629049301147,
        "learning_rate": 4.425275977151093e-05,
        "epoch": 0.56575682382134,
        "step": 2052
    },
    {
        "loss": 1.7669,
        "grad_norm": 1.7656632661819458,
        "learning_rate": 4.403572686378499e-05,
        "epoch": 0.5660325337744693,
        "step": 2053
    },
    {
        "loss": 2.3052,
        "grad_norm": 1.1929926872253418,
        "learning_rate": 4.381907710308395e-05,
        "epoch": 0.5663082437275986,
        "step": 2054
    },
    {
        "loss": 2.1999,
        "grad_norm": 1.3225170373916626,
        "learning_rate": 4.360281197265246e-05,
        "epoch": 0.5665839536807279,
        "step": 2055
    },
    {
        "loss": 2.0732,
        "grad_norm": 2.069035530090332,
        "learning_rate": 4.3386932953101886e-05,
        "epoch": 0.5668596636338572,
        "step": 2056
    },
    {
        "loss": 2.2866,
        "grad_norm": 1.2812458276748657,
        "learning_rate": 4.3171441522400316e-05,
        "epoch": 0.5671353735869865,
        "step": 2057
    },
    {
        "loss": 2.2475,
        "grad_norm": 1.6281911134719849,
        "learning_rate": 4.295633915586213e-05,
        "epoch": 0.5674110835401158,
        "step": 2058
    },
    {
        "loss": 2.1796,
        "grad_norm": 1.6283410787582397,
        "learning_rate": 4.2741627326138135e-05,
        "epoch": 0.5676867934932451,
        "step": 2059
    },
    {
        "loss": 2.5792,
        "grad_norm": 2.3537697792053223,
        "learning_rate": 4.252730750320538e-05,
        "epoch": 0.5679625034463744,
        "step": 2060
    },
    {
        "loss": 2.7767,
        "grad_norm": 0.9223921895027161,
        "learning_rate": 4.2313381154357235e-05,
        "epoch": 0.5682382133995038,
        "step": 2061
    },
    {
        "loss": 2.1799,
        "grad_norm": 1.513360619544983,
        "learning_rate": 4.209984974419312e-05,
        "epoch": 0.5685139233526331,
        "step": 2062
    },
    {
        "loss": 2.5377,
        "grad_norm": 1.7511061429977417,
        "learning_rate": 4.188671473460858e-05,
        "epoch": 0.5687896333057624,
        "step": 2063
    },
    {
        "loss": 1.9909,
        "grad_norm": 1.5094420909881592,
        "learning_rate": 4.1673977584785426e-05,
        "epoch": 0.5690653432588917,
        "step": 2064
    },
    {
        "loss": 2.1793,
        "grad_norm": 1.6907967329025269,
        "learning_rate": 4.14616397511815e-05,
        "epoch": 0.569341053212021,
        "step": 2065
    },
    {
        "loss": 1.5244,
        "grad_norm": 2.052013874053955,
        "learning_rate": 4.12497026875208e-05,
        "epoch": 0.5696167631651503,
        "step": 2066
    },
    {
        "loss": 2.313,
        "grad_norm": 1.5429530143737793,
        "learning_rate": 4.103816784478365e-05,
        "epoch": 0.5698924731182796,
        "step": 2067
    },
    {
        "loss": 0.9887,
        "grad_norm": 2.824737310409546,
        "learning_rate": 4.0827036671196564e-05,
        "epoch": 0.5701681830714089,
        "step": 2068
    },
    {
        "loss": 2.3088,
        "grad_norm": 1.6638696193695068,
        "learning_rate": 4.061631061222243e-05,
        "epoch": 0.5704438930245382,
        "step": 2069
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.0074188709259033,
        "learning_rate": 4.0405991110550615e-05,
        "epoch": 0.5707196029776674,
        "step": 2070
    },
    {
        "loss": 1.9035,
        "grad_norm": 1.8140687942504883,
        "learning_rate": 4.019607960608716e-05,
        "epoch": 0.5709953129307968,
        "step": 2071
    },
    {
        "loss": 1.9669,
        "grad_norm": 3.031019926071167,
        "learning_rate": 3.9986577535944734e-05,
        "epoch": 0.5712710228839261,
        "step": 2072
    },
    {
        "loss": 2.168,
        "grad_norm": 2.1395199298858643,
        "learning_rate": 3.9777486334432926e-05,
        "epoch": 0.5715467328370554,
        "step": 2073
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.5694125890731812,
        "learning_rate": 3.956880743304848e-05,
        "epoch": 0.5718224427901847,
        "step": 2074
    },
    {
        "loss": 2.3705,
        "grad_norm": 1.9346673488616943,
        "learning_rate": 3.936054226046535e-05,
        "epoch": 0.572098152743314,
        "step": 2075
    },
    {
        "loss": 2.2173,
        "grad_norm": 1.4458805322647095,
        "learning_rate": 3.915269224252499e-05,
        "epoch": 0.5723738626964433,
        "step": 2076
    },
    {
        "loss": 1.7013,
        "grad_norm": 1.8360121250152588,
        "learning_rate": 3.894525880222657e-05,
        "epoch": 0.5726495726495726,
        "step": 2077
    },
    {
        "loss": 1.9127,
        "grad_norm": 1.9774810075759888,
        "learning_rate": 3.873824335971736e-05,
        "epoch": 0.5729252826027019,
        "step": 2078
    },
    {
        "loss": 1.8328,
        "grad_norm": 2.3447539806365967,
        "learning_rate": 3.853164733228278e-05,
        "epoch": 0.5732009925558312,
        "step": 2079
    },
    {
        "loss": 2.0022,
        "grad_norm": 1.5578373670578003,
        "learning_rate": 3.8325472134336824e-05,
        "epoch": 0.5734767025089605,
        "step": 2080
    },
    {
        "loss": 1.4447,
        "grad_norm": 1.9645860195159912,
        "learning_rate": 3.8119719177412507e-05,
        "epoch": 0.5737524124620899,
        "step": 2081
    },
    {
        "loss": 2.071,
        "grad_norm": 1.737421989440918,
        "learning_rate": 3.79143898701519e-05,
        "epoch": 0.5740281224152192,
        "step": 2082
    },
    {
        "loss": 2.1725,
        "grad_norm": 1.04341459274292,
        "learning_rate": 3.770948561829669e-05,
        "epoch": 0.5743038323683485,
        "step": 2083
    },
    {
        "loss": 2.3413,
        "grad_norm": 1.995922565460205,
        "learning_rate": 3.7505007824678616e-05,
        "epoch": 0.5745795423214778,
        "step": 2084
    },
    {
        "loss": 2.0456,
        "grad_norm": 1.5133792161941528,
        "learning_rate": 3.7300957889209655e-05,
        "epoch": 0.5748552522746071,
        "step": 2085
    },
    {
        "loss": 1.9076,
        "grad_norm": 2.4048984050750732,
        "learning_rate": 3.709733720887259e-05,
        "epoch": 0.5751309622277364,
        "step": 2086
    },
    {
        "loss": 2.1995,
        "grad_norm": 1.4897050857543945,
        "learning_rate": 3.6894147177711346e-05,
        "epoch": 0.5754066721808657,
        "step": 2087
    },
    {
        "loss": 2.1869,
        "grad_norm": 1.5647690296173096,
        "learning_rate": 3.6691389186821643e-05,
        "epoch": 0.575682382133995,
        "step": 2088
    },
    {
        "loss": 2.1894,
        "grad_norm": 1.283636212348938,
        "learning_rate": 3.648906462434119e-05,
        "epoch": 0.5759580920871243,
        "step": 2089
    },
    {
        "loss": 2.0606,
        "grad_norm": 1.7562044858932495,
        "learning_rate": 3.6287174875440375e-05,
        "epoch": 0.5762338020402537,
        "step": 2090
    },
    {
        "loss": 2.1498,
        "grad_norm": 1.6597915887832642,
        "learning_rate": 3.6085721322312784e-05,
        "epoch": 0.576509511993383,
        "step": 2091
    },
    {
        "loss": 2.0238,
        "grad_norm": 1.2081407308578491,
        "learning_rate": 3.5884705344165606e-05,
        "epoch": 0.5767852219465123,
        "step": 2092
    },
    {
        "loss": 1.3747,
        "grad_norm": 2.1718149185180664,
        "learning_rate": 3.568412831721025e-05,
        "epoch": 0.5770609318996416,
        "step": 2093
    },
    {
        "loss": 1.8352,
        "grad_norm": 2.140721559524536,
        "learning_rate": 3.548399161465309e-05,
        "epoch": 0.5773366418527709,
        "step": 2094
    },
    {
        "loss": 1.4192,
        "grad_norm": 1.85344660282135,
        "learning_rate": 3.528429660668575e-05,
        "epoch": 0.5776123518059002,
        "step": 2095
    },
    {
        "loss": 2.0587,
        "grad_norm": 1.5253040790557861,
        "learning_rate": 3.5085044660475995e-05,
        "epoch": 0.5778880617590295,
        "step": 2096
    },
    {
        "loss": 1.5319,
        "grad_norm": 1.6899685859680176,
        "learning_rate": 3.4886237140158175e-05,
        "epoch": 0.5781637717121588,
        "step": 2097
    },
    {
        "loss": 1.4747,
        "grad_norm": 1.9248889684677124,
        "learning_rate": 3.46878754068241e-05,
        "epoch": 0.5784394816652881,
        "step": 2098
    },
    {
        "loss": 1.7096,
        "grad_norm": 1.3067866563796997,
        "learning_rate": 3.44899608185135e-05,
        "epoch": 0.5787151916184174,
        "step": 2099
    },
    {
        "loss": 2.5622,
        "grad_norm": 1.4873390197753906,
        "learning_rate": 3.4292494730204804e-05,
        "epoch": 0.5789909015715468,
        "step": 2100
    },
    {
        "loss": 1.6967,
        "grad_norm": 2.0286505222320557,
        "learning_rate": 3.4095478493806024e-05,
        "epoch": 0.5792666115246761,
        "step": 2101
    },
    {
        "loss": 1.704,
        "grad_norm": 1.7604905366897583,
        "learning_rate": 3.389891345814523e-05,
        "epoch": 0.5795423214778054,
        "step": 2102
    },
    {
        "loss": 1.9516,
        "grad_norm": 1.1181981563568115,
        "learning_rate": 3.370280096896145e-05,
        "epoch": 0.5798180314309347,
        "step": 2103
    },
    {
        "loss": 1.7822,
        "grad_norm": 1.6366349458694458,
        "learning_rate": 3.350714236889555e-05,
        "epoch": 0.580093741384064,
        "step": 2104
    },
    {
        "loss": 2.4911,
        "grad_norm": 1.7949085235595703,
        "learning_rate": 3.331193899748087e-05,
        "epoch": 0.5803694513371933,
        "step": 2105
    },
    {
        "loss": 2.0917,
        "grad_norm": 2.0612423419952393,
        "learning_rate": 3.311719219113413e-05,
        "epoch": 0.5806451612903226,
        "step": 2106
    },
    {
        "loss": 2.5013,
        "grad_norm": 0.9702154397964478,
        "learning_rate": 3.2922903283146246e-05,
        "epoch": 0.5809208712434519,
        "step": 2107
    },
    {
        "loss": 2.2563,
        "grad_norm": 1.5546104907989502,
        "learning_rate": 3.2729073603673366e-05,
        "epoch": 0.5811965811965812,
        "step": 2108
    },
    {
        "loss": 2.0909,
        "grad_norm": 1.6459929943084717,
        "learning_rate": 3.253570447972749e-05,
        "epoch": 0.5814722911497106,
        "step": 2109
    },
    {
        "loss": 2.5965,
        "grad_norm": 1.5266090631484985,
        "learning_rate": 3.234279723516757e-05,
        "epoch": 0.5817480011028399,
        "step": 2110
    },
    {
        "loss": 1.9834,
        "grad_norm": 1.2282764911651611,
        "learning_rate": 3.2150353190690455e-05,
        "epoch": 0.5820237110559692,
        "step": 2111
    },
    {
        "loss": 1.6774,
        "grad_norm": 2.0253570079803467,
        "learning_rate": 3.195837366382174e-05,
        "epoch": 0.5822994210090985,
        "step": 2112
    },
    {
        "loss": 2.0253,
        "grad_norm": 1.4667668342590332,
        "learning_rate": 3.17668599689068e-05,
        "epoch": 0.5825751309622277,
        "step": 2113
    },
    {
        "loss": 1.402,
        "grad_norm": 1.8841488361358643,
        "learning_rate": 3.15758134171018e-05,
        "epoch": 0.582850840915357,
        "step": 2114
    },
    {
        "loss": 2.0101,
        "grad_norm": 0.9826062321662903,
        "learning_rate": 3.138523531636477e-05,
        "epoch": 0.5831265508684863,
        "step": 2115
    },
    {
        "loss": 1.8208,
        "grad_norm": 1.1601115465164185,
        "learning_rate": 3.119512697144654e-05,
        "epoch": 0.5834022608216156,
        "step": 2116
    },
    {
        "loss": 2.4187,
        "grad_norm": 1.0357930660247803,
        "learning_rate": 3.1005489683881814e-05,
        "epoch": 0.5836779707747449,
        "step": 2117
    },
    {
        "loss": 2.5934,
        "grad_norm": 2.2734062671661377,
        "learning_rate": 3.081632475198043e-05,
        "epoch": 0.5839536807278742,
        "step": 2118
    },
    {
        "loss": 2.2673,
        "grad_norm": 1.0469940900802612,
        "learning_rate": 3.0627633470818264e-05,
        "epoch": 0.5842293906810035,
        "step": 2119
    },
    {
        "loss": 2.2527,
        "grad_norm": 1.3872789144515991,
        "learning_rate": 3.0439417132228408e-05,
        "epoch": 0.5845051006341329,
        "step": 2120
    },
    {
        "loss": 2.1438,
        "grad_norm": 1.4287861585617065,
        "learning_rate": 3.025167702479249e-05,
        "epoch": 0.5847808105872622,
        "step": 2121
    },
    {
        "loss": 1.9126,
        "grad_norm": 2.2744901180267334,
        "learning_rate": 3.0064414433831622e-05,
        "epoch": 0.5850565205403915,
        "step": 2122
    },
    {
        "loss": 2.212,
        "grad_norm": 1.5473707914352417,
        "learning_rate": 2.9877630641397748e-05,
        "epoch": 0.5853322304935208,
        "step": 2123
    },
    {
        "loss": 2.469,
        "grad_norm": 1.3832648992538452,
        "learning_rate": 2.969132692626476e-05,
        "epoch": 0.5856079404466501,
        "step": 2124
    },
    {
        "loss": 1.3547,
        "grad_norm": 2.1246941089630127,
        "learning_rate": 2.9505504563919938e-05,
        "epoch": 0.5858836503997794,
        "step": 2125
    },
    {
        "loss": 2.1025,
        "grad_norm": 1.6215893030166626,
        "learning_rate": 2.9320164826554986e-05,
        "epoch": 0.5861593603529087,
        "step": 2126
    },
    {
        "loss": 2.4612,
        "grad_norm": 1.0516278743743896,
        "learning_rate": 2.9135308983057396e-05,
        "epoch": 0.586435070306038,
        "step": 2127
    },
    {
        "loss": 1.9257,
        "grad_norm": 1.111746907234192,
        "learning_rate": 2.8950938299001907e-05,
        "epoch": 0.5867107802591673,
        "step": 2128
    },
    {
        "loss": 1.9219,
        "grad_norm": 1.8945564031600952,
        "learning_rate": 2.8767054036641627e-05,
        "epoch": 0.5869864902122967,
        "step": 2129
    },
    {
        "loss": 1.6524,
        "grad_norm": 2.1949493885040283,
        "learning_rate": 2.858365745489946e-05,
        "epoch": 0.587262200165426,
        "step": 2130
    },
    {
        "loss": 1.9109,
        "grad_norm": 1.0822337865829468,
        "learning_rate": 2.840074980935965e-05,
        "epoch": 0.5875379101185553,
        "step": 2131
    },
    {
        "loss": 1.4855,
        "grad_norm": 2.3868300914764404,
        "learning_rate": 2.8218332352258903e-05,
        "epoch": 0.5878136200716846,
        "step": 2132
    },
    {
        "loss": 2.39,
        "grad_norm": 1.3002216815948486,
        "learning_rate": 2.8036406332478038e-05,
        "epoch": 0.5880893300248139,
        "step": 2133
    },
    {
        "loss": 1.8861,
        "grad_norm": 1.4147875308990479,
        "learning_rate": 2.785497299553328e-05,
        "epoch": 0.5883650399779432,
        "step": 2134
    },
    {
        "loss": 0.7977,
        "grad_norm": 1.7754006385803223,
        "learning_rate": 2.767403358356795e-05,
        "epoch": 0.5886407499310725,
        "step": 2135
    },
    {
        "loss": 2.0188,
        "grad_norm": 1.396259069442749,
        "learning_rate": 2.7493589335343685e-05,
        "epoch": 0.5889164598842018,
        "step": 2136
    },
    {
        "loss": 2.4645,
        "grad_norm": 1.3920397758483887,
        "learning_rate": 2.7313641486232112e-05,
        "epoch": 0.5891921698373311,
        "step": 2137
    },
    {
        "loss": 2.0702,
        "grad_norm": 1.896933913230896,
        "learning_rate": 2.7134191268206478e-05,
        "epoch": 0.5894678797904604,
        "step": 2138
    },
    {
        "loss": 1.4467,
        "grad_norm": 2.194573163986206,
        "learning_rate": 2.6955239909832985e-05,
        "epoch": 0.5897435897435898,
        "step": 2139
    },
    {
        "loss": 2.0302,
        "grad_norm": 1.1729182004928589,
        "learning_rate": 2.677678863626253e-05,
        "epoch": 0.5900192996967191,
        "step": 2140
    },
    {
        "loss": 2.3921,
        "grad_norm": 1.0825941562652588,
        "learning_rate": 2.659883866922237e-05,
        "epoch": 0.5902950096498484,
        "step": 2141
    },
    {
        "loss": 2.1508,
        "grad_norm": 1.6441667079925537,
        "learning_rate": 2.6421391227007587e-05,
        "epoch": 0.5905707196029777,
        "step": 2142
    },
    {
        "loss": 2.0748,
        "grad_norm": 1.1395559310913086,
        "learning_rate": 2.6244447524472858e-05,
        "epoch": 0.590846429556107,
        "step": 2143
    },
    {
        "loss": 2.2449,
        "grad_norm": 1.430862307548523,
        "learning_rate": 2.606800877302411e-05,
        "epoch": 0.5911221395092363,
        "step": 2144
    },
    {
        "loss": 2.2193,
        "grad_norm": 1.554182767868042,
        "learning_rate": 2.5892076180610315e-05,
        "epoch": 0.5913978494623656,
        "step": 2145
    },
    {
        "loss": 2.3827,
        "grad_norm": 0.9855043888092041,
        "learning_rate": 2.571665095171504e-05,
        "epoch": 0.5916735594154949,
        "step": 2146
    },
    {
        "loss": 2.1047,
        "grad_norm": 1.7293336391448975,
        "learning_rate": 2.5541734287348297e-05,
        "epoch": 0.5919492693686242,
        "step": 2147
    },
    {
        "loss": 1.6952,
        "grad_norm": 1.998633861541748,
        "learning_rate": 2.536732738503843e-05,
        "epoch": 0.5922249793217536,
        "step": 2148
    },
    {
        "loss": 1.0093,
        "grad_norm": 2.295746326446533,
        "learning_rate": 2.519343143882372e-05,
        "epoch": 0.5925006892748829,
        "step": 2149
    },
    {
        "loss": 1.3189,
        "grad_norm": 2.1818203926086426,
        "learning_rate": 2.5020047639244327e-05,
        "epoch": 0.5927763992280122,
        "step": 2150
    },
    {
        "loss": 2.1407,
        "grad_norm": 1.5064994096755981,
        "learning_rate": 2.4847177173334046e-05,
        "epoch": 0.5930521091811415,
        "step": 2151
    },
    {
        "loss": 2.3712,
        "grad_norm": 1.1590842008590698,
        "learning_rate": 2.46748212246124e-05,
        "epoch": 0.5933278191342708,
        "step": 2152
    },
    {
        "loss": 2.409,
        "grad_norm": 1.5299749374389648,
        "learning_rate": 2.4502980973076252e-05,
        "epoch": 0.5936035290874001,
        "step": 2153
    },
    {
        "loss": 2.1598,
        "grad_norm": 2.403830051422119,
        "learning_rate": 2.433165759519186e-05,
        "epoch": 0.5938792390405294,
        "step": 2154
    },
    {
        "loss": 1.4551,
        "grad_norm": 1.776965618133545,
        "learning_rate": 2.4160852263886934e-05,
        "epoch": 0.5941549489936586,
        "step": 2155
    },
    {
        "loss": 1.9605,
        "grad_norm": 2.1932153701782227,
        "learning_rate": 2.399056614854237e-05,
        "epoch": 0.5944306589467879,
        "step": 2156
    },
    {
        "loss": 2.2889,
        "grad_norm": 1.3133037090301514,
        "learning_rate": 2.38208004149844e-05,
        "epoch": 0.5947063688999172,
        "step": 2157
    },
    {
        "loss": 1.7818,
        "grad_norm": 1.492964267730713,
        "learning_rate": 2.3651556225476635e-05,
        "epoch": 0.5949820788530465,
        "step": 2158
    },
    {
        "loss": 1.8111,
        "grad_norm": 1.3412678241729736,
        "learning_rate": 2.348283473871199e-05,
        "epoch": 0.5952577888061759,
        "step": 2159
    },
    {
        "loss": 2.4508,
        "grad_norm": 1.0029940605163574,
        "learning_rate": 2.3314637109804828e-05,
        "epoch": 0.5955334987593052,
        "step": 2160
    },
    {
        "loss": 1.505,
        "grad_norm": 2.478435516357422,
        "learning_rate": 2.3146964490282997e-05,
        "epoch": 0.5958092087124345,
        "step": 2161
    },
    {
        "loss": 1.7128,
        "grad_norm": 1.9413381814956665,
        "learning_rate": 2.297981802808008e-05,
        "epoch": 0.5960849186655638,
        "step": 2162
    },
    {
        "loss": 1.9573,
        "grad_norm": 1.868550181388855,
        "learning_rate": 2.281319886752735e-05,
        "epoch": 0.5963606286186931,
        "step": 2163
    },
    {
        "loss": 2.4689,
        "grad_norm": 1.1597363948822021,
        "learning_rate": 2.264710814934604e-05,
        "epoch": 0.5966363385718224,
        "step": 2164
    },
    {
        "loss": 1.4571,
        "grad_norm": 1.8290390968322754,
        "learning_rate": 2.248154701063959e-05,
        "epoch": 0.5969120485249517,
        "step": 2165
    },
    {
        "loss": 1.9087,
        "grad_norm": 1.7562016248703003,
        "learning_rate": 2.2316516584885716e-05,
        "epoch": 0.597187758478081,
        "step": 2166
    },
    {
        "loss": 1.9869,
        "grad_norm": 1.7766395807266235,
        "learning_rate": 2.215201800192871e-05,
        "epoch": 0.5974634684312103,
        "step": 2167
    },
    {
        "loss": 2.2755,
        "grad_norm": 1.345882534980774,
        "learning_rate": 2.1988052387971802e-05,
        "epoch": 0.5977391783843397,
        "step": 2168
    },
    {
        "loss": 2.2407,
        "grad_norm": 1.360931158065796,
        "learning_rate": 2.1824620865569324e-05,
        "epoch": 0.598014888337469,
        "step": 2169
    },
    {
        "loss": 2.4794,
        "grad_norm": 0.9859147071838379,
        "learning_rate": 2.1661724553619035e-05,
        "epoch": 0.5982905982905983,
        "step": 2170
    },
    {
        "loss": 1.9133,
        "grad_norm": 1.2597383260726929,
        "learning_rate": 2.1499364567354517e-05,
        "epoch": 0.5985663082437276,
        "step": 2171
    },
    {
        "loss": 2.3171,
        "grad_norm": 1.516716480255127,
        "learning_rate": 2.1337542018337588e-05,
        "epoch": 0.5988420181968569,
        "step": 2172
    },
    {
        "loss": 2.3301,
        "grad_norm": 1.2248464822769165,
        "learning_rate": 2.1176258014450512e-05,
        "epoch": 0.5991177281499862,
        "step": 2173
    },
    {
        "loss": 2.3218,
        "grad_norm": 1.0807381868362427,
        "learning_rate": 2.101551365988854e-05,
        "epoch": 0.5993934381031155,
        "step": 2174
    },
    {
        "loss": 2.2345,
        "grad_norm": 1.4870871305465698,
        "learning_rate": 2.0855310055152444e-05,
        "epoch": 0.5996691480562448,
        "step": 2175
    },
    {
        "loss": 1.4212,
        "grad_norm": 2.0892410278320312,
        "learning_rate": 2.0695648297040736e-05,
        "epoch": 0.5999448580093741,
        "step": 2176
    },
    {
        "loss": 2.4697,
        "grad_norm": 0.9497575759887695,
        "learning_rate": 2.053652947864233e-05,
        "epoch": 0.6002205679625034,
        "step": 2177
    },
    {
        "loss": 2.1852,
        "grad_norm": 1.4236196279525757,
        "learning_rate": 2.037795468932909e-05,
        "epoch": 0.6004962779156328,
        "step": 2178
    },
    {
        "loss": 2.5139,
        "grad_norm": 1.5733791589736938,
        "learning_rate": 2.0219925014748254e-05,
        "epoch": 0.6007719878687621,
        "step": 2179
    },
    {
        "loss": 1.8965,
        "grad_norm": 2.1890523433685303,
        "learning_rate": 2.0062441536815023e-05,
        "epoch": 0.6010476978218914,
        "step": 2180
    },
    {
        "loss": 1.4236,
        "grad_norm": 1.6930451393127441,
        "learning_rate": 1.9905505333705223e-05,
        "epoch": 0.6013234077750207,
        "step": 2181
    },
    {
        "loss": 2.2869,
        "grad_norm": 1.0714612007141113,
        "learning_rate": 1.9749117479847902e-05,
        "epoch": 0.60159911772815,
        "step": 2182
    },
    {
        "loss": 1.8015,
        "grad_norm": 1.823770523071289,
        "learning_rate": 1.959327904591791e-05,
        "epoch": 0.6018748276812793,
        "step": 2183
    },
    {
        "loss": 1.9111,
        "grad_norm": 1.5245975255966187,
        "learning_rate": 1.9437991098828624e-05,
        "epoch": 0.6021505376344086,
        "step": 2184
    },
    {
        "loss": 2.7257,
        "grad_norm": 1.3776094913482666,
        "learning_rate": 1.9283254701724696e-05,
        "epoch": 0.6024262475875379,
        "step": 2185
    },
    {
        "loss": 0.9275,
        "grad_norm": 1.9932795763015747,
        "learning_rate": 1.9129070913974623e-05,
        "epoch": 0.6027019575406672,
        "step": 2186
    },
    {
        "loss": 1.6363,
        "grad_norm": 1.4951473474502563,
        "learning_rate": 1.8975440791163634e-05,
        "epoch": 0.6029776674937966,
        "step": 2187
    },
    {
        "loss": 1.5316,
        "grad_norm": 2.0602409839630127,
        "learning_rate": 1.882236538508638e-05,
        "epoch": 0.6032533774469259,
        "step": 2188
    },
    {
        "loss": 1.6028,
        "grad_norm": 2.187009811401367,
        "learning_rate": 1.8669845743739846e-05,
        "epoch": 0.6035290874000552,
        "step": 2189
    },
    {
        "loss": 2.0652,
        "grad_norm": 1.8911701440811157,
        "learning_rate": 1.8517882911315997e-05,
        "epoch": 0.6038047973531845,
        "step": 2190
    },
    {
        "loss": 1.7526,
        "grad_norm": 2.5836424827575684,
        "learning_rate": 1.8366477928194768e-05,
        "epoch": 0.6040805073063138,
        "step": 2191
    },
    {
        "loss": 1.8422,
        "grad_norm": 2.0369365215301514,
        "learning_rate": 1.8215631830936953e-05,
        "epoch": 0.6043562172594431,
        "step": 2192
    },
    {
        "loss": 2.2456,
        "grad_norm": 1.3424807786941528,
        "learning_rate": 1.8065345652276977e-05,
        "epoch": 0.6046319272125724,
        "step": 2193
    },
    {
        "loss": 1.7301,
        "grad_norm": 2.139209032058716,
        "learning_rate": 1.791562042111592e-05,
        "epoch": 0.6049076371657017,
        "step": 2194
    },
    {
        "loss": 1.9731,
        "grad_norm": 1.947672724723816,
        "learning_rate": 1.776645716251454e-05,
        "epoch": 0.605183347118831,
        "step": 2195
    },
    {
        "loss": 2.0503,
        "grad_norm": 1.1698187589645386,
        "learning_rate": 1.761785689768608e-05,
        "epoch": 0.6054590570719603,
        "step": 2196
    },
    {
        "loss": 1.1123,
        "grad_norm": 2.0453054904937744,
        "learning_rate": 1.74698206439894e-05,
        "epoch": 0.6057347670250897,
        "step": 2197
    },
    {
        "loss": 2.1468,
        "grad_norm": 1.876061201095581,
        "learning_rate": 1.7322349414921968e-05,
        "epoch": 0.6060104769782189,
        "step": 2198
    },
    {
        "loss": 1.7339,
        "grad_norm": 2.1264426708221436,
        "learning_rate": 1.7175444220113013e-05,
        "epoch": 0.6062861869313482,
        "step": 2199
    },
    {
        "loss": 2.4184,
        "grad_norm": 1.3604564666748047,
        "learning_rate": 1.7029106065316434e-05,
        "epoch": 0.6065618968844775,
        "step": 2200
    },
    {
        "loss": 2.159,
        "grad_norm": 1.1400333642959595,
        "learning_rate": 1.6883335952404046e-05,
        "epoch": 0.6068376068376068,
        "step": 2201
    },
    {
        "loss": 1.9927,
        "grad_norm": 2.1234052181243896,
        "learning_rate": 1.6738134879358748e-05,
        "epoch": 0.6071133167907361,
        "step": 2202
    },
    {
        "loss": 2.3328,
        "grad_norm": 1.0563822984695435,
        "learning_rate": 1.6593503840267564e-05,
        "epoch": 0.6073890267438654,
        "step": 2203
    },
    {
        "loss": 2.3296,
        "grad_norm": 1.6793756484985352,
        "learning_rate": 1.6449443825314914e-05,
        "epoch": 0.6076647366969947,
        "step": 2204
    },
    {
        "loss": 1.8478,
        "grad_norm": 1.7167125940322876,
        "learning_rate": 1.6305955820775888e-05,
        "epoch": 0.607940446650124,
        "step": 2205
    },
    {
        "loss": 1.6271,
        "grad_norm": 1.7922617197036743,
        "learning_rate": 1.6163040809009368e-05,
        "epoch": 0.6082161566032533,
        "step": 2206
    },
    {
        "loss": 2.3049,
        "grad_norm": 1.9303284883499146,
        "learning_rate": 1.6020699768451398e-05,
        "epoch": 0.6084918665563827,
        "step": 2207
    },
    {
        "loss": 2.1494,
        "grad_norm": 1.119463324546814,
        "learning_rate": 1.5878933673608408e-05,
        "epoch": 0.608767576509512,
        "step": 2208
    },
    {
        "loss": 2.1444,
        "grad_norm": 1.4451477527618408,
        "learning_rate": 1.5737743495050684e-05,
        "epoch": 0.6090432864626413,
        "step": 2209
    },
    {
        "loss": 2.2324,
        "grad_norm": 1.0884082317352295,
        "learning_rate": 1.559713019940555e-05,
        "epoch": 0.6093189964157706,
        "step": 2210
    },
    {
        "loss": 2.2919,
        "grad_norm": 0.9277915954589844,
        "learning_rate": 1.545709474935082e-05,
        "epoch": 0.6095947063688999,
        "step": 2211
    },
    {
        "loss": 2.4263,
        "grad_norm": 0.9282097816467285,
        "learning_rate": 1.5317638103608312e-05,
        "epoch": 0.6098704163220292,
        "step": 2212
    },
    {
        "loss": 1.9518,
        "grad_norm": 1.6859849691390991,
        "learning_rate": 1.5178761216937077e-05,
        "epoch": 0.6101461262751585,
        "step": 2213
    },
    {
        "loss": 2.1811,
        "grad_norm": 1.4642419815063477,
        "learning_rate": 1.5040465040127016e-05,
        "epoch": 0.6104218362282878,
        "step": 2214
    },
    {
        "loss": 1.6128,
        "grad_norm": 1.859381079673767,
        "learning_rate": 1.490275051999237e-05,
        "epoch": 0.6106975461814171,
        "step": 2215
    },
    {
        "loss": 2.1612,
        "grad_norm": 1.452999234199524,
        "learning_rate": 1.4765618599365139e-05,
        "epoch": 0.6109732561345464,
        "step": 2216
    },
    {
        "loss": 2.2213,
        "grad_norm": 1.2001121044158936,
        "learning_rate": 1.4629070217088691e-05,
        "epoch": 0.6112489660876758,
        "step": 2217
    },
    {
        "loss": 1.5428,
        "grad_norm": 1.6095232963562012,
        "learning_rate": 1.4493106308011339e-05,
        "epoch": 0.6115246760408051,
        "step": 2218
    },
    {
        "loss": 2.2646,
        "grad_norm": 1.3125709295272827,
        "learning_rate": 1.4357727802979958e-05,
        "epoch": 0.6118003859939344,
        "step": 2219
    },
    {
        "loss": 2.4473,
        "grad_norm": 1.0036859512329102,
        "learning_rate": 1.4222935628833545e-05,
        "epoch": 0.6120760959470637,
        "step": 2220
    },
    {
        "loss": 2.3944,
        "grad_norm": 0.984937846660614,
        "learning_rate": 1.4088730708396903e-05,
        "epoch": 0.612351805900193,
        "step": 2221
    },
    {
        "loss": 2.0662,
        "grad_norm": 1.0445669889450073,
        "learning_rate": 1.3955113960474398e-05,
        "epoch": 0.6126275158533223,
        "step": 2222
    },
    {
        "loss": 2.0348,
        "grad_norm": 1.9457248449325562,
        "learning_rate": 1.3822086299843517e-05,
        "epoch": 0.6129032258064516,
        "step": 2223
    },
    {
        "loss": 1.9427,
        "grad_norm": 1.8512243032455444,
        "learning_rate": 1.3689648637248753e-05,
        "epoch": 0.6131789357595809,
        "step": 2224
    },
    {
        "loss": 1.6924,
        "grad_norm": 1.2359328269958496,
        "learning_rate": 1.355780187939526e-05,
        "epoch": 0.6134546457127102,
        "step": 2225
    },
    {
        "loss": 2.2083,
        "grad_norm": 0.956186830997467,
        "learning_rate": 1.3426546928942763e-05,
        "epoch": 0.6137303556658396,
        "step": 2226
    },
    {
        "loss": 1.8896,
        "grad_norm": 2.4627695083618164,
        "learning_rate": 1.3295884684499238e-05,
        "epoch": 0.6140060656189689,
        "step": 2227
    },
    {
        "loss": 2.6205,
        "grad_norm": 1.2022769451141357,
        "learning_rate": 1.3165816040614843e-05,
        "epoch": 0.6142817755720982,
        "step": 2228
    },
    {
        "loss": 1.3738,
        "grad_norm": 1.474199891090393,
        "learning_rate": 1.3036341887775838e-05,
        "epoch": 0.6145574855252275,
        "step": 2229
    },
    {
        "loss": 2.1158,
        "grad_norm": 2.6777584552764893,
        "learning_rate": 1.2907463112398365e-05,
        "epoch": 0.6148331954783568,
        "step": 2230
    },
    {
        "loss": 2.1465,
        "grad_norm": 1.2842572927474976,
        "learning_rate": 1.277918059682246e-05,
        "epoch": 0.6151089054314861,
        "step": 2231
    },
    {
        "loss": 2.3472,
        "grad_norm": 2.3660387992858887,
        "learning_rate": 1.2651495219306031e-05,
        "epoch": 0.6153846153846154,
        "step": 2232
    },
    {
        "loss": 1.8284,
        "grad_norm": 1.8834599256515503,
        "learning_rate": 1.2524407854018793e-05,
        "epoch": 0.6156603253377447,
        "step": 2233
    },
    {
        "loss": 1.7357,
        "grad_norm": 2.20939564704895,
        "learning_rate": 1.2397919371036271e-05,
        "epoch": 0.615936035290874,
        "step": 2234
    },
    {
        "loss": 2.5077,
        "grad_norm": 1.6398476362228394,
        "learning_rate": 1.2272030636333909e-05,
        "epoch": 0.6162117452440034,
        "step": 2235
    },
    {
        "loss": 2.2926,
        "grad_norm": 1.523042917251587,
        "learning_rate": 1.2146742511781129e-05,
        "epoch": 0.6164874551971327,
        "step": 2236
    },
    {
        "loss": 1.7421,
        "grad_norm": 2.2277653217315674,
        "learning_rate": 1.2022055855135361e-05,
        "epoch": 0.616763165150262,
        "step": 2237
    },
    {
        "loss": 2.2885,
        "grad_norm": 1.226054072380066,
        "learning_rate": 1.1897971520036221e-05,
        "epoch": 0.6170388751033913,
        "step": 2238
    },
    {
        "loss": 2.0906,
        "grad_norm": 1.0594748258590698,
        "learning_rate": 1.1774490355999713e-05,
        "epoch": 0.6173145850565206,
        "step": 2239
    },
    {
        "loss": 1.8988,
        "grad_norm": 2.7217626571655273,
        "learning_rate": 1.1651613208412315e-05,
        "epoch": 0.6175902950096499,
        "step": 2240
    },
    {
        "loss": 2.4053,
        "grad_norm": 1.0392266511917114,
        "learning_rate": 1.1529340918525223e-05,
        "epoch": 0.6178660049627791,
        "step": 2241
    },
    {
        "loss": 2.2932,
        "grad_norm": 1.1943144798278809,
        "learning_rate": 1.1407674323448702e-05,
        "epoch": 0.6181417149159084,
        "step": 2242
    },
    {
        "loss": 2.273,
        "grad_norm": 1.4821031093597412,
        "learning_rate": 1.1286614256146178e-05,
        "epoch": 0.6184174248690377,
        "step": 2243
    },
    {
        "loss": 2.2173,
        "grad_norm": 1.2412118911743164,
        "learning_rate": 1.1166161545428621e-05,
        "epoch": 0.618693134822167,
        "step": 2244
    },
    {
        "loss": 2.066,
        "grad_norm": 1.6744197607040405,
        "learning_rate": 1.1046317015948881e-05,
        "epoch": 0.6189688447752963,
        "step": 2245
    },
    {
        "loss": 1.9834,
        "grad_norm": 1.8718043565750122,
        "learning_rate": 1.0927081488196078e-05,
        "epoch": 0.6192445547284257,
        "step": 2246
    },
    {
        "loss": 2.1878,
        "grad_norm": 1.5810359716415405,
        "learning_rate": 1.080845577848988e-05,
        "epoch": 0.619520264681555,
        "step": 2247
    },
    {
        "loss": 1.7072,
        "grad_norm": 1.840686559677124,
        "learning_rate": 1.0690440698974957e-05,
        "epoch": 0.6197959746346843,
        "step": 2248
    },
    {
        "loss": 2.3535,
        "grad_norm": 0.9219791293144226,
        "learning_rate": 1.0573037057615498e-05,
        "epoch": 0.6200716845878136,
        "step": 2249
    },
    {
        "loss": 2.1903,
        "grad_norm": 0.9572901725769043,
        "learning_rate": 1.0456245658189579e-05,
        "epoch": 0.6203473945409429,
        "step": 2250
    },
    {
        "loss": 1.781,
        "grad_norm": 3.247880458831787,
        "learning_rate": 1.0340067300283652e-05,
        "epoch": 0.6206231044940722,
        "step": 2251
    },
    {
        "loss": 2.0931,
        "grad_norm": 1.0319430828094482,
        "learning_rate": 1.0224502779287237e-05,
        "epoch": 0.6208988144472015,
        "step": 2252
    },
    {
        "loss": 1.5514,
        "grad_norm": 2.2314071655273438,
        "learning_rate": 1.010955288638723e-05,
        "epoch": 0.6211745244003308,
        "step": 2253
    },
    {
        "loss": 2.268,
        "grad_norm": 1.1675422191619873,
        "learning_rate": 9.99521840856268e-06,
        "epoch": 0.6214502343534601,
        "step": 2254
    },
    {
        "loss": 2.0108,
        "grad_norm": 1.7707494497299194,
        "learning_rate": 9.881500128579292e-06,
        "epoch": 0.6217259443065895,
        "step": 2255
    },
    {
        "loss": 1.4174,
        "grad_norm": 1.9058332443237305,
        "learning_rate": 9.76839882498416e-06,
        "epoch": 0.6220016542597188,
        "step": 2256
    },
    {
        "loss": 2.042,
        "grad_norm": 1.4069105386734009,
        "learning_rate": 9.655915272100357e-06,
        "epoch": 0.6222773642128481,
        "step": 2257
    },
    {
        "loss": 2.2609,
        "grad_norm": 1.4939247369766235,
        "learning_rate": 9.54405024002164e-06,
        "epoch": 0.6225530741659774,
        "step": 2258
    },
    {
        "loss": 1.5858,
        "grad_norm": 1.807153344154358,
        "learning_rate": 9.432804494607239e-06,
        "epoch": 0.6228287841191067,
        "step": 2259
    },
    {
        "loss": 1.5887,
        "grad_norm": 1.7908167839050293,
        "learning_rate": 9.322178797476567e-06,
        "epoch": 0.623104494072236,
        "step": 2260
    },
    {
        "loss": 1.9462,
        "grad_norm": 1.280802845954895,
        "learning_rate": 9.212173906003973e-06,
        "epoch": 0.6233802040253653,
        "step": 2261
    },
    {
        "loss": 2.0349,
        "grad_norm": 1.5409642457962036,
        "learning_rate": 9.102790573313669e-06,
        "epoch": 0.6236559139784946,
        "step": 2262
    },
    {
        "loss": 2.4287,
        "grad_norm": 1.3996316194534302,
        "learning_rate": 8.994029548274452e-06,
        "epoch": 0.6239316239316239,
        "step": 2263
    },
    {
        "loss": 2.297,
        "grad_norm": 1.3907470703125,
        "learning_rate": 8.885891575494621e-06,
        "epoch": 0.6242073338847532,
        "step": 2264
    },
    {
        "loss": 2.4819,
        "grad_norm": 1.2431694269180298,
        "learning_rate": 8.778377395316894e-06,
        "epoch": 0.6244830438378826,
        "step": 2265
    },
    {
        "loss": 1.1836,
        "grad_norm": 2.1629180908203125,
        "learning_rate": 8.671487743813378e-06,
        "epoch": 0.6247587537910119,
        "step": 2266
    },
    {
        "loss": 1.2524,
        "grad_norm": 1.968767762184143,
        "learning_rate": 8.565223352780415e-06,
        "epoch": 0.6250344637441412,
        "step": 2267
    },
    {
        "loss": 2.2753,
        "grad_norm": 1.0028347969055176,
        "learning_rate": 8.459584949733657e-06,
        "epoch": 0.6253101736972705,
        "step": 2268
    },
    {
        "loss": 2.3232,
        "grad_norm": 2.0981218814849854,
        "learning_rate": 8.35457325790312e-06,
        "epoch": 0.6255858836503998,
        "step": 2269
    },
    {
        "loss": 1.927,
        "grad_norm": 1.550079345703125,
        "learning_rate": 8.250188996228114e-06,
        "epoch": 0.6258615936035291,
        "step": 2270
    },
    {
        "loss": 2.0134,
        "grad_norm": 1.194893717765808,
        "learning_rate": 8.146432879352406e-06,
        "epoch": 0.6261373035566584,
        "step": 2271
    },
    {
        "loss": 2.3711,
        "grad_norm": 1.9255881309509277,
        "learning_rate": 8.043305617619312e-06,
        "epoch": 0.6264130135097877,
        "step": 2272
    },
    {
        "loss": 1.7523,
        "grad_norm": 2.5369558334350586,
        "learning_rate": 7.94080791706685e-06,
        "epoch": 0.626688723462917,
        "step": 2273
    },
    {
        "loss": 1.6392,
        "grad_norm": 1.8462185859680176,
        "learning_rate": 7.838940479422874e-06,
        "epoch": 0.6269644334160464,
        "step": 2274
    },
    {
        "loss": 2.2415,
        "grad_norm": 0.9599829316139221,
        "learning_rate": 7.737704002100231e-06,
        "epoch": 0.6272401433691757,
        "step": 2275
    },
    {
        "loss": 2.0006,
        "grad_norm": 2.0309791564941406,
        "learning_rate": 7.637099178192153e-06,
        "epoch": 0.627515853322305,
        "step": 2276
    },
    {
        "loss": 1.0928,
        "grad_norm": 2.2983453273773193,
        "learning_rate": 7.537126696467278e-06,
        "epoch": 0.6277915632754343,
        "step": 2277
    },
    {
        "loss": 2.2755,
        "grad_norm": 1.6438928842544556,
        "learning_rate": 7.437787241365113e-06,
        "epoch": 0.6280672732285636,
        "step": 2278
    },
    {
        "loss": 2.1525,
        "grad_norm": 1.8761796951293945,
        "learning_rate": 7.339081492991284e-06,
        "epoch": 0.6283429831816929,
        "step": 2279
    },
    {
        "loss": 1.8971,
        "grad_norm": 1.5595768690109253,
        "learning_rate": 7.241010127112891e-06,
        "epoch": 0.6286186931348222,
        "step": 2280
    },
    {
        "loss": 2.1415,
        "grad_norm": 1.1720467805862427,
        "learning_rate": 7.1435738151538365e-06,
        "epoch": 0.6288944030879515,
        "step": 2281
    },
    {
        "loss": 1.7562,
        "grad_norm": 1.286242961883545,
        "learning_rate": 7.046773224190295e-06,
        "epoch": 0.6291701130410808,
        "step": 2282
    },
    {
        "loss": 1.7083,
        "grad_norm": 1.9532041549682617,
        "learning_rate": 6.95060901694613e-06,
        "epoch": 0.62944582299421,
        "step": 2283
    },
    {
        "loss": 1.9918,
        "grad_norm": 1.8722243309020996,
        "learning_rate": 6.855081851788314e-06,
        "epoch": 0.6297215329473393,
        "step": 2284
    },
    {
        "loss": 2.2776,
        "grad_norm": 1.4098765850067139,
        "learning_rate": 6.760192382722463e-06,
        "epoch": 0.6299972429004687,
        "step": 2285
    },
    {
        "loss": 2.229,
        "grad_norm": 1.382231593132019,
        "learning_rate": 6.6659412593883755e-06,
        "epoch": 0.630272952853598,
        "step": 2286
    },
    {
        "loss": 1.6862,
        "grad_norm": 1.500196933746338,
        "learning_rate": 6.572329127055521e-06,
        "epoch": 0.6305486628067273,
        "step": 2287
    },
    {
        "loss": 2.4667,
        "grad_norm": 1.137066125869751,
        "learning_rate": 6.479356626618649e-06,
        "epoch": 0.6308243727598566,
        "step": 2288
    },
    {
        "loss": 2.4566,
        "grad_norm": 2.1723363399505615,
        "learning_rate": 6.387024394593466e-06,
        "epoch": 0.6311000827129859,
        "step": 2289
    },
    {
        "loss": 2.4784,
        "grad_norm": 1.7534722089767456,
        "learning_rate": 6.295333063112174e-06,
        "epoch": 0.6313757926661152,
        "step": 2290
    },
    {
        "loss": 2.0101,
        "grad_norm": 1.6176584959030151,
        "learning_rate": 6.204283259919197e-06,
        "epoch": 0.6316515026192445,
        "step": 2291
    },
    {
        "loss": 1.8477,
        "grad_norm": 1.3175033330917358,
        "learning_rate": 6.11387560836687e-06,
        "epoch": 0.6319272125723738,
        "step": 2292
    },
    {
        "loss": 1.9275,
        "grad_norm": 1.4002331495285034,
        "learning_rate": 6.024110727411247e-06,
        "epoch": 0.6322029225255031,
        "step": 2293
    },
    {
        "loss": 1.6137,
        "grad_norm": 1.9523332118988037,
        "learning_rate": 5.934989231607702e-06,
        "epoch": 0.6324786324786325,
        "step": 2294
    },
    {
        "loss": 2.5994,
        "grad_norm": 1.556251883506775,
        "learning_rate": 5.846511731106852e-06,
        "epoch": 0.6327543424317618,
        "step": 2295
    },
    {
        "loss": 1.3262,
        "grad_norm": 1.9443122148513794,
        "learning_rate": 5.758678831650366e-06,
        "epoch": 0.6330300523848911,
        "step": 2296
    },
    {
        "loss": 1.9881,
        "grad_norm": 1.765241265296936,
        "learning_rate": 5.671491134566775e-06,
        "epoch": 0.6333057623380204,
        "step": 2297
    },
    {
        "loss": 2.7185,
        "grad_norm": 1.0632309913635254,
        "learning_rate": 5.584949236767345e-06,
        "epoch": 0.6335814722911497,
        "step": 2298
    },
    {
        "loss": 2.0938,
        "grad_norm": 1.7344903945922852,
        "learning_rate": 5.499053730742087e-06,
        "epoch": 0.633857182244279,
        "step": 2299
    },
    {
        "loss": 2.0814,
        "grad_norm": 1.4802525043487549,
        "learning_rate": 5.413805204555578e-06,
        "epoch": 0.6341328921974083,
        "step": 2300
    },
    {
        "loss": 1.9775,
        "grad_norm": 1.259933352470398,
        "learning_rate": 5.3292042418429996e-06,
        "epoch": 0.6344086021505376,
        "step": 2301
    },
    {
        "loss": 2.2502,
        "grad_norm": 1.1339912414550781,
        "learning_rate": 5.245251421806141e-06,
        "epoch": 0.6346843121036669,
        "step": 2302
    },
    {
        "loss": 1.9542,
        "grad_norm": 1.9300893545150757,
        "learning_rate": 5.161947319209459e-06,
        "epoch": 0.6349600220567962,
        "step": 2303
    },
    {
        "loss": 1.6832,
        "grad_norm": 1.8631885051727295,
        "learning_rate": 5.079292504376043e-06,
        "epoch": 0.6352357320099256,
        "step": 2304
    },
    {
        "loss": 1.6189,
        "grad_norm": 1.8978296518325806,
        "learning_rate": 4.997287543183804e-06,
        "epoch": 0.6355114419630549,
        "step": 2305
    },
    {
        "loss": 2.4181,
        "grad_norm": 1.3901324272155762,
        "learning_rate": 4.915932997061612e-06,
        "epoch": 0.6357871519161842,
        "step": 2306
    },
    {
        "loss": 1.6889,
        "grad_norm": 1.5323392152786255,
        "learning_rate": 4.835229422985376e-06,
        "epoch": 0.6360628618693135,
        "step": 2307
    },
    {
        "loss": 1.9304,
        "grad_norm": 1.6505764722824097,
        "learning_rate": 4.755177373474273e-06,
        "epoch": 0.6363385718224428,
        "step": 2308
    },
    {
        "loss": 1.7151,
        "grad_norm": 1.5206835269927979,
        "learning_rate": 4.675777396586945e-06,
        "epoch": 0.6366142817755721,
        "step": 2309
    },
    {
        "loss": 1.9644,
        "grad_norm": 1.9768928289413452,
        "learning_rate": 4.597030035917804e-06,
        "epoch": 0.6368899917287014,
        "step": 2310
    },
    {
        "loss": 2.4469,
        "grad_norm": 1.3724336624145508,
        "learning_rate": 4.5189358305932515e-06,
        "epoch": 0.6371657016818307,
        "step": 2311
    },
    {
        "loss": 1.5072,
        "grad_norm": 2.173816442489624,
        "learning_rate": 4.441495315267974e-06,
        "epoch": 0.63744141163496,
        "step": 2312
    },
    {
        "loss": 2.223,
        "grad_norm": 1.4396861791610718,
        "learning_rate": 4.364709020121371e-06,
        "epoch": 0.6377171215880894,
        "step": 2313
    },
    {
        "loss": 2.1592,
        "grad_norm": 1.3124828338623047,
        "learning_rate": 4.2885774708538275e-06,
        "epoch": 0.6379928315412187,
        "step": 2314
    },
    {
        "loss": 2.0674,
        "grad_norm": 2.107343912124634,
        "learning_rate": 4.213101188683144e-06,
        "epoch": 0.638268541494348,
        "step": 2315
    },
    {
        "loss": 2.2383,
        "grad_norm": 1.4540491104125977,
        "learning_rate": 4.138280690341045e-06,
        "epoch": 0.6385442514474773,
        "step": 2316
    },
    {
        "loss": 2.0366,
        "grad_norm": 1.6092463731765747,
        "learning_rate": 4.064116488069503e-06,
        "epoch": 0.6388199614006066,
        "step": 2317
    },
    {
        "loss": 2.2401,
        "grad_norm": 1.6011419296264648,
        "learning_rate": 3.990609089617348e-06,
        "epoch": 0.6390956713537359,
        "step": 2318
    },
    {
        "loss": 1.9908,
        "grad_norm": 1.2555598020553589,
        "learning_rate": 3.917758998236709e-06,
        "epoch": 0.6393713813068652,
        "step": 2319
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.3387057781219482,
        "learning_rate": 3.8455667126796715e-06,
        "epoch": 0.6396470912599945,
        "step": 2320
    },
    {
        "loss": 2.5081,
        "grad_norm": 1.5742247104644775,
        "learning_rate": 3.7740327271947316e-06,
        "epoch": 0.6399228012131238,
        "step": 2321
    },
    {
        "loss": 2.1403,
        "grad_norm": 0.9581618905067444,
        "learning_rate": 3.7031575315235157e-06,
        "epoch": 0.6401985111662531,
        "step": 2322
    },
    {
        "loss": 1.0912,
        "grad_norm": 2.012641191482544,
        "learning_rate": 3.632941610897389e-06,
        "epoch": 0.6404742211193825,
        "step": 2323
    },
    {
        "loss": 1.594,
        "grad_norm": 1.9773086309432983,
        "learning_rate": 3.5633854460341286e-06,
        "epoch": 0.6407499310725118,
        "step": 2324
    },
    {
        "loss": 1.6458,
        "grad_norm": 1.7605472803115845,
        "learning_rate": 3.4944895131346246e-06,
        "epoch": 0.6410256410256411,
        "step": 2325
    },
    {
        "loss": 2.1703,
        "grad_norm": 1.6288368701934814,
        "learning_rate": 3.426254283879682e-06,
        "epoch": 0.6413013509787703,
        "step": 2326
    },
    {
        "loss": 1.9759,
        "grad_norm": 1.5636988878250122,
        "learning_rate": 3.3586802254266914e-06,
        "epoch": 0.6415770609318996,
        "step": 2327
    },
    {
        "loss": 2.1893,
        "grad_norm": 1.4032988548278809,
        "learning_rate": 3.29176780040652e-06,
        "epoch": 0.6418527708850289,
        "step": 2328
    },
    {
        "loss": 2.0828,
        "grad_norm": 0.8691979050636292,
        "learning_rate": 3.2255174669202805e-06,
        "epoch": 0.6421284808381582,
        "step": 2329
    },
    {
        "loss": 2.0848,
        "grad_norm": 1.1090924739837646,
        "learning_rate": 3.159929678536244e-06,
        "epoch": 0.6424041907912875,
        "step": 2330
    },
    {
        "loss": 1.3778,
        "grad_norm": 2.027162551879883,
        "learning_rate": 3.0950048842867117e-06,
        "epoch": 0.6426799007444168,
        "step": 2331
    },
    {
        "loss": 2.0871,
        "grad_norm": 1.8874258995056152,
        "learning_rate": 3.0307435286649145e-06,
        "epoch": 0.6429556106975461,
        "step": 2332
    },
    {
        "loss": 2.1599,
        "grad_norm": 1.2377480268478394,
        "learning_rate": 2.9671460516220494e-06,
        "epoch": 0.6432313206506755,
        "step": 2333
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.4806486368179321,
        "learning_rate": 2.904212888564162e-06,
        "epoch": 0.6435070306038048,
        "step": 2334
    },
    {
        "loss": 1.5231,
        "grad_norm": 1.7946709394454956,
        "learning_rate": 2.841944470349245e-06,
        "epoch": 0.6437827405569341,
        "step": 2335
    },
    {
        "loss": 2.2498,
        "grad_norm": 1.65261971950531,
        "learning_rate": 2.7803412232842664e-06,
        "epoch": 0.6440584505100634,
        "step": 2336
    },
    {
        "loss": 1.6477,
        "grad_norm": 1.9098877906799316,
        "learning_rate": 2.7194035691222365e-06,
        "epoch": 0.6443341604631927,
        "step": 2337
    },
    {
        "loss": 2.0266,
        "grad_norm": 1.8434998989105225,
        "learning_rate": 2.6591319250593217e-06,
        "epoch": 0.644609870416322,
        "step": 2338
    },
    {
        "loss": 2.1521,
        "grad_norm": 1.8606235980987549,
        "learning_rate": 2.5995267037320138e-06,
        "epoch": 0.6448855803694513,
        "step": 2339
    },
    {
        "loss": 1.8724,
        "grad_norm": 1.5394266843795776,
        "learning_rate": 2.5405883132143092e-06,
        "epoch": 0.6451612903225806,
        "step": 2340
    },
    {
        "loss": 2.2424,
        "grad_norm": 1.4211312532424927,
        "learning_rate": 2.482317157014846e-06,
        "epoch": 0.6454370002757099,
        "step": 2341
    },
    {
        "loss": 2.6362,
        "grad_norm": 1.2989730834960938,
        "learning_rate": 2.424713634074205e-06,
        "epoch": 0.6457127102288392,
        "step": 2342
    },
    {
        "loss": 2.0921,
        "grad_norm": 1.050557017326355,
        "learning_rate": 2.3677781387622e-06,
        "epoch": 0.6459884201819686,
        "step": 2343
    },
    {
        "loss": 1.9377,
        "grad_norm": 3.14461612701416,
        "learning_rate": 2.3115110608751046e-06,
        "epoch": 0.6462641301350979,
        "step": 2344
    },
    {
        "loss": 2.7791,
        "grad_norm": 1.4017319679260254,
        "learning_rate": 2.2559127856330186e-06,
        "epoch": 0.6465398400882272,
        "step": 2345
    },
    {
        "loss": 2.3112,
        "grad_norm": 1.472895860671997,
        "learning_rate": 2.20098369367725e-06,
        "epoch": 0.6468155500413565,
        "step": 2346
    },
    {
        "loss": 2.4296,
        "grad_norm": 1.2565678358078003,
        "learning_rate": 2.1467241610676813e-06,
        "epoch": 0.6470912599944858,
        "step": 2347
    },
    {
        "loss": 2.4034,
        "grad_norm": 1.149032473564148,
        "learning_rate": 2.093134559280219e-06,
        "epoch": 0.6473669699476151,
        "step": 2348
    },
    {
        "loss": 2.1099,
        "grad_norm": 1.6953374147415161,
        "learning_rate": 2.040215255204214e-06,
        "epoch": 0.6476426799007444,
        "step": 2349
    },
    {
        "loss": 1.6678,
        "grad_norm": 2.2149832248687744,
        "learning_rate": 1.987966611139991e-06,
        "epoch": 0.6479183898538737,
        "step": 2350
    },
    {
        "loss": 2.3936,
        "grad_norm": 1.2002557516098022,
        "learning_rate": 1.9363889847963334e-06,
        "epoch": 0.648194099807003,
        "step": 2351
    },
    {
        "loss": 2.2054,
        "grad_norm": 2.061699151992798,
        "learning_rate": 1.8854827292880572e-06,
        "epoch": 0.6484698097601324,
        "step": 2352
    },
    {
        "loss": 2.449,
        "grad_norm": 1.0862534046173096,
        "learning_rate": 1.8352481931336095e-06,
        "epoch": 0.6487455197132617,
        "step": 2353
    },
    {
        "loss": 2.5706,
        "grad_norm": 1.1395684480667114,
        "learning_rate": 1.7856857202526168e-06,
        "epoch": 0.649021229666391,
        "step": 2354
    },
    {
        "loss": 2.2308,
        "grad_norm": 1.0892817974090576,
        "learning_rate": 1.7367956499635961e-06,
        "epoch": 0.6492969396195203,
        "step": 2355
    },
    {
        "loss": 2.1748,
        "grad_norm": 1.5614999532699585,
        "learning_rate": 1.6885783169816037e-06,
        "epoch": 0.6495726495726496,
        "step": 2356
    },
    {
        "loss": 1.9888,
        "grad_norm": 1.9471060037612915,
        "learning_rate": 1.6410340514159571e-06,
        "epoch": 0.6498483595257789,
        "step": 2357
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.1220917701721191,
        "learning_rate": 1.594163178767971e-06,
        "epoch": 0.6501240694789082,
        "step": 2358
    },
    {
        "loss": 2.275,
        "grad_norm": 1.0097157955169678,
        "learning_rate": 1.5479660199286928e-06,
        "epoch": 0.6503997794320375,
        "step": 2359
    },
    {
        "loss": 1.5037,
        "grad_norm": 2.091423749923706,
        "learning_rate": 1.5024428911767697e-06,
        "epoch": 0.6506754893851668,
        "step": 2360
    },
    {
        "loss": 2.1226,
        "grad_norm": 1.6930732727050781,
        "learning_rate": 1.457594104176241e-06,
        "epoch": 0.6509511993382961,
        "step": 2361
    },
    {
        "loss": 2.2343,
        "grad_norm": 1.617379903793335,
        "learning_rate": 1.413419965974405e-06,
        "epoch": 0.6512269092914255,
        "step": 2362
    },
    {
        "loss": 2.3239,
        "grad_norm": 1.445860743522644,
        "learning_rate": 1.3699207789997447e-06,
        "epoch": 0.6515026192445548,
        "step": 2363
    },
    {
        "loss": 2.2309,
        "grad_norm": 1.0348542928695679,
        "learning_rate": 1.3270968410598273e-06,
        "epoch": 0.6517783291976841,
        "step": 2364
    },
    {
        "loss": 1.9932,
        "grad_norm": 1.7715867757797241,
        "learning_rate": 1.2849484453392624e-06,
        "epoch": 0.6520540391508134,
        "step": 2365
    },
    {
        "loss": 1.7147,
        "grad_norm": 2.194772481918335,
        "learning_rate": 1.2434758803977374e-06,
        "epoch": 0.6523297491039427,
        "step": 2366
    },
    {
        "loss": 2.0892,
        "grad_norm": 2.2421677112579346,
        "learning_rate": 1.2026794301679968e-06,
        "epoch": 0.652605459057072,
        "step": 2367
    },
    {
        "loss": 2.4569,
        "grad_norm": 1.5252677202224731,
        "learning_rate": 1.1625593739539308e-06,
        "epoch": 0.6528811690102013,
        "step": 2368
    },
    {
        "loss": 1.9999,
        "grad_norm": 1.5891393423080444,
        "learning_rate": 1.1231159864286133e-06,
        "epoch": 0.6531568789633305,
        "step": 2369
    },
    {
        "loss": 1.2652,
        "grad_norm": 1.9303083419799805,
        "learning_rate": 1.0843495376324896e-06,
        "epoch": 0.6534325889164598,
        "step": 2370
    },
    {
        "loss": 2.1098,
        "grad_norm": 1.686020851135254,
        "learning_rate": 1.0462602929714792e-06,
        "epoch": 0.6537082988695891,
        "step": 2371
    },
    {
        "loss": 2.2652,
        "grad_norm": 0.9101353883743286,
        "learning_rate": 1.008848513215177e-06,
        "epoch": 0.6539840088227185,
        "step": 2372
    },
    {
        "loss": 1.8111,
        "grad_norm": 1.8346970081329346,
        "learning_rate": 9.721144544950767e-07,
        "epoch": 0.6542597187758478,
        "step": 2373
    },
    {
        "loss": 2.1742,
        "grad_norm": 1.5811257362365723,
        "learning_rate": 9.360583683027613e-07,
        "epoch": 0.6545354287289771,
        "step": 2374
    },
    {
        "loss": 2.0131,
        "grad_norm": 1.6601648330688477,
        "learning_rate": 9.006805014882824e-07,
        "epoch": 0.6548111386821064,
        "step": 2375
    },
    {
        "loss": 1.5445,
        "grad_norm": 1.652396321296692,
        "learning_rate": 8.659810962583726e-07,
        "epoch": 0.6550868486352357,
        "step": 2376
    },
    {
        "loss": 1.5837,
        "grad_norm": 1.8837788105010986,
        "learning_rate": 8.319603901748351e-07,
        "epoch": 0.655362558588365,
        "step": 2377
    },
    {
        "loss": 2.1851,
        "grad_norm": 1.0328856706619263,
        "learning_rate": 7.986186161529241e-07,
        "epoch": 0.6556382685414943,
        "step": 2378
    },
    {
        "loss": 1.6661,
        "grad_norm": 2.1217477321624756,
        "learning_rate": 7.659560024597223e-07,
        "epoch": 0.6559139784946236,
        "step": 2379
    },
    {
        "loss": 1.9466,
        "grad_norm": 1.4044290781021118,
        "learning_rate": 7.339727727125877e-07,
        "epoch": 0.6561896884477529,
        "step": 2380
    },
    {
        "loss": 2.0811,
        "grad_norm": 1.927931785583496,
        "learning_rate": 7.026691458776325e-07,
        "epoch": 0.6564653984008822,
        "step": 2381
    },
    {
        "loss": 2.1312,
        "grad_norm": 1.2519400119781494,
        "learning_rate": 6.720453362682344e-07,
        "epoch": 0.6567411083540116,
        "step": 2382
    },
    {
        "loss": 2.4735,
        "grad_norm": 1.255838394165039,
        "learning_rate": 6.421015535435171e-07,
        "epoch": 0.6570168183071409,
        "step": 2383
    },
    {
        "loss": 2.1777,
        "grad_norm": 1.2186139822006226,
        "learning_rate": 6.128380027069724e-07,
        "epoch": 0.6572925282602702,
        "step": 2384
    },
    {
        "loss": 2.496,
        "grad_norm": 1.3700556755065918,
        "learning_rate": 5.84254884105051e-07,
        "epoch": 0.6575682382133995,
        "step": 2385
    },
    {
        "loss": 1.7192,
        "grad_norm": 1.8368546962738037,
        "learning_rate": 5.563523934257186e-07,
        "epoch": 0.6578439481665288,
        "step": 2386
    },
    {
        "loss": 2.1405,
        "grad_norm": 2.185662031173706,
        "learning_rate": 5.291307216972574e-07,
        "epoch": 0.6581196581196581,
        "step": 2387
    },
    {
        "loss": 0.9144,
        "grad_norm": 2.0797226428985596,
        "learning_rate": 5.02590055286789e-07,
        "epoch": 0.6583953680727874,
        "step": 2388
    },
    {
        "loss": 2.4977,
        "grad_norm": 1.0803810358047485,
        "learning_rate": 4.767305758991314e-07,
        "epoch": 0.6586710780259167,
        "step": 2389
    },
    {
        "loss": 2.1457,
        "grad_norm": 1.0488547086715698,
        "learning_rate": 4.515524605754884e-07,
        "epoch": 0.658946787979046,
        "step": 2390
    },
    {
        "loss": 2.0398,
        "grad_norm": 1.467287302017212,
        "learning_rate": 4.270558816922732e-07,
        "epoch": 0.6592224979321754,
        "step": 2391
    },
    {
        "loss": 2.1458,
        "grad_norm": 1.8532572984695435,
        "learning_rate": 4.0324100695989797e-07,
        "epoch": 0.6594982078853047,
        "step": 2392
    },
    {
        "loss": 1.0521,
        "grad_norm": 1.8119730949401855,
        "learning_rate": 3.8010799942161946e-07,
        "epoch": 0.659773917838434,
        "step": 2393
    },
    {
        "loss": 2.1795,
        "grad_norm": 1.6899724006652832,
        "learning_rate": 3.5765701745247295e-07,
        "epoch": 0.6600496277915633,
        "step": 2394
    },
    {
        "loss": 1.881,
        "grad_norm": 1.9579764604568481,
        "learning_rate": 3.3588821475815103e-07,
        "epoch": 0.6603253377446926,
        "step": 2395
    },
    {
        "loss": 2.0617,
        "grad_norm": 1.8411517143249512,
        "learning_rate": 3.148017403739267e-07,
        "epoch": 0.6606010476978219,
        "step": 2396
    },
    {
        "loss": 2.3363,
        "grad_norm": 1.2787904739379883,
        "learning_rate": 2.943977386637098e-07,
        "epoch": 0.6608767576509512,
        "step": 2397
    },
    {
        "loss": 2.2629,
        "grad_norm": 1.3256088495254517,
        "learning_rate": 2.746763493189697e-07,
        "epoch": 0.6611524676040805,
        "step": 2398
    },
    {
        "loss": 1.6527,
        "grad_norm": 1.7358769178390503,
        "learning_rate": 2.556377073578475e-07,
        "epoch": 0.6614281775572098,
        "step": 2399
    },
    {
        "loss": 2.4362,
        "grad_norm": 1.2030481100082397,
        "learning_rate": 2.3728194312420126e-07,
        "epoch": 0.6617038875103392,
        "step": 2400
    },
    {
        "loss": 2.2509,
        "grad_norm": 1.2594655752182007,
        "learning_rate": 2.1960918228670635e-07,
        "epoch": 0.6619795974634685,
        "step": 2401
    },
    {
        "loss": 1.8289,
        "grad_norm": 1.3651340007781982,
        "learning_rate": 2.02619545838012e-07,
        "epoch": 0.6622553074165978,
        "step": 2402
    },
    {
        "loss": 2.3227,
        "grad_norm": 0.9885880947113037,
        "learning_rate": 1.863131500939086e-07,
        "epoch": 0.6625310173697271,
        "step": 2403
    },
    {
        "loss": 1.7691,
        "grad_norm": 1.6941756010055542,
        "learning_rate": 1.7069010669253926e-07,
        "epoch": 0.6628067273228564,
        "step": 2404
    },
    {
        "loss": 1.4465,
        "grad_norm": 1.8001970052719116,
        "learning_rate": 1.5575052259361177e-07,
        "epoch": 0.6630824372759857,
        "step": 2405
    },
    {
        "loss": 2.3079,
        "grad_norm": 1.1659091711044312,
        "learning_rate": 1.4149450007767684e-07,
        "epoch": 0.663358147229115,
        "step": 2406
    },
    {
        "loss": 2.4649,
        "grad_norm": 1.3016670942306519,
        "learning_rate": 1.2792213674545084e-07,
        "epoch": 0.6636338571822443,
        "step": 2407
    },
    {
        "loss": 2.025,
        "grad_norm": 1.7435057163238525,
        "learning_rate": 1.1503352551712754e-07,
        "epoch": 0.6639095671353736,
        "step": 2408
    },
    {
        "loss": 1.3579,
        "grad_norm": 1.6587189435958862,
        "learning_rate": 1.0282875463171193e-07,
        "epoch": 0.664185277088503,
        "step": 2409
    },
    {
        "loss": 1.9113,
        "grad_norm": 1.7990078926086426,
        "learning_rate": 9.130790764648733e-08,
        "epoch": 0.6644609870416323,
        "step": 2410
    },
    {
        "loss": 2.6217,
        "grad_norm": 1.0520849227905273,
        "learning_rate": 8.047106343638256e-08,
        "epoch": 0.6647366969947615,
        "step": 2411
    },
    {
        "loss": 2.1139,
        "grad_norm": 1.444040298461914,
        "learning_rate": 7.031829619345009e-08,
        "epoch": 0.6650124069478908,
        "step": 2412
    },
    {
        "loss": 2.2004,
        "grad_norm": 1.642564058303833,
        "learning_rate": 6.084967542637765e-08,
        "epoch": 0.6652881169010201,
        "step": 2413
    },
    {
        "loss": 1.579,
        "grad_norm": 2.017221450805664,
        "learning_rate": 5.20652659599663e-08,
        "epoch": 0.6655638268541494,
        "step": 2414
    },
    {
        "loss": 1.0706,
        "grad_norm": 1.9894359111785889,
        "learning_rate": 4.396512793475305e-08,
        "epoch": 0.6658395368072787,
        "step": 2415
    },
    {
        "loss": 1.7571,
        "grad_norm": 2.215092897415161,
        "learning_rate": 3.65493168065334e-08,
        "epoch": 0.666115246760408,
        "step": 2416
    },
    {
        "loss": 1.9825,
        "grad_norm": 1.4820153713226318,
        "learning_rate": 2.98178833460172e-08,
        "epoch": 0.6663909567135373,
        "step": 2417
    },
    {
        "loss": 1.8374,
        "grad_norm": 1.030704140663147,
        "learning_rate": 2.3770873638484513e-08,
        "epoch": 0.6666666666666666,
        "step": 2418
    },
    {
        "loss": 1.9455,
        "grad_norm": 1.4503127336502075,
        "learning_rate": 1.8408329083441365e-08,
        "epoch": 0.6669423766197959,
        "step": 2419
    },
    {
        "loss": 2.7401,
        "grad_norm": 1.1455098390579224,
        "learning_rate": 1.3730286394364466e-08,
        "epoch": 0.6672180865729253,
        "step": 2420
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.240035057067871,
        "learning_rate": 9.736777598434721e-09,
        "epoch": 0.6674937965260546,
        "step": 2421
    },
    {
        "loss": 1.7859,
        "grad_norm": 2.2082173824310303,
        "learning_rate": 6.427830036337401e-09,
        "epoch": 0.6677695064791839,
        "step": 2422
    },
    {
        "loss": 1.4164,
        "grad_norm": 1.9527403116226196,
        "learning_rate": 3.8034663620512e-09,
        "epoch": 0.6680452164323132,
        "step": 2423
    },
    {
        "loss": 1.5678,
        "grad_norm": 1.5620505809783936,
        "learning_rate": 1.8637045426928013e-09,
        "epoch": 0.6683209263854425,
        "step": 2424
    },
    {
        "loss": 1.7686,
        "grad_norm": 1.6769965887069702,
        "learning_rate": 6.085578584280605e-10,
        "epoch": 0.6685966363385718,
        "step": 2425
    },
    {
        "loss": 2.3304,
        "grad_norm": 1.4174110889434814,
        "learning_rate": 3.8034902316574915e-11,
        "epoch": 0.6688723462917011,
        "step": 2426
    },
    {
        "loss": 1.7685,
        "grad_norm": 1.4078336954116821,
        "learning_rate": 0.00019999984786041967,
        "epoch": 0.6691480562448304,
        "step": 2427
    },
    {
        "loss": 2.0822,
        "grad_norm": 1.6574302911758423,
        "learning_rate": 0.00019999904912888872,
        "epoch": 0.6694237661979597,
        "step": 2428
    },
    {
        "loss": 2.428,
        "grad_norm": 1.2420421838760376,
        "learning_rate": 0.00019999756577597318,
        "epoch": 0.669699476151089,
        "step": 2429
    },
    {
        "loss": 1.4655,
        "grad_norm": 1.9178813695907593,
        "learning_rate": 0.00019999539781182847,
        "epoch": 0.6699751861042184,
        "step": 2430
    },
    {
        "loss": 1.5653,
        "grad_norm": 2.1017096042633057,
        "learning_rate": 0.00019999254525129712,
        "epoch": 0.6702508960573477,
        "step": 2431
    },
    {
        "loss": 2.6622,
        "grad_norm": 0.9691053032875061,
        "learning_rate": 0.00019998900811390852,
        "epoch": 0.670526606010477,
        "step": 2432
    },
    {
        "loss": 1.8318,
        "grad_norm": 1.2590020895004272,
        "learning_rate": 0.00019998478642387893,
        "epoch": 0.6708023159636063,
        "step": 2433
    },
    {
        "loss": 1.4015,
        "grad_norm": 1.8590466976165771,
        "learning_rate": 0.0001999798802101112,
        "epoch": 0.6710780259167356,
        "step": 2434
    },
    {
        "loss": 1.462,
        "grad_norm": 2.339752435684204,
        "learning_rate": 0.00019997428950619463,
        "epoch": 0.6713537358698649,
        "step": 2435
    },
    {
        "loss": 2.1377,
        "grad_norm": 1.1384693384170532,
        "learning_rate": 0.00019996801435040476,
        "epoch": 0.6716294458229942,
        "step": 2436
    },
    {
        "loss": 2.2199,
        "grad_norm": 1.8584485054016113,
        "learning_rate": 0.000199961054785703,
        "epoch": 0.6719051557761235,
        "step": 2437
    },
    {
        "loss": 2.2437,
        "grad_norm": 1.7918767929077148,
        "learning_rate": 0.0001999534108597366,
        "epoch": 0.6721808657292528,
        "step": 2438
    },
    {
        "loss": 1.9504,
        "grad_norm": 1.9719724655151367,
        "learning_rate": 0.00019994508262483786,
        "epoch": 0.6724565756823822,
        "step": 2439
    },
    {
        "loss": 1.3428,
        "grad_norm": 2.029031991958618,
        "learning_rate": 0.0001999360701380243,
        "epoch": 0.6727322856355115,
        "step": 2440
    },
    {
        "loss": 1.7393,
        "grad_norm": 2.3022117614746094,
        "learning_rate": 0.00019992637346099784,
        "epoch": 0.6730079955886408,
        "step": 2441
    },
    {
        "loss": 2.0936,
        "grad_norm": 1.880266785621643,
        "learning_rate": 0.00019991599266014473,
        "epoch": 0.6732837055417701,
        "step": 2442
    },
    {
        "loss": 1.642,
        "grad_norm": 2.075995445251465,
        "learning_rate": 0.00019990492780653477,
        "epoch": 0.6735594154948994,
        "step": 2443
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.502747654914856,
        "learning_rate": 0.000199893178975921,
        "epoch": 0.6738351254480287,
        "step": 2444
    },
    {
        "loss": 1.0878,
        "grad_norm": 2.142122983932495,
        "learning_rate": 0.00019988074624873928,
        "epoch": 0.674110835401158,
        "step": 2445
    },
    {
        "loss": 2.3527,
        "grad_norm": 1.5653245449066162,
        "learning_rate": 0.00019986762971010747,
        "epoch": 0.6743865453542873,
        "step": 2446
    },
    {
        "loss": 2.3441,
        "grad_norm": 1.8223295211791992,
        "learning_rate": 0.00019985382944982507,
        "epoch": 0.6746622553074166,
        "step": 2447
    },
    {
        "loss": 2.3504,
        "grad_norm": 1.3910106420516968,
        "learning_rate": 0.00019983934556237247,
        "epoch": 0.674937965260546,
        "step": 2448
    },
    {
        "loss": 2.0329,
        "grad_norm": 1.568302035331726,
        "learning_rate": 0.00019982417814691048,
        "epoch": 0.6752136752136753,
        "step": 2449
    },
    {
        "loss": 1.3373,
        "grad_norm": 2.0844595432281494,
        "learning_rate": 0.00019980832730727942,
        "epoch": 0.6754893851668046,
        "step": 2450
    },
    {
        "loss": 2.1612,
        "grad_norm": 1.653977870941162,
        "learning_rate": 0.00019979179315199853,
        "epoch": 0.6757650951199339,
        "step": 2451
    },
    {
        "loss": 1.4661,
        "grad_norm": 2.0448644161224365,
        "learning_rate": 0.0001997745757942653,
        "epoch": 0.6760408050730632,
        "step": 2452
    },
    {
        "loss": 1.8467,
        "grad_norm": 1.6024473905563354,
        "learning_rate": 0.0001997566753519545,
        "epoch": 0.6763165150261925,
        "step": 2453
    },
    {
        "loss": 2.3016,
        "grad_norm": 1.7984565496444702,
        "learning_rate": 0.00019973809194761763,
        "epoch": 0.6765922249793217,
        "step": 2454
    },
    {
        "loss": 2.1042,
        "grad_norm": 2.1209824085235596,
        "learning_rate": 0.0001997188257084818,
        "epoch": 0.676867934932451,
        "step": 2455
    },
    {
        "loss": 2.0195,
        "grad_norm": 1.6860194206237793,
        "learning_rate": 0.00019969887676644907,
        "epoch": 0.6771436448855803,
        "step": 2456
    },
    {
        "loss": 2.3034,
        "grad_norm": 1.5097079277038574,
        "learning_rate": 0.0001996782452580955,
        "epoch": 0.6774193548387096,
        "step": 2457
    },
    {
        "loss": 1.6255,
        "grad_norm": 3.1955580711364746,
        "learning_rate": 0.00019965693132467012,
        "epoch": 0.6776950647918389,
        "step": 2458
    },
    {
        "loss": 1.9213,
        "grad_norm": 2.0501556396484375,
        "learning_rate": 0.00019963493511209403,
        "epoch": 0.6779707747449683,
        "step": 2459
    },
    {
        "loss": 2.5162,
        "grad_norm": 1.5676617622375488,
        "learning_rate": 0.00019961225677095953,
        "epoch": 0.6782464846980976,
        "step": 2460
    },
    {
        "loss": 1.8137,
        "grad_norm": 2.1612660884857178,
        "learning_rate": 0.00019958889645652877,
        "epoch": 0.6785221946512269,
        "step": 2461
    },
    {
        "loss": 2.116,
        "grad_norm": 1.15092134475708,
        "learning_rate": 0.000199564854328733,
        "epoch": 0.6787979046043562,
        "step": 2462
    },
    {
        "loss": 2.174,
        "grad_norm": 1.4885456562042236,
        "learning_rate": 0.00019954013055217137,
        "epoch": 0.6790736145574855,
        "step": 2463
    },
    {
        "loss": 2.2088,
        "grad_norm": 1.4840574264526367,
        "learning_rate": 0.00019951472529610974,
        "epoch": 0.6793493245106148,
        "step": 2464
    },
    {
        "loss": 1.9984,
        "grad_norm": 2.0463204383850098,
        "learning_rate": 0.00019948863873447959,
        "epoch": 0.6796250344637441,
        "step": 2465
    },
    {
        "loss": 2.1586,
        "grad_norm": 1.8261109590530396,
        "learning_rate": 0.00019946187104587674,
        "epoch": 0.6799007444168734,
        "step": 2466
    },
    {
        "loss": 1.1162,
        "grad_norm": 1.8611260652542114,
        "learning_rate": 0.00019943442241356034,
        "epoch": 0.6801764543700027,
        "step": 2467
    },
    {
        "loss": 2.6161,
        "grad_norm": 1.166460633277893,
        "learning_rate": 0.0001994062930254513,
        "epoch": 0.680452164323132,
        "step": 2468
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.4695755243301392,
        "learning_rate": 0.00019937748307413133,
        "epoch": 0.6807278742762614,
        "step": 2469
    },
    {
        "loss": 1.4228,
        "grad_norm": 1.737253189086914,
        "learning_rate": 0.00019934799275684136,
        "epoch": 0.6810035842293907,
        "step": 2470
    },
    {
        "loss": 1.9819,
        "grad_norm": 1.6768200397491455,
        "learning_rate": 0.0001993178222754803,
        "epoch": 0.68127929418252,
        "step": 2471
    },
    {
        "loss": 1.9874,
        "grad_norm": 1.9636647701263428,
        "learning_rate": 0.00019928697183660372,
        "epoch": 0.6815550041356493,
        "step": 2472
    },
    {
        "loss": 1.6266,
        "grad_norm": 2.199596405029297,
        "learning_rate": 0.00019925544165142232,
        "epoch": 0.6818307140887786,
        "step": 2473
    },
    {
        "loss": 2.2792,
        "grad_norm": 1.125197172164917,
        "learning_rate": 0.00019922323193580053,
        "epoch": 0.6821064240419079,
        "step": 2474
    },
    {
        "loss": 2.1973,
        "grad_norm": 1.6478254795074463,
        "learning_rate": 0.00019919034291025506,
        "epoch": 0.6823821339950372,
        "step": 2475
    },
    {
        "loss": 2.1298,
        "grad_norm": 1.36417818069458,
        "learning_rate": 0.0001991567747999534,
        "epoch": 0.6826578439481665,
        "step": 2476
    },
    {
        "loss": 2.0234,
        "grad_norm": 1.4786901473999023,
        "learning_rate": 0.0001991225278347121,
        "epoch": 0.6829335539012958,
        "step": 2477
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.7680326700210571,
        "learning_rate": 0.00019908760224899547,
        "epoch": 0.6832092638544252,
        "step": 2478
    },
    {
        "loss": 2.3202,
        "grad_norm": 1.6827765703201294,
        "learning_rate": 0.00019905199828191384,
        "epoch": 0.6834849738075545,
        "step": 2479
    },
    {
        "loss": 2.3722,
        "grad_norm": 1.5664458274841309,
        "learning_rate": 0.00019901571617722191,
        "epoch": 0.6837606837606838,
        "step": 2480
    },
    {
        "loss": 2.3824,
        "grad_norm": 1.5074424743652344,
        "learning_rate": 0.00019897875618331707,
        "epoch": 0.6840363937138131,
        "step": 2481
    },
    {
        "loss": 2.364,
        "grad_norm": 1.0338894128799438,
        "learning_rate": 0.00019894111855323773,
        "epoch": 0.6843121036669424,
        "step": 2482
    },
    {
        "loss": 2.4443,
        "grad_norm": 1.3234381675720215,
        "learning_rate": 0.00019890280354466162,
        "epoch": 0.6845878136200717,
        "step": 2483
    },
    {
        "loss": 2.5729,
        "grad_norm": 1.0419566631317139,
        "learning_rate": 0.000198863811419904,
        "epoch": 0.684863523573201,
        "step": 2484
    },
    {
        "loss": 2.4147,
        "grad_norm": 1.3521854877471924,
        "learning_rate": 0.00019882414244591577,
        "epoch": 0.6851392335263303,
        "step": 2485
    },
    {
        "loss": 1.6806,
        "grad_norm": 1.528481125831604,
        "learning_rate": 0.00019878379689428188,
        "epoch": 0.6854149434794596,
        "step": 2486
    },
    {
        "loss": 1.8406,
        "grad_norm": 1.6268601417541504,
        "learning_rate": 0.00019874277504121915,
        "epoch": 0.685690653432589,
        "step": 2487
    },
    {
        "loss": 1.8103,
        "grad_norm": 1.7301565408706665,
        "learning_rate": 0.00019870107716757468,
        "epoch": 0.6859663633857183,
        "step": 2488
    },
    {
        "loss": 1.5129,
        "grad_norm": 3.3490681648254395,
        "learning_rate": 0.0001986587035588237,
        "epoch": 0.6862420733388476,
        "step": 2489
    },
    {
        "loss": 2.4061,
        "grad_norm": 1.9014840126037598,
        "learning_rate": 0.0001986156545050678,
        "epoch": 0.6865177832919769,
        "step": 2490
    },
    {
        "loss": 1.6605,
        "grad_norm": 1.3492761850357056,
        "learning_rate": 0.00019857193030103275,
        "epoch": 0.6867934932451062,
        "step": 2491
    },
    {
        "loss": 1.6928,
        "grad_norm": 2.252805709838867,
        "learning_rate": 0.00019852753124606674,
        "epoch": 0.6870692031982355,
        "step": 2492
    },
    {
        "loss": 2.1061,
        "grad_norm": 1.1902276277542114,
        "learning_rate": 0.00019848245764413801,
        "epoch": 0.6873449131513648,
        "step": 2493
    },
    {
        "loss": 1.0899,
        "grad_norm": 2.234783172607422,
        "learning_rate": 0.00019843670980383306,
        "epoch": 0.6876206231044941,
        "step": 2494
    },
    {
        "loss": 1.3634,
        "grad_norm": 2.152210235595703,
        "learning_rate": 0.00019839028803835432,
        "epoch": 0.6878963330576234,
        "step": 2495
    },
    {
        "loss": 1.5288,
        "grad_norm": 1.9652756452560425,
        "learning_rate": 0.00019834319266551816,
        "epoch": 0.6881720430107527,
        "step": 2496
    },
    {
        "loss": 1.3078,
        "grad_norm": 2.5616140365600586,
        "learning_rate": 0.00019829542400775268,
        "epoch": 0.6884477529638819,
        "step": 2497
    },
    {
        "loss": 2.4152,
        "grad_norm": 1.620501160621643,
        "learning_rate": 0.00019824698239209536,
        "epoch": 0.6887234629170113,
        "step": 2498
    },
    {
        "loss": 2.4743,
        "grad_norm": 1.2995808124542236,
        "learning_rate": 0.00019819786815019109,
        "epoch": 0.6889991728701406,
        "step": 2499
    },
    {
        "loss": 2.2459,
        "grad_norm": 1.6380527019500732,
        "learning_rate": 0.0001981480816182896,
        "epoch": 0.6892748828232699,
        "step": 2500
    },
    {
        "loss": 2.3109,
        "grad_norm": 1.4601713418960571,
        "learning_rate": 0.00019809762313724344,
        "epoch": 0.6895505927763992,
        "step": 2501
    },
    {
        "loss": 1.9559,
        "grad_norm": 1.7744992971420288,
        "learning_rate": 0.00019804649305250538,
        "epoch": 0.6898263027295285,
        "step": 2502
    },
    {
        "loss": 2.2607,
        "grad_norm": 1.4753509759902954,
        "learning_rate": 0.00019799469171412635,
        "epoch": 0.6901020126826578,
        "step": 2503
    },
    {
        "loss": 2.1104,
        "grad_norm": 2.068026542663574,
        "learning_rate": 0.0001979422194767526,
        "epoch": 0.6903777226357871,
        "step": 2504
    },
    {
        "loss": 2.5959,
        "grad_norm": 1.3941117525100708,
        "learning_rate": 0.0001978890766996239,
        "epoch": 0.6906534325889164,
        "step": 2505
    },
    {
        "loss": 2.0626,
        "grad_norm": 2.136664390563965,
        "learning_rate": 0.0001978352637465703,
        "epoch": 0.6909291425420457,
        "step": 2506
    },
    {
        "loss": 1.8303,
        "grad_norm": 1.6340210437774658,
        "learning_rate": 0.00019778078098601053,
        "epoch": 0.691204852495175,
        "step": 2507
    },
    {
        "loss": 2.6325,
        "grad_norm": 1.7282949686050415,
        "learning_rate": 0.00019772562879094856,
        "epoch": 0.6914805624483044,
        "step": 2508
    },
    {
        "loss": 1.4051,
        "grad_norm": 2.209440231323242,
        "learning_rate": 0.00019766980753897184,
        "epoch": 0.6917562724014337,
        "step": 2509
    },
    {
        "loss": 1.5439,
        "grad_norm": 2.1094136238098145,
        "learning_rate": 0.00019761331761224815,
        "epoch": 0.692031982354563,
        "step": 2510
    },
    {
        "loss": 1.5838,
        "grad_norm": 2.0347964763641357,
        "learning_rate": 0.00019755615939752336,
        "epoch": 0.6923076923076923,
        "step": 2511
    },
    {
        "loss": 1.3722,
        "grad_norm": 1.9955190420150757,
        "learning_rate": 0.00019749833328611843,
        "epoch": 0.6925834022608216,
        "step": 2512
    },
    {
        "loss": 2.1648,
        "grad_norm": 1.7856695652008057,
        "learning_rate": 0.0001974398396739272,
        "epoch": 0.6928591122139509,
        "step": 2513
    },
    {
        "loss": 1.8539,
        "grad_norm": 1.9334957599639893,
        "learning_rate": 0.00019738067896141314,
        "epoch": 0.6931348221670802,
        "step": 2514
    },
    {
        "loss": 1.8515,
        "grad_norm": 1.6915000677108765,
        "learning_rate": 0.00019732085155360708,
        "epoch": 0.6934105321202095,
        "step": 2515
    },
    {
        "loss": 1.9934,
        "grad_norm": 2.665567636489868,
        "learning_rate": 0.00019726035786010404,
        "epoch": 0.6936862420733388,
        "step": 2516
    },
    {
        "loss": 2.0809,
        "grad_norm": 2.0973901748657227,
        "learning_rate": 0.00019719919829506083,
        "epoch": 0.6939619520264682,
        "step": 2517
    },
    {
        "loss": 2.3751,
        "grad_norm": 1.4813848733901978,
        "learning_rate": 0.0001971373732771928,
        "epoch": 0.6942376619795975,
        "step": 2518
    },
    {
        "loss": 2.5808,
        "grad_norm": 1.58046293258667,
        "learning_rate": 0.00019707488322977135,
        "epoch": 0.6945133719327268,
        "step": 2519
    },
    {
        "loss": 2.0744,
        "grad_norm": 1.9718610048294067,
        "learning_rate": 0.00019701172858062064,
        "epoch": 0.6947890818858561,
        "step": 2520
    },
    {
        "loss": 1.7625,
        "grad_norm": 1.5974448919296265,
        "learning_rate": 0.00019694790976211522,
        "epoch": 0.6950647918389854,
        "step": 2521
    },
    {
        "loss": 2.0754,
        "grad_norm": 2.06046199798584,
        "learning_rate": 0.0001968834272111763,
        "epoch": 0.6953405017921147,
        "step": 2522
    },
    {
        "loss": 2.0212,
        "grad_norm": 2.0819971561431885,
        "learning_rate": 0.00019681828136926962,
        "epoch": 0.695616211745244,
        "step": 2523
    },
    {
        "loss": 2.2369,
        "grad_norm": 2.1928372383117676,
        "learning_rate": 0.00019675247268240162,
        "epoch": 0.6958919216983733,
        "step": 2524
    },
    {
        "loss": 0.7722,
        "grad_norm": 2.116533041000366,
        "learning_rate": 0.00019668600160111706,
        "epoch": 0.6961676316515026,
        "step": 2525
    },
    {
        "loss": 2.2239,
        "grad_norm": 1.2445299625396729,
        "learning_rate": 0.00019661886858049536,
        "epoch": 0.696443341604632,
        "step": 2526
    },
    {
        "loss": 1.4954,
        "grad_norm": 1.9108330011367798,
        "learning_rate": 0.00019655107408014807,
        "epoch": 0.6967190515577613,
        "step": 2527
    },
    {
        "loss": 2.1828,
        "grad_norm": 1.2235792875289917,
        "learning_rate": 0.00019648261856421506,
        "epoch": 0.6969947615108906,
        "step": 2528
    },
    {
        "loss": 1.8824,
        "grad_norm": 2.283238649368286,
        "learning_rate": 0.0001964135025013621,
        "epoch": 0.6972704714640199,
        "step": 2529
    },
    {
        "loss": 2.2177,
        "grad_norm": 1.3418185710906982,
        "learning_rate": 0.00019634372636477684,
        "epoch": 0.6975461814171492,
        "step": 2530
    },
    {
        "loss": 2.5848,
        "grad_norm": 1.7076904773712158,
        "learning_rate": 0.00019627329063216635,
        "epoch": 0.6978218913702785,
        "step": 2531
    },
    {
        "loss": 1.5561,
        "grad_norm": 2.306755781173706,
        "learning_rate": 0.00019620219578575312,
        "epoch": 0.6980976013234078,
        "step": 2532
    },
    {
        "loss": 1.6276,
        "grad_norm": 1.8656635284423828,
        "learning_rate": 0.00019613044231227248,
        "epoch": 0.6983733112765371,
        "step": 2533
    },
    {
        "loss": 2.2898,
        "grad_norm": 1.3689645528793335,
        "learning_rate": 0.00019605803070296858,
        "epoch": 0.6986490212296664,
        "step": 2534
    },
    {
        "loss": 1.9804,
        "grad_norm": 1.7536125183105469,
        "learning_rate": 0.0001959849614535916,
        "epoch": 0.6989247311827957,
        "step": 2535
    },
    {
        "loss": 1.7371,
        "grad_norm": 1.4208391904830933,
        "learning_rate": 0.0001959112350643939,
        "epoch": 0.699200441135925,
        "step": 2536
    },
    {
        "loss": 2.5233,
        "grad_norm": 1.156062364578247,
        "learning_rate": 0.00019583685204012708,
        "epoch": 0.6994761510890544,
        "step": 2537
    },
    {
        "loss": 1.2722,
        "grad_norm": 1.6372511386871338,
        "learning_rate": 0.00019576181289003783,
        "epoch": 0.6997518610421837,
        "step": 2538
    },
    {
        "loss": 1.7189,
        "grad_norm": 2.0565733909606934,
        "learning_rate": 0.00019568611812786536,
        "epoch": 0.7000275709953129,
        "step": 2539
    },
    {
        "loss": 2.5789,
        "grad_norm": 1.0173113346099854,
        "learning_rate": 0.00019560976827183687,
        "epoch": 0.7003032809484422,
        "step": 2540
    },
    {
        "loss": 2.6263,
        "grad_norm": 1.052148699760437,
        "learning_rate": 0.00019553276384466498,
        "epoch": 0.7005789909015715,
        "step": 2541
    },
    {
        "loss": 2.412,
        "grad_norm": 1.5766009092330933,
        "learning_rate": 0.00019545510537354336,
        "epoch": 0.7008547008547008,
        "step": 2542
    },
    {
        "loss": 2.3825,
        "grad_norm": 1.4751063585281372,
        "learning_rate": 0.00019537679339014368,
        "epoch": 0.7011304108078301,
        "step": 2543
    },
    {
        "loss": 2.0718,
        "grad_norm": 1.7234073877334595,
        "learning_rate": 0.00019529782843061147,
        "epoch": 0.7014061207609594,
        "step": 2544
    },
    {
        "loss": 1.9666,
        "grad_norm": 1.8649992942810059,
        "learning_rate": 0.00019521821103556298,
        "epoch": 0.7016818307140887,
        "step": 2545
    },
    {
        "loss": 2.5767,
        "grad_norm": 1.0045950412750244,
        "learning_rate": 0.00019513794175008103,
        "epoch": 0.701957540667218,
        "step": 2546
    },
    {
        "loss": 2.1791,
        "grad_norm": 1.3455901145935059,
        "learning_rate": 0.00019505702112371167,
        "epoch": 0.7022332506203474,
        "step": 2547
    },
    {
        "loss": 1.9622,
        "grad_norm": 1.8489527702331543,
        "learning_rate": 0.00019497544971045988,
        "epoch": 0.7025089605734767,
        "step": 2548
    },
    {
        "loss": 2.0459,
        "grad_norm": 1.238292932510376,
        "learning_rate": 0.00019489322806878656,
        "epoch": 0.702784670526606,
        "step": 2549
    },
    {
        "loss": 1.9983,
        "grad_norm": 1.9307701587677002,
        "learning_rate": 0.00019481035676160375,
        "epoch": 0.7030603804797353,
        "step": 2550
    },
    {
        "loss": 1.7512,
        "grad_norm": 2.3230018615722656,
        "learning_rate": 0.00019472683635627175,
        "epoch": 0.7033360904328646,
        "step": 2551
    },
    {
        "loss": 2.0569,
        "grad_norm": 1.3667088747024536,
        "learning_rate": 0.00019464266742459435,
        "epoch": 0.7036118003859939,
        "step": 2552
    },
    {
        "loss": 2.3082,
        "grad_norm": 2.2502336502075195,
        "learning_rate": 0.00019455785054281574,
        "epoch": 0.7038875103391232,
        "step": 2553
    },
    {
        "loss": 1.5081,
        "grad_norm": 1.9201388359069824,
        "learning_rate": 0.00019447238629161575,
        "epoch": 0.7041632202922525,
        "step": 2554
    },
    {
        "loss": 2.2109,
        "grad_norm": 1.4631553888320923,
        "learning_rate": 0.00019438627525610668,
        "epoch": 0.7044389302453818,
        "step": 2555
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.7705590724945068,
        "learning_rate": 0.0001942995180258285,
        "epoch": 0.7047146401985112,
        "step": 2556
    },
    {
        "loss": 1.7107,
        "grad_norm": 1.3539307117462158,
        "learning_rate": 0.00019421211519474562,
        "epoch": 0.7049903501516405,
        "step": 2557
    },
    {
        "loss": 2.2641,
        "grad_norm": 1.6896722316741943,
        "learning_rate": 0.00019412406736124202,
        "epoch": 0.7052660601047698,
        "step": 2558
    },
    {
        "loss": 1.9177,
        "grad_norm": 1.47555673122406,
        "learning_rate": 0.0001940353751281179,
        "epoch": 0.7055417700578991,
        "step": 2559
    },
    {
        "loss": 1.9968,
        "grad_norm": 1.386629343032837,
        "learning_rate": 0.00019394603910258482,
        "epoch": 0.7058174800110284,
        "step": 2560
    },
    {
        "loss": 2.1534,
        "grad_norm": 1.4300298690795898,
        "learning_rate": 0.00019385605989626236,
        "epoch": 0.7060931899641577,
        "step": 2561
    },
    {
        "loss": 1.9398,
        "grad_norm": 2.029595375061035,
        "learning_rate": 0.000193765438125173,
        "epoch": 0.706368899917287,
        "step": 2562
    },
    {
        "loss": 1.499,
        "grad_norm": 2.0152382850646973,
        "learning_rate": 0.00019367417440973895,
        "epoch": 0.7066446098704163,
        "step": 2563
    },
    {
        "loss": 1.9085,
        "grad_norm": 1.3940216302871704,
        "learning_rate": 0.0001935822693747768,
        "epoch": 0.7069203198235456,
        "step": 2564
    },
    {
        "loss": 1.4623,
        "grad_norm": 2.5781161785125732,
        "learning_rate": 0.0001934897236494943,
        "epoch": 0.707196029776675,
        "step": 2565
    },
    {
        "loss": 1.7947,
        "grad_norm": 1.2433207035064697,
        "learning_rate": 0.0001933965378674851,
        "epoch": 0.7074717397298043,
        "step": 2566
    },
    {
        "loss": 1.3531,
        "grad_norm": 1.894423246383667,
        "learning_rate": 0.0001933027126667252,
        "epoch": 0.7077474496829336,
        "step": 2567
    },
    {
        "loss": 2.5596,
        "grad_norm": 1.9524738788604736,
        "learning_rate": 0.00019320824868956796,
        "epoch": 0.7080231596360629,
        "step": 2568
    },
    {
        "loss": 2.4038,
        "grad_norm": 1.7484796047210693,
        "learning_rate": 0.00019311314658274032,
        "epoch": 0.7082988695891922,
        "step": 2569
    },
    {
        "loss": 2.1453,
        "grad_norm": 1.637925624847412,
        "learning_rate": 0.00019301740699733758,
        "epoch": 0.7085745795423215,
        "step": 2570
    },
    {
        "loss": 1.8634,
        "grad_norm": 1.4694771766662598,
        "learning_rate": 0.00019292103058881987,
        "epoch": 0.7088502894954508,
        "step": 2571
    },
    {
        "loss": 1.799,
        "grad_norm": 1.237175703048706,
        "learning_rate": 0.00019282401801700675,
        "epoch": 0.7091259994485801,
        "step": 2572
    },
    {
        "loss": 2.0176,
        "grad_norm": 2.106781005859375,
        "learning_rate": 0.0001927263699460736,
        "epoch": 0.7094017094017094,
        "step": 2573
    },
    {
        "loss": 2.2273,
        "grad_norm": 1.4562643766403198,
        "learning_rate": 0.0001926280870445461,
        "epoch": 0.7096774193548387,
        "step": 2574
    },
    {
        "loss": 2.0418,
        "grad_norm": 1.9453612565994263,
        "learning_rate": 0.00019252916998529663,
        "epoch": 0.7099531293079681,
        "step": 2575
    },
    {
        "loss": 2.1298,
        "grad_norm": 2.0462398529052734,
        "learning_rate": 0.0001924296194455388,
        "epoch": 0.7102288392610974,
        "step": 2576
    },
    {
        "loss": 2.7475,
        "grad_norm": 1.2655682563781738,
        "learning_rate": 0.0001923294361068235,
        "epoch": 0.7105045492142267,
        "step": 2577
    },
    {
        "loss": 1.8866,
        "grad_norm": 1.4797855615615845,
        "learning_rate": 0.00019222862065503368,
        "epoch": 0.710780259167356,
        "step": 2578
    },
    {
        "loss": 1.8792,
        "grad_norm": 1.221189022064209,
        "learning_rate": 0.00019212717378038022,
        "epoch": 0.7110559691204853,
        "step": 2579
    },
    {
        "loss": 2.4751,
        "grad_norm": 2.1573691368103027,
        "learning_rate": 0.00019202509617739652,
        "epoch": 0.7113316790736146,
        "step": 2580
    },
    {
        "loss": 2.4215,
        "grad_norm": 1.2364680767059326,
        "learning_rate": 0.0001919223885449345,
        "epoch": 0.7116073890267439,
        "step": 2581
    },
    {
        "loss": 2.2234,
        "grad_norm": 1.8867073059082031,
        "learning_rate": 0.00019181905158615915,
        "epoch": 0.7118830989798731,
        "step": 2582
    },
    {
        "loss": 1.2544,
        "grad_norm": 2.4517056941986084,
        "learning_rate": 0.00019171508600854422,
        "epoch": 0.7121588089330024,
        "step": 2583
    },
    {
        "loss": 2.2653,
        "grad_norm": 2.025418519973755,
        "learning_rate": 0.00019161049252386687,
        "epoch": 0.7124345188861317,
        "step": 2584
    },
    {
        "loss": 2.3919,
        "grad_norm": 1.5688375234603882,
        "learning_rate": 0.00019150527184820352,
        "epoch": 0.712710228839261,
        "step": 2585
    },
    {
        "loss": 1.4131,
        "grad_norm": 1.8438376188278198,
        "learning_rate": 0.00019139942470192402,
        "epoch": 0.7129859387923904,
        "step": 2586
    },
    {
        "loss": 2.5273,
        "grad_norm": 1.634093165397644,
        "learning_rate": 0.0001912929518096877,
        "epoch": 0.7132616487455197,
        "step": 2587
    },
    {
        "loss": 2.3806,
        "grad_norm": 1.7708386182785034,
        "learning_rate": 0.0001911858539004374,
        "epoch": 0.713537358698649,
        "step": 2588
    },
    {
        "loss": 1.3529,
        "grad_norm": 2.110318660736084,
        "learning_rate": 0.0001910781317073956,
        "epoch": 0.7138130686517783,
        "step": 2589
    },
    {
        "loss": 1.5089,
        "grad_norm": 2.0960164070129395,
        "learning_rate": 0.0001909697859680582,
        "epoch": 0.7140887786049076,
        "step": 2590
    },
    {
        "loss": 2.3971,
        "grad_norm": 1.3390527963638306,
        "learning_rate": 0.00019086081742419056,
        "epoch": 0.7143644885580369,
        "step": 2591
    },
    {
        "loss": 2.5401,
        "grad_norm": 1.2097339630126953,
        "learning_rate": 0.00019075122682182154,
        "epoch": 0.7146401985111662,
        "step": 2592
    },
    {
        "loss": 2.2289,
        "grad_norm": 1.373940348625183,
        "learning_rate": 0.0001906410149112392,
        "epoch": 0.7149159084642955,
        "step": 2593
    },
    {
        "loss": 2.1969,
        "grad_norm": 1.2976428270339966,
        "learning_rate": 0.00019053018244698477,
        "epoch": 0.7151916184174248,
        "step": 2594
    },
    {
        "loss": 2.8637,
        "grad_norm": 1.1728603839874268,
        "learning_rate": 0.00019041873018784848,
        "epoch": 0.7154673283705542,
        "step": 2595
    },
    {
        "loss": 2.3371,
        "grad_norm": 1.7113457918167114,
        "learning_rate": 0.00019030665889686332,
        "epoch": 0.7157430383236835,
        "step": 2596
    },
    {
        "loss": 2.4711,
        "grad_norm": 1.0865716934204102,
        "learning_rate": 0.00019019396934130084,
        "epoch": 0.7160187482768128,
        "step": 2597
    },
    {
        "loss": 1.9258,
        "grad_norm": 1.6183764934539795,
        "learning_rate": 0.000190080662292665,
        "epoch": 0.7162944582299421,
        "step": 2598
    },
    {
        "loss": 2.2453,
        "grad_norm": 1.8644421100616455,
        "learning_rate": 0.00018996673852668763,
        "epoch": 0.7165701681830714,
        "step": 2599
    },
    {
        "loss": 1.934,
        "grad_norm": 2.60479998588562,
        "learning_rate": 0.00018985219882332237,
        "epoch": 0.7168458781362007,
        "step": 2600
    },
    {
        "loss": 2.1301,
        "grad_norm": 1.394610047340393,
        "learning_rate": 0.00018973704396674026,
        "epoch": 0.71712158808933,
        "step": 2601
    },
    {
        "loss": 2.5712,
        "grad_norm": 1.9470516443252563,
        "learning_rate": 0.00018962127474532329,
        "epoch": 0.7173972980424593,
        "step": 2602
    },
    {
        "loss": 2.2712,
        "grad_norm": 1.626834511756897,
        "learning_rate": 0.00018950489195166013,
        "epoch": 0.7176730079955886,
        "step": 2603
    },
    {
        "loss": 2.1956,
        "grad_norm": 1.1777598857879639,
        "learning_rate": 0.0001893878963825396,
        "epoch": 0.717948717948718,
        "step": 2604
    },
    {
        "loss": 2.2395,
        "grad_norm": 1.3734108209609985,
        "learning_rate": 0.0001892702888389462,
        "epoch": 0.7182244279018473,
        "step": 2605
    },
    {
        "loss": 1.9045,
        "grad_norm": 1.9275561571121216,
        "learning_rate": 0.00018915207012605375,
        "epoch": 0.7185001378549766,
        "step": 2606
    },
    {
        "loss": 2.4922,
        "grad_norm": 1.2691278457641602,
        "learning_rate": 0.00018903324105322083,
        "epoch": 0.7187758478081059,
        "step": 2607
    },
    {
        "loss": 2.4801,
        "grad_norm": 1.7910908460617065,
        "learning_rate": 0.00018891380243398414,
        "epoch": 0.7190515577612352,
        "step": 2608
    },
    {
        "loss": 1.81,
        "grad_norm": 2.446943998336792,
        "learning_rate": 0.000188793755086054,
        "epoch": 0.7193272677143645,
        "step": 2609
    },
    {
        "loss": 1.8471,
        "grad_norm": 1.1385990381240845,
        "learning_rate": 0.00018867309983130784,
        "epoch": 0.7196029776674938,
        "step": 2610
    },
    {
        "loss": 2.0105,
        "grad_norm": 1.932957649230957,
        "learning_rate": 0.00018855183749578539,
        "epoch": 0.7198786876206231,
        "step": 2611
    },
    {
        "loss": 2.4223,
        "grad_norm": 1.575217366218567,
        "learning_rate": 0.00018842996890968224,
        "epoch": 0.7201543975737524,
        "step": 2612
    },
    {
        "loss": 1.9346,
        "grad_norm": 1.8280212879180908,
        "learning_rate": 0.00018830749490734495,
        "epoch": 0.7204301075268817,
        "step": 2613
    },
    {
        "loss": 1.7634,
        "grad_norm": 2.319108247756958,
        "learning_rate": 0.0001881844163272644,
        "epoch": 0.7207058174800111,
        "step": 2614
    },
    {
        "loss": 2.1899,
        "grad_norm": 1.3225442171096802,
        "learning_rate": 0.00018806073401207122,
        "epoch": 0.7209815274331404,
        "step": 2615
    },
    {
        "loss": 1.8618,
        "grad_norm": 1.9290746450424194,
        "learning_rate": 0.0001879364488085287,
        "epoch": 0.7212572373862697,
        "step": 2616
    },
    {
        "loss": 2.2033,
        "grad_norm": 1.3935527801513672,
        "learning_rate": 0.00018781156156752834,
        "epoch": 0.721532947339399,
        "step": 2617
    },
    {
        "loss": 1.9633,
        "grad_norm": 2.3632850646972656,
        "learning_rate": 0.00018768607314408273,
        "epoch": 0.7218086572925283,
        "step": 2618
    },
    {
        "loss": 2.1805,
        "grad_norm": 1.2660397291183472,
        "learning_rate": 0.0001875599843973209,
        "epoch": 0.7220843672456576,
        "step": 2619
    },
    {
        "loss": 1.5633,
        "grad_norm": 1.3163321018218994,
        "learning_rate": 0.00018743329619048133,
        "epoch": 0.7223600771987869,
        "step": 2620
    },
    {
        "loss": 2.26,
        "grad_norm": 2.0188748836517334,
        "learning_rate": 0.00018730600939090706,
        "epoch": 0.7226357871519162,
        "step": 2621
    },
    {
        "loss": 1.5862,
        "grad_norm": 2.1899964809417725,
        "learning_rate": 0.00018717812487003876,
        "epoch": 0.7229114971050455,
        "step": 2622
    },
    {
        "loss": 0.9862,
        "grad_norm": 2.0358831882476807,
        "learning_rate": 0.00018704964350340982,
        "epoch": 0.7231872070581749,
        "step": 2623
    },
    {
        "loss": 1.1818,
        "grad_norm": 2.6448566913604736,
        "learning_rate": 0.00018692056617063924,
        "epoch": 0.7234629170113042,
        "step": 2624
    },
    {
        "loss": 1.6207,
        "grad_norm": 2.12972092628479,
        "learning_rate": 0.0001867908937554267,
        "epoch": 0.7237386269644334,
        "step": 2625
    },
    {
        "loss": 1.8902,
        "grad_norm": 1.8918668031692505,
        "learning_rate": 0.00018666062714554546,
        "epoch": 0.7240143369175627,
        "step": 2626
    },
    {
        "loss": 2.3467,
        "grad_norm": 1.704840064048767,
        "learning_rate": 0.0001865297672328374,
        "epoch": 0.724290046870692,
        "step": 2627
    },
    {
        "loss": 2.6604,
        "grad_norm": 2.0619242191314697,
        "learning_rate": 0.0001863983149132056,
        "epoch": 0.7245657568238213,
        "step": 2628
    },
    {
        "loss": 1.9785,
        "grad_norm": 1.4366942644119263,
        "learning_rate": 0.00018626627108660966,
        "epoch": 0.7248414667769506,
        "step": 2629
    },
    {
        "loss": 1.933,
        "grad_norm": 1.9525669813156128,
        "learning_rate": 0.00018613363665705816,
        "epoch": 0.7251171767300799,
        "step": 2630
    },
    {
        "loss": 1.7903,
        "grad_norm": 2.095716953277588,
        "learning_rate": 0.00018600041253260367,
        "epoch": 0.7253928866832092,
        "step": 2631
    },
    {
        "loss": 2.4304,
        "grad_norm": 1.1512497663497925,
        "learning_rate": 0.00018586659962533547,
        "epoch": 0.7256685966363385,
        "step": 2632
    },
    {
        "loss": 2.1372,
        "grad_norm": 1.9001259803771973,
        "learning_rate": 0.00018573219885137428,
        "epoch": 0.7259443065894678,
        "step": 2633
    },
    {
        "loss": 2.2967,
        "grad_norm": 1.900890588760376,
        "learning_rate": 0.00018559721113086498,
        "epoch": 0.7262200165425972,
        "step": 2634
    },
    {
        "loss": 1.4819,
        "grad_norm": 1.9671591520309448,
        "learning_rate": 0.00018546163738797152,
        "epoch": 0.7264957264957265,
        "step": 2635
    },
    {
        "loss": 2.4925,
        "grad_norm": 1.5490057468414307,
        "learning_rate": 0.00018532547855086925,
        "epoch": 0.7267714364488558,
        "step": 2636
    },
    {
        "loss": 1.5709,
        "grad_norm": 2.3190090656280518,
        "learning_rate": 0.00018518873555173983,
        "epoch": 0.7270471464019851,
        "step": 2637
    },
    {
        "loss": 1.9075,
        "grad_norm": 1.2940967082977295,
        "learning_rate": 0.0001850514093267637,
        "epoch": 0.7273228563551144,
        "step": 2638
    },
    {
        "loss": 1.9771,
        "grad_norm": 2.362931251525879,
        "learning_rate": 0.00018491350081611474,
        "epoch": 0.7275985663082437,
        "step": 2639
    },
    {
        "loss": 2.2162,
        "grad_norm": 1.0681334733963013,
        "learning_rate": 0.00018477501096395287,
        "epoch": 0.727874276261373,
        "step": 2640
    },
    {
        "loss": 2.5462,
        "grad_norm": 1.1827515363693237,
        "learning_rate": 0.00018463594071841847,
        "epoch": 0.7281499862145023,
        "step": 2641
    },
    {
        "loss": 1.7079,
        "grad_norm": 2.4208486080169678,
        "learning_rate": 0.000184496291031625,
        "epoch": 0.7284256961676316,
        "step": 2642
    },
    {
        "loss": 1.8102,
        "grad_norm": 3.5258119106292725,
        "learning_rate": 0.00018435606285965338,
        "epoch": 0.728701406120761,
        "step": 2643
    },
    {
        "loss": 1.964,
        "grad_norm": 2.1070141792297363,
        "learning_rate": 0.0001842152571625446,
        "epoch": 0.7289771160738903,
        "step": 2644
    },
    {
        "loss": 1.5665,
        "grad_norm": 2.20957088470459,
        "learning_rate": 0.00018407387490429396,
        "epoch": 0.7292528260270196,
        "step": 2645
    },
    {
        "loss": 2.7243,
        "grad_norm": 1.036902666091919,
        "learning_rate": 0.0001839319170528436,
        "epoch": 0.7295285359801489,
        "step": 2646
    },
    {
        "loss": 1.3743,
        "grad_norm": 2.1342039108276367,
        "learning_rate": 0.00018378938458007683,
        "epoch": 0.7298042459332782,
        "step": 2647
    },
    {
        "loss": 2.3254,
        "grad_norm": 1.3946855068206787,
        "learning_rate": 0.00018364627846181042,
        "epoch": 0.7300799558864075,
        "step": 2648
    },
    {
        "loss": 2.5125,
        "grad_norm": 1.004615306854248,
        "learning_rate": 0.00018350259967778908,
        "epoch": 0.7303556658395368,
        "step": 2649
    },
    {
        "loss": 1.0906,
        "grad_norm": 4.279098033905029,
        "learning_rate": 0.00018335834921167753,
        "epoch": 0.7306313757926661,
        "step": 2650
    },
    {
        "loss": 1.5313,
        "grad_norm": 1.9234697818756104,
        "learning_rate": 0.00018321352805105503,
        "epoch": 0.7309070857457954,
        "step": 2651
    },
    {
        "loss": 1.9333,
        "grad_norm": 1.649593710899353,
        "learning_rate": 0.0001830681371874073,
        "epoch": 0.7311827956989247,
        "step": 2652
    },
    {
        "loss": 1.7094,
        "grad_norm": 2.0554754734039307,
        "learning_rate": 0.0001829221776161211,
        "epoch": 0.7314585056520541,
        "step": 2653
    },
    {
        "loss": 2.2223,
        "grad_norm": 1.1868987083435059,
        "learning_rate": 0.00018277565033647606,
        "epoch": 0.7317342156051834,
        "step": 2654
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.6058199405670166,
        "learning_rate": 0.00018262855635163905,
        "epoch": 0.7320099255583127,
        "step": 2655
    },
    {
        "loss": 2.4259,
        "grad_norm": 1.8650039434432983,
        "learning_rate": 0.0001824808966686563,
        "epoch": 0.732285635511442,
        "step": 2656
    },
    {
        "loss": 2.2561,
        "grad_norm": 1.2285151481628418,
        "learning_rate": 0.00018233267229844744,
        "epoch": 0.7325613454645713,
        "step": 2657
    },
    {
        "loss": 1.9743,
        "grad_norm": 2.00795316696167,
        "learning_rate": 0.00018218388425579756,
        "epoch": 0.7328370554177006,
        "step": 2658
    },
    {
        "loss": 2.434,
        "grad_norm": 2.0067801475524902,
        "learning_rate": 0.0001820345335593514,
        "epoch": 0.7331127653708299,
        "step": 2659
    },
    {
        "loss": 1.3647,
        "grad_norm": 2.3959856033325195,
        "learning_rate": 0.00018188462123160522,
        "epoch": 0.7333884753239592,
        "step": 2660
    },
    {
        "loss": 1.6114,
        "grad_norm": 2.430133104324341,
        "learning_rate": 0.00018173414829890093,
        "epoch": 0.7336641852770885,
        "step": 2661
    },
    {
        "loss": 2.1406,
        "grad_norm": 1.9402343034744263,
        "learning_rate": 0.00018158311579141785,
        "epoch": 0.7339398952302179,
        "step": 2662
    },
    {
        "loss": 1.3241,
        "grad_norm": 1.7636348009109497,
        "learning_rate": 0.00018143152474316701,
        "epoch": 0.7342156051833472,
        "step": 2663
    },
    {
        "loss": 1.3679,
        "grad_norm": 1.7589037418365479,
        "learning_rate": 0.00018127937619198266,
        "epoch": 0.7344913151364765,
        "step": 2664
    },
    {
        "loss": 2.085,
        "grad_norm": 2.161895751953125,
        "learning_rate": 0.00018112667117951657,
        "epoch": 0.7347670250896058,
        "step": 2665
    },
    {
        "loss": 2.1088,
        "grad_norm": 1.3539081811904907,
        "learning_rate": 0.00018097341075122954,
        "epoch": 0.7350427350427351,
        "step": 2666
    },
    {
        "loss": 1.6256,
        "grad_norm": 1.7495681047439575,
        "learning_rate": 0.00018081959595638554,
        "epoch": 0.7353184449958643,
        "step": 2667
    },
    {
        "loss": 1.9772,
        "grad_norm": 1.997304916381836,
        "learning_rate": 0.0001806652278480432,
        "epoch": 0.7355941549489936,
        "step": 2668
    },
    {
        "loss": 1.5333,
        "grad_norm": 1.8761614561080933,
        "learning_rate": 0.00018051030748305,
        "epoch": 0.7358698649021229,
        "step": 2669
    },
    {
        "loss": 2.0117,
        "grad_norm": 2.1870219707489014,
        "learning_rate": 0.0001803548359220336,
        "epoch": 0.7361455748552522,
        "step": 2670
    },
    {
        "loss": 2.2983,
        "grad_norm": 2.1035079956054688,
        "learning_rate": 0.00018019881422939608,
        "epoch": 0.7364212848083815,
        "step": 2671
    },
    {
        "loss": 2.2981,
        "grad_norm": 1.5066081285476685,
        "learning_rate": 0.00018004224347330512,
        "epoch": 0.7366969947615108,
        "step": 2672
    },
    {
        "loss": 1.6265,
        "grad_norm": 1.5148366689682007,
        "learning_rate": 0.00017988512472568807,
        "epoch": 0.7369727047146402,
        "step": 2673
    },
    {
        "loss": 2.2953,
        "grad_norm": 1.5948748588562012,
        "learning_rate": 0.00017972745906222342,
        "epoch": 0.7372484146677695,
        "step": 2674
    },
    {
        "loss": 1.7871,
        "grad_norm": 1.4706783294677734,
        "learning_rate": 0.00017956924756233462,
        "epoch": 0.7375241246208988,
        "step": 2675
    },
    {
        "loss": 2.5158,
        "grad_norm": 1.3587647676467896,
        "learning_rate": 0.00017941049130918135,
        "epoch": 0.7377998345740281,
        "step": 2676
    },
    {
        "loss": 1.9664,
        "grad_norm": 1.7225620746612549,
        "learning_rate": 0.00017925119138965356,
        "epoch": 0.7380755445271574,
        "step": 2677
    },
    {
        "loss": 2.1674,
        "grad_norm": 1.1782219409942627,
        "learning_rate": 0.00017909134889436263,
        "epoch": 0.7383512544802867,
        "step": 2678
    },
    {
        "loss": 2.0326,
        "grad_norm": 1.6983397006988525,
        "learning_rate": 0.00017893096491763518,
        "epoch": 0.738626964433416,
        "step": 2679
    },
    {
        "loss": 2.461,
        "grad_norm": 1.2218165397644043,
        "learning_rate": 0.0001787700405575043,
        "epoch": 0.7389026743865453,
        "step": 2680
    },
    {
        "loss": 1.5598,
        "grad_norm": 1.9216513633728027,
        "learning_rate": 0.00017860857691570344,
        "epoch": 0.7391783843396746,
        "step": 2681
    },
    {
        "loss": 2.3571,
        "grad_norm": 1.6774190664291382,
        "learning_rate": 0.00017844657509765734,
        "epoch": 0.739454094292804,
        "step": 2682
    },
    {
        "loss": 2.6431,
        "grad_norm": 1.1411386728286743,
        "learning_rate": 0.000178284036212476,
        "epoch": 0.7397298042459333,
        "step": 2683
    },
    {
        "loss": 1.544,
        "grad_norm": 1.9840974807739258,
        "learning_rate": 0.0001781209613729456,
        "epoch": 0.7400055141990626,
        "step": 2684
    },
    {
        "loss": 1.5854,
        "grad_norm": 2.0928428173065186,
        "learning_rate": 0.00017795735169552233,
        "epoch": 0.7402812241521919,
        "step": 2685
    },
    {
        "loss": 2.2546,
        "grad_norm": 1.8667361736297607,
        "learning_rate": 0.00017779320830032334,
        "epoch": 0.7405569341053212,
        "step": 2686
    },
    {
        "loss": 1.9907,
        "grad_norm": 1.3129246234893799,
        "learning_rate": 0.00017762853231112052,
        "epoch": 0.7408326440584505,
        "step": 2687
    },
    {
        "loss": 2.3313,
        "grad_norm": 1.0726399421691895,
        "learning_rate": 0.00017746332485533116,
        "epoch": 0.7411083540115798,
        "step": 2688
    },
    {
        "loss": 1.9877,
        "grad_norm": 2.0646255016326904,
        "learning_rate": 0.00017729758706401202,
        "epoch": 0.7413840639647091,
        "step": 2689
    },
    {
        "loss": 2.3496,
        "grad_norm": 1.6401166915893555,
        "learning_rate": 0.00017713132007184985,
        "epoch": 0.7416597739178384,
        "step": 2690
    },
    {
        "loss": 2.1603,
        "grad_norm": 1.2685375213623047,
        "learning_rate": 0.00017696452501715523,
        "epoch": 0.7419354838709677,
        "step": 2691
    },
    {
        "loss": 2.0959,
        "grad_norm": 2.2400126457214355,
        "learning_rate": 0.0001767972030418533,
        "epoch": 0.7422111938240971,
        "step": 2692
    },
    {
        "loss": 2.0502,
        "grad_norm": 1.686639428138733,
        "learning_rate": 0.0001766293552914773,
        "epoch": 0.7424869037772264,
        "step": 2693
    },
    {
        "loss": 1.9599,
        "grad_norm": 2.124912738800049,
        "learning_rate": 0.00017646098291515937,
        "epoch": 0.7427626137303557,
        "step": 2694
    },
    {
        "loss": 2.0138,
        "grad_norm": 1.3477885723114014,
        "learning_rate": 0.00017629208706562417,
        "epoch": 0.743038323683485,
        "step": 2695
    },
    {
        "loss": 1.4809,
        "grad_norm": 1.5311561822891235,
        "learning_rate": 0.00017612266889917945,
        "epoch": 0.7433140336366143,
        "step": 2696
    },
    {
        "loss": 2.2426,
        "grad_norm": 1.7039214372634888,
        "learning_rate": 0.00017595272957570955,
        "epoch": 0.7435897435897436,
        "step": 2697
    },
    {
        "loss": 2.229,
        "grad_norm": 1.8243242502212524,
        "learning_rate": 0.00017578227025866615,
        "epoch": 0.7438654535428729,
        "step": 2698
    },
    {
        "loss": 2.3292,
        "grad_norm": 1.7465778589248657,
        "learning_rate": 0.00017561129211506163,
        "epoch": 0.7441411634960022,
        "step": 2699
    },
    {
        "loss": 2.3559,
        "grad_norm": 1.4228785037994385,
        "learning_rate": 0.00017543979631545967,
        "epoch": 0.7444168734491315,
        "step": 2700
    },
    {
        "loss": 2.2109,
        "grad_norm": 1.107954978942871,
        "learning_rate": 0.0001752677840339688,
        "epoch": 0.7446925834022609,
        "step": 2701
    },
    {
        "loss": 1.5806,
        "grad_norm": 2.02833890914917,
        "learning_rate": 0.0001750952564482326,
        "epoch": 0.7449682933553902,
        "step": 2702
    },
    {
        "loss": 2.0068,
        "grad_norm": 1.1495686769485474,
        "learning_rate": 0.00017492221473942355,
        "epoch": 0.7452440033085195,
        "step": 2703
    },
    {
        "loss": 2.0123,
        "grad_norm": 2.0142056941986084,
        "learning_rate": 0.00017474866009223295,
        "epoch": 0.7455197132616488,
        "step": 2704
    },
    {
        "loss": 1.7172,
        "grad_norm": 2.045184850692749,
        "learning_rate": 0.0001745745936948648,
        "epoch": 0.7457954232147781,
        "step": 2705
    },
    {
        "loss": 2.5255,
        "grad_norm": 1.4620834589004517,
        "learning_rate": 0.00017440001673902578,
        "epoch": 0.7460711331679074,
        "step": 2706
    },
    {
        "loss": 2.0945,
        "grad_norm": 1.3284417390823364,
        "learning_rate": 0.00017422493041991882,
        "epoch": 0.7463468431210367,
        "step": 2707
    },
    {
        "loss": 2.6098,
        "grad_norm": 1.2776542901992798,
        "learning_rate": 0.00017404933593623335,
        "epoch": 0.746622553074166,
        "step": 2708
    },
    {
        "loss": 1.5914,
        "grad_norm": 2.502129316329956,
        "learning_rate": 0.0001738732344901385,
        "epoch": 0.7468982630272953,
        "step": 2709
    },
    {
        "loss": 2.3758,
        "grad_norm": 1.2546757459640503,
        "learning_rate": 0.0001736966272872736,
        "epoch": 0.7471739729804245,
        "step": 2710
    },
    {
        "loss": 1.2588,
        "grad_norm": 2.0798683166503906,
        "learning_rate": 0.00017351951553674114,
        "epoch": 0.7474496829335538,
        "step": 2711
    },
    {
        "loss": 2.1037,
        "grad_norm": 1.905735731124878,
        "learning_rate": 0.00017334190045109718,
        "epoch": 0.7477253928866832,
        "step": 2712
    },
    {
        "loss": 1.8794,
        "grad_norm": 1.9388033151626587,
        "learning_rate": 0.0001731637832463446,
        "epoch": 0.7480011028398125,
        "step": 2713
    },
    {
        "loss": 2.1073,
        "grad_norm": 1.155442714691162,
        "learning_rate": 0.000172985165141923,
        "epoch": 0.7482768127929418,
        "step": 2714
    },
    {
        "loss": 2.6551,
        "grad_norm": 1.3428971767425537,
        "learning_rate": 0.00017280604736070215,
        "epoch": 0.7485525227460711,
        "step": 2715
    },
    {
        "loss": 2.115,
        "grad_norm": 1.325927495956421,
        "learning_rate": 0.00017262643112897198,
        "epoch": 0.7488282326992004,
        "step": 2716
    },
    {
        "loss": 1.7887,
        "grad_norm": 1.9338793754577637,
        "learning_rate": 0.00017244631767643573,
        "epoch": 0.7491039426523297,
        "step": 2717
    },
    {
        "loss": 2.3523,
        "grad_norm": 1.3762861490249634,
        "learning_rate": 0.0001722657082361999,
        "epoch": 0.749379652605459,
        "step": 2718
    },
    {
        "loss": 2.2481,
        "grad_norm": 1.098649501800537,
        "learning_rate": 0.0001720846040447675,
        "epoch": 0.7496553625585883,
        "step": 2719
    },
    {
        "loss": 1.6158,
        "grad_norm": 1.8829689025878906,
        "learning_rate": 0.00017190300634202797,
        "epoch": 0.7499310725117176,
        "step": 2720
    },
    {
        "loss": 1.829,
        "grad_norm": 1.3196536302566528,
        "learning_rate": 0.00017172091637125022,
        "epoch": 0.750206782464847,
        "step": 2721
    },
    {
        "loss": 1.9045,
        "grad_norm": 2.0067532062530518,
        "learning_rate": 0.00017153833537907257,
        "epoch": 0.7504824924179763,
        "step": 2722
    },
    {
        "loss": 2.1302,
        "grad_norm": 1.4953240156173706,
        "learning_rate": 0.00017135526461549584,
        "epoch": 0.7507582023711056,
        "step": 2723
    },
    {
        "loss": 1.4416,
        "grad_norm": 1.881122350692749,
        "learning_rate": 0.00017117170533387314,
        "epoch": 0.7510339123242349,
        "step": 2724
    },
    {
        "loss": 2.0927,
        "grad_norm": 1.5390641689300537,
        "learning_rate": 0.0001709876587909029,
        "epoch": 0.7513096222773642,
        "step": 2725
    },
    {
        "loss": 2.3366,
        "grad_norm": 1.4399704933166504,
        "learning_rate": 0.00017080312624661873,
        "epoch": 0.7515853322304935,
        "step": 2726
    },
    {
        "loss": 2.0482,
        "grad_norm": 1.79151451587677,
        "learning_rate": 0.00017061810896438232,
        "epoch": 0.7518610421836228,
        "step": 2727
    },
    {
        "loss": 2.4658,
        "grad_norm": 1.2066258192062378,
        "learning_rate": 0.00017043260821087311,
        "epoch": 0.7521367521367521,
        "step": 2728
    },
    {
        "loss": 2.1277,
        "grad_norm": 1.988601565361023,
        "learning_rate": 0.0001702466252560814,
        "epoch": 0.7524124620898814,
        "step": 2729
    },
    {
        "loss": 1.9696,
        "grad_norm": 1.6642844676971436,
        "learning_rate": 0.0001700601613732981,
        "epoch": 0.7526881720430108,
        "step": 2730
    },
    {
        "loss": 1.7783,
        "grad_norm": 1.5434019565582275,
        "learning_rate": 0.00016987321783910729,
        "epoch": 0.7529638819961401,
        "step": 2731
    },
    {
        "loss": 2.4715,
        "grad_norm": 1.4045732021331787,
        "learning_rate": 0.0001696857959333761,
        "epoch": 0.7532395919492694,
        "step": 2732
    },
    {
        "loss": 1.8758,
        "grad_norm": 1.7502086162567139,
        "learning_rate": 0.00016949789693924765,
        "epoch": 0.7535153019023987,
        "step": 2733
    },
    {
        "loss": 1.7569,
        "grad_norm": 1.797737717628479,
        "learning_rate": 0.00016930952214313044,
        "epoch": 0.753791011855528,
        "step": 2734
    },
    {
        "loss": 1.5237,
        "grad_norm": 1.7306400537490845,
        "learning_rate": 0.00016912067283469132,
        "epoch": 0.7540667218086573,
        "step": 2735
    },
    {
        "loss": 2.223,
        "grad_norm": 1.194682002067566,
        "learning_rate": 0.00016893135030684485,
        "epoch": 0.7543424317617866,
        "step": 2736
    },
    {
        "loss": 2.5344,
        "grad_norm": 1.3812929391860962,
        "learning_rate": 0.0001687415558557463,
        "epoch": 0.7546181417149159,
        "step": 2737
    },
    {
        "loss": 1.7512,
        "grad_norm": 2.9266433715820312,
        "learning_rate": 0.00016855129078078104,
        "epoch": 0.7548938516680452,
        "step": 2738
    },
    {
        "loss": 2.0575,
        "grad_norm": 1.9076043367385864,
        "learning_rate": 0.00016836055638455726,
        "epoch": 0.7551695616211745,
        "step": 2739
    },
    {
        "loss": 2.1589,
        "grad_norm": 2.0709362030029297,
        "learning_rate": 0.00016816935397289542,
        "epoch": 0.7554452715743039,
        "step": 2740
    },
    {
        "loss": 1.8276,
        "grad_norm": 1.2411571741104126,
        "learning_rate": 0.00016797768485482095,
        "epoch": 0.7557209815274332,
        "step": 2741
    },
    {
        "loss": 1.8358,
        "grad_norm": 2.434598684310913,
        "learning_rate": 0.0001677855503425537,
        "epoch": 0.7559966914805625,
        "step": 2742
    },
    {
        "loss": 2.1002,
        "grad_norm": 1.902623176574707,
        "learning_rate": 0.00016759295175150057,
        "epoch": 0.7562724014336918,
        "step": 2743
    },
    {
        "loss": 2.207,
        "grad_norm": 1.6697365045547485,
        "learning_rate": 0.00016739989040024481,
        "epoch": 0.7565481113868211,
        "step": 2744
    },
    {
        "loss": 2.8316,
        "grad_norm": 1.5590531826019287,
        "learning_rate": 0.0001672063676105387,
        "epoch": 0.7568238213399504,
        "step": 2745
    },
    {
        "loss": 1.4023,
        "grad_norm": 2.357003688812256,
        "learning_rate": 0.00016701238470729282,
        "epoch": 0.7570995312930797,
        "step": 2746
    },
    {
        "loss": 1.5554,
        "grad_norm": 1.973612666130066,
        "learning_rate": 0.00016681794301856865,
        "epoch": 0.757375241246209,
        "step": 2747
    },
    {
        "loss": 2.3163,
        "grad_norm": 2.0708250999450684,
        "learning_rate": 0.00016662304387556783,
        "epoch": 0.7576509511993383,
        "step": 2748
    },
    {
        "loss": 2.2295,
        "grad_norm": 1.18851900100708,
        "learning_rate": 0.00016642768861262466,
        "epoch": 0.7579266611524677,
        "step": 2749
    },
    {
        "loss": 1.7074,
        "grad_norm": 1.6642483472824097,
        "learning_rate": 0.00016623187856719534,
        "epoch": 0.758202371105597,
        "step": 2750
    },
    {
        "loss": 1.4614,
        "grad_norm": 1.9837757349014282,
        "learning_rate": 0.0001660356150798505,
        "epoch": 0.7584780810587263,
        "step": 2751
    },
    {
        "loss": 1.4558,
        "grad_norm": 2.012315511703491,
        "learning_rate": 0.0001658388994942643,
        "epoch": 0.7587537910118556,
        "step": 2752
    },
    {
        "loss": 2.6429,
        "grad_norm": 1.1653937101364136,
        "learning_rate": 0.00016564173315720694,
        "epoch": 0.7590295009649848,
        "step": 2753
    },
    {
        "loss": 2.2549,
        "grad_norm": 1.5010151863098145,
        "learning_rate": 0.00016544411741853366,
        "epoch": 0.7593052109181141,
        "step": 2754
    },
    {
        "loss": 2.4143,
        "grad_norm": 1.3036500215530396,
        "learning_rate": 0.00016524605363117746,
        "epoch": 0.7595809208712434,
        "step": 2755
    },
    {
        "loss": 2.0326,
        "grad_norm": 2.168898582458496,
        "learning_rate": 0.00016504754315113784,
        "epoch": 0.7598566308243727,
        "step": 2756
    },
    {
        "loss": 1.9251,
        "grad_norm": 1.2069553136825562,
        "learning_rate": 0.0001648485873374733,
        "epoch": 0.760132340777502,
        "step": 2757
    },
    {
        "loss": 2.3262,
        "grad_norm": 1.0849193334579468,
        "learning_rate": 0.0001646491875522904,
        "epoch": 0.7604080507306313,
        "step": 2758
    },
    {
        "loss": 2.2132,
        "grad_norm": 1.7190123796463013,
        "learning_rate": 0.0001644493451607362,
        "epoch": 0.7606837606837606,
        "step": 2759
    },
    {
        "loss": 1.8608,
        "grad_norm": 1.8535983562469482,
        "learning_rate": 0.00016424906153098695,
        "epoch": 0.76095947063689,
        "step": 2760
    },
    {
        "loss": 1.7867,
        "grad_norm": 1.8068147897720337,
        "learning_rate": 0.0001640483380342407,
        "epoch": 0.7612351805900193,
        "step": 2761
    },
    {
        "loss": 1.9786,
        "grad_norm": 1.2508560419082642,
        "learning_rate": 0.00016384717604470608,
        "epoch": 0.7615108905431486,
        "step": 2762
    },
    {
        "loss": 2.4428,
        "grad_norm": 2.0090620517730713,
        "learning_rate": 0.0001636455769395946,
        "epoch": 0.7617866004962779,
        "step": 2763
    },
    {
        "loss": 2.4799,
        "grad_norm": 1.9501097202301025,
        "learning_rate": 0.00016344354209910954,
        "epoch": 0.7620623104494072,
        "step": 2764
    },
    {
        "loss": 2.1884,
        "grad_norm": 1.4149338006973267,
        "learning_rate": 0.00016324107290643824,
        "epoch": 0.7623380204025365,
        "step": 2765
    },
    {
        "loss": 1.3459,
        "grad_norm": 2.048137903213501,
        "learning_rate": 0.00016303817074774077,
        "epoch": 0.7626137303556658,
        "step": 2766
    },
    {
        "loss": 2.2135,
        "grad_norm": 1.6303675174713135,
        "learning_rate": 0.00016283483701214242,
        "epoch": 0.7628894403087951,
        "step": 2767
    },
    {
        "loss": 2.2511,
        "grad_norm": 1.561040997505188,
        "learning_rate": 0.00016263107309172215,
        "epoch": 0.7631651502619244,
        "step": 2768
    },
    {
        "loss": 1.9917,
        "grad_norm": 1.1922889947891235,
        "learning_rate": 0.0001624268803815051,
        "epoch": 0.7634408602150538,
        "step": 2769
    },
    {
        "loss": 2.0534,
        "grad_norm": 1.6229736804962158,
        "learning_rate": 0.00016222226027945107,
        "epoch": 0.7637165701681831,
        "step": 2770
    },
    {
        "loss": 2.3795,
        "grad_norm": 1.364435076713562,
        "learning_rate": 0.00016201721418644674,
        "epoch": 0.7639922801213124,
        "step": 2771
    },
    {
        "loss": 0.9369,
        "grad_norm": 1.7568566799163818,
        "learning_rate": 0.00016181174350629452,
        "epoch": 0.7642679900744417,
        "step": 2772
    },
    {
        "loss": 2.2941,
        "grad_norm": 1.5534192323684692,
        "learning_rate": 0.00016160584964570432,
        "epoch": 0.764543700027571,
        "step": 2773
    },
    {
        "loss": 2.0435,
        "grad_norm": 1.8788443803787231,
        "learning_rate": 0.00016139953401428261,
        "epoch": 0.7648194099807003,
        "step": 2774
    },
    {
        "loss": 2.0345,
        "grad_norm": 2.0238149166107178,
        "learning_rate": 0.00016119279802452415,
        "epoch": 0.7650951199338296,
        "step": 2775
    },
    {
        "loss": 1.5013,
        "grad_norm": 1.7202228307724,
        "learning_rate": 0.00016098564309180072,
        "epoch": 0.7653708298869589,
        "step": 2776
    },
    {
        "loss": 2.2777,
        "grad_norm": 1.4626332521438599,
        "learning_rate": 0.00016077807063435325,
        "epoch": 0.7656465398400882,
        "step": 2777
    },
    {
        "loss": 2.1808,
        "grad_norm": 1.887181282043457,
        "learning_rate": 0.0001605700820732803,
        "epoch": 0.7659222497932175,
        "step": 2778
    },
    {
        "loss": 1.911,
        "grad_norm": 1.9404796361923218,
        "learning_rate": 0.0001603616788325299,
        "epoch": 0.7661979597463469,
        "step": 2779
    },
    {
        "loss": 2.173,
        "grad_norm": 1.5806920528411865,
        "learning_rate": 0.00016015286233888841,
        "epoch": 0.7664736696994762,
        "step": 2780
    },
    {
        "loss": 1.7644,
        "grad_norm": 1.7898703813552856,
        "learning_rate": 0.00015994363402197222,
        "epoch": 0.7667493796526055,
        "step": 2781
    },
    {
        "loss": 1.8551,
        "grad_norm": 1.5423808097839355,
        "learning_rate": 0.00015973399531421634,
        "epoch": 0.7670250896057348,
        "step": 2782
    },
    {
        "loss": 2.223,
        "grad_norm": 1.3976314067840576,
        "learning_rate": 0.00015952394765086618,
        "epoch": 0.7673007995588641,
        "step": 2783
    },
    {
        "loss": 1.7582,
        "grad_norm": 2.0585532188415527,
        "learning_rate": 0.0001593134924699663,
        "epoch": 0.7675765095119934,
        "step": 2784
    },
    {
        "loss": 1.489,
        "grad_norm": 2.3482322692871094,
        "learning_rate": 0.00015910263121235197,
        "epoch": 0.7678522194651227,
        "step": 2785
    },
    {
        "loss": 2.4806,
        "grad_norm": 0.9675496816635132,
        "learning_rate": 0.00015889136532163772,
        "epoch": 0.768127929418252,
        "step": 2786
    },
    {
        "loss": 2.6497,
        "grad_norm": 1.2715474367141724,
        "learning_rate": 0.00015867969624420923,
        "epoch": 0.7684036393713813,
        "step": 2787
    },
    {
        "loss": 2.2894,
        "grad_norm": 1.2064789533615112,
        "learning_rate": 0.00015846762542921165,
        "epoch": 0.7686793493245107,
        "step": 2788
    },
    {
        "loss": 1.7435,
        "grad_norm": 2.3338074684143066,
        "learning_rate": 0.00015825515432854132,
        "epoch": 0.76895505927764,
        "step": 2789
    },
    {
        "loss": 2.7193,
        "grad_norm": 1.5387542247772217,
        "learning_rate": 0.0001580422843968342,
        "epoch": 0.7692307692307693,
        "step": 2790
    },
    {
        "loss": 1.5657,
        "grad_norm": 2.1486237049102783,
        "learning_rate": 0.00015782901709145776,
        "epoch": 0.7695064791838986,
        "step": 2791
    },
    {
        "loss": 2.1096,
        "grad_norm": 2.032710075378418,
        "learning_rate": 0.00015761535387249893,
        "epoch": 0.7697821891370279,
        "step": 2792
    },
    {
        "loss": 1.5382,
        "grad_norm": 2.0776419639587402,
        "learning_rate": 0.0001574012962027562,
        "epoch": 0.7700578990901572,
        "step": 2793
    },
    {
        "loss": 1.2761,
        "grad_norm": 2.41182804107666,
        "learning_rate": 0.0001571868455477276,
        "epoch": 0.7703336090432865,
        "step": 2794
    },
    {
        "loss": 2.4783,
        "grad_norm": 1.384554147720337,
        "learning_rate": 0.00015697200337560263,
        "epoch": 0.7706093189964157,
        "step": 2795
    },
    {
        "loss": 1.5829,
        "grad_norm": 2.009765625,
        "learning_rate": 0.00015675677115725023,
        "epoch": 0.770885028949545,
        "step": 2796
    },
    {
        "loss": 1.9673,
        "grad_norm": 1.6342120170593262,
        "learning_rate": 0.0001565411503662107,
        "epoch": 0.7711607389026743,
        "step": 2797
    },
    {
        "loss": 1.3717,
        "grad_norm": 1.832908272743225,
        "learning_rate": 0.00015632514247868363,
        "epoch": 0.7714364488558036,
        "step": 2798
    },
    {
        "loss": 1.5766,
        "grad_norm": 2.026250123977661,
        "learning_rate": 0.00015610874897351978,
        "epoch": 0.771712158808933,
        "step": 2799
    },
    {
        "loss": 1.8272,
        "grad_norm": 1.5763484239578247,
        "learning_rate": 0.00015589197133220894,
        "epoch": 0.7719878687620623,
        "step": 2800
    },
    {
        "loss": 2.4763,
        "grad_norm": 1.2543888092041016,
        "learning_rate": 0.0001556748110388718,
        "epoch": 0.7722635787151916,
        "step": 2801
    },
    {
        "loss": 2.1369,
        "grad_norm": 0.9694482088088989,
        "learning_rate": 0.0001554572695802478,
        "epoch": 0.7725392886683209,
        "step": 2802
    },
    {
        "loss": 1.301,
        "grad_norm": 2.116532564163208,
        "learning_rate": 0.00015523934844568696,
        "epoch": 0.7728149986214502,
        "step": 2803
    },
    {
        "loss": 1.5094,
        "grad_norm": 2.32102108001709,
        "learning_rate": 0.00015502104912713772,
        "epoch": 0.7730907085745795,
        "step": 2804
    },
    {
        "loss": 1.2754,
        "grad_norm": 2.0595223903656006,
        "learning_rate": 0.0001548023731191385,
        "epoch": 0.7733664185277088,
        "step": 2805
    },
    {
        "loss": 1.0884,
        "grad_norm": 1.608815312385559,
        "learning_rate": 0.0001545833219188058,
        "epoch": 0.7736421284808381,
        "step": 2806
    },
    {
        "loss": 1.6279,
        "grad_norm": 2.042701482772827,
        "learning_rate": 0.00015436389702582574,
        "epoch": 0.7739178384339674,
        "step": 2807
    },
    {
        "loss": 1.9493,
        "grad_norm": 1.8677492141723633,
        "learning_rate": 0.00015414409994244182,
        "epoch": 0.7741935483870968,
        "step": 2808
    },
    {
        "loss": 2.635,
        "grad_norm": 1.371688723564148,
        "learning_rate": 0.00015392393217344673,
        "epoch": 0.7744692583402261,
        "step": 2809
    },
    {
        "loss": 2.1121,
        "grad_norm": 1.1190897226333618,
        "learning_rate": 0.0001537033952261699,
        "epoch": 0.7747449682933554,
        "step": 2810
    },
    {
        "loss": 2.1567,
        "grad_norm": 0.8995214104652405,
        "learning_rate": 0.0001534824906104693,
        "epoch": 0.7750206782464847,
        "step": 2811
    },
    {
        "loss": 1.9205,
        "grad_norm": 2.1508796215057373,
        "learning_rate": 0.00015326121983871903,
        "epoch": 0.775296388199614,
        "step": 2812
    },
    {
        "loss": 1.9682,
        "grad_norm": 1.6609231233596802,
        "learning_rate": 0.00015303958442580104,
        "epoch": 0.7755720981527433,
        "step": 2813
    },
    {
        "loss": 2.2938,
        "grad_norm": 1.8327466249465942,
        "learning_rate": 0.0001528175858890927,
        "epoch": 0.7758478081058726,
        "step": 2814
    },
    {
        "loss": 2.3795,
        "grad_norm": 1.169377326965332,
        "learning_rate": 0.00015259522574845836,
        "epoch": 0.7761235180590019,
        "step": 2815
    },
    {
        "loss": 2.0328,
        "grad_norm": 1.6757992506027222,
        "learning_rate": 0.00015237250552623715,
        "epoch": 0.7763992280121312,
        "step": 2816
    },
    {
        "loss": 2.7681,
        "grad_norm": 1.1964664459228516,
        "learning_rate": 0.00015214942674723425,
        "epoch": 0.7766749379652605,
        "step": 2817
    },
    {
        "loss": 2.2075,
        "grad_norm": 1.1752132177352905,
        "learning_rate": 0.0001519259909387088,
        "epoch": 0.7769506479183899,
        "step": 2818
    },
    {
        "loss": 2.5825,
        "grad_norm": 1.8816299438476562,
        "learning_rate": 0.0001517021996303651,
        "epoch": 0.7772263578715192,
        "step": 2819
    },
    {
        "loss": 2.1443,
        "grad_norm": 0.8637461066246033,
        "learning_rate": 0.00015147805435434037,
        "epoch": 0.7775020678246485,
        "step": 2820
    },
    {
        "loss": 2.5241,
        "grad_norm": 1.091278076171875,
        "learning_rate": 0.00015125355664519628,
        "epoch": 0.7777777777777778,
        "step": 2821
    },
    {
        "loss": 1.7191,
        "grad_norm": 1.9788261651992798,
        "learning_rate": 0.00015102870803990616,
        "epoch": 0.7780534877309071,
        "step": 2822
    },
    {
        "loss": 1.2311,
        "grad_norm": 2.0656120777130127,
        "learning_rate": 0.00015080351007784688,
        "epoch": 0.7783291976840364,
        "step": 2823
    },
    {
        "loss": 2.073,
        "grad_norm": 1.5949153900146484,
        "learning_rate": 0.00015057796430078594,
        "epoch": 0.7786049076371657,
        "step": 2824
    },
    {
        "loss": 1.8836,
        "grad_norm": 1.7977875471115112,
        "learning_rate": 0.00015035207225287314,
        "epoch": 0.778880617590295,
        "step": 2825
    },
    {
        "loss": 2.0263,
        "grad_norm": 2.0941972732543945,
        "learning_rate": 0.00015012583548062802,
        "epoch": 0.7791563275434243,
        "step": 2826
    },
    {
        "loss": 2.0276,
        "grad_norm": 1.483201026916504,
        "learning_rate": 0.00014989925553293105,
        "epoch": 0.7794320374965537,
        "step": 2827
    },
    {
        "loss": 2.2238,
        "grad_norm": 1.7909146547317505,
        "learning_rate": 0.0001496723339610113,
        "epoch": 0.779707747449683,
        "step": 2828
    },
    {
        "loss": 1.5484,
        "grad_norm": 1.8473204374313354,
        "learning_rate": 0.00014944507231843765,
        "epoch": 0.7799834574028123,
        "step": 2829
    },
    {
        "loss": 0.6479,
        "grad_norm": 1.8136262893676758,
        "learning_rate": 0.0001492174721611062,
        "epoch": 0.7802591673559416,
        "step": 2830
    },
    {
        "loss": 2.2789,
        "grad_norm": 1.576527714729309,
        "learning_rate": 0.0001489895350472316,
        "epoch": 0.7805348773090709,
        "step": 2831
    },
    {
        "loss": 2.0308,
        "grad_norm": 1.8443098068237305,
        "learning_rate": 0.0001487612625373344,
        "epoch": 0.7808105872622002,
        "step": 2832
    },
    {
        "loss": 2.7142,
        "grad_norm": 1.125590205192566,
        "learning_rate": 0.00014853265619423244,
        "epoch": 0.7810862972153295,
        "step": 2833
    },
    {
        "loss": 2.3097,
        "grad_norm": 2.0878379344940186,
        "learning_rate": 0.00014830371758302793,
        "epoch": 0.7813620071684588,
        "step": 2834
    },
    {
        "loss": 1.9791,
        "grad_norm": 1.4685138463974,
        "learning_rate": 0.000148074448271099,
        "epoch": 0.7816377171215881,
        "step": 2835
    },
    {
        "loss": 2.4249,
        "grad_norm": 1.2132461071014404,
        "learning_rate": 0.00014784484982808684,
        "epoch": 0.7819134270747174,
        "step": 2836
    },
    {
        "loss": 2.0303,
        "grad_norm": 1.5421963930130005,
        "learning_rate": 0.0001476149238258869,
        "epoch": 0.7821891370278468,
        "step": 2837
    },
    {
        "loss": 1.1411,
        "grad_norm": 2.191923141479492,
        "learning_rate": 0.00014738467183863626,
        "epoch": 0.782464846980976,
        "step": 2838
    },
    {
        "loss": 2.2992,
        "grad_norm": 1.637603998184204,
        "learning_rate": 0.00014715409544270472,
        "epoch": 0.7827405569341053,
        "step": 2839
    },
    {
        "loss": 1.7606,
        "grad_norm": 1.816900372505188,
        "learning_rate": 0.00014692319621668212,
        "epoch": 0.7830162668872346,
        "step": 2840
    },
    {
        "loss": 2.1919,
        "grad_norm": 1.2133840322494507,
        "learning_rate": 0.00014669197574136947,
        "epoch": 0.7832919768403639,
        "step": 2841
    },
    {
        "loss": 2.1981,
        "grad_norm": 1.8234187364578247,
        "learning_rate": 0.00014646043559976611,
        "epoch": 0.7835676867934932,
        "step": 2842
    },
    {
        "loss": 1.6976,
        "grad_norm": 2.5476579666137695,
        "learning_rate": 0.00014622857737706102,
        "epoch": 0.7838433967466225,
        "step": 2843
    },
    {
        "loss": 2.3078,
        "grad_norm": 1.5305330753326416,
        "learning_rate": 0.00014599640266061968,
        "epoch": 0.7841191066997518,
        "step": 2844
    },
    {
        "loss": 2.357,
        "grad_norm": 1.5477569103240967,
        "learning_rate": 0.00014576391303997555,
        "epoch": 0.7843948166528811,
        "step": 2845
    },
    {
        "loss": 2.0387,
        "grad_norm": 2.1204018592834473,
        "learning_rate": 0.00014553111010681683,
        "epoch": 0.7846705266060104,
        "step": 2846
    },
    {
        "loss": 1.7785,
        "grad_norm": 1.3153725862503052,
        "learning_rate": 0.00014529799545497785,
        "epoch": 0.7849462365591398,
        "step": 2847
    },
    {
        "loss": 2.3698,
        "grad_norm": 1.1353516578674316,
        "learning_rate": 0.00014506457068042613,
        "epoch": 0.7852219465122691,
        "step": 2848
    },
    {
        "loss": 2.1767,
        "grad_norm": 1.8483431339263916,
        "learning_rate": 0.0001448308373812532,
        "epoch": 0.7854976564653984,
        "step": 2849
    },
    {
        "loss": 2.4268,
        "grad_norm": 1.615150809288025,
        "learning_rate": 0.00014459679715766197,
        "epoch": 0.7857733664185277,
        "step": 2850
    },
    {
        "loss": 2.0136,
        "grad_norm": 1.7607051134109497,
        "learning_rate": 0.00014436245161195759,
        "epoch": 0.786049076371657,
        "step": 2851
    },
    {
        "loss": 2.2377,
        "grad_norm": 1.4964886903762817,
        "learning_rate": 0.00014412780234853453,
        "epoch": 0.7863247863247863,
        "step": 2852
    },
    {
        "loss": 1.6707,
        "grad_norm": 2.0228874683380127,
        "learning_rate": 0.0001438928509738676,
        "epoch": 0.7866004962779156,
        "step": 2853
    },
    {
        "loss": 1.8602,
        "grad_norm": 2.220397472381592,
        "learning_rate": 0.00014365759909649894,
        "epoch": 0.7868762062310449,
        "step": 2854
    },
    {
        "loss": 1.8236,
        "grad_norm": 2.0683515071868896,
        "learning_rate": 0.00014342204832702905,
        "epoch": 0.7871519161841742,
        "step": 2855
    },
    {
        "loss": 2.5604,
        "grad_norm": 1.7387360334396362,
        "learning_rate": 0.0001431862002781037,
        "epoch": 0.7874276261373035,
        "step": 2856
    },
    {
        "loss": 1.9642,
        "grad_norm": 1.869139313697815,
        "learning_rate": 0.0001429500565644049,
        "epoch": 0.7877033360904329,
        "step": 2857
    },
    {
        "loss": 2.0862,
        "grad_norm": 1.317097783088684,
        "learning_rate": 0.0001427136188026379,
        "epoch": 0.7879790460435622,
        "step": 2858
    },
    {
        "loss": 2.239,
        "grad_norm": 1.9567582607269287,
        "learning_rate": 0.000142476888611522,
        "epoch": 0.7882547559966915,
        "step": 2859
    },
    {
        "loss": 1.5002,
        "grad_norm": 1.911887526512146,
        "learning_rate": 0.00014223986761177776,
        "epoch": 0.7885304659498208,
        "step": 2860
    },
    {
        "loss": 1.6454,
        "grad_norm": 2.1953988075256348,
        "learning_rate": 0.0001420025574261175,
        "epoch": 0.7888061759029501,
        "step": 2861
    },
    {
        "loss": 1.6184,
        "grad_norm": 2.261439800262451,
        "learning_rate": 0.0001417649596792324,
        "epoch": 0.7890818858560794,
        "step": 2862
    },
    {
        "loss": 2.6074,
        "grad_norm": 1.2485105991363525,
        "learning_rate": 0.0001415270759977835,
        "epoch": 0.7893575958092087,
        "step": 2863
    },
    {
        "loss": 2.2925,
        "grad_norm": 1.018324375152588,
        "learning_rate": 0.00014128890801038823,
        "epoch": 0.789633305762338,
        "step": 2864
    },
    {
        "loss": 2.4032,
        "grad_norm": 1.7525118589401245,
        "learning_rate": 0.00014105045734761167,
        "epoch": 0.7899090157154673,
        "step": 2865
    },
    {
        "loss": 1.5639,
        "grad_norm": 2.2289743423461914,
        "learning_rate": 0.00014081172564195298,
        "epoch": 0.7901847256685967,
        "step": 2866
    },
    {
        "loss": 1.1191,
        "grad_norm": 2.0825352668762207,
        "learning_rate": 0.0001405727145278366,
        "epoch": 0.790460435621726,
        "step": 2867
    },
    {
        "loss": 2.0454,
        "grad_norm": 1.9865338802337646,
        "learning_rate": 0.00014033342564159874,
        "epoch": 0.7907361455748553,
        "step": 2868
    },
    {
        "loss": 1.9814,
        "grad_norm": 1.2901650667190552,
        "learning_rate": 0.00014009386062147837,
        "epoch": 0.7910118555279846,
        "step": 2869
    },
    {
        "loss": 0.9302,
        "grad_norm": 2.223355531692505,
        "learning_rate": 0.00013985402110760394,
        "epoch": 0.7912875654811139,
        "step": 2870
    },
    {
        "loss": 0.8348,
        "grad_norm": 1.8184618949890137,
        "learning_rate": 0.00013961390874198413,
        "epoch": 0.7915632754342432,
        "step": 2871
    },
    {
        "loss": 2.0236,
        "grad_norm": 2.332362174987793,
        "learning_rate": 0.00013937352516849475,
        "epoch": 0.7918389853873725,
        "step": 2872
    },
    {
        "loss": 2.011,
        "grad_norm": 1.814703345298767,
        "learning_rate": 0.0001391328720328693,
        "epoch": 0.7921146953405018,
        "step": 2873
    },
    {
        "loss": 2.2507,
        "grad_norm": 1.4724689722061157,
        "learning_rate": 0.00013889195098268567,
        "epoch": 0.7923904052936311,
        "step": 2874
    },
    {
        "loss": 2.1361,
        "grad_norm": 1.4370386600494385,
        "learning_rate": 0.00013865076366735717,
        "epoch": 0.7926661152467604,
        "step": 2875
    },
    {
        "loss": 2.2925,
        "grad_norm": 1.2658942937850952,
        "learning_rate": 0.0001384093117381189,
        "epoch": 0.7929418251998898,
        "step": 2876
    },
    {
        "loss": 1.9399,
        "grad_norm": 2.5953876972198486,
        "learning_rate": 0.00013816759684801862,
        "epoch": 0.7932175351530191,
        "step": 2877
    },
    {
        "loss": 2.0464,
        "grad_norm": 1.7679375410079956,
        "learning_rate": 0.00013792562065190338,
        "epoch": 0.7934932451061484,
        "step": 2878
    },
    {
        "loss": 1.4437,
        "grad_norm": 2.5739848613739014,
        "learning_rate": 0.00013768338480641025,
        "epoch": 0.7937689550592777,
        "step": 2879
    },
    {
        "loss": 1.2423,
        "grad_norm": 1.2588614225387573,
        "learning_rate": 0.0001374408909699529,
        "epoch": 0.794044665012407,
        "step": 2880
    },
    {
        "loss": 1.9248,
        "grad_norm": 1.6342358589172363,
        "learning_rate": 0.00013719814080271226,
        "epoch": 0.7943203749655362,
        "step": 2881
    },
    {
        "loss": 2.3805,
        "grad_norm": 1.2564464807510376,
        "learning_rate": 0.00013695513596662318,
        "epoch": 0.7945960849186655,
        "step": 2882
    },
    {
        "loss": 2.4851,
        "grad_norm": 1.17184317111969,
        "learning_rate": 0.00013671187812536507,
        "epoch": 0.7948717948717948,
        "step": 2883
    },
    {
        "loss": 1.6014,
        "grad_norm": 1.900879144668579,
        "learning_rate": 0.00013646836894434846,
        "epoch": 0.7951475048249241,
        "step": 2884
    },
    {
        "loss": 0.8564,
        "grad_norm": 2.7932493686676025,
        "learning_rate": 0.00013622461009070563,
        "epoch": 0.7954232147780534,
        "step": 2885
    },
    {
        "loss": 1.9373,
        "grad_norm": 2.6625380516052246,
        "learning_rate": 0.00013598060323327717,
        "epoch": 0.7956989247311828,
        "step": 2886
    },
    {
        "loss": 2.0229,
        "grad_norm": 1.4584085941314697,
        "learning_rate": 0.00013573635004260264,
        "epoch": 0.7959746346843121,
        "step": 2887
    },
    {
        "loss": 2.5633,
        "grad_norm": 1.0653736591339111,
        "learning_rate": 0.00013549185219090693,
        "epoch": 0.7962503446374414,
        "step": 2888
    },
    {
        "loss": 1.7159,
        "grad_norm": 1.7544327974319458,
        "learning_rate": 0.00013524711135209114,
        "epoch": 0.7965260545905707,
        "step": 2889
    },
    {
        "loss": 1.7719,
        "grad_norm": 1.6448591947555542,
        "learning_rate": 0.00013500212920171875,
        "epoch": 0.7968017645437,
        "step": 2890
    },
    {
        "loss": 1.9884,
        "grad_norm": 1.5746593475341797,
        "learning_rate": 0.00013475690741700643,
        "epoch": 0.7970774744968293,
        "step": 2891
    },
    {
        "loss": 2.3569,
        "grad_norm": 1.5542610883712769,
        "learning_rate": 0.0001345114476768104,
        "epoch": 0.7973531844499586,
        "step": 2892
    },
    {
        "loss": 2.6349,
        "grad_norm": 1.036682367324829,
        "learning_rate": 0.000134265751661617,
        "epoch": 0.7976288944030879,
        "step": 2893
    },
    {
        "loss": 2.4699,
        "grad_norm": 1.4986989498138428,
        "learning_rate": 0.0001340198210535292,
        "epoch": 0.7979046043562172,
        "step": 2894
    },
    {
        "loss": 2.2524,
        "grad_norm": 1.9009641408920288,
        "learning_rate": 0.0001337736575362571,
        "epoch": 0.7981803143093466,
        "step": 2895
    },
    {
        "loss": 1.7064,
        "grad_norm": 1.851048231124878,
        "learning_rate": 0.00013352726279510424,
        "epoch": 0.7984560242624759,
        "step": 2896
    },
    {
        "loss": 1.8827,
        "grad_norm": 1.6701948642730713,
        "learning_rate": 0.00013328063851695827,
        "epoch": 0.7987317342156052,
        "step": 2897
    },
    {
        "loss": 1.8567,
        "grad_norm": 1.9483550786972046,
        "learning_rate": 0.00013303378639027726,
        "epoch": 0.7990074441687345,
        "step": 2898
    },
    {
        "loss": 2.1044,
        "grad_norm": 1.2765190601348877,
        "learning_rate": 0.00013278670810508027,
        "epoch": 0.7992831541218638,
        "step": 2899
    },
    {
        "loss": 2.3415,
        "grad_norm": 1.3955984115600586,
        "learning_rate": 0.00013253940535293363,
        "epoch": 0.7995588640749931,
        "step": 2900
    },
    {
        "loss": 2.4021,
        "grad_norm": 1.216664433479309,
        "learning_rate": 0.00013229187982694148,
        "epoch": 0.7998345740281224,
        "step": 2901
    },
    {
        "loss": 2.0289,
        "grad_norm": 1.5950301885604858,
        "learning_rate": 0.00013204413322173212,
        "epoch": 0.8001102839812517,
        "step": 2902
    },
    {
        "loss": 2.0569,
        "grad_norm": 1.6794954538345337,
        "learning_rate": 0.00013179616723344844,
        "epoch": 0.800385993934381,
        "step": 2903
    },
    {
        "loss": 2.1925,
        "grad_norm": 0.920169472694397,
        "learning_rate": 0.00013154798355973423,
        "epoch": 0.8006617038875103,
        "step": 2904
    },
    {
        "loss": 2.354,
        "grad_norm": 1.5465381145477295,
        "learning_rate": 0.00013129958389972466,
        "epoch": 0.8009374138406397,
        "step": 2905
    },
    {
        "loss": 2.5422,
        "grad_norm": 1.6663326025009155,
        "learning_rate": 0.00013105096995403258,
        "epoch": 0.801213123793769,
        "step": 2906
    },
    {
        "loss": 2.343,
        "grad_norm": 1.061149001121521,
        "learning_rate": 0.00013080214342473898,
        "epoch": 0.8014888337468983,
        "step": 2907
    },
    {
        "loss": 2.3011,
        "grad_norm": 1.416717767715454,
        "learning_rate": 0.00013055310601537906,
        "epoch": 0.8017645437000276,
        "step": 2908
    },
    {
        "loss": 2.7049,
        "grad_norm": 1.3786323070526123,
        "learning_rate": 0.000130303859430933,
        "epoch": 0.8020402536531569,
        "step": 2909
    },
    {
        "loss": 1.801,
        "grad_norm": 1.510058879852295,
        "learning_rate": 0.0001300544053778119,
        "epoch": 0.8023159636062862,
        "step": 2910
    },
    {
        "loss": 1.4052,
        "grad_norm": 2.419059991836548,
        "learning_rate": 0.0001298047455638483,
        "epoch": 0.8025916735594155,
        "step": 2911
    },
    {
        "loss": 2.1474,
        "grad_norm": 1.7927213907241821,
        "learning_rate": 0.00012955488169828246,
        "epoch": 0.8028673835125448,
        "step": 2912
    },
    {
        "loss": 2.1927,
        "grad_norm": 1.5248408317565918,
        "learning_rate": 0.00012930481549175253,
        "epoch": 0.8031430934656741,
        "step": 2913
    },
    {
        "loss": 2.3035,
        "grad_norm": 1.242881417274475,
        "learning_rate": 0.00012905454865628103,
        "epoch": 0.8034188034188035,
        "step": 2914
    },
    {
        "loss": 1.4232,
        "grad_norm": 1.880986213684082,
        "learning_rate": 0.00012880408290526511,
        "epoch": 0.8036945133719328,
        "step": 2915
    },
    {
        "loss": 1.9669,
        "grad_norm": 1.5708521604537964,
        "learning_rate": 0.00012855341995346258,
        "epoch": 0.8039702233250621,
        "step": 2916
    },
    {
        "loss": 2.2039,
        "grad_norm": 1.4523605108261108,
        "learning_rate": 0.00012830256151698247,
        "epoch": 0.8042459332781914,
        "step": 2917
    },
    {
        "loss": 2.254,
        "grad_norm": 1.2275818586349487,
        "learning_rate": 0.00012805150931327107,
        "epoch": 0.8045216432313207,
        "step": 2918
    },
    {
        "loss": 1.8554,
        "grad_norm": 1.8469815254211426,
        "learning_rate": 0.0001278002650611024,
        "epoch": 0.80479735318445,
        "step": 2919
    },
    {
        "loss": 1.1927,
        "grad_norm": 2.0647873878479004,
        "learning_rate": 0.00012754883048056404,
        "epoch": 0.8050730631375793,
        "step": 2920
    },
    {
        "loss": 2.1923,
        "grad_norm": 1.0495142936706543,
        "learning_rate": 0.00012729720729304786,
        "epoch": 0.8053487730907086,
        "step": 2921
    },
    {
        "loss": 0.9891,
        "grad_norm": 2.203019380569458,
        "learning_rate": 0.00012704539722123589,
        "epoch": 0.8056244830438379,
        "step": 2922
    },
    {
        "loss": 2.4792,
        "grad_norm": 1.0845015048980713,
        "learning_rate": 0.00012679340198909063,
        "epoch": 0.8059001929969671,
        "step": 2923
    },
    {
        "loss": 1.9243,
        "grad_norm": 2.2317631244659424,
        "learning_rate": 0.00012654122332184117,
        "epoch": 0.8061759029500964,
        "step": 2924
    },
    {
        "loss": 2.5232,
        "grad_norm": 1.2074096202850342,
        "learning_rate": 0.00012628886294597358,
        "epoch": 0.8064516129032258,
        "step": 2925
    },
    {
        "loss": 1.483,
        "grad_norm": 2.1545045375823975,
        "learning_rate": 0.00012603632258921684,
        "epoch": 0.8067273228563551,
        "step": 2926
    },
    {
        "loss": 2.4091,
        "grad_norm": 1.3351801633834839,
        "learning_rate": 0.00012578360398053323,
        "epoch": 0.8070030328094844,
        "step": 2927
    },
    {
        "loss": 1.2524,
        "grad_norm": 1.691999912261963,
        "learning_rate": 0.00012553070885010426,
        "epoch": 0.8072787427626137,
        "step": 2928
    },
    {
        "loss": 2.1463,
        "grad_norm": 1.7590607404708862,
        "learning_rate": 0.00012527763892932113,
        "epoch": 0.807554452715743,
        "step": 2929
    },
    {
        "loss": 2.0845,
        "grad_norm": 1.5983412265777588,
        "learning_rate": 0.00012502439595077052,
        "epoch": 0.8078301626688723,
        "step": 2930
    },
    {
        "loss": 2.2604,
        "grad_norm": 1.2361479997634888,
        "learning_rate": 0.00012477098164822507,
        "epoch": 0.8081058726220016,
        "step": 2931
    },
    {
        "loss": 2.0943,
        "grad_norm": 1.8348016738891602,
        "learning_rate": 0.00012451739775662922,
        "epoch": 0.8083815825751309,
        "step": 2932
    },
    {
        "loss": 2.1966,
        "grad_norm": 1.5013258457183838,
        "learning_rate": 0.0001242636460120896,
        "epoch": 0.8086572925282602,
        "step": 2933
    },
    {
        "loss": 2.2756,
        "grad_norm": 1.5904654264450073,
        "learning_rate": 0.0001240097281518609,
        "epoch": 0.8089330024813896,
        "step": 2934
    },
    {
        "loss": 2.2176,
        "grad_norm": 1.8697360754013062,
        "learning_rate": 0.00012375564591433612,
        "epoch": 0.8092087124345189,
        "step": 2935
    },
    {
        "loss": 1.8122,
        "grad_norm": 1.7913260459899902,
        "learning_rate": 0.00012350140103903264,
        "epoch": 0.8094844223876482,
        "step": 2936
    },
    {
        "loss": 2.0875,
        "grad_norm": 2.360487937927246,
        "learning_rate": 0.0001232469952665823,
        "epoch": 0.8097601323407775,
        "step": 2937
    },
    {
        "loss": 1.5467,
        "grad_norm": 1.938569188117981,
        "learning_rate": 0.00012299243033871745,
        "epoch": 0.8100358422939068,
        "step": 2938
    },
    {
        "loss": 2.3517,
        "grad_norm": 1.7802103757858276,
        "learning_rate": 0.00012273770799826116,
        "epoch": 0.8103115522470361,
        "step": 2939
    },
    {
        "loss": 2.4416,
        "grad_norm": 1.2649040222167969,
        "learning_rate": 0.00012248282998911307,
        "epoch": 0.8105872622001654,
        "step": 2940
    },
    {
        "loss": 1.9314,
        "grad_norm": 1.7847634553909302,
        "learning_rate": 0.00012222779805623964,
        "epoch": 0.8108629721532947,
        "step": 2941
    },
    {
        "loss": 1.4474,
        "grad_norm": 2.007547378540039,
        "learning_rate": 0.00012197261394566013,
        "epoch": 0.811138682106424,
        "step": 2942
    },
    {
        "loss": 2.1268,
        "grad_norm": 1.4609376192092896,
        "learning_rate": 0.00012171727940443664,
        "epoch": 0.8114143920595533,
        "step": 2943
    },
    {
        "loss": 1.3257,
        "grad_norm": 1.6939711570739746,
        "learning_rate": 0.00012146179618066016,
        "epoch": 0.8116901020126827,
        "step": 2944
    },
    {
        "loss": 2.6489,
        "grad_norm": 1.0070141553878784,
        "learning_rate": 0.00012120616602344063,
        "epoch": 0.811965811965812,
        "step": 2945
    },
    {
        "loss": 1.6169,
        "grad_norm": 1.5341711044311523,
        "learning_rate": 0.00012095039068289288,
        "epoch": 0.8122415219189413,
        "step": 2946
    },
    {
        "loss": 1.453,
        "grad_norm": 1.7971445322036743,
        "learning_rate": 0.00012069447191012677,
        "epoch": 0.8125172318720706,
        "step": 2947
    },
    {
        "loss": 2.1572,
        "grad_norm": 1.4759881496429443,
        "learning_rate": 0.00012043841145723307,
        "epoch": 0.8127929418251999,
        "step": 2948
    },
    {
        "loss": 1.7707,
        "grad_norm": 2.1909518241882324,
        "learning_rate": 0.00012018221107727362,
        "epoch": 0.8130686517783292,
        "step": 2949
    },
    {
        "loss": 2.3726,
        "grad_norm": 1.4885576963424683,
        "learning_rate": 0.0001199258725242671,
        "epoch": 0.8133443617314585,
        "step": 2950
    },
    {
        "loss": 1.9168,
        "grad_norm": 1.6276214122772217,
        "learning_rate": 0.00011966939755317936,
        "epoch": 0.8136200716845878,
        "step": 2951
    },
    {
        "loss": 2.2852,
        "grad_norm": 1.211762547492981,
        "learning_rate": 0.00011941278791990899,
        "epoch": 0.8138957816377171,
        "step": 2952
    },
    {
        "loss": 2.2663,
        "grad_norm": 1.4005699157714844,
        "learning_rate": 0.00011915604538127753,
        "epoch": 0.8141714915908465,
        "step": 2953
    },
    {
        "loss": 1.4735,
        "grad_norm": 1.9680694341659546,
        "learning_rate": 0.00011889917169501594,
        "epoch": 0.8144472015439758,
        "step": 2954
    },
    {
        "loss": 2.1997,
        "grad_norm": 1.7755799293518066,
        "learning_rate": 0.00011864216861975308,
        "epoch": 0.8147229114971051,
        "step": 2955
    },
    {
        "loss": 2.4622,
        "grad_norm": 1.5221383571624756,
        "learning_rate": 0.00011838503791500371,
        "epoch": 0.8149986214502344,
        "step": 2956
    },
    {
        "loss": 2.2758,
        "grad_norm": 1.3968945741653442,
        "learning_rate": 0.00011812778134115637,
        "epoch": 0.8152743314033637,
        "step": 2957
    },
    {
        "loss": 2.4403,
        "grad_norm": 1.158410906791687,
        "learning_rate": 0.00011787040065946126,
        "epoch": 0.815550041356493,
        "step": 2958
    },
    {
        "loss": 1.9908,
        "grad_norm": 1.5031617879867554,
        "learning_rate": 0.00011761289763201841,
        "epoch": 0.8158257513096223,
        "step": 2959
    },
    {
        "loss": 2.4892,
        "grad_norm": 1.095367193222046,
        "learning_rate": 0.00011735527402176534,
        "epoch": 0.8161014612627516,
        "step": 2960
    },
    {
        "loss": 2.4361,
        "grad_norm": 1.6617584228515625,
        "learning_rate": 0.00011709753159246511,
        "epoch": 0.8163771712158809,
        "step": 2961
    },
    {
        "loss": 2.4638,
        "grad_norm": 1.5284240245819092,
        "learning_rate": 0.00011683967210869435,
        "epoch": 0.8166528811690102,
        "step": 2962
    },
    {
        "loss": 2.3901,
        "grad_norm": 1.2720972299575806,
        "learning_rate": 0.000116581697335831,
        "epoch": 0.8169285911221396,
        "step": 2963
    },
    {
        "loss": 2.184,
        "grad_norm": 1.9569464921951294,
        "learning_rate": 0.0001163236090400423,
        "epoch": 0.8172043010752689,
        "step": 2964
    },
    {
        "loss": 1.9498,
        "grad_norm": 1.3329850435256958,
        "learning_rate": 0.0001160654089882727,
        "epoch": 0.8174800110283982,
        "step": 2965
    },
    {
        "loss": 1.6621,
        "grad_norm": 1.7751951217651367,
        "learning_rate": 0.00011580709894823178,
        "epoch": 0.8177557209815274,
        "step": 2966
    },
    {
        "loss": 1.956,
        "grad_norm": 1.489539623260498,
        "learning_rate": 0.00011554868068838218,
        "epoch": 0.8180314309346567,
        "step": 2967
    },
    {
        "loss": 2.2629,
        "grad_norm": 1.9685108661651611,
        "learning_rate": 0.00011529015597792734,
        "epoch": 0.818307140887786,
        "step": 2968
    },
    {
        "loss": 1.5348,
        "grad_norm": 1.6007416248321533,
        "learning_rate": 0.00011503152658679959,
        "epoch": 0.8185828508409153,
        "step": 2969
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.6937506198883057,
        "learning_rate": 0.00011477279428564782,
        "epoch": 0.8188585607940446,
        "step": 2970
    },
    {
        "loss": 2.0201,
        "grad_norm": 1.9941288232803345,
        "learning_rate": 0.00011451396084582557,
        "epoch": 0.8191342707471739,
        "step": 2971
    },
    {
        "loss": 1.949,
        "grad_norm": 1.447764277458191,
        "learning_rate": 0.00011425502803937877,
        "epoch": 0.8194099807003032,
        "step": 2972
    },
    {
        "loss": 1.8145,
        "grad_norm": 1.4895514249801636,
        "learning_rate": 0.00011399599763903366,
        "epoch": 0.8196856906534326,
        "step": 2973
    },
    {
        "loss": 2.4702,
        "grad_norm": 1.7446653842926025,
        "learning_rate": 0.00011373687141818458,
        "epoch": 0.8199614006065619,
        "step": 2974
    },
    {
        "loss": 1.489,
        "grad_norm": 1.2853927612304688,
        "learning_rate": 0.00011347765115088194,
        "epoch": 0.8202371105596912,
        "step": 2975
    },
    {
        "loss": 1.966,
        "grad_norm": 1.5336673259735107,
        "learning_rate": 0.00011321833861181998,
        "epoch": 0.8205128205128205,
        "step": 2976
    },
    {
        "loss": 2.243,
        "grad_norm": 1.7626804113388062,
        "learning_rate": 0.00011295893557632473,
        "epoch": 0.8207885304659498,
        "step": 2977
    },
    {
        "loss": 2.0571,
        "grad_norm": 1.708965539932251,
        "learning_rate": 0.00011269944382034166,
        "epoch": 0.8210642404190791,
        "step": 2978
    },
    {
        "loss": 1.4214,
        "grad_norm": 2.407933473587036,
        "learning_rate": 0.00011243986512042377,
        "epoch": 0.8213399503722084,
        "step": 2979
    },
    {
        "loss": 2.3857,
        "grad_norm": 1.926830530166626,
        "learning_rate": 0.0001121802012537192,
        "epoch": 0.8216156603253377,
        "step": 2980
    },
    {
        "loss": 1.7426,
        "grad_norm": 1.5392200946807861,
        "learning_rate": 0.00011192045399795923,
        "epoch": 0.821891370278467,
        "step": 2981
    },
    {
        "loss": 2.391,
        "grad_norm": 1.3558064699172974,
        "learning_rate": 0.00011166062513144606,
        "epoch": 0.8221670802315963,
        "step": 2982
    },
    {
        "loss": 2.0482,
        "grad_norm": 1.163307547569275,
        "learning_rate": 0.00011140071643304055,
        "epoch": 0.8224427901847257,
        "step": 2983
    },
    {
        "loss": 2.0185,
        "grad_norm": 1.8943028450012207,
        "learning_rate": 0.00011114072968215019,
        "epoch": 0.822718500137855,
        "step": 2984
    },
    {
        "loss": 2.3431,
        "grad_norm": 1.6798467636108398,
        "learning_rate": 0.00011088066665871675,
        "epoch": 0.8229942100909843,
        "step": 2985
    },
    {
        "loss": 1.3473,
        "grad_norm": 2.4598379135131836,
        "learning_rate": 0.00011062052914320426,
        "epoch": 0.8232699200441136,
        "step": 2986
    },
    {
        "loss": 1.7347,
        "grad_norm": 1.851558804512024,
        "learning_rate": 0.00011036031891658671,
        "epoch": 0.8235456299972429,
        "step": 2987
    },
    {
        "loss": 2.0093,
        "grad_norm": 1.997546911239624,
        "learning_rate": 0.00011010003776033588,
        "epoch": 0.8238213399503722,
        "step": 2988
    },
    {
        "loss": 1.9652,
        "grad_norm": 1.8752354383468628,
        "learning_rate": 0.0001098396874564092,
        "epoch": 0.8240970499035015,
        "step": 2989
    },
    {
        "loss": 2.143,
        "grad_norm": 1.5166414976119995,
        "learning_rate": 0.00010957926978723745,
        "epoch": 0.8243727598566308,
        "step": 2990
    },
    {
        "loss": 2.2024,
        "grad_norm": 1.3006263971328735,
        "learning_rate": 0.00010931878653571269,
        "epoch": 0.8246484698097601,
        "step": 2991
    },
    {
        "loss": 2.085,
        "grad_norm": 1.8062877655029297,
        "learning_rate": 0.00010905823948517582,
        "epoch": 0.8249241797628895,
        "step": 2992
    },
    {
        "loss": 1.6628,
        "grad_norm": 1.103359580039978,
        "learning_rate": 0.00010879763041940468,
        "epoch": 0.8251998897160188,
        "step": 2993
    },
    {
        "loss": 2.4935,
        "grad_norm": 1.1136502027511597,
        "learning_rate": 0.00010853696112260165,
        "epoch": 0.8254755996691481,
        "step": 2994
    },
    {
        "loss": 2.3225,
        "grad_norm": 1.9053583145141602,
        "learning_rate": 0.00010827623337938144,
        "epoch": 0.8257513096222774,
        "step": 2995
    },
    {
        "loss": 2.2356,
        "grad_norm": 1.20779550075531,
        "learning_rate": 0.0001080154489747589,
        "epoch": 0.8260270195754067,
        "step": 2996
    },
    {
        "loss": 2.162,
        "grad_norm": 1.669803261756897,
        "learning_rate": 0.00010775460969413678,
        "epoch": 0.826302729528536,
        "step": 2997
    },
    {
        "loss": 2.2171,
        "grad_norm": 1.4252121448516846,
        "learning_rate": 0.00010749371732329359,
        "epoch": 0.8265784394816653,
        "step": 2998
    },
    {
        "loss": 2.0024,
        "grad_norm": 1.052636742591858,
        "learning_rate": 0.0001072327736483713,
        "epoch": 0.8268541494347946,
        "step": 2999
    },
    {
        "loss": 2.1332,
        "grad_norm": 1.7286028861999512,
        "learning_rate": 0.00010697178045586303,
        "epoch": 0.8271298593879239,
        "step": 3000
    },
    {
        "loss": 1.4651,
        "grad_norm": 0.9846868515014648,
        "learning_rate": 0.00010671073953260105,
        "epoch": 0.8274055693410532,
        "step": 3001
    },
    {
        "loss": 1.7574,
        "grad_norm": 1.8443336486816406,
        "learning_rate": 0.00010644965266574425,
        "epoch": 0.8276812792941826,
        "step": 3002
    },
    {
        "loss": 1.6334,
        "grad_norm": 2.3413472175598145,
        "learning_rate": 0.00010618852164276622,
        "epoch": 0.8279569892473119,
        "step": 3003
    },
    {
        "loss": 1.9806,
        "grad_norm": 1.811805248260498,
        "learning_rate": 0.0001059273482514428,
        "epoch": 0.8282326992004412,
        "step": 3004
    },
    {
        "loss": 2.5231,
        "grad_norm": 1.027018427848816,
        "learning_rate": 0.0001056661342798398,
        "epoch": 0.8285084091535705,
        "step": 3005
    },
    {
        "loss": 2.1955,
        "grad_norm": 1.182715892791748,
        "learning_rate": 0.00010540488151630102,
        "epoch": 0.8287841191066998,
        "step": 3006
    },
    {
        "loss": 1.9683,
        "grad_norm": 1.0215672254562378,
        "learning_rate": 0.00010514359174943567,
        "epoch": 0.8290598290598291,
        "step": 3007
    },
    {
        "loss": 2.5069,
        "grad_norm": 1.3721603155136108,
        "learning_rate": 0.00010488226676810644,
        "epoch": 0.8293355390129584,
        "step": 3008
    },
    {
        "loss": 2.3096,
        "grad_norm": 1.3000973463058472,
        "learning_rate": 0.00010462090836141705,
        "epoch": 0.8296112489660876,
        "step": 3009
    },
    {
        "loss": 1.2309,
        "grad_norm": 2.139836549758911,
        "learning_rate": 0.00010435951831870001,
        "epoch": 0.8298869589192169,
        "step": 3010
    },
    {
        "loss": 2.321,
        "grad_norm": 1.397984504699707,
        "learning_rate": 0.00010409809842950452,
        "epoch": 0.8301626688723462,
        "step": 3011
    },
    {
        "loss": 2.2092,
        "grad_norm": 1.425555944442749,
        "learning_rate": 0.00010383665048358402,
        "epoch": 0.8304383788254756,
        "step": 3012
    },
    {
        "loss": 2.2701,
        "grad_norm": 1.4053601026535034,
        "learning_rate": 0.00010357517627088406,
        "epoch": 0.8307140887786049,
        "step": 3013
    },
    {
        "loss": 2.2716,
        "grad_norm": 1.4675071239471436,
        "learning_rate": 0.00010331367758153014,
        "epoch": 0.8309897987317342,
        "step": 3014
    },
    {
        "loss": 1.452,
        "grad_norm": 2.4019219875335693,
        "learning_rate": 0.00010305215620581512,
        "epoch": 0.8312655086848635,
        "step": 3015
    },
    {
        "loss": 2.1388,
        "grad_norm": 1.4676927328109741,
        "learning_rate": 0.00010279061393418734,
        "epoch": 0.8315412186379928,
        "step": 3016
    },
    {
        "loss": 2.2941,
        "grad_norm": 1.4239190816879272,
        "learning_rate": 0.00010252905255723812,
        "epoch": 0.8318169285911221,
        "step": 3017
    },
    {
        "loss": 2.0643,
        "grad_norm": 2.0977461338043213,
        "learning_rate": 0.00010226747386568965,
        "epoch": 0.8320926385442514,
        "step": 3018
    },
    {
        "loss": 2.0159,
        "grad_norm": 1.7098963260650635,
        "learning_rate": 0.0001020058796503826,
        "epoch": 0.8323683484973807,
        "step": 3019
    },
    {
        "loss": 2.5713,
        "grad_norm": 1.2097939252853394,
        "learning_rate": 0.0001017442717022639,
        "epoch": 0.83264405845051,
        "step": 3020
    },
    {
        "loss": 1.9238,
        "grad_norm": 1.7309767007827759,
        "learning_rate": 0.00010148265181237462,
        "epoch": 0.8329197684036393,
        "step": 3021
    },
    {
        "loss": 1.3598,
        "grad_norm": 1.7020397186279297,
        "learning_rate": 0.00010122102177183741,
        "epoch": 0.8331954783567687,
        "step": 3022
    },
    {
        "loss": 1.8294,
        "grad_norm": 2.2867820262908936,
        "learning_rate": 0.00010095938337184459,
        "epoch": 0.833471188309898,
        "step": 3023
    },
    {
        "loss": 2.2721,
        "grad_norm": 1.1431084871292114,
        "learning_rate": 0.00010069773840364556,
        "epoch": 0.8337468982630273,
        "step": 3024
    },
    {
        "loss": 2.1992,
        "grad_norm": 1.4684916734695435,
        "learning_rate": 0.00010043608865853478,
        "epoch": 0.8340226082161566,
        "step": 3025
    },
    {
        "loss": 1.4031,
        "grad_norm": 3.000532865524292,
        "learning_rate": 0.00010017443592783942,
        "epoch": 0.8342983181692859,
        "step": 3026
    },
    {
        "loss": 1.8607,
        "grad_norm": 1.2760052680969238,
        "learning_rate": 9.991278200290702e-05,
        "epoch": 0.8345740281224152,
        "step": 3027
    },
    {
        "loss": 2.3638,
        "grad_norm": 1.6548832654953003,
        "learning_rate": 9.965112867509339e-05,
        "epoch": 0.8348497380755445,
        "step": 3028
    },
    {
        "loss": 1.8337,
        "grad_norm": 2.0796890258789062,
        "learning_rate": 9.938947773575014e-05,
        "epoch": 0.8351254480286738,
        "step": 3029
    },
    {
        "loss": 2.1137,
        "grad_norm": 1.4519598484039307,
        "learning_rate": 9.91278309762126e-05,
        "epoch": 0.8354011579818031,
        "step": 3030
    },
    {
        "loss": 2.2544,
        "grad_norm": 1.7569094896316528,
        "learning_rate": 9.886619018778753e-05,
        "epoch": 0.8356768679349325,
        "step": 3031
    },
    {
        "loss": 1.69,
        "grad_norm": 1.7482796907424927,
        "learning_rate": 9.860455716174067e-05,
        "epoch": 0.8359525778880618,
        "step": 3032
    },
    {
        "loss": 2.1994,
        "grad_norm": 1.0854768753051758,
        "learning_rate": 9.834293368928476e-05,
        "epoch": 0.8362282878411911,
        "step": 3033
    },
    {
        "loss": 1.5743,
        "grad_norm": 1.439757227897644,
        "learning_rate": 9.8081321561567e-05,
        "epoch": 0.8365039977943204,
        "step": 3034
    },
    {
        "loss": 1.9113,
        "grad_norm": 1.6518917083740234,
        "learning_rate": 9.781972256965707e-05,
        "epoch": 0.8367797077474497,
        "step": 3035
    },
    {
        "loss": 2.2171,
        "grad_norm": 1.0193111896514893,
        "learning_rate": 9.755813850453461e-05,
        "epoch": 0.837055417700579,
        "step": 3036
    },
    {
        "loss": 2.1576,
        "grad_norm": 1.607491135597229,
        "learning_rate": 9.729657115707708e-05,
        "epoch": 0.8373311276537083,
        "step": 3037
    },
    {
        "loss": 1.6055,
        "grad_norm": 2.0796730518341064,
        "learning_rate": 9.703502231804752e-05,
        "epoch": 0.8376068376068376,
        "step": 3038
    },
    {
        "loss": 2.2221,
        "grad_norm": 1.349316954612732,
        "learning_rate": 9.677349377808224e-05,
        "epoch": 0.8378825475599669,
        "step": 3039
    },
    {
        "loss": 1.8217,
        "grad_norm": 1.677044153213501,
        "learning_rate": 9.651198732767856e-05,
        "epoch": 0.8381582575130962,
        "step": 3040
    },
    {
        "loss": 2.2226,
        "grad_norm": 1.1904948949813843,
        "learning_rate": 9.625050475718262e-05,
        "epoch": 0.8384339674662256,
        "step": 3041
    },
    {
        "loss": 2.1227,
        "grad_norm": 1.1816672086715698,
        "learning_rate": 9.598904785677699e-05,
        "epoch": 0.8387096774193549,
        "step": 3042
    },
    {
        "loss": 2.0891,
        "grad_norm": 1.8917112350463867,
        "learning_rate": 9.57276184164686e-05,
        "epoch": 0.8389853873724842,
        "step": 3043
    },
    {
        "loss": 2.0205,
        "grad_norm": 1.1318806409835815,
        "learning_rate": 9.546621822607626e-05,
        "epoch": 0.8392610973256135,
        "step": 3044
    },
    {
        "loss": 2.2482,
        "grad_norm": 1.7077593803405762,
        "learning_rate": 9.520484907521864e-05,
        "epoch": 0.8395368072787428,
        "step": 3045
    },
    {
        "loss": 1.7091,
        "grad_norm": 1.8432697057724,
        "learning_rate": 9.494351275330184e-05,
        "epoch": 0.8398125172318721,
        "step": 3046
    },
    {
        "loss": 1.3789,
        "grad_norm": 2.3588976860046387,
        "learning_rate": 9.46822110495072e-05,
        "epoch": 0.8400882271850014,
        "step": 3047
    },
    {
        "loss": 2.283,
        "grad_norm": 1.9065544605255127,
        "learning_rate": 9.44209457527791e-05,
        "epoch": 0.8403639371381307,
        "step": 3048
    },
    {
        "loss": 2.2525,
        "grad_norm": 1.7205674648284912,
        "learning_rate": 9.415971865181261e-05,
        "epoch": 0.84063964709126,
        "step": 3049
    },
    {
        "loss": 2.5515,
        "grad_norm": 1.1520949602127075,
        "learning_rate": 9.389853153504135e-05,
        "epoch": 0.8409153570443894,
        "step": 3050
    },
    {
        "loss": 2.6193,
        "grad_norm": 0.9693681001663208,
        "learning_rate": 9.363738619062521e-05,
        "epoch": 0.8411910669975186,
        "step": 3051
    },
    {
        "loss": 2.2686,
        "grad_norm": 1.234629511833191,
        "learning_rate": 9.337628440643798e-05,
        "epoch": 0.8414667769506479,
        "step": 3052
    },
    {
        "loss": 2.1426,
        "grad_norm": 1.9177100658416748,
        "learning_rate": 9.311522797005541e-05,
        "epoch": 0.8417424869037772,
        "step": 3053
    },
    {
        "loss": 2.2955,
        "grad_norm": 1.2774535417556763,
        "learning_rate": 9.28542186687426e-05,
        "epoch": 0.8420181968569065,
        "step": 3054
    },
    {
        "loss": 2.5999,
        "grad_norm": 1.1333619356155396,
        "learning_rate": 9.259325828944205e-05,
        "epoch": 0.8422939068100358,
        "step": 3055
    },
    {
        "loss": 1.5134,
        "grad_norm": 1.4820725917816162,
        "learning_rate": 9.233234861876136e-05,
        "epoch": 0.8425696167631651,
        "step": 3056
    },
    {
        "loss": 1.3981,
        "grad_norm": 2.9008748531341553,
        "learning_rate": 9.207149144296085e-05,
        "epoch": 0.8428453267162944,
        "step": 3057
    },
    {
        "loss": 2.219,
        "grad_norm": 2.061447858810425,
        "learning_rate": 9.181068854794158e-05,
        "epoch": 0.8431210366694237,
        "step": 3058
    },
    {
        "loss": 1.3979,
        "grad_norm": 2.124830961227417,
        "learning_rate": 9.154994171923284e-05,
        "epoch": 0.843396746622553,
        "step": 3059
    },
    {
        "loss": 0.9532,
        "grad_norm": 1.4575525522232056,
        "learning_rate": 9.128925274198023e-05,
        "epoch": 0.8436724565756824,
        "step": 3060
    },
    {
        "loss": 2.3079,
        "grad_norm": 1.2830123901367188,
        "learning_rate": 9.102862340093314e-05,
        "epoch": 0.8439481665288117,
        "step": 3061
    },
    {
        "loss": 2.3965,
        "grad_norm": 1.2228797674179077,
        "learning_rate": 9.076805548043278e-05,
        "epoch": 0.844223876481941,
        "step": 3062
    },
    {
        "loss": 1.3682,
        "grad_norm": 1.9465515613555908,
        "learning_rate": 9.050755076439982e-05,
        "epoch": 0.8444995864350703,
        "step": 3063
    },
    {
        "loss": 2.1628,
        "grad_norm": 1.6603156328201294,
        "learning_rate": 9.024711103632218e-05,
        "epoch": 0.8447752963881996,
        "step": 3064
    },
    {
        "loss": 1.7286,
        "grad_norm": 1.7308645248413086,
        "learning_rate": 8.998673807924292e-05,
        "epoch": 0.8450510063413289,
        "step": 3065
    },
    {
        "loss": 2.2786,
        "grad_norm": 1.2284709215164185,
        "learning_rate": 8.972643367574793e-05,
        "epoch": 0.8453267162944582,
        "step": 3066
    },
    {
        "loss": 2.338,
        "grad_norm": 1.3109490871429443,
        "learning_rate": 8.946619960795373e-05,
        "epoch": 0.8456024262475875,
        "step": 3067
    },
    {
        "loss": 2.3397,
        "grad_norm": 1.5616205930709839,
        "learning_rate": 8.92060376574954e-05,
        "epoch": 0.8458781362007168,
        "step": 3068
    },
    {
        "loss": 2.068,
        "grad_norm": 1.6729544401168823,
        "learning_rate": 8.894594960551416e-05,
        "epoch": 0.8461538461538461,
        "step": 3069
    },
    {
        "loss": 2.6135,
        "grad_norm": 1.061684250831604,
        "learning_rate": 8.868593723264542e-05,
        "epoch": 0.8464295561069755,
        "step": 3070
    },
    {
        "loss": 1.5055,
        "grad_norm": 1.2970116138458252,
        "learning_rate": 8.842600231900638e-05,
        "epoch": 0.8467052660601048,
        "step": 3071
    },
    {
        "loss": 2.218,
        "grad_norm": 1.8253235816955566,
        "learning_rate": 8.816614664418396e-05,
        "epoch": 0.8469809760132341,
        "step": 3072
    },
    {
        "loss": 1.1179,
        "grad_norm": 2.101846218109131,
        "learning_rate": 8.790637198722265e-05,
        "epoch": 0.8472566859663634,
        "step": 3073
    },
    {
        "loss": 1.1303,
        "grad_norm": 2.641944646835327,
        "learning_rate": 8.764668012661214e-05,
        "epoch": 0.8475323959194927,
        "step": 3074
    },
    {
        "loss": 2.3456,
        "grad_norm": 2.338935136795044,
        "learning_rate": 8.73870728402754e-05,
        "epoch": 0.847808105872622,
        "step": 3075
    },
    {
        "loss": 1.9457,
        "grad_norm": 2.1265482902526855,
        "learning_rate": 8.71275519055563e-05,
        "epoch": 0.8480838158257513,
        "step": 3076
    },
    {
        "loss": 1.9731,
        "grad_norm": 1.9576748609542847,
        "learning_rate": 8.686811909920756e-05,
        "epoch": 0.8483595257788806,
        "step": 3077
    },
    {
        "loss": 2.2007,
        "grad_norm": 1.5514549016952515,
        "learning_rate": 8.660877619737856e-05,
        "epoch": 0.8486352357320099,
        "step": 3078
    },
    {
        "loss": 2.0692,
        "grad_norm": 1.3384737968444824,
        "learning_rate": 8.634952497560312e-05,
        "epoch": 0.8489109456851393,
        "step": 3079
    },
    {
        "loss": 2.3396,
        "grad_norm": 1.063280701637268,
        "learning_rate": 8.609036720878745e-05,
        "epoch": 0.8491866556382686,
        "step": 3080
    },
    {
        "loss": 1.6532,
        "grad_norm": 2.253605604171753,
        "learning_rate": 8.583130467119787e-05,
        "epoch": 0.8494623655913979,
        "step": 3081
    },
    {
        "loss": 1.7844,
        "grad_norm": 2.156034469604492,
        "learning_rate": 8.55723391364488e-05,
        "epoch": 0.8497380755445272,
        "step": 3082
    },
    {
        "loss": 2.0338,
        "grad_norm": 2.065577745437622,
        "learning_rate": 8.531347237749055e-05,
        "epoch": 0.8500137854976565,
        "step": 3083
    },
    {
        "loss": 1.4446,
        "grad_norm": 2.1744139194488525,
        "learning_rate": 8.505470616659712e-05,
        "epoch": 0.8502894954507858,
        "step": 3084
    },
    {
        "loss": 1.9976,
        "grad_norm": 2.3370401859283447,
        "learning_rate": 8.479604227535422e-05,
        "epoch": 0.8505652054039151,
        "step": 3085
    },
    {
        "loss": 2.1401,
        "grad_norm": 1.4204621315002441,
        "learning_rate": 8.453748247464694e-05,
        "epoch": 0.8508409153570444,
        "step": 3086
    },
    {
        "loss": 2.1702,
        "grad_norm": 1.9667210578918457,
        "learning_rate": 8.427902853464784e-05,
        "epoch": 0.8511166253101737,
        "step": 3087
    },
    {
        "loss": 2.3903,
        "grad_norm": 1.2874380350112915,
        "learning_rate": 8.40206822248047e-05,
        "epoch": 0.851392335263303,
        "step": 3088
    },
    {
        "loss": 1.3185,
        "grad_norm": 2.3786115646362305,
        "learning_rate": 8.37624453138284e-05,
        "epoch": 0.8516680452164324,
        "step": 3089
    },
    {
        "loss": 1.9845,
        "grad_norm": 1.5830649137496948,
        "learning_rate": 8.350431956968088e-05,
        "epoch": 0.8519437551695617,
        "step": 3090
    },
    {
        "loss": 1.2752,
        "grad_norm": 2.1902287006378174,
        "learning_rate": 8.324630675956295e-05,
        "epoch": 0.852219465122691,
        "step": 3091
    },
    {
        "loss": 2.0816,
        "grad_norm": 1.5748170614242554,
        "learning_rate": 8.298840864990231e-05,
        "epoch": 0.8524951750758203,
        "step": 3092
    },
    {
        "loss": 1.7047,
        "grad_norm": 1.3947064876556396,
        "learning_rate": 8.273062700634138e-05,
        "epoch": 0.8527708850289496,
        "step": 3093
    },
    {
        "loss": 1.9593,
        "grad_norm": 2.140996217727661,
        "learning_rate": 8.247296359372515e-05,
        "epoch": 0.8530465949820788,
        "step": 3094
    },
    {
        "loss": 2.4358,
        "grad_norm": 1.2278720140457153,
        "learning_rate": 8.221542017608926e-05,
        "epoch": 0.8533223049352081,
        "step": 3095
    },
    {
        "loss": 1.72,
        "grad_norm": 1.531976342201233,
        "learning_rate": 8.195799851664776e-05,
        "epoch": 0.8535980148883374,
        "step": 3096
    },
    {
        "loss": 2.2748,
        "grad_norm": 1.425230622291565,
        "learning_rate": 8.170070037778116e-05,
        "epoch": 0.8538737248414667,
        "step": 3097
    },
    {
        "loss": 1.8893,
        "grad_norm": 2.005856513977051,
        "learning_rate": 8.144352752102424e-05,
        "epoch": 0.854149434794596,
        "step": 3098
    },
    {
        "loss": 1.7374,
        "grad_norm": 2.24428391456604,
        "learning_rate": 8.118648170705417e-05,
        "epoch": 0.8544251447477254,
        "step": 3099
    },
    {
        "loss": 1.7346,
        "grad_norm": 2.481069564819336,
        "learning_rate": 8.092956469567829e-05,
        "epoch": 0.8547008547008547,
        "step": 3100
    },
    {
        "loss": 1.6567,
        "grad_norm": 2.13010573387146,
        "learning_rate": 8.067277824582209e-05,
        "epoch": 0.854976564653984,
        "step": 3101
    },
    {
        "loss": 2.1635,
        "grad_norm": 1.9664831161499023,
        "learning_rate": 8.041612411551727e-05,
        "epoch": 0.8552522746071133,
        "step": 3102
    },
    {
        "loss": 1.8681,
        "grad_norm": 1.4097321033477783,
        "learning_rate": 8.015960406188957e-05,
        "epoch": 0.8555279845602426,
        "step": 3103
    },
    {
        "loss": 1.8579,
        "grad_norm": 2.2335383892059326,
        "learning_rate": 7.990321984114684e-05,
        "epoch": 0.8558036945133719,
        "step": 3104
    },
    {
        "loss": 2.108,
        "grad_norm": 1.7534844875335693,
        "learning_rate": 7.964697320856701e-05,
        "epoch": 0.8560794044665012,
        "step": 3105
    },
    {
        "loss": 1.8452,
        "grad_norm": 1.513892650604248,
        "learning_rate": 7.939086591848595e-05,
        "epoch": 0.8563551144196305,
        "step": 3106
    },
    {
        "loss": 1.6485,
        "grad_norm": 1.9889748096466064,
        "learning_rate": 7.913489972428565e-05,
        "epoch": 0.8566308243727598,
        "step": 3107
    },
    {
        "loss": 1.9723,
        "grad_norm": 2.0131995677948,
        "learning_rate": 7.887907637838203e-05,
        "epoch": 0.8569065343258891,
        "step": 3108
    },
    {
        "loss": 2.1951,
        "grad_norm": 1.2923396825790405,
        "learning_rate": 7.862339763221311e-05,
        "epoch": 0.8571822442790185,
        "step": 3109
    },
    {
        "loss": 2.0117,
        "grad_norm": 1.5830464363098145,
        "learning_rate": 7.836786523622693e-05,
        "epoch": 0.8574579542321478,
        "step": 3110
    },
    {
        "loss": 1.019,
        "grad_norm": 2.0715110301971436,
        "learning_rate": 7.811248093986949e-05,
        "epoch": 0.8577336641852771,
        "step": 3111
    },
    {
        "loss": 2.3046,
        "grad_norm": 1.6722168922424316,
        "learning_rate": 7.785724649157299e-05,
        "epoch": 0.8580093741384064,
        "step": 3112
    },
    {
        "loss": 2.2531,
        "grad_norm": 1.3247883319854736,
        "learning_rate": 7.76021636387436e-05,
        "epoch": 0.8582850840915357,
        "step": 3113
    },
    {
        "loss": 2.0455,
        "grad_norm": 1.868454098701477,
        "learning_rate": 7.73472341277497e-05,
        "epoch": 0.858560794044665,
        "step": 3114
    },
    {
        "loss": 1.8119,
        "grad_norm": 1.5867598056793213,
        "learning_rate": 7.709245970390987e-05,
        "epoch": 0.8588365039977943,
        "step": 3115
    },
    {
        "loss": 1.7455,
        "grad_norm": 2.300626516342163,
        "learning_rate": 7.683784211148085e-05,
        "epoch": 0.8591122139509236,
        "step": 3116
    },
    {
        "loss": 2.2462,
        "grad_norm": 1.5134308338165283,
        "learning_rate": 7.65833830936457e-05,
        "epoch": 0.8593879239040529,
        "step": 3117
    },
    {
        "loss": 2.1073,
        "grad_norm": 0.9845792651176453,
        "learning_rate": 7.632908439250184e-05,
        "epoch": 0.8596636338571823,
        "step": 3118
    },
    {
        "loss": 2.4701,
        "grad_norm": 1.2600884437561035,
        "learning_rate": 7.607494774904911e-05,
        "epoch": 0.8599393438103116,
        "step": 3119
    },
    {
        "loss": 2.1763,
        "grad_norm": 1.8365317583084106,
        "learning_rate": 7.58209749031779e-05,
        "epoch": 0.8602150537634409,
        "step": 3120
    },
    {
        "loss": 1.9202,
        "grad_norm": 1.8827342987060547,
        "learning_rate": 7.55671675936571e-05,
        "epoch": 0.8604907637165702,
        "step": 3121
    },
    {
        "loss": 1.9918,
        "grad_norm": 1.4146853685379028,
        "learning_rate": 7.531352755812239e-05,
        "epoch": 0.8607664736696995,
        "step": 3122
    },
    {
        "loss": 1.4616,
        "grad_norm": 1.9798133373260498,
        "learning_rate": 7.506005653306419e-05,
        "epoch": 0.8610421836228288,
        "step": 3123
    },
    {
        "loss": 2.0343,
        "grad_norm": 2.264889717102051,
        "learning_rate": 7.480675625381584e-05,
        "epoch": 0.8613178935759581,
        "step": 3124
    },
    {
        "loss": 2.1761,
        "grad_norm": 1.2311811447143555,
        "learning_rate": 7.455362845454172e-05,
        "epoch": 0.8615936035290874,
        "step": 3125
    },
    {
        "loss": 1.7703,
        "grad_norm": 1.9605379104614258,
        "learning_rate": 7.430067486822533e-05,
        "epoch": 0.8618693134822167,
        "step": 3126
    },
    {
        "loss": 1.9391,
        "grad_norm": 2.061499834060669,
        "learning_rate": 7.404789722665752e-05,
        "epoch": 0.862145023435346,
        "step": 3127
    },
    {
        "loss": 2.1215,
        "grad_norm": 1.630196452140808,
        "learning_rate": 7.379529726042448e-05,
        "epoch": 0.8624207333884754,
        "step": 3128
    },
    {
        "loss": 1.8984,
        "grad_norm": 1.8331773281097412,
        "learning_rate": 7.354287669889608e-05,
        "epoch": 0.8626964433416047,
        "step": 3129
    },
    {
        "loss": 1.6125,
        "grad_norm": 3.158129930496216,
        "learning_rate": 7.32906372702139e-05,
        "epoch": 0.862972153294734,
        "step": 3130
    },
    {
        "loss": 1.9595,
        "grad_norm": 2.033151626586914,
        "learning_rate": 7.303858070127941e-05,
        "epoch": 0.8632478632478633,
        "step": 3131
    },
    {
        "loss": 2.373,
        "grad_norm": 1.7458573579788208,
        "learning_rate": 7.278670871774223e-05,
        "epoch": 0.8635235732009926,
        "step": 3132
    },
    {
        "loss": 1.8706,
        "grad_norm": 1.9130942821502686,
        "learning_rate": 7.253502304398814e-05,
        "epoch": 0.8637992831541219,
        "step": 3133
    },
    {
        "loss": 2.2009,
        "grad_norm": 1.1856597661972046,
        "learning_rate": 7.228352540312753e-05,
        "epoch": 0.8640749931072512,
        "step": 3134
    },
    {
        "loss": 2.3191,
        "grad_norm": 1.538215160369873,
        "learning_rate": 7.203221751698339e-05,
        "epoch": 0.8643507030603805,
        "step": 3135
    },
    {
        "loss": 1.6221,
        "grad_norm": 2.078900098800659,
        "learning_rate": 7.178110110607961e-05,
        "epoch": 0.8646264130135098,
        "step": 3136
    },
    {
        "loss": 1.6571,
        "grad_norm": 1.9270402193069458,
        "learning_rate": 7.15301778896292e-05,
        "epoch": 0.864902122966639,
        "step": 3137
    },
    {
        "loss": 2.2897,
        "grad_norm": 1.170041561126709,
        "learning_rate": 7.127944958552246e-05,
        "epoch": 0.8651778329197684,
        "step": 3138
    },
    {
        "loss": 1.5202,
        "grad_norm": 1.992793321609497,
        "learning_rate": 7.102891791031534e-05,
        "epoch": 0.8654535428728977,
        "step": 3139
    },
    {
        "loss": 1.7323,
        "grad_norm": 2.298795461654663,
        "learning_rate": 7.077858457921752e-05,
        "epoch": 0.865729252826027,
        "step": 3140
    },
    {
        "loss": 1.9903,
        "grad_norm": 1.5803183317184448,
        "learning_rate": 7.052845130608086e-05,
        "epoch": 0.8660049627791563,
        "step": 3141
    },
    {
        "loss": 2.4692,
        "grad_norm": 1.2258553504943848,
        "learning_rate": 7.027851980338751e-05,
        "epoch": 0.8662806727322856,
        "step": 3142
    },
    {
        "loss": 1.5705,
        "grad_norm": 1.3999404907226562,
        "learning_rate": 7.002879178223825e-05,
        "epoch": 0.8665563826854149,
        "step": 3143
    },
    {
        "loss": 1.9994,
        "grad_norm": 1.554381251335144,
        "learning_rate": 6.977926895234078e-05,
        "epoch": 0.8668320926385442,
        "step": 3144
    },
    {
        "loss": 1.869,
        "grad_norm": 1.1762323379516602,
        "learning_rate": 6.952995302199798e-05,
        "epoch": 0.8671078025916735,
        "step": 3145
    },
    {
        "loss": 1.9744,
        "grad_norm": 2.1126136779785156,
        "learning_rate": 6.928084569809626e-05,
        "epoch": 0.8673835125448028,
        "step": 3146
    },
    {
        "loss": 1.4049,
        "grad_norm": 2.5303170680999756,
        "learning_rate": 6.903194868609387e-05,
        "epoch": 0.8676592224979321,
        "step": 3147
    },
    {
        "loss": 2.1462,
        "grad_norm": 1.0680965185165405,
        "learning_rate": 6.878326369000915e-05,
        "epoch": 0.8679349324510615,
        "step": 3148
    },
    {
        "loss": 1.4834,
        "grad_norm": 2.072056293487549,
        "learning_rate": 6.853479241240898e-05,
        "epoch": 0.8682106424041908,
        "step": 3149
    },
    {
        "loss": 1.6536,
        "grad_norm": 1.9269914627075195,
        "learning_rate": 6.828653655439703e-05,
        "epoch": 0.8684863523573201,
        "step": 3150
    },
    {
        "loss": 2.1823,
        "grad_norm": 1.5292524099349976,
        "learning_rate": 6.803849781560214e-05,
        "epoch": 0.8687620623104494,
        "step": 3151
    },
    {
        "loss": 2.5688,
        "grad_norm": 1.1271212100982666,
        "learning_rate": 6.77906778941667e-05,
        "epoch": 0.8690377722635787,
        "step": 3152
    },
    {
        "loss": 1.4293,
        "grad_norm": 2.1147255897521973,
        "learning_rate": 6.754307848673506e-05,
        "epoch": 0.869313482216708,
        "step": 3153
    },
    {
        "loss": 1.3322,
        "grad_norm": 2.723280668258667,
        "learning_rate": 6.72957012884418e-05,
        "epoch": 0.8695891921698373,
        "step": 3154
    },
    {
        "loss": 2.2369,
        "grad_norm": 1.1208139657974243,
        "learning_rate": 6.704854799290022e-05,
        "epoch": 0.8698649021229666,
        "step": 3155
    },
    {
        "loss": 2.143,
        "grad_norm": 1.6174103021621704,
        "learning_rate": 6.68016202921907e-05,
        "epoch": 0.8701406120760959,
        "step": 3156
    },
    {
        "loss": 2.2087,
        "grad_norm": 1.149808645248413,
        "learning_rate": 6.655491987684924e-05,
        "epoch": 0.8704163220292253,
        "step": 3157
    },
    {
        "loss": 2.0327,
        "grad_norm": 1.5026648044586182,
        "learning_rate": 6.630844843585562e-05,
        "epoch": 0.8706920319823546,
        "step": 3158
    },
    {
        "loss": 2.2646,
        "grad_norm": 1.4679328203201294,
        "learning_rate": 6.606220765662213e-05,
        "epoch": 0.8709677419354839,
        "step": 3159
    },
    {
        "loss": 2.6997,
        "grad_norm": 1.1552139520645142,
        "learning_rate": 6.581619922498179e-05,
        "epoch": 0.8712434518886132,
        "step": 3160
    },
    {
        "loss": 2.276,
        "grad_norm": 1.1617558002471924,
        "learning_rate": 6.557042482517695e-05,
        "epoch": 0.8715191618417425,
        "step": 3161
    },
    {
        "loss": 1.6289,
        "grad_norm": 1.8651360273361206,
        "learning_rate": 6.532488613984775e-05,
        "epoch": 0.8717948717948718,
        "step": 3162
    },
    {
        "loss": 1.7502,
        "grad_norm": 1.85417902469635,
        "learning_rate": 6.507958485002048e-05,
        "epoch": 0.8720705817480011,
        "step": 3163
    },
    {
        "loss": 1.9352,
        "grad_norm": 1.712069034576416,
        "learning_rate": 6.483452263509623e-05,
        "epoch": 0.8723462917011304,
        "step": 3164
    },
    {
        "loss": 2.0446,
        "grad_norm": 1.9100532531738281,
        "learning_rate": 6.458970117283923e-05,
        "epoch": 0.8726220016542597,
        "step": 3165
    },
    {
        "loss": 1.9648,
        "grad_norm": 1.1125776767730713,
        "learning_rate": 6.434512213936554e-05,
        "epoch": 0.872897711607389,
        "step": 3166
    },
    {
        "loss": 1.6119,
        "grad_norm": 2.0343270301818848,
        "learning_rate": 6.410078720913149e-05,
        "epoch": 0.8731734215605184,
        "step": 3167
    },
    {
        "loss": 2.013,
        "grad_norm": 1.1266884803771973,
        "learning_rate": 6.385669805492207e-05,
        "epoch": 0.8734491315136477,
        "step": 3168
    },
    {
        "loss": 2.2294,
        "grad_norm": 1.3795298337936401,
        "learning_rate": 6.36128563478398e-05,
        "epoch": 0.873724841466777,
        "step": 3169
    },
    {
        "loss": 0.9097,
        "grad_norm": 1.1968284845352173,
        "learning_rate": 6.336926375729297e-05,
        "epoch": 0.8740005514199063,
        "step": 3170
    },
    {
        "loss": 2.1943,
        "grad_norm": 1.9504204988479614,
        "learning_rate": 6.312592195098442e-05,
        "epoch": 0.8742762613730356,
        "step": 3171
    },
    {
        "loss": 1.1502,
        "grad_norm": 2.205717086791992,
        "learning_rate": 6.288283259490006e-05,
        "epoch": 0.8745519713261649,
        "step": 3172
    },
    {
        "loss": 2.4843,
        "grad_norm": 1.5224113464355469,
        "learning_rate": 6.26399973532974e-05,
        "epoch": 0.8748276812792942,
        "step": 3173
    },
    {
        "loss": 2.2834,
        "grad_norm": 2.1022181510925293,
        "learning_rate": 6.239741788869429e-05,
        "epoch": 0.8751033912324235,
        "step": 3174
    },
    {
        "loss": 1.774,
        "grad_norm": 1.6952027082443237,
        "learning_rate": 6.215509586185733e-05,
        "epoch": 0.8753791011855528,
        "step": 3175
    },
    {
        "loss": 1.7781,
        "grad_norm": 2.0522899627685547,
        "learning_rate": 6.191303293179079e-05,
        "epoch": 0.8756548111386822,
        "step": 3176
    },
    {
        "loss": 1.6983,
        "grad_norm": 2.115887403488159,
        "learning_rate": 6.167123075572495e-05,
        "epoch": 0.8759305210918115,
        "step": 3177
    },
    {
        "loss": 2.1029,
        "grad_norm": 1.7955900430679321,
        "learning_rate": 6.142969098910498e-05,
        "epoch": 0.8762062310449408,
        "step": 3178
    },
    {
        "loss": 1.9772,
        "grad_norm": 2.184425115585327,
        "learning_rate": 6.118841528557954e-05,
        "epoch": 0.87648194099807,
        "step": 3179
    },
    {
        "loss": 1.4981,
        "grad_norm": 1.7746994495391846,
        "learning_rate": 6.094740529698933e-05,
        "epoch": 0.8767576509511993,
        "step": 3180
    },
    {
        "loss": 2.1991,
        "grad_norm": 1.5566439628601074,
        "learning_rate": 6.0706662673356006e-05,
        "epoch": 0.8770333609043286,
        "step": 3181
    },
    {
        "loss": 1.87,
        "grad_norm": 1.664442539215088,
        "learning_rate": 6.0466189062870706e-05,
        "epoch": 0.8773090708574579,
        "step": 3182
    },
    {
        "loss": 1.5749,
        "grad_norm": 2.2637414932250977,
        "learning_rate": 6.022598611188284e-05,
        "epoch": 0.8775847808105872,
        "step": 3183
    },
    {
        "loss": 1.9148,
        "grad_norm": 2.0408341884613037,
        "learning_rate": 5.9986055464888844e-05,
        "epoch": 0.8778604907637165,
        "step": 3184
    },
    {
        "loss": 2.0795,
        "grad_norm": 1.466660976409912,
        "learning_rate": 5.9746398764520814e-05,
        "epoch": 0.8781362007168458,
        "step": 3185
    },
    {
        "loss": 2.095,
        "grad_norm": 1.664318323135376,
        "learning_rate": 5.950701765153541e-05,
        "epoch": 0.8784119106699751,
        "step": 3186
    },
    {
        "loss": 2.0246,
        "grad_norm": 1.6659793853759766,
        "learning_rate": 5.926791376480247e-05,
        "epoch": 0.8786876206231045,
        "step": 3187
    },
    {
        "loss": 2.1964,
        "grad_norm": 1.2042721509933472,
        "learning_rate": 5.9029088741293936e-05,
        "epoch": 0.8789633305762338,
        "step": 3188
    },
    {
        "loss": 1.8248,
        "grad_norm": 3.165151357650757,
        "learning_rate": 5.879054421607252e-05,
        "epoch": 0.8792390405293631,
        "step": 3189
    },
    {
        "loss": 1.4075,
        "grad_norm": 1.9875586032867432,
        "learning_rate": 5.8552281822280564e-05,
        "epoch": 0.8795147504824924,
        "step": 3190
    },
    {
        "loss": 1.043,
        "grad_norm": 1.9385253190994263,
        "learning_rate": 5.831430319112892e-05,
        "epoch": 0.8797904604356217,
        "step": 3191
    },
    {
        "loss": 1.8769,
        "grad_norm": 1.3321912288665771,
        "learning_rate": 5.8076609951885655e-05,
        "epoch": 0.880066170388751,
        "step": 3192
    },
    {
        "loss": 2.3765,
        "grad_norm": 1.7627737522125244,
        "learning_rate": 5.7839203731864956e-05,
        "epoch": 0.8803418803418803,
        "step": 3193
    },
    {
        "loss": 2.3875,
        "grad_norm": 1.044076919555664,
        "learning_rate": 5.760208615641609e-05,
        "epoch": 0.8806175902950096,
        "step": 3194
    },
    {
        "loss": 1.8791,
        "grad_norm": 1.9237140417099,
        "learning_rate": 5.7365258848912096e-05,
        "epoch": 0.8808933002481389,
        "step": 3195
    },
    {
        "loss": 1.6561,
        "grad_norm": 1.8139064311981201,
        "learning_rate": 5.712872343073874e-05,
        "epoch": 0.8811690102012683,
        "step": 3196
    },
    {
        "loss": 2.2342,
        "grad_norm": 1.5398857593536377,
        "learning_rate": 5.689248152128357e-05,
        "epoch": 0.8814447201543976,
        "step": 3197
    },
    {
        "loss": 1.1138,
        "grad_norm": 2.4387152194976807,
        "learning_rate": 5.665653473792453e-05,
        "epoch": 0.8817204301075269,
        "step": 3198
    },
    {
        "loss": 2.2242,
        "grad_norm": 1.497999906539917,
        "learning_rate": 5.6420884696019085e-05,
        "epoch": 0.8819961400606562,
        "step": 3199
    },
    {
        "loss": 1.6406,
        "grad_norm": 1.483876347541809,
        "learning_rate": 5.6185533008893246e-05,
        "epoch": 0.8822718500137855,
        "step": 3200
    },
    {
        "loss": 1.8142,
        "grad_norm": 2.4031105041503906,
        "learning_rate": 5.595048128783026e-05,
        "epoch": 0.8825475599669148,
        "step": 3201
    },
    {
        "loss": 2.1827,
        "grad_norm": 1.3882907629013062,
        "learning_rate": 5.571573114205978e-05,
        "epoch": 0.8828232699200441,
        "step": 3202
    },
    {
        "loss": 2.0267,
        "grad_norm": 2.292588233947754,
        "learning_rate": 5.548128417874675e-05,
        "epoch": 0.8830989798731734,
        "step": 3203
    },
    {
        "loss": 1.7723,
        "grad_norm": 1.9425454139709473,
        "learning_rate": 5.5247142002980556e-05,
        "epoch": 0.8833746898263027,
        "step": 3204
    },
    {
        "loss": 1.9355,
        "grad_norm": 1.5405702590942383,
        "learning_rate": 5.50133062177638e-05,
        "epoch": 0.883650399779432,
        "step": 3205
    },
    {
        "loss": 1.49,
        "grad_norm": 2.5916695594787598,
        "learning_rate": 5.477977842400147e-05,
        "epoch": 0.8839261097325614,
        "step": 3206
    },
    {
        "loss": 2.3015,
        "grad_norm": 1.3498787879943848,
        "learning_rate": 5.454656022049005e-05,
        "epoch": 0.8842018196856907,
        "step": 3207
    },
    {
        "loss": 2.4767,
        "grad_norm": 1.3772410154342651,
        "learning_rate": 5.431365320390639e-05,
        "epoch": 0.88447752963882,
        "step": 3208
    },
    {
        "loss": 2.4275,
        "grad_norm": 1.8028225898742676,
        "learning_rate": 5.408105896879684e-05,
        "epoch": 0.8847532395919493,
        "step": 3209
    },
    {
        "loss": 1.9168,
        "grad_norm": 1.9014434814453125,
        "learning_rate": 5.384877910756649e-05,
        "epoch": 0.8850289495450786,
        "step": 3210
    },
    {
        "loss": 2.1302,
        "grad_norm": 1.2872718572616577,
        "learning_rate": 5.361681521046804e-05,
        "epoch": 0.8853046594982079,
        "step": 3211
    },
    {
        "loss": 1.703,
        "grad_norm": 2.4588890075683594,
        "learning_rate": 5.338516886559104e-05,
        "epoch": 0.8855803694513372,
        "step": 3212
    },
    {
        "loss": 2.0915,
        "grad_norm": 1.829559564590454,
        "learning_rate": 5.315384165885091e-05,
        "epoch": 0.8858560794044665,
        "step": 3213
    },
    {
        "loss": 1.5173,
        "grad_norm": 2.1478872299194336,
        "learning_rate": 5.2922835173978355e-05,
        "epoch": 0.8861317893575958,
        "step": 3214
    },
    {
        "loss": 2.3675,
        "grad_norm": 1.3949429988861084,
        "learning_rate": 5.269215099250814e-05,
        "epoch": 0.8864074993107252,
        "step": 3215
    },
    {
        "loss": 1.3059,
        "grad_norm": 1.3781676292419434,
        "learning_rate": 5.246179069376849e-05,
        "epoch": 0.8866832092638545,
        "step": 3216
    },
    {
        "loss": 1.332,
        "grad_norm": 2.2043817043304443,
        "learning_rate": 5.223175585487035e-05,
        "epoch": 0.8869589192169838,
        "step": 3217
    },
    {
        "loss": 1.79,
        "grad_norm": 1.9431706666946411,
        "learning_rate": 5.200204805069635e-05,
        "epoch": 0.8872346291701131,
        "step": 3218
    },
    {
        "loss": 1.9706,
        "grad_norm": 1.9722692966461182,
        "learning_rate": 5.177266885389016e-05,
        "epoch": 0.8875103391232424,
        "step": 3219
    },
    {
        "loss": 2.4954,
        "grad_norm": 0.8109360933303833,
        "learning_rate": 5.1543619834845814e-05,
        "epoch": 0.8877860490763717,
        "step": 3220
    },
    {
        "loss": 1.7472,
        "grad_norm": 1.9037978649139404,
        "learning_rate": 5.131490256169675e-05,
        "epoch": 0.888061759029501,
        "step": 3221
    },
    {
        "loss": 2.2868,
        "grad_norm": 1.5663230419158936,
        "learning_rate": 5.108651860030524e-05,
        "epoch": 0.8883374689826302,
        "step": 3222
    },
    {
        "loss": 1.4136,
        "grad_norm": 1.813768744468689,
        "learning_rate": 5.085846951425153e-05,
        "epoch": 0.8886131789357595,
        "step": 3223
    },
    {
        "loss": 2.3605,
        "grad_norm": 1.0378309488296509,
        "learning_rate": 5.0630756864823346e-05,
        "epoch": 0.8888888888888888,
        "step": 3224
    },
    {
        "loss": 1.2086,
        "grad_norm": 1.7742820978164673,
        "learning_rate": 5.0403382211005004e-05,
        "epoch": 0.8891645988420182,
        "step": 3225
    },
    {
        "loss": 1.6279,
        "grad_norm": 2.1170098781585693,
        "learning_rate": 5.0176347109466734e-05,
        "epoch": 0.8894403087951475,
        "step": 3226
    },
    {
        "loss": 2.1974,
        "grad_norm": 1.176639199256897,
        "learning_rate": 4.994965311455426e-05,
        "epoch": 0.8897160187482768,
        "step": 3227
    },
    {
        "loss": 1.8762,
        "grad_norm": 1.834285855293274,
        "learning_rate": 4.972330177827789e-05,
        "epoch": 0.8899917287014061,
        "step": 3228
    },
    {
        "loss": 1.0483,
        "grad_norm": 2.5102250576019287,
        "learning_rate": 4.949729465030193e-05,
        "epoch": 0.8902674386545354,
        "step": 3229
    },
    {
        "loss": 1.74,
        "grad_norm": 1.7239115238189697,
        "learning_rate": 4.927163327793433e-05,
        "epoch": 0.8905431486076647,
        "step": 3230
    },
    {
        "loss": 2.3971,
        "grad_norm": 1.4668323993682861,
        "learning_rate": 4.904631920611574e-05,
        "epoch": 0.890818858560794,
        "step": 3231
    },
    {
        "loss": 2.2269,
        "grad_norm": 1.1617431640625,
        "learning_rate": 4.882135397740916e-05,
        "epoch": 0.8910945685139233,
        "step": 3232
    },
    {
        "loss": 1.9574,
        "grad_norm": 1.229647159576416,
        "learning_rate": 4.8596739131989256e-05,
        "epoch": 0.8913702784670526,
        "step": 3233
    },
    {
        "loss": 2.0549,
        "grad_norm": 1.298132300376892,
        "learning_rate": 4.837247620763198e-05,
        "epoch": 0.891645988420182,
        "step": 3234
    },
    {
        "loss": 2.6755,
        "grad_norm": 1.5636039972305298,
        "learning_rate": 4.814856673970384e-05,
        "epoch": 0.8919216983733113,
        "step": 3235
    },
    {
        "loss": 2.1949,
        "grad_norm": 1.2749613523483276,
        "learning_rate": 4.792501226115147e-05,
        "epoch": 0.8921974083264406,
        "step": 3236
    },
    {
        "loss": 2.042,
        "grad_norm": 1.256103277206421,
        "learning_rate": 4.7701814302491264e-05,
        "epoch": 0.8924731182795699,
        "step": 3237
    },
    {
        "loss": 1.6994,
        "grad_norm": 2.0331711769104004,
        "learning_rate": 4.747897439179868e-05,
        "epoch": 0.8927488282326992,
        "step": 3238
    },
    {
        "loss": 1.4153,
        "grad_norm": 2.7413456439971924,
        "learning_rate": 4.72564940546979e-05,
        "epoch": 0.8930245381858285,
        "step": 3239
    },
    {
        "loss": 1.7115,
        "grad_norm": 1.5845668315887451,
        "learning_rate": 4.703437481435133e-05,
        "epoch": 0.8933002481389578,
        "step": 3240
    },
    {
        "loss": 2.2132,
        "grad_norm": 1.4929779767990112,
        "learning_rate": 4.681261819144933e-05,
        "epoch": 0.8935759580920871,
        "step": 3241
    },
    {
        "loss": 1.1278,
        "grad_norm": 2.277304172515869,
        "learning_rate": 4.659122570419956e-05,
        "epoch": 0.8938516680452164,
        "step": 3242
    },
    {
        "loss": 1.6507,
        "grad_norm": 1.8563802242279053,
        "learning_rate": 4.63701988683167e-05,
        "epoch": 0.8941273779983457,
        "step": 3243
    },
    {
        "loss": 2.0973,
        "grad_norm": 1.3217848539352417,
        "learning_rate": 4.61495391970122e-05,
        "epoch": 0.894403087951475,
        "step": 3244
    },
    {
        "loss": 1.6983,
        "grad_norm": 2.45139479637146,
        "learning_rate": 4.59292482009837e-05,
        "epoch": 0.8946787979046044,
        "step": 3245
    },
    {
        "loss": 2.6058,
        "grad_norm": 1.095239520072937,
        "learning_rate": 4.5709327388404756e-05,
        "epoch": 0.8949545078577337,
        "step": 3246
    },
    {
        "loss": 2.5383,
        "grad_norm": 0.8897823095321655,
        "learning_rate": 4.548977826491466e-05,
        "epoch": 0.895230217810863,
        "step": 3247
    },
    {
        "loss": 1.8145,
        "grad_norm": 1.9247181415557861,
        "learning_rate": 4.5270602333607936e-05,
        "epoch": 0.8955059277639923,
        "step": 3248
    },
    {
        "loss": 1.7712,
        "grad_norm": 1.445957064628601,
        "learning_rate": 4.505180109502415e-05,
        "epoch": 0.8957816377171216,
        "step": 3249
    },
    {
        "loss": 2.1812,
        "grad_norm": 1.8220881223678589,
        "learning_rate": 4.483337604713756e-05,
        "epoch": 0.8960573476702509,
        "step": 3250
    },
    {
        "loss": 2.1076,
        "grad_norm": 1.673911213874817,
        "learning_rate": 4.461532868534706e-05,
        "epoch": 0.8963330576233802,
        "step": 3251
    },
    {
        "loss": 2.4163,
        "grad_norm": 1.1032078266143799,
        "learning_rate": 4.439766050246566e-05,
        "epoch": 0.8966087675765095,
        "step": 3252
    },
    {
        "loss": 0.9564,
        "grad_norm": 2.1381866931915283,
        "learning_rate": 4.418037298871044e-05,
        "epoch": 0.8968844775296388,
        "step": 3253
    },
    {
        "loss": 1.5795,
        "grad_norm": 1.8860244750976562,
        "learning_rate": 4.396346763169239e-05,
        "epoch": 0.8971601874827682,
        "step": 3254
    },
    {
        "loss": 2.1942,
        "grad_norm": 0.9805299639701843,
        "learning_rate": 4.374694591640606e-05,
        "epoch": 0.8974358974358975,
        "step": 3255
    },
    {
        "loss": 1.596,
        "grad_norm": 1.892143964767456,
        "learning_rate": 4.353080932521947e-05,
        "epoch": 0.8977116073890268,
        "step": 3256
    },
    {
        "loss": 1.9185,
        "grad_norm": 1.6255478858947754,
        "learning_rate": 4.331505933786409e-05,
        "epoch": 0.8979873173421561,
        "step": 3257
    },
    {
        "loss": 1.7969,
        "grad_norm": 2.1777076721191406,
        "learning_rate": 4.309969743142448e-05,
        "epoch": 0.8982630272952854,
        "step": 3258
    },
    {
        "loss": 2.4212,
        "grad_norm": 1.3664878606796265,
        "learning_rate": 4.2884725080328323e-05,
        "epoch": 0.8985387372484147,
        "step": 3259
    },
    {
        "loss": 2.3192,
        "grad_norm": 1.2968440055847168,
        "learning_rate": 4.267014375633626e-05,
        "epoch": 0.898814447201544,
        "step": 3260
    },
    {
        "loss": 2.0515,
        "grad_norm": 1.9267792701721191,
        "learning_rate": 4.245595492853197e-05,
        "epoch": 0.8990901571546733,
        "step": 3261
    },
    {
        "loss": 1.4476,
        "grad_norm": 2.1139276027679443,
        "learning_rate": 4.2242160063311875e-05,
        "epoch": 0.8993658671078026,
        "step": 3262
    },
    {
        "loss": 1.4534,
        "grad_norm": 2.2079598903656006,
        "learning_rate": 4.202876062437523e-05,
        "epoch": 0.899641577060932,
        "step": 3263
    },
    {
        "loss": 2.5176,
        "grad_norm": 1.0364594459533691,
        "learning_rate": 4.1815758072714186e-05,
        "epoch": 0.8999172870140613,
        "step": 3264
    },
    {
        "loss": 2.1429,
        "grad_norm": 2.086498975753784,
        "learning_rate": 4.160315386660358e-05,
        "epoch": 0.9001929969671905,
        "step": 3265
    },
    {
        "loss": 2.3281,
        "grad_norm": 0.871562123298645,
        "learning_rate": 4.139094946159111e-05,
        "epoch": 0.9004687069203198,
        "step": 3266
    },
    {
        "loss": 1.0399,
        "grad_norm": 2.0637741088867188,
        "learning_rate": 4.117914631048736e-05,
        "epoch": 0.9007444168734491,
        "step": 3267
    },
    {
        "loss": 2.1283,
        "grad_norm": 1.6636141538619995,
        "learning_rate": 4.096774586335578e-05,
        "epoch": 0.9010201268265784,
        "step": 3268
    },
    {
        "loss": 2.2557,
        "grad_norm": 1.7920695543289185,
        "learning_rate": 4.0756749567502774e-05,
        "epoch": 0.9012958367797077,
        "step": 3269
    },
    {
        "loss": 1.6378,
        "grad_norm": 2.2030086517333984,
        "learning_rate": 4.054615886746782e-05,
        "epoch": 0.901571546732837,
        "step": 3270
    },
    {
        "loss": 2.0441,
        "grad_norm": 1.4776642322540283,
        "learning_rate": 4.0335975205013665e-05,
        "epoch": 0.9018472566859663,
        "step": 3271
    },
    {
        "loss": 2.3987,
        "grad_norm": 2.2870118618011475,
        "learning_rate": 4.0126200019116255e-05,
        "epoch": 0.9021229666390956,
        "step": 3272
    },
    {
        "loss": 1.5683,
        "grad_norm": 1.7645231485366821,
        "learning_rate": 3.991683474595498e-05,
        "epoch": 0.902398676592225,
        "step": 3273
    },
    {
        "loss": 2.2162,
        "grad_norm": 1.4635686874389648,
        "learning_rate": 3.970788081890299e-05,
        "epoch": 0.9026743865453543,
        "step": 3274
    },
    {
        "loss": 1.309,
        "grad_norm": 2.143045663833618,
        "learning_rate": 3.949933966851711e-05,
        "epoch": 0.9029500964984836,
        "step": 3275
    },
    {
        "loss": 2.0663,
        "grad_norm": 1.4285829067230225,
        "learning_rate": 3.9291212722528225e-05,
        "epoch": 0.9032258064516129,
        "step": 3276
    },
    {
        "loss": 2.1135,
        "grad_norm": 1.9366850852966309,
        "learning_rate": 3.9083501405831444e-05,
        "epoch": 0.9035015164047422,
        "step": 3277
    },
    {
        "loss": 1.4552,
        "grad_norm": 1.890572190284729,
        "learning_rate": 3.887620714047644e-05,
        "epoch": 0.9037772263578715,
        "step": 3278
    },
    {
        "loss": 1.4303,
        "grad_norm": 1.87503981590271,
        "learning_rate": 3.866933134565754e-05,
        "epoch": 0.9040529363110008,
        "step": 3279
    },
    {
        "loss": 2.3472,
        "grad_norm": 1.6004419326782227,
        "learning_rate": 3.846287543770412e-05,
        "epoch": 0.9043286462641301,
        "step": 3280
    },
    {
        "loss": 1.5813,
        "grad_norm": 2.111102819442749,
        "learning_rate": 3.825684083007096e-05,
        "epoch": 0.9046043562172594,
        "step": 3281
    },
    {
        "loss": 2.7658,
        "grad_norm": 1.319545865058899,
        "learning_rate": 3.805122893332844e-05,
        "epoch": 0.9048800661703887,
        "step": 3282
    },
    {
        "loss": 1.9913,
        "grad_norm": 1.525533676147461,
        "learning_rate": 3.784604115515293e-05,
        "epoch": 0.905155776123518,
        "step": 3283
    },
    {
        "loss": 2.4495,
        "grad_norm": 1.320066213607788,
        "learning_rate": 3.764127890031726e-05,
        "epoch": 0.9054314860766474,
        "step": 3284
    },
    {
        "loss": 2.3266,
        "grad_norm": 2.0712473392486572,
        "learning_rate": 3.743694357068089e-05,
        "epoch": 0.9057071960297767,
        "step": 3285
    },
    {
        "loss": 2.6316,
        "grad_norm": 1.1978816986083984,
        "learning_rate": 3.7233036565180524e-05,
        "epoch": 0.905982905982906,
        "step": 3286
    },
    {
        "loss": 2.1802,
        "grad_norm": 1.2917706966400146,
        "learning_rate": 3.702955927982034e-05,
        "epoch": 0.9062586159360353,
        "step": 3287
    },
    {
        "loss": 2.0713,
        "grad_norm": 1.6257010698318481,
        "learning_rate": 3.682651310766265e-05,
        "epoch": 0.9065343258891646,
        "step": 3288
    },
    {
        "loss": 2.3962,
        "grad_norm": 1.7804409265518188,
        "learning_rate": 3.6623899438818175e-05,
        "epoch": 0.9068100358422939,
        "step": 3289
    },
    {
        "loss": 1.8315,
        "grad_norm": 1.9455021619796753,
        "learning_rate": 3.642171966043658e-05,
        "epoch": 0.9070857457954232,
        "step": 3290
    },
    {
        "loss": 1.1687,
        "grad_norm": Infinity,
        "learning_rate": 3.642171966043658e-05,
        "epoch": 0.9073614557485525,
        "step": 3291
    },
    {
        "loss": 1.8712,
        "grad_norm": 1.8960657119750977,
        "learning_rate": 3.621997515669707e-05,
        "epoch": 0.9076371657016818,
        "step": 3292
    },
    {
        "loss": 2.0303,
        "grad_norm": 1.6303483247756958,
        "learning_rate": 3.601866730879878e-05,
        "epoch": 0.9079128756548112,
        "step": 3293
    },
    {
        "loss": 1.3405,
        "grad_norm": 2.215752601623535,
        "learning_rate": 3.581779749495134e-05,
        "epoch": 0.9081885856079405,
        "step": 3294
    },
    {
        "loss": 2.3818,
        "grad_norm": 1.5875178575515747,
        "learning_rate": 3.561736709036561e-05,
        "epoch": 0.9084642955610698,
        "step": 3295
    },
    {
        "loss": 1.9905,
        "grad_norm": 1.5965509414672852,
        "learning_rate": 3.541737746724401e-05,
        "epoch": 0.9087400055141991,
        "step": 3296
    },
    {
        "loss": 2.0959,
        "grad_norm": 1.4605242013931274,
        "learning_rate": 3.521782999477128e-05,
        "epoch": 0.9090157154673284,
        "step": 3297
    },
    {
        "loss": 2.4405,
        "grad_norm": 1.5719815492630005,
        "learning_rate": 3.5018726039105046e-05,
        "epoch": 0.9092914254204577,
        "step": 3298
    },
    {
        "loss": 1.4866,
        "grad_norm": 2.0236146450042725,
        "learning_rate": 3.48200669633666e-05,
        "epoch": 0.909567135373587,
        "step": 3299
    },
    {
        "loss": 0.9785,
        "grad_norm": 1.9589391946792603,
        "learning_rate": 3.462185412763136e-05,
        "epoch": 0.9098428453267163,
        "step": 3300
    },
    {
        "loss": 2.1279,
        "grad_norm": 1.3062962293624878,
        "learning_rate": 3.442408888891965e-05,
        "epoch": 0.9101185552798456,
        "step": 3301
    },
    {
        "loss": 1.9237,
        "grad_norm": 1.6631700992584229,
        "learning_rate": 3.422677260118753e-05,
        "epoch": 0.910394265232975,
        "step": 3302
    },
    {
        "loss": 1.5386,
        "grad_norm": 2.013770818710327,
        "learning_rate": 3.402990661531731e-05,
        "epoch": 0.9106699751861043,
        "step": 3303
    },
    {
        "loss": 2.8133,
        "grad_norm": 0.795272707939148,
        "learning_rate": 3.38334922791084e-05,
        "epoch": 0.9109456851392336,
        "step": 3304
    },
    {
        "loss": 2.5396,
        "grad_norm": 3.7064194679260254,
        "learning_rate": 3.3637530937268234e-05,
        "epoch": 0.9112213950923629,
        "step": 3305
    },
    {
        "loss": 1.7255,
        "grad_norm": 1.8847267627716064,
        "learning_rate": 3.3442023931402754e-05,
        "epoch": 0.9114971050454922,
        "step": 3306
    },
    {
        "loss": 1.9255,
        "grad_norm": 2.667102813720703,
        "learning_rate": 3.3246972600007476e-05,
        "epoch": 0.9117728149986214,
        "step": 3307
    },
    {
        "loss": 2.3284,
        "grad_norm": 0.8602168560028076,
        "learning_rate": 3.3052378278458174e-05,
        "epoch": 0.9120485249517507,
        "step": 3308
    },
    {
        "loss": 0.8952,
        "grad_norm": 1.5653448104858398,
        "learning_rate": 3.285824229900194e-05,
        "epoch": 0.91232423490488,
        "step": 3309
    },
    {
        "loss": 1.9677,
        "grad_norm": 1.199678897857666,
        "learning_rate": 3.26645659907478e-05,
        "epoch": 0.9125999448580093,
        "step": 3310
    },
    {
        "loss": 2.4192,
        "grad_norm": 1.2740753889083862,
        "learning_rate": 3.247135067965775e-05,
        "epoch": 0.9128756548111386,
        "step": 3311
    },
    {
        "loss": 1.5594,
        "grad_norm": 2.0692498683929443,
        "learning_rate": 3.227859768853777e-05,
        "epoch": 0.913151364764268,
        "step": 3312
    },
    {
        "loss": 2.3589,
        "grad_norm": 0.9782968163490295,
        "learning_rate": 3.208630833702859e-05,
        "epoch": 0.9134270747173973,
        "step": 3313
    },
    {
        "loss": 1.6822,
        "grad_norm": 1.5329530239105225,
        "learning_rate": 3.189448394159673e-05,
        "epoch": 0.9137027846705266,
        "step": 3314
    },
    {
        "loss": 2.0025,
        "grad_norm": 1.6439095735549927,
        "learning_rate": 3.170312581552548e-05,
        "epoch": 0.9139784946236559,
        "step": 3315
    },
    {
        "loss": 1.882,
        "grad_norm": 2.56380558013916,
        "learning_rate": 3.151223526890602e-05,
        "epoch": 0.9142542045767852,
        "step": 3316
    },
    {
        "loss": 1.8534,
        "grad_norm": 1.0861018896102905,
        "learning_rate": 3.1321813608628244e-05,
        "epoch": 0.9145299145299145,
        "step": 3317
    },
    {
        "loss": 1.9324,
        "grad_norm": 2.2335567474365234,
        "learning_rate": 3.113186213837193e-05,
        "epoch": 0.9148056244830438,
        "step": 3318
    },
    {
        "loss": 1.6308,
        "grad_norm": 2.6115095615386963,
        "learning_rate": 3.094238215859788e-05,
        "epoch": 0.9150813344361731,
        "step": 3319
    },
    {
        "loss": 2.5349,
        "grad_norm": 1.0799856185913086,
        "learning_rate": 3.075337496653887e-05,
        "epoch": 0.9153570443893024,
        "step": 3320
    },
    {
        "loss": 2.0965,
        "grad_norm": 1.2273530960083008,
        "learning_rate": 3.056484185619082e-05,
        "epoch": 0.9156327543424317,
        "step": 3321
    },
    {
        "loss": 2.551,
        "grad_norm": 0.9857565760612488,
        "learning_rate": 3.037678411830407e-05,
        "epoch": 0.915908464295561,
        "step": 3322
    },
    {
        "loss": 2.3473,
        "grad_norm": 1.3871675729751587,
        "learning_rate": 3.0189203040374304e-05,
        "epoch": 0.9161841742486904,
        "step": 3323
    },
    {
        "loss": 1.8902,
        "grad_norm": 1.493210792541504,
        "learning_rate": 3.0002099906633908e-05,
        "epoch": 0.9164598842018197,
        "step": 3324
    },
    {
        "loss": 1.1907,
        "grad_norm": 1.5578199625015259,
        "learning_rate": 2.9815475998043107e-05,
        "epoch": 0.916735594154949,
        "step": 3325
    },
    {
        "loss": 1.1733,
        "grad_norm": 2.293260097503662,
        "learning_rate": 2.962933259228129e-05,
        "epoch": 0.9170113041080783,
        "step": 3326
    },
    {
        "loss": 1.9341,
        "grad_norm": 1.5719857215881348,
        "learning_rate": 2.944367096373811e-05,
        "epoch": 0.9172870140612076,
        "step": 3327
    },
    {
        "loss": 2.2958,
        "grad_norm": 1.1940910816192627,
        "learning_rate": 2.9258492383504842e-05,
        "epoch": 0.9175627240143369,
        "step": 3328
    },
    {
        "loss": 2.2816,
        "grad_norm": 1.5198675394058228,
        "learning_rate": 2.9073798119365747e-05,
        "epoch": 0.9178384339674662,
        "step": 3329
    },
    {
        "loss": 1.9247,
        "grad_norm": 1.3509167432785034,
        "learning_rate": 2.8889589435789255e-05,
        "epoch": 0.9181141439205955,
        "step": 3330
    },
    {
        "loss": 2.7465,
        "grad_norm": 1.5595840215682983,
        "learning_rate": 2.8705867593919366e-05,
        "epoch": 0.9183898538737248,
        "step": 3331
    },
    {
        "loss": 2.1282,
        "grad_norm": 1.5546973943710327,
        "learning_rate": 2.8522633851567115e-05,
        "epoch": 0.9186655638268542,
        "step": 3332
    },
    {
        "loss": 1.7245,
        "grad_norm": 1.956006646156311,
        "learning_rate": 2.8339889463201786e-05,
        "epoch": 0.9189412737799835,
        "step": 3333
    },
    {
        "loss": 2.4539,
        "grad_norm": 1.2313709259033203,
        "learning_rate": 2.815763567994244e-05,
        "epoch": 0.9192169837331128,
        "step": 3334
    },
    {
        "loss": 1.396,
        "grad_norm": 2.4823648929595947,
        "learning_rate": 2.7975873749549288e-05,
        "epoch": 0.9194926936862421,
        "step": 3335
    },
    {
        "loss": 1.4187,
        "grad_norm": 2.137068510055542,
        "learning_rate": 2.7794604916415267e-05,
        "epoch": 0.9197684036393714,
        "step": 3336
    },
    {
        "loss": 2.3664,
        "grad_norm": 1.8769630193710327,
        "learning_rate": 2.761383042155733e-05,
        "epoch": 0.9200441135925007,
        "step": 3337
    },
    {
        "loss": 2.0434,
        "grad_norm": 1.8412739038467407,
        "learning_rate": 2.743355150260808e-05,
        "epoch": 0.92031982354563,
        "step": 3338
    },
    {
        "loss": 2.4724,
        "grad_norm": 0.8993573188781738,
        "learning_rate": 2.7253769393807337e-05,
        "epoch": 0.9205955334987593,
        "step": 3339
    },
    {
        "loss": 2.4618,
        "grad_norm": 1.1609258651733398,
        "learning_rate": 2.7074485325993537e-05,
        "epoch": 0.9208712434518886,
        "step": 3340
    },
    {
        "loss": 1.8921,
        "grad_norm": 1.3395472764968872,
        "learning_rate": 2.689570052659539e-05,
        "epoch": 0.921146953405018,
        "step": 3341
    },
    {
        "loss": 2.2469,
        "grad_norm": 1.306262493133545,
        "learning_rate": 2.671741621962357e-05,
        "epoch": 0.9214226633581473,
        "step": 3342
    },
    {
        "loss": 1.5418,
        "grad_norm": 2.179814100265503,
        "learning_rate": 2.6539633625662143e-05,
        "epoch": 0.9216983733112766,
        "step": 3343
    },
    {
        "loss": 1.6383,
        "grad_norm": 1.9862539768218994,
        "learning_rate": 2.636235396186032e-05,
        "epoch": 0.9219740832644059,
        "step": 3344
    },
    {
        "loss": 1.8896,
        "grad_norm": 1.3286619186401367,
        "learning_rate": 2.6185578441924118e-05,
        "epoch": 0.9222497932175352,
        "step": 3345
    },
    {
        "loss": 1.6635,
        "grad_norm": 1.8167833089828491,
        "learning_rate": 2.6009308276108123e-05,
        "epoch": 0.9225255031706645,
        "step": 3346
    },
    {
        "loss": 1.6872,
        "grad_norm": 1.7043339014053345,
        "learning_rate": 2.583354467120701e-05,
        "epoch": 0.9228012131237938,
        "step": 3347
    },
    {
        "loss": 1.8192,
        "grad_norm": 1.882558822631836,
        "learning_rate": 2.5658288830547438e-05,
        "epoch": 0.9230769230769231,
        "step": 3348
    },
    {
        "loss": 1.2051,
        "grad_norm": 1.6488596200942993,
        "learning_rate": 2.548354195397983e-05,
        "epoch": 0.9233526330300524,
        "step": 3349
    },
    {
        "loss": 1.7667,
        "grad_norm": 1.7086822986602783,
        "learning_rate": 2.5309305237870008e-05,
        "epoch": 0.9236283429831816,
        "step": 3350
    },
    {
        "loss": 1.4457,
        "grad_norm": 1.9625085592269897,
        "learning_rate": 2.5135579875091144e-05,
        "epoch": 0.923904052936311,
        "step": 3351
    },
    {
        "loss": 2.0416,
        "grad_norm": 1.8473222255706787,
        "learning_rate": 2.49623670550155e-05,
        "epoch": 0.9241797628894403,
        "step": 3352
    },
    {
        "loss": 2.1737,
        "grad_norm": 1.5299309492111206,
        "learning_rate": 2.4789667963506412e-05,
        "epoch": 0.9244554728425696,
        "step": 3353
    },
    {
        "loss": 1.9015,
        "grad_norm": 1.6494288444519043,
        "learning_rate": 2.461748378291001e-05,
        "epoch": 0.9247311827956989,
        "step": 3354
    },
    {
        "loss": 2.226,
        "grad_norm": 1.726065993309021,
        "learning_rate": 2.4445815692047202e-05,
        "epoch": 0.9250068927488282,
        "step": 3355
    },
    {
        "loss": 1.7844,
        "grad_norm": 2.288015842437744,
        "learning_rate": 2.4274664866205675e-05,
        "epoch": 0.9252826027019575,
        "step": 3356
    },
    {
        "loss": 1.7871,
        "grad_norm": 1.5450364351272583,
        "learning_rate": 2.410403247713169e-05,
        "epoch": 0.9255583126550868,
        "step": 3357
    },
    {
        "loss": 2.0347,
        "grad_norm": 1.4259750843048096,
        "learning_rate": 2.3933919693022168e-05,
        "epoch": 0.9258340226082161,
        "step": 3358
    },
    {
        "loss": 2.3703,
        "grad_norm": 1.3127411603927612,
        "learning_rate": 2.3764327678516707e-05,
        "epoch": 0.9261097325613454,
        "step": 3359
    },
    {
        "loss": 1.7691,
        "grad_norm": 2.5822741985321045,
        "learning_rate": 2.3595257594689557e-05,
        "epoch": 0.9263854425144747,
        "step": 3360
    },
    {
        "loss": 1.3744,
        "grad_norm": 1.6599239110946655,
        "learning_rate": 2.3426710599041647e-05,
        "epoch": 0.926661152467604,
        "step": 3361
    },
    {
        "loss": 2.585,
        "grad_norm": 1.1977485418319702,
        "learning_rate": 2.32586878454927e-05,
        "epoch": 0.9269368624207334,
        "step": 3362
    },
    {
        "loss": 1.5248,
        "grad_norm": 1.3652809858322144,
        "learning_rate": 2.3091190484373415e-05,
        "epoch": 0.9272125723738627,
        "step": 3363
    },
    {
        "loss": 2.0838,
        "grad_norm": 1.8293156623840332,
        "learning_rate": 2.2924219662417412e-05,
        "epoch": 0.927488282326992,
        "step": 3364
    },
    {
        "loss": 0.6405,
        "grad_norm": 1.4464436769485474,
        "learning_rate": 2.2757776522753504e-05,
        "epoch": 0.9277639922801213,
        "step": 3365
    },
    {
        "loss": 1.1428,
        "grad_norm": 2.4763152599334717,
        "learning_rate": 2.2591862204897896e-05,
        "epoch": 0.9280397022332506,
        "step": 3366
    },
    {
        "loss": 1.7444,
        "grad_norm": 1.6313822269439697,
        "learning_rate": 2.2426477844746273e-05,
        "epoch": 0.9283154121863799,
        "step": 3367
    },
    {
        "loss": 1.8023,
        "grad_norm": 1.503971815109253,
        "learning_rate": 2.2261624574566053e-05,
        "epoch": 0.9285911221395092,
        "step": 3368
    },
    {
        "loss": 2.1623,
        "grad_norm": 2.0900988578796387,
        "learning_rate": 2.2097303522988778e-05,
        "epoch": 0.9288668320926385,
        "step": 3369
    },
    {
        "loss": 2.0351,
        "grad_norm": 1.6210006475448608,
        "learning_rate": 2.193351581500217e-05,
        "epoch": 0.9291425420457678,
        "step": 3370
    },
    {
        "loss": 1.2342,
        "grad_norm": 2.1431543827056885,
        "learning_rate": 2.177026257194256e-05,
        "epoch": 0.9294182519988972,
        "step": 3371
    },
    {
        "loss": 2.0975,
        "grad_norm": 1.598764419555664,
        "learning_rate": 2.1607544911487177e-05,
        "epoch": 0.9296939619520265,
        "step": 3372
    },
    {
        "loss": 1.6388,
        "grad_norm": 2.0586628913879395,
        "learning_rate": 2.144536394764656e-05,
        "epoch": 0.9299696719051558,
        "step": 3373
    },
    {
        "loss": 0.9207,
        "grad_norm": 2.3003029823303223,
        "learning_rate": 2.128372079075679e-05,
        "epoch": 0.9302453818582851,
        "step": 3374
    },
    {
        "loss": 1.382,
        "grad_norm": 1.2992215156555176,
        "learning_rate": 2.1122616547471997e-05,
        "epoch": 0.9305210918114144,
        "step": 3375
    },
    {
        "loss": 2.1907,
        "grad_norm": 1.3253612518310547,
        "learning_rate": 2.0962052320756807e-05,
        "epoch": 0.9307968017645437,
        "step": 3376
    },
    {
        "loss": 2.6169,
        "grad_norm": 1.3124701976776123,
        "learning_rate": 2.0802029209878706e-05,
        "epoch": 0.931072511717673,
        "step": 3377
    },
    {
        "loss": 0.8804,
        "grad_norm": 2.435084581375122,
        "learning_rate": 2.0642548310400488e-05,
        "epoch": 0.9313482216708023,
        "step": 3378
    },
    {
        "loss": 2.1033,
        "grad_norm": 1.752563238143921,
        "learning_rate": 2.0483610714172962e-05,
        "epoch": 0.9316239316239316,
        "step": 3379
    },
    {
        "loss": 1.6292,
        "grad_norm": 1.827061653137207,
        "learning_rate": 2.03252175093272e-05,
        "epoch": 0.931899641577061,
        "step": 3380
    },
    {
        "loss": 2.4758,
        "grad_norm": 0.9827604293823242,
        "learning_rate": 2.0167369780267264e-05,
        "epoch": 0.9321753515301903,
        "step": 3381
    },
    {
        "loss": 1.6577,
        "grad_norm": 1.6954008340835571,
        "learning_rate": 2.0010068607662723e-05,
        "epoch": 0.9324510614833196,
        "step": 3382
    },
    {
        "loss": 1.7947,
        "grad_norm": 1.9215359687805176,
        "learning_rate": 1.9853315068441336e-05,
        "epoch": 0.9327267714364489,
        "step": 3383
    },
    {
        "loss": 2.2814,
        "grad_norm": 2.2264747619628906,
        "learning_rate": 1.9697110235781523e-05,
        "epoch": 0.9330024813895782,
        "step": 3384
    },
    {
        "loss": 2.2851,
        "grad_norm": 1.5070984363555908,
        "learning_rate": 1.9541455179105116e-05,
        "epoch": 0.9332781913427075,
        "step": 3385
    },
    {
        "loss": 1.3704,
        "grad_norm": 2.0246963500976562,
        "learning_rate": 1.938635096407011e-05,
        "epoch": 0.9335539012958368,
        "step": 3386
    },
    {
        "loss": 2.0039,
        "grad_norm": 1.6388250589370728,
        "learning_rate": 1.9231798652563194e-05,
        "epoch": 0.9338296112489661,
        "step": 3387
    },
    {
        "loss": 2.1465,
        "grad_norm": 1.17600417137146,
        "learning_rate": 1.90777993026926e-05,
        "epoch": 0.9341053212020954,
        "step": 3388
    },
    {
        "loss": 2.323,
        "grad_norm": 1.38994300365448,
        "learning_rate": 1.8924353968780827e-05,
        "epoch": 0.9343810311552248,
        "step": 3389
    },
    {
        "loss": 2.1632,
        "grad_norm": 1.983069658279419,
        "learning_rate": 1.8771463701357484e-05,
        "epoch": 0.9346567411083541,
        "step": 3390
    },
    {
        "loss": 1.7498,
        "grad_norm": 2.51069974899292,
        "learning_rate": 1.8619129547151936e-05,
        "epoch": 0.9349324510614834,
        "step": 3391
    },
    {
        "loss": 2.3265,
        "grad_norm": 1.5955711603164673,
        "learning_rate": 1.8467352549086303e-05,
        "epoch": 0.9352081610146127,
        "step": 3392
    },
    {
        "loss": 2.2815,
        "grad_norm": 1.3377183675765991,
        "learning_rate": 1.831613374626828e-05,
        "epoch": 0.9354838709677419,
        "step": 3393
    },
    {
        "loss": 2.3154,
        "grad_norm": 1.1707379817962646,
        "learning_rate": 1.8165474173983965e-05,
        "epoch": 0.9357595809208712,
        "step": 3394
    },
    {
        "loss": 1.507,
        "grad_norm": 1.556698203086853,
        "learning_rate": 1.801537486369077e-05,
        "epoch": 0.9360352908740005,
        "step": 3395
    },
    {
        "loss": 2.0454,
        "grad_norm": 1.8295155763626099,
        "learning_rate": 1.786583684301051e-05,
        "epoch": 0.9363110008271298,
        "step": 3396
    },
    {
        "loss": 2.1549,
        "grad_norm": 1.4649935960769653,
        "learning_rate": 1.7716861135722162e-05,
        "epoch": 0.9365867107802591,
        "step": 3397
    },
    {
        "loss": 2.5662,
        "grad_norm": 1.1037638187408447,
        "learning_rate": 1.7568448761754962e-05,
        "epoch": 0.9368624207333884,
        "step": 3398
    },
    {
        "loss": 2.2272,
        "grad_norm": 0.94023597240448,
        "learning_rate": 1.7420600737181404e-05,
        "epoch": 0.9371381306865177,
        "step": 3399
    },
    {
        "loss": 2.7984,
        "grad_norm": 1.5027493238449097,
        "learning_rate": 1.7273318074210354e-05,
        "epoch": 0.9374138406396471,
        "step": 3400
    },
    {
        "loss": 2.4978,
        "grad_norm": 1.4265508651733398,
        "learning_rate": 1.7126601781179984e-05,
        "epoch": 0.9376895505927764,
        "step": 3401
    },
    {
        "loss": 0.7754,
        "grad_norm": 1.8766827583312988,
        "learning_rate": 1.6980452862550933e-05,
        "epoch": 0.9379652605459057,
        "step": 3402
    },
    {
        "loss": 1.7956,
        "grad_norm": 1.2997292280197144,
        "learning_rate": 1.683487231889944e-05,
        "epoch": 0.938240970499035,
        "step": 3403
    },
    {
        "loss": 2.1852,
        "grad_norm": 1.4468737840652466,
        "learning_rate": 1.6689861146910645e-05,
        "epoch": 0.9385166804521643,
        "step": 3404
    },
    {
        "loss": 2.1831,
        "grad_norm": 1.5197426080703735,
        "learning_rate": 1.6545420339371388e-05,
        "epoch": 0.9387923904052936,
        "step": 3405
    },
    {
        "loss": 1.5022,
        "grad_norm": 2.68436598777771,
        "learning_rate": 1.6401550885163764e-05,
        "epoch": 0.9390681003584229,
        "step": 3406
    },
    {
        "loss": 1.8653,
        "grad_norm": 1.8564467430114746,
        "learning_rate": 1.625825376925817e-05,
        "epoch": 0.9393438103115522,
        "step": 3407
    },
    {
        "loss": 2.3853,
        "grad_norm": 1.6585912704467773,
        "learning_rate": 1.611552997270661e-05,
        "epoch": 0.9396195202646815,
        "step": 3408
    },
    {
        "loss": 1.4504,
        "grad_norm": 2.020214557647705,
        "learning_rate": 1.5973380472636047e-05,
        "epoch": 0.9398952302178109,
        "step": 3409
    },
    {
        "loss": 2.5261,
        "grad_norm": 1.1028836965560913,
        "learning_rate": 1.5831806242241597e-05,
        "epoch": 0.9401709401709402,
        "step": 3410
    },
    {
        "loss": 1.2916,
        "grad_norm": 2.9651710987091064,
        "learning_rate": 1.569080825077994e-05,
        "epoch": 0.9404466501240695,
        "step": 3411
    },
    {
        "loss": 1.0356,
        "grad_norm": 1.932867407798767,
        "learning_rate": 1.555038746356261e-05,
        "epoch": 0.9407223600771988,
        "step": 3412
    },
    {
        "loss": 2.1129,
        "grad_norm": 1.5339200496673584,
        "learning_rate": 1.5410544841949548e-05,
        "epoch": 0.9409980700303281,
        "step": 3413
    },
    {
        "loss": 2.0536,
        "grad_norm": 1.7006036043167114,
        "learning_rate": 1.5271281343342337e-05,
        "epoch": 0.9412737799834574,
        "step": 3414
    },
    {
        "loss": 1.4759,
        "grad_norm": 1.7902554273605347,
        "learning_rate": 1.5132597921177683e-05,
        "epoch": 0.9415494899365867,
        "step": 3415
    },
    {
        "loss": 1.7149,
        "grad_norm": 1.6799564361572266,
        "learning_rate": 1.4994495524921049e-05,
        "epoch": 0.941825199889716,
        "step": 3416
    },
    {
        "loss": 1.5397,
        "grad_norm": 1.7761414051055908,
        "learning_rate": 1.4856975100059923e-05,
        "epoch": 0.9421009098428453,
        "step": 3417
    },
    {
        "loss": 2.5804,
        "grad_norm": 1.188879370689392,
        "learning_rate": 1.472003758809749e-05,
        "epoch": 0.9423766197959746,
        "step": 3418
    },
    {
        "loss": 2.0096,
        "grad_norm": 1.5198452472686768,
        "learning_rate": 1.4583683926546187e-05,
        "epoch": 0.942652329749104,
        "step": 3419
    },
    {
        "loss": 2.1587,
        "grad_norm": 1.0788973569869995,
        "learning_rate": 1.4447915048921201e-05,
        "epoch": 0.9429280397022333,
        "step": 3420
    },
    {
        "loss": 1.3596,
        "grad_norm": 1.2686896324157715,
        "learning_rate": 1.4312731884734142e-05,
        "epoch": 0.9432037496553626,
        "step": 3421
    },
    {
        "loss": 2.0398,
        "grad_norm": 1.6906930208206177,
        "learning_rate": 1.4178135359486622e-05,
        "epoch": 0.9434794596084919,
        "step": 3422
    },
    {
        "loss": 1.3564,
        "grad_norm": 2.091996908187866,
        "learning_rate": 1.404412639466406e-05,
        "epoch": 0.9437551695616212,
        "step": 3423
    },
    {
        "loss": 1.4486,
        "grad_norm": 1.9240787029266357,
        "learning_rate": 1.3910705907729183e-05,
        "epoch": 0.9440308795147505,
        "step": 3424
    },
    {
        "loss": 2.1899,
        "grad_norm": 2.031609296798706,
        "learning_rate": 1.3777874812115831e-05,
        "epoch": 0.9443065894678798,
        "step": 3425
    },
    {
        "loss": 1.9878,
        "grad_norm": 1.428389310836792,
        "learning_rate": 1.3645634017222797e-05,
        "epoch": 0.9445822994210091,
        "step": 3426
    },
    {
        "loss": 2.5223,
        "grad_norm": 1.27607262134552,
        "learning_rate": 1.351398442840741e-05,
        "epoch": 0.9448580093741384,
        "step": 3427
    },
    {
        "loss": 1.5535,
        "grad_norm": 2.6495587825775146,
        "learning_rate": 1.3382926946979479e-05,
        "epoch": 0.9451337193272678,
        "step": 3428
    },
    {
        "loss": 2.4252,
        "grad_norm": 1.7500402927398682,
        "learning_rate": 1.3252462470195104e-05,
        "epoch": 0.9454094292803971,
        "step": 3429
    },
    {
        "loss": 1.6521,
        "grad_norm": 2.025418758392334,
        "learning_rate": 1.3122591891250468e-05,
        "epoch": 0.9456851392335264,
        "step": 3430
    },
    {
        "loss": 2.1185,
        "grad_norm": 1.7665470838546753,
        "learning_rate": 1.2993316099275799e-05,
        "epoch": 0.9459608491866557,
        "step": 3431
    },
    {
        "loss": 2.1774,
        "grad_norm": 2.2275586128234863,
        "learning_rate": 1.2864635979329176e-05,
        "epoch": 0.946236559139785,
        "step": 3432
    },
    {
        "loss": 2.1256,
        "grad_norm": 1.8818202018737793,
        "learning_rate": 1.2736552412390668e-05,
        "epoch": 0.9465122690929143,
        "step": 3433
    },
    {
        "loss": 2.2586,
        "grad_norm": 1.6684755086898804,
        "learning_rate": 1.2609066275356052e-05,
        "epoch": 0.9467879790460436,
        "step": 3434
    },
    {
        "loss": 2.5037,
        "grad_norm": 0.7366995215415955,
        "learning_rate": 1.248217844103099e-05,
        "epoch": 0.9470636889991728,
        "step": 3435
    },
    {
        "loss": 1.9368,
        "grad_norm": 2.7203075885772705,
        "learning_rate": 1.2355889778125018e-05,
        "epoch": 0.9473393989523021,
        "step": 3436
    },
    {
        "loss": 2.3947,
        "grad_norm": 1.223677396774292,
        "learning_rate": 1.2230201151245535e-05,
        "epoch": 0.9476151089054314,
        "step": 3437
    },
    {
        "loss": 2.4031,
        "grad_norm": 1.0964025259017944,
        "learning_rate": 1.2105113420891945e-05,
        "epoch": 0.9478908188585607,
        "step": 3438
    },
    {
        "loss": 2.2363,
        "grad_norm": 1.4518911838531494,
        "learning_rate": 1.1980627443449699e-05,
        "epoch": 0.9481665288116901,
        "step": 3439
    },
    {
        "loss": 2.4307,
        "grad_norm": 1.5697969198226929,
        "learning_rate": 1.185674407118459e-05,
        "epoch": 0.9484422387648194,
        "step": 3440
    },
    {
        "loss": 1.9187,
        "grad_norm": 2.0936999320983887,
        "learning_rate": 1.1733464152236717e-05,
        "epoch": 0.9487179487179487,
        "step": 3441
    },
    {
        "loss": 2.0996,
        "grad_norm": 1.8437085151672363,
        "learning_rate": 1.1610788530614758e-05,
        "epoch": 0.948993658671078,
        "step": 3442
    },
    {
        "loss": 1.6595,
        "grad_norm": 2.046584129333496,
        "learning_rate": 1.1488718046190294e-05,
        "epoch": 0.9492693686242073,
        "step": 3443
    },
    {
        "loss": 2.087,
        "grad_norm": 1.0593181848526,
        "learning_rate": 1.136725353469188e-05,
        "epoch": 0.9495450785773366,
        "step": 3444
    },
    {
        "loss": 2.0932,
        "grad_norm": 1.4913092851638794,
        "learning_rate": 1.124639582769943e-05,
        "epoch": 0.9498207885304659,
        "step": 3445
    },
    {
        "loss": 2.3096,
        "grad_norm": 1.0676724910736084,
        "learning_rate": 1.1126145752638572e-05,
        "epoch": 0.9500964984835952,
        "step": 3446
    },
    {
        "loss": 2.1587,
        "grad_norm": 1.2719178199768066,
        "learning_rate": 1.100650413277482e-05,
        "epoch": 0.9503722084367245,
        "step": 3447
    },
    {
        "loss": 1.35,
        "grad_norm": 2.2272520065307617,
        "learning_rate": 1.0887471787208082e-05,
        "epoch": 0.9506479183898539,
        "step": 3448
    },
    {
        "loss": 1.959,
        "grad_norm": 1.9942480325698853,
        "learning_rate": 1.0769049530866992e-05,
        "epoch": 0.9509236283429832,
        "step": 3449
    },
    {
        "loss": 2.1678,
        "grad_norm": 1.3632078170776367,
        "learning_rate": 1.0651238174503386e-05,
        "epoch": 0.9511993382961125,
        "step": 3450
    },
    {
        "loss": 1.5651,
        "grad_norm": 1.8330105543136597,
        "learning_rate": 1.053403852468664e-05,
        "epoch": 0.9514750482492418,
        "step": 3451
    },
    {
        "loss": 2.0742,
        "grad_norm": 1.046756386756897,
        "learning_rate": 1.0417451383798249e-05,
        "epoch": 0.9517507582023711,
        "step": 3452
    },
    {
        "loss": 1.7608,
        "grad_norm": 1.1161442995071411,
        "learning_rate": 1.0301477550026318e-05,
        "epoch": 0.9520264681555004,
        "step": 3453
    },
    {
        "loss": 2.3882,
        "grad_norm": 1.0512189865112305,
        "learning_rate": 1.0186117817360064e-05,
        "epoch": 0.9523021781086297,
        "step": 3454
    },
    {
        "loss": 1.6534,
        "grad_norm": 1.92354154586792,
        "learning_rate": 1.007137297558436e-05,
        "epoch": 0.952577888061759,
        "step": 3455
    },
    {
        "loss": 0.8378,
        "grad_norm": 2.5341358184814453,
        "learning_rate": 9.957243810274453e-06,
        "epoch": 0.9528535980148883,
        "step": 3456
    },
    {
        "loss": 1.3733,
        "grad_norm": 2.284719467163086,
        "learning_rate": 9.843731102790421e-06,
        "epoch": 0.9531293079680176,
        "step": 3457
    },
    {
        "loss": 1.9789,
        "grad_norm": 1.7179211378097534,
        "learning_rate": 9.730835630271918e-06,
        "epoch": 0.953405017921147,
        "step": 3458
    },
    {
        "loss": 2.59,
        "grad_norm": 1.0207120180130005,
        "learning_rate": 9.618558165632819e-06,
        "epoch": 0.9536807278742763,
        "step": 3459
    },
    {
        "loss": 1.8152,
        "grad_norm": 0.9339898824691772,
        "learning_rate": 9.50689947755602e-06,
        "epoch": 0.9539564378274056,
        "step": 3460
    },
    {
        "loss": 2.0913,
        "grad_norm": 0.8987830877304077,
        "learning_rate": 9.395860330488015e-06,
        "epoch": 0.9542321477805349,
        "step": 3461
    },
    {
        "loss": 2.2056,
        "grad_norm": 2.3813979625701904,
        "learning_rate": 9.285441484633761e-06,
        "epoch": 0.9545078577336642,
        "step": 3462
    },
    {
        "loss": 1.8967,
        "grad_norm": 2.5231552124023438,
        "learning_rate": 9.175643695951507e-06,
        "epoch": 0.9547835676867935,
        "step": 3463
    },
    {
        "loss": 1.0389,
        "grad_norm": 2.39864444732666,
        "learning_rate": 9.066467716147543e-06,
        "epoch": 0.9550592776399228,
        "step": 3464
    },
    {
        "loss": 2.3945,
        "grad_norm": 1.253906488418579,
        "learning_rate": 8.957914292671055e-06,
        "epoch": 0.9553349875930521,
        "step": 3465
    },
    {
        "loss": 1.7041,
        "grad_norm": 1.8086276054382324,
        "learning_rate": 8.849984168709091e-06,
        "epoch": 0.9556106975461814,
        "step": 3466
    },
    {
        "loss": 2.1048,
        "grad_norm": 1.5603086948394775,
        "learning_rate": 8.742678083181366e-06,
        "epoch": 0.9558864074993108,
        "step": 3467
    },
    {
        "loss": 1.7201,
        "grad_norm": 2.0328516960144043,
        "learning_rate": 8.635996770735278e-06,
        "epoch": 0.9561621174524401,
        "step": 3468
    },
    {
        "loss": 1.7552,
        "grad_norm": 2.5242905616760254,
        "learning_rate": 8.529940961740845e-06,
        "epoch": 0.9564378274055694,
        "step": 3469
    },
    {
        "loss": 1.8442,
        "grad_norm": 1.755151629447937,
        "learning_rate": 8.424511382285727e-06,
        "epoch": 0.9567135373586987,
        "step": 3470
    },
    {
        "loss": 1.659,
        "grad_norm": 2.255295753479004,
        "learning_rate": 8.319708754170241e-06,
        "epoch": 0.956989247311828,
        "step": 3471
    },
    {
        "loss": 2.568,
        "grad_norm": 1.1155606508255005,
        "learning_rate": 8.215533794902397e-06,
        "epoch": 0.9572649572649573,
        "step": 3472
    },
    {
        "loss": 2.0965,
        "grad_norm": 1.1971142292022705,
        "learning_rate": 8.11198721769304e-06,
        "epoch": 0.9575406672180866,
        "step": 3473
    },
    {
        "loss": 2.0566,
        "grad_norm": 1.8216736316680908,
        "learning_rate": 8.009069731450913e-06,
        "epoch": 0.9578163771712159,
        "step": 3474
    },
    {
        "loss": 2.4001,
        "grad_norm": 1.7424217462539673,
        "learning_rate": 7.906782040777815e-06,
        "epoch": 0.9580920871243452,
        "step": 3475
    },
    {
        "loss": 2.511,
        "grad_norm": 1.5282847881317139,
        "learning_rate": 7.805124845963863e-06,
        "epoch": 0.9583677970774745,
        "step": 3476
    },
    {
        "loss": 1.9591,
        "grad_norm": 1.7573896646499634,
        "learning_rate": 7.704098842982521e-06,
        "epoch": 0.9586435070306039,
        "step": 3477
    },
    {
        "loss": 1.2941,
        "grad_norm": 2.4104957580566406,
        "learning_rate": 7.603704723486016e-06,
        "epoch": 0.9589192169837331,
        "step": 3478
    },
    {
        "loss": 2.3218,
        "grad_norm": 2.0300509929656982,
        "learning_rate": 7.503943174800454e-06,
        "epoch": 0.9591949269368624,
        "step": 3479
    },
    {
        "loss": 1.9139,
        "grad_norm": 1.9206583499908447,
        "learning_rate": 7.404814879921274e-06,
        "epoch": 0.9594706368899917,
        "step": 3480
    },
    {
        "loss": 1.6083,
        "grad_norm": 2.016205072402954,
        "learning_rate": 7.306320517508413e-06,
        "epoch": 0.959746346843121,
        "step": 3481
    },
    {
        "loss": 2.3101,
        "grad_norm": 0.8290372490882874,
        "learning_rate": 7.20846076188173e-06,
        "epoch": 0.9600220567962503,
        "step": 3482
    },
    {
        "loss": 1.8652,
        "grad_norm": 1.8645319938659668,
        "learning_rate": 7.111236283016443e-06,
        "epoch": 0.9602977667493796,
        "step": 3483
    },
    {
        "loss": 2.076,
        "grad_norm": 1.276890754699707,
        "learning_rate": 7.0146477465384494e-06,
        "epoch": 0.9605734767025089,
        "step": 3484
    },
    {
        "loss": 1.7024,
        "grad_norm": 1.8492072820663452,
        "learning_rate": 6.9186958137198e-06,
        "epoch": 0.9608491866556382,
        "step": 3485
    },
    {
        "loss": 2.2536,
        "grad_norm": 1.3267364501953125,
        "learning_rate": 6.823381141474183e-06,
        "epoch": 0.9611248966087675,
        "step": 3486
    },
    {
        "loss": 1.62,
        "grad_norm": 2.0419230461120605,
        "learning_rate": 6.728704382352458e-06,
        "epoch": 0.9614006065618969,
        "step": 3487
    },
    {
        "loss": 1.8357,
        "grad_norm": 1.6078805923461914,
        "learning_rate": 6.634666184538119e-06,
        "epoch": 0.9616763165150262,
        "step": 3488
    },
    {
        "loss": 1.7273,
        "grad_norm": 1.3674463033676147,
        "learning_rate": 6.54126719184287e-06,
        "epoch": 0.9619520264681555,
        "step": 3489
    },
    {
        "loss": 2.2425,
        "grad_norm": 0.8381766080856323,
        "learning_rate": 6.448508043702317e-06,
        "epoch": 0.9622277364212848,
        "step": 3490
    },
    {
        "loss": 1.356,
        "grad_norm": 1.67000412940979,
        "learning_rate": 6.356389375171445e-06,
        "epoch": 0.9625034463744141,
        "step": 3491
    },
    {
        "loss": 2.144,
        "grad_norm": 2.030742883682251,
        "learning_rate": 6.264911816920327e-06,
        "epoch": 0.9627791563275434,
        "step": 3492
    },
    {
        "loss": 2.6006,
        "grad_norm": 0.9777554273605347,
        "learning_rate": 6.174075995229889e-06,
        "epoch": 0.9630548662806727,
        "step": 3493
    },
    {
        "loss": 1.7612,
        "grad_norm": 0.8773259520530701,
        "learning_rate": 6.0838825319874834e-06,
        "epoch": 0.963330576233802,
        "step": 3494
    },
    {
        "loss": 2.1228,
        "grad_norm": 2.1643686294555664,
        "learning_rate": 5.9943320446827155e-06,
        "epoch": 0.9636062861869313,
        "step": 3495
    },
    {
        "loss": 2.1505,
        "grad_norm": 1.1540182828903198,
        "learning_rate": 5.9054251464031875e-06,
        "epoch": 0.9638819961400606,
        "step": 3496
    },
    {
        "loss": 1.1179,
        "grad_norm": 1.394961953163147,
        "learning_rate": 5.817162445830349e-06,
        "epoch": 0.96415770609319,
        "step": 3497
    },
    {
        "loss": 1.7914,
        "grad_norm": 1.7759015560150146,
        "learning_rate": 5.729544547235266e-06,
        "epoch": 0.9644334160463193,
        "step": 3498
    },
    {
        "loss": 2.2763,
        "grad_norm": 0.9934884309768677,
        "learning_rate": 5.642572050474493e-06,
        "epoch": 0.9647091259994486,
        "step": 3499
    },
    {
        "loss": 2.0628,
        "grad_norm": 1.5546365976333618,
        "learning_rate": 5.556245550986039e-06,
        "epoch": 0.9649848359525779,
        "step": 3500
    },
    {
        "loss": 2.0383,
        "grad_norm": 1.711354374885559,
        "learning_rate": 5.470565639785163e-06,
        "epoch": 0.9652605459057072,
        "step": 3501
    },
    {
        "loss": 2.2846,
        "grad_norm": 1.4312782287597656,
        "learning_rate": 5.3855329034604555e-06,
        "epoch": 0.9655362558588365,
        "step": 3502
    },
    {
        "loss": 2.5594,
        "grad_norm": 1.0002822875976562,
        "learning_rate": 5.301147924169747e-06,
        "epoch": 0.9658119658119658,
        "step": 3503
    },
    {
        "loss": 1.5294,
        "grad_norm": 2.1357057094573975,
        "learning_rate": 5.217411279636153e-06,
        "epoch": 0.9660876757650951,
        "step": 3504
    },
    {
        "loss": 1.7044,
        "grad_norm": 1.9324557781219482,
        "learning_rate": 5.134323543144093e-06,
        "epoch": 0.9663633857182244,
        "step": 3505
    },
    {
        "loss": 2.052,
        "grad_norm": 1.1086695194244385,
        "learning_rate": 5.051885283535363e-06,
        "epoch": 0.9666390956713538,
        "step": 3506
    },
    {
        "loss": 2.1854,
        "grad_norm": 2.1421194076538086,
        "learning_rate": 4.970097065205326e-06,
        "epoch": 0.9669148056244831,
        "step": 3507
    },
    {
        "loss": 2.0142,
        "grad_norm": 2.136157989501953,
        "learning_rate": 4.888959448098907e-06,
        "epoch": 0.9671905155776124,
        "step": 3508
    },
    {
        "loss": 2.0677,
        "grad_norm": 1.100439190864563,
        "learning_rate": 4.8084729877068805e-06,
        "epoch": 0.9674662255307417,
        "step": 3509
    },
    {
        "loss": 1.7852,
        "grad_norm": 0.9901208281517029,
        "learning_rate": 4.72863823506201e-06,
        "epoch": 0.967741935483871,
        "step": 3510
    },
    {
        "loss": 1.9216,
        "grad_norm": 1.9925954341888428,
        "learning_rate": 4.649455736735275e-06,
        "epoch": 0.9680176454370003,
        "step": 3511
    },
    {
        "loss": 1.4603,
        "grad_norm": 2.724843978881836,
        "learning_rate": 4.570926034832134e-06,
        "epoch": 0.9682933553901296,
        "step": 3512
    },
    {
        "loss": 1.1913,
        "grad_norm": 2.719921827316284,
        "learning_rate": 4.493049666988858e-06,
        "epoch": 0.9685690653432589,
        "step": 3513
    },
    {
        "loss": 2.2116,
        "grad_norm": 1.8277714252471924,
        "learning_rate": 4.415827166368769e-06,
        "epoch": 0.9688447752963882,
        "step": 3514
    },
    {
        "loss": 1.6744,
        "grad_norm": 2.4227640628814697,
        "learning_rate": 4.339259061658652e-06,
        "epoch": 0.9691204852495175,
        "step": 3515
    },
    {
        "loss": 1.3831,
        "grad_norm": 1.7181038856506348,
        "learning_rate": 4.263345877065095e-06,
        "epoch": 0.9693961952026469,
        "step": 3516
    },
    {
        "loss": 1.387,
        "grad_norm": 1.3346505165100098,
        "learning_rate": 4.188088132310963e-06,
        "epoch": 0.9696719051557762,
        "step": 3517
    },
    {
        "loss": 1.811,
        "grad_norm": 1.8303489685058594,
        "learning_rate": 4.113486342631745e-06,
        "epoch": 0.9699476151089055,
        "step": 3518
    },
    {
        "loss": 2.7488,
        "grad_norm": 1.134303331375122,
        "learning_rate": 4.039541018772108e-06,
        "epoch": 0.9702233250620348,
        "step": 3519
    },
    {
        "loss": 2.2802,
        "grad_norm": 1.0824122428894043,
        "learning_rate": 3.966252666982384e-06,
        "epoch": 0.9704990350151641,
        "step": 3520
    },
    {
        "loss": 1.7937,
        "grad_norm": 2.4685208797454834,
        "learning_rate": 3.8936217890150455e-06,
        "epoch": 0.9707747449682933,
        "step": 3521
    },
    {
        "loss": 1.9204,
        "grad_norm": 1.9042389392852783,
        "learning_rate": 3.821648882121365e-06,
        "epoch": 0.9710504549214226,
        "step": 3522
    },
    {
        "loss": 2.1151,
        "grad_norm": 1.6567872762680054,
        "learning_rate": 3.75033443904792e-06,
        "epoch": 0.9713261648745519,
        "step": 3523
    },
    {
        "loss": 1.9835,
        "grad_norm": 1.198086142539978,
        "learning_rate": 3.679678948033305e-06,
        "epoch": 0.9716018748276812,
        "step": 3524
    },
    {
        "loss": 2.0313,
        "grad_norm": 2.1598236560821533,
        "learning_rate": 3.6096828928047e-06,
        "epoch": 0.9718775847808105,
        "step": 3525
    },
    {
        "loss": 2.1031,
        "grad_norm": 1.8197828531265259,
        "learning_rate": 3.540346752574619e-06,
        "epoch": 0.9721532947339399,
        "step": 3526
    },
    {
        "loss": 1.6851,
        "grad_norm": 2.1431491374969482,
        "learning_rate": 3.4716710020376466e-06,
        "epoch": 0.9724290046870692,
        "step": 3527
    },
    {
        "loss": 1.9518,
        "grad_norm": 3.0003316402435303,
        "learning_rate": 3.403656111367115e-06,
        "epoch": 0.9727047146401985,
        "step": 3528
    },
    {
        "loss": 1.3554,
        "grad_norm": 2.1746368408203125,
        "learning_rate": 3.3363025462119114e-06,
        "epoch": 0.9729804245933278,
        "step": 3529
    },
    {
        "loss": 2.1723,
        "grad_norm": 1.972434163093567,
        "learning_rate": 3.2696107676933763e-06,
        "epoch": 0.9732561345464571,
        "step": 3530
    },
    {
        "loss": 2.3537,
        "grad_norm": 1.610708236694336,
        "learning_rate": 3.203581232402009e-06,
        "epoch": 0.9735318444995864,
        "step": 3531
    },
    {
        "loss": 1.3784,
        "grad_norm": 2.318835973739624,
        "learning_rate": 3.138214392394423e-06,
        "epoch": 0.9738075544527157,
        "step": 3532
    },
    {
        "loss": 1.0343,
        "grad_norm": 2.074735164642334,
        "learning_rate": 3.0735106951902294e-06,
        "epoch": 0.974083264405845,
        "step": 3533
    },
    {
        "loss": 1.144,
        "grad_norm": 3.4222099781036377,
        "learning_rate": 3.009470583769003e-06,
        "epoch": 0.9743589743589743,
        "step": 3534
    },
    {
        "loss": 2.8844,
        "grad_norm": 1.3250733613967896,
        "learning_rate": 2.946094496567209e-06,
        "epoch": 0.9746346843121036,
        "step": 3535
    },
    {
        "loss": 1.4648,
        "grad_norm": 1.7519360780715942,
        "learning_rate": 2.883382867475215e-06,
        "epoch": 0.974910394265233,
        "step": 3536
    },
    {
        "loss": 1.5118,
        "grad_norm": 1.9565633535385132,
        "learning_rate": 2.8213361258343284e-06,
        "epoch": 0.9751861042183623,
        "step": 3537
    },
    {
        "loss": 1.5196,
        "grad_norm": 2.2722649574279785,
        "learning_rate": 2.759954696433864e-06,
        "epoch": 0.9754618141714916,
        "step": 3538
    },
    {
        "loss": 2.2326,
        "grad_norm": 1.375978946685791,
        "learning_rate": 2.699238999508169e-06,
        "epoch": 0.9757375241246209,
        "step": 3539
    },
    {
        "loss": 1.7492,
        "grad_norm": 1.6379469633102417,
        "learning_rate": 2.639189450733892e-06,
        "epoch": 0.9760132340777502,
        "step": 3540
    },
    {
        "loss": 2.1321,
        "grad_norm": 0.9535170793533325,
        "learning_rate": 2.5798064612269636e-06,
        "epoch": 0.9762889440308795,
        "step": 3541
    },
    {
        "loss": 2.1671,
        "grad_norm": 1.4478555917739868,
        "learning_rate": 2.521090437539897e-06,
        "epoch": 0.9765646539840088,
        "step": 3542
    },
    {
        "loss": 2.3807,
        "grad_norm": 1.2952229976654053,
        "learning_rate": 2.4630417816589592e-06,
        "epoch": 0.9768403639371381,
        "step": 3543
    },
    {
        "loss": 2.2707,
        "grad_norm": 1.1169934272766113,
        "learning_rate": 2.405660891001471e-06,
        "epoch": 0.9771160738902674,
        "step": 3544
    },
    {
        "loss": 1.7848,
        "grad_norm": 1.9704828262329102,
        "learning_rate": 2.348948158413e-06,
        "epoch": 0.9773917838433968,
        "step": 3545
    },
    {
        "loss": 1.4969,
        "grad_norm": 2.0409815311431885,
        "learning_rate": 2.292903972164728e-06,
        "epoch": 0.9776674937965261,
        "step": 3546
    },
    {
        "loss": 2.243,
        "grad_norm": 1.2399390935897827,
        "learning_rate": 2.2375287159507986e-06,
        "epoch": 0.9779432037496554,
        "step": 3547
    },
    {
        "loss": 2.0473,
        "grad_norm": 1.349942922592163,
        "learning_rate": 2.1828227688856505e-06,
        "epoch": 0.9782189137027847,
        "step": 3548
    },
    {
        "loss": 1.5847,
        "grad_norm": 1.8337032794952393,
        "learning_rate": 2.1287865055014565e-06,
        "epoch": 0.978494623655914,
        "step": 3549
    },
    {
        "loss": 2.5279,
        "grad_norm": 1.155045986175537,
        "learning_rate": 2.075420295745567e-06,
        "epoch": 0.9787703336090433,
        "step": 3550
    },
    {
        "loss": 2.1127,
        "grad_norm": 1.575735092163086,
        "learning_rate": 2.022724504977924e-06,
        "epoch": 0.9790460435621726,
        "step": 3551
    },
    {
        "loss": 1.4038,
        "grad_norm": 1.8640704154968262,
        "learning_rate": 1.9706994939686086e-06,
        "epoch": 0.9793217535153019,
        "step": 3552
    },
    {
        "loss": 2.2408,
        "grad_norm": 1.0284639596939087,
        "learning_rate": 1.919345618895363e-06,
        "epoch": 0.9795974634684312,
        "step": 3553
    },
    {
        "loss": 1.8076,
        "grad_norm": 2.05600643157959,
        "learning_rate": 1.86866323134115e-06,
        "epoch": 0.9798731734215606,
        "step": 3554
    },
    {
        "loss": 1.9675,
        "grad_norm": 2.1483116149902344,
        "learning_rate": 1.8186526782917212e-06,
        "epoch": 0.9801488833746899,
        "step": 3555
    },
    {
        "loss": 2.2016,
        "grad_norm": 1.6938190460205078,
        "learning_rate": 1.7693143021332736e-06,
        "epoch": 0.9804245933278192,
        "step": 3556
    },
    {
        "loss": 1.8072,
        "grad_norm": 1.4863429069519043,
        "learning_rate": 1.7206484406501078e-06,
        "epoch": 0.9807003032809485,
        "step": 3557
    },
    {
        "loss": 2.0226,
        "grad_norm": 1.397093415260315,
        "learning_rate": 1.6726554270222627e-06,
        "epoch": 0.9809760132340778,
        "step": 3558
    },
    {
        "loss": 2.0755,
        "grad_norm": 2.1277825832366943,
        "learning_rate": 1.625335589823318e-06,
        "epoch": 0.9812517231872071,
        "step": 3559
    },
    {
        "loss": 2.0109,
        "grad_norm": 1.513252854347229,
        "learning_rate": 1.578689253018062e-06,
        "epoch": 0.9815274331403364,
        "step": 3560
    },
    {
        "loss": 1.7948,
        "grad_norm": 1.4591878652572632,
        "learning_rate": 1.5327167359603378e-06,
        "epoch": 0.9818031430934657,
        "step": 3561
    },
    {
        "loss": 1.8067,
        "grad_norm": 1.4633210897445679,
        "learning_rate": 1.4874183533908237e-06,
        "epoch": 0.982078853046595,
        "step": 3562
    },
    {
        "loss": 1.0015,
        "grad_norm": 2.4727766513824463,
        "learning_rate": 1.442794415434867e-06,
        "epoch": 0.9823545629997242,
        "step": 3563
    },
    {
        "loss": 2.1004,
        "grad_norm": 2.239736795425415,
        "learning_rate": 1.3988452276004095e-06,
        "epoch": 0.9826302729528535,
        "step": 3564
    },
    {
        "loss": 1.7589,
        "grad_norm": 1.9794350862503052,
        "learning_rate": 1.3555710907758535e-06,
        "epoch": 0.9829059829059829,
        "step": 3565
    },
    {
        "loss": 2.6493,
        "grad_norm": 1.2106356620788574,
        "learning_rate": 1.3129723012279992e-06,
        "epoch": 0.9831816928591122,
        "step": 3566
    },
    {
        "loss": 1.742,
        "grad_norm": 2.052584171295166,
        "learning_rate": 1.2710491506000455e-06,
        "epoch": 0.9834574028122415,
        "step": 3567
    },
    {
        "loss": 2.0849,
        "grad_norm": 1.8746217489242554,
        "learning_rate": 1.2298019259095795e-06,
        "epoch": 0.9837331127653708,
        "step": 3568
    },
    {
        "loss": 1.6253,
        "grad_norm": 2.2450904846191406,
        "learning_rate": 1.1892309095466126e-06,
        "epoch": 0.9840088227185001,
        "step": 3569
    },
    {
        "loss": 1.9017,
        "grad_norm": 2.5689072608947754,
        "learning_rate": 1.149336379271626e-06,
        "epoch": 0.9842845326716294,
        "step": 3570
    },
    {
        "loss": 1.6443,
        "grad_norm": 2.337101697921753,
        "learning_rate": 1.1101186082137061e-06,
        "epoch": 0.9845602426247587,
        "step": 3571
    },
    {
        "loss": 2.0446,
        "grad_norm": 1.3166860342025757,
        "learning_rate": 1.0715778648686558e-06,
        "epoch": 0.984835952577888,
        "step": 3572
    },
    {
        "loss": 2.5276,
        "grad_norm": 1.4963759183883667,
        "learning_rate": 1.0337144130971642e-06,
        "epoch": 0.9851116625310173,
        "step": 3573
    },
    {
        "loss": 1.9639,
        "grad_norm": 1.2599345445632935,
        "learning_rate": 9.96528512122963e-07,
        "epoch": 0.9853873724841467,
        "step": 3574
    },
    {
        "loss": 1.8393,
        "grad_norm": 2.0278258323669434,
        "learning_rate": 9.600204165311266e-07,
        "epoch": 0.985663082437276,
        "step": 3575
    },
    {
        "loss": 1.6294,
        "grad_norm": 1.898707389831543,
        "learning_rate": 9.241903762662429e-07,
        "epoch": 0.9859387923904053,
        "step": 3576
    },
    {
        "loss": 1.7123,
        "grad_norm": 2.1998019218444824,
        "learning_rate": 8.890386366307679e-07,
        "epoch": 0.9862145023435346,
        "step": 3577
    },
    {
        "loss": 1.9991,
        "grad_norm": 1.019705057144165,
        "learning_rate": 8.545654382833168e-07,
        "epoch": 0.9864902122966639,
        "step": 3578
    },
    {
        "loss": 2.3338,
        "grad_norm": 1.6650663614273071,
        "learning_rate": 8.207710172370319e-07,
        "epoch": 0.9867659222497932,
        "step": 3579
    },
    {
        "loss": 2.4495,
        "grad_norm": 2.095137119293213,
        "learning_rate": 7.876556048579286e-07,
        "epoch": 0.9870416322029225,
        "step": 3580
    },
    {
        "loss": 1.4939,
        "grad_norm": 1.9032851457595825,
        "learning_rate": 7.552194278633628e-07,
        "epoch": 0.9873173421560518,
        "step": 3581
    },
    {
        "loss": 2.1206,
        "grad_norm": 1.1133726835250854,
        "learning_rate": 7.234627083204548e-07,
        "epoch": 0.9875930521091811,
        "step": 3582
    },
    {
        "loss": 1.97,
        "grad_norm": 1.592962384223938,
        "learning_rate": 6.92385663644568e-07,
        "epoch": 0.9878687620623104,
        "step": 3583
    },
    {
        "loss": 1.7064,
        "grad_norm": 2.1105380058288574,
        "learning_rate": 6.619885065978104e-07,
        "epoch": 0.9881444720154398,
        "step": 3584
    },
    {
        "loss": 2.5816,
        "grad_norm": 1.0634232759475708,
        "learning_rate": 6.322714452876021e-07,
        "epoch": 0.9884201819685691,
        "step": 3585
    },
    {
        "loss": 2.3484,
        "grad_norm": 1.830872654914856,
        "learning_rate": 6.032346831652324e-07,
        "epoch": 0.9886958919216984,
        "step": 3586
    },
    {
        "loss": 1.3986,
        "grad_norm": 1.9223016500473022,
        "learning_rate": 5.748784190244827e-07,
        "epoch": 0.9889716018748277,
        "step": 3587
    },
    {
        "loss": 2.1157,
        "grad_norm": 1.7013421058654785,
        "learning_rate": 5.472028470002388e-07,
        "epoch": 0.989247311827957,
        "step": 3588
    },
    {
        "loss": 1.8863,
        "grad_norm": 2.2985715866088867,
        "learning_rate": 5.202081565671924e-07,
        "epoch": 0.9895230217810863,
        "step": 3589
    },
    {
        "loss": 2.1015,
        "grad_norm": 1.9649122953414917,
        "learning_rate": 4.93894532538508e-07,
        "epoch": 0.9897987317342156,
        "step": 3590
    },
    {
        "loss": 1.7493,
        "grad_norm": 2.22973370552063,
        "learning_rate": 4.682621550646138e-07,
        "epoch": 0.9900744416873449,
        "step": 3591
    },
    {
        "loss": 2.4975,
        "grad_norm": 1.3516446352005005,
        "learning_rate": 4.433111996319128e-07,
        "epoch": 0.9903501516404742,
        "step": 3592
    },
    {
        "loss": 1.7493,
        "grad_norm": 3.3741352558135986,
        "learning_rate": 4.1904183706160674e-07,
        "epoch": 0.9906258615936036,
        "step": 3593
    },
    {
        "loss": 1.9252,
        "grad_norm": 2.0246922969818115,
        "learning_rate": 3.9545423350851873e-07,
        "epoch": 0.9909015715467329,
        "step": 3594
    },
    {
        "loss": 2.037,
        "grad_norm": 1.9908329248428345,
        "learning_rate": 3.725485504599502e-07,
        "epoch": 0.9911772814998622,
        "step": 3595
    },
    {
        "loss": 2.179,
        "grad_norm": 1.6594969034194946,
        "learning_rate": 3.5032494473462576e-07,
        "epoch": 0.9914529914529915,
        "step": 3596
    },
    {
        "loss": 2.4176,
        "grad_norm": 1.4000924825668335,
        "learning_rate": 3.287835684815166e-07,
        "epoch": 0.9917287014061208,
        "step": 3597
    },
    {
        "loss": 0.4885,
        "grad_norm": 1.6139792203903198,
        "learning_rate": 3.07924569178919e-07,
        "epoch": 0.9920044113592501,
        "step": 3598
    },
    {
        "loss": 1.3151,
        "grad_norm": 1.8510408401489258,
        "learning_rate": 2.87748089633344e-07,
        "epoch": 0.9922801213123794,
        "step": 3599
    },
    {
        "loss": 2.024,
        "grad_norm": 1.892707109451294,
        "learning_rate": 2.682542679786071e-07,
        "epoch": 0.9925558312655087,
        "step": 3600
    },
    {
        "loss": 1.8186,
        "grad_norm": 1.7563066482543945,
        "learning_rate": 2.4944323767485124e-07,
        "epoch": 0.992831541218638,
        "step": 3601
    },
    {
        "loss": 2.0334,
        "grad_norm": 1.7652803659439087,
        "learning_rate": 2.313151275076364e-07,
        "epoch": 0.9931072511717673,
        "step": 3602
    },
    {
        "loss": 2.2688,
        "grad_norm": 1.80357825756073,
        "learning_rate": 2.1387006158704037e-07,
        "epoch": 0.9933829611248967,
        "step": 3603
    },
    {
        "loss": 2.0163,
        "grad_norm": 2.4194729328155518,
        "learning_rate": 1.9710815934688153e-07,
        "epoch": 0.993658671078026,
        "step": 3604
    },
    {
        "loss": 2.3884,
        "grad_norm": 1.1529340744018555,
        "learning_rate": 1.8102953554380853e-07,
        "epoch": 0.9939343810311553,
        "step": 3605
    },
    {
        "loss": 1.6214,
        "grad_norm": 1.9351396560668945,
        "learning_rate": 1.6563430025655634e-07,
        "epoch": 0.9942100909842845,
        "step": 3606
    },
    {
        "loss": 2.574,
        "grad_norm": 1.2357418537139893,
        "learning_rate": 1.5092255888521366e-07,
        "epoch": 0.9944858009374138,
        "step": 3607
    },
    {
        "loss": 2.2308,
        "grad_norm": 1.563753366470337,
        "learning_rate": 1.3689441215044562e-07,
        "epoch": 0.9947615108905431,
        "step": 3608
    },
    {
        "loss": 1.7144,
        "grad_norm": 2.017179012298584,
        "learning_rate": 1.2354995609288321e-07,
        "epoch": 0.9950372208436724,
        "step": 3609
    },
    {
        "loss": 1.2816,
        "grad_norm": 1.8873552083969116,
        "learning_rate": 1.1088928207236838e-07,
        "epoch": 0.9953129307968017,
        "step": 3610
    },
    {
        "loss": 2.0424,
        "grad_norm": 1.867916226387024,
        "learning_rate": 9.89124767674321e-08,
        "epoch": 0.995588640749931,
        "step": 3611
    },
    {
        "loss": 2.0496,
        "grad_norm": 1.0573762655258179,
        "learning_rate": 8.761962217460617e-08,
        "epoch": 0.9958643507030603,
        "step": 3612
    },
    {
        "loss": 2.407,
        "grad_norm": 1.0336463451385498,
        "learning_rate": 7.701079560793467e-08,
        "epoch": 0.9961400606561897,
        "step": 3613
    },
    {
        "loss": 1.6049,
        "grad_norm": 1.689035177230835,
        "learning_rate": 6.708606969838549e-08,
        "epoch": 0.996415770609319,
        "step": 3614
    },
    {
        "loss": 2.384,
        "grad_norm": 0.8463537096977234,
        "learning_rate": 5.7845512393428504e-08,
        "epoch": 0.9966914805624483,
        "step": 3615
    },
    {
        "loss": 2.3323,
        "grad_norm": 1.7173316478729248,
        "learning_rate": 4.9289186956480435e-08,
        "epoch": 0.9969671905155776,
        "step": 3616
    },
    {
        "loss": 1.0375,
        "grad_norm": 3.5764126777648926,
        "learning_rate": 4.1417151966527404e-08,
        "epoch": 0.9972429004687069,
        "step": 3617
    },
    {
        "loss": 1.9777,
        "grad_norm": 1.8057458400726318,
        "learning_rate": 3.4229461317725195e-08,
        "epoch": 0.9975186104218362,
        "step": 3618
    },
    {
        "loss": 2.2174,
        "grad_norm": 1.5100804567337036,
        "learning_rate": 2.772616421899965e-08,
        "epoch": 0.9977943203749655,
        "step": 3619
    },
    {
        "loss": 2.1654,
        "grad_norm": 1.2389061450958252,
        "learning_rate": 2.1907305193757943e-08,
        "epoch": 0.9980700303280948,
        "step": 3620
    },
    {
        "loss": 1.0395,
        "grad_norm": 2.2415640354156494,
        "learning_rate": 1.6772924079511145e-08,
        "epoch": 0.9983457402812241,
        "step": 3621
    },
    {
        "loss": 1.4992,
        "grad_norm": 1.4817131757736206,
        "learning_rate": 1.232305602766326e-08,
        "epoch": 0.9986214502343534,
        "step": 3622
    },
    {
        "loss": 1.3352,
        "grad_norm": 2.1132097244262695,
        "learning_rate": 8.557731503278099e-09,
        "epoch": 0.9988971601874828,
        "step": 3623
    },
    {
        "loss": 2.3749,
        "grad_norm": 1.754172921180725,
        "learning_rate": 5.4769762847795e-09,
        "epoch": 0.9991728701406121,
        "step": 3624
    },
    {
        "loss": 2.0075,
        "grad_norm": 1.945261001586914,
        "learning_rate": 3.0808114639180317e-09,
        "epoch": 0.9994485800937414,
        "step": 3625
    },
    {
        "loss": 2.048,
        "grad_norm": 1.3098301887512207,
        "learning_rate": 1.3692534454712303e-09,
        "epoch": 0.9997242900468707,
        "step": 3626
    },
    {
        "loss": 2.2064,
        "grad_norm": 2.1578822135925293,
        "learning_rate": 3.4231394726580437e-10,
        "epoch": 1.0,
        "step": 3627
    },
    {
        "train_runtime": 8975.3056,
        "train_samples_per_second": 0.808,
        "train_steps_per_second": 0.404,
        "total_flos": 1.3395552223973376e+17,
        "train_loss": 2.0455463875546283,
        "epoch": 1.0,
        "step": 3627
    }
]